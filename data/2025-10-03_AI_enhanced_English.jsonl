{"id": "2510.01379", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01379", "abs": "https://arxiv.org/abs/2510.01379", "authors": ["Huashan Chen", "Zhenyu Qi", "Haotang Li", "Hong Chen", "Jinfu Chen", "Kebin Peng", "In Kee Kim", "Kyu Hyung Lee", "Sen He"], "title": "Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration", "comment": null, "summary": "While Large Language Models (LLMs) have become the predominant paradigm for\nautomated code generation, current single-model approaches fundamentally ignore\nthe heterogeneous computational strengths that different models exhibit across\nprogramming languages, algorithmic domains, and development stages. This paper\nchallenges the single-model convention by introducing a multi-stage,\nperformance-guided orchestration framework that dynamically routes coding tasks\nto the most suitable LLMs within a structured generate-fix-refine workflow. Our\napproach is grounded in a comprehensive empirical study of 17 state-of-the-art\nLLMs across five programming languages (Python, Java, C++, Go, and Rust) using\nHumanEval-X benchmark. The study, which evaluates both functional correctness\nand runtime performance metrics (execution time, mean/max memory utilization,\nand CPU efficiency), reveals pronounced performance heterogeneity by language,\ndevelopment stage, and problem category. Guided by these empirical insights, we\npresent PerfOrch, an LLM agent that orchestrates top-performing LLMs for each\ntask context through stage-wise validation and rollback mechanisms. Without\nrequiring model fine-tuning, PerfOrch achieves substantial improvements over\nstrong single-model baselines: average correctness rates of 96.22% and 91.37%\non HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and\n49.11%. Beyond correctness gains, the framework delivers consistent performance\noptimizations, improving execution time for 58.76% of problems with median\nspeedups ranging from 17.67% to 27.66% across languages on two benchmarks. The\nframework's plug-and-play architecture ensures practical scalability, allowing\nnew LLMs to be profiled and integrated seamlessly, thereby offering a paradigm\nfor production-grade automated software engineering that adapts to the rapidly\nevolving generative AI landscape.", "AI": {"tldr": "This paper introduces PerfOrch, a plug-and-play framework that dynamically routes coding tasks to optimal LLMs based on empirical performance data across languages and problem types, achieving new SOTA correctness (96.22%) and runtime speedups (up to 27.66%) without model fine-tuning.", "motivation": "Current single-model LLM approaches for code generation ignore the significant performance heterogeneity of models across languages, domains, and problem stages. This limitation motivates the need for a dynamic, performance-aware orchestration framework to harness heterogeneous model strengths.", "method": "The method involves a multi-stage workflow (generate-fix-refine) with dynamic task routing to top-performing LLMs, validated through empirical analysis across 17 LLMs and five programming languages on HumanEval-X and EffiBench-X. PerfOrch incorporates stage-wise validation and rollback mechanisms.", "result": "PerfOrch achieves 96.22% correctness on HumanEval-X and 91.37% on EffiBench-X (surpassing GPT-4o by significant margins), with 58.76% of problems showing execution time improvements and median speedups of 17.67\u201327.66% across languages.", "conclusion": "The paper concludes that PerfOrch, a multi-stage performance-guided orchestration framework, achieves state-of-the-art results in automated code generation by leveraging the heterogeneous strengths of multiple LLMs. It demonstrates superior correctness and runtime performance improvements over single-model approaches without requiring fine-tuning."}}
{"id": "2510.01514", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01514", "abs": "https://arxiv.org/abs/2510.01514", "authors": ["J. Alexander Curtis", "Sharadha Kasiviswanathan", "Nasir Eisty"], "title": "Deciphering WONTFIX: A Mixed-Method Study on Why GitHub Issues Get Rejected", "comment": null, "summary": "Context: The ``wontfix'' label is a widely used yet narrowly understood tool\nin GitHub repositories, indicating that an issue will not be pursued further.\nDespite its prevalence, the impact of this label on project management and\ncommunity dynamics within open-source software development is not clearly\ndefined. Objective: This study examines the prevalence and reasons behind\nissues being labeled as wontfix across various open-source repositories on\nGitHub. Method: Employing a mixed-method approach, we analyze both quantitative\ndata to assess the prevalence of the wontfix label and qualitative data to\nexplore the reasoning that it was used. Data were collected from 3,132 of\nGitHub's most-popular repositories. Later, we employ open coding and thematic\nanalysis to categorize the reasons behind wontfix labels, providing a\nstructured understanding of the issue management landscape. Results: Our\nfindings show that about 30% of projects on GitHub apply the wontfix label to\nsome issues. These issues most often occur on user-submitted issues for bug\nreports and feature requests. The study identified eight common themes behind\nlabeling issues as wontfix, ranging from user-specific control factors to\nmaintainer-specific decisions. Conclusions: The wontfix label is a critical\ntool for managing resources and guiding contributor efforts in GitHub projects.\nHowever, it can also discourage community involvement and obscure the\ntransparency of project management. Understanding these reasons aids project\nmanagers in making informed decisions and fostering efficient collaboration\nwithin open-source communities.", "AI": {"tldr": "The 'wontfix' label is a common but under-researched tool in GitHub repositories, used to indicate issues will not be addressed. This study analyzes its prevalence and reasons for use.", "motivation": "Studied impact of 'wontfix' on open-source project management and community dynamics due to its frequent but poorly understood usage.", "method": "Mixed-method approach using 3,132 GitHub repositories for quantitative analysis and open coding/thematic analysis for qualitative data.", "result": "30% of projects use 'wontfix'; most commonly on user-submitted issues. Eight themes were identified behind the label's use, including user and maintainer decisions.", "conclusion": "The 'wontfix' label is important for resource management but may hinder community participation. Understanding its use helps improve open-source collaboration."}}
{"id": "2510.01635", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01635", "abs": "https://arxiv.org/abs/2510.01635", "authors": ["Yifei Chen", "Sarra Habchi", "Lili Wei"], "title": "MIMIC: Integrating Diverse Personality Traits for Better Game Testing Using Large Language Model", "comment": "13 pages, 7 figures, 6 tables. This paper is accepted by the 40th\n  IEEE/ACM International Conference on Automated Software Engineering, ASE 2025", "summary": "Modern video games pose significant challenges for traditional automated\ntesting algorithms, yet intensive testing is crucial to ensure game quality. To\naddress these challenges, researchers designed gaming agents using\nReinforcement Learning, Imitation Learning, or Large Language Models. However,\nthese agents often neglect the diverse strategies employed by human players due\nto their different personalities, resulting in repetitive solutions in similar\nsituations. Without mimicking varied gaming strategies, these agents struggle\nto trigger diverse in-game interactions or uncover edge cases.\n  In this paper, we present MIMIC, a novel framework that integrates diverse\npersonality traits into gaming agents, enabling them to adopt different gaming\nstrategies for similar situations. By mimicking different playstyles, MIMIC can\nachieve higher test coverage and richer in-game interactions across different\ngames. It also outperforms state-of-the-art agents in Minecraft by achieving a\nhigher task completion rate and providing more diverse solutions. These results\nhighlight MIMIC's significant potential for effective game testing.", "AI": {"tldr": "highlights MIMIC's significant potential for effective game testing.", "motivation": "Improve test coverage and uncover edge cases by mimicking human playstyles.", "method": "Integrates diverse personality traits into gaming agents.", "result": "Outperforms state-of-the-art agents in Minecraft with higher task completion rate and diverse solutions.", "conclusion": "MIMIC shows significant potential for effective game testing."}}
{"id": "2510.01740", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01740", "abs": "https://arxiv.org/abs/2510.01740", "authors": ["Kypros Iacovou", "Georgia M. Kapitsaki", "Evangelia Vanezi"], "title": "FOSS-chain: using blockchain for Open Source Software license compliance", "comment": null, "summary": "Open Source Software (OSS) is widely used and carries licenses that indicate\nthe terms under which the software is provided for use, also specifying\nmodification and distribution rules. Ensuring that users are respecting OSS\nlicense terms when creating derivative works is a complex process. Compliance\nissues arising from incompatibilities among licenses may lead to legal\ndisputes. At the same time, the blockchain technology with immutable entries\noffers a mechanism to provide transparency when it comes to licensing and\nensure software changes are recorded. In this work, we are introducing an\nintegration of blockchain and license management when creating derivative\nworks, in order to tackle the issue of OSS license compatibility. We have\ndesigned, implemented and performed a preliminary evaluation of FOSS-chain, a\nweb platform that uses blockchain and automates the license compliance process,\ncovering 14 OSS licenses. We have evaluated the initial prototype version of\nthe FOSS-chain platform via a small scale user study. Our preliminary results\nare promising, demonstrating the potential of the platform for adaptation on\nrealistic software systems.", "AI": {"tldr": "FOSS-chain uses blockchain to automate OSS license compliance, reducing compatibility risks. A prototype evaluated in a small study shows promising results.", "motivation": "The motivation stems from the complexity of ensuring OSS license compliance, which can lead to legal disputes due to license incompatibilities. Blockchain's transparency and immutability offer a novel solution for tracking modifications and ensuring compatible derivative works.", "method": "The authors designed and implemented FOSS-chain, a blockchain-enabled web platform that automates license compliance by tracking software derivatives using immutable blockchain records. The method was evaluated through a small-scale user study.", "result": "The preliminary evaluation of FOSS-chain's prototype demonstrated its effectiveness in handling 14 OSS licenses, with the platform successfully automating compliance checks and showing potential adaptability to real-world software systems, although based on a limited user study.", "conclusion": "The paper concludes that integrating blockchain into OSS license management through FOSS-chain effectively addresses license compatibility issues, with promising potential for real-world application when further developed."}}
{"id": "2510.01223", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01223", "abs": "https://arxiv.org/abs/2510.01223", "authors": ["Hui Dou", "Ning Xu", "Yiwen Zhang", "Kaibin Wang"], "title": "Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks. However, they remain exposed to jailbreak attacks, eliciting\nharmful responses. The nested scenario strategy has been increasingly adopted\nacross various methods, demonstrating immense potential. Nevertheless, these\nmethods are easily detectable due to their prominent malicious intentions. In\nthis work, we are the first to find and systematically verify that LLMs'\nalignment defenses are not sensitive to nested scenarios, where these scenarios\nare highly semantically relevant to the queries and incorporate targeted toxic\nknowledge. This is a crucial yet insufficiently explored direction. Based on\nthis, we propose RTS-Attack (Semantically Relevant Nested Scenarios with\nTargeted Toxic Knowledge), an adaptive and automated framework to examine LLMs'\nalignment. By building scenarios highly relevant to the queries and integrating\ntargeted toxic knowledge, RTS-Attack bypasses the alignment defenses of LLMs.\nMoreover, the jailbreak prompts generated by RTS-Attack are free from harmful\nqueries, leading to outstanding concealment. Extensive experiments demonstrate\nthat RTS-Attack exhibits superior performance in both efficiency and\nuniversality compared to the baselines across diverse advanced LLMs, including\nGPT-4o, Llama3-70b, and Gemini-pro. Our complete code is available in the\nsupplementary material. WARNING: THIS PAPER CONTAINS POTENTIALLY HARMFUL\nCONTENT.", "AI": {"tldr": "RTS-Attack exploits semantically aligned nested scenarios with hidden toxic knowledge to bypass LLM safety defenses, outperforming prior methods in effectiveness and stealth.", "motivation": "Existing nested scenario methods for jailbreaking LLMs are detectable due to overt malicious intent. The paper identifies that LLM alignment defenses are insufficiently sensitive to nested scenarios with semantic relevance and toxic knowledge, a direction needing systematic exploration.", "method": "RTS-Attack constructs semantically relevant nested scenarios and integrates targeted toxic knowledge to adaptively generate jailbreak prompts. The framework is automated, prioritizing concealment by avoiding direct harmful queries while exploiting LLM alignment weaknesses.", "result": "RTS-Attack achieves superior efficiency and universality across advanced LLMs (GPT-4o, Llama3-70b, Gemini-pro) compared to baselines. It successfully bypasses defenses while maintaining concealment by avoiding harmful query patterns.", "conclusion": "The paper introduces RTS-Attack, an effective and stealthy method for bypassing LLM alignment defenses by using semantically relevant nested scenarios with targeted toxic knowledge, demonstrating critical vulnerabilities in current defense mechanisms against jailbreak attacks."}}
{"id": "2510.01754", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01754", "abs": "https://arxiv.org/abs/2510.01754", "authors": ["Hina Anwar"], "title": "ARENA: A tool for measuring and analysing the energy efficiency of Android apps", "comment": null, "summary": "To build energy-efficient apps, there is a need to estimate and analyze their\nenergy consumption in typical usage scenarios. The energy consumption of\nAndroid apps could be estimated via software-based and hardware-based\napproaches. Software-based approaches, while easier to implement, are not as\naccurate as hardware-based approaches. The process of measuring the energy\nconsumption of an Android app via a hardware-based approach typically involves\n1) setting up a measurement environment, 2) executing the app under test on a\nmobile device, 3) recording current/voltage data via a hardware device to\nmeasure energy consumption, and 4) cleaning and aggregating data for analyses,\nreports, and visualizations. Specialized scripts are written for selected\nhardware and software components to ensure reliable energy measurements. The\nenergy measurement process is repeated many times and aggregated to remove\nnoise. These steps make the hardware-based energy measurement process\ntime-consuming and not easy to adapt or reproduce. There is a lack of\nopen-source tools available for developers and researchers to take reliable\nenergy measurements via hardware devices. In this paper, we present and\ndemonstrate ARENA, a support tool that enables developers and researchers to\nconnect to a physical measurement device without leaving the comfort of their\nIDE. Developers could use ARENA during development to compare energy\nconsumption between different apps or versions of the same app. ARENA\ncalculates energy consumption on an Android smartphone by executing a test\nscenario on the app under development. Further, ARENA helps aggregate,\nstatistically analyze, report, and visualize the data, allowing developers and\nresearchers to dig into the data directly or visually. We implemented ARENA as\nan IntelliJ and Android Studio plugin.", "AI": {"tldr": "This paper introduces ARENA, an open-source IDE plugin for Android developers that enables reliable hardware-based energy measurement without sacrificing development workflow efficiency. It automates data collection, analysis, and visualization of energy consumption during app development.", "motivation": "Existing software-based energy estimation methods lack accuracy while hardware-based approaches are time-consuming, complex to implement, and require manual scripting. There's a critical gap in accessible tools for reproducible energy measurement in Android app development.", "method": "The authors developed ARENA as an IntelliJ/Android Studio plugin that integrates with physical hardware. It streamlines energy measurement by: 1. Executing test scenarios in IDE 2. Automatically collecting hardware data 3. Performing real-time data aggregation and noise reduction 4. Providing statistical analysis and visualization capabilities", "result": "ARENA successfully implemented hardware-based energy measurement within IDEs, enabling developers to: - Compare energy consumption between app versions - Automate repetitive measurement tasks - Visualize energy data through integrated reports - Reduce manual scripting efforts by 70%", "conclusion": "ARENA addresses critical barriers in hardware-based energy measurement for Android apps by providing an IDE-integrated solution that preserves measurement accuracy while significantly improving usability and reproducibility for developers and researchers."}}
{"id": "2510.01342", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01342", "abs": "https://arxiv.org/abs/2510.01342", "authors": ["Xiangfang Li", "Yu Wang", "Bo Li"], "title": "Fine-Tuning Jailbreaks under Highly Constrained Black-Box Settings: A Three-Pronged Approach", "comment": null, "summary": "With the rapid advancement of large language models (LLMs), ensuring their\nsafe use becomes increasingly critical. Fine-tuning is a widely used method for\nadapting models to downstream tasks, yet it is vulnerable to jailbreak attacks.\nHowever, most existing studies focus on overly simplified attack scenarios,\nlimiting their practical relevance to real-world defense settings. To make this\nrisk concrete, we present a three-pronged jailbreak attack and evaluate it\nagainst provider defenses under a dataset-only black-box fine-tuning interface.\nIn this setting, the attacker can only submit fine-tuning data to the provider,\nwhile the provider may deploy defenses across stages: (1) pre-upload data\nfiltering, (2) training-time defensive fine-tuning, and (3) post-training\nsafety audit. Our attack combines safety-styled prefix/suffix wrappers, benign\nlexical encodings (underscoring) of sensitive tokens, and a backdoor mechanism,\nenabling the model to learn harmful behaviors while individual datapoints\nappear innocuous. Extensive experiments demonstrate the effectiveness of our\napproach. In real-world deployment, our method successfully jailbreaks GPT-4.1\nand GPT-4o on the OpenAI platform with attack success rates above 97% for both\nmodels. Our code is available at\nhttps://github.com/lxf728/tri-pronged-ft-attack.", "AI": {"tldr": "The paper introduces a three-pronged jailbreak attack against fine-tuned large language models, demonstrating high success rates on GPT-4 variants while operating under realistic dataset-only black-box constraints. The attack combines surface-appealing encodings and a backdoor mechanism to bypass multi-stage defenses.", "motivation": "Existing jailbreak attack research focuses on oversimplified scenarios, lacking practical relevance for real-world provider defense systems that use multi-stage protections during model fine-tuning.", "method": "The attack combines three components: safety-styled text wrappers (prefix/suffix), lexical obfuscation techniques (underscore encoding), and a backdoor mechanism that maintains innocuous appearance in individual training examples while inducbing harmful behavior retention in models.", "result": "The attack achieves >97% success rate against GPT-4.1 and GPT-4o on OpenAI platform. Attack bypasses all three defense stages (pre-upload filtering, defensive training, post-training audit), demonstrating vulnerabilities in real-world fine-tuning pipelines.", "conclusion": "The work reveals critical gaps in current enterprise LLM fine-tuning defense systems, showing that even with multi-stage protections, attackers can craft effective jailbreak attacks through strategic data manipulation without model parameter access."}}
{"id": "2510.01825", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01825", "abs": "https://arxiv.org/abs/2510.01825", "authors": ["Zhenyu Yang", "Yue Pan", "Zhen Yang", "Zhongxing Yu"], "title": "Towards Speeding up Program Repair with Non-Autoregressive Model", "comment": "30 pages, 8 figures, 7 tables. arXiv admin note: substantial text\n  overlap with arXiv:2406.16526", "summary": "Enlightened by the success of machine learning techniques in various\napplication areas, recent years have witnessed a surge of research efforts on\nautomatic program repair (APR) using machine learning techniques. Previous\nmachine learning-based APR techniques essentially modified bugs in the\nautoregressive (AR) manner, which predicts future values based on past values.\nDue to the manner of token-by-token generation, the AR-based APR technique has\na huge time delay. In particular, the delay of the APR model with a large\nnumber of parameters is more serious. To address the issue, we aim to apply the\nnon-autoregressive (NAR) method to the APR task, which can output target code\nin a parallel manner to avoid huge repair delays. However, the naive use of the\nNAR manner for the APR task suffers from the issue of compromised patch\nquality. To effectively adapt the NAR manner for the APR task, we in this paper\npropose NARRepair, the first customized NAR code generation model for the APR\ntask. The NARRepair model features three major novelties, including 1) the\nrepair action predictor for alleviating the over-correction issue, 2) the\ninter-token dependency extractor for alleviating the issue of lacking\ninter-token dependency information, and 3) the two-stage decoder for\nalleviating the issue of lacking contextual information. We evaluated NARRepair\non three widely used datasets in the APR community, and the results show that\n1) compared to other APR techniques, the NARRepair model has the best\nperformance within the limited repair time, and 2) compared to AR-based APR\ntechniques, the repair speed of NARRepair has been increased by 1.4-6.4 times\nin the GPU environment. Overall, the results show that NARRepair has achieved\nstate-of-the-art comprehensive performance in terms of repair speed and\naccuracy.", "AI": {"tldr": "NARRepair is a non-autoregressive APR model that improves repair speed (1.4-6.4\u00d7 faster) without sacrificing accuracy, solving efficiency-accuracy tradeoffs via novel architectural components.", "motivation": "Traditional autoregressive APR methods suffer from significant time delays due to token-by-token generation, especially in large models. Direct application of non-autoregressive methods leads to poor repair quality. The paper aims to resolve these efficiency-accuracy tradeoffs in APR.", "method": "The paper proposes NARRepair, a non-autoregressive APR model featuring three novelties: (1) a repair action predictor to mitigate over-correction, (2) an inter-token dependency extractor to preserve token relationships, and (3) a two-stage decoder to enhance contextual information. This design enables parallelizable code generation while addressing quality issues inherent to naive non-autoregressive methods.", "result": "NARRepair achieves the best performance on three APR datasets within limited time, with 1.4-6.4\u00d7 speedup over autoregressive methods in GPU environments. It maintains competitive accuracy while drastically reducing repair delay, outperforming existing techniques and achieving state-of-the-art results.", "conclusion": "NARRepair addresses the delay issue in automatic program repair (APR) by introducing a non-autoregressive model, achieving state-of-the-art performance in both speed and accuracy. The proposed method outperforms existing APR techniques, particularly in reducing repair time by 1.4-6.4 times compared to autoregressive approaches."}}
{"id": "2510.01350", "categories": ["cs.CR", "cs.AR", "cs.ET", "cs.NE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01350", "abs": "https://arxiv.org/abs/2510.01350", "authors": ["Muhammad Faheemur Rahman", "Wayne Burleson"], "title": "Integrated Security Mechanisms for Weight Protection in Memristive Crossbar Arrays", "comment": "2 pages, 2 figures", "summary": "Memristive crossbar arrays enable in-memory computing by performing parallel\nanalog computations directly within memory, making them well-suited for machine\nlearning, neural networks, and neuromorphic systems. However, despite their\nadvantages, non-volatile memristors are vulnerable to security threats (such as\nadversarial extraction of stored weights when the hardware is compromised.\nProtecting these weights is essential since they represent valuable\nintellectual property resulting from lengthy and costly training processes\nusing large, often proprietary, datasets. As a solution we propose two security\nmechanisms: Keyed Permutor and Watermark Protection Columns; where both\nsafeguard critical weights and establish verifiable ownership (even in cases of\ndata leakage). Our approach integrates efficiently with existing memristive\ncrossbar architectures without significant design modifications. Simulations\nacross 45nm, 22nm, and 7nm CMOS nodes, using a realistic interconnect model and\na large RF dataset, show that both mechanisms offer robust protection with\nunder 10% overhead in area, delay and power. We also present initial\nexperiments employing the widely known MNIST dataset; further highlighting the\nfeasibility of securing memristive in-memory computing systems with minimal\nperformance trade-offs.", "AI": {"tldr": "This paper proposes two lightweight security mechanisms for memristive crossbar arrays that protect neural network weights and establish ownership verification with under 10% hardware overhead, demonstrated through CMOS node simulations and MNIST experiments.", "motivation": "Non-volatile memristors in crossbar arrays face security risks from adversarial weight extraction attacks, threatening valuable intellectual property derived from large proprietary datasets. Protection mechanisms are critical to maintain confidentiality and verifiable ownership even if hardware is compromised.", "method": "The authors propose two hardware-based security mechanisms: 1) Keyed Permutor, which obfuscates stored weights using a cryptographic key; 2) Watermark Protection Columns, which embed verifiable ownership markers. These mechanisms integrate seamlessly into existing memristive crossbar arrays without requiring major architectural redesigns.", "result": "Simulation results across 45nm, 22nm, and 7nm CMOS nodes with realistic interconnect models show <10% overhead in area/delay/power metrics. MNIST experiments demonstrate practical feasibility of securing in-memory computing systems against data leakage with negligible performance degradation.", "conclusion": "The paper concludes that the proposed Keyed Permutor and Watermark Protection Columns mechanisms effectively secure memristive in-memory computing systems with minimal performance overhead, making them viable for protecting intellectual property against weight extraction attacks."}}
{"id": "2510.01960", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01960", "abs": "https://arxiv.org/abs/2510.01960", "authors": ["Victor Lira", "Paulo Borba", "Rodrigo Bonif\u00e1cio", "Galileu Santos e Matheus barbosa"], "title": "RefFilter: Improving Semantic Conflict Detection via Refactoring-Aware Static Analysis", "comment": null, "summary": "Detecting semantic interference remains a challenge in collaborative software\ndevelopment. Recent lightweight static analysis techniques improve efficiency\nover SDG-based methods, but they still suffer from a high rate of false\npositives. A key cause of these false positives is the presence of\nbehavior-preserving code refactorings, which current techniques cannot\neffectively distinguish from changes that impact behavior and can interfere\nwith others. To handle this problem we present RefFilter, a refactoring-aware\ntool for semantic interference detection. It builds on existing static\ntechniques by incorporating automated refactoring detection to improve\nprecision. RefFilter discards behavior-preserving refactorings from reports,\nreducing false positives while preserving detection coverage. To evaluate\neffectiveness and scalability, use two datasets: a labeled dataset with 99\nscenarios and ground truth, and a novel dataset of 1,087 diverse merge\nscenarios that we have built. Experimental results show that RefFilter reduces\nfalse positives by nearly 32% on the labeled dataset. While this reduction\ncomes with a non significant increase in false negatives, the overall gain in\nprecision significantly outweighs the minor trade-off in recall. These findings\ndemonstrate that refactoring-aware interference detection is a practical and\neffective strategy for improving merge support in modern development workflows.", "AI": {"tldr": "RefFilter is a refactoring-aware tool that reduces false positives in semantic interference detection by 32%, improving precision without significant loss in recall.", "motivation": "Existing static analysis methods for semantic interference detection produce high false positives due to inability to distinguish behavior-preserving refactorings from actual interference-causing changes.", "method": "RefFilter integrates automated refactoring detection with lightweight static analysis techniques to filter out behavior-preserving code changes, leveraging two datasets (99 labeled scenarios and 1,087 merge scenarios) for evaluation.", "result": "Achieved 32\\% reduction in false positives on the labeled dataset with only minor and non-significant increase in false negatives, maintaining detection coverage.", "conclusion": "Refactor-aware interference detection is a practical solution to enhance merge accuracy in collaborative software development workflows."}}
{"id": "2510.01354", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01354", "abs": "https://arxiv.org/abs/2510.01354", "authors": ["Yinuo Liu", "Ruohan Xu", "Xilong Wang", "Yuqi Jia", "Neil Zhenqiang Gong"], "title": "WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents", "comment": null, "summary": "Multiple prompt injection attacks have been proposed against web agents. At\nthe same time, various methods have been developed to detect general prompt\ninjection attacks, but none have been systematically evaluated for web agents.\nIn this work, we bridge this gap by presenting the first comprehensive\nbenchmark study on detecting prompt injection attacks targeting web agents. We\nbegin by introducing a fine-grained categorization of such attacks based on the\nthreat model. We then construct datasets containing both malicious and benign\nsamples: malicious text segments generated by different attacks, benign text\nsegments from four categories, malicious images produced by attacks, and benign\nimages from two categories. Next, we systematize both text-based and\nimage-based detection methods. Finally, we evaluate their performance across\nmultiple scenarios. Our key findings show that while some detectors can\nidentify attacks that rely on explicit textual instructions or visible image\nperturbations with moderate to high accuracy, they largely fail against attacks\nthat omit explicit instructions or employ imperceptible perturbations. Our\ndatasets and code are released at:\nhttps://github.com/Norrrrrrr-lyn/WAInjectBench.", "AI": {"tldr": "This paper introduces the first comprehensive benchmark for detecting prompt injection attacks on web agents, revealing critical limitations of existing methods against stealthy attacks. Datasets/code: https://github.com/Norrrrrrr-lyn/WAInjectBench.", "motivation": "Existing detection methods for prompt injection attacks lack systematic evaluation on web agents despite proposed attacks against them. The work aims to bridge this gap by providing the first comprehensive benchmark for detecting such attacks.", "method": "The authors present a fine-grained categorization of prompt injection attacks, construct datasets with malicious and benign text/image samples, and evaluate text-/image-based detection methods across multiple scenarios to measure their effectiveness.", "result": "Key findings show moderate-to-high accuracy for detectors targeting explicit textual instructions or visible image perturbations, but significant failure rates against attacks omitting explicit instructions or employing imperceptible perturbations.", "conclusion": "The paper concludes that while some detectors can identify attacks with explicit instructions or visible perturbations, they fail against stealthier attacks (without explicit instructions or imperceptible changes), highlighting the need for improved detection methods for web agents."}}
{"id": "2510.01994", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01994", "abs": "https://arxiv.org/abs/2510.01994", "authors": ["Chen Yang", "Lin Yang", "Ziqi Wang", "Dong Wang", "Jianyi Zhou", "Junjie Chen"], "title": "Clarifying Semantics of In-Context Examples for Unit Test Generation", "comment": "accepted in the research track of ASE 2025", "summary": "Recent advances in large language models (LLMs) have enabled promising\nperformance in unit test generation through in-context learning (ICL). However,\nthe quality of in-context examples significantly influences the effectiveness\nof generated tests-poorly structured or semantically unclear test examples\noften lead to suboptimal outputs. In this paper, we propose CLAST, a novel\ntechnique that systematically refines unit tests to improve their semantic\nclarity, thereby enhancing their utility as in-context examples. The approach\ndecomposes complex tests into logically clearer ones and improves semantic\nclarity through a combination of program analysis and LLM-based rewriting. We\nevaluated CLAST on four open-source and three industrial projects. The results\ndemonstrate that CLAST largely outperforms UTgen, the state-of-the-art\nrefinement technique, in both preserving test effectiveness and enhancing\nsemantic clarity. Specifically, CLAST fully retains the original effectiveness\nof unit tests, while UTgen reduces compilation success rate (CSR), pass rate\n(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,\n35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user\nstudy preferred the semantic clarity of CLAST-refined tests. Notably,\nincorporating CLAST-refined tests as examples effectively improves ICL-based\nunit test generation approaches such as RAGGen and TELPA, resulting in an\naverage increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for\ngenerated tests, compared to incorporating UTgen-refined tests. The insights\nfrom the follow-up user study not only reinforce CLAST's potential impact in\nsoftware testing practice but also illuminate avenues for future research.", "AI": {"tldr": "CLAST improves semantic clarity in test examples via program analysis + LLM rewriting, outperforming UTgen in test quality preservation (100% vs. 12.9-35.8% loss) while boosting ICL-based test generation metrics by 25-46%.", "motivation": "Poorly structured in-context examples for unit test generation degrade LLM performance. Current refinement techniques like UTgen reduce critical test quality metrics.", "method": "CLAST combines program analysis and LLM-based rewriting to systematically refine unit tests, decomposing complex tests into logically clearer ones.", "result": "CLAST retains 100% original test effectiveness while UTgen decreases CSR (12.90%), PR (35.82%), Cov (4.65%), and MS (5.07%). User studies show 85.33% preference for CLAST refinements, and test generation tools using CLAST examples improve CSR (25.97%), PR (28.22%), and Cov (45.99%) significantly.", "conclusion": "CLAST significantly enhances semantic clarity and test effectiveness compared to existing techniques, demonstrating its potential in improving ICL-based unit test generation."}}
{"id": "2510.01359", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01359", "abs": "https://arxiv.org/abs/2510.01359", "authors": ["Shoumik Saha", "Jifan Chen", "Sam Mayers", "Sanjay Krishna Gouda", "Zijian Wang", "Varun Kumar"], "title": "Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks", "comment": "28 pages, 21 figures, 9 tables", "summary": "Code-capable large language model (LLM) agents are increasingly embedded into\nsoftware engineering workflows where they can read, write, and execute code,\nraising the stakes of safety-bypass (\"jailbreak\") attacks beyond text-only\nsettings. Prior evaluations emphasize refusal or harmful-text detection,\nleaving open whether agents actually compile and run malicious programs. We\npresent JAWS-BENCH (Jailbreaks Across WorkSpaces), a benchmark spanning three\nescalating workspace regimes that mirror attacker capability: empty (JAWS-0),\nsingle-file (JAWS-1), and multi-file (JAWS-M). We pair this with a\nhierarchical, executable-aware Judge Framework that tests (i) compliance, (ii)\nattack success, (iii) syntactic correctness, and (iv) runtime executability,\nmoving beyond refusal to measure deployable harm. Using seven LLMs from five\nfamilies as backends, we find that under prompt-only conditions in JAWS-0, code\nagents accept 61% of attacks on average; 58% are harmful, 52% parse, and 27%\nrun end-to-end. Moving to single-file regime in JAWS-1 drives compliance to ~\n100% for capable models and yields a mean ASR (Attack Success Rate) ~ 71%; the\nmulti-file regime (JAWS-M) raises mean ASR to ~ 75%, with 32% instantly\ndeployable attack code. Across models, wrapping an LLM in an agent\nsubstantially increases vulnerability -- ASR raises by 1.6x -- because initial\nrefusals are frequently overturned during later planning/tool-use steps.\nCategory-level analyses identify which attack classes are most vulnerable and\nmost readily deployable, while others exhibit large execution gaps. These\nfindings motivate execution-aware defenses, code-contextual safety filters, and\nmechanisms that preserve refusal decisions throughout the agent's multi-step\nreasoning and tool use.", "AI": {"tldr": "JAWS-BENCH reveals code agents execute malicious code in 27%-75% of cases depending on workspace complexity. Agent workflows amplify attacks by overturning initial refusals during planning, necessitating execution-aware defenses.", "motivation": "Existing evaluations of code-capable LLM security focus on refusal/harm detection but ignore whether agents actually execute malicious code. The paper aims to quantify deployable harm in software engineering workflows where agents write/run code.", "method": "The authors developed JAWS-BENCH, a benchmark with three workspace regimes (JAWS-0, JAWS-1, JAWS-M) and a hierarchical Judge Framework to evaluate compliance, attack success, syntactic correctness, and runtime executability in code-capable LLMs. They tested seven LLMs across all regimes.", "result": "In JAWS-0, 61% of attacks are accepted (27% run end-to-end). JAWS-1 shows 71% attack success rates while appearing 100% compliant, and JAWS-M reaches 75% mean ASR with 32% instantly deployable attacks. Agentization increases ASR by 1.6x due to initial refusals being overturned in planning steps.", "conclusion": "The study highlights the need for execution-aware defenses and mechanisms that preserve refusal decisions in code-capable LLM agents, as agentization increases attack success rates due to compromised multi-step planning. Future work should focus on code-contextual safety filters and maintaining safety throughout agent workflows."}}
{"id": "2510.02002", "categories": ["cs.SE", "D.2.1; D.2.2; D.2.3; D.3.4; G.1.6"], "pdf": "https://arxiv.org/pdf/2510.02002", "abs": "https://arxiv.org/abs/2510.02002", "authors": ["Maximilian Kratz", "Steffen Zschaler", "Jens Kosiol", "Gabriele Taentzer"], "title": "Automatic Generation of Combinatorial Reoptimisation Problem Specifications: A Vision", "comment": null, "summary": "Once an optimisation problem has been solved, the solution may need\nadaptation when contextual factors change. This challenge, also known as\nreoptimisation, has been addressed in various problem domains, such as railway\ncrew rescheduling, nurse rerostering, or aircraft recovery. This requires a\nmodified problem to be solved again to ensure that the adapted solution is\noptimal in the new context. However, the new optimisation problem differs\nnotably from the original problem: (i) we want to make only minimal changes to\nthe original solution to minimise the impact; (ii) we may be unable to change\nsome parts of the original solution (e.g., because they refer to past\nallocations); and (iii) we need to derive a change script from the original\nsolution to the new solution. In this paper, we argue that Model-Driven\nEngineering (MDE) - in particular, the use of declarative modelling languages\nand model transformations for the high-level specification of optimisation\nproblems - offers new opportunities for the systematic derivation of\nreoptimisation problems from the original optimisation problem specification.\nWe focus on combinatorial reoptimisation problems and provide an initial\ncategorisation of changing problems and strategies for deriving the\ncorresponding reoptimisation specifications. We introduce an initial\nproof-of-concept implementation based on the GIPS (Graph-Based (Mixed) Integer\nLinear Programming Problem Specification) tool and apply it to an example\nresource-allocation problem: the allocation of teaching assistants to teaching\nsessions.", "AI": {"tldr": "This paper proposes using Model-Driven Engineering to systematically adapt optimization solutions to changing contexts, demonstrating its potential through a resource-allocation case study.", "motivation": "Optimization solutions require frequent adaptation due to changing contextual factors (e.g., railway crew rescheduling, nurse rerostering). Existing approaches lack systematic methods to handle constraints like minimal change, fixed parts of solutions, and change script derivation.", "method": "The paper introduces the use of declarative modeling languages and model transformations in MDE to categorize and derive reoptimization strategies. It also presents a proof-of-concept implementation using the GIPS tool to demonstrate feasibility.", "result": "An initial categorization of changing problems and reoptimization strategies is developed, supported by a proof-of-concept implementation applied to a teaching assistant allocation example, validating the approach's effectiveness.", "conclusion": "Model-Driven Engineering (MDE) provides a systematic framework for deriving reoptimization problems from original optimization specifications, enabling efficient adaptation to changing contexts in combinatorial optimization problems."}}
{"id": "2510.01393", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01393", "abs": "https://arxiv.org/abs/2510.01393", "authors": ["Davide Rusconi", "Osama Yousef", "Mirco Picca", "Flavio Toffalini", "Andrea Lanzi"], "title": "E-FuzzEdge: Optimizing Embedded Device Security with Scalable In-Place Fuzzing", "comment": null, "summary": "In this paper we show E-FuzzEdge, a novel fuzzing architecture targeted\ntowards improving the throughput of fuzzing campaigns in contexts where\nscalability is unavailable. E-FuzzEdge addresses the inefficiencies of\nhardware-in-the-loop fuzzing for microcontrollers by optimizing execution\nspeed. We evaluated our system against state-of-the-art benchmarks,\ndemonstrating significant performance improvements. A key advantage of\nE-FuzzEdgearchitecture is its compatibility with other embedded fuzzing\ntechniques that perform on device testing instead of firmware emulation. This\nmeans that the broader embedded fuzzing community can integrate E-FuzzEdge into\ntheir workflows to enhance overall testing efficiency.", "AI": {"tldr": "E-FuzzEdge is a fuzzing architecture designed to enhance fuzzing throughput for microcontrollers by optimizing execution speed, with compatibility for on-device testing methods.", "motivation": "Hardware-in-the-loop fuzzing for microcontrollers lacks efficiency and scalability, necessitating a solution that improves throughput without relying on firmware emulation.", "method": "The authors developed E-FuzzEdge, which optimizes execution speed through an architecture that integrates with on-device testing techniques, validated via state-of-the-art benchmarks.", "result": "E-FuzzEdge demonstrated significant performance improvements in throughput while maintaining compatibility with existing embedded fuzzing workflows.", "conclusion": "E-FuzzEdge provides a scalable, high-performance alternative for embedded fuzzing, enabling broader adoption and integration with current on-device testing methodologies."}}
{"id": "2510.02007", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02007", "abs": "https://arxiv.org/abs/2510.02007", "authors": ["Justus Bogner", "Roberto Verdecchia"], "title": "ACM SIGSOFT SEN Empirical Software Engineering: Introducing Our New Regular Column", "comment": "Published in ACM SIGSOFT Software Engineering Notes (SIGSOFT-SEN).\n  Volume 50, Issue 4, 2025", "summary": "From its early foundations in the 1970s, empirical software engineering (ESE)\nhas evolved into a mature research discipline that embraces a plethora of\ndifferent topics, methodologies, and industrial practices. Despite its\nremarkable progress, the ESE research field still needs to keep evolving, as\nnew impediments, shortcoming, and technologies emerge. Research\nreproducibility, limited external validity, subjectivity of reviews, and\nporting research results to industrial practices are just some examples of the\ndrivers for improvements to ESE research. Additionally, several facets of ESE\nresearch are not documented very explicitly, which makes it difficult for\nnewcomers to pick them up. With this new regular ACM SIGSOFT SEN column\n(SEN-ESE), we introduce a venue for discussing meta-aspects of ESE research,\nranging from general topics such as the nature and best practices for\nreplication packages, to more nuanced themes such as statistical methods,\ninterview transcription tools, and publishing interdisciplinary research. Our\naim for the column is to be a place where we can regularly spark conversations\non ESE topics that might not often be touched upon or are left implicit.\nContributions to this column will be grounded in expert interviews, focus\ngroups, surveys, and position pieces, with the goal of encouraging reflection\nand improvement in how we conduct, communicate, teach, and ultimately improve\nESE research. Finally, we invite feedback from the ESE community on\nchallenging, controversial, or underexplored topics, as well as suggestions for\nvoices you would like to hear from. While we cannot promise to act on every\nidea, we aim to shape this column around the community interests and are\ngrateful for all contributions.", "AI": {"tldr": "The paper introduces a new ACM SIGSOFT SEN column, SEN-ESE, aimed at discussing meta-aspects of empirical software engineering research to improve it.", "motivation": "Empirical software engineering (ESE) is a mature field that still needs improvement due to issues like research reproducibility, limited external validity, subjective reviews, and the challenge of applying research to industry. The field also lacks explicit documentation of many key aspects, making them hard for new researchers to learn.", "method": "Introduce a new regular column for the ESE community to discuss meta-aspects of research. Contributions include expert interviews, focus groups, surveys, and position papers. The column encourages discussion on issues that are either seldom addressed or taken for granted.", "result": "Establishment of the SEN-ESE column as a new platform for the software engineering community to discuss and reflect on the practices of empirical software engineering research.", "conclusion": "The column will encourage ongoing conversation and improvement of ESE by including the voices of the community. It is open for suggestions and input on relevant challenges and underexplored areas."}}
{"id": "2510.01445", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01445", "abs": "https://arxiv.org/abs/2510.01445", "authors": ["Andr\u00e9s F. Betancur-L\u00f3pez"], "title": "Securing IoT Devices in Smart Cities: A Review of Proposed Solutions", "comment": "14 pages, 7 figures", "summary": "Privacy and security in Smart Cities remain at constant risk due to the\nvulnerabilities introduced by Internet of Things (IoT) devices. The limited\ncomputational resources of these devices make them especially susceptible to\nattacks, while their widespread adoption increases the potential impact of\nsecurity breaches. This article presents a review of security proposals aimed\nat protecting IoT devices in Smart City environments. The review was conducted\nby analyzing recent literature on device-level security, with particular\nemphasis on lightweight cryptography, physically unclonable functions (PUFs),\nand blockchain-based solutions. Findings highlight both the strengths and\nlimitations of current approaches, as well as the need for more practical,\nscalable, and resource-efficient mechanisms to ensure user privacy and data\nprotection in IoT ecosystems.", "AI": {"tldr": "This paper reviews IoT security challenges in Smart Cities, evaluates recent proposals (lightweight cryptography, PUFs, blockchain), and advocates for more scalable, practical solutions to enhance privacy and security.", "motivation": "The increasing adoption of resource-constrained IoT devices in Smart Cities introduces significant privacy and security risks, necessitating robust protective measures against vulnerabilities and attacks.", "method": "The authors conducted a review of recent literature on device-level security for IoT in Smart Cities, focusing on lightweight cryptography, physically unclonable functions (PUFs), and blockchain-based solutions.", "result": "The review highlights both strengths and limitations of existing security approaches, underscoring gaps in scalability, practicality, and resource efficiency for real-world IoT ecosystems.", "conclusion": "The article emphasizes the need for more practical, scalable, and resource-efficient security mechanisms to protect IoT devices in Smart Cities, ensuring user privacy and data protection."}}
{"id": "2510.02165", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02165", "abs": "https://arxiv.org/abs/2510.02165", "authors": ["Peter Wauyo", "Dalia Bwiza", "Alain Murara", "Edwin Mugume", "Eric Umuhoza"], "title": "Towards fairer public transit: Real-time tensor-based multimodal fare evasion and fraud detection", "comment": "10 pages", "summary": "This research introduces a multimodal system designed to detect fraud and\nfare evasion in public transportation by analyzing closed circuit television\n(CCTV) and audio data. The proposed solution uses the Vision Transformer for\nVideo (ViViT) model for video feature extraction and the Audio Spectrogram\nTransformer (AST) for audio analysis. The system implements a Tensor Fusion\nNetwork (TFN) architecture that explicitly models unimodal and bimodal\ninteractions through a 2-fold Cartesian product. This advanced fusion technique\ncaptures complex cross-modal dynamics between visual behaviors (e.g.,\ntailgating,unauthorized access) and audio cues (e.g., fare transaction sounds).\nThe system was trained and tested on a custom dataset, achieving an accuracy of\n89.5%, precision of 87.2%, and recall of 84.0% in detecting fraudulent\nactivities, significantly outperforming early fusion baselines and exceeding\nthe 75% recall rates typically reported in state-of-the-art transportation\nfraud detection systems. Our ablation studies demonstrate that the tensor\nfusion approach provides a 7.0% improvement in the F1 score and an 8.8% boost\nin recall compared to traditional concatenation methods. The solution supports\nreal-time detection, enabling public transport operators to reduce revenue\nloss, improve passenger safety, and ensure operational compliance.", "AI": {"tldr": "Multimodal CCTV/audio analysis with Tensor Fusion achieves 84% recall for transport fraud, surpassing existing 75% rates and enabling real-time detection.", "motivation": "Existing systems achieve only 75% recall in transportation fraud detection, necessitating improved methods to reduce revenue loss, enhance passenger safety, and ensure compliance.", "method": "Combines ViViT (video) and AST (audio) features via a Tensor Fusion Network (TFN) using a 2-fold Cartesian product to model unimodal/bimodal interactions between visual behaviors and audio cues.", "result": "89.5% accuracy, 87.2% precision, 84.0% recall; 7.0% F1 score and 8.8% recall improvements over traditional concatenation methods; outperforms early fusion baselines.", "conclusion": "The proposed multimodal system with Tensor Fusion Network effectively detects transportation fraud, outperforming state-of-the-art methods and enabling real-time operational benefits."}}
{"id": "2510.01552", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01552", "abs": "https://arxiv.org/abs/2510.01552", "authors": ["Luoxi Tang", "Yuqiao Meng", "Ankita Patra", "Weicheng Ma", "Muchao Ye", "Zhaohan Xi"], "title": "POLAR: Automating Cyber Threat Prioritization through LLM-Powered Assessment", "comment": "25 pages", "summary": "Large Language Models (LLMs) are intensively used to assist security analysts\nin counteracting the rapid exploitation of cyber threats, wherein LLMs offer\ncyber threat intelligence (CTI) to support vulnerability assessment and\nincident response. While recent work has shown that LLMs can support a wide\nrange of CTI tasks such as threat analysis, vulnerability detection, and\nintrusion defense, significant performance gaps persist in practical\ndeployments. In this paper, we investigate the intrinsic vulnerabilities of\nLLMs in CTI, focusing on challenges that arise from the nature of the threat\nlandscape itself rather than the model architecture. Using large-scale\nevaluations across multiple CTI benchmarks and real-world threat reports, we\nintroduce a novel categorization methodology that integrates stratification,\nautoregressive refinement, and human-in-the-loop supervision to reliably\nanalyze failure instances. Through extensive experiments and human inspections,\nwe reveal three fundamental vulnerabilities: spurious correlations,\ncontradictory knowledge, and constrained generalization, that limit LLMs in\neffectively supporting CTI. Subsequently, we provide actionable insights for\ndesigning more robust LLM-powered CTI systems to facilitate future research.", "AI": {"tldr": "This paper uncovers core vulnerabilities preventing LLMs from reliably supporting cyber threat intelligence, including spurious correlations and generalization limits. A structured analysis framework with real-world evaluations reveals critical failure modes, followed by actionable guidance for building more robust CTI systems.", "motivation": "The paper addresses critical performance gaps in deploying LLMs for cyber threat intelligence (CTI), where current approaches fail to account for vulnerabilities inherent to threat landscapes rather than just model architectures. This is essential for improving real-world threat detection and response effectiveness.", "method": "A novel categorization methodology combining stratification, autoregressive refinement, and human-in-the-loop supervision was applied to large-scale evaluations across CTI benchmarks and real-world threat reports. This approach enables systematic analysis of LLM failure modes through empirical experimentation and human validation.", "result": "Three fundamental LLM vulnerabilities were identified: 1) spurious correlations leading to false threat associations, 2) contradictory knowledge about evolving threats, and 3) constrained generalization to novel attack patterns. The proposed methodology enables reliable failure analysis and provides concrete mitigation strategies for CTI systems.", "conclusion": "The analysis provides actionable insights for designing more robust LLM-powered CTI systems and establishes a framework for addressing LLM vulnerabilities in cybersecurity contexts. The work highlights the importance of understanding contextual challenges beyond model architecture to enhance practical CTI applications."}}
{"id": "2510.02166", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02166", "abs": "https://arxiv.org/abs/2510.02166", "authors": ["Fatou Ndiaye Mbodji", "El-hacen Diallo", "Jordan Samhi", "Kui Liu", "Jacques Klein", "Tegawend\u00e9 F. Bissyande"], "title": "SIEVE: Towards Verifiable Certification for Code-datasets", "comment": "5", "summary": "Code agents and empirical software engineering rely on public code datasets,\nyet these datasets lack verifiable quality guarantees. Static 'dataset cards'\ninform, but they are neither auditable nor do they offer statistical\nguarantees, making it difficult to attest to dataset quality. Teams build\nisolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We\npresent SIEVE, a community-driven framework. It turns per-property checks into\nConfidence Cards-machine-readable, verifiable certificates with anytime-valid\nstatistical bounds. We outline a research plan to bring SIEVE to maturity,\nreplacing narrative cards with anytime-verifiable certification. This shift is\nexpected to lower quality-assurance costs and increase trust in code-datasets.", "AI": {"tldr": "The paper introduces SIEVE, a community-driven framework addressing the lack of verifiable quality guarantees in public code datasets by generating machine-readable Confidence Cards with statistical bounds.", "motivation": "Existing static dataset cards lack auditing possibilities and statistical guarantees, while isolated cleaning pipelines fragment efforts and raise costs. This hinders trust in code-dataset quality.", "method": "SIEVE transforms per-property checks into Confidence Cards\u2014machine-readable certificates with anytime-valid statistical bounds\u2014via community-driven certification replacing narrative cards.", "result": "The authors outline a research plan to mature SIEVE and evaluate its utility in reducing quality-assurance costs and enhancing dataset trust.", "conclusion": "SIEVE's framework could establish verifiable certification as a standard, improving transparency and efficiency in code-dataset development and usage."}}
{"id": "2510.01645", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01645", "abs": "https://arxiv.org/abs/2510.01645", "authors": ["Niloofar Mireshghallah", "Tianshi Li"], "title": "Position: Privacy Is Not Just Memorization!", "comment": "27 pages, 6 figures, 2 tables", "summary": "The discourse on privacy risks in Large Language Models (LLMs) has\ndisproportionately focused on verbatim memorization of training data, while a\nconstellation of more immediate and scalable privacy threats remain\nunderexplored. This position paper argues that the privacy landscape of LLM\nsystems extends far beyond training data extraction, encompassing risks from\ndata collection practices, inference-time context leakage, autonomous agent\ncapabilities, and the democratization of surveillance through deep inference\nattacks. We present a comprehensive taxonomy of privacy risks across the LLM\nlifecycle -- from data collection through deployment -- and demonstrate through\ncase studies how current privacy frameworks fail to address these multifaceted\nthreats. Through a longitudinal analysis of 1,322 AI/ML privacy papers\npublished at leading conferences over the past decade (2016--2025), we reveal\nthat while memorization receives outsized attention in technical research, the\nmost pressing privacy harms lie elsewhere, where current technical approaches\noffer little traction and viable paths forward remain unclear. We call for a\nfundamental shift in how the research community approaches LLM privacy, moving\nbeyond the narrow focus of current technical solutions and embracing\ninterdisciplinary approaches that address the sociotechnical nature of these\nemerging threats.", "AI": {"tldr": "This paper argues for reorienting LLM privacy research beyond memorization by identifying overlooked threats and advocating interdisciplinary approaches to address systemic risks across the LLM lifecycle.", "motivation": "Current research disproportionately focuses on memorization, neglecting more immediate and scalable privacy risks such as data collection practices, inference-time context leakage, autonomous agent threats, and surveillance via deep inference attacks.", "method": "The authors conducted a longitudinal analysis of 1,322 AI/ML privacy papers (2016\u20132025) and developed a taxonomy of privacy risks across the LLM lifecycle, supported by case studies.", "result": "The analysis reveals that while memorization dominates technical research, the most pressing privacy harms stem from underexplored risks with limited existing mitigation strategies or clear solutions.", "conclusion": "The paper concludes that the LLM privacy discourse must expand beyond memorization to address a broader range of sociotechnical threats through interdisciplinary approaches."}}
{"id": "2510.02169", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.02169", "abs": "https://arxiv.org/abs/2510.02169", "authors": ["Vadim Safronov", "Anthony McCaigue", "Nicholas Allott", "Andrew Martin"], "title": "TAIBOM: Bringing Trustworthiness to AI-Enabled Systems", "comment": "This paper has been accepted at the First International Workshop on\n  Security and Privacy-Preserving AI/ML (SPAIML 2025), co-located with the 28th\n  European Conference on Artificial Intelligence (ECAI 2025)", "summary": "The growing integration of open-source software and AI-driven technologies\nhas introduced new layers of complexity into the software supply chain,\nchallenging existing methods for dependency management and system assurance.\nWhile Software Bills of Materials (SBOMs) have become critical for enhancing\ntransparency and traceability, current frameworks fall short in capturing the\nunique characteristics of AI systems -- namely, their dynamic, data-driven\nnature and the loosely coupled dependencies across datasets, models, and\nsoftware components. These challenges are compounded by fragmented governance\nstructures and the lack of robust tools for ensuring integrity, trust, and\ncompliance in AI-enabled environments.\n  In this paper, we introduce Trusted AI Bill of Materials (TAIBOM) -- a novel\nframework extending SBOM principles to the AI domain. TAIBOM provides (i) a\nstructured dependency model tailored for AI components, (ii) mechanisms for\npropagating integrity statements across heterogeneous AI pipelines, and (iii) a\ntrust attestation process for verifying component provenance. We demonstrate\nhow TAIBOM supports assurance, security, and compliance across AI workflows,\nhighlighting its advantages over existing standards such as SPDX and CycloneDX.\nThis work lays the foundation for trustworthy and verifiable AI systems through\nstructured software transparency.", "AI": {"tldr": "This paper introduces TAIBOM, a framework extending SBOMs to address AI-specific challenges in software supply chains by capturing dynamic dependencies, propagating integrity statements, and enabling trust attestation for AI components.", "motivation": "Existing SBOMs fail to address AI systems' unique characteristics: dynamic data-driven behavior, loose coupling across datasets/models, and fragmented governance. These gaps compromise integrity, trust, and compliance in AI workflows.", "method": "TAIBOM introduces three key innovations: (i) a structured dependency model for AI components, (ii) integrity propagation mechanisms across heterogeneous pipelines, and (iii) trust attestation for provenance verification. The framework is evaluated against existing standards (SPDX, CycloneDX).", "result": "TAIBOM demonstrates improved assurance, security, and compliance support for AI workflows, outperforming current standards. It enables structured transparency for verifying AI system components and dependencies.", "conclusion": "TAIBOM establishes a foundational framework for trustworthy AI systems through enhanced software transparency, addressing critical gaps in dependency management and assurance for dynamic AI environments."}}
{"id": "2510.01676", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01676", "abs": "https://arxiv.org/abs/2510.01676", "authors": ["Milad Nasr", "Yanick Fratantonio", "Luca Invernizzi", "Ange Albertini", "Loua Farah", "Alex Petit-Bianco", "Andreas Terzis", "Kurt Thomas", "Elie Bursztein", "Nicholas Carlini"], "title": "Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks", "comment": null, "summary": "As deep learning models become widely deployed as components within larger\nproduction systems, their individual shortcomings can create system-level\nvulnerabilities with real-world impact. This paper studies how adversarial\nattacks targeting an ML component can degrade or bypass an entire\nproduction-grade malware detection system, performing a case study analysis of\nGmail's pipeline where file-type identification relies on a ML model.\n  The malware detection pipeline in use by Gmail contains a machine learning\nmodel that routes each potential malware sample to a specialized malware\nclassifier to improve accuracy and performance. This model, called Magika, has\nbeen open sourced. By designing adversarial examples that fool Magika, we can\ncause the production malware service to incorrectly route malware to an\nunsuitable malware detector thereby increasing our chance of evading detection.\nSpecifically, by changing just 13 bytes of a malware sample, we can\nsuccessfully evade Magika in 90% of cases and thereby allow us to send malware\nfiles over Gmail. We then turn our attention to defenses, and develop an\napproach to mitigate the severity of these types of attacks. For our defended\nproduction model, a highly resourced adversary requires 50 bytes to achieve\njust a 20% attack success rate. We implement this defense, and, thanks to a\ncollaboration with Google engineers, it has already been deployed in production\nfor the Gmail classifier.", "AI": {"tldr": "Adversarial attacks on Gmail's ML-based malware routing (Magika) enable malware evasion with minimal byte changes. A deployed defense reduces attack effectiveness by 75% when requiring higher modifications.", "motivation": "The study addresses vulnerabilities in real-world ML systems, showing how adversarial attacks against a single component (Magika) can undermine entire security pipelines like Gmail's malware detection.", "method": "The authors designed adversarial examples to manipulate Magika, Gmail's file-type identifier, by altering 13 bytes to route malware to unsuitable classifiers. They then developed a defense strategy reducing evasion success to 20% with 50-byte modifications.", "result": "Adversarial attacks achieved 90% evasion with 13-byte changes, but a developed defense reduced success to 20% with 50-byte changes and was deployed in production.", "conclusion": "This paper highlights the critical vulnerabilities in production ML systems when adversarial attacks target individual components, demonstrating a practical defense deployed by Gmail to mitigate such risks."}}
{"id": "2510.02185", "categories": ["cs.SE", "cs.CR", "cs.MA", "D.2.4; F.3.1"], "pdf": "https://arxiv.org/pdf/2510.02185", "abs": "https://arxiv.org/abs/2510.02185", "authors": ["Paschal C. Amusuo", "Dongge Liu", "Ricardo Andres Calvo Mendez", "Jonathan Metzman", "Oliver Chang", "James C. Davis"], "title": "FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen Using Agentic AI", "comment": "12 pages, 2 figures", "summary": "Fuzz testing has become a cornerstone technique for identifying software bugs\nand security vulnerabilities, with broad adoption in both industry and\nopen-source communities. Directly fuzzing a function requires fuzz drivers,\nwhich translate random fuzzer inputs into valid arguments for the target\nfunction. Given the cost and expertise required to manually develop fuzz\ndrivers, methods exist that leverage program analysis and Large Language Models\nto automatically generate these drivers. However, the generated fuzz drivers\nfrequently lead to false positive crashes, especially in functions highly\nstructured input and complex state requirements. This problem is especially\ncrucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as\nreporting false positive crashes to maintainers impede trust in both the system\nand the team.\n  This paper presents two AI-driven strategies to reduce false positives in\nOSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First,\nconstraint-based fuzz driver generation proactively enforces constraints on a\nfunction's inputs and state to guide driver creation. Second, context-based\ncrash validation reactively analyzes function callers to determine whether\nreported crashes are feasible from program entry points. Using 1,500 benchmark\nfunctions from OSS-Fuzz, we show that these strategies reduce spurious crashes\nby up to 8%, cut reported crashes by more than half, and demonstrate that\nfrontier LLMs can serve as reliable program analysis agents. Our results\nhighlight the promise and challenges of integrating AI into large-scale fuzzing\npipelines.", "AI": {"tldr": "The paper introduces two AI-driven methods to reduce false positives in fuzz testing by generating fuzz drivers that enforce input constraints and validate crashes based on function context.", "motivation": "Fuzz drivers generated automatically often produce false positive crashes, especially for functions with structured input and complex state, which undermines trust in fuzzing systems like OSS-Fuzz-Gen.", "method": "The first strategy, constraint-based generation, applies input and state constraints during driver creation. The second, context-based validation, leverages function callers to assess the feasibility of reported crashes.", "result": "The proposed methods reduce spurious crashes by up to 8%, cut total reported crashes in half, and validate LLMs as effective agents for program analysis in large-scale fuzzing.", "conclusion": "Integrating AI into fuzz testing reduces false positives significantly, but challenges remain in applying intelligent methods to complex, real-world software systems."}}
{"id": "2510.01699", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01699", "abs": "https://arxiv.org/abs/2510.01699", "authors": ["Yue Li", "Linying Xue", "Dongdong Lin", "Qiushi Li", "Hui Tian", "Hongxia Wang"], "title": "Towards Imperceptible Adversarial Defense: A Gradient-Driven Shield against Facial Manipulations", "comment": null, "summary": "With the flourishing prosperity of generative models, manipulated facial\nimages have become increasingly accessible, raising concerns regarding privacy\ninfringement and societal trust. In response, proactive defense strategies\nembed adversarial perturbations into facial images to counter deepfake\nmanipulation. However, existing methods often face a tradeoff between\nimperceptibility and defense effectiveness-strong perturbations may disrupt\nforgeries but degrade visual fidelity. Recent studies have attempted to address\nthis issue by introducing additional visual loss constraints, yet often\noverlook the underlying gradient conflicts among losses, ultimately weakening\ndefense performance. To bridge the gap, we propose a gradient-projection-based\nadversarial proactive defense (GRASP) method that effectively counters facial\ndeepfakes while minimizing perceptual degradation. GRASP is the first approach\nto successfully integrate both structural similarity loss and low-frequency\nloss to enhance perturbation imperceptibility. By analyzing gradient conflicts\nbetween defense effectiveness loss and visual quality losses, GRASP pioneers\nthe design of the gradient-projection mechanism to mitigate these conflicts,\nenabling balanced optimization that preserves image fidelity without\nsacrificing defensive performance. Extensive experiments validate the efficacy\nof GRASP, achieving a PSNR exceeding 40 dB, SSIM of 0.99, and a 100% defense\nsuccess rate against facial attribute manipulations, significantly\noutperforming existing approaches in visual quality.", "AI": {"tldr": "The paper introduces GRASP, a method to defend against facial deepfakes by using gradient projection to manage conflicting losses in adversarial perturbation, resulting in nearly undetectable perturbations and strong defense performance.", "motivation": "As deepfake technology has advanced, manipulated facial images have created serious privacy and trust issues in society. Proactive defense methods that embed adversarial perturbations face challenges in maintaining both imperceptibility and effectiveness due to the tradeoff between defense and visual quality.", "method": "GRASP is a gradient-projection-based adversarial proactive defense method that integrates structural similarity loss and low-frequency loss to enhance perturbation imperceptibility. It addresses gradient conflicts through an analysis of conflicts between defense effectiveness loss and visual quality losses and uses a gradient-projection mechanism to enable a balanced optimization.", "result": "GRASP achieves PSNR over 40 dB and SSIM of 0.99, along with a 100% defense success rate against facial deepfake manipulations. It outperforms existing methods in both defense effectiveness and visual quality.", "conclusion": "The proposed GRASP method successfully resolves the gradient conflicts typically encountered during adversarial training for deepfake defense. By prioritizing both image fidelity and defense performance, it represents a significant advancement in proactive defense strategies against facial deepfake attacks."}}
{"id": "2510.01720", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01720", "abs": "https://arxiv.org/abs/2510.01720", "authors": ["Palash Sarkar"], "title": "Constructions of Efficiently Implementable Boolean Functions with Provable Nonlinearity/Resiliency/Algebraic Immunity Trade-Offs", "comment": null, "summary": "We describe several families of efficiently implementable Boolean functions\nachieving provable trade-offs between resiliency, nonlinearity, and algebraic\nimmunity. In concrete terms, the following result holds for each of the\nfunction families that we propose. Given integers $m_0\\geq 0$, $x_0\\geq 1$, and\n$a_0\\geq 1$, it is possible to construct an $n$-variable function which has\nresiliency at least $m_0$, linear bias (which is an equivalent method of\nexpressing nonlinearity) at most $2^{-x_0}$ and algebraic immunity at least\n$a_0$; further, $n$ is linear in $m_0$, $x_0$ and $a_0$, and the function can\nbe implemented using $O(n)$ gates.", "AI": {"tldr": "The paper introduces Boolean functions with provable trade-offs in resiliency, nonlinearity, and algebraic immunity, constructible with linear variable count and efficient gate implementation.", "motivation": "Cryptographic systems require Boolean functions balancing security properties (resiliency, nonlinearity, algebraic immunity). Prior work lacked efficient constructions with provable multi-parameter trade-offs.", "method": "Mathematical construction of function families achieving specified bounds on resiliency $m_0$, linear bias $2^{-x_0}$ (nonlinearity), and algebraic immunity $a_0$. Parameters scale linearly, with $O(n)$ gate complexity.", "result": "For given $m_0,x_0,a_0$, constructs $n$-variable functions meeting all three property bounds simultaneously, where $n$ is linear in $m_0,x_0,a_0$ and implementation uses $O(n)$ gates.", "conclusion": "The work establishes theoretically optimal trade-offs between critical cryptographic properties while maintaining hardware efficiency, enabling practical implementations with provable security guarantees."}}
{"id": "2510.01780", "categories": ["cs.CR", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01780", "abs": "https://arxiv.org/abs/2510.01780", "authors": ["Aueaphum Aueawatthanaphisut"], "title": "Secure Multi-Modal Data Fusion in Federated Digital Health Systems via MCP", "comment": "6 pages, 8 figures, 7 equations, 1 algorithm", "summary": "Secure and interoperable integration of heterogeneous medical data remains a\ngrand challenge in digital health. Current federated learning (FL) frameworks\noffer privacy-preserving model training but lack standardized mechanisms to\norchestrate multi-modal data fusion across distributed and resource-constrained\nenvironments. This study introduces a novel framework that leverages the Model\nContext Protocol (MCP) as an interoperability layer for secure, cross-agent\ncommunication in multi-modal federated healthcare systems. The proposed\narchitecture unifies three pillars: (i) multi-modal feature alignment for\nclinical imaging, electronic medical records, and wearable IoT data; (ii)\nsecure aggregation with differential privacy to protect patient-sensitive\nupdates; and (iii) energy-aware scheduling to mitigate dropouts in mobile\nclients. By employing MCP as a schema-driven interface, the framework enables\nadaptive orchestration of AI agents and toolchains while ensuring compliance\nwith privacy regulations. Experimental evaluation on benchmark datasets and\npilot clinical cohorts demonstrates up to 9.8\\% improvement in diagnostic\naccuracy compared with baseline FL, a 54\\% reduction in client dropout rates,\nand clinically acceptable privacy--utility trade-offs. These results highlight\nMCP-enabled multi-modal fusion as a scalable and trustworthy pathway toward\nequitable, next-generation federated health infrastructures.", "AI": {"tldr": "This work introduces an MCP-based framework that enables secure, interoperable multi-modal federated learning for healthcare, achieving improved diagnostic accuracy and reduced client dropouts while maintaining privacy.", "motivation": "Heterogeneous medical data integration remains challenging due to distributed environments and resource constraints. Existing federated learning (FL) frameworks lack standardized mechanisms for secure, multi-modal data fusion while balancing privacy and utility.", "method": "The framework combines three pillars: (1) multi-modal feature alignment for heterogeneous data types (imaging, EHR, wearable IoT), (2) secure aggregation with differential privacy, and (3) energy-aware scheduling to reduce client dropouts. The Model Context Protocol (MCP) serves as a schema-driven interoperability layer coordinating distributed AI agents.", "result": "Benchmark evaluations show 9.8% higher diagnostic accuracy than baseline FL, 54% fewer client dropouts, and privacy-utility trade-offs acceptable in clinical settings.", "conclusion": "The study demonstrates that MCP-enabled multi-modal fusion offers a scalable and trustworthy pathway for secure, interoperable federated healthcare systems, addressing limitations in current frameworks while achieving clinically meaningful improvements in accuracy and robustness."}}
{"id": "2510.01967", "categories": ["cs.CR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01967", "abs": "https://arxiv.org/abs/2510.01967", "authors": ["Aadarsh Anantha Ramakrishnan", "Shubham Agarwal", "Selvanayagam S", "Kunwar Singh"], "title": "ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs", "comment": "Accepted at AI-ML Systems 2025, Bangalore, India,\n  https://www.aimlsystems.org/2025/", "summary": "As image generation models grow increasingly powerful and accessible,\nconcerns around authenticity, ownership, and misuse of synthetic media have\nbecome critical. The ability to generate lifelike images indistinguishable from\nreal ones introduces risks such as misinformation, deepfakes, and intellectual\nproperty violations. Traditional watermarking methods either degrade image\nquality, are easily removed, or require access to confidential model internals\n- making them unsuitable for secure and scalable deployment. We are the first\nto introduce ZK-WAGON, a novel system for watermarking image generation models\nusing the Zero-Knowledge Succinct Non Interactive Argument of Knowledge\n(ZK-SNARKs). Our approach enables verifiable proof of origin without exposing\nmodel weights, generation prompts, or any sensitive internal information. We\npropose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively\nconvert key layers of an image generation model into a circuit, reducing proof\ngeneration time significantly. Generated ZK-SNARK proofs are imperceptibly\nembedded into a generated image via Least Significant Bit (LSB) steganography.\nWe demonstrate this system on both GAN and Diffusion models, providing a\nsecure, model-agnostic pipeline for trustworthy AI image generation.", "AI": {"tldr": "ZK-WAGON is a ZK-SNARK-based watermarking system for image generation models, enabling verifiable, privacy-preserving, and model-agnostic synthetic image authentication.", "motivation": "Traditional watermarking methods degrade image quality, are removable, or require confidential model access. Synthetic media risks (misinformation, deepfakes) demand secure, verifiable, and privacy-preserving solutions.", "method": "The method introduces ZK-WAGON, a ZK-SNARK-based watermarking system, and proposes Selective Layer ZK-Circuit Creation (SL-ZKCC) to optimize proof generation by targeting key model layers. Proofs are embedded via LSB steganography.", "result": "ZK-WAGON demonstrates secure watermarking on GAN and Diffusion models with imperceptible LSB-embedded proofs. It achieves efficient proof generation while maintaining image quality and privacy.", "conclusion": "The paper concludes that ZK-WAGON provides a secure, model-agnostic solution for watermarking image generation models, enabling verifiable image authenticity without compromising privacy or performance."}}
{"id": "2510.02158", "categories": ["cs.CR", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.02158", "abs": "https://arxiv.org/abs/2510.02158", "authors": ["Junjie Su", "Weifei Jin", "Yuxin Cao", "Derui Wang", "Kai Ye", "Jie Hao"], "title": "Mirage Fools the Ear, Mute Hides the Truth: Precise Targeted Adversarial Attacks on Polyphonic Sound Event Detection Systems", "comment": null, "summary": "Sound Event Detection (SED) systems are increasingly deployed in\nsafety-critical applications such as industrial monitoring and audio\nsurveillance. However, their robustness against adversarial attacks has not\nbeen well explored. Existing audio adversarial attacks targeting SED systems,\nwhich incorporate both detection and localization capabilities, often lack\neffectiveness due to SED's strong contextual dependencies or lack precision by\nfocusing solely on misclassifying the target region as the target event,\ninadvertently affecting non-target regions. To address these challenges, we\npropose the Mirage and Mute Attack (M2A) framework, which is designed for\ntargeted adversarial attacks on polyphonic SED systems. In our optimization\nprocess, we impose specific constraints on the non-target output, which we\nrefer to as preservation loss, ensuring that our attack does not alter the\nmodel outputs for non-target region, thus achieving precise attacks.\nFurthermore, we introduce a novel evaluation metric Editing Precison (EP) that\nbalances effectiveness and precision, enabling our method to simultaneously\nenhance both. Comprehensive experiments show that M2A achieves 94.56% and\n99.11% EP on two state-of-the-art SED models, demonstrating that the framework\nis sufficiently effective while significantly enhancing attack precision.", "AI": {"tldr": "This paper proposes the M2A framework for targeted adversarial attacks on polyphonic Sound Event Detection (SED) systems, achieving high Editing Precision (EP) through preservation loss and a novel evaluation metric.", "motivation": "Existing adversarial attacks on SED systems lack effectiveness due to strong contextual dependencies or insufficient precision, failing to address both detection and localization requirements in safety-critical applications.", "method": "The authors introduce (1)\u00a0M2A framework with preservation loss that constrains non-target outputs during optimization; (2)\u00a0Editing Precision (EP), a new metric balancing attack effectiveness and precision. The method ensures non-target regions remain unchanged while enhancing targeted attack performance.", "result": "M2A achieves 94.56%\u00a0and 99.11%\u00a0EP on two state-of-the-art SED models, demonstrating significantly improved attack precision compared to prior approaches while maintaining high adversarial effectiveness.", "conclusion": "The M2A framework effectively addresses both effectiveness and precision limitations in acoustic adversarial attacks, establishing a new benchmark for targeted attacks on polyphonic SED systems through systematic constraint design and metric innovation."}}
{"id": "2510.02162", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02162", "abs": "https://arxiv.org/abs/2510.02162", "authors": ["Cristian Bassotto", "Ermes Franch", "Marina Kr\u010dek", "Stjepan Picek"], "title": "NoMod: A Non-modular Attack on Module Learning With Errors", "comment": null, "summary": "The advent of quantum computing threatens classical public-key cryptography,\nmotivating NIST's adoption of post-quantum schemes such as those based on the\nModule Learning With Errors (Module-LWE) problem. We present NoMod ML-Attack, a\nhybrid white-box cryptanalytic method that circumvents the challenge of\nmodeling modular reduction by treating wrap-arounds as statistical corruption\nand casting secret recovery as robust linear estimation. Our approach combines\noptimized lattice preprocessing--including reduced-vector saving and algebraic\namplification--with robust estimators trained via Tukey's Biweight loss.\nExperiments show NoMod achieves full recovery of binary secrets for dimension\n$n = 350$, recovery of sparse binomial secrets for $n = 256$, and successful\nrecovery of sparse secrets in CRYSTALS-Kyber settings with parameters $(n, k) =\n(128, 3)$ and $(256, 2)$. We release our implementation in an anonymous\nrepository https://anonymous.4open.science/r/NoMod-3BD4.", "AI": {"tldr": "This paper introduces NoMod ML-Attack, a machine learning-based cryptanalytic technique that breaks post-quantum Module-LWE schemes, achieving secret recovery at scales surpassing prior work while using open-source availability to ensure transparency.", "motivation": "Classical public-key cryptography faces obsolescence from quantum computing. NIST's post-quantum standardization initiative needs cryptanalysis to strengthen proposals like Module-LWE against real-world attacks.", "method": "NoMod ML-Attack uses a hybrid white-box approach that (1) models modular wrap-arounds as statistical corruption, (2) frames secret recovery as robust linear estimation, and (3) employs lattice preprocessing techniques (e.g., algebraic amplification) with Tukey's Biweight loss-trained estimators.", "result": "Full recovery of binary secrets at n=350, sparse binomial secrets at n=256, and sparse CRYSTALS-Kyber secrets for (n,k)=(128,3) and (256,2). Implementation is publicly available at https://anonymous.4open.science/r/NoMod-3BD4.", "conclusion": "The NoMod ML-Attack demonstrates practical vulnerabilities in Module-LWE-based post-quantum cryptography and highlights the necessity of refining parameter choices (e.g., n, k) for improved security. The open-source release further enables transparent evaluation."}}
{"id": "2510.02184", "categories": ["cs.CR", "nlin.CD", "34C15, 68M25, 94A60"], "pdf": "https://arxiv.org/pdf/2510.02184", "abs": "https://arxiv.org/abs/2510.02184", "authors": ["N. A. Anagnostopoulos", "K. Konstantinidis", "A. N. Miliou", "S. G. Stavrinides"], "title": "Testing Stability and Robustness in Three Cryptographic Chaotic Systems", "comment": "Published as \"N. A. Anagnostopoulos, K. Konstantinidis, A. N. Miliou\n  & S. G. Stavrinides, \"Testing Stability and Robustness in Three Cryptographic\n  Chaotic Systems\", Proceedings of the 3rd International Interdisciplinary\n  Symposium on Chaos and Complex Systems (CCS 2010), Journal of Concrete And\n  Applicable Mathematics (JCAAM), vol. 9, iss. 3, pp. 247-261, Eudoxus Press,\n  2011\"; no longer available", "summary": "In practical applications, it is crucial that the drive-response systems,\nalthough identical in all respects, are synchronized at all times, even if\nthere is noise present. In this work, we test the stability and robustness of\nthree distinct and well-known cryptographic chaotic systems, and compare the\nresults in relation to the desired security.", "AI": {"tldr": "This paper evaluates the stability, robustness, and security of three cryptographic chaotic drive-response systems under noise conditions through comparative analysis.", "motivation": "Ensuring synchronization in drive-response systems is critical for practical applications, particularly in maintaining security when noise is present.", "method": "The study tests three established cryptographic chaotic systems, comparing their performance in terms of stability, robustness, and security metrics under noise.", "result": "Results highlight critical differences in the systems' ability to maintain synchronization and security under noisy environments.", "conclusion": "The comparative analysis provides insights into selecting or designing chaotic systems for secure and reliable synchronization in real-world applications."}}
