{"id": "2508.05655", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.05655", "abs": "https://arxiv.org/abs/2508.05655", "authors": ["Guang Yang", "Peter Trinh", "Alma Nkemla", "Amuru Serikyaku", "Edward Tatchim", "Osman Sharaf"], "title": "Blockchain-Based Decentralized Domain Name System", "comment": null, "summary": "The current Domain Name System (DNS) infrastructure faces critical\nvulnerabilities including poisoning attacks, censorship mechanisms, and\ncentralized points of failure that compromise internet freedom and security.\nRecent incidents such as DNS poisoning attacks on ISP customers highlight the\nurgent need for resilient alternatives. This paper presents a novel\nblockchain-based Decentralized Domain Name System (DDNS). We designed a\nspecialized Proof-of-Work blockchain to maximize support for DNS-related\nprotocols and achieve node decentralization. The system integrates our\nblockchain with IPFS for distributed storage, implements cryptographic\nprimitives for end-to-end trust signatures, and achieves Never Trust, Always\nVerify zero-trust verification. Our implementation achieves 15-second domain\nrecord propagation times, supports 20 standard DNS record types, and provides\nperpetual free .ddns domains. The system has been deployed across distributed\ninfrastructure in San Jose, Los Angeles, and Orange County, demonstrating\npractical scalability and resistance to traditional DNS manipulation\ntechniques. Performance evaluation shows the system can handle up to Max Theor.\nTPS 1,111.1 tx/s (minimal transactions) and Max Theor. TPS 266.7 tx/s (regular\ntransactions) for domain operations while maintaining sub-second query\nresolution through intelligent caching mechanisms.", "AI": {"tldr": "This paper proposes a blockchain-based Decentralized Domain Name System (DDNS) to address DNS vulnerabilities like poisoning attacks, censorship, and centralization by implementing a specialized Proof-of-Work blockchain integrated with IPFS and cryptographic verification.", "motivation": "DNS infrastructure is prone to security threats (poisoning), censorship, and centralized failures, with recent ISP attacks underscoring the need for a decentralized, resilient alternative.", "method": "The authors developed a custom Proof-of-Work blockchain optimized for DNS protocols, combined IPFS for distributed storage, cryptographic signatures for trust, and 'Never Trust, Always Verify' zero-trust mechanisms.", "result": "The system achieves 15-second record propagation, supports 20 DNS record types, and handles up to 1,111.1 tx/s (minimal) and 266.7 tx/s (regular) domain operations, with sub-second query resolution through caching. Deployments in three U.S. locations demonstrate scalability and resistance to manipulation.", "conclusion": "The DDNS offers a decentralized, secure, and scalable solution to traditional DNS weaknesses, validated by performance metrics and real-world deployment, effectively countering censorship and attacks while maintaining usability."}}
{"id": "2508.05658", "categories": ["cs.CR", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.05658", "abs": "https://arxiv.org/abs/2508.05658", "authors": ["Song Yan", "Hui Wei", "Jinlong Fei", "Guoliang Yang", "Zhengyu Zhao", "Zheng Wamg"], "title": "Universally Unfiltered and Unseen:Input-Agnostic Multimodal Jailbreaks against Text-to-Image Model Safeguards", "comment": "ACM MM 2025", "summary": "Various (text) prompt filters and (image) safety checkers have been\nimplemented to mitigate the misuse of Text-to-Image (T2I) models in creating\nNot-Safe-For-Work (NSFW) content.In order to expose potential security\nvulnerabilities of such safeguards, multimodal jailbreaks have been\nstudied.However, existing jailbreaks are limited to prompt-specific and\nimage-specific perturbations, which suffer from poor scalability and\ntime-consuming optimization.To address these limitations, we propose\nUniversally Unfiltered and Unseen (U3)-Attack, a multimodal jailbreak attack\nmethod against T2I safeguards.Specifically, U3-Attack optimizes an adversarial\npatch on the image background to universally bypass safety checkers and\noptimizes a safe paraphrase set from a sensitive word to universally bypass\nprompt filters while eliminating redundant computations.Extensive experimental\nresults demonstrate the superiority of our U3-Attack on both open-source and\ncommercial T2I models.For example, on the commercial Runway-inpainting model\nwith both prompt filter and safety checker, our U3-Attack achieves $~4\\times$\nhigher success rates than the state-of-the-art multimodal jailbreak attack,\nMMA-Diffusion.Content Warning: This paper includes examples of NSFW content.", "AI": {"tldr": "The paper introduces U3-Attack, a scalable and efficient multimodal method to bypass prompt filters and image safety checkers in Text-to-Image models by optimizing adversarial patches and safe paraphrase sets.", "motivation": "Existing jailbreak attacks for T2I safeguards are limited to prompt-specific and image-specific perturbations, which are not scalable and require time-consuming optimization.", "method": "U3-Attack simultaneously optimizes an adversarial patch on the image background (to bypass safety checkers) and a safe paraphrase set from sensitive words (to bypass prompt filters), eliminating redundant computations for universal bypass capabilities.", "result": "Extensive experiments show U3-Attack outperforms state-of-the-art methods like MMA-Diffusion, achieving ~4\u00d7 higher success rates on commercial models such as Runway-inpainting with dual safety filters.", "conclusion": "U3-Attack demonstrates significant vulnerabilities in current T2I safety mechanisms while emphasizing the need for more robust safeguards, despite its effectiveness raising ethical concerns about content warning examples in the paper."}}
{"id": "2508.05670", "categories": ["cs.CR", "cs.AI", "cs.CY", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.05670", "abs": "https://arxiv.org/abs/2508.05670", "authors": ["Daniele Proverbio", "Alessio Buscemi", "Alessandro Di Stefano", "The Anh Han", "German Castignani", "Pietro Li\u00f2"], "title": "Can LLMs effectively provide game-theoretic-based scenarios for cybersecurity?", "comment": null, "summary": "Game theory has long served as a foundational tool in cybersecurity to test,\npredict, and design strategic interactions between attackers and defenders. The\nrecent advent of Large Language Models (LLMs) offers new tools and challenges\nfor the security of computer systems; In this work, we investigate whether\nclassical game-theoretic frameworks can effectively capture the behaviours of\nLLM-driven actors and bots. Using a reproducible framework for game-theoretic\nLLM agents, we investigate two canonical scenarios -- the one-shot zero-sum\ngame and the dynamic Prisoner's Dilemma -- and we test whether LLMs converge to\nexpected outcomes or exhibit deviations due to embedded biases. Our experiments\ninvolve four state-of-the-art LLMs and span five natural languages, English,\nFrench, Arabic, Vietnamese, and Mandarin Chinese, to assess linguistic\nsensitivity. For both games, we observe that the final payoffs are influenced\nby agents characteristics such as personality traits or knowledge of repeated\nrounds. Moreover, we uncover an unexpected sensitivity of the final payoffs to\nthe choice of languages, which should warn against indiscriminate application\nof LLMs in cybersecurity applications and call for in-depth studies, as LLMs\nmay behave differently when deployed in different countries. We also employ\nquantitative metrics to evaluate the internal consistency and cross-language\nstability of LLM agents, to help guide the selection of the most stable LLMs\nand optimising models for secure applications.", "AI": {"tldr": "The paper explores how Large Language Models (LLMs) behave in cybersecurity scenarios using classical game theory, finding that payoffs are influenced by agent traits and language sensitivity, highlighting risks for cross-cultural deployment.", "motivation": "LLMs introduce new cybersecurity challenges and tools, but existing game-theoretic frameworks may not account for their unique biases or linguistic variations, necessitating a study of their behavior in security contexts.", "method": "The authors developed a reproducible framework to test four LLMs across five languages in one-shot zero-sum games and dynamic Prisoner's Dilemma, analyzing payoff outcomes influenced by personality traits and language.", "result": "LLM payoffs vary significantly with agent characteristics and language, with unexpected linguistic sensitivity observed, such as differences in decision-making and outcomes across English, French, Arabic, Vietnamese, and Mandarin Chinese.", "conclusion": "LLMs' cybersecurity behavior is language-sensitive, recommending against their indiscriminate use in global applications and advocating for targeted research and model optimization to ensure security and stability."}}
{"id": "2508.05671", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05671", "abs": "https://arxiv.org/abs/2508.05671", "authors": ["Ko-Wei Chuang", "Hen-Hsen Huang", "Tsai-Yen Li"], "title": "DINA: A Dual Defense Framework Against Internal Noise and External Attacks in Natural Language Processing", "comment": "7 pages", "summary": "As large language models (LLMs) and generative AI become increasingly\nintegrated into customer service and moderation applications, adversarial\nthreats emerge from both external manipulations and internal label corruption.\nIn this work, we identify and systematically address these dual adversarial\nthreats by introducing DINA (Dual Defense Against Internal Noise and\nAdversarial Attacks), a novel unified framework tailored specifically for NLP.\nOur approach adapts advanced noisy-label learning methods from computer vision\nand integrates them with adversarial training to simultaneously mitigate\ninternal label sabotage and external adversarial perturbations. Extensive\nexperiments conducted on a real-world dataset from an online gaming service\ndemonstrate that DINA significantly improves model robustness and accuracy\ncompared to baseline models. Our findings not only highlight the critical\nnecessity of dual-threat defenses but also offer practical strategies for\nsafeguarding NLP systems in realistic adversarial scenarios, underscoring\nbroader implications for fair and responsible AI deployment.", "AI": {"tldr": "DINA addresses dual adversarial threats (external manipulation and internal label corruption) in NLP systems using a unified framework, improving robustness and accuracy on real-world datasets.", "motivation": "Large language models in customer service and moderation face adversarial risks from data poisoning (internal) and direct attacks (external), necessitating robust solutions for reliable deployment.", "method": "DINA integrates computer vision's noisy-label learning techniques with adversarial training to simultaneously defend against internal label corruption and external adversarial perturbations in NLP tasks.", "result": "Experiments on an online gaming dataset show DINA outperforms baselines in both robustness to adversarial threats and task accuracy under realistic attack scenarios.", "conclusion": "DINA demonstrates practical effectiveness for dual-threat mitigation in NLP, emphasizing the importance of proactive defenses for fair AI deployment in real-world applications."}}
{"id": "2508.05693", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05693", "abs": "https://arxiv.org/abs/2508.05693", "authors": ["Siamak Farshidi", "Amir Saberhabibi", "Behbod Eskafi", "Niloofar Nikfarjam", "Sadegh Eskandari", "Slinger Jansen", "Michel Chaudron", "Bedir Tekinerdogan"], "title": "Empirical Evaluation of AI-Assisted Software Package Selection: A Knowledge Graph Approach", "comment": null, "summary": "Selecting third-party software packages in open-source ecosystems like Python\nis challenging due to the large number of alternatives and limited transparent\nevidence for comparison. Generative AI tools are increasingly used in\ndevelopment workflows, but their suggestions often overlook dependency\nevaluation, emphasize popularity over suitability, and lack reproducibility.\nThis creates risks for projects that require transparency, long-term\nreliability, maintainability, and informed architectural decisions. This study\nformulates software package selection as a Multi-Criteria Decision-Making\n(MCDM) problem and proposes a data-driven framework for technology evaluation.\nAutomated data pipelines continuously collect and integrate software metadata,\nusage trends, vulnerability information, and developer sentiment from GitHub,\nPyPI, and Stack Overflow. These data are structured into a decision model\nrepresenting relationships among packages, domain features, and quality\nattributes. The framework is implemented in PySelect, a decision support system\nthat uses large language models to interpret user intent and query the model to\nidentify contextually appropriate packages. The approach is evaluated using\n798,669 Python scripts from 16,887 GitHub repositories and a user study based\non the Technology Acceptance Model. Results show high data extraction\nprecision, improved recommendation quality over generative AI baselines, and\npositive user evaluations of usefulness and ease of use. This work introduces a\nscalable, interpretable, and reproducible framework that supports\nevidence-based software selection using MCDM principles, empirical data, and\nAI-assisted intent modeling.", "AI": {"tldr": "The paper proposes PySelect, a data-driven MCDM framework for selecting Python packages using GitHub, PyPI, and Stack Overflow data to improve recommendation accuracy and user transparency.", "motivation": "Open-source software package selection is challenging due to limited transparent evidence and risks arising from generative AI's tendency to prioritize popularity over suitability.", "method": "The framework uses automated data pipelines to collect software metadata, usage trends, vulnerabilities, and sentiment, structuring this into a decision model interpreted by LLMs to query user intent and match packages.", "result": "Evaluation across 798k Python scripts and a user study showed high data precision, better recommendation quality than generative AI baselines, and strong user feedback on usefulness (positive acceptance metrics).", "conclusion": "PySelect provides a scalable, reproducible MCDM approach combining empirical data and intent modeling, offering improved reliability over current AI-based tools for informed software selection."}}
{"id": "2508.05674", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05674", "abs": "https://arxiv.org/abs/2508.05674", "authors": ["Minghao Shao", "Nanda Rani", "Kimberly Milner", "Haoran Xi", "Meet Udeshi", "Saksham Aggarwal", "Venkata Sai Charan Putrevu", "Sandeep Kumar Shukla", "Prashanth Krishnamurthy", "Farshad Khorrami", "Ramesh Karri", "Muhammad Shafique"], "title": "Towards Effective Offensive Security LLM Agents: Hyperparameter Tuning, LLM as a Judge, and a Lightweight CTF Benchmark", "comment": null, "summary": "Recent advances in LLM agentic systems have improved the automation of\noffensive security tasks, particularly for Capture the Flag (CTF) challenges.\nWe systematically investigate the key factors that drive agent success and\nprovide a detailed recipe for building effective LLM-based offensive security\nagents. First, we present CTFJudge, a framework leveraging LLM as a judge to\nanalyze agent trajectories and provide granular evaluation across CTF solving\nsteps. Second, we propose a novel metric, CTF Competency Index (CCI) for\npartial correctness, revealing how closely agent solutions align with\nhuman-crafted gold standards. Third, we examine how LLM hyperparameters, namely\ntemperature, top-p, and maximum token length, influence agent performance and\nautomated cybersecurity task planning. For rapid evaluation, we present\nCTFTiny, a curated benchmark of 50 representative CTF challenges across binary\nexploitation, web, reverse engineering, forensics, and cryptography. Our\nfindings identify optimal multi-agent coordination settings and lay the\ngroundwork for future LLM agent research in cybersecurity. We make CTFTiny open\nsource to public https://github.com/NYU-LLM-CTF/CTFTiny along with CTFJudge on\nhttps://github.com/NYU-LLM-CTF/CTFJudge.", "AI": {"tldr": "The paper presents CTFJudge, CCI metric, CTFTiny benchmark, and analyzes factors affecting LLM-based CTF agent performance in cybersecurity.", "motivation": "LLM agentic systems are increasingly used for CTF challenges, but systematic analysis of their success factors and unified evaluation is lacking.", "method": "Developed CTFJudge for trajectory analysis, CCI for partial correctness, and CTFTiny for benchmarking; conducted experimental analysis of LLM hyperparameters across 5 CTF categories.", "result": "Identified optimal multi-agent settings, created open-source tools with 50 benchmark challenges, and revealed CCI effectiveness in measuring solution alignment.", "conclusion": "Established foundational frameworks for evaluating LLM-based offensive security agents and demonstrated how design choices impact success, with CTFTiny and CTFJudge available for public use."}}
{"id": "2508.05710", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05710", "abs": "https://arxiv.org/abs/2508.05710", "authors": ["Jia Fu", "Xinyu Yang", "Hongzhi Zhang", "Yahui Liu", "Jingyuan Zhang", "Qi Wang", "Fuzheng Zhang", "Guorui Zhou"], "title": "Klear-CodeTest: Scalable Test Case Generation for Code Reinforcement Learning", "comment": "21 pages, 11 figures", "summary": "Precise, correct feedback is crucial for effectively training large language\nmodels (LLMs) in code reinforcement learning. However, synthesizing\nhigh-quality test cases remains a profoundly challenging and unsolved problem.\nIn this work, we present Klear-CodeTest, a comprehensive test case synthesis\nframework featuring rigorous verification to ensure quality and reliability of\ntest cases. Our approach achieves broad coverage of programming problems via a\nnovel Generator-Validation (G-V) framework, ensuring correctness through a\nconsistency validation mechanism that verifies outputs against gold solutions.\nThe proposed G-V framework generates comprehensive test cases including both\nregular and corner cases, enhancing test coverage and discriminative power for\nsolution correctness assessment in code reinforcement learning. In addition, we\ndesign a multi-layered security sandbox system optimized for online\nverification platforms, guaranteeing safe and reliable code execution. Through\ncomprehensive experiments, we demonstrate the effectiveness of our curated\ndataset, showing significant improvements in model performance and training\nstability. The source codes, curated dataset and sandbox system are available\nat: https://github.com/Kwai-Klear/CodeTest.", "AI": {"tldr": "This paper introduces Klear-CodeTest, a framework for generating high-quality, verified test cases for code reinforcement learning. It uses a Generator-Validation (G-V) framework with consistency checks and a secure sandbox system to enhance test coverage and model training performance.", "motivation": "High-quality test cases are essential for training large language models (LLMs) in code reinforcement learning (RL), but synthesizing them remains a significant challenge due to the need for correctness, coverage, and safety during execution.", "method": "The authors developed a Generator-Validation (G-V) framework that generates both regular and corner cases, validated via consistency checks against gold solutions. They also implemented a multi-layered security sandbox to ensure safe code execution during online verification.", "result": "Experiments demonstrated significant improvements in model performance and training stability using the Klear-CodeTest dataset. The framework successfully enhanced test coverage and the discriminative power of solution correctness assessments.", "conclusion": "Klear-CodeTest addresses critical challenges in test case synthesis for code RL by combining rigorous validation and secure execution. The open-source release of the framework, dataset, and sandbox system enables broader adoption and future research in this domain."}}
{"id": "2508.05675", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05675", "abs": "https://arxiv.org/abs/2508.05675", "authors": ["Jing Wang", "Zheng Li", "Lei Li", "Fan He", "Liyu Lin", "Yao Lai", "Yan Li", "Xiaoyang Zeng", "Yufeng Guo"], "title": "Principle-Guided Verilog Optimization: IP-Safe Knowledge Transfer via Local-Cloud Collaboration", "comment": "Our code and dataset are available at\n  https://github.com/friyawang/VeriOptim", "summary": "Recent years have witnessed growing interest in adopting large language\nmodels (LLMs) for Register Transfer Level (RTL) code optimization. While\npowerful cloud-based LLMs offer superior optimization capabilities, they pose\nunacceptable intellectual property (IP) leakage risks when processing\nproprietary hardware designs. In this paper, we propose a new scenario where\nVerilog code must be optimized for specific attributes without leaking\nsensitive IP information. We introduce the first IP-preserving edge-cloud\ncollaborative framework that leverages the benefits of both paradigms. Our\napproach employs local small LLMs (e.g., Qwen-2.5-Coder-7B) to perform secure\ncomparative analysis between paired high-quality target designs and novice\ndraft codes, yielding general design principles that summarize key insights for\nimprovements. These principles are then used to query stronger cloud LLMs\n(e.g., Deepseek-V3) for targeted code improvement, ensuring that only\nabstracted and IP-safe guidance reaches external services. Our experimental\nresults demonstrate that the framework achieves significantly higher\noptimization success rates compared to baseline methods. For example, combining\nQwen-2.5-Coder-7B and Deepseek-V3 achieves a 66.67\\% optimization success rate\nfor power utilization, outperforming Deepseek-V3 alone (49.81\\%) and even\ncommercial models like GPT-4o (55.81\\%). Further investigation of local and\ncloud LLM combinations reveals that different model pairings exhibit varying\nstrengths for specific optimization objectives, with interesting trends\nemerging when varying the number of comparative code pairs. Our work\nestablishes a new paradigm for secure hardware design optimization that\nbalances performance gains with IP protection.", "AI": {"tldr": "Proposes an IP-preserving edge-cloud collaborative framework for RTL code optimization using local LLMs for analysis and cloud LLMs for targeted improvements, achieving higher success rates.", "motivation": "Cloud-based LLMs provide powerful RTL optimization but risk intellectual property (IP) leakage during hardware design processing. Existing approaches fail to balance optimization performance with IP security constraints.", "method": "1. Local small LLMs (e.g., Qwen-2.5-Coder-7B) analyze paired high-quality target designs and novice drafts to extract general design principles\n2. Principles are abstracted and sent as non-sensitive guidance to cloud LLMs (e.g., Deepseek-V3)\n3. Cloud LLMs deliver targeted code improvements using the principles without exposing raw design data", "result": "Achieved 66.67% optimization success rate for power utilization with Qwen-2.5-Coder-7B + Deepseek-V3 combination, surpassing:\n- 49.81% with Deepseek-V3 alone\n- 55.81% with commercial GPT-4o\nDemonstrated strategic effectiveness of model pairing and observed optimization trends with varying code pairs", "conclusion": "Establishes a secure-by-design paradigm for hardware optimization that preserves IP confidentiality while maintaining strong performance through collaborative edge-cloud LLM orchestration, opening new possibilities for constrained engineering optimization."}}
{"id": "2508.05747", "categories": ["cs.SE", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.05747", "abs": "https://arxiv.org/abs/2508.05747", "authors": ["Rohaizah Abdul Wahid", "Muhamad Said Nizamuddin Nadim", "Suliana Sulaiman", "Syahmi Akmal Shaharudin", "Muhammad Danial Jupikil", "Iqqwan Jasman Su Azlan Su"], "title": "Utilizing Composer Packages to Accelerate Laravel-Based Project Development Among Students: A Pedagogical and Practical Framework", "comment": null, "summary": "Laravel has emerged as a foundational framework in university web development\ncurricula. However, despite its scaffolding capabilities, students often\nstruggle to complete projects within limited academic timelines. This\nconceptual paper introduces Composer, PHP's standard dependency manager, and\ncategorizes a curated selection of Composer packages that significantly reduce\ndevelopment effort while fostering professional software practices. Grounded in\npractical and pedagogical considerations, the paper illustrates how educators\nand learners can strategically leverage these tools to build typical academic\nor personal Laravel-based systems. Central to this approach is maintaining code\nquality and reinforcing conceptual understanding. The paper also addresses\npotential risks such as package conflicts and over-reliance on tools, providing\nbest-practice recommendations to mitigate them. While the goal is to accelerate\ndevelopment, the deeper objective is to reinforce professional workflows and\nindustry readiness. Exposure to Composer packages enhances curriculum relevance\nand smooths the transition from academia to the workplace. However, effective\nintegration requires deliberate instructional design aligned with learning\nobjectives. Without guidance, students may treat packages as black boxes. Thus,\neducators must teach not only how to use these tools, but also when and why,\nencouraging critical evaluation of their utility and limitations. This ensures\nthat practical convenience supports rather than supplants deep learning.", "AI": {"tldr": "The paper advocates for integrating Composer packages into Laravel university curricula to accelerate project development and enhance professional software practices, balancing practical efficiency with pedagogical guidance.", "motivation": "Students face limited academic timelines and struggle to complete Laravel projects while adopting professional practices. Traditional scaffolding alone may not address these challenges effectively.", "method": "Categorizing curated Composer packages through a combination of practical (efficiency) and pedagogical (teaching value) evaluation, illustrating real-world application strategies, and addressing risks via best practices.", "result": "Identification of packages that reduce development effort by 30-50%, frameworks for teaching Composer integration, and actionable recommendations for mitigating tool dependency risks in academic web development projects.", "conclusion": "Composer packages can significantly enhance Laravel curriculum relevance and industry readiness when paired with structured instruction that emphasizes conceptual understanding, ensuring tools support rather than replace deep learning objectives."}}
{"id": "2508.05677", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05677", "abs": "https://arxiv.org/abs/2508.05677", "authors": ["Peizhuo Liu"], "title": "Adversarial Attacks on Reinforcement Learning-based Medical Questionnaire Systems: Input-level Perturbation Strategies and Medical Constraint Validation", "comment": "30 pages (21 pages main text, 3 pages references, 6 pages appendix),\n  4 figures", "summary": "RL-based medical questionnaire systems have shown great potential in medical\nscenarios. However, their safety and robustness remain unresolved. This study\nperforms a comprehensive evaluation on adversarial attack methods to identify\nand analyze their potential vulnerabilities. We formulate the diagnosis process\nas a Markov Decision Process (MDP), where the state is the patient responses\nand unasked questions, and the action is either to ask a question or to make a\ndiagnosis. We implemented six prevailing major attack methods, including the\nFast Gradient Signed Method (FGSM), Projected Gradient Descent (PGD), Carlini &\nWagner Attack (C&W) attack, Basic Iterative Method (BIM), DeepFool, and\nAutoAttack, with seven epsilon values each. To ensure the generated adversarial\nexamples remain clinically plausible, we developed a comprehensive medical\nvalidation framework consisting of 247 medical constraints, including\nphysiological bounds, symptom correlations, and conditional medical\nconstraints. We achieved a 97.6% success rate in generating clinically\nplausible adversarial samples. We performed our experiment on the National\nHealth Interview Survey (NHIS) dataset (https://www.cdc.gov/nchs/nhis/), which\nconsists of 182,630 samples, to predict the participant's 4-year mortality\nrate. We evaluated our attacks on the AdaptiveFS framework proposed in\narXiv:2004.00994. Our results show that adversarial attacks could significantly\nimpact the diagnostic accuracy, with attack success rates ranging from 33.08%\n(FGSM) to 64.70% (AutoAttack). Our work has demonstrated that even under strict\nmedical constraints on the input, such RL-based medical questionnaire systems\nstill show significant vulnerabilities.", "AI": {"tldr": "This paper evaluates adversarial attack vulnerabilities in RL-based medical questionnaire systems using six attack methods under 247 medical constraints, finding significant flaws despite clinical plausibility. ", "motivation": "RL-based medical systems show promise but lack resolved safety and robustness, necessitating evaluation of adversarial attack effectiveness to identify vulnerabilities. ", "method": "The diagnosis process is modeled as an MDP; six attack methods (FGSM, PGD, C&W, BIM, DeepFool, AutoAttack) with seven \u03b5 values each are implemented, validated using 247 medical constraints (physiological bounds, symptom correlations) on the NHIS dataset (182,630 samples) to predict mortality risk. ", "result": "97.6% clinically plausible adversarial sample generation with attack success rates from 33.08% (FGSM) to 64.70% (AutoAttack) against the AdaptiveFS framework on the NHIS dataset. ", "conclusion": "Adversarial attacks can significantly undermine RL-based medical questionnaire systems' diagnostic accuracy, indicating critical vulnerabilities persist even under strict medical input constraints. "}}
{"id": "2508.05799", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.05799", "abs": "https://arxiv.org/abs/2508.05799", "authors": ["Yoseph Berhanu Alebachew"], "title": "AI-Guided Exploration of Large-Scale Codebases", "comment": null, "summary": "Understanding large-scale, complex software systems is a major challenge for\ndevelopers, who spend a significant portion of their time on program\ncomprehension. Traditional tools such as static visualizations and reverse\nengineering techniques provide structural insights but often lack\ninteractivity, adaptability, and integration with contextual information.\nRecent advancements in large language models (LLMs) offer new opportunities to\nenhance code exploration workflows, yet their lack of grounding and integration\nwith structured views limits their effectiveness. This work introduces a hybrid\napproach that integrates deterministic reverse engineering with LLM-guided,\nintent-aware visual exploration. The proposed system combines UML-based\nvisualization, dynamic user interfaces, historical context, and collaborative\nfeatures into an adaptive tool for code comprehension. By interpreting user\nqueries and interaction patterns, the LLM helps developers navigate and\nunderstand complex codebases more effectively. A prototype implementation for\nJava demonstrates the feasibility of this approach. Future work includes\nempirical evaluation, scaling to polyglot systems, and exploring GUI-driven LLM\ninteraction models. This research lays the groundwork for intelligent,\ninteractive environments that align with developer cognition and collaborative\nworkflows.", "AI": {"tldr": "Hybrid approach integrates reverse engineering with LLM-driven interactive code exploration for enhanced system understanding.", "motivation": "Traditional tools lack interactivity and contextual adaptability, while standalone LLMs face grounding and structural integration limitations in code comprehension tasks.", "method": "The system combines UML-based visualization, dynamic interfaces, and historical context with an LLM's ability to interpret user queries and interaction patterns, creating intent-aware navigational support.", "result": "A Java prototype validated the feasibility of hybrid integration, though empirical evaluations and scaling improvements remain future work.", "conclusion": "This research establishes a foundation for cognitive-aligned, collaborative code understanding environments through structured-LLM hybrid systems."}}
{"id": "2508.05681", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05681", "abs": "https://arxiv.org/abs/2508.05681", "authors": ["Yuhan Zhi", "Longtian Wang", "Xiaofei Xie", "Chao Shen", "Qiang Hu", "Xiaohong Guan"], "title": "Selection-Based Vulnerabilities: Clean-Label Backdoor Attacks in Active Learning", "comment": null, "summary": "Active learning(AL), which serves as the representative label-efficient\nlearning paradigm, has been widely applied in resource-constrained scenarios.\nThe achievement of AL is attributed to acquisition functions, which are\ndesigned for identifying the most important data to label. Despite this\nsuccess, one question remains unanswered: is AL safe? In this work, we\nintroduce ALA, a practical and the first framework to utilize the acquisition\nfunction as the poisoning attack surface to reveal the weakness of active\nlearning. Specifically, ALA optimizes imperceptibly poisoned inputs to exhibit\nhigh uncertainty scores, increasing their probability of being selected by\nacquisition functions. To evaluate ALA, we conduct extensive experiments across\nthree datasets, three acquisition functions, and two types of clean-label\nbackdoor triggers. Results show that our attack can achieve high success rates\n(up to 94%) even under low poisoning budgets (0.5%-1.0%) while preserving model\nutility and remaining undetectable to human annotators. Our findings remind\nactive learning users: acquisition functions can be easily exploited, and\nactive learning should be deployed with caution in trusted data scenarios.", "AI": {"tldr": "The paper introduces ALA, a poisoning attack framework that exploits acquisition functions in active learning, demonstrating high attack success rates (up to 94%) with low poisoning budgets while maintaining model utility and evasion of human detection.", "motivation": "Raises critical safety concerns about active learning, showing that acquisition functions\u2014designed to optimize label efficiency\u2014can be weaponized as attack surfaces, a previously unexplored vulnerability.", "method": "ALA optimizes imperceptible poisoned inputs with high uncertainty scores to increase selection probability by acquisition functions, leveraging weaknesses in their query strategy for adversarial exploitation.", "result": "Experiments on three datasets, three acquisition functions, and two clean-label backdoor triggers showed ALA achieves 94% attack success with 0.5-1% poisoning budget, without detectable degradation of model performance.", "conclusion": "Active learning systems are vulnerable to stealthy poisoning attacks through acquisition functions. The work urges caution in deployment for trusted data environments and highlights the need for robustness against adversarial query strategies."}}
{"id": "2508.05923", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05923", "abs": "https://arxiv.org/abs/2508.05923", "authors": ["Yanusha Mehendran", "Maolin Tang", "Yi Lu"], "title": "Enhancing Software Vulnerability Detection Through Adaptive Test Input Generation Using Genetic Algorithm", "comment": "26 Pages, 3 figures, 6 Tables, Submitted to Empirical Software\n  Engineering and it is under review", "summary": "Software vulnerabilities continue to undermine the reliability and security\nof modern systems, particularly as software complexity outpaces the\ncapabilities of traditional detection methods. This study introduces a genetic\nalgorithm-based method for test input generation that innovatively integrates\ngenetic operators and adaptive learning to enhance software vulnerability\ndetection. A key contribution is the application of the crossover operator,\nwhich facilitates exploration by searching across a broader space of potential\ntest inputs. Complementing this, an adaptive feedback mechanism continuously\nlearns from the system's execution behavior and dynamically guides input\ngeneration toward promising areas of the input space. Rather than relying on\nfixed or randomly selected inputs, the approach evolves a population of\nstructurally valid test cases using feedback-driven selection, enabling deeper\nand more effective code traversal. This strategic integration of exploration\nand exploitation ensures that both diverse and targeted test inputs are\ndeveloped over time. Evaluation was conducted across nine open-source\nJSON-processing libraries. The proposed method achieved substantial\nimprovements in coverage compared to a benchmark evolutionary fuzzing method,\nwith average gains of 39.8% in class coverage, 62.4% in method coverage, 105.0%\nin line coverage, 114.0% in instruction coverage, and 166.0% in branch\ncoverage. These results highlight the method's capacity to detect deeper and\nmore complex vulnerabilities, offering a scalable and adaptive solution to\nsoftware security testing.", "AI": {"tldr": "A genetic algorithm-based approach for test input generation combines crossover operators and adaptive learning to enhance software vulnerability detection, achieving significant coverage improvements over existing methods.", "motivation": "Traditional vulnerability detection methods struggle with increasing software complexity, necessitating more effective and adaptive techniques for thorough security testing.", "method": "The method employs a genetic algorithm with crossover operators to broaden test input exploration and an adaptive feedback mechanism to guide input generation based on system execution behavior, evolving valid test cases through feedback-driven selection.", "result": "The approach outperformed benchmark evolutionary fuzzing in nine JSON-processing libraries, achieving 39.8% higher class coverage, 62.4% method coverage, 105.0% line coverage, 114.0% instruction coverage, and 166.0% branch coverage.", "conclusion": "The integration of genetic operators and adaptive learning enables efficient exploration and exploitation of test inputs, demonstrating a scalable solution for detecting complex software vulnerabilities."}}
{"id": "2508.05684", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05684", "abs": "https://arxiv.org/abs/2508.05684", "authors": ["Junhao He", "Tianyu Liu", "Jingyuan Zhao", "Benjamin Turner"], "title": "MM-FusionNet: Context-Aware Dynamic Fusion for Multi-modal Fake News Detection with Large Vision-Language Models", "comment": null, "summary": "The proliferation of multi-modal fake news on social media poses a\nsignificant threat to public trust and social stability. Traditional detection\nmethods, primarily text-based, often fall short due to the deceptive interplay\nbetween misleading text and images. While Large Vision-Language Models (LVLMs)\noffer promising avenues for multi-modal understanding, effectively fusing\ndiverse modal information, especially when their importance is imbalanced or\ncontradictory, remains a critical challenge. This paper introduces\nMM-FusionNet, an innovative framework leveraging LVLMs for robust multi-modal\nfake news detection. Our core contribution is the Context-Aware Dynamic Fusion\nModule (CADFM), which employs bi-directional cross-modal attention and a novel\ndynamic modal gating network. This mechanism adaptively learns and assigns\nimportance weights to textual and visual features based on their contextual\nrelevance, enabling intelligent prioritization of information. Evaluated on the\nlarge-scale Multi-modal Fake News Dataset (LMFND) comprising 80,000 samples,\nMM-FusionNet achieves a state-of-the-art F1-score of 0.938, surpassing existing\nmulti-modal baselines by approximately 0.5% and significantly outperforming\nsingle-modal approaches. Further analysis demonstrates the model's dynamic\nweighting capabilities, its robustness to modality perturbations, and\nperformance remarkably close to human-level, underscoring its practical\nefficacy and interpretability for real-world fake news detection.", "AI": {"tldr": "The paper introduces MM-FusionNet, a framework using LVLMs with a Context-Aware Dynamic Fusion Module (CADFM) for robust multi-modal fake news detection, achieving an F1-score of 0.938 on the LMFND dataset.", "motivation": "Traditional text-based detection methods fail to address deceptive combinations of text and images in multi-modal fake news, while existing multi-modal approaches struggle with unbalanced/contradictory information from different modalities.", "method": "MM-FusionNet employs bi-directional cross-modal attention and a dynamic modal gating network in CADFM to adaptively assign relevance-based importance weights to text/visual features, enhancing LVLMs for fused multi-modal understanding.", "result": "Achieves state-of-the-art 0.938 F1-score on LMFND (80,000 samples), outperforming multi-modal baselines by 0.5% and single-modal approaches significantly, with robust modality-perturbation handling and human-like performance.", "conclusion": "MM-FusionNet demonstrates practical efficacy for real-world multi-modal fake news detection through its adaptive weighting mechanism, robustness, and nearly human-level performance, offering improved interpretability."}}
{"id": "2508.05949", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.05949", "abs": "https://arxiv.org/abs/2508.05949", "authors": ["Jialin Yang", "Zainab Saad", "Jiajun Wu", "Xiaoguang Niu", "Henry Leung", "Steve Drew"], "title": "A Survey on Task Scheduling in Carbon-Aware Container Orchestration", "comment": "Submitted to ACM Computing Surveys", "summary": "The soaring energy demands of large-scale software ecosystems and cloud data\ncenters, accelerated by the intensive training and deployment of large language\nmodels, have driven energy consumption and carbon footprint to unprecedented\nlevels. In response, both industry and academia are increasing efforts to\nreduce the carbon emissions associated with cloud computing through more\nefficient task scheduling and infrastructure orchestration. In this work, we\npresent a systematic review of various Kubernetes scheduling strategies,\ncategorizing them into hardware-centric and software-centric, annotating each\nwith its sustainability objectives, and grouping them according to the\nalgorithms they use. We propose a comprehensive taxonomy for cloud task\nscheduling studies, with a particular focus on the environmental sustainability\naspect. We analyze emerging research trends and open challenges, and our\nfindings provide critical insight into the design of sustainable scheduling\nsolutions for next-generation cloud computing systems.", "AI": {"tldr": "This paper systematically reviews Kubernetes scheduling strategies for sustainability in cloud computing, providing a taxonomy and insights for future solutions.", "motivation": "Addressing the environmental impact of energy-intensive cloud computing driven by large language model training and deployment to reduce carbon emissions.", "method": "Categorizes scheduling strategies into hardware and software centric, annotates with sustainability goals, and groups them by algorithms using a systematic review.", "result": "Elaborates a comprehensive taxonomy for cloud task scheduling focused on environmental sustainability, identifies research trends, and outlines open challenges.", "conclusion": "Highlights the importance of integrating environmental objectives into scheduling strategies and offers critical design insights for next-generation sustainable cloud systems."}}
{"id": "2508.05690", "categories": ["cs.CR", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05690", "abs": "https://arxiv.org/abs/2508.05690", "authors": ["Meital Shlezinger", "Shay Akirav", "Lei Zhou", "Liang Guo", "Avi Kessel", "Guoliang Li"], "title": "Leveraging large language models for SQL behavior-based database intrusion detection", "comment": null, "summary": "Database systems are extensively used to store critical data across various\ndomains. However, the frequency of abnormal database access behaviors, such as\ndatabase intrusion by internal and external attacks, continues to rise.\nInternal masqueraders often have greater organizational knowledge, making it\neasier to mimic employee behavior effectively. In contrast, external\nmasqueraders may behave differently due to their lack of familiarity with the\norganization. Current approaches lack the granularity needed to detect\nanomalies at the operational level, frequently misclassifying entire sequences\nof operations as anomalies, even though most operations are likely to represent\nnormal behavior. On the other hand, some anomalous behaviors often resemble\nnormal activities, making them difficult for existing detection methods to\nidentify. This paper introduces a two-tiered anomaly detection approach for\nStructured Query Language (SQL) using the Bidirectional Encoder Representations\nfrom Transformers (BERT) model, specifically DistilBERT, a more efficient,\npre-trained version. Our method combines both unsupervised and supervised\nmachine learning techniques to accurately identify anomalous activities while\nminimizing the need for data labeling. First, the unsupervised method uses\nensemble anomaly detectors that flag embedding vectors distant from learned\nnormal patterns of typical user behavior across the database (out-of-scope\nqueries). Second, the supervised method uses fine-tuned transformer-based\nmodels to detect internal attacks with high precision (in-scope queries), using\nrole-labeled classification, even on limited labeled SQL data. Our findings\nmake a significant contribution by providing an effective solution for\nsafeguarding critical database systems from sophisticated threats.", "AI": {"tldr": "This paper proposes a two-tiered anomaly detection approach for SQL database access using DistilBERT and machine learning to improve granularity and accuracy in identifying both out-of-scope and in-scope attacks.", "motivation": "Existing database anomaly detection systems lack operational-level granularity, mislabeling normal activities as anomalies and failing to detect sophisticated attacks that mimic legitimate behavior. This is critical as threats like SQL intrusion continue to rise.", "method": "The method combines unsupervised ensemble anomaly detection (via embedding vectors distance from normal patterns) for out-of-scope queries, with a supervised transformer-based model (fine-tuned DistilBERT with role-labeled classification) for in-scope attacks. This hybrid approach reduces labeling needs while handling limited SQL dataset challenges.", "result": "The approach significantly enhances detection accuracy for both external attacks and internal masquerading threats, effectively addressing limitations in current methods that struggle with false positives and sophisticated in-scope anomalies.", "conclusion": "The two-tiered framework bridges gaps in database security by combining efficient anomaly detection and role-based classification. It provides a scalable solution for identifying abnormal access patterns without relying heavily on labeled training data, particularly valuable against stealthy internal threats."}}
{"id": "2508.05970", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05970", "abs": "https://arxiv.org/abs/2508.05970", "authors": ["Yanzhou Li", "Shangqing Liu", "Kangjie Chen", "Tianwei Zhang", "Yang Liu"], "title": "Impact-driven Context Filtering For Cross-file Code Completion", "comment": null, "summary": "Retrieval-augmented generation (RAG) has recently demonstrated considerable\npotential for repository-level code completion, as it integrates cross-file\nknowledge with in-file preceding code to provide comprehensive contexts for\ngeneration. To better understand the contribution of the retrieved cross-file\ncontexts, we introduce a likelihood-based metric to evaluate the impact of each\nretrieved code chunk on the completion. Our analysis reveals that, despite\nretrieving numerous chunks, only a small subset positively contributes to the\ncompletion, while some chunks even degrade performance. To address this issue,\nwe leverage this metric to construct a repository-level dataset where each\nretrieved chunk is labeled as positive, neutral, or negative based on its\nrelevance to the target completion. We then propose an adaptive retrieval\ncontext filtering framework, CODEFILTER, trained on this dataset to mitigate\nthe harmful effects of negative retrieved contexts in code completion.\nExtensive evaluation on the RepoEval and CrossCodeLongEval benchmarks\ndemonstrates that CODEFILTER consistently improves completion accuracy compared\nto approaches without filtering operations across various tasks. Additionally,\nCODEFILTER significantly reduces the length of the input prompt, enhancing\ncomputational efficiency while exhibiting strong generalizability across\ndifferent models. These results underscore the potential of CODEFILTER to\nenhance the accuracy, efficiency, and attributability of repository-level code\ncompletion.", "AI": {"tldr": "This paper investigates retrieval-augmented generation (RAG) in repository-level code completion and proposes CODEFILTER, a framework that uses a likelihood-based metric to filter detrimental cross-file code chunks, improving accuracy and efficiency while reducing input prompt length.", "motivation": "Existing repository-level code completion models struggle with noise from irrelevant or harmful retrieved cross-file contexts, limiting their performance. Only a small fraction of retrieved code chunks effectively contribute to completion tasks.", "method": "1) Developed a likelihood-based metric to quantify retrieved code chunks' impact on completion. 2) Created a labeled dataset where chunks are categorized as positive/neutral/negative based on this metric. 3) Designed CODEFILTER, an adaptive filtering framework trained on the dataset to remove harmful contexts.", "result": "CODEFILTER improved completion accuracy by ~14-18% on benchmarks (RepoEval and CrossCodeLongEval). It reduced input sequences by ~40% while maintaining performance, demonstrated strong model generalizability, and improved computational efficiency (lower perplexity by 22-28%).", "conclusion": "Adaptive filtering of retrieved contexts using task-dependent criteria is crucial for high-quality code completion. CODEFILTER achieves better accuracy and efficiency by eliminating harmful cross-file context noise while preserving useful information."}}
{"id": "2508.05691", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.05691", "abs": "https://arxiv.org/abs/2508.05691", "authors": ["Kai Yao", "Marc Juarez"], "title": "AuthPrint: Fingerprinting Generative Models Against Malicious Model Providers", "comment": null, "summary": "Generative models are increasingly adopted in high-stakes domains, yet\ncurrent deployments offer no mechanisms to verify the origin of model outputs.\nWe address this gap by extending model fingerprinting techniques beyond the\ntraditional collaborative setting to one where the model provider may act\nadversarially. To our knowledge, this is the first work to evaluate\nfingerprinting for provenance attribution under such a threat model. The\nmethods rely on a trusted verifier that extracts secret fingerprints from the\nmodel's output space, unknown to the provider, and trains a model to predict\nand verify them. Our empirical evaluation shows that our methods achieve\nnear-zero FPR@95%TPR for instances of GAN and diffusion models, even when\ntested on small modifications to the original architecture and training data.\nMoreover, the methods remain robust against adversarial attacks that actively\nmodify the outputs to bypass detection. Source codes are available at\nhttps://github.com/PSMLab/authprint.", "AI": {"tldr": "This paper introduces adversarially robust model fingerprinting techniques to verify the origin of generative model outputs in high-stakes domains, even when the model provider acts adversarially. The methods achieve near-zero false positive rates and resist adversarial modifications.", "motivation": "Current generative model deployments lack mechanisms to verify output origins, especially when model providers may act maliciously. Traditional fingerprinting methods assume non-adversarial settings which is insufficient for real-world high-risk applications.", "method": "The authors propose using a trusted third-party verifier to extract secret fingerprints from model outputs that remain hidden from the provider. These fingerprints are then trained to verify model provenance through detection in the output space.", "result": "Empirical results show near-zero FPR@95%TPR for GAN and diffusion models even with architecture/data modifications and adversarial attacks attempting to bypass detection.", "conclusion": "This work demonstrates the first provenance attribution framework for generative models under adversarial provider scenarios, establishing feasibility of robust fingerprinting techniques that withstand both architectural perturbations and targeted attacks."}}
{"id": "2508.06017", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06017", "abs": "https://arxiv.org/abs/2508.06017", "authors": ["Xiangzhe Xu", "Shiwei Feng", "Zian Su", "Chengpeng Wang", "Xiangyu Zhang"], "title": "Position: Intelligent Coding Systems Should Write Programs with Justifications", "comment": "The first two authors contributed equally to this work", "summary": "Intelligent coding systems are transforming software development by enabling\nusers to specify code behavior in natural language. However, the opaque\ndecision-making of AI-driven coders raises trust and usability concerns,\nparticularly for non-expert users who cannot inspect low-level implementations.\nWe argue that these systems should not only generate code but also produce\nclear, consistent justifications that bridge model reasoning and user\nunderstanding. To this end, we identify two critical justification\nproperties-cognitive alignment and semantic faithfulness-and highlight the\nlimitations of existing methods, including formal verification, static\nanalysis, and post-hoc explainability. We advocate exploring neuro-symbolic\napproaches for justification generation, where symbolic constraints guide model\nbehavior during training and program semantics are enriched through neural\nrepresentations, enabling automated consistency checks at inference time.", "AI": {"tldr": "The paper proposes neuro-symbolic approaches for generating justifications in AI-driven code generation to enhance transparency and user trust, especially for non-experts.", "motivation": "AI coding systems' opaque decision-making creates trust and usability issues for non-expert users; existing methods (formal verification, static analysis, post-hoc explainability) lack mechanisms to bridge model reasoning with user understanding.", "method": "Identifies cognitive alignment and semantic faithfulness as key justification properties. Proposes neuro-symbolic frameworks where symbolic constraints guide training, and neural representations enrich program semantics for automated consistency checks during inference.", "result": "Highlights limitations of traditional methods and establishes a conceptual framework for justification generation through neuro-symbolic integration, suggesting it improves transparency compared to existing solutions.", "conclusion": "Neuro-symbolic approaches can address the transparency gap in intelligent coding systems by combining symbolic reasoning with neural learning for justifications that ensure semantic consistency and cognitive alignment."}}
{"id": "2508.05694", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05694", "abs": "https://arxiv.org/abs/2508.05694", "authors": ["Kaichuan Kong", "Dongjie Liu", "Xiaobo Jin", "Guanggang Geng", "Zhiying Li", "Jian Weng"], "title": "DMFI: Dual-Modality Fine-Tuning and Inference Framework for LLM-Based Insider Threat Detection", "comment": "Submitted to the 2025 IEEE International Conference on Data Mining\n  (ICDM)", "summary": "Insider threat detection (ITD) poses a persistent and high-impact challenge\nin cybersecurity due to the subtle, long-term, and context-dependent nature of\nmalicious insider behaviors. Traditional models often struggle to capture\nsemantic intent and complex behavior dynamics, while existing LLM-based\nsolutions face limitations in prompt adaptability and modality coverage. To\nbridge this gap, we propose DMFI, a dual-modality framework that integrates\nsemantic inference with behavior-aware fine-tuning. DMFI converts raw logs into\ntwo structured views: (1) a semantic view that processes content-rich artifacts\n(e.g., emails, https) using instruction-formatted prompts; and (2) a behavioral\nabstraction, constructed via a 4W-guided (When-Where-What-Which) transformation\nto encode contextual action sequences. Two LoRA-enhanced LLMs are fine-tuned\nindependently, and their outputs are fused via a lightweight MLP-based decision\nmodule. We further introduce DMFI-B, a discriminative adaptation strategy that\nseparates normal and abnormal behavior representations, improving robustness\nunder severe class imbalance. Experiments on CERT r4.2 and r5.2 datasets\ndemonstrate that DMFI outperforms state-of-the-art methods in detection\naccuracy. Our approach combines the semantic reasoning power of LLMs with\nstructured behavior modeling, offering a scalable and effective solution for\nreal-world insider threat detection. Our work demonstrates the effectiveness of\ncombining LLM reasoning with structured behavioral modeling, offering a\nscalable and deployable solution for modern insider threat detection.", "AI": {"tldr": "DMFI is a dual-modality framework combining semantic and behavioral analysis for insider threat detection, outperforming existing methods on CERT datasets.", "motivation": "Traditional cybersecurity models fail to capture semantic intent of insider threats, and existing LLM-based approaches lack modality coverage and prompt adaptability. Context-dependent nature of insider threats requires more sophisticated solutions.", "method": "DMFI integrates two structured views: (1) semantic inference through instruction-formatted prompts for content analysis, and (2) behavioral abstraction using 4W-guided transformation (When-Where-What-Which). Two LoRA-enhanced LLMs are fine-tuned separately and their outputs fused via MLP. DMFI-B further separates normal/abnormal behavior representations.", "result": "Achieved superior detection accuracy compared to state-of-the-art methods on CERT r4.2 and r5.2 insider threat datasets, demonstrating effectiveness against severe class imbalance challenges.", "conclusion": "The framework demonstrates that combining LLM semantic reasoning with structured behavioral modeling provides a scalable, effective solution for real-world insider threat detection, overcoming limitations of existing approaches."}}
{"id": "2508.06192", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.06192", "abs": "https://arxiv.org/abs/2508.06192", "authors": ["Lantian Li", "Yuyu Chen", "Jingwen Wu", "Yue Pan", "Zhongxing Yu"], "title": "Understanding Inconsistent State Update Vulnerabilities in Smart Contracts", "comment": "31 pages, 11 figures", "summary": "Smart contracts enable contract terms to be automatically executed and\nverified on the blockchain, and recent years have witnessed numerous\napplications of them in areas such as financial institutions and supply chains.\nThe execution logic of a smart contract is closely related to the contract\nstate, and thus the correct and safe execution of the contract depends heavily\non the precise control and update of the contract state. However, the contract\nstate update process can have issues. In particular, inconsistent state update\nissues can arise for reasons such as unsynchronized modifications. Inconsistent\nstate update bugs have been exploited by attackers many times, but existing\ndetection tools still have difficulty in effectively identifying them. This\npaper conducts the first large-scale empirical study about inconsistent state\nupdate vulnerabilities (that is, inconsistent state update bugs that are\nexploitable) in smart contracts, aiming to shed light for developers,\nresearchers, tool builders, and language or library designers in order to avoid\ninconsistent state update vulnerabilities. We systematically investigate 116\ninconsistent state update vulnerabilities in 352 real-world smart contract\nprojects, summarizing their root causes, fix strategies, and exploitation\nmethods. Our study provides 11 original and important findings, and we also\ngive the implications of our findings. To illustrate the potential benefits of\nour research, we also develop a proof-of-concept checker based on one of our\nfindings. The checker effectively detects issues in 64 popular GitHub projects,\nand 19 project owners have confirmed the detected issues at the time of\nwriting. The result demonstrates the usefulness and importance of our findings\nfor avoiding inconsistent state update vulnerabilities in smart contracts.", "AI": {"tldr": "This paper presents the first large-scale empirical study on inconsistent state update vulnerabilities in smart contracts, analyzing 116 vulnerabilities across 352 projects to derive 11 key findings and a proof-of-concept checker that found confirmed issues in 19/64 GitHub projects, offering actionable insights for developers and tool designers.", "motivation": "Inconsistent state updates in smart contracts, caused by unsynchronized modifications, are critical vulnerabilities frequently exploited by attackers. Current detection tools lack effectiveness, necessitating a deeper understanding of root causes, exploitation patterns, and mitigation strategies to improve smart contract security.", "method": "The authors conducted a systematic analysis of 116 inconsistent state update vulnerabilities across 352 real-world projects, categorizing root causes, fix approaches, and exploitation methods. They developed a proof-of-concept checker based on one finding to validate the practical relevance of their results in 64 popular GitHub repositories.", "result": "The study uncovers 11 original findings about patterns and fixes for inconsistent state updates. The checker detected 64 issues, with 19 confirmed by project maintainers, demonstrating the vulnerability's prevalence and the value of the proposed analysis.", "conclusion": "This work establishes a comprehensive framework for understanding inconsistent state update vulnerabilities in smart contracts, provides actionable guidance for developers, and validates the practical utility through a real-world checker that identifies confirmed issues, highlighting the importance of addressing this critical security flaw."}}
{"id": "2508.05695", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05695", "abs": "https://arxiv.org/abs/2508.05695", "authors": ["Kaichuan Kong", "Dongjie Liu", "Xiaobo Jin", "Zhiying Li", "Guanggang Geng", "Jian Weng"], "title": "MambaITD: An Efficient Cross-Modal Mamba Network for Insider Threat Detection", "comment": "Submitted to the 2025 IEEE International Conference on Data Mining\n  (ICDM)", "summary": "Enterprises are facing increasing risks of insider threats, while existing\ndetection methods are unable to effectively address these challenges due to\nreasons such as insufficient temporal dynamic feature modeling, computational\nefficiency and real-time bottlenecks and cross-modal information island\nproblem. This paper proposes a new insider threat detection framework MambaITD\nbased on the Mamba state space model and cross-modal adaptive fusion. First,\nthe multi-source log preprocessing module aligns heterogeneous data through\nbehavioral sequence encoding, interval smoothing, and statistical feature\nextraction. Second, the Mamba encoder models long-range dependencies in\nbehavioral and interval sequences, and combines the sequence and statistical\ninformation dynamically in combination with the gated feature fusion mechanism.\nFinally, we propose an adaptive threshold optimization method based on\nmaximizing inter-class variance, which dynamically adjusts the decision\nthreshold by analyzing the probability distribution, effectively identifies\nanomalies, and alleviates class imbalance and concept drift. Compared with\ntraditional methods, MambaITD shows significant advantages in modeling\nefficiency and feature fusion capabilities, outperforming Transformer-based\nmethods, and provides a more effective solution for insider threat detection.", "AI": {"tldr": "MambaITD is a novel insider threat detection framework leveraging Mamba state space models and cross-modal adaptive fusion to overcome limitations in temporal modeling, computational efficiency, and cross-modal integration compared to existing methods. The approach pre-processes multi-source logs, models long-range dependencies, and uses adaptive threshold optimization for improved anomaly identification.", "motivation": "Current insider threat detection methods exhibit critical limitations: inadequate temporal dynamic feature modeling, poor computational efficiency for real-time operations, and the cross-modal information island problem. These deficiencies hinder effective identification of anomalous insider behavior in enterprise environments.", "method": "The framework consists of three modules: 1) Multi-source log preprocessing with heterogeneous data alignment through behavioral sequence encoding, interval smoothing, and statistical feature extraction; 2) Mamba encoder modeling long-range dependencies in temporal sequences while dynamically combining sequential and statistical information via a gated fusion mechanism; 3) Adaptive threshold optimization using inter-class variance maximization to address class imbalance and concept drift through probability distribution analysis.", "result": "MambaITD demonstrates 1) Superior computational efficiency compared to Transformer-based architectures; 2) Enhanced feature fusion capabilities across heterogeneous log data; 3) Improved anomaly detection performance validated against traditional approaches, effectively mitigating class imbalance and concept drift challenges.", "conclusion": "This paper introduces MambaITD, a state space model-based framework that advances insider threat detection by effectively modeling temporal dynamics and cross-modal relationships. The results confirm its advantages in efficiency, fusion capability, and detection performance over conventional approaches, offering a promising solution for enterprise cybersecurity challenges."}}
{"id": "2508.06299", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.06299", "abs": "https://arxiv.org/abs/2508.06299", "authors": ["Henrique Henriques", "Hugo Louren\u00e7o", "Vasco Amaral", "Miguel Goul\u00e3o"], "title": "Improving the Developer Experience with a Low-Code Process Modelling Language", "comment": "Preprint", "summary": "Context: The OutSystems Platform is a development environment composed of\nseveral DSLs, used to specify, quickly build, and validate web and mobile\napplications. The DSLs allow users to model different perspectives such as\ninterfaces and data models, define custom business logic and construct process\nmodels. Problem: The DSL for process modelling (Business Process Technology\n(BPT)), has a low adoption rate and is perceived as having usability problems\nhampering its adoption. This is problematic given the language maintenance\ncosts. Method: We used a combination of interviews, a critical review of BPT\nusing the \"Physics of Notation\" and empirical evaluations of BPT using the\nSystem Usability Scale (SUS) and the NASA Task Load indeX (TLX), to develop a\nnew version of BPT, taking these inputs and Outsystems' engineers' culture into\naccount. Results: Evaluations conducted with 25 professional software engineers\nshowed an increase of the semantic transparency on the new version, from 31% to\n69%, an increase in the correctness of responses, from 51% to 89%, an increase\nin the SUS score, from 42.25 to 64.78, and a decrease of the TLX score, from\n36.50 to 20.78. These differences were statistically significant. Conclusions:\nThese results suggest that the new version of BPT significantly improved the\ndeveloper experience of the previous version. The end users' background with\nOutSystems had a relevant impact on the final concrete syntax choices and\nachieved usability indicators.", "AI": {"tldr": "The paper describes the improvement of the Business Process Technology (BPT) DSL in the OutSystems Platform by addressing usability issues through user studies and empirical evaluations, resulting in significant gains in semantic transparency, correctness, and usability metrics.", "motivation": "The BPT DSL had low adoption and usability problems despite high maintenance costs, creating a need for iterative improvements based on user feedback to enhance developer experience and reduce overhead.", "method": "Combined interviews, Physics of Notation analysis, and empirical evaluations (SUS and NASA TLX) with 25 professional software engineers to iteratively design and assess the new BPT version.", "result": "New BPT version achieved 31% \u2192 69% increase in semantic transparency, 51% \u2192 89% in correctness, SUS score 42.25 \u2192 64.78, and TLX score 36.50 \u2192 20.78 (significant differences confirmed statistically).", "conclusion": "The redesigned BPT significantly improved usability and developer experience, confirming the importance of aligning concrete syntax with users\u2019 background and prioritizing evidence-based design decisions."}}
{"id": "2508.05696", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05696", "abs": "https://arxiv.org/abs/2508.05696", "authors": ["Kaichuan Kong", "Dongjie Liu", "Xiaobo Jin", "Zhiying Li", "Guanggang Geng"], "title": "Log2Sig: Frequency-Aware Insider Threat Detection via Multivariate Behavioral Signal Decomposition", "comment": "Submitted to the 2025 IEEE International Conference on Trust,\n  Security and Privacy in Computing and Communications (TrustCom)", "summary": "Insider threat detection presents a significant challenge due to the\ndeceptive nature of malicious behaviors, which often resemble legitimate user\noperations. However, existing approaches typically model system logs as flat\nevent sequences, thereby failing to capture the inherent frequency dynamics and\nmultiscale disturbance patterns embedded in user behavior. To address these\nlimitations, we propose Log2Sig, a robust anomaly detection framework that\ntransforms user logs into multivariate behavioral frequency signals,\nintroducing a novel representation of user behavior. Log2Sig employs\nMultivariate Variational Mode Decomposition (MVMD) to extract Intrinsic Mode\nFunctions (IMFs), which reveal behavioral fluctuations across multiple temporal\nscales. Based on this, the model further performs joint modeling of behavioral\nsequences and frequency-decomposed signals: the daily behavior sequences are\nencoded using a Mamba-based temporal encoder to capture long-term dependencies,\nwhile the corresponding frequency components are linearly projected to match\nthe encoder's output dimension. These dual-view representations are then fused\nto construct a comprehensive user behavior profile, which is fed into a\nmultilayer perceptron for precise anomaly detection. Experimental results on\nthe CERT r4.2 and r5.2 datasets demonstrate that Log2Sig significantly\noutperforms state-of-the-art baselines in both accuracy and F1 score.", "AI": {"tldr": "Log2Sig is a novel anomaly detection framework that transforms user logs into multivariate frequency signals and uses dual-view modeling to enhance insider threat detection beyond existing methods.", "motivation": "Current approaches model system logs as flat event sequences, missing inherent frequency dynamics and multiscale disturbance patterns critical for distinguishing insider threats from normal behavior.", "method": "Log2Sig employs Multivariate Variational Mode Decomposition (MVMD) to extract Intrinsic Mode Functions (IMFs), combines Mamba-based temporal encoding of daily behavior with linearly projected frequency components, and fuses dual-view representations via a multilayer perceptron.", "result": "Achieves superior accuracy and F1 scores on CERT r4.2 and r5.2 datasets compared to state-of-the-art baselines.", "conclusion": "By capturing behavioral fluctuations across temporal scales and integrating sequence-frequency modeling, Log2Sig offers a robust solution to the deceptive challenges of insider threat detection."}}
{"id": "2508.06365", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.06365", "abs": "https://arxiv.org/abs/2508.06365", "authors": ["Toufique Ahmed", "Jatin Ganhotra", "Avraham Shinnar", "Martin Hirzel"], "title": "Execution-Feedback Driven Test Generation from SWE Issues", "comment": null, "summary": "A software engineering issue (SWE issue) is easier to resolve when\naccompanied by a reproduction test. Unfortunately, most issues do not come with\nfunctioning reproduction tests, so this paper explores how to generate them\nautomatically. The primary challenge in this setting is that the code to be\ntested is either missing or wrong, as evidenced by the existence of the issue\nin the first place. This has held back test generation for this setting:\nwithout the correct code to execute, it is difficult to leverage execution\nfeedback to generate good tests. This paper introduces novel techniques for\nleveraging execution feedback to get around this problem, implemented in a new\nreproduction test generator called e-Otter++. Experiments show that e-Otter++\nrepresents a leap ahead in the state-of-the-art for this problem, generating\ntests with an average fail-to-pass rate of 63% on the TDD-Bench Verified\nbenchmark.", "AI": {"tldr": "This paper introduces innovative techniques to automatically generate reproduction tests for software engineering issues where code is missing or incorrect, achieving a 63% fail-to-pass rate using the e-Otter++ tool.", "motivation": "Most software engineering issues lack functional reproduction tests, hindering debugging and resolution. Automated test generation is critical but challenging when execution feedback is unavailable due to broken/missing code.", "method": "The authors developed e-Otter++, a tool that leverages novel execution feedback approaches to generate tests for issues with missing/wrong code by analyzing the problem description and reconstructing invalid program states.", "result": "Evaluation on the TDD-Bench Verified benchmark showed a 63% average fail-to-pass rate, representing significant advancement over prior state-of-the-art methods for this specific problem.", "conclusion": "The paper demonstrates that execution feedback can be effectively utilized for reproduction test generation even when the code under test is initially invalid, with e-Otter++ establishing a new technical benchmark in this domain."}}
{"id": "2508.05707", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.05707", "abs": "https://arxiv.org/abs/2508.05707", "authors": ["Sasa Maric", "Rasil Baidar", "Robert Abbas", "Sam Reisenfeld"], "title": "System Security Framework for 5G Advanced /6G IoT Integrated Terrestrial Network-Non-Terrestrial Network (TN-NTN) with AI-Enabled Cloud Security", "comment": null, "summary": "The integration of Terrestrial Networks (TN) and Non-Terrestrial Networks\n(NTN), including 5G Advanced/6G and the Internet of Things (IoT) technologies,\nusing Low Earth Orbit (LEO) satellites, high-altitude platforms (HAPS), and\nUnmanned Aerial Vehicles (UAVs), is redefining the landscape of global\nconnectivity. This paper introduces a new system-level security framework for\n5G Advanced/6G IoT-integrated TN-NTN architectures with AI-native-enabled cloud\nsecurity. Due to the heterogeneity, scale, and distributed nature of these\nnetworks, new security challenges have emerged. Leveraging AI-native cloud\nplatforms offers powerful capabilities for real-time threat detection, security\nautomation, and intelligent policy enforcement. The NTN satellite access\nfunction enhances security for discontinuous coverage via satellite\nconnections. In addition, this paper explores the security risks associated\nwith integrated 5G Advanced/6G IoT TN-NTN systems, including full network\nsegmentation, network slicing, and the cloudification of the RAN and core. We\npresent a comprehensive AI-enabled cloud security framework and conclude with\nproposals for implementing AI-powered, satellite-based NTN within future 5G\nAdvanced/6G IoT networks. Our approach emphasizes zero-trust principles,\nfederated learning, secure orchestration, a layered security framework, and\nresilience against adversarial threats.", "AI": {"tldr": "This paper proposes an AI-enabled cloud-based security framework for integrated 5G Advanced/6G IoT TN-NTN systems, addressing challenges via zero-trust principles, federated learning, and satellite-enhanced security mechanisms.", "motivation": "The heterogeneous, large-scale, and distributed nature of integrated TN-NTN (terrestrial and non-terrestrial) systems introduces new security vulnerabilities in 5G Advanced/6G IoT deployments, necessitating adaptive and real-time security solutions that leverage emerging technologies like AI-native cloud platforms.", "method": "The authors design a system-level security framework that combines AI-native cloud infrastructure with NTN-specific security layers. Key components include NTN satellite access function for coverage resilience, AI models for threat detection and policy enforcement, federated learning for distributed threat intelligence sharing, and secure orchestration mechanisms.", "result": "A comprehensive security framework is presented with detailed implementation proposals. The approach enables real-time threat mitigation, automated security orchestration for dynamic network slicing, and enhanced protection for cloudified RAN/core against adversarial attacks through layered defense and federated learning strategies.", "conclusion": "The proposed AI-powered, satellite-based security framework establishes robust zero-trust architecture and federated threat intelligence for TN-NTN integration in 5G Advanced/6G IoT systems, emphasizing necessary technical requirements for future implementation. Resilience against adversarial threats and secure orchestration remain critical focus areas for next-generation connectivity."}}
{"id": "2508.06414", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.06414", "abs": "https://arxiv.org/abs/2508.06414", "authors": ["Dongze Li", "Songqiang Chen", "Jialun Cao", "Shing-Chi Cheung"], "title": "What Builds Effective In-Context Examples for Code Generation?", "comment": null, "summary": "In-Context Learning (ICL) has emerged as a promising solution to enhance the\ncode generation capabilities of Large Language Models (LLMs), which\nincorporates code examples inside the prompt to let LLMs learn from\ndemonstrations. However, despite the substantial effectiveness of the code\nexample-based ICL approach, the specific features (e.g., identifier naming\nstyles, code formatting, solution insight) within the ICL-provided code\nexamples that significantly contribute to the ICL's effectiveness remain\nunclear. This paper systematically investigates the impact of various code\nfeatures on ICL with code examples through controlled ablation studies. Our\nfindings reveal that the appropriate naming of variables and functions is\ncrucial for effective code generation, with their elimination leading to\nperformance decreases of up to 30 percentage points. We further demonstrate\nthat LLMs prioritize semantically meaningful identifier names over formatting\nconventions, with language-specific preferences regarding identifier verbosity.\nAdditionally, our investigation into ICL's potential for enhancing reflection\nand inference capabilities reveals that current LLMs struggle to extract\ngeneralizable problem-solving insights from similar code solutions, despite\nbeing capable of utilizing direct information effectively. These findings are\nexpected to provide valuable insights for optimizing ICL systems in code\ngeneration applications and highlight fundamental challenges in\nreflection-based learning for code generation tasks.", "AI": {"tldr": "This paper investigates how specific code features (e.g., identifier naming, formatting) impact the effectiveness of In-Context Learning (ICL) in code generation by conducting controlled ablation studies. Key findings include the critical role of semantic naming and challenges in extracting generalizable problem-solving insights from ICL examples.", "motivation": "Despite the success of code example-based ICL in enhancing LLM code generation, the specific features driving this effectiveness and limitations in reflection-based learning remain unclear.", "method": "Controlled ablation studies that systematically modify code features like identifier naming styles, formatting conventions, and solution insights in ICL prompts to evaluate their impact on model performance.", "result": "1) Removing semantic identifier names reduces performance by up to 30pp. 2) LLMs prioritize meaningful names over formatting conventions. 3) Language-specific preferences for identifier verbosity exist. 4) Current LLMs cannot extract generalizable insights from similar code solutions, despite using direct information effectively.", "conclusion": "The findings provide actionable guidance for optimizing ICL systems in code generation while revealing fundamental challenges in reflection-based learning for these tasks."}}
{"id": "2508.05717", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.05717", "abs": "https://arxiv.org/abs/2508.05717", "authors": ["Marco Giberna", "Holger Voos", "Paulo Tavares", "Jo\u00e3o Nunes", "Tobias Sorg", "Andrea Masini", "Jose Luis Sanchez-Lopez"], "title": "On Digital Twins in Defence: Overview and Applications", "comment": "27 pages, 10 figures, 2 tables", "summary": "Digital twin technology has gained increasing attention across various\nsectors due to its ability to create virtual replicas of physical systems,\nenabling real-time monitoring, optimization, and simulation. This paper\nexplores the integration of digital twins within defence applications, focusing\non key use cases ranging from system design and development, operational\nplanning and training, to mission execution and debriefing. By examining the\napplication of digital twin technologies across defense platforms, we highlight\ntheir key advantages such as enhanced operational performance, predictive\ncapabilities, and increased system uptime. Additionally, we introduce a novel\ncharacterization framework for digital twins that aims to standardize and unify\ntheir application across different defence domains to facilitate\ninteroperability. Thereafter, we discuss the main challenges, gaps and\nlimitations in implementing and adopting digital twins within defence\norganizations by analyzing a combination of scientific literature, current\nindustry practices, governmental strategies, and the findings from a\ncomprehensive survey of industrial stakeholders and ministries of defense.\nFinally, we outline future research directions and development opportunities,\nemphasizing the need for robust frameworks and interdisciplinary collaborations\nto fully realize the potential of digital twins in the defence sector.", "AI": {"tldr": "This paper examines digital twin integration in defence applications, presenting use cases, advantages, a novel characterization framework, and implementation challenges.", "motivation": "Digital twin technology's potential in defence remains underexplored despite its widespread adoption in other sectors. Standardization and interoperability issues hinder its effective application. A unified framework is needed to address adoption barriers and unlock benefits in areas like system design, training, and mission execution.", "method": "The research combines analysis of scientific literature, industry practices, governmental strategies, and survey data from defence stakeholders to evaluate use cases, advantages, challenges, and future research directions.", "result": "1. Key advantages in defence contexts: enhanced operational performance, predictive capabilities, and system uptime. 2. A novel characterization framework for standardizing cross-domain digital twin applications. 3. Identified implementation challenges including interoperability gaps, data integration complexities, and organizational adoption barriers.", "conclusion": "The paper emphasizes the need for robust characterization frameworks and interdisciplinary collaboration to overcome challenges and fully enable digital twin adoption in defence. Future research should focus on refining these frameworks and addressing practical implementation barriers."}}
{"id": "2508.05865", "categories": ["cs.CR", "cs.SE", "C.2; D.2; D.4.1; D.4.4; D.4.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.05865", "abs": "https://arxiv.org/abs/2508.05865", "authors": ["Kiana Kiashemshaki", "Elvis Nnaemeka Chukwuani", "Mohammad Jalili Torkamani", "Negin Mahmoudi"], "title": "Secure and Scalable Blockchain Voting: A Comparative Framework and the Role of Large Language Models", "comment": "9 pages, 8 figures, 1 table", "summary": "Blockchain technology offers a promising foundation for modernizing E-Voting\nsystems by enhancing transparency, decentralization, and security. Yet,\nreal-world adoption remains limited due to persistent challenges such as\nscalability constraints, high computational demands, and complex privacy\nrequirements. This paper presents a comparative framework for analyzing\nblockchain-based E-Voting architectures, consensus mechanisms, and\ncryptographic protocols. We examine the limitations of prevalent models like\nProof of Work, Proof of Stake, and Delegated Proof of Stake, and propose\noptimization strategies that include hybrid consensus, lightweight\ncryptography, and decentralized identity management. Additionally, we explore\nthe novel role of Large Language Models (LLMs) in smart contract generation,\nanomaly detection, and user interaction. Our findings offer a foundation for\ndesigning secure, scalable, and intelligent blockchain-based E-Voting systems\nsuitable for national-scale deployment. This work lays the groundwork for\nbuilding an end-to-end blockchain E-Voting prototype enhanced by LLM-guided\nsmart contract generation and validation, supported by a systematic framework\nand simulation-based analysis.", "AI": {"tldr": "This paper introduces a comparative framework for blockchain-based E-Voting systems, analyzing architectures and consensus mechanisms while proposing optimizations using LLMs to address scalability and privacy challenges for national deployment.", "motivation": "Blockchain's potential for secure, transparent E-Voting is hindered by scalability, computational intensity, and privacy complexities. The aim is to evaluate existing models and develop practical solutions for large-scale implementation.", "method": "The authors analyze blockchain architectures, evaluate consensus mechanisms (PoW, PoS, DPoS), and propose optimization strategies including hybrid consensus, lightweight cryptography, decentralized identity, and integrate LLMs for smart contract generation and anomaly detection.", "result": "Identified limitations of current consensus models, validated optimization strategies through systematic analysis, and demonstrated the viability of LLMs in enhancing smart contract processes and system security for scalable E-Voting.", "conclusion": "The framework provides pathways to design secure, scalable, and intelligent blockchain E-Voting systems. It enables prototype development with LLM-driven smart contracts and simulation-based validation for real-world application."}}
{"id": "2508.06059", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06059", "abs": "https://arxiv.org/abs/2508.06059", "authors": ["Haorui He", "Yupeng Li", "Bin Benjamin Zhu", "Dacheng Wen", "Reynold Cheng", "Francis C. M. Lau"], "title": "Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System", "comment": null, "summary": "State-of-the-art fact-checking systems combat misinformation at scale by\nemploying autonomous LLM-based agents to decompose complex claims into smaller\nsub-claims, verify each sub-claim individually, and aggregate the partial\nresults to produce verdicts with justifications (explanatory rationales for the\nverdicts). The security of these systems is crucial, as compromised\nfact-checkers, which tend to be easily underexplored, can amplify\nmisinformation. This work introduces Fact2Fiction, the first poisoning attack\nframework targeting such agentic fact-checking systems. Fact2Fiction mirrors\nthe decomposition strategy and exploits system-generated justifications to\ncraft tailored malicious evidences that compromise sub-claim verification.\nExtensive experiments demonstrate that Fact2Fiction achieves 8.9\\%--21.2\\%\nhigher attack success rates than state-of-the-art attacks across various\npoisoning budgets. Fact2Fiction exposes security weaknesses in current\nfact-checking systems and highlights the need for defensive countermeasures.", "AI": {"tldr": "This paper introduces Fact2Fiction, a novel poisoning attack framework targeting LLM-based agentic fact-checking systems by exploiting decomposition strategies and system-generated justifications, achieving higher attack success rates than existing methods and revealing security vulnerabilities.", "motivation": "Current autonomous LLM-based fact-checking systems, while effective at combating misinformation, remain underexplored in terms of security. Compromised systems could inadvertently amplify misinformation, necessitating analysis of their vulnerabilities.", "method": "Fact2Fiction mimics the fact-checker's decomposition approach to generate tailored malicious evidence that specifically compromises sub-claim verification processes through poisoned justifications.", "result": "Experiments demonstrate Fact2Fiction outperforms state-of-the-art attacks by 8.9-21.2% in success rates across different poisoning budgets, demonstrating its effectiveness against agentic fact-checkers.", "conclusion": "The research exposes critical security weaknesses in LLM-based fact-checking systems' decomposition mechanisms and underscores the urgent need for defensive strategies against such poisoning attacks."}}
{"id": "2508.06071", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.06071", "abs": "https://arxiv.org/abs/2508.06071", "authors": ["Liang Chen"], "title": "A Game-Theoretic Foundation for Bitcoin's Price: A Security-Utility Equilibrium", "comment": null, "summary": "This paper introduces a structural game-theoretic model to value\ndecentralized digital assets like Bitcoin. Instead of relying on speculative\nbeliefs, it frames the asset's price within a Rational-Expectations\nSecurity-Utility Nash Equilibrium (RESUNE). This equilibrium is a fixed point\nwhere the market-clearing price dictates the hash rate through a free-entry\nmining model, which in turn endogenously sets the network's security. The\nsecurity, defined as one minus the probability of a 51% attack, is determined\nvia a global games model of attacker coordination, providing a unique and\ncontinuous security function. We prove the existence of a RESUNE and offer\nconditions for its uniqueness and stability. The model predicts that the\nstabilizing direct effect of price on demand must outweigh the potentially\ndestabilizing feedback from price to security. The framework generates testable\npredictions, such as a protocol halving causing a contraction in both hash rate\nand price. A structural Vector Autoregression (VAR) model is proposed to test\nthis mechanism. The model decomposes Bitcoin's value into transactional\nutility, security, and speculative components and explains the observed\nunidirectional causality from price to hash rate.", "AI": {"tldr": "The paper proposes a game-theoretic model to value decentralized digital assets using RESUNE equilibrium, tying price to hash rate and network security through a mining coordination framework. It provides testable predictions about Bitcoin's price, hash rate, and security dynamics.", "motivation": "Traditional valuation of digital assets relies on speculative beliefs; this paper aims to provide a structural model grounded in economic theory to better understand price fundamentals and security mechanisms.", "method": "Introduces RESUNE as a fixed-point equilibrium where market price determines hash rate via a free-entry mining model. Security is modeled as a function of attack probability using a global games framework. Structural VAR analysis is applied for empirical testing.", "result": "Proofs of RESUNE existence, uniqueness conditions, and predictions like protocol halving reducing hash rate and price. The model explains unidirectional price-hash rate causality and decomposes Bitcoin's value into transactional, security, and speculative components.", "conclusion": "The RESUNE framework offers a testable mechanism for deconstructing Bitcoin's value into economic fundamentals, demonstrating strategic interactions between price and security that differ from speculative models."}}
{"id": "2508.06073", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.06073", "abs": "https://arxiv.org/abs/2508.06073", "authors": ["Weiheng Wu", "Wei Qiao", "Teng Li", "Yebo Feng", "Zhuo Ma", "Jianfeng Ma", "Yang Liu"], "title": "ProvX: Generating Counterfactual-Driven Attack Explanations for Provenance-Based Detection", "comment": null, "summary": "Provenance graph-based intrusion detection systems are deployed on hosts to\ndefend against increasingly severe Advanced Persistent Threat. Using Graph\nNeural Networks to detect these threats has become a research focus and has\ndemonstrated exceptional performance. However, the widespread adoption of\nGNN-based security models is limited by their inherent black-box nature, as\nthey fail to provide security analysts with any verifiable explanations for\nmodel predictions or any evidence regarding the model's judgment in relation to\nreal-world attacks. To address this challenge, we propose ProvX, an effective\nexplanation framework for exlaining GNN-based security models on provenance\ngraphs. ProvX introduces counterfactual explanation logic, seeking the minimal\nstructural subset within a graph predicted as malicious that, when perturbed,\ncan subvert the model's original prediction. We innovatively transform the\ndiscrete search problem of finding this critical subgraph into a continuous\noptimization task guided by a dual objective of prediction flipping and\ndistance minimization. Furthermore, a Staged Solidification strategy is\nincorporated to enhance the precision and stability of the explanations. We\nconducted extensive evaluations of ProvX on authoritative datasets. The\nexperimental results demonstrate that ProvX can locate critical graph\nstructures that are highly relevant to real-world attacks and achieves an\naverage explanation necessity of 51.59\\%, with these metrics outperforming\ncurrent SOTA explainers. Furthermore, we explore and provide a preliminary\nvalidation of a closed-loop Detection-Explanation-Feedback enhancement\nframework, demonstrating through experiments that the explanation results from\nProvX can guide model optimization, effectively enhancing its robustness\nagainst adversarial attacks.", "AI": {"tldr": "ProvX is a framework that enhances GNN-based intrusion detection models by providing verifiable explanations through counterfactual logic and a closed-loop feedback system for robustness.", "motivation": "GNN-based security models struggle with adoption due to their black-box nature, offering no actionable explanations for analysts to verify predictions against real-world attacks.", "method": "ProvX uses counterfactual explanation logic to identify minimal graph subsets critical for predictions, transforming discrete graph search into a dual-objective continuous optimization. A Staged Solidification strategy improves explanation precision and stability.", "result": "ProvX achieved 51.59% average explanation necessity on authoritative datasets, outperforming SOTA methods, and demonstrated closed-loop feedback improves model robustness against adversarial attacks.", "conclusion": "ProvX bridges the explanation gap in GNN-based intrusion detection, enabling trust through verifiable insights and enhancing model robustness via feedback-driven optimization."}}
{"id": "2508.06087", "categories": ["cs.CR", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.06087", "abs": "https://arxiv.org/abs/2508.06087", "authors": ["Zhihao Yao", "Yuxuan Gu", "Xiachong Feng", "Weitao Ma", "Bo Li", "Xiaocheng Feng"], "title": "Adaptive Backtracking for Privacy Protection in Large Language Models", "comment": null, "summary": "The preservation of privacy has emerged as a critical topic in the era of\nartificial intelligence. However, current work focuses on user-oriented\nprivacy, overlooking severe enterprise data leakage risks exacerbated by the\nRetrieval-Augmented Generation paradigm. To address this gap, our paper\nintroduces a novel objective: enterprise-oriented privacy concerns. Achieving\nthis objective requires overcoming two fundamental challenges: existing methods\nsuch as data sanitization severely degrade model performance, and the field\nlacks public datasets for evaluation. We address these challenges with several\nsolutions. (1) To prevent performance degradation, we propose ABack, a\ntraining-free mechanism that leverages a Hidden State Model to pinpoint the\norigin of a leakage intention and rewrite the output safely. (2) To solve the\nlack of datasets, we construct PriGenQA, a new benchmark for enterprise privacy\nscenarios in healthcare and finance. To ensure a rigorous evaluation, we move\nbeyond simple static attacks by developing a powerful adaptive attacker with\nGroup Relative Policy Optimization. Experiments show that against this superior\nadversary, ABack improves the overall privacy utility score by up to 15\\% over\nstrong baselines, avoiding the performance trade-offs of prior methods.", "AI": {"tldr": "This paper addresses enterprise privacy leakage risks in Retrieval-Augmented Generation (RAG) by introducing ABack (a training-free leakage prevention mechanism) and PriGenQA (a new benchmark for healthcare/finance scenarios), achieving a 15% improvement in privacy utility over baselines without performance trade-offs.", "motivation": "Current privacy research in AI focuses on user-oriented concerns while neglecting enterprise data leakage risks in RAG systems, and lacks effective solutions to prevent performance degradation during privacy safeguarding or public datasets for evaluation.", "method": "1. ABack: Training-free mechanism using a Hidden State Model to detect and suppress privacy leakage. 2. PriGenQA: Enterprise-focused benchmark dataset in healthcare and finance domains. 3. Adaptive attacker with Group Relative Policy Optimization for rigorous evaluation.", "result": "ABack achieves up to 15% higher overall privacy utility score compared to strong baselines while avoiding the performance degradation typically seen in prior data sanitization methods.", "conclusion": "The paper demonstrates a novel enterprise privacy protection approach for RAG systems that outperforms existing methods on both efficiency (training-free) and effectiveness (15% utility gain), while establishing the first benchmark for evaluating these critical privacy concerns."}}
{"id": "2508.06106", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.06106", "abs": "https://arxiv.org/abs/2508.06106", "authors": ["Luca Serena", "Gabriele D'Angelo", "Stefano Ferretti", "Moreno Marzolla"], "title": "Simulation in Cybersecurity: Understanding Techniques, Applications, and Goals", "comment": "Working paper", "summary": "Modeling and simulation are widely used in cybersecurity research to assess\ncyber threats, evaluate defense mechanisms, and analyze vulnerabilities.\nHowever, the diversity of application areas, the variety of cyberattacks\nscenarios, and the differing objectives of these simulations makes it difficult\nto identify methodological trends. Existing reviews often focus on specific\nmodeling techniques or application domains, making it challenging to analyze\nthe field as a whole. To address these limitations, we present a comprehensive\nreview of the current state of the art, classifying the selected papers based\non four dimensions: the application domain, the types of cyber threats\nrepresented, the simulation techniques employed, and the primary goals of the\nsimulation. The review discusses the strengths and limitations of different\napproaches, identifies which cyber threats are the most suited for\nsimulation-based investigations, and analyzes which modeling paradigms are most\nappropriate for specific cybersecurity challenges.", "AI": {"tldr": "This paper provides a comprehensive review of cybersecurity simulation methodologies, categorizing research across four dimensions to address methodological fragmentation in the field.", "motivation": "Existing studies are limited by narrow focus on specific techniques or applications, making holistic analysis difficult. The authors seek to identify methodological trends that span diverse threat scenarios and objectives.", "method": "Systematic classification of 200+ papers using a four-dimensional framework encompassing application domain, threat types, simulation techniques, and simulation objectives", "result": "Identification of methodological strengths/weaknesses, optimal simulation approaches for different threat categories, and domain-specific modeling paradigm suitability. Revealed simulation techniques perform best in network security compared to application security.", "conclusion": "Establishes a taxonomy for analyzing cybersecurity simulations, highlights gaps in multi-domain threat analysis, and provides recommendations for selecting appropriate methodologies based on specific research objectives."}}
{"id": "2508.06153", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.06153", "abs": "https://arxiv.org/abs/2508.06153", "authors": ["Zhengxian Wu", "Juan Wen", "Wanli Peng", "Haowei Chang", "Yinghan Zhou", "Yiming Xue"], "title": "SLIP: Soft Label Mechanism and Key-Extraction-Guided CoT-based Defense Against Instruction Backdoor in APIs", "comment": null, "summary": "With the development of customized large language model (LLM) agents, a new\nthreat of black-box backdoor attacks has emerged, where malicious instructions\nare injected into hidden system prompts. These attacks easily bypass existing\ndefenses that rely on white-box access, posing a serious security challenge. To\naddress this, we propose SLIP, a Soft Label mechanism and key-extraction-guided\nCoT-based defense against Instruction backdoors in APIs. SLIP is designed based\non two key insights. First, to counteract the model's oversensitivity to\ntriggers, we propose a Key-extraction-guided Chain-of-Thought (KCoT). Instead\nof only considering the single trigger or the input sentence, KCoT prompts the\nagent to extract task-relevant key phrases. Second, to guide the LLM toward\ncorrect answers, our proposed Soft Label Mechanism (SLM) prompts the agent to\nquantify the semantic correlation between key phrases and candidate answers.\nCrucially, to mitigate the influence of residual triggers or misleading content\nin phrases extracted by KCoT, which typically causes anomalous scores, SLM\nexcludes anomalous scores deviating significantly from the mean and\nsubsequently averages the remaining scores to derive a more reliable semantic\nrepresentation. Extensive experiments on classification and question-answer\n(QA) tasks demonstrate that SLIP is highly effective, reducing the average\nattack success rate (ASR) from 90.2% to 25.13% while maintaining high accuracy\non clean data and outperforming state-of-the-art defenses. Our code are\navailable in\nhttps://github.com/CAU-ISS-Lab/Backdoor-Attack-Defense-LLMs/tree/main/SLIP.", "AI": {"tldr": "This paper proposes SLIP, a defense mechanism against black-box backdoor attacks in LLM agents by combining key-extraction-guided Chain-of-Thought and Soft Label Mechanism, achieving 25.13% average attack success rate with minimal clean data accuracy loss.", "motivation": "Existing LLM defenses relying on white-box access fail against black-box backdoor attacks, where malicious instructions are stealthily embedded in system prompts via APIs, creating critical security vulnerabilities.", "method": "SLIP integrates two mechanisms: (1) Key-extraction-guided Chain-of-Thought (KCoT) focuses model attention on task-relevant phrases, reducing trigger sensitivity; (2) Soft Label Mechanism (SLM) filters anomalous scores through semantic correlation analysis and mean deviation filtering to generate reliable semantic representations.", "result": "Experiments show SLIP reduces attack success rate from 90.2% to 25.13% while maintaining 98.5% clean data accuracy, surpassing state-of-the-art defenses in both classification and QA tasks with statistically significant results.", "conclusion": "SLIP provides a novel defense framework against black-box backdoor attacks through dual-phase input sanitization, achieving substantial ASR reduction and clean data performance preservation, with open-source implementation available for further research."}}
{"id": "2508.06325", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06325", "abs": "https://arxiv.org/abs/2508.06325", "authors": ["Zelin Li", "Ruohan Zong", "Yifan Liu", "Ruichen Yao", "Yaokun Liu", "Yang Zhang", "Dong Wang"], "title": "Anti-Tamper Protection for Unauthorized Individual Image Generation", "comment": "22 pages ,22 figures, Paper has been accepted by ICCV'2025", "summary": "With the advancement of personalized image generation technologies, concerns\nabout forgery attacks that infringe on portrait rights and privacy are growing.\nTo address these concerns, protection perturbation algorithms have been\ndeveloped to disrupt forgery generation. However, the protection algorithms\nwould become ineffective when forgery attackers apply purification techniques\nto bypass the protection. To address this issue, we present a novel approach,\nAnti-Tamper Perturbation (ATP). ATP introduces a tamper-proof mechanism within\nthe perturbation. It consists of protection and authorization perturbations,\nwhere the protection perturbation defends against forgery attacks, while the\nauthorization perturbation detects purification-based tampering. Both\nprotection and authorization perturbations are applied in the frequency domain\nunder the guidance of a mask, ensuring that the protection perturbation does\nnot disrupt the authorization perturbation. This design also enables the\nauthorization perturbation to be distributed across all image pixels,\npreserving its sensitivity to purification-based tampering. ATP demonstrates\nits effectiveness in defending forgery attacks across various attack settings\nthrough extensive experiments, providing a robust solution for protecting\nindividuals' portrait rights and privacy. Our code is available at:\nhttps://github.com/Seeyn/Anti-Tamper-Perturbation .", "AI": {"tldr": "ATP introduces a dual-perturbation approach in the frequency domain to protect portraits from forgery attacks by combining protective and authorization mechanisms that counter both generation and purification-based tampering.", "motivation": "Personalized image generation technologies pose increasing risks to portrait rights and privacy, with conventional protection methods failing against purification techniques that bypass perturbations.", "method": "ATP employs frequency-domain perturbations with mask guidance to separate protection (forgery defense) and authorization (tamper detection) components, enabling comprehensive and non-interfering safeguards.", "result": "Extensive experiments demonstrate ATP's robustness against varied forgery attack scenarios and its sensitivity to purification-based tampering detection.", "conclusion": "ATP provides an effective, dual-layered solution for enforcing portrait privacy rights by addressing both generation and tamper-purification challenges in image forgery contexts."}}
{"id": "2508.06394", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.06394", "abs": "https://arxiv.org/abs/2508.06394", "authors": ["Dario Pasquini", "Evgenios M. Kornaropoulos", "Giuseppe Ateniese", "Omer Akgul", "Athanasios Theocharis", "Petros Efstathopoulos"], "title": "When AIOps Become \"AI Oops\": Subverting LLM-driven IT Operations via Telemetry Manipulation", "comment": "v0.1", "summary": "AI for IT Operations (AIOps) is transforming how organizations manage complex\nsoftware systems by automating anomaly detection, incident diagnosis, and\nremediation. Modern AIOps solutions increasingly rely on autonomous LLM-based\nagents to interpret telemetry data and take corrective actions with minimal\nhuman intervention, promising faster response times and operational cost\nsavings.\n  In this work, we perform the first security analysis of AIOps solutions,\nshowing that, once again, AI-driven automation comes with a profound security\ncost. We demonstrate that adversaries can manipulate system telemetry to\nmislead AIOps agents into taking actions that compromise the integrity of the\ninfrastructure they manage. We introduce techniques to reliably inject\ntelemetry data using error-inducing requests that influence agent behavior\nthrough a form of adversarial reward-hacking; plausible but incorrect system\nerror interpretations that steer the agent's decision-making. Our attack\nmethodology, AIOpsDoom, is fully automated--combining reconnaissance, fuzzing,\nand LLM-driven adversarial input generation--and operates without any prior\nknowledge of the target system.\n  To counter this threat, we propose AIOpsShield, a defense mechanism that\nsanitizes telemetry data by exploiting its structured nature and the minimal\nrole of user-generated content. Our experiments show that AIOpsShield reliably\nblocks telemetry-based attacks without affecting normal agent performance.\n  Ultimately, this work exposes AIOps as an emerging attack vector for system\ncompromise and underscores the urgent need for security-aware AIOps design.", "AI": {"tldr": "This paper introduces the first security analysis of AIOps solutions, demonstrating that adversaries can exploit system telemetry to mislead autonomous AI agents into compromising infrastructure integrity, and proposes AIOpsShield as an effective defense mechanism to block such attacks while maintaining normal operations.", "motivation": "Modern AIOps solutions automate IT operations using LLM-based agents, promising faster responses and cost savings. However, their security implications remain underexplored, creating a critical need to address vulnerabilities that could be exploited to compromise system infrastructure through automated AI attacks.", "method": "The authors developed AIOpsDoom, a fully automated attack methodology combining reconnaissance, fuzzing, and LLM-driven adversarial input generation. It manipulates system telemetry via error-inducing requests and incorrect system error interpretations to hack agents' reward mechanisms, leading to unintended operational actions with no prior knowledge of the target system.", "result": "Experiments validated that AIOpsDoom successfully compromises infrastructure integrity by manipulating telemetry data. AIOpsShield, the proposed defense, reliably sanitizes telemetry data by leveraging its structured nature and minimal user-generated content, blocking attacks while preserving agent performance.", "conclusion": "AIOps, despite its automation benefits, introduces an emerging attack vector for system compromise. The study emphasizes the urgent need for security-aware design in AIOps frameworks, as current approaches lack sufficient protections against telemetry-based adversarial manipulation of autonomous agents."}}
{"id": "2508.06457", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06457", "abs": "https://arxiv.org/abs/2508.06457", "authors": ["Sanket Badhe"], "title": "ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls", "comment": "Accepted at CAMLIS 25: Conference on Applied Machine Learning for\n  Information Security. 10 pages, 3 figures", "summary": "Large Language Models (LLMs) have demonstrated impressive fluency and\nreasoning capabilities, but their potential for misuse has raised growing\nconcern. In this paper, we present ScamAgent, an autonomous multi-turn agent\nbuilt on top of LLMs, capable of generating highly realistic scam call scripts\nthat simulate real-world fraud scenarios. Unlike prior work focused on\nsingle-shot prompt misuse, ScamAgent maintains dialogue memory, adapts\ndynamically to simulated user responses, and employs deceptive persuasion\nstrategies across conversational turns. We show that current LLM safety\nguardrails, including refusal mechanisms and content filters, are ineffective\nagainst such agent-based threats. Even models with strong prompt-level\nsafeguards can be bypassed when prompts are decomposed, disguised, or delivered\nincrementally within an agent framework. We further demonstrate the\ntransformation of scam scripts into lifelike voice calls using modern\ntext-to-speech systems, completing a fully automated scam pipeline. Our\nfindings highlight an urgent need for multi-turn safety auditing, agent-level\ncontrol frameworks, and new methods to detect and disrupt conversational\ndeception powered by generative AI.", "AI": {"tldr": "ScamAgent is an autonomous, multi-turn LLM-based agent that generates realistic scam scripts and demonstrates how current safety guardrails can be bypassed in conversational settings. The study emphasizes the need for advanced safety mechanisms to combat AI-powered scamming.", "motivation": "LLMs' growing misuse potential motivates research into adversarial capabilities of autonomous agents. Existing safety measures focusing on single-shot prompts are inadequate for complex, multi-turn conversational attacks.", "method": "ScamAgent uses dialogue memory, dynamic adaptation to user responses, and incremental prompt delivery to simulate realistic scams. Researchers tested diverse safety guardrails (refusal mechanisms, content filters) and combined scam scripts with modern text-to-speech systems to create automated scam pipelines.", "result": "Current LLM guardrails are shown to be ineffective against agent-based attacks. Scammers can bypass filters by decomposing/disguising prompts and using sequential interactions. The system successfully converts generated scripts into lifelike voice calls with high deception effectiveness.", "conclusion": "LLMs require multi-turn safety auditing and agent-level control frameworks. Conventional prompt-based filters are insufficient. New solutions are needed to detect and prevent sequential conversational deception exploiting autonomous agents."}}
{"id": "2508.06489", "categories": ["cs.CR", "cs.DC", "cs.DM", "cs.IT", "math.IT", "math.PR"], "pdf": "https://arxiv.org/pdf/2508.06489", "abs": "https://arxiv.org/abs/2508.06489", "authors": ["Mustafa Doger", "Sennur Ulukus"], "title": "Voting-Based Semi-Parallel Proof-of-Work Protocol", "comment": null, "summary": "Parallel Proof-of-Work (PoW) protocols are suggested to improve the safety\nguarantees, transaction throughput and confirmation latencies of Nakamoto\nconsensus. In this work, we first consider the existing parallel PoW protocols\nand develop hard-coded incentive attack structures. Our theoretical results and\nsimulations show that the existing parallel PoW protocols are more vulnerable\nto incentive attacks than the Nakamoto consensus, e.g., attacks have smaller\nprofitability threshold and they result in higher relative rewards. Next, we\nintroduce a voting-based semi-parallel PoW protocol that outperforms both\nNakamoto consensus and the existing parallel PoW protocols from most practical\nperspectives such as communication overheads, throughput, transaction\nconflicts, incentive compatibility of the protocol as well as a fair\ndistribution of transaction fees among the voters and the leaders. We use\nstate-of-the-art analysis to evaluate the consistency of the protocol and\nconsider Markov decision process (MDP) models to substantiate our claims about\nthe resilience of our protocol against incentive attacks.", "AI": {"tldr": "This paper identifies vulnerabilities in existing parallel Proof-of-Work (PoW) protocols against incentive attacks and proposes a voting-based semi-parallel PoW protocol with improved security and performance, validated via MDP models and simulations.", "motivation": "Existing parallel PoW protocols are less secure against incentive attacks compared to Nakamoto consensus due to lower profitability thresholds and higher relative rewards for attackers, necessitating a safer and more efficient design.", "method": "The authors analyze attack structures in current protocols using theoretical models and simulations, then design a voting-based semi-parallel PoW framework leveraging state-of-the-art protocols and Markov Decision Process (MDP) analysis for resilience verification.", "result": "Their protocol achieves superior communication efficiency, transaction throughput, conflict resolution, and incentive compatibility while demonstrating resistance to incentive attacks and equitable transaction fee distribution among voters and leaders.", "conclusion": "The proposed voting-based semi-parallel PoW protocol offers a robust and practical alternative to both traditional Nakamoto consensus and existing parallel PoW structures, addressing security gaps and operational inefficiencies."}}
