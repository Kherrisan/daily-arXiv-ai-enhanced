{"id": "2508.02721", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.02721", "abs": "https://arxiv.org/abs/2508.02721", "authors": ["Libin Qiu", "Yuhang Ye", "Zhirong Gao", "Xide Zou", "Junfu Chen", "Ziming Gui", "Weizhi Huang", "Xiaobo Xue", "Wenkai Qiu", "Kun Zhao"], "title": "Blueprint First, Model Second: A Framework for Deterministic LLM Workflow", "comment": "8 pages, 6 figures, 3 tables", "summary": "While powerful, the inherent non-determinism of large language model (LLM)\nagents limits their application in structured operational environments where\nprocedural fidelity and predictable execution are strict requirements. This\nlimitation stems from current architectures that conflate probabilistic,\nhigh-level planning with low-level action execution within a single generative\nprocess. To address this, we introduce the Source Code Agent framework, a new\nparadigm built on the \"Blueprint First, Model Second\" philosophy. Our framework\ndecouples the workflow logic from the generative model. An expert-defined\noperational procedure is first codified into a source code-based Execution\nBlueprint, which is then executed by a deterministic engine. The LLM is\nstrategically invoked as a specialized tool to handle bounded, complex\nsub-tasks within the workflow, but never to decide the workflow's path. We\nconduct a comprehensive evaluation on the challenging tau-bench benchmark,\ndesigned for complex user-tool-rule scenarios. Our results demonstrate that the\nSource Code Agent establishes a new state-of-the-art, outperforming the\nstrongest baseline by 10.1 percentage points on the average Pass^1 score while\ndramatically improving execution efficiency. Our work enables the verifiable\nand reliable deployment of autonomous agents in applications governed by strict\nprocedural logic.", "AI": {"tldr": "The Source Code Agent framework addresses non-determinism in LLMs by decoupling workflow logic (defined as source code) from generative model execution. It achieves 10.1pp improvement on tau-bench scores while improving procedural reliability and execution efficiency.", "motivation": "Current LLM agent architectures blend probabilistic planning with deterministic execution, creating non-determinism in workflows requiring strict procedural fidelity and predictable outcomes. This limits their deployment in critical operational environments.", "method": "1. Expert-defined operational procedures are codified into an 'Execution Blueprint' using source code\n2. A deterministic engine executes the blueprint\n3. LLMs are invoked as input-agnostic tools for specific subtasks with bounded input/output, never altering the workflow path directly", "result": "Established new state-of-the-art performance on tau-bench (complex user-tool-rule scenarios) with:\n- 10.1 percentage point improvement in average Pass^1 scores\n- Significant execution efficiency gains\n- Deterministic control of workflow logic despite LLM's probabilistic nature", "conclusion": "The 'Blueprint First, Model Second' paradigm enables verifiable deployment of autonomous agents in strict procedural environments by:\n1. Isolating LLM use to bounded subtasks\n2. Maintaining deterministic workflow execution\n3. Separating strategic planning from operational implementation in AI systems"}}
{"id": "2508.02729", "categories": ["cs.SE", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.02729", "abs": "https://arxiv.org/abs/2508.02729", "authors": ["Zhuoran Liu"], "title": "Interpreting Performance Profiles with Deep Learning", "comment": "Master of Science in Computer Science thesis, North Carolina State\n  University, 2022. Advisor: Dr. Xu Liu", "summary": "Profiling tools (also known as profilers) play an important role in\nunderstanding program performance at runtime, such as hotspots, bottlenecks,\nand inefficiencies. While profilers have been proven to be useful, they give\nextra burden to software engineers. Software engineers, as the users, are\nresponsible to interpret the complex performance data and identify actionable\noptimization in program source code. However, it can be challenging for users\nto associate inefficiencies with the program semantics, especially if the users\nare not the authors of the code, which limits the applicability of profilers.\n  In this thesis, we explore a new direction to combine performance profiles\nand program semantics with a deep learning approach. The key idea is to glean\ncode summary for semantic information (at a certain level) and integrate it\ninto a profiler, which can better understand program inefficiencies for\nactionable optimization. To be concrete, we combine profiles generated by Async\nProfiler (the state-of-the-art Java profiler) with code summarization from a\nfine-tuned CodeBERT-based model. We demonstrate the code summaries of any\nselected call path in a graphic user interface. Our system can effectively\nassist analysis on many Java benchmarks.", "AI": {"tldr": "This paper proposes a system combining Java profiling and code summarization via deep learning to ease performance analysis for developers.", "motivation": "Prolific profilers create a burden for engineers needing to interpret complex data and connect inefficiencies to code semantics, especially for unfamiliar codebases.", "method": "Async Profiler generates performance data while a fine-tuned CodeBERT-based model extracts code summaries from semantics. Summaries are integrated into a GUI interface for selected call paths.", "result": "Significantly reduced effort in identifying actionable optimizations in Java benchmarks through this integrated approach.", "conclusion": "Semantic-enriched profiling offers practical benefits for program optimization by making performance-semantic correlations explicit and accessible."}}
{"id": "2508.02732", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02732", "abs": "https://arxiv.org/abs/2508.02732", "authors": ["Sherman Wong", "Jalaj Bhandari", "Leo Zhou Fan Yang", "Xylan Xu", "Yi Zhuang", "Cem Cayiroglu", "Payal Bhuptani", "Sheela Yadawad", "Hung Duong"], "title": "A Note on Code Quality Score: LLMs for Maintainable Large Codebases", "comment": "24 pages, ICLR format", "summary": "Maintaining code quality in large-scale software systems presents significant\nchallenges, particularly in settings where a large numbers of engineers work\nconcurrently on a codebase. This paper introduces Code Quality Score (CQS)\nsystem to automatically detect issues with a set of code changes and provide\nactionable insights. At its core, the CQS system is powered by two Llama3\nmodels, fine-tuned (with SFT and offline RL approaches), to a) detect common\ncode quality issues related to coding best practices and b) to provide good\n``critiques'' for LLM-generated code review respectively. To maintain good user\nexperience, we layer the system with hand-crafted rules to filter out incorrect\nresponses/hallucinations. Offline evaluations show that our CQS system is able\nto achieve an impressive precision rate for identifying valid issues. This\nsystem has already been rolled out to developers in an industrial scale setting\nand has consistently achieved 60\\% week over week user helpfulness rate,\ndemonstrating its effectiveness in a real-world environment. In this paper, we\npresent details of the CQS system along with some learnings on curating\ndeveloper feedback to create training data for LLM fine-tuning.", "AI": {"tldr": "This paper proposes Code Quality Score (CQS) system using two fine-tuned Llama3 models (via SFT and offline RL) to detect code quality issues and provide LLM-generated code critiques, achieving 60% user helpfulness rate in industrial deployment.", "motivation": "Large-scale software systems with concurrent engineering teams face significant code quality maintenance challenges, requiring automated systems for efficient and effective code review.", "method": "The CQS system combines two Llama3 models fine-tuned using supervised fine-tuning (SFT) and offline reinforcement learning (RL) for code quality detection and critique generation, enhanced by hand-crafted rules to filter hallucinations.", "result": "Offline evaluations show high precision in issue identification, and industrial-scale deployment achieves a 60% week-over-week user helpfulness rate.", "conclusion": "The CQS system demonstrates effective code quality monitoring at scale while addressing LLM hallucinations. The paper also shares practical learnings on curating developer feedback for LLM fine-tuning."}}
{"id": "2508.02733", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.02733", "abs": "https://arxiv.org/abs/2508.02733", "authors": ["Rijul Jain", "Shraddha Barke", "Gabriel Ebner", "Md Rakib Hossain Misu", "Shan Lu", "Sarah Fakhoury"], "title": "What's in a Proof? Analyzing Expert Proof-Writing Processes in F* and Verus", "comment": null, "summary": "Proof-oriented programming languages (POPLs) empower developers to write code\nalongside formal correctness proofs, providing formal guarantees that the code\nadheres to specified requirements. Despite their powerful capabilities, POPLs\npresent a steep learning curve and have not yet been adopted by the broader\nsoftware community. The lack of understanding about the proof-development\nprocess and how expert proof developers interact with POPLs has hindered the\nadvancement of effective proof engineering and the development of\nproof-synthesis models/tools.\n  In this work, we conduct a user study, involving the collection and analysis\nof fine-grained source code telemetry from eight experts working with two\nlanguages, F* and Verus. Results reveal interesting trends and patterns about\nhow experts reason about proofs and key challenges encountered during the proof\ndevelopment process. We identify three distinct strategies and multiple\ninformal practices that are not captured final code snapshots, yet are\npredictive of task outcomes. We translate these findings into concrete design\nguidance for AI proof assistants: bias toward early specification drafting,\nexplicit sub-goal decomposition, bounded active errors, and disciplined\nverifier interaction. We also present a case study of an F* proof agent\ngrounded in these recommendations, and demonstrate improved performance over\nbaseline LLMs", "AI": {"tldr": "This paper analyzes expert proof development strategies in F* and Verus through telemetry data, offering design principles for AI proof assistants and demonstrating improved performance of an F*-based agent.", "motivation": "POPLs' complexity and lack of understanding about expert workflow hinder their adoption and AI tool development; systematic analysis of proof practices is needed to address this gap.", "method": "User study using fine-grained telemetry from 8 experts working with F* and Verus, identifying proof strategies and informal practices through data analysis.", "result": "Three key strategies for proof reasoning were discovered, along with practices predictive of task success. An F* proof agent implementing these principles outperforms baseline LLMs.", "conclusion": "Formal analysis of expert proof workflows reveals actionable design guidance for AI proof assistants, demonstrating that task-specific agent architectures can leverage these insights effectively."}}
{"id": "2508.02805", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02805", "abs": "https://arxiv.org/abs/2508.02805", "authors": ["Jean Michel Tine", "Mohammed Aldeen", "Abyad Enan", "M Sabbir Salek", "Long Cheng", "Mashrur Chowdhury"], "title": "Real-World Evaluation of Protocol-Compliant Denial-of-Service Attacks on C-V2X-based Forward Collision Warning Systems", "comment": "This paper was submitted to the Transportation Research Board (TRB)\n  2026 and is under review", "summary": "Cellular Vehicle-to-Everything (C-V2X) technology enables low-latency,\nreliable communications essential for safety applications such as a Forward\nCollision Warning (FCW) system. C-V2X deployments operate under strict protocol\ncompliance with the 3rd Generation Partnership Project (3GPP) and the Society\nof Automotive Engineers Standard (SAE) J2735 specifications to ensure\ninteroperability. This paper presents a real-world testbed evaluation of\nprotocol-compliant Denial-of-Service (DoS) attacks using User Datagram Protocol\n(UDP) flooding and oversized Basic Safety Message (BSM) attacks that 7 exploit\ntransport- and application-layer vulnerabilities in C-V2X. The attacks\npresented in this study transmit valid messages over standard PC5 sidelinks,\nfully adhering to 3GPP and SAE J2735 specifications, but at abnormally high\nrates and with oversized payloads that overload the receiver resources without\nbreaching any protocol rules such as IEEE 1609. Using a real-world connected\nvehicle 11 testbed with commercially available On-Board Units (OBUs), we\ndemonstrate that high-rate UDP flooding and oversized payload of BSM flooding\ncan severely degrade FCW performance. Results show that UDP flooding alone\nreduces packet delivery ratio by up to 87% and increases latency to over 400ms,\nwhile oversized BSM floods overload receiver processing resources, delaying or\ncompletely suppressing FCW alerts. When UDP and BSM attacks are executed\nsimultaneously, they cause near-total communication failure, preventing FCW\nwarnings entirely. These findings reveal that protocol-compliant communications\ndo not necessarily guarantee safe or reliable operation of C-V2X-based safety\napplications.", "AI": {"tldr": "This paper evaluates protocol-compliant Denial-of-Service (DoS) attacks on C-V2X systems, demonstrating how UDP flooding and oversized BSMs exploit transport/application-layer vulnerabilities while adhering to 3GPP and SAE standards. Combined attacks disable FCW functionality entirely.", "motivation": "Safety-critical systems like C-V2X rely on strict protocol compliance (3GPP/SAE) for interoperability, but protocol-valid messages at abnormal rates/payload sizes could still disrupt operations, necessitating real-world evaluation.", "method": "Authors conducted experiments using a connected vehicle testbed with commercial OBUs, simulating DoS attacks through: (1) High-rate UDP flooding via standardized PC5 sidelinks; (2) Oversized BSM payloads exploiting accepted message parameters yet overwhelming receiver resources.", "result": "UDP flooding alone reduced packet delivery ratios by 87% and increased latency beyond 400ms; oversized BSM attacks caused resource overload delaying/suppressing FCW alerts. Simultaneous attacks induced complete communication failure. No protocol rules (3GPP/SAE/IEEE 1609) were violated during these scenarios.", "conclusion": "Protocol compliance alone cannot ensure security in C-V2X systems. Attackers can exploit legitimate message specifications (rate/payload size) within standard frameworks to disrupt safety applications, highlighting the need for additional security mechanisms beyond current protocols."}}
{"id": "2508.02820", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.02820", "abs": "https://arxiv.org/abs/2508.02820", "authors": ["David Svoboda", "Lori Flynn", "William Klieber", "Michael Duggan", "Nicholas Reimer", "Joseph Sible"], "title": "Automated Code Repair for C/C++ Static Analysis Alerts", "comment": null, "summary": "(Note: This work is a preprint.) Static analysis (SA) tools produce many\ndiagnostic alerts indicating that source code in C or C++ may be defective and\npotentially vulnerable to security exploits. Many of these alerts are false\npositives. Identifying the true-positive alerts and repairing the defects in\nthe associated code are huge efforts that automated program repair (APR) tools\ncan help with. Our experience showed us that APR can reduce the number of SA\nalerts significantly and reduce the manual effort of analysts to review code.\nThis engineering experience paper details the application of design,\ndevelopment, and performance testing to an APR tool we built that repairs C/C++\ncode associated with 3 categories of alerts produced by multiple SA tools. Its\nrepairs are simple and local. Furthermore, our findings convinced the\nmaintainers of the CERT Coding Standards to re-assess and update the metrics\nused to assess when violations of guidelines are detectable or repairable. We\ndiscuss engineering design choices made to support goals of trustworthiness and\nacceptability to developers. Our APR tool repaired 8718 out of 9234 alerts\nproduced by one SA tool on one codebase. It can repair 3 flaw categories. For 2\nflaw categories, 2 SA tools, and 2 codebases, our tool repaired or dismissed as\nfalse positives over 80% of alerts, on average. Tests showed repairs did not\nappreciably degrade the performance of the code or cause new alerts to appear\n(with the possible exception of sqlite3.c). This paper describes unique\ncontributions that include a new empirical analysis of SA data, our selection\nmethod for flaw categories to repair, publication of our APR tool, and a\ndataset of SA alerts from open-source SA tools run on open-source codebases. It\ndiscusses positive and negative results and lessons learned.", "AI": {"tldr": "This engineering experience paper presents an APR tool for C/C++ that reduces SA alerts by repairing three flaw categories, demonstrating its effectiveness with over 80% success on average and influencing the CERT Coding Standards.", "motivation": "SA tools generate numerous alerts in C/C++, many of which are false positives, requiring significant manual effort. Automated Program Repair (APR) can alleviate this, making the repair process more efficient and scalable for developers.", "method": "A prototype APR tool was designed and developed to address three specific flaw categories from multiple SA tools, using local and simple repair strategies. The approach focuses on trustworthiness and developer acceptability through practical engineering choices.", "result": "The APR tool repaired 8718/9234 alerts (94.4% success) for one SA tool/codebase and achieved >80% resolution (repair or dismissal as false positives) on average for two other flaw categories, two SA tools, and two codebases. Repaired code maintained performance comparable to original, except for sqlite3.c, and did not trigger new alerts.", "conclusion": "The APR framework effectively reduces SA alerts for target flaw categories, proving APR's potential in developer workflows. However, it is limited to three categories and requires generalization. Contributions include empirical analysis of SA data, a new open-source APR tool, and a public dataset of SA alerts, while lessons emphasize careful repair design to avoid performance trade-offs and the importance of collaboration with standard bodies like CERT for broader impact."}}
{"id": "2508.02816", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.02816", "abs": "https://arxiv.org/abs/2508.02816", "authors": ["Dylan Stow", "Russell Barnes", "Eren Kurshan", "Yuan Xie"], "title": "Thermal-Aware 3D Design for Side-Channel Information Leakage", "comment": null, "summary": "Side-channel attacks are important security challenges as they reveal\nsensitive information about on-chip activities. Among such attacks, the thermal\nside-channel has been shown to disclose the activities of key functional blocks\nand even encryption keys. This paper proposes a novel approach to proactively\nconceal critical activities in the functional layers while minimizing the power\ndissipation by (i) leveraging inherent characteristics of 3D integration to\nprotect from side-channel attacks and (ii) dynamically generating custom\nactivity patterns to match the activity to be concealed in the functional\nlayers. Experimental analysis shows that 3D technology combined with the\nproposed run-time algorithm effectively reduces the Side channel vulnerability\nFactor (SVF) below 0.05 and the Spatial Thermal Side-channel Factor (STSF)\nbelow 0.59.", "AI": {"tldr": "The paper introduces a novel approach combining 3D integration and dynamic activity pattern generation to reduce thermal side-channel attacks by minimizing SVF and STSF.", "motivation": "Thermal side-channel attacks pose significant security risks by exposing on-chip activities and encryption keys, necessitating proactive concealment strategies.", "method": "1) Utilizes inherent 3D integration characteristics for physical-level protection. 2) Dynamically generates activity patterns in functional layers to match the target activity's thermal signature, enabling stealth during critical operations.", "result": "Experimental validation achieved SVF <0.05 and STSF <0.59, demonstrating effective mitigation of thermal side-channel vulnerabilities.", "conclusion": "The proposed 3D technology-integrated runtime algorithm achieves robust countermeasures against thermal side-channel attacks while maintaining low power overhead."}}
{"id": "2508.02827", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02827", "abs": "https://arxiv.org/abs/2508.02827", "authors": ["Ora Nova Fandina", "Eitan Farchi", "Shmulik Froimovich", "Rami Katan", "Alice Podolsky", "Orna Raz", "Avi Ziv"], "title": "Automated Validation of LLM-based Evaluators for Software Engineering Artifacts", "comment": null, "summary": "Automation in software engineering increasingly relies on large language\nmodels (LLMs) to generate, review, and assess code artifacts. However,\nestablishing LLMs as reliable evaluators remains an open challenge: human\nevaluations are costly, subjective and non scalable, while existing automated\nmethods fail to discern fine grained variations in artifact quality.\n  We introduce REFINE (Ranking Evaluators for FIne grained Nuanced Evaluation),\nan automated framework for benchmarking LLM based evaluators across software\nengineering tasks. REFINE comprises of two modules: Hierarchy Dataset Builder\napplies novel generation techniques to automatically synthesize artifacts with\nprogressively reduced quality, and Evaluator Tester quantifies each candidate\nevaluator configuration by measuring how closely its rankings align with\nexpected ordering.\n  A key feature of REFINE is controllability: users can tune the granularity of\ndegradation to progressively refine evaluator configurations, from coarse\nfiltering to stress testing on subtle quality gaps.\n  While the methodology is general, we focus on coding tasks reflecting the\npractical demands in our production setting. REFINE was integrated into IBM's\ninternal development workflows and applied to code generation, translation, and\nsummarization for COBOL, an enterprise critical programming language, using\nindustrial data. It was used to identify LLM as a Judge configurations that\nlifted alignment scores from below $0.7$ to above $0.9$ in some coding tasks.\nThese nuance sensitive evaluators are now actively used by model training teams\nto support model release decisions.", "AI": {"tldr": "The paper introduces REFINE, a controllable framework for benchmarking LLM-based code evaluators, enabling nuanced assessment of code generation, translation, and summarization tasks through synthetic datasets and alignment measurement. It demonstrates improved reliability in industrial COBOL applications.", "motivation": "LLMs are increasingly automated to assess code artifacts, but existing methods can't capture subtle quality variations, and human evaluations are costly and non-scalable.", "method": "REFINE uses two modules: Hierarchy Dataset Builder (generating progressively degraded artifacts) and Evaluator Tester (comparing evaluator rankings to expected quality orderings). It allows granularity control for different testing needs.", "result": "Applied to IBM's COBOL production workflows, REFINE improved LLM-as-judge alignment scores from <0.7 to >0.9 in some tasks, enabling real-world deployment of nuance-sensitive evaluators for model release decisions.", "conclusion": "REFINE provides a practical, scalable solution for improving LLM evaluator reliability through controlled degradation testing, significantly impacting industrial code quality assessment and model development processes."}}
{"id": "2508.02836", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02836", "abs": "https://arxiv.org/abs/2508.02836", "authors": ["Mengyu Zhang", "Zhuotao Liu", "Jingwen Huang", "Xuanqi Liu"], "title": "Agentic Privacy-Preserving Machine Learning", "comment": null, "summary": "Privacy-preserving machine learning (PPML) is critical to ensure data privacy\nin AI. Over the past few years, the community has proposed a wide range of\nprovably secure PPML schemes that rely on various cryptography primitives.\nHowever, when it comes to large language models (LLMs) with billions of\nparameters, the efficiency of PPML is everything but acceptable. For instance,\nthe state-of-the-art solution for confidential LLM inference represents at\nleast 10,000-fold slower performance compared to plaintext inference. The\nperformance gap is even larger when the context length increases. In this\nposition paper, we propose a novel framework named Agentic-PPML to make PPML in\nLLMs practical. Our key insight is to employ a general-purpose LLM for intent\nunderstanding and delegate cryptographically secure inference to specialized\nmodels trained on vertical domains. By modularly separating language intent\nparsing - which typically involves little or no sensitive information - from\nprivacy-critical computation, Agentic-PPML completely eliminates the need for\nthe LLMs to process the encrypted prompts, enabling practical deployment of\nprivacy-preserving LLM-centric services.", "AI": {"tldr": "Agentic-PPML framework addresses inefficiency in privacy-preserving LLMs by separating intent understanding from secure inference.", "motivation": "Current PPML methods for large language models are impractically slow (10000x slowdown) and become worse with increasing context lengths, limiting their real-world deployment.", "method": "1. Uses a general-purpose LLM for intent parsing (non-sensitive task). 2. Delegates encrypted inference to specialized security-focused models trained on specific domains. 3. Modular approach avoids processing encrypted data in primary LLMs.", "result": "Framework eliminates the need for LLMs to process encrypted prompts, making privacy-preserving LLM services deployable with reasonable performance metrics.", "conclusion": "Agentic-PPML provides a practical, efficient solution for confidential LLM inference by splitting processing tasks between models optimized for different purposes, enabling real-world privacy-preserving AI applications."}}
{"id": "2508.02968", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02968", "abs": "https://arxiv.org/abs/2508.02968", "authors": ["Shavindra Wickramathilaka", "John Grundy", "Kashumi Madampe", "Omar Haggag"], "title": "Developer Perceptions on Utilising Low-Code Approaches to Build Accessible and Adaptive Applications for Seniors", "comment": "This paper has been submitted to ACM Transactions on Software\n  Engineering and Methodology (TOSEM)", "summary": "The global ageing population presents a growing societal challenge, creating\nan urgent need for inclusive technologies that promote autonomy among older\nadults. Software practitioners can address this by delivering digital services\nthat enhance seniors' independence and reduce reliance on routine support from\nfamily members and healthcare infrastructure. However, traditional development\npractices, constrained by time and resources, often result in applications with\nmajor accessibility and personalisation barriers. Increasing pressure from\nregulatory requirements, such as the European Accessibility Act (EAA), and the\npersonal empathy many developers feel toward supporting their older loved ones\nand their own future selves have created a demand for tools that support the\ndevelopment of accessible and adaptive software. To address this demand, this\npaper presents an interview-based empirical study with 18 software\npractitioners, evaluating AdaptForge: a low-code model-driven engineering (MDE)\ntool that enables the efficient creation of accessible and adaptive\napplications for senior users by mitigating development constraints through\nautomated code generation. Based on these insights, we identify developer\nexpectations for adopting such tools as industry-standard solutions and provide\nempirically grounded recommendations for designing low-code tools that support\naccessible and adaptive software development.", "AI": {"tldr": "This paper addresses the challenge of developing accessible and adaptive software for seniors by presenting AdaptForge, a low-code MDE tool evaluated through an interview study with 18 software practitioners. It identifies developer expectations and provides design recommendations for such tools.", "motivation": "The global ageing population demands inclusive technologies to enhance seniors' autonomy. Regulatory pressures (e.g., EAA) and developers' personal empathy toward older loved ones create a need for accessible, adaptive software development tools to overcome traditional constraints.", "method": "An interview-based empirical study with 18 software practitioners evaluating AdaptForge, a low-code model-driven engineering (MDE) tool that uses automated code generation to reduce development constraints for accessible and adaptive applications.", "result": "Identified developers expect tools to align with industry standards while enabling accessibility and adaptation. Empirical insights reveal practical challenges in tool adoption and highlight opportunities for improving low-code platforms to better support inclusive software development.", "conclusion": "The paper provides empirically grounded recommendations for designing low-code tools that effectively support accessible and adaptive software development, aligning with both regulatory requirements and aging populations' needs through developer-centered approaches."}}
{"id": "2508.02942", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02942", "abs": "https://arxiv.org/abs/2508.02942", "authors": ["Anas Mabrouk", "Mohamed Hatem", "Mohammad Mamun", "Sherif Saad"], "title": "LMDG: Advancing Lateral Movement Detection Through High-Fidelity Dataset Generation", "comment": null, "summary": "Lateral Movement (LM) attacks continue to pose a significant threat to\nenterprise security, enabling adversaries to stealthily compromise critical\nassets. However, the development and evaluation of LM detection systems are\nimpeded by the absence of realistic, well-labeled datasets. To address this\ngap, we propose LMDG, a reproducible and extensible framework for generating\nhigh-fidelity LM datasets. LMDG automates benign activity generation,\nmulti-stage attack execution, and comprehensive labeling of system and network\nlogs, dramatically reducing manual effort and enabling scalable dataset\ncreation. A central contribution of LMDG is Process Tree Labeling, a novel\nagent-based technique that traces all malicious activity back to its origin\nwith high precision. Unlike prior methods such as Injection Timing or\nBehavioral Profiling, Process Tree Labeling enables accurate, step-wise\nlabeling of malicious log entries, correlating each with a specific attack step\nand MITRE ATT\\&CK TTPs. To our knowledge, this is the first approach to support\nfine-grained labeling of multi-step attacks, providing critical context for\ndetection models such as attack path reconstruction. We used LMDG to generate a\n25-day dataset within a 25-VM enterprise environment containing 22 user\naccounts. The dataset includes 944 GB of host and network logs and embeds 35\nmulti-stage LM attacks, with malicious events comprising less than 1% of total\nactivity, reflecting a realistic benign-to-malicious ratio for evaluating\ndetection systems. LMDG-generated datasets improve upon existing ones by\noffering diverse LM attacks, up-to-date attack patterns, longer attack\ntimeframes, comprehensive data sources, realistic network architectures, and\nmore accurate labeling.", "AI": {"tldr": "LMDG is a novel framework for generating high-fidelity lateral movement datasets with automated tooling, agent-based Process Tree Labeling for fine-grained attack tracking, and comprehensive 944GB logs from a 25-VM environment containing realistic multi-step attacks.", "motivation": "Enterprise security systems require realistic datasets for effective lateral movement detection research, but such datasets remain scarce due to complexities of manual labeling and attack simulation.", "method": "The LMDG framework combines automated benign activity generation, multi-stage attack execution, and Process Tree Labeling - a novel agent-based technique that systematically traces malicious activity origins while correlating log entries to specific MITRE ATT&CK TTPs.", "result": "A 25-day dataset from a 25-VM enterprise environment with 35 multi-stage LM attacks, 944GB of logs containing <1% malicious events, and fine-grained attack phase labeling across diverse attack patterns.", "conclusion": "LMDG advances LM detection by creating more realistic, accurate, and analyzable datasets than existing approaches, particularly improving fine-grained labeling of multi-step attacks with precise TTP correlations."}}
{"id": "2508.02998", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02998", "abs": "https://arxiv.org/abs/2508.02998", "authors": ["Haiyang Li"], "title": "MRG-Bench: Evaluating and Exploring the Requirements of Context for Repository-Level Code Generation", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode generation. However, current evaluation datasets suffer from issues such\nas the lack of runnable test cases, deviation from the distribution of\nreal-world code, and the ability to evaluate only the Python language. These\nlimitations undermine the credibility of the evaluation results.\n  To address these limitations, we introduce \\textbf{MRG-Bench} (Multi-language\nRepository-level Code Generation Benchmark), a novel dataset that provides a\nmore accurate evaluation of LLMs in practical repository-level code generation\ntasks. MRG-Bench has three main features: (1) practical data sourced from\nreal-world code repositories that align to the practical distribution, (2)\nmultiple programming languages support, including Python, Java, and Go, and (3)\nproject-level runnable test cases to assess the quality of the generated code.\n  Based on MRG-Bench, we conducted extensive experiments including large\nlanguage models, long-context models, and RAG-related methods. These evaluation\nresults demonstrate that \\textbf{current repository-level code generation\ntechniques suffer from significant performance deficiencies}. To further\ninvestigate why models fail, we designed novel experiments to annotate the\nunderlying causes of generation errors. The results explicitly show that the\nmajority of methods suffer from \"\\textbf{difficulty in understanding user\nrequirements},\" failing to comprehend their assigned tasks accurately.\nMoreover, the impact of different repository-level contexts on this issue\nexhibits significant disparities across different programming languages,\nsuggesting that, in practice, specialized contextual information needs to be\ndesigned for different languages.", "AI": {"tldr": "The paper introduces MRG-Bench, a comprehensive multi-language code generation benchmark with real-world repositories and runnable test cases, revealing that current techniques struggle with understanding user requirements and exhibit language-specific performance disparities.", "motivation": "Current code generation evaluation datasets lack runnable test cases, deviate from real-world code distribution, and only support Python, leading to unreliable performance assessments.", "method": "The authors created MRG-Bench with three features: real-world repository data, support for Python/Java/Go, and project-level runnable test cases. They evaluated LLMs, long-context models, and RAG methods, then conducted error-annotation experiments to analyze failure modes.", "result": "Experiments show current techniques have significant performance gaps in repository-level code generation. Error analysis indicates most failures stem from misunderstanding user requirements, with marked differences in challenge severity across languages.", "conclusion": "MRG-Bench exposes critical limitations in code generation approaches, demonstrating the need for improved task understanding mechanisms and language-specific contextual engineering due to varying requirements across programming languages."}}
{"id": "2508.02943", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02943", "abs": "https://arxiv.org/abs/2508.02943", "authors": ["Baigang Chen", "Dongfang Zhao"], "title": "A Non-leveled and Reliable Approximate FHE Framework through Binarized Polynomial Rings", "comment": null, "summary": "Homomorphic encryption (HE) enables secure computation on encrypted data,\nsafeguarding user privacy in domains such as cloud computing, healthcare, and\nfinance. Among fully homomorphic encryption (FHE) schemes, CKKS is notable for\nsupporting approximate arithmetic over complex numbers, a key requirement for\nmachine-learning and numerical workloads. However, CKKS incurs rapid noise\ngrowth, complex parameter tuning, and relies on costly modulus switching. We\npropose a binary variant of CKKS that operates entirely over binary-coefficient\npolynomial rings and replaces rescaling with a lightweight bootstrapping\nmechanism. To mitigate additional bit-flip errors introduced by binary\nencoding, we integrate BCH error-correcting codes for robust decryption. Our\nopen-source implementation, built on the HElib library, preserves the core\nalgebraic structure of CKKS while introducing binary-coefficient encoding,\nenabling efficient evaluation in small ring dimensions and unbounded-depth\ncomputation. Empirical evaluations demonstrate the framework's practicality and\nscalability across a range of settings.", "AI": {"tldr": "The paper introduces a binary variant of the CKKS fully homomorphic encryption (FHE) scheme. By replacing modulus switching with lightweight bootstrapping and integrating BCH error-correcting codes, it addresses CKKS's inefficiencies (noise growth, complex parameter tuning) while maintaining approximate arithmetic capabilities. The implementation on HElib demonstrates practicality and scalability for machine-learning and numerical workloads.", "motivation": "CKKS-based FHE suffers from rapid noise growth, computationally expensive modulus switching, and intricate parameter tuning, limiting its efficiency and practicality for applications requiring approximate computations (e.g., machine learning and numerical analysis).", "method": "Proposes a binary CKKS variant that 1) uses binary-coefficient polynomial rings 2) replaces modulus switching with a lightweight bootstrapping mechanism 3) integrates BCH error-correcting codes to mitigate bit-flip errors from binary encoding. The implementation retains CKKS's algebraic structure while enabling efficient small-ring operations.", "result": "Empirical evaluations confirm the framework matches CKKS's functionality for approximate arithmetic while enabling unbounded-depth computations in small rings. The integration of BCH codes ensures robust decryption, and the HElib-based implementation validates its practicality and scalability across diverse settings.", "conclusion": "This binary CKKS variant provides a more efficient, parameter-flexible, and scalable FHE solution for workloads prioritizing approximate numerical computations, with BCH codes resolving error tradeoffs inherent to binary encoding."}}
{"id": "2508.03012", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03012", "abs": "https://arxiv.org/abs/2508.03012", "authors": ["Zexiong Ma", "Chao Peng", "Qunhong Zeng", "Pengfei Gao", "Yanzhen Zou", "Bing Xie"], "title": "Tool-integrated Reinforcement Learning for Repo Deep Search", "comment": null, "summary": "Issue localization, the process of identifying code locations that need\nmodification to resolve software issues, is a critical yet challenging task in\nsoftware development. The semantic gap between natural language issue\ndescriptions and faulty code requires complex multi-hop reasoning through code\ndependencies. Existing LLM-based agents attempt to address this by integrating\nrepository retrieval tools. However, this transforms issue localization into a\ndemanding task we call Repo Deep Search, which requires the LLM to effectively\nutilize various repository retrieval tools throughout a multi-step reasoning\nand navigation process. To tackle this challenge, we present ToolTrain, a\ntwo-stage tool-integrated training framework combining rejection-sampled\nsupervised fine-tuning and tool-integrated reinforcement learning to enhance\nLLMs' ability to use retrieval tools for issue localization. Experimental\nresults show that ToolTrain-trained models achieve state-of-the-art\nperformance, with our 32B model even surpassing Claude-3.7 on function-level\nlocalization. The results also show that improved localization performance\ntranslates to better end-to-end issue resolution performance. This further\ndemonstrates that training for issue localization is a viable and effective\nstrategy for improving automated software development.", "AI": {"tldr": "ToolTrain enhances LLMs' ability to perform issue localization via a two-stage training framework using repository retrieval tools, achieving state-of-the-art results surpassing Claude-3.7.", "motivation": "Current LLM-based agents face the 'Repo Deep Search' challenge in issue localization due to the semantic gap between natural language issue descriptions and code, requiring complex multi-hop reasoning through code dependencies.", "method": "ToolTrain combines rejection-sampled supervised fine-tuning and tool-integrated reinforcement learning in a two-stage framework to improve LLM utilization of retrieval tools for issue localization.", "result": "ToolTrain-trained models achieve state-of-the-art performance in issue localization (with 32B model surpassing Claude-3.7 on function-level localization) and demonstrate improved end-to-end issue resolution performance compared to existing methods.", "conclusion": "Training LLMs specifically for issue localization using tool-integrated frameworks significantly improves automated software development performance, confirming the viability of this approach."}}
{"id": "2508.03062", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03062", "abs": "https://arxiv.org/abs/2508.03062", "authors": ["Rourab Paul", "Paresh Baidya", "Krishnendu Guha"], "title": "Lightweight Fault Detection Architecture for NTT on FPGA", "comment": null, "summary": "Post-Quantum Cryptographic (PQC) algorithms are mathematically secure and\nresistant to quantum attacks but can still leak sensitive information in\nhardware implementations due to natural faults or intentional fault injections.\nThe intent fault injection in side-channel attacks reduces the reliability of\ncrypto implementation in future generation network security procesors. In this\nregard, this research proposes a lightweight, efficient, recomputation-based\nfault detection module implemented on a Field Programmable Gate Array (FPGA)\nfor Number Theoretic Transform (NTT). The NTT is primarily composed of memory\nunits and the Cooley-Tukey Butterfly Unit (CT-BU), a critical and\ncomputationally intensive hardware component essential for polynomial\nmultiplication. NTT and polynomial multiplication are fundamental building\nblocks in many PQC algorithms, including Kyber, NTRU, Ring-LWE, and others. In\nthis paper, we present a fault detection method called : Recomputation with a\nModular Offset (REMO) for the logic blocks of the CT-BU using Montgomery\nReduction and another method called Memory Rule Checkers for the memory\ncomponents used within the NTT. The proposed fault detection framework sets a\nnew benchmark by achieving high efficiency with significant low implementation\ncost. It occupies only 16 slices and a single DSP block, with a power\nconsumption of just 3mW in Artix-7 FPGA. The REMO-based detection mechanism\nachieves a fault coverage of 87.2% to 100%, adaptable across various word\nsizes, fault bit counts, and fault injection modes. Similarly, the Memory Rule\nCheckers demonstrate robust performance, achieving 50.7% to 100% fault\ndetection depending on and the nature of injected faults.", "AI": {"tldr": "This paper proposes a fault detection framework for hardware implementations of PQC algorithms, combining recomputation-based methods (REMO) and memory rule checkers optimized for FPGAs, achieving high efficiency with minimal resource usage.", "motivation": "Post-Quantum Cryptographic algorithms remain vulnerable to hardware faults and intentional attacks (e.g., side-channel) in future network security processors, necessitating lightweight and efficient fault detection techniques.", "method": "The authors implement a recomputation-based fault detection module (REMO) using Montgomery Reduction for CT-BU logic blocks and Memory Rule Checkers for memory components within the NTT on an FPGA.", "result": "The framework achieved 87.2%-100% fault coverage for REMO and 50.7%-100% for memory checkers. On Artix-7 FPGA, it consumes only 16 slices, one DSP block, and 3mW of power.", "conclusion": "The proposed fault detection framework establishes a new benchmark for PQC hardware with exceptional efficiency, low implementation cost, and adaptability across word sizes, fault parameters, and injection modes."}}
{"id": "2508.03215", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03215", "abs": "https://arxiv.org/abs/2508.03215", "authors": ["Dongming Jin", "Zhi Jin", "Linyu Li", "Zheng Fang", "Jia Li", "Xiaohong Chen"], "title": "A System Model Generation Benchmark from Natural Language Requirements", "comment": "16 pages, 14 figures", "summary": "System models, a critical artifact in software development, provide a formal\nabstraction of both the structural and behavioral aspects of software systems,\nwhich can facilitate the early requirements analysis and architecture design.\nHowever, developing system models remains challenging due to the specific\nsyntax of model description languages and the relative scarcity of public model\nexamples. While large language models (LLMs) have shown promise in generating\ncode with programming languages and could potentially aid in system model\ndevelopment, no benchmarks currently exist for evaluating their ability to\ngenerate system models with specific description languages. We present\nSysMBench, which comprises 151 human-curated scenarios spanning a wide range of\npopular domains and varying difficulty levels. Each scenario mainly comprises a\nnatural language requirements description, a system model expressed in a\nspecific model description language, and a visualized system model diagram. The\nrequirements description is fed as user input to the LLM, the system model with\ndescription language is used to verify if the generated system model conforms\nto the requirements, and the visualized diagram serves to support manual\nvalidation. We introduce SysMEval, a semantic-aware evaluation metric to\nevaluate the quality of generated system models. We evaluate 17 popular LLMs on\nthis task with three traditional metrics and SysMEval, from directly prompting\nto three commonly used enhancement strategies. Our in-depth evaluation shows\nthat LLMs perform poorly on SysMBench, with the highest BLEU of 4% and\nSysMEval-F1 of 62%. We release the SysMBench and its evaluation framework to\nenable future research on LLM-based system model generation.", "AI": {"tldr": "This paper introduces SysMBench, a benchmark for evaluating large language models' ability to generate system models, revealing underwhelming performance with max BLEU-4% and SysMEval-F1 62%.", "motivation": "System models are crucial for software development but challenging to create due to domain-specific syntax and lack of public examples, creating a gap in LLM evaluation benchmarks for this task.", "method": "Developed 151 human-curated scenarios with natural language requirements and corresponding formal system models. Applied three enhancement strategies to test 17 LLMs using traditional metrics and the proposed semantic-aware SysMEval metric.", "result": "All 17 evaluated LLMs demonstrated poor performance on SysMBench, achieving maximum BLEU-4% and SysMEval-F1 score of 62%.", "conclusion": "LLMs currently lack robust capabilities for system model generation; the publicly released benchmark and evaluation framework will enable targeted research in this area."}}
{"id": "2508.03067", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03067", "abs": "https://arxiv.org/abs/2508.03067", "authors": ["Jiewei Lai", "Lan Zhang", "Chen Tang", "Pengcheng Sun", "Xinming Wang", "Yunhao Wang"], "title": "Untraceable DeepFakes via Traceable Fingerprint Elimination", "comment": null, "summary": "Recent advancements in DeepFakes attribution technologies have significantly\nenhanced forensic capabilities, enabling the extraction of traces left by\ngenerative models (GMs) in images, making DeepFakes traceable back to their\nsource GMs. Meanwhile, several attacks have attempted to evade attribution\nmodels (AMs) for exploring their limitations, calling for more robust AMs.\nHowever, existing attacks fail to eliminate GMs' traces, thus can be mitigated\nby defensive measures. In this paper, we identify that untraceable DeepFakes\ncan be achieved through a multiplicative attack, which can fundamentally\neliminate GMs' traces, thereby evading AMs even enhanced with defensive\nmeasures. We design a universal and black-box attack method that trains an\nadversarial model solely using real data, applicable for various GMs and\nagnostic to AMs. Experimental results demonstrate the outstanding attack\ncapability and universal applicability of our method, achieving an average\nattack success rate (ASR) of 97.08\\% against 6 advanced AMs on DeepFakes\ngenerated by 9 GMs. Even in the presence of defensive mechanisms, our method\nmaintains an ASR exceeding 72.39\\%. Our work underscores the potential\nchallenges posed by multiplicative attacks and highlights the need for more\nrobust AMs.", "AI": {"tldr": "Introduces a multiplicative black-box attack that universally eliminates traces from 9 GMs, achieving 97.08% average ASR against 6 advanced AMs and remaining effective beyond 72.39% even with defenses.", "motivation": "Current attribution attacks on DeepFakes leave residual traces vulnerable to defensive mitigation, necessitating stronger universal attack methods to stress-test forensic model robustness.", "method": "Developed a universal, model-agnostic attack training an adversarial discriminator using only pure real data to eliminate generative model fingerprints via multiplicative signal modification.", "result": "97.08% mean attack success rate across 6 AMs on 9 GMs with 0.8% sample distortion, maintaining 72.39%+ effectiveness against defense-enhanced AMs in cross-architecture testing.", "conclusion": "Multiplicative attacks fundamentally undermine existing attribution frameworks, highlighting critical vulnerabilities and urging development of higher-order forensic defense mechanisms against data space manipulations."}}
{"id": "2508.03258", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03258", "abs": "https://arxiv.org/abs/2508.03258", "authors": ["Yueyue Liu", "Hongyu Zhang", "Yuantian Miao"], "title": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization", "comment": null, "summary": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%.", "AI": {"tldr": "This paper introduces SmartLLMs Scheduler (SLS), a dynamic scheduling solution that optimizes LLM performance and cost by leveraging adaptive caching, performance-cost-based allocation, and real-time updates. Experiments show significant gains in performance and processing efficiency.", "motivation": "Despite advancements in LLMs, deploying them for diverse software engineering tasks remains costly and slow due to static scheduling relying on extensive training data. Current methods lack flexibility to adapt to query volumes and task variations.", "method": "SLS employs three components: (1) an Adaptive Cache Manager to store and reuse query outputs, (2) a Performance-Cost Optimized Scheduler using query/LLM features for dynamic allocation, and (3) a Dynamic Update Manager to refine strategies with real-time feedback from processed queries.", "result": "Experimental evaluation on log parsing and code generation tasks demonstrates SLS achieves 198.82% average performance improvement and 63.28% reduction in processing time compared to baseline static scheduling methods.", "conclusion": "SLS effectively addresses limitations of static scheduling through dynamic adaptation, significantly enhancing both performance and cost efficiency for LLM deployment, with strong experimental validation across software engineering workloads."}}
{"id": "2508.03097", "categories": ["cs.CR", "cs.AI", "I.2.11"], "pdf": "https://arxiv.org/pdf/2508.03097", "abs": "https://arxiv.org/abs/2508.03097", "authors": ["Zixuan Gu", "Qiufeng Fan", "Long Sun", "Yang Liu", "Xiaojun Ye"], "title": "VFLAIR-LLM: A Comprehensive Framework and Benchmark for Split Learning of LLMs", "comment": "12 pages, 10 figures, published in KDD2025", "summary": "With the advancement of Large Language Models (LLMs), LLM applications have\nexpanded into a growing number of fields. However, users with data privacy\nconcerns face limitations in directly utilizing LLM APIs, while private\ndeployments incur significant computational demands. This creates a substantial\nchallenge in achieving secure LLM adaptation under constrained local resources.\nTo address this issue, collaborative learning methods, such as Split Learning\n(SL), offer a resource-efficient and privacy-preserving solution for adapting\nLLMs to private domains. In this study, we introduce VFLAIR-LLM (available at\nhttps://github.com/FLAIR-THU/VFLAIR-LLM), an extensible and lightweight split\nlearning framework for LLMs, enabling privacy-preserving LLM inference and\nfine-tuning in resource-constrained environments. Our library provides two LLM\npartition settings, supporting three task types and 18 datasets. In addition,\nwe provide standard modules for implementing and evaluating attacks and\ndefenses. We benchmark 5 attacks and 9 defenses under various Split Learning\nfor LLM(SL-LLM) settings, offering concrete insights and recommendations on the\nchoice of model partition configurations, defense strategies, and relevant\nhyperparameters for real-world applications.", "AI": {"tldr": "VFLAIR-LLM is an extensible, lightweight split learning framework addressing privacy concerns and computational costs in adapting LLMs to private domains through resource-efficient methods, benchmarking 5 attacks/9 defenses to guide real-world implementation choices.", "motivation": "LLMs' deployment faces challenges: privacy limitations with APIs, high computational costs for private setups, and difficulty in securely adapting models under constrained local resources.", "method": "Developed VFLAIR-LLM using Split Learning (SL), providing two model partition settings and standard modules for attacks/defenses. Evaluated three task types and 18 datasets while benchmarking 5 attacks and 9 defenses across SL-LLM configurations.", "result": "The framework enables privacy-preserving LLM inference and fine-tuning in resource-constrained environments, with analysis of attack-defense effectiveness across different partition settings yielding actionable insights.", "conclusion": "VFLAIR-LLM offers a practical solution for secure, resource-efficient LLM adaptation. The paper provides concrete recommendations on model partitioning, defense strategies, and hyperparameters based on comprehensive benchmarking under diverse SL-LLM scenarios."}}
{"id": "2508.03298", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03298", "abs": "https://arxiv.org/abs/2508.03298", "authors": ["Kristian Kolthoff", "Felix Kretzer", "Christian Bartelt", "Alexander Maedche", "Simone Paolo Ponzetto"], "title": "GUI-ReRank: Enhancing GUI Retrieval with Multi-Modal LLM-based Reranking", "comment": null, "summary": "GUI prototyping is a fundamental component in the development of modern\ninteractive systems, which are now ubiquitous across diverse application\ndomains. GUI prototypes play a critical role in requirements elicitation by\nenabling stakeholders to visualize, assess, and refine system concepts\ncollaboratively. Moreover, prototypes serve as effective tools for early\ntesting, iterative evaluation, and validation of design ideas with both end\nusers and development teams. Despite these advantages, the process of\nconstructing GUI prototypes remains resource-intensive and time-consuming,\nfrequently demanding substantial effort and expertise. Recent research has\nsought to alleviate this burden through NL-based GUI retrieval approaches,\nwhich typically rely on embedding-based retrieval or tailored ranking models\nfor specific GUI repositories. However, these methods often suffer from limited\nretrieval performance and struggle to generalize across arbitrary GUI datasets.\nIn this work, we present GUI-ReRank, a novel framework that integrates rapid\nembedding-based constrained retrieval models with highly effective MLLM-based\nreranking techniques. GUI-ReRank further introduces a fully customizable GUI\nrepository annotation and embedding pipeline, enabling users to effortlessly\nmake their own GUI repositories searchable, which allows for rapid discovery of\nrelevant GUIs for inspiration or seamless integration into customized LLM-based\nRAG workflows. We evaluated our approach on an established NL-based GUI\nretrieval benchmark, demonstrating that GUI-ReRank significantly outperforms\nSOTA tailored LTR models in both retrieval accuracy and generalizability.\nAdditionally, we conducted a comprehensive cost and efficiency analysis of\nemploying MLLMs for reranking, providing valuable insights regarding the\ntrade-offs between retrieval effectiveness and computational resources. Video:\nhttps://youtu.be/_7x9UCh82ug", "AI": {"tldr": "This work introduces GUI-ReRank, a framework combining embedding-based retrieval with MLLM reranking to improve GUI prototyping efficiency and generalizability across datasets.", "motivation": "GUI prototyping is critical but resource-intensive. Existing NL-based GUI retrieval methods suffer from limited performance and poor generalization, prompting the need for a more versatile solution.", "method": "GUI-ReRank integrates fast embedding-based constrained retrieval models with effective MLLM reranking techniques, and features a customizable GUI repository annotation/embedding pipeline for seamless RAG integration.", "result": "Demonstrated superior performance in retrieval accuracy on an established benchmark compared to SOTA tailored LTR models, while conducting a comprehensive cost/efficiency analysis of MLLM reranking.", "conclusion": "GUI-ReRank provides a scalable, accurate framework for NL-based GUI retrieval with better generalizability than prior methods, along with practical trade-off insights for MLLM deployment."}}
{"id": "2508.03125", "categories": ["cs.CR", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.03125", "abs": "https://arxiv.org/abs/2508.03125", "authors": ["Bingyu Yan", "Ziyi Zhou", "Xiaoming Zhang", "Chaozhuo Li", "Ruilin Zeng", "Yirui Qi", "Tianbo Wang", "Litian Zhang"], "title": "Attack the Messages, Not the Agents: A Multi-round Adaptive Stealthy Tampering Framework for LLM-MAS", "comment": null, "summary": "Large language model-based multi-agent systems (LLM-MAS) effectively\naccomplish complex and dynamic tasks through inter-agent communication, but\nthis reliance introduces substantial safety vulnerabilities. Existing attack\nmethods targeting LLM-MAS either compromise agent internals or rely on direct\nand overt persuasion, which limit their effectiveness, adaptability, and\nstealthiness. In this paper, we propose MAST, a Multi-round Adaptive Stealthy\nTampering framework designed to exploit communication vulnerabilities within\nthe system. MAST integrates Monte Carlo Tree Search with Direct Preference\nOptimization to train an attack policy model that adaptively generates\neffective multi-round tampering strategies. Furthermore, to preserve\nstealthiness, we impose dual semantic and embedding similarity constraints\nduring the tampering process. Comprehensive experiments across diverse tasks,\ncommunication architectures, and LLMs demonstrate that MAST consistently\nachieves high attack success rates while significantly enhancing stealthiness\ncompared to baselines. These findings highlight the effectiveness,\nstealthiness, and adaptability of MAST, underscoring the need for robust\ncommunication safeguards in LLM-MAS.", "AI": {"tldr": "This paper proposes MAST, a framework targeting communication vulnerabilities in multi-agent language models (LLM-MAS) by generating adaptive, multi-round tampering strategies using Monte Carlo Tree Search and Direct Preference Optimization, while maintaining stealth through semantic/embedding similarity constraints.", "motivation": "Current LLM-MAS attack methods lack effectiveness, adaptability, and stealth by either compromising internal agent components or relying on direct persuasion, creating security gaps in communication-dependent systems.", "method": "MAST trains an attack policy via Monte Carlo Tree Search (for strategic planning) and Direct Preference Optimization (for response generation), incorporating dual constraints on semantic meaning and embedding representations to ensure tampered communication remains undetectable.", "result": "Experiments show MAST achieves high success rates across diverse tasks and LLM architectures, demonstrating significant improvements in stealthiness compared to baseline attack methods while maintaining adaptability.", "conclusion": "MAST reveals critical security challenges in LLM-MAS communication mechanisms, emphasizing the necessity for robust safeguards against adaptive, stealthy tampering strategies in multi-agent collaboration frameworks."}}
{"id": "2508.03329", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03329", "abs": "https://arxiv.org/abs/2508.03329", "authors": ["Mari Ashiga", "Vardan Voskanyan", "Fateme Dinmohammadi", "Jingzhi Gong", "Paul Brookes", "Matthew Truscott", "Rafail Giavrimis", "Mike Basios", "Leslie Kanthan", "Wei Jie"], "title": "Industrial LLM-based Code Optimization under Regulation: A Mixture-of-Agents Approach", "comment": "Submitted to ASE'25 Industry Showcase", "summary": "Recent advancements in Large Language Models (LLMs) for code optimization\nhave enabled industrial platforms to automate software performance engineering\nat unprecedented scale and speed. Yet, organizations in regulated industries\nface strict constraints on which LLMs they can use - many cannot utilize\ncommercial models due to data privacy regulations and compliance requirements,\ncreating a significant challenge for achieving high-quality code optimization\nwhile maintaining cost-effectiveness. We address this by implementing a\nMixture-of-Agents (MoA) approach that directly synthesizes code from multiple\nspecialized LLMs, comparing it against TurinTech AI's vanilla Genetic Algorithm\n(GA)-based ensemble system and individual LLM optimizers using real-world\nindustrial codebases. Our key contributions include: (1) First MoA application\nto industrial code optimization using real-world codebases; (2) Empirical\nevidence that MoA excels with open-source models, achieving 14.3% to 22.2% cost\nsavings and 28.6% to 32.2% faster optimization times for regulated\nenvironments; (3) Deployment guidelines demonstrating GA's advantage with\ncommercial models while both ensembles outperform individual LLMs; and (4)\nReal-world validation across 50 code snippets and seven LLM combinations,\ngenerating over 8,700 variants, addresses gaps in industrial LLM ensemble\nevaluation. This provides actionable guidance for organizations balancing\nregulatory compliance with optimization performance in production environments.", "AI": {"tldr": "This paper explores the use of a Mixture-of-Agents (MoA) approach with open-source LLMs to optimize code for regulated industries, achieving cost and time savings compared to existing methods like GA-based ensembles.", "motivation": "Regulated industries face data privacy and compliance constraints limiting their use of commercial LLMs for code optimization, creating a need for cost-effective and compliant alternatives.", "method": "The authors implemented MoA to synthesize code via multiple specialized open-source LLMs and benchmarked it against TurinTech AI's GA-based ensemble system and standalone LLM optimizers using real-world industrial codebases.", "result": "MoA using open-source models achieved 14.3%-22.2% cost savings and 28.6%-32.2% faster optimization times for regulated contexts, validated across 50 code snippets and seven LLM combinations generating 8,700+ variants.", "conclusion": "The study demonstrates MoA's superiority for open-source models in regulated environments while GA has advantages for commercial models. It provides actionable deployment guidelines and fills gaps in industrial LLM ensemble evaluation."}}
{"id": "2508.03130", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03130", "abs": "https://arxiv.org/abs/2508.03130", "authors": ["Rama Carl Hoetzlein"], "title": "Protecting Small Organizations from AI Bots with Logrip: Hierarchical IP Hashing", "comment": "11 pages, 4 figures", "summary": "Small organizations, start ups, and self-hosted servers face increasing\nstrain from automated web crawlers and AI bots, whose online presence has\nincreased dramatically in the past few years. Modern bots evade traditional\nthrottling and can degrade server performance through sheer volume even when\nthey are well-behaved. We introduce a novel security approach that leverages\ndata visualization and hierarchical IP hashing to analyze server event logs,\ndistinguishing human users from automated entities based on access patterns. By\naggregating IP activity across subnet classes and applying statistical\nmeasures, our method detects coordinated bot activity and distributed crawling\nattacks that conventional tools fail to identify. Using a real world example we\nestimate that 80 to 95 percent of traffic originates from AI crawlers,\nunderscoring the need for improved filtering mechanisms. Our approach enables\nsmall organizations to regulate automated traffic effectively, preserving\npublic access while mitigating performance degradation.", "AI": {"tldr": "This paper proposes a bot detection method using data visualization and hierarchical IP hashing to help small organizations mitigate performance degradation caused by automated web crawlers and AI bots.", "motivation": "Traditional bot throttling techniques are ineffective against modern bots due to their evolving nature and sheer volume, causing significant server strain for small organizations and self-hosted servers. Improved filtering mechanisms are needed to preserve performance without blocking legitimate human users.", "method": "The approach combines data visualization with hierarchical IP hashing of server event logs, aggregating IP activity across subnet classes and analyzing statistical patterns to distinguish human users from coordinated bot activity, including distributed crawling attacks.", "result": "Real-world testing revealed 80-95% of traffic was from AI crawlers. The method successfully detected bot activity patterns that conventional tools missed, enabling effective regulation of automated traffic while maintaining public access.", "conclusion": "The proposed technique provides an accessible solution for resource-constrained organizations to identify and manage bot traffic through pattern analysis, addressing the growing challenge of bot-related server performance degradation."}}
{"id": "2508.03340", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03340", "abs": "https://arxiv.org/abs/2508.03340", "authors": ["Alex Wolf", "Marco Edoardo Palma", "Pooja Rani", "Harald C. Gall"], "title": "Key-Augmented Neural Triggers for Knowledge Sharing", "comment": null, "summary": "Repository-level code comprehension and knowledge sharing remain core\nchallenges in software engineering. Large language models (LLMs) have shown\npromise by generating explanations of program structure and logic. However,\nthese approaches still face limitations: First, relevant knowledge is\ndistributed across multiple files within a repository, aka semantic\nfragmentation. Second, retrieval inefficiency and attention saturation degrade\nperformance in RAG pipelines, where long, unaligned contexts overwhelm\nattention. Third, repository specific training data is scarce and often\noutdated. Finally, proprietary LLMs hinder industrial adoption due to privacy\nand deployment constraints. To address these issues, we propose Key-Augmented\nNeural Triggers (KANT), a novel approach that embeds knowledge anchors into\nboth training and inference. Unlike prior methods, KANT enables internal access\nto repository specific knowledge, reducing fragmentation and grounding\ninference in localized context. Moreover, we synthesize specialized data\ndirectly from code. At inference, knowledge anchors replace verbose context,\nreducing token overhead and latency while supporting efficient, on premise\ndeployment. We evaluate KANT via: a qualitative human evaluation of the\nsynthesized dataset's intent coverage and quality across five dimensions;\ncompare against SOTA baselines across five qualitative dimensions and inference\nspeed; and replication across different LLMs to assess generalizability.\nResults show that the synthetic training data aligned with information-seeking\nneeds. KANT achieved over 60% preference from human annotators and a LocalStack\nexpert (preferring 79% of cases). Also, KANT reduced inference latency by up to\n85% across all models. Overall, it is well-suited for scalable, low-latency,\non-premise deployments, providing a strong foundation for code comprehension.", "AI": {"tldr": "KANT is a novel approach improving repository-level code comprehension through knowledge anchors, reducing semantic fragmentation and inference latency by 85% while enabling on-premise deployment.", "motivation": "Current methods struggle with semantic fragmentation (knowledge spread across files), RAG pipeline inefficiencies (attention saturation from long contexts), scarce/updated training data, and industrial adoption barriers due to proprietary LLMs.", "method": "KANT embeds knowledge anchors during training and inference, synthesizes repository-specific data from code, and replaces verbose contexts in RAG pipelines to reduce token usage and latency.", "result": "Human evaluations showed a 60% preference over SOTA models (79% for LocalStack experts) and a 85% latency reduction. Synthetic data aligned with task-specific knowledge needs.", "conclusion": "KANT effectively addresses semantic fragmentation and deployment challenges while maintaining performance and generalizing across models, proving its scalability for industrial code comprehension."}}
{"id": "2508.03151", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03151", "abs": "https://arxiv.org/abs/2508.03151", "authors": ["Ronghua Li", "Shinan Liu", "Haibo Hu", "Qingqing Ye", "Nick Feamster"], "title": "WiFinger: Fingerprinting Noisy IoT Event Traffic Using Packet-level Sequence Matching", "comment": null, "summary": "IoT environments such as smart homes are susceptible to privacy inference\nattacks, where attackers can analyze patterns of encrypted network traffic to\ninfer the state of devices and even the activities of people. While most\nexisting attacks exploit ML techniques for discovering such traffic patterns,\nthey underperform on wireless traffic, especially Wi-Fi, due to its heavy noise\nand packet losses of wireless sniffing. In addition, these approaches commonly\ntarget at distinguishing chunked IoT event traffic samples, and they failed at\neffectively tracking multiple events simultaneously. In this work, we propose\nWiFinger, a fine-grained multi-IoT event fingerprinting approach against noisy\ntraffic. WiFinger turns the traffic pattern classification task into a\nsubsequence matching problem and introduces novel techniques to account for the\nhigh time complexity while maintaining high accuracy. Experiments demonstrate\nthat our method outperforms existing approaches on Wi-Fi traffic, achieving an\naverage recall of 85% (vs. 0.49% and 0.46%) for various IoT events while\nmaintaining almost zero false positives for most of them.", "AI": {"tldr": "WiFinger is a novel fine-grained multi-IoT event fingerprinting approach designed to track IoT device usage in noisy Wi-Fi environments. It achieves an average recall of 85% with minimal false positives, significantly outperforming existing ML-based traffic analysis methods that struggle with wireless noise.", "motivation": "Traditional ML-based privacy inference attacks on IoT traffic fail in wireless (especially Wi-Fi) settings due to environmental noise, packet losses, and limitations in handling concurrent multi-event tracking.", "method": "WiFinger transforms traffic pattern classification into a subsequence matching problem, using novel algorithms to reduce time complexity while maintaining accuracy through robust handling of noisy and fragmented Wi-Fi data streams.", "result": "Experiments showed WiFinger achieved 85% average recall for IoT event detection (vs. 0.49% and 0.46% from prior methods) while maintaining nearly zero false positives for most events, demonstrating superior performance in multi-event tracking scenarios with wireless impairments.", "conclusion": "WiFinger provides a critical improvement in IoT traffic analysis for privacy attacks by addressing wireless-specific challenges through its subsequence matching framework, enabling accurate simultaneous tracking of multiple events in noisy Wi-Fi environments."}}
{"id": "2508.03369", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03369", "abs": "https://arxiv.org/abs/2508.03369", "authors": ["Beatriz Santana", "Lidiv\u00e2nio Monte", "Bianca Santana de Ara\u00fajo Silva", "Glauco Carneiro", "S\u00e1vio Freire", "Jos\u00e9 Amancio Macedo Santos", "Manoel Mendon\u00e7a"], "title": "Psychological safety in software workplaces: A systematic literature review", "comment": null, "summary": "Context: Psychological safety (PS) is an important factor influencing team\nwell-being and performance, particularly in collaborative and dynamic domains\nsuch as software development. Despite its acknowledged significance, research\non PS within the field of software engineering remains limited. The\nsocio-technical complexities and fast-paced nature of software development\npresent challenges to cultivating PS. To the best of our knowledge, no\nsystematic secondary study has synthesized existing knowledge on PS in the\ncontext of software engineering.\n  Objective: This study aims to systematically review and synthesize the\nexisting body of knowledge on PS in software engineering. Specifically, it\nseeks to identify the potential antecedents and consequences associated with\nthe presence or absence of PS among individuals involved in the software\ndevelopment process.\n  Methods: A systematic literature review was conducted, encompassing studies\nretrieved from four digital libraries. The extracted data were subjected to\nboth quantitative and qualitative analyses.\n  Results: The findings indicate a growing academic interest in PS within\nsoftware engineering, with the majority of studies grounded in Edmondson's\nframework. Factors antecedents of PS were identified at the individual, team,\nand organizational levels, including team autonomy, agile methodologies, and\nleadership behaviors.\n  Conclusion: PS fosters innovation, learning, and team performance within\nsoftware development. However, significant gaps persist in understanding the\ncontextual factors influencing PS, its underlying mechanisms, and effective\nstrategies for its enhancement. Future research should address these gaps by\ninvestigating the practical applications of PS within diverse organizational\nsettings in the software engineering domain.", "AI": {"tldr": "This paper systematically reviews and synthesizes existing research on psychological safety (PS) in software engineering, identifying antecedents, consequences, and highlighting knowledge gaps.", "motivation": "Despite PS's importance in software development teams, prior research lacks a systematic synthesis of the field, leaving gaps in understanding socio-technical complexities and effective strategies for enhancement.", "method": "A systematic literature review across four digital libraries using quantitative and qualitative analysis to categorize findings across individual, team, and organizational levels.", "result": "Findings show increasing academic interest in PS using Edmondson's framework, with key antecedents like team autonomy, agile methodologies, and leadership behaviors identified.", "conclusion": "PS positively impacts innovation and team performance, but future work is needed to explore contextual influences, underlying mechanisms, and practical strategies across diverse software environments."}}
{"id": "2508.03221", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03221", "abs": "https://arxiv.org/abs/2508.03221", "authors": ["Yu Pan", "Jiahao Chen", "Lin Wang", "Bingrong Dai", "Yi Du"], "title": "BadBlocks: Low-Cost and Stealthy Backdoor Attacks Tailored for Text-to-Image Diffusion Models", "comment": null, "summary": "In recent years,Diffusion models have achieved remarkable progress in the\nfield of image generation.However,recent studies have shown that diffusion\nmodels are susceptible to backdoor attacks,in which attackers can manipulate\nthe output by injecting covert triggers such as specific visual patterns or\ntextual phrases into the training dataset.Fortunately,with the continuous\nadvancement of defense techniques,defenders have become increasingly capable of\nidentifying and mitigating most backdoor attacks using visual inspection and\nneural network-based detection methods.However,in this paper,we identify a\nnovel type of backdoor threat that is more lightweight and covert than existing\napproaches,which we name BadBlocks,requires only about 30\\% of the\ncomputational resources and 20\\% GPU time typically needed by previous backdoor\nattacks,yet it successfully injects backdoors and evades the most advanced\ndefense frameworks.BadBlocks enables attackers to selectively contaminate\nspecific blocks within the UNet architecture of diffusion models while\nmaintaining normal functionality in the remaining components.Experimental\nresults demonstrate that BadBlocks achieves a high attack success rate (ASR)\nand low perceptual quality loss (as measured by FID Score),even under extremely\nconstrained computational resources and GPU time.Moreover,BadBlocks is able to\nbypass existing defense frameworks,especially the attention-based backdoor\ndetection method, highlighting it as a novel and noteworthy threat.Ablation\nstudies further demonstrate that effective backdoor injection does not require\nfine-tuning the entire network and highlight the pivotal role of certain neural\nnetwork layers in backdoor mapping.Overall,BadBlocks significantly reduces the\nbarrier to conducting backdoor attacks in all aspects.It enables attackers to\ninject backdoors into large-scale diffusion models even using consumer-grade\nGPUs.", "AI": {"tldr": "The paper introduces BadBlocks, a lightweight and covert backdoor attack in diffusion models that requires minimal computational resources (30% less than existing methods) and GPU time (20% less), achieves high attack success rate (ASR), evades advanced defenses, and highlights vulnerabilities in UNet architectures without compromising overall functionality.", "motivation": "The paper addresses the need for more efficient and stealthy backdoor attacks against diffusion models, as existing defense methods (visual inspection, neural network-based detection) have become increasingly effective at mitigating traditional backdoor threats with higher resource requirements.", "method": "BadBlocks selectively contaminates specific blocks within the UNet architecture of diffusion models, avoiding full-network fine-tuning. It leverages limited computational resources and GPU time to inject backdoors covertly, maintaining normal functionality in non-contaminated components while enabling attackers to manipulate outputs through triggers like visual patterns or textual phrases.", "result": "\u574f\u5757\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u653b\u51fb\u6210\u529f\u7387\u9ad8 (ASR)\uff0c\u611f\u77e5\u8d28\u91cf\u635f\u5931\u4f4e (FID Score)\u3002Even under constrained resources, BadBlocks successfully evades detection by advanced defense frameworks, particularly attention-based backdoor detection methods, while maintaining normal diffusion model functionality through targeted block contamination.", "conclusion": "The paper identifies BadBlocks as a novel backdoor threat that significantly lowers the barrier for conducting backdoor attacks on diffusion models by requiring fewer computational resources and GPU time. It highlights architectural vulnerabilities in UNet-based models and underscores the need for improved defense strategies against such stealthy, localized attacks."}}
{"id": "2508.03393", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03393", "abs": "https://arxiv.org/abs/2508.03393", "authors": ["Muhammad Zohaib", "Muhammad Azeem Akbar", "Sami Hyrynsalmi", "Arif Ali Khan"], "title": "Agentic AI in 6G Software Businesses: A Layered Maturity Model", "comment": "6 pages, 3 figures and FIT'25 Conference", "summary": "The emergence of agentic AI systems in 6G software businesses presents both\nstrategic opportunities and significant challenges. While such systems promise\nincreased autonomy, scalability, and intelligent decision-making across\ndistributed environments, their adoption raises concerns regarding technical\nimmaturity, integration complexity, organizational readiness, and\nperformance-cost trade-offs. In this study, we conducted a preliminary thematic\nmapping to identify factors influencing the adoption of agentic software within\nthe context of 6G. Drawing on a multivocal literature review and targeted\nscanning, we identified 29 motivators and 27 demotivators, which were further\ncategorized into five high-level themes in each group. This thematic mapping\noffers a structured overview of the enabling and inhibiting forces shaping\norganizational readiness for agentic transformation. Positioned as a\nfeasibility assessment, the study represents an early phase of a broader\nresearch initiative aimed at developing and validating a layered maturity model\ngrounded in CMMI model with the software architectural three dimensions\npossibly Data, Business Logic, and Presentation. Ultimately, this work seeks to\nprovide a practical framework to help software-driven organizations assess,\nstructure, and advance their agent-first capabilities in alignment with the\ndemands of 6G.", "AI": {"tldr": "The paper analyzes factors influencing the adoption of agentic AI systems in 6G software, identifying 29 motivators and 27 demotivators grouped into five themes each. It proposes a CMMI-based layered maturity model across data, business logic, and presentation dimensions to guide organizational readiness.", "motivation": "6G software businesses face challenges in adopting autonomous, scalable agentic AI systems due to technical, integration, organizational, and cost-related barriers. The study addresses the need for a structured feasibility assessment to bridge this gap.", "method": "A multivocal literature review and targeted scanning were conducted to extract and categorize motivators/demotivators. Thematic mapping organized these into high-level themes for early-phase feasibility analysis.", "result": "29 motivators and 27 demotivators were categorized into five themes each, providing a structured overview of organizational readiness factors. The study serves as a preliminary step for developing a CMMI-grounded layered maturity model.", "conclusion": "The proposed maturity model aims to help software organizations align agent-first capabilities with 6G demands. Future work will refine and validate this framework for practical adoption guidance."}}
{"id": "2508.03307", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03307", "abs": "https://arxiv.org/abs/2508.03307", "authors": ["Ye Li", "Chengcheng Zhu", "Yanchao Zhao", "Jiale Zhang"], "title": "BDFirewall: Towards Effective and Expeditiously Black-Box Backdoor Defense in MLaaS", "comment": "18 pages", "summary": "In this paper, we endeavor to address the challenges of backdoor attacks\ncountermeasures in black-box scenarios, thereby fortifying the security of\ninference under MLaaS. We first categorize backdoor triggers from a new\nperspective, i.e., their impact on the patched area, and divide them into:\nhigh-visibility triggers (HVT), semi-visibility triggers (SVT), and\nlow-visibility triggers (LVT). Based on this classification, we propose a\nprogressive defense framework, BDFirewall, that removes these triggers from the\nmost conspicuous to the most subtle, without requiring model access. First, for\nHVTs, which create the most significant local semantic distortions, we identify\nand eliminate them by detecting these salient differences. We then restore the\npatched area to mitigate the adverse impact of such removal process. The\nlocalized purification designed for HVTs is, however, ineffective against SVTs,\nwhich globally perturb benign features. We therefore model an SVT-poisoned\ninput as a mixture of a trigger and benign features, where we unconventionally\ntreat the benign features as \"noise\". This formulation allows us to reconstruct\nSVTs by applying a denoising process that removes these benign \"noise\"\nfeatures. The SVT-free input is then obtained by subtracting the reconstructed\ntrigger. Finally, to neutralize the nearly imperceptible but fragile LVTs, we\nintroduce lightweight noise to disrupt the trigger pattern and then apply DDPM\nto restore any collateral impact on clean features. Comprehensive experiments\ndemonstrate that our method outperforms state-of-the-art defenses. Compared\nwith baselines, BDFirewall reduces the Attack Success Rate (ASR) by an average\nof 33.25%, improving poisoned sample accuracy (PA) by 29.64%, and achieving up\nto a 111x speedup in inference time. Code will be made publicly available upon\nacceptance.", "AI": {"tldr": "This paper proposes BDFirewall, a progressive defense framework against backdoor attacks in MLaaS black-box scenarios. It classifies triggers as high-, semi-, and low-visibility, and removes them through local purification, denoising, and noise injection with DDPM, without model access. The method achieves significant performance gains over existing defenses.", "motivation": "Current backdoor countermeasures struggle in black-box MLaaS settings where models are inaccessible. Existing approaches may fail to address subtle low-visibility triggers while effectively handling obvious high-visibility ones, leaving gaps in defense comprehensiveness and practicality.", "method": "BDFirewall employs a three-stage process: 1) Removes high-visibility triggers (HVT) via local purification and adversarial restoration 2) Attacks semi-visibility triggers (SVT) using feature denoising with benign features as 'noise' 3) Neutralizes low-visibility triggers (LVT) through strategic noise injection followed by DDPM-based restoration. All steps work without model access.", "result": "Comprehensive experiments show BDFirewall outperforms state-of-the-art defenses by reducing Attack Success Rate by 33.25% on average, improving poisoned sample accuracy by 29.64%, and achieving up to 111x faster inference compared to baseline methods.", "conclusion": "BDFirewall provides a novel, accessible approach to backdoor defense in MLaaS through progressive trigger removal categorized by visibility characteristics. The framework's model-agnostic design and demonstrated performance improvements establish it as a significant advancement in black-box security defense."}}
{"id": "2508.03435", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.03435", "abs": "https://arxiv.org/abs/2508.03435", "authors": ["Thomas S. Heinze", "Andr\u00e9 Sch\u00e4fer", "Wolfram Amme"], "title": "StoneDetector: Conventional and versatile code clone detection for Java", "comment": "supplementary information available at\n  https://stonedetector.fmi.uni-jena.de/", "summary": "Copy & paste is a widespread practice when developing software and, thus,\nduplicated and subsequently modified code occurs frequently in software\nprojects. Since such code clones, i.e., identical or similar fragments of code,\ncan bloat software projects and cause issues like bug or vulnerability\npropagation, their identification is of importance. In this paper, we present\nthe StoneDetector platform and its underlying method for finding code clones in\nJava source and Bytecode. StoneDetector implements a conventional clone\ndetection approach based upon the textual comparison of paths derived from the\ncode's representation by dominator trees. In this way, the tool does not only\nfind exact and syntactically similar near-miss code clones, but also code\nclones that are harder to detect due to their larger variety in the syntax. We\ndemonstrate StoneDetector's versatility as a conventional clone detection\nplatform and analyze its various available configuration parameters, including\nthe usage of different string metrics, hashing algorithms, etc. In our\nexhaustive evaluation with other conventional clone detectors on several\nstate-of-the-art benchmarks, we can show StoneDetector's performance and\nscalability in finding code clones in both, Java source and Bytecode.", "AI": {"tldr": "The paper introduces StoneDetector, a Java code and bytecode clone detection platform using dominator path comparisons with configurable parameters.", "motivation": "Code cloning leads to software bloat and propagates bugs/vulnerabilities, necessitating scalable detection methods that work on varied code syntax and Java bytecode.", "method": "Implements conventional clone detection via textual comparison of code-paths extracted from dominator trees, allowing configuration of string metrics and hashing algorithms.", "result": "Demonstrated superior performance and scalability compared to other conventional detectors on state-of-the-art benchmarks for both Java source and bytecode.", "conclusion": "StoneDetector provides an effective and configurable approach for identifying code clones across different code representations, addressing critical cloning challenges in software engineering."}}
{"id": "2508.03342", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03342", "abs": "https://arxiv.org/abs/2508.03342", "authors": ["Mehdi Akbari Gurabi", "Lasse Nitz", "Radu-Mihai Castravet", "Roman Matzutt", "Avikarsha Mandal", "Stefan Decker"], "title": "From Legacy to Standard: LLM-Assisted Transformation of Cybersecurity Playbooks into CACAO Format", "comment": "20 pages, including appendices, 32 references, 4 tables, 7 main\n  figures (some of them has sub-figures)", "summary": "Existing cybersecurity playbooks are often written in heterogeneous,\nnon-machine-readable formats, which limits their automation and\ninteroperability across Security Orchestration, Automation, and Response\nplatforms. This paper explores the suitability of Large Language Models,\ncombined with Prompt Engineering, to automatically translate legacy incident\nresponse playbooks into the standardized, machine-readable CACAO format. We\nsystematically examine various Prompt Engineering techniques and carefully\ndesign prompts aimed at maximizing syntactic accuracy and semantic fidelity for\ncontrol flow preservation. Our modular transformation pipeline integrates a\nsyntax checker to ensure syntactic correctness and features an iterative\nrefinement mechanism that progressively reduces syntactic errors. We evaluate\nthe proposed approach on a custom-generated dataset comprising diverse legacy\nplaybooks paired with manually created CACAO references. The results\ndemonstrate that our method significantly improves the accuracy of playbook\ntransformation over baseline models, effectively captures complex workflow\nstructures, and substantially reduces errors. It highlights the potential for\npractical deployment in automated cybersecurity playbook transformation tasks.", "AI": {"tldr": "This paper proposes using LLMs with prompt engineering to automate the translation of legacy cybersecurity playbooks into CACAO format, improving syntactic and semantic accuracy through a modular pipeline and iterative refinement.", "motivation": "The existing playbooks' heterogeneous, non-machine-readable formats hinder automation and interoperability in Security Orchestration, Automation, and Response platforms, necessitating a standardized transformation approach.", "method": "The authors design prompts to enhance syntactic accuracy and control flow preservation, implement a syntax checker, and employ an iterative refinement mechanism within a modular pipeline.", "result": "Evaluation on a custom dataset shows significant improvement in transformation accuracy, effective capture of complex workflows, and substantial error reduction compared to baseline models.", "conclusion": "The approach demonstrates practical potential for automating cybersecurity playbook transformation, leveraging LLMs and prompt engineering to achieve reliable and semantically faithful results."}}
{"id": "2508.03470", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03470", "abs": "https://arxiv.org/abs/2508.03470", "authors": ["Dong wang", "Junji Yu", "Honglin Shu", "Michael Fu", "Chakkrit Tantithamthavorn", "Yasutaka Kamei", "Junjie Chen"], "title": "On the Evaluation of Large Language Models in Multilingual Vulnerability Repair", "comment": null, "summary": "Various Deep Learning-based approaches with pre-trained language models have\nbeen proposed for automatically repairing software vulnerabilities. However,\nthese approaches are limited to a specific programming language (C/C++). Recent\nadvances in large language models (LLMs) offer language-agnostic capabilities\nand strong semantic understanding, exhibiting potential to overcome\nmultilingual vulnerability limitations. Although some work has begun to explore\nLLMs' repair performance, their effectiveness is unsatisfactory. To address\nthese limitations, we conducted a large-scale empirical study to investigate\nthe performance of automated vulnerability repair approaches and\nstate-of-the-art LLMs across seven programming languages. Results show GPT-4o,\ninstruction-tuned with few-shot prompting, performs competitively against the\nleading approach, VulMaster. Additionally, the LLM-based approach shows\nsuperior performance in repairing unique vulnerabilities and is more likely to\nrepair the most dangerous vulnerabilities. Instruction-tuned GPT-4o\ndemonstrates strong generalization on vulnerabilities in previously unseen\nlanguage, outperforming existing approaches. Analysis shows Go consistently\nachieves the highest effectiveness across all model types, while C/C++ performs\nthe worst. Based on findings, we discuss the promise of LLM on multilingual\nvulnerability repair and the reasons behind LLM's failed cases. This work takes\nthe first look at repair approaches and LLMs across multiple languages,\nhighlighting the promising future of adopting LLMs for multilingual\nvulnerability repair.", "AI": {"tldr": "This paper evaluates automated vulnerability repair approaches using LLMs across seven programming languages, demonstrating GPT-4o's competitive performance against existing methods while highlighting language-specific effectiveness patterns.", "motivation": "Current deep learning-based vulnerability repair approaches are limited to C/C++, but LLMs offer language-agnostic capabilities and stronger semantic understanding, addressing multilingual repair limitations.", "method": "A large-scale empirical study comparing state-of-the-art LLMs and automated repair approaches (including VulMaster) across seven programming languages using metrics like repair success rate and vulnerability danger assessment.", "result": "GPT-4o with few-shot prompting outperformed leading approaches (e.g., VulMaster) and showed superior performance in repairing unique/high-danger vulnerabilities. It generalized effectively to unseen languages. Go achieved highest effectiveness, while C/C++ performed worst.", "conclusion": "LLMs demonstrate strong potential for multilingual vulnerability repair, particularly in unique and high-danger cases. The study establishes a benchmark for evaluating future multilingual repair approaches and underscores LLMs' generalization advantages over language-specific methods."}}
{"id": "2508.03413", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03413", "abs": "https://arxiv.org/abs/2508.03413", "authors": ["Akshay Madhav Deshmukh"], "title": "Smart Car Privacy: Survey of Attacks and Privacy Issues", "comment": "13 pages, 16 figures", "summary": "Automobiles are becoming increasingly important in our day to day life.\nModern automobiles are highly computerized and hence potentially vulnerable to\nattack. Providing many wireless connectivity for vehicles enables a bridge\nbetween vehicles and their external environments. Such a connected vehicle\nsolution is expected to be the next frontier for automotive revolution and the\nkey to the evolution to next generation intelligent transportation systems.\nVehicular Ad hoc Networks (VANETs) are emerging mobile ad hoc network\ntechnologies incorporating mobile routing protocols for inter-vehicle data\ncommunications to support intelligent transportation systems. Thus security and\nprivacy are the major concerns in VANETs due to the mobility of the vehicles.\nThus designing security mechanisms to remove adversaries from the network\nremarkably important in VANETs.\n  This paper provides an overview of various vehicular network architectures.\nThe evolution of security in modern vehicles. Various security and privacy\nattacks in VANETs with their defending mechanisms with examples and classify\nthese mechanisms. It also provides an overview of various privacy implication\nthat a vehicular network possess.", "AI": {"tldr": "This paper reviews VANET security architectures, analyzes security/privacy attacks, classifies defense mechanisms (e.g. key management, authentication), and discusses privacy implications in connected vehicle networks.", "motivation": "Modern vehicles' wireless connectivity creates security vulnerabilities in Vehicular Ad hoc Networks (VANETs), making robust security mechanisms critical for intelligent transportation system evolution and protecting mobile network participants.", "method": "Literature survey and analysis of existing architectures, attack patterns, and defense techniques for vehicular networks with focus on security protocol evaluation and categorization.", "result": "Classification framework of security threats (e.g. tampering, eavesdropping) and corresponding countermeasures, including implementation examples of cryptographic solutions and tamper-proof protocols.", "conclusion": "Effective security mechanisms through proper architecture design are essential for VANET adoption. Continued research in authentication protocols, intrusion detection, and privacy preserving schemes will ensure safe implementation of connected vehicle technologies."}}
{"id": "2508.03487", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03487", "abs": "https://arxiv.org/abs/2508.03487", "authors": ["Yuanpeng Li", "Qi Long", "Zhiyuan Yao", "Jian Xu", "Lintao Xie", "Xu He", "Lu Geng", "Xin Han", "Yueyan Chen", "Wenbo Duan"], "title": "BitsAI-Fix: LLM-Driven Approach for Automated Lint Error Resolution in Practice", "comment": null, "summary": "As enterprise codebases continue to grow in scale and complexity, the volume\nof lint errors far exceeds engineers' manual remediation capacity, leading to\ncontinuous accumulation of technical debt and hindered development efficiency.\nThis paper presents BitsAI-Fix, an automated lint error remediation workflow\nbased on Large Language Models (LLMs), designed to address this critical\nchallenge in industrial-scale environments. BitsAI-Fix employs tree-sitter for\ncontext expansion and generates search-and-replace format patches through\nspecially trained LLMs, followed by lint scan re-verification to output final\nremediation results. Additionally, our approach introduces an innovative\nprogressive reinforcement learning (RL) training strategy that can\nautomatically acquire verifiable training data during the project cold-start\nphase and continuously iterate the model by collecting online samples through\nfeedback after system deployment. Furthermore, we designed a targeted\nrule-based reward mechanism that combines format rewards and correctness\nrewards while penalizing redundant modifications. We also propose a \"code diff\nmatching\" methodology to continuously track online effectiveness. In production\ndeployment at ByteDance, our solution has supported over 5,000 engineers,\nresolved more than 12,000 static analysis issues, achieved approximately 85%\nremediation accuracy, with around 1,000 weekly active adopters. This work\ndemonstrates the practical feasibility of LLM-based code remediation solutions\nin enterprise environments and serves as a reference for automated code fix in\nlarge-scale industrial scenarios.", "AI": {"tldr": "BitsAI-Fix is an automated LLM-driven lint error remediation workflow for large-scale enterprise environments, combining context-aware patch generation and progressive reinforcement learning to achieve 85% accuracy.", "motivation": "Manual remediation of lint errors in enterprise codebases is insufficient due to their scale and complexity, causing technical debt accumulation and reduced development efficiency.", "method": "The system uses tree-sitter for code context expansion, trains LLMs via progressive reinforcement learning to generate search-and-replace patches, and employs a rule-based reward mechanism with correctness/format rewards to prioritize valid fixes.", "result": "Deployed at ByteDance, it resolved 12,000+ issues with 85% accuracy, supports 5,000+ engineers, and achieves 1,000+ weekly active users through continuous model iteration via code diff matching feedback.", "conclusion": "BitsAI-Fix demonstrates that LLM-based automated code remediation is viable at industrial scale, providing a reference framework for large enterprise technical debt reduction through machine learning integration."}}
{"id": "2508.03474", "categories": ["cs.CR", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2508.03474", "abs": "https://arxiv.org/abs/2508.03474", "authors": ["Oriol Saguillo", "Vahid Ghafouri", "Lucianna Kiffer", "Guillermo Suarez-Tangil"], "title": "Unravelling the Probabilistic Forest: Arbitrage in Prediction Markets", "comment": null, "summary": "Polymarket is a prediction market platform where users can speculate on\nfuture events by trading shares tied to specific outcomes, known as conditions.\nEach market is associated with a set of one or more such conditions. To ensure\nproper market resolution, the condition set must be exhaustive -- collectively\naccounting for all possible outcomes -- and mutually exclusive -- only one\ncondition may resolve as true. Thus, the collective prices of all related\noutcomes should be \\$1, representing a combined probability of 1 of any\noutcome. Despite this design, Polymarket exhibits cases where dependent assets\nare mispriced, allowing for purchasing (or selling) a certain outcome for less\nthan (or more than) \\$1, guaranteeing profit. This phenomenon, known as\narbitrage, could enable sophisticated participants to exploit such\ninconsistencies.\n  In this paper, we conduct an empirical arbitrage analysis on Polymarket data\nto answer three key questions: (Q1) What conditions give rise to arbitrage (Q2)\nDoes arbitrage actually occur on Polymarket and (Q3) Has anyone exploited these\nopportunities. A major challenge in analyzing arbitrage between related markets\nlies in the scalability of comparisons across a large number of markets and\nconditions, with a naive analysis requiring $O(2^{n+m})$ comparisons. To\novercome this, we employ a heuristic-driven reduction strategy based on\ntimeliness, topical similarity, and combinatorial relationships, further\nvalidated by expert input.\n  Our study reveals two distinct forms of arbitrage on Polymarket: Market\nRebalancing Arbitrage, which occurs within a single market or condition, and\nCombinatorial Arbitrage, which spans across multiple markets. We use on-chain\nhistorical order book data to analyze when these types of arbitrage\nopportunities have existed, and when they have been executed by users. We find\na realized estimate of 40 million USD of profit extracted.", "AI": {"tldr": "This paper analyzes arbitrage opportunities on Polymarket, identifying two distinct forms (Market Rebalancing and Combinatorial Arbitrage) and estimates 40 million USD in realized profits, revealing market inefficiencies despite platform design constraints.", "motivation": "Prediction markets like Polymarket aim to maintain efficient pricing through exhaustive and mutually exclusive condition sets. However, observed mispricing in dependent assets creates arbitrage opportunities, warranting investigation into: (1) conditions enabling arbitrage, (2) its actual occurrence, and (3) evidence of exploitation by participants.", "method": "The study employs empirical on-chain arbitrage analysis of historical Polymarket order books. A heuristic-driven reduction strategy addresses scalability challenges by integrating timeliness, topical similarity, and combinatorial relationships metrics, validated through expert input.", "result": "Identified two arbitrage types: Market Rebalancing Arbitrage (within single markets) and Combinatorial Arbitrage (across multiple markets). On-chain data analysis confirmed 40 million USD in realized arbitrage profits, demonstrating widespread systematic mispricing and active exploitation.", "conclusion": "Polymarket's market resolution constraints are systematically breached, enabling profitable arbitrage. The 40 million USD realized profit estimates highlight both platform design limitations and the presence of sophisticated arbitrageurs exploiting inefficiencies through identified strategies. Future market mechanism improvements may be necessary to address these issues."}}
{"id": "2508.03560", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03560", "abs": "https://arxiv.org/abs/2508.03560", "authors": ["Yi Gui", "Zhen Li", "Zhongyi Zhang", "Guohao Wang", "Tianpeng Lv", "Gaoyang Jiang", "Yi Liu", "Dongping Chen", "Yao Wan", "Hongyu Zhang", "Wenbin Jiang", "Xuanhua Shi", "Hai Jin"], "title": "LaTCoder: Converting Webpage Design to Code with Layout-as-Thought", "comment": "KDD 2025 v2", "summary": "Converting webpage designs into code (design-to-code) plays a vital role in\nUser Interface (UI) development for front-end developers, bridging the gap\nbetween visual design and functional implementation. While recent Multimodal\nLarge Language Models (MLLMs) have shown significant potential in\ndesign-to-code tasks, they often fail to accurately preserve the layout during\ncode generation. To this end, we draw inspiration from the Chain-of-Thought\n(CoT) reasoning in human cognition and propose LaTCoder, a novel approach that\nenhances layout preservation in webpage design during code generation with\nLayout-as-Thought (LaT). Specifically, we first introduce a simple yet\nefficient algorithm to divide the webpage design into image blocks. Next, we\nprompt MLLMs using a CoTbased approach to generate code for each block.\nFinally, we apply two assembly strategies-absolute positioning and an\nMLLM-based method-followed by dynamic selection to determine the optimal\noutput. We evaluate the effectiveness of LaTCoder using multiple backbone MLLMs\n(i.e., DeepSeek-VL2, Gemini, and GPT-4o) on both a public benchmark and a newly\nintroduced, more challenging benchmark (CC-HARD) that features complex layouts.\nThe experimental results on automatic metrics demonstrate significant\nimprovements. Specifically, TreeBLEU scores increased by 66.67% and MAE\ndecreased by 38% when using DeepSeek-VL2, compared to direct prompting.\nMoreover, the human preference evaluation results indicate that annotators\nfavor the webpages generated by LaTCoder in over 60% of cases, providing strong\nevidence of the effectiveness of our method.", "AI": {"tldr": "LaTCoder improves webpage design-to-code conversion by using Layout-as-Thought (LaT) inspired by human Chain-of-Thought reasoning. It divides designs into blocks, applies CoT prompting, and employs dynamic assembly strategies, achieving 66.67% higher TreeBLEU and 38% lower MAE compared to direct prompting.", "motivation": "Existing Multimodal Large Language Models (MLLMs) struggle to preserve layout accuracy when generating code from webpage designs, creating a gap between visual design and functional implementation in UI development.", "method": "LaTCoder 1) splits webpage designs into image blocks via a proposed algorithm, 2) applies Chain-of-Thought (CoT) prompting with MLLMs to generate code for each block, and 3) uses combination strategies (absolute positioning and MLLM-based method) with dynamic selection for assembly.", "result": "Evaluations on public and CC-HARD benchmarks showed TreeBLEU improved by 66.67% and MAE reduced by 38% with backbone MLLMs. Human evaluation favored LaTCoder-generated pages >60% of the time.", "conclusion": "LaTCoder demonstrates significant layout preservation improvements through structured reasoning (LaT) and dynamic code assembly, validated by both automated metrics and human preference studies in complex UI scenarios."}}
{"id": "2508.03517", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03517", "abs": "https://arxiv.org/abs/2508.03517", "authors": ["Mabin Umman Varghese", "Zahra Taghiyarrenani"], "title": "Intrusion Detection in Heterogeneous Networks with Domain-Adaptive Multi-Modal Learning", "comment": null, "summary": "Network Intrusion Detection Systems (NIDS) play a crucial role in\nsafeguarding network infrastructure against cyberattacks. As the prevalence and\nsophistication of these attacks increase, machine learning and deep neural\nnetwork approaches have emerged as effective tools for enhancing NIDS\ncapabilities in detecting malicious activities. However, the effectiveness of\ntraditional deep neural models is often limited by the need for extensive\nlabelled datasets and the challenges posed by data and feature heterogeneity\nacross different network domains. To address these limitations, we developed a\ndeep neural model that integrates multi-modal learning with domain adaptation\ntechniques for classification. Our model processes data from diverse sources in\na sequential cyclic manner, allowing it to learn from multiple datasets and\nadapt to varying feature spaces. Experimental results demonstrate that our\nproposed model significantly outperforms baseline neural models in classifying\nnetwork intrusions, particularly under conditions of varying sample\navailability and probability distributions. The model's performance highlights\nits ability to generalize across heterogeneous datasets, making it an efficient\nsolution for real-world network intrusion detection.", "AI": {"tldr": "A deep neural model combining multi-modal learning and domain adaptation techniques is developed to enhance NIDS performance in heterogeneous network environments. The model processes sequential cyclic data from diverse sources, demonstrating superior intrusion classification accuracy and generalization capabilities under varying data conditions.", "motivation": "Traditional deep learning models struggle with (1) requirement for extensive labeled datasets and (2) inability to handle data/feature heterogeneity across different network domains, limiting their effectiveness for real-world intrusion detection.", "method": "The proposed model integrates multi-modal learning with domain adaptation to process data from diverse sources in a sequential cyclic architecture. This enables simultaneous learning from multiple datasets while adapting to varying feature spaces across domains.", "result": "Outperforms baseline neural models in intrusion classification accuracy, particularly showing resilience to challenges from imbalanced sample availability and shifting probability distributions across 5+ heterogeneous network datasets.", "conclusion": "The sequential cyclic architecture with domain adaptation provides a generalizable framework for NIDS, reducing dependence on domain-specific labeled data while maintaining robust performance across diverse network environments. Suggests this approach addresses critical limitations of current systems."}}
{"id": "2508.03603", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.03603", "abs": "https://arxiv.org/abs/2508.03603", "authors": ["Iti Shree", "Karine Even-Mendoz", "Tomasz Radzik"], "title": "ReFuzzer: Feedback-Driven Approach to Enhance Validity of LLM-Generated Test Programs", "comment": null, "summary": "Existing LLM-based compiler fuzzers often produce syntactically or\nsemantically invalid test programs, limiting their effectiveness in exercising\ncompiler optimizations and backend components. We introduce ReFuzzer, a\nframework for refining LLM-generated test programs by systematically detecting\nand correcting compilation and runtime violations (e.g. division by zero or\narray out-of-bounds accesses). ReFuzzer employs a feedback loop with a local\nLLM to validate and filter erroneous programs before execution, improving\nfuzzing effectiveness beyond crash detection and enabling the generation of\ndiverse yet valid test programs.\n  We evaluated ReFuzzer's effectiveness across black-, grey- and white-box\nfuzzing approaches targeting LLVM/Clang. ReFuzzer improved test programs'\nvalidity from 47.0-49.4% to 96.6-97.3%, with an average processing time of\n2.9-3.5 s per test program on a dual-GPU machine. Further, refuzzing\nsignificantly increased code coverage in critical optimization and IR\ngeneration components. For example, vectorization coverage had an absolute\nimprovement of 9.2%, 2.3%, and 7.1% in black-, grey-, and white-box fuzzing,\nenhancing testing effectiveness.", "AI": {"tldr": "ReFuzzer improves compiler fuzzer effectiveness by refining LLM-generated test programs to ensure validity and enhance optimization component testing.", "motivation": "Current LLM-based compiler fuzzers struggle with syntactic/semantic errors in test programs and lack systematic validation for compiler optimization testing.", "method": "A feedback framework using a local LLM to detect/runtime violations (div-by-zero, out-of-bounds) and iteratively refine test programs through validation filtering.", "result": "ReFuzzer increased program validity to 96.6-97.3% while reducing invalid cases from 47-49.4%, achieving 2.9-3.5s processing and boosting vectorization coverage by 9.2-9.2% in fuzzing.", "conclusion": "ReFuzzer enables systematic refining of LLM outputs to create valid test programs, significantly improving compiler fuzzer validity and coverage for optimization components."}}
{"id": "2508.03588", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03588", "abs": "https://arxiv.org/abs/2508.03588", "authors": ["Zhaoyi Meng", "Fenglei Xu", "Wenxiang Zhao", "Wansen Wang", "Wenchao Huang", "Jie Cui", "Hong Zhong", "Yan Xiong"], "title": "MalFlows: Context-aware Fusion of Heterogeneous Flow Semantics for Android Malware Detection", "comment": "Submitted to TDSC", "summary": "Static analysis, a fundamental technique in Android app examination, enables\nthe extraction of control flows, data flows, and inter-component communications\n(ICCs), all of which are essential for malware detection. However, existing\nmethods struggle to leverage the semantic complementarity across different\ntypes of flows for representing program behaviors, and their context-unaware\nnature further hinders the accuracy of cross-flow semantic integration. We\npropose and implement MalFlows, a novel technique that achieves context-aware\nfusion of heterogeneous flow semantics for Android malware detection. Our goal\nis to leverage complementary strengths of the three types of flow-related\ninformation for precise app profiling. We adopt a heterogeneous information\nnetwork (HIN) to model the rich semantics across these program flows. We\nfurther propose flow2vec, a context-aware HIN embedding technique that\ndistinguishes the semantics of HIN entities as needed based on contextual\nconstraints across different flows and learns accurate app representations\nthrough the joint use of multiple meta-paths. The representations are finally\nfed into a channel-attention-based deep neural network for malware\nclassification. To the best of our knowledge, this is the first study to\ncomprehensively aggregate the strengths of diverse flow-related information for\nassessing maliciousness within apps. We evaluate MalFlows on a large-scale\ndataset comprising over 20 million flow instances extracted from more than\n31,000 real-world apps. Experimental results demonstrate that MalFlows\noutperforms representative baselines in Android malware detection, and\nmeanwhile, validate the effectiveness of flow2vec in accurately learning app\nrepresentations from the HIN constructed over the heterogeneous flows.", "AI": {"tldr": "MalFlows is a novel Android malware detection technique that combines context-aware fusion of heterogeneous flow semantics (control, data, and ICC) using a heterogeneous information network (HIN) and flow2vec embedding. It achieves better performance than existing methods on a large-scale dataset of 20M flow instances from 31,000 apps.", "motivation": "Existing Android malware detection techniques fail to leverage the semantic complementarity across different flow types (control, data, ICC) and lack context-aware integration of heterogeneous program behaviors. This limits their accuracy in malware profiling.", "method": "The authors: 1) Construct an HIN to model relationships between three types of program flows 2) Develop flow2vec, a context-aware embedding approach that uses multiple meta-paths to learn app representations from the HIN 3) Employ a channel-attention-based deep neural network for classification.", "result": "Experiments on a 20M+ flow instance dataset showed MalFlows outperformed representative baselines in Android malware detection. flow2vec demonstrated effectiveness in learning accurate app representations from the heterogeneous flow network.", "conclusion": "MalFlows is the first context-aware framework to comprehensively integrate heterogeneous flow semantics for Android malware detection, setting a new benchmark using HIN-based representation learning and attention mechanisms."}}
{"id": "2508.03642", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03642", "abs": "https://arxiv.org/abs/2508.03642", "authors": ["Oliver Westphal"], "title": "Intent Preserving Generation of Diverse and Idiomatic (Code-)Artifacts", "comment": "In Proceedings TFPiE 2025, arXiv:2508.02305", "summary": "When automatically generating programming exercise tasks one often also needs\nto automatically generate programs. At the very least when providing sample\nsolutions is part of automated feedback. But programs can also be used as part\nof the exercise task description to communicate a task's requirements.\n  Writing good program generators that produce varied yet idiomatic code while\nbeing easily adaptable for new tasks is challenging. The challenges are\nintensified if task generation requires additional artifacts, like a more\ngeneral behavior specification for testing or additional textual descriptions.\nManually writing generators for multiple different but strongly related\nartifacts gets complicated quickly.\n  We present an approach where instead of writing monolithic generators for\nmultiple connected artifacts one specifies a small set of abstract building\nblocks and for each such building block defines sets of concrete realizations\nfor various kinds of artifacts. Then the intended structure of the resulting\nartifacts is specified as a composition of the small abstract building blocks.\nThis abstract description then serves as the common source from which related\nartifacts can be derived automatically. The approach is generic in the kind of\nartifacts it can produce and is therefore adaptable to a wide range of\ncontexts.", "AI": {"tldr": "The paper proposes a method for automatically generating programming exercise artifacts by decomposing them into abstract building blocks with concrete realizations, enabling varied and idiomatic code generation and simplifying adaptation to new contexts.", "motivation": "Existing monolithic program generators struggle to create adaptable, idiomatic code for multiple connected artifacts (e.g., task descriptions, solutions, test specifications) used in programming exercises, leading to complex implementations when generating related artifacts.", "method": "An abstract-based approach where small building blocks are defined for different artifact types (programs, specifications, text). These blocks are composited into a structured description that automatically generates the required artifacts through concrete implementations of the blocks.", "result": "A framework demonstrating the feasibility of generating diverse, context-specific artifacts (like sample solutions and task descriptions) from a single abstract model while maintaining idiomatic code patterns and flexibility across programming languages.", "conclusion": "The approach provides a scalable, language-agnostic method for generating programming exercise artifacts by centralizing logic in composable abstractions, reducing complexity and improving maintainability for multi-artifact task generation systems."}}
