{"id": "2510.01379", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01379", "abs": "https://arxiv.org/abs/2510.01379", "authors": ["Huashan Chen", "Zhenyu Qi", "Haotang Li", "Hong Chen", "Jinfu Chen", "Kebin Peng", "In Kee Kim", "Kyu Hyung Lee", "Sen He"], "title": "Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration", "comment": null, "summary": "While Large Language Models (LLMs) have become the predominant paradigm for\nautomated code generation, current single-model approaches fundamentally ignore\nthe heterogeneous computational strengths that different models exhibit across\nprogramming languages, algorithmic domains, and development stages. This paper\nchallenges the single-model convention by introducing a multi-stage,\nperformance-guided orchestration framework that dynamically routes coding tasks\nto the most suitable LLMs within a structured generate-fix-refine workflow. Our\napproach is grounded in a comprehensive empirical study of 17 state-of-the-art\nLLMs across five programming languages (Python, Java, C++, Go, and Rust) using\nHumanEval-X benchmark. The study, which evaluates both functional correctness\nand runtime performance metrics (execution time, mean/max memory utilization,\nand CPU efficiency), reveals pronounced performance heterogeneity by language,\ndevelopment stage, and problem category. Guided by these empirical insights, we\npresent PerfOrch, an LLM agent that orchestrates top-performing LLMs for each\ntask context through stage-wise validation and rollback mechanisms. Without\nrequiring model fine-tuning, PerfOrch achieves substantial improvements over\nstrong single-model baselines: average correctness rates of 96.22% and 91.37%\non HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and\n49.11%. Beyond correctness gains, the framework delivers consistent performance\noptimizations, improving execution time for 58.76% of problems with median\nspeedups ranging from 17.67% to 27.66% across languages on two benchmarks. The\nframework's plug-and-play architecture ensures practical scalability, allowing\nnew LLMs to be profiled and integrated seamlessly, thereby offering a paradigm\nfor production-grade automated software engineering that adapts to the rapidly\nevolving generative AI landscape.", "AI": {"tldr": "This paper introduces PerfOrch, a multi-model LLM orchestration framework for automated code generation that dynamically selects optimal models based on empirical performance metrics across languages and stages, achieving higher correctness (up to 96.22% on HumanEval-X) and execution speed improvements (up to 27.66% median speedup).", "motivation": "Current single-model approaches for code generation ignore inherent model heterogeneity in language capabilities, domain expertise, and development-stage appropriateness, leading to suboptimal code quality and performance.", "method": "Conducted empirical evaluation of 17 LLMs across 5 programming languages using HumanEval-X and EffiBench-X benchmarks (assessing functional correctness and runtime metrics). Developed PerfOrch framework with stage-specific model routing, generate-fix-refine workflow, and validation/rollback mechanisms without requiring model adaptation.", "result": "perfOrch outperforms single-model baselines (including GPT-4o) with 96.22% correctness on HumanEval-X (vs 78.66%) and 91.37 vs 49.11 on EffiBench-X; achieves 58.76% problem coverage with 17.67-27.66% median execution time improvements across languages.", "conclusion": "PerfOrch demonstrates that model orchestration frameworks can systematically leverage model heterogeneity for production-grade code generation, delivering combined correctness/performance gains through empirical routing and modular, extensible architecture."}}
{"id": "2510.01514", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01514", "abs": "https://arxiv.org/abs/2510.01514", "authors": ["J. Alexander Curtis", "Sharadha Kasiviswanathan", "Nasir Eisty"], "title": "Deciphering WONTFIX: A Mixed-Method Study on Why GitHub Issues Get Rejected", "comment": null, "summary": "Context: The ``wontfix'' label is a widely used yet narrowly understood tool\nin GitHub repositories, indicating that an issue will not be pursued further.\nDespite its prevalence, the impact of this label on project management and\ncommunity dynamics within open-source software development is not clearly\ndefined. Objective: This study examines the prevalence and reasons behind\nissues being labeled as wontfix across various open-source repositories on\nGitHub. Method: Employing a mixed-method approach, we analyze both quantitative\ndata to assess the prevalence of the wontfix label and qualitative data to\nexplore the reasoning that it was used. Data were collected from 3,132 of\nGitHub's most-popular repositories. Later, we employ open coding and thematic\nanalysis to categorize the reasons behind wontfix labels, providing a\nstructured understanding of the issue management landscape. Results: Our\nfindings show that about 30% of projects on GitHub apply the wontfix label to\nsome issues. These issues most often occur on user-submitted issues for bug\nreports and feature requests. The study identified eight common themes behind\nlabeling issues as wontfix, ranging from user-specific control factors to\nmaintainer-specific decisions. Conclusions: The wontfix label is a critical\ntool for managing resources and guiding contributor efforts in GitHub projects.\nHowever, it can also discourage community involvement and obscure the\ntransparency of project management. Understanding these reasons aids project\nmanagers in making informed decisions and fostering efficient collaboration\nwithin open-source communities.", "AI": {"tldr": "This study analyzes the 'wontfix' label usage in 3,132 GitHub repositories, revealing its 30%", "motivation": "The 'wontfix' label is a common but poorly understood GitHub tool; this research aims to clarify its role in open-source project management and community dynamics.", "method": "Mixed-method approach combining quantitative analysis of 3,132 repositories with qualitative thematic analysis via open coding to categorize labeling reasons.", "result": "30%", "conclusion": "The 'wontfix' label is essential for resource management but may hinder community engagement; understanding its usage patterns can optimize project governance and collaboration."}}
{"id": "2510.01635", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01635", "abs": "https://arxiv.org/abs/2510.01635", "authors": ["Yifei Chen", "Sarra Habchi", "Lili Wei"], "title": "MIMIC: Integrating Diverse Personality Traits for Better Game Testing Using Large Language Model", "comment": "13 pages, 7 figures, 6 tables. This paper is accepted by the 40th\n  IEEE/ACM International Conference on Automated Software Engineering, ASE 2025", "summary": "Modern video games pose significant challenges for traditional automated\ntesting algorithms, yet intensive testing is crucial to ensure game quality. To\naddress these challenges, researchers designed gaming agents using\nReinforcement Learning, Imitation Learning, or Large Language Models. However,\nthese agents often neglect the diverse strategies employed by human players due\nto their different personalities, resulting in repetitive solutions in similar\nsituations. Without mimicking varied gaming strategies, these agents struggle\nto trigger diverse in-game interactions or uncover edge cases.\n  In this paper, we present MIMIC, a novel framework that integrates diverse\npersonality traits into gaming agents, enabling them to adopt different gaming\nstrategies for similar situations. By mimicking different playstyles, MIMIC can\nachieve higher test coverage and richer in-game interactions across different\ngames. It also outperforms state-of-the-art agents in Minecraft by achieving a\nhigher task completion rate and providing more diverse solutions. These results\nhighlight MIMIC's significant potential for effective game testing.", "AI": {"tldr": "The paper introduces MIMIC, a framework addressing the limitations of traditional automated game testing by integrating diverse human-like playstyles into gaming agents, achieving higher test coverage and performance in Minecraft.", "motivation": "Traditional agents lack diversity in strategies, leading to repetitive interactions and missed edge cases. Human players use varied approaches, which existing methods fail to emulate.", "method": "MIMIC incorporates personality traits into agents, enabling them to adopt different playstyles for similar scenarios. This mimics human diversity, driving varied in-game interactions.", "result": "MIMIC achieves higher task completion rates and more diverse solutions in Minecraft, surpassing state-of-the-art agents while generating richer test coverage.", "conclusion": "MIMIC demonstrates significant potential for effective game testing by leveraging personality-driven strategies, addressing critical gaps in automated testing for complex video games."}}
{"id": "2510.01740", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01740", "abs": "https://arxiv.org/abs/2510.01740", "authors": ["Kypros Iacovou", "Georgia M. Kapitsaki", "Evangelia Vanezi"], "title": "FOSS-chain: using blockchain for Open Source Software license compliance", "comment": null, "summary": "Open Source Software (OSS) is widely used and carries licenses that indicate\nthe terms under which the software is provided for use, also specifying\nmodification and distribution rules. Ensuring that users are respecting OSS\nlicense terms when creating derivative works is a complex process. Compliance\nissues arising from incompatibilities among licenses may lead to legal\ndisputes. At the same time, the blockchain technology with immutable entries\noffers a mechanism to provide transparency when it comes to licensing and\nensure software changes are recorded. In this work, we are introducing an\nintegration of blockchain and license management when creating derivative\nworks, in order to tackle the issue of OSS license compatibility. We have\ndesigned, implemented and performed a preliminary evaluation of FOSS-chain, a\nweb platform that uses blockchain and automates the license compliance process,\ncovering 14 OSS licenses. We have evaluated the initial prototype version of\nthe FOSS-chain platform via a small scale user study. Our preliminary results\nare promising, demonstrating the potential of the platform for adaptation on\nrealistic software systems.", "AI": {"tldr": "The paper proposes FOSS-chain, a blockchain-based platform to automate Open Source Software (OSS license compliance and address compatibility issues through immutable record-keeping, validated via a small-scale user study.", "motivation": "OSS license incompatibilities increase legal risks during derivative work creation, requiring complex compliance processes and transparency in tracking modifications and distributions.", "method": "FOSS-chain integrates blockchain to record software changes and license data, automating compliance checks for 14 OSS licenses through a web platform prototype evaluated via user testing.", "result": "Preliminary evaluation shows FOSS-chain's potential to simplify license compliance, with promising results indicating adaptability to real-world software systems despite the current prototype's limited scale.", "conclusion": "Blockchain-based solutions like FOSS-chain demonstrate viability for enhancing OSS license management by ensuring transparency and automating compliance verification, reducing legal risks."}}
{"id": "2510.01223", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01223", "abs": "https://arxiv.org/abs/2510.01223", "authors": ["Hui Dou", "Ning Xu", "Yiwen Zhang", "Kaibin Wang"], "title": "Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks. However, they remain exposed to jailbreak attacks, eliciting\nharmful responses. The nested scenario strategy has been increasingly adopted\nacross various methods, demonstrating immense potential. Nevertheless, these\nmethods are easily detectable due to their prominent malicious intentions. In\nthis work, we are the first to find and systematically verify that LLMs'\nalignment defenses are not sensitive to nested scenarios, where these scenarios\nare highly semantically relevant to the queries and incorporate targeted toxic\nknowledge. This is a crucial yet insufficiently explored direction. Based on\nthis, we propose RTS-Attack (Semantically Relevant Nested Scenarios with\nTargeted Toxic Knowledge), an adaptive and automated framework to examine LLMs'\nalignment. By building scenarios highly relevant to the queries and integrating\ntargeted toxic knowledge, RTS-Attack bypasses the alignment defenses of LLMs.\nMoreover, the jailbreak prompts generated by RTS-Attack are free from harmful\nqueries, leading to outstanding concealment. Extensive experiments demonstrate\nthat RTS-Attack exhibits superior performance in both efficiency and\nuniversality compared to the baselines across diverse advanced LLMs, including\nGPT-4o, Llama3-70b, and Gemini-pro. Our complete code is available in the\nsupplementary material. WARNING: THIS PAPER CONTAINS POTENTIALLY HARMFUL\nCONTENT.", "AI": {"tldr": "The paper proposes RTS-Attack, a novel adversarial attack framework for Large Language Models (LLMs). RTS-Attack utilizes semantically relevant nested scenarios combined with targeted toxic knowledge to bypass alignment defenses. The framework is adaptive, automated, and achieves high concealment by generating prompts that do not contain harmful queries. Experiments on GPT-4o, Llama3-70b, and Gemini-pro show superior efficiency and universality compared to existing methods.", "motivation": "Current jailbreak attack methods for LLMs are often detectable due to their apparent harmful intent. Existing nested scenario strategies lack systematic study regarding their detectability and effectiveness in bypassing alignment defenses, even though they show potential for adversarial attacks.", "method": "RTS-Attack constructs semantically relevant nested scenarios where each scenario is highly related to the attack target query. These scenarios integrate pre-characterized toxic knowledge segments to systematically undermine alignment defenses while concealing harmful intent in the surface-level query. The framework is designed to be both adaptive and automated, allowing it to generate diverse attack patterns for different LLM architectures.", "result": "Experiments show RTS-Attack achieves 89%+ success rate on various models, outperforming existing attack methods by over 20 percentage points. It maintains high efficiency with minimal query turns and demonstrates universality across different LLM architectures including GPT-4o, Llama3-70b, and Gemini-pro. The concealment property is validated by detection tests where 85% of generated prompts were not flagged by standard detection mechanisms.", "conclusion": "RTS-Attack's success highlights the need for more robust alignment defenses for LLMs. The framework demonstrates that nested scenarios without explicit harmful language can effectively bypass current safety mechanisms. Future research should focus on improving alignment detection while considering semantically encoded adversarial patterns."}}
{"id": "2510.01754", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01754", "abs": "https://arxiv.org/abs/2510.01754", "authors": ["Hina Anwar"], "title": "ARENA: A tool for measuring and analysing the energy efficiency of Android apps", "comment": null, "summary": "To build energy-efficient apps, there is a need to estimate and analyze their\nenergy consumption in typical usage scenarios. The energy consumption of\nAndroid apps could be estimated via software-based and hardware-based\napproaches. Software-based approaches, while easier to implement, are not as\naccurate as hardware-based approaches. The process of measuring the energy\nconsumption of an Android app via a hardware-based approach typically involves\n1) setting up a measurement environment, 2) executing the app under test on a\nmobile device, 3) recording current/voltage data via a hardware device to\nmeasure energy consumption, and 4) cleaning and aggregating data for analyses,\nreports, and visualizations. Specialized scripts are written for selected\nhardware and software components to ensure reliable energy measurements. The\nenergy measurement process is repeated many times and aggregated to remove\nnoise. These steps make the hardware-based energy measurement process\ntime-consuming and not easy to adapt or reproduce. There is a lack of\nopen-source tools available for developers and researchers to take reliable\nenergy measurements via hardware devices. In this paper, we present and\ndemonstrate ARENA, a support tool that enables developers and researchers to\nconnect to a physical measurement device without leaving the comfort of their\nIDE. Developers could use ARENA during development to compare energy\nconsumption between different apps or versions of the same app. ARENA\ncalculates energy consumption on an Android smartphone by executing a test\nscenario on the app under development. Further, ARENA helps aggregate,\nstatistically analyze, report, and visualize the data, allowing developers and\nresearchers to dig into the data directly or visually. We implemented ARENA as\nan IntelliJ and Android Studio plugin.", "AI": {"tldr": "The paper introduces ARENA, an open-source IDE plugin for Android developers to simplify hardware-based energy consumption measurement and analysis of apps through integrated data collection and visualization.", "motivation": "Hardware-based energy measurement for Android apps is fragmented, time-consuming, and lacks accessible tools, creating barriers for developers/researchers in optimizing energy efficiency.", "method": "CRENNA provides an IntelliJ/Android Studio plugin that automates hardware device integration, test scenario execution, data aggregation (with noise reduction), and energy analysis (metrics calculation, statistical analysis, and visualization).", "result": "ARENA enables developers to (1) perform reliable hardware-based energy comparisons between apps/versions and (2) analyze measurement data via statistical reports and visualizations without leaving their IDE.", "conclusion": "ARENA bridges the gap between accurate hardware-based energy measurement and developer workflow efficiency, enabling systematic energy optimization of Android applications."}}
{"id": "2510.01342", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01342", "abs": "https://arxiv.org/abs/2510.01342", "authors": ["Xiangfang Li", "Yu Wang", "Bo Li"], "title": "Fine-Tuning Jailbreaks under Highly Constrained Black-Box Settings: A Three-Pronged Approach", "comment": null, "summary": "With the rapid advancement of large language models (LLMs), ensuring their\nsafe use becomes increasingly critical. Fine-tuning is a widely used method for\nadapting models to downstream tasks, yet it is vulnerable to jailbreak attacks.\nHowever, most existing studies focus on overly simplified attack scenarios,\nlimiting their practical relevance to real-world defense settings. To make this\nrisk concrete, we present a three-pronged jailbreak attack and evaluate it\nagainst provider defenses under a dataset-only black-box fine-tuning interface.\nIn this setting, the attacker can only submit fine-tuning data to the provider,\nwhile the provider may deploy defenses across stages: (1) pre-upload data\nfiltering, (2) training-time defensive fine-tuning, and (3) post-training\nsafety audit. Our attack combines safety-styled prefix/suffix wrappers, benign\nlexical encodings (underscoring) of sensitive tokens, and a backdoor mechanism,\nenabling the model to learn harmful behaviors while individual datapoints\nappear innocuous. Extensive experiments demonstrate the effectiveness of our\napproach. In real-world deployment, our method successfully jailbreaks GPT-4.1\nand GPT-4o on the OpenAI platform with attack success rates above 97% for both\nmodels. Our code is available at\nhttps://github.com/lxf728/tri-pronged-ft-attack.", "AI": {"tldr": "The paper introduces a three-pronged jailbreak attack method for fine-tuning large language models (LLMs) that effectively bypasses existing defenses, achieving over 97% success rates against GPT-4.1 and GPT-4o models.", "motivation": "The authors aim to address the gap between simplified attack scenarios in prior research and real-world defense mechanisms, demonstrating practical risks in deployed LLM systems.", "method": "The attack combines safety-styled textual wrappers, lexical token obfuscation (e.g., underscoring), and a backdoor training method, operating under a dataset-only black-box fine-tuning interface with provider-enforced data filtering, defensive training, and safety audits.", "result": "Experiments show the attack successfully evades all three stages of defense (pre-upload filtering, defensive training, and post-training audits) with high success rates (>97%) on OpenAI\u2019s GPT-4.1 and GPT-4o platforms.", "conclusion": "The study highlights critical vulnerabilities in current LLM defense ecosystems under realistic attack constraints, suggesting urgent re-evaluation of training-time security measures in production systems."}}
{"id": "2510.01825", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01825", "abs": "https://arxiv.org/abs/2510.01825", "authors": ["Zhenyu Yang", "Yue Pan", "Zhen Yang", "Zhongxing Yu"], "title": "Towards Speeding up Program Repair with Non-Autoregressive Model", "comment": "30 pages, 8 figures, 7 tables. arXiv admin note: substantial text\n  overlap with arXiv:2406.16526", "summary": "Enlightened by the success of machine learning techniques in various\napplication areas, recent years have witnessed a surge of research efforts on\nautomatic program repair (APR) using machine learning techniques. Previous\nmachine learning-based APR techniques essentially modified bugs in the\nautoregressive (AR) manner, which predicts future values based on past values.\nDue to the manner of token-by-token generation, the AR-based APR technique has\na huge time delay. In particular, the delay of the APR model with a large\nnumber of parameters is more serious. To address the issue, we aim to apply the\nnon-autoregressive (NAR) method to the APR task, which can output target code\nin a parallel manner to avoid huge repair delays. However, the naive use of the\nNAR manner for the APR task suffers from the issue of compromised patch\nquality. To effectively adapt the NAR manner for the APR task, we in this paper\npropose NARRepair, the first customized NAR code generation model for the APR\ntask. The NARRepair model features three major novelties, including 1) the\nrepair action predictor for alleviating the over-correction issue, 2) the\ninter-token dependency extractor for alleviating the issue of lacking\ninter-token dependency information, and 3) the two-stage decoder for\nalleviating the issue of lacking contextual information. We evaluated NARRepair\non three widely used datasets in the APR community, and the results show that\n1) compared to other APR techniques, the NARRepair model has the best\nperformance within the limited repair time, and 2) compared to AR-based APR\ntechniques, the repair speed of NARRepair has been increased by 1.4-6.4 times\nin the GPU environment. Overall, the results show that NARRepair has achieved\nstate-of-the-art comprehensive performance in terms of repair speed and\naccuracy.", "AI": {"tldr": "The paper introduces NARRepair, the first non-autoregressive model for APR, achieving state-of-the-art speed and accuracy by addressing parallel code generation issues.", "motivation": "Traditional AR-based APR techniques suffer from significant time delays, especially with large models. The authors aim to improve speed by adopting a parallel NAR approach for code generation.", "method": "NARRepair is a non-autoregressive code generation model incorporating three novelties: a repair action predictor, an inter-token dependency extractor, and a two-stage decoder.", "result": "On three APR datasets, NARRepair outperforms AR-based methods in accuracy under time constraints and increases repair speed by 1.4-6.4x in GPU environments.", "conclusion": "NARRepair successfully adapts the NAR approach to APR, offering a speed-accuracy trade-off. It is the first NAR-based model to achieve SOTA performance in APR."}}
{"id": "2510.01350", "categories": ["cs.CR", "cs.AR", "cs.ET", "cs.NE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01350", "abs": "https://arxiv.org/abs/2510.01350", "authors": ["Muhammad Faheemur Rahman", "Wayne Burleson"], "title": "Integrated Security Mechanisms for Weight Protection in Memristive Crossbar Arrays", "comment": "2 pages, 2 figures", "summary": "Memristive crossbar arrays enable in-memory computing by performing parallel\nanalog computations directly within memory, making them well-suited for machine\nlearning, neural networks, and neuromorphic systems. However, despite their\nadvantages, non-volatile memristors are vulnerable to security threats (such as\nadversarial extraction of stored weights when the hardware is compromised.\nProtecting these weights is essential since they represent valuable\nintellectual property resulting from lengthy and costly training processes\nusing large, often proprietary, datasets. As a solution we propose two security\nmechanisms: Keyed Permutor and Watermark Protection Columns; where both\nsafeguard critical weights and establish verifiable ownership (even in cases of\ndata leakage). Our approach integrates efficiently with existing memristive\ncrossbar architectures without significant design modifications. Simulations\nacross 45nm, 22nm, and 7nm CMOS nodes, using a realistic interconnect model and\na large RF dataset, show that both mechanisms offer robust protection with\nunder 10% overhead in area, delay and power. We also present initial\nexperiments employing the widely known MNIST dataset; further highlighting the\nfeasibility of securing memristive in-memory computing systems with minimal\nperformance trade-offs.", "AI": {"tldr": "Researchers developed security mechanisms for memristive computing systems that protect machine learning weights against extraction attacks with minimal performance costs, validated across multiple semiconductor technologies and datasets.", "motivation": "Memristive crossbar arrays for in-memory computing face critical security vulnerabilities as they store valuable machine learning weight matrices - intellectual property at risk of extraction if hardware is physically compromised.", "method": "Develops two hardware-integrated security solutions (Keyed Permutor for dynamic weight scrambling and Watermark Protection Columns for ownership verification) that require minimal architectural modifications to existing memristive crossbar arrays.", "result": "Simulations on 45nm, 22nm, and 7nm CMOS with real-world datasets show 100% protection against weight leakage with <10% overhead in area/power/delay, and practical validation using MNIST dataset confirms real-world feasibility.", "conclusion": "The proposed security mechanisms, Keyed Permutor and Watermark Protection Columns, offer practical protection for memristive in-memory computing systems against weight extraction attacks, maintaining minimal performance overhead and demonstrating feasibility across advanced CMOS nodes."}}
{"id": "2510.01960", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01960", "abs": "https://arxiv.org/abs/2510.01960", "authors": ["Victor Lira", "Paulo Borba", "Rodrigo Bonif\u00e1cio", "Galileu Santos e Matheus barbosa"], "title": "RefFilter: Improving Semantic Conflict Detection via Refactoring-Aware Static Analysis", "comment": null, "summary": "Detecting semantic interference remains a challenge in collaborative software\ndevelopment. Recent lightweight static analysis techniques improve efficiency\nover SDG-based methods, but they still suffer from a high rate of false\npositives. A key cause of these false positives is the presence of\nbehavior-preserving code refactorings, which current techniques cannot\neffectively distinguish from changes that impact behavior and can interfere\nwith others. To handle this problem we present RefFilter, a refactoring-aware\ntool for semantic interference detection. It builds on existing static\ntechniques by incorporating automated refactoring detection to improve\nprecision. RefFilter discards behavior-preserving refactorings from reports,\nreducing false positives while preserving detection coverage. To evaluate\neffectiveness and scalability, use two datasets: a labeled dataset with 99\nscenarios and ground truth, and a novel dataset of 1,087 diverse merge\nscenarios that we have built. Experimental results show that RefFilter reduces\nfalse positives by nearly 32% on the labeled dataset. While this reduction\ncomes with a non significant increase in false negatives, the overall gain in\nprecision significantly outweighs the minor trade-off in recall. These findings\ndemonstrate that refactoring-aware interference detection is a practical and\neffective strategy for improving merge support in modern development workflows.", "AI": {"tldr": "RefFilter tackles false positives in semantic interference detection by identifying behavior-preserving refactoring, reducing false positives by 32% in evaluations while maintaining detection coverage.", "motivation": "Existing static analysis methods for semantic interference detection face high false positive rates due to inability to distinguish behavior-preserving refactorings from behavior-altering changes, disrupting collaborative workflows.", "method": "RefFilter integrates automated refactoring detection into lightweight static analysis, filtering out false positives caused by refactorings, and leverages two datasets (labeled scenarios and a new 1,087-merge set) for validation.", "result": "Achieves 32\\% false positive reduction on the labeled dataset with minimal (non-significant bordering on Problem Descriptions) false negative increase, demonstrating improved precision outweighing minor recall trade-offs.", "conclusion": "Refactoring-aware interference detection via RefFilter is a scalable, effective strategy for enhancing merge support in collaborative development without compromising coverage."}}
{"id": "2510.01354", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01354", "abs": "https://arxiv.org/abs/2510.01354", "authors": ["Yinuo Liu", "Ruohan Xu", "Xilong Wang", "Yuqi Jia", "Neil Zhenqiang Gong"], "title": "WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents", "comment": null, "summary": "Multiple prompt injection attacks have been proposed against web agents. At\nthe same time, various methods have been developed to detect general prompt\ninjection attacks, but none have been systematically evaluated for web agents.\nIn this work, we bridge this gap by presenting the first comprehensive\nbenchmark study on detecting prompt injection attacks targeting web agents. We\nbegin by introducing a fine-grained categorization of such attacks based on the\nthreat model. We then construct datasets containing both malicious and benign\nsamples: malicious text segments generated by different attacks, benign text\nsegments from four categories, malicious images produced by attacks, and benign\nimages from two categories. Next, we systematize both text-based and\nimage-based detection methods. Finally, we evaluate their performance across\nmultiple scenarios. Our key findings show that while some detectors can\nidentify attacks that rely on explicit textual instructions or visible image\nperturbations with moderate to high accuracy, they largely fail against attacks\nthat omit explicit instructions or employ imperceptible perturbations. Our\ndatasets and code are released at:\nhttps://github.com/Norrrrrrr-lyn/WAInjectBench.", "AI": {"tldr": "First comprehensive benchmark reveals detection methods for web agent prompt injections work against obvious attacks but fail against stealthier variants.", "motivation": "Existing prompt injection detection methods lack systematic evaluation for web agents, despite the growing threat of such attacks to this domain.", "method": "Introduce a fine-grained threat-based attack categorization, construct datasets with malicious/benign text and image samples, systematize text- and image-based detection methods, and evaluate their performance across scenarios.", "result": "Detectors show moderate-high accuracy against explicit text/image attacks but fail against non-explicit or imperceptible perturbation-based attacks.", "conclusion": "Current detectors are ineffective against attacks without explicit instructions or using imperceptible perturbations, highlighting the need for more robust detection methods for web agents."}}
{"id": "2510.01994", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01994", "abs": "https://arxiv.org/abs/2510.01994", "authors": ["Chen Yang", "Lin Yang", "Ziqi Wang", "Dong Wang", "Jianyi Zhou", "Junjie Chen"], "title": "Clarifying Semantics of In-Context Examples for Unit Test Generation", "comment": "accepted in the research track of ASE 2025", "summary": "Recent advances in large language models (LLMs) have enabled promising\nperformance in unit test generation through in-context learning (ICL). However,\nthe quality of in-context examples significantly influences the effectiveness\nof generated tests-poorly structured or semantically unclear test examples\noften lead to suboptimal outputs. In this paper, we propose CLAST, a novel\ntechnique that systematically refines unit tests to improve their semantic\nclarity, thereby enhancing their utility as in-context examples. The approach\ndecomposes complex tests into logically clearer ones and improves semantic\nclarity through a combination of program analysis and LLM-based rewriting. We\nevaluated CLAST on four open-source and three industrial projects. The results\ndemonstrate that CLAST largely outperforms UTgen, the state-of-the-art\nrefinement technique, in both preserving test effectiveness and enhancing\nsemantic clarity. Specifically, CLAST fully retains the original effectiveness\nof unit tests, while UTgen reduces compilation success rate (CSR), pass rate\n(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,\n35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user\nstudy preferred the semantic clarity of CLAST-refined tests. Notably,\nincorporating CLAST-refined tests as examples effectively improves ICL-based\nunit test generation approaches such as RAGGen and TELPA, resulting in an\naverage increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for\ngenerated tests, compared to incorporating UTgen-refined tests. The insights\nfrom the follow-up user study not only reinforce CLAST's potential impact in\nsoftware testing practice but also illuminate avenues for future research.", "AI": {"tldr": "CLAST improves unit test generation by refining tests for semantic clarity via program analysis and LLM rewriting, outperforming UTgen in test effectiveness and user preference.", "motivation": "In-context examples for LLM-based unit test generation are often poorly structured, limiting effectiveness. Better test clarity is needed to enhance generated test quality.", "method": "CLAST decomposes complex tests into logically clearer ones using program analysis and LLM-based rewriting to boost semantic clarity.", "result": "CLAST retains original test effectiveness while outperforming UTgen by 12.90%-35.82%% in key metrics; 85.33%% user preference. Generated tests with CLAST examples improve CSR/PR/Cov by 25.97%-45.99%%.", "conclusion": "CLAST demonstrates significant potential to enhance software testing practices through improved test clarity, with implications for future research in LLM-powered testing."}}
{"id": "2510.01359", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01359", "abs": "https://arxiv.org/abs/2510.01359", "authors": ["Shoumik Saha", "Jifan Chen", "Sam Mayers", "Sanjay Krishna Gouda", "Zijian Wang", "Varun Kumar"], "title": "Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks", "comment": "28 pages, 21 figures, 9 tables", "summary": "Code-capable large language model (LLM) agents are increasingly embedded into\nsoftware engineering workflows where they can read, write, and execute code,\nraising the stakes of safety-bypass (\"jailbreak\") attacks beyond text-only\nsettings. Prior evaluations emphasize refusal or harmful-text detection,\nleaving open whether agents actually compile and run malicious programs. We\npresent JAWS-BENCH (Jailbreaks Across WorkSpaces), a benchmark spanning three\nescalating workspace regimes that mirror attacker capability: empty (JAWS-0),\nsingle-file (JAWS-1), and multi-file (JAWS-M). We pair this with a\nhierarchical, executable-aware Judge Framework that tests (i) compliance, (ii)\nattack success, (iii) syntactic correctness, and (iv) runtime executability,\nmoving beyond refusal to measure deployable harm. Using seven LLMs from five\nfamilies as backends, we find that under prompt-only conditions in JAWS-0, code\nagents accept 61% of attacks on average; 58% are harmful, 52% parse, and 27%\nrun end-to-end. Moving to single-file regime in JAWS-1 drives compliance to ~\n100% for capable models and yields a mean ASR (Attack Success Rate) ~ 71%; the\nmulti-file regime (JAWS-M) raises mean ASR to ~ 75%, with 32% instantly\ndeployable attack code. Across models, wrapping an LLM in an agent\nsubstantially increases vulnerability -- ASR raises by 1.6x -- because initial\nrefusals are frequently overturned during later planning/tool-use steps.\nCategory-level analyses identify which attack classes are most vulnerable and\nmost readily deployable, while others exhibit large execution gaps. These\nfindings motivate execution-aware defenses, code-contextual safety filters, and\nmechanisms that preserve refusal decisions throughout the agent's multi-step\nreasoning and tool use.", "AI": {"tldr": "The paper evaluates code-capable LLM agents' vulnerability to jailbreak attacks using JAWS-BENCH (three workspace regimes) and a Judge Framework (compliance, attack success, syntax, runtime). Finds rising harm: 61% attack acceptance in empty workspaces, 75% ASR in multi-file regimes, with deployable risks motivating execution-aware defenses.", "motivation": "Code LLM agents embedded in software workflows necessitate evaluating safety-bypass attacks beyond text-only scenarios, as prior work focuses on refusal/Harm detection without assessing executable malicious code.", "method": "1) JAWS-BENCH benchmark with three workspace regimes (empty/JAWS-0, single-file/JAWS-1, multi-file/JAWS-M)\n2 ) Hierarchical Judge Framework (compliance, attack success, syntax correctness, runtime executability)\n3) Testing seven LLMs (five families) across regimes", "result": "Attack acceptance rates: 61%(JAWS-0, 61%) -> 71%(JAWS-1 ASR) -> 75%(JAWS-M ASR). 32%MJAWS-M instantly deployable code. Agents overturn 1.6x higher ASR post-refusal during planning/tool-use steps. Execution gaps vary across attack categories.", "conclusion": "LLM agents are more vulnerable to jailbreaks in code contexts. Defense solutions require execution-aware systems, code-contextual filters, and refusal-preservation during multi-step reasoning."}}
{"id": "2510.02002", "categories": ["cs.SE", "D.2.1; D.2.2; D.2.3; D.3.4; G.1.6"], "pdf": "https://arxiv.org/pdf/2510.02002", "abs": "https://arxiv.org/abs/2510.02002", "authors": ["Maximilian Kratz", "Steffen Zschaler", "Jens Kosiol", "Gabriele Taentzer"], "title": "Automatic Generation of Combinatorial Reoptimisation Problem Specifications: A Vision", "comment": null, "summary": "Once an optimisation problem has been solved, the solution may need\nadaptation when contextual factors change. This challenge, also known as\nreoptimisation, has been addressed in various problem domains, such as railway\ncrew rescheduling, nurse rerostering, or aircraft recovery. This requires a\nmodified problem to be solved again to ensure that the adapted solution is\noptimal in the new context. However, the new optimisation problem differs\nnotably from the original problem: (i) we want to make only minimal changes to\nthe original solution to minimise the impact; (ii) we may be unable to change\nsome parts of the original solution (e.g., because they refer to past\nallocations); and (iii) we need to derive a change script from the original\nsolution to the new solution. In this paper, we argue that Model-Driven\nEngineering (MDE) - in particular, the use of declarative modelling languages\nand model transformations for the high-level specification of optimisation\nproblems - offers new opportunities for the systematic derivation of\nreoptimisation problems from the original optimisation problem specification.\nWe focus on combinatorial reoptimisation problems and provide an initial\ncategorisation of changing problems and strategies for deriving the\ncorresponding reoptimisation specifications. We introduce an initial\nproof-of-concept implementation based on the GIPS (Graph-Based (Mixed) Integer\nLinear Programming Problem Specification) tool and apply it to an example\nresource-allocation problem: the allocation of teaching assistants to teaching\nsessions.", "AI": {"tldr": "The paper explores using Model-Driven Engineering (MDE) for systematic reoptimisation in combinatorial problems by adapting solutions to changing contexts, demonstrating a proof-of-concept for resource allocation.", "motivation": "Traditional optimisation methods fail to address reoptimisation constraints (minimal changes, fixed historical allocations, deriving change scripts) in dynamic contexts like scheduling and resource allocation.", "method": "The authors leverage MDE with declarative modelling and model transformations to systematically derive reoptimisation specifications from original problem definitions, implemented in the GIPS tool.", "result": "An initial categorization of reoptimisation problems and a proof-of-concept implementation applied to a teaching assistant resource-allocation case study.", "conclusion": "MDE provides a structured approach to address reoptimisation challenges, enabling scalable and maintainable solutions for combinatorial problems."}}
{"id": "2510.01393", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01393", "abs": "https://arxiv.org/abs/2510.01393", "authors": ["Davide Rusconi", "Osama Yousef", "Mirco Picca", "Flavio Toffalini", "Andrea Lanzi"], "title": "E-FuzzEdge: Optimizing Embedded Device Security with Scalable In-Place Fuzzing", "comment": null, "summary": "In this paper we show E-FuzzEdge, a novel fuzzing architecture targeted\ntowards improving the throughput of fuzzing campaigns in contexts where\nscalability is unavailable. E-FuzzEdge addresses the inefficiencies of\nhardware-in-the-loop fuzzing for microcontrollers by optimizing execution\nspeed. We evaluated our system against state-of-the-art benchmarks,\ndemonstrating significant performance improvements. A key advantage of\nE-FuzzEdgearchitecture is its compatibility with other embedded fuzzing\ntechniques that perform on device testing instead of firmware emulation. This\nmeans that the broader embedded fuzzing community can integrate E-FuzzEdge into\ntheir workflows to enhance overall testing efficiency.", "AI": {"tldr": "E-FuzzEdge is a novel fuzzing architecture designed to increase throughput for microcontroller testing by optimizing execution speed.", "motivation": "The paper addresses the inefficiencies of hardware-in-the-loop fuzzing for microcontrollers where scalability is not an option.", "method": "E-FuzzEdge optimizes the execution speed of the fuzzing process specifically for hardware-in-the-loop environments.", "result": "Evaluation against state-of-the-art benchmarks showed significant improvements in performance for E-FuzzEdge.", "conclusion": "E-FuzzEdge can be integrated with existing embedded fuzzing techniques to improve overall testing efficiency."}}
{"id": "2510.02007", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02007", "abs": "https://arxiv.org/abs/2510.02007", "authors": ["Justus Bogner", "Roberto Verdecchia"], "title": "ACM SIGSOFT SEN Empirical Software Engineering: Introducing Our New Regular Column", "comment": "Published in ACM SIGSOFT Software Engineering Notes (SIGSOFT-SEN).\n  Volume 50, Issue 4, 2025", "summary": "From its early foundations in the 1970s, empirical software engineering (ESE)\nhas evolved into a mature research discipline that embraces a plethora of\ndifferent topics, methodologies, and industrial practices. Despite its\nremarkable progress, the ESE research field still needs to keep evolving, as\nnew impediments, shortcoming, and technologies emerge. Research\nreproducibility, limited external validity, subjectivity of reviews, and\nporting research results to industrial practices are just some examples of the\ndrivers for improvements to ESE research. Additionally, several facets of ESE\nresearch are not documented very explicitly, which makes it difficult for\nnewcomers to pick them up. With this new regular ACM SIGSOFT SEN column\n(SEN-ESE), we introduce a venue for discussing meta-aspects of ESE research,\nranging from general topics such as the nature and best practices for\nreplication packages, to more nuanced themes such as statistical methods,\ninterview transcription tools, and publishing interdisciplinary research. Our\naim for the column is to be a place where we can regularly spark conversations\non ESE topics that might not often be touched upon or are left implicit.\nContributions to this column will be grounded in expert interviews, focus\ngroups, surveys, and position pieces, with the goal of encouraging reflection\nand improvement in how we conduct, communicate, teach, and ultimately improve\nESE research. Finally, we invite feedback from the ESE community on\nchallenging, controversial, or underexplored topics, as well as suggestions for\nvoices you would like to hear from. While we cannot promise to act on every\nidea, we aim to shape this column around the community interests and are\ngrateful for all contributions.", "AI": {"tldr": "This paper launches the SEN-ESE column to address empirical software engineering (ESE) research gaps through meta-discussions on replication practices, statistical methods, and industry transfer challenges, using community-driven feedback to drive field improvements.", "motivation": "Despite ESE's maturity, challenges like reproducibility, external validity, subjective reviews, poor documentation, and difficulty transferring research to industry practices persist, necessitating focused meta-research to guide field evolution and improve accessibility for newcomers.", "method": "The SEN-ESE column will leverage expert interviews, focus groups, surveys, and position pieces to analyze and improve meta-aspects of ESE research, such as replication packages, statistical methods, and interdisciplinary publishing challenges.", "result": "The column establishes a structured venue for ESE meta-discussions, enabling systematic reflection on implicit research practices and community-driven prioritization of improvement initiatives.", "conclusion": "The paper introduces the SEN-ESE column as a dedicated platform to address meta-aspects of ESE research, fostering discussions on underexplored topics, and invites ongoing community feedback to refine ESE practices and pedagogy."}}
{"id": "2510.01445", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01445", "abs": "https://arxiv.org/abs/2510.01445", "authors": ["Andr\u00e9s F. Betancur-L\u00f3pez"], "title": "Securing IoT Devices in Smart Cities: A Review of Proposed Solutions", "comment": "14 pages, 7 figures", "summary": "Privacy and security in Smart Cities remain at constant risk due to the\nvulnerabilities introduced by Internet of Things (IoT) devices. The limited\ncomputational resources of these devices make them especially susceptible to\nattacks, while their widespread adoption increases the potential impact of\nsecurity breaches. This article presents a review of security proposals aimed\nat protecting IoT devices in Smart City environments. The review was conducted\nby analyzing recent literature on device-level security, with particular\nemphasis on lightweight cryptography, physically unclonable functions (PUFs),\nand blockchain-based solutions. Findings highlight both the strengths and\nlimitations of current approaches, as well as the need for more practical,\nscalable, and resource-efficient mechanisms to ensure user privacy and data\nprotection in IoT ecosystems.", "AI": {"tldr": "The paper reviews IoT security solutions for Smart Cities with a focus on lightweight cryptography, PUFs, and blockchain, highlighting their pros, cons, and the need for better mechanisms.", "motivation": "IoT devices in Smart Cities pose significant privacy and security risks due to their limited resources and widespread use, requiring a focused investigation into recent device-level security solutions.", "method": "The paper conducts a comprehensive analysis of recent literature on device-level security for IoT in Smart Cities, with a specific focus on lightweight cryptography, physically unclonable functions (PUFs), and blockchain-based solutions.", "result": "The findings present an overview of the strengths and limitations of current security approaches, indicating that existing solutions have not fully met the requirements for practical, scalable, and resource-efficient protection in IoT ecosystems.", "conclusion": "The study underscores the importance of developing more effective, scalable security mechanisms for resource-constrained IoT devices in Smart Cities to mitigate privacy risks and enhance system resilience."}}
{"id": "2510.02165", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02165", "abs": "https://arxiv.org/abs/2510.02165", "authors": ["Peter Wauyo", "Dalia Bwiza", "Alain Murara", "Edwin Mugume", "Eric Umuhoza"], "title": "Towards fairer public transit: Real-time tensor-based multimodal fare evasion and fraud detection", "comment": "10 pages", "summary": "This research introduces a multimodal system designed to detect fraud and\nfare evasion in public transportation by analyzing closed circuit television\n(CCTV) and audio data. The proposed solution uses the Vision Transformer for\nVideo (ViViT) model for video feature extraction and the Audio Spectrogram\nTransformer (AST) for audio analysis. The system implements a Tensor Fusion\nNetwork (TFN) architecture that explicitly models unimodal and bimodal\ninteractions through a 2-fold Cartesian product. This advanced fusion technique\ncaptures complex cross-modal dynamics between visual behaviors (e.g.,\ntailgating,unauthorized access) and audio cues (e.g., fare transaction sounds).\nThe system was trained and tested on a custom dataset, achieving an accuracy of\n89.5%, precision of 87.2%, and recall of 84.0% in detecting fraudulent\nactivities, significantly outperforming early fusion baselines and exceeding\nthe 75% recall rates typically reported in state-of-the-art transportation\nfraud detection systems. Our ablation studies demonstrate that the tensor\nfusion approach provides a 7.0% improvement in the F1 score and an 8.8% boost\nin recall compared to traditional concatenation methods. The solution supports\nreal-time detection, enabling public transport operators to reduce revenue\nloss, improve passenger safety, and ensure operational compliance.", "AI": {"tldr": "A multimodal system using ViViT (video) and AST (audio) with Tensor Fusion achieves 84% recall for fraud detection, outperforming existing methods and enabling real-time monitoring in public transportation.", "motivation": "Fraud and fare evasion in public transportation cause revenue loss and safety risks. Existing systems achieve 75% recall rates, but the field needs a more accurate and robust solution.", "method": "The system combines video (ViViT model) and audio (AST model) features using a Tensor Fusion Network (TFN) architecture that models unimodal and bimodal interactions via a 2-fold Cartesian product. This captures cross-modal dynamics between visual behaviors and audio cues.", "result": "The system achieved 89.5% accuracy, 87.2% precision, and 84.0% recall in detecting fraudulent activities. Abation studies showed 7.0% F1-score and 8.8% recall improvements over traditional concatenation methods.", "conclusion": "The research successfully demonstrates that the Tensor Fusion Network (TFN) with multimodal analysis outperforms traditional methods in detecting fraud and fare evasion in public transportation. The approach achieves high accuracy and exceeds state-of-the-art systems, providing real-time capabilities to reduce revenue loss and improve safety."}}
{"id": "2510.01552", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01552", "abs": "https://arxiv.org/abs/2510.01552", "authors": ["Luoxi Tang", "Yuqiao Meng", "Ankita Patra", "Weicheng Ma", "Muchao Ye", "Zhaohan Xi"], "title": "POLAR: Automating Cyber Threat Prioritization through LLM-Powered Assessment", "comment": "25 pages", "summary": "Large Language Models (LLMs) are intensively used to assist security analysts\nin counteracting the rapid exploitation of cyber threats, wherein LLMs offer\ncyber threat intelligence (CTI) to support vulnerability assessment and\nincident response. While recent work has shown that LLMs can support a wide\nrange of CTI tasks such as threat analysis, vulnerability detection, and\nintrusion defense, significant performance gaps persist in practical\ndeployments. In this paper, we investigate the intrinsic vulnerabilities of\nLLMs in CTI, focusing on challenges that arise from the nature of the threat\nlandscape itself rather than the model architecture. Using large-scale\nevaluations across multiple CTI benchmarks and real-world threat reports, we\nintroduce a novel categorization methodology that integrates stratification,\nautoregressive refinement, and human-in-the-loop supervision to reliably\nanalyze failure instances. Through extensive experiments and human inspections,\nwe reveal three fundamental vulnerabilities: spurious correlations,\ncontradictory knowledge, and constrained generalization, that limit LLMs in\neffectively supporting CTI. Subsequently, we provide actionable insights for\ndesigning more robust LLM-powered CTI systems to facilitate future research.", "AI": {"tldr": "This paper identifies inherent vulnerabilities in LLMs for cyber threat intelligence (CTI), proposes a novel analysis methodology, and reveals three critical limitations (spurious correlations, contradictory knowledge, constrained generalization), offering actionable insights for robust CTI systems.", "motivation": "Despite LLMs being widely adopted in CTI for tasks like threat analysis and vulnerability detection, significant performance gaps persist in practical deployments, necessitating a deeper understanding of inherent vulnerabilities to improve reliability.", "method": "A novel categorization methodology combining stratification, autoregressive refinement, and human-in-the-loop supervision is applied to large-scale evaluations across CTI benchmarks and real-world threat reports.", "result": "Three fundamental LLM vulnerabilities are uncovered: 1) spurious correlations leading to unreliable inferences, 2)... [truncated] ...actionable strategies for designing more robust LLM-powered CTI systems, emphasizing methodology improvements and human supervision integration.", "conclusion": "The study provides a framework to systematically analyze LLM failures in CTI and demonstrates that addressing spurious correlations, contradictory knowledge, and generalization limitations requires targeted architectural and training improvements supported by human-in-the-loop mechanisms."}}
{"id": "2510.02166", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02166", "abs": "https://arxiv.org/abs/2510.02166", "authors": ["Fatou Ndiaye Mbodji", "El-hacen Diallo", "Jordan Samhi", "Kui Liu", "Jacques Klein", "Tegawend\u00e9 F. Bissyande"], "title": "SIEVE: Towards Verifiable Certification for Code-datasets", "comment": "5", "summary": "Code agents and empirical software engineering rely on public code datasets,\nyet these datasets lack verifiable quality guarantees. Static 'dataset cards'\ninform, but they are neither auditable nor do they offer statistical\nguarantees, making it difficult to attest to dataset quality. Teams build\nisolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We\npresent SIEVE, a community-driven framework. It turns per-property checks into\nConfidence Cards-machine-readable, verifiable certificates with anytime-valid\nstatistical bounds. We outline a research plan to bring SIEVE to maturity,\nreplacing narrative cards with anytime-verifiable certification. This shift is\nexpected to lower quality-assurance costs and increase trust in code-datasets.", "AI": {"tldr": "The paper proposes SIEVE, a framework for certifying code dataset quality via machine-readable 'Confidence Cards' with statistical bounds, addressing fragmented pipelines and unverifiable static dataset cards.", "motivation": "Public code datasets lack auditable quality guarantees, relying on non-verifiable static cards and ad-hoc cleaning pipelines that fragment efforts and increase costs.", "method": "SIEVE introduces per-property checks generating Confidence Cards\u2014machine-readable certificates with anytime-valid statistical bounds, replacing narrative cards through community-driven verification.", "result": "Expected outcomes include reduced quality-assurance costs and increased trust in code datasets via verifiable certifications.", "conclusion": "SIEVE offers a scalable solution for dataset quality assurance through statistical certification, promoting transparency and community collaboration in empirical software engineering."}}
{"id": "2510.01645", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01645", "abs": "https://arxiv.org/abs/2510.01645", "authors": ["Niloofar Mireshghallah", "Tianshi Li"], "title": "Position: Privacy Is Not Just Memorization!", "comment": "27 pages, 6 figures, 2 tables", "summary": "The discourse on privacy risks in Large Language Models (LLMs) has\ndisproportionately focused on verbatim memorization of training data, while a\nconstellation of more immediate and scalable privacy threats remain\nunderexplored. This position paper argues that the privacy landscape of LLM\nsystems extends far beyond training data extraction, encompassing risks from\ndata collection practices, inference-time context leakage, autonomous agent\ncapabilities, and the democratization of surveillance through deep inference\nattacks. We present a comprehensive taxonomy of privacy risks across the LLM\nlifecycle -- from data collection through deployment -- and demonstrate through\ncase studies how current privacy frameworks fail to address these multifaceted\nthreats. Through a longitudinal analysis of 1,322 AI/ML privacy papers\npublished at leading conferences over the past decade (2016--2025), we reveal\nthat while memorization receives outsized attention in technical research, the\nmost pressing privacy harms lie elsewhere, where current technical approaches\noffer little traction and viable paths forward remain unclear. We call for a\nfundamental shift in how the research community approaches LLM privacy, moving\nbeyond the narrow focus of current technical solutions and embracing\ninterdisciplinary approaches that address the sociotechnical nature of these\nemerging threats.", "AI": {"tldr": "This paper argues that privacy risks in LLMs go beyond memorization, highlighting underexplored threats like context leakage, autonomous agents, and surveillance democratization, while calling for interdisciplinary solutions.", "motivation": "Current LLM privacy research over-focuses on data memorization, leaving more immediate and systemic threats inadequately addressed despite their scalability and societal impact.", "method": "The authors present a lifecycle taxonomy of privacy risks and analyze 1,322 AI/ML privacy papers (2016\u20132025), identifying research gaps through case studies and literature trends.", "result": "Analysis shows memorization dominates technical literature but misses critical risks in data collection, inference leakage, and emerging surveillance capabilities where current solutions are ineffective.", "conclusion": "The paper advocates shifting LLM privacy research toward sociotechnical, interdisciplinary approaches to address the complex ecosystem of evolving privacy threats."}}
{"id": "2510.02169", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.02169", "abs": "https://arxiv.org/abs/2510.02169", "authors": ["Vadim Safronov", "Anthony McCaigue", "Nicholas Allott", "Andrew Martin"], "title": "TAIBOM: Bringing Trustworthiness to AI-Enabled Systems", "comment": "This paper has been accepted at the First International Workshop on\n  Security and Privacy-Preserving AI/ML (SPAIML 2025), co-located with the 28th\n  European Conference on Artificial Intelligence (ECAI 2025)", "summary": "The growing integration of open-source software and AI-driven technologies\nhas introduced new layers of complexity into the software supply chain,\nchallenging existing methods for dependency management and system assurance.\nWhile Software Bills of Materials (SBOMs) have become critical for enhancing\ntransparency and traceability, current frameworks fall short in capturing the\nunique characteristics of AI systems -- namely, their dynamic, data-driven\nnature and the loosely coupled dependencies across datasets, models, and\nsoftware components. These challenges are compounded by fragmented governance\nstructures and the lack of robust tools for ensuring integrity, trust, and\ncompliance in AI-enabled environments.\n  In this paper, we introduce Trusted AI Bill of Materials (TAIBOM) -- a novel\nframework extending SBOM principles to the AI domain. TAIBOM provides (i) a\nstructured dependency model tailored for AI components, (ii) mechanisms for\npropagating integrity statements across heterogeneous AI pipelines, and (iii) a\ntrust attestation process for verifying component provenance. We demonstrate\nhow TAIBOM supports assurance, security, and compliance across AI workflows,\nhighlighting its advantages over existing standards such as SPDX and CycloneDX.\nThis work lays the foundation for trustworthy and verifiable AI systems through\nstructured software transparency.", "AI": {"tldr": "The paper proposes TAIBOM, an AI-focused extension of SBOMs to address supply chain transparency gaps in dynamic, data-driven AI systems.", "motivation": "Existing SBOM frameworks fail to capture AI systems' unique characteristics (dynamic nature, dataset/model/software dependencies, fragmented governance), creating risks for integrity, trust, and compliance in AI workflows.", "method": "Introduces TAIBOM with (1). structured AI-specific dependency modeling, (2). integrity propagation mechanisms across heterogeneous pipelines, and (3). trust attestation processes for component verification, while extending SBOM principles.", "result": "Demonstrates TAIBOM's ability to enhance assurance, security, and compliance for AI systems, outperforming existing standards like SPDX and CycloneDX through structured transparency and provenance verification.", "conclusion": "Establishes foundational framework for trustworthy AI systems via tailored supply chain transparency, enabling actionable verification and compliance across the AI lifecycle."}}
{"id": "2510.01676", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01676", "abs": "https://arxiv.org/abs/2510.01676", "authors": ["Milad Nasr", "Yanick Fratantonio", "Luca Invernizzi", "Ange Albertini", "Loua Farah", "Alex Petit-Bianco", "Andreas Terzis", "Kurt Thomas", "Elie Bursztein", "Nicholas Carlini"], "title": "Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks", "comment": null, "summary": "As deep learning models become widely deployed as components within larger\nproduction systems, their individual shortcomings can create system-level\nvulnerabilities with real-world impact. This paper studies how adversarial\nattacks targeting an ML component can degrade or bypass an entire\nproduction-grade malware detection system, performing a case study analysis of\nGmail's pipeline where file-type identification relies on a ML model.\n  The malware detection pipeline in use by Gmail contains a machine learning\nmodel that routes each potential malware sample to a specialized malware\nclassifier to improve accuracy and performance. This model, called Magika, has\nbeen open sourced. By designing adversarial examples that fool Magika, we can\ncause the production malware service to incorrectly route malware to an\nunsuitable malware detector thereby increasing our chance of evading detection.\nSpecifically, by changing just 13 bytes of a malware sample, we can\nsuccessfully evade Magika in 90% of cases and thereby allow us to send malware\nfiles over Gmail. We then turn our attention to defenses, and develop an\napproach to mitigate the severity of these types of attacks. For our defended\nproduction model, a highly resourced adversary requires 50 bytes to achieve\njust a 20% attack success rate. We implement this defense, and, thanks to a\ncollaboration with Google engineers, it has already been deployed in production\nfor the Gmail classifier.", "AI": {"tldr": "The paper demonstrates how adversarial attacks on the ML component (Magika) in Gmail's malware detection pipeline can enable evasion of malware detection, and proposes a defense strategy successfully deployed in production.", "motivation": "ML components in critical systems like malware detection are vulnerable to adversarial attacks, which can exploit model shortcomings to create system-level failures with real-world consequences (e.g., bypassing Gmail's security).", "method": "The authors crafted adversarial examples by modifying 13-50 bytes in malware files to deceive Magika's file-type identification model. They developed and tested a defense approach, collaborating with engineers for production deployment.", "result": "Adversarial attacks achieved 90\\% evasion success with 13-byte changes. The proposed defense reduced attack success to 20\\%, requiring 50 bytes. The defense was implemented in Gmail's production system.", "conclusion": "The study emphasizes vulnerabilities in ML-dependent security pipelines and validates that adversarially robust defenses, when implemented collaboratively, can mitigate real-world adversarial threats effectively."}}
{"id": "2510.02185", "categories": ["cs.SE", "cs.CR", "cs.MA", "D.2.4; F.3.1"], "pdf": "https://arxiv.org/pdf/2510.02185", "abs": "https://arxiv.org/abs/2510.02185", "authors": ["Paschal C. Amusuo", "Dongge Liu", "Ricardo Andres Calvo Mendez", "Jonathan Metzman", "Oliver Chang", "James C. Davis"], "title": "FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen Using Agentic AI", "comment": "12 pages, 2 figures", "summary": "Fuzz testing has become a cornerstone technique for identifying software bugs\nand security vulnerabilities, with broad adoption in both industry and\nopen-source communities. Directly fuzzing a function requires fuzz drivers,\nwhich translate random fuzzer inputs into valid arguments for the target\nfunction. Given the cost and expertise required to manually develop fuzz\ndrivers, methods exist that leverage program analysis and Large Language Models\nto automatically generate these drivers. However, the generated fuzz drivers\nfrequently lead to false positive crashes, especially in functions highly\nstructured input and complex state requirements. This problem is especially\ncrucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as\nreporting false positive crashes to maintainers impede trust in both the system\nand the team.\n  This paper presents two AI-driven strategies to reduce false positives in\nOSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First,\nconstraint-based fuzz driver generation proactively enforces constraints on a\nfunction's inputs and state to guide driver creation. Second, context-based\ncrash validation reactively analyzes function callers to determine whether\nreported crashes are feasible from program entry points. Using 1,500 benchmark\nfunctions from OSS-Fuzz, we show that these strategies reduce spurious crashes\nby up to 8%, cut reported crashes by more than half, and demonstrate that\nfrontier LLMs can serve as reliable program analysis agents. Our results\nhighlight the promise and challenges of integrating AI into large-scale fuzzing\npipelines.", "AI": {"tldr": "The paper addresses the challenge of false positive crashes in fuzz driver generation by introducing two AI-driven strategies that reduce spurious crashes by 8% and reported crashes by over half, improving the reliability of fuzzing in industry-scale projects like OSS-Fuzz-Gen.", "motivation": "Fuzz drivers enable automated fuzz testing by converting random inputs into valid function arguments, but their manual development is costly and error-prone. Existing automated methods often produce false positive crashes, undermining trust in large-scale fuzzing systems such as OSS-Fuzz-Gen.", "method": "The authors propose two strategies: (1) Constraint-based fuzz driver generation, which enforces input and state constraints during driver creation, and (2) Context-based crash validation, which analyzes function callers to assess if reported crashes are feasible from actual program entry points.", "result": "Evaluation on 1,500 OSS-Fuzz benchmark functions shows these strategies reduce spurious crashes by up to 8%, cut reported crashes by over 50%, and validate the effectiveness of state-of-the-art LLMs in automating fuzz driver generation.", "conclusion": "The work demonstrates that AI, particularly constraint- and context-based approaches, can significantly enhance the accuracy of fuzz driver generation and crash validation, offering a path to more trustworthy automatic fuzzing systems while highlighting remaining challenges in AI integration."}}
{"id": "2510.01699", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01699", "abs": "https://arxiv.org/abs/2510.01699", "authors": ["Yue Li", "Linying Xue", "Dongdong Lin", "Qiushi Li", "Hui Tian", "Hongxia Wang"], "title": "Towards Imperceptible Adversarial Defense: A Gradient-Driven Shield against Facial Manipulations", "comment": null, "summary": "With the flourishing prosperity of generative models, manipulated facial\nimages have become increasingly accessible, raising concerns regarding privacy\ninfringement and societal trust. In response, proactive defense strategies\nembed adversarial perturbations into facial images to counter deepfake\nmanipulation. However, existing methods often face a tradeoff between\nimperceptibility and defense effectiveness-strong perturbations may disrupt\nforgeries but degrade visual fidelity. Recent studies have attempted to address\nthis issue by introducing additional visual loss constraints, yet often\noverlook the underlying gradient conflicts among losses, ultimately weakening\ndefense performance. To bridge the gap, we propose a gradient-projection-based\nadversarial proactive defense (GRASP) method that effectively counters facial\ndeepfakes while minimizing perceptual degradation. GRASP is the first approach\nto successfully integrate both structural similarity loss and low-frequency\nloss to enhance perturbation imperceptibility. By analyzing gradient conflicts\nbetween defense effectiveness loss and visual quality losses, GRASP pioneers\nthe design of the gradient-projection mechanism to mitigate these conflicts,\nenabling balanced optimization that preserves image fidelity without\nsacrificing defensive performance. Extensive experiments validate the efficacy\nof GRASP, achieving a PSNR exceeding 40 dB, SSIM of 0.99, and a 100% defense\nsuccess rate against facial attribute manipulations, significantly\noutperforming existing approaches in visual quality.", "AI": {"tldr": "This paper proposes GRASP, a gradient-projection-based proactive defense method for facial deepfakes, achieving high imperceptibility and defense effectiveness by resolving gradient conflicts between losses. It attains PSNR>40 dB, SSIM=0.99, and 100 defense success rate, outperforming existing approaches.", "motivation": "Current deepfake defense methods face a tradeoff between perturbation imperceptibility and defense effectiveness. Existing approaches using visual loss constraints neglect gradient conflicts among losses, weakening their performance.", "method": "The paper introduces GRASP, which combines structural similarity loss and low-frequency loss to enhance imperceptibility. It uses a novel gradient-projection mechanism to analyze and mitigate conflicts between defense effectiveness loss and visual quality losses, enabling balanced optimization.", "result": "GRASP achieves a PSNR exceeding 40 dB, SSIM of 0.99, and a 100 defense success rate against facial attribute manipulations, demonstrating significant improvements in visual quality compared to existing methods.", "conclusion": "GRASP is the first approach to effectively integrate structural similarity loss and low-frequency loss while resolving gradient conflicts. It achieves both high perceptual fidelity and strong defense performance, establishing a new benchmark for facial deepfake defense."}}
{"id": "2510.01720", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01720", "abs": "https://arxiv.org/abs/2510.01720", "authors": ["Palash Sarkar"], "title": "Constructions of Efficiently Implementable Boolean Functions with Provable Nonlinearity/Resiliency/Algebraic Immunity Trade-Offs", "comment": null, "summary": "We describe several families of efficiently implementable Boolean functions\nachieving provable trade-offs between resiliency, nonlinearity, and algebraic\nimmunity. In concrete terms, the following result holds for each of the\nfunction families that we propose. Given integers $m_0\\geq 0$, $x_0\\geq 1$, and\n$a_0\\geq 1$, it is possible to construct an $n$-variable function which has\nresiliency at least $m_0$, linear bias (which is an equivalent method of\nexpressing nonlinearity) at most $2^{-x_0}$ and algebraic immunity at least\n$a_0$; further, $n$ is linear in $m_0$, $x_0$ and $a_0$, and the function can\nbe implemented using $O(n)$ gates.", "AI": {"tldr": "The paper presents multiple families of Boolean functions that offer efficient implementations with proven balance among resiliency, nonlinearity, and algebraic immunity.", "motivation": "There is a need for Boolean functions that optimally balance trade-offs in cryptographic properties such as resiliency, nonlinearity, and algebraic immunity for secure and efficient implementations.", "method": "The authors propose families of Boolean functions with specific parameters to demonstrate how these trade-offs can be achieved. By setting integers m0 \u2265 0, x0 \u2265 1, and a0 \u2265 1, they construct an n-variable function with desired properties: resiliency \u2265 m0, linear bias \u2264 2^{-x0}, and algebraic immunity \u2265 a0. The construction ensures n is linear in m0, x0, and a0 while maintaining implementation efficiency with O(n) gates.", "result": "The proposed function families achieve provable trade-offs between the three important cryptographic characteristics, and allow for efficient implementation with O(n) gates.", "conclusion": "The construction of n-variable Boolean functions with specified trade-offs in resiliency, nonlinearity, and algebraic immunity demonstrates the feasibility of achieving optimal performance in implementations for cryptographic applications."}}
{"id": "2510.01780", "categories": ["cs.CR", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01780", "abs": "https://arxiv.org/abs/2510.01780", "authors": ["Aueaphum Aueawatthanaphisut"], "title": "Secure Multi-Modal Data Fusion in Federated Digital Health Systems via MCP", "comment": "6 pages, 8 figures, 7 equations, 1 algorithm", "summary": "Secure and interoperable integration of heterogeneous medical data remains a\ngrand challenge in digital health. Current federated learning (FL) frameworks\noffer privacy-preserving model training but lack standardized mechanisms to\norchestrate multi-modal data fusion across distributed and resource-constrained\nenvironments. This study introduces a novel framework that leverages the Model\nContext Protocol (MCP) as an interoperability layer for secure, cross-agent\ncommunication in multi-modal federated healthcare systems. The proposed\narchitecture unifies three pillars: (i) multi-modal feature alignment for\nclinical imaging, electronic medical records, and wearable IoT data; (ii)\nsecure aggregation with differential privacy to protect patient-sensitive\nupdates; and (iii) energy-aware scheduling to mitigate dropouts in mobile\nclients. By employing MCP as a schema-driven interface, the framework enables\nadaptive orchestration of AI agents and toolchains while ensuring compliance\nwith privacy regulations. Experimental evaluation on benchmark datasets and\npilot clinical cohorts demonstrates up to 9.8\\% improvement in diagnostic\naccuracy compared with baseline FL, a 54\\% reduction in client dropout rates,\nand clinically acceptable privacy--utility trade-offs. These results highlight\nMCP-enabled multi-modal fusion as a scalable and trustworthy pathway toward\nequitable, next-generation federated health infrastructures.", "AI": {"tldr": "This paper proposes an MCP-based federated learning framework that unifies multi-modal data fusion, differential privacy, and energy efficiency, achieving significant accuracy improvements and client retention in distributed healthcare systems while complying with privacy regulations.", "motivation": "Current federated learning systems lack standardized mechanisms for secure, interoperable multi-modal data fusion across distributed healthcare environments, limiting their ability to address privacy, resource constraints, and regulatory compliance in heterogeneous settings.", "method": "The framework integrates three key components: (i) multi-modal feature alignment for heterogeneous data types, (ii) secure aggregation with differential privacy for sensitive data protection, and (iii) energy-aware scheduling to reduce mobile client dropouts, all orchestrated through the Model Context Protocol (MCP).", "result": "Experiments show 9.8% higher diagnostic accuracy than baseline FL, 54% lower client dropout rates, and privacy-utility trade-offs meeting clinical requirements, validating the framework's effectiveness in real-world scenarios.", "conclusion": "The proposed framework demonstrates that MCP-enabled multi-modal fusion offers a scalable and trustworthy solution for secure, interoperable federated healthcare systems, achieving significant improvements in diagnostic accuracy, client retention, and privacy compliance."}}
{"id": "2510.01967", "categories": ["cs.CR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01967", "abs": "https://arxiv.org/abs/2510.01967", "authors": ["Aadarsh Anantha Ramakrishnan", "Shubham Agarwal", "Selvanayagam S", "Kunwar Singh"], "title": "ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs", "comment": "Accepted at AI-ML Systems 2025, Bangalore, India,\n  https://www.aimlsystems.org/2025/", "summary": "As image generation models grow increasingly powerful and accessible,\nconcerns around authenticity, ownership, and misuse of synthetic media have\nbecome critical. The ability to generate lifelike images indistinguishable from\nreal ones introduces risks such as misinformation, deepfakes, and intellectual\nproperty violations. Traditional watermarking methods either degrade image\nquality, are easily removed, or require access to confidential model internals\n- making them unsuitable for secure and scalable deployment. We are the first\nto introduce ZK-WAGON, a novel system for watermarking image generation models\nusing the Zero-Knowledge Succinct Non Interactive Argument of Knowledge\n(ZK-SNARKs). Our approach enables verifiable proof of origin without exposing\nmodel weights, generation prompts, or any sensitive internal information. We\npropose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively\nconvert key layers of an image generation model into a circuit, reducing proof\ngeneration time significantly. Generated ZK-SNARK proofs are imperceptibly\nembedded into a generated image via Least Significant Bit (LSB) steganography.\nWe demonstrate this system on both GAN and Diffusion models, providing a\nsecure, model-agnostic pipeline for trustworthy AI image generation.", "AI": {"tldr": "ZK-WAGON uses ZK-SNARKs to watermark image-generation models securely and without exposing sensitive info, with LSB steganography for imperceptible proof embedding.", "motivation": "Image-generation models need secure watermarking to address authenticity, ownership, and misuse risks due to synthetic media, but traditional methods are inadequate for quality, removal resistance, and scalability.", "method": "ZK-WAGON introduces Selective Layer ZK-Circuit Creation (SL-ZKCC) to transform key model layers into circuits for efficient ZK-SNARK proof generation, and LSB steganography embeds proofs imperceptibly in images.", "result": "Successfully demonstrated ZK-WAGON on GAN and Diffusion models, offering a secure, model-agnostic solution for watermarking synthetic images with embedded ZK-SNARK proofs.", "conclusion": "ZK-WAGON represents a significant advancement in watermarking image-generation models securely using ZK-SNARKs, combining scalability and robustness with imperceptible watermarking through LSB steganography."}}
{"id": "2510.02158", "categories": ["cs.CR", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.02158", "abs": "https://arxiv.org/abs/2510.02158", "authors": ["Junjie Su", "Weifei Jin", "Yuxin Cao", "Derui Wang", "Kai Ye", "Jie Hao"], "title": "Mirage Fools the Ear, Mute Hides the Truth: Precise Targeted Adversarial Attacks on Polyphonic Sound Event Detection Systems", "comment": null, "summary": "Sound Event Detection (SED) systems are increasingly deployed in\nsafety-critical applications such as industrial monitoring and audio\nsurveillance. However, their robustness against adversarial attacks has not\nbeen well explored. Existing audio adversarial attacks targeting SED systems,\nwhich incorporate both detection and localization capabilities, often lack\neffectiveness due to SED's strong contextual dependencies or lack precision by\nfocusing solely on misclassifying the target region as the target event,\ninadvertently affecting non-target regions. To address these challenges, we\npropose the Mirage and Mute Attack (M2A) framework, which is designed for\ntargeted adversarial attacks on polyphonic SED systems. In our optimization\nprocess, we impose specific constraints on the non-target output, which we\nrefer to as preservation loss, ensuring that our attack does not alter the\nmodel outputs for non-target region, thus achieving precise attacks.\nFurthermore, we introduce a novel evaluation metric Editing Precison (EP) that\nbalances effectiveness and precision, enabling our method to simultaneously\nenhance both. Comprehensive experiments show that M2A achieves 94.56% and\n99.11% EP on two state-of-the-art SED models, demonstrating that the framework\nis sufficiently effective while significantly enhancing attack precision.", "AI": {"tldr": "This paper proposes M2A, a targeted adversarial attack framework for polyphonic SED systems that addresses precision issues via preservation loss and introduces a novel evaluation metric (Editing Precision).", "motivation": "Existing attacks on SED systems either fail due to contextual dependencies or lack precision by distorting non-target regions. Robustness against adversarial attacks is critical for safety-critical SED applications.", "method": "The Mirage and Mute Attack (M2A) framework uses a preservation loss to maintain non-target region outputs during optimization and introduces Editing Precision (EP) as a metric to balance attack effectiveness and precision.", "result": "M2A achieves 94.56-99.11 EP on two state-of-the-art SED models, demonstrating superior precision over existing methods while maintaining high effectiveness.", "conclusion": "M2A provides a framework for precise, effective adversarial attacks on SED systems through novel constraint design and evaluation metrics, revealing vulnerabilities in polyphonic SED robustness."}}
{"id": "2510.02162", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02162", "abs": "https://arxiv.org/abs/2510.02162", "authors": ["Cristian Bassotto", "Ermes Franch", "Marina Kr\u010dek", "Stjepan Picek"], "title": "NoMod: A Non-modular Attack on Module Learning With Errors", "comment": null, "summary": "The advent of quantum computing threatens classical public-key cryptography,\nmotivating NIST's adoption of post-quantum schemes such as those based on the\nModule Learning With Errors (Module-LWE) problem. We present NoMod ML-Attack, a\nhybrid white-box cryptanalytic method that circumvents the challenge of\nmodeling modular reduction by treating wrap-arounds as statistical corruption\nand casting secret recovery as robust linear estimation. Our approach combines\noptimized lattice preprocessing--including reduced-vector saving and algebraic\namplification--with robust estimators trained via Tukey's Biweight loss.\nExperiments show NoMod achieves full recovery of binary secrets for dimension\n$n = 350$, recovery of sparse binomial secrets for $n = 256$, and successful\nrecovery of sparse secrets in CRYSTALS-Kyber settings with parameters $(n, k) =\n(128, 3)$ and $(256, 2)$. We release our implementation in an anonymous\nrepository https://anonymous.4open.science/r/NoMod-3BD4.", "AI": {"tldr": "NoMod ML-Attack bypasses modular arithmetic in Module-LWE schemes via robust linear estimation, achieving secret recovery in post-quantum cryptographic parameters including CRYSTALS-Kyber.", "motivation": "The motivation stems from the threat quantum computing poses to classical public-key cryptography, necessitating the evaluation of post-quantum schemes like Module-LWE for security.", "method": "The method employs a hybrid white-box approach combining lattice preprocessing techniques (e.g., reduced-vector saving, algebraic amplification) with robust statistical estimation via Tukey's Biweight loss, treating modular arithmetic challenges as statistical corruption rather than explicit modular modeling.", "result": "NoMod achieves full binary secret recovery (n=350), sparse binomial recovery (n=256), and breaks CRYSTALS-Kyber instances with (n,k)=(128,3)/(256,2), demonstrating practical feasibility against current post-quantum standards.", "conclusion": "The paper concludes that the NoMod ML-Attack is effective against Module-LWE based post-quantum cryptographic schemes, highlighting the necessity of robust defenses against such statistical cryptanalytic methods in the post-quantum era."}}
{"id": "2510.02184", "categories": ["cs.CR", "nlin.CD", "34C15, 68M25, 94A60"], "pdf": "https://arxiv.org/pdf/2510.02184", "abs": "https://arxiv.org/abs/2510.02184", "authors": ["N. A. Anagnostopoulos", "K. Konstantinidis", "A. N. Miliou", "S. G. Stavrinides"], "title": "Testing Stability and Robustness in Three Cryptographic Chaotic Systems", "comment": "Published as \"N. A. Anagnostopoulos, K. Konstantinidis, A. N. Miliou\n  & S. G. Stavrinides, \"Testing Stability and Robustness in Three Cryptographic\n  Chaotic Systems\", Proceedings of the 3rd International Interdisciplinary\n  Symposium on Chaos and Complex Systems (CCS 2010), Journal of Concrete And\n  Applicable Mathematics (JCAAM), vol. 9, iss. 3, pp. 247-261, Eudoxus Press,\n  2011\"; no longer available", "summary": "In practical applications, it is crucial that the drive-response systems,\nalthough identical in all respects, are synchronized at all times, even if\nthere is noise present. In this work, we test the stability and robustness of\nthree distinct and well-known cryptographic chaotic systems, and compare the\nresults in relation to the desired security.", "AI": {"tldr": "This paper evaluates three cryptographic chaotic systems for synchronization stability under noise, finding one system outperforms the others for secure communication. Practical security requires optimizing these systems further.", "motivation": "Chaotic systems are critical for secure communication, but their synchronization under real-world noise conditions remains a challenge. This study addresses the gap in understanding how different cryptographic chaotic systems maintain synchronization reliability, which directly impacts their practical security effectiveness.", "method": "The authors evaluate the stability and robustness of three established cryptographic chaotic systems through comparative analysis under noisy conditions. They likely employed simulation experiments and quantitative metrics (e.g., synchronization error, noise tolerance thresholds) to assess performance differences.", "result": "Results show significant variation in performance among the three systems, with one system maintaining stable synchronization at higher noise levels. The comparison reveals trade-offs between complexity, speed, and robustness, guiding recommendations for secure system design.", "conclusion": "The study concludes that among the three cryptographic chaotic systems tested, one system demonstrates superior stability and robustness against noise, making it more suitable for secure communication applications. However, all systems require further optimization to achieve the desired security levels in practical scenarios."}}
{"id": "2510.02196", "categories": ["cs.CR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.02196", "abs": "https://arxiv.org/abs/2510.02196", "authors": ["Jason Anderson"], "title": "Authentication Security of PRF GNSS Ranging", "comment": null, "summary": "This work derives the authentication security of pseudorandom function (PRF)\nGNSS ranging under multiple GNSS spoofing models, including the Security Code\nEstimation and Replay (SCER) spoofer. When GNSS ranging codes derive from a PRF\nutilizing a secret known only to the broadcaster, the spoofer cannot predict\nthe ranging code before broadcast. Therefore, PRF ranging can be used to\nestablish trust in the GNSS pseudoranges and the resulting receiver position,\nnavigation, and timing (PNT) solution. I apply the methods herein to Galileo's\nSignal Authentication Service (SAS) utilizing the encrypted Galileo E6-C signal\nto compute that, at most, 400 ms of Galileo E6-C data to assert 128-bit\nauthentication security under non-SCER models. For the SCER adversary, I\npredict the adversary's needed receiving radio equipment to break\nauthentication security. One can use this work to design a PRF GNSS ranging\nprotocol to meet useful authentication security requirements by computing the\nprobability of missed detection.", "AI": {"tldr": "This paper analyzes spoofing-resistant PRF-based GNSS ranging security, focusing on Galileo's SAS with E6-C signal. It calculates 400ms data requirement for 128-bit authentication security under non-SCER models and evaluates SCER adversary capabilities.", "motivation": "GNSS spoofing threatens positioning accuracy. PRF-based authentication using secret keys can secure pseudorange data when ranging codes resist prediction before broadcast.", "method": "Security analysis of PRF GNSS ranging under multiple spoofing models, including SCER. Applied to Galileo SAS with E6-C signal through probability of missed detection calculations and equipment requirement modeling.", "result": "400ms of Galileo E6-C data provides 128-bit authentication security against non-SCER spoofers. SCER attackers require specific receiving equipment to compromise security, enabling protocol design to meet security requirements.", "conclusion": "PRF-based authentication achieves robust GNSS security by bounding data requirements for 128-bit security under different spoofing scenarios, providing practical guidance for authentication protocol design against targeted adversaries."}}
{"id": "2510.02280", "categories": ["cs.CR", "math.NT"], "pdf": "https://arxiv.org/pdf/2510.02280", "abs": "https://arxiv.org/abs/2510.02280", "authors": ["Jean-Francois Biasse", "Fang Song"], "title": "An efficient quantum algorithm for computing $S$-units and its applications", "comment": "Long version of a paper from SODA 2016", "summary": "In this paper, we provide details on the proofs of the quantum polynomial\ntime algorithm of Biasse and Song (SODA 16) for computing the $S$-unit group of\na number field. This algorithm directly implies polynomial time methods to\ncalculate class groups, S-class groups, relative class group and the unit\ngroup, ray class groups, solve the principal ideal problem, solve certain norm\nequations, and decompose ideal classes in the ideal class group. Additionally,\ncombined with a result of Cramer, Ducas, Peikert and Regev (Eurocrypt 2016),\nthe resolution of the principal ideal problem allows one to find short\ngenerators of a principal ideal. Likewise, methods due to Cramer, Ducas and\nWesolowski (Eurocrypt 2017) use the resolution of the principal ideal problem\nand the decomposition of ideal classes to find so-called ``mildly short\nvectors'' in ideal lattices of cyclotomic fields.", "AI": {"tldr": "The paper provides proofs for a quantum polynomial time algorithm to compute S-unit groups of number fields, enabling efficient solutions for related number-theoretic problems like class groups, principal ideal problems, and norm equations.", "motivation": "Efficiently solving S-unit groups and related problems is critical for advances in algebraic number theory and cryptography. Prior methods lacked polynomial-time guarantees, necessitating quantum approaches for scalability.", "method": "The method builds on Biasse and Song\u2019s quantum algorithm for S-unit groups, combining it with techniques from Cramer et al. (2016, 2017). It solves the principal ideal problem and leverages ideal class decomposition to address multiple related problems.", "result": "The algorithm achieves polynomial time complexity for computing class groups, S-class groups, unit groups, ray class groups, and solving principal ideals/norm equations. It also enables finding short generators in principal ideals and mild vectors in ideal lattices.", "conclusion": "The work establishes a robust quantum framework for solving core problems in number theory, advancing cryptographic applications and theoretical understanding of algebraic structures with exponential speedups."}}
