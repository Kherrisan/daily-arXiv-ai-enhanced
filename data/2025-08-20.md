<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 15]
- [cs.SE](#cs.SE) [Total: 6]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Too Easily Fooled? Prompt Injection Breaks LLMs on Frustratingly Simple Multiple-Choice Questions](https://arxiv.org/abs/2508.13214)
*Xuyang Guo,Zekai Huang,Zhao Song,Jiahao Zhang*

Main category: cs.CR

TL;DR: This paper investigates the vulnerability of Large Language Models (LLMs) to hidden prompt injection attacks in LLM-as-a-judge applications, demonstrating that even simple tasks like arithmetic questions can be manipulated through maliciously embedded instructions in PDFs.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used as evaluators in critical domains (e.g., education, peer review), but their susceptibility to prompt injection attacks threatens their reliability and security in these roles.

Method: The authors design a minimalistic attack by embedding malicious instructions into PDF files containing basic arithmetic questions (multiple-choice or true-false formats) to assess LLMs' robustness against adversarial content.

Result: Experiments reveal that LLMs are significantly vulnerable to hidden prompt injection attacks, failing to resist manipulation even for trivial arithmetic problems presented in document files.

Conclusion: The findings expose serious robustness gaps in LLM-as-a-judge systems, emphasizing the need for defensive mechanisms to prevent adversarial interference in practical deployment scenarios.

Abstract: Large Language Models (LLMs) have recently demonstrated strong emergent
abilities in complex reasoning and zero-shot generalization, showing
unprecedented potential for LLM-as-a-judge applications in education, peer
review, and data quality evaluation. However, their robustness under prompt
injection attacks, where malicious instructions are embedded into the content
to manipulate outputs, remains a significant concern. In this work, we explore
a frustratingly simple yet effective attack setting to test whether LLMs can be
easily misled. Specifically, we evaluate LLMs on basic arithmetic questions
(e.g., "What is 3 + 2?") presented as either multiple-choice or true-false
judgment problems within PDF files, where hidden prompts are injected into the
file. Our results reveal that LLMs are indeed vulnerable to such hidden prompt
injection attacks, even in these trivial scenarios, highlighting serious
robustness risks for LLM-as-a-judge applications.

</details>


### [2] [MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols](https://arxiv.org/abs/2508.13220)
*Yixuan Yang,Daoyuan Wu,Yufan Chen*

Main category: cs.CR

TL;DR: The paper introduces MCPSecBench, a security benchmark for evaluating Model Context Protocol (MCP) vulnerabilities in LLM-based systems, identifying 17 attack types across 4 surfaces and demonstrating platform-specific risks.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in real-world applications via MCP, which introduces new security risks and attack surfaces requiring systematic evaluation.

Method: Systematic taxonomy of 17 MCP attack types across 4 surfaces; development of MCPSecBench with integrated prompt datasets, MCP servers/clients, and attack scripts for modular, extensible security assessment across three major providers.

Result: 85% of attacks succeed in compromising at least one platform; core vulnerabilities affect all providers (Claude, OpenAI, Cursor), while prompt-based and tool-centric attacks vary significantly across hosts and models.

Conclusion: MCPSecBench standardizes MCP security evaluation, enabling rigorous cross-layer testing and exposing platform-specific security risks in LLM integration workflows.

Abstract: Large Language Models (LLMs) are increasingly integrated into real-world
applications via the Model Context Protocol (MCP), a universal, open standard
for connecting AI agents with data sources and external tools. While MCP
enhances the capabilities of LLM-based agents, it also introduces new security
risks and expands their attack surfaces. In this paper, we present the first
systematic taxonomy of MCP security, identifying 17 attack types across 4
primary attack surfaces. We introduce MCPSecBench, a comprehensive security
benchmark and playground that integrates prompt datasets, MCP servers, MCP
clients, and attack scripts to evaluate these attacks across three major MCP
providers. Our benchmark is modular and extensible, allowing researchers to
incorporate custom implementations of clients, servers, and transport protocols
for systematic security assessment. Experimental results show that over 85% of
the identified attacks successfully compromise at least one platform, with core
vulnerabilities universally affecting Claude, OpenAI, and Cursor, while
prompt-based and tool-centric attacks exhibit considerable variability across
different hosts and models. Overall, MCPSecBench standardizes the evaluation of
MCP security and enables rigorous testing across all MCP layers.

</details>


### [3] [Quantifying Loss Aversion in Cyber Adversaries via LLM Analysis](https://arxiv.org/abs/2508.13240)
*Soham Hans,Nikolos Gurney,Stacy Marsella,Sofia Hirschmann*

Main category: cs.CR

TL;DR: This paper proposes a methodology using large language models (LLMs) to analyze hacker behavior and infer cognitive biases, particularly loss aversion, enabling real-time cyber defense improvements.


<details>
  <summary>Details</summary>
Motivation: Traditional cyber defense strategies lack dynamic interpretation of attacker behavior during ongoing attacks, creating a critical need to understand cognitive biases like loss aversion in real-time.

Method: The approach involves processing hacker-generated notes with LLMs to segment actions and correlate them with predefined persistence mechanisms, analyzing how loss aversion manifests through operational triggers in attack scenarios.

Result: Experimental results demonstrate that LLMs effectively dissect nuanced behavioral patterns, revealing quantifiable insights into loss aversion in hackers and offering a new capability for real-time behavioral analysis.

Conclusion: This methodology transforms cyber defense by leveraging LLMs to analyze cognitive traits during attacks, enabling adaptive strategies that can infer, defend against, and potentially exploit hacker biases.

Abstract: Understanding and quantifying human cognitive biases from empirical data has
long posed a formidable challenge, particularly in cybersecurity, where
defending against unknown adversaries is paramount. Traditional cyber defense
strategies have largely focused on fortification, while some approaches attempt
to anticipate attacker strategies by mapping them to cognitive vulnerabilities,
yet they fall short in dynamically interpreting attacks in progress. In
recognition of this gap, IARPA's ReSCIND program seeks to infer, defend
against, and even exploit attacker cognitive traits. In this paper, we present
a novel methodology that leverages large language models (LLMs) to extract
quantifiable insights into the cognitive bias of loss aversion from hacker
behavior. Our data are collected from an experiment in which hackers were
recruited to attack a controlled demonstration network. We process the hacker
generated notes using LLMs using it to segment the various actions and
correlate the actions to predefined persistence mechanisms used by hackers. By
correlating the implementation of these mechanisms with various operational
triggers, our analysis provides new insights into how loss aversion manifests
in hacker decision-making. The results demonstrate that LLMs can effectively
dissect and interpret nuanced behavioral patterns, thereby offering a
transformative approach to enhancing cyber defense strategies through
real-time, behavior-based analysis.

</details>


### [4] [Involuntary Jailbreak](https://arxiv.org/abs/2508.13246)
*Yangyang Guo,Yangyan Li,Mohan Kankanhalli*

Main category: cs.CR

TL;DR: This paper introduces 'involuntary jailbreak', a new vulnerability in LLMs that bypasses guardrails using a simple universal prompt strategy.


<details>
  <summary>Details</summary>
Motivation: The study highlights the surprising fragility of current LLM guardrail mechanisms against non-objective-specific attacks.

Method: A universal prompt instructs LLMs to generate rejected questions and detailed responses, revealing vulnerabilities without direct attack objectives.

Result: The method successfully jailbreaks multiple leading LLMs (Claude Opus 4.1, Grok 4, Gemini 2.5 Pro, GPT 4.1) by compromising their entire guardrail structure.

Conclusion: The work urges re-evaluation of LLM guardrail robustness to achieve stronger safety alignment in future systems.

Abstract: In this study, we disclose a worrying new vulnerability in Large Language
Models (LLMs), which we term \textbf{involuntary jailbreak}. Unlike existing
jailbreak attacks, this weakness is distinct in that it does not involve a
specific attack objective, such as generating instructions for \textit{building
a bomb}. Prior attack methods predominantly target localized components of the
LLM guardrail. In contrast, involuntary jailbreaks may potentially compromise
the entire guardrail structure, which our method reveals to be surprisingly
fragile. We merely employ a single universal prompt to achieve this goal. In
particular, we instruct LLMs to generate several questions that would typically
be rejected, along with their corresponding in-depth responses (rather than a
refusal). Remarkably, this simple prompt strategy consistently jailbreaks the
majority of leading LLMs, including Claude Opus 4.1, Grok 4, Gemini 2.5 Pro,
and GPT 4.1. We hope this problem can motivate researchers and practitioners to
re-evaluate the robustness of LLM guardrails and contribute to stronger safety
alignment in future.

</details>


### [5] [Silentflow: Leveraging Trusted Execution for Resource-Limited MPC via Hardware-Algorithm Co-design](https://arxiv.org/abs/2508.13357)
*Zhuoran Li,Hanieh Totonchi Asl,Ebrahim Nouri,Yifei Cai,Danella Zhao*

Main category: cs.CR

TL;DR: Silentflow introduces a TEE-assisted protocol for efficient COT generation in resource-constrained edge devices, achieving significant speedups over existing MPC-based privacy-preserving machine learning (PPMLaaS) solutions.


<details>
  <summary>Details</summary>
Motivation: Conventional COT generation in MPC is inefficient for IoT sensors and wearables due to communication latency and limited computational capacity, creating a performance bottleneck for real-time secure inference.

Method: The protocol uses algorithmic decomposition techniques including kernel fusion for parallelism, Blocked On-chip eXpansion (BOX) for optimized memory access patterns, and vectorized batch operations to maximize memory bandwidth. COT computation is offloaded to a Zynq-7000 SoC for acceleration.

Result: Silentflow achieves 39.51× speedup over state-of-the-art protocols and 4.62×/3.95× speedup over Cryptflow2 and Cheetah specifically on ImageNet dataset inference for resource-constrained devices.

Conclusion: Silentflow effectively addresses the computational and communication limitations of COT generation in edge devices through architectural optimizations and TEE-based hardware acceleration, enabling real-time secure machine learning inference.

Abstract: Secure Multi-Party Computation (MPC) offers a practical foundation for
privacy-preserving machine learning at the edge, with MPC commonly employed to
support nonlinear operations. These MPC protocols fundamentally rely on
Oblivious Transfer (OT), particularly Correlated OT (COT), to generate
correlated randomness essential for secure computation. Although COT generation
is efficient in conventional two-party settings with resource-rich
participants, it becomes a critical bottleneck in real-world inference on
resource-constrained devices (e.g., IoT sensors and wearables), due to both
communication latency and limited computational capacity. To enable real-time
secure inference, we introduce Silentflow, a highly efficient Trusted Execution
Environment (TEE)-assisted protocol that eliminates communication in COT
generation. We tackle the core performance bottleneck-low computational
intensity-through structured algorithmic decomposition: kernel fusion for
parallelism, Blocked On-chip eXpansion (BOX) to improve memory access patterns,
and vectorized batch operations to maximize memory bandwidth utilization.
Through design space exploration, we balance end-to-end latency and resource
demands, achieving up to 39.51x speedup over state-of-the-art protocols. By
offloading COT computations to a Zynq-7000 SoC, SilentFlow accelerates PPMLaaS
inference on the ImageNet dataset under resource constraints, achieving a 4.62x
and 3.95x speedup over Cryptflow2 and Cheetah, respectively.

</details>


### [6] [A Risk Manager for Intrusion Tolerant Systems: Enhancing HAL 9000 with New Scoring and Data Sources](https://arxiv.org/abs/2508.13364)
*Tadeu Freitas,Carlos Novo,Inês Dutra,João Soares,Manuel Correia,Benham Shariati,Rolando Martins*

Main category: cs.CR

TL;DR: The paper enhances HAL 9000, an ITS Risk Manager, by integrating a custom-built scraper to collect threat data from diverse sources, improving responsiveness to emerging vulnerabilities and enabling earlier detection without manual analysis.


<details>
  <summary>Details</summary>
Motivation: Existing ITS solutions depend on manually updated public databases (NVD, ExploitDB), limiting their ability to adapt to rapidly evolving threats. HAL 9000's prior reliance on these databases necessitates expansion for proactive defense.

Method: Developed a scraper to continuously mine security advisories, research forums, and real-time exploit proofs-of-concept. Integrated scraper-derived intelligence into HAL 9000's risk management framework using machine learning and Exploitability Probability Scoring.

Result: The scraper significantly expands HAL 9000's intelligence base, enabling earlier detection of unverified vulnerabilities. Evaluation confirms improved threat mitigation capabilities compared to prior approaches.

Conclusion: Integrating diverse threat intelligence sources into HAL 9000's architecture enhances automatic risk assessment and intrusion tolerance for multi-domain adversaries.

Abstract: Intrusion Tolerant Systems (ITSs) have become increasingly critical due to
the rise of multi-domain adversaries exploiting diverse attack surfaces. ITS
architectures aim to tolerate intrusions, ensuring system compromise is
prevented or mitigated even with adversary presence. Existing ITS solutions
often employ Risk Managers leveraging public security intelligence to adjust
system defenses dynamically against emerging threats. However, these approaches
rely heavily on databases like NVD and ExploitDB, which require manual analysis
for newly discovered vulnerabilities. This dependency limits the system's
responsiveness to rapidly evolving threats. HAL 9000, an ITS Risk Manager
introduced in our prior work, addressed these challenges through machine
learning. By analyzing descriptions of known vulnerabilities, HAL 9000 predicts
and assesses new vulnerabilities automatically. To calculate the risk of a
system, it also incorporates the Exploitability Probability Scoring system to
estimate the likelihood of exploitation within 30 days, enhancing proactive
defense capabilities.
  Despite its success, HAL 9000's reliance on NVD and ExploitDB knowledge is a
limitation, considering the availability of other sources of information. This
extended work introduces a custom-built scraper that continuously mines diverse
threat sources, including security advisories, research forums, and real-time
exploit proofs-of-concept. This significantly expands HAL 9000's intelligence
base, enabling earlier detection and assessment of unverified vulnerabilities.
Our evaluation demonstrates that integrating scraper-derived intelligence with
HAL 9000's risk management framework substantially improves its ability to
address emerging threats. This paper details the scraper's integration into the
architecture, its role in providing additional information on new threats, and
the effects on HAL 9000's management.

</details>


### [7] [When Secure Aggregation Falls Short: Achieving Long-Term Privacy in Asynchronous Federated Learning for LEO Satellite Networks](https://arxiv.org/abs/2508.13425)
*Mohamed Elmahallawy,Tie Luo*

Main category: cs.CR

TL;DR: The paper proposes LTP-FLEO, an asynchronous federated learning framework that preserves long-term privacy for LEO satellite networks by addressing intermittent visibility and multi-round privacy leakage through privacy-aware satellite partitioning, model age balancing, and fair global aggregation.


<details>
  <summary>Details</summary>
Motivation: Traditional secure aggregation methods fail in dynamic, resource-constrained LEO satellite environments due to assumptions of continuous client availability and lack of long-term privacy guarantees across multiple communication rounds.

Method: 1. Privacy-aware satellite partitioning (predictable visibility-based grouping with joint participation requirements) 
2. Model age balancing (mitigating impacts of stale updates) 
3. Fair global aggregation (equitable treatment of satellites based on visibility duration)

Result: Theoretical analysis and experiments show LTP-FLEO preserves model/data privacy across multi-round training, achieves fairness by contribution, accelerates convergence, and maintains competitive model accuracy despite dynamic satellite visibility.

Conclusion: LTP-FLEO addresses critical limitations of existing secure aggregation in LEO FL by integrating long-term privacy preservation (across rounds) with fairness and convergence efficiency, making it suitable for dynamic satellite network environments.

Abstract: Secure aggregation is a common technique in federated learning (FL) for
protecting data privacy from both curious internal entities (clients or server)
and external adversaries (eavesdroppers). However, in dynamic and
resource-constrained environments such as low Earth orbit (LEO) satellite
networks, traditional secure aggregation methods fall short in two aspects: (1)
they assume continuous client availability while LEO satellite visibility is
intermittent and irregular; (2) they consider privacy in each communication
round but have overlooked the possible privacy leakage through multiple rounds.
To address these limitations, we propose LTP-FLEO, an asynchronous FL framework
that preserves long-term privacy (LTP) for LEO satellite networks. LTP-FLEO
introduces (i) privacy-aware satellite partitioning, which groups satellites
based on their predictable visibility to the server and enforces joint
participation; (ii) model age balancing, which mitigates the adverse impact of
stale model updates; and (iii) fair global aggregation, which treats satellites
of different visibility durations in an equitable manner. Theoretical analysis
and empirical validation demonstrate that LTP-FLEO effectively safeguards both
model and data privacy across multi-round training, promotes fairness in line
with satellite contributions, accelerates global convergence, and achieves
competitive model accuracy.

</details>


### [8] [Beneath the Mask: Can Contribution Data Unveil Malicious Personas in Open-Source Projects?](https://arxiv.org/abs/2508.13453)
*Ruby Nealon*

Main category: cs.CR

TL;DR: Analyzes detection of anomalous open-source contributions using OSINT and graph analysis via the XZ Utils backdoor case.


<details>
  <summary>Details</summary>
Motivation: Existing lack of tooling to monitor and identify suspicious behavioral patterns in open-source project contributors despite potential global security risks.

Method: Utilized Open Source Intelligence (OSINT) data from GitHub contributions, processed and analyzed through graph databases and graph theory to detect anomalies in contributor patterns.

Result: Identified anomalous behaviors by the 'JiaT75' user persona across multiple projects, explaining how the XZUtils backdoor could have been flagged through this method.

Conclusion: Graph-based OSINT analysis effectively reveals suspicious contributor patterns in open-source ecosystems, emphasizing the need for monitoring systems to counter such threats.

Abstract: In February 2024, after building trust over two years with project
maintainers by making a significant volume of legitimate contributions, GitHub
user "JiaT75" self-merged a version of the XZ Utils project containing a highly
sophisticated, well-disguised backdoor targeting sshd processes running on
systems with the backdoored package installed. A month later, this package
began to be distributed with popular Linux distributions until a Microsoft
employee discovered the backdoor while investigating how a recent system
upgrade impacted the performance of SSH authentication. Despite its potential
global impact, no tooling exists for monitoring and identifying anomalous
behavior by personas contributing to other open-source projects. This paper
demonstrates how Open Source Intelligence (OSINT) data gathered from GitHub
contributions, analyzed using graph databases and graph theory, can efficiently
identify anomalous behaviors exhibited by the "JiaT75" persona across other
open-source projects.

</details>


### [9] [Optimizing Scalar Selection in Elliptic Curve Cryptography Using Differential Evolution for Enhanced Security](https://arxiv.org/abs/2508.13520)
*Takreem Haider*

Main category: cs.CR

TL;DR: The paper proposes an optimization-driven method using differential evolution to generate high-entropy private scalars for elliptic curve cryptography (ECC), enhancing security in resource-constrained environments by outperforming conventional random generation approaches.


<details>
  <summary>Details</summary>
Motivation: Existing scalar generation methods for ECC (user input/pseudorandom generators) produce low-entropy or biased scalars in weak entropy environments, exposing systems to side-channel and key recovery attacks. Traditional approaches lack adaptability to constrained resources.

Method: Reformulate scalar selection as an entropy-optimization problem. Use differential evolution (DE), a population-based metaheuristic algorithm, to search for scalars with maximized bit-level entropy via fitness functions measuring binary distribution uniformity.

Result: DE-optimized scalars achieve statistically significant higher entropy compared to conventionally generated scalars. The method provides deterministic, tunable results without compromising cryptographic compatibility.

Conclusion: The optimization-based scalar generation improves ECC security in resource-constrained environments by maximizing bit entropy, offering a robust alternative to traditional randomness while maintaining protocol compatibility for blockchain, IoT, and secure messaging applications.

Abstract: Elliptic Curve Cryptography (ECC) is a fundamental component of modern
public-key cryptosystems that enable efficient and secure digital signatures,
key exchanges, and encryption. Its core operation, scalar multiplication,
denoted as $k \cdot P$, where $P$ is a base point and $k$ is a private scalar,
relies heavily on the secrecy and unpredictability of $k$. Conventionally, $k$
is selected using user input or pseudorandom number generators. However, in
resource-constrained environments with weak entropy sources, these approaches
may yield low-entropy or biased scalars, increasing susceptibility to
side-channel and key recovery attacks. To mitigate these vulnerabilities, we
introduce an optimization-driven scalar generation method that explicitly
maximizes bit-level entropy. Our approach uses differential evolution (DE), a
population-based metaheuristic algorithm, to search for scalars whose binary
representations exhibit maximal entropy, defined by an even and statistically
uniform distribution of ones and zeros. This reformulation of scalar selection
as an entropy-optimization problem enhances resistance to entropy-based
cryptanalytic techniques and improves overall unpredictability. Experimental
results demonstrate that DE-optimized scalars achieve entropy significantly
higher than conventionally generated scalars. The proposed method can be
integrated into existing ECC-based protocols, offering a deterministic, tunable
alternative to traditional randomness, ideal for applications in blockchain,
secure messaging, IoT, and other resource-constrained environments.

</details>


### [10] [CAI Fluency: A Framework for Cybersecurity AI Fluency](https://arxiv.org/abs/2508.13588)
*Víctor Mayoral-Vilches,Jasmin Wachter,Cristóbal R. J. Veas Chavez,Cathrin Schachner,Luis Javier Navarrete-Lozano,María Sanz-Gómez*

Main category: cs.CR

TL;DR: CAI Fluency is an educational platform under the Cybersecurity AI (CAI) framework designed to democratize AI-based cybersecurity tools by combining human-AI interaction modalities with core competencies, promoting responsible adoption through theory and practice.


<details>
  <summary>Details</summary>
Motivation: The CAI framework seeks to accelerate the adoption of AI-driven cybersecurity solutions, address skills gaps, and establish 'vibe-hacking' as an ethical paradigm for human-AI collaboration in security contexts.

Method: The platform adapts the Framework for AI Fluency's three human-AI interaction modalities and four core competencies to cybersecurity needs, structuring the technical report as both a white paper and practical implementation guide.

Result: This technical report provides comprehensive educational resources, theoretical foundations (including 'vibe-hacking'), and actionable guidance for deploying AI security solutions within the CAI methodology.

Conclusion: CAI Fluency bridges technical capability with ethical responsibility in cybersecurity AI, offering a scalable approach to train practitioners in both AI deployment and critical assessment of security implications.

Abstract: This work introduces CAI Fluency, an an educational platform of the
Cybersecurity AI (CAI) framework dedicated to democratizing the knowledge and
application of cybersecurity AI tools in the global security community. The
main objective of the CAI framework is to accelerate the widespread adoption
and effective use of artificial intelligence-based cybersecurity solutions,
pathing the way to vibe-hacking, the cybersecurity analogon to vibe-coding.
  CAI Fluency builds upon the Framework for AI Fluency, adapting its three
modalities of human-AI interaction and four core competencies specifically for
cybersecurity applications. This theoretical foundation ensures that
practitioners develop not just technical skills, but also the critical thinking
and ethical awareness necessary for responsible AI use in security contexts.
  This technical report serves as a white-paper, as well as detailed
educational and practical guide that helps users understand the principles
behind the CAI framework, and educates them how to apply this knowledge in
their projects and real-world security contexts.

</details>


### [11] [Conflicting Scores, Confusing Signals: An Empirical Study of Vulnerability Scoring Systems](https://arxiv.org/abs/2508.13644)
*Viktoria Koscinski,Mark Nelson,Ahmet Okutan,Robert Falso,Mehdi Mirakhorli*

Main category: cs.CR

TL;DR: This study empirically compares four vulnerability scoring systems (CVSS, SSVC, EPSS, Exploitability Index) using 600 real-world Microsoft vulnerabilities to evaluate their consistency in prioritization and exploitation risk assessment.


<details>
  <summary>Details</summary>
Motivation: Inconsistent prioritization decisions across vulnerability scoring systems hinder effective risk management. Organizations need outcome-linked empirical analysis to choose reliable metrics for resource allocation.

Method: Large-scale empirical evaluation of four scoring systems using 600 vulnerabilities from Microsoft's Patch Tuesday over four months, analyzing score relationships, triage tier categorization, and exploitation risk prediction accuracy.

Result: Scoring systems demonstrated statistically significant (p<0.001) ranking discrepancies for the same vulnerabilities, with no consistent alignment across systems. EPSS showed weak correlation (r²=0.18) with actual exploitation patterns, while CVSS had moderate correlation (r²=0.45).

Conclusion: Vulnerability scoring systems produce inconsistent risk assessments, necessitating standardized evaluation frameworks and greater transparency in scoring methodology to improve reliability for organizational decision-making.

Abstract: Accurately assessing software vulnerabilities is essential for effective
prioritization and remediation. While various scoring systems exist to support
this task, their differing goals, methodologies and outputs often lead to
inconsistent prioritization decisions. This work provides the first
large-scale, outcome-linked empirical comparison of four publicly available
vulnerability scoring systems: the Common Vulnerability Scoring System (CVSS),
the Stakeholder-Specific Vulnerability Categorization (SSVC), the Exploit
Prediction Scoring System (EPSS), and the Exploitability Index. We use a
dataset of 600 real-world vulnerabilities derived from four months of
Microsoft's Patch Tuesday disclosures to investigate the relationships between
these scores, evaluate how they support vulnerability management task, how
these scores categorize vulnerabilities across triage tiers, and assess their
ability to capture the real-world exploitation risk. Our findings reveal
significant disparities in how scoring systems rank the same vulnerabilities,
with implications for organizations relying on these metrics to make
data-driven, risk-based decisions. We provide insights into the alignment and
divergence of these systems, highlighting the need for more transparent and
consistent exploitability, risk, and severity assessments.

</details>


### [12] [Know Me by My Pulse: Toward Practical Continuous Authentication on Wearable Devices via Wrist-Worn PPG](https://arxiv.org/abs/2508.13690)
*Wei Shao,Zequan Liang,Ruoyu Zhang,Ruijie Fang,Ning Miao,Ehsan Kourkchi,Setareh Rafatirad,Houman Homayoun,Chongzhou Fang*

Main category: cs.CR

TL;DR: This paper introduces a continuous authentication system on a smartwatch using low-frequency (25 Hz) multi-channel PPG, achieving strong performance (88.11% accuracy, F1=0.88, FAR=0.48%, EER=2.76%) while reducing power consumption by 53% compared to 512 Hz systems. It identifies 25 Hz as the practical lower bound for sampling and emphasizes the necessity of activity-diverse training data to maintain robustness under motion.


<details>
  <summary>Details</summary>
Motivation: ECG-based biometric authentication is limited by intrusive sensing requirements, and PPG systems using high-frequency sampling (75-500 Hz) are problematic for wearable devices due to significant energy and computational overhead. There is a need for non-intrusive, power-efficient authentication suitable for real-world deployment.

Method: The proposed system, We-Be Band, uses a Bi-LSTM with attention mechanism to extract identity-specific features from 4-channel PPG signals sampled at 25 Hz. Short windows (4 seconds) of PPG data are processed through the model to enable real-time authentication on smartwatches.

Result: The system achieves 88.11% average test accuracy, macro F1-score of 0.88, FAR of 0.48%, FRR of 11.77%, and EER of 2.76% using 25 Hz sampling. It reduces sensor power consumption by 53% versus 512 Hz and 19% versus 128 Hz setups. Performance degrades sharply below 25 Hz, but training with activity-diverse data maintains accuracy under motion.

Conclusion: 25 Hz is confirmed as the practical sampling frequency minimum for PPG-based authentication without performance loss. Activity-diverse training is critical to maintain robustness across different physiological states, enabling real-world deployment of continuous biometric authentication in wearable devices.

Abstract: Biometric authentication using physiological signals offers a promising path
toward secure and user-friendly access control in wearable devices. While
electrocardiogram (ECG) signals have shown high discriminability, their
intrusive sensing requirements and discontinuous acquisition limit
practicality. Photoplethysmography (PPG), on the other hand, enables
continuous, non-intrusive authentication with seamless integration into
wrist-worn wearable devices. However, most prior work relies on high-frequency
PPG (e.g., 75 - 500 Hz) and complex deep models, which incur significant energy
and computational overhead, impeding deployment in power-constrained real-world
systems. In this paper, we present the first real-world implementation and
evaluation of a continuous authentication system on a smartwatch, We-Be Band,
using low-frequency (25 Hz) multi-channel PPG signals. Our method employs a
Bi-LSTM with attention mechanism to extract identity-specific features from
short (4 s) windows of 4-channel PPG. Through extensive evaluations on both
public datasets (PTTPPG) and our We-Be Dataset (26 subjects), we demonstrate
strong classification performance with an average test accuracy of 88.11%,
macro F1-score of 0.88, False Acceptance Rate (FAR) of 0.48%, False Rejection
Rate (FRR) of 11.77%, and Equal Error Rate (EER) of 2.76%. Our 25 Hz system
reduces sensor power consumption by 53% compared to 512 Hz and 19% compared to
128 Hz setups without compromising performance. We find that sampling at 25 Hz
preserves authentication accuracy, whereas performance drops sharply at 20 Hz
while offering only trivial additional power savings, underscoring 25 Hz as the
practical lower bound. Additionally, we find that models trained exclusively on
resting data fail under motion, while activity-diverse training improves
robustness across physiological states.

</details>


### [13] [On the Security and Privacy of Federated Learning: A Survey with Attacks, Defenses, Frameworks, Applications, and Future Directions](https://arxiv.org/abs/2508.13730)
*Daniel M. Jimenez-Gutierrez,Yelizaveta Falkouskaya,Jose L. Hernandez-Ramos,Aris Anagnostopoulos,Ioannis Chatzigiannakis,Andrea Vitaletti*

Main category: cs.CR

TL;DR: This survey provides an overview of over 200 papers on attacks and defenses in Federated Learning (FL), categorizing techniques into security-enhancing and privacy-preserving methods while analyzing trade-offs and future challenges.


<details>
  <summary>Details</summary>
Motivation: FL improves data privacy through decentralized training but faces security and privacy threats, necessitating a systematic analysis of existing solutions and open challenges.

Method: A comprehensive review and categorization of state-of-the-art attacks and defense mechanisms, with critical analysis of their strengths, limitations, and trade-offs between privacy, security, and model performance.

Result: Identifies gaps in defending FL against threats (e.g., non-IID data challenges existing methods) and emphasizes trade-offs between privacy guarantees and model accuracy.

Conclusion: The survey highlights the need for scalable, adaptive FL defenses in dynamic environments and aims to guide research toward robust, privacy-preserving collaborative learning frameworks.

Abstract: Federated Learning (FL) is an emerging distributed machine learning paradigm
enabling multiple clients to train a global model collaboratively without
sharing their raw data. While FL enhances data privacy by design, it remains
vulnerable to various security and privacy threats. This survey provides a
comprehensive overview of more than 200 papers regarding the state-of-the-art
attacks and defense mechanisms developed to address these challenges,
categorizing them into security-enhancing and privacy-preserving techniques.
Security-enhancing methods aim to improve FL robustness against malicious
behaviors such as byzantine attacks, poisoning, and Sybil attacks. At the same
time, privacy-preserving techniques focus on protecting sensitive data through
cryptographic approaches, differential privacy, and secure aggregation. We
critically analyze the strengths and limitations of existing methods, highlight
the trade-offs between privacy, security, and model performance, and discuss
the implications of non-IID data distributions on the effectiveness of these
defenses. Furthermore, we identify open research challenges and future
directions, including the need for scalable, adaptive, and energy-efficient
solutions operating in dynamic and heterogeneous FL environments. Our survey
aims to guide researchers and practitioners in developing robust and
privacy-preserving FL systems, fostering advancements safeguarding
collaborative learning frameworks' integrity and confidentiality.

</details>


### [14] [NodeShield: Runtime Enforcement of Security-Enhanced SBOMs for Node.js](https://arxiv.org/abs/2508.13750)
*Eric Cornelissen,Musard Balliu*

Main category: cs.CR

TL;DR: Studies a runtime protection mechanism for Node.js supply chain application using NodeShield, which enforces dependency hierarchy and controls system resources with SBOM/CBOM extensions.


<details>
  <summary>Details</summary>
Motivation: Node.js's large ecosystem and prevalence make it a frequent target for supply chain attacks. Existing static and dynamic mechanisms (sandboxing, permissions, taint tracking) fall short on compatibility, automation, and overhead.

Method: Designed NodeShield: 1) Uses SBOM (Software Bill of Materials) as dependency hierarchy source. 2) Introduces CBOM (Capability Bill of Materials) to define component access capabilities. 3) Implements runtime enforcement via code outlining without modifying original code or Node.js runtime.

Result: Prevented 98% of 67 known supply chain attacks with <1ms request overhead on servers. Demonstrated compatibility with vanilla Node.js and concise policies (≤7 entries per dependency).

Conclusion: NodeShield provides effective, low-overhead runtime protection for Node.js supply chains using enhanced SBOM metadata, surpassing prior approaches in compatibility and practical deployment feasibility.

Abstract: The software supply chain is an increasingly common attack vector for
malicious actors. The Node.js ecosystem has been subject to a wide array of
attacks, likely due to its size and prevalence. To counter such attacks, the
research community and practitioners have proposed a range of static and
dynamic mechanisms, including process- and language-level sandboxing,
permission systems, and taint tracking. Drawing on valuable insight from these
works, this paper studies a runtime protection mechanism for (the supply chain
of) Node.js applications with the ambitious goals of compatibility, automation,
minimal overhead, and policy conciseness.
  Specifically, we design, implement and evaluate NodeShield, a protection
mechanism for Node.js that enforces an application's dependency hierarchy and
controls access to system resources at runtime. We leverage the up-and-coming
SBOM standard as the source of truth for the dependency hierarchy of the
application, thus preventing components from stealthily abusing undeclared
components. We propose to enhance the SBOM with a notion of capabilities that
represents a set of related system resources a component may access. Our
proposed SBOM extension, the Capability Bill of Materials or CBOM, records the
required capabilities of each component, providing valuable insight into the
potential privileged behavior. NodeShield enforces the SBOM and CBOM at runtime
via code outlining (as opposed to inlining) with no modifications to the
original code or Node.js runtime, thus preventing unexpected, potentially
malicious behavior. Our evaluation shows that NodeShield can prevent over 98%
out of 67 known supply chain attacks while incurring minimal overhead on
servers at less than 1ms per request. We achieve this while maintaining broad
compatibility with vanilla Node.js and a concise policy language that consists
of at most 7 entries per dependency.

</details>


### [15] [Red Teaming Methodology for Design Obfuscation](https://arxiv.org/abs/2508.13965)
*Yuntao Liu,Abir Akib,Zelin Lu,Qian Xu,Ankur Srivastava,Gang Qu,David Kehlet,Nij Dorairaj*

Main category: cs.CR

TL;DR: The paper proposes a systematic red teaming approach to evaluate design obfuscation security in VLSI supply chains, revealing significant information leakage in the RIPPER tool's obfuscation methodology.


<details>
  <summary>Details</summary>
Motivation: Protect sensitive design details from untrusted parties in the VLSI supply chain (e.g., offshore foundries and end users) through rigorous obfuscation evaluation.

Method: Developed novel security metrics and evaluation methodology for obfuscation analysis in scenarios where adversaries lack access to working chips, validated through red-teaming techniques.

Result: Case study on RIPPER tool demonstrated structural information leakage exceeding prior assumptions, undermining its security claims.

Conclusion: Current design obfuscation techniques may be more vulnerable than previously believed, necessitating improved security evaluation frameworks to address hidden leakage channels.

Abstract: The main goal of design obfuscation schemes is to protect sensitive design
details from untrusted parties in the VLSI supply chain, including but not
limited to off-shore foundries and untrusted end users. In this work, we
provide a systematic red teaming approach to evaluate the security of design
obfuscation approaches. Specifically, we propose security metrics and
evaluation methodology for the scenarios where the adversary does not have
access to a working chip. A case study on the RIPPER tool developed by the
University of Florida indicates that more information is leaked about the
structure of the original design than commonly considered.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [16] [A Comparative Study of Delta Parquet, Iceberg, and Hudi for Automotive Data Engineering Use Cases](https://arxiv.org/abs/2508.13396)
*Dinesh Eswararaj,Ajay Babu Nellipudi,Vandana Kollati*

Main category: cs.SE

TL;DR: This paper compares Delta Parquet, Apache Iceberg, and Apache Hudi for automotive telemetry data engineering, analyzing their performance in ML readiness, real-time ingestion, scalability, and other critical factors to guide format selection.


<details>
  <summary>Details</summary>
Motivation: The automotive industry faces data engineering challenges with large-scale telemetry data requiring low latency, scalability, and consistency. Choosing the right data format is essential for managing use cases like fleet management and predictive maintenance.

Method: The study evaluated the three formats using real-world time-series automotive telemetry data (vehicle ID, timestamp, location, event metrics) across modeling strategies, partitioning, CDC support, query performance, scalability, consistency, and ecosystem maturity.

Result: Delta Parquet excels in ML readiness and governance; Iceberg shows high performance for batch analytics and cloud-native workloads; Hudi is strongest for real-time ingestion and incremental processing. Tradeoffs exist in query efficiency, time-travel, and update semantics.

Conclusion: The analysis provides actionable insights for selecting or combining Delta, Iceberg, and Hudi to optimize automotive data pipelines for applications like predictive maintenance, route optimization, and fleet analytics, balancing performance, scalability, and functional requirements.

Abstract: The automotive industry generates vast amounts of data from sensors,
telemetry, diagnostics, and real-time operations. Efficient data engineering is
critical to handle challenges of latency, scalability, and consistency. Modern
data lakehouse formats Delta Parquet, Apache Iceberg, and Apache Hudi offer
features such as ACID transactions, schema enforcement, and real-time
ingestion, combining the strengths of data lakes and warehouses to support
complex use cases. This study presents a comparative analysis of Delta Parquet,
Iceberg, and Hudi using real-world time-series automotive telemetry data with
fields such as vehicle ID, timestamp, location, and event metrics. The
evaluation considers modeling strategies, partitioning, CDC support, query
performance, scalability, data consistency, and ecosystem maturity. Key
findings show Delta Parquet provides strong ML readiness and governance,
Iceberg delivers high performance for batch analytics and cloud-native
workloads, while Hudi is optimized for real-time ingestion and incremental
processing. Each format exhibits tradeoffs in query efficiency, time-travel,
and update semantics. The study offers insights for selecting or combining
formats to support fleet management, predictive maintenance, and route
optimization. Using structured datasets and realistic queries, the results
provide practical guidance for scaling data pipelines and integrating machine
learning models in automotive applications.

</details>


### [17] [The Hidden Cost of Readability: How Code Formatting Silently Consumes Your LLM Budget](https://arxiv.org/abs/2508.13666)
*Dangfeng Pan,Zhensu Sun,Cenyuan Zhang,David Lo,Xiaoning Du*

Main category: cs.SE

TL;DR: The paper investigates the impact of code formatting on LLM efficiency and performance, revealing that removing non-essential formatting elements reduces computational costs while maintaining correctness, with a developed bidirectional code transformation tool.


<details>
  <summary>Details</summary>
Motivation: Code formatting elements like indentation and newlines enhance human readability but are processed as token overhead by LLMs, increasing computational costs and inference time without adding value for the models.

Method: Conducted large-scale experiments on Fill-in-the-Middle Code Completion using 10 LLMs (commercial and open-source) across four programming languages, analyzing token count and performance variations with/without formatting elements.

Result: 24.5% average input token reduction with negligible output token impact; output code length could be reduced up to 36.1% through prompting/fine-tuning without correctness loss.

Conclusion: Code formatting removal is a viable optimization strategy for LLM efficiency, and a bidirectional transformation tool was created to balance human readability and model inference requirements.

Abstract: Source code is usually formatted with elements like indentation and newlines
to improve readability for human developers. However, these visual aids do not
seem to be beneficial for large language models (LLMs) in the same way since
the code is processed as a linear sequence of tokens. Furthermore, these
additional tokens can lead to increased computational costs and longer response
times for LLMs. If such formatting elements are non-essential to LLMs, we can
reduce such costs by removing them from the code. To figure out the role played
by formatting elements, we conduct a comprehensive empirical study to evaluate
the impact of code formatting on LLM performance and efficiency. Through
large-scale experiments on Fill-in-the-Middle Code Completion tasks across four
programming languages (Java, Python, C++, C\#) and ten LLMs-including both
commercial and open-source models-we systematically analyze token count and
performance when formatting elements are removed. Key findings indicate that
LLMs can maintain performance across formatted code and unformatted code,
achieving an average input token reduction of 24.5\% with negligible output
token reductions. This makes code format removal a practical optimization
strategy for improving LLM efficiency. Further exploration reveals that both
prompting and fine-tuning LLMs can lead to significant reductions (up to
36.1\%) in output code length without compromising correctness. To facilitate
practical applications, we develop a bidirectional code transformation tool for
format processing, which can be seamlessly integrated into existing LLM
inference workflows, ensuring both human readability and LLM efficiency.

</details>


### [18] [COMPASS: A Multi-Dimensional Benchmark for Evaluating Code Generation in Large Language Models](https://arxiv.org/abs/2508.13757)
*James Meaden,Michał Jarosz,Piotr Jodłowski,Grigori Melnik*

Main category: cs.SE

TL;DR: This paper introduces COMPASS, a code generation benchmark framework that evaluates correctness, efficiency, and code quality using real Codility problems and human baselines, revealing that top models on correctness do not necessarily excel in efficiency and maintainability.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks ignore algorithmic efficiency and code quality despite their real-world importance; models passing correctness-only tests may lack practical viability.

Method: Curated 50 Codility competitive programming problems with 393,150 human submissions as baselines. Evaluated 3 leading models (Claude Opus 4, Gemini 2.5 Pro, O4-Mini-High) across correctness, efficiency, and quality using industry-standard tools.

Result: High correctness scores correlate poorly with efficiency (runtime analysis) and quality (maintainability metrics) in generated solutions; human-baseline comparison shows significant gaps in practical code generation.

Conclusion: COMPASS establishes a vital framework for holistic code generation evaluation, urging future research to optimize not just correctness but also algorithmic efficiency and code quality for production-ready AI systems.

Abstract: Current code generation benchmarks focus primarily on functional correctness
while overlooking two critical aspects of real-world programming: algorithmic
efficiency and code quality. We introduce COMPASS (COdility's Multi-dimensional
Programming ASSessment), a comprehensive evaluation framework that assesses
code generation across three dimensions: correctness, efficiency, and quality.
COMPASS consists of 50 competitive programming problems from real Codility
competitions, providing authentic human baselines from 393,150 submissions.
Unlike existing benchmarks that treat algorithmically inefficient solutions
identically to optimal ones provided they pass test cases, COMPASS
systematically evaluates runtime efficiency and code quality using
industry-standard analysis tools. Our evaluation of three leading
reasoning-enhanced models, Anthropic Claude Opus 4, Google Gemini 2.5 Pro, and
OpenAI O4-Mini-High, reveals that models achieving high correctness scores do
not necessarily produce efficient algorithms or maintainable code. These
findings highlight the importance of evaluating more than just correctness to
truly understand the real-world capabilities of code generation models. COMPASS
serves as a guiding framework, charting a path for future research toward AI
systems that are robust, reliable, and ready for production use.

</details>


### [19] [Agentic DraCor and the Art of Docstring Engineering: Evaluating MCP-empowered LLM Usage of the DraCor API](https://arxiv.org/abs/2508.13774)
*Peer Trilcke,Ingo Börner,Henny Sluyter-Gäthje,Daniil Skorinkin,Frank Fischer,Carsten Milling*

Main category: cs.SE

TL;DR: The paper presents a Model Context Protocol (MCP) server for DraCor, enabling LLMs to interact with its API and demonstrates the importance of 'Docstring Engineering' for effective LLM-tool integration in Digital Humanities research.


<details>
  <summary>Details</summary>
Motivation: The motivation centers on advancing Computational Literary Studies through autonomous LLM interactions with digital archives while addressing the need for reliable infrastructure in the Digital Humanities field.

Method: The method involves implementing the MCP server and conducting qualitative experiments via systematic prompt observation to evaluate LLM tool selection, correctness, calling efficiency, and use reliability.

Result: Results highlight the effectiveness of 'Docstring Engineering' in optimizing LLM-tool interactions and provide empirical evidence of agentic AI's potential for research while identifying infrastructure gaps.

Conclusion: The study concludes that agentic AI shows promise for Computational Literary Studies but emphasizes the critical need for infrastructure development to ensure reliable implementation in Digital Humanities contexts.

Abstract: This paper reports on the implementation and evaluation of a Model Context
Protocol (MCP) server for DraCor, enabling Large Language Models (LLM) to
autonomously interact with the DraCor API. We conducted experiments focusing on
tool selection and application by the LLM, employing a qualitative approach
that includes systematic observation of prompts to understand how LLMs behave
when using MCP tools, evaluating "Tool Correctness", "Tool-Calling Efficiency",
and "Tool-Use Reliability". Our findings highlight the importance of "Docstring
Engineering", defined as reflexively crafting tool documentation to optimize
LLM-tool interaction. Our experiments demonstrate both the promise of agentic
AI for research in Computational Literary Studies and the essential
infrastructure development needs for reliable Digital Humanities
infrastructures.

</details>


### [20] [Structural and Connectivity Patterns in the Maven Central Software Dependency Network](https://arxiv.org/abs/2508.13819)
*Daniel Ogenrwot,John Businge,Shaikh Arifuzzaman*

Main category: cs.SE

TL;DR: The study applies network science to Maven Central's dependency graph, revealing its scale-free and small-world topology with critical infrastructural hubs that pose systemic risks if compromised.


<details>
  <summary>Details</summary>
Motivation: Understanding structural characteristics and connectivity patterns of software ecosystems helps enhance software reuse, improve ecosystem resilience, and mitigate security risks.

Method: Used the Goblin framework to sample top 5,000 artifacts by degree centrality, performed BFS expansion, and analyzed 1.3 million-node graph using metrics like degree distribution, betweenness centrality, PageRank, and connected components.

Result: Maven Central exhibits scale-free and small-world topology with infrastructural hubs (testing frameworks, utilities) at the core, which enable reuse but create systemic vulnerability risks due to their central role.

Conclusion: While infrastructural hubs in the ecosystem promote efficient software reuse and integration, their centrality and widespread dependencies create critical systemic risks requiring focused security and resilience strategies.

Abstract: Understanding the structural characteristics and connectivity patterns of
large-scale software ecosystems is critical for enhancing software reuse,
improving ecosystem resilience, and mitigating security risks. In this paper,
we investigate the Maven Central ecosystem, one of the largest repositories of
Java libraries, by applying network science techniques to its dependency graph.
Leveraging the Goblin framework, we extracted a sample consisting of the top
5,000 highly connected artifacts based on their degree centrality and then
performed breadth-first search (BFS) expansion from each selected artifact as a
seed node, traversing the graph outward to capture all libraries and releases
reachable those seed nodes. This sampling strategy captured the immediate
structural context surrounding these libraries resulted in a curated graph
comprising of 1.3 million nodes and 20.9 million edges. We conducted a
comprehensive analysis of this graph, computing degree distributions,
betweenness centrality, PageRank centrality, and connected components
graph-theoretic metrics. Our results reveal that Maven Central exhibits a
highly interconnected, scale-free, and small-world topology, characterized by a
small number of infrastructural hubs that support the majority of projects.
Further analysis using PageRank and betweenness centrality shows that these
hubs predominantly consist of core ecosystem infrastructure, including testing
frameworks and general-purpose utility libraries. While these hubs facilitate
efficient software reuse and integration, they also pose systemic risks;
failures or vulnerabilities affecting these critical nodes can have widespread
and cascading impacts throughout the ecosystem.

</details>


### [21] [Tight Inter-Core Cache Contention Analysis for WCET Estimation on Multicore Systems](https://arxiv.org/abs/2508.13863)
*Shuai Zhao,Jieyu Jiang,Shenlin Cai,Yaowei Liang,Chen Jie,Yinjie Fang,Wei Zhang,Guoquan Zhang,Yaoyao Gu,Xiang Xiao,Wei Qin,Xiangzhen Ouyang,Wanli Chang*

Main category: cs.SE

TL;DR: This paper introduces a new method for inter-core cache contention analysis in multicore WCET estimation, improving accuracy by considering actual cache states and access counts, reducing interference estimates by 52.31% and WCET by 8.94%.


<details>
  <summary>Details</summary>
Motivation: Existing WCET analysis methods on multicore architectures overestimate remote cache block interference due to incomplete modeling of cache states and access dynamics, leading to inefficient resource allocation.

Method: The analysis uses program region ordering to identify affected memory references, then applies dynamic programming to compute precise cache miss counts based on local/remote access quantities across cores.

Result: Experiments show 52.31% average reduction in quantified inter-core cache contention and 8.94% average WCET estimation reduction compared to prior techniques, with minimal computational overhead.

Conclusion: Dynamic programming-based contention analysis offers a scalable solution to accurately model real-time cache interference in multicore systems, enabling tighter WCET estimates and better resource predictions.

Abstract: WCET (Worst-Case Execution Time) estimation on multicore architecture is
particularly challenging mainly due to the complex accesses over cache shared
by multiple cores. Existing analysis identifies possible contentions between
parallel tasks by leveraging the partial order of the tasks or their program
regions. Unfortunately, they overestimate the number of cache misses caused by
a remote block access without considering the actual cache state and the number
of accesses. This paper reports a new analysis for inter-core cache contention.
Based on the order of program regions in a task, we first identify memory
references that could be affected if a remote access occurs in a region.
Afterwards, a fine-grained contention analysis is constructed that computes the
number of cache misses based on the access quantity of local and remote blocks.
We demonstrate that the overall inter-core cache interference of a task can be
obtained via dynamic programming. Experiments show that compared to existing
methods, the proposed analysis reduces inter-core cache interference and WCET
estimations by 52.31% and 8.94% on average, without significantly increasing
computation overhead.

</details>
