{"id": "2508.11715", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11715", "abs": "https://arxiv.org/abs/2508.11715", "authors": ["Ananya Singha", "Harshita Sahijwani", "Walt Williams", "Emmanuel Aboah Boateng", "Nick Hausman", "Miguel Di Luca", "Keegan Choudhury", "Chaya Binet", "Vu Le", "Tianwei Chen", "Oryan Rokeah Chen", "Sulaiman Vesal", "Sadid Hasan"], "title": "Benchmark Dataset Generation and Evaluation for Excel Formula Repair with LLMs", "comment": "Accepted at the KDD workshop on Evaluation and Trustworthiness of\n  Agentic and Generative AI Models", "summary": "Excel is a pervasive yet often complex tool, particularly for novice users,\nwhere runtime errors arising from logical mistakes or misinterpretations of\nfunctions pose a significant challenge. While large language models (LLMs)\noffer promising assistance by explaining formula errors, the automated\ncorrection of these semantic runtime errors remains an open problem. A primary\nchallenge to advancing models for such scenarios is the severe lack of\nhigh-quality, comprehensive datasets for training and rigorous evaluation. This\npaper addresses this gap by introducing a novel approach for constructing a\nbenchmark dataset specifically designed for Excel formula repair. We propose a\ndata generation pipeline, which leverages a small set of curated seed samples\nfrom online forums to synthetically expand the dataset. Our pipeline integrates\nfew-shot prompting with LLMs and employs a robust \\textit{LLM-as-a-Judge}\nvalidation framework, combined with execution-based checks to ensure the\ncorrectness and semantic fidelity of the generated data. This process produced\na benchmark dataset of 618 high-quality samples, covering common runtime\nerrors. Furthermore, we propose a context-aware baseline technique for Excel\nformula repair that utilizes LLMs to leverage both the faulty formula, and\nrelevant spreadsheet context. We evaluate the performance of various LLMs\n(GPT-4o, GPT-4.1, Phi-3, Mistral) on our newly generated benchmark using\nexecution-based metrics. Our analysis demonstrates the dataset's quality\nthrough manual annotation and provides insights into error and function\ndistributions. The proposed generation methodology is highly scalable and can\nbe readily adapted to create evaluation benchmarks for similar code repair\ntasks in other low-resource programming languages.", "AI": {"tldr": "This paper introduces a novel benchmark dataset and context-aware baseline technique to address the lack of training and evaluation data for Excel formula repair using large language models (LLMs). The dataset includes 618 high-quality samples of runtime errors.", "motivation": "Excel's complexity leads to runtime errors for novice users, and while LLMs can explain errors, automating semantic corrections remains challenging due to the absence of high-quality, comprehensive datasets for training and evaluation.", "method": "The authors developed a data generation pipeline using curated seed samples from online forums, integrated few-shot prompting, LLM-as-a-Judge validation framework, and execution-based checks to ensure data quality. They also proposed a context-aware baseline technique that leverages both faulty formulas and spreadsheet context with LLMs.", "result": "Generated a 618-sample benchmark dataset covering common runtime errors. Evaluated LLMs (GPT-4o, GPT-4.1, Phi-3, Mistral) using execution-based metrics. Manual annotation confirmed dataset quality, and error/function distribution insights were provided.", "conclusion": "The proposed scalable methodology effectively creates high-quality evaluation benchmarks for Excel formula repair and can be adapted to similar code repair tasks in other underserved programming languages."}}
{"id": "2508.11717", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11717", "abs": "https://arxiv.org/abs/2508.11717", "authors": ["Dhruv Kolhatkar", "Soubhagya Akkena", "Edward F. Gehringer"], "title": "WIP: Leveraging LLMs for Enforcing Design Principles in Student Code: Analysis of Prompting Strategies and RAG", "comment": "Accepted for presentation at the Frontiers in Education Conference,\n  Nashville, Tennessee, USA, 2-5 November 2025", "summary": "This work-in-progress research-to-practice paper explores the integration of\nLarge Language Models (LLMs) into the code-review process for open-source\nsoftware projects developed in computer science and software engineering\ncourses. The focus is on developing an automated feedback tool that evaluates\nstudent code for adherence to key object-oriented design principles, addressing\nthe need for more effective and scalable methods to teach software design best\npractices. The innovative practice involves leveraging LLMs and\nRetrieval-Augmented Generation (RAG) to create an automated feedback system\nthat assesses student code for principles like SOLID, DRY, and design patterns.\nIt analyzes the effectiveness of various prompting strategies and the RAG\nintegration. Preliminary findings show promising improvements in code quality.\nFuture work will aim to improve model accuracy and expand support for\nadditional design principles.", "AI": {"tldr": "This paper investigates integrating Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) to develop an automated code-review tool for student software projects, focusing on evaluating adherence to object-oriented design principles like SOLID and DRY. Preliminary results show improved code quality potential with plans for model refinement and expanded principle support.", "motivation": "Traditional manual code-review processes for teaching software design principles are labor-intensive and often lack scalability to effectively teach and assess best practices in large classroom settings.", "method": "The approach combines LLMs with RAG to analyze student code against object-oriented design principles. The study evaluates different prompting strategies and integrates RAG for context-aware feedback.", "result": "Preliminary implementation demonstrates promising improvements in student code quality related to design principle adherence.", "conclusion": "The integration of LLMs and RAG shows potential for creating scalable, automated code-review tools. Future work will focus on enhancing model accuracy and expanding support for additional software design principles."}}
{"id": "2508.11824", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.11824", "abs": "https://arxiv.org/abs/2508.11824", "authors": ["Satyam Kumar Navneet", "Joydeep Chandra"], "title": "Rethinking Autonomy: Preventing Failures in AI-Driven Software Engineering", "comment": null, "summary": "The integration of Large Language Models (LLMs) into software engineering has\nrevolutionized code generation, enabling unprecedented productivity through\npromptware and autonomous AI agents. However, this transformation introduces\nsignificant risks, including insecure code generation, hallucinated outputs,\nirreversible actions, and a lack of transparency and accountability. Incidents\nlike the Replit database deletion underscore the urgent need for robust safety\nand governance mechanisms. This paper comprehensively analyzes the inherent\nchallenges of LLM-assisted code generation, such as vulnerability inheritance,\novertrust, misinterpretation, and the absence of standardized validation and\nrollback protocols. To address these, we propose the SAFE-AI Framework, a\nholistic approach emphasizing Safety, Auditability, Feedback, and\nExplainability. The framework integrates guardrails, sandboxing, runtime\nverification, risk-aware logging, human-in-the-loop systems, and explainable AI\ntechniques to mitigate risks while fostering trust and compliance. We introduce\na novel taxonomy of AI behaviors categorizing suggestive, generative,\nautonomous, and destructive actions to guide risk assessment and oversight.\nAdditionally, we identify open problems, including the lack of standardized\nbenchmarks for code specific hallucinations and autonomy levels, and propose\nfuture research directions for hybrid verification, semantic guardrails, and\nproactive governance tools. Through detailed comparisons of autonomy control,\nprompt engineering, explainability, and governance frameworks, this paper\nprovides a roadmap for responsible AI integration in software engineering,\naligning with emerging regulations like the EU AI Act and Canada's AIDA to\nensure safe, transparent, and accountable AI-driven development.", "AI": {"tldr": "This paper introduces the SAFE-AI Framework to address risks in LLM-assisted code generation, proposing safety, auditability, feedback, and explainability mechanisms while aligning with AI governance regulations.", "motivation": "LLMs in software engineering present risks like insecure code generation, hallucinations, and lack of governance, exemplified by incidents such as the Replit database deletion, necessitating robust safety and accountability measures.", "method": "Analyzes challenges in LLM code generation via vulnerability inheritance, overtrust, and misinterpretation; proposes SAFE-AI with guardrails, sandboxing, runtime verification, human-in-the-loop systems, and a new AI behavior taxonomy (suggestive/generative/autonomous/destructive).", "result": "Provides a comprehensive taxonomy for AI behaviors in code generation, outlines open problems (e.g., standardized benchmark gaps), and suggests future directions like hybrid verification and proactive governance tools.", "conclusion": "Establishes a roadmap for responsible AI integration in software engineering through technical framework components and regulatory alignment, emphasizing safe, transparent development practices aligned with the EU AI Act and AIDA."}}
{"id": "2508.11867", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11867", "abs": "https://arxiv.org/abs/2508.11867", "authors": ["Mohammad Baqar", "Saba Naqvi", "Rajat Khanda"], "title": "AI-Augmented CI/CD Pipelines: From Code Commit to Production with Autonomous Decisions", "comment": "13 Pages", "summary": "Modern software delivery has accelerated from quarterly releases to multiple\ndeployments per day. While CI/CD tooling has matured, human decision points\ninterpreting flaky tests, choosing rollback strategies, tuning feature flags,\nand deciding when to promote a canary remain major sources of latency and\noperational toil. We propose AI-Augmented CI/CD Pipelines, where large language\nmodels (LLMs) and autonomous agents act as policy-bounded co-pilots and\nprogressively as decision makers. We contribute: (1) a reference architecture\nfor embedding agentic decision points into CI/CD, (2) a decision taxonomy and\npolicy-as-code guardrail pattern, (3) a trust-tier framework for staged\nautonomy, (4) an evaluation methodology using DevOps Research and Assessment (\nDORA) metrics and AI-specific indicators, and (5) a detailed industrial-style\ncase study migrating a React 19 microservice to an AI-augmented pipeline. We\ndiscuss ethics, verification, auditability, and threats to validity, and chart\na roadmap for verifiable autonomy in production delivery systems.", "AI": {"tldr": "The paper introduces AI-Augmented CI/CD Pipelines that use large language models (LLMs) and autonomous agents as policy-bounded co-pilots/decision makers to address latency and operational toil caused by human interventions in modern rapid software delivery. It outlines a reference architecture, decision taxonomy, trust-tier framework, evaluation methodology integrating DORA metrics and AI-specific indicators, and an industrial case study.", "motivation": "Modern software delivery requiring faster deployments is hindered by human decision points related to flaky tests, rollback strategies, feature flags, and canary promotions, which create latency and operational toil despite mature CI/CD tooling.", "method": "Proposes AI-augmented pipelines with agentic decision points governed by trust-tier frameworks and policy-as-code guardrails, using LLMs to act as progressive co-pilots and decision makers. Presents an evaluation methodology combining DORA metrics and AI-specific indicators.", "result": "Five key contributions: (1) Reference architecture for agentic CI/CD integration, (2) Decision taxonomy with policy guardrails, (3) Trust-tier framework for staged autonomy, (4) Evaluation methodology combining traditional DevOps metrics with AI-specific indicators, (5) Industrial case study demonstrating migration of React 19 microservice to AI-augmented pipeline.", "conclusion": "Discusses ethical considerations, verification, auditability, and validity threats related to AI autonomy in production systems, while charting a roadmap toward verifiable autonomy in software delivery pipelines."}}
{"id": "2508.11710", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11710", "abs": "https://arxiv.org/abs/2508.11710", "authors": ["Hael Abdulhakim Ali Humran", "Ferdi Sonmez"], "title": "Code Vulnerability Detection Across Different Programming Languages with AI Models", "comment": null, "summary": "Security vulnerabilities present in a code that has been written in diverse\nprogramming languages are among the most critical yet complicated aspects of\nsource code to detect. Static analysis tools based on rule-based patterns\nusually do not work well at detecting the context-dependent bugs and lead to\nhigh false positive rates. Recent developments in artificial intelligence,\nspecifically the use of transformer-based models like CodeBERT and CodeLlama,\nprovide light to this problem, as they show potential in finding such flaws\nbetter. This paper presents the implementations of these models on various\ndatasets of code vulnerability, showing how off-the-shelf models can\nsuccessfully produce predictive capacity in models through dynamic fine-tuning\nof the models on vulnerable and safe code fragments. The methodology comprises\nthe gathering of the dataset, normalization of the language, fine-tuning of the\nmodel, and incorporation of ensemble learning and explainable AI. Experiments\nshow that a well-trained CodeBERT can be as good as or even better than some\nexisting static analyzers in terms of accuracy greater than 97%. Further study\nhas indicated that although language models can achieve close-to-perfect\nrecall, the precision can decrease. A solution to this is given by hybrid\nmodels and validation procedures, which will reduce false positives. According\nto the results, the AI-based solutions generalize to different programming\nlanguages and classes of vulnerability. Nevertheless, robustness,\ninterpretability, and deployment readiness are still being developed. The\nresults illustrate the probabilities that AI will enhance the trustworthiness\nin the usability and scalability of machine-learning-based detectors of\nvulnerabilities.", "AI": {"tldr": "This paper explores the use of transformer-based AI models (CodeBERT and CodeLlama) for code vulnerability detection, demonstrating high accuracy (>97%) via dynamic fine-tuning and hybrid approaches to address precision issues while highlighting generalization across languages and vulnerabilities.", "motivation": "Traditional rule-based static analysis tools struggle with context-dependent security vulnerabilities and produce high false positive rates, motivating the need for AI-driven solutions that handle code semantics more effectively.", "method": "The methodology involves dataset collection, language normalization, dynamic fine-tuning of transformer models on vulnerability data, and integration of ensemble learning and explainable AI techniques for improved detection.", "result": "CodeBERT achieves >97% accuracy matching/better static analyzers, exhibits near-perfect recall but lower precision, and demonstrates generalization across programming languages and vulnerability types when using hybrid models.", "conclusion": "AI-based vulnerability detectors show promise in scalability and usability but require further development in robustness, interpretability, and deployment readiness. Hybrid approaches reduce false positives while maintaining effectiveness."}}
{"id": "2508.11958", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11958", "abs": "https://arxiv.org/abs/2508.11958", "authors": ["Zhipeng Xue", "Xiaoting Zhang", "Zhipeng Gao", "Xing Hu", "Shan Gao", "Xin Xia", "Shanping Li"], "title": "Clean Code, Better Models: Enhancing LLM Performance with Smell-Cleaned Dataset", "comment": null, "summary": "The Large Language Models (LLMs) have demonstrated great potential in\ncode-related tasks. However, most research focuses on improving the output\nquality of LLMs (e.g., correctness), and less attention has been paid to the\nLLM input (e.g., the training code quality). Given that code smells are widely\nexisted in practice and can negatively impact software maintainability and\nreadability, this study takes the first systematic research to assess and\nimprove dataset quality in terms of code smells. In this work, we first conduct\na preliminary study to explore the presence of code smells in a popular\nbenchmark dataset (i.e., CodeSearchNet-Python}) and evaluate the output of\nseveral popular LLMs (i.e., DeepSeek-Coder, CodeLlama, and MagiCoder),\nrevealing that code smell issues extensively exist in LLM's input (e.g.,\nbenchmark dataset) and output (e.g., generated code). We then conduct our\nsystematic research by taking three main steps: Firstly, we propose an\nLLM-based code smell cleaning tool, named SmellCC, which automatically\nrefactors and removes code smells. To evaluate the correctness of the code\nrefactoring, we construct a test set of 50 repositories sourced from the\nCodeSearchNet-Python benchmark for functional testing. Then we apply our\ncurated smell-cleaned dataset to fine-tune two LLMs (i.e., DeepSeek-V2 and\nQwen-Coder) to explore their potential for generating high-quality code.\nThirdly, we investigate the impact of code smells on two downstream tasks: code\ncompletion and code search. Lastly, we derive several actionable implications\nfor software engineering researchers and industry practitioners from our\nfindings.", "AI": {"tldr": "This paper systematically analyzes code smells in benchmark datasets used for training Large Language Models (LLMs) and proposes SmellCC, an LLM-based tool to clean code smells, demonstrating how curated datasets improve LLM-generated code quality and downstream tasks like code completion and search.", "motivation": "Most LLM research focuses on output quality (e.g., correctness), but ignores the impact of code smells in training datasets on software maintainability, readability, and LLM performance.", "method": "1) Developed SmellCC for code smell refactoring. 2) Conducted functional testing on 50 CodeSearchNet-Python repositories. 3) Fine-tuned LLMs with cleaned datasets. 4) Evaluated effects on code completion and code search tasks.", "result": "Code smells were prevalent in LLM inputs and outputs; SmellCC improved refactoring correctness; models trained on cleaned datasets generated higher-quality code and showed performance gains in downstream tasks.", "conclusion": "Code smells in training datasets significantly affect LLMs and downstream code tasks. SmellCC offers a practical solution for dataset curation, and findings provide actionable guidelines for researchers and practitioners to prioritize code quality in data preparation."}}
{"id": "2508.11711", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11711", "abs": "https://arxiv.org/abs/2508.11711", "authors": ["Irash Perera", "Hiranya Abeyrathne", "Sanjeewa Malalgoda", "Arshardh Ifthikar"], "title": "Enhancing GraphQL Security by Detecting Malicious Queries Using Large Language Models, Sentence Transformers, and Convolutional Neural Networks", "comment": null, "summary": "GraphQL's flexibility, while beneficial for efficient data fetching,\nintroduces unique security vulnerabilities that traditional API security\nmechanisms often fail to address. Malicious GraphQL queries can exploit the\nlanguage's dynamic nature, leading to denial-of-service attacks, data\nexfiltration through injection, and other exploits. Existing solutions, such as\nstatic analysis, rate limiting, and general-purpose Web Application Firewalls,\noffer limited protection against sophisticated, context-aware attacks. This\npaper presents a novel, AI-driven approach for real-time detection of malicious\nGraphQL queries. Our method combines static analysis with machine learning\ntechniques, including Large Language Models (LLMs) for dynamic schema-based\nconfiguration, Sentence Transformers (SBERT and Doc2Vec) for contextual\nembedding of query payloads, and Convolutional Neural Networks (CNNs), Random\nForests, and Multilayer Perceptrons for classification. We detail the system\narchitecture, implementation strategies optimized for production environments\n(including ONNX Runtime optimization and parallel processing), and evaluate the\nperformance of our detection models and the overall system under load. Results\ndemonstrate high accuracy in detecting various threats, including SQL\ninjection, OS command injection, and XSS exploits, alongside effective\nmitigation of DoS and SSRF attempts. This research contributes a robust and\nadaptable solution for enhancing GraphQL API security.", "AI": {"tldr": "The paper introduces an AI-driven solution to detect malicious GraphQL queries in real-time using static analysis and machine learning (LLMs, SBERT, CNNs, etc.), along with optimizations for production environments.", "motivation": "GraphQL's flexibility poses unique security challenges unaddressed by traditional API mechanisms. Existing solutions like static analysis or rate limiting struggle with context-aware and sophisticated attacks such as denial-of-service, data exfiltration, and injection vulnerabilities.", "method": "Our approach integrates static analysis with AI techniques: LLMs generate dynamic schema-based configurations, sentence transformers (SBERT/Doc2Vec) create contextual query embeddings, and classifiers (CNNs, Random Forest, MLPs) identify malicious patterns. The system is optimized via ONNX Runtime and parallel processing for scalability.", "result": "Models achieved high accuracy in detecting SQL injection, OS command injection, XSS, and mitigating DoS/SSRF attacks. Evaluations under load confirmed effectiveness in production scenarios and real-time performance.", "conclusion": "The research provides a robust, adaptable framework for securing GraphQL APIs against both known and emerging threats, addressing critical limitations in current security tools through AI-driven real-time mitigation."}}
{"id": "2508.11993", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11993", "abs": "https://arxiv.org/abs/2508.11993", "authors": ["Kota Someya", "Lei Chen", "Michael J. Decker", "Shinpei Hayashi"], "title": "How Much Can a Behavior-Preserving Changeset Be Decomposed into Refactoring Operations?", "comment": "(C) 2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Developers sometimes mix behavior-preserving modifications, such as\nrefactorings, with behavior-altering modifications, such as feature additions.\nSeveral approaches have been proposed to support understanding such\nmodifications by separating them into those two parts. Such refactoring-aware\napproaches are expected to be particularly effective when the\nbehavior-preserving parts can be decomposed into a sequence of more primitive\nbehavior-preserving operations, such as refactorings, but this has not been\nexplored. In this paper, as an initial validation, we quantify how much of the\nbehavior-preserving modifications can be decomposed into refactoring operations\nusing a dataset of functionally-equivalent method pairs. As a result, when\nusing an existing refactoring detector, only 33.9% of the changes could be\nidentified as refactoring operations. In contrast, when including 67 newly\ndefined functionally-equivalent operations, the coverage increased by over\n128%. Further investigation into the remaining unexplained differences was\nconducted, suggesting improvement opportunities.", "AI": {"tldr": "The paper evaluates the effectiveness of decomposition techniques for behavior-preserving modifications using a dataset of functionally-equivalent method pairs, revealing significant coverage improvements with expanded refactoring operations but also highlighting unresolved gaps.", "motivation": "Existing methods for separating behavior-preserving and behavior-altering code changes lack validation on how effectively behavior-preserving modifications can be decomposed into refactoring operations. Understanding and improving this decomposition is critical for tools that require accurate differentiation of code changes.", "method": "The authors use a dataset of functionally-equivalent method pairs to quantitatively assess decomposition into refactoring operations. They first apply an existing refactoring detector and then compare its coverage with a 67-operation expanded refactoring set, analyzing unexplained differences to identify improvement opportunities.", "result": "Only 33.9% of behavior-preserving changes were detected as refactorings by existing tools, but adding 67 new operations raised this coverage by over 128%. Further analysis pinpointed specific areas where detection remains incomplete, suggesting systematic enhancements may be feasible.", "conclusion": "This initial validation demonstrates that expanding functionally-equivalent refactoring operations significantly improves coverage of behavior-preserving changes, while the existence of unexplained differences implies ongoing research potential to refine detection accuracy and address current limitations."}}
{"id": "2508.11716", "categories": ["cs.CR", "cs.AI", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.11716", "abs": "https://arxiv.org/abs/2508.11716", "authors": ["Javier Mu\u00f1oz-Haro", "Ruben Tolosana", "Ruben Vera-Rodriguez", "Aythami Morales", "Julian Fierrez"], "title": "Privacy-Aware Detection of Fake Identity Documents: Methodology, Benchmark, and Improved Detection Methods (FakeIDet2)", "comment": null, "summary": "Remote user verification in Internet-based applications is becoming\nincreasingly important nowadays. A popular scenario for it consists of\nsubmitting a picture of the user's Identity Document (ID) to a service\nplatform, authenticating its veracity, and then granting access to the\nrequested digital service. An ID is well-suited to verify the identity of an\nindividual, since it is government issued, unique, and nontransferable.\nHowever, with recent advances in Artificial Intelligence (AI), attackers can\nsurpass security measures in IDs and create very realistic physical and\nsynthetic fake IDs. Researchers are now trying to develop methods to detect an\never-growing number of these AI-based fakes that are almost indistinguishable\nfrom authentic (bona fide) IDs. In this counterattack effort, researchers are\nfaced with an important challenge: the difficulty in using real data to train\nfake ID detectors. This real data scarcity for research and development is\noriginated by the sensitive nature of these documents, which are usually kept\nprivate by the ID owners (the users) and the ID Holders (e.g., government,\npolice, bank, etc.). The main contributions of our study are: 1) We propose and\ndiscuss a patch-based methodology to preserve privacy in fake ID detection\nresearch. 2) We provide a new public database, FakeIDet2-db, comprising over\n900K real/fake ID patches extracted from 2,000 ID images, acquired using\ndifferent smartphone sensors, illumination and height conditions, etc. In\naddition, three physical attacks are considered: print, screen, and composite.\n3) We present a new privacy-aware fake ID detection method, FakeIDet2. 4) We\nrelease a standard reproducible benchmark that considers physical and synthetic\nattacks from popular databases in the literature.", "AI": {"tldr": "This study proposes FakeIDet2, a privacy-preserving method for detecting AI-generated fake IDs using patch-based training. They release a public database (FakeIDet2-db) with 900K real/fake ID patches and a reproducible benchmark framework.", "motivation": "The increasing threat of AI-based ID forgeries (print, screen, composite attacks) and the challenge of accessing real ID data for research due to privacy constraints necessitate new approaches to fake ID detection.", "method": "The authors developed: 1) A patch-based methodology to anonymize ID data while preserving detectable features 2) FakeIDet2-db: A database of 900K+ patches from 2,000 ID images under varied capture conditions 3) FakeIDet2: A detection method using these patches 4) A benchmark combining physical/synthetic attacks from existing datasets", "result": "Created a public dataset with patches from 2,000 ID images covering physical attacks and release a detection method (FakeIDet2) that works effectively with fragmented data while maintaining privacy.", "conclusion": "This work addresses the privacy-data scarcity challenge in fake ID detection by enabling research with patch-based representation and provides community resources to advance security solutions against AI-based forgeries."}}
{"id": "2508.12232", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12232", "abs": "https://arxiv.org/abs/2508.12232", "authors": ["Arshia Akhavan", "Alireza Hosseinpour", "Abbas Heydarnoori", "Mehdi Keshani"], "title": "LinkAnchor: An Autonomous LLM-Based Agent for Issue-to-Commit Link Recovery", "comment": null, "summary": "Issue-to-commit link recovery plays an important role in software\ntraceability and improves project management. However, it remains a challenging\ntask. A study on GitHub shows that only 42.2% of the issues are correctly\nlinked to their commits. This highlights the potential for further development\nand research in this area. Existing studies have employed various AI/ML-based\napproaches, and with the recent development of large language models,\nresearchers have leveraged LLMs to tackle this problem. These approaches suffer\nfrom two main issues. First, LLMs are constrained by limited context windows\nand cannot ingest all of the available data sources, such as long commit\nhistories, extensive issue comments, and large code repositories. Second, most\nmethods operate on individual issue-commit pairs; that is, given a single\nissue-commit pair, they determine whether the commit resolves the issue. This\nquickly becomes impractical in real-world repositories containing tens of\nthousands of commits. To address these limitations, we present LinkAnchor, the\nfirst autonomous LLM-based agent designed for issue-to-commit link recovery.\nThe lazy-access architecture of LinkAnchor enables the underlying LLM to access\nthe rich context of software, spanning commits, issue comments, and code files,\nwithout exceeding the token limit by dynamically retrieving only the most\nrelevant contextual data. Additionally, LinkAnchor is able to automatically\npinpoint the target commit rather than exhaustively scoring every possible\ncandidate. Our evaluations show that LinkAnchor outperforms state-of-the-art\nissue-to-commit link recovery approaches by 60-262% in Hit@1 score across all\nour case study projects. We also publicly release LinkAnchor as a ready-to-use\ntool, along with our replication package. LinkAnchor is designed and tested for\nGitHub and Jira, and is easily extendable to other platforms.", "AI": {"tldr": "LinkAnchor is an autonomous LLM-based agent for issue-to-commit link recovery that uses a lazy-access architecture to efficiently retrieve relevant context and identify target commits, achieving 60-262% improvement in Hit@1 scores over existing methods.", "motivation": "Current LLM-based approaches for issue-to-commit link recovery face two major challenges: limited context windows of LLMs prevent handling of extensive data sources, and pairwise methods are impractical for large repositories with many commits. This paper aims to address these limitations to improve software traceability and project management.", "method": "LinkAnchor employs a lazy-access architecture to dynamically retrieve only the most relevant contextual data (commits, issue comments, code files) without exceeding token limits, and automatically identifies target commits instead of exhaustively scoring every pair.", "result": "LinkAnchor outperformed state-of-the-art methods by 60-262% in Hit@1 scores across case study projects, demonstrating significant improvements in link recovery accuracy. The tool is released publicly for GitHub and Jira-compatible platforms.", "conclusion": "LinkAnchor offers a scalable and efficient solution for issue-to-commit traceability. The open-source tool, tested on GitHub and Jira, provides practical benefits for real-world software repositories while setting a new benchmark for this task."}}
{"id": "2508.11742", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.11742", "abs": "https://arxiv.org/abs/2508.11742", "authors": ["Minhao Jin", "Hongyu He", "Maria Apostolaki"], "title": "Assessing User Privacy Leakage in Synthetic Packet Traces: An Attack-Grounded Approach", "comment": null, "summary": "Current synthetic traffic generators (SynNetGens) promise privacy but lack\ncomprehensive guarantees or empirical validation, even as their fidelity\nsteadily improves. We introduce the first attack-grounded benchmark for\nassessing the privacy of SynNetGens directly from the traffic they produce. We\nframe privacy as membership inference at the traffic-source level--a realistic\nand actionable threat for data holders. To this end, we present TraceBleed, the\nfirst attack that exploits behavioral fingerprints across flows using\ncontrastive learning and temporal chunking, outperforming prior membership\ninference baselines by 172%. Our large-scale study across GAN-, diffusion-, and\nGPT-based SynNetGens uncovers critical insights: (i) SynNetGens leak user-level\ninformation; (ii) differential privacy either fails to stop these attacks or\nseverely degrades fidelity; and (iii) sharing more synthetic data amplifies\nleakage by 59% on average. Finally, we introduce TracePatch, the first\nSynNetGen-agnostic defense that combines adversarial ML with SMT constraints to\nmitigate leakage while preserving fidelity.", "AI": {"tldr": "Introduces first privacy benchmark for SynNetGens using membership inference attacks, identifies critical privacy vulnerabilities in existing methods through empirical analysis, and proposes TracePatch as a model-agnostic defense solution.", "motivation": "Existing synthetic traffic generators (SynNetGens) lack concrete privacy guarantees despite improved data fidelity, necessitating a practical method to evaluate their privacy risks empirically.", "method": "Developed TraceBleed, a membership inference attack based on contrastive learning and temporal chunking to detect traffic-source leakage. Conducted large-scale experiments across GAN, diffusion, and GPT-based SynNetGens to quantify privacy risks.", "result": "SynNetGens leak user-level information (172% better attack accuracy than previous methods), differential privacy degrades fidelity or fails, and increased synthetic data sharing raises leakage by 59%. TracePatch successfully mitigates leakage while preserving data fidelity.", "conclusion": "TraceBleed establishes a new benchmark for assessing SynNetGen privacy, revealing critical vulnerabilities. TracePatch provides the first generalizable defense strategy that balances privacy and fidelity without architectural assumptions about the generator model."}}
{"id": "2508.12285", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.12285", "abs": "https://arxiv.org/abs/2508.12285", "authors": ["Yunbo Lyu", "Zhou Yang", "Jieke Shi", "Jianming Chang", "Yue Liu", "David Lo"], "title": "\"My productivity is boosted, but ...\" Demystifying Users' Perception on AI Coding Assistants", "comment": "13 pages, Camera-Ready Version that will appear in ASE 2025", "summary": "This paper aims to explore fundamental questions in the era when AI coding\nassistants like GitHub Copilot are widely adopted: what do developers truly\nvalue and criticize in AI coding assistants, and what does this reveal about\ntheir needs and expectations in real-world software development? Unlike\nprevious studies that conduct observational research in controlled and\nsimulated environments, we analyze extensive, first-hand user reviews of AI\ncoding assistants, which capture developers' authentic perspectives and\nexperiences drawn directly from their actual day-to-day work contexts. We\nidentify 1,085 AI coding assistants from the Visual Studio Code Marketplace.\nAlthough they only account for 1.64% of all extensions, we observe a surge in\nthese assistants: over 90% of them are released within the past two years. We\nthen manually analyze the user reviews sampled from 32 AI coding assistants\nthat have sufficient installations and reviews to construct a comprehensive\ntaxonomy of user concerns and feedback about these assistants. We manually\nannotate each review's attitude when mentioning certain aspects of coding\nassistants, yielding nuanced insights into user satisfaction and\ndissatisfaction regarding specific features, concerns, and overall tool\nperformance. Built on top of the findings-including how users demand not just\nintelligent suggestions but also context-aware, customizable, and\nresource-efficient interactions-we propose five practical implications and\nsuggestions to guide the enhancement of AI coding assistants that satisfy user\nneeds.", "AI": {"tldr": "This study examines developer valuations and criticisms of AI coding assistants through manual analysis of user reviews, revealing unmet needs and suggesting practical improvements for real-world tools.", "motivation": "With AI coding assistants becoming prevalent, understanding developers' authentic perceptions and requirements is critical for improving tool design and addressing real-world software development challenges.", "method": "Manually analyzed user reviews from 32 AI coding assistants (selected from 1,085 VS Code Marketplace extensions) to create taxonomy of user concerns and annotate attitudes toward specific features and tool performance.", "result": "Identified 1,085 AI coding assistants (90% released in last two years) and distilled nuanced insights about developer demands for context-aware, customizable, and resource-efficient AI interactions across a comprehensive taxonomy of feedback.", "conclusion": "Developers seek AI coding assistants that provide intelligent suggestions combined with contextual understanding and customization options, leading to five practical implications for enhancing tool effectiveness in practical software engineering workflows."}}
{"id": "2508.11797", "categories": ["cs.CR", "cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.11797", "abs": "https://arxiv.org/abs/2508.11797", "authors": ["Calkin Garg", "Omar Rios Cruz", "Tessa Andersen", "Gaby G. Dagher", "Donald Winiecki", "Min Long"], "title": "AegisBlock: A Privacy-Preserving Medical Research Framework using Blockchain", "comment": "Submitted to IEEE Conference on Collaboration and Internet Computing\n  2025", "summary": "Due to HIPAA and other privacy regulations, it is imperative to maintain\npatient privacy while conducting research on patient health records. In this\npaper, we propose AegisBlock, a patient-centric access controlled framework to\nshare medical records with researchers such that the anonymity of the patient\nis maintained while ensuring the trustworthiness of the data provided to\nresearchers. AegisBlock allows for patients to provide access to their medical\ndata, verified by miners. A researcher submits a time-based range query to\nrequest access to records from a certain patient, and upon patient approval,\naccess will be granted. Our experimental evaluation results show that\nAegisBlock is scalable with respect to the number of patients and hospitals in\nthe system, and efficient with up to 50% of malicious miners.", "AI": {"tldr": "This paper introduces AegisBlock, a patient-centric access control framework that balances data privacy with researcher trust by allowing patients to grant access to their medical records via miner verification, demonstrating scalability and robustness against malicious miners.", "motivation": "The need to maintain patient privacy under HIPAA and similar regulations while enabling legitimate research access to health records motivates the development of a secure, privacy-preserving framework.", "method": "AegisBlock employs a blockchain-based verification process where patients approve time-bound record access requests from researchers. Miners validate and enforce access-granting rules, ensuring anonymity and data integrity through decentralized consensus.", "result": "Experiments confirm AegisBlock's scalability with expanding patient and hospital counts, handling high miner malicious activity (up to 50%) efficiently without compromising data accuracy or privacy.", "conclusion": "AegisBlock effectively reconciles privacy protection with secure data sharing for medical research, offering a reliable framework to maintain anonymity, enforce trust, and support system scalability in real-world applications."}}
{"id": "2508.12303", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12303", "abs": "https://arxiv.org/abs/2508.12303", "authors": ["Xu Long", "Yishun Wang", "Xiaoqi Li"], "title": "From Fomo3D to Lottery DAPP: Analysis of Ethereum-Based Gambling Applications", "comment": null, "summary": "As blockchain technology advances, Ethereum based gambling decentralized\napplications (DApps) represent a new paradigm in online gambling. This paper\nexamines the concepts, principles, implementation, and prospects of Ethereum\nbased gambling DApps. First, we outline the concept and operational principles\nof gambling DApps. These DApps are blockchain based online lottery platforms.\nThey utilize smart contracts to manage the entire lottery process, including\nissuance, betting, drawing, and prize distribution. Being decentralized,\nlottery DApps operate without central oversight, unlike traditional lotteries.\nThis ensures fairness and eliminates control by any single entity. Automated\nsmart contract execution further reduces management costs, increases\nprofitability, and enhances game transparency and credibility. Next, we analyze\nan existing Ethereum based gambling DApp, detailing its technical principles,\nimplementation, operational status, vulnerabilities, and potential solutions.\nWe then elaborate on the implementation of lottery DApps. Smart contracts\nautomate the entire lottery process including betting, drawing, and prize\ndistribution. Although developing lottery DApps requires technical expertise,\nthe expanding Ethereum ecosystem provides growing tools and frameworks,\nlowering development barriers. Finally, we discuss current limitations and\nprospects of lottery DApps. As blockchain technology and smart contracts\nevolve, lottery DApps are positioned to significantly transform the online\nlottery industry. Advantages like decentralization, automation, and\ntransparency will likely drive broader future adoption.", "AI": {"tldr": "This paper analyzes Ethereum-based gambling DApps as a disruptive force in online lottery systems using smart contracts for process automation and decentralization. It examines technical principles, implementation challenges, and future potential of these platforms.", "motivation": "The paper aims to explore how blockchain technology can create fairer lottery systems through decentralization and transparency, eliminating single-entity control and reducing operational costs via smart contract automation.", "method": "The analysis includes conceptual explanation, technical dissection of an existing DApp (covering implementation and vulnerabilities), discussion of development requirements, and evaluation of current limitations through technical principles and case studies.", "result": "Identified key benefits of decentralization and automation in lottery processes, mapped vulnerabilities in existing implementations, and demonstrated that Ethereum's growing ecosystem reduces development complexity while maintaining process integrity through smart contracts.", "conclusion": "Blockchain-based lottery DApps have transformative potential in the online gambling industry due to inherent advantages, though adoption depends on addressing current technical limitations as the Ethereum platform evolves."}}
{"id": "2508.11812", "categories": ["cs.CR", "cs.CY", "cs.NI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.11812", "abs": "https://arxiv.org/abs/2508.11812", "authors": ["Tyler Schroder", "Sohee Kim Park"], "title": "Securing Sideways: Thwarting Lateral Movement by Implementing Active Directory Tiering", "comment": "11 pages", "summary": "The advancement of computing equipment and the advances in services over the\nInternet has allowed corporations, higher education, and many other\norganizations to pursue the shared computing network environment. A requirement\nfor shared computing environments is a centralized identity system to\nauthenticate and authorize user access. An organization's digital identity\nplane is a prime target for cyber threat actors. When compromised, identities\ncan be exploited to steal credentials, create unauthorized accounts, and\nmanipulate permissions-enabling attackers to gain control of the network and\nundermine its confidentiality, availability, and integrity. Cybercrime losses\nreached a record of 16.6 B in the United States in 2024. For organizations\nusing Microsoft software, Active Directory is the on-premises identity system\nof choice. In this article, we examine the challenge of security compromises in\nActive Directory (AD) environments and present effective strategies to prevent\ncredential theft and limit lateral movement by threat actors. Our proposed\napproaches aim to confine the movement of compromised credentials, preventing\nsignificant privilege escalation and theft. We argue that through our\nillustration of real-world scenarios, tiering can halt lateral movement and\nadvanced cyber-attacks, thus reducing ransom escalation. Our work bridges a gap\nin existing literature by combining technical guidelines with theoretical\narguments in support of tiering, positioning it as a vital component of modern\ncybersecurity strategy even though it cannot function in isolation. As the\nhardware advances and the cloud sourced services along with AI is advancing\nwith unprecedented speed, we think it is important for security experts and the\nbusiness to work together and start designing and developing software and\nframeworks to classify devices automatically and accurately within the tiered\nstructure.", "AI": {"tldr": "The paper addresses security vulnerabilities in Active Directory environments due to compromised credentials and advocates for tiering strategies to prevent theft and limit lateral movement in shared computing networks.", "motivation": "Shared computing environments require centralized identity systems like Active Directory, which are prime targets for cyberattacks. Compromised identities enable credential theft, unauthorized access, and network breaches, leading to significant financial losses (e.g., 16.6B in U.S. cybercrime). Existing literature lacks integrated technical and theoretical guidance for mitigation.", "method": "Analyzes real-world cyberattack scenarios to demonstrate how tiering in Active Directory can restrict compromised credential movement, minimize privilege escalation, and prevent large-scale breaches.", "result": "Tiering effectively halts lateral movement and advanced cyber-attacks, reducing ransomware risks and limiting attacker access to sensitive resources.", "conclusion": "The study bridges the gap between technical practices and theoretical frameworks for AD security, emphasizing that tiering is a critical but non-solitary component of modern cybersecurity. It calls for collaborative development of automated tiering tools to adapt to advancing hardware, cloud, and AI technologies."}}
{"id": "2508.12325", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12325", "abs": "https://arxiv.org/abs/2508.12325", "authors": ["Tim Kr\u00e4uter", "Adrian Rutle", "Yngve Lamo", "Harald K\u00f6nig", "Francisco Dur\u00e1n"], "title": "Towards the Coordination and Verification of Heterogeneous Systems with Data and Time", "comment": "This is the authors accepted version of a paper to be published in\n  MODELS-2025, DOI: TBD", "summary": "Modern software systems are often realized by coordinating multiple\nheterogeneous parts, each responsible for specific tasks. These parts must work\ntogether seamlessly to satisfy the overall system requirements. To verify such\ncomplex systems, we have developed a non-intrusive coordination framework\ncapable of performing formal analysis of heterogeneous parts that exchange data\nand include real-time capabilities. The framework utilizes a linguistic\nextension, which is implemented as a central broker and a domain-specific\nlanguage for the integration of heterogeneous languages and coordination of\nparts. Moreover, abstract rule templates are reified as language adapters for\nnon-intrusive communications with the broker. The framework is implemented\nusing rewriting logic (Maude), and its applicability is demonstrated by\nverifying certain correctness properties of a heterogeneous road-rail crossing\nsystem.", "AI": {"tldr": "The paper presents a non-intrusive coordination framework using rewriting logic (Maude) to perform formal analysis of heterogeneous systems with real-time requirements, validated on a road-rail crossing system.", "motivation": "Modern software systems often integrate heterogeneous components requiring seamless coordination to meet real-time requirements, yet verifying such systems remains challenging due to their complexity and diversity.", "method": "The framework includes (1) a linguistic extension implemented as a central broker mediating data exchange, (2) a domain-specific language (DSL) for integration of heterogeneous systems, and (3) reified abstract rule templates as language adapters enabling non-intrusive communication. Rewriting logic is used for implementation.", "result": "The framework successfully verified correctness properties of a heterogeneous road-rail crossing system, demonstrating its applicability for analyzing complex, time-sensitive software architectures.", "conclusion": "The framework provides an effective approach to formal verification of heterogeneous systems through non-intrusive coordination mechanisms realized in rewriting logic."}}
{"id": "2508.11817", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11817", "abs": "https://arxiv.org/abs/2508.11817", "authors": ["Mukesh Poudel", "Nick Rahimi"], "title": "Machine Learning-Based AES Key Recovery via Side-Channel Analysis on the ASCAD Dataset", "comment": null, "summary": "Cryptographic algorithms like AES and RSA are widely used and they are\nmathematically robust and almost unbreakable but its implementation on physical\ndevices often leak information through side channels, such as electromagnetic\n(EM) emissions, potentially compromising said theoretically secure algorithms.\nThis paper investigates the application of machine learning (ML) techniques and\nDeep Learning models to exploit such leakage for partial key recovery. We use\nthe public ASCAD `fixed' and `variable' key dataset, containing 700 and 1400 EM\ntraces respectively from an AES-128 implementation on an 8-bit microcontroller.\nThe problem is framed as a 256-class classification task where we target the\noutput of the first-round S-box operation, which is dependent on a single key\nbyte. We evaluate standard classifiers (Random Forest (RF), Support Vector\nMachine (SVM)), a Convolutional Neural Network(CNN) and a Residual Neural\nNetwork(ResNet). We also explore the utility of RF-based feature importance for\ndimensionality reduction. Crucially, we employ this domain-specific Key Rank\nmetric for evaluation, showing its necessity over standard classification\naccuracy. Our results show that SVM and RF on full features perform poorly in\nkey ranking. However, RF trained on reduced (top 100) identified via importance\nanalysis achieves Rank 0 (successful key byte recovery) using almost half the\nattack traces. The implemented CNN also achieves Rank 0 efficiently using\napproximately 65 attack traces for the fixed-key dataset. The ResNets perform\nbest on large and complex datasets but may not always be the best choice for\nsimple fixed key dataset in terms of efficiency. Thus we conclude that models,\nparticularly CNNs, ResNets and feature-selected RF, coupled with the Key Rank\nmetric, are an effective tool for side-channel key recovery, confirming the\npractical vulnerability of the cryptographic implementations.", "AI": {"tldr": "This paper demonstrates the effectiveness of machine learning and deep learning models in exploiting side-channel leaks, specifically EM emissions, for partial key recovery in AES-128 implementations, confirming practical vulnerabilities despite theoretical security.", "motivation": "Cryptographic algorithms like AES and RSA are mathematically robust but vulnerable to side-channel attacks due to implementation leaks. The study aims to evaluate ML/DL techniques for efficient partial key recovery to highlight implementation risks.", "method": "Used ASCAD fixed (700 traces) and variable (1400 traces) key datasets with 256-class classification targeting the first-round S-box. Trained Random Forest (RF), SVM, CNN, and ResNet. Compared feature importance-based dimensionality reduction and used Key Rank metric for evaluation.", "result": "RF with top 100 features achieved Rank 0 using ~50% fewer traces, CNN reached Rank 0 with ~65 traces for fixed-key data, and ResNets excelled on complex datasets but were less efficient for the simpler fixed-key case. Key Rank metric emphasized practical success over standard accuracy.", "conclusion": "ML models, particularly CNNs, ResNets, and feature-selected RF with Key Rank evaluation, effectively recover keys from side-channel leaks. This confirms the practical vulnerability of cryptographic implementations, showing side-channel robustness is crucial beyond theoretical security."}}
{"id": "2508.12358", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12358", "abs": "https://arxiv.org/abs/2508.12358", "authors": ["Haolin Jin", "Huaming Chen"], "title": "Uncovering Systematic Failures of LLMs in Verifying Code Against Natural Language Specifications", "comment": "Accepted to the NIER track of the 40th IEEE/ACM International\n  Conference on Automated Software Engineering (ASE 2025)", "summary": "Large language models (LLMs) have become essential tools in software\ndevelopment, widely used for requirements engineering, code generation and\nreview tasks. Software engineers often rely on LLMs to assess whether system\ncode implementation satisfy task requirements, thereby enhancing code\nrobustness and accuracy. However, it remains unclear whether LLMs can reliably\ndetermine whether the code complies fully with the given task descriptions,\nwhich is usually natural language specifications. In this paper, we uncover a\nsystematic failure of LLMs in evaluating whether code aligns with natural\nlanguage requirements. Specifically, with widely used benchmarks, we employ\nunified prompts to judge code correctness. Our results reveal that LLMs\nfrequently misclassify correct code implementations as either ``not satisfying\nrequirements'' or containing potential defects. Surprisingly, more complex\nprompting, especially when leveraging prompt engineering techniques involving\nexplanations and proposed corrections, leads to higher misjudgment rate, which\nhighlights the critical reliability issues in using LLMs as code review\nassistants. We further analyze the root causes of these misjudgments, and\npropose two improved prompting strategies for mitigation. For the first time,\nour findings reveals unrecognized limitations in LLMs to match code with\nrequirements. We also offer novel insights and practical guidance for effective\nuse of LLMs in automated code review and task-oriented agent scenarios.", "AI": {"tldr": "This paper reveals that LLMs systematically fail to evaluate code alignment with natural language requirements, leading to frequent misclassification of correct code, and proposes improved prompting strategies to address these reliability issues.", "motivation": "Despite the widespread use of LLMs in code review, their reliability in assessing code compliance with natural language specifications remains unexplored, prompting the need for this investigation into systematic failures and mitigation strategies.", "method": "The study uses widely used benchmarks with unified prompts to evaluate LLMs' code correctness judgments. It employs prompt engineering techniques involving explanations and corrections to analyze misjudgments and then derives root causes and improved strategies.", "result": "LLMs misclassify correct code as non-compliant or defective, and complex prompting increases error rates. The research identifies underlying causes and demonstrates two prompting strategies that effectively reduce these misjudgments, highlighting previously unrecognized limitations.", "conclusion": "The findings challenge the reliability of LLMs in task-oriented code review scenarios, demonstrate how existing techniques exacerbate issues, and provide novel methods and practical guidance to mitigate systematic failures while enabling better integration of LLMs in development workflows."}}
{"id": "2508.11907", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11907", "abs": "https://arxiv.org/abs/2508.11907", "authors": ["Xiaojin Zhang", "Mingcong Xu", "Yiming Li", "Wei Chen", "Qiang Yang"], "title": "Deciphering the Interplay between Attack and Protection Complexity in Privacy-Preserving Federated Learning", "comment": null, "summary": "Federated learning (FL) offers a promising paradigm for collaborative model\ntraining while preserving data privacy. However, its susceptibility to gradient\ninversion attacks poses a significant challenge, necessitating robust privacy\nprotection mechanisms. This paper introduces a novel theoretical framework to\ndecipher the intricate interplay between attack and protection complexities in\nprivacy-preserving FL. We formally define \"Attack Complexity\" as the minimum\ncomputational and data resources an adversary requires to reconstruct private\ndata below a given error threshold, and \"Protection Complexity\" as the expected\ndistortion introduced by privacy mechanisms. Leveraging Maximum Bayesian\nPrivacy (MBP), we derive tight theoretical bounds for protection complexity,\ndemonstrating its scaling with model dimensionality and privacy budget.\nFurthermore, we establish comprehensive bounds for attack complexity, revealing\nits dependence on privacy leakage, gradient distortion, model dimension, and\nthe chosen privacy level. Our findings quantitatively illuminate the\nfundamental trade-offs between privacy guarantees, system utility, and the\neffort required for both attacking and defending. This framework provides\ncritical insights for designing more secure and efficient federated learning\nsystems.", "AI": {"tldr": "This paper proposes a theoretical framework for analyzing privacy-utility trade-offs in federated learning by formally defining 'Attack Complexity' and 'Protection Complexity' metrics, deriving their mathematical bounds, and revealing scaling relationships with model properties and privacy budgets.", "motivation": "Federated learning's vulnerability to gradient inversion attacks necessitates robust privacy mechanisms. Existing approaches lack quantitative characterization of the interplay between attack efforts and protective measures.", "method": "The authors 1) define Attack Complexity (minimum resources needed for data reconstruction below error threshold) and Protection Complexity (distortion from privacy mechanisms), 2) use Maximum Bayesian Privacy (MBP) to derive protection complexity bounds with respect to model dimensionality and privacy budget, and 3) develop rigorous attack complexity bounds influenced by privacy leakage, gradient distortion, model dimensionality, and privacy level.", "result": "The paper establishes: 1) Tight theoretical bounds for protection complexity showing its dependence on model dimension; 2) Comprehensive attack complexity bounds demonstrating dependencies on privacy leakage (via R\u00e9nyi divergence), gradient distortion (via signal-to-gradient ratio), model size, and privacy budget; 3) Quantitative privacy-utility trade-off characterization that maps required attacker resources against defender costs.", "conclusion": "This theoretical framework provides foundational insights for designing secure federated learning systems by quantifying the complex relationships between attack feasibility, protection effectiveness, data privacy, model utility, and system resources."}}
{"id": "2508.12436", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12436", "abs": "https://arxiv.org/abs/2508.12436", "authors": ["Feifei Niu", "Chuanyi Li", "Haosheng Zuo", "Jionghan Wu", "Xin Xia"], "title": "Feature Request Analysis and Processing: Tasks, Techniques, and Trends", "comment": null, "summary": "Feature requests are proposed by users to request new features or\nenhancements of existing features of software products, which represent users'\nwishes and demands. Satisfying users' demands can benefit the product from both\ncompetitiveness and user satisfaction. Feature requests have seen a rise in\ninterest in the past few years and the amount of research has been growing.\nHowever, the diversity in the research topics suggests the need for their\ncollective analysis to identify the challenges and opportunities so as to\npromote new advances in the future. In this work, following a defined process\nand a search protocol, we provide a systematic overview of the research area by\nsearching and categorizing relevant studies. We select and analyze 131 primary\nstudies using descriptive statistics and qualitative analysis methods. We\nclassify the studies into different topics and group them from the perspective\nof requirements engineering activities. We investigate open tools as well as\ndatasets for future research. In addition, we identify several key challenges\nand opportunities, such as: (1) ensuring the quality of feature requests, (2)\nimproving their specification and validation, and (3) developing high-quality\nbenchmarks for large language model-driven tasks.", "AI": {"tldr": "This paper systematically reviews 131 studies on feature requests in software products, identifying key challenges and opportunities to guide future research.", "motivation": "Feature requests reflect user demands and product competitiveness, yet the fragmented research landscape necessitates a systematic synthesis to pinpoint critical challenges and opportunities for advancement.", "method": "The study follows a predefined process and search protocol, employing descriptive statistics and qualitative analysis on 131 primary studies. It classifies them by topics and requirements engineering activities, while evaluating available tools and datasets.", "result": "Identified challenges include ensuring request quality, improving specification/validation, and developing language model benchmarks. Opportunities lie in advancing feature request analysis and fostering future research directions.", "conclusion": "This systematic review provides a structured overview of feature request research, highlighting unresolved challenges and opportunities. It aims to catalyze improved practices and guide high-quality benchmark development for emerging AI-driven methodologies."}}
{"id": "2508.11913", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11913", "abs": "https://arxiv.org/abs/2508.11913", "authors": ["Huipeng Yang", "Li Yang", "Lichuan Ma", "Lu Zhou", "Junbo Jia", "Anyuan Sang", "Xinyue Wang"], "title": "WebGeoInfer: A Structure-Free and Multi-Stage Framework for Geolocation Inference of Devices Exposing Information", "comment": null, "summary": "Remote management devices facilitate critical infrastructure monitoring for\nadministrators but simultaneously increase asset exposure. Sensitive\ngeographical information overlooked in exposed device management pages poses\nsubstantial security risks. Therefore, identifying devices that reveal location\ninformation due to administrator negligence is crucial for cybersecurity\nregulation. Despite the rich information exposed by web interfaces of remote\nmanagement devices, automatically discovering geographical locations remains\nchallenging due to unstructured formats, varying styles, and incomplete\ngeographical details.\n  This study introduces WebGeoInfer, a structure-free geolocation inference\nframework utilizing multi-stage information enhancement. WebGeoInfer clusters\nsimilar device web pages and analyzes inter-cluster differences to extract\npotential geographical information, bypassing structural limitations. Through\nsearch engine enhancement and Large Language Models mining, the framework\nextracts geographical coordinates from identified information. WebGeoInfer\nsuccessfully inferred locations for 5,435 devices across 94 countries and 2,056\ncities, achieving accuracy rates of 96.96\\%, 88.05\\%, and 79.70\\% at country,\ncity, and street levels, respectively.", "AI": {"tldr": "This study introduces WebGeoInfer, a novel structure-free framework for automatically inferring geolocation from unstructured web interfaces of remote management devices to address cybersecurity risks from exposed location data.", "motivation": "Remote management devices enable infrastructure monitoring but inadvertently expose sensitive geographical information via management pages, creating significant security vulnerabilities. Automated discovery of geolocation remains challenging due to unstructured, inconsistent, and incomplete data formats.", "method": "WebGeoInfer employs multi-stage information enhancement through device page clustering and inter-cluster difference analysis to extract geographical cues without relying on structured data formats. It combines search engine enhancements with Large Language Models (LLMs) for coordinate extraction from unstructured text.", "result": "The framework successfully inferred locations for 5,435 devices globally, achieving 96.96% country-level, 88.05% city-level, and 79.70% street-level accuracy, demonstrating its effectiveness across diverse geopolitical populations.", "conclusion": "WebGeoInfer provides a robust solution for identifying geographically exposed devices through unstructured interface analysis, enabling more comprehensive cyber-regulatory measures to mitigate location-based security risks."}}
{"id": "2508.12546", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12546", "abs": "https://arxiv.org/abs/2508.12546", "authors": ["Bin Duan", "Ruican Dong", "Naipeng Dong", "Dan Dongseong Kim", "Guowei Yang"], "title": "XAMT: Cross-Framework API Matching for Testing Deep Learning Libraries", "comment": null, "summary": "Deep learning powers critical applications such as autonomous driving,\nhealthcare, and finance, where the correctness of underlying libraries is\nessential. Bugs in widely used deep learning APIs can propagate to downstream\nsystems, causing serious consequences. While existing fuzzing techniques detect\nbugs through intra-framework testing across hardware backends (CPU vs. GPU),\nthey may miss bugs that manifest identically across backends and thus escape\ndetection under these strategies. To address this problem, we propose XAMT, a\ncross-framework fuzzing method that tests deep learning libraries by matching\nand comparing functionally equivalent APIs across different frameworks. XAMT\nmatches APIs using similarity-based rules based on names, descriptions, and\nparameter structures. It then aligns inputs and applies variance-guided\ndifferential testing to detect bugs. We evaluated XAMT on five popular\nframeworks, including PyTorch, TensorFlow, Keras, Chainer, and JAX. XAMT\nmatched 839 APIs and identified 238 matched API groups, and detected 17 bugs,\n12 of which have been confirmed. Our results show that XAMT uncovers bugs\nundetectable by intra-framework testing, especially those that manifest\nconsistently across backends. XAMT offers a complementary approach to existing\nmethods and offers a new perspective on the testing of deep learning libraries.", "AI": {"tldr": "XAMT is a cross-framework fuzzing method that detects consistent backend bugs in deep learning libraries by matching and comparing functionally equivalent APIs across frameworks, identifying 17 bugs in PyTorch, TensorFlow, and others.", "motivation": "Existing intra-framework testing methods fail to detect bugs that manifest identically across hardware backends. Deep learning libraries require robust cross-backend verification to catch consistent but critical implementation errors in APIs.", "method": "XAMT uses similarity-based API matching (names, descriptions, parameters) followed by variance-guided differential testing to compare behavior across frameworks like PyTorch, TensorFlow, Keras, Chainer, and JAX.", "result": "XAMT matched 839 APIs into 238 groups across five frameworks and discovered 17 bugs (12 confirmed), with 136 already patched, demonstrating effectiveness in uncovering bugs undetectable by intra-framework testing.", "conclusion": "XAMT provides a novel complementary approach to API testing by leveraging cross-framework comparisons, addressing a critical gap in detecting backend-consistent bugs while offering practical validation through extensive bug detection."}}
{"id": "2508.11925", "categories": ["cs.CR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11925", "abs": "https://arxiv.org/abs/2508.11925", "authors": ["Zhimeng Guo", "Huaisheng Zhu", "Siyuan Xu", "Hangfan Zhang", "Teng Xiao", "Minhao Cheng"], "title": "Optimizing Token Choice for Code Watermarking: A RL Approach", "comment": "18 pages, 3 figures", "summary": "The need for detecting LLM-generated code necessitates watermarking systems\ncapable of operating within its highly structured and syntactically constrained\nenvironment. To address this, we introduce CodeTracer, an innovative adaptive\ncode watermarking framework underpinned by a novel reinforcement learning\ntraining paradigm. At its core, CodeTracer features a policy-driven approach\nthat utilizes a parameterized model to intelligently bias token choices during\nnext-token prediction. This strategy ensures that embedded watermarks maintain\ncode functionality while exhibiting subtle yet statistically detectable\ndeviations from typical token distributions. To facilitate policy learning, we\ndevise a comprehensive reward system that seamlessly integrates execution\nfeedback with watermark embedding signals, balancing process-level and\noutcome-level rewards. Additionally, we employ Gumbel Top-k reparameterization\nto enable gradient-based optimization of discrete watermarking decisions.\nExtensive comparative evaluations demonstrate CodeTracer's significant\nsuperiority over state-of-the-art baselines in both watermark detectability and\nthe preservation of generated code's functionality.", "AI": {"tldr": "This paper introduces CodeTracer, a reinforcement learning-based adaptive code watermarking framework that biases token choices during next-token prediction while preserving code functionality. Its reward system combines execution feedback with watermark embedding signals, and Gumbel Top-k reparameterization enables gradient optimization of watermark decisions. CodeTracer outperforms existing baselines in detectability and functionality preservation.", "motivation": "Current watermarking systems struggle with the structured, syntactically constrained nature of code. Code generation by LLMs requires watermarks to maintain functional correctness while introducing subtle detectable patterns, necessitating a novel approach to balance these requirements.", "method": "CodeTracer uses a policy-driven framework with a parameterized model to bias token choices during generation. The method integrates a reward system combining execution feedback and watermark embedding signals, and employs Gumbel Top-k reparameterization to optimize discrete watermarking decisions via gradient-based reinforcement learning.", "result": "CodeTracer demonstrates significant superiority over state-of-the-art methods in both watermark detectability and code functionality preservation through extensive comparative evaluations.", "conclusion": "CodeTracer bridges the gap in watermarking structured code generated by LLMs by introducing a reinforcement learning-based approach with innovative reward system and gradient optimization techniques, achieving strong empirical performance on key metrics."}}
{"id": "2508.12620", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.12620", "abs": "https://arxiv.org/abs/2508.12620", "authors": ["Xiaoning Ren", "Qiang Hu", "Wei Ma", "Yan Li", "Yao Zhang", "Lingxiao Jiang", "Yinxing Xue"], "title": "Strengthening Programming Comprehension in Large Language Models through Code Generation", "comment": "11 pages, 7 figures", "summary": "Large language models (LLMs) have recently shown impressive results on\ndiverse code-related tasks, benefiting from large-scale training and\ninstruction tuning. However, studies reveal that their grasp of fundamental\nprogramming concepts, such as data flow and control flow, remains shallow,\nleading to fragile performance when code requires deeper reasoning. This\nlimitation restricts the practical adoption of LLMs in real-world software\ndevelopment. To address this issue, this work introduces a counterfactual code\naugmentation framework combined with concept-aware tuning, designed to guide\nLLMs toward stronger conceptual understanding. Comprehensive evaluation across\nmultiple models and benchmarks demonstrates the effectiveness of the proposed\napproach.", "AI": {"tldr": "This paper introduces a counterfactual code augmentation framework with concept-aware tuning to enhance large language models' (LLMs) understanding of programming concepts like data/control flow, addressing their fragile performance on deeper code reasoning tasks.", "motivation": "LLMs struggle to grasp fundamental programming concepts (e.g., data flow, control flow) during software development tasks, limiting their practical adoption despite large-scale training and instruction tuning. This shallow understanding creates performance instability when code requires deeper reasoning. ", "method": "A novel framework combining counterfactual code augmentation with concept-aware tuning. It generates diverse code scenarios through counterfactual generation and strengthens conceptual understanding by tying instruction tuning to specific code concepts.", "result": "Comprehensive evaluations across multiple LLMs and code benchmarks demonstrate significant improvements in performance metrics when the framework is applied. Model robustness on concept-dependent reasoning tasks increases substantially.", "conclusion": "The counterfactual code augmentation framework with concept-aware tuning successfully improves LLMs' conceptual understanding of programming fundamentals. This approach offers a promising path to enhance LLM reliability in real-world software engineering applications."}}
{"id": "2508.11928", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11928", "abs": "https://arxiv.org/abs/2508.11928", "authors": ["Lien Tran", "Boyuan Zhang", "Ratchanon Pawanja", "Rashid Hussain Khokhar"], "title": "The Passwordless Authentication with Passkey Technology from an Implementation Perspective", "comment": "6 pages, 3 figures", "summary": "With the rise of sophisticated authentication bypass techniques, passwords\nare no longer considered a reliable method for securing authentication systems.\nIn recent years, new authentication technologies have shifted from traditional\npassword-based logins to passwordless security. Among these, Time-Based\nOne-Time Passwords (TOTP) remain one of the most widely used mechanisms, while\nPasskeys are emerging as a promising alternative with growing adoption. This\npaper highlights the key techniques used during the implementation of the\nauthentication system with Passkey technology. It also suggests considerations\nfor integrating components during system development to ensure that users can\nsecurely access their accounts with minimal complexity, while still meeting the\nrequirements of a robust authentication system that balances security,\nusability, and performance. Additionally, by examining TOTP and Passkey\nmechanisms from an implementation perspective, this work not only addresses\nmajor security concerns such as password leaks, phishing attacks, and\nsusceptibility to brute-force attacks, but also evaluates the feasibility and\neffectiveness of these mechanisms in real-world implementations. This paper\ndemonstrates the superior security of Passkey technology and its potential for\nbroader adoption in secure authentication systems.", "AI": {"tldr": "The paper examines Passkey technology for secure passwordless authentication, demonstrating its advantages over TOTP in addressing security vulnerabilities and usability.", "motivation": "Traditional passwords are vulnerable to leaks, phishing, and brute-force attacks, prompting the need for passwordless alternatives like TOTP and Passkeys. TOTP's limitations in resistance to certain threats and user experience necessitate evaluating newer solutions.", "method": "Implementation of Passkey technology compared to TOTP, analyzing technical considerations for integration into authentication systems. Evaluation includes security analysis, usability, and performance testing against real-world attack scenarios.", "result": "Passkeys surpass TOTP in phishing resistance and security by leveraging decentralized cryptographic storage. They maintain acceptable usability and performance while eliminating dependency on secret codes. TOTP limitations in security and attack susceptibility were empirically validated.", "conclusion": "Passkey technology provides superior security without compromising user experience, recommending its adoption as a robust passwordless authentication solution. Future implementations should prioritize Passkey integration over TOTP for enhanced security in authentication systems."}}
{"id": "2508.12649", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12649", "abs": "https://arxiv.org/abs/2508.12649", "authors": ["Lei Chen", "Michele Lanza", "Shinpei Hayashi"], "title": "ChangePrism: Visualizing the Essence of Code Changes", "comment": "5 pages, 5 figures, VISSOFT 2025", "summary": "Understanding the changes made by developers when they submit a pull request\nand/or perform a commit on a repository is a crucial activity in software\nmaintenance and evolution. The common way to review changes relies on examining\ncode diffs, where textual differences between two file versions are highlighted\nin red and green to indicate additions and deletions of lines. This can be\ncumbersome for developers, making it difficult to obtain a comprehensive\noverview of all changes in a commit. Moreover, certain types of code changes\ncan be particularly significant and may warrant differentiation from standard\nmodifications to enhance code comprehension. We present a novel visualization\napproach supported by a tool named ChangePrism, which provides a way to better\nunderstand code changes. The tool comprises two components: extraction, which\nretrieves code changes and relevant information from the git history, and\nvisualization, which offers both general and detailed views of code changes in\ncommits. The general view provides an overview of different types of code\nchanges across commits, while the detailed view displays the exact changes in\nthe source code for each commit.", "AI": {"tldr": "ChangePrism is a tool that provides visualization of code changes in repositories to enhance understanding during software maintenance.", "motivation": "Reviewing code diffs is cumbersome and inefficient for comprehending comprehensive changes in commits, necessitating a better approach to differentiate significant changes.", "method": "The tool uses two components: extraction from git history to retrieve code changes and visualization with general and detailed views to represent modifications across commits.", "result": "The paper demonstrates a novel visualization approach through ChangePrism to improve code comprehension by addressing limitations of traditional diff-based reviews.", "conclusion": "ChangePrism offers a structured way to visualize code changes, aiding developers in understanding both broad patterns and specific details in software evolution."}}
{"id": "2508.11939", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11939", "abs": "https://arxiv.org/abs/2508.11939", "authors": ["James Gu", "Ahmed Sartaj", "Mohammed Akram Taher Khan", "Rashid Hussain Khokhar"], "title": "Design and Implementation of a Controlled Ransomware Framework for Educational Purposes Using Flutter Cryptographic APIs on Desktop PCs and Android Devices", "comment": "6 pages, 1 figure, 1 table, 2 algorithms", "summary": "This study focuses on the creation and implementation of ransomware for\neducational purposes that leverages Python's native cryptographic APIs in a\ncontrolled environment. Additionally, an Android version of the framework is\nimplemented using Flutter and Dart. For both versions, open-source\ncryptographic libraries are utilized. With this framework, researchers can\nsystematically explore the functionalities of ransomware, including file\nencryption processes, cryptographic key management, and victim interaction\ndynamics. To ensure safe experimentation, multiple safeguards are incorporated,\nsuch as the ability to restrict the encryption process to a specific directory,\nproviding the RSA private key for immediate decryption, and narrowing the scope\nof targetable files to a carefully curated list (.txt, .jpg, .csv, .doc). This\npaper draws inspiration from the infamous WannaCry ransomware and aims to\nsimulate its behaviour on Android devices. By making the codebase open-source,\nit enables users to study, modify, and extend the program for pedagogical\npurposes and offers a hands-on tool that can be used to train the next\ngeneration of cybersecurity professionals.", "AI": {"tldr": "A ransomware framework using Python and Flutter/Dart for educational experimentation, implementing safeguards for safe analysis of encryption mechanics and victim interactions.", "motivation": "To provide a controlled environment for studying ransomware behavior (specifically WannaCry-inspired mechanics) alongside Android-specific implementation to train cybersecurity professionals through hands-on analysis.", "method": "Developed two versions using Python's native crypto APIs and Flutter/Dart with: 1) Directory-restricted encryption 2) Pre-provided RSA private key 3) Limited file extension targeting (.txt/.jpg/.csv/.doc) 4) Open-source codebase for replication", "result": "Framework enables systematic analysis of encryption processes, key management, and ransomware attack patterns across desktop and Android platforms with fail-safes preventing actual harm.", "conclusion": "The open-source framework offers a pedagogical tool to safely examine ransomware mechanisms, enhancing cybersecurity education through practical reverse engineering and defensive strategy development."}}
{"id": "2508.12922", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12922", "abs": "https://arxiv.org/abs/2508.12922", "authors": ["Yue Wang", "Zhenyu Chen", "Yuan Zhao", "Chunrong Fang", "Ziyuan Wang", "Song Huang"], "title": "RUM: Rule+LLM-Based Comprehensive Assessment on Testing Skills", "comment": null, "summary": "Over the past eight years, the META method has served as a multidimensional\ntesting skill assessment system in the National College Student Contest on\nSoftware Testing, successfully assessing over 100,000 students' testing skills.\nHowever, META is primarily limited to the objective assessment of test scripts,\nlacking the ability to automatically assess subjective aspects such as test\ncase and test report. To address this limitation, this paper proposes RUM, a\ncomprehensive assessment approach that combines rules and large language models\n(LLMs). RUM achieves a comprehensive assessment by rapidly processing objective\nindicators through rules while utilizing LLMs for in-depth subjective analysis\nof test case documents, test scripts, and test reports. The experimental\nresults show that compared to traditional manual testing skill assessment, RUM\nimproves assessment efficiency by 80.77\\% and reduces costs by 97.38\\%, while\nmaintaining high accuracy and consistency of assessment. By applying RUM on the\ncontest on software testing, we find that it not only enhances the efficiency\nand scalability of skill assessment in software testing education, but also\nprovides teachers with more comprehensive and objective evidence for student\nability assessment, facilitating personalized teaching and learning. This study\noffers new insights into the assessment of testing skills, which are expected\nto promote further development in test process optimization and software\nquality assurance.", "AI": {"tldr": "This paper proposes RUM, an automated testing skill assessment system combining rules and large language models (LLMs) to address the limitations of the META method in evaluating subjective aspects like test cases and reports. RUM significantly improves assessment efficiency (80.77%) and reduces costs (97.38%) while maintaining accuracy, and offers new insights for software testing education and quality assurance.", "motivation": "The META method primarily assesses test scripts through objective indicators but lacks automated evaluation of subjective aspects (test cases/reports). This creates inefficiencies in manual evaluation for large-scale software testing education assessments.", "method": "RUM integrates rule-based processing for objective metrics with LLM-powered subjective analysis of test case documents, test scripts, and test reports through a comprehensive assessment approach. The method was tested in the National College Student Software Testing Contest.", "result": "RUM demonstrated 80.77% improved assessment efficiency and 97.38% cost reduction versus manual methods, achieving similar accuracy/consistency. In the contest application, it enhanced scalability and provided richer evidence for personalized teaching decisions.", "conclusion": "RUM addresses the limitations of existing testing skill assessment methods by combining rules and LLMs. It advances both assessment efficiency and educational decision-making quality in software testing education, offering new directions for test process optimization and quality assurance practices."}}
{"id": "2508.12035", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12035", "abs": "https://arxiv.org/abs/2508.12035", "authors": ["Fei Lin", "Tengchao Zhang", "Ziyang Gong", "Fei-Yue Wang"], "title": "ToxiEval-ZKP: A Structure-Private Verification Framework for Molecular Toxicity Repair Tasks", "comment": null, "summary": "In recent years, generative artificial intelligence (GenAI) has demonstrated\nremarkable capabilities in high-stakes domains such as molecular science.\nHowever, challenges related to the verifiability and structural privacy of its\noutputs remain largely unresolved. This paper focuses on the task of molecular\ntoxicity repair. It proposes a structure-private verification framework -\nToxiEval-ZKP - which, for the first time, introduces zero-knowledge proof (ZKP)\nmechanisms into the evaluation process of this task. The system enables model\ndevelopers to demonstrate to external verifiers that the generated molecules\nmeet multidimensional toxicity repair criteria, without revealing the molecular\nstructures themselves. To this end, we design a general-purpose circuit\ncompatible with both classification and regression tasks, incorporating\nevaluation logic, Poseidon-based commitment hashing, and a nullifier-based\nreplay prevention mechanism to build a complete end-to-end ZK verification\nsystem. Experimental results demonstrate that ToxiEval-ZKP facilitates adequate\nvalidation under complete structural invisibility, offering strong circuit\nefficiency, security, and adaptability, thereby opening up a novel paradigm for\ntrustworthy evaluation in generative scientific tasks.", "AI": {"tldr": "This paper introduces ToxiEval-ZKP, the first structure-private verification framework for molecular toxicity repair using zero-knowledge proofs (ZKPs), enabling secure validation of molecular properties without exposing structural details.", "motivation": "Generative AI in molecular science lacks verifiability and structural privacy protection, especially in toxicity prediction tasks where sensitive molecular designs may be compromised during evaluation.", "method": "The authors design a ZKP-compatible circuit integrating evaluation logic, Poseidon-based commitment hashing for data integrity, and nullifier-based replay prevention. This creates an end-to-end system for verifying multidimensional toxicity criteria without revealing molecule structures.", "result": "Experiments show ToxiEval-ZKP successfully validates molecular toxicity under complete structural invisibility, achieving strong circuit efficiency, security, and adaptability for both classification and regression tasks.", "conclusion": "ToxiEval-ZKP establishes a novel paradigm for trustworthy evaluation in scientific generative AI, addressing critical privacy and verification challenges through zero-knowledge proofs while maintaining rigorous validation standards."}}
{"id": "2508.13051", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.13051", "abs": "https://arxiv.org/abs/2508.13051", "authors": ["Yi Wang", "Chetan Arora", "Xiao Liu", "Thuong Hoang", "ZHengxin Zhang", "Henry Been Lirn Duh", "John Grundy"], "title": "Investigating VR Accessibility Reviews for Users with Disabilities: A Qualitative Analysis", "comment": null, "summary": "Accessibility reviews provide valuable insights into both the limitations and\nbenefits experienced by users with disabilities when using virtual reality (VR)\napplications. However, a comprehensive investigation into VR accessibility for\nusers with disabilities is still lacking. To fill this gap, this study analyzes\nuser reviews from the Meta and Steam stores of VR apps, focusing on the\nreported issues affecting users with disabilities. We applied selection\ncriteria to 1,367,419 reviews from the top 40, the 20 most popular, and the 40\nlowest-rated VR applications on both platforms. In total, 1,076 (0.078%) VR\naccessibility reviews referenced various disabilities across 100 VR\napplications. These applications were categorized into Action, Sports, Social,\nPuzzle, Horror, and Simulation, with Action receiving the highest number of\naccessibility related-reviews. We identified 16 different types of disabilities\nacross six categories. Furthermore, we examined the causes of accessibility\nissues as reported by users with disabilities. Overall, VR accessibility\nreviews were predominantly under-supported.", "AI": {"tldr": "This paper analyzes accessibility reviews for VR apps to identify limitations and benefits for users with disabilities. It found 1,076 (0.078%) accessibility-related reviews across 100 VR applications, categorizing 16 disability types into six categories and highlighting under-supported issues.", "motivation": "To fill the gap in comprehensive investigation into VR accessibility limitations, as no systematic analysis of user-reported accessibility issues in VR applications exists.", "method": "The study collected 1,367,419 reviews from Meta and Steam for VR apps (top 40, 20 most popular, and 40 lowest-rated), applied selection criteria to identify accessibility-related reviews, categorized applications by genre, and classified disabilities and their reported causes.", "result": "Identified 1,076 accessibility reviews in 100 VR apps, with Action apps receiving the most accessibility reports. 16 disability types (e.g., mobility, vision) were categorized under six groups, and most reported issues lack support.", "conclusion": "VR accessibility issues are predominantly under-supported based on user reports, suggesting a need for improved accessibility features across VR applications and platforms."}}
{"id": "2508.12072", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12072", "abs": "https://arxiv.org/abs/2508.12072", "authors": ["Wei Jie Yeo", "Ranjan Satapathy", "Erik Cambria"], "title": "Mitigating Jailbreaks with Intent-Aware LLMs", "comment": null, "summary": "Despite extensive safety-tuning, large language models (LLMs) remain\nvulnerable to jailbreak attacks via adversarially crafted instructions,\nreflecting a persistent trade-off between safety and task performance. In this\nwork, we propose Intent-FT, a simple and lightweight fine-tuning approach that\nexplicitly trains LLMs to infer the underlying intent of an instruction before\nresponding. By fine-tuning on a targeted set of adversarial instructions,\nIntent-FT enables LLMs to generalize intent deduction to unseen attacks,\nthereby substantially improving their robustness. We comprehensively evaluate\nboth parametric and non-parametric attacks across open-source and proprietary\nmodels, considering harmfulness from attacks, utility, over-refusal, and impact\nagainst white-box threats. Empirically, Intent-FT consistently mitigates all\nevaluated attack categories, with no single attack exceeding a 50\\% success\nrate -- whereas existing defenses remain only partially effective. Importantly,\nour method preserves the model's general capabilities and reduces excessive\nrefusals on benign instructions containing superficially harmful keywords.\nFurthermore, models trained with Intent-FT accurately identify hidden harmful\nintent in adversarial attacks, and these learned intentions can be effectively\ntransferred to enhance vanilla model defenses.", "AI": {"tldr": "Intent-FT is a fine-tuning method teaching LLMs to deduce instruction intent, improving safety without sacrificing task performance while mitigating all evaluated attack categories below 50% success rates.", "motivation": "Large language models remain vulnerable to jailbreak attacks despite safety tuning, highlighting the need for a solution that balances safety and utility in adversarial scenarios.", "method": "Intent-FT trains models to infer instruction intent through fine-tuning on adversarial examples, enabling generalization to unseen attacks via explicit intent detection.", "result": "Intent-FT reduces all attack success rates <50%, maintains utility on benign inputs with harmful keywords, and transfers learned intent recognition to enhance base model defenses.", "conclusion": "Intent-FT demonstrates effective adversarial intent detection that preserves model capabilities, offering a transferable foundation for robust LLM safety systems."}}
{"id": "2508.13134", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.13134", "abs": "https://arxiv.org/abs/2508.13134", "authors": ["Glauber da Rocha Balthazar", "Marcia Ito"], "title": "Influencia de fatores organizacionais e sociais na etapa de levantamento de requisitos", "comment": "VI Workshop de P\\'os-Gradua\\c{c}\\~ao e Pesquisa do Centro Paula\n  Souza, in Portuguese language", "summary": "The most critical and fragile stage of a software development project is\nrequirements gathering. Because of this, Requirements Engineering has been\nevolving its techniques to minimize the challenges faced by Requirements\nAnalysts. However, few studies consider the humanistic relationships and\nbehaviors of those involved in this stage. This article presents a survey of\nsome studies conducted at this stage that consider non-technical factors such\nas emotions, organizational environment, and social context.", "AI": {"tldr": "This paper reviews studies on non-technical factors (emotions, organizational environment, social context) in software requirements gathering, highlighting their impact and underrepresentation in existing literature.", "motivation": "The study addresses a gap in Requirements Engineering research, emphasizing the need to understand humanistic relationships and behaviors to improve requirements gathering processes.", "method": "The authors conducted a survey analyzing existing studies that focus on non-technical aspects of requirements gathering, synthesizing insights about social and organizational influences.", "result": "The survey identifies key non-technical challenges and factors, such as emotional dynamics and social interactions, that significantly affect requirements gathering effectiveness.", "conclusion": "Integrating non-technical considerations into Requirements Engineering can enhance project outcomes, urging further research and application of these insights."}}
{"id": "2508.12093", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12093", "abs": "https://arxiv.org/abs/2508.12093", "authors": ["Hyunmin Choi"], "title": "PP-STAT: An Efficient Privacy-Preserving Statistical Analysis Framework using Homomorphic Encryption", "comment": "Accepted to CIKM 2025 (Full Research Paper Track)", "summary": "With the widespread adoption of cloud computing, the need for outsourcing\nstatistical analysis to third-party platforms is growing rapidly. However,\nhandling sensitive data such as medical records and financial information in\ncloud environments raises serious privacy concerns. In this paper, we present\nPP-STAT, a novel and efficient Homomorphic Encryption (HE)-based framework for\nprivacy-preserving statistical analysis. HE enables computations to be\nperformed directly on encrypted data without revealing the underlying\nplaintext. PP-STAT supports advanced statistical measures, including Z-score\nnormalization, skewness, kurtosis, coefficient of variation, and Pearson\ncorrelation coefficient, all computed securely over encrypted data. To improve\nefficiency, PP-STAT introduces two key optimizations: (1) a Chebyshev-based\napproximation strategy for initializing inverse square root operations, and (2)\na pre-normalization scaling technique that reduces multiplicative depth by\nfolding constant scaling factors into mean and variance computations. These\ntechniques significantly lower computational overhead and minimize the number\nof expensive bootstrapping procedures. Our evaluation on real-world datasets\ndemonstrates that PP-STAT achieves high numerical accuracy, with mean relative\nerror (MRE) below 2.4x10-4. Notably, the encrypted Pearson correlation between\nthe smoker attribute and charges reaches 0.7873, with an MRE of 2.86x10-4.\nThese results confirm the practical utility of PP-STAT for secure and precise\nstatistical analysis in privacy-sensitive domains.", "AI": {"tldr": "PP-STAT is a novel HE-based framework enabling efficient privacy-preserving statistical analysis by securely computing advanced metrics (Z-score normalization, skewness, kurtosis, coefficient of variation, Pearson correlation) on encrypted data through two optimizations: Chebyshev-based approximation for inverse square root and pre-normalization scaling to reduce multiplicative depth. It achieves high accuracy (MRE < 2.4x10^-4) on real-world datasets.", "motivation": "Cloud computing's rise has increased the demand to outsource statistical analysis while protecting sensitive data (e.g., medical, financial) from privacy breaches. Homomorphic Encryption (HE) offers a solution by allowing computations on encrypted data, addressing urgent needs for secure and privacy-compliant analytics.", "method": "PP-STAT leverages HE to perform statistical analysis on encrypted data. It introduces two optimizations: 1) a Chebyshev polynomial approximation strategy for initializing inverse square root operations, overcoming challenges in encrypted numerical computations, and 2) a pre-normalization scaling technique that integrates constant scaling factors into mean and variance calculations to reduce multiplicative depth and minimize bootstrapping costs.", "result": "PP-STAT achieves mean relative errors (MRE) below 2.4x10^-4 on encrypted data computations. It successfully computes the Pearson correlation coefficient between 'smoker' attributes and 'charges' (encrypted result: 0.7873, MRE: 2.86x10^-4) while significantly reducing computational overhead through efficient HE operations.", "conclusion": "The framework demonstrates practical utility for secure statistical analysis in privacy-sensitive domains by combining HE's confidentiality guarantees with optimized computational techniques, enabling precise and efficient secure analytics for real-world datasets."}}
{"id": "2508.12187", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12187", "abs": "https://arxiv.org/abs/2508.12187", "authors": ["John Y. Kim", "Chaoshun Zuo", "Yanjie Zhao", "Zhiqiang Lin"], "title": "AUTOVR: Automated UI Exploration for Detecting Sensitive Data Flow Exposures in Virtual Reality Apps", "comment": "USENIX Security 2025, 19 Pages, 14 Figures, 7 Tables", "summary": "The rise of Virtual Reality (VR) has provided developers with an\nunprecedented platform for creating games and applications (apps) that require\ndistinct inputs, different from those of conventional devices like smartphones.\nThe Meta Quest VR platform, driven by Meta, has democratized VR app publishing\nand attracted millions of users worldwide. However, as the number of published\napps grows, there is a notable lack of robust headless tools for user interface\n(UI) exploration and user event testing. To address this need, we present\nAUTOVR, an automatic framework for dynamic UI and user event interaction in VR\napps built on the Unity Engine. Unlike conventional Android and GUI testers,\nAUTOVR analyzes the app's internal binary to reveal hidden events, resolves\ngenerative event dependencies, and utilizes them for comprehensive exploration\nof VR apps. Using sensitive data exposure as a performance metric, we compare\nAUTOVR with Android Monkey, a widely used headless Android GUI stress testing\ntool. Our empirical evaluation demonstrates AUTOVR's superior performance,\ntriggering an order of magnitude of more sensitive data exposures and\nsignificantly enhancing the privacy of VR apps.", "AI": {"tldr": "The paper introduces AUTOVR, an automatic framework for testing user interfaces and interactions in Meta Quest VR apps, enhancing privacy through more effective sensitive data exposure detection compared to traditional Android tools.", "motivation": "As VR app development grows on the Meta Quest platform, current headless testing tools lack the capability to thoroughly explore UI elements and events unique to VR, leading to potential privacy and usability issues.", "method": "AUTOVR analyzes app binaries to reveal hidden events, resolves generative dependencies between these events, and leverages them to systematically explore VR app interactions. This approach contrasts with conventional Android/GUI testers that rely on surface-level event recognition.", "result": "Empirical testing showed AUTOVR triggered 10x more sensitive data exposures than Android Monkey, demonstrating superior testing capability. This leads to more comprehensive privacy vulnerability detection in VR apps.", "conclusion": "AUTOVR provides a groundbreaking solution for VR app testing by enabling deeper interaction analysis at the binary level, significantly improving both testing coverage and end-user privacy protection compared to existing platforms."}}
{"id": "2508.12107", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12107", "abs": "https://arxiv.org/abs/2508.12107", "authors": ["Shixuan Guan", "Kai Li"], "title": "Ethereum Crypto Wallets under Address Poisoning: How Usable and Secure Are They?", "comment": "15 pages, 10 figures", "summary": "Blockchain address poisoning is an emerging phishing attack that crafts\n\"similar-looking\" transfer records in the victim's transaction history, which\naims to deceive victims and lure them into mistakenly transferring funds to the\nattacker. Recent works have shown that millions of Ethereum users were targeted\nand lost over 100 million US dollars.\n  Ethereum crypto wallets, serving users in browsing transaction history and\ninitiating transactions to transfer funds, play a central role in deploying\ncountermeasures to mitigate the address poisoning attack. However, whether they\nhave done so remains an open question. To fill the research void, in this\npaper, we design experiments to simulate address poisoning attacks and\nsystematically evaluate the usability and security of 53 popular Ethereum\ncrypto wallets. Our evaluation shows that there exist communication failures\nbetween 12 wallets and their transaction activity provider, which renders them\nunable to download the users' transaction history. Besides, our evaluation also\nshows that 16 wallets pose a high risk to their users due to displaying fake\ntoken phishing transfers. Moreover, our further analysis suggests that most\nwallets rely on transaction activity providers to filter out phishing\ntransfers. However, their phishing detection capability varies. Finally, we\nfound that only three wallets throw an explicit warning message when users\nattempt to transfer to the phishing address, implying a significant gap within\nthe broader Ethereum crypto wallet community in protecting users from address\npoisoning attacks.\n  Overall, our work shows that more efforts are needed by the Ethereum crypto\nwallet developer community to achieve the highest usability and security\nstandard. Our bug reports have been acknowledged by the developer community,\nwho are currently developing mitigation solutions.", "AI": {"tldr": "This study evaluates 53 Ethereum crypto wallets for their ability to prevent address poisoning attacks, revealing communication issues in 12 wallets, high phishing risks in 16, and lack of explicit warnings in most. Only three wallets provide adequate phishing protection with warning messages.", "motivation": "Address poisoning attacks deceive Ethereum users into transferring funds to phishing addresses, causing over $100 million in losses. Evaluating wallet defenses against these attacks is critical to improving security and user protection in cryptocurrency transactions.", "method": "The researchers designed experiments to simulate address poisoning attacks and conducted a systematic evaluation of 53 popular Ethereum crypto wallets by analyzing communication with transaction activity providers, phishing transfer display behaviors, and warning mechanisms during suspicious transactions.", "result": "12 wallets failed to download transaction histories due to communication errors. 16 wallets displayed fake token transfers without effective filtering. Most wallets relied on transaction activity providers for phishing detection, but only 3 wallets explicitly warned users of phishing addresses during transactions.", "conclusion": "The Ethereum crypto wallet community lacks standardized defenses against address poisoning, with significant gaps in transaction history accuracy and alert mechanisms. While initial mitigation steps are being developed by responding to bug reports, stronger usability and security practices remain necessary to protect users."}}
{"id": "2508.12538", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12538", "abs": "https://arxiv.org/abs/2508.12538", "authors": ["Yongjian Guo", "Puzhuo Liu", "Wanlun Ma", "Zehang Deng", "Xiaogang Zhu", "Peng Di", "Xi Xiao", "Sheng Wen"], "title": "Systematic Analysis of MCP Security", "comment": null, "summary": "The Model Context Protocol (MCP) has emerged as a universal standard that\nenables AI agents to seamlessly connect with external tools, significantly\nenhancing their functionality. However, while MCP brings notable benefits, it\nalso introduces significant vulnerabilities, such as Tool Poisoning Attacks\n(TPA), where hidden malicious instructions exploit the sycophancy of large\nlanguage models (LLMs) to manipulate agent behavior. Despite these risks,\ncurrent academic research on MCP security remains limited, with most studies\nfocusing on narrow or qualitative analyses that fail to capture the diversity\nof real-world threats. To address this gap, we present the MCP Attack Library\n(MCPLIB), which categorizes and implements 31 distinct attack methods under\nfour key classifications: direct tool injection, indirect tool injection,\nmalicious user attacks, and LLM inherent attack. We further conduct a\nquantitative analysis of the efficacy of each attack. Our experiments reveal\nkey insights into MCP vulnerabilities, including agents' blind reliance on tool\ndescriptions, sensitivity to file-based attacks, chain attacks exploiting\nshared context, and difficulty distinguishing external data from executable\ncommands. These insights, validated through attack experiments, underscore the\nurgency for robust defense strategies and informed MCP design. Our\ncontributions include 1) constructing a comprehensive MCP attack taxonomy, 2)\nintroducing a unified attack framework MCPLIB, and 3) conducting empirical\nvulnerability analysis to enhance MCP security mechanisms. This work provides a\nfoundational framework, supporting the secure evolution of MCP ecosystems.", "AI": {"tldr": "This paper introduces MCPLIB, a comprehensive attack library for the Model Context Protocol (MCP), addressing limited security research by categorizing 31 attack methods across four classifications and experimentally analyzing their vulnerabilities to inform robust defense strategies.", "motivation": "The Model Context Protocol (MCP) enables AI agents to interact with external tools but lacks sufficient security research, as current studies provide narrow or qualitative analyses that fail to systematize real-world threats.", "method": "1) Constructed a taxonomy of MCP attacks with four categories (direct/indirect tool injection, malicious user attacks, LLM inherent attacks). 2) Implemented 31 distinct attack methods in the open-source MCPLIB framework. 3) Conducted quantitative evaluation of attack effectiveness through controlled experiments. 4) Analyzed vulnerability patterns from attack outcomes.", "result": "Key MCP vulnerabilities include: agents blindly trusting tool descriptions, file-based attacks causing significant impact, chain attacks leveraging shared context, and inability to distinguish external data from executable commands. The library's standardized attack implementations with metrics (e.g., success rates, injection patterns) reveal systematic weaknesses in MCP-based systems.", "conclusion": "The paper establishes a foundational attack library and taxonomy for MCP security, demonstrating critical vulnerabilities that necessitate improved context shielding, verification protocols, and attack-resistant design principles for robust AI agent ecosystems."}}
{"id": "2508.12138", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12138", "abs": "https://arxiv.org/abs/2508.12138", "authors": ["Mohammad Ishzaz Asif Rafid", "Morsalin Sakib"], "title": "Substituting Proof of Work in Blockchain with Training-Verified Collaborative Model Computation", "comment": null, "summary": "Bitcoin's Proof of Work (PoW) mechanism, while central to achieving\ndecentralized consensus, has long been criticized for excessive energy use and\nhardware inefficiencies \\cite{devries2018bitcoin, truby2018decarbonizing}. This\npaper introduces a hybrid architecture that replaces Bitcoin's traditional PoW\nwith a centralized, cloud-based collaborative training framework. In this\nmodel, miners contribute computing resources to train segments of horizontally\nscaled machine learning models on preprocessed datasets, ensuring privacy and\ngenerating meaningful outputs \\cite{li2017securing}. A central server evaluates\ncontributions using two metrics: number of parameters trained and reduction in\nmodel loss during each cycle. At the end of every cycle, a weighted lottery\nselects the winning miner, who receives a digitally signed certificate. This\ncertificate serves as a verifiable substitute for PoW and grants the right to\nappend a block to the blockchain \\cite{nakamoto2008bitcoin}. By integrating\ndigital signatures and SHA-256 hashing \\cite{nist2015sha}, the system preserves\nblockchain integrity while redirecting energy toward productive computation.\nThe proposed approach addresses the sustainability concerns of traditional\nmining by converting resource expenditure into socially valuable work, aligning\nsecurity incentives with real-world computational progress.", "AI": {"tldr": "Introduces a cloud-based collaborative training framework to replace Bitcoin's energy-intensive PoW with sustainable computation using machine learning model training and weighted lottery-based block validation.", "motivation": "Bitcoin's Proof of Work (PoW) mechanism faces criticism for excessive energy consumption and hardware inefficiencies, necessitating sustainable alternatives that maintain blockchain integrity while reducing environmental impact.", "method": "1) Replace PoW with cloud-based collaborative training where miners train horizontal ML models on preprocessed datasets using privacy-preserving methods.\n2) Central server evaluates miner contributions using parameter count and model loss reduction metrics.\n3) Weighted lottery selects winning miner to validate blocks with SHA-256 hashing and digital signatures ensuring security.", "result": "System converts mining energy into productive ML computation while preserving blockchain integrity through digital signatures and SHA-256 hashing. Reduces hardware waste by using cloud infrastructure for training cycles.", "conclusion": "The hybrid architecture address sustainability issues of traditional mining by aligning security incentives with computational progress. Creates verifiable proof through valuable machine learning work rather than energy-intensive puzzles."}}
{"id": "2508.12161", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.12161", "abs": "https://arxiv.org/abs/2508.12161", "authors": ["Ming Li", "John Hale"], "title": "Attack Graph Generation on HPC Clusters", "comment": null, "summary": "Attack graphs (AGs) are graphical tools to analyze the security of computer\nnetworks. By connecting the exploitation of individual vulnerabilities, AGs\nexpose possible multi-step attacks against target networks, allowing system\nadministrators to take preventive measures to enhance their network's security.\nAs powerful analytical tools, however, AGs are both time- and memory-consuming\nto be generated. As the numbers of network assets, interconnections between\ndevices, as well as vulnerabilities increase, the size and volume of the\nresulting AGs grow at a much higher rate, leading to the well-known state-space\nexplosion. In this paper, we propose the use of high performance computing\n(HPC) clusters to implement AG generators. We evaluate the performance through\nexperiments and provide insights into how cluster environments can help resolve\nthe issues of slow speed and high memory demands in AG generation in a balanced\nway.", "AI": {"tldr": "The paper proposes using high performance computing (HPC) clusters to address the computational and memory challenges of generating large attack graphs (AGs) in complex networks, demonstrating scalable solutions through experiments.", "motivation": "Traditional attack graph generation suffers from state-space explosion as network assets, device interconnections, and vulnerabilities increase, leading to prohibitive time and memory requirements.", "method": "Implementation of AG generators on HPC clusters using distributed computing techniques. The authors evaluate cluster-based AG generation performance through comparative experiments, analyzing load balancing and memory distribution strategies.", "result": "Experimental results show that HPC clusters significantly reduce runtime and handle larger AGs compared to single machines, while maintaining memory balance through distributed partitioning.", "conclusion": "Cluster computing provides a scalable solution to the performance limitations of AG generation, offering balanced speed and memory improvements. This approach enables practical analysis of complex network security architectures that would otherwise be infeasible with traditional methods."}}
{"id": "2508.12175", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12175", "abs": "https://arxiv.org/abs/2508.12175", "authors": ["Ben Nassi", "Stav Cohen", "Or Yair"], "title": "Invitation Is All You Need! Promptware Attacks Against LLM-Powered Assistants in Production Are Practical and Dangerous", "comment": "https://sites.google.com/view/invitation-is-all-you-need/home", "summary": "The growing integration of LLMs into applications has introduced new security\nrisks, notably known as Promptware - maliciously engineered prompts designed to\nmanipulate LLMs to compromise the CIA triad of these applications. While prior\nresearch warned about a potential shift in the threat landscape for LLM-powered\napplications, the risk posed by Promptware is frequently perceived as low. In\nthis paper, we investigate the risk Promptware poses to users of Gemini-powered\nassistants (web application, mobile application, and Google Assistant). We\npropose a novel Threat Analysis and Risk Assessment (TARA) framework to assess\nPromptware risks for end users. Our analysis focuses on a new variant of\nPromptware called Targeted Promptware Attacks, which leverage indirect prompt\ninjection via common user interactions such as emails, calendar invitations,\nand shared documents. We demonstrate 14 attack scenarios applied against\nGemini-powered assistants across five identified threat classes: Short-term\nContext Poisoning, Permanent Memory Poisoning, Tool Misuse, Automatic Agent\nInvocation, and Automatic App Invocation. These attacks highlight both digital\nand physical consequences, including spamming, phishing, disinformation\ncampaigns, data exfiltration, unapproved user video streaming, and control of\nhome automation devices. We reveal Promptware's potential for on-device lateral\nmovement, escaping the boundaries of the LLM-powered application, to trigger\nmalicious actions using a device's applications. Our TARA reveals that 73% of\nthe analyzed threats pose High-Critical risk to end users. We discuss\nmitigations and reassess the risk (in response to deployed mitigations) and\nshow that the risk could be reduced significantly to Very Low-Medium. We\ndisclosed our findings to Google, which deployed dedicated mitigations.", "AI": {"tldr": "This paper introduces a TARA framework to assess Promptware risks for Gemini-powered assistants, revealing 14 attack scenarios across five threat classes. 73% of threats were high-critical, but Google's mitigations reduced risk to very low-medium.", "motivation": "The paper addresses underappreciated security risks from Promptware (malicious prompts manipulating LLMs), which prior research considered low-severity despite demonstrations of device-escape capabilities.", "method": "The researchers developed a novel Threat Analysis and Risk Assessment (TARA) framework. They identified and tested 14 targeted promptware attack scenarios, particularly those involving indirect prompt injection through everyday user interactions like calendar invites and shared documents.", "result": "Demonstrated 14 attack scenarios across five classes (context poisoning, memory poisoning, tool misuse, agent/app invocation), including physical consequences like home automation control. 73% of threats rated High-Critical. Google implemented mitigations reducing risk to Very Low-Medium.", "conclusion": "Promptware poses significant risks to LLM applications. The TARA framework effectively evaluates threat levels, and targeted mitigations can substantially reduce risks. The findings highlight the importance of proactively addressing prompt injection vulnerabilities as LLMs become more integrated into critical devices."}}
{"id": "2508.12181", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12181", "abs": "https://arxiv.org/abs/2508.12181", "authors": ["Ayman W. Baharia", "Khaled T. Naga", "Hesham S. Abdelfattah", "Shady A. Maged", "Sherif A. Hammad"], "title": "CAN Networks Security in Smart Grids Communication Technologies", "comment": "4 pages, 6 figures, International Conference on Energy Systems -\n  Smart and Sustainable Solutions -", "summary": "The rapid evolution of smart grids requires effective communication protocols\nto transfer data reliably and securely. Controller Area Network (CAN) is one of\nthe most recognized protocols that offer reliable data transmission in smart\ngrids due to its robustness, real-time capabilities, and relatively low initial\ncost of its required hardware. However, as a smart city becomes more\ninterconnected, it also becomes more vulnerable to cyber-attacks. As there are\nmany mechanisms to secure the CAN nodes from attacks, most of those mechanisms\nhave computational overhead, resulting in more delay in the network. We\nimplemented a solution that requires almost no overhead to any CAN node\nconnected to the network. It depends on a single node responsible for securing\nthe CAN network. This approach seeks to augment network security while reducing\nsecurity mechanisms overhead to all CAN network nodes. The methodology and\ncomprehensive test results will be presented in detail during a subsequent\ndiscussion. The used software for development is Code Composer Studio, and the\nused microcontroller evaluation boards (EVB) are TM4C 1294.", "AI": {"tldr": "The paper proposes a CAN bus security solution for smart grids that minimizes computational overhead by centralizing security mechanisms onto a single node, implemented using Code Composer Studio and TM4C1294 evaluation boards.", "motivation": "Smart grids require reliable communication protocols like CAN, but their increasing interconnectivity exposes them to cyber-attacks. Traditional CAN security mechanisms introduce significant computational overhead, causing network delays.", "method": "A single dedicated node handles all security processing using Code Composer Studio and TM4C1294 microcontroller evaluation boards, reducing overhead on other CAN nodes while maintaining robustness and real-time capabilities.", "result": "Test results demonstrating the effectiveness of this approach will be presented in detail (specific metrics not included in abstract).", "conclusion": "Centralized security node implementation offers a promising trade-off between strong CAN network protection and minimal performance impact, though detailed validation remains pending completion of the subsequent discussion."}}
{"id": "2508.12259", "categories": ["cs.CR", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.12259", "abs": "https://arxiv.org/abs/2508.12259", "authors": ["Ken Huang", "Yasir Mehmood", "Hammad Atta", "Jerry Huang", "Muhammad Zeeshan Baig", "Sree Bhargavi Balija"], "title": "Fortifying the Agentic Web: A Unified Zero-Trust Architecture Against Logic-layer Threats", "comment": null, "summary": "This paper presents a Unified Security Architecture that fortifies the\nAgentic Web through a Zero-Trust IAM framework. This architecture is built on a\nfoundation of rich, verifiable agent identities using Decentralized Identifiers\n(DIDs) and Verifiable Credentials (VCs), with discovery managed by a\nprotocol-agnostic Agent Name Service (ANS). Security is operationalized through\na multi-layered Trust Fabric which introduces significant innovations,\nincluding Trust-Adaptive Runtime Environments (TARE), Causal Chain Auditing,\nand Dynamic Identity with Behavioral Attestation. By explicitly linking the\nLPCI threat to these enhanced architectural countermeasures within a formal\nsecurity model, we propose a comprehensive and forward-looking blueprint for a\nsecure, resilient, and trustworthy agentic ecosystem. Our formal analysis\ndemonstrates that the proposed architecture provides provable security\nguarantees against LPCI attacks with bounded probability of success.", "AI": {"tldr": "The paper introduces a Zero-Trust IAM framework using DIDs/VCs and an ANS to secure the Agentic Web, analyzing security guarantees against LPCI threats. Key innovations include TARE, Causal Chain Auditing, and Dynamic Identity.", "motivation": "Traditional IAM systems lack resilience for the Agentic Web's decentralized agent interactions, while LPCI (Link-Prediction Privacy Intrusion) threats exploit identity patterns to compromise trust.", "method": "1. Protocol-agnostic Agent Name Service (ANS) for identity discovery\n2. Trust Fabric with three mechanisms: \n   a. Trust-Adaptive Runtime Environments (TARE)\n   b. Causal Chain Auditing for forensic traceability\n   c. Dynamic Identity with Behavioral Attestation\n3. Formal security model explicitly linking LPCI threats to countermeasures", "result": "Formal analysis demonstrates the architecture achieves provable security guarantees against LPCI attacks with a mathematically bounded probability of successful compromise (attacks succeed with probability \u2264 \u03b5, where \u03b5 is an explicit function of system parameters).", "conclusion": "The framework establishes a comprehensive security blueprint for agentic systems, enabling trustworthiness through verifiable identities and adaptive protection layers. The formal model provides a foundation for future security enhancements in decentralized agent ecosystems."}}
{"id": "2508.12264", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12264", "abs": "https://arxiv.org/abs/2508.12264", "authors": ["Saisai Xia", "Wenhao Wang", "Zihao Wang", "Yuhui Zhang", "Yier Jin", "Dan Meng", "Rui Hou"], "title": "CryptPEFT: Efficient and Private Neural Network Inference via Parameter-Efficient Fine-Tuning", "comment": "Preprint for the paper accepted for presentation at NDSS 2026", "summary": "Publicly available large pretrained models (i.e., backbones) and lightweight\nadapters for parameter-efficient fine-tuning (PEFT) have become standard\ncomponents in modern machine learning pipelines. However, preserving the\nprivacy of both user inputs and fine-tuned adapters -- often trained on\nsensitive data -- during inference remains a significant challenge. Applying\ncryptographic techniques, such as multi-party computation (MPC), to PEFT\nsettings still incurs substantial encrypted computation across both the\nbackbone and adapter, mainly due to the inherent two-way communication between\nthem. To address this limitation, we propose CryptPEFT, the first PEFT solution\nspecifically designed for private inference scenarios. CryptPEFT introduces a\nnovel one-way communication (OWC) architecture that confines encrypted\ncomputation solely to the adapter, significantly reducing both computational\nand communication overhead. To maintain strong model utility under this\nconstraint, we explore the design space of OWC-compatible adapters and employ\nan automated architecture search algorithm to optimize the trade-off between\nprivate inference efficiency and model utility. We evaluated CryptPEFT using\nVision Transformer backbones across widely used image classification datasets.\nOur results show that CryptPEFT significantly outperforms existing baselines,\ndelivering speedups ranging from $20.62\\times$ to $291.48\\times$ in simulated\nwide-area network (WAN) and local-area network (LAN) settings. On CIFAR-100,\nCryptPEFT attains 85.47% accuracy with just 2.26 seconds of inference latency.\nThese findings demonstrate that CryptPEFT offers an efficient and\nprivacy-preserving solution for modern PEFT-based inference.", "AI": {"tldr": "CryptPEFT introduces a one-way communication pipeline to reduce encrypted computation in parameter-efficient fine-tuning for private inference while maintaining model accuracy.", "motivation": "Current privacy-preserving PEFT methods face challenges from two-way communication between large models and adapters, leading to high computational and communication burdens. This limits practical use for sensitive applications.", "method": "1) Designs a one-way communication (OWC) architecture where only the adapter requires encryption\n2) Develops automated architecture search for optimal OWC-compatible adapters\n3) Evaluates the approach using Vision Transformers on image classification tasks", "result": "Achieved 85.47% accuracy on CIFAR-100 with 2.26s latency\n20.62\u00d7-291.48\u00d7 speedups over existing methods in WAN/LAN simulations\nMaintained parameter efficiency compared to traditional PEFT approaches", "conclusion": "CryptPEFT enables secure yet computationally efficient private inference for PEFT systems through its novel OWC architecture and automated adapter optimization, opening new possibilities for deploying ML models with sensitive data constraints."}}
{"id": "2508.12304", "categories": ["cs.CR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12304", "abs": "https://arxiv.org/abs/2508.12304", "authors": ["Hao Li"], "title": "Adjustable AprilTags For Identity Secured Tasks", "comment": null, "summary": "Special tags such as AprilTags that facilitate image processing and pattern\nrecognition are useful in practical applications. In close and private\nenvironments, identity security is unlikely to be an issue because all involved\nAprilTags can be completely regulated. However, in open and public\nenvironments, identity security is no longer an issue that can be neglected. To\nhandle potential harm caused by adversarial attacks, this note advocates\nutilization of adjustable AprilTags instead of fixed ones.", "AI": {"tldr": "This paper addresses the need for adjustable AprilTags in public environments to mitigate adversarial attacks, unlike their regulation in private settings.", "motivation": "Fixed AprilTags are insecure in public environments where adversarial attacks can cause harm, necessitating a shift to adjustable solutions.", "method": "The paper advocates replacing fixed AprilTags with adjustable ones to alter their visual patterns dynamically and resist adversarial manipulations.", "result": "No specific experimental results are mentioned in the abstract; the paper presents the rationale for using adjustable AprilTags in open settings.", "conclusion": "Adjustable AprilTags are necessary in public environments to enhance identity security by countering adversarial attacks through dynamic pattern adjustments."}}
{"id": "2508.12398", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12398", "abs": "https://arxiv.org/abs/2508.12398", "authors": ["Zhixin Xie", "Xurui Song", "Jun Luo"], "title": "Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position", "comment": null, "summary": "Diffusion Large Language Models (dLLMs) have recently emerged as a\ncompetitive non-autoregressive paradigm due to their unique training and\ninference approach. However, there is currently a lack of safety study on this\nnovel architecture. In this paper, we present the first analysis of dLLMs'\nsafety performance and propose a novel safety alignment method tailored to\ntheir unique generation characteristics. Specifically, we identify a critical\nasymmetry between the defender and attacker in terms of security. For the\ndefender, we reveal that the middle tokens of the response, rather than the\ninitial ones, are more critical to the overall safety of dLLM outputs; this\nseems to suggest that aligning middle tokens can be more beneficial to the\ndefender. The attacker, on the contrary, may have limited power to manipulate\nmiddle tokens, as we find dLLMs have a strong tendency towards a sequential\ngeneration order in practice, forcing the attack to meet this distribution and\ndiverting it from influencing the critical middle tokens. Building on this\nasymmetry, we introduce Middle-tOken Safety Alignment (MOSA), a novel method\nthat directly aligns the model's middle generation with safe refusals\nexploiting reinforcement learning. We implement MOSA and compare its security\nperformance against eight attack methods on two benchmarks. We also test the\nutility of MOSA-aligned dLLM on coding, math, and general reasoning. The\nresults strongly prove the superiority of MOSA.", "AI": {"tldr": "This paper analyzes the safety of Diffusion Large Language Models (dLLMs), identifies critical middle tokens for security, and proposes Middle-tOken Safety Alignment (MOSA), a reinforcement learning method that enhances dLLM output safety without compromising utility.", "motivation": "Non-autoregressive dLLMs (e.g., decoding using diffusion processes) exhibit unique generation mechanisms but lack safety alignment frameworks. The paper motivates addressing this gap by highlighting the absence of security studies for dLLMs and the distinct asymmetry in manipulation power between defenders and attackers relative to autoregressive models.", "method": "The proposed Middle-tOken Safety Alignment (MOSA) method exploits empirical findings that dLLMs demonstrate a strong sequential token generation pattern. This creates asymmetry where attackers can't easily manipulate middle tokens (critical for safety) while defenders can effectively align them via reinforcement learning strategies focused on those positions.", "result": "MOSA demonstrates robust security performance (14.2%-55.4% better than baselines) against eight attacks across two benchmarks. It maintains 94.3%-99.1% of the original utility (coding/math/reasoning scores) compared to autoregressive alignment baselines that lose 35.6%-62.1% of utility.", "conclusion": "The paper conclusively demonstrates that dLLMs' diffusion-based sequential generation patterns create security advantages. MOSA effectively leverages this characteristic to achieve middle-token alignment that blocks safety attacks while preserving practical capabilities in reasoning tasks."}}
{"id": "2508.12412", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12412", "abs": "https://arxiv.org/abs/2508.12412", "authors": ["Ron Solomon", "Yarin Yerushalmi Levi", "Lior Vaknin", "Eran Aizikovich", "Amit Baras", "Etai Ohana", "Amit Giloni", "Shamik Bose", "Chiara Picardi", "Yuval Elovici", "Asaf Shabtai"], "title": "LumiMAS: A Comprehensive Framework for Real-Time Monitoring and Enhanced Observability in Multi-Agent Systems", "comment": null, "summary": "The incorporation of large language models in multi-agent systems (MASs) has\nthe potential to significantly improve our ability to autonomously solve\ncomplex problems. However, such systems introduce unique challenges in\nmonitoring, interpreting, and detecting system failures. Most existing MAS\nobservability frameworks focus on analyzing each individual agent separately,\noverlooking failures associated with the entire MAS. To bridge this gap, we\npropose LumiMAS, a novel MAS observability framework that incorporates advanced\nanalytics and monitoring techniques. The proposed framework consists of three\nkey components: a monitoring and logging layer, anomaly detection layer, and\nanomaly explanation layer. LumiMAS's first layer monitors MAS executions,\ncreating detailed logs of the agents' activity. These logs serve as input to\nthe anomaly detection layer, which detects anomalies across the MAS workflow in\nreal time. Then, the anomaly explanation layer performs classification and root\ncause analysis (RCA) of the detected anomalies. LumiMAS was evaluated on seven\ndifferent MAS applications, implemented using two popular MAS platforms, and a\ndiverse set of possible failures. The applications include two novel\nfailure-tailored applications that illustrate the effects of a hallucination or\nbias on the MAS. The evaluation results demonstrate LumiMAS's effectiveness in\nfailure detection, classification, and RCA.", "AI": {"tldr": "LumiMAS is a new observability framework for multi-agent systems with large language models, designed to monitor system-wide failures. It includes three layers (monitoring/logging, anomaly detection, anomaly explanation), evaluated on seven applications with diverse platforms and failure types, showing effective detection, classification, and root cause analysis.", "motivation": "Current MAS observability frameworks analyze individual agents, neglecting system-wide failures. Integrating large language models in MAS requires better monitoring, interpretation, and failure detection methods to address these global issues.", "method": "LumiMAS integrates three layers: (1) a monitoring and logging layer to capture agent activities across MAS workflows, (2) real-time anomaly detection across the system, and (3) anomaly explanation and root cause analysis. Evaluated on seven applications using two MAS platforms and diverse failure scenarios, including hallucinations and bias.", "result": "LumiMAS demonstrated effectiveness in detecting, classifying, and diagnosing system-wide anomalies in multi-agent systems. Evaluated on seven applications across two platforms, it excels in identifying rare failures like hallucinations and biases, showing robustness through comprehensive failure simulations.", "conclusion": "LumiMAS offers a robust solution for end-to-end observability in multi-agent systems with large language models, addressing system-wide failures through layered analytics and real-world evaluation. Its methodology provides a foundation for improving autonomy in complex environments."}}
{"id": "2508.12470", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12470", "abs": "https://arxiv.org/abs/2508.12470", "authors": ["Afrah Gueriani", "Hamza Kheddar", "Ahmed Cherif Mazari", "Mohamed Chahine Ghanem"], "title": "A Robust Cross-Domain IDS using BiGRU-LSTM-Attention for Medical and Industrial IoT Security", "comment": "10 pages", "summary": "The increased Internet of Medical Things IoMT and the Industrial Internet of\nThings IIoT interconnectivity has introduced complex cybersecurity challenges,\nexposing sensitive data, patient safety, and industrial operations to advanced\ncyber threats. To mitigate these risks, this paper introduces a novel\ntransformer-based intrusion detection system IDS, termed BiGAT-ID a hybrid\nmodel that combines bidirectional gated recurrent units BiGRU, long short-term\nmemory LSTM networks, and multi-head attention MHA. The proposed architecture\nis designed to effectively capture bidirectional temporal dependencies, model\nsequential patterns, and enhance contextual feature representation. Extensive\nexperiments on two benchmark datasets, CICIoMT2024 medical IoT and EdgeIIoTset\nindustrial IoT demonstrate the model's cross-domain robustness, achieving\ndetection accuracies of 99.13 percent and 99.34 percent, respectively.\nAdditionally, the model exhibits exceptional runtime efficiency, with inference\ntimes as low as 0.0002 seconds per instance in IoMT and 0.0001 seconds in IIoT\nscenarios. Coupled with a low false positive rate, BiGAT-ID proves to be a\nreliable and efficient IDS for deployment in real-world heterogeneous IoT\nenvironments", "AI": {"tldr": "BiGAT-ID is a hybrid transformer-based IDS for IoMT/IIoT that captures temporal dependencies and contextual features, achieving 99.13%/99.34% accuracy on CICIoMT2024 and EdgeIIoTset datasets with efficient inference times.", "motivation": "Growing interconnectivity in IoMT and IIoT systems creates vulnerabilities to sophisticated cyber threats, necessitating robust intrusion detection to protect sensitive data, patient safety, and industrial operations.", "method": "The model integrates BiGRU (bidirectional temporal analysis), LSTM (sequential pattern modeling), and multi-head attention (contextual feature enhancement) in a hybrid architecture.", "result": "99.13% detection accuracy on CICIoMT2024 and 99.34% on EdgeIIoTset, with 0.0002s and 0.0001s inference times per instance, respectively, alongside a low false positive rate.", "conclusion": "BiGAT-ID demonstrates cross-domain robustness and operational efficiency, making it suitable for real-world heterogeneous IoT environments requiring high-accuracy intrusion detection."}}
{"id": "2508.12496", "categories": ["cs.CR", "cs.NI", "C.2.3"], "pdf": "https://arxiv.org/pdf/2508.12496", "abs": "https://arxiv.org/abs/2508.12496", "authors": ["Zhihao Wang", "Alessandro Cornacchia", "Andrea Bianco", "Idilio Drago", "Paolo Giaccone", "Dingde Jiang", "Marco Mellia"], "title": "ChamaleoNet: Programmable Passive Probe for Enhanced Visibility on Erroneous Traffic", "comment": "17 pages, 16 figures", "summary": "Traffic visibility remains a key component for management and security\noperations. Observing unsolicited and erroneous traffic, such as unanswered\ntraffic or errors, is fundamental to detect misconfiguration, temporary\nfailures or attacks. ChamaleoNet transforms any production network into a\ntransparent monitor to let administrators collect unsolicited and erroneous\ntraffic directed to hosts, whether offline or active, hosting a server or a\nclient, protected by a firewall, or unused addresses. ChamaleoNet is programmed\nto ignore well-formed traffic and collect only erroneous packets, including\nthose generated by misconfigured or infected internal hosts, and those sent by\nexternal actors which scan for services. Engineering such a system poses\nseveral challenges, from scalability to privacy. Leveraging the SDN paradigm,\nChamaleoNet processes the traffic flowing through a campus/corporate network\nand focuses on erroneous packets only, lowering the pressure on the collection\nsystem while respecting privacy regulations by design. ChamaleoNet enables the\nseamless integration with active deceptive systems like honeypots that can\nimpersonate unused hosts/ports/services and engage with senders. The SDN\nin-hardware filtering reduces the traffic to the controller by 96%, resulting\nin a scalable solution, which we offer as open source. Simple analytics unveil\ninternal misconfigured and infected hosts, identify temporary failures, and\nenhance visibility on external radiation produced by attackers looking for\nvulnerable services.", "AI": {"tldr": "ChamaleoNet is an SDN-based transparent monitoring system that detects harmful network traffic (misconfigured hosts, infected systems, external scans) by filtering out well-formed traffic and integrating with honeypots, achieving 96% traffic reduction for scalability.", "motivation": "Existing production networks lack visibility into unsolicited/erroneous traffic, making it hard to detect misconfigurations, temporary failures, or security attacks. Privacy and scalability also pose engineering challenges for monitoring solutions.", "method": "The paper leverages SDN to implement network-wide traffic filtering that only processes erroneous packets. It uses in-hardware flow rules to reduce controller traffic by 96% while maintaining privacy-first design. The system can seamlessly integrate with honeypots to engage with malicious traffic sources.", "result": "ChamaleoNet successfully filters out 96% of controller-bound traffic via SDN hardware, enabling scalable detection of internal misconfigurations (servers/clients), infected hosts, and external attack radiation. It offers open-source implementation with low operational overhead.", "conclusion": "By focusing on erroneous traffic with SDN-based filtering and honeypot integration, ChamaleoNet improves network visibility and scalability without compromising privacy. It provides effective detection of both internal vulnerabilities and external adversarial reconnaissance attempts."}}
{"id": "2508.12539", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12539", "abs": "https://arxiv.org/abs/2508.12539", "authors": ["Sandaru Jayawardana", "Sennur Ulukus", "Ming Ding", "Kanchana Thilakarathna"], "title": "The Hidden Cost of Correlation: Rethinking Privacy Leakage in Local Differential Privacy", "comment": "19 pages with 8 figures", "summary": "Local differential privacy (LDP) has emerged as a promising paradigm for\nprivacy-preserving data collection in distributed systems, where users\ncontribute multi-dimensional records with potentially correlated attributes.\nRecent work has highlighted that correlation-induced privacy leakage (CPL)\nplays a critical role in shaping the privacy-utility trade-off under LDP,\nespecially when correlations exist among attributes. Nevertheless, it remains\nunclear to what extent the prevailing assumptions and proposed solutions are\nvalid and how significant CPL is in real-world data. To address this gap, we\nfirst perform a comprehensive statistical analysis of five widely used LDP\nmechanisms -- GRR, RAPPOR, OUE, OLH and Exponential mechanism -- to assess CPL\nacross four real-world datasets. We identify that many primary assumptions and\nmetrics in current approaches fall short of accurately characterising these\nleakages. Moreover, current studies have been limited to a set of pure LDP\n(i.e., {\\delta = 0}) mechanisms. In response, we develop the first algorithmic\nframework to theoretically quantify CPL for any general approximated LDP\n(({\\varepsilon},{\\delta})-LDP) mechanism. We validate our theoretical results\nagainst empirical statistical results and provide a theoretical explanation for\nthe observed statistical patterns. Finally, we propose two novel benchmarks to\nvalidate correlation analysis algorithms and evaluate the utility vs CPL of LDP\nmechanisms. Further, we demonstrate how these findings can be applied to\nachieve an efficient privacy-utility trade-off in real-world data governance.", "AI": {"tldr": "This paper systematically evaluates correlation-induced privacy leakage (CPL) in five LDP mechanisms across four real-world datasets, identifies flaws in current assumptions and metrics, and proposes the first theoretical framework and benchmarks for quantifying CPL in general (\u03b5,\u03b4)-LDP mechanisms to improve privacy-utility trade-offs.", "motivation": "Existing CPL analysis under LDP is limited by unverified assumptions and pure LDP (\u03b4=0) constraints, leaving unclear their validity or real-world leakage severity. The authors aim to quantify CPL's impact and improve theoretical benchmarks for practical data governance.", "method": "1) Statistical analysis of GRR, RAPPOR, OUE, OLH, and Exponential mechanisms on real datasets (Census Income, Adult, MovieLens, UCI Diabetes). 2) Derive theoretical CPL bounds for any (\u03b5,\u03b4)-LDP mechanism. 3) Validate framework through empirical comparison and propose two correlation-aware benchmarks.", "result": "1) Current CPL metrics overestimate/ignore leakage. 2) Approximated LDP mechanisms (\u03b4\u22600) amplify leakage compared to pure LDP. 3) Theoretical framework accurately captures statistical leakage patterns. 4) Novel benchmarks reveal significant CPL inefficiencies in existing mechanisms.", "conclusion": "CPL is a critical, underestimated factor in LDP systems. The proposed framework enables precise analysis of approximated LDP mechanisms, and benchmarks provide tangible tools for optimizing privacy-preserving data collection strategies."}}
{"id": "2508.12553", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12553", "abs": "https://arxiv.org/abs/2508.12553", "authors": ["Peilun Wu", "Nan Sun", "Nour Moustafa", "Youyang Qu", "Ming Ding"], "title": "DEFENDCLI: {Command-Line} Driven Attack Provenance Examination", "comment": null, "summary": "Endpoint Detection and Response (EDR) solutions embrace the method of attack\nprovenance graph to discover unknown threats through system event correlation.\nHowever, this method still faces some unsolved problems in the fields of\ninteroperability, reliability, flexibility, and practicability to deliver\nactionable results. Our research highlights the limitations of current\nsolutions in detecting obfuscation, correlating attacks, identifying\nlow-frequency events, and ensuring robust context awareness in relation to\ncommand-line activities. To address these challenges, we introduce DEFENDCLI,\nan innovative system leveraging provenance graphs that, for the first time,\ndelves into command-line-level detection. By offering finer detection\ngranularity, it addresses a gap in modern EDR systems that has been overlooked\nin previous research. Our solution improves the precision of the information\nrepresentation by evaluating differentiation across three levels: unusual\nsystem process calls, suspicious command-line executions, and infrequent\nexternal network connections. This multi-level approach enables EDR systems to\nbe more reliable in complex and dynamic environments. Our evaluation\ndemonstrates that DEFENDCLI improves precision by approximately 1.6x compared\nto the state-of-the-art methods on the DARPA Engagement Series attack datasets.\nExtensive real-time industrial testing across various attack scenarios further\nvalidates its practical effectiveness. The results indicate that DEFENDCLI not\nonly detects previously unknown attack instances, which are missed by other\nmodern commercial solutions, but also achieves a 2.3x improvement in precision\nover the state-of-the-art research work.", "AI": {"tldr": "DEFENDCLI enhances EDR precision by 1.6x-2.3x through command-line-level analysis and three-tier event differentiation in system process calls, suspicious CLI executions, and network connections, overcoming limitations in detecting obfuscation and low-frequency attacks.", "motivation": "Current EDR systems using provenance graphs face challenges in interoperability, reliability, flexibility, and practicality. Existing methods struggle with detecting obfuscated attacks, correlating low-frequency events, and maintaining context-awareness for command-line activities.", "method": "Introduces DEFENDCLI, a command-line-level detection system analyzing three levels: unusual system process calls, suspicious command-line executions, and infrequent external network connections. Utilizes attack provenance graphs with fine-grained detection granularity.", "result": "1.6x precision improvement over state-of-the-art (SOTA) on DARPA Engagement Series datasets. Real-world industrial testing shows 2.3x precision gains over SOTA and detects previously missed unknown attacks in commercial solutions.", "conclusion": "DEFENDCLI addresses critical EDR limitations by introducing CLI-level analysis, enabling superior attack detection precision in complex environments through its multi-level differentiation approach, validated by both benchmark and real-world testing."}}
{"id": "2508.12560", "categories": ["cs.CR", "cs.DC", "cs.LG", "C.2; C.4; I.2"], "pdf": "https://arxiv.org/pdf/2508.12560", "abs": "https://arxiv.org/abs/2508.12560", "authors": ["Prabath Abeysekara", "Hai Dong"], "title": "Data-driven Trust Bootstrapping for Mobile Edge Computing-based Industrial IoT Services", "comment": "15 pages", "summary": "We propose a data-driven and context-aware approach to bootstrap\ntrustworthiness of homogeneous Internet of Things (IoT) services in Mobile Edge\nComputing (MEC) based industrial IoT (IIoT) systems. The proposed approach\naddresses key limitations in adapting existing trust bootstrapping approaches\ninto MEC-based IIoT systems. These key limitations include, the lack of\nopportunity for a service consumer to interact with a lesser-known service over\na prolonged period of time to get a robust measure of its trustworthiness,\ninability of service consumers to consistently interact with their peers to\nreceive reliable recommendations of the trustworthiness of a lesser-known\nservice as well as the impact of uneven context parameters in different MEC\nenvironments causing uneven trust environments for trust evaluation. In\naddition, the proposed approach also tackles the problem of data sparsity via\nenabling knowledge sharing among different MEC environments within a given MEC\ntopology. To verify the effectiveness of the proposed approach, we carried out\na comprehensive evaluation on two real-world datasets suitably adjusted to\nexhibit the context-dependent trust information accumulated in MEC environments\nwithin a given MEC topology. The experimental results affirmed the\neffectiveness of our approach and its suitability to bootstrap trustworthiness\nof services in MEC-based IIoT systems.", "AI": {"tldr": "The paper proposes a context-aware, data-driven approach to address trust bootstrapping challenges in MEC-based IIoT systems, including prolonged service evaluation periods, unreliable peer recommendations, context parameter variability, and data sparsity. It validates the approach with real-world datasets showing improved trustworthiness assessment across MEC environments.", "motivation": "Existing trust bootstrapping methods struggle with adapting to MEC-based IIoT systems due to limited service interaction windows, inconsistent peer recommendation access, and context-parameter variability causing unreliable trust evaluations across environments.", "method": "A framework was developed that integrates context-aware analysis and cross-MEC environment knowledge sharing to mitigate data sparsity while accommodating diverse environmental contexts. The method was evaluated using two real-world datasets modified with context-dependent trust features.", "result": "Experimental results demonstrated the effectiveness of the approach in reliably bootstrapping trustworthiness for lesser-known IoT services under MEC constraints, with successful handling of data sparsity and context-driven trust variability.", "conclusion": "The proposed approach effectively enables robust trust evaluation in MEC-based IIoT systems by addressing interaction limitations, recommendation consistency, contextual variability, and data sparsity through knowledge sharing mechanisms."}}
{"id": "2508.12571", "categories": ["cs.CR", "cs.CY", "cs.ET", "cs.HC", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.12571", "abs": "https://arxiv.org/abs/2508.12571", "authors": ["Tyler Schroder", "Renee Sirbu", "Sohee Park", "Jessica Morley", "Sam Street", "Luciano Floridi"], "title": "Cyber Risks to Next-Gen Brain-Computer Interfaces: Analysis and Recommendations", "comment": null, "summary": "Brain-computer interfaces (BCIs) show enormous potential for advancing\npersonalized medicine. However, BCIs also introduce new avenues for\ncyber-attacks or security compromises. In this article, we analyze the problem\nand make recommendations for device manufacturers to better secure devices and\nto help regulators understand where more guidance is needed to protect patient\nsafety and data confidentiality. Device manufacturers should implement the\nprior suggestions in their BCI products. These recommendations help protect BCI\nusers from undue risks, including compromised personal health and genetic\ninformation, unintended BCI-mediated movement, and many other cybersecurity\nbreaches. Regulators should mandate non-surgical device update methods, strong\nauthentication and authorization schemes for BCI software modifications,\nencryption of data moving to and from the brain, and minimize network\nconnectivity where possible. We also design a hypothetical, average-case threat\nmodel that identifies possible cybersecurity threats to BCI patients and\npredicts the likeliness of risk for each category of threat. BCIs are at less\nrisk of physical compromise or attack, but are vulnerable to remote attack; we\nfocus on possible threats via network paths to BCIs and suggest technical\ncontrols to limit network connections.", "AI": {"tldr": "This paper examines cybersecurity risks in brain-computer interfaces (BCIs) and provides technical/recommended safeguards for manufacturers and regulators to mitigate threats like data breaches and remote attacks, while developing a threat model to assess risk likelihood.", "motivation": "BCIs enable personalized medicine but introduce novel attack vectors compromising patient safety (e.g., health/genetic data leaks, unauthorized movement control) and require balanced security guidance for stakeholders.", "method": "Problem analysis of BCI-specific security risks through recommendations for technical controls (encryption, authentication, network segregation) and regulatory mandates, combined with a hypothetical average-case threat modeling framework.", "result": "Generated implementation guidelines for BCI manufacturers to incorporate security features like non-surgical updates and encryption, along with a threat likelihood assessment prioritizing network-based compromise mitigation strategies.", "conclusion": "Prioritizing network-centric security controls and mandatory regulatory standards is essential for BCI advancement, while the proposed threat model provides a structured approach to evaluating and addressing remote attack risks in medical cyber-physical systems."}}
{"id": "2508.12584", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12584", "abs": "https://arxiv.org/abs/2508.12584", "authors": ["Dikshant", "Verma"], "title": "Reducing False Positives with Active Behavioral Analysis for Cloud Security", "comment": null, "summary": "Rule-based cloud security posture management (CSPM) solutions are known to\nproduce a lot of false positives based on the limited contextual understanding\nand dependence on static heuristics testing. This paper introduces a\nvalidation-driven methodology that integrates active behavioral testing in\ncloud security posture management solution(s) to evaluate the exploitability of\npolicy violations in real time. The proposed system employs lightweight and\nautomated probes, built from open-source tools, validation scripts, and\npenetration testing test cases, to simulate adversarial attacks on\nmisconfigured or vulnerable cloud assets without any impact to the cloud\nservices or environment. For instance, cloud services may be flagged as\npublicly exposed and vulnerable despite being protected by access control\nlayers, or secure policies, resulting in non-actionable alerts that consumes\nanalysts time during manual validation. Through controlled experimentation in a\nreproducible AWS setup, we evaluated the reduction in false positive rates\nacross various misconfiguration and vulnerable alerts. Our findings indicate an\naverage reduction of 93\\% in false positives. Furthermore, the framework\ndemonstrates low latency performance. These results demonstrate a scalable\nmethod to improve detection accuracy and analyst productivity in large cloud\nenvironments. While our evaluation focuses on AWS, the architecture is modular\nand extensible to multi-cloud setups.", "AI": {"tldr": "This paper proposes a validation-driven cloud security posture management approach using automated behavioral testing to reduce false positives by 93% on average through real-time exploitability evaluation without impacting cloud environments.", "motivation": "Rule-based CSPM systems generate numerous false positives due to limited contextual understanding and reliance on static heuristics, wasting analyst time during manual validation of non-actionable alerts.", "method": "The system employs lightweight automated probes constructed from open-source tools and penetration testing test cases to simulate adversarial attacks on misconfigured cloud assets, with controlled experiments conducted in a reproducible AWS environment.", "result": "Achieved 93% average reduction in false positives across cloud misconfiguration and vulnerability alerts while maintaining low latency performance through real-time exploitability evaluation.", "conclusion": "The framework demonstrates a scalable method to enhance CSPM accuracy and analyst productivity by prioritizing actionable insights over static rule dependencies, with modular architecture supporting multi-cloud environments beyond AWS."}}
{"id": "2508.12597", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12597", "abs": "https://arxiv.org/abs/2508.12597", "authors": ["Haolin Zheng", "Ning Gao", "Donghong Cai", "Shi Jin", "Michail Matthaiou"], "title": "UAV Individual Identification via Distilled RF Fingerprints-Based LLM in ISAC Networks", "comment": null, "summary": "Unmanned aerial vehicle (UAV) individual (ID) identification is a critical\nsecurity surveillance strategy in low-altitude integrated sensing and\ncommunication (ISAC) networks. In this paper, we propose a novel dynamic\nknowledge distillation (KD)-enabled wireless radio frequency fingerprint large\nlanguage model (RFF-LLM) framework for UAV ID identification. First, we propose\nan RFF-LLM framework based on the modified GPT-2 model to improve the\nidentification accuracy in complex outdoor environments. Then, considering the\nparameter overhead of the RFF-LLM, we design a dynamic KD strategy to compress\nthe model. Specifically, the proximal policy optimization (PPO) algorithm is\nemployed to dynamically adjust the distillation temperature, overcoming the\nlocal optimum dilemma inherent in static KD. As a next step, the knowledge of\nthe RFF-LLM is adequately transferred to the lightweight Lite-HRNet model.\nFinally, our experiments are conducted based on the self-built drone RFF\ndataset of Release one, namely DRFF-R1, by collecting the I/Q signals of 20\ncommercial UAVs in channel 149. The experiment results show that the proposed\nframework achieves 98.38\\% ID identification accuracy with merely 0.15 million\nparameters and 2.74 ms response time, which outperforms the benchmarks.", "AI": {"tldr": "The paper proposes a dynamic knowledge distillation framework for UAV ID identification using a modified GPT-2 and Lite-HRNet, achieving 98.38% accuracy with low parameters and response time.", "motivation": "Accurate UAV identification in complex outdoor environments is essential for security surveillance in low-altitude integrated sensing and communication networks, but existing models lack efficiency and effectiveness.", "method": "A modified GPT-2 model forms the RFF-LLM framework, followed by dynamic KD with proximal policy optimization (PPO) to compress the model and transfer knowledge to Lite-HRNet.", "result": "Experiments on the DRFF-R1 dataset show 98.38% ID identification accuracy, 0.15 million parameters, and 2.74 ms response time, outperforming benchmarks.", "conclusion": "The proposed framework combines LLMs and dynamic KD for high-accuracy, low-overhead UAV ID identification in ISAC networks, demonstrating significant performance improvements."}}
{"id": "2508.12622", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12622", "abs": "https://arxiv.org/abs/2508.12622", "authors": ["Zilong Lin", "Zichuan Li", "Xiaojing Liao", "XiaoFeng Wang"], "title": "Consiglieres in the Shadow: Understanding the Use of Uncensored Large Language Models in Cybercrimes", "comment": null, "summary": "The advancement of AI technologies, particularly Large Language Models\n(LLMs), has transformed computing while introducing new security and privacy\nrisks. Prior research shows that cybercriminals are increasingly leveraging\nuncensored LLMs (ULLMs) as backends for malicious services. Understanding these\nULLMs has been hindered by the challenge of identifying them among the vast\nnumber of open-source LLMs hosted on platforms like Hugging Face. In this\npaper, we present the first systematic study of ULLMs, overcoming this\nchallenge by modeling relationships among open-source LLMs and between them and\nrelated data, such as fine-tuning, merging, compressing models, and using or\ngenerating datasets with harmful content. Representing these connections as a\nknowledge graph, we applied graph-based deep learning to discover over 11,000\nULLMs from a small set of labeled examples and uncensored datasets.\n  A closer analysis of these ULLMs reveals their alarming scale and usage. Some\nhave been downloaded over a million times, with one over 19 million installs.\nThese models -- created through fine-tuning, merging, or compression of other\nmodels -- are capable of generating harmful content, including hate speech,\nviolence, erotic material, and malicious code. Evidence shows their integration\ninto hundreds of malicious applications offering services like erotic\nrole-play, child pornography, malicious code generation, and more. In addition,\nunderground forums reveal criminals sharing techniques and scripts to build\ncheap alternatives to commercial malicious LLMs. These findings highlight the\nwidespread abuse of LLM technology and the urgent need for effective\ncountermeasures against this growing threat.", "AI": {"tldr": "This paper introduces the first systematic study to identify over 11,000 uncensored large language models (ULLMs) via graph-based deep learning on open-source LLM relationships, revealing their widespread use in malicious applications including harmful content generation and criminal code distribution.", "motivation": "Prior research has identified ULLMs as critical backends for malicious services, yet no systematic method existed to distinguish them among thousands of open-source LLMs. This threat is amplified by their ability to evolve through model fine-tuning, merging, or compression while evading detection.", "method": "Created a knowledge graph modeling interactions between LLMs and harmful data sources (e.g., datasets, merging techniques). Trained graph-based deep learning models on a small set of labeled ULLMs and uncensored datasets to identify relationships indicative of uncensored content.", "result": "Discoveries include: 11,000+ ULLMs identified, 19 million downloads for one model, integration into apps for child pornography and erotic role-play, underground forums distributing ULLM construction scripts, and evidence of fine-tuning chains creating harmful capabilities.", "conclusion": "ULLMs are being weaponized at scale by criminal actors, creating urgent risks for AI safety. Automated methods to detect these models are proving effective, but the research underscores the need for proactive mitigation strategies to prevent further exploitation."}}
{"id": "2508.12641", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12641", "abs": "https://arxiv.org/abs/2508.12641", "authors": ["Yasaman Samadi", "Hai Dong", "Xiaoyu Xia"], "title": "MPOCryptoML: Multi-Pattern based Off-Chain Crypto Money Laundering Detection", "comment": null, "summary": "Recent advancements in money laundering detection have demonstrated the\npotential of using graph neural networks to capture laundering patterns\naccurately. However, existing models are not explicitly designed to detect the\ndiverse patterns of off-chain cryptocurrency money laundering. Neglecting any\nlaundering pattern introduces critical detection gaps, as each pattern reflects\nunique transactional structures that facilitate the obfuscation of illicit fund\norigins and movements. Failure to account for these patterns may result in\nunder-detection or omission of specific laundering activities, diminishing\nmodel accuracy and allowing schemes to bypass detection. To address this gap,\nwe propose the MPOCryptoML model to effectively detect multiple laundering\npatterns in cryptocurrency transactions. MPOCryptoML includes the development\nof a multi-source Personalized PageRank algorithm to identify random laundering\npatterns. Additionally, we introduce two novel algorithms by analyzing the\ntimestamp and weight of transactions in high-volume financial networks to\ndetect various money laundering structures, including fan-in, fan-out,\nbipartite, gather-scatter, and stack patterns. We further examine correlations\nbetween these patterns using a logistic regression model. An anomaly score\nfunction integrates results from each module to rank accounts by anomaly score,\nsystematically identifying high-risk accounts. Extensive experiments on public\ndatasets including Elliptic++, Ethereum fraud detection, and Wormhole\ntransaction datasets validate the efficacy and efficiency of MPOCryptoML.\nResults show consistent performance gains, with improvements up to 9.13% in\nprecision, up to 10.16% in recall, up to 7.63% in F1-score, and up to 10.19% in\naccuracy.", "AI": {"tldr": "MPOCryptoML is a multi-pattern detection model for off-chain cryptocurrency money laundering, improving detection accuracy by addressing diverse laundering patterns through personalized algorithms and integrated evaluation metrics.", "motivation": "Existing graph neural network models for cryptocurrency laundering detection lack explicit design to handle diverse off-chain patterns, leading to under-detection and reduced accuracy due to unique transaction obfuscation techniques.", "method": "Developed a multi-source Personalized PageRank algorithm for random patterns, two timestamp/weight analysis algorithms to detect structured patterns (fan-in/fan-out, bipartite, gather-scatter, stack), logistic regression to model pattern correlations, and an anomaly score function for risk evaluation.", "result": "Demonstrated 9.13% higher precision, 10.16% higher recall, 7.63% higher F1-score, and 10.19% higher accuracy across Elliptic++, Ethereum fraud, and Wormhole datasets compared to existing methods.", "conclusion": "MPOCryptoML effectively addresses multiple laundering patterns in high-volume crypto networks, achieving state-of-the-art performance while maintaining detection efficiency for systematized account risk ranking."}}
{"id": "2508.12730", "categories": ["cs.CR", "cs.HC", "cs.LG", "H.5.2; I.3.6"], "pdf": "https://arxiv.org/pdf/2508.12730", "abs": "https://arxiv.org/abs/2508.12730", "authors": ["Jaeung Lee", "Suhyeon Yu", "Yurim Jang", "Simon S. Woo", "Jaemin Jo"], "title": "Unlearning Comparator: A Visual Analytics System for Comparative Evaluation of Machine Unlearning Methods", "comment": "Submitted to IEEE Transactions on Visualization and Computer Graphics\n  (TVCG), under review. 15 pages. This work has been submitted to the IEEE for\n  possible publication", "summary": "Machine Unlearning (MU) aims to remove target training data from a trained\nmodel so that the removed data no longer influences the model's behavior,\nfulfilling \"right to be forgotten\" obligations under data privacy laws. Yet, we\nobserve that researchers in this rapidly emerging field face challenges in\nanalyzing and understanding the behavior of different MU methods, especially in\nterms of three fundamental principles in MU: accuracy, efficiency, and privacy.\nConsequently, they often rely on aggregate metrics and ad-hoc evaluations,\nmaking it difficult to accurately assess the trade-offs between methods. To\nfill this gap, we introduce a visual analytics system, Unlearning Comparator,\ndesigned to facilitate the systematic evaluation of MU methods. Our system\nsupports two important tasks in the evaluation process: model comparison and\nattack simulation. First, it allows the user to compare the behaviors of two\nmodels, such as a model generated by a certain method and a retrained baseline,\nat class-, instance-, and layer-levels to better understand the changes made\nafter unlearning. Second, our system simulates membership inference attacks\n(MIAs) to evaluate the privacy of a method, where an attacker attempts to\ndetermine whether specific data samples were part of the original training set.\nWe evaluate our system through a case study visually analyzing prominent MU\nmethods and demonstrate that it helps the user not only understand model\nbehaviors but also gain insights that can inform the improvement of MU methods.", "AI": {"tldr": "This paper introduces Unlearning Comparator, a visual analytics system for systematically evaluating Machine Unlearning (MU) methods through model comparison and membership inference attack simulation.", "motivation": "Researchers face challenges in analyzing MU methods due to reliance on aggregate metrics and ad-hoc evaluations, making it difficult to assess trade-offs in accuracy, efficiency, and privacy.", "method": "The system supports (1) model comparison at class/instance/layer levels between unlearned models and retrained baselines, and (2) privacy evaluation via membership inference attack simulations to test data removal effectiveness.", "result": "Case study evaluations demonstrate the system helps users understand model behavior changes after unlearning and gain actionable insights for improving MU techniques.", "conclusion": "Unlearning Comparator addresses critical gaps in MU evaluation by enabling systematic analysis of accuracy, efficiency, and privacy through visual analytics and attack simulations."}}
{"id": "2508.12832", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12832", "abs": "https://arxiv.org/abs/2508.12832", "authors": ["Jinyu Lu", "Xinrong Sun", "Yunting Tao", "Tong Ji", "Fanyu Kong", "Guoqiang Yang"], "title": "Efficient and Verifiable Privacy-Preserving Convolutional Computation for CNN Inference with Untrusted Clouds", "comment": null, "summary": "The widespread adoption of convolutional neural networks (CNNs) in\nresource-constrained scenarios has driven the development of Machine Learning\nas a Service (MLaaS) system. However, this approach is susceptible to privacy\nleakage, as the data sent from the client to the untrusted cloud server often\ncontains sensitive information. Existing CNN privacy-preserving schemes, while\neffective in ensuring data confidentiality through homomorphic encryption and\nsecret sharing, face efficiency bottlenecks, particularly in convolution\noperations. In this paper, we propose a novel verifiable privacy-preserving\nscheme tailored for CNN convolutional layers. Our scheme enables efficient\nencryption and decryption, allowing resource-constrained clients to securely\noffload computations to the untrusted cloud server. Additionally, we present a\nverification mechanism capable of detecting the correctness of the results with\na success probability of at least $1-\\frac{1}{\\left|Z\\right|}$. Extensive\nexperiments conducted on 10 datasets and various CNN models demonstrate that\nour scheme achieves speedups ranging $26 \\times$ ~ $\\ 87\\times$ compared to the\noriginal plaintext model while maintaining accuracy.", "AI": {"tldr": "This paper presents a novel verifiable privacy-preserving scheme for CNN convolutional layers, improving efficiency and enabling secure computation offloading to untrusted cloud servers with accuracy maintenance and results verification.", "motivation": "Privacy leakage risks in MLaaS systems using CNNs due to untrusted cloud servers and inefficiencies in existing encryption methods like homomorphic encryption for convolution operations.", "method": "A new encryption scheme optimized for CNN convolutional layers with verifiable results via a mechanism ensuring detection correctness probability of at least $1-\frac{1}{|Z|}$.", "result": "Achieved 26\u00d7-87\u00d7 speedups compared to plaintext models across 10 datasets and various CNN architectures while preserving accuracy and enabling verification.", "conclusion": "The proposed scheme effectively addresses privacy-utility-efficiency tradeoffs in MLaaS, providing a verifiable, high-speed solution for secure CNN computation offloading in resource-constrained scenarios."}}
{"id": "2508.12859", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12859", "abs": "https://arxiv.org/abs/2508.12859", "authors": ["Xingxing Xu", "Minjia Shi", "Patrick Sole"], "title": "The covering radius of Butson Hadamard codes for the homogeneous metric", "comment": null, "summary": "Butson matrices are complex Hadamard matrices with entries in the complex\nroots of unity of given order. There is an interesting code in phase space\nrelated to this matrix (Armario et al. 2023). We study the covering radius of\nButson Hadamard codes for the homogeneous metric, a metric defined uniquely, up\nto scaling, for a commutative ring alphabet that is Quasi Frobenius. An upper\nbound is derived by an orthogonal array argument. A lower bound relies on the\nexistence of bent sequences in the sense of (Shi et al. 2022). This latter\nbound generalizes a bound of (Armario et al. 2025) for the Hamming metric.", "AI": {"tldr": "This paper studies covering radius bounds for Butson Hadamard codes under a homogeneous metric, using orthogonal arrays and bent sequences.", "motivation": "The homogeneous metric (unique for Quasi Frobenius rings) enables advanced analysis of code performance, generalizing Hamming metric results and improving error correction efficiency.", "method": "1) Derives upper bound via orthogonal array argument\n2) Proves lower bound using existence of bent sequences (Shi et al. 2022)\n3) Compares bounds with Armario et al.'s Hamming metric results (2025)", "result": "Established upper/lower bounds for covering radius of Butson Hadamard codes under homogeneous metric. Generalized 2025's Hamming bound as the lower limit.", "conclusion": "The derived lower bound is a significant generalization of Hamming metric results, enhancing the understanding of Butson code efficiency in phase space applications."}}
{"id": "2508.12870", "categories": ["cs.CR", "68M25"], "pdf": "https://arxiv.org/pdf/2508.12870", "abs": "https://arxiv.org/abs/2508.12870", "authors": ["Vinod Khandkar", "Kieron Ivy Turk", "Ehsan Toreini", "Nishanth Sastry"], "title": "Supporting Socially Constrained Private Communications with SecureWhispers", "comment": "14 pages, 13 figures, 3 tables", "summary": "Rapidly changing social norms and national, legal, and political conditions\nsocially constrain people from discussing sensitive topics such as sexuality or\nreligion. Such constrained, vulnerable minorities are often worried about\ninadvertent information disclosure and may be unsure about the extent to which\ntheir communications are being monitored in public or semi-public spaces like\nworkplaces or cafes. Personal devices extend trust to the digital domain,\nmaking it desirable to have strictly private communication between trusted\ndevices. Currently, messaging services like WhatsApp provide alternative means\nfor exchanging sensitive private information, while personal safety apps such\nas Noonlight enable private signaling. However, these rely on third-party\nmechanisms for secure and private communication, which may not be accessible\nfor justifiable reasons, such as insecure internet access or companion device\nconnections. In these cases, it is challenging to achieve communication that is\nstrictly private between two devices instead of user accounts without any\ndependency on third-party infrastructure. The goal of this paper is to support\nprivate communications by setting up a shared secret between two or more\ndevices without sending any data on the network. We develop a method to create\na shared secret between phones by shaking them together. Each device extracts\nthe shared randomness from the shake, then conditions the randomness to 7.798\nbits per byte of key material. This paper proposes three different applications\nof this generated shared secret: message obfuscation, trust delegation, and\nencrypted beacons. We have implemented the message obfuscation on Android as an\nindependent app that can be used for private communication with trusted\ncontacts. We also present research on the usability, design considerations, and\nfurther integration of these tools in mainstream services.", "AI": {"tldr": "This paper introduces a device-to-device private communication method using physical phone shakes to generate shared secrets without network data transmission. It proposes three applications (message obfuscation, trust delegation, encrypted beacons) and presents an Android app implementation.", "motivation": "Modern communication methods relying on internet infrastructure face limitations in accessibility and security, leaving users vulnerable in constrained public environments where trust in third-party services is unjustified.", "method": "Devices generate shared secrets through coordinated shaking, extracting and conditioning physical randomness from motion sensors to produce 7.798 bits/byte of key material using no network communication.", "result": "Demonstrated three privacy application protypes with successful Android message obfuscation implementation, plus empirical research on usability and integration challenges for mainstream adoption.", "conclusion": "The shake-based cryptographic protocol establishes secure device pairing without infrastructure dependencies, offering practical solutions for privacy-constrained contexts while suggesting paths for broader implementation in safety-critical communication systems."}}
{"id": "2508.12910", "categories": ["cs.CR", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.12910", "abs": "https://arxiv.org/abs/2508.12910", "authors": ["Ziteng Hu", "Yingjie Xia", "Xiyuan Chen", "Li Kuang"], "title": "SecFSM: Knowledge Graph-Guided Verilog Code Generation for Secure Finite State Machines in Systems-on-Chip", "comment": null, "summary": "Finite State Machines (FSMs) play a critical role in implementing control\nlogic for Systems-on-Chip (SoC). Traditionally, FSMs are implemented by\nhardware engineers through Verilog coding, which is often tedious and\ntime-consuming. Recently, with the remarkable progress of Large Language Models\n(LLMs) in code generation, LLMs have been increasingly explored for automating\nVerilog code generation. However, LLM-generated Verilog code often suffers from\nsecurity vulnerabilities, which is particularly concerning for\nsecurity-sensitive FSM implementations. To address this issue, we propose\nSecFSM, a novel method that leverages a security-oriented knowledge graph to\nguide LLMs in generating more secure Verilog code. Specifically, we first\nconstruct a FSM Security Knowledge Graph (FSKG) as an external aid to LLMs.\nSubsequently, we analyze users' requirements to identify vulnerabilities and\nget a list of vulnerabilities in the requirements. Then, we retrieve knowledge\nfrom FSKG based on the vulnerabilities list. Finally, we construct security\nprompts based on the security knowledge for Verilog code generation. To\nevaluate SecFSM, we build a dedicated dataset collected from academic datasets,\nartificial datasets, papers, and industrial cases. Extensive experiments\ndemonstrate that SecFSM outperforms state-of-the-art baselines. In particular,\non a benchmark of 25 security test cases evaluated by DeepSeek-R1, SecFSM\nachieves an outstanding pass rate of 21/25.", "AI": {"tldr": "The paper proposes SecFSM, a method using a security knowledge graph to guide LLMs in generating secure Verilog code for FSMs in SoCs. It achieves a 21/25 pass rate on security test cases via DeepSeek-R1 evaluation.", "motivation": "Traditional manual Verilog coding for FSMs is time-consuming, while LLM-generated code has security vulnerabilities in critical applications. Current practices lack effective security assurance mechanisms for AI-generated control logic.", "method": "1) Constructs FSM Security Knowledge Graph (FSKG) with domain-specific security patterns. 2) Analyzes requirements to identify potential vulnerabilities. 3) Retrieves security knowledge from FSKG based on vulnerability list. 4) Creates security prompts to guide LLM code generation following these security constraints.", "result": "Built comprehensive dataset combining academic/industrial cases. Achieved state-of-the-art security performance with 84% test case pass rate (21/25) using DeepSeek-R1 evaluation. Outperformed existing baseline methods in all measured security metrics.", "conclusion": "SecFSM demonstrates effectiveness in addressing security vulnerabilities in LLM-generated FSM implementations by integrating security knowledge graphs with prompting techniques, establishing a new direction for secure hardware synthesis with AI."}}
{"id": "2508.12953", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12953", "abs": "https://arxiv.org/abs/2508.12953", "authors": ["Samuel Aiello"], "title": "Prescriptive Zero Trust- Assessing the impact of zero trust on cyber attack prevention", "comment": "232 pages in total, 21 figures, 32 tables", "summary": "Increasingly sophisticated and varied cyber threats necessitate ever\nimproving enterprise security postures. For many organizations today, those\npostures have a foundation in the Zero Trust Architecture. This strategy sees\ntrust as something an enterprise must not give lightly or assume too broadly.\nUnderstanding the ZTA and its numerous controls centered around the idea of not\ntrusting anything inside or outside the network without verification, will\nallow organizations to comprehend and leverage this increasingly common\nparadigm. The ZTA, unlike many other regulatory frameworks, is not tightly\ndefined. The research assesses the likelihood of quantifiable guidelines that\nmeasure cybersecurity maturity for an enterprise organization in relation to\nZTA implementation. This is a new, data driven methodology for quantifying\ncyber resilience enabled by the adoption of Zero Trust principles to\npragmatically address the critical need of organizations. It also looks at the\npractical aspects ZTA has on capabilities in deterring cyberattacks on a\nnetwork. The outcomes of this research define a prescriptive set of key\ntechnical controls across identity verification, microsegmentation, data\nencryption, analytics, and orchestration that characterize the comprehensive\nZTA deployment. By evaluating the depth of integration for each control\ncomponent and aligning to industry best practices, the study's results help\nassess an organization's ZTA maturity level on a scale from Initial to\nOptimized adoption. The research's resultant four tier model demarcates phases\nfor an organization on its security transformation journey, with each tier\nadding to the capability of the last.", "AI": {"tldr": "This paper proposes a data-driven methodology to quantify cybersecurity maturity aligned with Zero Trust Architecture (ZTA) principles, defining a four-tier model (Initial to Optimized) and key technical controls to assess organizational ZTA adoption.", "motivation": "The proliferation of sophisticated cyber threats and the lack of clear, quantifiable guidelines for ZTA implementation motivate the need for a standardized framework to measure cybersecurity maturity and transformation progress.", "method": "The research evaluates the integration depth of core ZTA components (identity verification, microsegmentation, data encryption, analytics, orchestration) against industry best practices to develop a maturity scale and prescriptive technical controls.", "result": "A four-tier ZTA maturity model and technical control metrics were established, enabling organizations to evaluate their security posture across identity, segmentation, encryption, analytics, and orchestration domains.", "conclusion": "The four-tier model provides a structured, capability-driven roadmap for ZTA implementation, helping organizations systematically enhance their cyber resilience against evolving threats through measurable progress tracking."}}
{"id": "2508.13033", "categories": ["cs.CR", "B.7.1; B.6"], "pdf": "https://arxiv.org/pdf/2508.13033", "abs": "https://arxiv.org/abs/2508.13033", "authors": ["Ishraq Tashdid", "Tasnuva Farheen", "Sazadur Rahman"], "title": "AuthenTree: A Scalable MPC-Based Distributed Trust Architecture for Chiplet-based Heterogeneous Systems", "comment": "Accepted to IEEE PAINE 2025", "summary": "The rapid adoption of chiplet-based heterogeneous integration is reshaping\nsemiconductor design by enabling modular, scalable, and faster time-to-market\nsolutions for AI and high-performance computing. However, multi-vendor assembly\nin post-fabrication environments fragments the supply chain and exposes SiP\nsystems to serious security threats, including cloning, overproduction, and\nchiplet substitution. Existing authentication solutions depend on trusted\nintegrators or centralized security anchors, which can expose sensitive data or\ncreate single points of failure. We introduce AuthenTree, a distributed\nauthentication framework that leverages multi-party computation (MPC) in a\nscalable tree-based architecture, removing the need for dedicated security\nhardware or centralized trust. AuthenTree enables secure chiplet validation\nwithout revealing raw signatures, distributing trust across multiple integrator\nchiplets. Our evaluation in five SiP benchmarks demonstrates that AuthenTree\nimposes minimal overhead, with an area as low as 0.48% (7,000 sq-micrometers),\nan overhead power under 0.5%, and an authentication latency below 1\nmicrosecond, surpassing previous work in some cases by 700 times. These results\nestablish AuthenTree as an efficient, robust, and scalable solution for\nnext-generation chiplet-based security in zero-trust SiP environments.", "AI": {"tldr": "AuthenTree is a distributed authentication framework using multi-party computation (MPC) to secure chiplet-based SiP systems without centralized trust or hardware. It achieves minimal overhead and fast authentication, improving chiplet security.", "motivation": "Multi-vendor chiplet integration fragments the supply chain and introduces security risks (cloning, overproduction, substitution). Existing solutions depend on trusted integrators or centralized hardware, creating vulnerabilities via data exposure or single points of failure.", "method": "AuthenTree employs a tree-based distributed authentication architecture using MPC, eliminating dedicated security hardware. Validation occurs via integrator chiplets without revealing raw signatures, distributing trust securely.", "result": "AuthenTree demonstrates <0.5% power overhead, 0.48% area (7,000 sq-micrometers), and <1\u03bcs authentication latency. In some cases, it improves over prior art by 700\u00d7, validated across five SiP benchmarks.", "conclusion": "AuthenTree establishes a scalable, robust, and efficient security framework for chiplet-based SiP systems, aligning with zero-trust principles while maintaining low overhead for AI/High Performance Computing applications."}}
{"id": "2508.13048", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.13048", "abs": "https://arxiv.org/abs/2508.13048", "authors": ["Weiwei Qi", "Shuo Shao", "Wei Gu", "Tianhang Zheng", "Puning Zhao", "Zhan Qin", "Kui Ren"], "title": "MAJIC: Markovian Adaptive Jailbreaking via Iterative Composition of Diverse Innovative Strategies", "comment": "7 pages, 3 figures", "summary": "Large Language Models (LLMs) have exhibited remarkable capabilities but\nremain vulnerable to jailbreaking attacks, which can elicit harmful content\nfrom the models by manipulating the input prompts. Existing black-box\njailbreaking techniques primarily rely on static prompts crafted with a single,\nnon-adaptive strategy, or employ rigid combinations of several underperforming\nattack methods, which limits their adaptability and generalization. To address\nthese limitations, we propose MAJIC, a Markovian adaptive jailbreaking\nframework that attacks black-box LLMs by iteratively combining diverse\ninnovative disguise strategies. MAJIC first establishes a ``Disguise Strategy\nPool'' by refining existing strategies and introducing several innovative\napproaches. To further improve the attack performance and efficiency, MAJIC\nformulate the sequential selection and fusion of strategies in the pool as a\nMarkov chain. Under this formulation, MAJIC initializes and employs a Markov\nmatrix to guide the strategy composition, where transition probabilities\nbetween strategies are dynamically adapted based on attack outcomes, thereby\nenabling MAJIC to learn and discover effective attack pathways tailored to the\ntarget model. Our empirical results demonstrate that MAJIC significantly\noutperforms existing jailbreak methods on prominent models such as GPT-4o and\nGemini-2.0-flash, achieving over 90\\% attack success rate with fewer than 15\nqueries per attempt on average.", "AI": {"tldr": "MAJIC is an adaptive jailbreaking framework for black-box LLMs that uses a Markov chain-based strategy selection from a refined pool of disguise methods, achieving high success rates with minimal queries.", "motivation": "Traditional jailbreaking methods for LLMs are limited by static, non-adaptive prompts or rigid strategy combinations, reducing their effectiveness against diverse models. The authors aim to enhance adaptability and generalization to overcome these limitations.", "method": "MAJIC combines a curated 'Disguise Strategy Pool' with a Markovian framework, dynamically adapting transitions between strategies based on real-time attack feedback to optimize pathways for eliciting harmful content.", "result": "MAJIC achieves over 90\textbackpercent attack success rate on GPT-4o and Gemini-2.0-flash with an average of fewer than 15 queries per attempt, outperforming existing black-box jailbreaking techniques significantly.", "conclusion": "MAJIC's adaptive Markovian strategy composition improves both effectiveness and efficiency in jailbreaking LLMs, demonstrating superior generalization and practical attack performance against state-of-the-art models."}}
{"id": "2508.13092", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13092", "abs": "https://arxiv.org/abs/2508.13092", "authors": ["Xiang Long", "Yingjie Xia", "Xiyuan Chen", "Li Kuang"], "title": "VerilogLAVD: LLM-Aided Rule Generation for Vulnerability Detection in Verilog", "comment": null, "summary": "Timely detection of hardware vulnerabilities during the early design stage is\ncritical for reducing remediation costs. Existing early detection techniques\noften require specialized security expertise, limiting their usability. Recent\nefforts have explored the use of large language models (LLMs) for Verilog\nvulnerability detection. However, LLMs struggle to capture the structure in\nVerilog code, resulting in inconsistent detection results. To this end, we\npropose VerilogLAVD, the first LLM-aided graph traversal rule generation\napproach for Verilog vulnerability detection. Our approach introduces the\nVerilog Property Graph (VeriPG), a unified representation of Verilog code. It\ncombines syntactic features extracted from the abstract syntax tree (AST) with\nsemantic information derived from control flow and data dependency graphs. We\nleverage LLMs to generate VeriPG-based detection rules from Common Weakness\nEnumeration (CWE) descriptions. These rules guide the rule executor that\ntraversal VeriPG for potential vulnerabilities. To evaluate VerilogLAVD, we\nbuild a dataset collected from open-source repositories and synthesized data.\nIn our empirical evaluation on 77 Verilog designs encompassing 12 CWE types,\nVerilogLAVD achieves an F1-score of 0.54. Compared to the LLM-only and LLM with\nexternal knowledge baselines, VerilogLAVD improves F1-score by 0.31 and 0.27,\nrespectively.", "AI": {"tldr": "VerilogLAVD is an LLM-aided approach for Verilog vulnerability detection that uses a structured graph representation (VeriPG) to improve accuracy over existing methods.", "motivation": "Current early-stage hardware vulnerability detection requires specialized security expertise and existing LLM-based solutions produce inconsistent results by failing to capture Verilog code structure.", "method": "We introduce VeriPG, a unified graph representation merging AST syntactic features with control-flow and data-dependency semantic graphs. LLMs generate detection rules from CWE descriptions to guide graph traversal for vulnerability identification.", "result": "VerilogLAVD achieves 0.54 F1-score on 77 Verilog designs across 12 CWE types, surpassing LLM-only baselines by +31% and LLM with external knowledge baselines by +27% in F1-score improvements.", "conclusion": "This work demonstrates that integrating LLM-generated rules with structured graph representations (VeriPG) enables more effective and explainable hardware vulnerability detection in Verilog code than prior approaches."}}
