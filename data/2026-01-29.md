<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 21]
- [cs.CR](#cs.CR) [Total: 17]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Achieving Productivity Gains with AI-based IDE features: A Journey at Google](https://arxiv.org/abs/2601.19964)
*Maxim Tabachnyk,Xu Shu,Alexander Frömmgen,Pavel Sychev,Vahid Meimand,Ilia Krets,Stanislav Pyatykh,Abner Araujo,Kristóf Molnár,Satish Chandra*

Main category: cs.SE

TL;DR: Google shares its experience in improving AI-powered IDE tools like code completion and natural language-driven code transformation, focusing on latency, UX, and suggestion quality through cross-layer optimizations.


<details>
  <summary>Details</summary>
Motivation: To enhance developer productivity in enterprise environments by refining AI-based IDE features with measurable impact.

Method: Iterative refinement of code completion and Transform Code features across UI, backend, and model layers, supported by rigorous experimentation.

Result: Improved latency, user experience, and suggestion quality leading to tangible productivity gains for developers.

Conclusion: Effective deployment of AI developer tools in enterprise settings requires coordinated optimization across multiple system layers and continuous evaluation.

Abstract: We discuss Google's journey in developing and refining two internal AI-based IDE features: code completion and natural-language-driven code transformation (Transform Code). We address challenges in latency, user experience and suggestion quality, all backed by rigorous experimentation. The article serves as an example of how to refine AI developer tools across the user interface, backend, and model layers, to deliver tangible productivity improvements in an enterprise setting.

</details>


### [2] [Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis](https://arxiv.org/abs/2601.20103)
*Darshan Deshpande,Anand Kannappan,Rebecca Qian*

Main category: cs.SE

TL;DR: This paper introduces TRACE, a benchmark with 517 verified code trajectories across 54 reward exploit categories, to evaluate LLMs' ability to detect reward hacking; findings show improved detection in contrastive settings and poor performance on semantic-level exploits.


<details>
  <summary>Details</summary>
Motivation: Reward hacking in code generation environments is a growing concern, and as LLMs are increasingly used as evaluators, it's critical to understand their ability to detect such exploits.

Method: Proposed a taxonomy of 54 reward exploit categories and introduced TRACE, a benchmark with 517 human-verified testing trajectories; evaluated models in both isolated classification and contrastive anomaly detection setups.

Result: Models perform significantly better in contrastive settings (e.g., GPT-5.2 achieving 63% detection) than in isolated classification (45%); performance drops on semantically contextualized hacks; ablation studies show detection is sensitive to benign-to-hacked ratio and cluster size.

Conclusion: State-of-the-art LLMs still struggle with detecting semantically contextualized reward hacks, but perform better in contrastive anomaly detection settings than in isolated classification; TRACE provides a valuable benchmark for future research.

Abstract: Recent advances in reinforcement learning for code generation have made robust environments essential to prevent reward hacking. As LLMs increasingly serve as evaluators in code-based RL, their ability to detect reward hacking remains understudied. In this paper, we propose a novel taxonomy of reward exploits spanning across 54 categories and introduce TRACE (Testing Reward Anomalies in Code Environments), a synthetically curated and human-verified benchmark containing 517 testing trajectories. Unlike prior work that evaluates reward hack detection in isolated classification scenarios, we contrast these evaluations with a more realistic, contrastive anomaly detection setup on TRACE. Our experiments reveal that models capture reward hacks more effectively in contrastive settings than in isolated classification settings, with GPT-5.2 with highest reasoning mode achieving the best detection rate at 63%, up from 45% in isolated settings on TRACE. Building on this insight, we demonstrate that state-of-the-art models struggle significantly more with semantically contextualized reward hacks compared to syntactically contextualized ones. We further conduct qualitative analyses of model behaviors, as well as ablation studies showing that the ratio of benign to hacked trajectories and analysis cluster sizes substantially impact detection performance. We release the benchmark and evaluation harness to enable the community to expand TRACE and evaluate their models.

</details>


### [3] [Are We All Using Agents the Same Way? An Empirical Study of Core and Peripheral Developers Use of Coding Agents](https://arxiv.org/abs/2601.20106)
*Shamse Tasnim Cynthia,Joy Krishan Das,Banani Roy*

Main category: cs.SE

TL;DR: This paper analyzes 9,427 agentic PRs to compare how core and peripheral developers interact with autonomous AI coding agents, revealing differences in usage, review, modification, and verification behaviors, with core developers applying stricter integration standards.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the lack of understanding about how core and peripheral developers interact with autonomous AI coding agents, despite known differences in AI tool usage. The study aims to uncover behavioral patterns in this new agentic paradigm to improve collaboration and integration practices.

Method: The paper conducts the first empirical study of 9,427 agentic pull requests (PRs), combining qualitative and quantitative analysis to compare the behavior of core and peripheral developers in using, reviewing, modifying, and verifying AI-generated code contributions.

Result: Key findings include: (1) peripheral developers use agents more broadly across tasks, while core developers focus on documentation and testing and have higher merge rates; (2) core developers participate slightly more in reviews, with both groups emphasizing evolvability; (3) agentic PRs are less modified, but when changes occur, refactoring is common; and (4) peripheral developers more frequently merge without CI checks, whereas core developers consistently require passing verification.

Conclusion: The study concludes that developer experience significantly influences how coding agents are used, reviewed, and integrated, with core developers adhering to stricter integration and verification practices, while peripheral developers exhibit more lenient behaviors, particularly in skipping CI checks. These findings provide actionable insights for improving collaboration between developers and autonomous coding agents.

Abstract: Autonomous AI agents are transforming software development and redefining how developers collaborate with AI. Prior research shows that the adoption and use of AI-powered tools differ between core and peripheral developers. However, it remains unclear how this dynamic unfolds in the emerging era of autonomous coding agents. In this paper, we present the first empirical study of 9,427 agentic PRs, examining how core and peripheral developers use, review, modify, and verify agent-generated contributions prior to acceptance. Through a mix of qualitative and quantitative analysis, we make four key contributions. First, a subset of peripheral developers use agents more often, delegating tasks evenly across bug fixing, feature addition, documentation, and testing. In contrast, core developers focus more on documentation and testing, yet their agentic PRs are frequently merged into the main/master branch. Second, core developers engage slightly more in review discussions than peripheral developers, and both groups focus on evolvability issues. Third, agentic PRs are less likely to be modified, but when they are, both groups commonly perform refactoring. Finally, peripheral developers are more likely to merge without running CI checks, whereas core developers more consistently require passing verification before acceptance. Our analysis offers a comprehensive view of how developer experience shapes integration offer insights for both peripheral and core developers on how to effectively collaborate with coding agents.

</details>


### [4] [Beyond Bug Fixes: An Empirical Investigation of Post-Merge Code Quality Issues in Agent-Generated Pull Requests](https://arxiv.org/abs/2601.20109)
*Shamse Tasnim Cynthia,Al Muttakin,Banani Roy*

Main category: cs.SE

TL;DR: This study analyzes 1,210 merged AI-generated bug-fix pull requests in Python repositories to assess post-merge code quality using SonarQube. It finds that while agent-generated PRs are frequently merged, they often introduce new code quality issues, especially code smells, and that larger PRs correlate with higher issue counts. Normalizing by code churn reduces apparent differences across agents.


<details>
  <summary>Details</summary>
Motivation: The increasing use of AI coding agents has led to more automated pull requests being merged with minimal human oversight. However, the post-merge code quality of these AI-generated changes remains poorly understood, as prior evaluations focus on benchmarks rather than real-world merged code. This work aims to fill that gap.

Method: The authors analyzed 1,210 merged agent-generated bug-fix PRs from Python projects in the AIDev dataset. Using SonarQube, they conducted a differential analysis between base and merged commits to detect newly introduced code quality issues. They evaluated issue frequency, density, severity, and rule-level prevalence across five AI agents, normalizing by code churn.

Result: Raw issue counts varied across agents but became comparable when normalized by code churn, indicating that larger PRs drive higher issue counts. Code smells were the most common issue, especially at critical and major severity levels, while actual bugs were rarer but often more severe. Merged status did not correlate with high code quality, as many PRs introduced new issues.

Conclusion: Merge approval does not ensure good code quality in AI-generated PRs. Code smells are prevalent, and systematic post-merge quality assurance mechanisms are necessary to ensure the reliability of agent-generated code.

Abstract: The increasing adoption of AI coding agents has increased the number of agent-generated pull requests (PRs) merged with little or no human intervention. Although such PRs promise productivity gains, their post-merge code quality remains underexplored, as prior work has largely relied on benchmarks and controlled tasks rather than large-scale post-merge analyses. To address this gap, we analyze 1,210 merged agent-generated bug-fix PRs from Python repositories in the AIDev dataset. Using SonarQube, we perform a differential analysis between base and merged commits to identify code quality issues newly introduced by PR changes. We examine issue frequency, density, severity, and rule-level prevalence across five agents. Our results show that apparent differences in raw issue counts across agents largely disappear after normalizing by code churn, indicating that higher issue counts are primarily driven by larger PRs. Across all agents, code smells dominate, particularly at critical and major severities, while bugs are less frequent but often severe. Overall, our findings show that merge success does not reliably reflect post-merge code quality, highlighting the need for systematic quality checks for agent-generated bug-fix PRs.

</details>


### [5] [Usage, Effects and Requirements for AI Coding Assistants in the Enterprise: An Empirical Study](https://arxiv.org/abs/2601.20112)
*Maja Vukovic,Rangeet Pan,Tin Kam Ho,Rahul Krishna,Raju Pavuluri,Michele Merler*

Main category: cs.SE

TL;DR: This paper investigates the readiness of AI coding assistants and CodeLLMs for real-world and enterprise software engineering through a survey of 57 developers and analysis of 35 user surveys, identifying key requirements for their effective integration.


<details>
  <summary>Details</summary>
Motivation: To assess whether current AI coding assistants and CodeLLMs are suitable for enterprise use and understand their impact on software engineering processes and user experience.

Method: Conducted a survey with 57 developers from diverse domains and skill levels, and performed a review and analysis of 35 existing user surveys on AI coding assistant usage, experiences, and expectations.

Result: Identified key user experiences, challenges, and expectations regarding AI coding assistants; derived practical requirements for improving their effectiveness in professional software engineering contexts.

Conclusion: While CodeLLMs show promise, they are not yet fully ready for seamless enterprise integration; successful adoption requires addressing usability, reliability, trust, and workflow compatibility through targeted improvements in AI coding assistants.

Abstract: The rise of large language models (LLMs) has accelerated the development of automated techniques and tools for supporting various software engineering tasks, e.g., program understanding, code generation, software testing, and program repair. As CodeLLMs are being employed toward automating these tasks, one question that arises, especially in enterprise settings, is whether these coding assistants and the code LLMs that power them are ready for real-world projects and enterprise use cases, and how do they impact the existing software engineering process and user experience. In this paper we survey 57 developers from different domains and with varying software engineering skill about their experience with AI coding assistants and CodeLLMs. We also reviewed 35 user surveys on the usage, experience and expectations of professionals and students using AI coding assistants and CodeLLMs. Based on our study findings and analysis of existing surveys, we discuss the requirements for AI-powered coding assistants.

</details>


### [6] [Not All Tokens Matter: Data-Centric Optimization for Efficient Code Summarization](https://arxiv.org/abs/2601.20147)
*Saima Afrin,Zaiyu Cheng,Tushar Sharma,Alexander Serebrenik,Massimiliano Di Penta,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Instruction-tuned Language Models ILMs have become essential components of modern AI systems, demonstrating exceptional versatility across a wide range of natural language and reasoning tasks. Among their most impactful applications is code generation, where ILMs--commonly referred to as Code Language Models CLMs--have demonstrated remarkable capability. This strength stems from their defining feature: the use of explicit task instructions during fine-tuning, which enables them to bridge natural language and code by translating human intent into executable code. While much of their progress has been driven by advances in scaling laws and training methodologies, one critical aspect remains underexplored--the impact of system prompts on the performance of both general-purpose ILMs and specialized CLMs when instantiated to assist users with code generation activities. In this study, we take a first step toward bridging this gap by systematically evaluating how system prompts of varying instructional detail, along with model scale, prompting strategy, and programming language, affect ILMs and CLMs in code generation tasks. Our evaluation framework, spanning 120 model configurations, reveals that (1) the influence of system prompts increases with model scale; (2) few-shot prompting reduces this effect compared to zero-shot; and (3) programming language matters, with Java showing greater sensitivity to system prompt variations than Python.

</details>


### [7] [LogSieve: Task-Aware CI Log Reduction for Sustainable LLM-Based Analysis](https://arxiv.org/abs/2601.20148)
*Marcus Emmanuel Barnes,Taher A. Ghaleb,Safwat Hassan*

Main category: cs.SE

TL;DR: LogSieve is a lightweight, semantics-preserving log reduction technique for CI workflows that reduces log volume by 42% on average while maintaining high semantic fidelity, enabling more efficient and sustainable LLM-based analysis.


<details>
  <summary>Details</summary>
Motivation: CI logs are growing in size and complexity, making manual and automated analysis costly and environmentally unsustainable; existing log reduction methods are not optimized for unstructured CI logs or downstream reasoning tasks like root cause analysis.

Method: LogSieve employs an RCA-aware, semantics-preserving approach that uses embedding-based classifiers to identify and retain high-information log lines relevant to root cause analysis, filtering out low-information content while minimizing semantic loss.

Result: Evaluated on CI logs from 20 Android projects, LogSieve achieves 42% line reduction and 40% token reduction with high semantic retention (Cosine = 0.93, GPTScore = 0.93, 80% exact-match accuracy) and near-human classification accuracy (97%).

Conclusion: LogSieve enables efficient, scalable, and sustainable log processing for CI workflows by reducing data volume before LLM inference, lowering computational cost and environmental impact while preserving interpretability and supporting downstream reasoning tasks.

Abstract: Logs are essential for understanding Continuous Integration (CI) behavior, particularly for diagnosing build failures and performance regressions. Yet their growing volume and verbosity make both manual inspection and automated analysis increasingly costly, time-consuming, and environmentally costly. While prior work has explored log compression, anomaly detection, and LLM-based log analysis, most efforts target structured system logs rather than the unstructured, noisy, and verbose logs typical of CI workflows.
  We present LogSieve, a lightweight, RCA-aware and semantics-preserving log reduction technique that filters low-information lines while retaining content relevant to downstream reasoning. Evaluated on CI logs from 20 open-source Android projects using GitHub Actions, LogSieve achieves an average 42% reduction in lines and 40% reduction in tokens with minimal semantic loss. This pre-inference reduction lowers computational cost and can proportionally reduce energy use (and associated emissions) by decreasing the volume of data processed during LLM inference.
  Compared with structure-first baselines (LogZip and random-line removal), LogSieve preserves much higher semantic and categorical fidelity (Cosine = 0.93, GPTScore = 0.93, 80% exact-match accuracy). Embedding-based classifiers automate relevance detection with near-human accuracy (97%), enabling scalable and sustainable integration of semantics-aware filtering into CI workflows. LogSieve thus bridges log management and LLM reasoning, offering a practical path toward greener and more interpretable CI automation.

</details>


### [8] [Cascaded Vulnerability Attacks in Software Supply Chains](https://arxiv.org/abs/2601.20158)
*Laura Baird,Armin Moin*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Most of the current software security analysis tools assess vulnerabilities in isolation. However, sophisticated software supply chain security threats often stem from cascaded vulnerability and security weakness chains that span dependent components. Moreover, although the adoption of Software Bills of Materials (SBOMs) has been accelerating, downstream vulnerability findings vary substantially across SBOM generators and analysis tools. We propose a novel approach to SBOM-driven security analysis methods and tools. We model vulnerability relationships over dependency structure rather than treating scanner outputs as independent records. We represent enriched SBOMs as heterogeneous graphs with nodes being the SBOM components and dependencies, the known software vulnerabilities, and the known software security weaknesses. We then train a Heterogeneous Graph Attention Network (HGAT) to predict whether a component is associated with at least one known vulnerability. Since documented multi-vulnerability chains are scarce, we model cascade discovery as a link prediction problem over CVE pairs using a multi-layer perceptron neural network. This way, we produce ranked candidate links that can be composed into multi-step paths. The HGAT component classifier achieves an Accuracy of 91.03% and an F1-score of 74.02%.

</details>


### [9] [How do Agents Refactor: An Empirical Study](https://arxiv.org/abs/2601.20160)
*Lukas Ottenhof,Daniel Penner,Abram Hindle,Thibaud Lutellier*

Main category: cs.SE

TL;DR: This study compares AI agent and human refactoring in Java across 172 projects, finding agents mostly perform annotation changes while developers make broader structural improvements, with only Cursor causing a significant rise in code smells.


<details>
  <summary>Details</summary>
Motivation: While software agents are increasingly used in development workflows, there is limited understanding of how they perform Java refactoring in practice, what types of changes they make, and how these affect code quality compared to human refactorings.

Method: The study analyzes refactoring pull requests from software development agents and human developers across 86 Java projects each. It uses RefactoringMiner to identify refactoring types and DesigniteJava 3.0 to detect code smells before and after refactoring commits, enabling a comparative analysis of changes made by agents versus developers.

Result: Agent refactorings are predominantly focused on annotation-related changes—the top five refactoring types by agents are all annotation-based—unlike the more diverse and structural refactorings performed by developers. Despite differences in refactoring types, most agents do not significantly alter code smell levels; however, Cursor is the only agent showing a statistically significant increase in code smells post-refactoring.

Conclusion: Agent-based refactoring in Java differs significantly from human-driven refactoring, with agents primarily making annotation-related changes rather than broader structural improvements. Among the agents evaluated, only Cursor shows a statistically significant increase in code smells after refactoring, raising concerns about its impact on code quality.

Abstract: Software development agents such as Claude Code, GitHub Copilot, Cursor Agent, Devin, and OpenAI Codex are being increasingly integrated into developer workflows. While prior work has evaluated agent capabilities for code completion and task automation, there is little work investigating how these agents perform Java refactoring in practice, the types of changes they make, and their impact on code quality. In this study, we present the first analysis of agentic refactoring pull requests in Java, comparing them to developer refactorings across 86 projects per group. Using RefactoringMiner and DesigniteJava 3.0, we identify refactoring types and detect code smells before and after refactoring commits. Our results show that agent refactorings are dominated by annotation changes (the 5 most common refactoring types done by agents are annotation related), in contrast to the diverse structural improvements typical of developers. Despite these differences in refactoring types, we find Cursor to be the only model to show a statistically significant increase in refactoring smells.

</details>


### [10] [Who Writes the Docs in SE 3.0? Agent vs. Human Documentation Pull Requests](https://arxiv.org/abs/2601.20171)
*Kazuma Yamasaki,Joseph Ayobami Joshua,Tasha Settewong,Mahmoud Alfadel,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: AI agents are heavily involved in software documentation, submitting more PRs than humans, but their changes are often accepted without sufficient review, posing risks to documentation quality and human-AI collaboration in SE3.0.


<details>
  <summary>Details</summary>
Motivation: With the rise of AI agents in software engineering (SE3.0), it is crucial to understand their impact on documentation tasks—which are critical for developer understanding and software usability—given that existing research has primarily focused on code-related tasks and overlooked documentation.

Method: The study uses the AIDev tool to analyze 1,997 documentation-related pull requests (PRs) authored by both AI agents and human developers in software repositories, focusing on the frequency, nature, and human follow-up of documentation changes.

Result: AI agents submit significantly more documentation-related PRs than humans, and their contributions are typically accepted with little or no follow-up modification by human developers, indicating limited scrutiny and potential risks to documentation quality.

Conclusion: AI agents are making significant contributions to software documentation workflows, often with minimal human intervention, which raises concerns about documentation quality and the effectiveness of current review practices in human-AI collaborative environments.

Abstract: As software engineering moves toward SE3.0, AI agents are increasingly used to carry out development tasks and contribute changes to software projects. It is therefore important to understand the extent of these contributions and how human developers review and intervene, since these factors shape the risks of delegating work to AI agents. While recent studies have examined how AI agents support software development tasks (e.g., code generation, issue resolution, and PR automation), their role in documentation tasks remains underexplored-even though documentation is widely consumed and shapes how developers understand and use software.
  Using the AIDev, we analyze 1,997 documentation-related pull requests (PRs) authored by AI agents and human developers, where documentation PRs are those that create or modify project documentation artifacts. We find that AI agents submit substantially more documentation-related PRs than humans in the studied repositories. We further observe that agent-authored documentation edits are typically integrated with little follow-up modification from humans, raising concerns about review practices and the reliability of agent-generated documentation. Overall, while AI agents already contribute substantially to documentation workflows, our results suggest concerns for emerging challenges for documentation quality assurance and human-AI collaboration in SE3.0.

</details>


### [11] [Control Models for In-IDE Code Completion](https://arxiv.org/abs/2601.20223)
*Aral de Moor,Yana Hrynevich,Hleb Badzeika,Vladyslav Furda,Marko Kojic,Artem Savelev,Kostadin Cvejoski,Darya Rovdo,Ekaterina Garanina*

Main category: cs.SE

TL;DR: Control models for LLM-powered code completion in JetBrains IDEs improve efficiency and quality by using ML classifiers to trigger inference and filter suggestions, validated through offline evaluation and A/B testing.


<details>
  <summary>Details</summary>
Motivation: To enhance LLM-based code completion in IDEs by reducing unnecessary LLM requests and aligning suggestions better with user intent through intelligent filtering and triggering mechanisms.

Method: Developed and evaluated boosting- and transformer-based ML classifiers on an offline dataset of real code completions from 98 users; assessed performance across diverse programming languages and deployed in a production A/B test.

Result: Boosting-based models showed strong offline classification performance across syntactically diverse languages; A/B testing in production demonstrated improved code completion efficiency and quality metrics.

Conclusion: Auxiliary control models can significantly improve the integration of LLM-driven features in IDEs, suggesting promising directions for future research and practical deployment.

Abstract: We introduce control models for LLM-powered code completion in JetBrains IDEs: ML classifiers which trigger inference and filter the generated suggestions to better align them with users and reduce unnecessary requests. To this end, we evaluate boosting- and transformer-based architectures on an offline dataset of real code completions with n=98 users. We further evaluate the offline classification performance of our boosting-based approach on a range of syntactically diverse languages; and perform an A/B study in a production environment where they improve completion efficiency and quality metrics. With this study, we hope to demonstrate the potential in using auxiliary models for smarter in-IDE integration of LLM-driven features, highlight fruitful future directions, and open problems.

</details>


### [12] [Understanding npm Developers' Practices, Challenges, and Recommendations for Secure Package Development](https://arxiv.org/abs/2601.20240)
*Anthony Peruma,Truman Choy,Gerald Lee,Italo De Oliveira Santos*

Main category: cs.SE

TL;DR: A survey of 75 npm package developers reveals that while security is prioritized, perceived package security is only moderate due to concerns about supply chain attacks and dependency vulnerabilities; developers are dissatisfied with current tools (40% satisfaction) and face barriers like time constraints and false positives, calling for improved tooling, documentation, account security, and education.


<details>
  <summary>Details</summary>
Motivation: To understand how npm package developers perceive and manage security, given increasing threats from third-party package vulnerabilities that compromise application integrity.

Method: An online survey with 75 npm package developers was conducted, followed by mixed-methods analysis of responses covering security understanding, practices, tools, barriers, and improvement suggestions.

Result: Developers prioritize security but perceive moderate security in their packages, citing supply chain attacks, dependency issues, and malicious code. Only 40% are satisfied with existing npm security tools due to alert fatigue. Automated methods like 2FA and npm audit are preferred over code reviews. Many remove dependencies due to abandonment or vulnerabilities and patch quickly when issues arise. Major barriers include time constraints and high false-positive rates.

Conclusion: The findings highlight key security challenges in the npm ecosystem and offer actionable insights for developers, maintainers, and platform stakeholders to improve tooling, documentation, account protection, and education, thereby strengthening trust and security in npm.

Abstract: Background: The Node Package Manager (npm) ecosystem plays a vital role in modern software development by providing a vast repository of packages and tools that developers can use to implement their software systems. However, recent vulnerabilities in third-party packages have led to serious security breaches, compromising the integrity of applications that depend on them. Objective: This study investigates how npm package developers perceive and handle security in their work. We examined developers' understanding of security risks, the practices and tools they use, the barriers to stronger security measures, and their suggestions for improving the npm ecosystem's security. Method: We conducted an online survey with 75 npm package developers and undertook a mixed-methods approach to analyzing their responses. Results: While developers prioritize security, they perceive their packages as only moderately secure, with concerns about supply chain attacks, dependency vulnerabilities, and malicious code. Only 40% are satisfied with the current npm security tools due to issues such as alert fatigue. Automated methods such as two-factor authentication and npm audit are favored over code reviews. Many drop dependencies due to abandonment or vulnerabilities, and typically respond to vulnerabilities in their packages by quickly releasing patches. Key barriers include time constraints and high false-positive rates. To improve npm security, developers seek better detection tools, clearer documentation, stronger account protections, and more education initiatives. Conclusion: Our findings will benefit npm package contributors and maintainers by highlighting prevalent security challenges and promoting discussions on best practices to strengthen security and trustworthiness within the npm landscape.

</details>


### [13] [How Software Engineering Research Overlooks Local Industry: A Smaller Economy Perspective](https://arxiv.org/abs/2601.20382)
*Klara Borowa,Andrzej Zalewski,Lech Madeyski*

Main category: cs.SE

TL;DR: Researchers from smaller, non-English speaking countries face growing challenges due to the research-industry gap in software engineering; this paper uses reflexive analysis of the ICSE FOSE survey and personal insights to propose improvements for better collaboration in such economies.


<details>
  <summary>Details</summary>
Motivation: To highlight the challenges faced by minority research communities, especially those from smaller and non-English speaking countries, in the context of the growing research-industry gap in software engineering.

Method: Reflexive thematic analysis of the ICSE FOSE community survey combined with the authors' experiences as researchers from Poland.

Result: Identification of key issues affecting smaller software engineering communities and a set of actionable recommendations to strengthen research-industry collaboration in such contexts.

Conclusion: The research-industry gap is a critical issue for software engineering communities in smaller, non-English speaking economies; addressing it through targeted recommendations can improve collaboration and research impact in these regions.

Abstract: The software engineering researchers from countries with smaller economies, particularly non-English speaking ones, represent valuable minorities within the software engineering community. As researchers from Poland, we represent such a country. We analyzed the ICSE FOSE (Future of Software Engineering) community survey through reflexive thematic analysis to show our viewpoint on key software community issues. We believe that the main problem is the growing research-industry gap, which particularly impacts smaller communities and small local companies. Based on this analysis and our experiences, we present a set of recommendations for improvements that would enhance software engineering research and industrial collaborations in smaller economies.

</details>


### [14] [Comprehension vs. Adoption: Evaluating a Language Workbench Through a Family of Experiments](https://arxiv.org/abs/2601.20394)
*Giovanna Broccia,Maurice H. ter Beek,Walter Cazzola,Luca Favalli,Francesco Bertolotti,Alessio Ferrari*

Main category: cs.SE

TL;DR: This paper evaluates the comprehensibility and user acceptance of Neverlang, a modular language workbench, through a family of experiments. Using a tailored Method Evaluation Model, it assesses how well users understand the meta-language and their perceptions of ease of use, usefulness, and intention to use. Results show users comprehend the syntax well and find the tool useful, but struggle with ease of use. Surprisingly, higher comprehensibility does not correlate with higher acceptance, indicating that adoption depends on perceived usability rather than understanding alone.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of language workbenches often overlook user-centered factors like comprehensibility and acceptance; this study aims to address that gap.

Method: A family of experiments using a tailored Method Evaluation Model (MEM) to assess comprehensibility and user acceptance across three iterations with academic participants.

Result: Users showed good comprehension of Neverlang's syntax and perceived it as useful with intention to use, but ease of use was a challenge; no significant correlation was found between comprehensibility and user acceptance.

Conclusion: Comprehensibility of a language workbench's meta-language does not necessarily influence user acceptance; perceived ease of use and usefulness are more critical for adoption.

Abstract: Language workbenches are tools that enable the definition, reuse, and composition of programming languages and their ecosystems, aiming to streamline language development. To facilitate their adoption by language designers, the comprehensibility of the language used to define other languages is an important aspect to evaluate. Moreover, considering that language workbenches are relatively new tools, user acceptance emerges as a crucial factor to be accounted for during their assessment. Current literature often neglects user-centred aspects like comprehensibility and acceptance in the assessment of this breed of tools. This paper addresses this gap through a family of experiments assessing Neverlang, a modular language workbench. The study adopts a tailored version of the Method Evaluation Model (MEM) to evaluate the comprehensibility of Neverlang's meta-language and programs, as well as user acceptance in terms of perceived ease of use, perceived usefulness, and intention to use. It also investigates the relationships among these dimensions. The experiments were conducted in three iterations involving participants from academia. The results reveal that users demonstrate sufficient comprehension of Neverlang's meta-language, particularly concerning its syntax, express a favourable perception of its usefulness, and indicate their intention to use it. However, the results also indicate that Neverlang's ease of use remains a challenge. Additionally, variations in the perceived ease of use and perceived usefulness, whether low or high, influence the users' intention to use the tool. Surprisingly, no significant correlation is found between comprehensibility and user acceptance. Notably, higher comprehensibility of the meta-language does not necessarily translate into greater acceptance, underscoring the complex interplay between comprehension and adoption.

</details>


### [15] [On the Impact of AGENTS.md Files on the Efficiency of AI Coding Agents](https://arxiv.org/abs/2601.20404)
*Jai Lal Lulla,Seyedmoein Mohsenimofidi,Matthias Galster,Jie M. Zhang,Sebastian Baltes,Christoph Treude*

Main category: cs.SE

TL;DR: AGENTS.md files significantly improve the efficiency of AI coding agents by reducing execution time and token usage during pull request processing, without affecting task completion.


<details>
  <summary>Details</summary>
Motivation: As AI coding agents are increasingly used to autonomously contribute to software repositories, there is limited understanding of how repository-level configuration artifacts, such as AGENTS.md, influence their efficiency and behavior.

Method: The study analyzes 10 GitHub repositories and 124 pull requests, comparing the performance of AI coding agents (e.g., Codex, Claude Code) in two conditions: with and without an AGENTS.md file. Metrics include wall-clock execution time and token usage.

Result: The presence of AGENTS.md is associated with a 28.64% reduction in median runtime and a 16.58% decrease in output token consumption, while task completion rates remain comparable.

Conclusion: The presence of AGENTS.md files improves the operational efficiency of AI coding agents by reducing runtime and token consumption without compromising task completion, suggesting that repository-level configuration artifacts play a significant role in agent performance and should be systematically studied and utilized in practice.

Abstract: AI coding agents such as Codex and Claude Code are increasingly used to autonomously contribute to software repositories. However, little is known about how repository-level configuration artifacts affect operational efficiency of the agents. In this paper, we study the impact of AGENTS.md files on the runtime and token consumption of AI coding agents operating on GitHub pull requests. We analyze 10 repositories and 124 pull requests, executing agents under two conditions: with and without an AGENTS.md file. We measure wall-clock execution time and token usage during agent execution. Our results show that the presence of AGENTS.md is associated with a lower median runtime ($Δ28.64$%) and reduced output token consumption ($Δ16.58$%), while maintaining a comparable task completion behavior. Based on these results, we discuss immediate implications for the configuration and deployment of AI coding agents in practice, and outline a broader research agenda on the role of repository-level instructions in shaping the behavior, efficiency, and integration of AI coding agents in software development workflows.

</details>


### [16] [An Empirical Evaluation of Modern MLOps Frameworks](https://arxiv.org/abs/2601.20415)
*Jon Marcos-Mercadé,Unai Lopez-Novoa,Mikel Egaña Aranguren*

Main category: cs.SE

TL;DR: This paper evaluates MLflow, Metaflow, Apache Airflow, and Kubeflow Pipelines across key criteria using real-world ML workflows, providing a weighted, practical comparison to guide tool selection in MLOps.


<details>
  <summary>Details</summary>
Motivation: With the growing integration of AI in industry, developers require guidance in selecting appropriate MLOps tools. The complexity and diversity of available tools necessitate a systematic, practical comparison to support informed decision-making in managing the ML model lifecycle.

Method: The authors conduct an empirical evaluation of four MLOps tools—MLflow, Metaflow, Apache Airflow, and Kubeflow Pipelines—using six criteria: Ease of installation, Configuration flexibility, Interoperability, Code instrumentation complexity, Result interpretability, and Documentation. Two common ML workflows—MNIST digit classification and IMDB sentiment classification using BERT—are implemented using each tool, and the results are analyzed with weighted scoring based on criterion importance.

Result: MLflow performs best in ease of use, interpretability, and documentation, making it ideal for small to medium-scale projects. Kubeflow Pipelines excels in scalability and configuration flexibility but has higher setup complexity. Apache Airflow shows strong orchestration capabilities and interoperability, though with more complex code instrumentation. Metaflow offers a developer-friendly interface but lags in some documentation and flexibility aspects.

Conclusion: The study concludes that the suitability of MLOps tools depends on the specific use case and prioritized criteria; MLflow is best suited for simplicity and interpretability, Kubeflow Pipelines for scalability in complex environments, while Airflow excels in orchestration flexibility, helping practitioners make informed decisions based on scenario-specific requirements.

Abstract: Given the increasing adoption of AI solutions in professional environments, it is necessary for developers to be able to make informed decisions about the current tool landscape. This work empirically evaluates various MLOps (Machine Learning Operations) tools to facilitate the management of the ML model lifecycle: MLflow, Metaflow, Apache Airflow, and Kubeflow Pipelines. The tools are evaluated by assessing the criteria of Ease of installation, Configuration flexibility, Interoperability, Code instrumentation complexity, result interpretability, and Documentation when implementing two common ML scenarios: Digit classifier with MNIST and Sentiment classifier with IMDB and BERT. The evaluation is completed by providing weighted results that lead to practical conclusions on which tools are best suited for different scenarios.

</details>


### [17] [Challenges in Android Data Disclosure: An Empirical Study](https://arxiv.org/abs/2601.20459)
*Mugdha Khedkar,Michael Schlichtig,Mohamed Soliman,Eric Bodden*

Main category: cs.SE

TL;DR: This paper studies 683 Android developers' experiences with Google Play's Data Safety Section, revealing challenges in data classification despite confidence in data awareness, and calls for better tools and guidance for compliant privacy reporting.


<details>
  <summary>Details</summary>
Motivation: To understand developers' experiences and challenges in accurately reporting app data collection in the Google Play Store's Data Safety Section, given the complexity of large codebases and regulatory expectations.

Method: The paper combines a survey of 41 Android developers with an analysis of 172 online developer discussions involving 642 additional developers, using empirical methods to identify challenges in completing Google Play's Data Safety Section form.

Result: Developers are confident in identifying data collected by their apps but struggle to correctly classify it according to DSS categories, often relying on manual processes and external resources, leading to uncertainty and risk of rejection due to discrepancies with Google's requirements.

Conclusion: The study highlights the need for clearer guidance and improved tooling to help developers meet privacy reporting requirements on the Google Play Store, as current practices lead to uncertainty and potential non-compliance despite developer confidence in data awareness.

Abstract: Current legal frameworks enforce that Android developers accurately report the data their apps collect. However, large codebases can make this reporting challenging. This paper employs an empirical approach to understand developers' experience with Google Play Store's Data Safety Section (DSS) form.
  We first survey 41 Android developers to understand how they categorize privacy-related data into DSS categories and how confident they feel when completing the DSS form. To gain a broader and more detailed view of the challenges developers encounter during the process, we complement the survey with an analysis of 172 online developer discussions, capturing the perspectives of 642 additional developers. Together, these two data sources represent insights from 683 developers.
  Our findings reveal that developers often manually classify the privacy-related data their apps collect into the data categories defined by Google-or, in some cases, omit classification entirely-and rely heavily on existing online resources when completing the form. Moreover, developers are generally confident in recognizing the data their apps collect, yet they lack confidence in translating this knowledge into DSS-compliant disclosures. Key challenges include issues in identifying privacy-relevant data to complete the form, limited understanding of the form, and concerns about app rejection due to discrepancies with Google's privacy requirements.
  These results underscore the need for clearer guidance and more accessible tooling to support developers in meeting privacy-aware reporting obligations.

</details>


### [18] [DRAINCODE: Stealthy Energy Consumption Attacks on Retrieval-Augmented Code Generation via Context Poisoning](https://arxiv.org/abs/2601.20615)
*Yanlin Wang,Jiadong Wu,Tianyue Jiang,Mingwei Liu,Jiachi Chen,Chong Wang,Ensheng Shi,Xilin Liu,Yuchi Ma,Zibin Zheng*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in code generation by leveraging retrieval-augmented generation (RAG) methods. However, the computational costs associated with LLM inference, particularly in terms of latency and energy consumption, have received limited attention in the security context. This paper introduces DrainCode, the first adversarial attack targeting the computational efficiency of RAG-based code generation systems. By strategically poisoning retrieval contexts through a mutation-based approach, DrainCode forces LLMs to produce significantly longer outputs, thereby increasing GPU latency and energy consumption. We evaluate the effectiveness of DrainCode across multiple models. Our experiments show that DrainCode achieves up to an 85% increase in latency, a 49% increase in energy consumption, and more than a 3x increase in output length compared to the baseline. Furthermore, we demonstrate the generalizability of the attack across different prompting strategies and its effectiveness compared to different defenses. The results highlight DrainCode as a potential method for increasing the computational overhead of LLMs, making it useful for evaluating LLM security in resource-constrained environments. We provide code and data at https://github.com/DeepSoftwareAnalytics/DrainCode.

</details>


### [19] [Lila: Decentralized Build Reproducibility Monitoring for the Functional Package Management Model](https://arxiv.org/abs/2601.20662)
*Julien Malka,Arnout Engelen*

Main category: cs.SE

TL;DR: This paper presents Lila, a decentralized system for large-scale reproducibility monitoring in functional package management ecosystems, addressing the challenge of ensuring software build integrity through distributed verification and reporting.


<details>
  <summary>Details</summary>
Motivation: The increasing sophistication of attacks on software build systems necessitates stronger integrity guarantees. Reproducible builds offer such assurances, but effective monitoring at scale remains an unsolved problem, especially in maintaining transparency across large software ecosystems.

Method: The authors propose Lila, a decentralized system for reproducibility assessment that leverages the functional package management model. Lila supports distributed reporting and aggregation of build results into a reproducibility database, facilitating large-scale monitoring.

Result: Lila enables scalable and decentralized reproducibility monitoring, allowing for distributed build result reporting and aggregation, which supports both practitioners and empirical research in build reproducibility.

Conclusion: Lila provides a decentralized solution for reproducibility monitoring in functional package management systems, enabling scalable and distributed assessment of build reproducibility, thereby enhancing trust and transparency in software distribution.

Abstract: Ensuring the integrity of software build artifacts is an increasingly important concern for modern software engineering, driven by increasingly sophisticated attacks on build systems, distribution channels, and development infrastructures. Reproducible builds $\unicode{x2013}$ where binaries built independently from the same source code can be verified to be bit-for-bit identical to the distributed artifacts $\unicode{x2013}$ provide a principled foundation for transparency and trust in software distribution.
  Despite their potential, the large-scale adoption of reproducible builds faces two significant challenges: achieving high reproducibility rates across vast software collections and establishing reproducibility monitoring infrastructure that can operate at very large scale. While recent studies have shown that high reproducibility rates are achievable at scale $\unicode{x2013}$ demonstrated by the Nix ecosystem achieving over 90% reproducibility on more than 80,000 packages $\unicode{x2013}$ the problem of effective reproducibility monitoring remains largely unsolved.
  In this work, we address the reproducibility monitoring challenge by introducing Lila, a decentralized system for reproducibility assessment tailored to the functional package management model. Lila enables distributed reporting of build results and aggregation into a reproducibility database, benefiting both practitioners and future empirical build reproducibility studies.

</details>


### [20] [ProfInfer: An eBPF-based Fine-Grained LLM Inference Profiler](https://arxiv.org/abs/2601.20755)
*Bohua Zou,Debayan Roy,Dhimankumar Yogesh Airao,Weihao Xu,Binqi Sun,Yutao Liu,Haibo Chen*

Main category: cs.SE

TL;DR: This paper presents a fine-grained, non-intrusive profiling framework for LLM inference engines using eBPF, enabling real-time, operator-level visibility into performance with minimal overhead, demonstrated on llama.cpp.


<details>
  <summary>Details</summary>
Motivation: Existing LLM inference engines lack operator-level visibility, making it difficult to diagnose performance bottlenecks or determine resource utilization; this hinders optimization and efficient deployment.

Method: The authors design a dynamic, non-intrusive profiling system using eBPF technology that attaches probes to runtime functions across multiple layers without source code modification, applied to llama.cpp and similar architectures.

Result: The framework achieves less than 4% runtime overhead with high fidelity, generating detailed visualizations of operators, execution timelines, hardware counters, and system behavior for workloads like dense inference and MoE routing.

Conclusion: The proposed eBPF-based profiling framework enables transparent, fine-grained, and non-intrusive performance analysis of LLM inference engines, making it a practical tool for optimization and resource-aware deployment.

Abstract: As large language models (LLMs) move from research to production, understanding how inference engines behave in real time has become both essential and elusive. Unlike general-purpose engines such as ONNX Runtime, today's LLM inference systems offer little operator-level visibility, leaving developers blind to where time and resources go. Even basic questions -- is this workload memory-bound or compute-bound? -- often remain unanswered. To close this gap, we develop a fine-grained, non-intrusive profiling framework for modern LLM inference engines, exemplified by llama.cpp but applicable to similar runtime architectures. Built on extended Berkeley Packet Filter (eBPF) technology, our system dynamically attaches probes to runtime functions across multiple layers -- without modifying or recompiling the source. It transforms collected traces into rich visualizations of operators, graphs, timelines, and hardware counter trends, exposing how dense inference, Mixture-of-Experts routing, and operator offloading behave in practice. With less than 4% runtime overhead and high profiling fidelity, our framework makes LLM inference both transparent and diagnosable, turning performance profiling into a practical tool for optimization, scheduling, and resource-aware deployment.

</details>


### [21] [Context-Augmented Code Generation Using Programming Knowledge Graphs](https://arxiv.org/abs/2601.20810)
*Shahd Seddik,Fahd Seddik,Iman Saberi,Fatemeh Fard,Minh Hieu Huynh,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: The paper proposes Programming Knowledge Graph (PKG) to improve code generation in Large Language Models by enabling fine-grained, semantically structured retrieval and reducing hallucinations via re-ranking, achieving up to 20% improvement in pass@1 accuracy on code generation benchmarks.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) struggle with complex code generation tasks due to insufficient or irrelevant external knowledge; existing Retrieval-Augmented Generation (RAG) methods suffer from imprecise retrieval and hallucinated outputs, motivating the need for more effective knowledge integration.

Method: The authors introduce Programming Knowledge Graph (PKG), which structures programming knowledge into fine-grained semantic nodes, applies tree pruning for precise retrieval, and uses a re-ranking mechanism that integrates non-RAG solutions to reduce hallucinations and improve solution quality.

Result: Evaluations on HumanEval and MBPP benchmarks show up to 20% improvement in pass@1 accuracy and a 34% gain over baselines on MBPP, with minimal degradation on already-correct solutions.

Conclusion: PKG combined with re-ranking significantly enhances code generation for complex problems by improving retrieval precision and reducing hallucinations, outperforming existing RAG approaches while preserving performance on simpler tasks.

Abstract: Large Language Models (LLMs) excel at code generation but struggle with complex problems. Retrieval-Augmented Generation (RAG) mitigates this issue by integrating external knowledge, yet retrieval models often miss relevant context, and generation models hallucinate with irrelevant data. We propose Programming Knowledge Graph (PKG) for semantic representation and fine-grained retrieval of code and text. Our approach enhances retrieval precision through tree pruning and mitigates hallucinations via a re-ranking mechanism that integrates non-RAG solutions. Structuring external data into finer-grained nodes improves retrieval granularity. Evaluations on HumanEval and MBPP show up to 20% pass@1 accuracy gains and a 34% improvement over baselines on MBPP. Our findings demonstrate that our proposed PKG approach along with re-ranker effectively address complex problems while maintaining minimal negative impact on solutions that are already correct without RAG. The replication package is published at https://github.com/iamshahd/ProgrammingKnowledgeGraph

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [22] [What is the AGI in Offensive Security ?](https://arxiv.org/abs/2601.19968)
*Youngwoong Cho*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: What is the AGI in Offensive Security? One can break it down into two questions : (1) any offensive security tasks could be reduced into symbolic language manipulation (language representation + reasoning), (2) powerful language model (LLM) are enough to "deal with" any symbolic language manipulation. This paper can formally model a target system as a state machine and a hacker as an interactive symbolic agent. And it shows that every interaction in an offensive engagement can be encoded as a finite string. This paper provides definitions, short lemmas, and open discussion.

</details>


### [23] [Benchmarking LLAMA Model Security Against OWASP Top 10 For LLM Applications](https://arxiv.org/abs/2601.19970)
*Nourin Shahin,Izzat Alsmadi*

Main category: cs.CR

TL;DR: This study evaluates Llama and Llama Guard models against OWASP Top 10 LLM vulnerabilities, finding smaller specialized models like Llama-Guard-3-1B outperform larger base models in threat detection with lower latency.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly deployed in enterprise settings, their security vulnerabilities threaten data privacy and system integrity, necessitating rigorous evaluation under standardized frameworks like OWASP Top 10 for LLMs.

Method: The authors used the FABRIC testbed with NVIDIA A30 GPUs to evaluate five standard Llama models and five Llama Guard variants on 100 adversarial prompts across ten OWASP LLM vulnerability categories, measuring threat detection accuracy, response safety, and computational overhead.

Result: Llama-Guard-3-1B achieved the highest detection rate (76%) with low latency (0.165s/test), while base models like Llama-3.1-8B failed to detect threats (0% accuracy) despite higher latency (0.754s/test); an inverse relationship between model size and security effectiveness was observed.

Conclusion: Smaller, purpose-built models are more effective than larger general-purpose LLMs for security tasks; the authors release an open-source benchmark dataset to support reproducible AI security research.

Abstract: As large language models (LLMs) move from research prototypes to enterprise systems, their security vulnerabilities pose serious risks to data privacy and system integrity. This study benchmarks various Llama model variants against the OWASP Top 10 for LLM Applications framework, evaluating threat detection accuracy, response safety, and computational overhead. Using the FABRIC testbed with NVIDIA A30 GPUs, we tested five standard Llama models and five Llama Guard variants on 100 adversarial prompts covering ten vulnerability categories. Our results reveal significant differences in security performance: the compact Llama-Guard-3-1B model achieved the highest detection rate of 76% with minimal latency (0.165s per test), whereas base models such as Llama-3.1-8B failed to detect threats (0% accuracy) despite longer inference times (0.754s). We observe an inverse relationship between model size and security effectiveness, suggesting that smaller, specialized models often outperform larger general-purpose ones in security tasks. Additionally, we provide an open-source benchmark dataset including adversarial prompts, threat labels, and attack metadata to support reproducible research in AI security, [1].

</details>


### [24] [Reference-Free Spectral Analysis of EM Side-Channels for Always-on Hardware Trojan Detection](https://arxiv.org/abs/2601.20163)
*Mahsa Tahghigh,Hassan Salmani*

Main category: cs.CR

TL;DR: A reference-free method using STFT and GMMs detects always-on hardware Trojans by identifying persistent statistical patterns in EM side-channel signals, validated on AES-128.


<details>
  <summary>Details</summary>
Motivation: Most side-channel detection methods require golden references, which are often unavailable; this limits practical deployment for detecting hardware Trojans.

Method: Combines Short-Time Fourier Transform (STFT) at multiple window sizes with Gaussian Mixture Models (GMMs) to analyze statistical structure in EM side-channel data.

Result: Evaluation on AES-128 shows that HT-free circuits exhibit variable statistical structure, whereas circuits with always-on HTs show persistent patterns with fewer, more consistent GMM components.

Conclusion: The proposed reference-free approach effectively detects always-on hardware Trojans using time-frequency EM analysis and GMMs without needing golden references.

Abstract: Always-on hardware Trojans (HTs) pose a critical risk to trusted microelectronics, yet most side-channel detection methods rely on unavailable golden references. We present a reference-free approach that combines time-frequency EM analysis with Gaussian Mixture Models (GMMs). By applying Short-Time Fourier Transform (STFT) at multiple window sizes, we show that HT-free circuits exhibit fluctuating statistical structure, while always-on HTs leave persistent footprints with fewer, more consistent mixture components. Results on AES-128 demonstrate feasibility without requiring reference models.

</details>


### [25] [Securing AI Agents in Cyber-Physical Systems: A Survey of Environmental Interactions, Deepfake Threats, and Defenses](https://arxiv.org/abs/2601.20184)
*Mohsen Hatami,Van Tuan Pham,Hozefa Lakadawala,Yu Chen*

Main category: cs.CR

TL;DR: This survey examines emerging security threats to AI agents in cyber-physical systems, particularly those enabled by generative AI and the Model Context Protocol, and advocates for a holistic, lifecycle-based defense approach using the SENTINEL framework, supported by a real-world smart grid case study.


<details>
  <summary>Details</summary>
Motivation: The growing integration of AI agents into CPS introduces novel security risks that transcend traditional cyber or physical threats, especially with the rise of generative AI and dynamic protocols like MCP. There is a need to systematically understand and address these emerging threats in safety-critical environments.

Method: The authors conduct a comprehensive literature review on security threats to AI agents in CPS, focusing on deepfake attacks, environmental interactions, and Model Context Protocol (MCP)-mediated vulnerabilities. They apply the SENTINEL framework—a lifecycle-aware methodology—for threat characterization, feasibility analysis under CPS constraints, defense selection, and validation, and present an end-to-end case study in a smart grid context.

Result: The analysis reveals that deepfake and semantic manipulation attacks can compromise AI agent perception and reasoning in CPS, while MCP expands the attack surface. The smart grid case study demonstrates that timing, noise, and false-positive costs limit the effectiveness of detection-only defenses, necessitating more robust, integrated security architectures.

Conclusion: The survey emphasizes that detection mechanisms alone are insufficient for securing AI agents in safety-critical cyber-physical systems (CPS), advocating for provenance- and physics-grounded trust mechanisms, defense-in-depth architectures, and continuous validation via the SENTINEL framework to achieve trustworthy AI-enabled CPS.

Abstract: The increasing integration of AI agents into cyber-physical systems (CPS) introduces new security risks that extend beyond traditional cyber or physical threat models. Recent advances in generative AI enable deepfake and semantic manipulation attacks that can compromise agent perception, reasoning, and interaction with the physical environment, while emerging protocols such as the Model Context Protocol (MCP) further expand the attack surface through dynamic tool use and cross-domain context sharing. This survey provides a comprehensive review of security threats targeting AI agents in CPS, with a particular focus on environmental interactions, deepfake-driven attacks, and MCP-mediated vulnerabilities. We organize the literature using the SENTINEL framework, a lifecycle-aware methodology that integrates threat characterization, feasibility analysis under CPS constraints, defense selection, and continuous validation. Through an end-to-end case study grounded in a real-world smart grid deployment, we quantitatively illustrate how timing, noise, and false-positive costs constrain deployable defenses, and why detection mechanisms alone are insufficient as decision authorities in safety-critical CPS. The survey highlights the role of provenance- and physics-grounded trust mechanisms and defense-in-depth architectures, and outlines open challenges toward trustworthy AI-enabled CPS.

</details>


### [26] [Eliciting Least-to-Most Reasoning for Phishing URL Detection](https://arxiv.org/abs/2601.20270)
*Holly Trikilis,Pasindu Marasinghe,Fariza Rashid,Suranga Seneviratne*

Main category: cs.CR

TL;DR: The paper proposes a Least-to-Most prompting framework with an answer sensitivity mechanism to improve LLM-based phishing URL detection, achieving performance comparable to supervised models with less training data.


<details>
  <summary>Details</summary>
Motivation: While LLMs show promise in phishing URL detection, their reasoning processes are not well understood; this work aims to enhance and study their reasoning capabilities for better accuracy.

Method: The authors introduce a Least-to-Most prompting strategy with an answer sensitivity mechanism that iteratively decomposes the task and refines reasoning; evaluated on three URL datasets using four state-of-the-art LLMs, compared to one-shot and supervised baselines.

Result: The proposed framework outperforms one-shot prompting and matches the performance of supervised models despite using significantly less training data; ablation studies confirm the effectiveness of iterative reasoning and answer sensitivity.

Conclusion: Least-to-Most prompting with answer sensitivity enhances reasoning in LLMs for phishing detection, offering a data-efficient and effective alternative to traditional supervised approaches.

Abstract: Phishing continues to be one of the most prevalent attack vectors, making accurate classification of phishing URLs essential. Recently, large language models (LLMs) have demonstrated promising results in phishing URL detection. However, their reasoning capabilities that enabled such performance remain underexplored. To this end, in this paper, we propose a Least-to-Most prompting framework for phishing URL detection. In particular, we introduce an "answer sensitivity" mechanism that guides Least-to-Most's iterative approach to enhance reasoning and yield higher prediction accuracy. We evaluate our framework using three URL datasets and four state-of-the-art LLMs, comparing against a one-shot approach and a supervised model. We demonstrate that our framework outperforms the one-shot baseline while achieving performance comparable to that of the supervised model, despite requiring significantly less training data. Furthermore, our in-depth analysis highlights how the iterative reasoning enabled by Least-to-Most, and reinforced by our answer sensitivity mechanism, drives these performance gains. Overall, we show that this simple yet powerful prompting strategy consistently outperforms both one-shot and supervised approaches, despite requiring minimal training or few-shot guidance. Our experimental setup can be found in our Github repository github.sydney.edu.au/htri0928/least-to-most-phishing-detection.

</details>


### [27] [SemBind: Binding Diffusion Watermarks to Semantics Against Black-Box Forgery Attacks](https://arxiv.org/abs/2601.20310)
*Xin Zhang,Zijin Yang,Kejiang Chen,Linfeng Ma,Weiming Zhang,Nenghai Yu*

Main category: cs.CR

TL;DR: SemBind is a semantic-aware defense framework that binds latent watermarks to image content using a contrastive-learned semantic masker, effectively resisting black-box forgery in diffusion models while remaining compatible with existing watermarking techniques and preserving image quality.


<details>
  <summary>Details</summary>
Motivation: Latent-based watermarks in diffusion models are vulnerable to black-box forgery attacks, where attackers can transfer valid watermarks to unauthorized images using only black-box access and a single watermarked image. This undermines trust in image provenance, necessitating a defense that prevents unauthorized watermark embedding while preserving practicality and image fidelity.

Method: SemBind introduces a learned semantic masker trained via contrastive learning to generate invariant latent codes for the same text prompt and orthogonal codes across different prompts. These codes modulate the latent representation before applying standard watermarking, binding the watermark to image semantics. The method includes a tunable mask-ratio parameter to balance robustness and anti-forgery strength.

Result: Evaluated across four mainstream latent-based watermarking schemes, SemBind significantly reduces false acceptance rates under black-box forgery attacks. It achieves strong anti-forgery performance while maintaining watermark robustness and image quality, offering a controllable trade-off via the mask-ratio parameter.

Conclusion: SemBind provides an effective and generalizable defense against black-box forgery attacks on latent-based watermarks in latent diffusion models by binding watermarks to image semantics, thereby preserving provenance integrity while maintaining image quality and compatibility with existing watermarking methods.

Abstract: Latent-based watermarks, integrated into the generation process of latent diffusion models (LDMs), simplify detection and attribution of generated images. However, recent black-box forgery attacks, where an attacker needs at least one watermarked image and black-box access to the provider's model, can embed the provider's watermark into images not produced by the provider, posing outsized risk to provenance and trust. We propose SemBind, the first defense framework for latent-based watermarks that resists black-box forgery by binding latent signals to image semantics via a learned semantic masker. Trained with contrastive learning, the masker yields near-invariant codes for the same prompt and near-orthogonal codes across prompts; these codes are reshaped and permuted to modulate the target latent before any standard latent-based watermark. SemBind is generally compatible with existing latent-based watermarking schemes and keeps image quality essentially unchanged, while a simple mask-ratio parameter offers a tunable trade-off between anti-forgery strength and robustness. Across four mainstream latent-based watermark methods, our SemBind-enabled anti-forgery variants markedly reduce false acceptance under black-box forgery while providing a controllable robustness-security balance.

</details>


### [28] [UnlearnShield: Shielding Forgotten Privacy against Unlearning Inversion](https://arxiv.org/abs/2601.20325)
*Lulu Xue,Shengshan Hu,Wei Lu,Ziqi Zhou,Yufei Song,Jianhong Cheng,Minghui Li,Yanjun Zhang,Leo Yu Zhang*

Main category: cs.CR

TL;DR: UnlearnShield is the first defense against machine unlearning inversion, using constrained directional perturbations in cosine space to protect privacy without sacrificing model performance.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the emergence of unlearning inversion attacks, which can reconstruct data supposedly erased by machine unlearning, exposing critical privacy risks due to the lack of dedicated defenses.

Method: UnlearnShield introduces directional perturbations in the cosine representation space and employs a constraint module to regulate these perturbations, balancing privacy, accuracy, and forgetting efficacy.

Result: Experiments show that UnlearnShield achieves a favorable trade-off between privacy protection, model accuracy, and effective forgetting, demonstrating its effectiveness against unlearning inversion.

Conclusion: UnlearnShield effectively addresses the privacy vulnerabilities associated with machine unlearning inversion by reducing the risk of data reconstruction while maintaining model utility.

Abstract: Machine unlearning is an emerging technique that aims to remove the influence of specific data from trained models, thereby enhancing privacy protection. However, recent research has uncovered critical privacy vulnerabilities, showing that adversaries can exploit unlearning inversion to reconstruct data that was intended to be erased. Despite the severity of this threat, dedicated defenses remain lacking. To address this gap, we propose UnlearnShield, the first defense specifically tailored to counter unlearning inversion. UnlearnShield introduces directional perturbations in the cosine representation space and regulates them through a constraint module to jointly preserve model accuracy and forgetting efficacy, thereby reducing inversion risk while maintaining utility. Experiments demonstrate that it achieves a good trade-off among privacy protection, accuracy, and forgetting.

</details>


### [29] [Multimodal Multi-Agent Ransomware Analysis Using AutoGen](https://arxiv.org/abs/2601.20346)
*Asifullah Khan,Aimen Wadood,Mubashar Iqbal,Umme Zahoora*

Main category: cs.CR

TL;DR: This paper proposes a multimodal multi-agent ransomware analysis framework that integrates static, dynamic, and network data via specialized agents and a fusion agent, using transformer-based classification and interagent feedback for iterative feature refinement, achieving high accuracy and robustness in ransomware family classification and enabling trustworthy zero-day detection through confidence-aware abstention.


<details>
  <summary>Details</summary>
Motivation: Traditional ransomware detection methods (e.g., static analysis, heuristic scanning, behavioral analysis) are limited when used in isolation, necessitating a more robust, adaptive, and multimodal approach to handle evolving ransomware, including zero-day variants and polymorphic strains.

Method: A multimodal multi-agent architecture is introduced, where specialized agents process static, dynamic, and network-level data using autoencoder-based feature extraction. A fusion agent integrates these representations, which are then classified using a transformer. An interagent feedback mechanism iteratively refines features by suppressing low-confidence information, and confidence-aware abstention supports reliable deployment.

Result: The framework achieves up to 0.936 Macro-F1 score for ransomware family classification, reduces calibration error, and shows stable convergence over 100 epochs with over +0.75 improvement in agent quality and a final composite score of ~0.88 without fine-tuning. It outperforms single-modality and non-adaptive fusion baselines.

Conclusion: The proposed multimodal multi-agent framework provides a practical and effective approach for improving real-world ransomware defense systems by leveraging adaptive fusion, interagent feedback, and confidence-aware classification, outperforming single-modality and non-adaptive baselines.

Abstract: Ransomware has become one of the most serious cybersecurity threats causing major financial losses and operational disruptions worldwide.Traditional detection methods such as static analysis, heuristic scanning and behavioral analysis often fall short when used alone. To address these limitations, this paper presents multimodal multi agent ransomware analysis framework designed for ransomware classification. Proposed multimodal multiagent architecture combines information from static, dynamic and network sources. Each data type is handled by specialized agent that uses auto encoder based feature extraction. These representations are then integrated through a fusion agent. After that fused representation are used by transformer based classifier. It identifies the specific ransomware family. The agents interact through an interagent feedback mechanism that iteratively refines feature representations by suppressing low confidence information. The framework was evaluated on large scale datasets containing thousands of ransomware and benign samples. Multiple experiments were conducted on ransomware dataset. It outperforms single modality and nonadaptive fusion baseline achieving improvement of up to 0.936 in Macro-F1 for family classification and reducing calibration error. Over 100 epochs, the agentic feedback loop displays a stable monotonic convergence leading to over +0.75 absolute improvement in terms of agent quality and a final composite score of around 0.88 without fine tuning of the language models. Zeroday ransomware detection remains family dependent on polymorphism and modality disruptions. Confidence aware abstention enables reliable real world deployment by favoring conservativeand trustworthy decisions over forced classification. The findings indicate that proposed approach provides a practical andeffective path toward improving real world ransomware defense systems.

</details>


### [30] [LIFT: Byzantine Resilient Hub-Sampling](https://arxiv.org/abs/2601.20368)
*Mohamed Amine Legheraba,Nour Rachdi,Maria Gradinariu Potop-Butucaru,Sébastien Tixeuil*

Main category: cs.CR

TL;DR: Elevator, a peer sampling protocol for decentralized applications, is vulnerable to Byzantine adversaries; LIFT, a secure extension using cryptographic PRNG for hub selection, resists up to 10% Byzantine nodes.


<details>
  <summary>Details</summary>
Motivation: Evaluate Elevator's resilience against Byzantine adversaries and develop a more secure alternative for robust decentralized network topologies.

Method: Analyze Elevator under Byzantine attacks and propose LIFT, which uses a cryptographically secure pseudo-random number generator (PRNG) for hub selection to prevent adversarial manipulation.

Result: Elevator fails with only 2% Byzantine nodes, whereas LIFT maintains robustness with up to 10% Byzantine nodes.

Conclusion: Secure randomness in hub selection is critical for Byzantine resilience; LIFT provides a more reliable foundation for decentralized systems facing adversarial threats.

Abstract: Recently, a novel peer sampling protocol, Elevator, was introduced to construct network topologies tailored for emerging decentralized applications such as federated learning and blockchain. Elevator builds hub-based topologies in a fully decentralized manner, randomly selecting hubs among participating nodes. These hubs, acting as central nodes connected to the entire network, can be leveraged to accelerate message dissemination. Simulation results have shown that Elevator converges rapidly (within 3--4 cycles) and exhibits robustness against crash failures and churn. However, its resilience to Byzantine adversaries has not been investigated. In this work, we provide the first evaluation of Elevator under Byzantine adversaries and show that even a small fraction (2%) of Byzantine nodes is sufficient to subvert the network. As a result, we introduce LIFT, a new protocol that extends Elevator by employing a cryptographically secure pseudo-random number generator (PRNG) for hub selection, thereby mitigating Byzantine manipulation. In contrast, LIFT withstands adversarial infiltration and remains robust with up to 10% Byzantine nodes. These results highlight the necessity of secure randomness in decentralized hub formation and position LIFT as a more reliable building block for Byzantine-resilient decentralized systems.

</details>


### [31] [A High-Performance Fractal Encryption Framework and Modern Innovations for Secure Image Transmission](https://arxiv.org/abs/2601.20374)
*Sura Khalid Salsal,Eman Shaker Mahmood,Farah Tawfiq Abdul Hussien,Maryam Mahdi Alhusseini,Azhar Naji Alyahya,Nikolai Safiullin*

Main category: cs.CR

TL;DR: This paper introduces fractal encryption using Fourier transforms for enhanced image security and efficiency, outperforming traditional methods in speed and image quality.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the limitations of classical encryption algorithms, which face a trade-off between security, image fidelity, and computational efficiency, necessitating a more robust and efficient solution for image encryption in the face of growing data security threats.

Method: The paper proposes a novel image encryption method combining fractal encryption with Fourier transforms, mathematically formulating the approach and evaluating its performance against traditional encryption techniques in terms of security, efficiency, and image quality.

Result: The proposed fractal-based encryption method achieves faster encryption/decryption times and higher image fidelity compared to conventional techniques, effectively addressing gaps in prior research.

Conclusion: Fractal encryption based on Fourier transforms significantly improves encryption/decryption time and image fidelity compared to traditional methods, offering a promising direction for secure and efficient image encryption in the digital era.

Abstract: The current digital era, driven by growing threats to data security, requires a robust image encryption technique. Classical encryption algorithms suffer from a trade-off among security, image fidelity, and computational efficiency. This paper aims to enhance the performance and efficiency of image encryption. This is done by proposing Fractal encryption based on Fourier transforms as a new method of image encryption, leveraging state-of-the-art technology. The new approach considered here intends to enhance both security and efficiency in image encryption by comparing Fractal Encryption with basic methods. The suggested system also aims to optimise encryption/ decryption times and preserve image quality. This paper provides an introduction to Image Encryption using the fractal-based method, its mathematical formulation, and its comparative efficiency against publicly known traditional encryption methods. As a result, after filling the gaps identified in previous research, it has significantly improved both its encryption/decryption time and image fidelity compared to other techniques. In this paper, directions for future research and possible improvements are outlined for attention.

</details>


### [32] [Towards Quantum-Safe O-RAN -- Experimental Evaluation of ML-KEM-Based IPsec on the E2 Interface](https://arxiv.org/abs/2601.20378)
*Mario Perera,Michael Mackay,Max Hashem Eiza,Alessandro Raschellà,Nathan Shone,Mukesh Kumar Maheshwari*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As Open Radio Access Network (O-RAN) deployments expand and adversaries adopt 'store-now, decrypt-later' strategies, operators need empirical data on the cost of migrating critical control interfaces to post-quantum cryptography (PQC). This paper experimentally evaluates the impact of integrating a NIST-aligned module-lattice KEM (ML-KEM, CRYSTALS-Kyber) into IKEv2/IPsec protecting the E2 interface between the 5G Node B (gNB) and the Near-Real-Time RAN Intelligent Controller (Near-RT RIC). Using an open-source testbed built from srsRAN, Open5GS, FlexRIC and strongSwan (with liboqs), we compare three configurations: no IPsec, classical ECDH-based IPsec, and ML-KEM-based IPsec. The study focuses on IPsec tunnel-setup latency and the runtime behaviour of Near-RT RIC xApps under realistic signalling workloads. Results from repeated, automated runs show that ML-KEM integration adds a small overhead to tunnel establishment, which is approximately 3~5 ms in comparison to classical IPsec, while xApp operation and RIC control loops remain stable in our experiments. These findings indicate that ML-KEM based IPsec on the E2 interface is practically feasible and inform quantum-safe migration strategies for O-RAN deployments.

</details>


### [33] [Fuzzy Private Set Union via Oblivious Key Homomorphic Encryption Retrieval](https://arxiv.org/abs/2601.20400)
*Jean-Guillaume Dumas,Aude Maignan,Luiza Soezima*

Main category: cs.CR

TL;DR: This paper introduces Fuzzy Private Set Union (FPSU), allowing privacy-preserving union computation over approximate data, and proposes OKHER, a new cryptographic primitive that enables efficient FPSU protocols with tunable communication costs based on data structure.


<details>
  <summary>Details</summary>
Motivation: Traditional Private Set Union (PSU) protocols require exact matches for element inclusion, which limits their applicability in scenarios with noisy or approximate data. The motivation is to extend PSU to a 'fuzzy' context where similar elements (within a certain distance) are treated as equivalent, thereby enabling more practical and efficient private computations over real-world, imprecise datasets.

Method: The authors introduce Oblivious Key Homomorphic Encryption Retrieval (OKHER), an improved variant of OKVR, and apply it in combination with homomorphic encryption to construct FPSU protocols. They formally define the FPSU functionality and security model, and design protocols optimized for different data structures using the $l_\infty$ distance in a fuzzy setting where elements within a radius $\delta$ are considered equivalent.

Result: The FPSU protocols achieve asymptotic communication complexity ranging from $O(dm\log(\delta n))$ to $O(d^2m\log(\delta^2n))$, depending on the structure of the receiver's dataset. The use of OKHER and $l_\infty$-based ball patterns allows efficient handling of high-dimensional fuzzy sets.

Conclusion: The paper concludes that the proposed Fuzzy Private Set Union (FPSU) protocols, leveraging the OKHER sub-protocol and homomorphic encryption, efficiently handle approximated union computations in a privacy-preserving manner, with communication complexity adaptable to the structure of the data.

Abstract: Private Set Multi-Party Computations are protocols that allow parties to jointly and securely compute functions: apart from what is deducible from the output of the function, the input sets are kept private. Then, a Private Set Union (PSU), resp. Intersection (PSI), is a protocol that allows parties to jointly compute the union, resp. the intersection, between their private sets. Now a structured PSI, is a PSI where some structure of the sets can allow for more efficient protocols. For instance in Fuzzy PSI, elements only need to be close enough, instead of equal, to be part of the intersection. We present in this paper, Fuzzy PSU protocols (FPSU), able to efficiently take into account approximations in the union. For this, we introduce a new efficient sub-protocol, called Oblivious Key Homomorphic Encryption Retrieval (OKHER), improving on Oblivious Key-Value Retrieval (OKVR) techniques in our setting. In the fuzzy context, the receiver set $X=\{x_i\}_{1..n}$ is replaced by ${\mathcal B}_δ(X)$, the union of $n$ balls of dimension $d$ with radius $δ$, centered at the $x_i$. The sender set is just its $m$ points of dimension $d$. Then the FPSU functionality corresponds to $X \sqcup \{y \in Y, y \notin {\mathcal B}_δ(X)\}$. Thus, we formally define the FPSU functionality and security properties, and propose several protocols tuned to the patterns of the balls using the $l_\infty$ distance. Using our OKHER routine and homomorphic encryption, we are for instance able to obtain a FPSU protocols with an asymptotic communication volume bound ranging from $O(dm\log(δ{n}))$ to $O(d^2m\log(δ^2n))$, depending on the receiver data set structure.

</details>


### [34] [TÄMU: Emulating Trusted Applications at the (GlobalPlatform)-API Layer](https://arxiv.org/abs/2601.20507)
*Philipp Mao,Li Shi,Marcel Busch,Mathias Payer*

Main category: cs.CR

TL;DR: TÄMU enables dynamic analysis of Trusted Applications in mobile TEEs through API-layer rehosting and greedy high-level emulation, discovering 17 zero-days across 67 TAs from four TEEs, highlighting a critical gap in current vendor testing practices.


<details>
  <summary>Details</summary>
Motivation: The closed-source nature and fragmentation of mobile TEEs limit dynamic analysis of Trusted Applications, leaving critical security flaws undetected; most testing is restricted to static analysis, which is less effective at uncovering runtime vulnerabilities.

Method: TÄMU uses API-layer interposition and leverages standardized GlobalPlatform TEE APIs to rehost TAs; it introduces greedy high-level emulation to prioritize manual effort based on potential coverage gain during fuzzing, enabling scalable dynamic analysis across fragmented, closed-source TEE environments.

Result: TÄMU successfully emulated 67 TAs across four TEEs and uncovered 17 zero-day vulnerabilities in 11 different TAs through fuzzing campaigns, demonstrating the platform's effectiveness and the underdeveloped state of dynamic analysis in current TEE ecosystems.

Conclusion: TÄMU effectively bridges the gap in dynamic analysis capabilities for mobile TEEs by enabling practical fuzzing and debugging of Trusted Applications across multiple TEEs, demonstrating significant real-world impact through the discovery of 17 zero-day vulnerabilities.

Abstract: Mobile devices rely on Trusted Execution Environments (TEEs) to execute security-critical code and protect sensitive assets. This security-critical code is modularized in components known as Trusted Applications (TAs). Vulnerabilities in TAs can compromise the TEE and, thus, the entire system. However, the closed-source nature and fragmentation of mobile TEEs severely hinder dynamic analysis of TAs, limiting testing efforts to mostly static analyses. This paper presents TÄMU, a rehosting platform enabling dynamic analysis of TAs, specifically fuzzing and debugging, by interposing their execution at the API layer. To scale to many TAs across different TEEs, TÄMU leverages the standardization of TEE APIs, driven by the GlobalPlatform specifications. For the remaining TEE-specific APIs not shared across different TEEs, TÄMU introduces the notion of greedy high-level emulation, a technique that allows prioritizing manual rehosting efforts based on the potential coverage gain during fuzzing. We implement TÄMU and use it to emulate 67 TAs across four TEEs. Our fuzzing campaigns yielded 17 zero-day vulnerabilities across 11 TAs. These results indicate a deficit of dynamic analysis capabilities across the TEE ecosystem, where not even vendors with source code unlocked these capabilities for themselves. TÄMU promises to close this gap by bringing effective and practical dynamic analysis to the mobile TEE domain.

</details>


### [35] [IoT Device Identification with Machine Learning: Common Pitfalls and Best Practices](https://arxiv.org/abs/2601.20548)
*Kahraman Kostas,Rabia Yasa Kostas*

Main category: cs.CR

TL;DR: This paper identifies common pitfalls in IoT device identification using machine learning and provides guidelines to improve reproducibility and generalizability.


<details>
  <summary>Details</summary>
Motivation: Existing literature on ML-based device identification often suffers from methodological flaws, lack of reproducibility, and poor generalization; this work aims to systematically address these issues.

Method: The authors conduct a critical review of device identification methods, analyzing trade-offs between unique and class-based identification, challenges in data heterogeneity and feature extraction, and shortcomings in evaluation practices.

Result: Key pitfalls such as improper data augmentation and misuse of session identifiers are identified, and a robust framework for accurate and reproducible IoT device identification is proposed.

Conclusion: Improving rigor in data handling, feature engineering, and evaluation metrics is essential for advancing trustworthy and generalizable IoT security models.

Abstract: This paper critically examines the device identification process using machine learning, addressing common pitfalls in existing literature. We analyze the trade-offs between identification methods (unique vs. class based), data heterogeneity, feature extraction challenges, and evaluation metrics. By highlighting specific errors, such as improper data augmentation and misleading session identifiers, we provide a robust guideline for researchers to enhance the reproducibility and generalizability of IoT security models.

</details>


### [36] [/dev/SDB: Software Defined Boot -- A novel standard for diskless booting anywhere and everywhere](https://arxiv.org/abs/2601.20629)
*Aditya Mitra,Hamza Haroon,Amaan Rais Shah,Mohammad Elham Rasooli,Bogdan Itsam Dorantes Nikolaev,Tuğçe Ballı*

Main category: cs.CR

TL;DR: The proposed system /dev/SDB enables global, secure, wireless access to authorized operating systems over Wi-Fi and cellular networks, allowing users to work from anywhere without being restricted to wired corporate networks.


<details>
  <summary>Details</summary>
Motivation: Traditional network-based booting is limited to wired connections and small geographical areas. With increasing complexity and diversity of operating systems, especially in enterprises, there is a need for flexible, remote, and policy-compliant access to user-specific operating systems.

Method: /dev/SDB introduces a standardized system for network-based operating system access using wireless (Wi-Fi and cellular) connectivity, decoupling OS access from physical or wired network constraints.

Result: The system allows users anywhere in the world to securely access their authorized operating systems without requiring a corporate wired network, supporting remote work while maintaining OS policies and reducing hardware redundancy.

Conclusion: Dev/SDB extends network booting beyond wired infrastructure, enabling globally accessible, secure, and policy-compliant operating system access over wireless networks, enhancing flexibility in enterprise computing environments.

Abstract: A computer is nothing but a device that processes the instructions supplied to it. However, as computers evolved, the instructions or codes started to be more complicated. As computers started to be used by non-technical people, it became imperative that the users be able to use the machine without having underlying knowledge of the code or the hardware. And operating system became the backbone for translating the inputs from the user to actual operation on the hardware. With the increasing complexity and the choices of operating system, it became clear that different groups of people, especially in an enterprise scenario, required different operating systems. Installing them all on a single machine, for shared computers became a difficult task, giving rise to network-based booting. But network-based booting was confined to only wired connectivity, keeping it restricted to very small geographical areas. The proposed system, /dev/SDB, is aimed at creating a standard where any user, anyone on the globe, can access the operating system authorized to them without having to be on the corporate network. It aims to offer the same over Wi-Fi as well as cellular connectivity, ensuring employees can truly work from anywhere, while following the policies for operating systems and without redundant hardware.

</details>


### [37] [Supply Chain Insecurity: Exposing Vulnerabilities in iOS Dependency Management Systems](https://arxiv.org/abs/2601.20638)
*David Schmidt,Sebastian Schrittwieser,Edgar Weippl*

Main category: cs.CR

TL;DR: This paper examines security vulnerabilities in iOS dependency managers like CocoaPods, showing that leaked dependency data enables attackers to perform supply chain attacks via dependency confusion and hijacked URLs, potentially impacting millions of users.


<details>
  <summary>Details</summary>
Motivation: Despite the critical role of dependency managers in software security, the iOS ecosystem—especially CocoaPods—has received limited scrutiny. Given the increasing prevalence of supply chain attacks, the authors aim to uncover and quantify security weaknesses in iOS dependency management systems.

Method: The authors analyze dependency metadata exposure in 9,212 iOS apps, investigate attack vectors such as dependency confusion and URL/domain hijacking in CocoaPods, Carthage, and SwiftPM, and assess real-world impact by examining public GitHub repositories. They also conduct a cross-platform comparison with Cargo, Go modules, Maven, npm, and pip to derive mitigation insights.

Result: The analysis reveals widespread exposure of internal dependency names and versions in popular iOS apps, enabling dependency confusion attacks. Attackers can hijack abandoned domains or GitHub URLs to compromise dependencies. One hijacked CocoaPod could affect 63 apps and millions of users. Public repository inspection confirms the use of vulnerable dependencies.

Conclusion: The study concludes that iOS dependency management systems, particularly CocoaPods, are vulnerable to supply chain attacks due to information leakage, abandoned resource hijacking, and lack of proper verification mechanisms. The work emphasizes the need for improved security practices and proposes mitigation strategies by drawing comparisons with other mature ecosystems.

Abstract: Dependency management systems are a critical component in software development, enabling projects to incorporate existing functionality efficiently. However, misconfigurations and malicious actors in these systems pose severe security risks, leading to supply chain attacks. Despite the widespread use of smartphone apps, the security of dependency management systems in the iOS software supply chain has received limited attention. In this paper, we focus on CocoaPods, one of the most widely used dependency management systems for iOS app development, but also examine the security of Carthage and Swift Package Manager (SwiftPM). We demonstrate that iOS apps expose internal package names and versions. Attackers can exploit this leakage to register previously unclaimed dependencies in CocoaPods, enabling remote code execution (RCE) on developer machines and build servers. Additionally, we show that attackers can compromise dependencies by reclaiming abandoned domains and GitHub URLs. Analyzing a dataset of 9,212 apps, we quantify how many apps are susceptible to these vulnerabilities. Further, we inspect the use of vulnerable dependencies within public GitHub repositories. Our findings reveal that popular apps disclose internal dependency information, enabling dependency confusion attacks. Furthermore, we show that hijacking a single CocoaPod library through an abandoned domain could compromise 63 iOS apps, affecting millions of users. Finally, we compare iOS dependency management systems with Cargo, Go modules, Maven, npm, and pip to discuss mitigation strategies for the identified threats.

</details>


### [38] [Decentralized Identity in Practice: Benchmarking Latency, Cost, and Privacy](https://arxiv.org/abs/2601.20716)
*Abylay Satybaldy,Kamil Tylinski,Jiahua Xu*

Main category: cs.CR

TL;DR: This paper benchmarks three ledger-based Decentralized Identifier (DID) methods—Ethereum, Hedera, and XRP Ledger—evaluating latency, cost, and metadata leakage using a unified setup and SDKs, revealing key architectural trade-offs.


<details>
  <summary>Details</summary>
Motivation: Despite growing deployment of DID systems on distributed ledgers, there is a lack of systematic, cross-platform empirical studies assessing their real-world performance and privacy characteristics.

Method: The authors conduct an empirical benchmarking study using reference SDKs for Ethereum, Hedera, and XRP Ledger under a consistent experimental framework. They measure latency (normalized by block/consensus interval), transaction cost (normalized by native transfer fee), and privacy leakage using a Metadata-Leakage Score (MLS), an entropy-based metric in bits per operation.

Result: Ethereum offers fast off-chain DID creation but high on-chain latency and cost. XRP Ledger provides deterministic, low-cost transactions but higher metadata leakage due to verbose payloads. Hedera achieves low latency, low fees, and minimal metadata leakage, though minor variability stems from SDK processing pipelines.

Conclusion: The study demonstrates that ledger architecture and SDK implementation significantly influence DID system performance and privacy, beyond just consensus mechanisms, offering actionable insights for selecting and configuring DIDs under specific operational constraints.

Abstract: Decentralized Identifiers (DIDs) are increasingly deployed on distributed ledgers, yet systematic cross-platform evidence on their operational behavior remains limited. We present an empirical benchmarking study of three prominent ledger-based DID methods - Ethereum, Hedera, and XRP Ledger - using reference Software Development Kits (SDKs) under a unified experimental setup. We measure latency, transaction cost, and on-chain metadata exposure, normalizing latency by each platform's block or consensus interval and cost by its native value transfer fee. Privacy leakage is quantified using a Metadata-Leakage Score (MLS), an entropy-based measure expressed in bits per operation.
  Our results reveal distinct architectural trade-offs. Ethereum enables near-instant, off-chain DID creation, but incurs the highest latency and cost for on-chain lifecycle operations. XRPL delivers deterministic and stable latency with fixed, low fees, yet exhibits higher metadata leakage due to more verbose transaction payloads. Hedera achieves the lowest on-chain latency and low fees with minimal metadata leakage, while occasional variance arises from SDK-side processing and confirmation pipelines.
  Overall, the findings show that ledger architecture and SDK workflows play a major role in shaping DID latency, cost, and metadata exposure, complementing the effects of the underlying consensus mechanism. These results provide evidence-based insights to support informed selection and configuration of DID systems under performance and privacy constraints.

</details>
