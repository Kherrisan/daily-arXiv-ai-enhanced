<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 3]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Stealing AI Model Weights Through Covert Communication Channels](https://arxiv.org/abs/2510.00151)
*Valentin Barbaza,Alan Rodrigo Diaz-Rizo,Hassan Aboushady,Spyridon Raptis,Haralampos-G. Stratigopoulos*

Main category: cs.CR

TL;DR: The paper discusses an AI model stealing attack through hardware Trojans and wireless interception, validated with various models.


<details>
  <summary>Details</summary>
Motivation: High cost and proprietary value of AI models make them targets for stealing attacks, creating a need for understanding their vulnerabilities.

Method: The attack compromises a device with a hardware Trojan to leak weights via hidden communication channels then uses wireless devices to intercept and reconstruct the stolen data.

Result: Demonstration showed the attack works across multiple AI models, with techniques addressing bit error rates to enhance mitigation and recovery.

Conclusion: The research underscores the vulnerability of AI hardware and wireless systems to stealing, with implications for future defenses against such covert attacks.

Abstract: AI models are often regarded as valuable intellectual property due to the
high cost of their development, the competitive advantage they provide, and the
proprietary techniques involved in their creation. As a result, AI model
stealing attacks pose a serious concern for AI model providers. In this work,
we present a novel attack targeting wireless devices equipped with AI hardware
accelerators. The attack unfolds in two phases. In the first phase, the
victim's device is compromised with a hardware Trojan (HT) designed to covertly
leak model weights through a hidden communication channel, without the victim
realizing it. In the second phase, the adversary uses a nearby wireless device
to intercept the victim's transmission frames during normal operation and
incrementally reconstruct the complete weight matrix. The proposed attack is
agnostic to both the AI model architecture and the hardware accelerator used.
We validate our approach through a hardware-based demonstration involving four
diverse AI models of varying types and sizes. We detail the design of the HT
and the covert channel, highlighting their stealthy nature. Additionally, we
analyze the impact of bit error rates on the reception and propose an error
mitigation technique. The effectiveness of the attack is evaluated based on the
accuracy of the reconstructed models with stolen weights and the time required
to extract them. Finally, we explore potential defense mechanisms.

</details>


### [2] [Calyx: Privacy-Preserving Multi-Token Optimistic-Rollup Protocol](https://arxiv.org/abs/2510.00164)
*Dominik Apel,Zeta Avarikioti,Matteo Maffei,Yuheng Wang*

Main category: cs.CR

TL;DR: Introduce Calyx, a multi-token optimistic-Rollup protocol ensuring full L2 transaction privacy and efficient fraud-proofing with low on-chain costs.


<details>
  <summary>Details</summary>
Motivation: Current rollups need transaction plaintext on-chain for data availability, leading to privacy issues. There's a lack of solutions that provide transaction privacy while maintaining the benefits of rollup scalability.

Method: Calyx uses a one-step fraud-proof mechanism and other techniques to guarantee sender, recipient, amount, and token-type privacy. It also supports atomic multi-token transactions and includes a transaction fee scheme for sustainability.

Result: Single transaction cost is ~$0.06 (0.00002 ETH) with constant on-chain cost asymptotically. Implementation is analyzed and evaluated.

Conclusion: Calyx is the first protocol to enable private assets without compromising performance in a multi-token optimistic rollup setting, proving feasibility of private on-chain asset transfers with moderate cost.

Abstract: Rollup protocols have recently received significant attention as a promising
class of Layer 2 (L2) scalability solutions. By utilizing the Layer 1 (L1)
blockchain solely as a bulletin board for a summary of the executed
transactions and state changes, rollups enable secure off-chain execution while
avoiding the complexity of other L2 mechanisms. However, to ensure data
availability, current rollup protocols require the plaintext of executed
transactions to be published on-chain, resulting in inherent privacy
limitations.
  In this paper, we address this problem by introducing Calyx, the first
privacy-preserving multi-token optimistic-Rollup protocol. Calyx guarantees
full payment privacy for all L2 transactions, revealing no information about
the sender, recipient, transferred amount, or token type. The protocol further
supports atomic execution of multiple multi-token transactions and introduces a
transaction fee scheme to enable broader application scenarios while ensuring
the sustainable operation of the protocol. To enforce correctness, Calyx adopts
an efficient one-step fraud-proof mechanism. We analyze the security and
privacy guarantees of the protocol and provide an implementation and
evaluation. Our results show that executing a single transaction costs
approximately $0.06 (0.00002 ETH) and incurs only constant-size on-chain cost
in asymptotic terms.

</details>


### [3] [CHAI: Command Hijacking against embodied AI](https://arxiv.org/abs/2510.00181)
*Luis Burbano,Diego Ortiz,Qi Sun,Siwei Yang,Haoqin Tu,Cihang Xie,Yinzhi Cao,Alvaro A Cardenas*

Main category: cs.CR

TL;DR: CHAI is a new class of prompt-based attacks exploiting multimodal reasoning in embodied AI, demonstrating superior effectiveness against LVLMs and emphasizing the need for advanced defenses.


<details>
  <summary>Details</summary>
Motivation: Next-generation embodied AI systems' security risks need investigation due to their multimodal language interpretation capabilities introducing novel vulnerabilities.

Method: CHAI employs deceptive visual inputs, token space search, prompt dictionaries, and attacker model guidance to generate Visual Attack Prompts targeting LVLMs.

Result: CHAI outperformed state-of-the-art attacks across drone landing, autonomous driving, object tracking, and real robotic vehicle scenarios.

Conclusion: The paper highlights the urgent need for defenses beyond traditional adversarial robustness to counter attacks leveraging the multimodal reasoning strengths of embodied AI systems.

Abstract: Embodied Artificial Intelligence (AI) promises to handle edge cases in
robotic vehicle systems where data is scarce by using common-sense reasoning
grounded in perception and action to generalize beyond training distributions
and adapt to novel real-world situations. These capabilities, however, also
create new security risks. In this paper, we introduce CHAI (Command Hijacking
against embodied AI), a new class of prompt-based attacks that exploit the
multimodal language interpretation abilities of Large Visual-Language Models
(LVLMs). CHAI embeds deceptive natural language instructions, such as
misleading signs, in visual input, systematically searches the token space,
builds a dictionary of prompts, and guides an attacker model to generate Visual
Attack Prompts. We evaluate CHAI on four LVLM agents; drone emergency landing,
autonomous driving, and aerial object tracking, and on a real robotic vehicle.
Our experiments show that CHAI consistently outperforms state-of-the-art
attacks. By exploiting the semantic and multimodal reasoning strengths of
next-generation embodied AI systems, CHAI underscores the urgent need for
defenses that extend beyond traditional adversarial robustness.

</details>
