<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 18]
- [cs.SE](#cs.SE) [Total: 8]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [The WASM Cloak: Evaluating Browser Fingerprinting Defenses Under WebAssembly based Obfuscation](https://arxiv.org/abs/2508.21219)
*A H M Nazmus Sakib,Mahsin Bin Akram,Joseph Spracklen,Sahan Kalutarage,Raveen Wijewickrama,Igor Bilogrevic,Murtuza Jadliwala*

Main category: cs.CR

TL;DR: WASM obfuscation bypasses academic JS-based fingerprinting detectors but not practical browser tools, highlighting a need for updated defense strategies and revealing risks for future attacks.


<details>
  <summary>Details</summary>
Motivation: The adoption of WebAssembly (WASM) introduces a potential blind spot in browser fingerprinting defenses, as adversaries can obfuscate malicious logic by converting JavaScript to WASM's binary format.

Method: The authors developed an automated pipeline to translate JS fingerprinting scripts into WASM-obfuscated variants, testing them against research-based detectors and commercial in-browser tools.

Result: Research-based detectors relying on JS source code analysis showed moderate vulnerability due to outdated datasets and WASM incompatibility, while commercial tools like browser extensions and native features remained effective against obfuscated scripts.

Conclusion: The study reveals a gap between academic and practical defense strategies against WASM-based obfuscation, emphasizing the need to strengthen detection approaches and anticipate future evasive techniques.

Abstract: Browser fingerprinting defenses have historically focused on detecting
JavaScript(JS)-based tracking techniques. However, the widespread adoption of
WebAssembly (WASM) introduces a potential blind spot, as adversaries can
convert JS to WASM's low-level binary format to obfuscate malicious logic. This
paper presents the first systematic evaluation of how such WASM-based
obfuscation impacts the robustness of modern fingerprinting defenses. We
develop an automated pipeline that translates real-world JS fingerprinting
scripts into functional WASM-obfuscated variants and test them against two
classes of defenses: state-of-the-art detectors in research literature and
commercial, in-browser tools. Our findings reveal a notable divergence:
detectors proposed in the research literature that rely on feature-based
analysis of source code show moderate vulnerability, stemming from outdated
datasets or a lack of WASM compatibility. In contrast, defenses such as browser
extensions and native browser features remained completely effective, as their
API-level interception is agnostic to the script's underlying implementation.
These results highlight a gap between academic and practical defense strategies
and offer insights into strengthening detection approaches against WASM-based
obfuscation, while also revealing opportunities for more evasive techniques in
future attacks.

</details>


### [2] [Locus: Agentic Predicate Synthesis for Directed Fuzzing](https://arxiv.org/abs/2508.21302)
*Jie Zhu,Chihao Shen,Ziyang Li,Jiahao Yu,Yizheng Chen,Kexin Pei*

Main category: cs.CR

TL;DR: Locus automates predicate synthesis for directed fuzzing, improving efficiency 41.6× and discovering eight new vulnerabilities, addressing limitations of manual constraints and branch-distance-based approaches.


<details>
  <summary>Details</summary>
Motivation: Existing directed fuzzers rely on branch distances or manual constraints that are either imprecise or non-generalizable. This limits their effectiveness in discovering deeply nested vulnerabilities across diverse programs.

Method: Locus synthesizes semantically meaningful predicates as milestones using an agentic framework with program analysis tools and symbolic execution. These predicates guide fuzzing to reject unproductive paths while ensuring strict relaxation of target constraints.

Result: Locus achieves 41.6× average speedup over eight state-of-the-art fuzzers for real-world vulnerability discovery, identifying eight previously unpatched bugs with one vulnerability already receiving a draft patch.

Conclusion: Locus significantly enhances directed fuzzing efficiency by synthesizing predicates to capture progress toward target states, outperforming existing methods with a 41.6x speedup and discovering eight new unpatched vulnerabilities.

Abstract: Directed fuzzing aims to find program inputs that lead to specified target
program states. It has broad applications, such as debugging system crashes,
confirming reported bugs, and generating exploits for potential
vulnerabilities. This task is inherently challenging because target states are
often deeply nested in the program, while the search space manifested by
numerous possible program inputs is prohibitively large. Existing approaches
rely on branch distances or manually-specified constraints to guide the search;
however, the branches alone are often insufficient to precisely characterize
progress toward reaching the target states, while the manually specified
constraints are often tailored for specific bug types and thus difficult to
generalize to diverse target states and programs.
  We present Locus, a novel framework to improve the efficiency of directed
fuzzing. Our key insight is to synthesize predicates to capture fuzzing
progress as semantically meaningful intermediate states, serving as milestones
towards reaching the target states. When used to instrument the program under
fuzzing, they can reject executions unlikely to reach the target states, while
providing additional coverage guidance. To automate this task and generalize to
diverse programs, Locus features an agentic framework with program analysis
tools to synthesize and iteratively refine the candidate predicates, while
ensuring the predicates strictly relax the target states to prevent false
rejections via symbolic execution. Our evaluation shows that Locus
substantially improves the efficiency of eight state-of-the-art fuzzers in
discovering real-world vulnerabilities, achieving an average speedup of 41.6x.
So far, Locus has found eight previously unpatched bugs, with one already
acknowledged with a draft patch.

</details>


### [3] [LLM-driven Provenance Forensics for Threat Investigation and Detection](https://arxiv.org/abs/2508.21323)
*Kunal Mukherjee,Murat Kantarcioglu*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce PROVSEEK, an LLM-powered agentic framework for automated
provenance-driven forensic analysis and threat intelligence extraction.
PROVSEEK employs specialized toolchains to dynamically retrieve relevant
context by generating precise, context-aware queries that fuse a vectorized
threat report knowledge base with data from system provenance databases. The
framework resolves provenance queries, orchestrates multiple role-specific
agents to mitigate hallucinations, and synthesizes structured, ground-truth
verifiable forensic summaries. By combining agent orchestration with
Retrieval-Augmented Generation (RAG) and chain-of-thought (CoT) reasoning,
PROVSEEK enables adaptive multi-step analysis that iteratively refines
hypotheses, verifies supporting evidence, and produces scalable, interpretable
forensic explanations of attack behaviors. By combining provenance data with
agentic reasoning, PROVSEEK establishes a new paradigm for grounded agentic
forecics to investigate APTs. We conduct a comprehensive evaluation on publicly
available DARPA datasets, demonstrating that PROVSEEK outperforms
retrieval-based methods for intelligence extraction task, achieving a 34%
improvement in contextual precision/recall; and for threat detection task,
PROVSEEK achieves 22%/29% higher precision/recall compared to both a baseline
agentic AI approach and State-Of-The-Art (SOTA) Provenance-based Intrusion
Detection System (PIDS).

</details>


### [4] [Risks and Compliance with the EU's Core Cyber Security Legislation](https://arxiv.org/abs/2508.21386)
*Jukka Ruohonen,Jesper Løffler Nielsen,Jakub Skórczynski*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The European Union (EU) has long favored a risk-based approach to regulation.
Such an approach is also used in recent cyber security legislation enacted in
the EU. Risks are also inherently related to compliance with the new
legislation. Objective: The paper investigates how risks are framed in the EU's
five core cyber security legislative acts, whether the framings indicate
convergence or divergence between the acts and their risk concepts, and what
qualifying words and terms are used when describing the legal notions of risks.
Method : The paper's methodology is based on qualitative legal interpretation
and taxonomy-building. Results: The five acts have an encompassing coverage of
different cyber security risks, including but not limited to risks related to
technical, organizational, and human security as well as those not originating
from man-made actions. Both technical aspects and assets are used to frame the
legal risk notions in many of the legislative acts. A threat-centric viewpoint
is also present in one of the acts. Notable gaps are related to acceptable
risks, non-probabilistic risks, and residual risks. Conclusion: The EU's new
cyber security legislation has significantly extended the risk-based approach
to regulations. At the same time, complexity and compliance burden have
increased. With this point in mind, the paper concludes with a few practical
takeaways about means to deal with compliance and research it.

</details>


### [5] [zkLoRA: Fine-Tuning Large Language Models with Verifiable Security via Zero-Knowledge Proofs](https://arxiv.org/abs/2508.21393)
*Guofu Liao,Taotao Wang,Shengli Zhang,Jiqun Zhang,Shi Long,Dacheng Tao*

Main category: cs.CR

TL;DR: zkLoRA: A secure, verifiable framework combining zero-knowledge proofs and parameter-efficient fine-tuning for LLMs in untrusted environments.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs is computationally intensive and raises security/privacy concerns in untrusted environments, while existing methods like LoRA lack verifiability under zero-knowledge constraints.

Method: The framework integrates LoRA with zero-knowledge proofs (ZKPs), using cryptographic techniques like lookup arguments, sumcheck protocols, and polynomial commitments to verify arithmetic and non-arithmetic operations in Transformer architectures while preserving privacy.

Result: zkLoRA achieves end-to-end verifiability for forward/backward propagation and parameter updates during LoRA fine-tuning, demonstrating practicality on LLaMA with up to 13B parameters through GPU-based implementations.

Conclusion: zkLoRA bridges the gap between parameter-efficient fine-tuning and secure verification in untrusted environments, enabling trustworthy LLM deployment.

Abstract: Fine-tuning large language models (LLMs) is crucial for adapting them to
specific tasks, yet it remains computationally demanding and raises concerns
about correctness and privacy, particularly in untrusted environments. Although
parameter-efficient methods like Low-Rank Adaptation (LoRA) significantly
reduce resource requirements, ensuring the security and verifiability of
fine-tuning under zero-knowledge constraints remains an unresolved challenge.
To address this, we introduce zkLoRA, the first framework to integrate LoRA
fine-tuning with zero-knowledge proofs (ZKPs), achieving provable security and
correctness. zkLoRA employs advanced cryptographic techniques -- such as lookup
arguments, sumcheck protocols, and polynomial commitments -- to verify both
arithmetic and non-arithmetic operations in Transformer-based architectures.
The framework provides end-to-end verifiability for forward propagation,
backward propagation, and parameter updates during LoRA fine-tuning, while
safeguarding the privacy of model parameters and training data. Leveraging
GPU-based implementations, zkLoRA demonstrates practicality and efficiency
through experimental validation on open-source LLMs like LLaMA, scaling up to
13 billion parameters. By combining parameter-efficient fine-tuning with ZKPs,
zkLoRA bridges a critical gap, enabling secure and trustworthy deployment of
LLMs in sensitive or untrusted environments.

</details>


### [6] [An Empirical Study of Vulnerable Package Dependencies in LLM Repositories](https://arxiv.org/abs/2508.21417)
*Shuhan Liu,Xing Hu,Xin Xia,David Lo,Xiaohu Yang*

Main category: cs.CR

TL;DR: LLMs have pervasive, long-disclosed dependency vulnerabilities (50% for +56 months) and 75% include vulnerable packages, revealing critical supply chain security risks.


<details>
  <summary>Details</summary>
Motivation: Existing research overlooks LLM dependency supply chain vulnerabilities despite their critical security risks due to heavy reliance on external packages.

Method: Empirical analysis of 52 open-source LLMs' dependencies and vulnerabilities, examination of repository vulnerability management practices, and comparison to the Python ecosystem.

Result: 50% of LLM vulnerabilities remain undisclosed for >56 months (vs. Python) and 75.8% of LLMs contain vulnerable dependencies in config files.

Conclusion: This study advances understanding of LLM supply chain risks, provides practitioner insights, and highlights security improvement directions.

Abstract: Large language models (LLMs) have developed rapidly in recent years,
revolutionizing various fields. Despite their widespread success, LLMs heavily
rely on external code dependencies from package management systems, creating a
complex and interconnected LLM dependency supply chain. Vulnerabilities in
dependencies can expose LLMs to security risks. While existing research
predominantly focuses on model-level security threats, vulnerabilities within
the LLM dependency supply chain have been overlooked. To fill this gap, we
conducted an empirical analysis of 52 open-source LLMs, examining their
third-party dependencies and associated vulnerabilities. We then explored
activities within the LLM repositories to understand how maintainers manage
third-party vulnerabilities in practice. Finally, we compared third-party
dependency vulnerabilities in the LLM ecosystem to those in the Python
ecosystem. Our results show that half of the vulnerabilities in the LLM
ecosystem remain undisclosed for more than 56.2 months, significantly longer
than those in the Python ecosystem. Additionally, 75.8% of LLMs include
vulnerable dependencies in their configuration files. This study advances the
understanding of LLM supply chain risks, provides insights for practitioners,
and highlights potential directions for improving the security of the LLM
supply chain.

</details>


### [7] [RepoMark: A Code Usage Auditing Framework for Code Large Language Models](https://arxiv.org/abs/2508.21432)
*Wenjie Qu,Yuguang Zhou,Bo Wang,Wengrui Zheng,Yuexin Li,Jinyuan Jia,Jiaheng Zhang*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The rapid development of Large Language Models (LLMs) for code generation has
transformed software development by automating coding tasks with unprecedented
efficiency.
  However, the training of these models on open-source code repositories (e.g.,
from GitHub) raises critical ethical and legal concerns, particularly regarding
data authorization and open-source license compliance. Developers are
increasingly questioning whether model trainers have obtained proper
authorization before using repositories for training, especially given the lack
of transparency in data collection.
  To address these concerns, we propose a novel data marking framework RepoMark
to audit the data usage of code LLMs. Our method enables repository owners to
verify whether their code has been used in training, while ensuring semantic
preservation, imperceptibility, and theoretical false detection rate (FDR)
guarantees. By generating multiple semantically equivalent code variants,
RepoMark introduces data marks into the code files, and during detection,
RepoMark leverages a novel ranking-based hypothesis test to detect memorization
within the model. Compared to prior data auditing approaches, RepoMark
significantly enhances sample efficiency, allowing effective auditing even when
the user's repository possesses only a small number of code files.
  Experiments demonstrate that RepoMark achieves a detection success rate over
90\% on small code repositories under a strict FDR guarantee of 5\%. This
represents a significant advancement over existing data marking techniques, all
of which only achieve accuracy below 55\% under identical settings. This
further validates RepoMark as a robust, theoretically sound, and promising
solution for enhancing transparency in code LLM training, which can safeguard
the rights of repository owners.

</details>


### [8] [Time Tells All: Deanonymization of Blockchain RPC Users with Zero Transaction Fee (Extended Version)](https://arxiv.org/abs/2508.21440)
*Shan Wang,Ming Yang,Yu Liu,Yue Zhang,Shuaiqing Zhang,Zhen Ling,Jiannong Cao,Xinwen Fu*

Main category: cs.CR

TL;DR: This paper introduces a zero-cost, real-world effective deanonymization attack on blockchain RPC users by exploiting timing correlations between network traffic and ledger data, achieving >95% accuracy across major blockchains.


<details>
  <summary>Details</summary>
Motivation: RPC services in public blockchains introduce insufficiently studied privacy risks. Existing deanonymization approaches either are inapplicable or require transaction fees, but this work addresses a new attack vector that requires no active participation from the adversary.

Method: The authors exploit temporal correlations between transaction confirmation timestamps on the blockchain ledger and the timestamps of TCP packets sent by the victim when querying transaction status. They mathematically model the attack, analyze ledger data, and conduct real-world experiments.

Result: The attack achieves >95% success in linking victims' IP addresses to pseudonyms on Ethereum, Bitcoin, and Solana blockchains, with no transaction fees incurred by the attacker.

Conclusion: The paper demonstrates a highly effective and cost-free deanonymization attack on blockchain RPC users, emphasizing the critical privacy vulnerabilities in such services and the threat posed by passive adversaries with network infrastructure access.

Abstract: Remote Procedure Call (RPC) services have become a primary gateway for users
to access public blockchains. While they offer significant convenience, RPC
services also introduce critical privacy challenges that remain insufficiently
examined. Existing deanonymization attacks either do not apply to blockchain
RPC users or incur costs like transaction fees assuming an active network
eavesdropper. In this paper, we propose a novel deanonymization attack that can
link an IP address of a RPC user to this user's blockchain pseudonym. Our
analysis reveals a temporal correlation between the timestamps of transaction
confirmations recorded on the public ledger and those of TCP packets sent by
the victim when querying transaction status. We assume a strong passive
adversary with access to network infrastructure, capable of monitoring traffic
at network border routers or Internet exchange points. By monitoring network
traffic and analyzing public ledgers, the attacker can link the IP address of
the TCP packet to the pseudonym of the transaction initiator by exploiting the
temporal correlation. This deanonymization attack incurs zero transaction fee.
We mathematically model and analyze the attack method, perform large-scale
measurements of blockchain ledgers, and conduct real-world attacks to validate
the attack. Our attack achieves a high success rate of over 95% against normal
RPC users on various blockchain networks, including Ethereum, Bitcoin and
Solana.

</details>


### [9] [SoK: Large Language Model-Generated Textual Phishing Campaigns End-to-End Analysis of Generation, Characteristics, and Detection](https://arxiv.org/abs/2508.21457)
*Fengchao Chen,Tingmin Wu,Van Nguyen,Carsten Rudolph*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Phishing is a pervasive form of social engineering in which attackers
impersonate trusted entities to steal information or induce harmful actions.
Text-based phishing dominates for its low cost, scalability, and
concealability, advantages recently amplified by large language models (LLMs)
that enable ``Phishing-as-a-Service'' attacks at scale within minutes. Despite
the growing research into LLM-facilitated phishing attacks, consolidated
systematic research on the phishing attack life cycle remains scarce. In this
work, we present the first systematization of knowledge (SoK) on LLM-generated
phishing, offering an end-to-end analysis that spans generation techniques,
attack features, and mitigation strategies. We introduce
Generation-Characterization-Defense (GenCharDef), which systematizes the ways
in which LLM-generated phishing differs from traditional phishing across
methodologies, security perspectives, data dependencies, and evaluation
practices. This framework highlights unique challenges of LLM-driven phishing,
providing a coherent foundation for understanding the evolving threat landscape
and guiding the design of more resilient defenses.

</details>


### [10] [Detecting Stealthy Data Poisoning Attacks in AI Code Generators](https://arxiv.org/abs/2508.21636)
*Cristina Improta*

Main category: cs.CR

TL;DR: This study shows existing defenses against stealthy, triggerless dataset poisoning attacks in code generation models are ineffective, requiring new trigger-independent detection methods to secure AI-assisted code generation.


<details>
  <summary>Details</summary>
Motivation: DL models for natural language-to-code generation face stealthy data poisoning attacks that replace secure code with vulnerable implementations without detectable triggers, making existing detection methods inadequate to distinguish poisoned samples from clean ones.

Method: The researchers conducted targeted poisoning on three DL models (CodeBERT, CodeT5+, AST-T5) and evaluated the effectiveness of spectral signatures analysis, activation clustering, and static analysis as defense mechanisms under stealthy threat conditions.

Result: All tested defense methods failed to adequately detect triggerless poisoning: representation-based approaches couldn't isolate poisoned samples, while static analysis had high false positive/negative rates.

Conclusion: The study highlights the need for more robust, trigger-independent defenses for AI-assisted code generation as existing methods struggle with detecting stealthy, triggerless poisoning attacks.

Abstract: Deep learning (DL) models for natural language-to-code generation have become
integral to modern software development pipelines. However, their heavy
reliance on large amounts of data, often collected from unsanitized online
sources, exposes them to data poisoning attacks, where adversaries inject
malicious samples to subtly bias model behavior. Recent targeted attacks
silently replace secure code with semantically equivalent but vulnerable
implementations without relying on explicit triggers to launch the attack,
making it especially hard for detection methods to distinguish clean from
poisoned samples. We present a systematic study on the effectiveness of
existing poisoning detection methods under this stealthy threat model.
Specifically, we perform targeted poisoning on three DL models (CodeBERT,
CodeT5+, AST-T5), and evaluate spectral signatures analysis, activation
clustering, and static analysis as defenses. Our results show that all methods
struggle to detect triggerless poisoning, with representation-based approaches
failing to isolate poisoned samples and static analysis suffering false
positives and false negatives, highlighting the need for more robust,
trigger-independent defenses for AI-assisted code generation.

</details>


### [11] [Towards a Decentralized IoT Onboarding for Smart Homes Using Consortium Blockchain](https://arxiv.org/abs/2508.21480)
*Narges Dadkhah,Khan Reaz,Gerhard Wunder*

Main category: cs.CR

TL;DR: This paper introduces a blockchain-based decentralized onboarding framework for smart home IoT devices, addressing security and scalability challenges. It achieves rapid verification, strong authentication, and efficient real-time performance through consortium blockchain and smart contracts.


<details>
  <summary>Details</summary>
Motivation: Traditional IoT onboarding depends on centralized PKI models and manufacturer-controlled keys, posing security risks and limiting user digital sovereignty. These limitations hinder scalable IoT adoption in smart homes.

Method: A decentralized framework integrating consortium blockchain for secure onboarding, extending network-layer techniques to the application layer. Key features include smart contracts for device registration, key revocation, access control, and risk detection. Evaluated using Tamarin Prover and a prototype implementation.

Result: Prototype verification completed in 0.34 seconds, confirming scalability for constrained devices. Blockchain-based approach achieved high throughput and low latency, enabling near real-time IoT data processing while maintaining security under adversarial analysis.

Conclusion: The proposed blockchain-based onboarding framework effectively enhances security, transparency, and scalability for smart home IoT systems, demonstrating viability for real-world deployment through rapid verification and robust performance metrics.

Abstract: The increasing adoption of smart home devices and IoT-based security systems
presents significant opportunities to enhance convenience, safety, and risk
management for homeowners and service providers. However, secure
onboarding-provisioning credentials and establishing trust with cloud
platforms-remains a considerable challenge. Traditional onboarding methods
often rely on centralized Public Key Infrastructure (PKI) models and
manufacturer-controlled keys, which introduce security risks and limit the
user's digital sovereignty. These limitations hinder the widespread deployment
of scalable IoT solutions. This paper presents a novel onboarding framework
that builds upon existing network-layer onboarding techniques and extends them
to the application layer to address these challenges. By integrating consortium
blockchain technology, we propose a decentralized onboarding mechanism that
enhances transparency, security, and monitoring for smart home architectures.
The architecture supports device registration, key revocation, access control
management, and risk detection through event-driven alerts across dedicated
blockchain channels and smart contracts. To evaluate the framework, we formally
model the protocol using the Tamarin Prover under the Dolev-Yao adversary
model. The analysis focuses on authentication, token integrity, key
confidentiality, and resilience over public channels. A prototype
implementation demonstrates the system's viability in smart home settings, with
verification completing in 0.34 seconds, highlighting its scalability and
suitability for constrained devices and diverse stakeholders. Additionally,
performance evaluation shows that the blockchain-based approach effectively
handles varying workloads, maintains high throughput and low latency, and
supports near real-time IoT data processing.

</details>


### [12] [Generalized Encrypted Traffic Classification Using Inter-Flow Signals](https://arxiv.org/abs/2508.21558)
*Federica Bianchi,Edoardo Di Paolo,Angelo Spognardi*

Main category: cs.CR

TL;DR: The paper introduces a new encrypted traffic classification model that works directly on raw PCAP data, captures temporal correlations and packet volume distributions using inter-flow signals, and outperforms existing methods with up to 99% accuracy across multiple tasks and datasets.


<details>
  <summary>Details</summary>
Motivation: Existing encrypted traffic classification methods require prior assumptions about traffic types and lack generalizability across different tasks.

Method: The proposed model operates on raw PCAP data and uses inter-flow signals as an innovative representation to capture temporal correlations and packet volume distributions across flows.

Result: The model achieves up to 99% accuracy in various classification tasks and outperforms established methods across multiple datasets.

Conclusion: The novel encrypted traffic classification model demonstrates robustness and adaptability by achieving high accuracy without traffic-type assumptions and generalizing across tasks.

Abstract: In this paper, we present a novel encrypted traffic classification model that
operates directly on raw PCAP data without requiring prior assumptions about
traffic type. Unlike existing methods, it is generalizable across multiple
classification tasks and leverages inter-flow signals - an innovative
representation that captures temporal correlations and packet volume
distributions across flows. Experimental results show that our model
outperforms well-established methods in nearly every classification task and
across most datasets, achieving up to 99% accuracy in some cases, demonstrating
its robustness and adaptability.

</details>


### [13] [Agentic Discovery and Validation of Android App Vulnerabilities](https://arxiv.org/abs/2508.21579)
*Ziyue Wang,Liyi Zhou*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Existing Android vulnerability detection tools overwhelm teams with thousands
of low-signal warnings yet uncover few true positives. Analysts spend days
triaging these results, creating a bottleneck in the security pipeline.
Meanwhile, genuinely exploitable vulnerabilities often slip through, leaving
opportunities open to malicious counterparts.
  We introduce A2, a system that mirrors how security experts analyze and
validate Android vulnerabilities through two complementary phases: (i) Agentic
Vulnerability Discovery, which reasons about application security by combining
semantic understanding with traditional security tools; and (ii) Agentic
Vulnerability Validation, which systematically validates vulnerabilities across
Android's multi-modal attack surface-UI interactions, inter-component
communication, file system operations, and cryptographic computations.
  On the Ghera benchmark (n=60), A2 achieves 78.3% coverage, surpassing
state-of-the-art analyzers (e.g., APKHunt 30.0%). Rather than overwhelming
analysts with thousands of warnings, A2 distills results into 82 speculative
vulnerability findings, including 47 Ghera cases and 28 additional true
positives. Crucially, A2 then generates working Proof-of-Concepts (PoCs) for 51
of these speculative findings, transforming them into validated vulnerability
findings that provide direct, self-confirming evidence of exploitability.
  In real-world evaluation on 169 production APKs, A2 uncovers 104
true-positive zero-day vulnerabilities. Among these, 57 (54.8%) are
self-validated with automatically generated PoCs, including a medium-severity
vulnerability in a widely used application with over 10 million installs.

</details>


### [14] [Condense to Conduct and Conduct to Condense](https://arxiv.org/abs/2508.21602)
*Tomasz Kazana*

Main category: cs.CR

TL;DR: The paper provides the first examples of low-conductance permutations and links them to Multi-Source-Somewhere-Condensers' information-theoretic properties.


<details>
  <summary>Details</summary>
Motivation: The search for low-conductance permutations was initiated and motivated by Dodis et al. in their previous work to enhance cryptographic network security.

Method: The authors started by generating low-conductance permutations as per Dodis et al.'s requirements and then established a general equivalence between low-conductance permutations and the information-theoretical properties of Multi-Source-Somewhere-Condensers.

Result: The paper achieved the creation of the first low-conductance permutations and formalized a characterization showing their equivalence to Multi-Source-Somewhere-Condensers' properties.

Conclusion: This analysis connects low-conductance permutations to an important information-theoretical construct, suggesting implications for the design and analysis of cryptographic systems.

Abstract: In this paper we give the first examples of low-conductance permutations. The
notion of conductance of permutations was introduced in the paper
"Indifferentiability of Confusion-Diffusion Networks" by Dodis et al., where
the search for low-conductance permutations was initiated and motivated. In
this paper we not only give the desired examples, but also make a general
characterization of the problem -- i.e. we show that low-conductance
permutations are equivalent to permutations that have the information-theoretic
properties of the so-called Multi-Source-Somewhere-Condensers.

</details>


### [15] [Hybrid Cryptographic Monitoring System for Side-Channel Attack Detection on PYNQ SoCs](https://arxiv.org/abs/2508.21606)
*Nishant Chinnasami,Rasha Karakchi*

Main category: cs.CR

TL;DR: A machine learning-based dual-detection framework detects AES-128 anomalies in real-time on embedded systems with high accuracy and low computational overhead, outperforming traditional thresholding methods without requiring hardware changes.


<details>
  <summary>Details</summary>
Motivation: AES-128 faces practical security threats via timing and fault injection attacks in embedded systems. Existing protections either require hardware modifications or lack real-time feasibility, necessitating a lightweight, non-invasive solution.

Method: A lightweight framework combining statistical thresholding on execution time and a Random Forest classifier trained on block-level anomalies from simulated delays/ciphertext corruption. Implemented on CPU and FPGA (PYNQ-Z1) with real-time capabilities.

Result: The ML approach achieved higher accuracy than static thresholds while maintaining real-time performance on embedded platforms. The framework operates without hardware performance counters or AES modification, validated on PYNQ-Z1 FPGA.

Conclusion: The proposed dual-detection framework is suitable for low-power, resource-constrained systems, offering accurate and computationally efficient anomaly detection without modifying AES internals or hardware counters.

Abstract: AES-128 encryption is theoretically secure but vulnerable in practical
deployments due to timing and fault injection attacks on embedded systems. This
work presents a lightweight dual-detection framework combining statistical
thresholding and machine learning (ML) for real-time anomaly detection. By
simulating anomalies via delays and ciphertext corruption, we collect timing
and data features to evaluate two strategies: (1) a statistical threshold
method based on execution time and (2) a Random Forest classifier trained on
block-level anomalies. Implemented on CPU and FPGA (PYNQ-Z1), our results show
that the ML approach outperforms static thresholds in accuracy, while
maintaining real-time feasibility on embedded platforms. The framework operates
without modifying AES internals or relying on hardware performance counters.
This makes it especially suitable for low-power, resource-constrained systems
where detection accuracy and computational efficiency must be balanced.

</details>


### [16] [I Stolenly Swear That I Am Up to (No) Good: Design and Evaluation of Model Stealing Attacks](https://arxiv.org/abs/2508.21654)
*Daryna Oliynyk,Rudolf Mayer,Kathrin Grosse,Andreas Rauber*

Main category: cs.CR

TL;DR: This paper addresses evaluation standardization gaps in model stealing attacks by providing the first comprehensive framework and methodology for comparing image classification attacks while identifying key research directions.


<details>
  <summary>Details</summary>
Motivation: The lack of standardized design and evaluation of model stealing attacks hinders comparison of prior works and assessment of progress, prompting the need for a unified methodology.

Method: The authors study substitution model attacks against image classification models, propose a comprehensive threat model, develop a framework for attack comparison, analyze existing attack setups, and derive best practices for attack development.

Result: The paper introduces a framework for attack comparison, identifies most-studied tasks/models in the field, presents best practices for attack development stages, and highlights open research questions transferable to other domains.

Conclusion: The paper establishes the first generic evaluation methodology for model stealing attacks, providing standardized frameworks and best practices to improve comparison and progress in the field.

Abstract: Model stealing attacks endanger the confidentiality of machine learning
models offered as a service. Although these models are kept secret, a malicious
party can query a model to label data samples and train their own substitute
model, violating intellectual property. While novel attacks in the field are
continually being published, their design and evaluations are not standardised,
making it challenging to compare prior works and assess progress in the field.
This paper is the first to address this gap by providing recommendations for
designing and evaluating model stealing attacks. To this end, we study the
largest group of attacks that rely on training a substitute model -- those
attacking image classification models. We propose the first comprehensive
threat model and develop a framework for attack comparison. Further, we analyse
attack setups from related works to understand which tasks and models have been
studied the most. Based on our findings, we present best practices for attack
development before, during, and beyond experiments and derive an extensive list
of open research questions regarding the evaluation of model stealing attacks.
Our findings and recommendations also transfer to other problem domains, hence
establishing the first generic evaluation methodology for model stealing
attacks.

</details>


### [17] [Cybersecurity AI: Hacking the AI Hackers via Prompt Injection](https://arxiv.org/abs/2508.21669)
*Víctor Mayoral-Vilches,Per Mannermaa Rynning*

Main category: cs.CR

TL;DR: AI cybersecurity tools vulnerable to 'prompt injection' attacks via malicious web servers; paper demonstrates exploits against CAI framework and proposes layered defenses against this XSS-like threat.


<details>
  <summary>Details</summary>
Motivation: AI-powered vulnerability hunters face novel risks when interacting with hostile servers, as attackers can hijack their execution flow via adversarial prompts.

Method: Demonstrated proof-of-concept exploits against the Cybersecurity AI (CAI) framework and CLI tool, combined with multi-layered defense mechanisms.

Result: Successfully exploited CAI framework through crafted responses, validated systemic vulnerabilities, and proposed effective multi-layered mitigations.

Conclusion: Prompt injection is a systemic issue in LLM-based cybersecurity AI, requiring proactive defenses similar to how XSS was addressed in web applications.

Abstract: We demonstrate how AI-powered cybersecurity tools can be turned against
themselves through prompt injection attacks. Prompt injection is reminiscent of
cross-site scripting (XSS): malicious text is hidden within seemingly trusted
content, and when the system processes it, that text is transformed into
unintended instructions. When AI agents designed to find and exploit
vulnerabilities interact with malicious web servers, carefully crafted reponses
can hijack their execution flow, potentially granting attackers system access.
We present proof-of-concept exploits against the Cybersecurity AI (CAI)
framework and its CLI tool, and detail our mitigations against such attacks in
a multi-layered defense implementation. Our findings indicate that prompt
injection is a recurring and systemic issue in LLM-based architectures, one
that will require dedicated work to address, much as the security community has
had to do with XSS in traditional web applications.

</details>


### [18] [OptMark: Robust Multi-bit Diffusion Watermarking via Inference Time Optimization](https://arxiv.org/abs/2508.21727)
*Jiazheng Xing,Hai Ci,Hongbin Xu,Hangjie Yuan,Yong Liu,Mike Zheng Shou*

Main category: cs.CR

TL;DR: OptMark is an optimization-based watermarking method for diffusion-generated images that embeds a robust multi-bit watermark into the intermediate latents. It uses early structural and late detail watermark insertion with tailored regularization, and adjoint gradient methods to optimize memory usage, achieving imperceptible and resilient watermarking against various attacks.


<details>
  <summary>Details</summary>
Motivation: Current diffusion watermarking approaches either have limited capacity for user tracking (zero-bit) or lack robustness against transformations and generative attacks (multi-bit). There is a need for a comprehensive solution with high capacity, visibility, and resilience without excessive memory consumption.

Method: OptMark embeds watermarks into intermediate latents of the diffusion denoising process. It strategically inserts structural watermarks early (to resist generative attacks) and detail watermarks late (to withstand transformations) with regularization terms for image quality and imperceptibility. Adjoint gradient methods reduce memory usage from O(N) to O(1) during optimization.

Result: Experiments show OptMark achieves imperceptible multi-bit watermarking while demonstrating robust resilience against valuemetric transformations, geometric transformations, editing, and regeneration attacks. The adjoint method successfully addresses memory consumption challenges.

Conclusion: OptMark overcomes limitations of existing diffusion watermarking by combining optimization-based structural/detail watermark design with memory-efficient adjoint gradients, providing a robust and scalable solution for copyright protection and user tracking in diffusion-generated images.

Abstract: Watermarking diffusion-generated images is crucial for copyright protection
and user tracking. However, current diffusion watermarking methods face
significant limitations: zero-bit watermarking systems lack the capacity for
large-scale user tracking, while multi-bit methods are highly sensitive to
certain image transformations or generative attacks, resulting in a lack of
comprehensive robustness. In this paper, we propose OptMark, an
optimization-based approach that embeds a robust multi-bit watermark into the
intermediate latents of the diffusion denoising process. OptMark strategically
inserts a structural watermark early to resist generative attacks and a detail
watermark late to withstand image transformations, with tailored regularization
terms to preserve image quality and ensure imperceptibility. To address the
challenge of memory consumption growing linearly with the number of denoising
steps during optimization, OptMark incorporates adjoint gradient methods,
reducing memory usage from O(N) to O(1). Experimental results demonstrate that
OptMark achieves invisible multi-bit watermarking while ensuring robust
resilience against valuemetric transformations, geometric transformations,
editing, and regeneration attacks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [19] [Model-Driven Quantum Code Generation Using Large Language Models and Retrieval-Augmented Generation](https://arxiv.org/abs/2508.21097)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper introduces a novel research direction for model-to-text/code
transformations by leveraging Large Language Models (LLMs) that can be enhanced
with Retrieval-Augmented Generation (RAG) pipelines. The focus is on quantum
and hybrid quantum-classical software systems, where model-driven approaches
can help reduce the costs and mitigate the risks associated with the
heterogeneous platform landscape and lack of developers' skills. We validate
one of the proposed ideas regarding generating code out of UML model instances
of software systems. This Python code uses a well-established library, called
Qiskit, to execute on gate-based or circuit-based quantum computers. The RAG
pipeline that we deploy incorporates sample Qiskit code from public GitHub
repositories. Experimental results show that well-engineered prompts can
improve CodeBLEU scores by up to a factor of four, yielding more accurate and
consistent quantum code. However, the proposed research direction can go beyond
this through further investigation in the future by conducting experiments to
address our other research questions and ideas proposed here, such as deploying
software system model instances as the source of information in the RAG
pipelines, or deploying LLMs for code-to-code transformations, for instance,
for transpilation use cases.

</details>


### [20] [Learning to Generate Unit Test via Adversarial Reinforcement Learning](https://arxiv.org/abs/2508.21107)
*Dongjun Lee,Changho Hwang,Kimin Lee*

Main category: cs.SE

TL;DR: UTRL trains LLMs adversarially to generate high-quality unit tests outperforming state-of-the-art models via reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the gap in methods for training LLMs to generate high-quality unit tests, as existing approaches relying on human-written examples (supervised fine-tuning) and non-adversarial methods are insufficiently explored and effective.

Method: UTRL is a reinforcement learning framework that adversarially trains two LLMs: a unit test generator and a code generator. The test generator maximizes a discrimination reward by creating tests that expose faults in the code generator's solutions, while the code generator maximizes a code reward by producing solutions that pass these tests.

Result: Unit tests generated by Qwen3-4B using UTRL demonstrate higher quality than those from supervised fine-tuning and outperform GPT-4.1 in both test efficacy and alignment with ground-truth evaluations.

Conclusion: The study concludes that the proposed UTRL framework effectively trains LLMs to generate high-quality unit tests, outperforming both supervised fine-tuning and existing models like GPT-4.1 in test quality and code evaluation alignment.

Abstract: Unit testing is a core practice in programming, enabling systematic
evaluation of programs produced by human developers or large language models
(LLMs). Given the challenges in writing comprehensive unit tests, LLMs have
been employed to automate test generation, yet methods for training LLMs to
produce high-quality tests remain underexplored. In this work, we propose UTRL,
a novel reinforcement learning framework that trains an LLM to generate
high-quality unit tests given a programming instruction. Our key idea is to
iteratively train two LLMs, the unit test generator and the code generator, in
an adversarial manner via reinforcement learning. The unit test generator is
trained to maximize a discrimination reward, which reflects its ability to
produce tests that expose faults in the code generator's solutions, and the
code generator is trained to maximize a code reward, which reflects its ability
to produce solutions that pass the unit tests generated by the test generator.
In our experiments, we demonstrate that unit tests generated by Qwen3-4B
trained via UTRL show higher quality compared to unit tests generated by the
same model trained via supervised fine-tuning on human-written ground-truth
unit tests, yielding code evaluations that more closely align with those
induced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL
outperforms frontier models such as GPT-4.1 in generating high-quality unit
tests, highlighting the effectiveness of UTRL in training LLMs for this task.

</details>


### [21] [Automated Bug Triaging using Instruction-Tuned Large Language Models](https://arxiv.org/abs/2508.21156)
*Kiana Kiashemshaki,Arsham Khosravani,Alireza Hosseinpour,Arshia Akhavan*

Main category: cs.SE

TL;DR: The paper introduces a lightweight bug triaging framework using LLMs with LoRA and constrained decoding, achieving strong shortlist performance and showing promise for human-in-the-loop deployment.


<details>
  <summary>Details</summary>
Motivation: Bug triaging in large projects is often slow and inconsistent, necessitating efficient and reliable automated solutions.

Method: A lightweight framework utilizing instruction-tuned large language models (LLM) with LoRA adapters and candidate-constrained decoding for valid assignments.

Result: Achieved high shortlist quality (Hit at 10 up to 0.753) on EclipseJDT and Mozilla datasets, with notable improvement in accuracy on recent data snapshots.

Conclusion: The proposed framework demonstrates potential for real-world application in bug triaging, particularly with human involvement, and presents a practical alternative to existing costly methods.

Abstract: Bug triaging, the task of assigning new issues to developers, is often slow
and inconsistent in large projects. We present a lightweight framework that
instruction-tuned large language model (LLM) with LoRA adapters and uses
candidate-constrained decoding to ensure valid assignments. Tested on
EclipseJDT and Mozilla datasets, the model achieves strong shortlist quality
(Hit at 10 up to 0.753) despite modest exact Top-1 accuracy. On recent
snapshots, accuracy rises sharply, showing the framework's potential for
real-world, human-in-the-loop triaging. Our results suggest that
instruction-tuned LLMs offer a practical alternative to costly feature
engineering and graph-based methods.

</details>


### [22] [The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management](https://arxiv.org/abs/2508.21433)
*Tobias Lindenbauer,Igor Slinko,Ludwig Felder,Egor Bogomolov,Yaroslav Zharov*

Main category: cs.SE

TL;DR: Simple observation-masking is better than expensive LLM summarization for managing context in software engineering agents on SWE-bench Verified, cutting costs in half while maintaining similar performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of managing long, expensive context histories in LLM-based agents used for complex tasks. While existing approaches like OpenHands use LLM summarization, the authors question whether this complexity delivers meaningful performance advantages over simpler alternatives like observation omission.


Method: The researchers conducted a systematic comparison between LLM-based summarization and observation-masking strategies in the SWE-agent framework across five diverse model configurations. They evaluated cost and performance (solve rate) on SWE-bench Verified, a benchmark for software engineering agent evaluation.


Result: Observation masking reduced computation costs by 50% compared to raw agents while matching or exceeding the solve rates of summarization-based approaches. For Qwen3-Coder 480B specifically, masking improved the solve rate from 53.8% to 54.8% at a lower cost than summarization. This pattern held consistently across all five tested model configurations.


Conclusion: The study concludes that for context management in software engineering agents on SWE-bench Verified, simple observation-masking strategies outperform complex LLM-based summarization in terms of cost-effectiveness while maintaining or slightly improving solve rates.


Abstract: Large Language Model (LLM)-based agents solve complex tasks through iterative
reasoning, exploration, and tool-use, a process that can result in long,
expensive context histories. While state-of-the-art Software Engineering ( SE)
agents like OpenHands or Cursor use LLM-based summarization to tackle this
issue, it is unclear whether the increased complexity offers tangible
performance benefits compared to simply omitting older observations. We present
a systematic comparison of these strategies within SWE-agent on SWE-bench
Verified across five diverse model configurations. We find that a simple
observation-masking strategy halves cost relative to a raw agent while
matching, and sometimes slightly exceeding, the solve rate of LLM
summarization. For example, with Qwen3-Coder 480B, masking improves solve rate
from 53.8% (raw agent) to 54.8%, while remaining competitive with summarization
at a lower cost. These results suggest that, at least within SWE-agent on
SWE-bench Verified, the most effective and efficient context management can be
the simplest. We release code and data for reproducibility

</details>


### [23] [Enhancing Semantic Understanding in Pointer Analysis using Large Language Models](https://arxiv.org/abs/2508.21454)
*Baijun Cheng,Kailong Wang,Ling Shi,Haoyu Wang,Yao Guo,Ding Li,Xiangqun Chen*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Pointer analysis has been studied for over four decades. However, existing
frameworks continue to suffer from the propagation of incorrect facts. A major
limitation stems from their insufficient semantic understanding of code,
resulting in overly conservative treatment of user-defined functions. Recent
advances in large language models (LLMs) present new opportunities to bridge
this gap. In this paper, we propose LMPA (LLM-enhanced Pointer Analysis), a
vision that integrates LLMs into pointer analysis to enhance both precision and
scalability. LMPA identifies user-defined functions that resemble system APIs
and models them accordingly, thereby mitigating erroneous cross-calling-context
propagation. Furthermore, it enhances summary-based analysis by inferring
initial points-to sets and introducing a novel summary strategy augmented with
natural language. Finally, we discuss the key challenges involved in realizing
this vision.

</details>


### [24] [Reusable Test Suites for Reinforcement Learning](https://arxiv.org/abs/2508.21553)
*Jørn Eirik Betten,Quentin Mazouni,Dennis Gross,Pedro Lind,Helge Spieker*

Main category: cs.SE

TL;DR: This paper introduces Multi-Policy Test Case Selection (MPTCS), a policy-agnostic RL testing framework that automatically selects diverse, reusable test cases by combining multiple policies' perspectives on difficulty and coverage, enhancing deployment reliability.


<details>
  <summary>Details</summary>
Motivation: Current RL policy testing methods generate policy-specific test suites, limiting their reuse for other agents. This creates inefficiencies in validating agent reliability for deployment. The paper addresses this gap by proposing a method that generates reusable, policy-agnostic test cases to systematically identify common flaws across diverse RL agents.

Method: MPTCS employs a policy-based selection process using three criteria: solvability (test case difficulty), diversity (coverage of state space), and generalizability. It calculates a difficulty score for test cases across policies and introduces a discretized descriptor surface inspired by quality-diversity algorithms to systematically diversify the test suite. Candidate test cases are filtered from any policy testing framework's output.

Result: Experiments demonstrate that increasing policy diversity in selection improves the method's effectiveness and cost-efficiency. The descriptor surface successfully covers state space regions and identifies policies that expose faulty behaviors. The difficulty score metric reliably ranks test cases for cross-policy relevance.

Conclusion: MPTCS provides a robust framework for selecting reusable RL test cases across diverse policies. By leveraging multiple policies and diversity-promotion techniques, the method enhances testing effectiveness for detecting agent flaws while ensuring cost-efficiency through policy-agnostic test suite design.

Abstract: Reinforcement learning (RL) agents show great promise in solving sequential
decision-making tasks. However, validating the reliability and performance of
the agent policies' behavior for deployment remains challenging. Most
reinforcement learning policy testing methods produce test suites tailored to
the agent policy being tested, and their relevance to other policies is
unclear. This work presents Multi-Policy Test Case Selection (MPTCS), a novel
automated test suite selection method for RL environments, designed to extract
test cases generated by any policy testing framework based on their
solvability, diversity, and general difficulty. MPTCS uses a set of policies to
select a diverse collection of reusable policy-agnostic test cases that reveal
typical flaws in the agents' behavior. The set of policies selects test cases
from a candidate pool, which can be generated by any policy testing method,
based on a difficulty score. We assess the effectiveness of the difficulty
score and how the method's effectiveness and cost depend on the number of
policies in the set. Additionally, a method for promoting diversity in the test
suite, a discretized general test case descriptor surface inspired by
quality-diversity algorithms, is examined to determine how it covers the state
space and which policies it triggers to produce faulty behaviors.

</details>


### [25] [Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects, Vulnerabilities, and Complexity](https://arxiv.org/abs/2508.21634)
*Domenico Cotroneo,Cristina Improta,Pietro Liguori*

Main category: cs.SE

TL;DR: Large-scale analysis shows AI-generated code is simpler but more vulnerable; human code is complex yet less maintainable, urging specialized QA for AI-assisted programming.


<details>
  <summary>Details</summary>
Motivation: Understanding differences in software quality (reliability, maintainability, security) between AI code assistants and human developers is critical as AI integrates into software workflows.

Method: We evaluated over 500k code samples in Python and Java, classifying defects via Orthogonal Defect Classification and security vulnerabilities using Common Weakness Enumeration across three LLMs (ChatGPT, DeepSeek-Coder, Qwen-Coder) and human-authored code.

Result: AI-generated code shows higher simplicity, repetition, unused constructs, hardcoded debugging, and high-risk security vulnerabilities. Human code exhibits greater structural complexity but more maintainability issues.

Conclusion: The study reveals distinct defect profiles between AI-generated and human-written code, emphasizing the necessity for tailored quality assurance practices in AI-assisted programming to address differences in simplicity, security risks, and maintainability.

Abstract: As AI code assistants become increasingly integrated into software
development workflows, understanding how their code compares to human-written
programs is critical for ensuring reliability, maintainability, and security.
In this paper, we present a large-scale comparison of code authored by human
developers and three state-of-the-art LLMs, i.e., ChatGPT, DeepSeek-Coder, and
Qwen-Coder, on multiple dimensions of software quality: code defects, security
vulnerabilities, and structural complexity. Our evaluation spans over 500k code
samples in two widely used languages, Python and Java, classifying defects via
Orthogonal Defect Classification and security vulnerabilities using the Common
Weakness Enumeration. We find that AI-generated code is generally simpler and
more repetitive, yet more prone to unused constructs and hardcoded debugging,
while human-written code exhibits greater structural complexity and a higher
concentration of maintainability issues. Notably, AI-generated code also
contains more high-risk security vulnerabilities. These findings highlight the
distinct defect profiles of AI- and human-authored code and underscore the need
for specialized quality assurance practices in AI-assisted programming.

</details>


### [26] [The Integration of Agile Methodologies in DevOps Practices within the Information Technology Industry](https://arxiv.org/abs/2508.21811)
*Ashley Hourigan,Ridewaan Hanslo*

Main category: cs.SE

TL;DR: This study explores Agile-DevOps integration in IT, using interviews to identify how Agile's flexibility enhances DevOps practices, achieving faster, resilient software delivery.


<details>
  <summary>Details</summary>
Motivation: Driven by the IT industry's shift toward rapid software delivery, this study addresses the need to critically assess the applicability of Agile methodologies within DevOps practices to balance flexibility and operational resilience.

Method: The research employed eleven semi-structured interviews with Agile and DevOps practitioners, utilizing thematic analysis to extract 51 codes and synthesize 19 themes across the DevOps lifecycle.

Result: The analysis revealed 19 synthesized themes detailing the integration of Agile into DevOps, leading to a novel framework that clarifies their interrelationship and supports their combined use in software development.

Conclusion: The study concludes that Agile methods are feasible and beneficial when integrated into DevOps practices, enhancing collaborative and iterative software delivery processes.

Abstract: The demand for rapid software delivery in the Information Technology (IT)
industry has significantly intensified, emphasising the need for faster
software products and service releases with enhanced features to meet customer
expectations. Agile methodologies are replacing traditional approaches such as
Waterfall, where flexibility, iterative development and adaptation to change
are favoured over rigid planning and execution. DevOps, a subsequent evolution
from Agile, emphasises collaborative efforts in development and operations
teams, focusing on continuous integration and deployment to deliver resilient
and high-quality software products and services. This study aims to critically
assess both Agile and DevOps practices in the IT industry to identify the
feasibility and applicability of Agile methods in DevOps practices. Eleven
semi-structured interviews were conducted with Agile and DevOps practitioners
in varying capacities across several sectors within the IT industry. Through
thematic analysis, 51 unique codes were extracted and synthesised into 19
themes that reported on each phase of the DevOps lifecycle, specifically
regarding the integration and implementation of Agile methods into DevOps
practices. Based on the findings, a new understanding detailing the
interrelationship of Agile methods in DevOps practices was discussed that met
the research objectives.

</details>
