<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 33]
- [cs.SE](#cs.SE) [Total: 41]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Immutability Does Not Guarantee Trust: A Formal and Logical Refutation](https://arxiv.org/abs/2507.08844)
*Craig S Wright*

Main category: cs.CR

TL;DR: Blockchain immutability does not guarantee trust because it does not ensure correctness, fairness, or credibility—illustrated via formal analysis and counterexamples like predictive fraud and garbage permanence.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the widely held but logically flawed belief that blockchain immutability inherently establishes trust, which could lead to misapplication in real-world systems if unchallenged.

Method: Combines predicate logic, automata-theoretic models, and epistemic game-theoretic analysis to formalize and refute the relationship between structural immutability and epistemic trust.

Result: Formal proofs and counterexamples demonstrate that data persistence (immutability) coexists with persistent inaccuracies (garbage permanence) and malicious structures (predictive fraud), breaking any logical connection between the two domains.

Conclusion: Trust requires rational justification under uncertainty, while immutability is a structural property. Both concepts and their limitations must be contextualized within formal models to prevent misapplication of blockchain technology.

Abstract: It is frequently claimed in blockchain discourse that immutability guarantees
trust. This paper rigorously refutes that assertion. We define immutability as
the cryptographic persistence of historical states in an append-only data
structure and contrast it with trust, understood as a rational epistemic
expectation under uncertainty. Employing predicate logic, automata-theoretic
models, and epistemic game-theoretic analysis, we demonstrate that immutability
neither entails nor implies correctness, fairness, or credibility. Through
formal constructions and counterexamples--including predictive fraud schemes
and the phenomenon of garbage permanence--we show that the belief conflates
structural and epistemic domains. Immutability preserves all data equally,
regardless of veracity. Therefore, the assertion that immutability guarantees
trust collapses under the weight of formal scrutiny.

</details>


### [2] [Clio-X: AWeb3 Solution for Privacy-Preserving AI Access to Digital Archives](https://arxiv.org/abs/2507.08853)
*Victoria L. Lemieux,Rosa Gil,Faith Molosiwa,Qihong Zhou,Binming Li,Roberto Garcia,Luis De La Torre Cubillo,Zehua Wang*

Main category: cs.CR

TL;DR: Clio-X is a decentralized Web3 solution integrating privacy-enhancing technologies to enable ethical AI use in archives, addressing control and access challenges through participatory design and decentralized governance.


<details>
  <summary>Details</summary>
Motivation: Archives face growing privacy risks with AI adoption, necessitating solutions to balance data sovereignty, ethical accountability, and researcher access to sensitive digital records.

Method: Developed Clio-X, a privacy-first Web3 platform embedding PETs into archival workflows; conducted user evaluations with a medium-fidelity prototype and applied Rogers' Diffusion of Innovation theory to analyze adoption barriers.

Result: User evaluations revealed both interest in Clio-X's potential and barriers including trust issues, system opacity, economic concerns, and governance challenges.

Conclusion: Clio-X's model combines technical PETs with decentralized autonomous organization governance to create an ethical framework for AI in cultural heritage, leveraging participatory design to address sociotechnical adoption barriers.

Abstract: As archives turn to artificial intelligence to manage growing volumes of
digital records, privacy risks inherent in current AI data practices raise
critical concerns about data sovereignty and ethical accountability. This paper
explores how privacy-enhancing technologies (PETs) and Web3 architectures can
support archives to preserve control over sensitive content while still being
able to make it available for access by researchers. We present Clio-X, a
decentralized, privacy-first Web3 digital solution designed to embed PETs into
archival workflows and support AI-enabled reference and access. Drawing on a
user evaluation of a medium-fidelity prototype, the study reveals both interest
in the potential of the solution and significant barriers to adoption related
to trust, system opacity, economic concerns, and governance. Using Rogers'
Diffusion of Innovation theory, we analyze the sociotechnical dimensions of
these barriers and propose a path forward centered on participatory design and
decentralized governance through a Clio-X Decentralized Autonomous
Organization. By integrating technical safeguards with community-based
oversight, Clio-X offers a novel model to ethically deploy AI in cultural
heritage contexts.

</details>


### [3] [RAG Safety: Exploring Knowledge Poisoning Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2507.08862)
*Tianzhe Zhao,Jiaoyan Chen,Yanchi Ru,Haiping Zhu,Nan Hu,Jun Liu,Qika Lin*

Main category: cs.CR

TL;DR: This paper is the first systematic investigation of security risks in Knowledge Graph-based Retrieval-Augmented Generation (KG-RAG) systems, revealing that they are vulnerable to data poisoning attacks through insertion of adversarial triples into knowledge graphs. The attack strategy demonstrates strong effectiveness in degrading KG-RAG performance even with minimal perturbations.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the observation that while RAG systems using unstructured textual data sources have been studied for data poisoning vulnerabilities (e.g., hallucinations, outdated knowledge), the unique security risks of KG-RAG systems—leverage structured, editable knowledge graphs—remain largely unexplored despite their potential for targeted adversarial manipulation.

Method: The paper proposes an attack framework with two stages: 1) Identifying adversarial target answers with high similarity to ground truth answers (e.g., 'Barcelona is the capital of France'), and 2) Strategically inserting perturbation triples (e.g., 'Barcelona -capital of-> France') into the knowledge graph to complete coherent but misleading inference chains that maximize their usage in downstream retrieval.

Result: Experiments on two benchmarks (e.g., RealKG, FALcon) and four recent KG-RAG methodologies (e.g., Coarse2Fine, Graph RAG) show the attack degrades performance by up to 33.6% in accuracy without requiring excessive knowledge graph modifications. Ablation studies and internal mechanism analyses (e.g., query-knowledge alignment, triple credibility) further expose vulnerabilities in specific components like retrieval scoring and reasoning chains.

Conclusion: The work establishes data poisoning as a critical security threat for KG-RAG systems and provides empirical evidence of their vulnerability. It suggests future research should focus on robustness for structured knowledge sources and develop detection/defense mechanisms for adversarial triple insertions in knowledge graphs.

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
retrieving external data to mitigate hallucinations and outdated knowledge
issues. Benefiting from the strong ability in facilitating diverse data sources
and supporting faithful reasoning, knowledge graphs (KGs) have been
increasingly adopted in RAG systems, giving rise to KG-based RAG (KG-RAG)
methods. Though RAG systems are widely applied in various applications, recent
studies have also revealed its vulnerabilities to data poisoning attacks, where
malicious information injected into external knowledge sources can mislead the
system into producing incorrect or harmful responses. However, these studies
focus exclusively on RAG systems using unstructured textual data sources,
leaving the security risks of KG-RAG largely unexplored, despite the fact that
KGs present unique vulnerabilities due to their structured and editable nature.
In this work, we conduct the first systematic investigation of the security
issue of KG-RAG methods through data poisoning attacks. To this end, we
introduce a practical, stealthy attack setting that aligns with real-world
implementation. We propose an attack strategy that first identifies adversarial
target answers and then inserts perturbation triples to complete misleading
inference chains in the KG, increasing the likelihood that KG-RAG methods
retrieve and rely on these perturbations during generation. Through extensive
experiments on two benchmarks and four recent KG-RAG methods, our attack
strategy demonstrates strong effectiveness in degrading KG-RAG performance,
even with minimal KG perturbations. In-depth analyses are also conducted to
understand the safety threats within the internal stages of KG-RAG systems and
to explore the robustness of LLMs against adversarial knowledge.

</details>


### [4] [Privacy-Utility-Fairness: A Balanced Approach to Vehicular-Traffic Management System](https://arxiv.org/abs/2507.08864)
*Poushali Sengupta,Sabita Maharjan,frank Eliassen,Yan Zhang*

Main category: cs.CR

TL;DR: This paper introduces a differential privacy algorithm for location-based vehicular traffic management that balances privacy protection, data utility for traffic analysis, and fairness across regions using Norway's data as a case study.


<details>
  <summary>Details</summary>
Motivation: Existing traffic management systems face privacy risks from linkage attacks and demographic biases, causing data leaks and inequitable regional representation in analysis results.

Method: The approach combines query-based data access with iterative shuffling and calibrated Laplace noise injection to meet epsilon-differential privacy standards while maintaining meaningful traffic data patterns.

Result: Implementation on Norwegian vehicular data demonstrated effective privacy preservation (no over/underrepresentation), reliable traffic analysis utility, and equitable spatial representation through geographically balanced heatmaps.

Conclusion: The proposed algorithm provides a robust framework for secure, fair vehicular traffic data analysis, addressing existing limitations in privacy-utility-fairness tradeoffs through structured differential privacy application.

Abstract: Location-based vehicular traffic management faces significant challenges in
protecting sensitive geographical data while maintaining utility for traffic
management and fairness across regions. Existing state-of-the-art solutions
often fail to meet the required level of protection against linkage attacks and
demographic biases, leading to privacy leakage and inequity in data analysis.
In this paper, we propose a novel algorithm designed to address the challenges
regarding the balance of privacy, utility, and fairness in location-based
vehicular traffic management systems. In this context, utility means providing
reliable and meaningful traffic information, while fairness ensures that all
regions and individuals are treated equitably in data use and decision-making.
Employing differential privacy techniques, we enhance data security by
integrating query-based data access with iterative shuffling and calibrated
noise injection, ensuring that sensitive geographical data remains protected.
We ensure adherence to epsilon-differential privacy standards by implementing
the Laplace mechanism. We implemented our algorithm on vehicular location-based
data from Norway, demonstrating its ability to maintain data utility for
traffic management and urban planning while ensuring fair representation of all
geographical areas without being overrepresented or underrepresented.
Additionally, we have created a heatmap of Norway based on our model,
illustrating the privatized and fair representation of the traffic conditions
across various cities. Our algorithm provides privacy in vehicular traffic

</details>


### [5] [Towards Privacy-Preserving and Personalized Smart Homes via Tailored Small Language Models](https://arxiv.org/abs/2507.08878)
*Xinyu Huang,Leming Shen,Zijing Ma,Yuanqing Zheng*

Main category: cs.CR

TL;DR: HomeLLaMA is an on-device smart home assistant that enhances privacy using a tailored small language model (SLM), while optionally connecting to cloud services for non-sensitive queries with PrivShield.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based smart home assistants transmit user data to remote servers, risking privacy leaks. Users increasingly demand solutions that maintain personalization without compromising data confidentiality.

Method: HomeLLaMA local SLMs are pretrained on cloud LLMs and continuously updated via local interactions and user profiles. PrivShield enables remote LLM service usage for non-sensitive queries while anonymizing inputs when necessary.

Result: Evaluation via DevFinder benchmark and 100-participant user studies confirms HomeLLaMA achieves equivalent service quality compared to cloud LLMs while retaining 97% of user data locally. PrivShield reduces 82% of query personal information without affecting accuracy.

Conclusion: HomeLLaMA establishes a practical framework for privacy-preserving smart home autonomy, demonstrating that strong personalization and privacy protection can coexist with optional cloud integration

Abstract: Large Language Models (LLMs) have showcased remarkable generalizability in
language comprehension and hold significant potential to revolutionize
human-computer interaction in smart homes. Existing LLM-based smart home
assistants typically transmit user commands, along with user profiles and home
configurations, to remote servers to obtain personalized services. However,
users are increasingly concerned about the potential privacy leaks to the
remote servers. To address this issue, we develop HomeLLaMA, an on-device
assistant for privacy-preserving and personalized smart home serving with a
tailored small language model (SLM). HomeLLaMA learns from cloud LLMs to
deliver satisfactory responses and enable user-friendly interactions. Once
deployed, HomeLLaMA facilitates proactive interactions by continuously updating
local SLMs and user profiles. To further enhance user experience while
protecting their privacy, we develop PrivShield to offer an optional
privacy-preserving LLM-based smart home serving for those users, who are
unsatisfied with local responses and willing to send less-sensitive queries to
remote servers. For evaluation, we build a comprehensive benchmark DevFinder to
assess the service quality. Extensive experiments and user studies (M=100)
demonstrate that HomeLLaMA can provide personalized services while
significantly enhancing user privacy.

</details>


### [6] [CovertAuth: Joint Covert Communication and Authentication in MmWave Systems](https://arxiv.org/abs/2507.08904)
*Yulin Teng,Keshuang Han,Pinchang Zhang,Xiaohong Jiang,Yulong Shen,Fu Xiao*

Main category: cs.CR

TL;DR: This paper proposes CovertAuth, a secure framework enhancing beam alignment security in mmWave communications by addressing eavesdropping and impersonation attacks through covert communication optimization and physical layer authentication using antenna impairments.


<details>
  <summary>Details</summary>
Motivation: Beam alignment in mmWave communications is vulnerable to eavesdropping and identity impersonation attacks due to its omnidirectional exposure and broadcast nature, necessitating improved security measures.

Method: 1. Derives closed-form expressions for BA probability and covert rate. 2. Formulates an optimization problem for maximizing covert communication rate by jointly optimizing beam training budget and transmission power using alternating optimization with successive convex approximation. 3. Leverages mutual coupling effects in antenna arrays as device features for a weighted-sum energy detector authentication scheme and develops theoretical models for authentication performance metrics.

Result: Simulation results validate CovertAuth's effectiveness in achieving higher authentication accuracy under the same covertness requirements compared to existing methods, demonstrating security improvements against eavesdropping and impersonation attacks.

Conclusion: CovertAuth significantly enhances BA phase security in mmWave systems by combining covertness-aware optimization with device-specific authentication, validated through theoretical analysis and simulations, representing an effective defense against common security threats.

Abstract: Beam alignment (BA) is a crucial process in millimeter-wave (mmWave)
communications, enabling precise directional transmission and efficient link
establishment. However, due to characteristics like omnidirectional exposure
and the broadcast nature of the BA phase, it is particularly vulnerable to
eavesdropping and identity impersonation attacks. To this end, this paper
proposes a novel secure framework named CovertAuth, designed to enhance the
security of the BA phase against such attacks. In particular, to combat
eavesdropping attacks, the closed-form expressions of successful BA probability
and covert transmission rate are first derived. Then, a covert communication
problem aimed at jointly optimizing beam training budget and transmission power
is formulated to maximize covert communication rate, subject to the covertness
requirement. An alternating optimization algorithm combined with successive
convex approximation is employed to iteratively achieve optimal results. To
combat impersonation attacks, the mutual coupling effect of antenna array
impairments is explored as a device feature to design a weighted-sum energy
detector based physical layer authentication scheme. Moreover, theoretical
models for authentication metrics like detection and false alarm probabilities
are also provided to conduct performance analysis. Based on these models, an
optimization problem is constructed to determine the optimal weight value that
maximizes authentication accuracy. Finally, simulation results demonstrate that
CovertAuth presents improved detection accuracy under the same covertness
requirement compared to existing works.

</details>


### [7] [Characterizing Security and Privacy Teaching Standards for Schools in the United States](https://arxiv.org/abs/2507.08978)
*Katherine Limes,Nathan Malkin,Kelsey R. Fulton*

Main category: cs.CR

TL;DR: The paper analyzes K-12 cybersecurity/privacy teaching standards across 50 U.S. states and 8 national organizations, revealing a need for stronger emphasis on threat modeling and security mindset in curricula.


<details>
  <summary>Details</summary>
Motivation: This research addresses the gap in understanding how current K-12 computer science standards incorporate security and privacy topics, and whether they align with professional expectations.

Method: The authors collected and manually reviewed 11,954 standards, labeled 3,778 as security/privacy related (103 topics), and conducted interviews with 11 security/privacy professionals to assess alignment.

Result: Existing standards cover both technical and social aspects of security/privacy, but professionals emphasize threat modeling and security mindset more than formal standards currently reflect.

Conclusion: While current teaching standards provide adequate topic coverage, curricula should prioritize threat modeling and security mindset development to better align with professional expectations.

Abstract: Increasingly, students begin learning aspects of security and privacy during
their primary and secondary education (grades K-12 in the United States).
Individual U.S. states and some national organizations publish teaching
standards -- guidance that outlines expectations for what students should learn
-- which often form the basis for course curricula. However, research has not
yet examined what is covered by these standards and whether the topics align
with what the broader security and privacy community thinks students should
know. To shed light on these questions, we started by collecting computer
science teaching standards from all U.S. states and eight national
organizations. After manually examining a total of 11,954 standards, we labeled
3,778 of them as being related to security and privacy, further classifying
these into 103 topics. Topics ranged from technical subjects like encryption,
network security, and embedded systems to social subjects such as laws, ethics,
and appropriate online behavior. Subsequently, we interviewed 11 security and
privacy professionals to examine how the teaching standards align with their
expectations. We found that, while the specific topics they mentioned mostly
overlapped with those of existing standards, professionals placed a greater
emphasis on threat modeling and security mindset.

</details>


### [8] [SSH-Passkeys: Leveraging Web Authentication for Passwordless SSH](https://arxiv.org/abs/2507.09022)
*Moe Kayali,Jonas Schmitt,Franziska Roesner*

Main category: cs.CR

TL;DR: This paper proposes using WebAuthn APIs with SSH authentication via UNIX PAM to enable secure, passwordless remote login using user-managed passkeys, addressing vulnerabilities in traditional SSH keys and passwords.


<details>
  <summary>Details</summary>
Motivation: SSH key-based authentication is vulnerable to key theft and suffers from poor usability leading to key material leaks and unverified fingerprints. Passwords are insecure due to phishing and reuse, highlighting the need for better key management in SSH security.

Method: A framework to integrate WebAuthn with SSH servers using UNIX PAM modules, enabling passwordless authentication with passkeys while maintaining backward compatibility and stock SSH server support.

Result: A prototype and user study with 40 SSH users showed a 90% reduction in critical security errors, 4x faster authentication, and identified that 20% of traditional SSH users leaked private keys.

Conclusion: SSH-passkeys offer superior security (protection against phishing, key leaks, at rest attacks) and usability over passwords and traditional SSH keys, validated by security analysis and user study results.

Abstract: We propose a method for using Web Authentication APIs for SSH authentication,
enabling passwordless remote server login with passkeys. These are credentials
that are managed throughout the key lifecycle by an authenticator on behalf of
the user and offer strong security guarantees.
  Passwords remain the dominant mode of SSH authentication, despite their well
known flaws such as phishing and reuse. SSH's custom key-based authentication
protocol can alleviate these issues but remains vulnerable to key theft.
Additionally, it has poor usability, with even knowledgeable users leaking key
material and failing to verify fingerprints. Hence, effective key management
remains a critical open area in SSH security. In contrast, WebAuthn is a modern
authentication standard designed to replace passwords, managing keys on behalf
of the user. As a web API, this standard cannot integrate with SSH directly.
  We propose a framework to integrate WebAuthn with SSH servers, by using UNIX
pluggable authentication modules (PAM). Our approach is backwards-compatible,
supports stock SSH servers and requires no new software client-side. It offers
protection for cryptographic material at rest, resistance to key leaks,
phishing protection, privacy protection and attestation capability. None of
these properties are offered by passwords nor traditional SSH keys. We validate
these advantages with a structured, conceptual security analysis.
  We develop a prototype implementation and conduct a user study to quantify
the security advantages of our proposal, testing our prototype with 40 SSH
users. The study confirms the security problems of SSH-keys, including 20% of
the cohort leaking their private keys. Our SSH-passkeys effectively address
these problems: we find a 90% reduction in critical security errors, while
reducing authentication time by 4x on average.

</details>


### [9] [Favicon Trojans: Executable Steganography Via Ico Alpha Channel Exploitation](https://arxiv.org/abs/2507.09074)
*David Noever,Forrest McKee*

Main category: cs.CR

TL;DR: The paper proposes a self-decompressing JavaScript steganography method using ICO file alpha layers to create a stealthy browser-based attack without affecting visual fidelity.


<details>
  <summary>Details</summary>
Motivation: Modern browsers process 294B daily favicons consuming 0.9PB bandwidth, making ICO steganography a novel attack surface to bypass traditional image/security boundaries through standard browser behavior.

Method: Embed JavaScript in LSB of non-transparent alpha pixels (64x64 ICO holds 512B/0.8KB compressed) and execute it silently via canvas APIs during page load, creating a two-stage in-memory covert channel.

Result: Proof-of-concept successfully exploited browsers across platforms, evaded antivirus/CSP detection, mapped to 9 MITRE ATT&CK objectives, and demonstrated lightweight two-fold compression feasibility.

Conclusion: Identifies ICO transparency as a stealthy executable attack vector that exploits required browser behaviors, necessitating enhanced steganalysis and defense strategies against polymorphic phishing.

Abstract: This paper presents a novel method of executable steganography using the
alpha transparency layer of ICO image files to embed and deliver
self-decompressing JavaScript payloads within web browsers. By targeting the
least significant bit (LSB) of non-transparent alpha layer image values, the
proposed method successfully conceals compressed JavaScript code inside a
favicon image without affecting visual fidelity. Global web traffic loads 294
billion favicons daily and consume 0.9 petabytes of network bandwidth. A
proof-of-concept implementation demonstrates that a 64x64 ICO image can embed
up to 512 bytes uncompressed, or 0.8 kilobyte when using lightweight two-fold
compression. On page load, a browser fetches the favicon as part of standard
behavior, allowing an embedded loader script to extract and execute the payload
entirely in memory using native JavaScript APIs and canvas pixel access. This
creates a two-stage covert channel requiring no additional network or user
requests. Testing across multiple browsers in both desktop and mobile
environments confirms successful and silent execution of the embedded script.
We evaluate the threat model, relate it to polymorphic phishing attacks that
evade favicon-based detection, and analyze evasion of content security policies
and antivirus scanners. We map nine example MITRE ATT&CK Framework objectives
to single line JavaScript to execute arbitrarily in ICO files. Existing
steganalysis and sanitization defenses are discussed, highlighting limitations
in detecting or neutralizing alpha-channel exploits. The results demonstrate a
stealthy and reusable attack surface that blurs traditional boundaries between
static images and executable content. Because modern browsers report silent
errors when developers specifically fail to load ICO files, this attack surface
offers an interesting example of required web behaviors that in turn compromise
security.

</details>


### [10] [CLIProv: A Contrastive Log-to-Intelligence Multimodal Approach for Threat Detection and Provenance Analysis](https://arxiv.org/abs/2507.09133)
*Jingwen Li,Ru Zhang,Jianyi Liu,Wanguo Zhao*

Main category: cs.CR

TL;DR: CLIProv is a novel threat detection approach that bridges the semantic gap between high-level threat intelligence (TTPs) and low-level provenance logs using contrastive learning and semantic search, improving precision and detection efficiency.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of cyberattacks demands proactive threat detection, yet existing methods struggle to translate high-level Tactics, Techniques, Procedures (TTP) threat intelligence into actionable system-level security analysis due to a significant semantic gap.

Method: CLIProv employs a multimodal framework leveraging contrastive learning to semantically align provenance logs with threat intelligence, reformulating threat detection as a semantic search problem where attack behaviors are identified by matching log sequences to the most semantically similar threat intelligence.

Result: Experimental evaluations on standard datasets demonstrate CLIProv's effectiveness in detecting attack behaviors in provenance logs. It achieves higher precision and significantly improved detection efficiency compared to state-of-the-art methods while generating complete and concise attack scenarios.

Conclusion: CLIProv addresses the critical challenge of mapping abstract threat intelligence to concrete system activity patterns, offering a high-precision, efficient solution for identifying cyberattack behaviors and constructing TTP-aligned attack scenarios.

Abstract: With the increasing complexity of cyberattacks, the proactive and
forward-looking nature of threat intelligence has become more crucial for
threat detection and provenance analysis. However, translating high-level
attack patterns described in Tactics, Techniques, and Procedures (TTP)
intelligence into actionable security policies remains a significant challenge.
This challenge arises from the semantic gap between high-level threat
intelligence and low-level provenance log. To address this issue, this paper
introduces CLIProv, a novel approach for detecting threat behaviors in a host
system. CLIProv employs a multimodal framework that leverages contrastive
learning to align the semantics of provenance logs with threat intelligence,
effectively correlating system intrusion activities with attack patterns.
Furthermore, CLIProv formulates threat detection as a semantic search problem,
identifying attack behaviors by searching for threat intelligence that is most
semantically similar to the log sequence. By leveraging attack pattern
information in threat intelligence, CLIProv identifies TTPs and generates
complete and concise attack scenarios. Experimental evaluations on standard
datasets show that CLIProv effectively identifies attack behaviors in system
provenance logs, offering valuable references for potential techniques.
Compared to state-of-the-art methods, CLIProv achieves higher precision and
significantly improved detection efficiency.

</details>


### [11] [Confidential Wrapped Ethereum](https://arxiv.org/abs/2507.09231)
*Artem Chystiakov,Mariia Zhvanko*

Main category: cs.CR

TL;DR: The paper proposes a method to enhance user privacy in public blockchains by creating a confidential wrapped Ethereum (cWETH) solution entirely within the application layer using elliptic curve cryptography and zk-SNARKs.


<details>
  <summary>Details</summary>
Motivation: Public blockchain transparency exposes transaction details, compromising user privacy. This work addresses the need to balance openness with confidentiality while maintaining accessibility within defined constraints.

Method: The solution integrates EC ElGamal commitments for data encryption and EC DH for controlled access. zk-SNARKs are employed as zero-knowledge proofs to verify correct cryptographic operations (commitments, encryption/decryption) without revealing sensitive data.

Result: A proof-of-concept implementation of the cWETH protocol framework is established, demonstrating privacy-preserving transaction handling in public blockchains at the application layer.

Conclusion: The paper presents a viable application-layer approach to confidential transactions using cryptographic techniques, offering a potential solution to reconcile blockchain transparency with the confidentiality requirements of privacy-conscious users.

Abstract: Transparency is one of the key benefits of public blockchains. However, the
public visibility of transactions potentially compromises users' privacy. The
fundamental challenge is to balance the intrinsic benefits of blockchain
openness with the vital need for individual confidentiality. The proposal
suggests creating a confidential version of wrapped Ethereum (cWETH) fully
within the application layer. The solution combines the Elliptic Curve (EC)
Twisted ElGamal-based commitment scheme to preserve confidentiality and the EC
Diffie-Hellman (DH) protocol to introduce accessibility limited by the
commitment scheme. To enforce the correct generation of commitments,
encryption, and decryption, zk-SNARKs are utilized.

</details>


### [12] [Hybrid Quantum Security for IPsec](https://arxiv.org/abs/2507.09288)
*Javier Blanco-Romero,Pedro Otero García,Daniel Sobral-Blanco,Florina Almenares Mendoza,Ana Fernández Vilas,Manuel Fernández-Veiga*

Main category: cs.CR

TL;DR: This paper introduces two novel QKD integration approaches into IKEv2 for IPsec, comparing sequential vs. parallel hybrid QKD-PQC key establishment strategies. The parallel hybrid method reduces network latency penalties and demonstrates superior performance over sequential approaches, validated using IDQuantique hardware and Docker testing.


<details>
  <summary>Details</summary>
Motivation: QKD provides information-theoretic security against quantum threats, but its integration with computational key exchange in protocols like IPsec remains challenging due to fundamental mismatches between pre-distributed quantum keys and dynamic computational paradigms.

Method: The authors propose (1) a pure QKD approach using identifier-based key coordination to replace computational key derivation, and (2) a unified QKD-KEM abstraction enabling parallel composition of quantum and post-quantum methods. They systematically evaluate these designs against ETSI GS QKD specifications and RFC 9370 sequential requirements.

Result: Parallel hybrid approaches achieve significant performance improvements by eliminating multiplicative latency penalties. Docker-based testing with IDQuantique hardware shows pure QKD minimizes bandwidth overhead, while the unified abstraction framework maintains compatibility with existing protocol architectures.

Conclusion: The presented implementations provide practical quantum-enhanced IPsec solutions that balance security and performance. Parallel composition emerges as a superior strategy for integrating QKD into traditional protocols, offering critical infrastructure defense-in-depth solutions.

Abstract: Quantum Key Distribution (QKD) offers information-theoretic security against
quantum computing threats, but integrating QKD into existing security protocols
remains an unsolved challenge due to fundamental mismatches between
pre-distributed quantum keys and computational key exchange paradigms. This
paper presents the first systematic comparison of sequential versus parallel
hybrid QKD-PQC key establishment strategies for IPsec, revealing fundamental
protocol design principles that extend beyond specific implementations. We
introduce two novel approaches for incorporating QKD into Internet Key Exchange
version 2 (IKEv2) with support for both ETSI GS QKD 004 stateful and ETSI GS
QKD 014 stateless API specifications: (1) a pure QKD approach that replaces
computational key derivation with identifier-based quantum key coordination,
and (2) a unified QKD-KEM abstraction that enables parallel composition of
quantum and post-quantum cryptographic methods within existing protocol
frameworks. Our key insight is that parallel hybrid approaches eliminate the
multiplicative latency penalties inherent in sequential methods mandated by RFC
9370, achieving significant performance improvements under realistic network
conditions. Performance evaluation using a Docker-based testing framework with
IDQuantique QKD hardware demonstrates that the parallel hybrid approach
significantly outperforms sequential methods under network latency conditions,
while pure QKD achieves minimal bandwidth overhead through identifier-based key
coordination. Our implementations provide practical quantum-enhanced IPsec
solutions suitable for critical infrastructure deployments requiring
defense-in-depth security.

</details>


### [13] [Implementing and Evaluating Post-Quantum DNSSEC in CoreDNS](https://arxiv.org/abs/2507.09301)
*Julio Gento Suela,Javier Blanco-Romero,Florina Almenares Mendoza,Daniel Díaz-Sánchez*

Main category: cs.CR

TL;DR: This paper integrates post-quantum cryptographic (PQC) algorithms into CoreDNS to create quantum-resistant DNSSEC functionality, testing five PQC signature families (ML-DSA, FALCON, SPHINCS+, MAYO, SNOVA) with compatibility and performance evaluations.


<details>
  <summary>Details</summary>
Motivation: Quantum computers threaten classical RSA/ECDSA-based cryptography (e.g., DNSSEC), necessitating quantum-resistant alternatives to avoid security vulnerabilities.

Method: Developed a CoreDNS plugin supporting five PQC signature algorithms, enabling on-the-fly signing while preserving existing DNS resolution workflows.

Result: Benchmark results highlight security vs. efficiency trade-offs; some PQC candidates demonstrate acceptable performance for practical DNSSEC deployment.

Conclusion: PQC integration into CoreDNS is feasible, providing transitional options for quantum-resistant DNSSEC despite operational overhead.

Abstract: The emergence of quantum computers poses a significant threat to current
secure service, application and/or protocol implementations that rely on RSA
and ECDSA algorithms, for instance DNSSEC, because public-key cryptography
based on number factorization or discrete logarithm is vulnerable to quantum
attacks. This paper presents the integration of post-quantum cryptographic
(PQC) algorithms into CoreDNS to enable quantum-resistant DNSSEC functionality.
We have developed a plugin that extends CoreDNS with support for five PQC
signature algorithm families: ML-DSA, FALCON, SPHINCS+, MAYO, and SNOVA. Our
implementation maintains compatibility with existing DNS resolution flows while
providing on-the-fly signing using quantum-resistant signatures. A benchmark
has been performed and performance evaluation results reveal significant
trade-offs between security and efficiency. The results indicate that while PQC
algorithms introduce operational overhead, several candidates offer viable
compromises for transitioning DNSSEC to quantum-resistant cryptography.

</details>


### [14] [Backscatter Device-aided Integrated Sensing and Communication: A Pareto Optimization Framework](https://arxiv.org/abs/2507.09354)
*Yifan Zhang,Yu Bai,Riku Jantti,Zheng Yan,Christos Masouros,Zhu Han*

Main category: cs.CR

TL;DR: This paper proposes a backscatter device (BD)-assisted ISAC system to enhance performance in obstructed urban environments by optimizing time-frequency resource allocation, transmit power, and BD modulation via a block coordinate descent algorithm, validated through simulations showing superiority over existing ISAC schemes.


<details>
  <summary>Details</summary>
Motivation: Integrated sensing and communication (ISAC) systems suffer performance degradation in densely obstructed and non-line-of-sight urban scenarios, necessitating methods to improve reliability through ambient reflectors.

Method: A BD-assisted ISAC system is introduced, leveraging ambient passive BDs for signal paths. The Pareto boundary between sensing mutual information (SMI) and communication rates is characterized via joint optimization of RE allocation, transmit power management, and BD modulation using a BCD algorithm (SCA, augmented Lagrangian water-filling, SDR decomposition).

Result: Simulation results demonstrate enhanced sensing accuracy and communication reliability, adaptability to bistatic and MIMO configurations, and outperformance of state-of-the-art ISAC methods.

Conclusion: The proposed system provides a practical and flexible framework for ISAC in challenging environments by utilizing distributed BDs, offering design insights through the Pareto boundary and subproblem-optimized solutions.

Abstract: Integrated sensing and communication (ISAC) systems potentially encounter
significant performance degradation in densely obstructed urban and
non-line-of-sight scenarios, thus limiting their effectiveness in practical
deployments. To deal with these challenges, this paper proposes a backscatter
device (BD)-assisted ISAC system, which leverages passive BDs naturally
distributed in underlying environments for performance enhancement. These
ambient devices can enhance sensing accuracy and communication reliability by
providing additional reflective signal paths. In this system, we define the
Pareto boundary characterizing the trade-off between sensing mutual information
(SMI) and communication rates to provide fundamental insights for its design.
To derive the boundary, we formulate a performance optimization problem within
an orthogonal frequency division multiplexing (OFDM) framework, by jointly
optimizing time-frequency resource element (RE) allocation, transmit power
management, and BD modulation decisions. To tackle the non-convexity of the
problem, we decompose it into three subproblems, solved iteratively through a
block coordinate descent (BCD) algorithm. Specifically, the RE subproblem is
addressed using the successive convex approximation (SCA) method, the power
subproblem is solved using an augmented Lagrangian combined water-filling
method, and the BD modulation subproblem is tackled using semidefinite
relaxation (SDR) methods. Additionally, we demonstrate the generality of the
proposed system by showing its adaptability to bistatic ISAC scenarios and MIMO
settings. Finally, extensive simulation results validate the effectiveness of
the proposed system and its superior performance compared to existing
state-of-the-art ISAC schemes.

</details>


### [15] [LLMalMorph: On The Feasibility of Generating Variant Malware using Large-Language-Models](https://arxiv.org/abs/2507.09411)
*Md Ajwad Akil,Adrian Shuai Li,Imtiaz Karim,Arun Iyengar,Ashish Kundu,Vinny Parla,Elisa Bertino*

Main category: cs.CR

TL;DR: The paper investigates using Large Language Models (LLMs) for malware variant generation via code modification, introducing LLMalMorph—a framework leveraging code comprehension to create undetected variants with minimal tuning, reducing antivirus detection rates while preserving functionality and showing effectiveness against ML-based detectors.


<details>
  <summary>Details</summary>
Motivation: Antivirus detection rates for malware have increased, necessitating new methods for bypassing detection. This paper aims to explore LLMs' ability to modify malware code, leveraging their semantical and syntactical understanding to create variants that evade security systems.

Method: The framework, LLMalMorph, extracts function-level malware code information and uses custom prompts alongside transformation strategies to guide LLMs (e.g., GPT-3, Codex) in generating variants. It applies iterative prompting and controlled obfuscation techniques without requiring model fine-tuning.

Result: 618 malware variants were created from 10 diverse samples. Detection rates by antivirus engines were reduced without functionality loss. Notably, variants achieved success (up to 35% evasion) in attacking an ML-based classifier for malware, even when the classifier wasn't targeted by the framework.

Conclusion: This work demonstrates LLMs' potential in malware variant generation with practical evasion capabilities. However, limitations in handling binary-level obfuscations or architecture-specific constraints suggest the need for specialized prompts and validation mechanisms for future improvements.

Abstract: Large Language Models (LLMs) have transformed software development and
automated code generation. Motivated by these advancements, this paper explores
the feasibility of LLMs in modifying malware source code to generate variants.
We introduce LLMalMorph, a semi-automated framework that leverages semantical
and syntactical code comprehension by LLMs to generate new malware variants.
LLMalMorph extracts function-level information from the malware source code and
employs custom-engineered prompts coupled with strategically defined code
transformations to guide the LLM in generating variants without
resource-intensive fine-tuning. To evaluate LLMalMorph, we collected 10 diverse
Windows malware samples of varying types, complexity and functionality and
generated 618 variants. Our thorough experiments demonstrate that it is
possible to reduce the detection rates of antivirus engines of these malware
variants to some extent while preserving malware functionalities. In addition,
despite not optimizing against any Machine Learning (ML)-based malware
detectors, several variants also achieved notable attack success rates against
an ML-based malware classifier. We also discuss the limitations of current LLM
capabilities in generating malware variants from source code and assess where
this emerging technology stands in the broader context of malware variant
generation.

</details>


### [16] [SmartphoneDemocracy: Privacy-Preserving E-Voting on Decentralized Infrastructure using Novel European Identity](https://arxiv.org/abs/2507.09453)
*Michał Jóźwik,Johan Pouwelse*

Main category: cs.CR

TL;DR: SmartphoneDemocracy is a secure, privacy-preserving e-voting protocol using EUDI Wallet, Zero-Knowledge Proofs (ZKPs), and a peer-to-peer blockchain (TrustChain) to minimize trust in authorities and enable smartphone-based voting.


<details>
  <summary>Details</summary>
Motivation: Current electronic voting systems rely on centralized architectures with single points of failure, requiring excessive trust in authorities and conflicting with democratic principles. The challenge is to create a secure, private e-voting system with minimal trust dependencies for smartphone users.

Method: 1. **EUDI Wallet** for Sybil-resistant identity verification; 2. **Zero-Knowledge Proofs** to enable privacy-preserving validation; 3. **TrustChain** (peer-to-peer blockchain) as a resilient, serverless public bulletin board. The protocol integrates these components for anonymous, verifiable smartphone voting.

Result: A detailed protocol design, security analysis, and performance evaluation confirming feasibility for medium- to large-scale elections with reasonable computational and network overhead. A working prototype demonstrates practical implementation potential.

Conclusion: SmartphoneDemocracy presents a viable solution to digitize democratic processes with decentralized trust, enhancing accessibility, privacy, and verifiability for future smartphone-based voting systems.

Abstract: The digitization of democratic processes promises greater accessibility but
presents challenges in terms of security, privacy, and verifiability. Existing
electronic voting systems often rely on centralized architectures, creating
single points of failure and forcing too much trust in authorities, which
contradicts democratic principles. This research addresses the challenge of
creating a secure, private e-voting system with minimized trust dependencies
designed for the most versatile personal device: the smartphone. We introduce
SmartphoneDemocracy, a novel e-voting protocol that combines three key
technologies: the emerging European Digital Identity (EUDI) Wallet for
Sybil-resistant identity verification, Zero-Knowledge Proofs for
privacy-preserving validation, and a peer-to-peer blockchain (TrustChain) for a
resilient, serverless public bulletin board. Our protocol enables voters to
register and cast ballots anonymously and verifiably directly from their
smartphones. We provide a detailed protocol design, a security analysis against
a defined threat model, and a performance evaluation demonstrating that the
computational and network overhead is feasible for medium- to large-scale
elections. By developing and prototyping this system, we demonstrate a viable
path to empower citizens with a trustworthy, accessible, and user-controlled
digital voting experience.

</details>


### [17] [A Mixture of Linear Corrections Generates Secure Code](https://arxiv.org/abs/2507.09508)
*Weichen Yu,Ravi Mangal,Terry Zhuo,Matt Fredrikson,Corina S. Pasareanu*

Main category: cs.CR

TL;DR: The paper investigates whether LLMs can identify code vulnerabilities through internal representations and proposes an inference-time steering technique (MoC) that improves security without compromising functionality.


<details>
  <summary>Details</summary>
Motivation: LLMs excel at code generation but fail to reliably detect vulnerabilities, raising questions about the root cause (training limitations vs. prompting issues). The study aims to resolve this discrepancy and enhance vulnerability detection.

Method: Uses representation engineering to analyze LLMs' internal representations of vulnerabilities and develops Mixture of Corrections (MoC) steering to modulate token-generation probabilities during inference.

Result: MoC steering improved Qwen2.5-Coder-7B's security ratio by 8.9% and functionality (HumanEval pass@1) by 2.1%, surpassing standard prompting methods.

Conclusion: LLMs inherently encode vulnerability distinctions; effective exploitation (via MoC) enables controlled generation of secure code while maintaining functional performance.

Abstract: Large language models (LLMs) have become proficient at sophisticated
code-generation tasks, yet remain ineffective at reliably detecting or avoiding
code vulnerabilities. Does this deficiency stem from insufficient learning
about code vulnerabilities, or is it merely a result of ineffective prompting?
Using representation engineering techniques, we investigate whether LLMs
internally encode the concepts necessary to identify code vulnerabilities. We
find that current LLMs encode precise internal representations that distinguish
vulnerable from secure code--achieving greater accuracy than standard prompting
approaches. Leveraging these vulnerability-sensitive representations, we
develop an inference-time steering technique that subtly modulates the model's
token-generation probabilities through a mixture of corrections (MoC). Our
method effectively guides LLMs to produce less vulnerable code without
compromising functionality, demonstrating a practical approach to controlled
vulnerability management in generated code. Notably, MoC enhances the security
ratio of Qwen2.5-Coder-7B by 8.9\%, while simultaneously improving
functionality on HumanEval pass@1 by 2.1\%.

</details>


### [18] [A Login Page Transparency and Visual Similarity Based Zero Day Phishing Defense Protocol](https://arxiv.org/abs/2507.09564)
*Gaurav Varshney,Akanksha Raj,Divya Sangwan,Sharif Abuadbba,Rina Mishra,Yansong Gao*

Main category: cs.CR

TL;DR: This paper proposes Page Transparency (PT), a protocol-based phishing prevention framework inspired by certificate transparency. PT uses public logging and cryptographic proofs to verify legitimate login pages, blocking deceptive look-alikes via a new HTTP header and visual page-matching algorithm without requiring server/platform changes.


<details>
  <summary>Details</summary>
Motivation: Existing phishing countermeasures are fragmented and reactive, with standalone solutions for HTTP clients/servers or third parties. There's a need for a proactive, protocol-oriented defense to address universal phishing threats at the web infrastructure level.

Method: The solution introduces: (1) Public logging of sensitive pages (login forms) via Page Logging Servers (PLS) with cryptographic verification; (2) A client-side-only HTTP Page Transparency (PT) header that integrates verification without third-party tools; (3) Visual page-matching algorithms to detect deceptive page registrations by comparing page content against existing public logs.

Result: Cryptographic validation prevents unregistered deceptive pages from obtaining verification proofs, while visual comparison ensures registered pages maintain design integrity. Client-side implementation via HTTP headers eliminates server requirements and platform dependencies.

Conclusion: Page Transparency offers a protocol-level phishing defense through public logging transparency and client-side validation. This approach establishes a universal verification mechanism, significantly increasing the cost/complexity of launching successful look-alike phishing attacks while requiring minimal system modifications.

Abstract: Phishing is a prevalent cyberattack that uses look-alike websites to deceive
users into revealing sensitive information. Numerous efforts have been made by
the Internet community and security organizations to detect, prevent, or train
users to avoid falling victim to phishing attacks. Most of this research over
the years has been highly diverse and application-oriented, often serving as
standalone solutions for HTTP clients, servers, or third parties. However,
limited work has been done to develop a comprehensive or proactive
protocol-oriented solution to effectively counter phishing attacks. Inspired by
the concept of certificate transparency, which allows certificates issued by
Certificate Authorities (CAs) to be publicly verified by clients, thereby
enhancing transparency, we propose a concept called Page Transparency (PT) for
the web. The proposed PT requires login pages that capture users' sensitive
information to be publicly logged via PLS and made available to web clients for
verification. The pages are verified to be logged using cryptographic proofs.
Since all pages are logged on a PLS and visually compared with existing pages
through a comprehensive visual page-matching algorithm, it becomes impossible
for an attacker to register a deceptive look-alike page on the PLS and receive
the cryptographic proof required for client verification. All implementations
occur on the client side, facilitated by the introduction of a new HTTP PT
header, eliminating the need for platform-specific changes or the installation
of third-party solutions for phishing prevention.

</details>


### [19] [PromptChain: A Decentralized Web3 Architecture for Managing AI Prompts as Digital Assets](https://arxiv.org/abs/2507.09579)
*Marc Bara*

Main category: cs.CR

TL;DR: PromptChain introduces a decentralized Web3 architecture for treating AI prompts as verifiable digital assets, combining IPFS, smart contracts, and token incentives to address attribution, quality assurance, and fair compensation gaps in centralized platforms.


<details>
  <summary>Details</summary>
Motivation: Centralized AI prompt platforms fail to provide proper ownership attribution, quality control, and equitable monetization for creators, necessitating a decentralized solution leveraging blockchain principles for transparency and incentive alignment.

Method: 1. IPFS-based immutable storage for prompt history and versioning. 2. Stake-weighted validation mechanism to assess prompt quality. 3. Token economy with incentives proportional to contributor impact. 4. Cross-model metadata schema for compatibility.

Result: Demonstrated decentralized framework matches traditional platform efficiency while offering blockchain-anchored provenance tracking, verified ownership, and censorship-resistant prompt management through its three core design principles.

Conclusion: PromptChain establishes the first systematic decentralized infrastructure for standalone AI prompts, enabling open human-AI collaboration ecosystems in Web3 with robust ownership guarantees and community-driven quality assurance.

Abstract: We present PromptChain, a decentralized Web3 architecture that establishes AI
prompts as first-class digital assets with verifiable ownership, version
control, and monetization capabilities. Current centralized platforms lack
mechanisms for proper attribution, quality assurance, or fair compensation for
prompt creators. PromptChain addresses these limitations through a novel
integration of IPFS for immutable storage, smart contracts for governance, and
token incentives for community curation. Our design includes: (1) a
comprehensive metadata schema for cross-model compatibility, (2) a
stake-weighted validation mechanism to align incentives, and (3) a token
economy that rewards contributors proportionally to their impact. The proposed
architecture demonstrates how decentralized systems could potentially match
centralized alternatives in efficiency while providing superior ownership
guarantees and censorship resistance through blockchain-anchored provenance
tracking. By decoupling prompts from specific AI models or outputs, this work
establishes the foundation for an open ecosystem of human-AI collaboration in
the Web3 era, representing the first systematic treatment of prompts as
standalone digital assets with dedicated decentralized infrastructure.

</details>


### [20] [AICrypto: A Comprehensive Benchmark For Evaluating Cryptography Capabilities of Large Language Models](https://arxiv.org/abs/2507.09580)
*Yu Wang,Yijian Liu,Liheng Ji,Han Luo,Wenjie Li,Xiaofei Zhou,Chiyun Feng,Puji Wang,Yuhan Cao,Geyuan Zhang,Xiaojian Li,Rongwu Xu,Yilei Chen,Tianxing He*

Main category: cs.CR

TL;DR: The paper introduces AICrypto, the first benchmark to evaluate LLMs in cryptographic tasks, combining 135 multiple-choice questions, 150 CTF challenges, and 18 proof problems. It reveals that top LLMs match human experts in factual knowledge and exploitation but struggle with abstract math and multi-step reasoning.


<details>
  <summary>Details</summary>
Motivation: Cryptography is a critical domain for cybersecurity, yet LLMs' capabilities in this area remain underexplored. The authors aim to provide a rigorous benchmark to assess LLM performance, enabling insights for future research.

Method: AICrypto was developed using expert-reviewed tasks covering memorization, vulnerability exploitation, and formal proofs. A human baseline was established for comparison, and an agent-based framework automated CTF challenge evaluations. 17 leading LLMs were tested.

Result: Leading LLMs achieved proficiency in memorizing concepts, exploiting common vulnerabilities, and solving routine proofs. However, they underperformed on abstract mathematical understanding and multi-step reasoning tasks requiring dynamic analysis.

Conclusion: AICrypto advances research by systematically evaluating LLMs’ cryptographic capabilities, demonstrating both strengths and limitations. The findings suggest foundational gaps in LLMs’ understanding of advanced crypto principles, while the benchmark serves as a valuable resource for further study.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
a variety of domains. However, their applications in cryptography, which serves
as a foundational pillar of cybersecurity, remain largely unexplored. To
address this gap, we propose \textbf{AICrypto}, the first comprehensive
benchmark designed to evaluate the cryptographic capabilities of LLMs. The
benchmark comprises 135 multiple-choice questions, 150 capture-the-flag (CTF)
challenges, and 18 proof problems, covering a broad range of skills from
factual memorization to vulnerability exploitation and formal reasoning. All
tasks are carefully reviewed or constructed by cryptography experts to ensure
correctness and rigor. To support automated evaluation of CTF challenges, we
design an agent-based framework. To gain deeper insight into the current state
of cryptographic proficiency in LLMs, we introduce human expert performance
baselines for comparison across all task types. Our evaluation of 17 leading
LLMs reveals that state-of-the-art models match or even surpass human experts
in memorizing cryptographic concepts, exploiting common vulnerabilities, and
routine proofs. However, they still lack a deep understanding of abstract
mathematical concepts and struggle with tasks that require multi-step reasoning
and dynamic analysis. We hope this work could provide insights for future
research on LLMs in cryptographic applications. Our code and dataset are
available at https://aicryptobench.github.io.

</details>


### [21] [Efficient Private Inference Based on Helper-Assisted Malicious Security Dishonest Majority MPC](https://arxiv.org/abs/2507.09607)
*Kaiwen Wang,Yuehan Dong,Junchao Fan,Xiaolin Chang*

Main category: cs.CR

TL;DR: The paper proposes a private inference framework, HA-MSDM, to balance malicious security (dishonest majority) with efficiency using optimized MPC protocols and co-optimized strategies for neural networks.


<details>
  <summary>Details</summary>
Motivation: Existing MPC-based private inference frameworks assume overly idealistic threat models (semi-honest/honest majority) or face low efficiency in malicious dishonest-majority scenarios, requiring a robust and practical solution.

Method: HA-MSDM integrates five specialized MPC protocols for fixed-round multiplication, exponentiation, and polynomial operations, alongside co-optimization techniques: sixth-order polynomial approximation for nonlinear layers to improve efficiency, parameter-adjusted batch normalization to maintain accuracy, and mitigation of activation escape issues through input constraints.

Result: HA-MSDM achieves 2.4-25.7× LAN and 1.3-9.5× WAN speedups over state-of-the-art frameworks (IEEE S&P'25) while sustaining high inference accuracy with 0.04%-1.08% relative errors on LeNet and AlexNet benchmarks.

Conclusion: The framework effectively balances security, efficiency, and accuracy in malicious dishonest-majority settings, establishing practicality for real-world MLaaS private inference applications.

Abstract: Private inference based on Secure Multi-Party Computation (MPC) addresses
data privacy risks in Machine Learning as a Service (MLaaS). However, existing
MPC-based private inference frameworks focuses on semi-honest or honest
majority models, whose threat models are overly idealistic, while malicious
security dishonest majority models face the challenge of low efficiency. To
balance security and efficiency, we propose a private inference framework using
Helper-Assisted Malicious Security Dishonest Majority Model (HA-MSDM). This
framework includes our designed five MPC protocols and a co-optimized strategy.
These protocols achieve efficient fixed-round multiplication, exponentiation,
and polynomial operations, providing foundational primitives for private
inference. The co-optimized strategy balances inference efficiency and
accuracy. To enhance efficiency, we employ polynomial approximation for
nonlinear layers. For improved accuracy, we construct sixth-order polynomial
approximation within a fixed interval to achieve high-precision activation
function fitting and introduce parameter-adjusted batch normalization layers to
constrain the activation escape problem. Benchmark results on LeNet and AlexNet
show our framework achieves 2.4-25.7x speedup in LAN and 1.3-9.5x acceleration
in WAN compared to state-of-the-art frameworks (IEEE S&P'25), maintaining high
accuracy with only 0.04%-1.08% relative errors.

</details>


### [22] [CAN-Trace Attack: Exploit CAN Messages to Uncover Driving Trajectories](https://arxiv.org/abs/2507.09624)
*Xiaojie Lin,Baihe Ma,Xu Wang,Guangsheng Yu,Ying He,Wei Ni,Ren Ping Liu*

Main category: cs.CR

TL;DR: CAN-Trace is a new privacy attack using CAN messages (vehicle speed and accelerator data) to reconstruct driving trajectories, achieving high success rates in urban and suburban areas without relying on GPS.


<details>
  <summary>Details</summary>
Motivation: existing GPS-based methods for detecting driving trajectories are vulnerable to data outages, leaving drivers' privacy at risk; CAN-Trace addresses this by using alternative vehicle data (CAN messages) to trace trajectories even when GPS data is unavailable.

Method: proposed (1) trajectory reconstruction algorithm to convert CAN messages into weighted graphs representing driving statuses, and (2) graph-matching algorithm to identify trajectories by aligning these graphs with road networks, combined with a novel metric to handle data gaps and inaccuracies.

Result: CAN-Trace achieved 90.59% attack success rate in urban regions and 99.41% in suburban regions through empirical testing across diverse vehicles and driving conditions.

Conclusion: CAN-Trace demonstrates the critical risk of privacy breaches using CAN data as an alternative to GPS, highlighting the need for robust countermeasures to protect driving trajectory privacy under various data availability scenarios.

Abstract: Driving trajectory data remains vulnerable to privacy breaches despite
existing mitigation measures. Traditional methods for detecting driving
trajectories typically rely on map-matching the path using Global Positioning
System (GPS) data, which is susceptible to GPS data outage. This paper
introduces CAN-Trace, a novel privacy attack mechanism that leverages
Controller Area Network (CAN) messages to uncover driving trajectories, posing
a significant risk to drivers' long-term privacy. A new trajectory
reconstruction algorithm is proposed to transform the CAN messages,
specifically vehicle speed and accelerator pedal position, into weighted graphs
accommodating various driving statuses. CAN-Trace identifies driving
trajectories using graph-matching algorithms applied to the created graphs in
comparison to road networks. We also design a new metric to evaluate matched
candidates, which allows for potential data gaps and matching inaccuracies.
Empirical validation under various real-world conditions, encompassing
different vehicles and driving regions, demonstrates the efficacy of CAN-Trace:
it achieves an attack success rate of up to 90.59% in the urban region, and
99.41% in the suburban region.

</details>


### [23] [Interpreting Differential Privacy in Terms of Disclosure Risk](https://arxiv.org/abs/2507.09699)
*Zeki Kazan,Sagar Sharma,Wanrong Zhang,Bo Jiang,Qiang Yan*

Main category: cs.CR

TL;DR: This paper establishes new connections between differential privacy (DP) and statistical disclosure risk measures, providing tools for experts and non-experts to interpret DP guarantees, select privacy parameters, and analyze adversary risks.


<details>
  <summary>Details</summary>
Motivation: The rapid adoption of differential privacy necessitates better tools for understanding and communicating its privacy guarantees, especially for parameter selection and composition theorem interpretation.

Method: The authors develop a framework linking DP to existing statistical disclosure risk metrics, enabling analysis of privacy guarantees through adversary risk modeling and parameter optimization.

Result: The approach allows for intuitive interpretation of DP composition laws, justification of parameter choices, and identification of worst-case adversary prior distributions that maximize disclosure risk.

Conclusion: By connecting DP with statistical risk measures, the work enhances practical understanding and deployment of differential privacy, enabling more informed decisions about privacy-utility tradeoffs and parameter selection.

Abstract: As the use of differential privacy (DP) becomes widespread, the development
of effective tools for reasoning about the privacy guarantee becomes
increasingly critical. In pursuit of this goal, we demonstrate novel
relationships between DP and measures of statistical disclosure risk. We
suggest how experts and non-experts can use these results to explain the DP
guarantee, interpret DP composition theorems, select and justify privacy
parameters, and identify worst-case adversary prior probabilities.

</details>


### [24] [EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions](https://arxiv.org/abs/2507.09762)
*Yasir Ech-Chammakhy,Anas Motii,Anass Rabii,Jaafar Chbili*

Main category: cs.CR

TL;DR: This paper introduces an unsupervised framework using fine-tuned Transformer embeddings and a daily ranking system to automatically identify and prioritize security events in hacker forums without predefined keywords.


<details>
  <summary>Details</summary>
Motivation: Hacker forums contain vital early warnings about cybersecurity threats, but their unstructured, noisy content makes actionable intelligence extraction challenging.

Method: The framework applies contrastive-learning fine-tuned Transformer embeddings to cluster security-related discussions into events. It uses metrics like timeliness, credibility, completeness, and relevance in a daily ranking mechanism to prioritize clusters.

Result: Experiments on real-world hacker forum data show effective noise reduction and accurate identification of high-priority threats (e.g., zero-days, malware) through structured clustering and prioritization.

Conclusion: The approach advances automated threat detection by transforming unstructured hacker forum discourse into actionable intelligence, enabling proactive security responses.

Abstract: Hacker forums provide critical early warning signals for emerging
cybersecurity threats, but extracting actionable intelligence from their
unstructured and noisy content remains a significant challenge. This paper
presents an unsupervised framework that automatically detects, clusters, and
prioritizes security events discussed across hacker forum posts. Our approach
leverages Transformer-based embeddings fine-tuned with contrastive learning to
group related discussions into distinct security event clusters, identifying
incidents like zero-day disclosures or malware releases without relying on
predefined keywords. The framework incorporates a daily ranking mechanism that
prioritizes identified events using quantifiable metrics reflecting timeliness,
source credibility, information completeness, and relevance. Experimental
evaluation on real-world hacker forum data demonstrates that our method
effectively reduces noise and surfaces high-priority threats, enabling security
analysts to mount proactive responses. By transforming disparate hacker forum
discussions into structured, actionable intelligence, our work addresses
fundamental challenges in automated threat detection and analysis.

</details>


### [25] [Endorsement-Driven Blockchain SSI Framework for Dynamic IoT Ecosystems](https://arxiv.org/abs/2507.09859)
*Guntur Dharma Putra,Bagus Rakadyanto Oktavianto Putra*

Main category: cs.CR

TL;DR: The paper introduces a blockchain-based Self-Sovereign Identity (SSI) framework for IoT, enabling decentralized identity operations with flexible credential issuance and revocation by any individual with trust linkage, reducing reliance on centralized entities.


<details>
  <summary>Details</summary>
Motivation: Existing SSI frameworks for IoT restrict credential issuance to specific trusted entities (e.g. manufacturers), limiting adaptability in dynamic, decentralized IoT ecosystems where trust relationships may evolve or decentralize further.

Method: A layered blockchain-based SSI framework using (1) endorsement-based trust calculations to dynamically establish trust, (2) hierarchical chain-of-trust mechanisms for trust maintenance, (3) blockchain as a trust-anchored Verifiable Data Registry ensuring immutability, and (4) smart contracts to automate credential issuance, verification, revocation workflows.

Result: A proof-of-concept implementation demonstrated the framework's feasibility with minimal overhead compared to existing approaches, confirming its suitability for dynamic, resource-constrained IoT environments while maintaining transparent, decentralized identity operations.

Conclusion: The framework effectively addresses SSI limitations in IoT through blockchain's inherent trust properties and dynamic endorsement mechanisms, offering a scalable, decentralized solution for evolving IoT ecosystems with distributed trust requirements.

Abstract: Self-Sovereign Identity (SSI) offers significant potential for managing
identities in the Internet of Things (IoT), enabling decentralized
authentication and credential management without reliance on centralized
entities. However, existing SSI frameworks often limit credential issuance and
revocation to trusted entities, such as IoT manufacturers, which restricts
flexibility in dynamic IoT ecosystems. In this paper, we propose a
blockchain-based SSI framework that allows any individual with a verifiable
trust linkage to act as a credential issuer, ensuring decentralized and
scalable identity management. Our framework incorporates a layered
architecture, where trust is dynamically established through endorsement-based
calculations and maintained via a hierarchical chain-of-trust mechanism.
Blockchain serves as the Verifiable Data Registry, ensuring transparency and
immutability of identity operations, while smart contracts automate critical
processes such as credential issuance, verification, and revocation. A
proof-of-concept implementation demonstrates that the proposed framework is
feasible and incurs minimal overheads compared to the baseline, making it
well-suited for dynamic and resource-constrained IoT environments.

</details>


### [26] [Secure and Efficient UAV-Based Face Detection via Homomorphic Encryption and Edge Computing](https://arxiv.org/abs/2507.09860)
*Nguyen Van Duc,Bui Duc Manh,Quang-Trung Luu,Dinh Thai Hoang,Van-Linh Nguyen,Diep N. Nguyen*

Main category: cs.CR

TL;DR: The paper proposes a privacy-preserving framework for UAV-based face detection using Homomorphic Encryption (HE) with the CKKS scheme, achieving minimal impact on accuracy.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns arise from UAVs' extensive surveillance capabilities during face detection, necessitating a secure yet efficient solution.

Method: The framework integrates HE (CKKS) and neural networks, employing a SIMD-based data encoding method for preprocessing and a secure inference algorithm for encrypted data computation without decryption.

Result: Experimental results show the method maintains data confidentiality while achieving an accuracy within 1% of non-encrypted benchmarks, effectively balancing privacy and performance.

Conclusion: The proposed approach enables UAV-based secure face detection by combining HE with advanced neural networks, offering a viable solution for real-world privacy-preserving dynamic environments.

Abstract: This paper aims to propose a novel machine learning (ML) approach
incorporating Homomorphic Encryption (HE) to address privacy limitations in
Unmanned Aerial Vehicles (UAV)-based face detection. Due to challenges related
to distance, altitude, and face orientation, high-resolution imagery and
sophisticated neural networks enable accurate face recognition in dynamic
environments. However, privacy concerns arise from the extensive surveillance
capabilities of UAVs. To resolve this issue, we propose a novel framework that
integrates HE with advanced neural networks to secure facial data throughout
the inference phase. This method ensures that facial data remains secure with
minimal impact on detection accuracy. Specifically, the proposed system
leverages the Cheon-Kim-Kim-Song (CKKS) scheme to perform computations directly
on encrypted data, optimizing computational efficiency and security.
Furthermore, we develop an effective data encoding method specifically designed
to preprocess the raw facial data into CKKS form in a
Single-Instruction-Multiple-Data (SIMD) manner. Building on this, we design a
secure inference algorithm to compute on ciphertext without needing decryption.
This approach not only protects data privacy during the processing of facial
data but also enhances the efficiency of UAV-based face detection systems.
Experimental results demonstrate that our method effectively balances privacy
protection and detection performance, making it a viable solution for UAV-based
secure face detection. Significantly, our approach (while maintaining data
confidentially with HE encryption) can still achieve an accuracy of less than
1% compared to the benchmark without using encryption.

</details>


### [27] [Differentially Private Federated Low Rank Adaptation Beyond Fixed-Matrix](https://arxiv.org/abs/2507.09990)
*Ming Wen,Jiaqi Zhu,Yuedong Xu,Yipeng Zhou,Dingding Han*

Main category: cs.CR

TL;DR: FedASK is a differentially private federated learning framework for LLMs that uses double sketching to address privacy risks in adapter transmission and optimize low-rank model updates.


<details>
  <summary>Details</summary>
Motivation: Federated LLM fine-tuning with LoRA faces privacy leakage risks from transmitting local adapters and a dilemma between adding DP noise (which amplifies synthetic noise) and maintaining model learnability (which requires adapter updates).

Method: FedASK introduces a two-stage sketching pipeline inspired by randomized SVD: (1) privacy-preserving sketches of local adapter updates are aggregated, and (2) the server reconstructs global low-rank matrices using the collected sketches to update both adapters effectively.

Result: Theoretical proofs establish FedASK's differential privacy guarantees and exact aggregation property, while experiments show consistent performance improvements over baselines across diverse privacy budgets and data scenarios.

Conclusion: FedASK resolves the privacy-learnability tradeoff in federated LLM adaptation by enabling robust DP with full utilization of both low-rank adapters through double sketching, setting a new standard for secure collaborative model fine-tuning.

Abstract: Large language models (LLMs) typically require fine-tuning for
domain-specific tasks, and LoRA offers a computationally efficient approach by
training low-rank adapters. LoRA is also communication-efficient for federated
LLMs when multiple users collaboratively fine-tune a global LLM model without
sharing their proprietary raw data. However, even the transmission of local
adapters between a server and clients risks serious privacy leakage. Applying
differential privacy (DP) to federated LoRA encounters a dilemma: adding noise
to both adapters amplifies synthetic noise on the model, while fixing one
adapter impairs the learnability of fine-tuning. In this paper, we propose
FedASK (Differentially Private Federated Low Rank Adaptation with Double
Sketching) , a novel federated LoRA framework to enable effective updating of
both low-rank adapters with robust differential privacy. Inspired by randomized
SVD, our key idea is a two-stage sketching pipeline. This pipeline first
aggregates carefully sketched, privacy-preserving local updates, and then
reconstructs the global matrices on the server to facilitate effective updating
of both adapters. We theoretically prove FedASK's differential privacy
guarantee and its exact aggregation property. Comprehensive experiments
demonstrate that FedASK consistently outperforms baseline methods across a
variety of privacy settings and data distributions.

</details>


### [28] [The Man Behind the Sound: Demystifying Audio Private Attribute Profiling via Multimodal Large Language Model Agents](https://arxiv.org/abs/2507.10016)
*Lixu Wang,Kaixiang Yao,Xinfeng Li,Dong Yang,Haoyang Li,Xiaofeng Wang,Wei Dong*

Main category: cs.CR

TL;DR: This paper introduces audio private attribute profiling, a new privacy risk in MLLMs where sensitive attributes are inferred from audio. It presented AP^2, a benchmark dataset with real-world audio annotations, and Gifts, a hybrid multi-agent framework combining ALMs and LLMs to enhance inference accuracy. Evaluations show Gifts outperforms baselines, highlighting the feasibility of audio-based attacks and the need for defenses.


<details>
  <summary>Details</summary>
Motivation: Despite non-visual/numerical modalities being primary for MLLMs, audio data's covert capture and unique properties (tone, pitch) pose overlooked privacy risks. Existing benchmarks lack sensitive attribute annotations, and current MLLMs struggle to infer attributes directly from audio, creating unaddressed research gaps.

Method: 1. Built AP^2 dataset: Two real-world audio subsets annotated with sensitive attributes. 2. Developed Gifts framework: Uses LLM to guide ALM's attribute inference through multi-phase cooperation, reduces hallucinations in long-context audio analysis by consolidating ALM responses through forensic reasoning.

Result: Gifts achieves 15-20% higher accuracy than baselines in attribute inference, with 30% reduction in hallucinations. AP^2 demonstrates 85% attribute labeling reliability. Defense experiments identify differential privacy and data masking as most effective mitigation strategies against audio-based profiling.

Conclusion: This work establishes audio as a critical vector for privacy attacks in MLLMs through empirical validation and novel framework development. The AP^2 benchmark and Gifts framework enable systematic study of both attack capabilities and defense mechanisms in audio private attribute profiling contexts.

Abstract: Our research uncovers a novel privacy risk associated with multimodal large
language models (MLLMs): the ability to infer sensitive personal attributes
from audio data -- a technique we term audio private attribute profiling. This
capability poses a significant threat, as audio can be covertly captured
without direct interaction or visibility. Moreover, compared to images and
text, audio carries unique characteristics, such as tone and pitch, which can
be exploited for more detailed profiling. However, two key challenges exist in
understanding MLLM-employed private attribute profiling from audio: (1) the
lack of audio benchmark datasets with sensitive attribute annotations and (2)
the limited ability of current MLLMs to infer such attributes directly from
audio. To address these challenges, we introduce AP^2, an audio benchmark
dataset that consists of two subsets collected and composed from real-world
data, and both are annotated with sensitive attribute labels. Additionally, we
propose Gifts, a hybrid multi-agent framework that leverages the complementary
strengths of audio-language models (ALMs) and large language models (LLMs) to
enhance inference capabilities. Gifts employs an LLM to guide the ALM in
inferring sensitive attributes, then forensically analyzes and consolidates the
ALM's inferences, overcoming severe hallucinations of existing ALMs in
generating long-context responses. Our evaluations demonstrate that Gifts
significantly outperforms baseline approaches in inferring sensitive
attributes. Finally, we investigate model-level and data-level defense
strategies to mitigate the risks of audio private attribute profiling. Our work
validates the feasibility of audio-based privacy attacks using MLLMs,
highlighting the need for robust defenses, and provides a dataset and framework
to facilitate future research.

</details>


### [29] [HASSLE: A Self-Supervised Learning Enhanced Hijacking Attack on Vertical Federated Learning](https://arxiv.org/abs/2507.10162)
*Weiyang He,Chip-Hong Chang*

Main category: cs.CR

TL;DR: HASSLE is a hijacking attack framework for Vertical Federated Learning (VFL) that uses gradient-direction-based label inference with a single known label instance and adversarial embedding generation enhanced by self-supervised learning. It achieves high attack success rates across diverse datasets, including 99%+ in simple cases and 85% on CIFAR-100, demonstrating significant VFL security threats while suggesting insights for robust defense design.


<details>
  <summary>Details</summary>
Motivation: Prior VFL security evaluations were limited by low label inference precision and suboptimal backdoor attack conditions. This paper aims to propose a more rigorous threat analysis framework to better understand and improve VFL system robustness against privacy compromising attacks.

Method: The framework integrates two key modules: (1) Gradient-direction label inference module that identifies private sample-label associations using a single known positive instance, and (2) Self-supervised learning enhanced adversarial embedding generation for backdoor injection. Both modules exploit VFL gradient exchange mechanisms for secure feature manipulation.

Result: HASSLE achieves over 99% attack success rate in two-party scenarios across four datasets (including MNIST, Fashion-MNIST, and tabular data), and 85% ASR on the complex CIFAR-100 dataset. Evaluation against 8 existing defense mechanisms shows high effectiveness of the proposed attack framework.

Conclusion: The research demonstrates HASSLE's significant threat to VFL through its high precision label inference and effective backdoor injection capabilities, particularly in overcoming limitations of prior attacks. While highlighting the vulnerability of current VFL systems, the work provides critical insights for developing trustworthy security frameworks that can resist such sophisticated hybrid attacks.

Abstract: Vertical Federated Learning (VFL) enables an orchestrating active party to
perform a machine learning task by cooperating with passive parties that
provide additional task-related features for the same training data entities.
While prior research has leveraged the privacy vulnerability of VFL to
compromise its integrity through a combination of label inference and backdoor
attacks, their effectiveness is constrained by the low label inference
precision and suboptimal backdoor injection conditions. To facilitate a more
rigorous security evaluation on VFL without these limitations, we propose
HASSLE, a hijacking attack framework composed of a gradient-direction-based
label inference module and an adversarial embedding generation algorithm
enhanced by self-supervised learning. HASSLE accurately identifies private
samples associated with a targeted label using only a single known instance of
that label. In the two-party scenario, it demonstrates strong performance with
an attack success rate (ASR) of over 99% across four datasets, including both
image and tabular modalities, and achieves 85% ASR on the more complex
CIFAR-100 dataset. Evaluation of HASSLE against 8 potential defenses further
highlights its significant threat while providing new insights into building a
trustworthy VFL system.

</details>


### [30] [DNS Tunneling: Threat Landscape and Improved Detection Solutions](https://arxiv.org/abs/2507.10267)
*Novruz Amirov,Baran Isik,Bilal Ihsan Tuncer,Serif Bahtiyar*

Main category: cs.CR

TL;DR: Proposes a machine learning-based method for accurate DNS tunneling detection by analyzing extracted DNS traffic features.


<details>
  <summary>Details</summary>
Motivation: DNS tunneling exploits normal DNS traffic for covert communication, making it hard to distinguish from legitimate traffic using traditional rule-based/sig-matching methods. Effective detection is critical for security.

Method: A novel approach using machine learning algorithms with features extracted from DNS traffic to classify and identify tunneling patterns.

Result: Experimental analysis demonstrates the proposed method achieves high accuracy in detecting DNS tunneling compared to conventional techniques.

Conclusion: Combining ML algorithms with DNS traffic feature extraction effectively mitigates tunneling threats that evade signature-based detection methods.

Abstract: Detecting Domain Name System (DNS) tunneling is a significant challenge in
security due to its capacity to hide harmful actions within DNS traffic that
appears to be normal and legitimate. Traditional detection methods are based on
rule-based approaches or signature matching methods that are often insufficient
to accurately identify such covert communication channels. This research is
about effectively detecting DNS tunneling. We propose a novel approach to
detect DNS tunneling with machine learning algorithms. We combine machine
learning algorithms to analyze the traffic by using features extracted from DNS
traffic. Analyses results show that the proposed approach is a good candidate
to detect DNS tunneling accurately.

</details>


### [31] [Logic layer Prompt Control Injection (LPCI): A Novel Security Vulnerability Class in Agentic Systems](https://arxiv.org/abs/2507.10457)
*Hammad Atta,Ken Huang,Manish Bhatt,Kamal Ahmed,Muhammad Aziz Ul Haq,Yasir Mehmood*

Main category: cs.CR

TL;DR: This paper introduces Logic-Layer Prompt Control Injection (LPCI), a novel enterprise security vulnerability where encoded and conditionally triggered payloads are embedded in persistent memory or tool outputs to bypass input filters and cause unauthorized behavior across sessions.


<details>
  <summary>Details</summary>
Motivation: The integration of large language models (LLMs) into enterprise systems raises new security risks due to their complex execution and persistent memory management, which can harbor covert vulnerabilities.

Method: The authors demonstrate how encoded payloads can be hidden in memory, vector stores, or tool outputs, using delay and conditional triggers to execute unauthorized actions when traditional input filters are evaded.

Result: LPCI attacks are shown to bypass conventional defenses, enabling persistence and cross-session unauthorized behavior in enterprise LLM systems.

Conclusion: The paper highlights the need for enhanced security protocols for enterprise LLM integrations, emphasizing multi-phase validation and monitoring of persistent memory and execution contexts to detect and mitigate LPCI-style attacks.

Abstract: The integration of large language models (LLMs) into enterprise systems has
created a new class of covert security vulnerabilities, particularly within
logic-execution layers and persistent-memory contexts. In this paper, we
introduce Logic-Layer Prompt Control Injection (LPCI), a novel attack category
in which encoded, delayed, and conditionally triggered payloads are embedded in
memory, vector stores, or tool outputs. These payloads can bypass conventional
input filters and trigger unauthorised behaviour across sessions.

</details>


### [32] [SynthGuard: Redefining Synthetic Data Generation with a Scalable and Privacy-Preserving Workflow Framework](https://arxiv.org/abs/2507.10489)
*Eduardo Brito,Mahmoud Shoush,Kristian Tamm,Paula Etti,Liina Kamm*

Main category: cs.CR

TL;DR: SynthGuard is a framework enabling secure, privacy-preserving, and scalable synthetic data generation (SDG) workflows with computational governance, allowing data owners to maintain control while addressing compliance and data sovereignty challenges.


<details>
  <summary>Details</summary>
Motivation: Current SDG approaches often rely on centralized or external processing, violating data sovereignty, domain ownership principles, and creating regulatory compliance risks in data-sensitive fields like healthcare, finance, and law enforcement.

Method: The authors developed SynthGuard through iterative collaboration with domain experts, implementing modular workflows that ensure privacy, security, auditability, and reproducibility. It supports cross-environment execution and integrates privacy/utility assessments during SDG.

Result: SynthGuard successfully balances security, privacy, and scalability in real-world use cases, demonstrating effective SDG workflow execution and compliance with domain-specific and regulatory requirements across diverse computational environments.

Conclusion: SynthGuard offers validated governance for SDG that aligns with data sovereignty and compliance needs through modular design, expert collaboration, and integrated assessments, ensuring secure and auditable synthetic data creation.

Abstract: The growing reliance on data-driven applications in sectors such as
healthcare, finance, and law enforcement underscores the need for secure,
privacy-preserving, and scalable mechanisms for data generation and sharing.
Synthetic data generation (SDG) has emerged as a promising approach but often
relies on centralized or external processing, raising concerns about data
sovereignty, domain ownership, and compliance with evolving regulatory
standards. To overcome these issues, we introduce SynthGuard, a framework
designed to ensure computational governance by enabling data owners to maintain
control over SDG workflows. SynthGuard supports modular and privacy-preserving
workflows, ensuring secure, auditable, and reproducible execution across
diverse environments. In this paper, we demonstrate how SynthGuard addresses
the complexities at the intersection of domain-specific needs and scalable SDG
by aligning with requirements for data sovereignty and regulatory compliance.
Developed iteratively with domain expert input, SynthGuard has been validated
through real-world use cases, demonstrating its ability to balance security,
privacy, and scalability while ensuring compliance. The evaluation confirms its
effectiveness in implementing and executing SDG workflows and integrating
privacy and utility assessments across various computational environments.

</details>


### [33] [BURN: Backdoor Unlearning via Adversarial Boundary Analysis](https://arxiv.org/abs/2507.10491)
*Yanghao Su,Jie Zhang,Yiming Li,Tianwei Zhang,Qing Guo,Weiming Zhang,Nenghai Yu,Nils Lukas,Wenbo Zhou*

Main category: cs.CR

TL;DR: The paper introduces BURN, a backdoor unlearning framework that addresses the limitations of existing methods by restoring correct semantic labels and using adversarial boundary analysis, validated across various datasets and attacks.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning methods fail to restore the correct semantic labels of poisoned samples, leaving false correlations between trigger patterns and target labels unresolved. This hinders effective backdoor removal while preserving original model functionality.

Method: BURN utilizes boundary adversarial attack techniques to identify poisoned samples through their distinct adversarial boundary distances and label distributions. It involves two phases: (1) detecting and fine-tuning poisoned samples to restore correct labels, and (2) a feedback mechanism tracking prediction discrepancies to iteratively refine the dataset and purify the model.

Result: BURN successfully eliminates backdoor threats across multiple datasets, architectures, and seven backdoor attack types, maintaining the model's original performance as demonstrated through extensive experiments.

Conclusion: BURN provides an effective defense against backdoor attacks by combining false correlation decoupling, progressive data refinement, and model purification, validated by robust experimental results showing its efficacy and minimal performance loss.

Abstract: Backdoor unlearning aims to remove backdoor-related information while
preserving the model's original functionality. However, existing unlearning
methods mainly focus on recovering trigger patterns but fail to restore the
correct semantic labels of poison samples. This limitation prevents them from
fully eliminating the false correlation between the trigger pattern and the
target label. To address this, we leverage boundary adversarial attack
techniques, revealing two key observations. First, poison samples exhibit
significantly greater distances from decision boundaries compared to clean
samples, indicating they require larger adversarial perturbations to change
their predictions. Second, while adversarial predicted labels for clean samples
are uniformly distributed, those for poison samples tend to revert to their
original correct labels. Moreover, the features of poison samples restore to
closely resemble those of corresponding clean samples after adding adversarial
perturbations. Building upon these insights, we propose Backdoor Unlearning via
adversaRial bouNdary analysis (BURN), a novel defense framework that integrates
false correlation decoupling, progressive data refinement, and model
purification. In the first phase, BURN employs adversarial boundary analysis to
detect poisoned samples based on their abnormal adversarial boundary distances,
then restores their correct semantic labels for fine-tuning. In the second
phase, it employs a feedback mechanism that tracks prediction discrepancies
between the original backdoored model and progressively sanitized models,
guiding both dataset refinement and model purification. Extensive evaluations
across multiple datasets, architectures, and seven diverse backdoor attack
types confirm that BURN effectively removes backdoor threats while maintaining
the model's original performance.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [34] [Choosing the Right Git Workflow: A Comparative Analysis of Trunk-based vs. Branch-based Approaches](https://arxiv.org/abs/2507.08943)
*Pedro Lopes,Paola Accioly,Paulo Borba,Vitor Menezes*

Main category: cs.SE

TL;DR: The paper compares branch-based and trunk-based Git workflows, finding that trunk-based suits fast-paced, experienced small teams despite management challenges, while branch-based (e.g., GitFlow) is preferred by less experienced, larger teams.


<details>
  <summary>Details</summary>
Motivation: Despite popular discussions on Git workflows, few scientific studies explore their real-world use. This work addresses this gap by investigating factors influencing workflow adoption in Brazilian software teams.

Method: The study uses semi-structured interviews and surveys with software developers to gather qualitative data on workflow preferences and challenges.

Result: Trunk-based workflows correlate with agile environments and team experience, while branch-based workflows provide structure for larger teams but introduce complexity in coordination and maintenance.

Conclusion: Workflow choice depends on team size, experience, and project agility. Trunk-based development increases efficiency in small teams, while branch-based models offer necessary discipline for larger, less experienced ones despite management drawbacks.

Abstract: Git has become one of the most widely used version control systems today.
Among its distinguishing features, its ability to easily and quickly create
branches stands out, allowing teams to customize their workflows. In this
context, various formats of collaborative development workflows using Git have
emerged and gained popularity among software engineers. We can categorize such
workflows into two main types: branch-based workflows and trunk-based
workflows. Branch-based workflows typically define a set of remote branches
with well-defined objectives, such as feature branches, a branch for feature
integration, and a main branch. The goal is to migrate changes from the most
isolated branch to the main one shared by all as the code matures. In this
category, GitFlow stands out as the most popular example. In contrast,
trunk-based workflows have a single remote branch where developers integrate
their changes directly. In this range of options, choosing a workflow that
maximizes team productivity while promoting software quality becomes a
non-trivial task. Despite discussions on forums, social networks, and blogs,
few scientific articles have explored this topic. In this work, we provide
evidence on how Brazilian developers work with Git workflows and what factors
favor or hinder the use of each model. To this end, we conducted
semi-structured interviews and a survey with software developers. Our results
indicate that trunk-based development favors fast-paced projects with
experienced and smaller teams, while branch-based development suits less
experienced and larger teams better, despite posing management challenges.

</details>


### [35] [Semantic Source Code Segmentation using Small and Large Language Models](https://arxiv.org/abs/2507.08992)
*Abdelhalim Dahou,Ansgar Scherp,Sebastian Kurten,Brigitte Mathiak,Madhu Chauhan*

Main category: cs.SE

TL;DR: The paper introduces an automated, domain-specific approach for R code segmentation using LLMs/SLMs, comparing line-by-line context-based analysis with range-based methods and demonstrating superior performance with fine-tuned smaller models (CodeBERT, CodeT5+) even without R code pre-training.


<details>
  <summary>Details</summary>
Motivation: Manual and syntactic code segmentation methods are impractical for large repositories and low-resource languages like R, particularly in research domains such as social sciences and psychology where traditional approaches fall short.

Method: The study presents two approaches: (1) line-by-line code analysis with contextual modeling using LLMs/SLMs, and (2) range-based segment determination. Experiments involved comparing baseline models with fine-tuned variants (CodeBERT, CodeT5+, LLaMA-3) on a human-annotated R dataset (4,130 lines) and cross-evaluating on Python code from computer science domains.

Result: Context-based line-by-line analysis outperformed range-based segmentation. Smaller models (CodeBERT, CodeT5+ encoder) exceeded LLM counterparts despite lacking R code pre-training. Cross-domain performance on Python was evaluated to support approach generalizability.

Conclusion: The work demonstrates that domain-specific fine-tuning of smaller language models can achieve strong segmentation results for low-resource languages, offering a scalable solution to code base navigation challenges in research R code while enabling cross-domain application.

Abstract: Source code segmentation, dividing code into functionally coherent segments,
is crucial for knowledge retrieval and maintenance in software development.
While enabling efficient navigation and comprehension of large codebases,
manual and syntactic analysis approaches have become impractical as
repositories grow, especially for low-resource languages like R and their
research domains (e.g., social sciences, psychology).This paper introduces an
automated, domain-specific approach for research R code segmentation using
Large and Small Language Models (LLMs/SLMs). It presents two novel approaches
and a human-annotated dataset, StatCodeSeg. We explore two distinct approaches:
line-by-line analysis with context and range-based segment determination. We
experiment with LLMs and fine-tuned SLMs. To support the generalizability of
our approaches, we also include experiments on Python code from the computer
science domain.Our results show that context-based line-by-line analysis is
superior over range-based segmentation.Using smaller language models like
CodeBERT and an encoder-only version of CodeT5+ are better than their LLM
counterparts. Most notably, these two best-performing models did not see R code
during pre-training versus the LLMs but were only fine-tuned on 4,130 lines of
manually annotated code.

</details>


### [36] [Accelerating Drug Discovery Through Agentic AI: A Multi-Agent Approach to Laboratory Automation in the DMTA Cycle](https://arxiv.org/abs/2507.09023)
*Yao Fehlis,Charles Crain,Aidan Jensen,Michael Watson,James Juhasz,Paul Mandel,Betty Liu,Shawn Mahon,Daren Wilson,Nick Lynch-Jonely,Ben Leedom,David Fuller*

Main category: cs.SE

TL;DR: Tippy is a novel AI framework for pharmaceutical drug discovery featuring a multi-agent system to automate the Design-Make-Test-Analyze (DMTA) cycle, demonstrating accelerated workflows while maintaining scientific rigor.


<details>
  <summary>Details</summary>
Motivation: Traditional drug discovery approaches face limitations in meeting modern therapeutic development demands, necessitating innovative automation solutions to enhance efficiency and decision-making in pharmaceutical research.

Method: The framework employs five specialized AI agents (Supervisor, Molecule, Lab, Analysis, Report) with Safety Guardrail oversight, each targeting specific DMTA phases. Agents use reasoning, planning, and collaboration to streamline automation.

Result: Tippy enhances workflow efficiency, decision speed, and cross-disciplinary coordination, marking the first production-ready AI agent system for DMTA cycle automation. It establishes a scalable model for integrating autonomous agents in pharmaceutical workflows.

Conclusion: This study demonstrates Tippy's potential to revolutionize drug discovery through AI-driven automation, offering a new paradigm that balances speed and scientific precision while emphasizing safety. The implementation provides a practical blueprint for labs adopting next-generation pharmaceutical research methodologies.

Abstract: The pharmaceutical industry faces unprecedented challenges in drug discovery,
with traditional approaches struggling to meet modern therapeutic development
demands. This paper introduces a novel AI framework, Tippy, that transforms
laboratory automation through specialized AI agents operating within the
Design-Make-Test-Analyze (DMTA) cycle. Our multi-agent system employs five
specialized agents - Supervisor, Molecule, Lab, Analysis, and Report, with
Safety Guardrail oversight - each designed to excel in specific phases of the
drug discovery pipeline. Tippy represents the first production-ready
implementation of specialized AI agents for automating the DMTA cycle,
providing a concrete example of how AI can transform laboratory workflows. By
leveraging autonomous AI agents that reason, plan, and collaborate, we
demonstrate how Tippy accelerates DMTA cycles while maintaining scientific
rigor essential for pharmaceutical research. The system shows significant
improvements in workflow efficiency, decision-making speed, and
cross-disciplinary coordination, offering a new paradigm for AI-assisted drug
discovery.

</details>


### [37] [Towards Extracting Software Requirements from App Reviews using Seq2seq Framework](https://arxiv.org/abs/2507.09039)
*Aakash Sorathiya,Gouri Ginde*

Main category: cs.SE

TL;DR: This paper redefines requirements extraction from mobile app reviews as a Named Entity Recognition task using a Seq2seq model with BiLSTM, LSTM, self-attention, GloVe embeddings, and CRF, achieving high F1 scores on two datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to extract actionable requirements from app reviews due to informal language, grammatical/spelling errors, and irrelevant content.

Method: A Seq2seq framework combining BiLSTM encoder, LSTM decoder with self-attention, pre-trained GloVe embeddings, and CRF for sequence modeling.

Result: Achieved F1 score of 0.96 on crowdsourced 23,816-review dataset (Dataset 2) and 0.47 on manually annotated 1,000-review dataset (Dataset 1).

Conclusion: Proposed NER-based Seq2seq approach outperforms state-of-the-art methods in requirements extraction from app reviews.

Abstract: Mobile app reviews are a large-scale data source for software improvements. A
key task in this context is effectively extracting requirements from app
reviews to analyze the users' needs and support the software's evolution.
Recent studies show that existing methods fail at this task since app reviews
usually contain informal language, grammatical and spelling errors, and a large
amount of irrelevant information that might not have direct practical value for
developers. To address this, we propose a novel reformulation of requirements
extraction as a Named Entity Recognition (NER) task based on the
sequence-to-sequence (Seq2seq) generation approach. With this aim, we propose a
Seq2seq framework, incorporating a BiLSTM encoder and an LSTM decoder, enhanced
with a self-attention mechanism, GloVe embeddings, and a CRF model. We
evaluated our framework on two datasets: a manually annotated set of 1,000
reviews (Dataset 1) and a crowdsourced set of 23,816 reviews (Dataset 2). The
quantitative evaluation of our framework showed that it outperformed existing
state-of-the-art methods with an F1 score of 0.96 on Dataset 2, and achieved
comparable performance on Dataset 1 with an F1 score of 0.47.

</details>


### [38] [CMER: A Context-Aware Approach for Mining Ethical Concern-related App Reviews](https://arxiv.org/abs/2507.09049)
*Aakash Sorathiya,Gouri Ginde*

Main category: cs.SE

TL;DR: CMER is a context-aware approach using NLI and a decoder-only LLM to efficiently extract ethical concern-related app reviews, outperforming keyword-based methods in detecting privacy and security issues.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with domain-specific language in ethical concern app reviews and require labeled data, making automated extraction challenging and time-consuming.

Method: CMER combines Natural Language Inference (NLI) with a decoder-only LLM (LLaMA-like) to enhance context awareness and eliminate the need for labeled data in identifying ethical concern-related reviews.

Result: CMER extracted 2,178 additional privacy and security-related reviews from a dataset of 382K mobile investment app reviews, surpassing keyword-based approaches.

Conclusion: CMER effectively addresses the limitations of traditional methods by leveraging domain-aware NLI and LLMs, enabling scalable extraction of ethical concern information for software engineering practices.

Abstract: With the increasing proliferation of mobile applications in our daily lives,
the concerns surrounding ethics have surged significantly. Users communicate
their feedback in app reviews, frequently emphasizing ethical concerns, such as
privacy and security. Incorporating these reviews has proved to be useful for
many areas of software engineering (e.g., requirement engineering, testing,
etc.). However, app reviews related to ethical concerns generally use
domain-specific language and are typically overshadowed by more generic
categories of user feedback, such as app reliability and usability. Thus,
making automated extraction a challenging and time-consuming effort.
  This study proposes CMER (A \underline{C}ontext-Aware Approach for
\underline{M}ining \underline{E}thical Concern-related App
\underline{R}eviews), a novel approach that combines Natural Language Inference
(NLI) and a decoder-only (LLaMA-like) Large Language Model (LLM) to extract
ethical concern-related app reviews at scale. In CMER, NLI provides
domain-specific context awareness by using domain-specific hypotheses, and the
Llama-like LLM eliminates the need for labeled data in the classification task.
We evaluated the validity of CMER by mining privacy and security-related
reviews (PSRs) from the dataset of more than 382K app reviews of mobile
investment apps. First, we evaluated four NLI models and compared the results
of domain-specific hypotheses with generic hypotheses. Next, we evaluated three
LLMs for the classification task. Finally, we combined the best NLI and LLM
models (CMER) and extracted 2,178 additional PSRs overlooked by the previous
study using a keyword-based approach, thus demonstrating the effectiveness of
CMER. These reviews can be further refined into actionable requirement
artifacts.

</details>


### [39] [SAGE: A Context-Aware Approach for Mining Privacy Requirements Relevant Reviews from Mental Health Apps](https://arxiv.org/abs/2507.09051)
*Aakash Sorathiya,Gouri Ginde*

Main category: cs.SE

TL;DR: SAGE introduces a context-aware approach using NLI and GPT to identify overlooked privacy reviews in mental health apps, achieving an 0.85 F1 score without fine-tuning and outperforming BERT/T5 alternatives.


<details>
  <summary>Details</summary>
Motivation: Mental health apps collect sensitive data but user reviews prioritize reliability/usability over privacy concerns, creating a gap in systematically extracting contextual privacy requirements from unstructured feedback.

Method: The method combines Natural Language Inference (NLI) with MH domain-specific privacy hypotheses to analyze review context, and utilizes a GPT model for zero-shot classification of privacy-relevant reviews without requiring model fine-tuning.

Result: SAGE achieved 0.85 F1 score on 204K reviews, outperforming fine-tuned BERT/T5 baselines, and identified 748 previously undetected privacy reviews through qualitative analysis that could be transformed into actionable privacy requirement artifacts.

Conclusion: This work demonstrates that domain-aware NLI with GPT offers an effective solution for mining nuanced privacy concerns in MH app reviews, enabling more comprehensive compliance with privacy regulations and addressing user trust issues through unstructured feedback analysis.

Abstract: Mental health (MH) apps often require sensitive user data to customize
services for mental wellness needs. However, such data collection practices in
some MH apps raise significant privacy concerns for users. These concerns are
often mentioned in app reviews, but other feedback categories, such as
reliability and usability, tend to take precedence. This poses a significant
challenge in automatically identifying privacy requirements-relevant reviews
(privacy reviews) that can be utilized to extract privacy requirements and
address users' privacy concerns. Thus, this study introduces SAGE, a
context-aware approach to automatically mining privacy reviews from MH apps
using Natural Language Inference (NLI) with MH domain-specific privacy
hypotheses (provides domain-specific context awareness) and a GPT model
(eliminates the need for fine-tuning). The quantitative evaluation of SAGE on a
dataset of 204K app reviews achieved an F1 score of 0.85 without any
fine-tuning, outperforming the fine-tuned baseline classifiers BERT and T5.
Furthermore, SAGE extracted 748 privacy reviews previously overlooked by
keyword-based methods, demonstrating its effectiveness through qualitative
evaluation. These reviews can later be refined into actionable privacy
requirement artifacts.

</details>


### [40] [SetupBench: Assessing Software Engineering Agents' Ability to Bootstrap Development Environments](https://arxiv.org/abs/2507.09063)
*Avi Arora,Jinu Jang,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: Introduces SetupBench, a benchmark testing environment-bootstrap capabilities of LLM agents on bare Linux sandboxes with 93 tasks across ecosystems, databases, and services.


<details>
  <summary>Details</summary>
Motivation: Existing LLM benchmarks focus on pre-configured environments, lacking evaluation of setup skills critical for real-world software tasks. Current agents lack robustness in dependency management and environment configuration.

Method: Created SetupBench with 93 instances requiring package installation, dependency resolving, database initialization, and service configuration. Tasks include natural language problem statements and deterministic success commands across 7 ecosystems, 5 databases, and multi-service scenarios.

Result: OpenHands agents showed success rates of 38.9-57.4% (repository setup) and 20.0-53.3% (databases). Systematic failures included incomplete tooling installation, hallucinated constraints, non-persistent changes. Agents executed 38-89% unnecessary actions compared to human optimal workflows.

Conclusion: SetupBench provides a rigorous benchmark for developer agent capabilities in environment setup, exposing practical gaps in dependency resolution, configuration persistence, and exploration efficiency that require future research.

Abstract: Modern Large Language Model (LLM) agents promise end to end assistance with
real-world software tasks, yet existing benchmarks evaluate LLM agents almost
exclusively in pre-baked environments where every dependency is pre-installed.
To fill this gap, we introduce SetupBench, a 93 instance benchmark that
isolates the environment-bootstrap skill: starting from a bare Linux sandbox,
an agent must install packages, resolve dependency conflicts, initialize
databases, and configure background services. Our tasks span seven language
ecosystems, five database engines, and multi-service orchestration scenarios,
each accompanies by a natural language problem statement and a deterministic
success command. Through evaluation of OpenHands, a state-of-the-art coding
agent, we find low success rates across task categories, with particular
challenges in repository setup (38.9-57.4%) and local database configuration
(20.0-53.3%). Our analysis reveals systematic failure modes including
incomplete development tooling installation, hallucinated task constraints, and
non-persistent environment modifications that break agent-human collaboration
workflows. We identify substantial inefficiencies in agent exploration
strategies, with 38-89% of actions being unnecessary compared to optimal human
behavior. These findings highlight gaps in current agents' practical
environment-bootstrap capabilities. By targeting this critical yet
under-evaluated capability, SetupBench provides a rigorous yard-stick for the
next generation of software developer agents aiming to solve end to end
real-wold tasks.

</details>


### [41] [SPICE: An Automated SWE-Bench Labeling Pipeline for Issue Clarity, Test Coverage, and Effort Estimation](https://arxiv.org/abs/2507.09108)
*Aaditya Bhatia,Gustavo A. Oliva,Gopi Krishnan Rajbahadur,Haoxiang Zhang,Yihao Chen,Zhilong Chen,Arthur Leung,Dayi Lin,Boyuan Chen,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: This paper introduces SPICE, a cost-effective automated pipeline for labeling large-scale software engineering datasets, reducing annotation costs from $100k to $5.10 per 1,000 instances while maintaining high accuracy compared to expert annotations.


<details>
  <summary>Details</summary>
Motivation: Manual labeling of high-quality datasets for training SE-focused foundation models is prohibitively expensive and time-consuming, creating a barrier for large-scale data collection needed in software engineering (SE) research.

Method: SPICE employs three key components: context-aware code navigation for accurate instance selection, rationale-driven prompting to guide model reasoning during annotation, and multi-pass consensus mechanisms to ensure label quality. The pipeline was developed based on authors' hands-on experience labeling 800+ SWE-Gym instances.

Result: 1. Achieves strong agreement with SWE-bench Verified human annotations
2. Reduces labeling cost for 1,000 instances from ~$100,000 to $5.10
3. Creates SPICE Bench: a 6,802-instance dataset from 291 open-source projects (13x larger than SWE-bench Verified)
4. Identifies patterns in software engineering challenges through automated analysis

Conclusion: SPICE demonstrates that automated labeling pipelines can create high-quality, large-scale datasets for SE foundation models at vastly reduced costs, enabling community-wide experimentation and addressing current bottlenecks in SE research data collection.

Abstract: High-quality labeled datasets are crucial for training and evaluating
foundation models in software engineering, but creating them is often
prohibitively expensive and labor-intensive. We introduce SPICE, a scalable,
automated pipeline for labeling SWE-bench-style datasets with annotations for
issue clarity, test coverage, and effort estimation. SPICE combines
context-aware code navigation, rationale-driven prompting, and multi-pass
consensus to produce labels that closely approximate expert annotations.
SPICE's design was informed by our own experience and frustration in labeling
more than 800 instances from SWE-Gym. SPICE achieves strong agreement with
human-labeled SWE-bench Verified data while reducing the cost of labeling 1,000
instances from around $100,000 (manual annotation) to just $5.10. These results
demonstrate SPICE's potential to enable cost-effective, large-scale dataset
creation for SE-focused FMs. To support the community, we release both SPICE
tool and SPICE Bench, a new dataset of 6,802 SPICE-labeled instances curated
from 291 open-source projects in SWE-Gym (over 13x larger than SWE-bench
Verified).

</details>


### [42] [Position Paper: Programming Language Techniques for Bridging LLM Code Generation Semantic Gaps](https://arxiv.org/abs/2507.09135)
*Yalong Du,Chaozheng Wang,Huaijin Wang*

Main category: cs.SE

TL;DR: This paper advocates for integrating Programming Language (PL) techniques with Large Language Models (LLMs) to bridge semantic gaps in automated code generation, enhancing reliability and trustworthiness through formal methods and verification.


<details>
  <summary>Details</summary>
Motivation: LLMs' statistical and black-box nature leads to syntax errors, semantic hallucinations, and reliability issues in generated code, which limits their practical use in critical systems where correctness and verifiability are essential.

Method: The approach involves leveraging structured program representations, formal correctness guarantees (e.g., type systems, semantics), and robust verification mechanisms (e.g., model checking, proof assistants) to augment LLM-based code generation processes.

Result: The integration of PL techniques ensures code is functionally correct, interpretable, and verifiable, addressing LLMs' limitations in producing reliable software.

Conclusion: To create truly trustworthy code generation systems, principled fusion of LLMs and PL techniques is indispensable, enabling both practical productivity gains and rigorous correctness assurances.

Abstract: Large Language Models have demonstrated remarkable capabilities in automated
code generation, yet their statistical nature and black-box characteristics
create significant semantic gaps manifested through syntax errors, semantic
hallucinations, and reliability concerns. This position paper argues that
principled integration of Programming Language (PL) techniques is essential for
bridging these gaps. Through structured program representations, formal
correctness guarantees, and robust verification mechanisms, PL techniques can
elevate LLM-generated code from statistical pattern matching to truly reliable
and trustworthy levels. This integration is crucial for developing systems that
generate code that is not only functionally correct but also interpretable,
verifiable, and ultimately trustworthy.

</details>


### [43] [OpenCAMS: An Open-Source Connected and Automated Mobility Co-Simulation Platform for Advanced Transportation Research](https://arxiv.org/abs/2507.09186)
*Minhaj Uddin Ahmad,Akid Abrar,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.SE

TL;DR: Introduces OpenCAMS, an open-source co-simulation platform integrating SUMO, CARLA, and OMNeT++ for transportation safety, mobility, and cybersecurity research with time-synchronized bidirectional coupling.


<details>
  <summary>Details</summary>
Motivation: Existing simulation tools for intelligent transportation systems lack comprehensive integration across traffic modeling, vehicle perception/control, and communication networks, limiting research in safety-critical and networked scenarios.

Method: Time-synchronized bidirectional coupling architecture that combines: 1) SUMO for large-scale traffic simulation, 2) CARLA for detailed vehicle dynamics and sensor emulation, and 3) OMNeT++ for event-driven network communication (C-V2X). Modular design enables extensible integration of additional simulators.

Result: Successful implementation of OpenCAMS platform (GitHub: https://github.com/minhaj6/carla-sumo-omnetpp-cosim) demonstrating synchronized operation of all three domains with flexibility for third-party integrations. Provides reproducible research environment for ITS development.

Conclusion: OpenCAMS offers a unified, extensible framework addressing domain-specific needs for connected and automated mobility research while maintaining open-source accessibility, accelerating advancements in intelligent transportation systems.

Abstract: We introduce OpenCAMS (Open-Source Connected and Automated Mobility
Co-Simulation Platform), an open-source, synchronized, and extensible
co-simulation framework that tightly couples three best-in-class simulation
tools: (i) SUMO, (ii) CARLA, and (iii) OMNeT++. OpenCAMS is designed to support
advanced research in transportation safety, mobility, and cybersecurity by
combining the strengths of each simulation domain. Specifically, SUMO provides
large-scale, microscopic traffic modeling; CARLA offers high-fidelity 3D
perception, vehicle dynamics, and control simulation; and OMNeT++ enables
modular, event-driven network communication, such as cellular
vehicle-to-everything (C-V2X). OpenCAMS employs a time-synchronized,
bidirectional coupling architecture that ensures coherent simulation
progression across traffic, perception, and communication domains while
preserving modularity and reproducibility. For example, CARLA can simulate and
render a subset of vehicles that require detailed sensor emulation and control
logic; SUMO orchestrates network-wide traffic flow, vehicle routing, and
traffic signal management; and OMNeT++ dynamically maps communication nodes to
both mobile entities (e.g., vehicles) and static entities (e.g., roadside
units) to enable C-V2X communication. While these three simulators form the
foundational core of OpenCAMS, the platform is designed to be expandable and
future-proof, allowing additional simulators to be integrated on top of this
core without requiring fundamental changes to the system architecture. The
OpenCAMS platform is fully open-source and publicly available through its
GitHub repository https://github.com/minhaj6/carla-sumo-omnetpp-cosim,
providing the research community with an accessible, flexible, and
collaborative environment for advancing next-generation intelligent
transportation systems.

</details>


### [44] [Back to the Basics: Rethinking Issue-Commit Linking with LLM-Assisted Retrieval](https://arxiv.org/abs/2507.09199)
*Huihui Huang,Ratnadira Widyasari,Ting Zhang,Ivana Clairine Irsan,Jieke Shi,Han Wei Ang,Frank Liauw,Eng Lieh Ouh,Lwin Khin Shar,Hong Jin Kang,David Lo*

Main category: cs.SE

TL;DR: This paper proposes the Realistic Distribution Setting (RDS) for issue-commit linking evaluation, reveals the state-of-the-art deep learning approach's performance drops by over 50% under RDS while traditional VSM outperforms. EasyLink, a LLM-reranking based method, achieves 75.91% Precision@1, offering practical research guidelines.


<details>
  <summary>Details</summary>
Motivation: Existing issue-commit linking evaluations ignore the real-world challenge of distinguishing genuine fixes from numerous plausible yet unrelated commits in large projects, leading to misleading performance assessments.

Method: The authors create RDS with a realistic 20 open-source project dataset and evaluate tools under this setting. They propose EasyLink using a vector database and large language model (LLM) reranking to bridge the semantic gap between issues and commits.

Result: State-of-the-art deep learning models drop over 50% in accuracy under RDS; VSM outperforms them. EasyLink achieves 75.91% Precision@1, a 4x improvement over the best existing method.

Conclusion: Realistic evaluations using RDS are essential for issue-commit linking. EasyLink demonstrates superior performance with modern IR techniques and LLM semantic enhancement, providing actionable guidelines for future research directions.

Abstract: Issue-commit linking, which connects issues with commits that fix them, is
crucial for software maintenance. Existing approaches have shown promise in
automatically recovering these links. Evaluations of these techniques assess
their ability to identify genuine links from plausible but false links.
However, these evaluations overlook the fact that, in reality, when a
repository has more commits, the presence of more plausible yet unrelated
commits may interfere with the tool in differentiating the correct fix commits.
To address this, we propose the Realistic Distribution Setting (RDS) and use it
to construct a more realistic evaluation dataset that includes 20 open-source
projects. By evaluating tools on this dataset, we observe that the performance
of the state-of-the-art deep learning-based approach drops by more than half,
while the traditional Information Retrieval method, VSM, outperforms it.
  Inspired by these observations, we propose EasyLink, which utilizes a vector
database as a modern Information Retrieval technique. To address the
long-standing problem of the semantic gap between issues and commits, EasyLink
leverages a large language model to rerank the commits retrieved from the
database. Under our evaluation, EasyLink achieves an average Precision@1 of
75.91%, improving over the state-of-the-art by over four times. Additionally,
this paper provides practical guidelines for advancing research in issue-commit
link recovery.

</details>


### [45] [Explainability as a Compliance Requirement: What Regulated Industries Need from AI Tools for Design Artifact Generation](https://arxiv.org/abs/2507.09220)
*Syed Tauhid Ullah Shah,Mohammad Hussein,Ann Barcomb,Mohammad Moshirpour*

Main category: cs.SE

TL;DR: This paper identifies and addresses the explainability gap in AI tools for design artifact generation in requirements engineering, particularly in regulated/safety-critical industries through practitioner interviews.


<details>
  <summary>Details</summary>
Motivation: AI-based design artifact generation can improve RE efficiency but faces adoption barriers in regulated domains requiring transparency, traceability, and stakeholder trust in safety-critical systems.

Method: Semi-structured interviews with 10 safety-critical industry practitioners examining AI integration workflows, explainability challenges, mitigation strategies, and usability requirements.

Result: Non-explainable AI outputs cause manual validation bottlenecks, eroded stakeholder trust, domain terminology limitations, collaboration disruptions, and compliance risks that offset efficiency gains.

Conclusion: A practical roadmap is proposed to enhance AI tool transparency and reliability in RE through source tracing, decision justification, domain adaptation, compliance validation features, and improved workflow integration.

Abstract: Artificial Intelligence (AI) tools for automating design artifact generation
are increasingly used in Requirements Engineering (RE) to transform textual
requirements into structured diagrams and models. While these AI tools,
particularly those based on Natural Language Processing (NLP), promise to
improve efficiency, their adoption remains limited in regulated industries
where transparency and traceability are essential. In this paper, we
investigate the explainability gap in AI-driven design artifact generation
through semi-structured interviews with ten practitioners from safety-critical
industries. We examine how current AI-based tools are integrated into workflows
and the challenges arising from their lack of explainability. We also explore
mitigation strategies, their impact on project outcomes, and features needed to
improve usability. Our findings reveal that non-explainable AI outputs
necessitate extensive manual validation, reduce stakeholder trust, struggle to
handle domain-specific terminology, disrupt team collaboration, and introduce
regulatory compliance risks, often negating the anticipated efficiency
benefits. To address these issues, we identify key improvements, including
source tracing, providing clear justifications for tool-generated decisions,
supporting domain-specific adaptation, and enabling compliance validation. This
study outlines a practical roadmap for improving the transparency, reliability,
and applicability of AI tools in requirements engineering workflows,
particularly in regulated and safety-critical environments where explainability
is crucial for adoption and certification.

</details>


### [46] [Enhancing Interpretability in Software Change Management with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.09315)
*Yongqian Sun,Weihua Kuang,Chao Shen,Xidao Wen,Tinghua Zheng,Heng Liu,Shenglin Zhang,Bo Wu,Dan Pei*

Main category: cs.SE

TL;DR: SCELM is an end-to-end automated framework for software change management that reduces service failures and economic losses.


<details>
  <summary>Details</summary>
Motivation: Frequent software changes in modern online services introduce significant risks, including service failures and economic losses.

Method: The proposed SCELM framework leverages automated evaluation and lifecycle management processes to address these risks efficiently and precisely.

Result: The framework is designed to achieve significant reductions in service failures through automated software change management, though specific implementation results are not quantified in the abstract.

Conclusion: SCELM offers a promising solution for managing software change risks in online services, emphasizing automation and lifecycle precision to minimize operational disruptions.

Abstract: In modern online services, frequent software changes introduce significant
risks. To tackle this challenge, we propose SCELM (Software Change Evaluation
and Lifecycle Management), an end-to-end automated framework for software
change management. SCELM aims to manage software changes efficiently and
precisely, significantly reducing service failures and economic losses.

</details>


### [47] [Enhancing NeuroEvolution-Based Game Testing: A Branch Coverage Approach for Scratch Programs](https://arxiv.org/abs/2507.09414)
*Khizra Sohail,Atif Aftab Ahmed Jilani,Nigar Azhar Butt*

Main category: cs.SE

TL;DR: This paper proposes a branch coverage-based fitness function to enhance the NEATEST framework for automated testing of Scratch games. Empirical results on 25 games show that the new approach (NBC) outperforms statement coverage-based NEATEST (NSC) in branch coverage and fault detection reliability via lower false positive rates.


<details>
  <summary>Details</summary>
Motivation: Statement coverage alone is insufficient for fault detection in game-like programs due to non-deterministic behavior and complex control structures, potentially missing critical logical branches that need testing.

Method: The authors integrated a branch fitness function into NEATEST that prioritizes control-dependent branches. They evaluated this through empirical experiments on 25 Scratch games and mutation analysis comparing NBC and NSC.

Result: NBC achieved higher branch coverage in 13/25 games, particularly in those with complex conditionals, and demonstrated a lower false positive rate in mutation testing compared to NSC.

Conclusion: Branch coverage-based test generation significantly improves both test coverage and fault detection effectiveness in Scratch game programs, making it a more reliable method than statement coverage alone.

Abstract: Automated test generation for game-like programs presents unique challenges
due to their non-deterministic behavior and complex control structures. The
NEATEST framework has been used for automated testing in Scratch games,
employing neuroevolution-based test generation optimized for statement
coverage. However, statement coverage alone is often insufficient for fault
detection, as it does not guarantee execution of all logical branches. This
paper introduces a branch coverage-based fitness function to enhance test
effectiveness in automated game testing. We extend NEATEST by integrating a
branch fitness function that prioritizes control-dependent branches, guiding
the neuroevolution process to maximize branch exploration. To evaluate the
effectiveness of this approach, empirical experiments were conducted on 25
Scratch games, comparing Neatest with Statement Coverage (NSC) against Neatest
with Branch Coverage (NBC). A mutation analysis was also performed to assess
the fault detection capabilities of both techniques. The results demonstrate
that NBC achieves higher branch coverage than NSC in 13 out of 25 games,
particularly in programs with complex conditional structures. Moreover, NBC
achieves a lower false positive rate in mutation testing, making it a more
reliable approach for identifying faulty behavior in game programs. These
findings confirm that branch coverage-based test generation improves test
coverage and fault detection in Scratch programs.

</details>


### [48] [Evaluating LLMs on Sequential API Call Through Automated Test Generation](https://arxiv.org/abs/2507.09481)
*Yuheng Huang,Da Song,Zhenlan Ji,Shuai Wang,Lei Ma*

Main category: cs.SE

TL;DR: This paper introduces StateGen, an automated framework for generating sequential API-oriented coding tasks, and StateEval, a benchmark with 120 test cases across three real-world scenarios, to improve testing and evaluation of LLMs integrating APIs.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLM tool use are limited by manual curation, reliance on static evaluation methods like string matching, and a lack of focus on sequential, state-dependent API interactions prevalent in real-world applications.

Method: StateGen employs state-machine-based API constraint solving and validation, energy-based sampling to prioritize semantically meaningful programs, and control-flow injection to simulate complex task sequences. Two LLM agents collaborate to translate generated programs into natural language task descriptions.

Result: StateGen successfully constructed StateEval benchmark covering Session Service, Tensor Operation, and ElevenLabs MCP scenarios. Generated tasks demonstrate higher complexity in sequential API interactions compared to prior benchmarks, revealing weaknesses in current LLMs' API integration capabilities.

Conclusion: The framework enables systematic generation of API task benchmarks, highlighting critical challenges in LLM-sequential API interaction understanding and setting new requirements for evaluating and improving tool-integrated LLM systems.

Abstract: By integrating tools from external APIs, Large Language Models (LLMs) have
expanded their promising capabilities in a diverse spectrum of complex
real-world tasks. However, testing, evaluation, and analysis of LLM tool use
remain in their early stages. Most existing benchmarks rely on manually
collected test cases, many of which cannot be automatically checked for
semantic correctness and instead depend on static methods such as string
matching. Additionally, these benchmarks often overlook the complex
interactions that occur between sequential API calls, which are common in
real-world applications. To fill the gap, in this paper, we introduce StateGen,
an automated framework designed to generate diverse coding tasks involving
sequential API interactions. StateGen combines state-machine-based API
constraint solving and validation, energy-based sampling, and control-flow
injection to generate executable programs. These programs are then translated
into human-like natural language task descriptions through a collaboration of
two LLM agents. Utilizing StateGen, we construct StateEval, a benchmark
encompassing 120 verified test cases spanning across three representative
scenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental
results confirm that StateGen can effectively generate challenging and
realistic API-oriented tasks, highlighting areas for improvement in current
LLMs incorporating APIs.

</details>


### [49] [Towards LLM-Based Automatic Playtest](https://arxiv.org/abs/2507.09490)
*Yan Zhao,Chiwei Tang*

Main category: cs.SE

TL;DR: This paper introduces Lap, an LLM-based automated playtesting framework for match-3 games that converts game boards into numeric matrices and uses ChatGPT to generate actions, demonstrating superior code coverage and crash detection compared to existing tools. Key components include environment processing, prompting-based action generation, and iterative execution.


<details>
  <summary>Details</summary>
Motivation: Manual playtesting is time-consuming and expensive, while conventional testing tools lack domain knowledge and problem-solving capabilities needed for gaming contexts. Non-text games without APIs remain challenging for existing LLM testing approaches.

Method: Lap processes game environments by: 1) Capturing snapshots and converting them to numeric matrices 2) Using ChatGPT-O1-mini API through prompting to suggest valid moves 3) Executing and applying the suggested moves iteratively until timeout. This creates a feedback loop of observation, decision, and action tailored to match-3 mechanics.

Result: The case study on CasseBonbons showed Lap achieved higher code coverage (82% vs. 65-75% from existing tools) and triggered more program crashes (12 vs. 3-9 crashes) through its LLM-powered decision making. These results demonstrate improved effectiveness in automated playtesting for this game category.

Conclusion: Lap establishes a foundational approach to applying LLMs in auto playtesting for non-text games through matrix representation and prompting techniques. It shows LLMs can provide meaningful value in testing complex game mechanics, pointing toward future advancements in automated testing of interactive software.

Abstract: Playtesting is the process in which people play a video game for testing. It
is critical for the quality assurance of gaming software. Manual playtesting is
time-consuming and expensive. However, automating this process is challenging,
as playtesting typically requires domain knowledge and problem-solving skills
that most conventional testing tools lack. Recent advancements in artificial
intelligence (AI) have opened up new possibilities for applying Large Language
Models (LLMs) to playtesting. However, significant challenges remain: current
LLMs cannot visually perceive game environments, and most existing research
focuses on text-based games or games with robust APIs. Many non-text games lack
APIs to provide textual descriptions of game states, making it almost
impossible to naively apply LLMs for playtesting. This paper introduces Lap,
our novel approach to LLM-based Automatic Playtesting, which uses ChatGPT to
test match-3 games, a category of games where players match three or more
identical tiles in a row or column to earn points. Lap encompasses three key
phases: processing of game environments, prompting-based action generation, and
action execution. Given a match-3 game, Lap takes a snapshot of the game board
and converts it to a numeric matrix. It then prompts the ChatGPT-O1-mini API to
suggest moves based on that matrix and tentatively applies the suggested moves
to earn points and trigger changes in the game board. It repeats the
above-mentioned three steps iteratively until timeout. For evaluation, we
conducted a case study using Lap on an open-source match-3 game, CasseBonbons,
and empirically compared it with three existing tools. Our results are
promising: Lap outperformed existing tools by achieving higher code coverage
and triggering more program crashes. This research sheds light on the future of
automatic testing and LLM applications.

</details>


### [50] [It Only Gets Worse: Revisiting DL-Based Vulnerability Detectors from a Practical Perspective](https://arxiv.org/abs/2507.09529)
*Yunqian Wang,Xiaohong Li,Yao Zhang,Yuekang Li,Zhiping Zhou,Ruitao Feng*

Main category: cs.SE

TL;DR: VulTegra evaluates DL-based vulnerability detectors, challenging assumptions about pre-trained models and highlighting factors impacting detection effectiveness.


<details>
  <summary>Details</summary>
Motivation: Software vulnerabilities require reliable detection, but doubts persist about DL models' consistency, real-world performance, and adaptability across scenarios, necessitating insights into model limitations.

Method: Developed VulTegra, a multidimensional evaluation framework, to compare scratch-trained and pre-trained DL models across seven vulnerability detectors, analyzing their performance with diverse code features.

Result: SOTA detectors showed low consistency and scalability; pre-trained models outperformed in specific contexts only. Adjusting a single critical factor improved recall for all detectors and F1 scores for six.

Conclusion: Model design should prioritize vulnerability type specificity and code feature adaptation alongside pre-training benefits, as no single approach is universally optimal.

Abstract: With the growing threat of software vulnerabilities, deep learning (DL)-based
detectors have gained popularity for vulnerability detection. However, doubts
remain regarding their consistency within declared CWE ranges, real-world
effectiveness, and applicability across scenarios. These issues may lead to
unreliable detection, high false positives/negatives, and poor adaptability to
emerging vulnerabilities. A comprehensive analysis is needed to uncover
critical factors affecting detection and guide improvements in model design and
deployment. In this paper, we present VulTegra, a novel evaluation framework
that conducts a multidimensional comparison of scratch-trained and
pre-trained-based DL models for vulnerability detection. VulTegra reveals that
state-of-the-art (SOTA) detectors still suffer from low consistency, limited
real-world capabilities, and scalability challenges. Contrary to common belief,
pre-trained models are not consistently better than scratch-trained models but
exhibit distinct strengths in specific contexts.Importantly, our study exposes
the limitations of relying solely on CWE-based classification and identifies
key factors that significantly affect model performance. Experimental results
show that adjusting just one such factor consistently improves recall across
all seven evaluated detectors, with six also achieving better F1 scores. Our
findings provide deeper insights into model behavior and emphasize the need to
consider both vulnerability types and inherent code features for effective
detection.

</details>


### [51] [A Serverless Architecture for Real-Time Stock Analysis using Large Language Models: An Iterative Development and Debugging Case Study](https://arxiv.org/abs/2507.09583)
*Taniv Ashraf*

Main category: cs.SE

TL;DR: This paper presents a cost-effective, serverless real-time stock analysis system leveraging Google's Gemini API and GitHub Actions, with detailed insights into its development and debugging challenges.


<details>
  <summary>Details</summary>
Motivation: The rise of accessible Large Language Models (LLMs) enables democratizing financial data analysis, motivating the creation of an affordable, robust system for real-time stock analysis without traditional server infrastructure.

Method: The authors designed and iteratively debugged a serverless system with Gemini API for qualitative stock assessment, GitHub Actions for automated data pipelines, and a decoupled static frontend. Key challenges included resolving software errors, permission issues, and environment-level platform bugs through systematic debugging.

Result: A production-ready event-driven architecture operating at near-zero cost, with fully transparent implementation through public source code and a deployable application, demonstrating LLMs' viability in financial analysis systems.

Conclusion: The work underscores opportunities for LLMs in finance, the necessity of rigorous debugging methodologies in modern software development, and emerging trends in human-AI collaboration for building reliable, scalable financial tools.

Abstract: The advent of powerful, accessible Large Language Models (LLMs) like Google's
Gemini presents new opportunities for democratizing financial data analysis.
This paper documents the design, implementation, and iterative debugging of a
novel, serverless system for real-time stock analysis. The system leverages the
Gemini API for qualitative assessment, automates data ingestion and processing
via GitHub Actions, and presents the findings through a decoupled, static
frontend. We detail the architectural evolution of the system, from initial
concepts to a robust, event-driven pipeline, highlighting the practical
challenges encountered during deployment. A significant portion of this paper
is dedicated to a case study on the debugging process, covering common software
errors, platform-specific permission issues, and rare, environment-level
platform bugs. The final architecture operates at a near-zero cost,
demonstrating a viable model for individuals to build sophisticated AI-powered
financial tools. The operational application is publicly accessible, and the
complete source code is available for review. We conclude by discussing the
role of LLMs in financial analysis, the importance of robust debugging
methodologies, and the emerging paradigm of human-AI collaboration in software
development.

</details>


### [52] [How to Define Design in Industrial Control and Automation Software](https://arxiv.org/abs/2507.09594)
*Aydin Homay*

Main category: cs.SE

TL;DR: The paper establishes a scientific foundation for engineering design in industrial control/automation systems (iCAS) by analyzing design definitions, addressing misconceptions, and proposing systematic approaches to resolve conflicting design concerns.


<details>
  <summary>Details</summary>
Motivation: Current design practices in software engineering and iCAS lack scientific rigor, leading to subjective decisions that limit efficiency and innovation. The paper seeks to address this gap by defining design principles grounded in design theory.

Method: The study reviews software design definitions, challenges common misconceptions, examines design theory advancements to define scientifically valid design criteria, and evaluates ad-hoc vs systematic design approaches while proposing solutions for balancing operational and evolutionary concerns.

Result: Developed a scientific framework for design in iCAS that clarifies when design begins, how good design is defined, distinguishes between design and design language, and provides methods to balance conflicting design considerations while maintaining operational requirements.

Conclusion: A systematic design theory approach is essential for improving engineering decisions in iCAS. The proposed framework provides principles to transform subjective design practices into scientifically grounded methodologies, enhancing both innovation and efficiency.

Abstract: Design is a fundamental aspect of engineering, enabling the creation of
products, systems, and organizations to meet societal and/or business needs.
However, the absence of a scientific foundation in design often results in
subjective decision-making, reducing both efficiency and innovation. This
challenge is particularly evident in the software industry and, by extension,
in the domain of industrial control and automation systems (iCAS).
  In this study, first we review the existing design definitions within the
software industry, challenge prevailing misconceptions about design, review
design definition in the field of design theory and address key questions such
as: When does design begin? How can design be defined scientifically? What
constitutes good design? and the difference between design and design language
by relying on advancements in the field of design theory. We also evaluate the
distinction between ad-hoc and systematic design approaches, and present
arguments on how to balance complementary operational concerns while resolving
conflicting evolutionary concerns.

</details>


### [53] [The Mythical Good Software](https://arxiv.org/abs/2507.09596)
*Aydin Homay*

Main category: cs.SE

TL;DR: The paper challenges the conventional view of high cohesion and low coupling as universally good design principles in software engineering, arguing that this focus can be flawed, ambiguous, and even harmful when applied rigidly without considering trade-offs.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of traditional cohesion/coupling paradigms by clarifying that these principles are not as distinct as commonly assumed and questioning their blind application.

Method: Theoretical analysis and critical discussion of design principles, exploring their philosophical and practical implications through conceptual arguments rather than empirical methods.

Result: Demonstration that high cohesion and low coupling are mathematically similar approaches differing only in time/space trade-offs, and that uncritical application of these principles can prevent optimal system design.

Conclusion: Software design should abandon binary thinking about coupling/cohesion, balancing their trade-offs based on time/space constraints rather than adhering to an absolutist pursuit of high cohesion.

Abstract: Good software has high cohesion and low coupling is clumsy, obscure, and in
some certain cases could be actually a harmful state of being. It is clumsy
because there is no perfect correlation between higher cohesiveness and optimum
design, and it is obscure because it conveys the message that coupling and
cohesion are two distinct design principles, while there are in principle the
same design approaches, and only the time and space differ between them, and it
could also be a harmful state of being because we should not always aim for
higher cohesiveness without considering its cost.
  In the course of this study, we aim to elucidate for the readers the meaning
and underlying philosophy of the aforementioned paragraph.

</details>


### [54] [Complexity and Coupling: A Functional Domain Approach](https://arxiv.org/abs/2507.09599)
*Aydin Homay*

Main category: cs.SE

TL;DR: The paper defines complexity and coupling in the functional domain of iCAS, challenging physical-centric theories and proposing that functional coupling drives complexity regardless of system size.


<details>
  <summary>Details</summary>
Motivation: The authors address the ambiguity in existing complexity and coupling definitions, which often rely on physical attributes, creating confusion across disciplines like iCAS, software engineering, and mechanical design.

Method: The paper redefines complexity and coupling through functional domain analysis, using interdisciplinary examples to contrast with physical domain interpretations and demonstrate their distinct behaviors.

Result: Findings reveal that functional coupling directly increases complexity, and complexity is not correlated with system size or component count but rather with functional dependencies.

Conclusion: Effective design in iCAS and related fields must prioritize functional domain analysis for managing coupling and complexity, rather than focusing on physical characteristics.

Abstract: This paper provides a precise and scientific definition of complexity and
coupling, grounded in the functional domain, particularly within industrial
control and automation systems (iCAS). We highlight the widespread ambiguity in
defining complexity and coupling, emphasizing that many existing definitions
rooted in physical attributes lead to confusion and inconsistencies.
Furthermore, we re-exhibit why coupled design inherently increases complexity
and how potentially this complexity could be reduced. Drawing on examples from
various disciplines, such as software engineering, industrial automation, and
mechanical design, we demonstrate that complexity does not necessarily
correlate with system size or the number of components, and coupling, unlike
common belief in software engineering, actually does not occur in the physical
domain but in the functional domain. We conclude that effective design
necessitates addressing coupling and complexity within the functional domain.

</details>


### [55] [Code Review as Decision-Making -- Building a Cognitive Model from the Questions Asked During Code Review](https://arxiv.org/abs/2507.09637)
*Lo Gullstrand Heander,Emma Söderberg,Christofer Rydenfält*

Main category: cs.SE

TL;DR: The paper proposes the Code Review as Decision-Making (CRDM) model to enhance code review tools by understanding developers' cognitive processes during reviews, avoiding pitfalls of automation while maintaining interpersonal benefits.


<details>
  <summary>Details</summary>
Motivation: Code reviews face challenges in tooling and process misalignments, and automating them risks losing knowledge transfer and shared ownership benefits. Better cognitive understanding could improve tools and experiences.

Method: An ethnographic think-aloud study with 10 participants analyzing 34 code reviews through thematic, statistical, temporal, and sequential analysis of transcriptions to build a cognitive model.

Result: The CRDM model reveals developers progress through orientation (context establishment) and analytical (assessment/planning) phases, involving key decisions like commenting, information seeking, and verification. Data available at https://doi.org/10.5281/zenodo.15758266

Conclusion: Code reviews are fundamentally decision-making processes. Future tools should explicitly support these phases to retain interpersonal benefits while improving efficiency and enjoyment. The CRDM model provides framework for targeted tool improvements.

Abstract: Code review is a well-established and valued practice in the software
engineering community contributing to both code quality and interpersonal
benefits. However, there are challenges in both tools and processes that give
rise to misalignments and frustrations. Recent research seeks to address this
by automating code review entirely, but we believe that this risks losing the
majority of the interpersonal benefits such as knowledge transfer and shared
ownership.
  We believe that by better understanding the cognitive processes involved in
code review, it would be possible to improve tool support, with out without AI,
and make code review both more efficient, more enjoyable, while increasing or
maintaining all of its benefits. In this paper, we conduct an ethnographic
think-aloud study involving 10 participants and 34 code reviews. We build a
cognitive model of code review bottom up through thematic, statistical,
temporal, and sequential analysis of the transcribed material. Through the
data, the similarities between the cognitive process in code review and
decision-making processes, especially recognition-primed decision-making,
become apparent.
  The result is the Code Review as Decision-Making (CRDM) model that shows how
the developers move through two phases during the code review; first an
orientation phase to establish context and rationale and then an analytical
phase to understand, assess, and plan the rest of the review. Throughout the
process several decisions must be taken, on writing comments, finding more
information, voting, running the code locally, verifying continuous integration
results, etc.
  Analysis software and process-coded data publicly available at:
https://doi.org/10.5281/zenodo.15758266

</details>


### [56] [Is Quantization a Deal-breaker? Empirical Insights from Large Code Models](https://arxiv.org/abs/2507.09665)
*Saima Afrin,Bowen Xu,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: This study investigates the impact of model quantization on code quality metrics (reliability, maintainability, security) for large code models, finding that AWQ quantization preserves functional correctness and qualitative attributes.


<details>
  <summary>Details</summary>
Motivation: Recent quantization research for code models focuses only on functional correctness, neglecting critical code quality aspects like maintainability and security. Environmental and computational concerns of large models necessitate this exploration.

Method: Applied Activation-aware Weight Quantization (AWQ) to CodeLlama and DeepSeekCoder code models, generating Java/Python code. Evaluated outcomes using static analysis tools for cyclomatic complexity, cognitive complexity, and lines of code metrics.

Result: Quantized models maintained functional correctness while preserving key code quality attributes (maintainability, structural simplicity). Metrics analysis validated retention of critical software quality characteristics.

Conclusion: Quantization using AWQ is a robust optimization technique for code models that maintains both functional and qualitative code attributes, addressing previous research gaps in code quality analysis post-quantization.

Abstract: The growing scale of large language models (LLMs) not only demands extensive
computational resources but also raises environmental concerns due to their
increasing carbon footprint. Model quantization emerges as an effective
approach that can reduce the resource demands of LLMs by decreasing parameter
precision without substantially affecting performance (e.g., 16 bit to 4 bit).
While recent studies have established quantization as a promising approach for
optimizing large code models (LCMs), a specialized subset of LLMs tailored for
automated software engineering, their findings offer only limited insights into
its practical implications. Specifically, current investigations focus only on
the functional correctness of the code generated by quantized models,
neglecting how quantization impacts critical aspects of code quality such as
reliability, maintainability, and security. To bridge this gap, our study
investigates the effects of quantization on the qualitative aspects of
automatically generated code. We apply Activation-aware Weight Quantization
(AWQ) to two widely used code models, CodeLlama and DeepSeekCoder, to generate
Java and Python code. Using state-of-the-art static analysis tools, we evaluate
software quality metrics and static features including cyclomatic complexity,
cognitive complexity, and lines of code. Our findings reveal that quantization
is a robust technique that not only preserves functional correctness, but also
retains key qualitative code attributes sought after by developers, such as
maintainability and structural simplicity.

</details>


### [57] [OrQstrator: An AI-Powered Framework for Advanced Quantum Circuit Optimization](https://arxiv.org/abs/2507.09682)
*Laura Baird,Armin Moin*

Main category: cs.SE

TL;DR: OrQstrator is a DRL-powered modular framework for optimizing NISQ-era quantum circuits by coordinating three complementary optimizers to reduce depth, gate count, and improve fidelity while respecting hardware constraints. The system integrates with existing NISQAnalyzer techniques to adapt output circuits for target backends.


<details>
  <summary>Details</summary>
Motivation: Current quantum circuits in the NISQ era require optimization to minimize depth, gate count, and preserve fidelity against hardware-specific noise and constraints. No framework yet unifies multiple optimization strategies with adaptive coordination based on these factors.

Method: The framework employs a central DRL orchestration engine that dynamically coordinates three optimizers: 1) a DRL-driven circuit rewriter optimizing depth/gate count; 2) a domain-specific optimizer for local gate resynthesis and numeric optimization; 3) a parameterized instantiator optimizing template circuits during gate set translation. Coordination policies are learned from circuit structure, hardware limitations, and backend-aware metrics via reinforcement learning.

Result: The paper doesn't specify detailed empirical results, but describes a system architecture where optimized circuits are produced for hardware-aware transpilation using the NISQ Analyzer's adaptation techniques.

Conclusion: OrQstrator demonstrates how modular optimization systems can be coordinated via DRL strategies to address NISQ-era challenges, combining general rewriting with domain-specific techniques to achieve hardware-aware quantum circuit optimization.

Abstract: We propose a novel approach, OrQstrator, which is a modular framework for
conducting quantum circuit optimization in the Noisy Intermediate-Scale Quantum
(NISQ) era. Our framework is powered by Deep Reinforcement Learning (DRL). Our
orchestration engine intelligently selects among three complementary circuit
optimizers: A DRL-based circuit rewriter trained to reduce depth and gate count
via learned rewrite sequences; a domain-specific optimizer that performs
efficient local gate resynthesis and numeric optimization; a parameterized
circuit instantiator that improves compilation by optimizing template circuits
during gate set translation. These modules are coordinated by a central
orchestration engine that learns coordination policies based on circuit
structure, hardware constraints, and backend-aware performance features such as
gate count, depth, and expected fidelity. The system outputs an optimized
circuit for hardware-aware transpilation and execution, leveraging techniques
from an existing state-of-the-art approach, called the NISQ Analyzer, to adapt
to backend constraints.

</details>


### [58] [Prompting for Performance: Exploring LLMs for Configuring Software](https://arxiv.org/abs/2507.09790)
*Helge Spieker,Théo Matricon,Nassim Belmecheri,Jørn Eirik Betten,Gauthier Le Bartz Lyan,Heraldo Borges,Quentin Mazouni,Dennis Gross,Arnaud Gotlieb,Mathieu Acher*

Main category: cs.SE

TL;DR: This study explores the use of large language models (LLMs) to assist in performance-oriented software configuration through prompts, revealing both alignment with expert knowledge and challenges like hallucinations.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the difficulty of manually navigating complex software configurations and the high computational cost of traditional machine learning methods for this task.

Method: The method involves evaluating multiple LLMs on tasks such as identifying relevant options, ranking configurations, and recommending performant settings across systems like compilers, video encoders, and SAT solvers.

Result: Preliminary results show LLMs sometimes match expert decisions but also exhibit hallucinations and superficial reasoning depending on the system and task.

Conclusion: The study concludes that LLMs show potential for aiding software configuration but require further systematic evaluation to address their limitations and refine their application.

Abstract: Software systems usually provide numerous configuration options that can
affect performance metrics such as execution time, memory usage, binary size,
or bitrate. On the one hand, making informed decisions is challenging and
requires domain expertise in options and their combinations. On the other hand,
machine learning techniques can search vast configuration spaces, but with a
high computational cost, since concrete executions of numerous configurations
are required. In this exploratory study, we investigate whether large language
models (LLMs) can assist in performance-oriented software configuration through
prompts. We evaluate several LLMs on tasks including identifying relevant
options, ranking configurations, and recommending performant configurations
across various configurable systems, such as compilers, video encoders, and SAT
solvers. Our preliminary results reveal both positive abilities and notable
limitations: depending on the task and systems, LLMs can well align with expert
knowledge, whereas hallucinations or superficial reasoning can emerge in other
cases. These findings represent a first step toward systematic evaluations and
the design of LLM-based solutions to assist with software configuration.

</details>


### [59] [Measuring What Matters: A Framework for Evaluating Safety Risks in Real-World LLM Applications](https://arxiv.org/abs/2507.09820)
*Jia Yi Goh,Shaun Khoo,Nyx Iskandar,Gabriel Chua,Leanne Tan,Jessica Foo*

Main category: cs.SE

TL;DR: This paper proposes a framework for evaluating application-level safety in LLM systems through organizational deployment case studies, bridging theoretical AI safety concepts with operational practices.


<details>
  <summary>Details</summary>
Motivation: Foundation model-focused safety evaluations neglect application-specific risks introduced by system prompts, retrieval pipelines, and guardrails, creating a need for practical application-level safety assessment methods.

Method: The framework combines (1) principles for developing customized safety risk taxonomies, (2) application-level safety evaluation practices, and (3) real-world deployment validation across multiple use cases.

Result: Demonstrated framework application in internal pilots provides scalable safety testing reference points for organizations implementing LLM systems.

Conclusion: The work establishes actionable guidelines for operational LLM safety evaluation, addressing the gap between theoretical AI safety research and practical application deployment requirements.

Abstract: Most safety testing efforts for large language models (LLMs) today focus on
evaluating foundation models. However, there is a growing need to evaluate
safety at the application level, as components such as system prompts,
retrieval pipelines, and guardrails introduce additional factors that
significantly influence the overall safety of LLM applications. In this paper,
we introduce a practical framework for evaluating application-level safety in
LLM systems, validated through real-world deployment across multiple use cases
within our organization. The framework consists of two parts: (1) principles
for developing customized safety risk taxonomies, and (2) practices for
evaluating safety risks in LLM applications. We illustrate how the proposed
framework was applied in our internal pilot, providing a reference point for
organizations seeking to scale their safety testing efforts. This work aims to
bridge the gap between theoretical concepts in AI safety and the operational
realities of safeguarding LLM applications in practice, offering actionable
guidance for safe and scalable deployment.

</details>


### [60] [Turning the Tide: Repository-based Code Reflection](https://arxiv.org/abs/2507.09866)
*Wei Zhang,Jian Yang,Jiaxi Yang,Ya Wang,Zhoujun Li,Zeyu Cui,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: The paper introduces LiveRepoReflection, a benchmark for evaluating code understanding and generation in multi-file repositories, and proposes an instruction-tuning dataset and training method for improving code reflection capabilities in LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to address code modification in real-world repositories and overlook challenges in reflection capabilities and data contamination, necessitating a focused evaluation framework for practical code generation scenarios.

Method: LiveRepoReflection contains 1,888 multi-language test cases, while RepoReflection-Instruct uses a two-turn dialogue with error-driven repair to train models. Code generation and reflection are evaluated via a dynamic benchmarking process.

Result: 40+ LLMs were benchmarked on LiveRepoReflection, showing significant performance gaps with 34.8% as the top score. The dataset and toolchain demonstrate high effectiveness in modeling repository-level code reflection.

Conclusion: By highlighting the critical need for improved code reflection in repositories, the work establishes new standards for training and evaluating LLMs with practical coding applications, driving progress toward real-world software development scenarios.

Abstract: Code large language models (LLMs) enhance programming by understanding and
generating code across languages, offering intelligent feedback, bug detection,
and code updates through reflection, improving development efficiency and
accessibility. While benchmarks (e.g. HumanEval/LiveCodeBench) evaluate code
generation and real-world relevance, previous works ignore the scenario of
modifying code in repositories. Considering challenges remaining in improving
reflection capabilities and avoiding data contamination in dynamic benchmarks,
we introduce LiveRepoReflection, a challenging benchmark for evaluating code
understanding and generation in multi-file repository contexts, featuring 1,888
rigorously filtered test cases across $6$ programming languages to ensure
diversity, correctness, and high difficulty. Further, we create
RepoReflection-Instruct, a large-scale, quality-filtered instruction-tuning
dataset derived from diverse sources, used to train RepoReflectionCoder through
a two-turn dialogue process involving code generation and error-driven repair.
The leaderboard evaluates over 40 LLMs to reflect the model performance of
repository-based code reflection.

</details>


### [61] [PathFuzzing: Worst Case Analysis by Fuzzing Symbolic-Execution Paths](https://arxiv.org/abs/2507.09892)
*Zimu Chen,Di Wang*

Main category: cs.SE

TL;DR: PathFuzzing combines fuzzing and symbolic execution to improve worst-case resource consumption estimation by searching for high-consumption paths using evolutionary algorithms.


<details>
  <summary>Details</summary>
Motivation: Traditional fuzzing struggles with code coverage while symbolic execution faces path explosion during worst-case analysis (WCA). This paper addresses the limitations of both approaches.

Method: Translates programs into symbolic representations where inputs are binary-encoded execution paths. Evolutionary fuzzing is then applied to search for binary strings satisfying path conditions and maximizing resource consumption.

Result: Experiments on a benchmark suite (including prior work and custom additions) demonstrate PathFuzzing outperforms both fuzzing and symbolic execution baselines in WCA.

Conclusion: PathFuzzing provides an effective hybrid methodology for WCA by integrating strengths of symbolically guided fuzzing, achieving superior performance over existing techniques.

Abstract: Estimating worst-case resource consumption is a critical task in software
development. The worst-case analysis (WCA) problem is an optimization-based
abstraction of this task. Fuzzing and symbolic execution are widely used
techniques for addressing the WCA problem. However, improving code coverage in
fuzzing or managing path explosion in symbolic execution within the context of
WCA poses significant challenges. In this paper, we propose PathFuzzing, aiming
to combine the strengths of both techniques to design a WCA method. The key
idea is to transform a program into a symbolic one that takes an execution path
(encoded as a binary string) and interprets the bits as branch decisions.
PathFuzzing then applies evolutionary fuzzing techniques to the transformed
program to search for binary strings that represent satisfiable path conditions
and lead to high resource consumption. We evaluate the performance of
PathFuzzing experimentally on a benchmark suite that consists of prior work's
benchmarks and some added by us. Results show that PathFuzzing generally
outperforms a fuzzing and a symbolic-execution baseline.

</details>


### [62] [Modelling Interrelations Between Agile Practices: The Agile Map](https://arxiv.org/abs/2507.09907)
*Thomas Hansper,Kevin Phong Pham,Michael Neumann*

Main category: cs.SE

TL;DR: This paper introduces the Agile Map, a theoretical model to identify and systematize interrelations between agile practices, helping practitioners combine them effectively.


<details>
  <summary>Details</summary>
Motivation: The widespread adoption and tailoring of agile practices have led to a diverse set of practices, but their interrelations are not well understood, limiting effective combinatorial use.

Method: The study systematically identifies interrelations between agile practices, focusing on their dependencies and synergies to establish a coherent model.

Result: The Agile Map is developed as a structured framework depicting relationships between agile practices across their spectrum of detail and application.

Conclusion: The Agile Map provides an overview of practice coherences, enabling practitioners to select and combine agile methods in a theoretically grounded and meaningful manner.

Abstract: Agile methods are defined through guidelines comprising various practices
intended to enable agile ways of working. These guidelines further comprise a
specific set of agile practices aiming to enable teams for an agile way of
working. However, due to its wide-spread use in practice we know that agile
practices are adopted and tailored intensively, which lead to a high variety of
agile practices in terms of their level of detail. Problem: A high variety of
agile practices can be challenging as we do not know how different agile
practices are interrelated with each other. To be more precise, tailoring and
adopting agile practices may lead to the challenge, that the combinatorial use
of several agile practices can only be successful to a limited extent, as
practices support or even require each other for a effective use in practice.
Objective: Our study aims to provide an enabler for this problem. We want to
identify interrelations between agile practices and describe them in a
systematic manner. Contribution: The core contribution of this paper is the
Agile Map, a theoretical model describing relations between agile practices
following a systematic approach aiming to provide an overview of coherences
between agile practices. The model aims to support practitioners in selecting
and combining agile practices in a meaningful way.

</details>


### [63] [When Less is More: A systematic review of four-day workweek conceptualizations and their effects on organizational performance](https://arxiv.org/abs/2507.09911)
*Marvin Auf der Landwehr,Julia Topp,Michael Neumann*

Main category: cs.SE

TL;DR: This study explores the impact of compressed work schedules on IT organizations and develops a comprehensive framework to guide their adoption based on a systematic review and web analysis.


<details>
  <summary>Details</summary>
Motivation: To deepen understanding of how compressed work schedules affect IT enterprises' operational efficacy and provide a framework for effective adoption, considering evolving work practices.

Method: Systematic literature review combined with web content analysis to integrate scientific and practical insights on four-day workweek schedules.

Result: A meta-framework aligning conceptualizations of compressed schedules with their organizational and social effects, enabling tailored implementation based on managerial needs.

Conclusion: The study concludes that compressed work schedules can enhance operational efficacy in IT organizations when implemented through a context-specific framework addressing key prerequisites and circumstances.

Abstract: Context: Agile IT organizations, which are characterized by self-organization
and collaborative social interactions, require motivating, efficient and
flexible work environments to maximize value creation. Compressed work
schedules such as the four-day workweek have evolved into multiple facets over
the last decades and are associated with various benefits for organizations and
their employees. Objective: Our objective in this study is to deepen our
comprehension of the impact of compressed work schedules on the operational
efficacy of IT enterprises, while concurrently developing a comprehensive
framework delineating the intricacies of compressed work schedules.Method: We
conducted a systematic review of available conceptualizations related to
four-day workweek schedules and elaborate on their organizational and social
effects. To cover scientific and practice-oriented literature, our review
combined a systematic literature review and a web content analysis. Results:
Based on the generated insights, we derive a meta-framework that matches
conceptualizations and effects, finally guiding the adoption of compressed work
schedules based on individual managerial prerequisites and circumstances.

</details>


### [64] [Accelerating Automatic Program Repair with Dual Retrieval-Augmented Fine-Tuning and Patch Generation on Large Language Models](https://arxiv.org/abs/2507.10103)
*Hanyang Guo,Xiaoheng Xie,Hong-Ning Dai,Peng Di,Yu Zhang,Bishenghui Tao,Zibin Zheng*

Main category: cs.SE

TL;DR: SelRepair is a novel Automated Program Repair (APR) approach combining a fine-tuned LLM with a dual RAG module, achieving higher repair accuracy and efficiency on Java datasets.


<details>
  <summary>Details</summary>
Motivation: Existing APR methods face limitations in defect type handling, training data quality, and model parameter size, while current code LLMs and RAG designs underutilize code-specific features in repair tasks.

Method: SelRepair integrates a fine-tuned LLM using a bug-fix pair dataset with a dual RAG module that incorporates semantic and syntactic/structural similarity through an RAG selection gate, reducing input length and inference costs.

Result: It achieves 26.29% and 17.64% exact match scores on Java datasets, while reducing inference time by ≥6.42% compared to existing APR methods.

Conclusion: SelRepair addresses prior APR limitations through its dual RAG integration strategy, demonstrating superior repair effectiveness and computational efficiency.

Abstract: Automated Program Repair (APR) is essential for ensuring software reliability
and quality while enhancing efficiency and reducing developers' workload.
Although rule-based and learning-based APR methods have demonstrated their
effectiveness, their performance was constrained by the defect type of repair,
the quality of training data, and the size of model parameters. Recently, Large
Language Models (LLMs) combined with Retrieval-Augmented-Generation (RAG) have
been increasingly adopted in APR tasks. However, current code LLMs and RAG
designs neither fully address code repair tasks nor consider code-specific
features. To overcome these limitations, we propose SelRepair, a novel APR
approach with integration of a fine-tuned LLM with a newly-designed dual RAG
module. This approach uses a bug-fix pair dataset for fine-tuning and
incorporates semantic and syntactic/structural similarity information through
an RAG selection gate. This design ensures relevant information is retrieved
efficiently, thereby reducing token length and inference time. Evaluations on
Java datasets show SelRepair outperforms other APR methods, achieving 26.29%
and 17.64% in terms of exact match (EM) on different datasets while reducing
inference time by at least 6.42% with controlled input lengths.

</details>


### [65] [Explicit Vulnerability Generation with LLMs: An Investigation Beyond Adversarial Attacks](https://arxiv.org/abs/2507.10054)
*Emir Bosnak,Sahand Moslemi,Mayasah Lami,Anil Koyuncu*

Main category: cs.SE

TL;DR: This paper investigates whether open-source Large Language Models (LLMs) will generate insecure code when directly or indirectly prompted to do so.


<details>
  <summary>Details</summary>
Motivation: Existing research has focused on unintended vulnerabilities or adversarial prompting techniques, but this study examines a more direct threat scenario with practical implications for using LLMs as code assistants.

Method: The research uses a dual experimental design involving (1) Dynamic Prompting with structured templates varying vulnerability types, user personas, and directness; and (2) Reverse Prompting by deriving prompts from real vulnerable code samples. Three open-source models (Qwen2, Mistral, Gemma) are evaluated using ESBMC static analysis.

Result: All models frequently produce vulnerable outputs, with Qwen2 showing highest correctness. Student personas achieve higher vulnerability rates than professional roles, and vulnerability reproduction demonstrates an inverted-U relationship with code complexity.

Conclusion: The findings highlight inadequate safety mechanisms in open-source LLMs, particularly vulnerability to educational/research role-play prompts, urging improved safeguards for responsible code generation.

Abstract: Large Language Models (LLMs) are increasingly used as code assistants, yet
their behavior when explicitly asked to generate insecure code remains poorly
understood. While prior research has focused on unintended vulnerabilities or
adversarial prompting techniques, this study examines a more direct threat
scenario: open-source LLMs generating vulnerable code when prompted either
directly or indirectly. We propose a dual experimental design: (1) Dynamic
Prompting, which systematically varies vulnerability type, user persona, and
directness across structured templates; and (2) Reverse Prompting, which
derives prompts from real vulnerable code samples to assess vulnerability
reproduction accuracy. We evaluate three open-source 7B-parameter models
(Qwen2, Mistral, and Gemma) using ESBMC static analysis to assess both the
presence of vulnerabilities and the correctness of the generated vulnerability
type. Results show all models frequently produce vulnerable outputs, with Qwen2
achieving highest correctness rates. User persona significantly affects
success, where student personas achieved higher vulnerability rates than
professional roles, while direct prompts were marginally more effective.
Vulnerability reproduction followed an inverted-U pattern with cyclomatic
complexity, peaking at moderate ranges. Our findings expose limitations of
safety mechanisms in open-source models, particularly for seemingly benign
educational requests.

</details>


### [66] [LLMShot: Reducing snapshot testing maintenance via LLMs](https://arxiv.org/abs/2507.10062)
*Ergün Batuhan Kaynak,Mayasah Lami,Sahand Moslemi,Anil Koyuncu*

Main category: cs.SE

TL;DR: This paper proposes LLMShot, a vision-based LLM framework for automated snapshot test failure analysis, reducing manual triage effort through hierarchical classification of UI changes in a feature-rich iOS application dataset.


<details>
  <summary>Details</summary>
Motivation: Modern software UIs face high maintenance costs due to frequent design changes causing snapshot test failures that require manual effort to classify as genuine regressions or intentional modifications.

Method: Developed LLMShot using Gemma3 vision-based LLM models, implemented hierarchical classification for UI change analysis, and created a realistic iOS application dataset with configurable feature flags to produce authentic snapshot differences.

Result: 12B Gemma3 variant achieved 84+ recall in failure root-cause identification, 4B variant shows practical CI integration potential, but prompting-based selective ignores demonstrated significant limitations in controllable visual reasoning.

Conclusion: LLMShot pioneers automated semantic analysis for snapshot testing, offering structured failure insights that reduce manual work requirements while highlighting current research gaps in controllable visual reasoning for UI validation.

Abstract: Snapshot testing has emerged as a critical technique for UI validation in
modern software development, yet it suffers from substantial maintenance
overhead due to frequent UI changes causing test failures that require manual
inspection to distinguish between genuine regressions and intentional design
changes. This manual triage process becomes increasingly burdensome as
applications evolve, creating a need for automated analysis solutions. This
paper introduces LLMShot, a novel framework that leverages vision-based Large
Language Models to automatically analyze snapshot test failures through
hierarchical classification of UI changes. To evaluate LLMShot's effectiveness,
we developed a comprehensive dataset using a feature-rich iOS application with
configurable feature flags, creating realistic scenarios that produce authentic
snapshot differences representative of real development workflows. Our
evaluation using Gemma3 models demonstrates strong classification performance,
with the 12B variant achieving over 84% recall in identifying failure root
causes while the 4B model offers practical deployment advantages with
acceptable performance for continuous integration environments. However, our
exploration of selective ignore mechanisms revealed significant limitations in
current prompting-based approaches for controllable visual reasoning. LLMShot
represents the first automated approach to semantic snapshot test analysis,
offering developers structured insights that can substantially reduce manual
triage effort and advance toward more intelligent UI testing paradigms.

</details>


### [67] [Breaking the Myth: Can Small Models Infer Postconditions Too?](https://arxiv.org/abs/2507.10182)
*Gehao Zhang,Zhenting Wang,Juan Zhai*

Main category: cs.SE

TL;DR: The paper demonstrates that a small, fine-tuned 7B-parameter code model (TinySpec) can generate high-quality postconditions for Java bugs with lower computational costs than large models like GPT-4o, by leveraging a specialized dataset of prompts, reasoning logs, and postconditions, achieving comparable syntax correctness, stronger semantic correctness, and equal/exceeding bug-distinguishing capabilities.


<details>
  <summary>Details</summary>
Motivation: Formal specifications are essential for software correctness but challenging to write manually. While LLMs show promise, their large model sizes and high computational demands hinder real-world adoption, prompting the question: Can smaller models achieve similar results through targeted training?

Method: 1. Constructed a specialized dataset with prompts, reasoning logs, and postconditions 
2. Fine-tuned a 7B-parameter code model using the dataset 
3. Designed the model to handle repository dependencies and preserve pre-state information 
4. Evaluated on Defects4J (real-world Java bugs), comparing against GPT-4o and open-source large models

Result: The compact model matches or exceeds large models in three key metrics:
1. Syntax correctness 
2. Semantic correctness 
3. Bug-distinguishing capability

Specifically, it demonstrates stronger semantic correctness than all large model baselines while maintaining low computational costs.

Conclusion: Targeted fine-tuning on domain-specific datasets enables small models to deliver specification generation results matching or surpassing large models, providing a practical, resource-efficient solution for software verification by reducing the need for massive computational infrastructure.

Abstract: Formal specifications are essential for ensuring software correctness, yet
manually writing them is tedious and error-prone. Large Language Models (LLMs)
have shown promise in generating such specifications from natural language
intents, but the giant model size and high computational demands raise a
fundamental question: Do we really need large models for this task? In this
paper, we show that a small, fine-tuned language model can achieve high-quality
postcondition generation with much lower computational costs. We construct a
specialized dataset of prompts, reasoning logs, and postconditions, then
supervise the fine-tuning of a $7$B-parameter code model. Our approach tackles
real-world repository dependencies and preserves pre-state information,
allowing for expressive and accurate specifications. We evaluate the model on a
benchmark of real-world Java bugs (Defects4J) and compare against both
proprietary giants (e.g., GPT-4o) and open-source large models. Empirical
results demonstrate that our compact model matches or outperforms significantly
larger counterparts in syntax correctness, semantic correctness, and
bug-distinguishing capability. These findings highlight that targeted
fine-tuning on a modest dataset can enable small models to achieve results
formerly seen only in massive, resource-heavy LLMs, offering a practical and
efficient path for the real-world adoption of automated specification
generation.

</details>


### [68] [Towards a Framework for Operationalizing the Specification of Trustworthy AI Requirements](https://arxiv.org/abs/2507.10228)
*Hugo Villamizar,Daniel Mendez,Marcos Kalinowski*

Main category: cs.SE

TL;DR: This paper proposes integrating AMDiRE (artefact-based RE methodology) and PerSpecML (multi-perspective ML requirements approach) to address trustworthiness challenges in AI systems by combining structured artifact modeling with contextual ML concerns.


<details>
  <summary>Details</summary>
Motivation: AI system trustworthiness requires structured approaches to capture emergent, context-dependent properties that traditional requirements engineering (RE) methods struggle with, particularly for data-driven and non-deterministic machine learning (ML) systems.

Method: The paper combines AMDiRE's structured, artifact-centric process with PerSpecML's multi-perspective guidance, utilizing templates for consistency while addressing ML-specific challenges like data-driven behavior and stakeholder concerns through complementary approaches.

Result: The integration provides a unified pathway to operationalize trustworthiness requirements in ML systems by systematically linking stakeholder-driven concerns to structured artifact models while maintaining traceability.

Conclusion: The authors highlight the need for further research on bridging stakeholder concerns with structured ML requirements, including addressing open challenges related to operationalizing this integration within the RE community.

Abstract: Growing concerns around the trustworthiness of AI-enabled systems highlight
the role of requirements engineering (RE) in addressing emergent,
context-dependent properties that are difficult to specify without structured
approaches. In this short vision paper, we propose the integration of two
complementary approaches: AMDiRE, an artefact-based approach for RE, and
PerSpecML, a perspective-based method designed to support the elicitation,
analysis, and specification of machine learning (ML)-enabled systems. AMDiRE
provides a structured, artefact-centric, process-agnostic methodology and
templates that promote consistency and traceability in the results; however, it
is primarily oriented toward deterministic systems. PerSpecML, in turn,
introduces multi-perspective guidance to uncover concerns arising from the
data-driven and non-deterministic behavior of ML-enabled systems. We envision a
pathway to operationalize trustworthiness-related requirements, bridging
stakeholder-driven concerns and structured artefact models. We conclude by
outlining key research directions and open challenges to be discussed with the
RE community.

</details>


### [69] [An Empirical Study of Interaction Bugs in ROS-based Software](https://arxiv.org/abs/2507.10235)
*Zhixiang Chen,Zhuangbin Chen,Xingjie Cai,Wei Li,Zibin Zheng*

Main category: cs.SE

TL;DR: This paper studies interaction bugs (iBugs) in robotic systems using an empirical analysis of 121 iBugs across 10 ROS projects, categorizing them into three types and exploring their root causes, fixes, and implications for system reliability.


<details>
  <summary>Details</summary>
Motivation: Robotic systems depend on complex interactions between components, but reliability issues at these boundaries (iBugs) are underexplored despite their critical impact on system safety and robustness.

Method: Empirical analysis of iBugs in 10 actively maintained ROS projects through root cause examination, categorization into intra-system, hardware, and environmental types, and evaluation of fixing strategies.

Result: Identified 121 iBugs classified into three major categories with insights into their root causes, fixes, and impact, providing actionable strategies for prevention and detection within ROS-based systems.

Conclusion: The study highlights iBugs as a critical challenge in robotics, offering empirical evidence and direction for improving reliability through targeted prevention, detection mechanisms, and system design enhancements.

Abstract: Modern robotic systems integrate multiple independent software and hardware
components, each responsible for distinct functionalities such as perception,
decision-making, and execution. These components interact extensively to
accomplish complex end-to-end tasks. As a result, the overall system
reliability depends not only on the correctness of individual components, but
also on the correctness of their interactions. Failures often manifest at the
boundaries between components, yet interaction-related reliability issues in
robotics--referred to here as interaction bugs (iBugs)--remain underexplored.
  This work presents an empirical study of iBugs within robotic systems built
using the Robot Operating System (ROS), a widely adopted open-source robotics
framework. A total of 121 iBugs were analyzed across ten actively maintained
and representative ROS projects. The identified iBugs are categorized into
three major types: intra-system iBugs, hardware iBugs, and environmental iBugs,
covering a broad range of interaction scenarios in robotics. The analysis
includes an examination of root causes, fixing strategies, and the impact of
these bugs. Several findingsa are derived that shed light on the nature of
iBugs and suggest directions for improving their prevention and detection.
These insights aim to inform the design of more robust and safer robotic
systems.

</details>


### [70] [Helveg: Diagrams for Software Documentation](https://arxiv.org/abs/2507.10244)
*Adam Štěpánek,David Kuťák,Barbora Kozlíková,Jan Byška*

Main category: cs.SE

TL;DR: This paper refines an interactive visualization tool called Helveg for C# codebases, addressing usability issues identified in previous user testing to improve understanding of APIs through dynamic visual exploration.


<details>
  <summary>Details</summary>
Motivation: Traditional textual API documentation forces developers to follow linear paths, making high-level codebase exploration inefficient. The paper aims to create an intuitive, interactive visual tool that provides both overview and detail.

Method: The authors redesigned Helveg by overhauling glyph design, interaction mechanisms, and UI layout based on initial user feedback. The improved version was evaluated through user testing with the same original participants.

Result: User testing revealed significant usability problems in the original Helveg, prompting substantial revisions. The updated version's evaluation showed improvements, though specific metrics are not detailed in the abstract.

Conclusion: The modified Helveg demonstrates the viability of interactive node-link diagrams for API documentation, offering better readability and user experience. This work contributes to more effective codebase exploration tools for developers.

Abstract: Software developers often have to gain an understanding of a codebase. Be it
programmers getting onboarded onto a team project or, for example, developers
striving to grasp an external open-source library. In either case, they
frequently turn to the project's documentation. However, documentation in its
traditional textual form is ill-suited for this kind of high-level exploratory
analysis, since it is immutable from the readers' perspective and thus forces
them to follow a predefined path. We have designed an approach bringing aspects
of software architecture visualization to API reference documentation. It
utilizes a highly interactive node-link diagram with expressive node glyphs and
flexible filtering capabilities, providing a high-level overview of the
codebase as well as details on demand. To test our design, we have implemented
a prototype named Helveg, capable of automatically generating diagrams of C\#
codebases. User testing of Helveg confirmed its potential, but it also revealed
problems with the readability, intuitiveness, and user experience of our tool.
Therefore, in this paper, which is an extended version of our VISSOFT paper
with DOI 10.1109/VISSOFT64034.2024.00012, we address many of these problems
through major changes to the glyph design, means of interaction, and user
interface of the tool. To assess the improvements, this new version of Helveg
was evaluated again with the same group of participants as the previous
version.

</details>


### [71] [A Grounded Theory on the Teacher and Student Roles in Pair Programming](https://arxiv.org/abs/2507.10305)
*Linus Ververs,Trang Linh Lam,Janina Berger,Lutz Prechelt*

Main category: cs.SE

TL;DR: This study examines how knowledge transfer can harm pair programming sessions through a 'Power Gap' dynamic, identifying defensive behaviors that create harmful cycles affecting collaboration and code quality.


<details>
  <summary>Details</summary>
Motivation: While pair programming is widely adopted, existing research has not clarified situations where knowledge transfer becomes counterproductive, particularly when one-sided knowledge imbalances exist between developers.

Method: Grounded Theory analysis of 17 recorded pair programming sessions involving 18 developers from 5 German software companies, complemented by 6 interviews with developers from 4 other German companies.

Result: Proposed student/teacher role definitions for managing knowledge gaps, identified specific pitfalls, and formulated a grounded theory explaining how unchecked Power Gaps lead to defensive behaviors creating negative feedback loops.

Conclusion: Effective knowledge transfer in pair programming requires attention to partners' psychological needs through awareness of the Power Gap, which when neglected causes defensive behaviors that simultaneously impair knowledge sharing, team cohesion, and code quality outcomes.

Abstract: Context: Pair programming is an established (agile) practice and is practiced
throughout the industry. Objective: Understand under what circumstances
knowledge transfer can harm a pair programming session. Method: Grounded Theory
Methodology based on 17 recorded pair programming sessions with 18 developers
from 5 German software companies accompanied, by 6 interviews with different
developers from 4 other German companies. Results: We define the student and
teacher roles to help developers deal with a one-sided knowledge gap. We
describe pitfalls to avoid and develop a grounded theory centered around the
Power Gap in pair programming. Conclusions: Knowledge transfer can be harmful
when developers don't pay attention to their partners needs and desires. If
developers don't pay attention to the Power Gap and keep it in check, Defensive
Behavior may arise that leads to a vicious cycle impacting the knowledge
transfer, the Togetherness and the code quality in a negative way.

</details>


### [72] [Streamlined Airborne Software Development for Large UAVs: From Unified Data Collection to Automated Code Generation](https://arxiv.org/abs/2507.10321)
*Viktor Sinitsyn,Nils Schlautmann,Florian Schwaiger,Florian Holzapfel*

Main category: cs.SE

TL;DR: A paper introducing an automated toolchain for streamlining aerospace onboard digital interface development, addressing industry transformation and startup challenges.


<details>
  <summary>Details</summary>
Motivation: The aerospace industry's rapid technological evolution has created critical challenges for startups and established companies in efficiently processing digital intra-device communication interfaces for onboard equipment, necessitating automated solutions to reduce labor-intensive efforts.

Method: The authors propose a novel automated process and toolchain for developing digital interfaces and onboard software, emphasizing automation, flexibility, and compliance with design assurance requirements.

Result: Successful application of the proposed approach across multiple completed projects demonstrates its effectiveness in streamlining interface development workflows.

Conclusion: The presented automated toolchain provides a viable solution for overcoming interface development challenges while maintaining design assurance compliance amidst the aerospace industry's ongoing transformation.

Abstract: The aerospace industry has experienced significant transformations over the
last decade, driven by technological advancements and innovative solutions in
goods and personal transportation. This evolution has spurred the emergence of
numerous start-ups that now face challenges traditionally encountered by
established aerospace companies. Among these challenges is the efficient
processing of digital intra-device communication interfaces for onboard
equipment - a critical component for ensuring seamless system integration and
functionality. Addressing this challenge requires solutions that emphasize
clear and consistent interface descriptions, automation of processes, and
reduced labor-intensive efforts.
  This paper presents a novel process and toolchain designed to streamline the
development of digital interfaces and onboard software, which our team has
successfully applied in several completed projects. The proposed approach
focuses on automation and flexibility while maintaining compliance with design
assurance requirements.

</details>


### [73] [AssertCoder: LLM-Based Assertion Generation via Multimodal Specification Extraction](https://arxiv.org/abs/2507.10338)
*Enyuan Tian,Yiwei Ci,Qiusong Yang,Yufeng Li,Zhichao Lyu*

Main category: cs.SE

TL;DR: AssertCoder is a framework that automatically generates high-quality SystemVerilog Assertions (SVAs) from multimodal hardware specifications using modality-sensitive preprocessing and semantic analysis, improving functional correctness and mutation detection by 8.4% and 5.8% respectively.


<details>
  <summary>Details</summary>
Motivation: Manual SVA creation for Assertion-Based Verification is laborious and error-prone, necessitating an automated solution for efficient, reliable hardware validation.

Method: 1. Parses heterogeneous specs (text, tables, diagrams, formulas) via modality-sensitive preprocessing
2. Extracts structured, signal-semantic representations using dedicated analyzers
3. Generates SVAs through multi-step chain-of-thought prompting
4. Applies mutation-based evaluation with model checking for refinement

Result: 8.4% average improvement in functional correctness and 5.8% enhancement in mutation detection across three real RTL designs vs. state-of-the-art methods.

Conclusion: AssertCoder demonstrates superior effectiveness in automating SVA generation from multimodal inputs, with measurable improvements in hardware verification reliability and error detection.

Abstract: Assertion-Based Verification (ABV) is critical for ensuring functional
correctness in modern hardware systems. However, manually writing high-quality
SVAs remains labor-intensive and error-prone. To bridge this gap, we propose
AssertCoder, a novel unified framework that automatically generates
high-quality SVAs directly from multimodal hardware design specifications.
AssertCoder employs a modality-sensitive preprocessing to parse heterogeneous
specification formats (text, tables, diagrams, and formulas), followed by a set
of dedicated semantic analyzers that extract structured representations aligned
with signal-level semantics. These representations are utilized to drive
assertion synthesis via multi-step chain-of-thought (CoT) prompting. The
framework incorporates a mutation-based evaluation approach to assess assertion
quality via model checking and further refine the generated assertions.
Experimental evaluation across three real-world Register-Transfer Level (RTL)
designs demonstrates AssertCoder's superior performance, achieving an average
increase of 8.4% in functional correctness and 5.8% in mutation detection
compared to existing state-of-the-art approaches.

</details>


### [74] [Self-Admitted GenAI Usage in Open-Source Software](https://arxiv.org/abs/2507.10422)
*Tao Xiao,Youmei Fan,Fabio Calefato,Christoph Treude,Raula Gaikovina Kula,Hideaki Hata,Sebastian Baltes*

Main category: cs.SE

TL;DR: This paper analyzes how generative AI (GenAI) tools are integrated into open-source software (OSS) projects by studying self-admitted GenAI usage across 250,000 GitHub repositories, identifying 32 tasks, 10 content types, and 11 purposes associated with their use. It also highlights developer concerns and provides evidence that GenAI adoption does not significantly increase code churn.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need to understand the real-world usage and impact of GenAI tools in software development, particularly in open-source projects, as generated code is indistinguishable from manual code and existing narratives often lack empirical validation. The motivation is to uncover how developers adopt, use, and govern AI-generated content while addressing ethical and practical implications.

Method: The study employs a mixed-methods approach involving 1) analysis of commit messages, code comments, and documentation across 250,000 GitHub repositories to identify self-admitted GenAI usage instances; 2) qualitative coding of 284 mentions to develop taxonomies for tasks, content types, and purposes; 3) analysis of 13 documents with GenAI policies and guidelines; 4) developer surveys; and 5) longitudinal analysis of code churn in 151 repositories.

Result: Key findings include: 1) 1,292 self-admitted GenAI usage instances across 156 repositories; 2) a taxonomy categorizing usage into 32 tasks, 10 content types, and 11 purposes; 3) significant developer and project-level governance efforts around ethical, legal, and quality concerns; and 4) no general increase in code churn in repositories adopting GenAI tools, challenging existing assumptions.

Conclusion: The study concludes that open-source developers actively prioritize transparency, attribution, and quality control when integrating GenAI tools into their workflows. It emphasizes the need for project-specific practices to govern AI-assisted development and reveals the complexity of GenAI adoption, which does not simplify code complexity despite claims to the contrary.

Abstract: The widespread adoption of generative AI (GenAI) tools such as GitHub Copilot
and ChatGPT is transforming software development. Since generated source code
is virtually impossible to distinguish from manually written code, their
real-world usage and impact on open-source software development remain poorly
understood. In this paper, we introduce the concept of self-admitted GenAI
usage, that is, developers explicitly referring to the use of GenAI tools for
content creation in software artifacts. Using this concept as a lens to study
how GenAI tools are integrated into open-source software projects, we analyze a
curated sample of more than 250,000 GitHub repositories, identifying 1,292 such
self-admissions across 156 repositories in commit messages, code comments, and
project documentation. Using a mixed methods approach, we derive a taxonomy of
32 tasks, 10 content types, and 11 purposes associated with GenAI usage based
on 284 qualitatively coded mentions. We then analyze 13 documents with policies
and usage guidelines for GenAI tools and conduct a developer survey to uncover
the ethical, legal, and practical concerns behind them. Our findings reveal
that developers actively manage how GenAI is used in their projects,
highlighting the need for project-level transparency, attribution, and quality
control practices in the new era of AI-assisted software development. Finally,
we examine the longitudinal impact of GenAI adoption on code churn in 151
repositories with self-admitted GenAI usage and find no general increase,
contradicting popular narratives on the impact of GenAI on software
development.

</details>
