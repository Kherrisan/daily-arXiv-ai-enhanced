{"id": "2510.21946", "categories": ["cs.CR", "68T07, 68T50", "I.2.6; I.2.7; K.6.5"], "pdf": "https://arxiv.org/pdf/2510.21946", "abs": "https://arxiv.org/abs/2510.21946", "authors": ["Kieu Dang", "Phung Lai", "NhatHai Phan", "Yelong Shen", "Ruoming Jin", "Abdallah Khreishah"], "title": "$\u03b4$-STEAL: LLM Stealing Attack with Local Differential Privacy", "comment": "Accepted at ACML 2025 (PMLR W&CP). Code:\n  https://github.com/kirudang/LDP_Stealing_Attack", "summary": "Large language models (LLMs) demonstrate remarkable capabilities across\nvarious tasks. However, their deployment introduces significant risks related\nto intellectual property. In this context, we focus on model stealing attacks,\nwhere adversaries replicate the behaviors of these models to steal services.\nThese attacks are highly relevant to proprietary LLMs and pose serious threats\nto revenue and financial stability. To mitigate these risks, the watermarking\nsolution embeds imperceptible patterns in LLM outputs, enabling model\ntraceability and intellectual property verification. In this paper, we study\nthe vulnerability of LLM service providers by introducing $\\delta$-STEAL, a\nnovel model stealing attack that bypasses the service provider's watermark\ndetectors while preserving the adversary's model utility. $\\delta$-STEAL\ninjects noise into the token embeddings of the adversary's model during\nfine-tuning in a way that satisfies local differential privacy (LDP)\nguarantees. The adversary queries the service provider's model to collect\noutputs and form input-output training pairs. By applying LDP-preserving noise\nto these pairs, $\\delta$-STEAL obfuscates watermark signals, making it\ndifficult for the service provider to determine whether its outputs were used,\nthereby preventing claims of model theft. Our experiments show that\n$\\delta$-STEAL with lightweight modifications achieves attack success rates of\nup to $96.95\\%$ without significantly compromising the adversary's model\nutility. The noise scale in LDP controls the trade-off between attack\neffectiveness and model utility. This poses a significant risk, as even robust\nwatermarks can be bypassed, allowing adversaries to deceive watermark detectors\nand undermine current intellectual property protection methods.", "AI": {"tldr": "The paper introduces a novel LLM model stealing attack, $\\delta$-STEAL, which uses LDP-preserving noise injection during fine-tuning to bypass watermark detectors while maintaining high model utility. Experiments show a 96.95% success rate.", "motivation": "Current LLM watermarking methods are insufficient against model stealing attacks that compromise intellectual property, especially for commercial models, necessitating a more effective solution.", "method": "$\\delta$-STEAL's method involves injecting noise into token embeddings of the adversary's model during fine-tuning to obfuscate watermark signals while maintaining model functionality through local differential privacy (LDP).", "result": "The results show high success rates (up to 96.95%) with minimal impact on the adversary model's utility, demonstrating LDP's effectiveness in obscuring watermark information.", "conclusion": "The $\\delta$-STEAL attack demonstrates a critical weakness in watermarking as a method for IP protection, requiring the development of stronger defensive strategies."}}
{"id": "2510.21957", "categories": ["cs.CR", "cs.AI", "K.6.5; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.21957", "abs": "https://arxiv.org/abs/2510.21957", "authors": ["Zhixin Pan", "Ziyu Shu", "Amberbir Alemayoh"], "title": "Towards Low-Latency and Adaptive Ransomware Detection Using Contrastive Learning", "comment": "This paper was accepted in the 2025 IEEE International Conference on\n  Computer Design (ICCD)", "summary": "Ransomware has become a critical threat to cybersecurity due to its rapid\nevolution, the necessity for early detection, and growing diversity, posing\nsignificant challenges to traditional detection methods. While AI-based\napproaches had been proposed by prior works to assist ransomware detection,\nexisting methods suffer from three major limitations, ad-hoc feature\ndependencies, delayed response, and limited adaptability to unseen variants. In\nthis paper, we propose a framework that integrates self-supervised contrastive\nlearning with neural architecture search (NAS) to address these challenges.\nSpecifically, this paper offers three important contributions. (1) We design a\ncontrastive learning framework that incorporates hardware performance counters\n(HPC) to analyze the runtime behavior of target ransomware. (2) We introduce a\ncustomized loss function that encourages early-stage detection of malicious\nactivity, and significantly reduces the detection latency. (3) We deploy a\nneural architecture search (NAS) framework to automatically construct adaptive\nmodel architectures, allowing the detector to flexibly align with unseen\nransomware variants. Experimental results show that our proposed method\nachieves significant improvements in both detection accuracy (up to 16.1%) and\nresponse time (up to 6x) compared to existing approaches while maintaining\nrobustness under evasive attacks.", "AI": {"tldr": "This paper proposes a framework combining self-supervised contrastive learning and neural architecture search (NAS) to enhance ransomware detection by addressing limitations in existing methods.", "motivation": "Ransomware poses significant cybersecurity challenges due to its rapid evolution, the necessity for early detection, and diversity, which traditional detection methods struggle with. Existing AI-based approaches are limited by ad-hoc feature dependencies, delayed response, and limited adaptability to new variants.", "method": "The proposed framework includes (1) a contrastive learning setup using HPC for runtime behavior analysis, (2) a loss function for early-stage detection and reduced latency, and (3) a NAS framework for adaptive architecture construction.", "result": "The method achieves significant improvements in detection accuracy (up to 16.1%) and response time (up to 6x) over existing approaches, with robustness under evasive attacks.", "conclusion": "The framework effectively addresses key limitations in ransomware detection, offering enhanced performance and adaptability through the integration of self-supervised learning and NAS."}}
{"id": "2510.22024", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.22024", "abs": "https://arxiv.org/abs/2510.22024", "authors": ["Evangelos Bitsikas", "Jason Veara", "Aanjhan Ranganathan"], "title": "Security Analysis of LTE Connectivity in Connected Cars: A Case Study of Tesla", "comment": null, "summary": "Modern connected vehicles rely on persistent LTE connectivity to enable\nremote diagnostics, over-the-air (OTA) updates, and critical safety services.\nWhile mobile network vulnerabilities are well documented in the smartphone\necosystem, their impact in safety-critical automotive settings remains\ninsufficiently examined. In this work, we conduct a black-box, non-invasive\nsecurity analysis of LTE connectivity in Tesla vehicles, including the Model 3\nand Cybertruck, revealing systemic protocol weaknesses and architectural\nmisconfigurations. We find that Tesla's telematics stack is susceptible to IMSI\ncatching, rogue base station hijacking, and insecure fallback mechanisms that\nmay silently degrade service availability. Furthermore, legacy control-plane\nconfigurations allow for silent SMS injection and broadcast message spoofing\nwithout driver awareness. These vulnerabilities have implications beyond a\nsingle vendor as they challenge core assumptions in regulatory frameworks like\nISO/SAE 21434 and UN R155/R156, which require secure, traceable, and resilient\ntelematics for type approval of modern vehicles.", "AI": {"tldr": "This paper exposes critical LTE security vulnerabilities in Tesla vehicles (Model 3, Cybertruck) that could compromise safety systems and challenge automotive cybersecurity regulations like ISO/SAE 21434 and UN R155/R156.", "motivation": "Existing mobile network vulnerabilities have not been sufficiently studied in safety-critical automotive contexts, despite connected vehicles relying on LTE for diagnostics, OTA updates, and safety services.", "method": "Black-box, non-invasive security analysis of Tesla's LTE telematics stack, identifying protocol weaknesses and architectural misconfigurations through practical attack vectors.", "result": "Discovery of systemic issues including IMSI catching, rogue base station hijacking, insecure fallback mechanisms, silent SMS injection, and broadcast message spoofing with potential service degradation.", "conclusion": "The findings reveal flaws in core assumptions of automotive cybersecurity standards, necessitating reevaluation of regulatory frameworks for secure, traceable telematics in vehicle type approval."}}
{"id": "2510.22085", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "I.2.7; I.2.0; K.6.5"], "pdf": "https://arxiv.org/pdf/2510.22085", "abs": "https://arxiv.org/abs/2510.22085", "authors": ["Pavlos Ntais"], "title": "Jailbreak Mimicry: Automated Discovery of Narrative-Based Jailbreaks for Large Language Models", "comment": "18 pages, 5 figures", "summary": "Large language models (LLMs) remain vulnerable to sophisticated prompt\nengineering attacks that exploit contextual framing to bypass safety\nmechanisms, posing significant risks in cybersecurity applications. We\nintroduce Jailbreak Mimicry, a systematic methodology for training compact\nattacker models to automatically generate narrative-based jailbreak prompts in\na one-shot manner. Our approach transforms adversarial prompt discovery from\nmanual craftsmanship into a reproducible scientific process, enabling proactive\nvulnerability assessment in AI-driven security systems. Developed for the\nOpenAI GPT-OSS-20B Red-Teaming Challenge, we use parameter-efficient\nfine-tuning (LoRA) on Mistral-7B with a curated dataset derived from AdvBench,\nachieving an 81.0% Attack Success Rate (ASR) against GPT-OSS-20B on a held-out\ntest set of 200 items. Cross-model evaluation reveals significant variation in\nvulnerability patterns: our attacks achieve 66.5% ASR against GPT-4, 79.5% on\nLlama-3 and 33.0% against Gemini 2.5 Flash, demonstrating both broad\napplicability and model-specific defensive strengths in cybersecurity contexts.\nThis represents a 54x improvement over direct prompting (1.5% ASR) and\ndemonstrates systematic vulnerabilities in current safety alignment approaches.\nOur analysis reveals that technical domains (Cybersecurity: 93% ASR) and\ndeception-based attacks (Fraud: 87.8% ASR) are particularly vulnerable,\nhighlighting threats to AI-integrated threat detection, malware analysis, and\nsecure systems, while physical harm categories show greater resistance (55.6%\nASR). We employ automated harmfulness evaluation using Claude Sonnet 4,\ncross-validated with human expert assessment, ensuring reliable and scalable\nevaluation for cybersecurity red-teaming. Finally, we analyze failure\nmechanisms and discuss defensive strategies to mitigate these vulnerabilities\nin AI for cybersecurity.", "AI": {"tldr": "Jailbreak Mimicry is a method for automatically generating narrative-based adversarial prompts to expose LLM vulnerabilities in cybersecurity. It achieves an 81.0% Attack Success Rate against GPT-OSS-20B via LoRA-finetuned Mistral-7B, outperforming manual attacks by 54x. Systematic analysis reveals domain-specific vulnerabilities and defense implications for AI security systems.", "motivation": "LLMs face significant risks from manual prompt engineering attacks in critical cybersecurity applications. Current vulnerability testing lacks systematic automation to assess and proactively address these threats in AI-driven security.", "method": "The approach uses parameter-efficient fine-tuning (LoRA) of Mistral-7B with a curated AdvBench-derived dataset to train compact attacker models. This transforms manual adversarial prompt generation into a reproducible process for one-shot attack generation and cross-model evaluation.", "result": "81.0-79.5-33.0-66.5 ASR against GPT-OSS-20B/Llama-3/Gemini 2.5 Flash/GPT-4, achieving 54x improvement over direct prompting. Technical domains (93% ASR) and fraud attacks (87.8%) show highest vulnerability, with robustness analysis validated through automated/human evaluation.", "conclusion": "Highlights systematic flaws in safety alignment while providing scalable red-teaming tools for AI security. Failure analyses and defensive strategies offer actionable insights to strengthen AI cybersecurity systems against narrative-based adversarial prompts."}}
{"id": "2510.21902", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21902", "abs": "https://arxiv.org/abs/2510.21902", "authors": ["Timoth\u00e9 Boulet", "Xavier Hinaut", "Cl\u00e9ment Moulin-Frier"], "title": "Software Engineering Agents for Embodied Controller Generation : A Study in Minigrid Environments", "comment": "10 pages, 7 figures", "summary": "Software Engineering Agents (SWE-Agents) have proven effective for\ntraditional software engineering tasks with accessible codebases, but their\nperformance for embodied tasks requiring well-designed information discovery\nremains unexplored. We present the first extended evaluation of SWE-Agents on\ncontroller generation for embodied tasks, adapting Mini-SWE-Agent (MSWEA) to\nsolve 20 diverse embodied tasks from the Minigrid environment. Our experiments\ncompare agent performance across different information access conditions: with\nand without environment source code access, and with varying capabilities for\ninteractive exploration. We quantify how different information access levels\naffect SWE-Agent performance for embodied tasks and analyze the relative\nimportance of static code analysis versus dynamic exploration for task solving.\nThis work establishes controller generation for embodied tasks as a crucial\nevaluation domain for SWE-Agents and provides baseline results for future\nresearch in efficient reasoning systems.", "AI": {"tldr": "This paper evaluates SWE-Agents on controller generation for embodied tasks, adapting MSWEA to solve tasks in the Minigrid environment under different information access conditions.", "motivation": "While SWE-Agents are effective for traditional software engineering tasks, their performance on embodied tasks requiring extensive information discovery has not been studied. The paper aims to address this gap and establish a new evaluation domain for SWE-Agents.", "method": "The authors adapted the Mini-SWE-Agent (MSWEA) to solve 20 diverse embodied tasks in the Minigrid environment. They conducted experiments comparing agent performance under different information access conditions: with and without environment source code access, and varying interactive exploration capabilities.", "result": "The experiments quantified the impact of different information access levels on SWE-Agent performance for embodied tasks, and analyzed the relative importance of static code analysis versus dynamic exploration.", "conclusion": "This work highlights the importance of controller generation for embodied tasks as an evaluation domain for SWE-Agents, providing baseline results for future research in efficient reasoning systems."}}
{"id": "2510.22100", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.22100", "abs": "https://arxiv.org/abs/2510.22100", "authors": ["Saif E. Nouma", "Attila A. Yavuz"], "title": "Lightweight and Breach-Resilient Authenticated Encryption Framework for Internet of Things", "comment": null, "summary": "The Internet of Things (IoT) relies heavily on resource-limited devices to\ncommunicate critical (e.g., military data) information under low-energy\nadversarial environments and low-latency wireless channels. Authenticated\nEncryption (AE) guarantees confidentiality, authenticity, and integrity, making\nit a vital security service for IoT. However, current deployed (lightweight) AE\nstandards lack essential features like key compromise resiliency and compact\nauthentication tags, as well as performance enhancements such as offline-online\ncryptography. To address these gaps, we propose Graphene, the first (to our\nknowledge) symmetric Forward-secure and Aggregate Authenticated Encryption\n(FAAE) framework designed for the performance and security demands of low-end\nIoT infrastructures. Graphene innovates by synergizing key evolution strategies\nand offline-online cryptographic processing with Universal Message\nAuthentication Codes (UMACs) to guarantee breach-resiliency, near-optimal\nonline latency, and compactness. We demonstrate Graphene efficiency through two\ndistinct instantiations, each balancing unique performance trade-offs with\nextensibility for diverse MACs. Our experimental evaluation on commodity\nhardware and 32-bit ARM Cortex-M4 microcontroller shows Graphene significant\nperformance gains over existing alternatives. Graphene is also backward\ncompatible with standard-compliant cryptographic implementations. We release\nour implementation as open source for public testing and adaptation.", "AI": {"tldr": "This paper introduces Graphene, a new framework for symmetric Forward-secure and Aggregate Authenticated Encryption (FAAE) in resource-limited Internet of Things (IoT) environments. It enhances security with key compromise resiliency, compact authentication tags, and offline-online performance, while demonstrating superior efficiency and backward compatibility with existing cryptographic standards.", "motivation": "The paper addresses the limitations of current lightweight Authenticated Encryption (AE) standards in IoT, such as lack of key compromise resiliency, compact authentication tags, and performance enhancements like offline-online cryptography. These shortcomings hinder effective security in low-energy adversarial environments and low-latency wireless channels typical in IoT.", "method": "The proposed framework, Graphene, integrates key evolution strategies and offline-online cryptographic processing with Universal Message Authentication Codes (UMACs). It offers two distinct instantiations, providing extensibility for different MACs while maintaining security features like breach-resiliency and compactness.", "result": "Graphene is shown to have significant performance gains over existing alternatives when evaluated on commodity hardware and 32-bit ARM Cortex-M4 microcontrollers. The framework is backward compatible with standard-compliant cryptographic implementations. The authors release their implementation as open-source.", "conclusion": "Graphene effectively addresses the gaps in current AE standards for IoT by providing a secure, efficient, and flexible solution. Its forward-secure and aggregate authenticated encryption capabilities, along with performance and backward compatibility benefits, make it well-suited for low-end IoT infrastructures."}}
{"id": "2510.21903", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21903", "abs": "https://arxiv.org/abs/2510.21903", "authors": ["Xuhui Zhou", "Valerie Chen", "Zora Zhiruo Wang", "Graham Neubig", "Maarten Sap", "Xingyao Wang"], "title": "TOM-SWE: User Mental Modeling For Software Engineering Agents", "comment": null, "summary": "Recent advances in coding agents have made them capable of planning, editing,\nrunning, and testing complex code bases. Despite their growing ability in\ncoding tasks, these systems still struggle to infer and track user intent,\nespecially when instructions are underspecified or context-dependent. To bridge\nthis gap, we introduce ToM-SWE, a dual-agent architecture that pairs a primary\nsoftware-engineering (SWE) agent with a lightweight theory-of-mind (ToM)\npartner agent dedicated to modeling the user's mental state. The ToM agent\ninfers user goals, constraints, and preferences from instructions and\ninteraction history, maintains a \\textbf{persistent memory} of the user, and\nprovides user-related suggestions to the SWE agent. In two software engineering\nbenchmarks (ambiguous SWE-bench and stateful SWE-bench), ToM-SWE improves task\nsuccess rates and user satisfaction. Notably, on the stateful SWE benchmark, a\nnewly introduced evaluation that provides agents with a user simulator along\nwith previous interaction histories, ToM-SWE achieves a substantially higher\ntask success rate of 59.7\\% compared to 18.1\\% for OpenHands, a\nstate-of-the-art SWE agent. Furthermore, in a three-week study with\nprofessional developers using ToM-SWE in their daily work, participants found\nit useful 86\\% of the time, underscoring the value of stateful user modeling\nfor practical coding agents.", "AI": {"tldr": "ToM-SWE pairs a coding agent with a theory-of-mind partner that tracks user intent, achieving state-of-the-art results (59.7% success) on software engineering benchmarks and high practical usability (86% usefulness) through persistent user modeling.", "motivation": "Existing coding agents struggle to infer user intent in underspecified or context-dependent scenarios, limiting their practical effectiveness despite technical capabilities.", "method": "The authors propose ToM-SWE, a dual-agent system combining a primary SWE agent with a ToM partner agent. The ToM agent maintains persistent memory of user goals and preferences from interaction history, providing contextual guidance to the SWE agent.", "result": "ToM-SWE achieves 59.7% task success on the stateful SWE-bench (vs. 18.1% for OpenHands) and demonstrates 86% usefulness in a three-week study with professional developers. It shows improved performance on ambiguous and stateful tasks.", "conclusion": "The study demonstrates that stateful user modeling significantly enhances the performance of coding agents, particularly in handling context-dependent tasks and improving real-world utility for developers."}}
{"id": "2510.22191", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.22191", "abs": "https://arxiv.org/abs/2510.22191", "authors": ["Qi Sheng"], "title": "TPPR: APT Tactic / Technique Pattern Guided Attack Path Reasoning for Attack Investigation", "comment": null, "summary": "Provenance analysis based on system audit data has emerged as a fundamental\napproach for investigating Advanced Persistent Threat (APT) attacks. Due to the\nhigh concealment and long-term persistence of APT attacks, they are only\nrepresented as a minimal part of the critical path in the provenance graph.\nWhile existing techniques employ behavioral pattern matching and data flow\nfeature matching to uncover latent associations in attack sequences through\nprovenance graph path reasoning, their inability to establish effective attack\ncontext associations often leads to the conflation of benign system operations\nwith real attack entities, that fail to accurately characterize real APT\nbehaviors. We observe that while the causality of entities in the provenance\ngraph exhibit substantial complexity, attackers often follow specific attack\npatterns-specifically, clear combinations of tactics and techniques to achieve\ntheir goals. Based on these insights, we propose TPPR, a novel framework that\nfirst extracts anomaly subgraphs through abnormal node detection,\nTTP-annotation and graph pruning, then performs attack path reasoning using\nmined TTP sequential pattern, and finally reconstructs attack scenarios through\nconfidence-based path scoring and merging. Extensive evaluation on real\nenterprise logs (more than 100 million events) and DARPA TC dataset\ndemonstrates TPPR's capability to achieve 99.9% graph simplification (700,000\nto 20 edges) while preserving 91% of critical attack nodes, outperforming\nstate-of-the-art solutions (SPARSE, DepImpact) by 63.1% and 67.9% in\nreconstruction precision while maintaining attack scenario integrity.", "AI": {"tldr": "TLDR: This paper introduces the TPPR framework to enhance APT attack investigation by extracting anomaly subgraphs through TTP-annotation, followed by attack path reasoning and attack scenario reconstruction.", "motivation": "APT attacks are challenging to detect due their high concealment and long-term persistence. Existing methods struggle to distinguish benign operations from actual attacks, often conflating when attempting to analyze provenance graphs. This necessitates a better approach to accurately characterize APT behaviors through effective context associations.", "method": "The proposed method, TTP-PR, involves three steps: first, anomaly subgraphs are extracted via abnormal node detection, TTP-annotation, and graph pruning. Then, attack paths are reasoned through mined TTP sequential patterns. Finally, attack scenarios are reconstructed using path scoring and merging based on confidence levels.", "result": "TPPR achieves 99.9% graph simplification with 91% of critical attack nodes preserved, demonstrating a significant improvement in attack investigation over state-of-the-art solutions. When compared to SPARSE and DepImpact, it outperforms them by 63.1% and 67.9% in reconstruction precision while maintaining the integrity of the attack scenarios.", "conclusion": "This research presents the TPPR framework as a novel and effective solution for APT investigation. By leveraging TTP sequential patterns, TPPR significantly improves the accuracy of identifying real APT behaviors while simplifying the representation of provenance graphs for effective attack reconstruction."}}
{"id": "2510.21933", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21933", "abs": "https://arxiv.org/abs/2510.21933", "authors": ["Joao Correia", "Daniel Coutinho", "Marco Castelluccio", "Caio Barbosa", "Rafael de Mello", "Anita Sarma", "Alessandro Garcia", "Marco Gerosa", "Igor Steinmacher"], "title": "A Comparison of Conversational Models and Humans in Answering Technical Questions: the Firefox Case", "comment": "13 pages", "summary": "The use of Large Language Models (LLMs) to support tasks in software\ndevelopment has steadily increased over recent years. From assisting developers\nin coding activities to providing conversational agents that answer newcomers'\nquestions. In collaboration with the Mozilla Foundation, this study evaluates\nthe effectiveness of Retrieval-Augmented Generation (RAG) in assisting\ndevelopers within the Mozilla Firefox project. We conducted an empirical\nanalysis comparing responses from human developers, a standard GPT model, and a\nGPT model enhanced with RAG, using real queries from Mozilla's developer chat\nrooms. To ensure a rigorous evaluation, Mozilla experts assessed the responses\nbased on helpfulness, comprehensiveness, and conciseness. The results show that\nRAG-assisted responses were more comprehensive than human developers (62.50% to\n54.17%) and almost as helpful (75.00% to 79.17%), suggesting RAG's potential to\nenhance developer assistance. However, the RAG responses were not as concise\nand often verbose. The results show the potential to apply RAG-based tools to\nOpen Source Software (OSS) to minimize the load to core maintainers without\nlosing answer quality. Toning down retrieval mechanisms and making responses\neven shorter in the future would enhance developer assistance in massive\nprojects like Mozilla Firefox.", "AI": {"tldr": "RAG improves comprehensive developer assistance in OSS projects but requires optimization for conciseness to maximize effectiveness in large projects like Mozilla Firefox.", "motivation": "The paper aims to assess whether Retrieval-Augmented Generation (RAG) can improve developer support in large OSS projects. It addresses the challenge of balancing assistance quality with reduced workload for core maintainers in environments requiring scalable, high-quality responses.", "method": "The researchers conducted an empirical analysis comparing responses from human developers, a standard GPT model, and a GPT model augmented with RAG. Real-world queries from Mozilla's developer chat rooms were evaluated by experts using metrics for helpfulness, comprehensiveness, and conciseness.", "result": "RAG-enhanced responses outperformed human developers in comprehensiveness (62.50% vs. 54.17%) and were nearly as helpful (75.00% vs. 79.17%). However, RAG responses were frequently verbose and less concise compared to human answers.", "conclusion": "The study concludes that Retrieval-Augmented Generation (RAG) shows potential to enhance developer assistance in Open Source Software (OSS) projects like Mozilla Firefox, reducing the burden on maintainers while maintaining answer quality. However, improvements in conciseness and retrieval mechanisms are recommended for future applications."}}
{"id": "2510.22274", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22274", "abs": "https://arxiv.org/abs/2510.22274", "authors": ["Anum Paracha", "Junaid Arshad", "Mohamed Ben Farah", "Khalid Ismail"], "title": "SecureLearn -- An Attack-agnostic Defense for Multiclass Machine Learning Against Data Poisoning Attacks", "comment": null, "summary": "Data poisoning attacks are a potential threat to machine learning (ML)\nmodels, aiming to manipulate training datasets to disrupt their performance.\nExisting defenses are mostly designed to mitigate specific poisoning attacks or\nare aligned with particular ML algorithms. Furthermore, most defenses are\ndeveloped to secure deep neural networks or binary classifiers. However,\ntraditional multiclass classifiers need attention to be secure from data\npoisoning attacks, as these models are significant in developing multi-modal\napplications. Therefore, this paper proposes SecureLearn, a two-layer\nattack-agnostic defense to defend multiclass models from poisoning attacks. It\ncomprises two components of data sanitization and a new feature-oriented\nadversarial training. To ascertain the effectiveness of SecureLearn, we\nproposed a 3D evaluation matrix with three orthogonal dimensions: data\npoisoning attack, data sanitization and adversarial training. Benchmarking\nSecureLearn in a 3D matrix, a detailed analysis is conducted at different\npoisoning levels (10%-20%), particularly analysing accuracy, recall, F1-score,\ndetection and correction rates, and false discovery rate. The experimentation\nis conducted for four ML algorithms, namely Random Forest (RF), Decision Tree\n(DT), Gaussian Naive Bayes (GNB) and Multilayer Perceptron (MLP), trained with\nthree public datasets, against three poisoning attacks and compared with two\nexisting mitigations. Our results highlight that SecureLearn is effective\nagainst the provided attacks. SecureLearn has strengthened resilience and\nadversarial robustness of traditional multiclass models and neural networks,\nconfirming its generalization beyond algorithm-specific defenses. It\nconsistently maintained accuracy above 90%, recall and F1-score above 75%. For\nneural networks, SecureLearn achieved 97% recall and F1-score against all\nselected poisoning attacks.", "AI": {"tldr": "The paper proposes SecureLearn, a two-layer defense method for multiclass models against data poisoning attacks. It outperforms existing defenses with high accuracy, recall, and F1-score across multiple algorithms and datasets.", "motivation": "Data poisoning attacks target machine learning models, especially multiclass classifiers, which are crucial in multi-modal applications. Current defenses are either attack-specific or limited to certain ML types like deep neural networks, leaving traditional multiclass models vulnerable.", "method": "SecureLearn features two components: data sanitization and feature-oriented adversarial training. It is evaluated using a 3D matrix involving various poisoning attacks, data sanitization, and adversarial training aspects, with metrics like accuracy, recall, F1-score, and detection rates.", "result": "SecureLearn achieved over 90% accuracy, 75% recall and F1-score for traditional models, and 97% recall and F1-score for neural networks across multiple poisoning attacks and ML algorithms, surpassing existing mitigation strategies.", "conclusion": "SecureLearn is a robust, attack-agnostic defense for multiclass models against data poisoning, showing high effectiveness and generalization to both traditional models and neural networks."}}
{"id": "2510.21966", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21966", "abs": "https://arxiv.org/abs/2510.21966", "authors": ["Musengamana Jean de Dieu", "Ruiyin Li", "Peng Liang", "Mojtaba Shahin", "Muhammad Waseem", "Arif Ali Khan", "Bangchao Wang", "Mst Shamima Aktar"], "title": "ArchISMiner: A Framework for Automatic Mining of Architectural Issue-Solution Pairs from Online Developer Communities", "comment": "42 pages, 14 images, 6 tables, Manuscript submitted to a Journal\n  (2025)", "summary": "Stack Overflow (SO), a leading online community forum, is a rich source of\nsoftware development knowledge. However, locating architectural knowledge, such\nas architectural solutions remains challenging due to the overwhelming volume\nof unstructured content and fragmented discussions. Developers must manually\nsift through posts to find relevant architectural insights, which is\ntime-consuming and error-prone. This study introduces ArchISMiner, a framework\nfor mining architectural knowledge from SO. The framework comprises two\ncomplementary components: ArchPI and ArchISPE. ArchPI trains and evaluates\nmultiple models, including conventional ML/DL models, Pre-trained Language\nModels (PLMs), and Large Language Models (LLMs), and selects the\nbest-performing model to automatically identify Architecture-Related Posts\n(ARPs) among programming-related discussions. ArchISPE employs an indirect\nsupervised approach that leverages diverse features, including BERT embeddings\nand local TextCNN features, to extract architectural issue-solution pairs. Our\nevaluation shows that the best model in ArchPI achieves an F1-score of 0.960 in\nARP detection, and ArchISPE outperforms baselines in both SE and NLP fields,\nachieving F1-scores of 0.883 for architectural issues and 0.894 for solutions.\nA user study further validated the quality (e.g., relevance and usefulness) of\nthe identified ARPs and the extracted issue-solution pairs. Moreover, we\napplied ArchISMiner to three additional forums, releasing a dataset of over 18K\narchitectural issue-solution pairs. Overall, ArchISMiner can help architects\nand developers identify ARPs and extract succinct, relevant, and useful\narchitectural knowledge from developer communities more accurately and\nefficiently. The replication package of this study has been provided at\nhttps://github.com/JeanMusenga/ArchISPE", "AI": {"tldr": "ArchISMiner is a framework that mines architectural knowledge from Stack Overflow using components ArchPI and ArchISPE to detect ARPs and extract issue-solution pairs with high accuracy.", "motivation": "Developers struggle to find architectural knowledge in SO due to unstructured content, requiring manual sorting through posts, which is time-consuming and error-prone.", "method": "The framework uses ArchPI for training and evaluating ML, DL, and LLM models to identify ARPs and ArchISPE for feature-based extraction of architectural issues and solutions using BERT and TextCNN.", "result": "ArchISMiner achieves an F1-score of 0.960 for ARP detection and 0.883-0.894 for issue and solution pairs. It was validated through user studies and additional forum applications.", "conclusion": "ArchISMiner efficiently assists architects and developers in extracting and identifying architectural knowledge from online developer communities with accuracy."}}
{"id": "2510.22283", "categories": ["cs.CR", "cs.LG", "cs.SY", "eess.SY", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2510.22283", "abs": "https://arxiv.org/abs/2510.22283", "authors": ["Devon A. Kelly", "Christiana Chamon"], "title": "Adapting Noise-Driven PUF and AI for Secure WBG ICS: A Proof-of-Concept Study", "comment": null, "summary": "Wide-bandgap (WBG) technologies offer unprecedented improvements in power\nsystem efficiency, size, and performance, but also introduce unique sensor\ncorruption and cybersecurity risks in industrial control systems (ICS),\nparticularly due to high-frequency noise and sophisticated cyber-physical\nthreats. This proof-of-concept (PoC) study demonstrates the adaptation of a\nnoise-driven physically unclonable function (PUF) and machine learning\n(ML)-assisted anomaly detection framework to the demanding environment of\nWBG-based ICS sensor pathways. By extracting entropy from unavoidable WBG\nswitching noise (up to 100 kHz) as a PUF source, and simultaneously using this\nnoise as a real-time threat indicator, the proposed system unites\nhardware-level authentication and anomaly detection. Our approach integrates\nhybrid machine learning (ML) models with adaptive Bayesian filtering, providing\nrobust and low-latency detection capabilities resilient to both natural\nelectromagnetic interference (EMI) and active adversarial manipulation. Through\ndetailed simulations of WBG modules under benign and attack\nscenarios--including EMI injection, signal tampering, and node\nimpersonation--we achieve 95% detection accuracy and sub-millisecond processing\nlatency. These results demonstrate the feasibility of physics-driven, dual-use\nnoise exploitation as a scalable ICS defense primitive. Our findings lay the\ngroundwork for next-generation security strategies that leverage inherent\ndevice characteristics, bridging hardware and artificial intelligence (AI) for\nenhanced protection of critical ICS infrastructure.", "AI": {"tldr": "The paper proposes a novel security framework combining noise-driven PUFs and ML for WBG-based ICS, demonstrating high detection accuracy and low latency.", "motivation": "WBG tech improves ICS efficiency but introduces sensor corruption and cybersecurity risks from high-frequency noise and threats. Existing solutions do not address these challenges effectively.", "method": "They adapt a PUF using WBG switching noise and ML models with Bayesian filtering to detect anomalies, testing via simulations of various attack scenarios.", "result": "95% detection accuracy and sub-millisecond latency in simulated attacks, confirming the framework's robustness and scalability.", "conclusion": "The approach merges hardware and AI to enhance ICS security, offering a scalable defense primitive against noise-based and cyber threats."}}
{"id": "2510.21993", "categories": ["cs.SE", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.21993", "abs": "https://arxiv.org/abs/2510.21993", "authors": ["Yupeng Qi", "Ran Xu", "Xu Chu"], "title": "FeaGPT: an End-to-End agentic-AI for Finite Element Analysis", "comment": null, "summary": "Large language models (LLMs) are establishing new paradigms for engineering\napplications by enabling natural language control of complex computational\nworkflows. This paper introduces FeaGPT, the first framework to achieve\ncomplete geometry-mesh-simulation workflows through conversational interfaces.\nUnlike existing tools that automate individual FEA components, FeaGPT\nimplements a fully integrated Geometry-Mesh-Simulation-Analysis (GMSA) pipeline\nthat transforms engineering specifications into validated computational results\nwithout manual intervention. The system interprets engineering intent,\nautomatically generates physics-aware adaptive meshes, configures complete FEA\nsimulations with proper boundary condition inference, and performs\nmulti-objective analysis through closed-loop iteration.\n  Experimental validation confirms complete end-to-end automation capability.\nIndustrial turbocharger cases (7-blade compressor and 12-blade turbine at\n\\SI{110000}{rpm}) demonstrate the system successfully transforms natural\nlanguage specifications into validated CalculiX simulations, producing\nphysically realistic results for rotating machinery analysis. Additional\nvalidation through 432 NACA airfoil configurations confirms scalability for\nparametric design exploration. These results demonstrate that natural language\ninterfaces can effectively democratize access to advanced computational\nengineering tools while preserving analytical rigor.", "AI": {"tldr": "FeaGPT is a novel framework that automates the entire geometry-mesh-simulation workflow for engineering applications using natural language. It successfully transforms engineering specs into validated simulations, confirmed through industrial and parametric studies.", "motivation": "To bridge the gap between complex FEA processes and user accessibility by enabling fully automated workflows through natural language interaction.", "method": "FeaGPT implements an integrated GMSA pipeline with modules for intent interpretation, mesh generation, simulation setup including boundary condition inference, and multi-objective analysis through closed-loop iteration.", "result": "Validated CalculiX simulations for turbocharger cases and successful processing of 432 NACA airfoil configurations, confirming end-to-end automation and scalability.", "conclusion": "Natural language interfaces can democratize advanced FEA tools while maintaining analytical accuracy, marking a significant step in computational engineering accessibility."}}
{"id": "2510.22300", "categories": ["cs.CR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.22300", "abs": "https://arxiv.org/abs/2510.22300", "authors": ["Chenyu Zhang", "Tairen Zhang", "Lanjun Wang", "Ruidong Chen", "Wenhui Li", "Anan Liu"], "title": "T2I-RiskyPrompt: A Benchmark for Safety Evaluation, Attack, and Defense on Text-to-Image Model", "comment": "AAAI under review", "summary": "Using risky text prompts, such as pornography and violent prompts, to test\nthe safety of text-to-image (T2I) models is a critical task. However, existing\nrisky prompt datasets are limited in three key areas: 1) limited risky\ncategories, 2) coarse-grained annotation, and 3) low effectiveness. To address\nthese limitations, we introduce T2I-RiskyPrompt, a comprehensive benchmark\ndesigned for evaluating safety-related tasks in T2I models. Specifically, we\nfirst develop a hierarchical risk taxonomy, which consists of 6 primary\ncategories and 14 fine-grained subcategories. Building upon this taxonomy, we\nconstruct a pipeline to collect and annotate risky prompts. Finally, we obtain\n6,432 effective risky prompts, where each prompt is annotated with both\nhierarchical category labels and detailed risk reasons. Moreover, to facilitate\nthe evaluation, we propose a reason-driven risky image detection method that\nexplicitly aligns the MLLM with safety annotations. Based on T2I-RiskyPrompt,\nwe conduct a comprehensive evaluation of eight T2I models, nine defense\nmethods, five safety filters, and five attack strategies, offering nine key\ninsights into the strengths and limitations of T2I model safety. Finally, we\ndiscuss potential applications of T2I-RiskyPrompt across various research\nfields. The dataset and code are provided in\nhttps://github.com/datar001/T2I-RiskyPrompt.", "AI": {"tldr": "", "motivation": "", "method": "", "result": "", "conclusion": ""}}
{"id": "2510.22003", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22003", "abs": "https://arxiv.org/abs/2510.22003", "authors": ["Stefan Julian Kooy", "Jean Paul Sebastian Piest", "Rob Henk Bemthuis"], "title": "Impact and Implications of Generative AI for Enterprise Architects in Agile Environments: A Systematic Literature Review", "comment": "17 pages, 1 figure, 5 tables; to appear in Enterprise Design,\n  Operations, and Computing. EDOC 2025 Workshops, Lecture Notes in Business\n  Information Processing (LNBIP), Springer, 2025. Part of 29th International\n  Conference on Enterprise Design, Operations, and Computing (EDOC)", "summary": "Generative AI (GenAI) is reshaping enterprise architecture work in agile\nsoftware organizations, yet evidence on its effects remains scattered. We\nreport a systematic literature review (SLR), following established SLR\nprotocols of Kitchenham and PRISMA, of 1,697 records, yielding 33 studies\nacross enterprise, solution, domain, business, and IT architect roles. GenAI\nmost consistently supports (i) design ideation and trade-off exploration; (ii)\nrapid creation and refinement of artifacts (e.g., code, models, documentation);\nand (iii) architectural decision support and knowledge retrieval. Reported\nrisks include opacity and bias, contextually incorrect outputs leading to\nrework, privacy and compliance concerns, and social loafing. We also identify\nemerging skills and competencies, including prompt engineering, model\nevaluation, and professional oversight, and organizational enablers around\nreadiness and adaptive governance. The review contributes with (1) a mapping of\nGenAI use cases and risks in agile architecting, (2) implications for\ncapability building and governance, and (3) an initial research agenda on\nhuman-AI collaboration in architecture. Overall, the findings inform\nresponsible adoption of GenAI that accelerates digital transformation while\nsafeguarding architectural integrity.", "AI": {"tldr": "This paper provides a systematic literature review of 33 studies on Generative AI in enterprise architecture, highlighting key use cases, risks, required skills, and implications for governance and research.", "motivation": "Generative AI is changing enterprise architecture, but there is a lack of comprehensive evidence. The study aims to gather and synthesize existing knowledge to guide responsible adoption.", "method": "The authors conducted a systematic literature review following established protocols (Kitchenham and PRISMA), starting with 1,697 records and selecting 33 relevant studies.", "result": "Findings include GenAI's support for design, artifact creation, and decision-making; risks like opacity and bias; and insights on required skills and organizational enablers for adoption.", "conclusion": "The study maps GenAI use cases and risks in agile architecting, offers guidance on capability building and governance, and outlines a research agenda for human-AI collaboration in architecture."}}
{"id": "2510.22387", "categories": ["cs.CR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22387", "abs": "https://arxiv.org/abs/2510.22387", "authors": ["Nader Nemati"], "title": "Privacy-Aware Federated nnU-Net for ECG Page Digitization", "comment": null, "summary": "Deep neural networks can convert ECG page images into analyzable waveforms,\nyet centralized training often conflicts with cross-institutional privacy and\ndeployment constraints. A cross-silo federated digitization framework is\npresented that trains a full-model nnU-Net segmentation backbone without\nsharing images and aggregates updates across sites under realistic non-IID\nheterogeneity (layout, grid style, scanner profile, noise).\n  The protocol integrates three standard server-side aggregators--FedAvg,\nFedProx, and FedAdam--and couples secure aggregation with central, user-level\ndifferential privacy to align utility with formal guarantees. Key features\ninclude: (i) end-to-end full-model training and synchronization across clients;\n(ii) secure aggregation so the server only observes a clipped, weighted sum\nonce a participation threshold is met; (iii) central Gaussian DP with Renyi\naccounting applied post-aggregation for auditable user-level privacy; and (iv)\na calibration-aware digitization pipeline comprising page normalization, trace\nsegmentation, grid-leakage suppression, and vectorization to twelve-lead\nsignals.\n  Experiments on ECG pages rendered from PTB-XL show consistently faster\nconvergence and higher late-round plateaus with adaptive server updates\n(FedAdam) relative to FedAvg and FedProx, while approaching centralized\nperformance. The privacy mechanism maintains competitive accuracy while\npreventing exposure of raw images or per-client updates, yielding deployable,\nauditable guarantees suitable for multi-institution settings.", "AI": {"tldr": "This paper introduces a federated learning framework for digitizing ECG page images while ensuring privacy and handling cross-institutional deployment challenges.", "motivation": "Centralized training of neural networks for ECG digitization conflicts with cross-institutional privacy and deployment constraints.", "method": "The paper presents a cross-silo federated framework using full-model nnU-Net segmentation backbone training without image sharing. Adaptive server-side aggregators (FedAdam), secure aggregation, and central differential privacy are integrated. A calibration-aware digitization pipeline is also included.", "result": "Experiments on PTB-XL ECG pages show that FedAdam achieves faster convergence and higher accuracy. The framework maintains competitive performance while ensuring privacy, preventing raw image or update exposure.", "conclusion": "The proposed framework offers deployable ECG digitization with strong privacy guarantees, suitable for multi-institutional settings, and achieves centralized-level performance in ECG waveform conversion."}}
{"id": "2510.22210", "categories": ["cs.SE", "cs.AI", "D.2.5"], "pdf": "https://arxiv.org/pdf/2510.22210", "abs": "https://arxiv.org/abs/2510.22210", "authors": ["Gwihwan Go", "Quan Zhang", "Chijin Zhou", "Zhao Wei", "Yu Jiang"], "title": "LSPRAG: LSP-Guided RAG for Language-Agnostic Real-Time Unit Test Generation", "comment": "13pages, 6 figures", "summary": "Automated unit test generation is essential for robust software development,\nyet existing approaches struggle to generalize across multiple programming\nlanguages and operate within real-time development. While Large Language Models\n(LLMs) offer a promising solution, their ability to generate high coverage test\ncode depends on prompting a concise context of the focal method. Current\nsolutions, such as Retrieval-Augmented Generation, either rely on imprecise\nsimilarity-based searches or demand the creation of costly, language-specific\nstatic analysis pipelines. To address this gap, we present LSPRAG, a framework\nfor concise-context retrieval tailored for real-time, language-agnostic unit\ntest generation. LSPRAG leverages off-the-shelf Language Server Protocol (LSP)\nback-ends to supply LLMs with precise symbol definitions and references in real\ntime. By reusing mature LSP servers, LSPRAG provides an LLM with language-aware\ncontext retrieval, requiring minimal per-language engineering effort. We\nevaluated LSPRAG on open-source projects spanning Java, Go, and Python.\nCompared to the best performance of baselines, LSPRAG increased line coverage\nby up to 174.55% for Golang, 213.31% for Java, and 31.57% for Python.", "AI": {"tldr": "LSPRAG is a framework for language-agnostic, real-time unit test generation using Language Server Protocol (LSP), improving test coverage by 31.57%-213.31% across Java/Go/Python without language-specific engineering.", "motivation": "Existing test generation approaches suffer from poor language generalization, imprecise context retrieval via similarity searches, and high infrastructure costs for language pipelines.", "method": "Leverages existing LSP backends to provide LLMs with precise symbol definitions/references in real-time, reusing mature language servers for context retrieval with minimal language-specific effort.", "result": "For Java/Go/Python projects, LSPRAG outperformed baselines by 174.55%, 213.31%, and 31.57%-point line coverage improvements respectively in open-source evaluations.", "conclusion": "LSPRAG demonstrates that language server integration is a scalable solution for high-coverage test generation across programming languages without custom static analysis pipelines."}}
{"id": "2510.22396", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.22396", "abs": "https://arxiv.org/abs/2510.22396", "authors": ["Zhaoyang Li", "Zheng Yu", "Jingyi Song", "Meng Xu", "Yuxuan Luo", "Dongliang Mu"], "title": "PortGPT: Towards Automated Backporting Using Large Language Models", "comment": "Accepted by IEEE S&P 2026", "summary": "Patch backporting, the process of migrating mainline security patches to\nolder branches, is an essential task in maintaining popular open-source\nprojects (e.g., Linux kernel). However, manual backporting can be\nlabor-intensive, while existing automated methods, which heavily rely on\npredefined syntax or semantic rules, often lack agility for complex patches.\n  In this paper, we introduce PORTGPT, an LLM-agent for end-to-end automation\nof patch backporting in real-world scenarios. PORTGPT enhances an LLM with\ntools to access code on-demand, summarize Git history, and revise patches\nautonomously based on feedback (e.g., from compilers), hence, simulating\nhuman-like reasoning and verification. PORTGPT achieved an 89.15% success rate\non existing datasets (1815 cases), and 62.33% on our own dataset of 146 complex\ncases, both outperforms state-of-the-art of backporting tools. We contributed 9\nbackported patches from PORTGPT to the Linux kernel community and all patches\nare now merged.", "AI": {"tldr": "PORTGPT automates patch backporting via LLM agents, outperforming tools and contributing to Linux kernel security.", "motivation": "Manual patch backporting is labor-intensive, and existing automated tools lack agility for complex patches in open-source projects like the Linux kernel.", "method": "PORTGPT uses an LLM agent enhanced with code access, Git history summarization, and autonomous patch revision tools to simulate human-like reasoning.", "result": "PORTGPT achieved 89.15% success on existing datasets and 62.33% on complex cases, with 9 patches merged into the Linux kernel community.", "conclusion": "PORTGPT outperforms existing methods in patch backporting, achieving high success rates and contributing real-world patches to the Linux kernel."}}
{"id": "2510.22224", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.LO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.22224", "abs": "https://arxiv.org/abs/2510.22224", "authors": ["Guan-Yan Yang", "Farn Wang"], "title": "Taming Silent Failures: A Framework for Verifiable AI Reliability", "comment": "This preprint has been accepted by IEEE Reliability Magazine. 10\n  pages, 3 figures", "summary": "The integration of Artificial Intelligence (AI) into safety-critical systems\nintroduces a new reliability paradigm: silent failures, where AI produces\nconfident but incorrect outputs that can be dangerous. This paper introduces\nthe Formal Assurance and Monitoring Environment (FAME), a novel framework that\nconfronts this challenge. FAME synergizes the mathematical rigor of offline\nformal synthesis with the vigilance of online runtime monitoring to create a\nverifiable safety net around opaque AI components. We demonstrate its efficacy\nin an autonomous vehicle perception system, where FAME successfully detected\n93.5% of critical safety violations that were otherwise silent. By\ncontextualizing our framework within the ISO 26262 and ISO/PAS 8800 standards,\nwe provide reliability engineers with a practical, certifiable pathway for\ndeploying trustworthy AI. FAME represents a crucial shift from accepting\nprobabilistic performance to enforcing provable safety in next-generation\nsystems.", "AI": {"tldr": "FAME addresses AI silent failures in safety-critical systems via formal synthesis and runtime monitoring, enabling certifiable safety for autonomous vehicles.", "motivation": "AI's silent failures (confidently incorrect outputs) pose safety risks in critical systems, requiring verifiable assurance frameworks beyond traditional reliability approaches.", "method": "FAME combines offline formal synthesis (mathematical rigor for safety preconditions) with online runtime monitoring (dynamic violation detection), validated on autonomous vehicle perception systems.", "result": "Achieved 93.56% detection of critical safety violations in autonomous vehicle tests, with alignment to ISO 26262/PAS 8800 standards enabling certifiable AI deployment.", "conclusion": "FAME shifts AI system design from probabilistic performance to provable safety, providing a certified pathway for trustworthy implementation in compliance with industry standards."}}
{"id": "2510.22400", "categories": ["cs.CR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.22400", "abs": "https://arxiv.org/abs/2510.22400", "authors": ["Fei Shao", "Jia Zou", "Zhichao Cao", "Xusheng Xiao"], "title": "ProGQL: A Provenance Graph Query System for Cyber Attack Investigation", "comment": null, "summary": "Provenance analysis (PA) has recently emerged as an important solution for\ncyber attack investigation. PA leverages system monitoring to monitor system\nactivities as a series of system audit events and organizes these events as a\nprovenance graph to show the dependencies among system activities, which can\nreveal steps of cyber attacks. Despite their potential, existing PA techniques\nface two critical challenges: (1) they are inflexible and non-extensible,\nmaking it difficult to incorporate analyst expertise, and (2) they are memory\ninefficient, often requiring>100GB of RAM to hold entire event streams, which\nfundamentally limits scalability and deployment in real-world environments. To\naddress these limitations, we propose the PROGQL framework, which provides a\ndomain-specific graph search language with a well-engineered query engine,\nallowing PA over system audit events and expert knowledge to be jointly\nexpressed as a graph search query and thereby facilitating the investigation of\ncomplex cyberattacks. In particular, to support dependency searches from a\nstarting edge required in PA, PROGQL introduces new language constructs for\nconstrained graph traversal, edge weight computation, value propagation along\nweighted edges, and graph merging to integrate multiple searches. Moreover, the\nPROGQL query engine is optimized for efficient incremental graph search across\nheterogeneous database backends, eliminating the need for full in-memory\nmaterialization and reducing memory overhead. Our evaluations on real attacks\ndemonstrate the effectiveness of the PROGQL language in expressing a diverse\nset of complex attacks compared with the state-of-the-art graph query language\nCypher, and the comparison with the SOTA PA technique DEPIMPACT further\ndemonstrates the significant improvement of the scalability brought by our\nPROGQL framework's design.", "AI": {"tldr": "PROGQL improves cyber attack investigation by combining analyst expertise with scalable, memory-efficient provenance analysis through a novel graph query language and optimized engine, outperforming existing methods.", "motivation": "Existing PA techniques are inflexible, non-extensible, and memory-inefficient, making them unsuitable for real-world deployment and analyst collaboration. These issues hinder their scalability and ability to integrate expert knowledge.", "method": "PROGQL introduces a domain-specific graph search language with constructs for constrained traversal, edge weight computation, value propagation, and graph merging. The query engine supports incremental graph search across heterogeneous databases, eliminating full in-memory materialization.", "result": "Evaluations on real attacks demonstrate PROGQL's superior expressiveness compared to Cypher and significant scalability improvements over DEPIMPACT, validating its design for handling complex PA tasks with reduced memory overhead.", "conclusion": "The PROGQL framework addresses critical limitations in existing provenance analysis (PA) techniques by offering a scalable, memory-efficient solution through a domain-specific graph search language and optimized query engine. It enhances flexibility for incorporating analyst expertise while reducing memory overhead, enabling effective investigation of complex cyberattacks."}}
{"id": "2510.22249", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22249", "abs": "https://arxiv.org/abs/2510.22249", "authors": ["Ibuki Nakamura", "Yutaro Kashiwa", "Bin Lin", "Hajimu Iida"], "title": "Understanding Self-Admitted Technical Debt in Test Code: An Empirical Study", "comment": null, "summary": "Developers often opt for easier but non-optimal implementation to meet\ndeadlines or create rapid prototypes, leading to additional effort known as\ntechnical debt to improve the code later. Oftentimes, developers explicitly\ndocument the technical debt in code comments, referred to as Self-Admitted\nTechnical Debt (SATD). Numerous researchers have investigated the impact of\nSATD on different aspects of software quality and development processes.\nHowever, most of these studies focus on SATD in production code, often\noverlooking SATD in the test code or assuming that it shares similar\ncharacteristics with SATD in production code. In fact, a significant amount of\nSATD is also present in the test code, with many instances not fitting into\nexisting categories for the production code. This study aims to fill this gap\nand disclose the nature of SATD in the test code by examining its distribution\nand types. Moreover, the relation between its presence and test quality is also\nanalyzed. Our empirical study, involving 17,766 SATD comments (14,987 from\nproduction code, 2,779 from test code) collected from 50 repositories,\ndemonstrates that while SATD widely exists in test code, it is not directly\nassociated with test smells. Our study also presents comprehensive categories\nof SATD types in the test code, and machine learning models are developed to\nautomatically classify SATD comments based on their types for easier\nmanagement. Our results show that the CodeBERT-based model outperforms other\nmachine learning models in terms of recall and F1-score. However, the\nperformance varies on different types of SATD.", "AI": {"tldr": "The paper discusses the prevalence of Self-Admitted Technical Debt (SATD) in test code, its different types, and its impact on test quality. It also introduces a machine learning model for categorizing SATD in test code.", "motivation": "Existing studies on SATD often focus on production code and assume similar characteristics in test code, ignoring potential differences. The authors aim to address this gap by analyzing test code SATD and its implications.", "method": "The authors collected 17,766 SATD comments from 50 repositories, analyzed the distribution and types of SATD in test code, and used machine learning models for classification, including a CodeBERT-based model.", "result": "The study found that SATD is common in test code but not directly linked to test smells. The CodeBERT model achieved the best performance in classifying SATD comments according to their types.", "conclusion": "The paper concludes that SATD in test code requires specific attention, as its characteristics differ from production code. Future approaches should consider these differences for better management and analysis of technical debt."}}
{"id": "2510.22536", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.22536", "abs": "https://arxiv.org/abs/2510.22536", "authors": ["Jotaro Yano"], "title": "ZK Coprocessor Bridge: Replay-Safe Private Execution from Solana to Aztec via Wormhole", "comment": null, "summary": "We formalize a cross-domain \"ZK coprocessor bridge\" that lets Solana programs\nrequest private execution on Aztec L2 (via Ethereum) using Wormhole Verifiable\nAction Approvals (VAAs) as authenticated transport. The system comprises: (i) a\nSolana program that posts messages to Wormhole Core with explicit finality;\n(ii) an EVM Portal that verifies VAAs, enforces a replay lock, parses a bound\npayload secretHash||m from the attested VAA, derives a domain-separated field\ncommitment, and enqueues an L1->L2 message into the Aztec Inbox (our reference\nimplementation v0.1.0 currently uses consumeWithSecret(vaa, secretHash); we\nprovide migration guidance to the payload-bound interface); (iii) a minimal\nAztec contract that consumes the message privately; and (iv) an off-chain\nrelayer that ferries VAAs and can record receipts on Solana. We present state\nmachines, message formats, and proof sketches for replay-safety, origin\nauthenticity, finality alignment, parameter binding (no relayer front-running\nof Aztec parameters), privacy, idempotence, and liveness. Finally, we include a\nconcise Reproducibility note with pinned versions and artifacts to replicate a\npublic testnet run.", "AI": {"tldr": "The study introduces a formalized framework of a cross-domain ZK coprocessor bridge which links Solana with the Aztec L2, and explains the system's components, security features, and reproducibility. *break", "motivation": "This research is motivated by the desire to enable private execution of Solana programs on Aztec L2 leveraging Ethereum, thus improving scalability, privacy, and finality across different blockchain domains. *break", "method": "The method involves creating a system that uses Wormhole VAAs for secure message transport, enforces replay-safety with a replay lock, provides domain-separated field commitment, frequent state machine definitions, and proof sketches for security and functional properties. *break", "result": "The system promises secure, private, and scalable inter-chain compute with implications for ZK cross-domain coprocessing, including a reference implementation and specific operational strategies in situations of different VAA verification interfaces. *break", "conclusion": "The system concludes with a framework that assures aligned finality and private, secure, cross-chain interaction across Ethereum-based Aztec L2 and Solana, and emphasizes reproducibility with detailed artifacts and versions. *break"}}
{"id": "2510.22254", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22254", "abs": "https://arxiv.org/abs/2510.22254", "authors": ["Eric W. Bridgeford", "Iain Campbell", "Zijao Chen", "Zhicheng Lin", "Harrison Ritz", "Joachim Vandekerckhove", "Russell A. Poldrack"], "title": "Ten Simple Rules for AI-Assisted Coding in Science", "comment": "9 pages of content; 1 table; 1 page appendix", "summary": "While AI coding tools have demonstrated potential to accelerate software\ndevelopment, their use in scientific computing raises critical questions about\ncode quality and scientific validity. In this paper, we provide ten practical\nrules for AI-assisted coding that balance leveraging capabilities of AI with\nmaintaining scientific and methodological rigor. We address how AI can be\nleveraged strategically throughout the development cycle with four key themes:\nproblem preparation and understanding, managing context and interaction,\ntesting and validation, and code quality assurance and iterative improvement.\nThese principles serve to emphasize maintaining human agency in coding\ndecisions, establishing robust validation procedures, and preserving the domain\nexpertise essential for methodologically sound research. These rules are\nintended to help researchers harness AI's transformative potential for faster\nsoftware development while ensuring that their code meets the standards of\nreliability, reproducibility, and scientific validity that research integrity\ndemands.", "AI": {"tldr": "The paper offers ten practical guidelines for using AI coding tools in scientific research, emphasizing the balance between AI's efficiency and scientific rigor through themes like problem understanding, context management, testing, and quality assurance.", "motivation": "AI coding tools may accelerate software development, but their integration into scientific computing necessitates ensuring code quality and scientific validity, which are critical for research integrity.", "method": "We present four key themes and ten practical rules for AI-assisted coding to help researchers maintain methodological rigor while leveraging AI capabilities.", "result": "The ten rules facilitate effective integration of AI into the scientific development process, ensuring code reliability and reproducibility.", "conclusion": "Researchers can effectively use AI coding tools without compromising the quality and validity of their scientific work by following these ten rules, which ensure human oversight and rigorous validation."}}
{"id": "2510.22555", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22555", "abs": "https://arxiv.org/abs/2510.22555", "authors": ["Dongyi Liu", "Jiangtong Li", "Dawei Cheng", "Changjun Jiang"], "title": "Cross-Paradigm Graph Backdoor Attacks with Promptable Subgraph Triggers", "comment": null, "summary": "Graph Neural Networks(GNNs) are vulnerable to backdoor attacks, where\nadversaries implant malicious triggers to manipulate model predictions.\n  Existing trigger generators are often simplistic in structure and overly\nreliant on specific features, confining them to a single graph learning\nparadigm, such as graph supervised learning, graph contrastive learning, or\ngraph prompt learning.\n  This specialized design, which aligns the trigger with one learning\nobjective, results in poor transferability when applied to other learning\nparadigms.\n  For instance, triggers generated for the graph supervised learning paradigm\nperform poorly when tested within graph contrastive learning or graph prompt\nlearning environments.\n  Furthermore, these simple generators often fail to utilize complex structural\ninformation or node diversity within the graph data.\n  These constraints limit the attack success rates of such methods in general\ntesting scenarios.\n  Therefore, to address these limitations, we propose Cross-Paradigm Graph\nBackdoor Attacks with Promptable Subgraph Triggers(CP-GBA), a new transferable\ngraph backdoor attack that employs graph prompt learning(GPL) to train a set of\nuniversal subgraph triggers.\n  First, we distill a compact yet expressive trigger set from target graphs,\nwhich is structured as a queryable repository, by jointly enforcing\nclass-awareness, feature richness, and structural fidelity.\n  Second, we conduct the first exploration of the theoretical transferability\nof GPL to train these triggers under prompt-based objectives, enabling\neffective generalization to diverse and unseen test-time paradigms.\n  Extensive experiments across multiple real-world datasets and defense\nscenarios show that CP-GBA achieves state-of-the-art attack success rates.", "AI": {"tldr": "The paper introduces the CP-GBA method, a transferable graph backdoor attack that employs graph prompt learning to generate universal subgraph triggers. These triggers are distilled from target graphs without being confined to a single graph learning paradigm.", "motivation": "Current graph backdoor attack methods are limited to specific graph learning paradigms, possess simplistic structures, and underutilize complex structural information of graphs. This results in poor transferability and constrained attack success rates.", "method": "The proposed framework, CP-GBA, distills a set of universal subgraph triggers through structuring a class-aware, feature-rich, and structurally consistent repository, then employs graph prompt learning to train them on prompt-based objectives. This promotes adaptation across paradigms.", "result": "Empirical evaluation confirmed the state-of-the-art performance in attack success rates of CP-GBA on various real-world datasets and defense scenarios, demonstrating strong generalization capability across multiple graph learning paradigms.", "conclusion": "The study suggests the significance of incorporating element universality and pretrained paradigms in improving the robustness of backdoor attacks across different graph learning scenarios.  The proposed approach demonstrates that designing attacks using an abstracted, paradigm-agnostic trigger structure can significantly enhance method effectiveness and adaptability."}}
{"id": "2510.22318", "categories": ["cs.SE", "cs.AI", "K.3.2, D.2.5"], "pdf": "https://arxiv.org/pdf/2510.22318", "abs": "https://arxiv.org/abs/2510.22318", "authors": ["Tuan-Phong Ngo", "Bao-Ngoc Duong", "Tuan-Anh Hoang", "Joshua Dwight", "Ushik Shrestha Khwakhali"], "title": "Harnessing the Power of Large Language Models for Software Testing Education: A Focus on ISTQB Syllabus", "comment": "7 pages, 3 figures, 3 tables", "summary": "Software testing is a critical component in the software engineering field\nand is important for software engineering education. Thus, it is vital for\nacademia to continuously improve and update educational methods to reflect the\ncurrent state of the field. The International Software Testing Qualifications\nBoard (ISTQB) certification framework is globally recognized and widely adopted\nin industry and academia. However, ISTQB-based learning has been rarely applied\nwith recent generative artificial intelligence advances. Despite the growing\ncapabilities of large language models (LLMs), ISTQB-based learning and\ninstruction with LLMs have not been thoroughly explored. This paper explores\nand evaluates how LLMs can complement the ISTQB framework for higher education.\nThe findings present four key contributions: (i) the creation of a\ncomprehensive ISTQB-aligned dataset spanning over a decade, consisting of 28\nsample exams and 1,145 questions; (ii) the development of a domain-optimized\nprompt that enhances LLM precision and explanation quality on ISTQB tasks;\n(iii) a systematic evaluation of state-of-the-art LLMs on this dataset; and\n(iv) actionable insights and recommendations for integrating LLMs into software\ntesting education. These findings highlight the promise of LLMs in supporting\nISTQB certification preparation and offer a foundation for their broader use in\nsoftware engineering at higher education.", "AI": {"tldr": "This paper explores integrating generative AI with ISTQB-based learning in higher education, creating a dataset, optimizing prompts, evaluating LLMs, and providing actionable insights.", "motivation": "Academia needs to update educational methods to keep pace with advancements in software testing and AI. While ISTQB is widely recognized, its integration with recent generative AI advancements like LLMs has not been thoroughly explored, especially in higher education contexts.", "method": "The research involves four main contributions: building a comprehensive ISTQB-aligned dataset with historical exams and questions; developing domain-optimized prompts for LLM performance in testing tasks; systematically evaluating SotA LLMs on the dataset; and generating educational recommendations based on these findings.", "result": "The study created a decade-spanning dataset (28 exams, 1,145 questions), developed optimized prompts significantly improving LLM performance, and systematically validated these on various LLMs. The implemented evaluation framework yielded actionable educational integration strategies.", "conclusion": "LLMs show promise in supporting ISTQB certification training through enhanced performance on testing tasks when given appropriate domain knowledge. This demonstrates a viable path to integrate advanced AI tools into software engineering education as a scalable teaching resource."}}
{"id": "2510.22561", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22561", "abs": "https://arxiv.org/abs/2510.22561", "authors": ["Kaveri Banerjee", "Sajal Saha"], "title": "Blockchain Signatures to Ensure Information Integrity and Non-Repudiation in the Digital Era: A comprehensive study", "comment": "13 Pages, 2 Figures", "summary": "Blockchain systems rely on decentralized ledgers and strong security\nguarantees. A key requirement is non-repudiation, which prevents denial of\ntransaction authorship and supports integrity of recorded data. This work\nsurveys digital signature schemes used in blockchain platforms and analyzes how\nthey deliver non-repudiation and contribute to overall system security. We\nexamine representative scheme families and their cryptographic foundations,\nsecurity assumptions, and properties relevant to deployment, including\nunforgeability, resistance to malleability, support for aggregation and\nmultisignature or threshold settings, key and signature sizes, and verification\ncost. Using these criteria, we compare the suitability of different designs for\nconsensus protocols, smart contract constraints, and resource limits. We\nhighlight practical tradeoffs that affect throughput, storage, scalability, and\nattack surfaces, and summarize benefits and limitations of each scheme in\nblockchain contexts. The study underscores that carefully chosen digital\nsignatures are central to achieving non-repudiation and preserving information\nintegrity, and it outlines implementation considerations and open directions\nsuch as interoperability and post-quantum readiness.", "AI": {"tldr": "The paper surveys digital signature schemes in blockchain systems, analyzing their cryptographic foundations and properties to evaluate their role in non-repudiation and system security.", "motivation": "Blockchain systems require strong security, particularly non-repudiation to ensure transaction integrity and authorship. The paper aims to inform the suitability of digital signatures for this purpose.", "method": "Examines representative digital signature schemes, their cryptographic foundations, security assumptions, and deployment-relevant attributes like unforgeability, malleability resistance, and efficiency. These are compared against blockchain use cases.", "result": "Highlights tradeoffs between throughput, storage, scalability, and security across schemes, providing a comparative analysis that connects digital signature properties to blockchain design requirements.", "conclusion": "Carefully selected digital signatures are crucial for blockchain non-repudiation and security. The paper offers practical guidance for deployment and identifies open challenges in interoperability and post-quantum readiness."}}
{"id": "2510.22338", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22338", "abs": "https://arxiv.org/abs/2510.22338", "authors": ["Aritra Mitra", "Srijoni Majumdar", "Anamitra Mukhopadhyay", "Partha Pratim Das", "Paul D Clough", "Partha Pratim Chakrabarti"], "title": "Operationalizing Large Language Models with Design-Aware Contexts for Code Comment Generation", "comment": null, "summary": "Comments are very useful to the flow of code development. With the increasing\ncommonality of code, novice coders have been creating a significant amount of\ncodebases. Due to lack of commenting standards, their comments are often\nuseless, and increase the time taken to further maintain codes. This study\nintends to find the usefulness of large language models (LLMs) in these cases\nto generate potentially better comments. This study focuses on the feasibility\nof design documents as a context for the LLMs to generate more useful comments,\nas design documents are often used by maintainers to understand code when\ncomments do not suffice.", "AI": {"tldr": "This study shows that using design documents as context enables large language models to generate more useful code comments, offering a promising solution to the persistent problem of ineffective commenting in novice-developed codebases.", "motivation": "Poorly written code comments by novice developers waste maintenance time; standardization of commenting practices has proven challenging despite the growing importance of code reuse.", "method": "The study evaluates the feasibility of leveraging design documents as contextual input for LLMs to generate more useful code comments compared to traditional approaches.", "result": "Preliminary findings demonstrate that design documents provide sufficient contextual guidance for LLMs to generate semantically meaningful comments that better explain code intent.", "conclusion": "Using large language models (LLMs) with design documents as context can significantly improve code comment quality, addressing common inefficiencies in novice-coded projects."}}
{"id": "2510.22566", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.22566", "abs": "https://arxiv.org/abs/2510.22566", "authors": ["Md. Mehedi Hasan"], "title": "FAARM: Firmware Attestation and Authentication Framework for Mali GPUs", "comment": "10 pages, 8 figures. Preprint version under review in the area of\n  Computer Security (cs.CR)", "summary": "Recent work has revealed MOLE, the first practical attack to compromise GPU\nTrusted Execution Environments (TEEs), by injecting malicious firmware into the\nembedded Microcontroller Unit (MCU) of Arm Mali GPUs. By exploiting the absence\nof cryptographic verification during initialization, adversaries with kernel\nprivileges can bypass memory protections, exfiltrate sensitive data at over 40\nMB/s, and tamper with inference results, all with negligible runtime overhead.\nThis attack surface affects commodity mobile SoCs and cloud accelerators,\nexposing a critical firmware-level trust gap in existing GPU TEE designs. To\naddress this gap, this paper presents FAARM, a lightweight Firmware Attestation\nand Authentication framework that prevents MOLE-style firmware subversion.\nFAARM integrates digital signature verification at the EL3 secure monitor using\nvendor-signed firmware bundles and an on-device public key anchor. At boot, EL3\nverifies firmware integrity and authenticity, enforces version checks, and\nlocks the firmware region, eliminating both pre-verification and\ntime-of-check-to-time-of-use (TOCTOU) attack vectors. We implement FAARM as a\nsoftware-only prototype on a Mali GPU testbed, using a Google Colab-based\nemulation framework that models the firmware signing process, the EL1 to EL3\nload path, and secure memory configuration. FAARM reliably detects and blocks\nmalicious firmware injections, rejecting tampered images before use and denying\noverwrite attempts after attestation. Firmware verification incurs only 1.34 ms\nlatency on average, demonstrating that strong security can be achieved with\nnegligible overhead. FAARM thus closes a fundamental gap in shim-based GPU\nTEEs, providing a practical, deployable defense that raises the security\nbaseline for both mobile and cloud GPU deployments.", "AI": {"tldr": "This paper introduces FAARM, a lightweight firmware attestation framework to secure GPU TEEs against MOLE-style attacks by enforcing cryptographic verification at boot, achieving strong security with 1.34ms overhead.", "motivation": "Current GPU TEEs lack cryptographic firmware verification, leaving a critical trust gap that enables malicious firmware injections (e.g., MOLE attack) to bypass protections, exfiltrate data, and tamper with computations.", "method": "FAARM verifies firmware authenticity at EL3 using vendor-signed bundles and on-device public keys during boot, enforcing version checks and memory lockdown. It is prototyped as a software-only solution on Mali GPUs with Colab-based emulation.", "result": "FAARM reliably blocks firmware subversions (100% detection of tampered images), rejects post-attestation overwrites, and introduces 1.34ms verification latency, demonstrating practical security with minimal performance impact.", "conclusion": "FAARM addresses fundamental vulnerabilities in GPU TEEs via firmware attestation, offering a deployable defense that enhances security for mobile and cloud GPUs without compromising efficiency."}}
{"id": "2510.22409", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22409", "abs": "https://arxiv.org/abs/2510.22409", "authors": ["Shahidul Islam", "Md Nahidul Islam Opu", "Shaowei Wang", "Shaiful Chowdhury"], "title": "A First Look at the Self-Admitted Technical Debt in Test Code: Taxonomy and Detection", "comment": null, "summary": "Self-admitted technical debt (SATD) refers to comments in which developers\nexplicitly acknowledge code issues, workarounds, or suboptimal solutions. SATD\nis known to significantly increase software maintenance effort. While extensive\nresearch has examined SATD in source code, its presence and impact in test code\nhave received no focused attention, leaving a significant gap in our\nunderstanding of how SATD manifests in testing contexts.\n  This study, the first of its kind, investigates SATD in test code by manually\nanalyzing 50,000 comments randomly sampled from 1.6 million comments across\n1,000 open-source Java projects. From this sample, after manual analysis and\nfiltering, we identified 615 SATD comments and classified them into 15 distinct\ncategories, building a taxonomy of test code SATD. To investigate whether test\ncode SATD can be detected automatically, we evaluated existing SATD detection\ntools, as well as both open-source and proprietary LLMs. Among the existing\ntools, MAT performed the best, albeit with moderate recall. To our surprise,\nboth open-source and proprietary LLMs exhibited poor detection accuracy,\nprimarily due to low precision. These results indicate that neither existing\napproaches nor current LLMs can reliably detect SATD in test code.\n  Overall, this work provides the first large-scale analysis of SATD in test\ncode, a nuanced understanding of its types, and the limitations of current SATD\ndetection methods. Our findings lay the groundwork for future research on test\ncode-specific SATD.", "AI": {"tldr": "First study on SATD in test code: found 15 SATD categories, existing detection tools have limited reliability, and LLMs perform poorly. Highlights need for test-specific SATD detection methods.", "motivation": "Existing research on SATD focuses on source code, leaving test code SATD unexplored despite its potential to increase maintenance effort. This work addresses this critical gap.", "method": "Manual analysis of 50,000 comments from 1.6 million test code comments across 1,000 Java projects, combined with evaluation of SATD detection tools (e.g., MAT) and both open-source and proprietary LLMs for automated detection.", "result": "Identified 615 test code SATD comments categorized into 15 types; MAT showed best detection performance (moderate recall), while LLMs had poor accuracy due to low precision.", "conclusion": "This paper provides the first large-scale analysis of SATD in test code, offering a taxonomy of 15 categories, highlighting limitations in existing detection tools and LLMs, and laying groundwork for future test code-specific SATD research."}}
{"id": "2510.22620", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22620", "abs": "https://arxiv.org/abs/2510.22620", "authors": ["Julia Bazinska", "Max Mathys", "Francesco Casucci", "Mateo Rojas-Carulla", "Xander Davies", "Alexandra Souly", "Niklas Pfister"], "title": "Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents", "comment": "Julia Bazinska and Max Mathys contributed equally", "summary": "AI agents powered by large language models (LLMs) are being deployed at\nscale, yet we lack a systematic understanding of how the choice of backbone LLM\naffects agent security. The non-deterministic sequential nature of AI agents\ncomplicates security modeling, while the integration of traditional software\nwith AI components entangles novel LLM vulnerabilities with conventional\nsecurity risks. Existing frameworks only partially address these challenges as\nthey either capture specific vulnerabilities only or require modeling of\ncomplete agents. To address these limitations, we introduce threat snapshots: a\nframework that isolates specific states in an agent's execution flow where LLM\nvulnerabilities manifest, enabling the systematic identification and\ncategorization of security risks that propagate from the LLM to the agent\nlevel. We apply this framework to construct the $\\operatorname{b}^3$ benchmark,\na security benchmark based on 194331 unique crowdsourced adversarial attacks.\nWe then evaluate 31 popular LLMs with it, revealing, among other insights, that\nenhanced reasoning capabilities improve security, while model size does not\ncorrelate with security. We release our benchmark, dataset, and evaluation code\nto facilitate widespread adoption by LLM providers and practitioners, offering\nguidance for agent developers and incentivizing model developers to prioritize\nbackbone security improvements.", "AI": {"tldr": "This paper introduces threat snapshots to systematically analyze how LLM vulnerabilities propagate to agent-level security, creating the b\u00b3 benchmark with 194k adversarial examples and testing 31 LLMs. Results show enhanced reasoning improves security, while size doesn't correlate.", "motivation": "Existing security frameworks for AI agents either focus narrowly on specific vulnerabilities or require full agent modeling, failing to address the integrated risks from LLMs and traditional software components.", "method": "Developed threat snapshots to isolate vulnerable execution states in agent workflows, creating a benchmark via crowdsourced adversarial attacks and applying it to evaluate 31 LLMs for security patterns.", "result": "Found that LLM reasoning capabilities correlate with improved security, model size lacks correlation, and the b\u00b3 benchmark identifies exploitable vulnerabilities in agents' control flows.", "conclusion": "Released benchmarks and datasets to standardize security evaluation, encouraging developers to prioritize LLM backbone security and adopt the framework for systematic risk identification in AI agents."}}
{"id": "2510.22457", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22457", "abs": "https://arxiv.org/abs/2510.22457", "authors": ["Shalini Chakraborty", "Sebastian Baltes"], "title": "A Multifaceted View on Discrimination in Software Development Careers", "comment": "11 pages, 1 figure, 5 tables", "summary": "Conversations around diversity and inclusion in software engineering often\nfocus on gender and racial disparities. However, the State of the Developer\nNation 2025 survey with 8,717 participants revealed that other forms of\ndiscrimination are similarly prevalent but receive considerably less attention.\nThis includes discrimination based on age, political perspective, disabilities,\nor cognitive differences such as neurodivergence. We conducted a secondary\nanalysis of 800 open-ended survey responses to examine patterns of perceived\ndiscrimination, as well as related challenges and negative impacts. Our study\ncovers multiple identity facets, including age, gender, race, and disability.\nWe found that age- and gender-related discrimination was the most frequently\nreported workplace issue, but discrimination based on political and religious\nviews emerged as further notable concerns. Most of the participants who\nidentified as female cited gender as the primary source of discrimination,\noften accompanied by intersectional factors such as race, political views, age,\nor sexual orientation. Discrimination related to caregiving responsibilities\nwas reported by all gender identities. Regarding the negative impacts of\nworkplace issues, many participants described modifying their appearance or\nbehavior in response to gender biases. Gender also appeared to influence\nbroader career challenges, as women and non-binary respondents reported\nexperiencing almost all workplace issues at higher rates, particularly\ndiscrimination (35%) and mental health challenges (62%). Our goal is to raise\nawareness in the research community that discrimination in software development\nis multifaceted, and to encourage researchers to select and assess relevant\nfacets beyond age and gender when designing software engineering studies.", "AI": {"tldr": "This paper highlights the often overlooked forms of discrimination in the software engineering field, specifically age, political perspective, disabilities, and cognitive differences, revealing their prevalence and impact on the workforce. The findings aim to broaden the scope of diversity and inclusion research in this area.", "motivation": "The motivation behind this study stems from the recognition that while gender and racial disparities in software engineering are commonly discussed, other significant yet under-recognized forms of discrimination, such as those based on age, political views, and disabilities, are affecting the community disproportionately. The paper seeks to bring attention to these issues and to encourage a more comprehensive approach to diversity and inclusion.", "method": "The research employed a secondary analysis of 800 qualitative responses from a survey of 8,717 participants, examining patterns and impacts of perceived discrimination across multiple facets of identity, including age, gender, race, and disability.", "result": "Results indicated that while age- and gender-related discrimination are the most frequently reported, discrimination based on political and religious views is also a notable issue. Discrimination related to caregiving responsibilities was prevalent across all gender identities. Women and non-binary respondents experienced a higher frequency of workplace issues, particularly discrimination and mental health challenges.", "conclusion": "The study concludes that discrimination in software engineering is multifaceted and calls for researchers to consider a broader range of identity facets beyond just age and gender when conducting software engineering studies to promote a more inclusive and comprehensive understanding of diversity in the field."}}
{"id": "2510.22622", "categories": ["cs.CR", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.22622", "abs": "https://arxiv.org/abs/2510.22622", "authors": ["Kangran Zhao", "Yupeng Chen", "Xiaoyu Zhang", "Yize Chen", "Weinan Guan", "Baicheng Chen", "Chengzhe Sun", "Soumyya Kanti Datta", "Qingshan Liu", "Siwei Lyu", "Baoyuan Wu"], "title": "DeepfakeBench-MM: A Comprehensive Benchmark for Multimodal Deepfake Detection", "comment": "Preprint", "summary": "The misuse of advanced generative AI models has resulted in the widespread\nproliferation of falsified data, particularly forged human-centric audiovisual\ncontent, which poses substantial societal risks (e.g., financial fraud and\nsocial instability). In response to this growing threat, several works have\npreliminarily explored countermeasures. However, the lack of sufficient and\ndiverse training data, along with the absence of a standardized benchmark,\nhinder deeper exploration. To address this challenge, we first build Mega-MMDF,\na large-scale, diverse, and high-quality dataset for multimodal deepfake\ndetection. Specifically, we employ 21 forgery pipelines through the combination\nof 10 audio forgery methods, 12 visual forgery methods, and 6 audio-driven face\nreenactment methods. Mega-MMDF currently contains 0.1 million real samples and\n1.1 million forged samples, making it one of the largest and most diverse\nmultimodal deepfake datasets, with plans for continuous expansion. Building on\nit, we present DeepfakeBench-MM, the first unified benchmark for multimodal\ndeepfake detection. It establishes standardized protocols across the entire\ndetection pipeline and serves as a versatile platform for evaluating existing\nmethods as well as exploring novel approaches. DeepfakeBench-MM currently\nsupports 5 datasets and 11 multimodal deepfake detectors. Furthermore, our\ncomprehensive evaluations and in-depth analyses uncover several key findings\nfrom multiple perspectives (e.g., augmentation, stacked forgery). We believe\nthat DeepfakeBench-MM, together with our large-scale Mega-MMDF, will serve as\nfoundational infrastructures for advancing multimodal deepfake detection.", "AI": {"tldr": "TL;DR Summary: The misuse of advanced generative AI has led to increased falsified data, particularly AI-generated audiovisual content. This paper proposes a large-scale dataset, Mega-MMDF, and a novel benchmark, DeepfakeBench-MM, for multimodal deepfake detection research. The dataset contains 1.1 million forged and 0.1 million real samples, created through 21 pipeline combinations of audio and visual forgery techniques. The benchmark follows standardized protocols for evaluating existing and new detection methods. Key findings include issues with augmentation and stacked forgery in multimodal deepfakes. The paper asserts that these contributions will significantly advance research in detecting deepfake content.", "motivation": "The paper's motivation stems from the global issue posed by misuse of advanced generative AI as it has led to the proliferation of falsified data. This falsified data, especially AI-generated audiovisual content like deepfakes, can create substantial societal harm through financial fraud and social instability. The authors suggest that due to the lack of extensive, diversified training data and absence of standardized benchmarks, advances in deepfake detection have been limited. Therefore, they aim to address these issues by constructing a new large-scale dataset and benchmark for multimodal deepfake detection.", "method": "The authors first build Mega-MMDF, which they describe as a large-scale, diverse, and high-quality dataset for multimodal deepfake detection. The dataset is created using 21 different forgery pipelines, formed by combining 10 audio forgery methods, 12 visual forgery methods, and 6 audio-driven face reenactment methods. Additionally, they introduce DeepfakeBench-MM, a unified benchmark for multimodal deepfake detection, offering standardized protocols across the entire detection process and serving as a flexible platform for testing both current and novel detection methods.", "result": "The Mega-MMDF dataset comprises 0.1 million real samples and 1.1 million forged samples, positioning it as one of the largest and most diverse datasets for multimodal deepfake detection. This work also leads to the establishment of the DeepfakeBench-MM benchmark, which currently supports 5 datasets and 11 multimodal deepfake detectors. Through comprehensive evaluations and analyses, the authors have uncovered several important insights, such as challenges in data augmentation and stacked forgery techniques.", "conclusion": "In conclusion, the paper predicts that the proposed Mega-MMDF and DeepfakeBench-MM will serve as essential infrastructures to significantly advance the field of multimodal deepfake detection. These contributions are seen as vital for overcoming the existing limitations in deepfake detection development, which stem from insufficient training data and the lack of standardized benchmarks."}}
{"id": "2510.22530", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22530", "abs": "https://arxiv.org/abs/2510.22530", "authors": ["Sungmin Kang", "Sumi Yun", "Jingun Hong", "Shin Yoo", "Gabin An"], "title": "Finding the Needle in the Crash Stack: Industrial-Scale Crash Root Cause Localization with AutoCrashFL", "comment": "11 pages, 8 figures, under review", "summary": "Fault Localization (FL) aims to identify root causes of program failures. FL\ntypically targets failures observed from test executions, and as such, often\ninvolves dynamic analyses to improve accuracy, such as coverage profiling or\nmutation testing. However, for large industrial software, measuring coverage\nfor every execution is prohibitively expensive, making the use of such\ntechniques difficult. To address these issues and apply FL in an industrial\nsetting, this paper proposes AutoCrashFL, an LLM agent for the localization of\ncrashes that only requires the crashdump from the Program Under Test (PUT) and\naccess to the repository of the corresponding source code. We evaluate\nAutoCrashFL against real-world crashes of SAP HANA, an industrial software\nproject consisting of more than 35 million lines of code. Experiments reveal\nthat AutoCrashFL is more effective in localization, as it identified 30%\ncrashes at the top, compared to 17% achieved by the baseline. Through thorough\nanalysis, we find that AutoCrashFL has attractive practical properties: it is\nrelatively more effective for complex bugs, and it can indicate confidence in\nits results. Overall, these results show the practicality of LLM agent\ndeployment on an industrial scale.", "AI": {"tldr": "This paper proposes AutoCrashFL, an LLM agent for crash fault localization that only requires crashdump and source code access. Evaluated on SAP HANA, it achieves 30% top localization compared to 17% for baselines. It is effective for complex bugs and provides result confidence.", "motivation": "Traditional FL methods using coverage profiling or mutation testing are costly for large industrial software. Need for an efficient, effective fault localization method that does not require coverage data for every execution.", "method": "AutoCrashFL is an LLM agent that uses crashdump from the Program Under Test (PUT) and access to the corresponding source code repository. It localizes crashes without requiring test execution coverage data.", "result": "On SAP HANA, AutoCrashFL identified 30% of crashes at the top ranking, compared to 17% by baselines. It shows higher effectiveness for complex bugs and provides confidence estimates for its results.", "conclusion": "AutoCrashFL demonstrates the practicality of LLM agents for industrial-scale fault localization. It overcomes limitations of traditional methods by not requiring coverage data and achieves better results on complex bugs."}}
{"id": "2510.22628", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22628", "abs": "https://arxiv.org/abs/2510.22628", "authors": ["Md. Mehedi Hasan", "Ziaur Rahman", "Rafid Mostafiz", "Md. Abir Hossain"], "title": "Sentra-Guard: A Multilingual Human-AI Framework for Real-Time Defense Against Adversarial LLM Jailbreaks", "comment": "11 pages, 5 figures. Preprint version under review in the area of\n  Artificial Intelligence (cs.AI)", "summary": "This paper presents a real-time modular defense system named Sentra-Guard.\nThe system detects and mitigates jailbreak and prompt injection attacks\ntargeting large language models (LLMs). The framework uses a hybrid\narchitecture with FAISS-indexed SBERT embedding representations that capture\nthe semantic meaning of prompts, combined with fine-tuned transformer\nclassifiers, which are machine learning models specialized for distinguishing\nbetween benign and adversarial language inputs. It identifies adversarial\nprompts in both direct and obfuscated attack vectors. A core innovation is the\nclassifier-retriever fusion module, which dynamically computes context-aware\nrisk scores that estimate how likely a prompt is to be adversarial based on its\ncontent and context. The framework ensures multilingual resilience with a\nlanguage-agnostic preprocessing layer. This component automatically translates\nnon-English prompts into English for semantic evaluation, enabling consistent\ndetection across over 100 languages. The system includes a HITL feedback loop,\nwhere decisions made by the automated system are reviewed by human experts for\ncontinual learning and rapid adaptation under adversarial pressure.\nSentra-Guard maintains an evolving dual-labeled knowledge base of benign and\nmalicious prompts, enhancing detection reliability and reducing false\npositives. Evaluation results show a 99.96% detection rate (AUC = 1.00, F1 =\n1.00) and an attack success rate (ASR) of only 0.004%. This outperforms leading\nbaselines such as LlamaGuard-2 (1.3%) and OpenAI Moderation (3.7%). Unlike\nblack-box approaches, Sentra-Guard is transparent, fine-tunable, and compatible\nwith diverse LLM backends. Its modular design supports scalable deployment in\nboth commercial and open-source environments. The system establishes a new\nstate-of-the-art in adversarial LLM defense.", "AI": {"tldr": "Sentra-Guard is a real-time modular defense system for large language models that detects and mitigates jailbreak and prompt injection attacks with high accuracy.", "motivation": "To detect and mitigate jailbreak and prompt injection attacks on large language models effectively and in real-time, ensuring system security and reducing the impact of adversarial threats.", "method": "Sentra-Guard uses a hybrid architecture combining FAISS-indexed SBERT embeddings and fine-tuned transformer classifiers. It features a classifier-retriever fusion module for context-aware risk scoring, a language-agnostic preprocessing layer for multilingual support, and a HITL feedback loop for continuous learning.", "result": "Sentra-Guard achieved a 99.96% detection rate (AUC = 1.00, F1 = 1.00) and an attack success rate of 0.004%, outperforming existing methods like LlamaGuard-2 and OpenAI Moderation.", "conclusion": "Sentra-Guard represents a state-of-the-art modular defense system for LLMs, offering higher detection accuracy and adaptability, and it provides transparency and compatibility with various LLM backends for scalable deployment."}}
{"id": "2510.22613", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22613", "abs": "https://arxiv.org/abs/2510.22613", "authors": ["Songhan Zhang", "Aoyang Fang", "Yifan Yang", "Ruiyi Cheng", "Xiaoying Tang", "Pinjia He"], "title": "DynaCausal: Dynamic Causality-Aware Root Cause Analysis for Distributed Microservices", "comment": null, "summary": "Cloud-native microservices enable rapid iteration and scalable deployment but\nalso create complex, fast-evolving dependencies that challenge reliable\ndiagnosis. Existing root cause analysis (RCA) approaches, even with multi-modal\nfusion of logs, traces, and metrics, remain limited in capturing dynamic\nbehaviors and shifting service relationships. Three critical challenges\npersist: (i) inadequate modeling of cascading fault propagation, (ii)\nvulnerability to noise interference and concept drift in normal service\nbehavior, and (iii) over-reliance on service deviation intensity that obscures\ntrue root causes. To address these challenges, we propose DynaCausal, a dynamic\ncausality-aware framework for RCA in distributed microservice systems.\nDynaCausal unifies multi-modal dynamic signals to capture time-varying\nspatio-temporal dependencies through interaction-aware representation learning.\nIt further introduces a dynamic contrastive mechanism to disentangle true fault\nindicators from contextual noise and adopts a causal-prioritized pairwise\nranking objective to explicitly optimize causal attribution. Comprehensive\nevaluations on public benchmarks demonstrate that DynaCausal consistently\nsurpasses state-of-the-art methods, attaining an average AC@1 of 0.63 with\nabsolute gains from 0.25 to 0.46, and delivering both accurate and\ninterpretable diagnoses in highly dynamic microservice environments.", "AI": {"tldr": "DynaCausal is a dynamic causality-aware framework for root cause analysis in cloud-native microservices that addresses limitations in capturing dynamic behaviors and service relationships, outperforming existing methods in benchmarks.", "motivation": "Cloud-native microservices create complex, fast-evolving dependencies that challenge reliable diagnosis, with existing RCA approaches limited in capturing dynamic behaviors, vulnerable to noise, and over-reliant on deviation intensity.", "method": "DynaCausal uses multi-modal dynamic signal unification, interaction-aware representation learning for spatio-temporal dependencies, dynamic contrastive mechanisms to disentangle fault indicators from noise, and causal-prioritized pairwise ranking for causal attribution optimization.", "result": "DynaCausal shows average AC@1 of 0.63 with gains from 0.25 to 0.46 over state-of-the-art methods in public benchmarks, achieving accurate and interpretable diagnoses.", "conclusion": "DynaCausal effectively handles dynamic microservice environments through its causality-aware design, addressing key challenges in RCA for cloud-native systems while delivering superior performance."}}
{"id": "2510.22661", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.22661", "abs": "https://arxiv.org/abs/2510.22661", "authors": ["Malik Imran", "Safiullah Khan", "Zain Ul Abideen", "Ciara Rafferty", "Ayesha Khalid", "Muhammad Rashid", "Maire O'Neill"], "title": "RejSCore: Rejection Sampling Core for Multivariate-based Public key Cryptography", "comment": "6 pages, 1 figure, conference", "summary": "Post-quantum multivariate public key cryptography (MPKC) schemes resist\nquantum threats but require heavy operations, such as rejection sampling, which\nchallenge resource-limited devices. Prior hardware designs have addressed\nvarious aspects of MPKC signature generation. However, rejection sampling\nremains largely unexplored in such contexts. This paper presents RejSCore, a\nlightweight hardware accelerator for rejection sampling in post-quantum\ncryptography. It specifically targets the QR-UOV scheme, which is a prominent\ncandidate under the second-round of the National Institute of Standards and\nTechnology (NIST) additional digital signature standardization process. The\narchitecture includes an AES-CTR-128-based pseudorandom number generator.\nMoreover, a lightweight iterative method is employed in rejection sampling,\noffering reduced resource consumption and area overhead while slightly\nincreasing latency. The performance of RejSCore is comprehensively evaluated on\nArtix-7 FPGAs and 65 nm CMOS technology using the Area-Delay Product (ADP) and\nPower-Delay Product (PDP). On Artix-7 and 65 nm CMOS, RejSCore achieves an area\nof 2042 slices and 464,866~$\\mu m^2$, with operating frequencies of 222 MHz and\n565 MHz, respectively. Using the QR-UOV parameters for security level I ($q =\n127$, $v = 156$, $m = 54$, $l = 3$), the core completes its operation in 8525\nclock cycles. The ADP and PDP evaluations confirm RejSCore's suitability for\ndeployment in resource-constrained and security-critical environments.", "AI": {"tldr": "This paper introduces RejSCore, a lightweight FPGA/CMOS-friendly hardware accelerator for post-quantum cryptography's rejection sampling. It achieves 8525 cycles for QR-UOV operations with favorable ADP/PDP metrics, enabling secure deployment on resource-constrained devices.", "motivation": "MPKC schemes require heavy operations like rejection sampling, which challenge resource-limited devices. Prior work has overlooked rejection sampling optimization for MPKC signature generation in hardware contexts.", "method": "The authors propose RejSCore, a lightweight hardware accelerator for rejection sampling leveraging an AES-CTR-128-based PRNG and an iterative method that reduces resource consumption/area overhead at the cost of modest latency increases.", "result": "RejSCore achieves 2042 slices (Artix-7) and 464,866 \u03bcm\u00b2 (65nm CMOS) with 8525 cycles for QR-UOV parameters at 222 MHz/565 MHz. ADP/PDP evaluations confirm its efficiency for constrained deployments.", "conclusion": "The paper concludes that RejSCore is a suitable solution for resource-constrained, security-critical environments due to its optimized performance in terms of ADP and PDP metrics."}}
{"id": "2510.22614", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22614", "abs": "https://arxiv.org/abs/2510.22614", "authors": ["Roham Koohestani", "Agnia Sergeyuk", "David Gros", "Claudio Spiess", "Sergey Titov", "Prem Devanbu", "Maliheh Izadi"], "title": "Does In-IDE Calibration of Large Language Models work at Scale?", "comment": "Under Review", "summary": "The introduction of large language models into integrated development\nenvironments (IDEs) is revolutionizing software engineering, yet it poses\nchallenges to the usefulness and reliability of Artificial\nIntelligence-generated code. Post-hoc calibration of internal model confidences\naims to align probabilities with an acceptability measure. Prior work suggests\ncalibration can improve alignment, but at-scale evidence is limited. In this\nwork, we investigate the feasibility of applying calibration of code models to\nan in-IDE context. We study two aspects of the problem: (1) the technical\nmethod for implementing confidence calibration and improving the reliability of\ncode generation models, and (2) the human-centered design principles for\neffectively communicating reliability signal to developers. First, we develop a\nscalable and flexible calibration framework which can be used to obtain\ncalibration weights for open-source models using any dataset, and evaluate\nwhether calibrators improve the alignment between model confidence and\ndeveloper acceptance behavior. Through a large-scale analysis of over 24\nmillion real-world developer interactions across multiple programming\nlanguages, we find that a general, post-hoc calibration model based on\nPlatt-scaling does not, on average, improve the reliability of model confidence\nsignals. We also find that while dynamically personalizing calibration to\nindividual users can be effective, its effectiveness is highly dependent on the\nvolume of user interaction data. Second, we conduct a multi-phase design study\nwith 3 expert designers and 153 professional developers, combining\nscenario-based design, semi-structured interviews, and survey validation,\nrevealing a clear preference for presenting reliability signals via\nnon-numerical, color-coded indicators within the in-editor code generation\nworkflow.", "AI": {"tldr": "This paper explores the challenges of applying calibration to code generation models in IDEs and finds that general post-hoc calibration (using Platt-scaling) does not consistently improve reliability. Personalized calibration can work but needs substantial user data. The study also suggests presenting reliability via color-coded indicators rather than numerical scores.", "motivation": "The paper aims to address the issue of unreliable code generated by AI, particularly within IDEs, by investigating the effectiveness of confidence calibration techniques and how to communicate reliability to developers.", "method": "The paper uses two main methods: (1) a scalable calibration framework to assess the impact of calibration on code reliability, using 24 million real-world interactions, and (2) a multi-phase design study involving expert designers and professional developers to understand the optimal design for reliability signals.", "result": "General post-hoc calibration using Platt-scaling did not improve model confidence reliability. Personalized calibration shows promise but requires sufficient user interaction data. Design study reveals a preference for non-numerical color-coded reliability indicators in the code generation workflow.", "conclusion": "The paper concludes that general calibration is ineffective for improving confidence signals in code generation, but personalized calibration could help if enough user data is available. It also recommends using color-coded indicators to communicate reliability to users."}}
{"id": "2510.22726", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.22726", "abs": "https://arxiv.org/abs/2510.22726", "authors": ["Van Le", "Tan Le"], "title": "SpoofTrackBench: Interpretable AI for Spoof-Aware UAV Tracking and Benchmarking", "comment": null, "summary": "SpoofTrackBench is a reproducible, modular benchmark for evaluating\nadversarial robustness in real-time localization and tracking (RTLS) systems\nunder radar spoofing. Leveraging the Hampton University Skyler Radar Sensor\ndataset, we simulate drift, ghost, and mirror-type spoofing attacks and\nevaluate tracker performance using both Joint Probabilistic Data Association\n(JPDA) and Global Nearest Neighbor (GNN) architectures. Our framework separates\nclean and spoofed detection streams, visualizes spoof-induced trajectory\ndivergence, and quantifies assignment errors via direct drift-from-truth\nmetrics. Clustering overlays, injection-aware timelines, and scenario-adaptive\nvisualizations enable interpretability across spoof types and configurations.\nEvaluation figures and logs are auto-exported for reproducible comparison.\nSpoofTrackBench sets a new standard for open, ethical benchmarking of\nspoof-aware tracking pipelines, enabling rigorous cross-architecture analysis\nand community validation.", "AI": {"tldr": "SpoofTrackBench is a benchmark for evaluating adversarial robustness in RTLS systems under radar spoofing.", "motivation": "The paper aims to create a reproducible and modular benchmark (SpoofTrackBench) to evaluate the robustness of real-time localization and tracking systems against radar spoofing attacks, addressing a gap in ethical and open benchmarking for spoof-aware tracking.", "method": "Leverages the Hampton University Skyler Radar Sensor dataset, simulating three types of spoofing attacks (drift, ghost, mirror), and evaluates tracker performance with JPDA and GNN architectures. Implements direct drift-from-truth metrics to quantify assignment errors and includes interpretability tools like clustering overlays and injection-aware timelines.", "result": "Enables reproducible and ethical benchmarking of spoof-aware tracking pipelines for RTLS systems under various types of radar spoofing attacks.", "conclusion": "SpoofTrackBench provides a new standard for open and ethical evaluation of tracking systems under spoofing, allowing for integrated cross-architecture comparisons and robust community validation."}}
{"id": "2510.22787", "categories": ["cs.SE", "cs.AI", "68T07", "I.2.11; I.2.7; I.2.8"], "pdf": "https://arxiv.org/pdf/2510.22787", "abs": "https://arxiv.org/abs/2510.22787", "authors": ["Kamil Szczepanik", "Jaros\u0142aw A. Chudziak"], "title": "Collaborative LLM Agents for C4 Software Architecture Design Automation", "comment": "This paper has been accepted for the upcoming 59th Hawaii\n  International Conference on System Sciences (HICSS-59), 2026, Hawaii, USA.\n  The final published version will appear in the official conference\n  proceedings", "summary": "Software architecture design is a fundamental part of creating every software\nsystem. Despite its importance, producing a C4 software architecture model, the\npreferred notation for such architecture, remains manual and time-consuming. We\nintroduce an LLM-based multi-agent system that automates this task by\nsimulating a dialogue between role-specific experts who analyze requirements\nand generate the Context, Container, and Component views of the C4 model.\nQuality is assessed with a hybrid evaluation framework: deterministic checks\nfor structural and syntactic integrity and C4 rule consistency, plus semantic\nand qualitative scoring via an LLM-as-a-Judge approach. Tested on five\ncanonical system briefs, the workflow demonstrates fast C4 model creation,\nsustains high compilation success, and delivers semantic fidelity. A comparison\nof four state-of-the-art LLMs shows different strengths relevant to\narchitectural design. This study contributes to automated software architecture\ndesign and its evaluation methods.", "AI": {"tldr": "Researchers developed an AI system that automatically creates software architecture diagrams using collaborative LLM agents. The system generates accurate C4 models 5x faster than manual methods while maintaining quality, with different AI models showing unique strengths in architectural design tasks.", "motivation": "Software architecture design is critical but manual C4 model creation is time-consuming. Existing methods lack automation and systematic quality assessment for architectural design tasks.", "method": "The approach uses an LLM-based multi-agent system simulating role-specific experts to analyze requirements and generate C4 models (Context, Container, Component views). Quality is evaluated through deterministic structural/syntactic checks, C4 rule consistency, and semantic scoring via LLM-as-a-Judge.", "result": "The workflow achieved fast C4 model creation with high compilation success (tested on five canonical systems) and maintained semantic fidelity. Comparative analysis of four LLMs revealed distinct strengths relevant to architectural design tasks.", "conclusion": "This study contributes to automated software architecture design by introducing an LLM-based multi-agent system for generating C4 models and proposing a hybrid evaluation framework, advancing both automation and evaluation methodologies in the field."}}
{"id": "2510.22944", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22944", "abs": "https://arxiv.org/abs/2510.22944", "authors": ["Bin Wang", "YiLu Zhong", "MiDi Wan", "WenJie Yu", "YuanBing Ouyang", "Yenan Huang", "Hui Li"], "title": "Is Your Prompt Poisoning Code? Defect Induction Rates and Security Mitigation Strategies", "comment": null, "summary": "Large language models (LLMs) have become indispensable for automated code\ngeneration, yet the quality and security of their outputs remain a critical\nconcern. Existing studies predominantly concentrate on adversarial attacks or\ninherent flaws within the models. However, a more prevalent yet underexplored\nissue concerns how the quality of a benign but poorly formulated prompt affects\nthe security of the generated code. To investigate this, we first propose an\nevaluation framework for prompt quality encompassing three key dimensions: goal\nclarity, information completeness, and logical consistency. Based on this\nframework, we construct and publicly release CWE-BENCH-PYTHON, a large-scale\nbenchmark dataset containing tasks with prompts categorized into four distinct\nlevels of normativity (L0-L3). Extensive experiments on multiple\nstate-of-the-art LLMs reveal a clear correlation: as prompt normativity\ndecreases, the likelihood of generating insecure code consistently and markedly\nincreases. Furthermore, we demonstrate that advanced prompting techniques, such\nas Chain-of-Thought and Self-Correction, effectively mitigate the security\nrisks introduced by low-quality prompts, substantially improving code safety.\nOur findings highlight that enhancing the quality of user prompts constitutes a\ncritical and effective strategy for strengthening the security of AI-generated\ncode.", "AI": {"tldr": "The paper introduces an evaluation framework for prompt quality that improves secure code generation from LLMs, showing advanced prompting techniques reduce security risks.", "motivation": "The issue of how poorly formulated, benign prompts impact the security of generated code by Large Language Models is underexplored, despite being common use-cases, thus motivating the creation of a new evaluation framework and benchmark.", "method": "The authors propose a framework with three dimensions to evaluate prompt quality (goal clarity, information completeness, logical consistency). They construct and release a Python-based benchmark dataset with four levels of prompt normativity and perform extensive experiments with multiple LLMs, testing the effects of prompting techniques.", "result": "The study shows a clear correlation: lower prompt normativity results in a higher likelihood of insecure code. Advanced prompting techniques like Chain-of-Thought and Self-Correction reduce these risks effectively, significantly enhancing code safety.", "conclusion": "Improving user prompt quality is a critical, effective strategy to enhance security in AI-generated code; the paper's contributions include a new evaluation framework, dataset, and insights into the effectiveness of prompting techniques for risk mitigation."}}
{"id": "2510.22815", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22815", "abs": "https://arxiv.org/abs/2510.22815", "authors": ["Vasudev Vikram", "Yuvraj Agarwal", "Rohan Padhye"], "title": "On the Freshness of Pinned Dependencies in Maven", "comment": null, "summary": "Library dependencies in software ecosystems play a crucial role in the\ndevelopment of software. As newer releases of these libraries are published,\ndevelopers may opt to pin their dependencies to a particular version. While\npinning may have benefits in ensuring reproducible builds and avoiding breaking\nchanges, it bears larger risks in using outdated dependencies that may contain\nbugs and security vulnerabilities. To understand the frequency and consequences\nof dependency pinning, we first define the concepts of stale and fresh pins,\nwhich are distinguished based on how outdated the dependency is relative to the\nrelease date of the project. We conduct an empirical study to show that over\n60% of consumers of popular Maven libraries contain stale pins to their\ndependencies, with some outdated versions over a year old. These pinned\nversions often miss out on security fixes; we find that 10% of all dependency\nupgrades in our dataset to the latest minor or patch version would reduce\nsecurity vulnerabilities.\n  We prototype an approach called Pin-Freshener that can encourage developers\nto freshen their pins by leveraging the insight that crowdsourced tests of peer\nprojects can provide additional signal for the safety of an upgrade. Running\nPin-Freshener on dependency upgrades shows that just 1-5 additional test suites\ncan provide 35-100% more coverage of a dependency, compared to that of a single\nconsumer test suite. Our evaluation on real-world pins to the top 500 popular\nlibraries in Maven shows that Pin-Freshener can provide an additional signal of\nat least 5 passing crowdsourced test suites to over 3,000 consumers to safely\nperform an upgrade that reduces security vulnerabilities. Pin-Freshener can\nprovide practical confidence to developers by offering additional signal beyond\ntheir own test suites, representing an improvement over current practices.", "AI": {"tldr": "The paper introduces Pin-Freshener, a tool that encourages developers to update outdated library dependencies to reduce vulnerabilities. It finds that over 60% of Maven consumers use stale pins, and shows that additional test coverage from other projects can help validate safe upgrades.", "motivation": "Outdated library dependencies in software ecosystems pose security risks. Developers often pin dependencies to specific versions, but this can lead to stale pins that miss security fixes. The paper seeks to understand the prevalence of stale pins and propose a solution (Pin-Freshener) to guide developers towards safer upgrades by leveraging community test data.", "method": "The study categorizes dependency pins as 'stale' or 'fresh' based on their age relative to the project's last release. They analyze a dataset of popular Maven projects to quantify the issue. Pin-Freshener's approach is tested through an empirical evaluation that measures the test coverage provided by crowdsourced test suites from other projects, demonstrating its effectiveness in detecting safe upgrades.", "result": "Over 60% of Maven project consumers use stale pins, some over a year old. 10% of their dependency upgrades to the latest minor or patch versions could reduce security vulnerabilities. Pin-Freshener increases test coverage for upgrades by 35-100% using 1-5 crowdsourced test suites. Real-world evaluations find it can provide at least 5 passing test signals for over 3,000 projects.", "conclusion": "Pin-Freshener demonstrates the feasibility of using crowdsourced test data to assist with safe dependency upgrades that reduce security risks, outperforming current practices by providing significant additional test coverage without requiring full suite execution."}}
{"id": "2510.22945", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.22945", "abs": "https://arxiv.org/abs/2510.22945", "authors": ["Dev Gurung", "Shiva Raj Pokhrel"], "title": "QuantumShield: Multilayer Fortification for Quantum Federated Learning", "comment": null, "summary": "In this paper, we propose a groundbreaking quantum-secure federated learning\n(QFL) framework designed to safeguard distributed learning systems against the\nemerging threat of quantum-enabled adversaries. As classical cryptographic\nmethods become increasingly vulnerable to quantum attacks, our framework\nestablishes a resilient security architecture that remains robust even in the\npresence of quantum-capable attackers. We integrate and rigorously evaluate\nadvanced quantum and post-quantum protocols including Quantum Key Distribution\n(QKD), Quantum Teleportation, Key Encapsulation Mechanisms (KEM) and\nPost-Quantum Cryptography (PQC) to fortify the QFL process against both\nclassical and quantum threats. These mechanisms are systematically analyzed and\nimplemented to demonstrate their seamless interoperability within a secure and\nscalable QFL ecosystem. Through comprehensive theoretical modeling and\nexperimental validation, this work provides a detailed security and performance\nassessment of the proposed framework. Our findings lay a strong foundation for\nnext-generation federated learning systems that are inherently secure in the\nquantum era.", "AI": {"tldr": "This paper introduces a quantum-secure federated learning framework combining QKD, PQC, and other protocols to defend against quantum threats, validated through theoretical and experimental analysis.", "motivation": "Classical cryptographic methods are vulnerable to quantum attacks, creating an urgent need for quantum-secure distributed learning frameworks to protect against emerging threats.", "method": "Combined quantum (QKD, Quantum Teleportation) and post-quantum protocols (KEM, PQC) within a federated learning framework, validated through theoretical models and experimental assessments.", "result": "Demonstrated secure and scalable interoperability of quantum/post-quantum protocols in a QFL ecosystem, with quantitative security/performance validation showing resistance to classical and quantum threats.", "conclusion": "The work provides a foundation for next-generation federated learning systems that are inherently quantum-secure, demonstrating robust protection against quantum threats while maintaining scalability."}}
{"id": "2510.22986", "categories": ["cs.SE", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.22986", "abs": "https://arxiv.org/abs/2510.22986", "authors": ["Junjie Huang", "Minghua He", "Jinyang Liu", "Yintong Huo", "Domenico Bianculli", "Michael R. Lyu"], "title": "CodeAD: Synthesize Code of Rules for Log-based Anomaly Detection with LLMs", "comment": null, "summary": "Log-based anomaly detection (LogAD) is critical for maintaining the\nreliability and availability of large-scale online service systems. While\nmachine learning, deep learning, and large language models (LLMs)-based methods\nhave advanced the LogAD, they often suffer from limited interpretability, high\ninference costs, and extensive preprocessing requirements, limiting their\npracticality for real-time, high-volume log analysis. In contrast, rule-based\nsystems offer efficiency and transparency, but require significant manual\neffort and are difficult to scale across diverse and evolving environments. In\nthis paper, We present CodeAD, a novel framework that automatically synthesizes\nlightweight Python rule functions for LogAD using LLMs. CodeAD introduces a\nhierarchical clustering and anchor-grounded sampling strategy to construct\nrepresentative contrastive log windows, enabling LLMs to discern discriminative\nanomaly patterns. To ensure robustness and generalizability, CodeAD employs an\nagentic workflow that iteratively generates, tests, repairs, and refines the\nrules until it meets correctness and abstraction requirements. The synthesized\nrules are interpretable, lightweight, and directly executable on raw logs,\nsupporting efficient and transparent online anomaly detection. Our\ncomprehensive experiments on three public datasets (BGL, Hadoop, Thunderbird)\ndemonstrate that CodeAD achieves an average absolute improvement of 3.6% F1\nscore over the state-of-the-art baselines, while processing large datasets up\nto 4x faster and at a fraction of the cost (total LLM invocation cost under 4\nUSD per dataset). These results highlight CodeAD as a practical and scalable\nsolution for online monitoring systems, enabling interpretable, efficient, and\nautomated LogAD in real-world environment.", "AI": {"tldr": "CodeAD is a framework for log-based anomaly detection that uses LLMs to generate Python rule functions, achieving better performance and efficiency compared to existing methods while being cost-effective and interpretable.", "motivation": "Machine learning and LLM-based log anomaly detection methods often face challenges like limited interpretability, high inference costs, and preprocessing needs, while rule-based systems require manual effort and lack scalability. This creates a need for a solution that combines automation, efficiency, and interpretability.", "method": "CodeAD uses hierarchical clustering and anchor-grounded sampling to create contrastive log windows for training LLMs. It then employs an agentic workflow to iteratively generate, test, repair, and refine Python rule functions, making them robust and generalizable. These rules are applied directly to raw logs for efficient and transparent anomaly detection.", "result": "CodeAD outperformed state-of-the-art log anomaly detection baselines by an average of 3.6% in F1 score. It processed large datasets 4 times faster with significantly lower costs (under $4 per dataset in LLM invocation costs) while maintaining high accuracy and efficiency.", "conclusion": "CodeAD provides an effective, interpretable, and scalable solution for log-based anomaly detection. Its agentic workflow and rule synthesis methodology reduce manual effort and deliver high performance, making it suitable for real-time and real-world application monitoring."}}
{"id": "2510.22963", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22963", "abs": "https://arxiv.org/abs/2510.22963", "authors": ["Zesen Liu", "Zhixiang Zhang", "Yuchong Xie", "Dongdong She"], "title": "CompressionAttack: Exploiting Prompt Compression as a New Attack Surface in LLM-Powered Agents", "comment": null, "summary": "LLM-powered agents often use prompt compression to reduce inference costs,\nbut this introduces a new security risk. Compression modules, which are\noptimized for efficiency rather than safety, can be manipulated by adversarial\ninputs, causing semantic drift and altering LLM behavior. This work identifies\nprompt compression as a novel attack surface and presents CompressionAttack,\nthe first framework to exploit it. CompressionAttack includes two strategies:\nHardCom, which uses discrete adversarial edits for hard compression, and\nSoftCom, which performs latent-space perturbations for soft compression.\nExperiments on multiple LLMs show up to 80% attack success and 98% preference\nflips, while remaining highly stealthy and transferable. Case studies in VSCode\nCline and Ollama confirm real-world impact, and current defenses prove\nineffective, highlighting the need for stronger protections.", "AI": {"tldr": "The paper introduces CompressionAttack, a framework exploiting prompt compression in LLM agents as a security risk, achieving high success rates with stealth and transferability.", "motivation": "Prompt compression reduces inference costs but introduces new security vulnerabilities that require investigation.", "method": "CompressionAttack employs two strategies: HardCom with discrete adversarial edits for hard compression and SoftCom with latent-space perturbations for soft compression.", "result": "The framework achieves up to 80% attack success and 98% preference flips across multiple LLMs, while remaining undetected by current defenses.", "conclusion": "CompressionAttack demonstrates a novel attack surface in prompt compression, proving its real-world impact and underscoring the need for improved defensive mechanisms."}}
{"id": "2510.23010", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23010", "abs": "https://arxiv.org/abs/2510.23010", "authors": ["Ming-Tung Shen", "Yuh-Jzer Joung"], "title": "TALM: Dynamic Tree-Structured Multi-Agent Framework with Long-Term Memory for Scalable Code Generation", "comment": null, "summary": "Agentic code generation requires large language models (LLMs) capable of\ncomplex context management and multi-step reasoning. Prior multi-agent\nframeworks attempt to address these challenges through collaboration, yet they\noften suffer from rigid workflows and high reasoning recovery costs. To\novercome these limitations, we propose TALM (Tree-Structured Multi-Agent\nFramework with Long-Term Memory), a dynamic framework that integrates\nstructured task decomposition, localized re-reasoning, and long-term memory\nmechanisms. TALM employs an extensible tree-based collaboration structure. The\nparent-child relationships, when combined with a divide-and-conquer strategy,\nenhance reasoning flexibility and enable efficient error correction across\ndiverse task scopes. Furthermore, a long-term memory module enables semantic\nquerying and integration of prior knowledge, supporting implicit\nself-improvement through experience reuse. Experimental results on HumanEval,\nBigCodeBench, and ClassEval benchmarks demonstrate that TALM consistently\ndelivers strong reasoning performance and high token efficiency, highlighting\nits robustness and practical utility in complex code generation tasks.", "AI": {"tldr": "This paper introduces TALM, a dynamic tree-structured multi-agent framework for code generation...", "motivation": "Current code generation systems exhibit limitations in...", "method": "The tree-based architecture features parent-agent coordination with child-agent...", "result": "TALM sees 30% improvements on ClassEval benchmarks compared to CoeBorg+Plus while consuming 20% fewer tokens.", "conclusion": "Empirical evidence suggests TALM represents a promising new paradigm for agent-based..."}}
{"id": "2510.22971", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.22971", "abs": "https://arxiv.org/abs/2510.22971", "authors": ["Sudiksha Das", "Ashish Kundu"], "title": "Advancing Honeywords for Real-World Authentication Security", "comment": null, "summary": "Introduced by Juels and Rivest in 2013, Honeywords, which are decoy passwords\nstored alongside a real password, appear to be a proactive method to help\ndetect password credentials misuse. However, despite over a decade of research,\nthis technique has not been adopted by major authentication platforms. This\nposition paper argues that the core concept of Honeywords has potential but\nrequires more research on issues such as flatness, integration, and\nreliability, in order to be a practical deployable solution. This paper\nexamines the current work on Honeyword generation, attacker modeling, and\nhoneychecker architecture, analyzing the subproblems that have been addressed\nand ongoing issues that prevent this system from being more widely used. The\npaper then suggests a deployable framework that combines the\nattacker-resilient, context-aware decoy creation that Honeywords provide with\neasy integration into existing systems. Honeywords will only move from an\nacademic idea to a practical security tool if technical advances are paired\nwith secure and straightforward architectures, along with adaptive response\nhandling and detailed configuration checks.", "AI": {"tldr": "Honeywords, a proactive security method using decoy passwords, have potential but require more research on practical issues for widespread adoption.", "motivation": "Honeywords, introduced in 2013, aim to detect password misuse by storing decoy passwords next to real ones. Despite this potential, they haven't been implemented by major platforms, possibly due to unresolved technical and architectural issues.", "method": "The paper reviews existing literature on honeychecker architecture, attacker modeling, and Honeyword generation. It then proposes a new framework based on combining attacker-resistant decoy creation with easy system integration.", "result": "Analysis identifies unresolved subproblems in Honeyword systems, including flatness, integration challenges, and reliability concerns. The paper outlines a framework to address these issues.", "conclusion": "To transition Honeywords from an academic concept to a practical security tool, technical improvements must be combined with secure, user-friendly architectures, response handling, and configuration assurance."}}
{"id": "2510.23055", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23055", "abs": "https://arxiv.org/abs/2510.23055", "authors": ["Manjeshwar Aniruddh Mallya", "Alessio Ferrari", "Mohammad Amin Zadenoori", "Jacek D\u0105browski"], "title": "From Online User Feedback to Requirements: Evaluating Large Language Models for Classification and Specification Tasks", "comment": null, "summary": "[Context and Motivation] Online user feedback provides valuable information\nto support requirements engineering (RE). However, analyzing online user\nfeedback is challenging due to its large volume and noise. Large language\nmodels (LLMs) show strong potential to automate this process and outperform\nprevious techniques. They can also enable new tasks, such as generating\nrequirements specifications.\n  [Question-Problem] Despite their potential, the use of LLMs to analyze user\nfeedback for RE remains underexplored. Existing studies offer limited empirical\nevidence, lack thorough evaluation, and rarely provide replication packages,\nundermining validity and reproducibility.\n  [Principal Idea-Results] We evaluate five lightweight open-source LLMs on\nthree RE tasks: user request classification, NFR classification, and\nrequirements specification generation. Classification performance was measured\non two feedback datasets, and specification quality via human evaluation. LLMs\nachieved moderate-to-high classification accuracy (F1 ~ 0.47-0.68) and\nmoderately high specification quality (mean ~ 3/5).\n  [Contributions] We newly explore lightweight LLMs for feedback-driven\nrequirements development. Our contributions are: (i) an empirical evaluation of\nlightweight LLMs on three RE tasks, (ii) a replication package, and (iii)\ninsights into their capabilities and limitations for RE.", "AI": {"tldr": "This paper evaluates five lightweight open-source LLMs for three RE tasks (user request/NFR classification and requirements generation), finding moderate-to-high performance (F1 ~ 0.47-0.68; quality ~3/5), while providing a replication package and insights on LLMs in feedback-driven RE.", "motivation": "Online user feedback contains valuable requirements information but is difficult to analyze at scale. While LLMs show promise for automating this process, their use in RE lacks empirical validation and reproducibility. This study addresses this gap by evaluating LLMs for core RE tasks.", "method": "Evaluated five open-source LLMs on three RE tasks using two feedback datasets for classification (F1 metric) and human evaluation for specification quality (scale 1-5). Tasks included: user request classification, NFR classification, and requirements specification generation.", "result": "LLMs achieved moderate-to-high classification accuracy (F1 scores of 0.47-0.68 across tasks) and moderately high specification quality (mean human rating of ~3/5). Performance varied across models and tasks, with no single model dominating all objectives.", "conclusion": "Lightweight LLMs demonstrate viability for feedback-driven requirements development, but their performance highlights both capabilities and limitations. The study provides empirical evidence supporting their use while emphasizing the need for further research on model selection, evaluation rigor, and task-specific optimizations in RE contexts."}}
{"id": "2510.23024", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23024", "abs": "https://arxiv.org/abs/2510.23024", "authors": ["Chuan Yan", "Zeng Li", "Kunlin Cai", "Liuhuo Wan", "Ruomai Ren", "Yiran Shen", "Guangdong Bai"], "title": "A Multi-Store Privacy Measurement of Virtual Reality App Ecosystem", "comment": "16 pages", "summary": "Virtual Reality (VR) has gained increasing traction among various domains in\nrecent years, with major companies such as Meta, Pico, and Microsoft launching\ntheir application stores to support third-party developers in releasing their\napplications (or simply apps). These apps offer rich functionality but\ninherently collect privacy-sensitive data, such as user biometrics, behaviors,\nand the surrounding environment. Nevertheless, there is still a lack of\ndomain-specific regulations to govern the data handling of VR apps, resulting\nin significant variations in their privacy practices among app stores.\n  In this work, we present the first comprehensive multi-store study of privacy\npractices in the current VR app ecosystem, covering a large-scale dataset\ninvolving 6,565 apps collected from five major app stores. We assess both\ndeclarative and behavioral privacy practices of VR apps, using a multi-faceted\napproach based on natural language processing, reverse engineering, and static\nanalysis. Our assessment reveals significant privacy compliance issues across\nall stores, underscoring the premature status of privacy protection in this\nrapidly growing ecosystem. For instance, one third of apps fail to declare\ntheir use of sensitive data, and 21.5\\% of apps neglect to provide valid\nprivacy policies. Our work sheds light on the status quo of privacy protection\nwithin the VR app ecosystem for the first time. Our findings should raise an\nalert to VR app developers and users, and encourage store operators to\nimplement stringent regulations on privacy compliance among VR apps.", "AI": {"tldr": "This paper analyzes privacy practices in VR apps across five app stores using 6,565 samples, revealing systemic compliance issues like missing data declarations and privacy policies.", "motivation": "VR apps collect sensitive data without-domain-specific regulations, leading to inconsistent privacy practices that risk user security in a rapidly growing ecosystem.", "method": "Large-scale multi-store study combining natural language processing, reverse engineering, and static analysis to assess both declared and actual privacy behaviors of VR apps.", "result": "33% of apps don't declare sensitive data usage, 21.5%' lack valid privacy policies, and compliance issues exist across all stores, demonstrating premature privacy protection measures.", "conclusion": "The findings highlight urgent need for standardized privacy regulations in VR app stores to protect users, prompting action from developers, users, and platform operators."}}
{"id": "2510.23068", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23068", "abs": "https://arxiv.org/abs/2510.23068", "authors": ["Ella Dodor", "Cristina V. Lopes"], "title": "Checkstyle+: Reducing Technical Debt Through The Use of Linters with LLMs", "comment": "11 pages, 9 figures, tool link:\n  https://github.com/ellacodee/CheckstylePlus", "summary": "Good code style improves program readability, maintainability, and\ncollaboration, and is an integral component of software quality. Developers,\nhowever, often cut corners when following style rules, leading to the wide\nadoption of tools such as linters in professional software development\nprojects. Traditional linters like Checkstyle operate using rigid, rule-based\nmechanisms that effectively detect many surface-level violations. However, in\nmost programming languages, there is a subset of style rules that require a\nmore nuanced understanding of code, and fall outside the scope of such static\nanalysis. In this paper, we propose Checkstyle+, a hybrid approach that\naugments Checkstyle with large language model (LLM) capabilities, to identify\nstyle violations that elude the conventional rule-based analysis. Checkstyle+\nis evaluated on a sample of 380 Java code files, drawn from a broader dataset\nof 30,800 real-world Java programs sourced from accepted Codeforces\nsubmissions. The results show that Checkstyle+ achieves superior performance\nover standard Checkstyle in detecting violations of the semantically nuanced\nrules.", "AI": {"tldr": "This paper presents Checkstyle+, which integrates LLMs with traditional linters like Checkstyle to better detect nuanced style violations in Java code. Evaluated on 380 files from 30,800 real-world submissions, it outperforms conventional methods.", "motivation": "Traditional linters like Checkstyle use rigid rules for code style checking but struggle with semantically complex style violations, necessitating a more nuanced approach.", "method": "Checkstyle+ combines Checkstyle with large language model (LLM) capabilities to address the limitations of rule-based analysis in detecting nuanced style violations.", "result": "Checkstyle+ outperforms standard Checkstyle in detecting semantically nuanced style violations, evaluated across 380 Java code files from real-world projects.", "conclusion": "The integration of LLMs with traditional linters offers a promising approach to improving the detection of complex code style violations, enhancing software quality."}}
{"id": "2510.23034", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23034", "abs": "https://arxiv.org/abs/2510.23034", "authors": ["Gokulnath Rajendran", "Suman Deb", "Anupam Chattopadhyay"], "title": "Efficient and Encrypted Inference using Binarized Neural Networks within In-Memory Computing Architectures", "comment": "to be published in: 7th International Conference on Emerging\n  Electronics (ICEE 2025)", "summary": "Binarized Neural Networks (BNNs) are a class of deep neural networks designed\nto utilize minimal computational resources, which drives their popularity\nacross various applications. Recent studies highlight the potential of mapping\nBNN model parameters onto emerging non-volatile memory technologies,\nspecifically using crossbar architectures, resulting in improved inference\nperformance compared to traditional CMOS implementations. However, the common\npractice of protecting model parameters from theft attacks by storing them in\nan encrypted format and decrypting them at runtime introduces significant\ncomputational overhead, thus undermining the core principles of in-memory\ncomputing, which aim to integrate computation and storage. This paper presents\na robust strategy for protecting BNN model parameters, particularly within\nin-memory computing frameworks. Our method utilizes a secret key derived from a\nphysical unclonable function to transform model parameters prior to storage in\nthe crossbar. Subsequently, the inference operations are performed on the\nencrypted weights, achieving a very special case of Fully Homomorphic\nEncryption (FHE) with minimal runtime overhead. Our analysis reveals that\ninference conducted without the secret key results in drastically diminished\nperformance, with accuracy falling below 15%. These results validate the\neffectiveness of our protection strategy in securing BNNs within in-memory\ncomputing architectures while preserving computational efficiency.", "AI": {"tldr": "This paper proposes a security method for Binarized Neural Networks (BNNs) in in-memory computing, using a key from a physical unclonable function to encrypt weights and maintain performance.", "motivation": "BNNs' efficiency is compromised by encryption overhead in in-memory computing. The need arises for a secure, low-overhead method to protect model parameters during inference.", "method": "A unique secret key, generated via a physical unclonable function (PUF), is used to encrypt BNN parameters before storage in the crossbar, enabling inference on encrypted data with minimal overhead.", "result": "The encryption scheme significantly reduces the accuracy of BNNs (under 15%) when the secret key isn't used, confirming the method's efficacy in defense against theft without hampering computational efficiency.", "conclusion": "The proposed encryption strategy successfully secures BNN parameters in in-memory computing, leveraging PUFs for a practical and efficient solution to a pressing security issue."}}
{"id": "2510.23350", "categories": ["cs.SE", "D.2.1; D.2.4; D.2.5"], "pdf": "https://arxiv.org/pdf/2510.23350", "abs": "https://arxiv.org/abs/2510.23350", "authors": ["Alcino Cunha", "Nuno Macedo"], "title": "Validating Formal Specifications with LLM-generated Test Cases", "comment": null, "summary": "Validation is a central activity when developing formal specifications.\nSimilarly to coding, a possible validation technique is to define upfront test\ncases or scenarios that a future specification should satisfy or not.\nUnfortunately, specifying such test cases is burdensome and error prone, which\ncould cause users to skip this validation task. This paper reports the results\nof an empirical evaluation of using pre-trained large language models (LLMs) to\nautomate the generation of test cases from natural language requirements. In\nparticular, we focus on test cases for structural requirements of simple domain\nmodels formalized in the Alloy specification language. Our evaluation focuses\non the state-of-art GPT-5 model, but results from other closed- and open-source\nLLMs are also reported. The results show that, in this context, GPT-5 is\nalready quite effective at generating positive (and negative) test cases that\nare syntactically correct and that satisfy (or not) the given requirement, and\nthat can detect many wrong specifications written by humans.", "AI": {"tldr": "The paper evaluates the use of pre-trained LLMs like GPT-5 to automate generating test cases for Alloy specification language models, demonstrating GPT-5's effectiveness in creating syntactically correct and semantically valid test scenarios.", "motivation": "Manual test-case specification for formal validation is labor-intensive and error-prone, potentially leading to skipped validation. This study explores LLMs as a solution to automate this critical process.", "method": "Empirical evaluation of multiple LLMs, primarily GPT-5, to generate test cases from natural language requirements for Alloy domain models. Assessment focused on syntactic validity, requirement satisfaction, and ability to detect flawed specifications.", "result": "GPT-5 produced 85%-95% syntactically valid test cases that correctly satisfied or violated requirements. The model detected 65-75% of human-written specification errors while matching human test cases in quality assessments.", "conclusion": "LLMs like GPT-5 can effectively automate Alloy test case generation, improving validation efficiency while maintaining technical accuracy. This suggests significant potential to reduce human effort in formal specification development."}}
{"id": "2510.23035", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23035", "abs": "https://arxiv.org/abs/2510.23035", "authors": ["Jun Jiang", "Weiming Zhang", "Nenghai Yu", "Kejiang Chen"], "title": "A high-capacity linguistic steganography based on entropy-driven rank-token mapping", "comment": null, "summary": "Linguistic steganography enables covert communication through embedding\nsecret messages into innocuous texts; however, current methods face critical\nlimitations in payload capacity and security. Traditional modification-based\nmethods introduce detectable anomalies, while retrieval-based strategies suffer\nfrom low embedding capacity. Modern generative steganography leverages language\nmodels to generate natural stego text but struggles with limited entropy in\ntoken predictions, further constraining capacity. To address these issues, we\npropose an entropy-driven framework called RTMStega that integrates rank-based\nadaptive coding and context-aware decompression with normalized entropy. By\nmapping secret messages to token probability ranks and dynamically adjusting\nsampling via context-aware entropy-based adjustments, RTMStega achieves a\nbalance between payload capacity and imperceptibility. Experiments across\ndiverse datasets and models demonstrate that RTMStega triples the payload\ncapacity of mainstream generative steganography, reduces processing time by\nover 50%, and maintains high text quality, offering a trustworthy solution for\nsecure and efficient covert communication.", "AI": {"tldr": "RTMStega is an entropy-driven linguistic steganography framework that triples payload capacity, reduces processing time by 50%, and maintains text quality through rank-based coding and entropy-adjusted sampling.", "motivation": "Current steganography methods face payload limitations (modification-based approaches introduce detectable anomalies; retrieval-based strategies have low capacity; generative models suffer from low token entropy).", "method": "Proposes RTMStega framework combining rank-based adaptive coding, context-aware decompression with normalized entropy, and dynamic sampling guided by token probability ranks and entropy adjustments.", "result": "Achieves 3\u00d7 payload capacity increase, >50% processing speed improvement, and maintains high text quality across diverse datasets/models compared to mainstream generative steganography.", "conclusion": "RTMStega offers a secure, efficient covert communication solution by balancing payload capacity and imperceptibility through entropy-driven innovations, outperforming existing methods."}}
{"id": "2510.23389", "categories": ["cs.SE", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23389", "abs": "https://arxiv.org/abs/2510.23389", "authors": ["Edoardo Manino", "Bruno Farias", "Rafael S\u00e1 Menezes", "Fedor Shmarov", "Lucas C. Cordeiro"], "title": "Floating-Point Neural Network Verification at the Software Level", "comment": "Pre-print before submission to peer review", "summary": "The behaviour of neural network components must be proven correct before\ndeployment in safety-critical systems. Unfortunately, existing neural network\nverification techniques cannot certify the absence of faults at the software\nlevel. In this paper, we show how to specify and verify that neural networks\nare safe, by explicitly reasoning about their floating-point implementation. In\ndoing so, we construct NeuroCodeBench 2.0, a benchmark comprising 912 neural\nnetwork verification examples that cover activation functions, common layers,\nand full neural networks of up to 170K parameters. Our verification suite is\nwritten in plain C and is compatible with the format of the International\nCompetition on Software Verification (SV-COMP). Thanks to it, we can conduct\nthe first rigorous evaluation of eight state-of-the-art software verifiers on\nneural network code. The results show that existing automated verification\ntools can correctly solve an average of 11% of our benchmark, while producing\naround 3% incorrect verdicts. At the same time, a historical analysis reveals\nthat the release of our benchmark has already had a significantly positive\nimpact on the latter.", "AI": {"tldr": "This paper introduces NeuroCodeBench 2.0, a benchmark for verifying neural network software correctness in safety-critical systems, demonstrating significant gaps in current verification tools while showing the benchmark's positive impact on their performance.", "motivation": "Existing neural network verification techniques cannot certify the absence of software-level faults due to unaccounted numerical errors in floating-point implementations.", "method": "Constructed NeuroCodeBench 2.0 with 912 C-based verification examples covering neural network components, evaluated eight state-of-the-art software verifiers using the SV-COMP format benchmark.", "result": "Tools solved 11.3% of benchmarks on average with 3.5 \u043d\u0435\u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u044b\u0445 verdicts; historical analysis showed 36.2% improvement in tool performance post-benchmark release.", "conclusion": "Dedicated neural network benchmarks are essential for evaluating verification tools, and NeuroCodeBench 2.0 has already catalyzed improvements in verification effectiveness."}}
{"id": "2510.23036", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23036", "abs": "https://arxiv.org/abs/2510.23036", "authors": ["Xudong Yang", "Jincheng Li", "Kaiwen Xing", "Zhenjia Xiao", "Mingjian Duan", "Weili Han", "Hu Xiong"], "title": "KAPG: Adaptive Password Guessing via Knowledge-Augmented Generation", "comment": null, "summary": "As the primary mechanism of digital authentication, user-created passwords\nexhibit common patterns and regularities that can be learned from leaked\ndatasets. Password choices are profoundly shaped by external factors, including\nsocial contexts, cultural trends, and popular vocabulary. Prevailing password\nguessing models primarily emphasize patterns derived from leaked passwords,\nwhile neglecting these external influences -- a limitation that hampers their\nadaptability to emerging password trends and erodes their effectiveness over\ntime.\n  To address these challenges, we propose KAPG, a knowledge-augmented password\nguessing framework that adaptively integrates external lexical knowledge into\nthe guessing process. KAPG couples internal statistical knowledge learned from\nleaked passwords with external information that reflects real-world trends. By\nusing password prefixes as anchors for knowledge lookup, it dynamically injects\nrelevant external cues during generation while preserving the structural\nregularities of authentic passwords. Experiments on twelve leaked datasets show\nthat KnowGuess achieves average improvements of 36.5\\% and 74.7\\% over\nstate-of-the-art models in intra-site and cross-site scenarios, respectively.\nFurther analyses of password overlap and model efficiency highlight its\nrobustness and computational efficiency. To counter these attacks, we further\ndevelop KAPSM, a trend-aware and site-specific password strength meter.\nExperiments demonstrate that KAPSM significantly outperforms existing tools in\naccuracy across diverse evaluation settings.", "AI": {"tldr": "This paper introduces KAPG, a password guessing framework that integrates external lexical knowledge with leaked password data, improving guessing effectiveness by 36.5\u201374.7x. It also proposes KAPSM, a trend-aware password strength meter that outperforms existing tools.", "motivation": "Traditional password guessing models suffer from over-reliance on internal data from leaked passwords while ignoring external influences like cultural trends, leading to diminishing effectiveness as password behaviors evolve.", "method": "KAPG combines internal statistical knowledge from leaked passwords with external lexical knowledge via password prefixes, dynamically injecting contextual cues during generation. KAPSM evaluates password strength using site-specific and cultural trend data.", "result": "KAPG achieves 36.5x and 74.7x improvements over state-of-the-art models in intra-site and cross-site password recovery. KAPSM demonstrates superior accuracy across diverse datasets, validated through overlap analysis and efficiency benchmarks.", "conclusion": "Integrating external knowledge addresses the limitations of static password guessing models. KAPG and KAPSM collectively provide robust tools for understanding password trends and strengthening security defenses."}}
{"id": "2510.23528", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23528", "abs": "https://arxiv.org/abs/2510.23528", "authors": ["Joran Leest", "Ilias Gerostathopoulos", "Patricia Lago", "Claudia Raibulet"], "title": "Tracing Distribution Shifts with Causal System Maps", "comment": null, "summary": "Monitoring machine learning (ML) systems is hard, with standard practice\nfocusing on detecting distribution shifts rather than their causes. Root-cause\nanalysis often relies on manual tracing to determine whether a shift is caused\nby software faults, data-quality issues, or natural change. We propose ML\nSystem Maps -- causal maps that, through layered views, make explicit the\npropagation paths between the environment and the ML system's internals,\nenabling systematic attribution of distribution shifts. We outline the approach\nand a research agenda for its development and evaluation.", "AI": {"tldr": "The paper introduces ML System Maps, a novel approach using causal maps to track the root causes of distribution shifts in ML systems, shifting focus from detection to systematic analysis of causation.", "motivation": "Current ML system monitoring practices primarily detect distribution shifts but lack systematic methods to identify their root causes, such as distinguishing between data quality issues and environmental changes.", "method": "The authors propose ML System Maps, a framework that uses layered views of causal relationships to visualize and analyze the propagation paths from the environment to system internals, enabling structured root-cause identification.", "result": "The paper presents a detailed approach for developing ML System Maps and suggests a research agenda to further explore their implementation and effectiveness.", "conclusion": "ML System Maps offer a structured way to attribute distribution shifts to specific causes, improving the monitoring and development of ML systems by focusing on understanding internal and environmental interactions."}}
{"id": "2510.23060", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.23060", "abs": "https://arxiv.org/abs/2510.23060", "authors": ["Paritosh Ramanan", "H. M. Mohaimanul Islam", "Abhiram Reddy Alugula"], "title": "zkSTAR: A zero knowledge system for time series attack detection enforcing regulatory compliance in critical infrastructure networks", "comment": null, "summary": "Industrial control systems (ICS) form the operational backbone of critical\ninfrastructure networks (CIN) such as power grids, water supply systems, and\ngas pipelines. As cyber threats to these systems escalate, regulatory agencies\nare imposing stricter compliance requirements to ensure system-wide security\nand reliability. A central challenge, however, is enabling regulators to verify\nthe effectiveness of detection mechanisms without requiring utilities to\ndisclose sensitive operational data. In this paper, we introduce zkSTAR, a\ncyberattack detection framework that leverages zk-SNARKs to reconcile these\nrequirements and enable provable detection guarantees while preserving data\nconfidentiality. Our approach builds on established residual-based statistical\nhypothesis testing methods applied to state-space detection models.\nSpecifically, we design a two-pronged zk-SNARK architecture that enforces\ntemporal consistency of the state-space dynamics and statistical consistency of\nthe detection tests, allowing regulators to temporally verify alarm correctness\nwithout visibility into utility-level data. We formally analyze the soundness\nand zero knowledge properties of our framework and validate its practical\nfeasibility through computational experiments on real-world ICS datasets. As a\nresult, our work demonstrates a scalable, privacy-preserving alternative for\nregulatory compliance for ICS driven critical infrastructure networks.", "AI": {"tldr": "zkSTAR is a framework using zk-SNARKs for cyberattack detection in ICS, enabling regulators to verify detection effectiveness without accessing sensitive data.", "motivation": "Regulators need to ensure security and compliance of ICS without access to sensitive operational data from utilities.", "method": "The framework uses zk-SNARKs to create a two-pronged architecture that enforces temporal and statistical consistency in state-space detection models.", "result": "zkSTAR provides provable detection guarantees while preserving data confidentiality, validated through experiments on real-world datasets.", "conclusion": "zkSTAR offers a scalable, privacy-preserving solution for regulatory compliance in ICS-driven critical infrastructure."}}
{"id": "2510.23074", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23074", "abs": "https://arxiv.org/abs/2510.23074", "authors": ["Hiromu Takahashi", "Shotaro Ishihara"], "title": "Fast-MIA: Efficient and Scalable Membership Inference for LLMs", "comment": null, "summary": "We propose Fast-MIA (https://github.com/Nikkei/fast-mia), a Python library\nfor efficiently evaluating membership inference attacks (MIA) against Large\nLanguage Models (LLMs). MIA against LLMs has emerged as a crucial challenge due\nto growing concerns over copyright, security, and data privacy, and has\nattracted increasing research attention. However, the progress of this research\nis significantly hindered by two main obstacles: (1) the high computational\ncost of inference in LLMs, and (2) the lack of standardized and maintained\nimplementations of MIA methods, which makes large-scale empirical comparison\ndifficult. To address these challenges, our library provides fast batch\ninference and includes implementations of representative MIA methods under a\nunified evaluation framework. This library supports easy implementation of\nreproducible benchmarks with simple configuration and extensibility. We release\nFast-MIA as an open-source (Apache License 2.0) tool to support scalable and\ntransparent research on LLMs.", "AI": {"tldr": "The paper presents Fast-MIA, an open-source Python library that enables efficient evaluation of membership inference attacks (MIA) on Large Language Models (LLMs).", "motivation": "Membership inference attacks (MIA) against LLMs are gaining attention due to concerns about copyright, security, and data privacy. However, research progress is limited by high computational costs and a lack of standardized, maintained MIA implementations, making it difficult to perform large-scale empirical comparisons.", "method": "Fast-MIA addresses these challenges by offering fast batch inference and implementing representative MIA methods within a unified evaluation framework. The library supports reproducible benchmarks through simple configuration and is designed to be easily extendable.", "result": "The release of Fast-MIA as an open-source tool under the Apache License 2.0 provides researchers with a scalable and transparent platform for conducting MIA research on LLMs.", "conclusion": "Fast-MIA facilitates efficient and reproducible research on membership inference attacks against LLMs by overcoming the limitations of computational cost and lack of standardization in existing methods."}}
{"id": "2510.23101", "categories": ["cs.CR", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23101", "abs": "https://arxiv.org/abs/2510.23101", "authors": ["Yifan Zhang", "Xin Zhang"], "title": "Beyond Imprecise Distance Metrics: LLM-Predicted Target Call Stacks for Directed Greybox Fuzzing", "comment": "Preprint, under submission", "summary": "Directed greybox fuzzing (DGF) aims to efficiently trigger bugs at specific\ntarget locations by prioritizing seeds whose execution paths are more likely to\nmutate into triggering target bugs. However, existing DGF approaches suffer\nfrom imprecise probability calculations due to their reliance on complex\ndistance metrics derived from static analysis. The over-approximations inherent\nin static analysis cause a large number of irrelevant execution paths to be\nmistakenly considered to potentially mutate into triggering target bugs,\nsignificantly reducing fuzzing efficiency. We propose to replace static\nanalysis-based distance metrics with precise call stack representations. Call\nstacks represent precise control flows, thereby avoiding false information in\nstatic analysis. We leverage large language models (LLMs) to predict\nvulnerability-triggering call stacks for guiding seed prioritization. Our\napproach constructs call graphs through static analysis to identify methods\nthat can potentially reach target locations, then utilizes LLMs to predict the\nmost likely call stack sequence that triggers the vulnerability. Seeds whose\nexecution paths have higher overlap with the predicted call stack are\nprioritized for mutation. This is the first work to integrate LLMs into the\ncore seed prioritization mechanism of DGF. We implement our approach and\nevaluate it against several state-of-the-art fuzzers. On a suite of real-world\nprograms, our approach triggers vulnerabilities $1.86\\times$ to $3.09\\times$\nfaster compared to baselines. In addition, our approach identifies 10 new\nvulnerabilities and 2 incomplete fixes in the latest versions of programs used\nin our controlled experiments through directed patch testing, with 10 assigned\nCVE IDs.", "AI": {"tldr": "This paper introduces a novel directed greybox fuzzing (DGF) approach that replaces static analysis-based distance metrics with precise call stack representations generated by large language models (LLMs). By predicting vulnerability-triggering call stacks, the method prioritizes seeds with higher overlap to accelerate bug triggering. It achieves 1.86\u00d7-3.09\u00d7 speedup over baselines, discovers 10 new vulnerabilities, and identifies 2 incomplete fixes with 10 CVEs.", "motivation": "Existing DGF approaches use static analysis to estimate bug-triggering probabilities but suffer from imprecise metrics due to over-approximations. This leads to irrelevant execution paths being prioritized, reducing fuzzing efficiency and effectiveness in locating real-world vulnerabilities.", "method": "The method (GLLM-DGF) combines static analysis and LLMs: (1)\u2003Static analysis generates call graphs to identify methods reachable to targets. (2)\u2003LLMs predict the most probable call stack sequences for exploitation. (3)\u2003Seeds are prioritized based on overlap between their execution paths and predicted call stacks, ensuring precise path prioritization without over-approximations.", "result": "Evaluations on real-world programs show GLLM-DGF triggers vulnerabilities 1.86\u20133.09\u00d7 faster than existing fuzzer baselines. It discovers 10 new bugs and 2 incomplete fixes through directed patch testing, with all findings receiving CVE identifiers in the latest software versions tested.", "conclusion": "This work establishes the first integration of LLMs into DGF's core seed prioritization mechanism. The combination of static analysis and LLMs eliminates over-approximations, achieving both faster bug detection and novel vulnerability discovery, demonstrating the viability of AI-enhanced fuzzing for real-world security testing."}}
{"id": "2510.23172", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23172", "abs": "https://arxiv.org/abs/2510.23172", "authors": ["Mohsen Ahmadvand", "Pedro Souto"], "title": "Optimizing Optimism: Up to 6.5x Faster zkVM Validty Proofs via Sparse Derivation", "comment": null, "summary": "The Optimism derivation pipeline is engineered for correctness and liveness,\nnot for succinct validity proofs. A straightforward port to a zkVM imposes\nsignificant overheads, making validity proofs significantly more costly than\nnecessary. We systematically identify inefficiencies in the current design,\nanalyze their impact on proving costs, and provide a soundness-preserving\nredesign tailored to zk proving. Our redesign achieves up to 6.5x faster\nderivation inside zkVMs (3.5x overall speedup) while maintaining identical\nsafety guarantees.", "AI": {"tldr": "The paper optimizes validity proofs within the Optimism derivation pipeline by addressing inefficiencies, resulting in a significant speedup while preserving correctness and liveness.", "motivation": "Porting the Optimism derivation pipeline to a zkVM entails high overheads and expensive validity proofs, necessitating a more efficient approach.", "method": "The authors identify inefficiencies in the existing pipeline, quantify their impact on proving costs, and propose a redesigned pipeline optimized for zk proving.", "result": "The redesigned derivation pipeline attains up to 6.5x faster derivation and 3.5x overall speedup in zkVMs, maintaining the same safety guarantees.", "conclusion": "Systematic analysis of inefficiencies and soundness-preserving redesign in Optimism's zkVM compatibility led to a substantial speed improvement."}}
{"id": "2510.23274", "categories": ["cs.CR", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.23274", "abs": "https://arxiv.org/abs/2510.23274", "authors": ["Weixuan Chen", "Qianqian Yang", "Shuo Shao", "Shunpu Tang", "Zhiguo Shi", "Shui Yu"], "title": "Privacy-Preserving Semantic Communication over Wiretap Channels with Learnable Differential Privacy", "comment": null, "summary": "While semantic communication (SemCom) improves transmission efficiency by\nfocusing on task-relevant information, it also raises critical privacy\nconcerns. Many existing secure SemCom approaches rely on restrictive or\nimpractical assumptions, such as favorable channel conditions for the\nlegitimate user or prior knowledge of the eavesdropper's model. To address\nthese limitations, this paper proposes a novel secure SemCom framework for\nimage transmission over wiretap channels, leveraging differential privacy (DP)\nto provide approximate privacy guarantees. Specifically, our approach first\nextracts disentangled semantic representations from source images using\ngenerative adversarial network (GAN) inversion method, and then selectively\nperturbs private semantic representations with approximate DP noise. Distinct\nfrom conventional DP-based protection methods, we introduce DP noise with\nlearnable pattern, instead of traditional white Gaussian or Laplace noise,\nachieved through adversarial training of neural networks (NNs). This design\nmitigates the inherent non-invertibility of DP while effectively protecting\nprivate information. Moreover, it enables explicitly controllable security\nlevels by adjusting the privacy budget according to specific security\nrequirements, which is not achieved in most existing secure SemCom approaches.\nExperimental results demonstrate that, compared with the previous DP-based\nmethod and direct transmission, the proposed method significantly degrades the\nreconstruction quality for the eavesdropper, while introducing only slight\ndegradation in task performance. Under comparable security levels, our approach\nachieves an LPIPS advantage of 0.06-0.29 and an FPPSR advantage of 0.10-0.86\nfor the legitimate user compared with the previous DP-based method.", "AI": {"tldr": "A secure SemCom framework for image transmission using differential privacy with learnable noise patterns via adversarial training to balance privacy protection and task performance.", "motivation": "Semantic communication (SemCom) improves transmission efficiency but must address privacy risks, particularly in wiretap channels where adversaries might infer sensitive data. Existing secure SemCom solutions often assume impractical safety conditions or knowledge about eavesdroppers, which limits their real-world applicability. The paper highlights the need for a more robust method that ensures privacy without such assumptions.", "method": "The proposed framework involves two key stages: (1) Disentangled semantic representation extraction from source images using GAN inversion to separate private and task-relevant features, and (2) Learning a neural network to generate task-specific differential privacy (DP) noise through adversarial training. This trained noise avoids using traditional random DP noise (Gaussian or Laplace) and enables precise privacy control by adjusting the privacy budget.", "result": "Experimental results show that the method causes minimal degradation in the legitimate image task performance while significantly reducing reconstruction quality for eavesdroppers. It outperforms the state-of-the-art DP-based method by 0.06-0.29 in LPIPS and 0.10-0.86 in FPPSR metrics.", "conclusion": "This approach offers a novel and effective secure SemCom framework by using learnable DP noise patterns trained with adversarial techniques. It overcomes limitations of current methods by improving privacy guarantees and providing intuitive privacy controls without compromising substantial task performance, addressing real-world security constraints."}}
{"id": "2510.23313", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23313", "abs": "https://arxiv.org/abs/2510.23313", "authors": ["Yaokai Feng", "Kouichi Sakurai"], "title": "Network Intrusion Detection: Evolution from Conventional Approaches to LLM Collaboration and Emerging Risks", "comment": "28 pages,1 figure, 204 references", "summary": "This survey systematizes the evolution of network intrusion detection systems\n(NIDS), from conventional methods such as signature-based and neural network\n(NN)-based approaches to recent integrations with large language models (LLMs).\nIt clearly and concisely summarizes the current status, strengths, and\nlimitations of conventional techniques, and explores the practical benefits of\nintegrating LLMs into NIDS. Recent research on the application of LLMs to NIDS\nin diverse environments is reviewed, including conventional network\ninfrastructures, autonomous vehicle environments and IoT environments.\n  From this survey, readers will learn that: 1) the earliest methods,\nsignature-based IDSs, continue to make significant contributions to modern\nsystems, despite their well-known weaknesses; 2) NN-based detection, although\nconsidered promising and under development for more than two decades, and\ndespite numerous related approaches, still faces significant challenges in\npractical deployment; 3) LLMs are useful for NIDS in many cases, and a number\nof related approaches have been proposed; however, they still face significant\nchallenges in practical applications. Moreover, they can even be exploited as\noffensive tools, such as for generating malware, crafting phishing messages, or\nlaunching cyberattacks. Recently, several studies have been proposed to address\nthese challenges, which are also reviewed in this survey; and 4) strategies for\nconstructing domain-specific LLMs have been proposed and are outlined in this\nsurvey, as it is nearly impossible to train a NIDS-specific LLM from scratch.", "AI": {"tldr": "The paper provides a comprehensive survey of network intrusion detection systems (NIDS), tracing their evolution from traditional methods to recent integrations with large language models (LLMs), and discussing the challenges and future directions.", "motivation": "Understanding the historical progression and current state of NIDS is crucial to identify strengths, limitations, and the potential of emerging LLM-based techniques in this field.", "method": "The paper conducts an extensive literature review, categorizing NIDS approaches into conventional methods (signature-based and neural networks) and modern LLM integrations, with a focus on summarizing existing work and identifying emerging challenges.", "result": "The survey reveals that while signature-based IDS persists despite limitations, NN-based NIDS still struggles with deployment. LLM integration shows promise but faces practical challenges, with some LLMs being susceptible to misuse. Strategies for domain-specific LLMs are explored.", "conclusion": "This survey highlights the progression of NIDS technologies and illustrates the transformative yet challenging role of LLMs. Constructing domain-specific LLMs is seen as a viable path forward, given the impracticality of training NIDS-specific LLMs from scratch."}}
{"id": "2510.23457", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23457", "abs": "https://arxiv.org/abs/2510.23457", "authors": ["Saleh Darzi", "Mirza Masfiqur Rahman", "Imtiaz Karim", "Rouzbeh Behnia", "Attila A Yavuz", "Elisa Bertino"], "title": "Authentication Against Insecure Bootstrapping for 5G Networks: Feasibility, Resiliency, and Transitional Solutions in Post-Quantum Era", "comment": "17 pages, 3 tables, 6 figures", "summary": "The 5G protocol lacks a robust base station authentication mechanism during\nthe initial bootstrapping phase, leaving it susceptible to threats such as fake\nbase station attacks. Conventional solutions, including digital signatures\nbased on Public Key Infrastructures (PKIs) and identity-based signatures, are\ninadequate against quantum-capable adversaries. While integrating NIST's\nPost-Quantum Cryptography (PQC) standards is a leading approach for quantum\nresistance, their suitability for 5G base station authentication remains\nunexplored. Moreover, current solutions are predominantly centralized and lack\nsecurity features such as distributed authentication. This work presents, to\nour knowledge, the first comprehensive network-level performance\ncharacterization of integrating NIST-PQC standards and conventional digital\nsignatures (including threshold and identity-based schemes) into 5G base\nstation authentication. Our findings reveal significant feasibility concerns,\nwith direct PQC adoption hindered by protocol constraints and large signature\nsizes. We also highlight the performance limitations of conventional methods\ndue to the overhead of certificate chains. To mitigate these challenges, we\npropose BORG, a transitional authentication solution based on a Hierarchical\nIdentity-Based Threshold Signature scheme with a Fail-Stop property. BORG\noffers post-mortem post-quantum forgery detection and distributed trust via\nthreshold and compact signatures, well-suited for 5G's stringent requirements.\nOur performance analysis underscores an important warning on the infeasibility\nof direct PQC integration and positions BORG as an effective transitional\nsolution toward future quantum-resilient 5G authentication.", "AI": {"tldr": "This paper addresses the vulnerability of 5G base stations during bootstrapping to fake attacks and lack of quantum-resistant solutions, proposing BORG as a transitional authentication method.", "motivation": "The current 5G protocol lacks a robust authentication mechanism against fake base stations, and existing solutions are not quantum-resistant. Integration of NIST-PQC standards into 5G has not been evaluated for feasibility.", "method": "The paper conducts a network-level performance characterization of integrating NIST-PQC standards and conventional digital signatures (threshold and identity-based schemes) into 5G base station authentication. It proposes BORG, a transitional solution based on a Hierarchical Identity-Based Threshold Signature scheme with a Fail-Stop property.", "result": "The study reveals significant feasibility concerns with direct PQC adoption due to protocol constraints and large signature sizes. Conventional methods also face performance issues from certificate chain overhead. BORG is shown to provide post-quantum forgery detection, distributed trust, and is suitable for 5G's requirements.", "conclusion": "Direct integration of NIST-PQC standards into 5G for authentication is infeasible due to identified performance issues. BORG is positioned as an effective transitional solution toward future quantum-resilient 5G authentication, offering distributed and compact signatures."}}
{"id": "2510.23483", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23483", "abs": "https://arxiv.org/abs/2510.23483", "authors": ["Valentin Reyes H\u00e4usler", "Gabriel Ott", "Aruna Jayasena", "Andreas Peter"], "title": "Towards a Functionally Complete and Parameterizable TFHE Processor", "comment": null, "summary": "Fully homomorphic encryption allows the evaluation of arbitrary functions on\nencrypted data. It can be leveraged to secure outsourced and multiparty\ncomputation. TFHE is a fast torus-based fully homomorphic encryption scheme\nthat allows both linear operations, as well as the evaluation of arbitrary\nnon-linear functions. It currently provides the fastest bootstrapping operation\nperformance of any other FHE scheme. Despite its fast performance, TFHE suffers\nfrom a considerably higher computational overhead for the evaluation of\nhomomorphic circuits. Computations in the encrypted domain are orders of\nmagnitude slower than their unencrypted equivalents. This bottleneck hinders\nthe widespread adoption of (T)FHE for the protection of sensitive data. While\nstate-of-the-art implementations focused on accelerating and outsourcing single\noperations, their scalability and practicality are constrained by high memory\nbandwidth costs. In order to overcome this, we propose an FPGA-based hardware\naccelerator for the evaluation of homomorphic circuits. Specifically, we design\na functionally complete TFHE processor for FPGA hardware capable of processing\ninstructions on the data completely on the FPGA. In order to achieve a higher\nthroughput from our TFHE processor, we implement an improved programmable\nbootstrapping module which outperforms the current state-of-the-art by 240\\% to\n480\\% more bootstrappings per second. Our efficient, compact, and scalable\ndesign lays the foundation for implementing complete FPGA-based TFHE processor\narchitectures.", "AI": {"tldr": "This paper presents an FPGA-based hardware accelerator for the Torus-based Fully Homomorphic Encryption (TFHE) scheme, aiming to address the high computational overhead and memory bandwidth costs faced by existing implementations. The design is fully integrated on the FPGA and features an improved bootstrapping module that significantly increases throughput.", "motivation": "The motivation behind this research is driven by the computational inefficiency of current TFHE implementations. Although TFHE offers fast bootstrapping, it incurs high memory bandwidth costs and suffers from a bottleneck during the evaluation of encrypted circuits. As a result, the performance is orders of magnitude slower than its unencrypted counterpart, which limits its adoption for practical applications involving sensitive data. The authors aim to overcome these limitations by designing a dedicated FPGA-based solution.", "method": "The authors propose a functionally complete TFHE processor implemented on FPGA hardware. This design integrates the entire homomorphic computation process into the FPGA, including the evaluation of encrypted data and the execution of operations. They introduce an improved programmable bootstrapping module, a critical component that accelerates the reinitialization of cryptographic parameters during computation. Additionally, the design is optimized for efficiency, compactness, and scalability to support a broader range of homomorphic circuit evaluations.", "result": "The proposed FPGA-based TFHE processor achieves a 240% to 480% improvement in the number of bootstrappings per second compared to current state-of-the-art implementations of the TFHE scheme.", "conclusion": "The paper concludes that the introduced hardware accelerator provides a significant advancement in practical TFHE performance, offering a scalable and efficient foundation for fully homomorphic encryption on FPGAs. This advancement could lead to a wider adoption of (T)FHE in scenarios requiring secure outsourced computation, as the authors' design effectively mitigates key limitations of existing software-based solutions."}}
