<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 18]
- [cs.SE](#cs.SE) [Total: 13]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [The Beautiful Deception: How 256 Bits Pretend to be Infinity](https://arxiv.org/abs/2510.12802)
*Alexander Towell*

Main category: cs.CR

TL;DR: This paper reveals how cryptographic systems 'store infinity in 256 bits' by using computational hardness and lazy evaluation to simulate infinite randomness. Despite proving true random oracles are impossible, it shows finite automata (like 256-bit entropy) can create sequences that appear infinite to computationally limited observers, explaining why practical cryptography works.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the core challenge of computational cryptography: achieving practical security without access to true infinite randomness. The paper aims to highlight the inherent 'deception' in cryptographic systems—relying on finite information to simulate unbounded randomness—while justifying how this 'beautiful lie' works effectively. The study bridges the gap between theoretical impossibility and real-world cryptographic needs, emphasizing how computational limitations enable this simulation.

Method: The paper combines mathematical proofs with computational demonstrations. It first rigorously proves the impossibility of true random oracles (theoretical idealized cryptographic tools). Then, it introduces lazy evaluation techniques, enabling finite automata to 'pretend' to be infinite by generating data on-demand rather than storing it entirely. Practical Python implementations are included to demonstrate how 256-bit entropy can produce pseudorandom sequences that pass indistinguishability tests for computationally bounded adversaries, effectively creating the illusion of infinite randomness.

Result: The paper's key result is demonstrating that sequences generated from 256-bit entropy through lazy evaluation and computational hardness are indistinguishable from truly random sequences for computationally bounded observers. This validates the practical feasibility of current cryptographic methods in simulating 'infinite' randomness, despite the absence of true random oracles. The Python implementations empirically confirm this claim.

Conclusion: The conclusion is that finite systems, through techniques like lazy evaluation and computational hardness, can simulate infinite randomness effectively, enabling practical cryptographic security despite the impossibility of true random oracles. The paper asserts that cryptographic 'randomness' relies on computational constraints rather than genuine infinite entropy. The TLDR summary for this paper would be that by using 256 bits, the paper demonstrates how cryptographic methods leverage finite entropy to mimic infinite randomness, ensuring that sequences appear random to computationally bounded observers, thereby achieving the 'beautiful lie' of computational indistinguishability. The motivation centers on exposing the theoretical gap between real-world cryptographic practices and the ideal of true randomness, while the method employs mathematical proofs and lazy evaluation to construct finite automata that simulate infinite behavior. The result is the validation that 256-bit entropy can generate sequences indistinguishable from infinite randomness within computational limits. The paper ultimately underscores computational complexity as the cornerstone of cryptographic 'randomness'.

Abstract: How do you store infinity in 256 bits? This paper explores the fundamental
deception at the heart of computational cryptography: using finite information
to simulate infinite randomness. We prove why true random oracles are
impossible, then show how lazy evaluation creates a beautiful lie -- a finite
automaton that successfully pretends to be infinite. We reveal that
``randomness'' in cryptography is actually computational hardness in disguise,
demonstrating through Python implementations how 256 bits of entropy can
generate sequences indistinguishable from infinite randomness to any
computationally bounded observer.How do you store infinity in 256 bits? This
paper explores the fundamental deception at the heart of computational
cryptography: using finite information to simulate infinite randomness. We
prove why true random oracles are impossible, then show how lazy evaluation
creates a beautiful lie -- a finite automaton that successfully pretends to be
infinite. We reveal that ``randomness'' in cryptography is actually
computational hardness in disguise, demonstrating through Python
implementations how 256 bits of entropy can generate sequences
indistinguishable from infinite randomness to any computationally bounded
observer.

</details>


### [2] [Applying Graph Analysis for Unsupervised Fast Malware Fingerprinting](https://arxiv.org/abs/2510.12811)
*ElMouatez Billah Karbab,Mourad Debbabi*

Main category: cs.CR

TL;DR: TrapNet is an unsupervised malware grouping framework using graph community detection and a novel PCA-based fuzzy hashing technique (FloatHash) to analyze malware similarity via static analysis.


<details>
  <summary>Details</summary>
Motivation: The overwhelming volume of daily malware samples necessitates automated, scalable solutions for preliminary triage and grouping based on semantic similarity.

Method: 1) Detect/pack/unpack malware 2) Generate semantics-preserving digests via ordered assembly item analysis 3) Create similarity networks using FloatHash vectors 4]Apply community detection algorithms for malware clustering.

Result: Achieves high coverage/purity in malware community detection with runtime efficiency surpassing existing state-of-the-art methods.

Conclusion: TrapNet demonstrates effective malware clustering through statistical analysis and graph community detection, outperforming alternatives in scalability and accuracy.

Abstract: Malware proliferation is increasing at a tremendous rate, with hundreds of
thousands of new samples identified daily. Manual investigation of such a vast
amount of malware is an unrealistic, time-consuming, and overwhelming task. To
cope with this volume, there is a clear need to develop specialized techniques
and efficient tools for preliminary filtering that can group malware based on
semantic similarity. In this paper, we propose TrapNet, a novel, scalable, and
unsupervised framework for malware fingerprinting and grouping. TrapNet employs
graph community detection techniques for malware fingerprinting and family
attribution based on static analysis, as follows: (1) TrapNet detects packed
binaries and unpacks them using known generic packer tools. (2) From each
malware sample, it generates a digest that captures the underlying semantics.
Since the digest must be dense, efficient, and suitable for similarity
checking, we designed FloatHash (FH), a novel numerical fuzzy hashing technique
that produces a short real-valued vector summarizing the underlying assembly
items and their order. FH is based on applying Principal Component Analysis
(PCA) to ordered assembly items (e.g., opcodes, function calls) extracted from
the malware's assembly code. (3) Representing malware with short numerical
vectors enables high-performance, large-scale similarity computation, which
allows TrapNet to build a malware similarity network. (4) Finally, TrapNet
employs state-of-the-art community detection algorithms to identify dense
communities, which represent groups of malware with similar semantics. Our
extensive evaluation of TrapNet demonstrates its effectiveness in terms of the
coverage and purity of the detected communities, while also highlighting its
runtime efficiency, which outperforms other state-of-the-art solutions.

</details>


### [3] [We Can Hide More Bits: The Unused Watermarking Capacity in Theory and in Practice](https://arxiv.org/abs/2510.12812)
*Aleksandar Petrov,Pierre Fernandez,Tomáš Souček,Hady Elsahar*

Main category: cs.CR

TL;DR: This paper establishes theoretical upper bounds for image watermarking capacity under PSNR and robustness constraints, revealing a significant gap between current models and potential limits. It demonstrates a 4× capacity increase to 1024 bits using ChunkySeal, proving existing methods haven't reached fundamental limits.


<details>
  <summary>Details</summary>
Motivation: Current robust watermarking methods lag far behind theoretical capacity limits despite deep learning advancements, raising concerns about architectural saturation. The paper seeks to quantify this gap and validate whether higher capacities are fundamentally possible.

Method: 1. Analytical derivation of image-carrying capacity upper bounds using PSNR/linear robustness constraints; 2. Empirical validation through minimal setups; 3. Model scaling experiments with ChunkySeal (1024-bit implementation).

Result: a) Theoretical capacity exceeds current capabilities by orders of magnitude; b. Performance gaps persist in simplified experiments; c. ChunkySeal achieves 4× higher capacity (1024 bits), maintaining quality/robustness, demonstrating architectural shortcomings rather than theoretical barriers.

Conclusion: Watermarking capacity saturation remains unachieved, with substantial room for architectural innovation and training methodology improvements. The persistence of theoretical-practical gaps indicates key challenges remain in model design and optimization strategies.

Abstract: Despite rapid progress in deep learning-based image watermarking, the
capacity of current robust methods remains limited to the scale of only a few
hundred bits. Such plateauing progress raises the question: How far are we from
the fundamental limits of image watermarking? To this end, we present an
analysis that establishes upper bounds on the message-carrying capacity of
images under PSNR and linear robustness constraints. Our results indicate
theoretical capacities are orders of magnitude larger than what current models
achieve. Our experiments show this gap between theoretical and empirical
performance persists, even in minimal, easily analysable setups. This suggests
a fundamental problem. As proof that larger capacities are indeed possible, we
train ChunkySeal, a scaled-up version of VideoSeal, which increases capacity 4
times to 1024 bits, all while preserving image quality and robustness. These
findings demonstrate modern methods have not yet saturated watermarking
capacity, and that significant opportunities for architectural innovation and
training strategies remain.

</details>


### [4] [ARTeX: Anonymity Real-world-assets Token eXchange](https://arxiv.org/abs/2510.12821)
*Jaeseong Lee,Junghee Lee*

Main category: cs.CR

TL;DR: This paper proposes ARTeX, a novel token trading platform to address privacy challenges in Real-World Asset (RWA token transactions by resolving anonymity issues while preventing illegal activities.


<details>
  <summary>Details</summary>
Motivation: Blockchain transparency compromises trader anonymity, especially for RWA tokens where existing privacy solutions for fungible tokens (FTs/NFTs fail due to unique characteristics and methodological limitations.

Method: Introduces ARTeX platform that addresses existing method shortcomings through anonymity protection mechanisms and enhanced safeguards against illicit activities.

Result: Achieves effective anonymity protection for RWA token transactions while maintaining security against illegal operations through the proposed platform design.

Conclusion: ARTeX offers a viable solution for balancing privacy and regulatory compliance in RWA token trading, addressing critical gaps in current blockchain privacy solutions.

Abstract: This paper addresses one of the most noteworthy issues in the recent virtual
asset market, the privacy concerns related to token transactions of Real-World
Assets tokens, known as RWA tokens. Following the advent of Bitcoin, the
virtual asset market has experienced explosive growth, spawning movements to
link real-world assets with virtual assets. However, due to the transparency
principle of blockchain technology, the anonymity of traders cannot be
guaranteed. In the existing blockchain environment, there have been instances
of protecting the privacy of fungible tokens (FTs) using mixer services.
Moreover, numerous studies have been conducted to secure the privacy of
non-fungible tokens (NFTs). However, due to the unique characteristics of RWA
tokens and the limitations of each study, it has been challenging to achieve
the goal of anonymity protection effectively. This paper proposes a new token
trading platform, the ARTeX, designed to resolve these issues. This platform
not only addresses the shortcomings of existing methods but also ensures the
anonymity of traders while enhancing safeguards against illegal activities.

</details>


### [5] [SimKey: A Semantically Aware Key Module for Watermarking Language Models](https://arxiv.org/abs/2510.12828)
*Shingo Kodama,Haya Diwan,Lucas Rosenblatt,R. Teal Witter,Niv Cohen*

Main category: cs.CR

TL;DR: SimKey enhances LLM watermarking by encoding semantically consistent keys, making watermarks resistant to paraphrasing while preventing adversarial misuse through meaning-based key generation.


<details>
  <summary>Details</summary>
Motivation: Current LLM watermarking methods are vulnerable to surface-level text edits like paraphrasing and adversarial attacks that falsely attribute unrelated harmful content. These weaknesses create reputational risks for model owners and limit practical deployment effectiveness.

Method: SimKey utilizes locality-sensitive hashing over semantic embeddings to generate watermark keys tied to the meaning of prior context. This allows paraphrased content to retain the same watermark while semantically unrelated text receives different keys, integrated with existing watermarking schemes.

Result: Experimental results show SimKey significantly improves watermark robustness to paraphrasing and translation tasks while effectively preventing false attribution attacks. The approach establishes semantic-aware keying as a practical advancement in text watermarking research.

Conclusion: SimKey introduces a semantic-aware watermarking approach, enhancing robustness against paraphrasing and preventing false attribution of harmful content by generating keys based on contextual meaning rather than surface-level text features.

Abstract: The rapid spread of text generated by large language models (LLMs) makes it
increasingly difficult to distinguish authentic human writing from machine
output. Watermarking offers a promising solution: model owners can embed an
imperceptible signal into generated text, marking its origin. Most leading
approaches seed an LLM's next-token sampling with a pseudo-random key that can
later be recovered to identify the text as machine-generated, while only
minimally altering the model's output distribution. However, these methods
suffer from two related issues: (i) watermarks are brittle to simple
surface-level edits such as paraphrasing or reordering; and (ii) adversaries
can append unrelated, potentially harmful text that inherits the watermark,
risking reputational damage to model owners. To address these issues, we
introduce SimKey, a semantic key module that strengthens watermark robustness
by tying key generation to the meaning of prior context. SimKey uses
locality-sensitive hashing over semantic embeddings to ensure that paraphrased
text yields the same watermark key, while unrelated or semantically shifted
text produces a different one. Integrated with state-of-the-art watermarking
schemes, SimKey improves watermark robustness to paraphrasing and translation
while preventing harmful content from false attribution, establishing
semantic-aware keying as a practical and extensible watermarking direction.

</details>


### [6] [Local Differential Privacy for Federated Learning with Fixed Memory Usage and Per-Client Privacy](https://arxiv.org/abs/2510.12908)
*Rouzbeh Behnia,Jeremiah Birrell,Arman Riasi,Reza Ebrahimi,Kaushik Dutta,Thang Hoang*

Main category: cs.CR

TL;DR: L-RDP enables secure federated learning by using low-memory operations and delivering provable privacy guarantees for intermittent participants, solving resource constraints and privacy compliance issues in sensitive domains like healthcare.


<details>
  <summary>Details</summary>
Motivation: Existing FL systems face privacy risks from leaked client updates and global models. Current LDP methods for FL are resource-intensive (causing client dropouts) and缺乏 reliable privacy guarantees under asynchronous participation, undermining model fairness and regulatory compliance in sensitive domains.

Method: L-RDP is a Local Differential Privacy (LDP) method designed for FL. It employs constant memory usage to mitigate client dropouts and offers rigorous privacy guarantees by accounting for intermittent client participation through an asynchronous participation model.

Result: L-RDP reduces memory demands to prevent client dropouts and provides provable per-client privacy guarantees even with intermittent participation. This enables FL to maintain model generalizability, fairness, and compliance with privacy regulations in healthcare and other sensitive contexts.

Conclusion: The proposed L-RDP method addresses key challenges in federated learning (FL) by ensuring constant memory usage to reduce client dropouts and providing rigorous per-client privacy guarantees, making FL more viable for sensitive domains like healthcare while complying with regulations like HIPAA and GDPR.

Abstract: Federated learning (FL) enables organizations to collaboratively train models
without sharing their datasets. Despite this advantage, recent studies show
that both client updates and the global model can leak private information,
limiting adoption in sensitive domains such as healthcare. Local differential
privacy (LDP) offers strong protection by letting each participant privatize
updates before transmission. However, existing LDP methods were designed for
centralized training and introduce challenges in FL, including high resource
demands that can cause client dropouts and the lack of reliable privacy
guarantees under asynchronous participation. These issues undermine model
generalizability, fairness, and compliance with regulations such as HIPAA and
GDPR. To address them, we propose L-RDP, a DP method designed for LDP that
ensures constant, lower memory usage to reduce dropouts and provides rigorous
per-client privacy guarantees by accounting for intermittent participation.

</details>


### [7] [From misinformation to climate crisis: Navigating vulnerabilities in the cyber-physical-social systems](https://arxiv.org/abs/2510.13058)
*Tooba Aamir,Marthie Grobler,Giovanni Russello*

Main category: cs.CR

TL;DR: This paper argues that human-induced social vulnerabilities are critical but overlooked drivers of systemic risk in cyber-physical-social-climate systems. It shows how cognitive biases, misperceptions, and fragmented decision-making propagate failures across these systems, advocating for integrated resilience strategies that prioritize social dimensions.


<details>
  <summary>Details</summary>
Motivation: Social vulnerabilities (e.g., misinformation, resistance to policy change) are frequently overlooked despite significantly shaping resilience and climate adaptation. Ignoring these human factors risks cascading failures in interdependent systems.

Method: Analyzes interdependencies across cyber, physical, and social systems through cognitive biases, risk misperception, and decision-making silos. Demonstrates how these factors cascade across systems, amplifying climate risks.

Result: Uncovers how unaddressed social vulnerabilities lead to resource misallocation, weakened policy effectiveness, and amplified climate-related risks. Highlights the need for holistic approaches to system resilience.

Conclusion: The chapter stresses the importance of integrating human factors like social vulnerabilities and cognitive biases into decision-making frameworks to enhance resilience in the cyber-physical-social-climate nexus. Addressing these factors is critical for coherent climate adaptation strategies.

Abstract: Within the cyber-physical-social-climate nexus, all systems are deeply
interdependent: cyber infrastructure facilitates communication, data
processing, and automation across physical systems (such as power grids and
networks), while social infrastructure provides the human capital and societal
norms necessary for the system's functionality. Any disruption within any of
these components, whether due to human error or system mismanagement, can
propagate throughout the network, amplifying vulnerabilities and creating a
significantly scaled impact. This chapter explores the critical role of human
vulnerabilities within the cyber-physical-social-climate nexus, focusing on the
interdependencies across cyber, physical, and social systems and how these
interdependencies can scale in a climate context. While cyber and physical
vulnerabilities are readily apparent, social vulnerabilities (such as
misinformation, resistance to policy change, and lack of public awareness)
often go unaddressed despite their profound impact on resilience and climate
adaptation. Social infrastructure, including human capital, societal norms, and
policy frameworks, shapes community responses and underpins adaptive capacity,
yet it is also a significant point of failure when overlooked. This chapter
examines how human cognitive biases, risk misperception, and decision-making
silos within interconnected systems can lead to resource misallocation and
weakened policy effectiveness. These factors are analyzed to demonstrate how
inadequate responses across cyber-physical-social layers can cascade,
amplifying climate-related risks. By addressing these human factors and
aligning decision-making frameworks, we aim to strengthen resilience and foster
cohesive adaptation strategies that account for the intricate interrelations of
cyber-physical-social-climate systems.

</details>


### [8] [From base cases to backdoors: An Empirical Study of Unnatural Crypto-API Misuse](https://arxiv.org/abs/2510.13102)
*Victor Olaiya,Adwait Nadkarni*

Main category: cs.CR

TL;DR: This paper analyzes 5,704 crypto-API invocations to characterize unnatural misuse, revealing tool limitations and providing actionable insights for improving detection of unconventional security vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Existing tools fail to detect non-trivial variants of cryptographic API misuse. Understanding real-world usage patterns and 'unnatural' misuse is critical to guiding tool design for robust detection.

Method: The authors conducted a large-scale qualitative analysis of 5,704 crypto-API invocations from 20,508 Android apps. They used a complexity metric to stratify data, manual reverse engineering, minimal example development, and native code exploration.

Result: Two detailed taxonomies of unnatural misuse, 17 key findings (e.g., highly unusual misuse, evasive code), and evidence that popular tools struggle with even mildly unconventional API usage.

Conclusion: The study highlights four key takeaways for improving tools to detect unnatural cryptographic API misuse, emphasizing the need for methods that can address unconventional usage patterns and evasive code.

Abstract: Tools focused on cryptographic API misuse often detect the most basic
expressions of the vulnerable use, and are unable to detect non-trivial
variants. The question of whether tools should be designed to detect such
variants can only be answered if we know how developers use and misuse
cryptographic APIs in the wild, and in particular, what the unnatural usage of
such APIs looks like. This paper presents the first large-scale study that
characterizes unnatural crypto-API usage through a qualitative analysis of
5,704 representative API invocations. We develop an intuitive complexity metric
to stratify 140,431 crypto-API invocations obtained from 20,508 Android
applications, allowing us to sample 5,704 invocations that are representative
of all strata, with each stratum consisting of invocations with similar
complexity/naturalness. We qualitatively analyze the 5,704 sampled invocations
using manual reverse engineering, through an in-depth investigation that
involves the development of minimal examples and exploration of native code.
Our study results in two detailed taxonomies of unnatural crypto-API misuse,
along with 17 key findings that show the presence of highly unusual misuse,
evasive code, and the inability of popular tools to reason about even mildly
unconventional usage. Our findings lead to four key takeaways that inform
future work focused on detecting unnatural crypto-API misuse.

</details>


### [9] [ShuffleV: A Microarchitectural Defense Strategy against Electromagnetic Side-Channel Attacks in Microprocessors](https://arxiv.org/abs/2510.13111)
*Nuntipat Narkthong,Yukui Luo,Xiaolin Xu*

Main category: cs.CR

TL;DR: ShuffleV is a microarchitecture defense against electromagnetic side-channel attacks by randomizing instruction execution via moving target defense, validated experimentally on RISC-V and FPGA.


<details>
  <summary>Details</summary>
Motivation: Electromagnetic (EM) side-channel attacks exploit microprocessor leakage to extract sensitive data (e.g., cryptographic keys, neural network hyperparameters). Existing defenses lack automatic, hardware-integrated solutions.

Method: ShuffleV implements moving target defense (MTD): hardware units shuffle instruction execution order and insert dummy instructions randomly. It provides six RISC-V-based design options and an FPGA simulator to evaluate performance overhead and trace randomness.

Result: ShuffleV successfully defends AES encryption and neural network inference against EM SCAs on Xilinx PYNQ-Z2 FPGA, achieving automatic protection with no software modifications or user intervention. Performance overheads are quantifiable via the provided simulator.

Conclusion: ShuffleV demonstrates effective countermeasures for EM side-channel vulnerabilities through hardware-driven instruction randomization, adaptable to diverse applications with minimal user effort.

Abstract: The run-time electromagnetic (EM) emanation of microprocessors presents a
side-channel that leaks the confidentiality of the applications running on
them. Many recent works have demonstrated successful attacks leveraging such
side-channels to extract the confidentiality of diverse applications, such as
the key of cryptographic algorithms and the hyperparameter of neural network
models. This paper proposes ShuffleV, a microarchitecture defense strategy
against EM Side-Channel Attacks (SCAs). ShuffleV adopts the moving target
defense (MTD) philosophy, by integrating hardware units to randomly shuffle the
execution order of program instructions and optionally insert dummy
instructions, to nullify the statistical observation by attackers across
repetitive runs. We build ShuffleV on the open-source RISC-V core and provide
six design options, to suit different application scenarios. To enable rapid
evaluation, we develop a ShuffleV simulator that can help users to (1) simulate
the performance overhead for each design option and (2) generate an execution
trace to validate the randomness of execution on their workload. We implement
ShuffleV on a Xilinx PYNQ-Z2 FPGA and validate its performance with two
representative victim applications against EM SCAs, AES encryption, and neural
network inference. The experimental results demonstrate that ShuffleV can
provide automatic protection for these applications, without any user
intervention or software modification.

</details>


### [10] [Privacy-Aware Framework of Robust Malware Detection in Indoor Robots: Hybrid Quantum Computing and Deep Neural Networks](https://arxiv.org/abs/2510.13136)
*Tan Le,Van Le,Sachin Shetty*

Main category: cs.CR

TL;DR: This paper introduces a quantum-boosted AI framework for securing CPS robots against DoS attacks with 95.2% accuracy, privacy protection, and scalable deployment capabilities.


<details>
  <summary>Details</summary>
Motivation: Indoor robotic systems in CPS face DoS attacks that compromise critical functions like localization and control. Existing solutions lack privacy and scalability, necessitating a novel attack detection approach for adversarial environments.

Method: The framework uses hybrid quantum computing with deep neural networks, integrating quantum-enhanced feature encoding and dropout-optimized deep learning to counter DoS attacks without requiring handcrafted thresholds or persistent beacon data.

Result: The system achieves 95.2% detection accuracy under privacy constraints while demonstrating robust generalization, interpretability, and resilience through modular circuit design.

Conclusion: This work advances trustworthy AI for secure, autonomous CPS operations by developing a privacy-aware malware detection framework that combines quantum computing and deep learning.

Abstract: Indoor robotic systems within Cyber-Physical Systems (CPS) are increasingly
exposed to Denial of Service (DoS) attacks that compromise localization,
control and telemetry integrity. We propose a privacy-aware malware detection
framework for indoor robotic systems, which leverages hybrid quantum computing
and deep neural networks to counter DoS threats in CPS, while preserving
privacy information. By integrating quantum-enhanced feature encoding with
dropout-optimized deep learning, our architecture achieves up to 95.2%
detection accuracy under privacy-constrained conditions. The system operates
without handcrafted thresholds or persistent beacon data, enabling scalable
deployment in adversarial environments. Benchmarking reveals robust
generalization, interpretability and resilience against training instability
through modular circuit design. This work advances trustworthy AI for secure,
autonomous CPS operations.

</details>


### [11] [GRIDAI: Generating and Repairing Intrusion Detection Rules via Collaboration among Multiple LLM-based Agents](https://arxiv.org/abs/2510.13257)
*Jiarui Li,Yuhan Chai,Lei Du,Chenyun Duan,Hao Yan,Zhaoquan Gu*

Main category: cs.CR

TL;DR: This paper introduces GRIDAI, a framework using LLM agents to automate intrusion detection rule generation and repair, reducing redundancy by identifying relationships between new attacks and existing rules while mitigating LLM hallucination errors through real-time validation.


<details>
  <summary>Details</summary>
Motivation: Existing rule-based intrusion detection systems lack mechanisms to manage redundancy from new attacks and rule variants, leading to inefficient rule expansion and oversight of attack relationships.

Method: GRIDAI employs collaborative LLM agents to (1) distinguish new attacks from variants of existing rules, (2 ) generate new rules for novel attacks, (3) repair existing rules with enhanced generalization for variants, and (4 ) incorporate real-time validation tools and representative attack samples to correct LLM hallucinations during rule creation.

Result: Experiments on datasets with 50 attack types demonstrated GRIDAI's ability to accurately identify attack relationships, generate/repair rules with high efficiency, and reduce LLM-generated errors through its validation mechanisms.

Conclusion: GRIDAI provides an effective end-to-end solution for automated intrusion detection rule management, addressing redundancy and hallucination issues while improving rule generalization across attack variants.

Abstract: Rule-based network intrusion detection systems play a crucial role in the
real-time detection of Web attacks. However, most existing works primarily
focus on automatically generating detection rules for new attacks, often
overlooking the relationships between new attacks and existing rules, which
leads to significant redundancy within the ever-expanding ruleset. To address
this issue, we propose GRIDAI, a novel end-to-end framework for the automated
Generation and Repair of Intrusion Detection rules through collaboration among
multiple LLM-based agents. Unlike traditional methods, GRIDAI first assesses
the nature of incoming attack samples. If the sample represents a new attack
type, it is used to generate a new rule. Otherwise, the sample is identified as
a variant of an attack already covered by an existing rule and used to repair
the rule by updating the corresponding signature, thereby enhancing its
generalization capability. Additionally, to mitigate syntactic and semantic
errors in rules caused by LLM hallucinations, we incorporate a tool-based
real-time validation mechanism and a representative attack sample maintained
for each rule, enabling fully automated rule generation and repair.
Comprehensive experiments were conducted on a public dataset containing seven
types of attacks and a private dataset with 43 attack types. The results
demonstrate that GRIDAI accurately identifies the relationships between new
attack samples and existing rules, efficiently generates and repairs rules to
handle new attacks and variants, and effectively mitigates the impact of LLM
hallucinations.

</details>


### [12] [Fast Authenticated and Interoperable Multimedia Healthcare Data over Hybrid-Storage Blockchains](https://arxiv.org/abs/2510.13318)
*Jucai Yang,Liang Li,Yiwei Gu,Haiqin Wu*

Main category: cs.CR

TL;DR: FAITH addresses blockchain latency in healthcare by using off-chain ZKPs and Proxy Re-Encryption, reducing user-side verification latency for large EHRs by 98%.


<details>
  <summary>Details</summary>
Motivation: Existing blockchain-based healthcare systems suffer from high latency in user-side hash re-computation for large multimedia data, limiting their practicality in time-sensitive clinical scenarios.

Method: FAITH employs off-chain storage providers to generate verifiable proofs via recursive Zero-Knowledge Proofs (ZKPs) and uses Proxy Re-Encryption (PRE) for re-encryption verification. Metadata and proofs are stored on-chain for public verification.

Result: Experiments show FAITH reduces user-side verification latency by 98%, from 4 seconds to ~70 ms for a 5 GB encrypted file, while maintaining data privacy and integrity.

Conclusion: FAITH enables practical, secure, and interoperable healthcare data sharing over hybrid-storage blockchains through efficient ZKPs and PRE, making it suitable for time-critical applications.

Abstract: The integration of blockchain technology into healthcare presents a paradigm
shift for secure data management, enabling decentralized and tamper-proof
storage and sharing of sensitive Electronic Health Records (EHRs). However,
existing blockchain-based healthcare systems, while providing robust access
control, commonly overlook the high latency in user-side re-computation of
hashes for integrity verification of large multimedia data, impairing their
practicality, especially in time-sensitive clinical scenarios. In this paper,
we propose FAITH, an innovative scheme for \underline{F}ast
\underline{A}uthenticated and \underline{I}nteroperable mul\underline{T}imedia
\underline{H}ealthcare data storage and sharing over hybrid-storage
blockchains. Rather than user-side hash re-computations, FAITH lets an
off-chain storage provider generate verifiable proofs using recursive
Zero-Knowledge Proofs (ZKPs), while the user only needs to perform lightweight
verification. For flexible access authorization, we leverage Proxy
Re-Encryption (PRE) and enable the provider to conduct ciphertext
re-encryption, in which the re-encryption correctness can be verified via ZKPs
against the malicious provider. All metadata and proofs are recorded on-chain
for public verification. We provide a comprehensive analysis of FAITH's
security regarding data privacy and integrity. We implemented a prototype of
FAITH, and extensive experiments demonstrated its practicality for
time-critical healthcare applications, dramatically reducing user-side
verification latency by up to $98\%$, bringing it from $4$ s down to around
$70$ ms for a $5$ GB encrypted file.

</details>


### [13] [Injection, Attack and Erasure: Revocable Backdoor Attacks via Machine Unlearning](https://arxiv.org/abs/2510.13322)
*Baogang Song,Dongdong Zhao,Jianwen Xiang,Qiben Xu,Zizhuo Yu*

Main category: cs.CR

TL;DR: This paper proposes revocable backdoor attacks that optimize triggers for both attack effectiveness and unlearnability, enabling stealthy attacks with post-objective removal capabilities, as demonstrated on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing backdoor attacks leave detectable traces that persist after unlearning. The authors aim to develop attacks that can be proactively and thoroughly removed after achieving their objectives, addressing traceability issues.

Method: The approach frames trigger optimization as a bilevel problem, combining backdoor injection and unlearning simulations. It uses deterministic sample partitioning and PCGrad to resolve optimization conflicts between attack success and unlearnability objectives.

Result: Experiments on CIFAR-10 and ImageNet show the method achieves state-of-the-art attack success rates (ASR) while enabling effective backdoor removal via unlearning, outperforming previous attacks in revocability.

Conclusion: This work introduces revocable backdoor attacks, offering a new research direction that challenges the security of ML systems by enabling backdoor removal post-attack, while maintaining comparable attack performance to existing methods.

Abstract: Backdoor attacks pose a persistent security risk to deep neural networks
(DNNs) due to their stealth and durability. While recent research has explored
leveraging model unlearning mechanisms to enhance backdoor concealment,
existing attack strategies still leave persistent traces that may be detected
through static analysis. In this work, we introduce the first paradigm of
revocable backdoor attacks, where the backdoor can be proactively and
thoroughly removed after the attack objective is achieved. We formulate the
trigger optimization in revocable backdoor attacks as a bilevel optimization
problem: by simulating both backdoor injection and unlearning processes, the
trigger generator is optimized to achieve a high attack success rate (ASR)
while ensuring that the backdoor can be easily erased through unlearning. To
mitigate the optimization conflict between injection and removal objectives, we
employ a deterministic partition of poisoning and unlearning samples to reduce
sampling-induced variance, and further apply the Projected Conflicting Gradient
(PCGrad) technique to resolve the remaining gradient conflicts. Experiments on
CIFAR-10 and ImageNet demonstrate that our method maintains ASR comparable to
state-of-the-art backdoor attacks, while enabling effective removal of backdoor
behavior after unlearning. This work opens a new direction for backdoor attack
research and presents new challenges for the security of machine learning
systems.

</details>


### [14] [Towards Trusted Service Monitoring: Verifiable Service Level Agreements](https://arxiv.org/abs/2510.13370)
*Fernando Castillo,Eduardo Brito,Sebastian Werner,Pille Pullonen-Raudvere,Jonathan Heiss*

Main category: cs.CR

TL;DR: This work enables verifiable SLA monitoring via TEEs and zero-knowledge proofs, ensuring trustless compliance verification with strong security properties and high scalability.


<details>
  <summary>Details</summary>
Motivation: SLA monitoring suffers from trust conflicts when providers self-report metrics, incentivizing underreporting of violations. Existing solutions lack cryptographic assurance for tamper-proof enforcement.

Method: The approach converts machine-readable SLA clauses into verifiable predicates within TEEs, collects timestamped telemetry in Merkle trees, and uses zero-knowledge proofs to generate cryptographically verifiable compliance evidence without exposing raw data.

Result: The prototype achieves linear scalability beyond 1 million events/hour with near-constant-time proof generation/verification, demonstrating practical feasibility for large-scale service monitoring.

Conclusion: The paper presents a framework for trustless SLA enforcement using TEEs and zero-knowledge proofs, enabling automated compliance verification with strong cryptographic guarantees.

Abstract: Service Level Agreement (SLA) monitoring in service-oriented environments
suffers from inherent trust conflicts when providers self-report metrics,
creating incentives to underreport violations. We introduce a framework for
generating verifiable SLA violation claims through trusted hardware monitors
and zero-knowledge proofs, establishing cryptographic foundations for genuine
trustworthiness in service ecosystems. Our approach starts with
machine-readable SLA clauses converted into verifiable predicates and monitored
within Trusted Execution Environments. These monitors collect timestamped
telemetry, organize measurements into Merkle trees, and produce signed
attestations. Zero-knowledge proofs aggregate Service-Level Indicators to
evaluate compliance, generating cryptographic proofs verifiable by
stakeholders, arbitrators, or insurers in disputes, without accessing
underlying data. This ensures three security properties: integrity,
authenticity, and validity. Our prototype demonstrates linear scaling up to
over 1 million events per hour for measurements with near constant-time proof
generation and verification for single violation claims, enabling trustless SLA
enforcement through cryptographic guarantees for automated compliance
verification in service monitoring.

</details>


### [15] [Toward Efficient Inference Attacks: Shadow Model Sharing via Mixture-of-Experts](https://arxiv.org/abs/2510.13451)
*Li Bai,Qingqing Ye,Xinwei Zhang,Sen Zhang,Zi Liang,Jianliang Xu,Haibo Hu*

Main category: cs.CR

TL;DR: The paper introduces SHAPOOL, a shadow pool training framework that reduces the computational cost of training multiple shadow models by jointly training shared models using a Mixture-of-Experts mechanism, while maintaining attack effectiveness.


<details>
  <summary>Details</summary>
Motivation: Machine learning models are susceptible to inference attacks that require a large number of shadow models, which are costly to train and use independently.

Method: SHAPOOL employs a Mixture-of-Experts mechanism as a shadow pool to train multiple shared models jointly. It introduces path-choice routing, pathway regularization, and pathway alignment to ensure model diversity and consistency with target models.

Result: SHAPOOL significantly reduces the computational cost of shadow model construction while maintaining comparable performance in membership inference attacks.

Conclusion: The proposed SHAPOOL framework offers an efficient and effective solution for training shadow models, improving the practicality of inference attacks with minimal resource consumption.

Abstract: Machine learning models are often vulnerable to inference attacks that expose
sensitive information from their training data. Shadow model technique is
commonly employed in such attacks, such as membership inference. However, the
need for a large number of shadow models leads to high computational costs,
limiting their practical applicability. Such inefficiency mainly stems from the
independent training and use of these shadow models. To address this issue, we
present a novel shadow pool training framework SHAPOOL, which constructs
multiple shared models and trains them jointly within a single process. In
particular, we leverage the Mixture-of-Experts mechanism as the shadow pool to
interconnect individual models, enabling them to share some sub-networks and
thereby improving efficiency. To ensure the shared models closely resemble
independent models and serve as effective substitutes, we introduce three novel
modules: path-choice routing, pathway regularization, and pathway alignment.
These modules guarantee random data allocation for pathway learning, promote
diversity among shared models, and maintain consistency with target models. We
evaluate SHAPOOL in the context of various membership inference attacks and
show that it significantly reduces the computational cost of shadow model
construction while maintaining comparable attack performance.

</details>


### [16] [Who Speaks for the Trigger? Dynamic Expert Routing in Backdoored Mixture-of-Experts Transformers](https://arxiv.org/abs/2510.13462)
*Xin Zhao,Xiaojun Chen,Bingshan Liu,Haoyu Gao,Zhendong Zhao,Yilong Chen*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) with Mixture-of-Experts (MoE) architectures
achieve impressive performance and efficiency by dynamically routing inputs to
specialized subnetworks, known as experts. However, this sparse routing
mechanism inherently exhibits task preferences due to expert specialization,
introducing a new and underexplored vulnerability to backdoor attacks. In this
work, we investigate the feasibility and effectiveness of injecting backdoors
into MoE-based LLMs by exploiting their inherent expert routing preferences. We
thus propose BadSwitch, a novel backdoor framework that integrates task-coupled
dynamic trigger optimization with a sensitivity-guided Top-S expert tracing
mechanism. Our approach jointly optimizes trigger embeddings during pretraining
while identifying S most sensitive experts, subsequently constraining the Top-K
gating mechanism to these targeted experts. Unlike traditional backdoor attacks
that rely on superficial data poisoning or model editing, BadSwitch primarily
embeds malicious triggers into expert routing paths with strong task affinity,
enabling precise and stealthy model manipulation. Through comprehensive
evaluations across three prominent MoE architectures (Switch Transformer,
QwenMoE, and DeepSeekMoE), we demonstrate that BadSwitch can efficiently hijack
pre-trained models with up to 100% success rate (ASR) while maintaining the
highest clean accuracy (ACC) among all baselines. Furthermore, BadSwitch
exhibits strong resilience against both text-level and model-level defense
mechanisms, achieving 94.07% ASR and 87.18% ACC on the AGNews dataset. Our
analysis of expert activation patterns reveals fundamental insights into MoE
vulnerabilities. We anticipate this work will expose security risks in MoE
systems and contribute to advancing AI safety.

</details>


### [17] [How Blind and Low-Vision Users Manage Their Passwords](https://arxiv.org/abs/2510.13538)
*Alexander Ponticello,Filipo Sharevski,Simon Anell,Katharina Krombholz*

Main category: cs.CR

TL;DR: This paper investigates the use of password managers by Blind and Low-Vision (BLV) users and highlights their convenience-driven adoption despite accessibility barriers to security features. It emphasizes the need for practical accessibility improvements to ensure secure password management.


<details>
  <summary>Details</summary>
Motivation: The motivation is that managing passwords securely and conveniently remains a significant challenge for many users, including BLV individuals, due to accessibility issues in existing password managers that hinder secure practices.

Method: The study involved a qualitative interview with 33 Blind and Low-Vision (BLV) participants to understand their password management practices in the context of accessibility barriers.

Result: The study found that BLV users primarily adopt password managers for convenience in storing and retrieving passwords, but avoid security features like password generation due to accessibility barriers. This results in insecure practices such as password reuse and writing crucial information in braille.

Conclusion: The paper concludes by emphasizing that at-the-moment accessibility and usability improvements for password managers can establish trust and ensure secure practices while respecting BLV users' sense of agency.

Abstract: Managing passwords securely and conveniently is still an open problem for
many users. Existing research has examined users' password management
strategies and identified pain points, such as security concerns, leading to
insecure practices. We investigate how Blind and Low-Vision (BLV) users tackle
this problem and how password managers can assist them. This paper presents the
results of a qualitative interview study with N = 33 BLV participants. We found
that all participants utilize password managers to some extent, which they
perceive as fairly accessible. However, the adoption is mainly driven by the
convenience of storing and retrieving passwords. The security advantages -
generating strong, random passwords - were avoided mainly due to the absence of
practical accessibility. Password managers do not adhere to BLV users'
underlying needs for agency, which stem from experiences with inaccessible
software and vendors who deprioritize accessibility issues. Underutilization of
password managers leads BLV users to adopt insecure practices, such as reusing
predictable passwords or resorting to 'security through obscurity' by writing
important credentials in braille. We conclude our analysis by discussing the
need to implement practical accessibility and usability improvements for
password managers as a way of establishing trust and secure practices while
maintaining BLV users' agency.

</details>


### [18] [In-Browser LLM-Guided Fuzzing for Real-Time Prompt Injection Testing in Agentic AI Browsers](https://arxiv.org/abs/2510.13543)
*Avihay Cohen*

Main category: cs.CR

TL;DR: The paper introduces a browser-based LLM-guided fuzzing framework for real-time detection of prompt injection vulnerabilities in agentic AI browsers.


<details>
  <summary>Details</summary>
Motivation: Current agentic AI browsers, while powerful for web task automation, are susceptible to prompt injection attacks that can bypass security measures and allow malicious actions.

Method: The authors developed a fuzzing framework that operates within the browser, using an LLM to guide the testing process and identify vulnerabilities in real-time.

Result: The framework successfully uncovered prompt injection vulnerabilities in agentic AI browsers, demonstrating its effectiveness in real-time detection.

Conclusion: The proposed LLM-guided fuzzing framework is an innovative solution to detect and mitigate prompt injection attacks in AI-integrated web browsers.

Abstract: Large Language Model (LLM) based agents integrated into web browsers (often
called agentic AI browsers) offer powerful automation of web tasks. However,
they are vulnerable to indirect prompt injection attacks, where malicious
instructions hidden in a webpage deceive the agent into unwanted actions. These
attacks can bypass traditional web security boundaries, as the AI agent
operates with the user privileges across sites. In this paper, we present a
novel fuzzing framework that runs entirely in the browser and is guided by an
LLM to automatically discover such prompt injection vulnerabilities in real
time.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [19] [AutoCode: LLMs as Problem Setters for Competitive Programming](https://arxiv.org/abs/2510.12803)
*Shang Zhou,Zihan Zheng,Kaiyuan Liu,Zeyu Shen,Zerui Cheng,Zexing Chen,Hansen He,Jianzhu Yao,Huanzhi Mao,Qiuyang Mang,Tianfu Fu,Beichen Li,Dongruixuan Li,Wenhao Chai,Zhuang Liu,Aleksandra Korolova,Peter Henderson,Natasha Jaques,Pramod Viswanath,Saining Xie,Jingbo Shang*

Main category: cs.SE

TL;DR: AutoCode generates high-quality competitive programming problems with 99\% consistency to official judgments, surpassing prior methods and verified by grandmaster-level experts.


<details>
  <summary>Details</summary>
Motivation: Manually crafting competitive programming problems requires precise constraints and algorithm targeting. This work tests if LLMs can automate this complex task to produce truly challenging, contest-grade problems.

Method: AutoCode uses multi-round validation to create problem statements/test cases, generates novel variants via random seed problems, and cross-verifies solutions against test cases to filter malformed problems.

Result: Achieved 99\% consistency with official judgments (vs. <81\\% for HardTests), produced contest-quality novel problems validated by human experts at Grandmaster level, and maintained both correctness and algorithm-specific targeting.

Conclusion: AutoCode demonstrates LLMs can reliably generate competition-grade programming problems through systematic validation, achieving state-of-the-art performance and expert-verified quality comparable to elite human problem authors.

Abstract: Writing competitive programming problems is exacting. Authors must: set
constraints, input distributions, and edge cases that rule out shortcuts;
target specific algorithms (e.g., max-flow, dynamic programming, data
structures); and calibrate complexity beyond the reach of most competitors. We
argue that this makes for an ideal test of general large language model
capabilities and study whether they can do this reliably. We introduce
AutoCode, which uses multiple rounds of validation to yield competition-grade
problem statements and test cases. On held-out problems, AutoCode test suites
approach 99% consistency with official judgments, a significant improvement
over current state-of-the-art methods like HardTests, which achieve less than
81%. Furthermore, starting with a random seed problem, AutoCode can create
novel variants with reference and brute-force solutions. By cross-verifying
these generated solutions against test cases, we can further filter out
malformed problems. Our system ensures high correctness, as verified by human
experts. AutoCode successfully produces novel problems judged by
Grandmaster-level (top 0.3%) competitive programmers to be of contest quality.

</details>


### [20] [SpareCodeSearch: Searching for Code Context When You Have No Spare GPU](https://arxiv.org/abs/2510.12948)
*Minh Nguyen*

Main category: cs.SE

TL;DR: This paper proposes a keyword-search based Retrieval-Augmented Generation (RAG) framework for Code Language Models (CLMs), demonstrating sufficient effectiveness without semantic search's resource demands, achieving 0.748 and 0.725 chRF scores on Kotlin and Python benchmarks.


<details>
  <summary>Details</summary>
Motivation: Semantic search in RAG frameworks requires high computational resources, making them unsuitable for lightweight applications like in-IDE code completion. This paper addresses the need for a resource-efficient retrieval method.

Method: The authors replace semantic search with a keyword-based approach to retrieve code contexts, enabling lightweight deployment without compromising retrieval quality for CLM input prompts.

Result: The method achieved 0.748 chRF score on the Kotlin track and 0.725 on the Python track of the Code Context Competition benchmark, proving its effectiveness for code completion tasks.

Conclusion: Keyword search can efficiently retrieve relevant code contexts for CLMs while reducing computational requirements, making RAG applicable to resource-constrained environments like IDEs.

Abstract: Retrieval-Augmented Generation (RAG) frameworks aim to enhance Code Language
Models (CLMs) by including another module for retrieving relevant context to
construct the input prompt. However, these retrieval modules commonly use
semantic search, requiring substantial computational resources for training and
hosting these embedded models, making them infeasible to integrate into
lightweight applications such as in-IDE AI-based code completion. In this
solution paper, we prove that using keyword-search is sufficient to retrieve
relevant and useful code context inside large codebases, without the need for
extensive GPU resources. The usefulness of code contexts found by our solution
is demonstrated through their completion results on the Code Context
Competition's benchmark, reaching 0.748 and 0.725 chRF scores on Kotlin and
Python tracks, respectively.

</details>


### [21] [ADPerf: Investigating and Testing Performance in Autonomous Driving Systems](https://arxiv.org/abs/2510.13078)
*Tri Minh-Triet Pham,Diego Elias Costa,Weiyi Shang,Jinqiu Yang*

Main category: cs.SE

TL;DR: This paper introduces ADPerf, a tool to generate realistic LiDAR point cloud test cases that expose latency issues in 3D obstacle detection modules of autonomous driving systems (Apollo and Autoware), highlighting their impact on system reliability and downstream modules.


<details>
  <summary>Details</summary>
Motivation: Obstacle detection latency and resilience to LiDAR data changes in autonomous vehicles are under-researched despite critical safety implications. Prior studies lack systematic analysis of latency propagation across industry-grade systems.

Method: 1) Comprehensive performance evaluation of obstacle detection modules in Apollo and Autoware
2. Development of ADPerf to synthesize latency-exposing point cloud scenarios
3. Stress-testing 3D detection modules and analyzing latency propagation through trajectory prediction components.

Result: Experimental validation demonstrates ADPerf effectively exposes latency bottlenecks in 3D detection, with latency propagation significantly impacting downstream modules. Results quantify how increased detection latency reduces obstacle availability for critical decision-making processes.

Conclusion: 3D obstacle detection is a critical latency bottleneck in autonomous systems requiring rigorous performance testing. Latency cascades across modules, undermining overall system reliability, necessitating improved design considerations for time-critical autonomous driving workflows.

Abstract: Obstacle detection is crucial to the operation of autonomous driving systems,
which rely on multiple sensors, such as cameras and LiDARs, combined with code
logic and deep learning models to detect obstacles for time-sensitive
decisions. Consequently, obstacle detection latency is critical to the safety
and effectiveness of autonomous driving systems. However, the latency of the
obstacle detection module and its resilience to various changes in the LiDAR
point cloud data are not yet fully understood. In this work, we present the
first comprehensive investigation on measuring and modeling the performance of
the obstacle detection modules in two industry-grade autonomous driving
systems, i.e., Apollo and Autoware. Learning from this investigation, we
introduce ADPerf, a tool that aims to generate realistic point cloud data test
cases that can expose increased detection latency. Increasing latency decreases
the availability of the detected obstacles and stresses the capabilities of
subsequent modules in autonomous driving systems, i.e., the modules may be
negatively impacted by the increased latency in obstacle detection.
  We applied ADPerf to stress-test the performance of widely used 3D obstacle
detection modules in autonomous driving systems, as well as the propagation of
such tests on trajectory prediction modules. Our evaluation highlights the need
to conduct performance testing of obstacle detection components, especially 3D
obstacle detection, as they can be a major bottleneck to increased latency of
the autonomous driving system. Such an adverse outcome will also further
propagate to other modules, reducing the overall reliability of autonomous
driving systems.

</details>


### [22] [TRUSTVIS: A Multi-Dimensional Trustworthiness Evaluation Framework for Large Language Models](https://arxiv.org/abs/2510.13106)
*Ruoyu Sun,Da Song,Jiayang Song,Yuheng Huang,Lei Ma*

Main category: cs.SE

TL;DR: TRUSTVIS is an interactive framework for evaluating LLM trustworthiness, combining robust methods and user-friendly visualizations to detect vulnerabilities and guide model improvements.


<details>
  <summary>Details</summary>
Motivation: The motivation addresses critical concerns about Large Language Models' (LLMs) safety and robustness, seeking to provide a reliable and accessible evaluation framework to identify vulnerabilities.

Method: The framework integrates perturbation methods like AutoDAN and employs majority voting across evaluation methods, supported by an interactive visualization interface.

Result: Preliminary case studies on models like Vicuna-7b, Llama2-7b, and GPT-3.5 demonstrate TRUSTVIS's effectiveness in identifying safety/robustness issues and its utility for detailed result exploration.

Conclusion: The paper concludes that TRUSTVIS effectively enhances the accessibility and reliability of assessing LLM trustworthiness, enabling targeted improvements through its interactive interface.

Abstract: As Large Language Models (LLMs) continue to revolutionize Natural Language
Processing (NLP) applications, critical concerns about their trustworthiness
persist, particularly in safety and robustness. To address these challenges, we
introduce TRUSTVIS, an automated evaluation framework that provides a
comprehensive assessment of LLM trustworthiness. A key feature of our framework
is its interactive user interface, designed to offer intuitive visualizations
of trustworthiness metrics. By integrating well-known perturbation methods like
AutoDAN and employing majority voting across various evaluation methods,
TRUSTVIS not only provides reliable results but also makes complex evaluation
processes accessible to users. Preliminary case studies on models like
Vicuna-7b, Llama2-7b, and GPT-3.5 demonstrate the effectiveness of our
framework in identifying safety and robustness vulnerabilities, while the
interactive interface allows users to explore results in detail, empowering
targeted model improvements. Video Link: https://youtu.be/k1TrBqNVg8g

</details>


### [23] [Isolating Compiler Bugs through Compilation Steps Analysis](https://arxiv.org/abs/2510.13128)
*Yujie Liu,Mingxuan Zhu,Shengyu Cheng,Dan Hao*

Main category: cs.SE

TL;DR: The paper introduces CompSCAN, a novel compiler bug isolation technique that analyzes compilation steps sequences to achieve higher effectiveness and efficiency compared to existing methods, validated on 185 real-world LLVM/GCC bugs.


<details>
  <summary>Details</summary>
Motivation: Compiler bugs propagate widely but current isolation techniques lack causal analysis of internal compilation steps, limiting their effectiveness despite mutations of compilation inputs.

Method: CompSCAN's three-stage process: (1). Extracting failure-inducing compilation steps sequence; (2). Identifying bug-causing steps and collecting compiler code elements; (3). Calculating suspicious scores for code elements and generating rank-ordered bug isolation results.

Result: CompSCAN outperforms state-of-the-art techniques (ETEM/ODFL) with 50/85/100/123 bugs isolated in Top-1/3/5/10 ranks respectively, achieving 44.51%-50.18%/31.58%-49.12 relative improvements while running faster per bug.

Conclusion: CompSCAN demonstrates superior effectiveness and efficiency in compiler bug isolation through step-wise sequence analysis, overcoming limitations of input-mutation-based approaches.

Abstract: Compilers are essential to software systems, and their bugs can propagate to
dependent software. Ensuring compiler correctness is critical. However,
isolating compiler bugs remains challenging due to the internal complexity of
compiler execution. Existing techniques primarily mutate compilation inputs to
generate passing and failing tests, but often lack causal analysis of internal
steps, limiting their effectiveness.
  To address this limitation, we propose CompSCAN, a novel compiler bug
isolation technique that applies analysis over the sequence of compilation
steps. CompSCAN follows a three-stage process: (1) extracting the array of
compilation steps that leads to the original failure, (2) identifying
bug-causing steps and collecting corresponding compiler code elements, and (3)
calculating suspicious scores for each code element and outputting a suspicious
ranking list as the bug isolation result.
  We evaluate CompSCAN on 185 real-world LLVM and GCC bugs. Results show that
CompSCAN outperforms state-of-the-art techniques in both effectiveness and
efficiency. CompSCAN successfully isolates 50, 85, 100, and 123 bugs within the
Top-1/3/5/10 ranks, respectively. Compared with ETEM and ODFL, two
state-of-the-art compiler bug isolation techniques, CompSCAN achieves relative
improvements of 44.51% / 50.18% / 36.24% / 24.49% over ETEM, and 31.58% /
49.12% / 44.93% / 21.78% over ODFL on those metrics. Moreover, CompSCAN runs
faster on average per bug than both baselines.

</details>


### [24] [GRACE: Globally-Seeded Representation-Aware Cluster-Specific Evolution for Compiler Auto-Tuning](https://arxiv.org/abs/2510.13176)
*Haolin Pan,Chao Zha,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: Introduces GRACE, a ML-based framework for compiler optimization that achieves significant code size reductions (10-10.2% over opt -Oz) with under 1s/program tuning time through pass sequence clustering and coreset evolution.


<details>
  <summary>Details</summary>
Motivation: Standard compiler heuristics are suboptimal for program-specific optimizations like code size reduction. Iterative compilation is too slow for practical use, while ML approaches lack generalization to unseen programs. There is a need for a framework that balances search efficiency, optimization quality, and generalization.

Method: GRACE leverages pass synergies and a weighted scoring method to generate high-quality initial pass sequences. It uses contrastive learning with pass sequence-based data augmentation to create similarity-aware clusters and applies evolutionary search within clusters to produce a coreset of specialized sequences. Test-time optimization involves selecting and refining the best coreset sequence.

Result: GRACE reduces LLVM IR instruction counts by 10.09% on LLVM 10.0.0 and 10.19% on LLVM 18.1.6 compared to opt -Oz across 7 diverse datasets. It achieves these results with an average tuning time of less than 1 second per program.

Conclusion: GRACE is a state-of-the-art framework for compiler auto-tuning that combines efficient search space reduction, contrastive learning, and evolutionary clustering to achieve robust code size optimization with minimal tuning time (under 1s/program).

Abstract: Compiler pass selection and phase ordering present a significant challenge in
achieving optimal program performance, particularly for objectives like code
size reduction. Standard compiler heuristics offer general applicability but
often yield suboptimal, program-specific results due to their one-size-fits-all
nature. While iterative compilation can find tailored solutions, its
prohibitive search cost limits practical use. Machine learning approaches
promise faster inference but frequently struggle with generalization to unseen
programs. This paper introduces GRACE, a novel framework for compiler
auto-tuning, demonstrated for LLVM IR instruction count optimization. GRACE
effectively curtails the search space by leveraging pass synergies and a
weighted scoring method to generate initial high-quality candidate sequences
and a pass pool. It then employs contrastive learning, using pass
sequence-based data augmentation, to create program embeddings that facilitate
similarity-aware clustering. Evolutionary search within these clusters yields a
coreset of $k$ specialized pass sequences designed for robust generalization to
unseen programs. At test time, GRACE efficiently selects the best coreset
sequence and refines it using lightweight techniques. Experimental results on
seven diverse datasets show that GRACE reduces LLVM IR instruction count by an
average of 10.09% on LLVM 10.0.0 and 10.19% on LLVM 18.1.6 compared to opt -Oz,
while incurring an average tuning time of less than 1s per program,
demonstrating its state-of-the-art performance and practical effectiveness.

</details>


### [25] [Synergy-Guided Compiler Auto-Tuning of Nested LLVM Pass Pipelines](https://arxiv.org/abs/2510.13184)
*Haolin Pan,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: The paper introduces an LLVM New Pass Manager-aligned auto-tuning framework that uses formal grammars and structure-aware genetic algorithms to generate valid optimization pipelines, achieving 13.62%% better instruction count reduction than standard optimizations.


<details>
  <summary>Details</summary>
Motivation: Legacy linear-pass auto-tuning methods are incompatible with LLVM's hierarchical New Pass Manager, leading to invalid pipelines and suboptimal optimizations.

Method: Developed 1) formal grammar for valid nested pipelines, 2 fuel-based representation, 3 structure-aware genetic algorithm with pass relationship mining and refinement stages.

Result: 13.62%%*/18.1.6 evaluation shows superior instruction count reduction over LLVM's -O3 optimization, demonstrating effective navigation of complex constrained search space.

Conclusion: Framework establishes native hierarchical pipeline optimization for LLVM, outperforming legacy approaches while guaranteeing syntactic validity.

Abstract: Compiler optimization relies on sequences of passes to improve program
performance. Selecting and ordering these passes automatically, known as
compiler auto-tuning, is challenging due to the large and complex search space.
Existing approaches generally assume a linear sequence of passes, a model
compatible with legacy compilers but fundamentally misaligned with the
hierarchical design of the LLVM New Pass Manager. This misalignment prevents
them from guaranteeing the generation of syntactically valid optimization
pipelines. In this work, we present a new auto-tuning framework built from the
ground up for the New Pass Manager. We introduce a formal grammar to define the
space of valid nested pipelines and a forest-based data structure for their
native representation. Upon this foundation, we develop a structure-aware
Genetic Algorithm whose operators manipulate these forests directly, ensuring
that all candidate solutions are valid by construction. The framework first
mines synergistic pass relationships to guide the search. An optional
refinement stage further explores subtle performance variations arising from
different valid structural arrangements.
  We evaluate our approach on seven benchmark datasets using LLVM 18.1.6. The
discovered pipelines achieve an average of 13.62% additional instruction count
reduction compared to the standard opt -Oz optimization level, showing that our
framework is capable of navigating this complex, constrained search space to
identify valid and effective pass pipelines.

</details>


### [26] [Towards Richer Challenge Problems for Scientific Computing Correctness](https://arxiv.org/abs/2510.13423)
*Matthew Sottile,Mohit Tekriwal,John Sarracino*

Main category: cs.SE

TL;DR: This paper advocates for specialized challenge problems to enhance formal methods/programming languages verification in scientific computing by addressing domain-specific correctness dimensions and bridging gaps in community understanding.


<details>
  <summary>Details</summary>
Motivation: Current FM/PL verification techniques struggle with the complexities of scientific computing (SC) applications, partly due to a lack of shared understanding of SC's correctness requirements between the SC and PL/FM communities. This gap limits progress in ensuring machine-verifiable correctness for SC systems.

Method: The authors propose designing specialized challenge problems to address correctness in scientific computing, complementing existing FM/PL benchmarks. They outline dimensions of correctness and criteria for creating challenges, such as mathematical precision, numerical stability, and domain-specific constraints.

Result: The paper proposes several dimensions of correctness relevant to SC and provides guidelines for designing challenge problems to evaluate verification techniques in this domain. These contributions aim to standardize and improve methods for addressing SC's unique correctness challenges.

Conclusion: The paper concludes that developing specialized challenge problems, tailored to the unique dimensions of correctness in scientific computing, is essential for advancing FM/PL verification techniques and ensuring their effectiveness in real-world SC applications through cross-community collaboration.

Abstract: Correctness in scientific computing (SC) is gaining increasing attention in
the formal methods (FM) and programming languages (PL) community. Existing
PL/FM verification techniques struggle with the complexities of realistic SC
applications. Part of the problem is a lack of a common understanding between
the SC and PL/FM communities of machine-verifiable correctness challenges and
dimensions of correctness in SC applications.
  To address this gap, we call for specialized challenge problems to inform the
development and evaluation of FM/PL verification techniques for correctness in
SC. These specialized challenges are intended to augment existing problems
studied by FM/PL researchers for general programs to ensure the needs of SC
applications can be met. We propose several dimensions of correctness relevant
to scientific computing, and discuss some guidelines and criteria for designing
challenge problems to evaluate correctness in scientific computing.

</details>


### [27] [Verifying a Sparse Matrix Algorithm Using Symbolic Execution](https://arxiv.org/abs/2510.13424)
*Alexander C. Wilton*

Main category: cs.SE

TL;DR: Symbolic execution enhances testing of math-heavy scientific software by identifying bugs missed by traditional methods, validated via sparse matrix algorithms.


<details>
  <summary>Details</summary>
Motivation: Scientific software is inherently complex and optimized mathematically, making it susceptible to hard-to-detect bugs that traditional testing methods may fail to uncover.

Method: Symbolic execution is employed to create tests analogous to traditional unit tests. This approach is applied specifically to a sparse matrix algorithm, allowing for thorough verification by exploring multiple execution paths.

Result: The methodology successfully demonstrates stronger verification guarantees for sparse matrix algorithms, highlighting symbolic execution's ability to reveal edge-case vulnerabilities not captured by conventional testing.

Conclusion: Symbolic execution offers a robust methodology for testing complex scientific software, providing verification guarantees that traditional testing methods lack, particularly effective in identifying subtle bugs in mathematical algorithms like sparse matrix computations.

Abstract: Scientific software is, by its very nature, complex. It is mathematical and
highly optimized which makes it prone to subtle bugs not as easily detected by
traditional testing. We outline how symbolic execution can be used to write
tests similar to traditional unit tests while providing stronger verification
guarantees and apply this methodology to a sparse matrix algorithm.

</details>


### [28] [OpenDerisk: An Industrial Framework for AI-Driven SRE, with Design, Implementation, and Case Studies](https://arxiv.org/abs/2510.13561)
*Peng Di,Faqiang Chen,Xiao Bai,Hongjun Yang,Qingfeng Li,Ganglin Wei,Jian Mou,Feng Shi,Keting Chen,Peng Tang,Zhitao Shen,Zheng Li,Wenhui Shi,Junwei Guo,Hang Yu*

Main category: cs.SE

TL;DR: OpenDerisk is a specialized multi-agent framework solving SRE challenges through causal reasoning and domain-specific collaboration. It outperforms existing solutions and scales to production environments at Ant Group, now open-sourced.


<details>
  <summary>Details</summary>
Motivation: Modern software complexity overwhelms SRE teams, and existing AI solutions lack causal reasoning or are improperly tailored to SRE's investigative workflows. This creates a need for specialized automation that emulates expert diagnostic reasoning.

Method: The authors developed OpenDerisk, an open-source multi-agent framework comprising a diagnostic-native collaboration model, pluggable reasoning engine, knowledge engine, and the MCP protocol to coordinate specialist agents for multi-domain SRE tasks.

Result: OpenDerisk outperforms state-of-the-art baselines in accuracy and efficiency, and is successfully deployed at Ant Group with 3,000+ daily active users across diverse scenarios, demonstrating industrial scalability.

Conclusion: OpenDerisk effectively addresses SRE challenges through a specialized multi-agent framework that excels in complex diagnostics, validated by industrial deployment and performance gains.

Abstract: The escalating complexity of modern software imposes an unsustainable
operational burden on Site Reliability Engineering (SRE) teams, demanding
AI-driven automation that can emulate expert diagnostic reasoning. Existing
solutions, from traditional AI methods to general-purpose multi-agent systems,
fall short: they either lack deep causal reasoning or are not tailored for the
specialized, investigative workflows unique to SRE. To address this gap, we
present OpenDerisk, a specialized, open-source multi-agent framework
architected for SRE. OpenDerisk integrates a diagnostic-native collaboration
model, a pluggable reasoning engine, a knowledge engine, and a standardized
protocol (MCP) to enable specialist agents to collectively solve complex,
multi-domain problems. Our comprehensive evaluation demonstrates that
OpenDerisk significantly outperforms state-of-the-art baselines in both
accuracy and efficiency. This effectiveness is validated by its large-scale
production deployment at Ant Group, where it serves over 3,000 daily users
across diverse scenarios, confirming its industrial-grade scalability and
practical impact. OpenDerisk is open source and available at
https://github.com/derisk-ai/OpenDerisk/

</details>


### [29] [Auto-repair without test cases: How LLMs fix compilation errors in large industrial embedded code](https://arxiv.org/abs/2510.13575)
*Han Fu,Sigrid Eldh,Kristian Wiklund,Andreas Ermedahl,Philipp Haller,Cyrille Artho*

Main category: cs.SE

TL;DR: LLMs enable test-case-free compilation error repair in industrial CI, achieving 63% resolution accuracy and reducing debugging time from hours to minutes.


<details>
  <summary>Details</summary>
Motivation: Compilation errors in co-developed hardware/software industrial systems are challenging to automate because existing methods require test cases, which are unavailable for non-compilable code.

Method: The study collected over 40,000 commits and evaluated four state-of-the-art LLMs in an industrial CI system, comparing LLM-generated repairs to manual fixes by human programmers.

Result: LLMs resolved 63% of compilation errors in the dataset, with 83% of fixes deemed reasonable and 65% of successful repairs completed within 8 minutes (vs. hours manually).

Conclusion: LLMs can be effectively employed in industrial CI systems to resolve compilation errors in embedded systems, offering high accuracy and significantly reducing debugging time compared to manual methods.

Abstract: The co-development of hardware and software in industrial embedded systems
frequently leads to compilation errors during continuous integration (CI).
Automated repair of such failures is promising, but existing techniques rely on
test cases, which are not available for non-compilable code.
  We employ an automated repair approach for compilation errors driven by large
language models (LLMs). Our study encompasses the collection of more than 40000
commits from the product's source code. We assess the performance of an
industrial CI system enhanced by four state-of-the-art LLMs, comparing their
outcomes with manual corrections provided by human programmers. LLM-equipped CI
systems can resolve up to 63 % of the compilation errors in our baseline
dataset. Among the fixes associated with successful CI builds, 83 % are deemed
reasonable. Moreover, LLMs significantly reduce debugging time, with the
majority of successful cases completed within 8 minutes, compared to hours
typically required for manual debugging.

</details>


### [30] [Property Testing for Ocean Models. Can We Specify It? (Invited Talk)](https://arxiv.org/abs/2510.13692)
*Deepak A. Cherian*

Main category: cs.SE

TL;DR: This paper explores using property tests derived from geophysical fluid dynamics theory to validate ocean models, addressing the 'oracle problem' in model testing. While the approach shows theoretical promise, practical feasibility remains to be determined.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve the oracle problem in testing ocean numerical models by leveraging property-testing methodologies from computer science, specifically those developed by Prof. John Hughes, to create physics-based validation criteria.

Method: The author proposes framing simple idealized geophysical fluid dynamics (GFD) problems as property tests, inspired by property-testing literature, to address the oracle problem in ocean model validation.

Result: The author demonstrates that physics (GFD theory) naturally aligns with property testing formulations, but notes that practical implementation details (e.g., which specific tests are most valuable) remain to be validated.

Conclusion: The paper concludes that while the approach of using GFD-based property tests shows promise, further research is needed to determine the feasibility and utility of specific tests in practical ocean modeling contexts.

Abstract: I take inspiration from the property-testing literature, particularly the
work of Prof. John Hughes, and explore how such ideas might be applied to
numerical models of the ocean. Specifically, I ask whether geophysical fluid
dynamics (GFD) theory, expressed as property tests, might be used to address
the oracle problem of testing the correctness of ocean models. I propose that a
number of simple idealized GFD problems can be framed as property tests. These
examples clearly illustrate how physics naturally lends itself to specifying
property tests. Which of these proposed tests might be most feasible and
useful, remains to be seen.

</details>


### [31] [On Pretraining for Project-Level Code Completion](https://arxiv.org/abs/2510.13697)
*Maksim Sapronov,Evgeniy Glukhov*

Main category: cs.SE

TL;DR: By optimizing repository-level pretraining and using a new RoPE parameter, the 1.5B OpenCoder model matches state-of-the-art performance on code benchmarks with far less data, enabling practical code completion in resource-limited scenarios.


<details>
  <summary>Details</summary>
Motivation: The paper investigates how repository-processing strategies impact in-context learning in code models to enable efficient context-aware code completions, especially for resource-constrained settings.

Method: The method involved extending the model's context window by training on an additional 1B tokens of curated data, testing various repository-processing strategies, and adapting the RoPE scaling parameter. The model's performance was evaluated on the Long Code Arena benchmark.

Result: The model achieved comparable performance on the Long Code Arena benchmark despite using a smaller dataset than competitors. Repository-processing techniques and the RoPE scaling adjustment improved results, while a simpler file-level approach at the original sequence length remained effective.

Conclusion: The study demonstrates that repository-level pretraining with optimized techniques, such as the new RoPE scaling parameter, can achieve strong performance even with limited data, while simpler methods remain effective for constrained environments.

Abstract: Repository-level pretraining is commonly used to enable large language models
for code to leverage codebase-wide context. This enhances their ability to
generate accurate and context-aware code completions. In this work, we
investigate how different repository-processing strategies affect in-context
learning in OpenCoder, a 1.5B-parameter model. We extend its context window
from 4,096 to 16,384 tokens by training on additional 1B tokens of curated
repository-level data. Despite relying on a smaller dataset than competing
models (which often use hundreds of billions of tokens), our model achieves
comparable performance on the Long Code Arena benchmark. We find that various
repository-processing techniques yield similarly strong results, with the
primary gain coming from adapting to a new rotary positional embedding (RoPE)
scaling parameter. Finally, we show that a simpler file-level training approach
at the original sequence length remains highly effective, opening up
repository-level code completion research to settings with more constrained
data and compute resources.

</details>
