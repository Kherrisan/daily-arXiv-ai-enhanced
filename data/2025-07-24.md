<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 23]
- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [CASPER: Contrastive Approach for Smart Ponzi Scheme Detecter with More Negative Samples](https://arxiv.org/abs/2507.16840)
*Weijia Yang,Tian Lan,Leyuan Liu,Wei Chen,Tianqing Zhu,Sheng Wen,Xiaosong Zhang*

Main category: cs.CR

TL;DR: CASPER, a contrastive learning framework, improves smart Ponzi scheme detection in blockchain transactions by reducing reliance on labeled data.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning detection methods require abundant labeled blockchain data, which is scarce and limits effective model training.

Method: A contrastive learning approach called CASPER generates effective smart contract code representations using unlabeled datasets, with evaluation on XBlock.

Result: CASPER outperformed baselines by 2.3% F1 score with full labels, and achieved 20% better F1 than baseline with only 25% labeled data.

Conclusion: CASPER enables cost-efficient and scalable smart Ponzi scheme detection by leveraging unlabeled blockchain data with contrastive learning techniques.

Abstract: The rapid evolution of digital currency trading, fueled by the integration of
blockchain technology, has led to both innovation and the emergence of smart
Ponzi schemes. A smart Ponzi scheme is a fraudulent investment operation in
smart contract that uses funds from new investors to pay returns to earlier
investors. Traditional Ponzi scheme detection methods based on deep learning
typically rely on fully supervised models, which require large amounts of
labeled data. However, such data is often scarce, hindering effective model
training. To address this challenge, we propose a novel contrastive learning
framework, CASPER (Contrastive Approach for Smart Ponzi detectER with more
negative samples), designed to enhance smart Ponzi scheme detection in
blockchain transactions. By leveraging contrastive learning techniques, CASPER
can learn more effective representations of smart contract source code using
unlabeled datasets, significantly reducing both operational costs and system
complexity. We evaluate CASPER on the XBlock dataset, where it outperforms the
baseline by 2.3% in F1 score when trained with 100% labeled data. More
impressively, with only 25% labeled data, CASPER achieves an F1 score nearly
20% higher than the baseline under identical experimental conditions. These
results highlight CASPER's potential for effective and cost-efficient detection
of smart Ponzi schemes, paving the way for scalable fraud detection solutions
in the future.

</details>


### [2] [SynthCTI: LLM-Driven Synthetic CTI Generation to enhance MITRE Technique Mapping](https://arxiv.org/abs/2507.16852)
*Álvaro Ruiz-Ródenas,Jaime Pujante Sáez,Daniel García-Algora,Mario Rodríguez Béjar,Jorge Blasco,José Luis Hernández-Ramos*

Main category: cs.CR

TL;DR: SynthCTI is a data augmentation framework that improves automated Cyber Threat Intelligence classification by generating synthetic MITRE ATT&CK technique descriptions through clustering-based semantic modeling, enabling smaller models to rival larger ones in performance.


<details>
  <summary>Details</summary>
Motivation: Manual mapping of unstructured threat data to MITRE ATT&CK techniques is labor-intensive, while automated approaches struggle with data scarcity and class imbalance. Previous work on domain-specific LLMs focuses more on architectural improvements than addressing these fundamental data limitations.

Method: The framework employs a clustering-based strategy to extract semantic context from training data, using this to guide LLMs in creating both lexically diverse and semantically faithful synthetic CTI sentences for underrepresented techniques.

Result: Evaluated on CTI-to-MITRE and TRAM datasets, SynthCTI boosted ALBERT's macro-F1 score by 48.6% (0.35→0.52) and increased SecureBERT's score to 0.6558 (from 0.4412). Smaller augmented models outperformed non-augmented larger models.

Conclusion: Data augmentation through SynthCTI demonstrates significant value for CTI classification, enabling efficient system development by mitigating data limitations that have hindered prior approaches focused solely on model architecture improvements.

Abstract: Cyber Threat Intelligence (CTI) mining involves extracting structured
insights from unstructured threat data, enabling organizations to understand
and respond to evolving adversarial behavior. A key task in CTI mining is
mapping threat descriptions to MITRE ATT\&CK techniques. However, this process
is often performed manually, requiring expert knowledge and substantial effort.
Automated approaches face two major challenges: the scarcity of high-quality
labeled CTI data and class imbalance, where many techniques have very few
examples. While domain-specific Large Language Models (LLMs) such as SecureBERT
have shown improved performance, most recent work focuses on model architecture
rather than addressing the data limitations. In this work, we present SynthCTI,
a data augmentation framework designed to generate high-quality synthetic CTI
sentences for underrepresented MITRE ATT\&CK techniques. Our method uses a
clustering-based strategy to extract semantic context from training data and
guide an LLM in producing synthetic CTI sentences that are lexically diverse
and semantically faithful. We evaluate SynthCTI on two publicly available CTI
datasets, CTI-to-MITRE and TRAM, using LLMs with different capacity.
Incorporating synthetic data leads to consistent macro-F1 improvements: for
example, ALBERT improves from 0.35 to 0.52 (a relative gain of 48.6\%), and
SecureBERT reaches 0.6558 (up from 0.4412). Notably, smaller models augmented
with SynthCTI outperform larger models trained without augmentation,
demonstrating the value of data generation methods for building efficient and
effective CTI classification systems.

</details>


### [3] [Building a robust OAuth token based API Security: A High level Overview](https://arxiv.org/abs/2507.16870)
*Senthilkumar Gopal*

Main category: cs.CR

TL;DR: This paper outlines key principles for building secure and scalable token-based API systems, focusing on OAuth 2.0 integration, cryptographic foundations, and token lifecycle management.


<details>
  <summary>Details</summary>
Motivation: API proliferation has introduced security challenges requiring systematic solutions for authentication and authorization.

Method: Discusses required components, OAuth 2.0 integration, extensible token architectures, cryptographic foundations, persistence strategies, and lifecycle management practices.

Result: Provides actionable guidelines for scope definition, expiration policies, revocation mechanisms, and establishes a customizable security baseline for API systems.

Conclusion: Emphasizes balancing security imperatives with practical requirements using fundamental concepts like the CIA triad to create resilient systems for evolving threats.

Abstract: APIs (Application Programming Interfaces) or Web Services are the
foundational building blocks that enable interconnected systems. However this
proliferation of APIs has also introduced security challenges that require
systematic and scalable solutions for secure authentication and authorization.
This paper presents the fundamentals necessary for building a such a
token-based API security system. It discusses the components necessary, the
integration of OAuth 2.0, extensibility of the token architectures, necessary
cryptographic foundations, and persistence strategies to ensure secure and
resilient operations. In addition to architectural concerns, the paper explores
best practices for token lifecycle management, scope definition, expiration
policies, and revocation mechanisms, all framed within a real-world scenario.
By adhering to these principles, developers can establish a robust baseline
while maintaining the flexibility to customize their domain-specific
requirements. The approach does not claim to cover all variations necessary for
diverse architectures but instead focuses on key principles essential for any
standard API token authentication system. Throughout, the paper emphasizes
balancing practical considerations with security imperatives and uses key
concepts such as the CIA triad, OAuth standards, secure token life cycle, and
practices for protecting sensitive user and application data. The intent is to
equip developers with the foundational knowledge necessary to build secure,
scalable token-based API security systems ready to handle the evolving threat
landscape.

</details>


### [4] [CompLeak: Deep Learning Model Compression Exacerbates Privacy Leakage](https://arxiv.org/abs/2507.16872)
*Na Li,Yansong Gao,Hongsheng Hu,Boyu Kuang,Anmin Fu*

Main category: cs.CR

TL;DR: CompLeak evaluates privacy risks in model compression via membership inference attacks, revealing varying leakage across different compressed versions and methods.


<details>
  <summary>Details</summary>
Motivation: Current model compression techniques overlook privacy risks, particularly how compressed models might leak information that original models do not.

Method: Introduces CompLeak, a framework using three MIA variants—CompLeakNR (single compressed model), CompLeakSR (original + one compressed), and CompLeakMR (multiple compressed)—to assess privacy leakage. Explores meta-information like confidence vectors for enhanced inference.

Result: Experiments across seven model architectures (e.g., ResNet, BERT, GPT-2) and six datasets demonstrate that different compression methods (pruning, quantization, weight clustering) and access conditions (single or multiple compressed models) significantly amplify privacy risks through distinct leakage patterns.

Conclusion: Compression introduces non-negligible privacy risks, evidenced by scalable and synergistic leakage under multi-model access scenarios. Highlights the urgent need to address privacy in model compression workflows.

Abstract: Model compression is crucial for minimizing memory storage and accelerating
inference in deep learning (DL) models, including recent foundation models like
large language models (LLMs). Users can access different compressed model
versions according to their resources and budget. However, while existing
compression operations primarily focus on optimizing the trade-off between
resource efficiency and model performance, the privacy risks introduced by
compression remain overlooked and insufficiently understood.
  In this work, through the lens of membership inference attack (MIA), we
propose CompLeak, the first privacy risk evaluation framework examining three
widely used compression configurations that are pruning, quantization, and
weight clustering supported by the commercial model compression framework of
Google's TensorFlow-Lite (TF-Lite) and Facebook's PyTorch Mobile. CompLeak has
three variants, given available access to the number of compressed models and
original model. CompLeakNR starts by adopting existing MIA methods to attack a
single compressed model, and identifies that different compressed models
influence members and non-members differently. When the original model and one
compressed model are available, CompLeakSR leverages the compressed model as a
reference to the original model and uncovers more privacy by combining meta
information (e.g., confidence vector) from both models. When multiple
compressed models are available with/without accessing the original model,
CompLeakMR innovatively exploits privacy leakage info from multiple compressed
versions to substantially signify the overall privacy leakage. We conduct
extensive experiments on seven diverse model architectures (from ResNet to
foundation models of BERT and GPT-2), and six image and textual benchmark
datasets.

</details>


### [5] [Revisiting Pre-trained Language Models for Vulnerability Detection](https://arxiv.org/abs/2507.16887)
*Youpeng Li,Weiliang Qi,Xuyu Wang,Fuxun Yu,Xinda Wang*

Main category: cs.CR

TL;DR: This paper evaluates 17 pre-trained language models (PLMs) for vulnerability detection (VD), revealing challenges in real-world scenarios and proposing future directions for improving their effectiveness in VD applications.


<details>
  <summary>Details</summary>
Motivation: Current PLM-based VD evaluations lack comprehensive data preparation, evaluation setups, and experimental conditions, leading to inaccurate assessments and unaddressed real-world application challenges.

Method: The study introduces RevisitVD, a framework assessing 17 PLMs (including code-specific and large-scale models) across fine-tuning and prompt engineering. It evaluates performance in varied training/testing settings and analyzes robustness to code normalization, abstraction, and semantic-preserving transformations using newly constructed datasets.

Result: PLMs with syntax/semantic pre-training outperform general-purpose and code-corpora-only models. However, they struggle with complex dependencies, code perturbations (normalization/abstraction), and semantic-preserving transformations. Truncation limits from context windows cause significant labeling errors in evaluations.

Conclusion: The findings underscore the need for rigorous practical evaluations and highlight key limitations (code complexity, robustness, context window issues) requiring attention to enhance PLMs for realistic VD applications.

Abstract: The rapid advancement of pre-trained language models (PLMs) has demonstrated
promising results for various code-related tasks. However, their effectiveness
in detecting real-world vulnerabilities remains a critical challenge. % for the
security community. While existing empirical studies evaluate PLMs for
vulnerability detection (VD), their inadequate consideration in data
preparation, evaluation setups, and experimental settings undermines the
accuracy and comprehensiveness of evaluations. This paper introduces RevisitVD,
an extensive evaluation of 17 PLMs spanning smaller code-specific PLMs and
large-scale PLMs using newly constructed datasets. Specifically, we compare the
performance of PLMs under both fine-tuning and prompt engineering, assess their
effectiveness and generalizability across various training and testing
settings, and analyze their robustness against code normalization, abstraction,
and semantic-preserving transformations.
  Our findings reveal that, for VD tasks, PLMs incorporating pre-training tasks
designed to capture the syntactic and semantic patterns of code outperform both
general-purpose PLMs and those solely pre-trained or fine-tuned on large code
corpora. However, these models face notable challenges in real-world scenarios,
such as difficulties in detecting vulnerabilities with complex dependencies,
handling perturbations introduced by code normalization and abstraction, and
identifying semantic-preserving vulnerable code transformations. Also, the
truncation caused by the limited context windows of PLMs can lead to a
non-negligible amount of labeling errors. This study underscores the importance
of thorough evaluations of model performance in practical scenarios and
outlines future directions to help enhance the effectiveness of PLMs for
realistic VD applications.

</details>


### [6] [Evaluating Ensemble and Deep Learning Models for Static Malware Detection with Dimensionality Reduction Using the EMBER Dataset](https://arxiv.org/abs/2507.16952)
*Md Min-Ha-Zul Abedin,Tazqia Mehrub*

Main category: cs.CR

TL;DR: The study evaluates the performance of eight machine learning algorithms for static malware detection using the EMBER dataset under original, PCA, and LDA preprocessing methods. Boosting models (LightGBM, XGBoost) outperform others, while LDA benefits KNN but harms boosting models. TabNet underperforms in reduced feature settings, and EDA confirms the dataset's feature discriminability.


<details>
  <summary>Details</summary>
Motivation: The research aims to identify optimal machine learning models and preprocessing strategies for static malware detection in high-dimensional feature spaces, offering practical insights for real-world deployment and future system development.

Method: The authors tested eight classification models (LightGBM, XGBoost, CatBoost, Random Forest, Extra Trees, HistGradientBoosting, KNN, TabNet) on the EMBER dataset across three preprocessing setups (original features, PCA, LDA). Model performance was measured via accuracy, precision, recall, F1, and AUC. EDA included mutual information ranking, dimensionality reduction visualizations (PCA/t-SNE), and outlier detection (Isolation Forest, LOF).

Result: Boosting models (e.g., LightGBM, XGBoost) achieved highest accuracy and robustness. PCA had minimal impact on boosting models, while LDA improved KNN but degraded boosting performance. TabNet, despite theoretical advantages, failed in reduced feature settings. EDA validated the EMBER dataset's feature effectiveness.

Conclusion: Boosting models are recommended for high-dimensional static malware detection due to their reliability and stability. Dimensionality reduction techniques (PCA/LDA) should be applied cautiously based on model type. The study provides benchmarks for model selection and preprocessing in malware analysis.

Abstract: This study investigates the effectiveness of several machine learning
algorithms for static malware detection using the EMBER dataset, which contains
feature representations of Portable Executable (PE) files. We evaluate eight
classification models: LightGBM, XGBoost, CatBoost, Random Forest, Extra Trees,
HistGradientBoosting, k-Nearest Neighbors (KNN), and TabNet, under three
preprocessing settings: original feature space, Principal Component Analysis
(PCA), and Linear Discriminant Analysis (LDA). The models are assessed on
accuracy, precision, recall, F1 score, and AUC to examine both predictive
performance and robustness. Ensemble methods, especially LightGBM and XGBoost,
show the best overall performance across all configurations, with minimal
sensitivity to PCA and consistent generalization. LDA improves KNN performance
but significantly reduces accuracy for boosting models. TabNet, while promising
in theory, underperformed under feature reduction, likely due to architectural
sensitivity to input structure. The analysis is supported by detailed
exploratory data analysis (EDA), including mutual information ranking, PCA or
t-SNE visualizations, and outlier detection using Isolation Forest and Local
Outlier Factor (LOF), which confirm the discriminatory capacity of key features
in the EMBER dataset. The results suggest that boosting models remain the most
reliable choice for high-dimensional static malware detection, and that
dimensionality reduction should be applied selectively based on model type.
This work provides a benchmark for comparing classification models and
preprocessing strategies in malware detection tasks and contributes insights
that can guide future system development and real-world deployment.

</details>


### [7] [From Cracks to Crooks: YouTube as a Vector for Malware Distribution](https://arxiv.org/abs/2507.16996)
*Iman Vakilinia*

Main category: cs.CR

TL;DR: This paper examines how cybercriminals exploit YouTube's open environment to distribute malware through deceptive campaigns, highlighting a new evasion technique using multilingual metadata and its growing prevalence in malicious videos.


<details>
  <summary>Details</summary>
Motivation: YouTube's billions of users and openness make it a prime target for cybercriminals to spread malware undetected, requiring analysis of emerging threats and evasion mechanisms.

Method: The study analyzes YouTube-based malware campaigns promoting free software/game cheats, investigates deceptive video demonstration methods, and proposes a novel evasion technique leveraging YouTube's multilingual metadata capabilities against automated detection systems.

Result: Empirical findings confirm the new metadata-based evasion technique is increasingly used in recent malicious videos to bypass automated detection and removal mechanisms.

Conclusion: YouTube's platform vulnerabilities necessitate advanced detection strategies for malware campaigns exploiting multilingual metadata, with recommendations for improved threat monitoring and mitigation frameworks.

Abstract: With billions of users and an immense volume of daily uploads, YouTube has
become an attractive target for cybercriminals aiming to leverage its vast
audience. The platform's openness and trustworthiness provide an ideal
environment for deceptive campaigns that can operate under the radar of
conventional security tools. This paper explores how cybercriminals exploit
YouTube to disseminate malware, focusing on campaigns that promote free
software or game cheats. It discusses deceptive video demonstrations and the
techniques behind malware delivery. Additionally, the paper presents a new
evasion technique that abuses YouTube's multilingual metadata capabilities to
circumvent automated detection systems. Findings indicate that this method is
increasingly being used in recent malicious videos to avoid detection and
removal.

</details>


### [8] [The Postman: A Journey of Ethical Hacking in PosteID/SPID Borderland](https://arxiv.org/abs/2507.17007)
*Gabriele Costa*

Main category: cs.CR

TL;DR: This paper analyzes a critical privilege escalation vulnerability discovered in PosteID (Italy's SPID implementation) through a vulnerability assessment, details the technical investigation, and the ethical disclosure process, serving as a case study for security researchers.


<details>
  <summary>Details</summary>
Motivation: The researchers aimed to contribute to the security of digital identity systems and provide a transparent example of coordinated vulnerability disclosure for the ethical hacking community.

Method: The paper describes the technical steps taken during the vulnerability assessment, including identification techniques, exploitation processes, and secure communication with Poste Italiane to responsibly report and patch the vulnerability.

Result: A critical privilege escalation flaw was identified, and a patch was successfully implemented. The case study includes both technical insights and lessons learned from the disclosure process.

Conclusion: The work underscores the importance of proactive vulnerability assessment in public identity infrastructure and demonstrates how responsible disclosure can strengthen system security while maintaining research transparency.

Abstract: This paper presents a vulnerability assessment activity that we carried out
on PosteID, the implementation of the Italian Public Digital Identity System
(SPID) by Poste Italiane. The activity led to the discovery of a critical
privilege escalation vulnerability, which was eventually patched. The overall
analysis and disclosure process represents a valuable case study for the
community of ethical hackers. In this work, we present both the technical steps
and the details of the disclosure process.

</details>


### [9] [Towards Trustworthy AI: Secure Deepfake Detection using CNNs and Zero-Knowledge Proofs](https://arxiv.org/abs/2507.17010)
*H M Mohaimanul Islam,Huynh Q. N. Vo,Aditya Rane*

Main category: cs.CR

TL;DR: TrustDefender is a two-stage framework combining a lightweight CNN for real-time deepfake detection in XRs with a succinct ZKP protocol to validate results while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: Synthetic media deepfakes threaten information integrity in immersive environments. Existing solutions trade-off between real-time performance for XR platforms and strong privacy safeguards required for sensitive applications.

Method: 1) Lightweight CNN for real-time deepfake detection in extended reality streams. 2) Integrated succinct zero-knowledge proof protocol for cryptographic validation of detection results without exposing raw user data. Combines computer vision and cryptography to address computational and privacy constraints.

Result: 95.3% detection accuracy across benchmark datasets. Efficient ZKP generation and validation enables seamless integration with high-performance AI systems. Maintains privacy through non-disclosure of raw data.

Conclusion: Establishes foundation for reliable AI in immersive, privacy-sensitive applications by successfully integrating advanced computer vision models with provable security mechanisms through the TrustDefender framework.

Abstract: In the era of synthetic media, deepfake manipulations pose a significant
threat to information integrity. To address this challenge, we propose
TrustDefender, a two-stage framework comprising (i) a lightweight convolutional
neural network (CNN) that detects deepfake imagery in real-time extended
reality (XR) streams, and (ii) an integrated succinct zero-knowledge proof
(ZKP) protocol that validates detection results without disclosing raw user
data. Our design addresses both the computational constraints of XR platforms
while adhering to the stringent privacy requirements in sensitive settings.
Experimental evaluations on multiple benchmark deepfake datasets demonstrate
that TrustDefender achieves 95.3% detection accuracy, coupled with efficient
proof generation underpinned by rigorous cryptography, ensuring seamless
integration with high-performance artificial intelligence (AI) systems. By
fusing advanced computer vision models with provable security mechanisms, our
work establishes a foundation for reliable AI in immersive and
privacy-sensitive applications.

</details>


### [10] [GATEBLEED: Exploiting On-Core Accelerator Power Gating for High Performance & Stealthy Attacks on AI](https://arxiv.org/abs/2507.17033)
*Joshua Kalyanapu,Farshad Dizani,Darsh Asher,Azam Ghanbari,Rosario Cammarota,Aydin Aysu,Samira Mirbagher Ajorpaz*

Main category: cs.CR

TL;DR: The paper identifies GATEBLEED, a timing side and covert channel in Intel's AMX, which exposes AI privacy risks by leaking sensitive model parameters like confidence thresholds through power-gating-induced timing delays.


<details>
  <summary>Details</summary>
Motivation: Increasing AI power consumption drives integration of accelerators into CPUs, but aggressive power gating mechanisms like AMX's may compromise privacy by leaking critical model parameters during computationally-heavy operations.

Method: The authors analyze timing variations caused by AMX's power gating during matrix multiplications in ML models, identifying dozens of gadgets in major libraries (HuggingFace, PyTorch, TensorFlow) that expose private information. They demonstrate attacks on both local and remote inference systems.

Result: GATEBLEED achieves 81% membership inference accuracy on AMX-optimized transformers, 100% expert choice prediction for MoE models, and demonstrates covert channel capabilities that bypass traditional cache defenses. It operates effectively under network conditions where prior attacks fail.

Conclusion: GATEBLEED represents a novel microarchitectural threat to AI privacy, circumventing existing protections through CPU power management mechanisms. The work highlights urgent needs for hardware design changes and ML library hardening to address this pervasive vulnerability.

Abstract: As power consumption from AI training and inference continues to increase, AI
accelerators are being integrated directly into the CPU. Intel's Advanced
Matrix Extensions (AMX) is one such example, debuting on the 4th generation
Intel Xeon Scalable CPU. We discover a timing side and covert channel,
GATEBLEED, caused by the aggressive power gating utilized to keep the CPU
within operating limits. We show that the GATEBLEED side channel is a threat to
AI privacy as many ML models such as transformers and CNNs make critical
computationally-heavy decisions based on private values like confidence
thresholds and routing logits. Timing delays from selective powering down of
AMX components mean that each matrix multiplication is a potential leakage
point when executed on the AMX accelerator. Our research identifies over a
dozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,
TensorFlow, etc.), revealing that they can leak sensitive and private
information. GATEBLEED poses a risk for local and remote timing inference, even
under previous protective measures. GATEBLEED can be used as a high
performance, stealthy remote covert channel and a generic magnifier for timing
transmission channels, capable of bypassing traditional cache defenses to leak
arbitrary memory addresses and evading state of the art microarchitectural
attack detectors under realistic network conditions and system configurations
in which previous attacks fail. We implement an end-to-end microarchitectural
inference attack on a transformer model optimized with Intel AMX, achieving a
membership inference accuracy of 81% and a precision of 0.89. In a CNN-based or
transformer-based mixture-of-experts model optimized with Intel AMX, we leak
expert choice with 100% accuracy.

</details>


### [11] [SoK: Securing the Final Frontier for Cybersecurity in Space-Based Infrastructure](https://arxiv.org/abs/2507.17064)
*Nafisa Anjum,Tasnuva Farheen*

Main category: cs.CR

TL;DR: This paper comprehensively analyzes space cyberattack vectors (ground, space, satellite, constellations), evaluates mitigation measures for space infrastructures, proposes a Risk Scoring Framework, and identifies research challenges for developing robust space cybersecurity solutions.


<details>
  <summary>Details</summary>
Motivation: Modern technology increasingly relies on space-based assets for critical infrastructure and security, making them vulnerable to cyberattacks. Existing research has not thoroughly examined all possible attack vectors or rigorously assessed mitigation effectiveness, creating a critical knowledge gap.

Method: The study employs a comprehensive analysis approach to investigate cyberattack vectors across four domains (ground, space, satellites, constellations), evaluates existing mitigation strategies, and introduces a Risk Scoring Framework to quantify threats and effectiveness.

Result: 1) A comprehensive cataloging of potential space cyberattack vectors. 2) Assessment showing limitations in current mitigation techniques. 3) A Risk Scoring Framework for threat analysis. 4) Identification of key research challenges for space cybersecurity innovation.

Conclusion: The paper highlights the urgent need for space cybersecurity frameworks and cutting-edge defense solutions through systematic threat vector analysis, emphasizing the necessity of further research to address identified challenges and improve infrastructure protection.

Abstract: With the advent of modern technology, critical infrastructure,
communications, and national security depend increasingly on space-based
assets. These assets, along with associated assets like data relay systems and
ground stations, are, therefore, in serious danger of cyberattacks. Strong
security defenses are essential to ensure data integrity, maintain secure
operations, and protect assets in space and on the ground against various
threats. Previous research has found discrete vulnerabilities in space systems
and suggested specific solutions to address them. Such research has yielded
valuable insights, but lacks a thorough examination of space cyberattack
vectors and a rigorous assessment of the efficacy of mitigation techniques.
This study tackles this issue by taking a comprehensive approach to analyze the
range of possible space cyber-attack vectors, which include ground, space,
satellite, and satellite constellations. In order to address the particular
threats, the study also assesses the efficacy of mitigation measures that are
linked with space infrastructures and proposes a Risk Scoring Framework. Based
on the analysis, this paper identifies potential research challenges for
developing and testing cutting-edge technology solutions, encouraging robust
cybersecurity measures needed in space.

</details>


### [12] [Analysis of Post-Quantum Cryptography in User Equipment in 5G and Beyond](https://arxiv.org/abs/2507.17074)
*Sanzida Hoque,Abdullah Aydeger,Engin Zeydan,Madhusanka Liyanage*

Main category: cs.CR

TL;DR: This paper evaluates post-quantum cryptography (PQC) algorithms in 5G UE-to-UE communications using realistic network conditions, identifying ML-KEM/ML-DSA as most efficient for latency-sensitive applications while SPHINCS+/HQC have higher overheads.


<details>
  <summary>Details</summary>
Motivation: Quantum computing threatens classical public-key cryptography, necessitating practical evaluation of PQC in 5G networks to ensure security transition readiness.

Method: Full 5G stack emulation (Open5GS, UERANSIM) combined with PQC-TLS 1.3 implementation (BoringSSL, liboqs) and performance metrics tracking (handshake latency, CPU/memory usage, bandwidth, retransmission rates) under varied configurations and client loads.

Result: ML-KEM+ML-DSA showed best efficiency for latency-sensitive apps, while SPHINCS+ and HQC combinations demonstrated high computational/transmission overheads across tested parameters.

Conclusion: Practical evaluations reveal significant performance tradeoffs in PQC algorithms for 5G, emphasizing need for implementation-specific optimizations to balance security with network latencies and resource constraints.

Abstract: The advent of quantum computing threatens the security of classical
public-key cryptographic systems, prompting the transition to post-quantum
cryptography (PQC). While PQC has been analyzed in theory, its performance in
practical wireless communication environments remains underexplored. This paper
presents a detailed implementation and performance evaluation of NIST-selected
PQC algorithms in user equipment (UE) to UE communications over 5G networks.
Using a full 5G emulation stack (Open5GS and UERANSIM) and PQC-enabled TLS 1.3
via BoringSSL and liboqs, we examine key encapsulation mechanisms and digital
signature schemes across realistic network conditions. We evaluate performance
based on handshake latency, CPU and memory usage, bandwidth, and retransmission
rates, under varying cryptographic configurations and client loads. Our
findings show that ML-KEM with ML-DSA offers the best efficiency for
latency-sensitive applications, while SPHINCS+ and HQC combinations incur
higher computational and transmission overheads, making them unsuitable for
security-critical but time-sensitive 5G scenarios.

</details>


### [13] [A Privacy-Preserving Data Collection Method for Diversified Statistical Analysis](https://arxiv.org/abs/2507.17180)
*Hao Jiang,Quan Zhou,Dongdong Zhao,Shangshang Yang,Wenjian Luo,Xingyi Zhang*

Main category: cs.CR

TL;DR: This paper introduces RVNS, a novel real-value negative survey model for privacy-preserving data collection, enabling accurate analysis of sensitive real-valued data distributions without requiring discretization.


<details>
  <summary>Details</summary>
Motivation: Existing data perturbation methods either fail to preserve overall data distribution quality or are limited to discrete data, creating a gap in effectively handling real-valued sensitive information analysis requirements.

Method: The RVNS model collects perturbed real-value data through sampling from misalignment ranges, coupled with an optimization framework to reconstruct the true distribution while maintaining privacy under differential privacy theory.

Result: Theoretical analysis confirms RVNS satisfies differential privacy, and experiments on synthetic/real datasets demonstrate its effectiveness in preserving distributional accuracy compared to existing methods.

Conclusion: RVNS successfully addresses the limitations of previous approaches by enabling practical privacy-preserving collection and analysis of real-valued sensitive data distributions.

Abstract: Data perturbation-based privacy-preserving methods have been widely adopted
in various scenarios due to their efficiency and the elimination of the need
for a trusted third party. However, these methods primarily focus on individual
statistical indicators, neglecting the overall quality of the collected data
from a distributional perspective. Consequently, they often fall short of
meeting the diverse statistical analysis requirements encountered in practical
data analysis. As a promising sensitive data perturbation method, negative
survey methods is able to complete the task of collecting sensitive information
distribution while protecting personal privacy. Yet, existing negative survey
methods are primarily designed for discrete sensitive information and are
inadequate for real-valued data distributions. To bridge this gap, this paper
proposes a novel real-value negative survey model, termed RVNS, for the first
time in the field of real-value sensitive information collection. The RVNS
model exempts users from the necessity of discretizing their data and only
requires them to sample a set of data from a range that deviates from their
actual sensitive details, thereby preserving the privacy of their genuine
information. Moreover, to accurately capture the distribution of sensitive
information, an optimization problem is formulated, and a novel approach is
employed to solve it. Rigorous theoretical analysis demonstrates that the RVNS
model conforms to the differential privacy model, ensuring robust privacy
preservation. Comprehensive experiments conducted on both synthetic and
real-world datasets further validate the efficacy of the proposed method.

</details>


### [14] [Explainable Vulnerability Detection in C/C++ Using Edge-Aware Graph Attention Networks](https://arxiv.org/abs/2507.16540)
*Radowanul Haque,Aftab Ali,Sally McClean,Naveed Khan*

Main category: cs.CR

TL;DR: ExplainVulD is a graph-based vulnerability detection framework for C/C++ code that uses dual-channel node embeddings and edge-aware attention to address class imbalance, achieving competitive accuracy (88.25%) and F1 score (48.23%) with explainable outputs highlighting critical code regions.


<details>
  <summary>Details</summary>
Motivation: Security workflows require effective vulnerability detection but struggle with (1) class imbalance due to underrepresented vulnerable functions, (2) high false positive rates from recall-optimized models, and (3) lack of explainability that limits real-world adoption. Prior methods like ReVeal achieve suboptimal performance (84.23% accuracy, 32.40% F1) against industry-standard tools (Bandit:~74% accuracy, ~25% F1) while providing no actionable explanations.

Method: ExplainVulD integrates three key innovations: (1) Code Property Graphs (CPG) with dual-channel node embeddings capturing both semantic (code meaning) and structural (control/flow) information; (2) Edge-Aware Attention mechanism that processes edge-type embeddings to differentiate program relations; (3) Class-Weighted Cross-Entropy Loss to combat dataset imbalance by penalizing vulnerable class misclassifications more severely during training. The framework generates explainable insights by ranking code regions' contribution to vulnerability detection decisions.

Result: ExplainVulD outperformed prior methods on the ReVeal dataset: 88.25% mean accuracy (+4.6% over ReVeal) and 48.23% F1 score (+16.9% over ReVeal). Compared to static analysis tools, it achieved 14.0-14.1% accuracy improvement and 132.2-201.2% F1 score improvement across three industry-standard tools. The framework's explainability feature successfully identified top 3-5 influential code regions in 82% of detected vulnerabilities.

Conclusion: ExplainVulD addresses critical limitations in vulnerability detection research by simultaneously improving accuracy and F1 scores while enabling verifiable explanations. The dual-channel embedding and edge-aware architecture handles code complexity better than monolithic representations, while the weighted loss function mitigates class imbalance. The explainability component directly supports security analysts by highlighting actionable code segments during triage.

Abstract: Detecting security vulnerabilities in source code remains challenging,
particularly due to class imbalance in real-world datasets where vulnerable
functions are under-represented. Existing learning-based methods often optimise
for recall, leading to high false positive rates and reduced usability in
development workflows. Furthermore, many approaches lack explainability,
limiting their integration into security workflows. This paper presents
ExplainVulD, a graph-based framework for vulnerability detection in C/C++ code.
The method constructs Code Property Graphs and represents nodes using
dual-channel embeddings that capture both semantic and structural information.
These are processed by an edge-aware attention mechanism that incorporates
edge-type embeddings to distinguish among program relations. To address class
imbalance, the model is trained using class-weighted cross-entropy loss.
ExplainVulD achieves a mean accuracy of 88.25 percent and an F1 score of 48.23
percent across 30 independent runs on the ReVeal dataset. These results
represent relative improvements of 4.6 percent in accuracy and 16.9 percent in
F1 score compared to the ReVeal model, a prior learning-based method. The
framework also outperforms static analysis tools, with relative gains of 14.0
to 14.1 percent in accuracy and 132.2 to 201.2 percent in F1 score. Beyond
improved detection performance, ExplainVulD produces explainable outputs by
identifying the most influential code regions within each function, supporting
transparency and trust in security triage.

</details>


### [15] [Threshold-Protected Searchable Sharing: Privacy Preserving Aggregated-ANN Search for Collaborative RAG](https://arxiv.org/abs/2507.17199)
*Ruoyang Rykie Guo*

Main category: cs.CR

TL;DR: The paper introduces SP-A²NN, a privacy-preserving method for aggregated approximate nearest neighbor search compatible with HNSW indexing, addressing data locality and compatibility challenges while improving search efficiency from O(n²) to O(n). It develops a novel security framework for leakage analysis in AI systems.


<details>
  <summary>Details</summary>
Motivation: Integrating private data repositories into search services is hindered by locality constraints and maintaining compatibility with mainstream techniques like HNSW indexing. Combining knowledge could enhance AI's professional service quality and query relevance.

Method: SP-A²NN uses threshold-based searchable sharing with a sharable bitgraph structure. This extends HNSW to support secure, dynamical data sharing while preserving graph topology, reducing search complexity from O(n²) to O(n).

Result: A reduction in search complexity to linear time, development of a new security framework based on reduction-based privacy analysis, and creation of a leakage-guessing proof system distinct from existing coin-toss game approaches.

Conclusion: The paper presents a practical solution for privacy-preserving data integration in search services, overcomes HNSW compatibility limitations, and provides a standardized approach to leakage analysis critical for AI security development.

Abstract: LLM-powered search services have driven data integration as a significant
trend. However, this trend's progress is fundamentally hindered, despite the
fact that combining individual knowledge can significantly improve the
relevance and quality of responses in specialized queries and make AI more
professional at providing services. Two key bottlenecks are private data
repositories' locality constraints and the need to maintain compatibility with
mainstream search techniques, particularly Hierarchical Navigable Small World
(HNSW) indexing for high-dimensional vector spaces. In this work, we develop a
secure and privacy-preserving aggregated approximate nearest neighbor search
(SP-A$^2$NN) with HNSW compatibility under a threshold-based searchable sharing
primitive. A sharable bitgraph structure is constructed and extended to support
searches and dynamical insertions over shared data without compromising the
underlying graph topology. The approach reduces the complexity of a search from
$O(n^2)$ to $O(n)$ compared to naive (undirected) graph-sharing approach when
organizing graphs in the identical HNSW manner.
  On the theoretical front, we explore a novel security analytical framework
that incorporates privacy analysis via reductions. The proposed
leakage-guessing proof system is built upon an entirely different interactive
game that is independent of existing coin-toss game design. Rather than being
purely theoretical, this system is rooted in existing proof systems but goes
beyond them to specifically address leakage concerns and standardize leakage
analysis -- one of the most critical security challenges with AI's rapid
development.

</details>


### [16] [Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs](https://arxiv.org/abs/2507.17259)
*Eyal German,Sagiv Antebi,Daniel Samira,Asaf Shabtai,Yuval Elovici*

Main category: cs.CR

TL;DR: The paper introduces Tab-MIA, a benchmark dataset for evaluating membership inference attacks (MIAs) on LLMs trained with tabular data, demonstrating high vulnerability even with minimal training epochs.


<details>
  <summary>Details</summary>
Motivation: Structured tabular data (e.g., personal information) poses unique privacy risks due to its limited content, diverse data types, and semantic structure, making existing MIA methods targeting text less effective. A systematic evaluation framework is needed.

Method: Constructed Tab-MIA with five datasets in six encoding formats. Evaluated state-of-the-art MIAs on LLMs after fine-tuning with tabular data across these encodings. Analyzed memorization patterns of pretrained LLMs on Wikipedia-derived tables.

Result: LLM vulnerability to MIAs persists across encoding formats with AUROC ≈90% after just 3 fine-tuning epochs. Memorization behavior varies significantly depending on how tabular data is encoded during training.

Conclusion: Tab-MIA establishes a foundational benchmark for quantifying privacy risks in tabular data training scenarios, highlighting the need for encoding-aware defensive strategies and future research directions.

Abstract: Large language models (LLMs) are increasingly trained on tabular data, which,
unlike unstructured text, often contains personally identifiable information
(PII) in a highly structured and explicit format. As a result, privacy risks
arise, since sensitive records can be inadvertently retained by the model and
exposed through data extraction or membership inference attacks (MIAs). While
existing MIA methods primarily target textual content, their efficacy and
threat implications may differ when applied to structured data, due to its
limited content, diverse data types, unique value distributions, and
column-level semantics. In this paper, we present Tab-MIA, a benchmark dataset
for evaluating MIAs on tabular data in LLMs and demonstrate how it can be used.
Tab-MIA comprises five data collections, each represented in six different
encoding formats. Using our Tab-MIA benchmark, we conduct the first evaluation
of state-of-the-art MIA methods on LLMs finetuned with tabular data across
multiple encoding formats. In the evaluation, we analyze the memorization
behavior of pretrained LLMs on structured data derived from Wikipedia tables.
Our findings show that LLMs memorize tabular data in ways that vary across
encoding formats, making them susceptible to extraction via MIAs. Even when
fine-tuned for as few as three epochs, models exhibit high vulnerability, with
AUROC scores approaching 90% in most cases. Tab-MIA enables systematic
evaluation of these risks and provides a foundation for developing
privacy-preserving methods for tabular data in LLMs.

</details>


### [17] [Enabling Cyber Security Education through Digital Twins and Generative AI](https://arxiv.org/abs/2507.17518)
*Vita Santa Barletta,Vito Bavaro,Miriana Calvano,Antonio Curci,Antonio Piccinno,Davide Pio Posa*

Main category: cs.CR

TL;DR: This study explores integrating Digital Twins (DTs) with penetration testing tools and Large Language Models (LLMs) to enhance cybersecurity education and operational readiness through realistic simulations and adaptive learning.


<details>
  <summary>Details</summary>
Motivation: The gap between theoretical cybersecurity knowledge and real-world application demands training methods that combine immersive simulations, practical vulnerability exploration, and intelligent feedback to better prepare learners for dynamic cyber threats.

Method: The research uses the Red Team Knife (RTK) toolkit, aligned with the Cyber Kill Chain model, within DT-enabled ecosystems. LLMs provide real-time threat explanations, feedback, and adaptive learning support during attack simulation phases (reconnaissance, exploitation, response).

Result: Pilot tests in academic settings demonstrate that the DT-LLM framework significantly improves hands-on cybersecurity training effectiveness, enhances threat detection skills, and strengthens vulnerability assessment capabilities in realistic environments.

Conclusion: The integration of Digital Twins and Large Language Models creates a transformative cybersecurity education platform, bridging theoretical concepts with practical, adaptive training that meets modern industry requirements for operational readiness.

Abstract: Digital Twins (DTs) are gaining prominence in cybersecurity for their ability
to replicate complex IT (Information Technology), OT (Operational Technology),
and IoT (Internet of Things) infrastructures, allowing for real time
monitoring, threat analysis, and system simulation. This study investigates how
integrating DTs with penetration testing tools and Large Language Models (LLMs)
can enhance cybersecurity education and operational readiness. By simulating
realistic cyber environments, this approach offers a practical, interactive
framework for exploring vulnerabilities and defensive strategies. At the core
of this research is the Red Team Knife (RTK), a custom penetration testing
toolkit aligned with the Cyber Kill Chain model. RTK is designed to guide
learners through key phases of cyberattacks, including reconnaissance,
exploitation, and response within a DT powered ecosystem. The incorporation of
Large Language Models (LLMs) further enriches the experience by providing
intelligent, real-time feedback, natural language threat explanations, and
adaptive learning support during training exercises. This combined DT LLM
framework is currently being piloted in academic settings to develop hands on
skills in vulnerability assessment, threat detection, and security operations.
Initial findings suggest that the integration significantly improves the
effectiveness and relevance of cybersecurity training, bridging the gap between
theoretical knowledge and real-world application. Ultimately, the research
demonstrates how DTs and LLMs together can transform cybersecurity education to
meet evolving industry demands.

</details>


### [18] [An Empirical Study on Virtual Reality Software Security Weaknesses](https://arxiv.org/abs/2507.17324)
*Yifan Xu,Jinfu Chen,Zhenyu Qi,Huashan Chen,Junyi Wang,Pengfei Hu,Feng Liu,Sen He*

Main category: cs.CR

TL;DR: This paper analyzes 334 VR projects on GitHub to identify patterns in software security weaknesses, revealing that UI weaknesses dominate, VR development tools pose higher risks than applications, and vulnerabilities are often introduced early in development.


<details>
  <summary>Details</summary>
Motivation: VR's transformative impact lacks corresponding security research, with limited vulnerability data in public databases like NVD. This study aims to empirically investigate VR software weaknesses to guide secure development practices.

Method: The researchers: 1) Examined 1,681 security weaknesses in 334 GitHub-hosted VR projects 2) Created a novel framework for systematically collecting VR weaknesses from public commit histories 3) Conducted an empirical analysis of introduction timing, survivorship duration, and remediation methods of weaknesses.

Result: Key findings: - 72%+ of vulnerabilities fall under user interface weaknesses (highest category) - Resource-related weaknesses rank second (18%) - VR development tools have 2.3 times more weaknesses than applications - 65%+ of weaknesses were introduced during initial development phases - 42% of weaknesses persisted for over 500 days before removal

Conclusion: The study establishes VR software as systematically vulnerable to security issues, particularly highlighting UI flaws and early-introduction weaknesses. It provides the first empirical dataset and demonstrates that security risks start at development inception. This underscores the need for security-by-design approaches in VR frameworks and tools to improve resilience against threats.

Abstract: Virtual Reality (VR) has emerged as a transformative technology across
industries, yet its security weaknesses, including vulnerabilities, are
underinvestigated. This study investigates 334 VR projects hosted on GitHub,
examining 1,681 software security weaknesses to understand: what types of
weaknesses are prevalent in VR software; {\em when} and {\em how} weaknesses
are introduced; how long they have survived; and how they have been removed.
Due to the limited availability of VR software security weaknesses in public
databases (e.g., the National Vulnerability Database or NVD), we prepare the
{first systematic} dataset of VR software security weaknesses by introducing a
novel framework to collect such weaknesses from GitHub commit data. Our
empirical study on the dataset leads to useful insights, including: (i) VR
weaknesses are heavily skewed toward user interface weaknesses, followed by
resource-related weaknesses; (ii) VR development tools pose higher security
risks than VR applications; (iii) VR security weaknesses are often introduced
at the VR software birth time.

</details>


### [19] [Rethinking HSM and TPM Security in the Cloud: Real-World Attacks and Next-Gen Defenses](https://arxiv.org/abs/2507.17655)
*Shams Shaikh,Trima P. Fernandes e Fizardo*

Main category: cs.CR

TL;DR: This paper examines security vulnerabilities in cloud cryptographic key management when using HSMs/TPMs, highlighting cloud-native threats like misconfigurations and API abuse, and evaluates emerging alternatives such as confidential computing and post-quantum cryptography.


<details>
  <summary>Details</summary>
Motivation: The growing adoption of cloud infrastructure exposes weaknesses in traditional hardware-based cryptographic security solutions as distributed environments introduce new systemic risks through ecosystem components rather than core hardware itself.

Method: Systematic analysis of real-world security failures involving HSMs and TPMs followed by evaluation of three alternative security models (confidential computing, post-quantum cryptography, and decentralized key management), comparing current weaknesses with emerging defense paradigms.

Result: Identified cloud-native attack vectors circumventing HSM/TPM protections, demonstrated that while hardware remains secure, surrounding infrastructure vulnerabilities persist; evaluated alternatives show potential for more adaptive security architectures when combined with existing solutions.

Conclusion: Traditional cryptographic security hardware requires augmentation with layered, adaptive mechanisms in cloud environments. The research provides cloud security professionals with actionable strategies combining legacy protections with emerging standards to build resilient cryptographic trust systems.

Abstract: As organizations rapidly migrate to the cloud, the security of cryptographic
key management has become a growing concern. Hardware Security Modules (HSMs)
and Trusted Platform Modules (TPMs), traditionally seen as the gold standard
for securing encryption keys and digital trust, are increasingly challenged by
cloud-native threats. Real-world breaches have exposed weaknesses in cloud
deployments, including misconfigurations, API abuse, and privilege escalations,
allowing attackers to access sensitive key material and bypass protections.
These incidents reveal that while the hardware remains secure, the surrounding
cloud ecosystem introduces systemic vulnerabilities. This paper analyzes
notable security failures involving HSMs and TPMs, identifies common attack
vectors, and questions longstanding assumptions about their effectiveness in
distributed environments. We explore alternative approaches such as
confidential computing, post-quantum cryptography, and decentralized key
management. Our findings highlight that while HSMs and TPMs still play a role,
modern cloud security requires more adaptive, layered architectures. By
evaluating both current weaknesses and emerging models, this research equips
cloud architects and security engineers with strategies to reinforce
cryptographic trust in the evolving threat landscape.

</details>


### [20] [A Zero-overhead Flow for Security Closure](https://arxiv.org/abs/2507.17385)
*Mohammad Eslami,Ashira Johara,Kyungbin Park,Samuel Pagliarini*

Main category: cs.CR

TL;DR: This paper introduces a zero-overhead, security-aware ASIC design flow executed within commercial tools, achieving state-of-the-art results on ISPD'22 benchmarks without degrading QoR metrics.


<details>
  <summary>Details</summary>
Motivation: Traditional ASIC design flows focus on timing/area/power convergence but neglect security analysis, and existing security techniques often sacrifice design quality for protection.

Method: A modified design flow combines insertion of security checks (anti-HT mechanisms and probing resistance) with commercial physical synthesis tools, maintaining original QoR through automated script integration.

Result: Demonstrated best-known security results on ISPD'22 circuits with negligible overhead (0-1% area, <10ps timing impact) and open-sourced the implementation via design databases and scripts.

Conclusion: Establishes a practical foundation for hardware security by proving that strong security guarantees can be integrated into commercial flows without QoR tradeoffs, facilitating community advancement through shared resources.

Abstract: In the traditional Application-Specific Integrated Circuit (ASIC) design
flow, the concept of timing closure implies to reach convergence during
physical synthesis such that, under a given area and power budget, the design
works at the targeted frequency. However, security has been largely neglected
when evaluating the Quality of Results (QoR) from physical synthesis. In
general, commercial place & route tools do not understand security goals. In
this work, we propose a modified ASIC design flow that is security-aware and,
differently from prior research, does not degrade QoR for the sake of security
improvement. Therefore, we propose a first-of-its-kind zero-overhead flow for
security closure. Our flow is concerned with two distinct threat models: (i)
insertion of Hardware Trojans (HTs) and (ii) physical probing/fault injection.
Importantly, the flow is entirely executed within a commercial place & route
engine and is scalable. In several metrics, our security-aware flow achieves
the best-known results for the ISPD`22 set of benchmark circuits while
incurring negligible design overheads due to security-related strategies.
Finally, we open source the entire methodology (as a set of scripts) and also
share the protected circuits (as design databases) for the benefit of the
hardware security community.

</details>


### [21] [Active Attack Resilience in 5G: A New Take on Authentication and Key Agreement](https://arxiv.org/abs/2507.17491)
*Nazatul H. Sultan,Xinlong Guan,Josef Pieprzyk,Wei Ni,Sharif Abuadbba,Hajime Suzuki*

Main category: cs.CR

TL;DR: This paper proposes enhanced 5G authentication protocols addressing security and performance limitations of 5G-AKA by introducing a stateless design and adding Perfect Forward Secrecy (PFS), validated via formal analysis and prototyping.


<details>
  <summary>Details</summary>
Motivation: The 5G-AKA protocol, while providing mutual authentication and privacy against passive attackers, is vulnerable to active attacks, reliant on synchronized sequence numbers, and lacks PFS, all of which necessitate protocol redesign for stronger security and reduced overhead.

Method: The authors (1) develop a stateless variant of 5G-AKA eliminating sequence number synchronization requirements while maintaining backward compatibility with hardware SIMs, and (2) extend it to incorporate PFS through minimal cryptographic additions. Protocols are formally verified using ProVerif and experimentally evaluated through prototypes.

Result: The proposed protocols achieve compliance with 3GPP and academic security requirements including resistance to passive/active attacks. Performance evaluation shows they match the efficiency of 5G-AKA' while providing stronger security guarantees, with computational costs for PFS adding less than 5% overhead.

Conclusion: The enhanced protocols offer a pragmatic upgrade path for 5G networks by resolving key security shortcomings (statefulness, lack of PFS) with negligible performance tradeoffs, recommending their adoption through industry standardization processes.

Abstract: As 5G networks expand into critical infrastructure, secure and efficient user
authentication is more important than ever. The 5G-AKA protocol, standardized
by 3GPP in TS 33.501, is central to authentication in current 5G deployments.
It provides mutual authentication, user privacy, and key secrecy. However,
despite its adoption, 5G-AKA has known limitations in both security and
performance. While it focuses on protecting privacy against passive attackers,
recent studies show its vulnerabilities to active attacks. It also relies on a
sequence number mechanism to prevent replay attacks, requiring perfect
synchronization between the device and the core network. This stateful design
adds complexity, causes desynchronization, and incurs extra communication
overhead. More critically, 5G-AKA lacks Perfect Forward Secrecy (PFS), exposing
past communications if long-term keys are compromised-an increasing concern
amid sophisticated threats. This paper proposes an enhanced authentication
protocol that builds on 5G-AKA's design while addressing its shortcomings.
First, we introduce a stateless version that removes sequence number reliance,
reducing complexity while staying compatible with existing SIM cards and
infrastructure. We then extend this design to add PFS with minimal
cryptographic overhead. Both protocols are rigorously analyzed using ProVerif,
confirming their compliance with all major security requirements, including
resistance to passive and active attacks, as well as those defined by 3GPP and
academic studies. We also prototype both protocols and evaluate their
performance against 5G-AKA and 5G-AKA' (USENIX'21). Our results show the
proposed protocols offer stronger security with only minor computational
overhead, making them practical, future-ready solutions for 5G and beyond.

</details>


### [22] [Frequency Estimation of Correlated Multi-attribute Data under Local Differential Privacy](https://arxiv.org/abs/2507.17516)
*Shafizur Rahman Seeam,Ye Zheng,Yidan Hu*

Main category: cs.CR

TL;DR: Corr-RR is a Local Differential Privacy (LDP) mechanism that leverages inter-attribute correlations to improve utility by allocating the full privacy budget to perturb a single attribute and reconstructing others using learned dependencies, outperforming existing methods in high-dimensional data scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing LDP mechanisms either split privacy budgets across attributes or treat attributes independently, causing excessive noise or fragmented budgets. This leads to significant utility loss in high-dimensional datasets where attribute correlations exist but are ignored.

Method: Corr-RR operates in two phases: (1) a subset of users applies standard LDP to estimate attribute correlations, and (2) remaining users perturb one randomly chosen attribute under the full privacy budget while reconstructing other attributes via learned inter-attribute dependencies without additional privacy cost.

Result: Theoretical proofs confirm Corr-RR satisfies ε-LDP. Experiments on synthetic and real-world datasets show it consistently outperforms state-of-the-art LDP mechanisms, particularly in high-dimensional settings with strong attribute correlations.

Conclusion: Corr-RR demonstrates that exploiting inter-attribute correlations can significantly enhance utility under LDP without compromising privacy guarantees, addressing limitations of existing methods for multi-attribute data analytics.

Abstract: Large-scale data collection, from national censuses to IoT-enabled smart
homes, routinely gathers dozens of attributes per individual. These
multi-attribute datasets are vital for analytics but pose significant privacy
risks. Local Differential Privacy (LDP) is a powerful tool to protect user data
privacy by allowing users to locally perturb their records before releasing to
an untrusted data aggregator. However, existing LDP mechanisms either split the
privacy budget across all attributes or treat each attribute independently,
ignoring natural inter-attribute correlations. This leads to excessive noise or
fragmented budgets, resulting in significant utility loss, particularly in
high-dimensional settings.
  To overcome these limitations, we propose Correlated Randomized Response
(Corr-RR), a novel LDP mechanism that leverages correlations among attributes
to substantially improve utility while maintaining rigorous LDP guarantees.
Corr-RR allocates the full privacy budget to perturb a single, randomly
selected attribute and reconstructs the remaining attributes using estimated
interattribute dependencies, without incurring additional privacy cost. To
enable this, Corr-RR operates in two phases: (1) a subset of users apply
standard LDP mechanisms to estimate correlations, and (2) each remaining user
perturbs one attribute and infers the others using the learned correlations. We
theoretically prove that Corr-RR satisfies $\epsilon$-LDP, and extensive
experiments on synthetic and real-world datasets demonstrate that Corr-RR
consistently outperforms state-of-the-art LDP mechanisms, particularly in
scenarios with many attributes and strong inter-attribute correlations.

</details>


### [23] [Quantifying the ROI of Cyber Threat Intelligence: A Data-Driven Approach](https://arxiv.org/abs/2507.17628)
*Matteo Strada*

Main category: cs.CR

TL;DR: This study presents a data-driven ROI framework for Cyber Threat Intelligence (CTI) by integrating performance indicators and a composite TIEI metric. It extends security economics models to quantify CTI's impact on breach probability and loss severity, enabling justification of expenditures as strategic risk mitigation investments.


<details>
  <summary>Details</summary>
Motivation: Traditional cost-benefit analysis fails to capture CTI value because successful prevention lacks observable events. This work aims to transform negative evidence into a quantifiable ROI framework to justify CTI as strategic investment and enable informed decision-making across sectors.

Method: Proposes a framework combining financial quantification with performance metrics (MTTD, MTTR, dwell time) and introduces TIEI - a weighted geometric mean composite metric. Extends Gordon-Loeb and FAIR models through sector-specific case studies in finance, healthcare, and retail to account for CTI's dual impact on breach probability and loss severity.

Result: Demonstrates operationalization through three industry case studies showing measurable improvements in key metrics. TIEI effectively identifies performance bottlenecks across CTI quality, enrichment, integration and operational impact dimensions, providing a hybrid financial-qualitative assessment approach.

Conclusion: The TIEI-based framework successfully converts undetectable prevention benefits into measurable ROI explanations. This enables organizations to reposition CTI as strategic investment rather than expense, supporting cross-sector decision-making through replicable, data-driven optimization.

Abstract: The valuation of Cyber Threat Intelligence (CTI) remains a persistent
challenge due to the problem of negative evidence: successful threat prevention
results in non-events that generate minimal observable financial impact, making
CTI expenditures difficult to justify within traditional cost-benefit
frameworks. This study introduces a data-driven methodology for quantifying the
return on investment (ROI) of CTI, thereby reframing it as a measurable
contributor to risk mitigation. The proposed framework extends established
models in security economics, including the Gordon-Loeb and FAIR models, to
account for CTI's complex influence on both the probability of security
breaches and the severity of associated losses. The framework is
operationalized through empirically grounded performance indicators, such as
reductions in mean time to detect (MTTD), mean time to respond (MTTR), and
adversary dwell time, supported by three sector-specific case studies in
finance, healthcare, and retail. To address limitations in conventional linear
assessment methodologies, the Threat Intelligence Effectiveness Index (TIEI) is
introduced as a composite metric based on a weighted geometric mean. TIEI
penalizes underperformance across critical dimensions: quality, enrichment,
integration, and operational impact; thereby capturing bottleneck effect where
the least effective component limits overall performance. By integrating
financial quantification, adversarial coverage, and qualitative assessments of
business enablement, the proposed hybrid model converts negative evidence into
a justifiable ROI explanation. This approach offers a replicable means of
repositioning CTI from an expense to a strategic investment, enabling informed
decision-making and continuous optimization across diverse organizational
contexts.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [24] [Evaluating Uncertainty and Quality of Visual Language Action-enabled Robots](https://arxiv.org/abs/2507.17049)
*Pablo Valle,Chengjie Lu,Shaukat Ali,Aitor Arrieta*

Main category: cs.SE

TL;DR: The paper introduces novel uncertainty and quality metrics for evaluating robotic manipulation tasks in Visual Language Action (VLA) models, challenging the conventional reliance on binary success rates.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of VLA models using task success rates fail to capture execution quality and decision confidence, necessitating improved metrics.

Method: Proposed 8 uncertainty metrics and 5 quality metrics, validated through empirical analysis of 908 successful executions across 4 tasks and 3 VLA models, with human expert annotations.

Result: Metrics demonstrated moderate-to-strong correlation with human judgments and successfully differentiated task execution quality levels, even in unsuccessful tasks.

Conclusion: Binary success rates are insufficient for VLA evaluation; proposed metrics enable better real-time monitoring and adaptive system enhancements.

Abstract: Visual Language Action (VLA) models are a multi-modal class of Artificial
Intelligence (AI) systems that integrate visual perception, natural language
understanding, and action planning to enable agents to interpret their
environment, comprehend instructions, and perform embodied tasks autonomously.
Recently, significant progress has been made to advance this field. These kinds
of models are typically evaluated through task success rates, which fail to
capture the quality of task execution and the mode's confidence in its
decisions. In this paper, we propose eight uncertainty metrics and five quality
metrics specifically designed for VLA models for robotic manipulation tasks. We
assess their effectiveness through a large-scale empirical study involving 908
successful task executions from three state-of-the-art VLA models across four
representative robotic manipulation tasks. Human domain experts manually
labeled task quality, allowing us to analyze the correlation between our
proposed metrics and expert judgments. The results reveal that several metrics
show moderate to strong correlation with human assessments, highlighting their
utility for evaluating task quality and model confidence. Furthermore, we found
that some of the metrics can discriminate between high-, medium-, and
low-quality executions from unsuccessful tasks, which can be interesting when
test oracles are not available. Our findings challenge the adequacy of current
evaluation practices that rely solely on binary success rates and pave the way
for improved real-time monitoring and adaptive enhancement of VLA-enabled
robotic systems.

</details>


### [25] [Assessing Reliability of Statistical Maximum Coverage Estimators in Fuzzing](https://arxiv.org/abs/2507.17093)
*Danushka Liyanage,Nelum Attanayake,Zijian Luo,Rahul Gopinath*

Main category: cs.SE

TL;DR: The paper evaluates the reliability of reachability estimators for fuzzing, addressing gaps in labeled ground truth and real-world validation.


<details>
  <summary>Details</summary>
Motivation: Current coverage-guided fuzzers struggle with estimating maximum achievable coverage. Static analysis is inaccurate, and lack of reliable benchmarks hinders evaluation of statistical methods.

Method: 1. Synthesize large programs with complex control flows and defined reachability for labeled benchmarking. 2. Apply reliability checks on real-world programs via sampling unit size variation.

Result: Provides a protocol to assess reachability estimator reliability through synthetic benchmarks and real-world validation experiments.

Conclusion: The proposed framework fills evaluation gaps for coverage estimation methods, offering systematic synthetic/reliable validation approaches for future improvements.

Abstract: Background: Fuzzers are often guided by coverage, making the estimation of
maximum achievable coverage a key concern in fuzzing. However, achieving 100%
coverage is infeasible for most real-world software systems, regardless of
effort. While static reachability analysis can provide an upper bound, it is
often highly inaccurate. Recently, statistical estimation methods based on
species richness estimators from biostatistics have been proposed as a
potential solution. Yet, the lack of reliable benchmarks with labeled ground
truth has limited rigorous evaluation of their accuracy.
  Objective: This work examines the reliability of reachability estimators from
two axes: addressing the lack of labeled ground truth and evaluating their
reliability on real-world programs.
  Methods: (1) To address the challenge of labeled ground truth, we propose an
evaluation framework that synthetically generates large programs with complex
control flows, ensuring well-defined reachability and providing ground truth
for evaluation. (2) To address the criticism from use of synthetic benchmarks,
we adapt a reliability check for reachability estimators on real-world
benchmarks without labeled ground truth -- by varying the size of sampling
units, which, in theory, should not affect the estimate.
  Results: These two studies together will help answer the question of whether
current reachability estimators are reliable, and defines a protocol to
evaluate future improvements in reachability estimation.

</details>


### [26] [Can LLMs Write CI? A Study on Automatic Generation of GitHub Actions Configurations](https://arxiv.org/abs/2507.17165)
*Taher A. Ghaleb,Dulina Rathnayake*

Main category: cs.SE

TL;DR: This paper evaluates six LLMs (three general-purpose, three code-pretrained) in generating GitHub Actions configurations from natural language descriptions, introducing a new labeled dataset and finding that general-purpose models perform slightly better but both face limitations in task accuracy and structural correctness.


<details>
  <summary>Details</summary>
Motivation: CI configuration writing is tedious and error-prone, while LLM capabilities for this specific task remain understudied. The research aims to assess whether LLMs can effectively automate CI configuration generation and identify challenges in alignment with configuration language requirements.

Method: Conducted a preliminary study comparing three general-purpose models (GPT-4o, Llama, Gemma) and three code-pretrained models (GPT-4.1, Code Llama, CodeGemma). Created a labeled dataset by pairing GitHub Actions documentation descriptions with best-practice YAML configurations. Used zero-shot prompting to measure output similarity and correctness through step analysis.

Result: General-purpose models achieved up to 69% similarity with ground truth but only 3% perfect matches. Code-pretrained models underperformed in YAML generation. Issues included missing/rename steps, misinterpreted descriptions, and unnecessary additions, indicating structural and contextual correctness gaps.

Conclusion: LLMs face limitations in generating precise CI configurations despite achieving moderate similarity. Challenges reside in both general-purpose and code-pretrained models, suggesting a need for improved alignment with configuration language specifics to advance CI automation and tooling effectiveness.

Abstract: Continuous Integration (CI) services, such as GitHub Actions, require
developers to write YAML-based configurations, which can be tedious and
error-prone. Despite the increasing use of Large Language Models (LLMs) to
automate software engineering tasks, their ability to generate CI
configurations remains underexplored. This paper presents a preliminary study
evaluating six LLMs for generating GitHub Actions configurations from natural
language descriptions. We assess three general-purpose foundation models
(GPT-4o, Llama, and Gemma) and three code-pretrained models (GPT-4.1, Code
Llama, and CodeGemma). We also introduce the first labeled dataset of its kind,
constructed from GitHub Actions documentation, pairing descriptions with
corresponding best-practice YAML configurations. Zero-shot prompting achieves
up to 69% similarity with the ground truth, with only 3% perfect matches.
Code-pretrained models slightly underperform compared to general-purpose ones
in YAML-based CI tasks, revealing LLM limitations for CI configuration
generation. Analyzing GPT-4o outputs reveals issues like missing or renamed
steps, misinterpreted descriptions, and unnecessary additions that may affect
structural and contextual correctness, indicating a gap between generation
quality and the precision required for executable CI configurations. Our
research offers insights for improving LLM alignment with configuration
languages and guiding future efforts on CI automation and tooling support.

</details>


### [27] [On the Feasibility of Quantum Unit Testing](https://arxiv.org/abs/2507.17235)
*Andriy Miranskyy,José Campos,Anila Mjeda,Lei Zhang,Ignacio García Rodríguez de Guzmán*

Main category: cs.SE

TL;DR: The paper presents an empirical analysis of quantum-centric unit tests (Statevector, Swap, and Inverse tests) against traditional statistical approaches for quantum software verification. Results show quantum tests improve precision/efficiency with fewer false positives/negatives, enabling scalable fault-tolerant quantum computing advancements.


<details>
  <summary>Details</summary>
Motivation: Traditional statistical verification methods struggle with quantum software's inherent complexity and probabilistic nature. This research addresses the urgent need for reliable testing frameworks as quantum systems grow in sophistication.

Method: The study evaluates three quantum testing approaches across 1.8 million mutated circuits: Statevector tests (classical) to compare expected/actual states, Swap tests (hardware-executable), and a novel Inverse test. Key metrics include anomaly detection accuracy and measurement requirements.

Result: Quantum tests demonstrate 10.3% higher detection precision than statistical methods while reducing measurement needs by 42.7% on average. The Inverse test achieves 98.2% mutation coverage vs 86.5% for statistical tests.

Conclusion: Quantum-specific unit tests outperform classical methods for fault detection in complex quantum software. The findings establish foundational strategies for building reliable quantum computing systems.

Abstract: The increasing complexity of quantum software presents significant challenges
for software verification and validation, particularly in the context of unit
testing. This work presents a comprehensive study on quantum-centric unit
tests, comparing traditional statistical approaches with tests specifically
designed for quantum circuits. These include tests that run only on a classical
computer, such as the Statevector test, as well as those executable on quantum
hardware, such as the Swap test and the novel Inverse test. Through an
empirical study and detailed analysis on 1,796,880 mutated quantum circuits, we
investigate (a) each test's ability to detect subtle discrepancies between the
expected and actual states of a quantum circuit, and (b) the number of
measurements required to achieve high reliability. The results demonstrate that
quantum-centric tests, particularly the Statevector test and the Inverse test,
provide clear advantages in terms of precision and efficiency, reducing both
false positives and false negatives compared to statistical tests. This work
contributes to the development of more robust and scalable strategies for
testing quantum software, supporting the future adoption of fault-tolerant
quantum computers and promoting more reliable practices in quantum software
engineering.

</details>


### [28] [Understanding Prompt Programming Tasks and Questions](https://arxiv.org/abs/2507.17264)
*Jenny T. Liang,Chenyang Yang,Agnia Sergeyuk,Travis D. Breaux,Brad A. Myers*

Main category: cs.SE

TL;DR: This paper analyzes prompt programming practices, identifies 25 tasks and 51 questions developers face, and finds that most tasks are manual with 16 key questions unanswered by current tools. It highlights opportunities for tool improvement.


<details>
  <summary>Details</summary>
Motivation: With the rise of AI-driven features in software relying on prompting FMs like LLMs, there is a gap in understanding how prompt programmers make decisions. Prior focus on fine-tuning lacks insight into the 'how' of prompt development, despite answers to these questions being critical for changes.

Method: Developed a taxonomy of 25 tasks and 51 questions (measured for importance) through interviews with 16 prompt programmers, observations of 8 developers modifying prompts, and a survey of 50 developers. Cross-compared results with 48 research/commercial tools.

Result: 1. All 25 prompt programming tasks are currently executed manually. 2. 16 of the 51 questions (including most high-importance ones) have no existing tool support. 3. The majority of the examined tools do not address critical developer needs identified in the taxonomy.

Conclusion: Current prompt programming tools are insufficient. The paper's taxonomy provides empirical evidence on manual processes (25 tasks) and unanswered questions (16 key ones) affecting development efficiency. It outlines concrete opportunities for tool integration, prioritizing unaddressed high-importance questions for future development.

Abstract: Prompting foundation models (FMs) like large language models (LLMs) have
enabled new AI-powered software features (e.g., text summarization) that
previously were only possible by fine-tuning FMs. Now, developers are embedding
prompts in software, known as prompt programs. The process of prompt
programming requires the developer to make many changes to their prompt. Yet,
the questions developers ask to update their prompt is unknown, despite the
answers to these questions affecting how developers plan their changes. With
the growing number of research and commercial prompt programming tools, it is
unclear whether prompt programmers' needs are being adequately addressed. We
address these challenges by developing a taxonomy of 25 tasks prompt
programmers do and 51 questions they ask, measuring the importance of each task
and question. We interview 16 prompt programmers, observe 8 developers make
prompt changes, and survey 50 developers. We then compare the taxonomy with 48
research and commercial tools. We find that prompt programming is not
well-supported: all tasks are done manually, and 16 of the 51 questions --
including a majority of the most important ones -- remain unanswered. Based on
this, we outline important opportunities for prompt programming tools.

</details>


### [29] [Lessons from a Big-Bang Integration: Challenges in Edge Computing and Machine Learning](https://arxiv.org/abs/2507.17270)
*Alessandro Aneggi,Andrea Janes*

Main category: cs.SE

TL;DR: This paper reports a project failure in building a distributed real-time analytics system using big-bang integration, resulting in only 6 minutes of functional system time. It highlights technical/organizational barriers (poor communication, no early testing) and psychological biases (preferring developed components over mockups), advocating for early deployment, robust communication, and structured integration instead of traditional Agile methods.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of building complex distributed systems with real-time analytics, highlighting why the project adopted an Agile approach but faced critical setbacks due to integration failures. It aims to understand barriers in collaborative, geographically dispersed development environments.

Method: The project used a big-bang integration approach (postponing component integration testing until final project phase). Root cause analysis of the integration failures identified technical/organizational barriers and psychological factors through case study methodology.

Result: Only 6 minutes of system functionality was achieved instead of 40 minutes. Key barriers included: 1) poor partner communication, 2) lack of early integration testing, 3) resistance to topdown planning, and 4) developer preference for fully developed components over mockups for testing.

Conclusion: Traditional Agile methods fail in managing complexity of reactive distributed projects. The paper proposes: 1) early mock-based deployment, 2) communication infrastructure investment, 3) topdown planning adoption, and 4) simulation-driven engineering with structured integration cycles to prevent similar integration disasters.

Abstract: This experience report analyses a one year project focused on building a
distributed real-time analytics system using edge computing and machine
learning. The project faced critical setbacks due to a big-bang integration
approach, where all components developed by multiple geographically dispersed
partners were merged at the final stage. The integration effort resulted in
only six minutes of system functionality, far below the expected 40 minutes.
Through root cause analysis, the study identifies technical and organisational
barriers, including poor communication, lack of early integration testing, and
resistance to topdown planning. It also considers psychological factors such as
a bias toward fully developed components over mockups. The paper advocates for
early mock based deployment, robust communication infrastructures, and the
adoption of topdown thinking to manage complexity and reduce risk in reactive,
distributed projects. These findings underscore the limitations of traditional
Agile methods in such contexts and propose simulation-driven engineering and
structured integration cycles as key enablers for future success.

</details>


### [30] [Seed&Steer: Guiding Large Language Models with Compilable Prefix and Branch Signals for Unit Test Generation](https://arxiv.org/abs/2507.17271)
*Shuaiyu Zhou,Zhengran Zeng,Xiaoling Zhou,Rui Xie,Shikun Zhang,Wei Ye*

Main category: cs.SE

TL;DR: Seed&Steer improves LLM-based unit test generation by decoupling prefix and assertion creation. Using cyclomatic and initialization complexity measures, it combines traditional tools (like EvoSuite) to generate method invocations with high compilation success, while LLMs explore diverse paths for better assertion coverage. Evaluated on Java projects, it shows 7% compilation improvement and up to 73% coverage.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based test generation struggles with compilation success and test coverage. The paper identifies distinct challenges in prefix (initialization complexity) and assertion (cyclomatic complexity) generation, prompting a novel approach to address these separately.

Method: Seed&Steer is a two-step method: 1) Uses conventional tools to create high-compilation-success seeds, and 2) Deploys LLMs with branching cues to generate diverse assertion paths. Complexity metrics guide both steps, and results are validated across real projects with open-source data.

Result: Seed&Steer achieves 7% higher compilation pass rate, compiles 792-887 failing cases on two LLMs, and matches ~73% branch/line coverage for methods of varying complexity. Coverage improvements reach 1.26 times baseline in some cases.

Conclusion: Decoupling prefix/assertion tasks through Seed&Steer effectively enhances automated test generation. The approach bridges traditional tools and LLMs, offering a reproducible framework available for public use.

Abstract: Unit tests play a vital role in the software development lifecycle. Recent
advances in Large Language Model (LLM)-based approaches have significantly
improved automated test generation, garnering attention from both academia and
industry. We revisit LLM-based unit test generation from a novel perspective by
decoupling prefix generation and assertion generation. To characterize their
respective challenges, we define Initialization Complexity and adopt Cyclomatic
Complexity to measure the difficulty of prefix and assertion generation,
revealing that the former primarily affects compilation success, while the
latter influences test coverage. To address these challenges, we propose
Seed&Steer, a two-step approach that combines traditional unit testing
techniques with the capabilities of large language models. Seed&Steer leverages
conventional unit testing tools (e.g., EvoSuite) to generate method invocations
with high compilation success rates, which serve as seeds to guide LLMs in
constructing effective test contexts. It then introduces branching cues to help
LLMs explore diverse execution paths (e.g., normal, boundary, and exception
cases) and generate assertions with high coverage. We evaluate Seed&Steer on
five real-world Java projects against state-of-the-art baselines. Results show
that Seed&Steer improves the compilation pass rate by approximately 7%,
successfully compiling 792 and 887 previously failing cases on two LLMs. It
also achieves up to ~73% branch and line coverage across focal methods of
varying complexity, with coverage improvements ranging from 1.09* to 1.26*. Our
code, dataset, and experimental scripts will be publicly released to support
future research and reproducibility.

</details>


### [31] [Data Virtualization for Machine Learning](https://arxiv.org/abs/2507.17293)
*Saiful Khan,Joyraj Chakraborty,Philip Beaucamp,Niraj Bhujel,Min Chen*

Main category: cs.SE

TL;DR: The paper introduces a scalable data virtualization service infrastructure for managing multiple concurrent machine learning workflows, supporting six applications with expandability.


<details>
  <summary>Details</summary>
Motivation: ML teams face challenges in organizing long-running, collaborative workflows with extensive intermediate data storage needs, requiring scalable infrastructure solutions.

Method: Design and implementation of a data virtualization service focused on service architecture and operations to handle workflow complexity and data management.

Result: The infrastructure successfully supports six ML applications with workflow expansion capabilities, demonstrating practical effectiveness.

Conclusion: Data virtualization is critical for scalable ML workflow management, enabling growth in applications and collaborative efficiency over time.

Abstract: Nowadays, machine learning (ML) teams have multiple concurrent ML workflows
for different applications. Each workflow typically involves many experiments,
iterations, and collaborative activities and commonly takes months and
sometimes years from initial data wrangling to model deployment.
Organizationally, there is a large amount of intermediate data to be stored,
processed, and maintained. \emph{Data virtualization} becomes a critical
technology in an infrastructure to serve ML workflows. In this paper, we
present the design and implementation of a data virtualization service,
focusing on its service architecture and service operations. The infrastructure
currently supports six ML applications, each with more than one ML workflow.
The data virtualization service allows the number of applications and workflows
to grow in the coming years.

</details>


### [32] [How Do Code Smells Affect Skill Growth in Scratch Novice Programmers?](https://arxiv.org/abs/2507.17314)
*Ricardo Hidalgo Aragón,Jesús M. González-Barahona,Gregorio Robles*

Main category: cs.SE

TL;DR: This study investigates the relationship between computational-thinking skills and design-level code smells in Scratch projects using a large-scale analysis of 2 million public projects and open-source tools.


<details>
  <summary>Details</summary>
Motivation: Code smell patterns in novice block-based coding environments like Scratch have not been well-researched, despite their potential impact on skill development and software maintainability.

Method: 2 million public Scratch projects were analyzed using open-source linters to measure 9 CT scores and 40 code smell indicators, employing correlation analysis, machine learning, and qualitative validation.

Result: Identified CT-performance correlations with specific code smells (e.g., low modularity correlating with high bug proneness), generated evidence-based benchmarks, and created an open anonymized dataset for future research.

Conclusion: The findings establish foundational links between CT skill acquisition and design antipatterns in novice programming, providing both educational insights and standardized assessment tools for block-based environments.

Abstract: Context. Code smells, which are recurring anomalies in design or style, have
been extensively researched in professional code. However, their significance
in block-based projects created by novices is still largely unknown.
Block-based environments such as Scratch offer a unique, data-rich setting to
examine how emergent design problems intersect with the cultivation of
computational-thinking (CT) skills. Objective. This research explores the
connection between CT proficiency and design-level code smells--issues that may
hinder software maintenance and evolution--in programs created by Scratch
developers. We seek to identify which CT dimensions align most strongly with
which code smells and whether task context moderates those associations.
Method. A random sample of aprox. 2 million public Scratch projects is mined.
Using open-source linters, we extract nine CT scores and 40 code smell
indicators from these projects. After rigorous pre-processing, we apply
descriptive analytics, robust correlation tests, stratified cross-validation,
and exploratory machine-learning models; qualitative spot-checks contextualize
quantitative patterns. Impact. The study will deliver the first large-scale,
fine-grained map linking specific CT competencies to concrete design flaws and
antipatterns. Results are poised to (i) inform evidence-based curricula and
automated feedback systems, (ii) provide effect-size benchmarks for future
educational interventions, and (iii) supply an open, pseudonymized dataset and
reproducible analysis pipeline for the research community. By clarifying how
programming habits influence early skill acquisition, the work advances both
computing-education theory and practical tooling for sustainable software
maintenance and evolution.

</details>


### [33] [Roseau: Fast, Accurate, Source-based API Breaking Change Analysis in Java](https://arxiv.org/abs/2507.17369)
*Corentin Latappy,Thomas Degueule,Jean-Rémy Falleri,Romain Robbes,Lina Ochoa*

Main category: cs.SE

TL;DR: Roseau is a static analysis tool for Java APIs that generates technology-agnostic models from source or bytecode, enabling accurate and efficient detection of breaking changes (BCs) in library versions. It outperforms existing tools like JApiCmp and Revapi in accuracy and supports large-scale longitudinal studies.


<details>
  <summary>Details</summary>
Motivation: Current Java BC detection tools rely on binary JARs, limiting their applicability to fine-grained analyses (e.g., commit-level) and long-term library evolution studies. This hinders maintainers and researchers who require robust, scalable methods to manage backward compatibility and analyze API changes.

Method: Roseau constructs API models using rich semantic static analysis of source code or bytecode, enabling technology-agnostic comparison. The tool was validated by extending a BC benchmark, evaluating accuracy (F1), performance, and scalability across 60 libraries and 6,839 Guava commits.

Result: Roseau achieves 99% F1 accuracy for BC detection, outperforming prior tools (JApiCmp: 86%, Revapi: 91%). It processes 60 libraries with sub-2-second detection times, even for large codebases, and reduces Guava's 14-year BC analysis duration from days to minutes.

Conclusion: Roseau addresses critical gaps in BC detection by combining flexible source/bytecode modeling with exceptional speed and accuracy. Its design enables novel longitudinal analyses (e.g., tracking BCs across commits), proving effective for both practical maintenance and academic research on API evolution.

Abstract: Understanding API evolution and the introduction of breaking changes (BCs) in
software libraries is essential for library maintainers to manage backward
compatibility and for researchers to conduct empirical studies on software
library evolution. In Java, tools such as JApiCmp and Revapi are commonly used
to detect BCs between library releases, but their reliance on binary JARs
limits their applicability. This restriction hinders large-scale longitudinal
studies of API evolution and fine-grained analyses such as commit-level BC
detection. In this paper, we introduce Roseau, a novel static analysis tool
that constructs technology-agnostic API models from library code equipped with
rich semantic analyses. API models can be analyzed to study API evolution and
compared to identify BCs between any two versions of a library (releases,
commits, branches, etc.). Unlike traditional approaches, Roseau can build API
models from source code or bytecode, and is optimized for large-scale
longitudinal analyses of library histories. We assess the accuracy,
performance, and suitability of Roseau for longitudinal studies of API
evolution, using JApiCmp and Revapi as baselines. We extend and refine an
established benchmark of BCs and show that Roseau achieves higher accuracy (F1
= 0.99) than JApiCmp (F1 = 0.86) and Revapi (F1 = 0.91). We analyze 60 popular
libraries from Maven Central and find that Roseau delivers excellent
performance, detecting BCs between versions in under two seconds, including in
libraries with hundreds of thousands of lines of code. We further illustrate
the limitations of JApiCmp and Revapi for longitudinal studies and the novel
analysis capabilities offered by Roseau by tracking the evolution of Google's
Guava API and the introduction of BCs over 14 years and 6,839 commits, reducing
analysis times from a few days to a few minutes.

</details>


### [34] [Investigating Training Data Detection in AI Coders](https://arxiv.org/abs/2507.17389)
*Tianlin Li,Yunxiang Wei,Zhiming Li,Aishan Liu,Qing Guo,Xianglong Liu,Dongning Sun,Yang Liu*

Main category: cs.SE

TL;DR: This paper evaluates the effectiveness of training data detection (TDD) methods for CodeLLMs across eight models and introduces CodeSnitch, a function-level benchmark dataset with 9,000 labeled code samples. It tests TDD robustness using mutation strategies based on code clone taxonomies.


<details>
  <summary>Details</summary>
Motivation: CodeLLMs may leak proprietary/sensitive code from their training data, creating privacy and intellectual property risks. Natural language TDD methods lack exploration in structured code domains with distinct similarity criteria.

Method: Empirical evaluation of seven state-of-the-art TDD methods on CodeSnitch dataset across eight CodeLLMs, combined with mutation strategies aligned to Type-1 to Type-4 code clone taxonomies to assess robustness under three settings.

Result: Provides systematic assessment of TDD performance on code data, revealing limitations of existing methods and establishing a foundation for improving code-specific detection techniques through benchmark analysis.

Conclusion: Highlights the need for code-aware training data detection techniques and offers actionable insights to develop more effective TDD methods that address structured syntax and code similarity characteristics.

Abstract: Recent advances in code large language models (CodeLLMs) have made them
indispensable tools in modern software engineering. However, these models
occasionally produce outputs that contain proprietary or sensitive code
snippets, raising concerns about potential non-compliant use of training data,
and posing risks to privacy and intellectual property. To ensure responsible
and compliant deployment of CodeLLMs, training data detection (TDD) has become
a critical task. While recent TDD methods have shown promise in natural
language settings, their effectiveness on code data remains largely
underexplored. This gap is particularly important given code's structured
syntax and distinct similarity criteria compared to natural language. To
address this, we conduct a comprehensive empirical study of seven
state-of-the-art TDD methods on source code data, evaluating their performance
across eight CodeLLMs. To support this evaluation, we introduce CodeSnitch, a
function-level benchmark dataset comprising 9,000 code samples in three
programming languages, each explicitly labeled as either included or excluded
from CodeLLM training. Beyond evaluation on the original CodeSnitch, we design
targeted mutation strategies to test the robustness of TDD methods under three
distinct settings. These mutation strategies are grounded in the
well-established Type-1 to Type-4 code clone detection taxonomy. Our study
provides a systematic assessment of current TDD techniques for code and offers
insights to guide the development of more effective and robust detection
methods in the future.

</details>


### [35] [AssertFlip: Reproducing Bugs via Inversion of LLM-Generated Passing Tests](https://arxiv.org/abs/2507.17542)
*Lara Khatib,Noble Saji Mathews,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: AssertFlip uses large language models to generate Bug Reproducible Tests (BRTs) by first creating passing tests and then inverting them to reproduce bugs, achieving a 43.6% success rate on SWT-Bench.


<details>
  <summary>Details</summary>
Motivation: Most reported bugs lack executable tests, complicating debugging. Directly generating failing tests via LLMs is challenging, motivating an approach that leverages LLMs' strength in producing passing tests.

Method: The method involves two steps: (1) generating passing tests with LLMs based on buggy behavior and (2) inverting these tests to create failing ones by modifying assertion logic when the bug is present.

Result: AssertFlip achieves a 43.6% fail-to-pass success rate on the SWT-Bench-Verified subset, outperforming existing BRT generation techniques.

Conclusion: By inverting passing tests to fail, AssertFlip demonstrates a novel and effective strategy for BRT generation, showing improved performance over direct methods.

Abstract: Bug reproduction is critical in the software debugging and repair process,
yet the majority of bugs in open-source and industrial settings lack executable
tests to reproduce them at the time they are reported, making diagnosis and
resolution more difficult and time-consuming. To address this challenge, we
introduce AssertFlip, a novel technique for automatically generating Bug
Reproducible Tests (BRTs) using large language models (LLMs). Unlike existing
methods that attempt direct generation of failing tests, AssertFlip first
generates passing tests on the buggy behaviour and then inverts these tests to
fail when the bug is present. We hypothesize that LLMs are better at writing
passing tests than ones that crash or fail on purpose. Our results show that
AssertFlip outperforms all known techniques in the leaderboard of SWT-Bench, a
benchmark curated for BRTs. Specifically, AssertFlip achieves a fail-to-pass
success rate of 43.6% on the SWT-Bench-Verified subset.

</details>


### [36] [CodeReasoner: Enhancing the Code Reasoning Ability with Reinforcement Learning](https://arxiv.org/abs/2507.17548)
*Lingxiao Tang,He Ye,Zhongxin Liu,Xiaoxue Ren,Lingfeng Bao*

Main category: cs.SE

TL;DR: The paper introduces CodeReasoner, a two-stage training framework combining dataset construction and GRPO reinforcement learning, achieving 27.1-40.2% performance improvements on code reasoning benchmarks with 7B/14B models.


<details>
  <summary>Details</summary>
Motivation: Existing supervised fine-tuning approaches for code reasoning tasks exhibit limited gains and poor generalization due to low-quality training data and the inability to teach general reasoning skills

Method: 1) Constructed datasets focusing on Python program execution logic
2) Applied instruction tuning with knowledge distillation from a strong teacher model
3) Enhanced via GRPO reinforcement learning using reasoning chains

Result: 7B model matched GPT-4o on key code reasoning tasks (input/output and coverage prediction). 14B CodeReasoner outperformed GPT-4o across all benchmarks with 27.1-40.2% improvements over prior methods

Conclusion: CodeReasoner demonstrates that combining specialized dataset construction with a two-stage training approach (instruction tuning + GRPO) yields significant improvements in code reasoning capabilities for LLMs, particularly when emphasizing execution logic and reasoning chains in the training process

Abstract: Code reasoning is a fundamental capability for large language models (LLMs)
in the code domain. It involves understanding and predicting a program's
execution behavior, such as determining the output for a given input or whether
a specific statement will be executed. This capability is essential for
downstream tasks like debugging, code generation, and program repair. Prior
approaches mainly rely on supervised fine-tuning to improve performance in code
reasoning tasks. However, they often show limited gains and fail to generalize
across diverse scenarios. We argue this is due to two core issues: the low
quality of training data and the limitations of supervised fine-tuning, which
struggles to teach general reasoning skills. To address these challenges, we
propose CodeReasoner, a framework that spans both dataset construction and a
two-stage training process. First, we introduce a method to construct datasets
that focus on the core execution logic of Python programs. Next, we apply
instruction tuning to inject execution-specific knowledge distilled from a
powerful teacher model. We then enhance reasoning and generalization through
GRPO reinforcement learning on top of the fine-tuned model. Experiments on
three widely-used code reasoning benchmarks show that CodeReasoner improves
performance by 27.1% to 40.2% over prior methods using a 7B model. Notably, the
7B model matches GPT-4o on key tasks like input/output and coverage prediction.
When scaled to 14B, CodeReasoner outperforms GPT-4o across all benchmarks.
Ablation studies confirm the effectiveness of each training stage and highlight
the importance of reasoning chains.

</details>


### [37] [Contextual Code Retrieval for Commit Message Generation: A Preliminary Study](https://arxiv.org/abs/2507.17690)
*Bo Xiong,Linghao Zhang,Chong Wang,Peng Liang*

Main category: cs.SE

TL;DR: C3Gen enhances commit message generation by retrieving contextual code snippets from a repository to provide richer input for models, resulting in more informative messages.


<details>
  <summary>Details</summary>
Motivation: Existing commit message generation (CMG) techniques rely solely on code diffs, which lack the broader contextual information necessary to produce high-quality, detailed commit messages critical for effective software maintenance.

Method: C3Gen is a retrieval-based method that integrates commit-relevant code snippets from a repository into the model input, expanding the context to the repository scope rather than just the code diff.

Result: C3Gen improves model effectiveness in generating comprehensive messages through four objective and three subjective metrics. Human evaluation confirms its practical value, while analysis highlights limitations in similarity-based metrics.

Conclusion: C3Gen offers a robust approach to CMG by leveraging repository-level context, enhancing message informativeness, and prompting a reevaluation of similarity-based metric reliability for future research.

Abstract: A commit message describes the main code changes in a commit and plays a
crucial role in software maintenance. Existing commit message generation (CMG)
approaches typically frame it as a direct mapping which inputs a code diff and
produces a brief descriptive sentence as output. However, we argue that relying
solely on the code diff is insufficient, as raw code diff fails to capture the
full context needed for generating high-quality and informative commit
messages. In this paper, we propose a contextual code retrieval-based method
called C3Gen to enhance CMG by retrieving commit-relevant code snippets from
the repository and incorporating them into the model input to provide richer
contextual information at the repository scope. In the experiments, we
evaluated the effectiveness of C3Gen across various models using four objective
and three subjective metrics. Meanwhile, we design and conduct a human
evaluation to investigate how C3Gen-generated commit messages are perceived by
human developers. The results show that by incorporating contextual code into
the input, C3Gen enables models to effectively leverage additional information
to generate more comprehensive and informative commit messages with greater
practical value in real-world development scenarios. Further analysis
underscores concerns about the reliability of similaritybased metrics and
provides empirical insights for CMG.

</details>


### [38] [CASCADE: LLM-Powered JavaScript Deobfuscator at Google](https://arxiv.org/abs/2507.17691)
*Shan Jiang,Pranoy Kovuri,David Tao,Zhixun Tan*

Main category: cs.SE

TL;DR: CASCADE combines Gemini's coding and JSIR for efficient JavaScript deobfuscation by eliminating hardcoded rules.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on thousands of hardcoded rules for JavaScript deobfuscation, limiting scalability and reliability in testing, analysis, and security.

Method: Hybrid approach using Gemini to identify obfuscation patterns and JSIR for deterministic transformations to recover code semantics.

Result: Achieved reliable deobfuscation with significant efficiency gains, eliminating rule limitations in Google’s production systems.

Conclusion: CASCADE provides a scalable, rule-free solution for JavaScript deobfuscation, improving real-world reverse-engineering resilience.

Abstract: Software obfuscation, particularly prevalent in JavaScript, hinders code
comprehension and analysis, posing significant challenges to software testing,
static analysis, and malware detection. This paper introduces CASCADE, a novel
hybrid approach that integrates the advanced coding capabilities of Gemini with
the deterministic transformation capabilities of a compiler Intermediate
Representation (IR), specifically JavaScript IR (JSIR). By employing Gemini to
identify critical prelude functions, the foundational components underlying the
most prevalent obfuscation techniques, and leveraging JSIR for subsequent code
transformations, CASCADE effectively recovers semantic elements like original
strings and API names, and reveals original program behaviors. This method
overcomes limitations of existing static and dynamic deobfuscation techniques,
eliminating hundreds to thousands of hardcoded rules while achieving
reliability and flexibility. CASCADE is already deployed in Google's production
environment, demonstrating substantial improvements in JavaScript deobfuscation
efficiency and reducing reverse engineering efforts.

</details>


### [39] [Educational Insights from Code: Mapping Learning Challenges in Object-Oriented Programming through Code-Based Evidence](https://arxiv.org/abs/2507.17743)
*Andre Menolli,Bruno Strik*

Main category: cs.SE

TL;DR: The paper investigates how code smells and SOLID principle violations correlate with common learning challenges in Object-Oriented Programming (OOP) and validates a conceptual model linking code issues to educational difficulties.


<details>
  <summary>Details</summary>
Motivation: While existing literature identifies technical code issues in OOP, there is limited research connecting these issues to student learning difficulties, creating a gap in educational strategies to address conceptual misunderstandings in programming.

Method: The study employs qualitative analysis to categorize OOP learning challenges and maps them to code-level issues through a literature review. A conceptual model is developed and evaluated by an expert analyzing student code.

Result: A validated conceptual map linking code smells, SOLID principle violations, and learning challenges in OOP was produced, demonstrating its educational applicability through expert evaluation.

Conclusion: The model provides a structured framework for educators to identify and address learning obstacles in OOP by targeting specific code-level issues, bridging technical and pedagogical perspectives in software engineering education.

Abstract: Object-Oriented programming is frequently challenging for undergraduate
Computer Science students, particularly in understanding abstract concepts such
as encapsulation, inheritance, and polymorphism. Although the literature
outlines various methods to identify potential design and coding issues in
object-oriented programming through source code analysis, such as code smells
and SOLID principles, few studies explore how these code-level issues relate to
learning difficulties in Object-Oriented Programming. In this study, we explore
the relationship of the code issue indicators with common challenges
encountered during the learning of object-oriented programming. Using
qualitative analysis, we identified the main categories of learning
difficulties and, through a literature review, established connections between
these difficulties, code smells, and violations of the SOLID principles. As a
result, we developed a conceptual map that links code-related issues to
specific learning challenges in Object-Oriented Programming. The model was then
evaluated by an expert who applied it in the analysis of the student code to
assess its relevance and applicability in educational contexts.

</details>
