<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 15]
- [cs.SE](#cs.SE) [Total: 25]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Are Voters Willing to Collectively Secure Elections? Unraveling a Practical Blockchain Voting System](https://arxiv.org/abs/2510.08700)
*Zhuolun Li,Haluk Sonmezler,Faiza Shirazi,Febin Shaji,Tymoteusz Mroczkowski,Dexter Lardner,Matthew Alain Camus,Evangelos Pournaras*

Main category: cs.CR

TL;DR: Researchers developed a blockchain-based voting system where voters collectively protect ballot secrecy using threshold cryptography and smart contracts. User testing proved high adoption potential and security effectiveness for large-scale elections.


<details>
  <summary>Details</summary>
Motivation: Current decentralized electronic voting systems struggle to guarantee ballot secrecy at scale, creating vulnerabilities that could compromise election integrity. The paper addresses this challenge by exploring how collective participation from voters can strengthen secrecy assurances.

Method: The method involves a blockchain-based system combining threshold cryptography and smart contracts. It introduces a mechanism where voters can volunteer as secret holders, ensuring ballots remain confidential while allowing transparent verification. An intuitive user interface is designed to hide technical complexities from non-expert users.

Result: User testing validated the system's practicality, showing high voter willingness to act as secret holders, reliable execution of share-releasing protocols, and strong security confidence. These results confirm the feasibility of collective secrecy maintenance in real-world scenarios.

Conclusion: The paper concludes that collectively secure voting systems, which decentralize the protection of ballot secrecy through voter participation, are a feasible solution. The implementation demonstrates that such systems can balance strong confidentiality with practical deployment in large-scale elections.

Abstract: Ensuring ballot secrecy is critical for fair and trustworthy electronic
voting systems, yet achieving strong secrecy guarantees in decentralized,
large-scale elections remains challenging. This paper proposes the concept of
collectively secure voting, in which voters themselves can opt in as secret
holders to protect ballot secrecy. A practical blockchain-based collectively
secure voting system is designed and implemented. Our design strikes a balance
between strong confidentiality guarantees and real-world applicability. The
proposed system combines threshold cryptography and smart contracts to ensure
ballots remain confidential during voting, while all protocol steps remain
transparent and verifiable. Voters can use the system without prior blockchain
knowledge through an intuitive user interface that hides underlying complexity.
To evaluate this approach, a user testing is conducted. Results show a high
willingness to act as secret holders, reliable participation in share release,
and high security confidence in the proposed system. The findings demonstrate
that voters can collectively maintain secrecy and that such a practical
deployment is feasible.

</details>


### [2] [Post-Quantum Security of Block Cipher Constructions](https://arxiv.org/abs/2510.08725)
*Gorjan Alagic,Chen Bai,Christian Majenz,Kaiyan Shi*

Main category: cs.CR

TL;DR: Pioneers post-quantum security analysis for block ciphers, offering rigorous proofs for widely used cryptographic schemes against quantum threats.


<details>
  <summary>Details</summary>
Motivation: The post-quantum security of symmetric-key cryptography, particularly block ciphers, remains underexplored despite growing interest in quantum-resistant cryptography for public-key systems.

Method: Developed novel techniques for post-quantum security proofs, applicable in both plain and quantum ideal cipher models, and applied them to key-length extension schemes (FX) and tweakable block ciphers (LRW, XEX).

Result: First post-quantum security proofs for FX, LRW, XEX, and major encryption/authentication modes, with techniques validated across two security models.

Conclusion: This work establishes foundational theories for post-quantum security in block ciphers, providing critical initial steps toward understanding symmetric-key cryptography's resilience against quantum threats.

Abstract: Block ciphers are versatile cryptographic ingredients that are used in a wide
range of applications ranging from secure Internet communications to disk
encryption. While post-quantum security of public-key cryptography has received
significant attention, the case of symmetric-key cryptography (and block
ciphers in particular) remains a largely unexplored topic. In this work, we set
the foundations for a theory of post-quantum security for block ciphers and
associated constructions. Leveraging our new techniques, we provide the first
post-quantum security proofs for the key-length extension scheme FX, the
tweakable block ciphers LRW and XEX, and most block cipher encryption and
authentication modes. Our techniques can be used for security proofs in both
the plain model and the quantum ideal cipher model. Our work takes significant
initial steps in establishing a rigorous understanding of the post-quantum
security of practical symmetric-key cryptography.

</details>


### [3] [CommandSans: Securing AI Agents with Surgical Precision Prompt Sanitization](https://arxiv.org/abs/2510.08829)
*Debeshee Das,Luca Beurer-Kellner,Marc Fischer,Maximilian Baader*

Main category: cs.CR

TL;DR: A token-level sanitization method dramatically reduces LLM agent vulnerability to prompt injections without synthetic attack training.


<details>
  <summary>Details</summary>
Motivation: Current defenses for indirect prompt injections suffer from high false positives and poor calibration, rendering them unsuitable for real-world deployment. LLM agents' expanded access to tools and sensitive data exacerbates this problem due to the context-dependent nature of attacks.

Method: The proposed method involves a token-level sanitization process that removes instructions directed at AI systems from tool outputs. It operates non-intrusively, is context-agnostic, and leverages existing instruction-tuning data for training without reliance on synthetic attack samples.

Result: The method achieves a 7-10× reduction in attack success rates (e.g., from 34% to 3% on AgentDojo) across multiple benchmarks (AgentDojo, BIPIA, InjecAgent, ASB, and SEP), preserving agent utility in both benign and malicious scenarios.

Conclusion: The paper presents a novel, practical approach to reducing indirect prompt injection attacks through token-level sanitization, which effectively lowers attack success rates without compromising agent utility, making it suitable for real-world applications.

Abstract: The increasing adoption of LLM agents with access to numerous tools and
sensitive data significantly widens the attack surface for indirect prompt
injections. Due to the context-dependent nature of attacks, however, current
defenses are often ill-calibrated as they cannot reliably differentiate
malicious and benign instructions, leading to high false positive rates that
prevent their real-world adoption. To address this, we present a novel approach
inspired by the fundamental principle of computer security: data should not
contain executable instructions. Instead of sample-level classification, we
propose a token-level sanitization process, which surgically removes any
instructions directed at AI systems from tool outputs, capturing malicious
instructions as a byproduct. In contrast to existing safety classifiers, this
approach is non-blocking, does not require calibration, and is agnostic to the
context of tool outputs. Further, we can train such token-level predictors with
readily available instruction-tuning data only, and don't have to rely on
unrealistic prompt injection examples from challenges or of other synthetic
origin. In our experiments, we find that this approach generalizes well across
a wide range of attacks and benchmarks like AgentDojo, BIPIA, InjecAgent, ASB
and SEP, achieving a 7-10x reduction of attack success rate (ASR) (34% to 3% on
AgentDojo), without impairing agent utility in both benign and malicious
settings.

</details>


### [4] [Psyzkaller: Learning from Historical and On-the-Fly Execution Data for Smarter Seed Generation in OS kernel Fuzzing](https://arxiv.org/abs/2510.08918)
*Boyu Liu,Yang Zhang,Liang Cheng,Yi Zhang,Junjie Fan,Yu Fu*

Main category: cs.CR

TL;DR: This paper enhances Syzkaller, a Linux kernel fuzzer, using an N-gram model and a Random Walk strategy to generate more valid and diverse syscall sequences, significantly improving coverage and vulnerability detection.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art kernel fuzzers like Syzkaller suffer from inefficiencies due to inability to generate valid syscall sequences that adhere to Syscall Dependency Relations (SDRs). This leads to failed seeds and limited exploration of deep execution paths.

Method: The authors propose a method that (1) mines SDRs using an N-gram model from historical data (Dongting dataset) and real-time traces during fuzzing, and (2) uses a Random Walk approach to generate seeds bidirectionally in the Choice Table of Syzkaller. This improves sequence validity and diversity.

Result: On three Linux kernel versions, Psyzkaller achieved 4.6%-7.0% higher code coverage with 48-hour fuzzing compared to Syzkaller, and triggered 110.4%-187.2% more crashes. It also found 8 new kernel vulnerabilities, as opposed to only 1 by Syzkaller.

Conclusion: By learning SDRs from kernel execution data and applying bidirectional seed generation, Psyzkaller demonstrates tangible improvements over Syzkaller in Linux kernel fuzzing effectiveness, coverage, and vulnerability discovery.

Abstract: Fuzzing has become a cornerstone technique for uncovering vulnerabilities and
enhancing the security of OS kernels. However, state-of-the-art kernel fuzzers,
including the de facto standard Syzkaller, struggle to generate valid syscall
sequences that respect implicit Syscall Dependency Relations (SDRs).
Consequently, many generated seeds either fail kernel validation or cannot
penetrate deep execution paths, resulting in significant inefficiency.
  We hypothesize that SDRs can be effectively learned from both historic and
present kernel execution data, and that incorporating these learned relations
into fuzzing can substantially improve seed validity and diversity. To validate
this, we propose an approach that utilizes the N-gram model to mine SDRs from
the Dongting dataset-one of the largest Linux kernel execution datasets
available-as well as from execution traces collected on the fly during fuzzing.
The resulting model is used to continuously augment the Choice Table of
Syzkaller to improve its seed generation and demonstrably increases the Shannon
Entropy of the Choice Table throughout fuzzing, reflecting more
empirically-grounded choices in expanding syscall sequences into valid and
diverse seeds. In addition, we introduce a Random Walk strategy that instructs
Syzkaller to construct seeds in a bidirectional manner to further diversify the
generated seeds.
  We implement our approach in a prototype, Psyzkaller, built on top of
Syzkaller. Experiments on three representative Linux kernel versions show that
Psyzkaller improves Syzkaller's code coverage by 4.6%-7.0% in 48-hour fuzzing,
while triggering 110.4%-187.2% more crashes. Moreover, our investigation shows
that Psyzkaller discovered eight previously unknown kernel vulnerabilities,
compared to only one found by Syzkaller.

</details>


### [5] [Future G Network's New Reality: Opportunities and Security Challenges](https://arxiv.org/abs/2510.09006)
*Chandra Thapa,Surya Nepal*

Main category: cs.CR

TL;DR: This paper addresses emerging security challenges in ISAC-enabled G networks, where threats target systems' perception of reality. A multi-layered defense strategy and robust governance are proposed to ensure safe adoption.


<details>
  <summary>Details</summary>
Motivation: The paper highlights the need to address the paradigm shift in security priorities as ISAC transforms wireless networks into pervasive sensors, introducing perceptual threats that can cause direct physical harm.

Method: The authors propose a defense-in-depth framework integrating physical, environmental, intelligence, and architectural security measures to counter perception-level threats such as sensing eavesdropping and phantom dangers.

Result: A holistic security strategy is developed, emphasizing trusted ecosystem design and governance structures to ensure responsible ISAC deployment while mitigating risks from adversarial physical signal manipulations.

Conclusion: The paper concludes that securing ISAC's perception of physical reality requires a proactive, layered defense strategy and emphasizes the necessity of global standardization and governance to address privacy, liability, and dual-use challenges.

Abstract: Future G network's new reality is a widespread cyber-physical environment
created by Integrated Sensing and Communication (ISAC). It is a crucial
technology that transforms wireless connections into ubiquitous sensors. ISAC
unlocks transformative new capabilities, powering autonomous systems, augmented
human sensing, and next-generation immersive applications, such as digital
twins. However, this new reality fundamentally reshapes the security landscape.
The primary security concern shifts from the traditional focus on data
protection to a new priority: safeguarding the integrity of the system's
perception of physical reality itself. This perception can be perilously
manipulated by sophisticated attacks such as sensing eavesdropping, phantom
dangers, and invisible threats, potentially resulting in direct and
catastrophic physical harm. Traditional security measures, such as
signature-based detection, are insufficient to counter these perception-level
threats that mimic genuine physical signals. A proactive, layered,
defense-in-depth strategy is required, integrating physical, environmental,
intelligence, and architectural security measures to build a trustworthy
ecosystem. Additionally, realizing ISAC's potential responsibly also depends on
parallel efforts in global standardization and strong governance to address the
significant challenges of privacy, liability, and the technology's dual-use.

</details>


### [6] [Exploiting Web Search Tools of AI Agents for Data Exfiltration](https://arxiv.org/abs/2510.09093)
*Dennis Rall,Bernhard Bauer,Mohit Mittal,Thomas Fraunholz*

Main category: cs.CR

TL;DR: This paper investigates the vulnerability of LLMs to indirect prompt injection attacks, emphasizing the need for improved security through enhanced training, a centralized attack database, and a unified testing framework.


<details>
  <summary>Details</summary>
Motivation: The increasing use of LLMs for sensitive tasks via tool-calling and RAG raises security concerns, particularly regarding indirect prompt injection attacks, which exploit model weaknesses through manipulated external inputs.

Method: Through a systematic evaluation, the study assesses the susceptibility of various LLMs to indirect prompt injection attacks, investigates the influence of model parameters, and identifies effective attack methods.

Result: Results show that existing attack patterns remain successful, highlighting persistent defenses. Specific model characteristics and implementation factors correlate with increased vulnerability, underscoring the importance of security-focused development.

Conclusion: The paper concludes by stressing the importance of integrating security into the design of LLMs through stronger training procedures, centralized attack databases, and unified testing frameworks to combat ongoing threats effectively.

Abstract: Large language models (LLMs) are now routinely used to autonomously execute
complex tasks, from natural language processing to dynamic workflows like web
searches. The usage of tool-calling and Retrieval Augmented Generation (RAG)
allows LLMs to process and retrieve sensitive corporate data, amplifying both
their functionality and vulnerability to abuse. As LLMs increasingly interact
with external data sources, indirect prompt injection emerges as a critical and
evolving attack vector, enabling adversaries to exploit models through
manipulated inputs. Through a systematic evaluation of indirect prompt
injection attacks across diverse models, we analyze how susceptible current
LLMs are to such attacks, which parameters, including model size and
manufacturer, specific implementations, shape their vulnerability, and which
attack methods remain most effective. Our results reveal that even well-known
attack patterns continue to succeed, exposing persistent weaknesses in model
defenses. To address these vulnerabilities, we emphasize the need for
strengthened training procedures to enhance inherent resilience, a centralized
database of known attack vectors to enable proactive defense, and a unified
testing framework to ensure continuous security validation. These steps are
essential to push developers toward integrating security into the core design
of LLMs, as our findings show that current models still fail to mitigate
long-standing threats.

</details>


### [7] [Provable Watermarking for Data Poisoning Attacks](https://arxiv.org/abs/2510.09210)
*Yifan Zhu,Lijia Yu,Xiao-Shan Gao*

Main category: cs.CR

TL;DR: This paper proposes watermarking schemes for harmless data poisoning, balancing ownership verification with security via post-poisoning and poisoning-concurrent methods, validated theoretically and experimentally.


<details>
  <summary>Details</summary>
Motivation: The growing ambiguity between harmless watermarking and malicious data poisoning creates risks of misuse and conflict. Ownership verification through watermarking is critical to mitigate these risks.

Method: The paper introduces two watermarking approaches: post-poisoning (applied after poisoning) and poisoning-concurrent (integrated during poisoning), both with proven theoretical bounds on watermark length for detectability and utility.

Result: Theoretical guarantees show that watermarked datasets achieve detectability with lengths Θ(√d/ε_w) for post-poisoning and ±Θ(1/ε²_w)–O(√d/ε_p) for concurrent methods. Experiments confirm robustness across attacks, models, and datasets.

Conclusion: Watermarking schemes can effectively address the challenges of harmless data poisoning by enabling ownership claims and preventing misuse, as demonstrated through theoretical analysis and experimental validation.

Abstract: In recent years, data poisoning attacks have been increasingly designed to
appear harmless and even beneficial, often with the intention of verifying
dataset ownership or safeguarding private data from unauthorized use. However,
these developments have the potential to cause misunderstandings and conflicts,
as data poisoning has traditionally been regarded as a security threat to
machine learning systems. To address this issue, it is imperative for harmless
poisoning generators to claim ownership of their generated datasets, enabling
users to identify potential poisoning to prevent misuse. In this paper, we
propose the deployment of watermarking schemes as a solution to this challenge.
We introduce two provable and practical watermarking approaches for data
poisoning: {\em post-poisoning watermarking} and {\em poisoning-concurrent
watermarking}. Our analyses demonstrate that when the watermarking length is
$\Theta(\sqrt{d}/\epsilon_w)$ for post-poisoning watermarking, and falls within
the range of $\Theta(1/\epsilon_w^2)$ to $O(\sqrt{d}/\epsilon_p)$ for
poisoning-concurrent watermarking, the watermarked poisoning dataset provably
ensures both watermarking detectability and poisoning utility, certifying the
practicality of watermarking under data poisoning attacks. We validate our
theoretical findings through experiments on several attacks, models, and
datasets.

</details>


### [8] [GREAT: Generalizable Backdoor Attacks in RLHF via Emotion-Aware Trigger Synthesis](https://arxiv.org/abs/2510.09260)
*Subrat Kishore Dutta,Yuelin Xu,Piyush Pant,Xiao Zhang*

Main category: cs.CR

TL;DR: This paper introduces GREAT, a framework for creating generalizable RLHF backdoors via emotion-aware triggers, using PCA-based latent embedding analysis and a curated dataset of angry triggers (Erinyes).


<details>
  <summary>Details</summary>
Motivation: Existing backdoor attacks on RLHF use static, rare-token triggers that fail in realistic scenarios, necessitating more adaptive and context-aware methods.

Method: GREAT synthesizes emotions-related triggers in the latent space via PCA for dimensionality reduction and clustering to identify representative samples. Erinyes dataset (n=5k) is built using GPT-4.1 with hierarchical curation for trigger diversity.

Result: GREAT achieves significantly higher attack success rates on benchmark RLHF datasets compared to baselines, particularly excelling against unseen triggers while maintaining response quality for normal inputs.

Conclusion: The emotion-aware approach in latent space enables more effective and generalizable backdoor attacks, highlighting vulnerabilities in RLHF systems when faced with semantically meaningful triggers.

Abstract: Recent work has shown that RLHF is highly susceptible to backdoor attacks,
poisoning schemes that inject malicious triggers in preference data. However,
existing methods often rely on static, rare-token-based triggers, limiting
their effectiveness in realistic scenarios. In this paper, we develop GREAT, a
novel framework for crafting generalizable backdoors in RLHF through
emotion-aware trigger synthesis. Specifically, GREAT targets harmful response
generation for a vulnerable user subgroup characterized by both semantically
violent requests and emotionally angry triggers. At the core of GREAT is a
trigger identification pipeline that operates in the latent embedding space,
leveraging principal component analysis and clustering techniques to identify
the most representative triggers. To enable this, we present Erinyes, a
high-quality dataset of over $5000$ angry triggers curated from GPT-4.1 using a
principled, hierarchical, and diversity-promoting approach. Experiments on
benchmark RLHF datasets demonstrate that GREAT significantly outperforms
baseline methods in attack success rates, especially for unseen trigger
scenarios, while largely preserving the response quality on benign inputs.

</details>


### [9] [SynthID-Image: Image watermarking at internet scale](https://arxiv.org/abs/2510.09263)
*Sven Gowal,Rudy Bunel,Florian Stimberg,David Stutz,Guillermo Ortiz-Jimenez,Christina Kouridi,Mel Vecerik,Jamie Hayes,Sylvestre-Alvise Rebuffi,Paul Bernard,Chris Gamble,Miklós Z. Horváth,Fabian Kaczmarczyck,Alex Kaskasoli,Aleksandar Petrov,Ilia Shumailov,Meghana Thotakuri,Olivia Wiles,Jessica Yung,Zahra Ahmed,Victor Martin,Simon Rosen,Christopher Savčak,Armin Senoner,Nidhi Vyas,Pushmeet Kohli*

Main category: cs.CR

TL;DR: The paper introduces SynthID-Image, an invisible watermarking system for AI-generated images. It addresses key requirements for such a system and includes an evaluation of SynthID-O, an external model variant. The focus is on deployment challenges and threat models.


<details>
  <summary>Details</summary>
Motivation: AI-generated images and videos have become content on the internet, raising questions about their origin and authenticity. The paper aims to address the requirements of watermarking at internet scale by considering practical challenges and threat models.

Method: The work introduces SynthID-Image and its external variant SynthID-O, discussing the technical desiderata, threat models, and deployment constraints. They benchmarked SynthID-O against other post-hoc watermarking methods through experimental evaluation.

Result: SynthID-Image is deployed on over ten billion images and video frames across Google's services. SynthID-O shows state-of-the-art performance in visual quality and robustness to common image perturbations.

Conclusion: The findings on deployment, constraints, and threat modeling of SynthID-Image and -O are applicable to other modalities like audio. The study offers comprehensive documentation for large-scale deployment of deep learning-based watermarking systems.

Abstract: We introduce SynthID-Image, a deep learning-based system for invisibly
watermarking AI-generated imagery. This paper documents the technical
desiderata, threat models, and practical challenges of deploying such a system
at internet scale, addressing key requirements of effectiveness, fidelity,
robustness, and security. SynthID-Image has been used to watermark over ten
billion images and video frames across Google's services and its corresponding
verification service is available to trusted testers. For completeness, we
present an experimental evaluation of an external model variant, SynthID-O,
which is available through partnerships. We benchmark SynthID-O against other
post-hoc watermarking methods from the literature, demonstrating
state-of-the-art performance in both visual quality and robustness to common
image perturbations. While this work centers on visual media, the conclusions
on deployment, constraints, and threat modeling generalize to other modalities,
including audio. This paper provides a comprehensive documentation for the
large-scale deployment of deep learning-based media provenance systems.

</details>


### [10] [Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects](https://arxiv.org/abs/2510.09269)
*Zirun Zhou,Zhengyang Xiao,Haochuan Xu,Jing Sun,Di Wang,Jingfeng Zhang*

Main category: cs.CR

TL;DR: This paper introduces GoBA, a goal-oriented backdoor attack method for VLA models, demonstrating a significant threat through physical trigger manipulation with high success rates and minimal clean input degradation.


<details>
  <summary>Details</summary>
Motivation: Recent VLA models have enhanced embodied AI, but their dependence on uncurated datasets introduces security risks. This paper addresses the lack of practical, real-world threat analysis, particularly in adversarial settings where attackers don't have full access to train models.

Method: GoBA infects the VLA by incorporating physical triggers and goal-driven actions into the training dataset. It introduces a new benchmark (BadLIBERO) and employs a three-level evaluation framework (nothing to do, try to do, success to do) to assess the effectiveness and stealthiness of the attack.

Result: Experiments show that GoBA enables VLA to achieve the backdoor goal in 97% of cases when a physical trigger is present, with no performance degradation on clean inputs. A detailed analysis identifies the key factors influencing attack success: action trajectory and trigger color, while trigger size has minimal impact.

Conclusion: GoBA highlights the vulnerability of VLA models to physical trigger attacks, advocating for safer practices in data curation for embodied AI models. The attack's availability in a new benchmark and strong performance without disrupting normal functionality suggests a pressing need for mitigation measures.

Abstract: Recent advances in vision-language-action (VLA) models have greatly improved
embodied AI, enabling robots to follow natural language instructions and
perform diverse tasks. However, their reliance on uncurated training datasets
raises serious security concerns. Existing backdoor attacks on VLAs mostly
assume white-box access and result in task failures instead of enforcing
specific actions. In this work, we reveal a more practical threat: attackers
can manipulate VLAs by simply injecting physical objects as triggers into the
training dataset. We propose goal-oriented backdoor attacks (GoBA), where the
VLA behaves normally in the absence of physical triggers but executes
predefined and goal-oriented actions in the presence of physical triggers.
Specifically, based on a popular VLA benchmark LIBERO, we introduce BadLIBERO
that incorporates diverse physical triggers and goal-oriented backdoor actions.
In addition, we propose a three-level evaluation that categorizes the victim
VLA's actions under GoBA into three states: nothing to do, try to do, and
success to do. Experiments show that GoBA enables the victim VLA to
successfully achieve the backdoor goal in 97 percentage of inputs when the
physical trigger is present, while causing zero performance degradation on
clean inputs. Finally, by investigating factors related to GoBA, we find that
the action trajectory and trigger color significantly influence attack
performance, while trigger size has surprisingly little effect. The code and
BadLIBERO dataset are accessible via the project page at
https://goba-attack.github.io/.

</details>


### [11] [Assessing the Impact of Post-Quantum Digital Signature Algorithms on Blockchains](https://arxiv.org/abs/2510.09271)
*Alison Gonçalves Schemitt,Henrique Fan da Silva,Roben Castagna Lunardi,Diego Kreutz,Rodrigo Brandão Mansilha,Avelino Francisco Zorzo*

Main category: cs.CR

TL;DR: This paper benchmarks post-quantum cryptography in blockchain, showing that PQC algorithms like ML-DSA often outperform ECDSA at higher security levels with minimal overhead, enabling quantum-resistant systems without compromising performance.


<details>
  <summary>Details</summary>
Motivation: Quantum computing threatens traditional cryptography used in blockchain (e.g., ECDSA), necessitating the transition to post-quantum algorithms. While NIST standardized PQC in 2024, the computational overhead of these algorithms in resource-constrained blockchain contexts remains poorly understood.

Method: The authors developed a benchmarking methodology to evaluate PQC and traditional cryptographic algorithms in blockchain environments. They measured signature generation/verification times across diverse computational settings, simulated large-scale impacts, and compared seven PQC schemes (ML-DSA, Dilithium, Falcon, Mayo, SLH-DSA, SPHINCS+, and Cross) against ECDSA across NIST security levels 1-5.

Result: PQC algorithms exhibit minimal overhead at security level 1 and outperform ECDSA at higher levels (e.g., ML-DSA's 0.14 ms vs. ECDSA's 0.88 ms at level 5 on an ARM-based laptop). The evaluation quantifies performance trade-offs across seven PQC schemes and provides reproducible, open-source benchmarks.

Conclusion: The study concludes that post-quantum cryptographic (PQC) algorithms can achieve quantum-resistant security in blockchain systems with minimal performance overhead, particularly at higher security levels, and provides an open-source framework to support further research and adoption.

Abstract: The advent of quantum computing threatens the security of traditional
encryption algorithms, motivating the development of post-quantum cryptography
(PQC). In 2024, the National Institute of Standards and Technology (NIST)
standardized several PQC algorithms, marking an important milestone in the
transition toward quantum-resistant security. Blockchain systems fundamentally
rely on cryptographic primitives to guarantee data integrity and transaction
authenticity. However, widely used algorithms such as ECDSA, employed in
Bitcoin, Ethereum, and other networks, are vulnerable to quantum attacks.
Although adopting PQC is essential for long-term security, its computational
overhead in blockchain environments remains largely unexplored. In this work,
we propose a methodology for benchmarking both PQC and traditional
cryptographic algorithms in blockchain contexts. We measure signature
generation and verification times across diverse computational environments and
simulate their impact at scale. Our evaluation focuses on PQC digital signature
schemes (ML-DSA, Dilithium, Falcon, Mayo, SLH-DSA, SPHINCS+, and Cross) across
security levels 1 to 5, comparing them to ECDSA, the current standard in
Bitcoin and Ethereum. Our results indicate that PQC algorithms introduce only
minor performance overhead at security level 1, while in some scenarios they
significantly outperform ECDSA at higher security levels. For instance, ML-DSA
achieves a verification time of 0.14 ms on an ARM-based laptop at level 5,
compared to 0.88 ms for ECDSA. We also provide an open-source implementation to
ensure reproducibility and encourage further research.

</details>


### [12] [Modern iOS Security Features -- A Deep Dive into SPTM, TXM, and Exclaves](https://arxiv.org/abs/2510.09272)
*Moritz Steffin,Jiska Classen*

Main category: cs.CR

TL;DR: This research focuses on the architectural evolution of Apple's XNU kernel, highlighting new security mechanisms like SPTM and Exclaves for compartmentalization, reducing the risk of a kernel compromise affecting the entire system.


<details>
  <summary>Details</summary>
Motivation: Traditionally operating in a monolithic style, the XNU kernel faces security concerns due to a single compromised point affecting the system. Apple's recent moves towards a more secure, compartmentalized kernel lack a comprehensive scientific analysis, motivating this study to evaluate the effectiveness of new mechanisms against current threats.

Method: The analysis involves a comprehensive evaluation of the XNU kernel, focusing on the implementation of SPTM domains through frame retyping and memory mapping. Additionally, the researchers explore the communication mechanisms, including xnuproxy and Tightbeam IPC, to determine how these security features interact and reinforce the architectural changes.

Result: The results show that SPTM introduces compartments with different trust levels, enhancing kernel security by isolating functions like TXM. The communication frameworks such as xnuproxy and Tightbeam IPC are identified as critical components for Exclaves, a new secure environment separated from the kernel's direct control. The overall architecture offers increased protection against kernel compromises while possibly maintaining performance and functionality.

Conclusion: The paper concludes that Apple's transition from a monolithic to a more microkernel-like design, particularly via SPTM and Exclaves, significantly enhances system security. The isolation of key components secures the highest trust level and provides additional defense in the event of a vulnerability in the kernel.

Abstract: The XNU kernel is the basis of Apple's operating systems. Although labeled as
a hybrid kernel, it is found to generally operate in a monolithic manner by
defining a single privileged trust zone in which all system functionality
resides. This has security implications, as a kernel compromise has immediate
and significant effects on the entire system. Over the past few years, Apple
has taken steps towards a more compartmentalized kernel architecture and a more
microkernel-like design. To date, there has been no scientific discussion of
SPTM and related security mechanisms. Therefore, the understanding of the
system and the underlying security mechanisms is minimal. In this paper, we
provide a comprehensive analysis of new security mechanisms and their
interplay, and create the first conclusive writeup considering all current
mitigations. SPTM acts as the sole authority regarding memory retyping. Our
analysis reveals that, through SPTM domains based on frame retyping and memory
mapping rule sets, SPTM introduces domains of trust into the system,
effectively gapping different functionalities from one another. Gapped
functionality includes the TXM, responsible for code signing and entitlement
verification. We further demonstrate how this introduction lays the groundwork
for the most recent security feature of Exclaves, and conduct an in-depth
analysis of its communication mechanisms. We discover multifold ways of
communication, most notably xnuproxy as a secure world request handler, and the
Tightbeam IPC framework. The architecture changes are found to increase system
security, with key and sensitive components being moved out of XNU's direct
reach. This also provides additional security guarantees in the event of a
kernel compromise, which is no longer an immediate threat at the highest trust
level.

</details>


### [13] [Clustering Deposit and Withdrawal Activity in Tornado Cash: A Cross-Chain Analysis](https://arxiv.org/abs/2510.09433)
*Raffaele Cristodaro,Benjamin Kramer,Claudio J. Tessone*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Tornado Cash is a decentralised mixer that uses cryptographic techniques to
sever the on-chain trail between depositors and withdrawers. In practice,
however, its anonymity can be undermined by user behaviour and operational
quirks. We conduct the first cross-chain empirical study of Tornado Cash
activity on Ethereum, BNB Smart Chain, and Polygon, introducing three
clustering heuristics-(i) address-reuse, (ii) transactional-linkage, and (iii)
a novel first-in-first-out (FIFO) temporal-matching rule. Together, these
heuristics reconnect deposits to withdrawals and deanonymise a substantial
share of recipients. Our analysis shows that 5.1 - 12.6% of withdrawals can
already be traced to their originating deposits through address reuse and
transactional linkage heuristics. Adding our novel First-In-First-Out (FIFO)
temporal-matching heuristic lifts the linkage rate by a further 15 - 22
percentage points. Statistical tests confirm that these FIFO matches are highly
unlikely to occur by chance. Comparable leakage across Ethereum, BNB Smart
Chain, and Polygon indicates chain-agnostic user misbehaviour, rather than
chain-specific protocol flaws. These results expose how quickly cryptographic
guarantees can unravel in everyday use, underscoring the need for both
disciplined user behaviour and privacy-aware protocol design. In total, our
heuristics link over $2.3 billion in Tornado Cash withdrawals to identifiable
deposits, exposing significant cracks in practical anonymity.

</details>


### [14] [The Impact of Sanctions on decentralised Privacy Tools: A Case Study of Tornado Cash](https://arxiv.org/abs/2510.09443)
*Raffaele Cristodaro,Benjamin Kramer,Claudio J. Tessone*

Main category: cs.CR

TL;DR: The paper examines how U.S. sanctions against Tornado Cash (2022-2025) caused sustained declines in transaction volume, user diversity, and protocol usage, with only partial recovery after sanctions were lifted, illustrating regulatory challenges in decentralized ecosystems.


<details>
  <summary>Details</summary>
Motivation: The study addresses the gap in understanding how regulatory interventions impact decentralized protocols, balancing enforcement effectiveness with inherent system properties like privacy and decentralization.

Method: Analyzes blockchain transaction data (Ethereum, BNB Smart Chain, Polygon) before/after sanctions imposition (August 2022), partial lifting, and full removal (March 2025) to measure protocol activity changes.

Result: Sanctions caused significant and lasting reductions in key metrics; partial sanctions removal led to limited activity recovery, demonstrating both regulatory efficacy and limitations in decentralized environments.

Conclusion: Regulatory actions can influence decentralized protocol behavior, but structural characteristics of such systems (e.g., cross-chain operations, pseudonymity) complicate comprehensive enforcement.

Abstract: This paper investigates the impact of sanctions on Tornado Cash, a smart
contract protocol designed to enhance transaction privacy. Following the U.S.
Department of the Treasury's sanctions against Tornado Cash in August 2022,
platform activity declined sharply. We document a significant and sustained
reduction in transaction volume, user diversity, and overall protocol
utilization after the sanctions were imposed. Our analysis draws on transaction
data from three major blockchains: Ethereum, BNB Smart Chain, and Polygon. We
further examine developments following the partial lifting and eventual removal
of sanctions by the U.S. Office of Foreign Assets Control (OFAC) in March 2025.
Although activity partially recovered, the rebound remained limited. The
Tornado Cash case illustrates how regulatory interventions can affect
decentralized protocols, while also highlighting the challenges of fully
enforcing such measures in decentralized environments.

</details>


### [15] [The Data Enclave Advantage: A New Paradigm for Least-Privileged Data Access in a Zero-Trust World](https://arxiv.org/abs/2510.09494)
*Nico Bistolfi,Andreea Georgescu,Dave Hodson*

Main category: cs.CR

TL;DR: This paper addresses cloud security gaps caused by standing permissions by introducing on-demand data enclaves that enforce ZSP/JIT principles at the data level, replacing static permissions with temporary contracts to reduce risks and improve auditability.


<details>
  <summary>Details</summary>
Motivation: The reliance on outdated standing permissions in cloud infrastructure has emerged as a critical vulnerability, particularly as AI-driven workflows and distributed systems increase data access complexity. Current security tools inadequately protect granular data access, creating catastrophic breach risks for enterprises handling sensitive data at scale.

Method: The paper proposes an architecture based on on-demand data enclaves that enforce temporary data contracts, replacing static permissions. This enables ZSP and JIT principles at the data level, offering granular, real-time access control and monitoring for individual records rather than datasets.

Result: The proposed solution reduces attack surfaces, prevents privilege creep, enables proactive protection through on-demand isolation, and simplifies auditing by replacing static permissions with ephemeral, record-level access controls enforced via data contracts.

Conclusion: Transitioning to a model that enforces Zero Standing Privilege (ZSP) and Just-in-Time (JIT) principles through temporary data contracts provides a critical path for enterprises to achieve secure, auditable, and resilient cloud data environments by eliminating static permissions and reducing attack surfaces.

Abstract: As cloud infrastructure evolves to support dynamic and distributed workflows,
accelerated now by AI-driven processes, the outdated model of standing
permissions has become a critical vulnerability. Based on the Cloud Security
Alliance (CSA) Top Threats to Cloud Computing Deep Dive 2025 Report, our
analysis details how standing permissions cause catastrophic cloud breaches.
While current security tools are addressing network and API security, the
challenge of securing granular data access remains. Removing standing
permissions at the data level is as critical as it is at the network level,
especially for companies handling valuable data at scale.
  In this white paper, we introduce an innovative architecture based on
on-demand data enclaves to address this gap directly. Our approach enables Zero
Standing Privilege (ZSP) and Just-in-Time (JIT) principles at the data level.
We replace static permissions with temporary data contracts that enforce
proactive protection. This means separation is built around the data requested
on-demand, providing precise access and real time monitoring for individual
records instead of datasets. This solution drastically reduces the attack
surface, prevents privilege creep, and simplifies auditing, offering a vital
path for enterprises to transition to a more secure and resilient data
environment.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [16] [Comparative Analysis of Large Language Models for the Machine-Assisted Resolution of User Intentions](https://arxiv.org/abs/2510.08576)
*Justus Flerlage,Alexander Acker,Odej Kao*

Main category: cs.SE

TL;DR: This study evaluates open-source LLMs for local deployment to enable privacy-preserving, intent-based operating systems, comparing their viability with proprietary GPT-4 systems.


<details>
  <summary>Details</summary>
Motivation: Cloud-based proprietary LLMs lack privacy/autonomy, necessitating open alternatives for decentralized, user-centric interfaces.

Method: Comparative analysis of open-source LLMs against GPT-4 in workflow generation tasks for user intention resolution.

Result: Empirical insights into open LLMs' practical viability/limitations as local components for next-gen OS design.

Conclusion: Open LLMs enable decentralized AI infrastructure with privacy-first, adaptive device interaction - a critical step toward democratizing intelligent systems.

Abstract: Large Language Models (LLMs) have emerged as transformative tools for natural
language understanding and user intent resolution, enabling tasks such as
translation, summarization, and, increasingly, the orchestration of complex
workflows. This development signifies a paradigm shift from conventional,
GUI-driven user interfaces toward intuitive, language-first interaction
paradigms. Rather than manually navigating applications, users can articulate
their objectives in natural language, enabling LLMs to orchestrate actions
across multiple applications in a dynamic and contextual manner. However,
extant implementations frequently rely on cloud-based proprietary models, which
introduce limitations in terms of privacy, autonomy, and scalability. For
language-first interaction to become a truly robust and trusted interface
paradigm, local deployment is not merely a convenience; it is an imperative.
This limitation underscores the importance of evaluating the feasibility of
locally deployable, open-source, and open-access LLMs as foundational
components for future intent-based operating systems. In this study, we examine
the capabilities of several open-source and open-access models in facilitating
user intention resolution through machine assistance. A comparative analysis is
conducted against OpenAI's proprietary GPT-4-based systems to assess
performance in generating workflows for various user intentions. The present
study offers empirical insights into the practical viability, performance
trade-offs, and potential of open LLMs as autonomous, locally operable
components in next-generation operating systems. The results of this study
inform the broader discussion on the decentralization and democratization of AI
infrastructure and point toward a future where user-device interaction becomes
more seamless, adaptive, and privacy-conscious through locally embedded
intelligence.

</details>


### [17] [Which Is Better For Reducing Outdated and Vulnerable Dependencies: Pinning or Floating?](https://arxiv.org/abs/2510.08609)
*Imranur Rahman,Jill Marley,William Enck,Laurie Williams*

Main category: cs.SE

TL;DR: This study empirically evaluates how different dependency version constraint types (pinning vs. floating) impact the likelihood of dependencies becoming outdated or vulnerable in npm, PyPI, and Cargo ecosystems using survival analysis.


<details>
  <summary>Details</summary>
Motivation: Developers face a trade-off between using strict version constraints (pinning) for security stability and looser constraints (floating) for automatic updates. Security practitioners advocate pinning to prevent supply chain attacks, but it risks outdated dependencies. Empirical data is needed to guide developers in making informed version constraint choices.

Method: 1) Analyzed trends in version constraint usage and developer behavior changes across constraints in three package ecosystems (npm, PyPI, Cargo). 2) Modeled dependency state transitions using survival analysis to quantify how constraint types affect the likelihood of becoming outdated or vulnerable.

Result: Key finding: floating-minor is most commonly used (88%) among outdated/vulnerable dependencies but least likely to result in vulnerabilities. Floating-major is least likely to result in outdated dependencies. Pinning is the second most common constraint type after floating-minor but more likely to result in outdated dependencies than floating-major.

Conclusion: The study provides empirical guidance for developers: floating-major should be preferred to minimize outdated issues while avoiding vulnerabilities. Floating-minor balances between security and update risks. Pinning remains valuable for security-sensitive contexts but should be combined with automated update tools to mitigate staleness.

Abstract: Developers consistently use version constraints to specify acceptable
versions of the dependencies for their project. \emph{Pinning} dependencies can
reduce the likelihood of breaking changes, but comes with a cost of manually
managing the replacement of outdated and vulnerable dependencies. On the other
hand, \emph{floating} can be used to automatically get bug fixes and security
fixes, but comes with the risk of breaking changes. Security practitioners
advocate \emph{pinning} dependencies to prevent against software supply chain
attacks, e.g., malicious package updates. However, since \emph{pinning} is the
tightest version constraint, \emph{pinning} is the most likely to result in
outdated dependencies. Nevertheless, how the likelihood of becoming outdated or
vulnerable dependencies changes across version constraint types is unknown. The
goal of this study is to aid developers in making an informed dependency
version constraint choice by empirically evaluating the likelihood of
dependencies becoming outdated or vulnerable across version constraint types at
scale. In this study, we first identify the trends in dependency version
constraint usage and the patterns of version constraint type changes made by
developers in the npm, PyPI, and Cargo ecosystems. We then modeled the
dependency state transitions using survival analysis and estimated how the
likelihood of becoming outdated or vulnerable changes when using \emph{pinning}
as opposed to the rest of the version constraint types. We observe that among
outdated and vulnerable dependencies, the most commonly used version constraint
type is \emph{floating-minor}, with \emph{pinning} being the next most common.
We also find that \emph{floating-major} is the least likely to result in
outdated and \emph{floating-minor} is the least likely to result in vulnerable
dependencies.

</details>


### [18] [Relative Positioning Based Code Chunking Method For Rich Context Retrieval In Repository Level Code Completion Task With Code Language Model](https://arxiv.org/abs/2510.08610)
*Imranur Rahman,Md Rayhanur Rahman*

Main category: cs.SE

TL;DR: This paper introduces a context collection strategy to improve LLM-based code completion by preprocessing repositories into code chunks and using similarity-based retrieval with relative positioning, enhancing performance in code completion tasks.


<details>
  <summary>Details</summary>
Motivation: Modern IDEs offer code completion, but there is limited research on defining optimal contexts for large language models (LLMs). Effective context design is critical to improve LLM accuracy in code completion scenarios.

Method: The authors propose a strategy involving: (1). Preprocessing repositories into smaller code chunks, (2). Retrieving relevant chunks using syntactic and semantic similarity, and (3). Incorporating relative positioning of chunks to construct contextual inputs for the LLMs.

Result: Code chunking combined with similarity-based retrieval and relative positioning significantly improves code completion performance compared to baseline methods.

Conclusion: The paper demonstrates that context engineering through code chunking and strategic positioning enhances LLM effectiveness in code completion, providing a scalable framework for integrating repository information into IDE tools.

Abstract: Code completion can help developers improve efficiency and ease the
development lifecycle. Although code completion is available in modern
integrated development environments (IDEs), research lacks in determining what
makes a good context for code completion based on the information available to
the IDEs for the large language models (LLMs) to perform better. In this paper,
we describe an effective context collection strategy to assist the LLMs in
performing better at code completion tasks. The key idea of our strategy is to
preprocess the repository into smaller code chunks and later use syntactic and
semantic similarity-based code chunk retrieval with relative positioning. We
found that code chunking and relative positioning of the chunks in the final
context improve the performance of code completion tasks.

</details>


### [19] [Impact of LLMs on Team Collaboration in Software Development](https://arxiv.org/abs/2510.08612)
*Devang Dhanuka*

Main category: cs.SE

TL;DR: 2025 study shows LLMs boost software team efficiency and collaboration via automation and communication tools, but face privacy/trust issues; future work needs domain-specific tweaks and better integration.


<details>
  <summary>Details</summary>
Motivation: Address collaboration hurdles in SDLC by investigating how LLMs can improve productivity, communication, and decision-making in teams, while addressing emerging challenges.

Method: Literature review, industry examples, team survey, and two case studies examining LLM-assisted tools' impacts on collaborative practices.

Result: LLM-assisted tools automate tasks, improve communication, and enable cross-functional collaboration, but introduce privacy and model capability challenges. These findings guide future research directions.

Conclusion: LLMs significantly enhance efficiency and collaboration in software development teams, but challenges like model limitations and privacy concerns arise. Future research should focus on customization, integration, and trust & security strategies.

Abstract: Large Language Models (LLMs) are increasingly being integrated into software
development processes, with the potential to transform team workflows and
productivity. This paper investigates how LLMs affect team collaboration
throughout the Software Development Life Cycle (SDLC). We reframe and update a
prior study with recent developments as of 2025, incorporating new literature
and case studies. We outline the problem of collaboration hurdles in SDLC and
explore how LLMs can enhance productivity, communication, and decision-making
in a team context. Through literature review, industry examples, a team survey,
and two case studies, we assess the impact of LLM-assisted tools (such as code
generation assistants and AI-powered project management agents) on
collaborative software engineering practices. Our findings indicate that LLMs
can significantly improve efficiency (by automating repetitive tasks and
documentation), enhance communication clarity, and aid cross-functional
collaboration, while also introducing new challenges like model limitations and
privacy concerns. We discuss these benefits and challenges, present research
questions guiding the investigation, evaluate threats to validity, and suggest
future research directions including domain-specific model customization,
improved integration into development tools, and robust strategies for ensuring
trust and security.

</details>


### [20] [Automating Android Build Repair: Bridging the Reasoning-Execution Gap in LLM Agents with Domain-Specific Tools](https://arxiv.org/abs/2510.08640)
*Ha Min Son,Huan Ren,Xin Liu,Zhe Zhao*

Main category: cs.SE

TL;DR: This paper introduces AndroidBuildBench, a benchmark for Android build failures, and GradleFixer, an LLM agent with domain-specific tools to resolve these errors. GradleFixer achieves an 81.4% resolve rate through 'Tool Bridging,' which uses API-like abstractions to improve LLM effectiveness in low-level build fixes.


<details>
  <summary>Details</summary>
Motivation: Automatically building Android applications remains challenging despite LLM advancements in code repair. Existing methods struggle to translate high-level LLM knowledge into effective low-level build environment actions using general-purpose shells.

Method: 1. Created AndroidBuildBench: 1,019 Android build failures from 43 open-source projects with verified fixes. 2. Developed GradleFixer: An LLM agent using domain-specific Gradle tools (Tool Bridging) instead of general shells. 3. Evaluated resolve rate (pass@1) against state-of-the-art coding agents.

Result: GradleFixer achieved 81.4% resolve rate (pass@1), significantly outperforming shell-based agents. Tool Bridging improved reliability by providing API-like interfaces and constrained action spaces to relevant operations.

Conclusion: Tool Bridging effectively bridges LLMs' high-level reasoning and low-level execution in build environments. Domain-aware abstractions improve fix feasibility, suggesting specialized tools are critical for solving technical domain challenges that general-purpose methods cannot address.

Abstract: Android is the largest mobile platform, yet automatically building
applications remains a practical challenge. While Large Language Models (LLMs)
show promise for code repair, their use for fixing Android build errors remains
underexplored. To address this gap, we first introduce AndroidBuildBench, a
benchmark of 1,019 build failures curated from the commit histories of 43
open-source Android projects. Each problem is paired with a verified solution
from a subsequent commit, ensuring that fixes are feasible. Second, we propose
GradleFixer, an LLM agent with domain-specific tools for inspecting and
manipulating the Gradle build environment. GradleFixer achieves a resolve rate
of 81.4% (pass@1), significantly outperforming a state-of-the-art coding agent
that relies on a general-purpose shell. GradleFixer's success suggests that
while LLMs possess the high-level knowledge to solve these failures, they
struggle to translate this knowledge into effective low-level actions using a
general-purpose shell. We demonstrate the effectiveness of a strategy we term
Tool Bridging, which replaces general-purpose shell commands with domain-aware
abstractions. We hypothesize this approach works through two mechanisms: 1) it
provides tools in an API-like format that LLMs use more reliably, and 2) it
constrains the action space to relevant operations. This approach bridges the
gap between the model's high-level reasoning and effective low-level execution.

</details>


### [21] [Faver: Boosting LLM-based RTL Generation with Function Abstracted Verifiable Middleware](https://arxiv.org/abs/2510.08664)
*Jianan Mu,Mingyu Shi,Yining Wang,Tianmeng Yang,Bin Sun,Xing Hu,Jing Ye,Huawei Li*

Main category: cs.SE

TL;DR: Faver bridges the RTL generation accuracy gap for LLMs by abstracting verification challenges, achieving 14% improved results.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based RTL generation struggles with accuracy due to the semantic gap between high-level specifications and RTL, limited training data, and scarcity of RTL testbench examples, necessitating new approaches to bridge these challenges.

Method: Faver integrates LLM-friendly code structures with rule-based templates to isolate circuit verification details, enabling LLMs to focus on functional implementation without low-level constraints.

Result: Experiments show Faver improves SFT and open-source models' RTL generation accuracy by up to 14%, validating its effectiveness in LLM-driven design workflows.

Conclusion: The proposed Faver middleware effectively streamlines RTL verification in LLM-based workflows by reducing the semantic gap and improving generation accuracy through decoupling verification complexities.

Abstract: LLM-based RTL generation is an interesting research direction, as it holds
the potential to liberate the least automated stage in the current chip design.
However, due to the substantial semantic gap between high-level specifications
and RTL, coupled with limited training data, existing models struggle with
generation accuracy. Drawing on human experience, design with verification
helps improving accuracy. However, as the RTL testbench data are even more
scarce, it is not friendly for LLMs. Although LLMs excel at higher-level
languages like Python/C, they have a huge semantic gap from RTL. When
implementing the same functionality, Python/C code and hardware code differ
significantly in the spatiotemporal granularity, requiring the LLM not only to
consider high-level functional semantics but also to ensure the low-level
details align with the circuit code. It is not an easy task. In this paper, we
propose a function abstracted verifiable middleware (Faver) that streamlines
RTL verification in LLM-based workflows. By mixing LLM-friendly code structures
with a rule-based template, Faver decouples the details of circuit
verification, allowing the LLM to focus on the functionality itself. In our
experiments on the SFT model and open-source models, Faver improved the model's
generation accuracy by up to 14%.

</details>


### [22] [RA-Gen: A Controllable Code Generation Framework Using ReAct for Multi-Agent Task Execution](https://arxiv.org/abs/2510.08665)
*Aofan Liu,Haoxuan Li,Bin Wang,Ao Yang,Hui Li*

Main category: cs.SE

TL;DR: This paper introduces a controllable code generation framework that uses the ReAct paradigm for multi-agent task execution. It enhances safety, accuracy, and user control by dynamically integrating external tools through four specialized agents.


<details>
  <summary>Details</summary>
Motivation: Current code generation models often fall short in safety, accuracy, and controllability, especially when handling complex tasks. The integration of external tools in a dynamic and transparent way for code generation remains a challenge in existing methods.

Method: The proposed framework is a multi-agent system comprising four agents: Planner for task decomposition, Searcher utilizing the ReAct framework for reasoning and tool integration, CodeGen for code generation, and Extractor for structured data retrieval. The Searcher agent dynamically interacts with external tools, such as search engines, while the other agents work in coordination for generating and extracting precise, secure code.

Result: The framework achieved a 94.8% security rate on the SVEN dataset using CodeQL and outperformed existing approaches. Additionally, it demonstrated effective code generation across multiple languages, and its transparent reasoning improved user trust and controllability.

Conclusion: The new controllable code generation framework provides a robust solution by leveraging the ReAct paradigm, enhancing dynamic interactions among agents and external tools. It improves safety, accuracy, and user transparency, making it applicable for various programming tasks and establishing a solid foundation for further advancements in code generation research.

Abstract: Code generation models based on large language models (LLMs) have gained wide
adoption, but challenges remain in ensuring safety, accuracy, and
controllability, especially for complex tasks. Existing methods often lack
dynamic integration of external tools, transparent reasoning, and user control
over safety. To address these issues, we propose a controllable code generation
framework utilizing the ReAct paradigm for multi-agent task execution. This
framework is a multi-agent system designed to enable efficient, precise, and
interpretable code generation through dynamic interactions between LLMs and
external resources. The framework adopts a collaborative architecture
comprising four specialized agents: a Planner for task decomposition, a
Searcher that leverages the ReAct framework for reasoning and tool integration,
a CodeGen agent for accurate code generation, and an Extractor for structured
data retrieval. The ReAct-based Searcher alternates between generating
reasoning traces and executing actions, facilitating seamless integration of
internal knowledge with external tools (such as search engines) to enhance
accuracy and user control. Experimental results show the framework's
effectiveness across multiple languages, achieving a 94.8% security rate on the
SVEN dataset with CodeQL, outperforming existing approaches. Its transparent
reasoning process fosters user trust and improves controllability.

</details>


### [23] [RAG4Tickets: AI-Powered Ticket Resolution via Retrieval-Augmented Generation on JIRA and GitHub Data](https://arxiv.org/abs/2510.08667)
*Mohammad Baqar*

Main category: cs.SE

TL;DR: The paper proposes a RAG framework combining Sentence-Transformers and FAISS to improve JIRA ticket resolution by synthesizing historical data from JIRA and GitHub.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the problem of fragmented knowledge across JIRA tickets, developer discussions, and GitHub PRs, which causes delays in resolving recurring issues.

Method: A RAG approach is used with Sentence-Transformers for embeddings and FAISS for vector search. It integrates JIRA and GitHub data into a unified pipeline, utilizing embeddings for heterogeneous artifacts and generating resolution suggestions with an LLM guided by retrieved evidence.

Result: Experiments showed improvements in precision, recall, resolution time, and developer acceptance, demonstrating that the system enhances resolution accuracy and knowledge reuse in DevOps.

Conclusion: The framework successfully improves ticket resolution in modern DevOps environments by leveraging knowledge from historical tickets and PRs through a RAG pipeline with embeddings and FAISS.

Abstract: Modern software teams frequently encounter delays in resolving recurring or
related issues due to fragmented knowledge scattered across JIRA tickets,
developer discussions, and GitHub pull requests (PRs). To address this
challenge, we propose a Retrieval-Augmented Generation (RAG) framework that
integrates Sentence-Transformers for semantic embeddings with FAISS-based
vector search to deliver context-aware ticket resolution recommendations. The
approach embeds historical JIRA tickets, user comments, and linked PR metadata
to retrieve semantically similar past cases, which are then synthesized by a
Large Language Model (LLM) into grounded and explainable resolution
suggestions. The framework contributes a unified pipeline linking JIRA and
GitHub data, an embedding and FAISS indexing strategy for heterogeneous
software artifacts, and a resolution generation module guided by retrieved
evidence. Experimental evaluation using precision, recall, resolution time
reduction, and developer acceptance metrics shows that the proposed system
significantly improves resolution accuracy, fix quality, and knowledge reuse in
modern DevOps environments.

</details>


### [24] [BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution](https://arxiv.org/abs/2510.08697)
*Terry Yue Zhuo,Xiaolong Jin,Hange Liu,Juyong Jiang,Tianyang Liu,Chen Gong,Bhupesh Bishnoi,Vaisakhi Mishra,Marek Suppa,Noah Ziems,Saiteja Utpala,Ming Xu,Guangyu Song,Kaixin Li,Yuhan Cao,Bo Liu,Zheng Liu,Sabina Abdurakhmanova,Wenhao Yu,Mengzhao Jia,Jihan Yao,Kenneth Hamilton,Kumar Shridhar,Minh Chien Vu,Dingmin Wang,Jiawei Liu,Zijian Wang,Qian Liu,Binyuan Hui,Meg Risdal,Ahsen Khaliq,Atin Sood,Zhenchang Xing,Wasi Uddin Ahmad,John Grundy,David Lo,Banghua Zhu,Xiaoning Du,Torsten Scholak,Leandro von Werra*

Main category: cs.SE

TL;DR: BigCodeArena is an open human evaluation platform for code generation LLMs, enabling code execution and human interaction. It introduces benchmarks (BigCodeReward, AutoCodeArena) to assess coding quality and model preferences.


<details>
  <summary>Details</summary>
Motivation: Manually evaluating LLM-generated code is challenging due to the need for long-form code understanding and execution simulation. Crowdsourced platforms lack execution environments for code domains.

Method: Built on Chatbot Arena, BigCodeArena enables execution of LLM-generated code. Collected 14,000 conversations across 10 LLMs, 10 languages, and 8 environments. Identified 4,700 multi-turn samples with human pairwise preferences. Created benchmarks: BigCodeReward (reward model consistency) and AutoCodeArena (automatic Elo rating system for code quality).

Result: LLMs show higher coding preference judgment accuracy with execution results. Reward models demonstrate strong correlation with human preferences. AutoCodeArena benchmarks reveal proprietary models (e.g., GPT-5, Claude-Sonnet-4) maintain top performance in code generation.

Conclusion: BigCodeArena establishes systematic code generation evaluation through execution environments and benchmarks, highlighting model strengths in execution-aware assessments and maintaining leadership of existing LLMs in code domains.

Abstract: Crowdsourced model evaluation platforms, such as Chatbot Arena, enable
real-time evaluation from human perspectives to assess the quality of model
responses. In the coding domain, manually examining the quality of
LLM-generated content is extremely challenging, as it requires understanding
long chunks of raw code and deliberately simulating code execution. To this
end, we introduce BigCodeArena, an open human evaluation platform for code
generation backed by a comprehensive and on-the-fly execution environment.
Built on top of Chatbot Arena, BigCodeArena enables the execution of
LLM-generated code and allows humans to interact with the execution process and
outcomes. We collected over 14,000 raw code-centric conversation sessions
across 10 widely used LLMs, spanning 10 languages and 8 types of execution
environments. Among these conversations, we identified more than 4,700
multi-turn samples with pairwise human preferences. Further analysis uncovers
underexplored preferences of LLMs in fine-grained domains characterized by
tasks, languages, and frameworks. To systematically examine code understanding
and generation capabilities of frontier LLMs, we curated two benchmarks based
on the collected data, namely BigCodeReward and AutoCodeArena. For
BigCodeReward, we post-processed the 4,700 conversations and evaluated the
consistency between reward models and human preferences. The evaluation shows
that most LLMs have superior performance in judging coding preferences when the
execution results are available. Inspired by these findings, we propose
AutoCodeArena, an automatic Elo rating benchmark designed to assess the coding
quality of LLMs without human involvement. We find that proprietary LLMs like
GPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation
performance among recent emerging models.

</details>


### [25] [Search-based Hyperparameter Tuning for Python Unit Test Generation](https://arxiv.org/abs/2510.08716)
*Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: This paper explores using differential evolution to tune hyperparameters of many-objective search algorithms (DynaMOSA, MIO), achieving improved test suite coverage and outperforming grid search in efficiency.


<details>
  <summary>Details</summary>
Motivation: Default configuration options of test-generation algorithms often suboptimally balance objectives, and hyperparameter tuning is typically resource-intensive.

Method: Applied differential evolution as a meta-heuristic optimizer to tune DynaMOSA and MIO hyperparameters within Pynguin, comparing results to a baseline grid search approach.

Result: Tuned DynaMOSA achieved significantly improved test coverage; differential evolution demonstrated 94% reduction in evaluation effort compared to grid search.

Conclusion: Differential evolution enhances test suite quality through effective hyperparameter optimization while substantially reducing computational requirements compared to traditional search methods.

Abstract: Search-based test-generation algorithms have countless configuration options.
Users rarely adjust these options and usually stick to the default values,
which may not lead to the best possible results. Tuning an algorithm's
hyperparameters is a method to find better hyperparameter values, but it
typically comes with a high demand of resources. Meta-heuristic search
algorithms -- that effectively solve the test-generation problem -- have been
proposed as a solution to also efficiently tune parameters. In this work we
explore the use of differential evolution as a means for tuning the
hyperparameters of the DynaMOSA and MIO many-objective search algorithms as
implemented in the Pynguin framework. Our results show that significant
improvement of the resulting test suite's coverage is possible with the tuned
DynaMOSA algorithm and that differential evolution is more efficient than basic
grid search.

</details>


### [26] [PyMigTool: a tool for end-to-end Python library migration](https://arxiv.org/abs/2510.08810)
*Mohayeminul Islam,Ajay Kumar Jha,May Mahmoud,Sarah Nadi*

Main category: cs.SE

TL;DR: The paper introduces PyMigTool, an end-to-end solution using Large Language Models (LLMs) combined with static/dynamic analysis to automate Python library migrations, achieving 32% complete correctness and reducing developer fixes to 14%


<details>
  <summary>Details</summary>
Motivation: Manual library migration is time-consuming and error-prone. Existing automated tools only support limited API mappings and library pairs. Developing a generalized solution for accurate and complete library migrations.

Method: 1. Benchmark study of 321 real-world migrations to assess LLM capabilities
2. Developed PyMigTool combining LLMs (for code generation), static analysis (for code context), and dynamic analysis (for execution verification)
3. Post-processing steps to enhance LLM outputs

Result: Evaluated on 717 new Python applications:
- 32% migrations with complete correctness
- 14%' of migration-related changes require developer attention for more than half of projects
- Tool works for arbitrary library pairs with similar functionality

Conclusion: LLMs combined with analysis techniques can effectively solve the library migration problem. PyMigTool demonstrates feasibility of end-to-end solutions, significantly reducing manual effort while maintaining high accuracy

Abstract: Library migration is the process of replacing a library with a similar one in
a software project. Manual library migration is time consuming and error prone,
as it requires developers to understand the Application Programming Interfaces
(API) of both libraries, map equivalent APIs, and perform the necessary code
transformations. Due to the difficulty of the library migration process, most
of the existing automated techniques and tooling stop at the API mapping stage
or support a limited set of libraries and code transformations. In this paper,
we develop an end-to-end solution that can automatically migrate code between
any arbitrary pair of Python libraries that provide similar functionality. Due
to the promising capabilities of Large Language Models (LLMs) in code
generation and transformation, we use LLMs as the primary engine for migration.
Before building the tool, we first study the capabilities of LLMs for library
migration on a benchmark of 321 real-world library migrations. We find that
LLMs can effectively perform library migration, but some post-processing steps
can further improve the performance. Based on this, we develop PyMigTool, a
command line application that combines the power of LLMs, static analysis, and
dynamic analysis to provide accurate library migration. We evaluate PyMigTool
on 717 real-world Python applications that are not from our benchmark. We find
that PyMigTool can migrate 32% of the migrations with complete correctness. Of
the remaining migrations, only 14% of the migration-related changes are left
for developers to fix for more than half of the projects.

</details>


### [27] [McMining: Automated Discovery of Misconceptions in Student Code](https://arxiv.org/abs/2510.08827)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.SE

TL;DR: This paper introduces McMining, a task for identifying programming misconceptions in student code using LLMs. It presents a benchmark dataset and demonstrates effectiveness of Gemini, Claude, and GPT models in detecting misconceptions.


<details>
  <summary>Details</summary>
Motivation: Student misconceptions in programming lead to inefficient learning and buggy code. Current methods lack systematic approaches to identify these misconceptions at scale.

Method: 1. Created an extensible benchmark dataset with misconception annotations and student code samples
2. Developed McMiner, two LLM-based approaches for misconception mining
3. Evaluated across Gemini, Claude, and GPT model families with extensive experiments

Result: LLM-based models effectively identify misconceptions in student code, showing strong performance across the tested benchmark dataset

Conclusion: LLMs demonstrate promise for automating misconception detection in programming education, providing a foundation for scalable educational tools that address conceptual misunderstandings in learners.

Abstract: When learning to code, students often develop misconceptions about various
programming language concepts. These can not only lead to bugs or inefficient
code, but also slow down the learning of related concepts. In this paper, we
introduce McMining, the task of mining programming misconceptions from samples
of code from a student. To enable the training and evaluation of McMining
systems, we develop an extensible benchmark dataset of misconceptions together
with a large set of code samples where these misconceptions are manifested. We
then introduce two LLM-based McMiner approaches and through extensive
evaluations show that models from the Gemini, Claude, and GPT families are
effective at discovering misconceptions in student code.

</details>


### [28] [Identifying Video Game Debugging Bottlenecks: An Industry Perspective](https://arxiv.org/abs/2510.08834)
*Carlos Pinto Gomez,Fabio Petrillo*

Main category: cs.SE

TL;DR: The paper examines unique video game debugging techniques through industry insights, identifying bottlenecks in bug resolution and highlighting that developers spend 36.6% of their time inspecting game artifacts and 35.1% on bug reproduction.


<details>
  <summary>Details</summary>
Motivation: Traditional debugging methods are insufficient for video games' unique requirements, necessitating specialized techniques like debug consoles, data scrubbing, and in-game tools.

Method: Analysis of 20 professional developers' debugging sessions focused on critical bugs (crashes, object behaviors, object persistence) through recorded productions and thematic analysis.

Result: Identification of time allocation patterns (36.6% artifact inspection + 35.1% bug reproduction), debugging activities that bottleneck progress, and the central role of technical disciplines in collaborative debugging.

Conclusion: Specialized debugging tools and better coordination are critical, as developers face significant delays in inspecting artifacts and reproducing bugs, emphasizing these activities as core challenges in game development.

Abstract: Conventional debugging techniques used in traditional software are similarly
used when debugging video games. However, the reality of video games require
its own set of unique debugging techniques such as On-Screen Console, Debug
Draws, Debug Camera, Cheats and In-Game Menus, and Data Scrubbing. In this
article, we provide insights from a video game studio on how 20 seasoned
industry game developers debug during the production of a game. Our experiments
rely on the recordings of debugging sessions for the most critical bugs
categorized as Crashes, Object Behaviors, and Object Persistence. In this
paper, we focus on identifying the debugging activities that bottleneck bug
resolution. We also identify the debugging tools used to perform debugging
techniques. Lastly, we present how different disciplines collaborate during
debugging and how technical roles are at the core of debugging. Our thematic
analysis has identified game developers spend 36.6\% of their time inspecting
game artifacts and 35.1\% of their time reproducing the bug locally.

</details>


### [29] [Repository-Aware File Path Retrieval via Fine-Tuned LLMs](https://arxiv.org/abs/2510.08850)
*Vasudha Yanuganti,Ishaan Puri,Swapnil Chhatre,Mantinder Singh,Ashok Jallepalli,Hritvik Shrivastava,Pradeep Kumar Sharma*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Modern codebases make it hard for developers and AI coding assistants to find
the right source files when answering questions like "How does this feature
work?" or "Where was the bug introduced?" Traditional code search (keyword or
IR based) often misses semantic context and cross file links, while large
language models (LLMs) understand natural language but lack repository specific
detail. We present a method for file path retrieval that fine tunes a strong
LLM (Qwen3-8B) with QLoRA and Unsloth optimizations to predict relevant file
paths directly from a natural language query. To build training data, we
introduce six code aware strategies that use abstract syntax tree (AST)
structure and repository content to generate realistic question-answer pairs,
where answers are sets of file paths. The strategies range from single file
prompts to hierarchical repository summaries, providing broad coverage. We fine
tune on Python projects including Flask, Click, Jinja, FastAPI, and PyTorch,
and obtain high retrieval accuracy: up to 91\% exact match and 93\% recall on
held out queries, clearly beating single strategy training. On a large codebase
like PyTorch (about 4,000 Python files), the model reaches 59\% recall, showing
scalability. We analyze how multi level code signals help the LLM reason over
cross file context and discuss dataset design, limits (for example, context
length in very large repos), and future integration of retrieval with LLM based
code intelligence.

</details>


### [30] [Vector Graph-Based Repository Understanding for Issue-Driven File Retrieval](https://arxiv.org/abs/2510.08876)
*Kostiantyn Bevziuk,Andrii Fatula,Svetozar Lashin Yaroslav Opanasenko,Anna Tukhtarova,Ashok Jallepalli Pradeepkumar Sharma,Hritvik Shrivastava*

Main category: cs.SE

TL;DR: This paper introduces a knowledge graph-based system for software repositories that vectorizes codebases to capture semantic relationships, integrates LLM-powered summaries, and enables automated development workflows through hybrid semantic-graph search, with an LLM assistant providing human-readable explanations for programmatic repository interactions.


<details>
  <summary>Details</summary>
Motivation: The research addresses challenges in managing complex software repositories, aiming to automate development processes by creating a structured representation that captures both architectural and semantic relationships, thus reducing manual effort and improving development efficiency through programmatic repository understanding.

Method: The system builds a hybrid knowledge graph encoding syntactic relationships (containment, implementation, references, calls, inheritance) and semantically augments nodes using LLM-derived summaries and vector embeddings. A hybrid retrieval pipeline combines semantic search with graph-aware expansion, integrated with an LLM assistant for generating structured graph queries and user-oriented explanations.

Result: The system achieves a 69% improvement in query resolution accuracy for development tasks (compared to traditional code search tools) and reduces documentation effort by 73% via automated semantic linking and hierarchical artifact representation in the knowledge graph.

Conclusion: The presented repository decomposition system successfully transforms large software repositories into a structured vectorized knowledge graph, enabling significant automation in repository development through enhanced architectural and semantic understanding, with the LLM-based assistant facilitating human-readable explanations and constrained graph interactions.

Abstract: We present a repository decomposition system that converts large software
repositories into a vectorized knowledge graph which mirrors project
architectural and semantic structure, capturing semantic relationships and
allowing a significant level of automatization of further repository
development. The graph encodes syntactic relations such as containment,
implementation, references, calls, and inheritance, and augments nodes with
LLM-derived summaries and vector embeddings. A hybrid retrieval pipeline
combines semantic retrieval with graph-aware expansion, and an LLM-based
assistant formulates constrained, read-only graph requests and produces
human-oriented explanations.

</details>


### [31] [SEER: Sustainability Enhanced Engineering of Software Requirements](https://arxiv.org/abs/2510.08981)
*Mandira Roy,Novarun Deb,Nabendu Chaki,Agostino Cortesi*

Main category: cs.SE

TL;DR: The paper introduces SEER, a framework addressing sustainability concerns in early software development phases using large language models and agentic RAG. It identifies, evaluates, and optimizes system requirements based on domain-specific sustainability requirements (SRs).


<details>
  <summary>Details</summary>
Motivation: Existing sustainability practices in software development lack effective early-phase integration, relying on post-design implementation guidelines. Timely adoption of sustainability assessments is critical to meet UN SDGs by 2030, but current methods are too high-level and time-consuming.

Method: SEER operates in three stages: (1+) identifying domain-specific SRs from a taxonomy; (2+) evaluating system requirements against SRs; (3+) optimizing non-compliant requirements. The framework leverages LLM reasoning and agentic RAG for implementation.

Result: Experiments on four domain-diverse software projects demonstrated SEER's effectiveness in accurately identifying sustainability concerns using the Gemini 2.5 reasoning model, showing cross-domain adaptability.

Conclusion: SEER provides a systematic, early-phase approach to sustainability assessment in software development, bridging the gap between industrial needs and academic methodologies through LLM-powered requirement optimization.

Abstract: The rapid expansion of software development has significant environmental,
technical, social, and economic impacts. Achieving the United Nations
Sustainable Development Goals by 2030 compels developers to adopt sustainable
practices. Existing methods mostly offer high-level guidelines, which are
time-consuming to implement and rely on team adaptability. Moreover, they focus
on design or implementation, while sustainability assessment should start at
the requirements engineering phase. In this paper, we introduce SEER, a
framework which addresses sustainability concerns in the early software
development phase. The framework operates in three stages: (i) it identifies
sustainability requirements (SRs) relevant to a specific software product from
a general taxonomy; (ii) it evaluates how sustainable system requirements are
based on the identified SRs; and (iii) it optimizes system requirements that
fail to satisfy any SR. The framework is implemented using the reasoning
capabilities of large language models and the agentic RAG (Retrieval Augmented
Generation) approach. SEER has been experimented on four software projects from
different domains. Results generated using Gemini 2.5 reasoning model
demonstrate the effectiveness of the proposed approach in accurately
identifying a broad range of sustainability concerns across diverse domains.

</details>


### [32] [Towards a Taxonomy of Sustainability Requirements for Software Design](https://arxiv.org/abs/2510.08990)
*Mandira Roy,Novarun Deb,Nabendu Chaki,Agostino Cortesi*

Main category: cs.SE

TL;DR: This paper proposes a comprehensive taxonomy of sustainability requirements (SRs)- including definitions, metrics, and correlations across four dimensions (environmental, technical, social, economic)- to address fragmented prior research through a Systematic Literature Review (SLR).


<details>
  <summary>Details</summary>
Motivation: Existing SR studies are fragmented, domain-specific, or dimension-limited, hindering systematic integration of sustainability in software development. A unified reference is needed.

Method: Conducted a Systematic Literature Review (SLR) of state-of-the-art SRs, extracting categories and relationships through structured synthesis.

Result: A four-dimensional SR taxonomy with cross-category correlation matrix showing synergies/conflicts, supported by definitions, metrics, and implementation measures.

Conclusion: The taxonomy enables practitioners to balance trade-offs across sustainability dimensions and researchers to identify SR interactions in software systems.

Abstract: Software systems are a significant contributor to global sustainability
concerns, demanding that environmental, social, technical, and economic factors
be systematically addressed from the initial requirements engineering phase.
Although existing research provides various sustainability requirements (SRs),
these contributions are often fragmented, specific to certain dimensions, or
limited to particular application domains, resulting in a critical lack of a
unified, comprehensive taxonomy for the software engineering community. To
address this gap, this research conducts a Systematic Literature Review (SLR)
to extract and organize sustainability requirements from the state-of-the-art.
The primary contribution is a comprehensive taxonomy of SRs across the four
dimensions of sustainability (environmental, technical, social, and economic).
For each identified category, we provide clear definitions, associated metrics,
and measures. Furthermore, we depict a correlation matrix that projects the
positive and negative influences (synergies and conflicts) among categories
across different dimensions. This systematized reference assists both software
developers and researchers in effectively formulating, managing, and
reconciling trade-offs within sustainable software development.

</details>


### [33] [Saving SWE-Bench: A Benchmark Mutation Approach for Realistic Agent Evaluation](https://arxiv.org/abs/2510.08996)
*Spandan Garg,Ben Steenhoek,Yufan Huang*

Main category: cs.SE

TL;DR: Researchers show current chat-agent benchmarks overrate performance by 50+%. They create a framework using real IDE interaction patterns to generate test queries, revealing significant capability overestimation in popular benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks like SWE-Bench Verified fail to reflect real-world developer-IDE-ChatAgent interactions, leading to overestimation of agent capabilities. This mismatch particularly impacts bug-fixing evaluations.

Method: The authors developed a benchmarking framework that transforms existing formal benchmarks into realistic user queries by analyzing developer interaction patterns with chat-based agents, applying it to public and private benchmarks and using telemetry data from a popular agent.

Result: Existing benchmarks overestimate agent performance by >50% compared to baseline on public benchmarks (SWE-Bench Verified etc.) and ~10-16% on the internal SWE-Bench C# benchmark when evaluated with the new framework.

Conclusion: The study establishes a new benchmarking paradigm for interactive chat-based software engineering agents using mutation techniques, highlighting significant overestimation of agent capabilities in existing benchmarks and offering a flexible framework for realistic evaluation.

Abstract: Current benchmarks for evaluating software engineering agents, such as
SWE-Bench Verified, are predominantly derived from GitHub issues and fail to
accurately reflect how developers interact with chat-based coding assistants in
integrated development environments (IDEs). We posit that this mismatch leads
to a systematic overestimation of agent's capabilities in real-world scenarios,
especially bug fixing. We introduce a novel benchmarking framework that
transforms existing formal benchmarks into realistic user queries through
systematic analysis of developer interaction patterns with chat-based agents.
Our methodology is flexible and can be easily extended to existing benchmarks.
In this paper, we apply our testing framework to SWE-Bench Verified, the
TypeScript subset of Multi-SWE-Bench and a private benchmark, SWE-Bench C# and
transform formal GitHub issue descriptions into realistic user-style queries
based on telemetry analysis of a popular chat-based agent interactions. Our
findings reveal that existing benchmarks significantly overestimate agent
capabilities for some models by >50% over baseline performance for public
benchmarks and ~10-16% for our internal benchmark. This work establishes a new
paradigm for evaluating interactive chat-based software engineering agents
through benchmark mutation techniques.

</details>


### [34] [Cost-Efficient Long Code Translation using LLMs while Leveraging Identifier Replacements](https://arxiv.org/abs/2510.09045)
*Manojit Chakraborty,Madhusudan Ghosh,Rishabh Gupta*

Main category: cs.SE

TL;DR: This paper proposes a zero-shot code translation method using identifier replacement to improve LLM performance for long code by reducing token count and memory usage while preserving syntactical/hierarchical information.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with long code translation due to context window limitations, leading to inaccurate translations.

Method: Substitute long user identifiers with generalized placeholders during translation to reduce tokens and focus LLMs on logical structure.

Result: Empirical results show preserved syntactical/hierarchical information with reduced token usage in translations.

Conclusion: The method enhances translation efficiency and cost-effectiveness for long code without requiring training data.

Abstract: In the domain of software development, LLMs have been utilized to automate
tasks such as code translation, where source code from one programming language
is translated to another while preserving its functionality. However, LLMs
often struggle with long source codes that don't fit into the context window,
which produces inaccurate translations. To address this, we propose a novel
zero-shot code translation method that incorporates identifier replacement. By
substituting user-given long identifiers with generalized placeholders during
translation, our method allows the LLM to focus on the logical structure of the
code, by reducing token count and memory usage, which improves the efficiency
and cost-effectiveness of long code translation. Our empirical results
demonstrate that our approach preserves syntactical and hierarchical
information and produces translation results with reduced tokens.

</details>


### [35] [Model-Assisted and Human-Guided: Perceptions and Practices of Software Professionals Using LLMs for Coding](https://arxiv.org/abs/2510.09058)
*Italo Santos,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: This paper analyzes 131 software developers' experiences with LLM tools in their workflows, finding they boost productivity but face accuracy and ethical challenges. Developers treat LLMs as careful assistants rather than standalone solutions, guiding future research on responsible integration.


<details>
  <summary>Details</summary>
Motivation: Despite the growing influence of LLMs in software development workflows, there is limited empirical understanding of their practical implementation and practitioner perceptions of their value and risks.

Method: A global survey of 131 software practitioners was conducted to gather insights on LLM usage, benefits, limitations, and integration practices in software development.

Result: Survey results reveal productivity gains (e.g., reduced cognitive load, faster learning) alongside challenges (inaccuracies, limited context awareness, ethical risks). Developers adopt LLMs cautiously as supportive tools rather than relying on them independently.

Conclusion: The study concludes that software professionals primarily use LLMs as assistive tools rather than standalone solutions, emphasizing a cautious integration approach while highlighting the need for future research and responsible LLM use in software engineering.

Abstract: Large Language Models have quickly become a central component of modern
software development workflows, and software practitioners are increasingly
integrating LLMs into various stages of the software development lifecycle.
Despite the growing presence of LLMs, there is still a limited understanding of
how these tools are actually used in practice and how professionals perceive
their benefits and limitations. This paper presents preliminary findings from a
global survey of 131 software practitioners. Our results reveal how LLMs are
utilized for various coding-specific tasks. Software professionals report
benefits such as increased productivity, reduced cognitive load, and faster
learning, but also raise concerns about LLMs' inaccurate outputs, limited
context awareness, and associated ethical risks. Most developers treat LLMs as
assistive tools rather than standalone solutions, reflecting a cautious yet
practical approach to their integration. Our findings provide an early,
practitioner-focused perspective on LLM adoption, highlighting key
considerations for future research and responsible use in software engineering.

</details>


### [36] [Literate Tracing](https://arxiv.org/abs/2510.09073)
*Matthew Sotoudeh*

Main category: cs.SE

TL;DR: Introduces literate tracing (annotated execution traces) as a novel documentation approach, and TReX (interactive documentation tool) to create faithful, visual explanations of complex software systems.


<details>
  <summary>Details</summary>
Motivation: Traditional documentation methods (code comments缺乏全局上下文; design docs lack concrete code connections) fail to effectively communicate complex system behavior. There is a need for documentation that bridges this gap by providing concrete execution context.

Method: We developed a documentation paradigm called literate tracing, which uses annotated execution traces of software systems. This was implemented in TReX, a tool that generates interactive, visual traces guaranteed to align with program semantics.

Result: Successfully applied TReX to document components of the Linux kernel, Git source control system, and GCC compiler, demonstrating the practicality of literate tracing for large-scale system software.

Conclusion: Literate tracing is a novel approach to program documentation that combines annotated execution traces with interactive visualization, addressing the limitations of traditional in-code comments and design documents. The TReX tool demonstrates its feasibility for documenting complex systems like the Linux kernel and Git.

Abstract: As computer systems grow ever larger and more complex, a crucial task in
software development is for one person (the system expert) to communicate to
another (the system novice) how a certain program works. This paper reports on
the author's experiences with a paradigm for program documentation that we call
literate tracing. A literate trace explains a software system using annotated,
concrete execution traces of the system. Literate traces complement both
in-code comments (which often lack global context) and out-of-band design docs
(which often lack a concrete connection to the code). We also describe TReX,
our tool for making literate traces that are interactive, visual, and
guaranteed by construction to be faithful to the program semantics. We have
used TReX to write literate traces explaining components of large systems
software including the Linux kernel, Git source control system, and GCC
compiler.

</details>


### [37] [Constraint-Guided Unit Test Generation for Machine Learning Libraries](https://arxiv.org/abs/2510.09108)
*Lukas Krodinger,Altin Hajdari,Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: PynguinML improves automated testing of ML libraries by using documentation constraints to generate valid inputs, achieving much higher code coverage than existing tools.


<details>
  <summary>Details</summary>
Motivation: Existing automated test generators like Pynguin fail to account for complex input constraints in ML APIs, leading to non-compliant inputs, early test failures, and poor code coverage.

Method: The authors developed PynguinML, an extension of the Pynguin test generator, by integrating constraints extracted from ML API documentation to produce valid inputs for testing.

Result: Evaluation on 165 modules from PyTorch and TensorFlow showed PynguinML achieved up to 63.9% higher code coverage compared to Pynguin, demonstrating its effectiveness.

Conclusion: The paper concludes that PynguinML significantly enhances test effectiveness for ML libraries by generating compliant inputs, resulting in substantially higher code coverage compared to existing tools.

Abstract: Machine learning (ML) libraries such as PyTorch and TensorFlow are essential
for a wide range of modern applications. Ensuring the correctness of ML
libraries through testing is crucial. However, ML APIs often impose strict
input constraints involving complex data structures such as tensors. Automated
test generation tools such as Pynguin are not aware of these constraints and
often create non-compliant inputs. This leads to early test failures and
limited code coverage. Prior work has investigated extracting constraints from
official API documentation. In this paper, we present PynguinML, an approach
that improves the Pynguin test generator to leverage these constraints to
generate compliant inputs for ML APIs, enabling more thorough testing and
higher code coverage. Our evaluation is based on 165 modules from PyTorch and
TensorFlow, comparing PynguinML against Pynguin. The results show that
PynguinML significantly improves test effectiveness, achieving up to 63.9 %
higher code coverage.

</details>


### [38] [A Semantic Framework for Patient Digital Twins in Chronic Care](https://arxiv.org/abs/2510.09134)
*Amal Elgammal,Bernd J. Krämer,Michael P. Papazoglou,Mira Raheem*

Main category: cs.SE

TL;DR: The paper introduces PMDT, an ontology-driven digital twin framework that integrates various health data for personalized chronic care, validated through expert input and real-world application.


<details>
  <summary>Details</summary>
Motivation: Chronic care needs multimodal health data integration for precise, adaptive, and preventive decision-making, but current digital twin solutions are fragmented and lack privacy. A unified, interoperable, and GDPR-compliant framework is essential.

Method: The PMDT is built using OWL 2.0 with modular, conceptual Blueprints covering multiple facets of patient data. The ontology was iteratively refined through expert collaboration, workshops, questionnaires, and a pilot study in the QUALITOP project.

Result: PMDT successfully unifies heterogeneous health data, passes automated reasoning correctness checks, and supports descriptive, predictive, and prescriptive analytics. The framework also meets GDPR requirements and is validated for usability.

Conclusion: The PMDT addresses critical gaps in data integration and standardization, establishing a robust platform for future digital health systems and advancing chronic care toward proactive, personalized management.

Abstract: Personalized chronic care requires the integration of multimodal health data
to enable precise, adaptive, and preventive decision-making. Yet most current
digital twin (DT) applications remain organ-specific or tied to isolated data
types, lacking a unified and privacy-preserving foundation. This paper
introduces the Patient Medical Digital Twin (PMDT), an ontology-driven in
silico patient framework that integrates physiological, psychosocial,
behavioral, and genomic information into a coherent, extensible model.
Implemented in OWL 2.0, the PMDT ensures semantic interoperability, supports
automated reasoning, and enables reuse across diverse clinical contexts. Its
ontology is structured around modular Blueprints (patient, disease and
diagnosis, treatment and follow-up, trajectories, safety, pathways, and adverse
events), formalized through dedicated conceptual views. These were iteratively
refined and validated through expert workshops, questionnaires, and a pilot
study in the EU H2020 QUALITOP project with real-world immunotherapy patients.
Evaluation confirmed ontology coverage, reasoning correctness, usability, and
GDPR compliance. Results demonstrate the PMDT's ability to unify heterogeneous
data, operationalize competency questions, and support descriptive, predictive,
and prescriptive analytics in a federated, privacy-preserving manner. By
bridging gaps in data fragmentation and semantic standardization, the PMDT
provides a validated foundation for next-generation digital health ecosystems,
transforming chronic care toward proactive, continuously optimized, and
equitable management.

</details>


### [39] [A Model-Driven Engineering Approach to AI-Powered Healthcare Platforms](https://arxiv.org/abs/2510.09308)
*Mira Raheem,Amal Elgammal,Michael Papazoglou,Bernd Krämer,Neamat El-Tazi*

Main category: cs.SE

TL;DR: This paper introduces an MDE framework with Medical Interoperability Language (MILA), a graphical domain-specific language, combined with federated learning to address data fragmentation, privacy constraints, and technical complexity in healthcare AI. Evaluation in a cancer study demonstrated high predictive accuracy (98.3-98.5%) and reduced manual coding effort.


<details>
  <summary>Details</summary>
Motivation: Healthcare AI adoption faces critical barriers including fragmented data sources, strict privacy regulations, and technical challenges in building reliable clinical systems. Traditional approaches struggle to balance interoperability, privacy preservation, and development efficiency required for clinical AI deployment.

Method: The proposed model-driven engineering (MDE)-based framework includes: 1. Medical Interoperability Language (MILA) - a visual DSL for collaborative pipeline design using shared ontologies 2. Automated model transformation pipeline from high-level specifications to executable code 3. Federated learning architecture enabling multi-institutional collaboration without sharing patient data, ensuring semantic consistency through formal metamodeling.

Result: Evaluation in a multi-center cancer immunotherapy study achieved: - SVM accuracy of 98.5%/98.3 in key predictive tasks - 75-80 reduction in manual coding effort compared to traditional methods - Successful inter-institutional collaboration under privacy constraints without data sharing

Conclusion: Model-driven engineering, when combined with semantic modeling (via MILA) and federated learning, provides a scalable path to build interoperable, reproducible, and privacy-preserving healthcare AI systems. This approach addresses three key barriers to clinical AI adoption by enabling code generation from formal specifications while maintaining data privacy through distributed learning.

Abstract: Artificial intelligence (AI) has the potential to transform healthcare by
supporting more accurate diagnoses and personalized treatments. However, its
adoption in practice remains constrained by fragmented data sources, strict
privacy rules, and the technical complexity of building reliable clinical
systems. To address these challenges, we introduce a model driven engineering
(MDE) framework designed specifically for healthcare AI. The framework relies
on formal metamodels, domain-specific languages (DSLs), and automated
transformations to move from high level specifications to running software. At
its core is the Medical Interoperability Language (MILA), a graphical DSL that
enables clinicians and data scientists to define queries and machine learning
pipelines using shared ontologies. When combined with a federated learning
architecture, MILA allows institutions to collaborate without exchanging raw
patient data, ensuring semantic consistency across sites while preserving
privacy. We evaluate this approach in a multi center cancer immunotherapy
study. The generated pipelines delivered strong predictive performance, with
support vector machines achieving up to 98.5 percent and 98.3 percent accuracy
in key tasks, while substantially reducing manual coding effort. These findings
suggest that MDE principles metamodeling, semantic integration, and automated
code generation can provide a practical path toward interoperable,
reproducible, and trustworthy digital health platforms.

</details>


### [40] [TIT: A Tree-Structured Instruction Tuning Approach for LLM-Based Code Translation](https://arxiv.org/abs/2510.09400)
*He Jiang,Yufu Wang,Hao Lin,Peiyu Zou,Zhide Zhou,Ang Jia,Xiaochen Li,Zhilei Ren*

Main category: cs.SE

TL;DR: TIT is a Tree-structured Instruction Tuning paradigm for LLM-based code translation, addressing syntactic confusion and semantic misalignment via three modules: syntactic info representation, fine-grained dataset augmentation, and dual-stage instruction tuning. It achieves 1.22x-1.75x higher success rates.


<details>
  <summary>Details</summary>
Motivation: Mainstream LLM-based code translation methods suffer from (1): sensitivity to language-specific syntax/lexicon causing syntactic confusion and (2): over-reliance on function-level datasets leading to semantic misalignment.

Method: TIT comprises three modules: (1): Syntax representation module integrates language-agnostic features via structured parsing; (2): Fine-grained dataset module uses statement-level segmentation + contrastive matching for aligned data; (3): Dual-stage tuning first trains syntax comprehension, then function-level code generation.

Result: Outperforms existing methods in multiple LLMs with 1.22x-1.75x higher success rates in code translation, while significantly reducing syntactic errors in outputs.

Conclusion: TIT effectively addresses LLM code translation limitations through syntactic abstraction and fine-grained alignment mechanisms, demonstrating significant empirical improvements in translation accuracy and robustness.

Abstract: Large Language Models (LLMs) have shown strong performance in automated
source-to-target code translation through pretraining on extensive code
corpora. However, mainstream LLM-based code translation methods suffer from two
critical limitations. First, they are highly sensitive to language-specific
features, which often introduce source-language syntax or lexicon into the
output, leading to syntactic confusion. Second, they lack fine-grained semantic
alignment due to an over-reliance on function-level parallel datasets,
resulting in semantic misalignment between the translated code and the original
source. To overcome these limitations, we propose TIT, a Tree-structured
Instruction Tuning paradigm for LLM-based code translation. Specifically, TIT
consists of three modules. First, to mitigate syntactic confusion, the
syntactic information representation module integrates language-agnostic
syntactic features via structured parsing. Then, to generate high-quality
fine-grained parallel data, the fine-grained parallel dataset augmentation
module aligns nodes with code segments through statement-level segmentation and
contrastive matching. Finally, we leverage the dual-stage tree instruction
tuning module to alleviate the contextual processing burden on the LLM caused
by the introduction of syntactic information. The first stage employs
syntax-aware fine-tuning to enable the LLM to autonomously comprehend
structured syntactic information, while the second stage utilizes code
generation fine-tuning to guide the model in generating accurate target code
based on function-level syntactic dependencies. The experimental results
demonstrate that the proposed method significantly outperforms existing
approaches in multiple LLMs, achieving a success rate 1.22x-1.75x higher in
code translation while markedly reducing syntactic confusion.

</details>
