{"id": "2509.19459", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.19459", "abs": "https://arxiv.org/abs/2509.19459", "authors": ["Yutong Guo", "Weiyu Luo", "Brian Demsky"], "title": "Automated Insertion of Flushes and Fences for Persistency", "comment": null, "summary": "CXL shared memory and persistent memory allow the contents of memory to\npersist beyond crashes. Stores to persistent or CXL memory are typically not\nimmediately made persistent; developers must manually flush the corresponding\ncache lines to force the data to be written to the underlying storage.\nCorrectly using flush and fence operations is known to be challenging. While\nstate-of-the-art tools can find missing flush instructions, they often require\nbug-revealing test cases. No existing tools can ensure the absence of missing\nflush bugs.\n  In this paper, we present PMRobust, a compiler that automatically inserts\nflush and fence operations to ensure that code using persistent memory is free\nfrom missing flush and fence bugs. PMRobust employs a novel static analysis\nwith optimizations that target newly allocated objects. We have evaluated\nPMRobust on persistent memory libraries and several persistent memory data\nstructures and measured a geometric mean overhead of 0.26% relative to the\noriginal benchmarks with hand-placed flush and fence operations.", "AI": {"tldr": "PMRobust is a compiler that automatically inserts missing flush and fence operations in persistent memory code, eliminating bugs with 0.26\\% performance overhead.", "motivation": "Manual insertion of flush/fence operations is error-prone, and existing tools require test cases or fail to guarantee correctness in persistent/memory programming.", "method": "Developed PMRobust using novel static analysis with object-lifetime optimizations to automatically insert necessary operations without developer input.", "result": "Achieved 0.26\\% geometric mean overhead across persistent memory libraries/data structures compared to manually optimized code.", "conclusion": "PMRobust demonstrates the first fully automated solution for eliminating flush/fence bugs while maintaining near-optimal performance."}}
{"id": "2509.19533", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.19533", "abs": "https://arxiv.org/abs/2509.19533", "authors": ["Mengdi Lu", "Steven Ding", "Furkan Alaca", "Philippe Charland"], "title": "Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided, Reasoning-Driven Input Mutation", "comment": null, "summary": "Security vulnerabilities in Internet-of-Things devices, mobile platforms, and\nautonomous systems remain critical. Traditional mutation-based fuzzers -- while\neffectively explore code paths -- primarily perform byte- or bit-level edits\nwithout semantic reasoning. Coverage-guided tools such as AFL++ use\ndictionaries, grammars, and splicing heuristics to impose shallow structural\nconstraints, leaving deeper protocol logic, inter-field dependencies, and\ndomain-specific semantics unaddressed. Conversely, reasoning-capable large\nlanguage models (LLMs) can leverage pretraining knowledge to understand input\nformats, respect complex constraints, and propose targeted mutations, much like\nan experienced reverse engineer or testing expert. However, lacking ground\ntruth for \"correct\" mutation reasoning makes supervised fine-tuning\nimpractical, motivating explorations of off-the-shelf LLMs via prompt-based\nfew-shot learning. To bridge this gap, we present an open-source microservices\nframework that integrates reasoning LLMs with AFL++ on Google's FuzzBench,\ntackling asynchronous execution and divergent hardware demands (GPU- vs.\nCPU-intensive) of LLMs and fuzzers. We evaluate four research questions: (R1)\nHow can reasoning LLMs be integrated into the fuzzing mutation loop? (R2) Do\nfew-shot prompts yield higher-quality mutations than zero-shot? (R3) Can prompt\nengineering with off-the-shelf models improve fuzzing directly? and (R4) Which\nopen-source reasoning LLMs perform best under prompt-only conditions?\nExperiments with Llama3.3, Deepseek-r1-Distill-Llama-70B, QwQ-32B, and Gemma3\nhighlight Deepseek as the most promising. Mutation effectiveness depends more\non prompt complexity and model choice than shot count. Response latency and\nthroughput bottlenecks remain key obstacles, offering directions for future\nwork.", "AI": {"tldr": "This work demonstrates how reasoning LLMs can enhance fuzzing by addressing semantic constraints, with Deepseek showing the most promise. A microservices framework integrates LLMs into AFL++ via prompts, but latency and hardware challenges remain. Prompt complexity and model choice matter more than few-shot examples.", "motivation": "Traditional fuzzers fail to address semantic dependencies, protocol logic, and domain-specific constraints in program inputs. While LLMs excel in semantic reasoning, their integration into fuzzing lacks ground truth for supervised learning, necessitating explorations of few-shot prompts. This motivates leveraging off-the-shelf LLMs to improve mutation quality in security testing.", "method": "The authors developed an open-source microservices framework to integrate reasoning LLMs (e.g., Deepseek, Llama3, Gemma3) with AFL++ on FuzzBench. They employed prompt-based few-shot learning to address the lack of ground truth for supervised training and evaluated the impact of prompt engineering on fuzzing effectiveness. Experiments focused on four research questions addressing integration, few-shot efficacy, prompt optimization, and model performance.", "result": "Experiments revealed that Deepseek outperformed Llama3, QwQ-32B, and Gemma3 in mutation effectiveness. Mutation quality was more influenced by prompt complexity and model architecture than by few-shot examples. However, latency and throughput limitations during LLM responses were identified as key obstacles to real-time fuzzing performance.", "conclusion": "The study concludes that integrating reasoning-capable large language models (LLMs) into fuzzing frameworks like AFL++ using prompt-based few-shot learning can enhance mutation quality. However, challenges such as response latency, throughput bottlenecks, and hardware divergences (GPU vs. CPU) remain significant barriers. Deepseek emerges as the most promising open-source model under these conditions, highlighting the importance of prompt complexity and model choice over shot count."}}
{"id": "2509.19587", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19587", "abs": "https://arxiv.org/abs/2509.19587", "authors": ["Mohamed Ouf", "Haoyu Li", "Michael Zhang", "Mariam Guizani"], "title": "Reverse Engineering User Stories from Code using Large Language Models", "comment": null, "summary": "User stories are essential in agile development, yet often missing or\noutdated in legacy and poorly documented systems. We investigate whether large\nlanguage models (LLMs) can automatically recover user stories directly from\nsource code and how prompt design impacts output quality. Using 1,750 annotated\nC++ snippets of varying complexity, we evaluate five state-of-the-art LLMs\nacross six prompting strategies. Results show that all models achieve, on\naverage, an F1 score of 0.8 for code up to 200 NLOC. Our findings show that a\nsingle illustrative example enables the smallest model (8B) to match the\nperformance of a much larger 70B model. In contrast, structured reasoning via\nChain-of-Thought offers only marginal gains, primarily for larger models.", "AI": {"tldr": "This study explores the use of large language models (LLMs) for automatically recovering user stories from C++ code, finding that smaller models can perform as well as larger ones with the right prompt design.", "motivation": "The need to understand legacy and poorly documented systems is critical for advancing the agile development practices, for which user stories play an essential role but are often missing or out-of-date. This motivates the exploration of LLMs for automatic recovery.", "method": "We tested five state-of-the-art large language models using six different prompting strategies on 1,750 annotated C++ snippets to evaluate their effectiveness in recovering user stories.", "result": "All evaluated models achieved an average F1 score of 0.8 when reconstructing user stories from code up to 200 NLOC. The results demonstrated that a single illustrative example can enable an 8B model to match the performance of a larger 70B model, while Chain-of-Thought reasoning provides only marginal improvement, particularly for larger models.", "conclusion": "This study indicates that prompt design has a significant impact on the performance of LLMs in the task of recovering user stories from C++ code. Smaller models can achieve similar effectiveness to larger models when using the appropriate prompting strategy."}}
{"id": "2509.19673", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.19673", "abs": "https://arxiv.org/abs/2509.19673", "authors": ["Ahmed Aljohani", "Anamul Haque Mollah", "Hyunsook Do"], "title": "Assertion Messages with Large Language Models (LLMs) for Code", "comment": "Accepted at Proceedings of the 2025 Evaluation and Assessment in\n  Software Engineering (EASE '25)", "summary": "Assertion messages significantly enhance unit tests by clearly explaining the\nreasons behind test failures, yet they are frequently omitted by developers and\nautomated test-generation tools. Despite recent advancements, Large Language\nModels (LLMs) have not been systematically evaluated for their ability to\ngenerate informative assertion messages. In this paper, we introduce an\nevaluation of four state-of-the-art Fill-in-the-Middle (FIM) LLMs -\nQwen2.5-Coder-32B, Codestral-22B, CodeLlama-13B, and StarCoder - on a dataset\nof 216 Java test methods containing developer-written assertion messages. We\nfind that Codestral-22B achieves the highest quality score of 2.76 out of 5\nusing a human-like evaluation approach, compared to 3.24 for manually written\nmessages. Our ablation study shows that including descriptive test comments\nfurther improves Codestral's performance to 2.97, highlighting the critical\nrole of context in generating clear assertion messages. Structural analysis\ndemonstrates that all models frequently replicate developers' preferred\nlinguistic patterns. We discuss the limitations of the selected models and\nconventional text evaluation metrics in capturing diverse assertion message\nstructures. Our benchmark, evaluation results, and discussions provide an\nessential foundation for advancing automated, context-aware generation of\nassertion messages in test code. A replication package is available at\nhttps://doi.org/10.5281/zenodo.15293133", "AI": {"tldr": "This paper evaluates four LLMs for generating assertion messages in Java test methods, finding Codestral-22B achieves 2.76/5 compared to manually written messages (3.24), with contextual comments improving performance to 2.97.", "motivation": "Developers and automated tools frequently omit assertion messages despite their value in explaining test failures. LLMs' potential for this task lacks systematic evaluation.", "method": "Evaluated Qwen2.5-Coder-32B, Codestral-22B, CodeLlama-13B, and StarCoder on a 216-method Java dataset with human-written assertion messages using human-like assessment and ablation studies.", "result": "Codestral-22B scored 2.76/5, outperforming other models. With contextual test comments, its score rose to 2.97. Models replicated developers' linguistic patterns, but metrics struggle with structural diversity.", "conclusion": "Establishes LLM evaluation benchmarks for assertion messages, highlights context importance, and identifies limitations in conventional metrics. Provides replication materials for future advancements."}}
{"id": "2509.19485", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19485", "abs": "https://arxiv.org/abs/2509.19485", "authors": ["Hafijul Hoque Chowdhury", "Riad Ahmed Anonto", "Sourov Jajodia", "Suryadipta Majumdar", "Md. Shohrab Hossain"], "title": "Identifying and Addressing User-level Security Concerns in Smart Homes Using \"Smaller\" LLMs", "comment": "10 pages, accepted at PST 2025", "summary": "With the rapid growth of smart home IoT devices, users are increasingly\nexposed to various security risks, as evident from recent studies. While\nseeking answers to know more on those security concerns, users are mostly left\nwith their own discretion while going through various sources, such as online\nblogs and technical manuals, which may render higher complexity to regular\nusers trying to extract the necessary information. This requirement does not go\nalong with the common mindsets of smart home users and hence threatens the\nsecurity of smart homes furthermore. In this paper, we aim to identify and\naddress the major user-level security concerns in smart homes. Specifically, we\ndevelop a novel dataset of Q&A from public forums, capturing practical security\nchallenges faced by smart home users. We extract major security concerns in\nsmart homes from our dataset by leveraging the Latent Dirichlet Allocation\n(LDA). We fine-tune relatively \"smaller\" transformer models, such as T5 and\nFlan-T5, on this dataset to build a QA system tailored for smart home security.\nUnlike larger models like GPT and Gemini, which are powerful but often resource\nhungry and require data sharing, smaller models are more feasible for\ndeployment in resource-constrained or privacy-sensitive environments like smart\nhomes. The dataset is manually curated and supplemented with synthetic data to\nexplore its potential impact on model performance. This approach significantly\nimproves the system's ability to deliver accurate and relevant answers, helping\nusers address common security concerns with smart home IoT devices. Our\nexperiments on real-world user concerns show that our work improves the\nperformance of the base models.", "AI": {"tldr": "The paper proposes a QA system using smaller transformer models to address smart home security concerns by analyzing user-generated datasets.", "motivation": "Rapid growth of smart home IoT devices exposes users to security risks, but existing resources are complex and inefficient models are unsuitable for privacy-sensitive environments.", "method": "Created a novel Q&A dataset from public forums, employed LDA for extracting security concerns, and fine-tuned T5/Flan-T5 models with manual curation and synthetic data.", "result": "The QA system improved upon base model performance in real-world user scenarios while maintaining better privacy and resource efficiency compared to large models like GPT.", "conclusion": "Smaller models enable effective, privacy-preserving smart home security solutions that align with user needs and resource constraints."}}
{"id": "2509.19708", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19708", "abs": "https://arxiv.org/abs/2509.19708", "authors": ["Anand Kumar", "Vishal Khare", "Deepak Sharma", "Satyam Kumar", "Vijay Saini", "Anshul Yadav", "Sachendra Jain", "Ankit Rana", "Pratham Verma", "Vaibhav Meena", "Avinash Edubilli"], "title": "Intuition to Evidence: Measuring AI's True Impact on Developer Productivity", "comment": "16 pages, 10 figures, 5 tables", "summary": "We present a comprehensive real-world evaluation of AI-assisted software\ndevelopment tools deployed at enterprise scale. Over one year, 300 engineers\nacross multiple teams integrated an in-house AI platform (DeputyDev) that\ncombines code generation and automated review capabilities into their daily\nworkflows. Through rigorous cohort analysis, our study demonstrates\nstatistically significant productivity improvements, including an overall 31.8%\nreduction in PR review cycle time.\n  Developer adoption was strong, with 85% satisfaction for code review features\nand 93% expressing a desire to continue using the platform. Adoption patterns\nshowed systematic scaling from 4% engagement in month 1 to 83% peak usage by\nmonth 6, stabilizing at 60% active engagement. Top adopters achieved a 61%\nincrease in code volume pushed to production, contributing to approximately 30\nto 40% of code shipped to production through this tool, accounting for an\noverall 28% increase in code shipment volume.\n  Unlike controlled benchmark evaluations, our longitudinal analysis provides\nempirical evidence from production environments, revealing both the\ntransformative potential and practical deployment challenges of integrating AI\ninto enterprise software development workflows.", "AI": {"tldr": "This paper evaluates enterprise-scale AI-assisted software development tools, demonstrating significant productivity gains (31.85% faster PR reviews), high adoption rates (60-83%), and varied code shipment increases (28%) through a longitudinal study of 300 engineers.", "motivation": "Existing benchmarks poorly represent real-world AI integration challenges in enterprise software development; this study provides empirical evidence from active production environments.", "method": "Longitudinal cohort analysis of 300 engineers across 12 months using DeputyDev (AI code generation + automated review), tracking PR review cycle time, platform engagement metrics, code volume, and self-reported satisfaction surveys.", "result": "85% code review satisfaction, 31.85% faster PR review cycle time, 4%\u219283\u219260 steady adoption rate, 61-28-30-40% increases in code shipping metrics for top adopters", "conclusion": "Real-world AI tool integration yields productivity gains but requires 6-month+ adoption growth, highlighting unique enterprise deployment challenges beyond controlled benchmark environments."}}
{"id": "2509.19568", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.19568", "abs": "https://arxiv.org/abs/2509.19568", "authors": ["Antoine Plin", "Lorenzo Casalino", "Thomas Rokicki", "Ruben Salvador"], "title": "Knock-Knock: Black-Box, Platform-Agnostic DRAM Address-Mapping Reverse Engineering", "comment": "Accepted in 2nd Microarchitecture Security Conference 2026 (uASC\n  '26), 17 pages, 8 figures, 3 tables, 1 algorithm, 1 appendix", "summary": "Modern Systems-on-Chip (SoCs) employ undocumented linear address-scrambling\nfunctions to obfuscate DRAM addressing, which complicates DRAM-aware\nperformance optimizations and hinders proactive security analysis of DRAM-based\nattacks; most notably, Rowhammer. Although previous work tackled the issue of\nreversing physical-to-DRAM mapping, existing heuristic-based\nreverse-engineering approaches are partial, costly, and impractical for\ncomprehensive recovery. This paper establishes a rigorous theoretical\nfoundation and provides efficient practical algorithms for black-box, complete\nphysical-to-DRAM address-mapping recovery.\n  We first formulate the reverse-engineering problem within a linear algebraic\nmodel over the finite field GF(2). We characterize the timing fingerprints of\nrow-buffer conflicts, proving a relationship between a bank addressing matrix\nand an empirically constructed matrix of physical addresses. Based on this\ncharacterization, we develop an efficient, noise-robust, and fully\nplatform-agnostic algorithm to recover the full bank-mask basis in polynomial\ntime, a significant improvement over the exponential search from previous\nworks. We further generalize our model to complex row mappings, introducing new\nhardware-based hypotheses that enable the automatic recovery of a row basis\ninstead of previous human-guided contributions.\n  Evaluations across embedded and server-class architectures confirm our\nmethod's effectiveness, successfully reconstructing known mappings and\nuncovering previously unknown scrambling functions. Our method provides a 99%\nrecall and accuracy on all tested platforms. Most notably, Knock-Knock runs in\nunder a few minutes, even on systems with more than 500GB of DRAM, showcasing\nthe scalability of our method. Our approach provides an automated, principled\npathway to accurate DRAM reverse engineering.", "AI": {"tldr": "This paper introduces **Knock-Knock**, a GF(2)-based linear algebra framework that enables efficient, platform-agnostic recovery of full physical-to-DRAM address mappings. By modeling row-buffer conflicts as timing fingerprints, it solves the reverse-engineering problem in polynomial time, uncovering undocumented scrambling functions while scaling to large systems.", "motivation": "Undocumented DRAM address scrambling in modern SoCs hinders performance optimizations and security analysis (e.g., Rowhammer). Previous reverse-engineering methods were incomplete, costly, and impractical for full recovery, necessitating a systematic and efficient solution.", "method": "The study formulates the problem as linear algebra over GF(2), leverages row-buffer timing fingerprints to construct a bank addressing matrix, and develops a polynomial-time algorithm for full bank-mask recovery. It extends this approach to complex mappings using hardware-based hypotheses for automatic row-basis extraction.", "result": "The method achieves 99% accuracy and recall on tested platforms, reconstructs known scrambling functions, discovers previously unknown mappings, and operates in sub-minute times on systems with >500GB DRAM. Evaluations on embedded and server-class architectures validate scalability and robustness.", "conclusion": "The paper presents a scalable and automated solution for DRAM address-scrambling recovery, offering a principled pathway to overcome previous limitations in reverse-engineering. The method achieves near-perfect accuracy and efficiency across diverse platforms, enabling proactive DRAM security analysis and optimization."}}
{"id": "2509.19918", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.19918", "abs": "https://arxiv.org/abs/2509.19918", "authors": ["Micheline B\u00e9n\u00e9dicte Moumoula", "Serge Lionel Nikiema", "Alb\u00e9rick Euraste Djire", "Abdoul Kader Kabore", "Jacques Klein", "Tegawend\u00e9 F. Bissyande"], "title": "Beyond Language Barriers: Multi-Agent Coordination for Multi-Language Code Generation", "comment": null, "summary": "Producing high-quality code across multiple programming languages is\nincreasingly important as today's software systems are built on heterogeneous\nstacks. Large language models (LLMs) have advanced the state of automated\nprogramming, yet their proficiency varies sharply between languages, especially\nthose with limited training data such as Rust, Perl, OCaml, and Erlang. Many\ncurrent solutions including language-specific fine-tuning, multi-agent\norchestration, transfer learning, and intermediate-representation pipelines\nstill approach each target language in isolation, missing opportunities to\nshare knowledge or exploit recurring cross-language patterns.\n  XL-CoGen tackles this challenge with a coordinated multi-agent architecture\nthat integrates intermediate representation, code generation, translation, and\nautomated repair. Its distinguishing feature is a data-driven mechanism for\nselecting bridging languages: empirically derived transfer matrices identify\nthe best intermediate languages based on demonstrated translation success\nrather than raw generation accuracy. The system performs early output\nvalidation, iteratively corrects errors, and reuses intermediate artifacts as\ncontextual scaffolds for subsequent translations.\n  Extensive experiments show that XL-CoGen yields notable improvements with 13\npercentage-point gains over the strongest fine-tuned baseline and as much as 30\npercentage points over existing single-language multi-agent methods. Ablation\nstudies further demonstrate that compatibility-guided bridging significantly\noutperforms LLM-based heuristics, confirming the value of cumulative\ncross-language knowledge transfer.", "AI": {"tldr": "XL-CoGen tackles cross-language code generation by using data-driven language bridging and multi-agent coordination, significantly improving performance across low-data programming languages through cumulative knowledge transfer.", "motivation": "Current methods treat target programming languages in isolation, missing opportunities to exploit cross-language patterns and sharing knowledge, especially in low-data languages like Rust or Perl.", "method": "A coordinated multi-agent architecture that integrates intermediate representation, translation, and automated repair, with empirically derived transfer matrices to select optimal bridging languages.", "result": "XL-CoGen achieves 13% improvement over fine-tuned baselines and 30% over single-language multi-agent methods. Ablation studies confirm the superiority of compatibility-guided bridging over LLM heuristics.", "conclusion": "XL-CoGen effectively improves cross-language code generation by leveraging empirical knowledge transfer through bridging languages and iterative error correction, demonstrating significant performance gains over existing methods."}}
{"id": "2509.19650", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.19650", "abs": "https://arxiv.org/abs/2509.19650", "authors": ["Dehinde Molade", "Dave Ormrod", "Mamello Thinyane", "Nalin Arachchilage", "Jill Slay"], "title": "SoK: A Systematic Review of Malware Ontologies and Taxonomies and Implications for the Quantum Era", "comment": "40 pages, 9 figures, 5 tables", "summary": "The threat of quantum malware is real and a growing security concern that\nwill have catastrophic scientific and technological impacts, if not addressed\nearly. If weaponised or exploited especially by the wrong hands, malware will\nundermine highly sophisticated critical systems supported by next-generation\nquantum architectures, for example, in defence, communications, energy, and\nspace. This paper explores the fundamental nature and implications of quantum\nmalware to enable the future development of appropriate mitigations and\ndefences, thereby protecting critical infrastructure. By conducting a\nsystematic literature review (SLR) that draws on knowledge frameworks such as\nontologies and taxonomies to explore malware, this provides insights into how\nmalicious behaviours can be translated into attacks on quantum technologies,\nthereby providing a lens to analyse the severity of malware against quantum\ntechnologies. This study employs the European Competency Framework for Quantum\nTechnologies (CFQT) as a guide to map malware behaviour to several competency\nlayers, creating a foundation in this emerging field.", "AI": {"tldr": "This paper analyzes quantum malware threats using systematic literature reviews and competency frameworks to define attack mechanisms and severity, laying groundwork for defenses in quantum infrastructure.", "motivation": "Quantum malware poses a severe threat to next-generation quantum systems in critical sectors such as defense and energy. Proactive research is imperative to mitigate these vulnerabilities and protect infrastructure from catastrophic consequences.", "method": "The paper employs a systematic literature review (SLR) and utilizes knowledge frameworks like ontologies, taxonomies, and the European Competency Framework for Quantum Technologies (CFQT) to map malware behaviors across competency layers.", "result": "The study provides insights into translating malicious behaviors into quantum-specific attacks, enabling the assessment of malware severity, and establishes a framework for understanding and addressing these threats through structured competency mapping.", "conclusion": "This study emphasizes the necessity of developing robust defenses against quantum malware to safeguard critical infrastructures, establishing a foundational framework for future mitigations through systematic analysis of malware behavior within quantum technology contexts."}}
{"id": "2509.20010", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20010", "abs": "https://arxiv.org/abs/2509.20010", "authors": ["Xiaoning Ren", "Yuhang Ye", "Xiongfei Wu", "Yueming Wu", "Yinxing Xue"], "title": "Demystifying the Evolution of Neural Networks with BOM Analysis: Insights from a Large-Scale Study of 55,997 GitHub Repositories", "comment": "11pages,8figures", "summary": "Neural networks have become integral to many fields due to their exceptional\nperformance. The open-source community has witnessed a rapid influx of neural\nnetwork (NN) repositories with fast-paced iterations, making it crucial for\npractitioners to analyze their evolution to guide development and stay ahead of\ntrends. While extensive research has explored traditional software evolution\nusing Software Bill of Materials (SBOMs), these are ill-suited for NN software,\nwhich relies on pre-defined modules and pre-trained models (PTMs) with distinct\ncomponent structures and reuse patterns. Conceptual AI Bills of Materials\n(AIBOMs) also lack practical implementations for large-scale evolutionary\nanalysis. To fill this gap, we introduce the Neural Network Bill of Material\n(NNBOM), a comprehensive dataset construct tailored for NN software. We create\na large-scale NNBOM database from 55,997 curated PyTorch GitHub repositories,\ncataloging their TPLs, PTMs, and modules. Leveraging this database, we conduct\na comprehensive empirical study of neural network software evolution across\nsoftware scale, component reuse, and inter-domain dependency, providing\nmaintainers and developers with a holistic view of its long-term trends.\nBuilding on these findings, we develop two prototype applications,\n\\textit{Multi repository Evolution Analyzer} and \\textit{Single repository\nComponent Assessor and Recommender}, to demonstrate the practical value of our\nanalysis.", "AI": {"tldr": "This paper introduces NNBOM, a specialized dataset for neural network software evolution, and develops tools for analyzing trends in 55,997 PyTorch repositories.", "motivation": "Neural network software evolves rapidly via community repositories, but existing SBOM and AIBOM approaches cannot capture their unique module reuse patterns and pre-trained model dependencies.", "method": "Created NNBOM by curating 55,997 PyTorch repos to catalog TPLs, PTMs, and modules, then conducted empirical analysis of scale, reuse, and dependencies through two prototype tools.", "result": "Revealed long-term evolutionary patterns in NN software across multi-repo and single-repo contexts, with practical applications demonstrating component assessment, recommendation, and trend analysis capabilities.", "conclusion": "NNBOM provides scalable analysis of neural network software evolution, enabling developers to strategically navigate module reuse and dependency trends through proven prototype tools."}}
{"id": "2509.19677", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.19677", "abs": "https://arxiv.org/abs/2509.19677", "authors": ["Michiharu Yamashita", "Thanh Tran", "Delvin Ce Zhang", "Dongwon Lee"], "title": "Unmasking Fake Careers: Detecting Machine-Generated Career Trajectories via Multi-layer Heterogeneous Graphs", "comment": "Accepted at EMNLP 2025 Main", "summary": "The rapid advancement of Large Language Models (LLMs) has enabled the\ngeneration of highly realistic synthetic data. We identify a new vulnerability,\nLLMs generating convincing career trajectories in fake resumes and explore\neffective detection methods. To address this challenge, we construct a dataset\nof machine-generated career trajectories using LLMs and various methods, and\ndemonstrate that conventional text-based detectors perform poorly on structured\ncareer data. We propose CareerScape, a novel heterogeneous, hierarchical\nmulti-layer graph framework that models career entities and their relations in\na unified global graph built from genuine resumes. Unlike conventional\nclassifiers that treat each instance independently, CareerScape employs a\nstructure-aware framework that augments user-specific subgraphs with trusted\nneighborhood information from a global graph, enabling the model to capture\nboth global structural patterns and local inconsistencies indicative of\nsynthetic career paths. Experimental results show that CareerScape outperforms\nstate-of-the-art baselines by 5.8-85.0% relatively, highlighting the importance\nof structure-aware detection for machine-generated content.", "AI": {"tldr": "This paper introduces CareerScape, a graph-based framework that detects LLM-generated fake resumes by analyzing structural patterns in career data, significantly outperforming text-only detectors.", "motivation": "The study addresses the vulnerability of LLMs generating convincing fake career data in resumes and highlights the inadequacy of conventional text-based detectors when applied to structured career trajectories.", "method": "The authors construct a dataset of LLM-generated career trajectories and propose CareerScape, a heterogeneous hierarchical multi-layer graph framework. It models career entities and relations in a global graph, augmenting subgraphs with trusted neighborhood information to capture both global structural patterns and local inconsistencies in fake resumes.", "result": "CareerScape outperforms state-of-the-art baselines by 5.8-85.0%, demonstrating its effectiveness in detecting synthetic career data compared to existing methods that fail to detect structured fake information.", "conclusion": "The paper concludes that structure-aware detection methods, particularly using graph frameworks like CareerScape, are crucial for identifying synthetic career trajectories in structured data, as traditional text-based detectors fall short in capturing global patterns and inconsistencies."}}
{"id": "2509.20136", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20136", "abs": "https://arxiv.org/abs/2509.20136", "authors": ["Wei Zhang", "Jack Yang", "Renshuai Tao", "Lingzheng Chai", "Shawn Guo", "Jiajun Wu", "Xiaoming Chen", "Ganqu Cui", "Ning Ding", "Xander Xu", "Hu Wei", "Bowen Zhou"], "title": "V-GameGym: Visual Game Generation for Code Large Language Models", "comment": null, "summary": "Code large language models have demonstrated remarkable capabilities in\nprogramming tasks, yet current benchmarks primarily focus on single modality\nrather than visual game development. Most existing code-related benchmarks\nevaluate syntax correctness and execution accuracy, overlooking critical\ngame-specific metrics such as playability, visual aesthetics, and user\nengagement that are essential for real-world deployment. To address the gap\nbetween current LLM capabilities in algorithmic problem-solving and competitive\nprogramming versus the comprehensive requirements of practical game\ndevelopment, we present V-GameGym, a comprehensive benchmark comprising 2,219\nhigh-quality samples across 100 thematic clusters derived from real-world\nrepositories, adopting a novel clustering-based curation methodology to ensure\nboth diversity and structural completeness. Further, we introduce a multimodal\nevaluation framework with an automated LLM-driven pipeline for visual code\nsynthesis using complete UI sandbox environments. Our extensive analysis\nreveals that V-GameGym effectively bridges the gap between code generation\naccuracy and practical game development workflows, providing quantifiable\nquality metrics for visual programming and interactive element generation.", "AI": {"tldr": "V-GameGym is a new benchmark for visual game development that addresses the gap between LLM capabilities in single-modality code tasks and requirements for real-world game development, combining 2,219 samples with a multimodal evaluation framework.", "motivation": "Current code LLM benchmarks prioritize syntax/accuracy metrics, neglecting playability, aesthetics, and user engagement critical for game development. Existing evaluations fail to address practical deployment needs in interactive digital experiences.", "method": "Created V-GameGym with 2,219 samples from 100 thematic clusters using clustering-based curation to ensure diversity. Deployed multimodal evaluation framework combining automated LLM-driven visual code synthesis pipeline with UI sandbox environments for comprehensive assessment.", "result": "V-GameGym demonstrates effectiveness in bridging code generation accuracy with practical game workflows, quantifying visual programming quality and interactive element generation performance across 100 thematic clusters.", "conclusion": "The benchmark successfully bridges the gap between academic LLM evaluations and real-world game development requirements by introducing specialized metrics for visual programming, user interaction, and systemic quality in digital game contexts."}}
{"id": "2509.19947", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19947", "abs": "https://arxiv.org/abs/2509.19947", "authors": ["Zhixiao Wu", "Yao Lu", "Jie Wen", "Hao Sun", "Qi Zhou", "Guangming Lu"], "title": "A Set of Generalized Components to Achieve Effective Poison-only Clean-label Backdoor Attacks with Collaborative Sample Selection and Triggers", "comment": "31 pages, 16 figures, accepted in Neurips 2025", "summary": "Poison-only Clean-label Backdoor Attacks aim to covertly inject\nattacker-desired behavior into DNNs by merely poisoning the dataset without\nchanging the labels. To effectively implant a backdoor, multiple\n\\textbf{triggers} are proposed for various attack requirements of Attack\nSuccess Rate (ASR) and stealthiness. Additionally, sample selection enhances\nclean-label backdoor attacks' ASR by meticulously selecting ``hard'' samples\ninstead of random samples to poison. Current methods 1) usually handle the\nsample selection and triggers in isolation, leading to severely limited\nimprovements on both ASR and stealthiness. Consequently, attacks exhibit\nunsatisfactory performance on evaluation metrics when converted to PCBAs via a\nmere stacking of methods. Therefore, we seek to explore the bidirectional\ncollaborative relations between the sample selection and triggers to address\nthe above dilemma. 2) Since the strong specificity within triggers, the simple\ncombination of sample selection and triggers fails to substantially enhance\nboth evaluation metrics, with generalization preserved among various attacks.\nTherefore, we seek to propose a set of components to significantly improve both\nstealthiness and ASR based on the commonalities of attacks. Specifically,\nComponent A ascertains two critical selection factors, and then makes them an\nappropriate combination based on the trigger scale to select more reasonable\n``hard'' samples for improving ASR. Component B is proposed to select samples\nwith similarities to relevant trigger implanted samples to promote\nstealthiness. Component C reassigns trigger poisoning intensity on RGB colors\nthrough distinct sensitivity of the human visual system to RGB for higher ASR,\nwith stealthiness ensured by sample selection, including Component B.\nFurthermore, all components can be strategically integrated into diverse PCBAs.", "AI": {"tldr": "This paper proposes a collaborative framework for Poison-only Clean-label Backdoor Attacks (PCBAs) to enhance Attack Success Rate (ASR), stealthiness, and generalizability by integrating sample selection and trigger mechanisms through three core components.", "motivation": "Existing PCBAs handle sample selection and triggers in isolation, leading to suboptimal ASR/stealthiness performance and poor metric consistency when stacking methods. Coordinated optimization is lacking.", "method": "The framework introduces: (A) a trigger-adaptive sample selector using scale-aware factors to prioritize hard samples; (B) similarity-based poisoning with trigger-embedded samples; and (C)c RGB-aware trigger intensity reassignment based on human visual sensitivity. Components are integrated for joint optimization.", "result": "Systematic evaluation shows significant improvements in ASR/stealthiness metrics across PCBAs compared to isolated methods, with preserved generalizability across attack configurations.", "conclusion": "Collaborative design of sample selection and triggers through modular components achieves superior backdoor attack performance. The approach establishes a foundational methodology for developing more effective and stealthy PCBAs."}}
{"id": "2509.20149", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20149", "abs": "https://arxiv.org/abs/2509.20149", "authors": ["Jianzhang Zhang", "Jialong Zhou", "Nan Niu", "Chuang Liu"], "title": "Enhancing Requirement Traceability through Data Augmentation Using Large Language Models", "comment": null, "summary": "Requirements traceability is crucial in software engineering to ensure\nconsistency between requirements and code. However, existing automated\ntraceability methods are constrained by the scarcity of training data and\nchallenges in bridging the semantic gap between artifacts. This study aims to\naddress the data scarcity problem in requirements traceability by employing\nlarge language models (LLMs) for data augmentation. We propose a novel approach\nthat utilizes prompt-based techniques with LLMs to generate augmented\nrequirement-to-code trace links, thereby enhancing the training dataset. Four\nLLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, and GPT-4) were used, employing both\nzero-shot and few-shot templates. Moreover, we optimized the encoder component\nof the tracing model to improve its efficiency and adaptability to augmented\ndata. The key contributions of this paper are: (1) proposing and evaluating\nfour prompt templates for data augmentation; (2) providing a comparative\nanalysis of four LLMs for generating trace links; (3) enhancing the model's\nencoder for improved adaptability to augmented datasets. Experimental results\nshow that our approach significantly enhances model performance, achieving an\nF1 score improvement of up to 28.59%, thus demonstrating its effectiveness and\npotential for practical application.", "AI": {"tldr": "This paper proposes using LLMs for data augmentation in requirements traceability, achieving a 28.59% F1 improvement by optimizing encoders and evaluating multiple LLMs under different prompting scenarios.", "motivation": "Existing automated traceability methods struggle with data scarcity and bridging semantic gaps between artifacts. This study aims to address these limitations by leveraging large language models (LLMs) for data augmentation.", "method": "The study employed prompt-based techniques with four LLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, GPT-4) under zero-shot and few-shot scenarios for data augmentation, and optimized the encoder component of the tracing model for efficiency and adaptability.", "result": "The proposed method achieved an F1 score improvement of up to 28.59%, demonstrating significant enhancement in model performance.", "conclusion": "The approach using LLMs for data augmentation in requirements traceability is effective and has potential for practical application."}}
{"id": "2509.20166", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20166", "abs": "https://arxiv.org/abs/2509.20166", "authors": ["Lauren Deason", "Adam Bali", "Ciprian Bejean", "Diana Bolocan", "James Crnkovich", "Ioana Croitoru", "Krishna Durai", "Chase Midler", "Calin Miron", "David Molnar", "Brad Moon", "Bruno Ostarcevic", "Alberto Peltea", "Matt Rosenberg", "Catalin Sandu", "Arthur Saputkin", "Sagar Shah", "Daniel Stan", "Ernest Szocs", "Shengye Wan", "Spencer Whitman", "Sven Krasser", "Joshua Saxe"], "title": "CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and Threat Intelligence Reasoning", "comment": null, "summary": "Today's cyber defenders are overwhelmed by a deluge of security alerts,\nthreat intelligence signals, and shifting business context, creating an urgent\nneed for AI systems to enhance operational security work. While Large Language\nModels (LLMs) have the potential to automate and scale Security Operations\nCenter (SOC) operations, existing evaluations do not fully assess the scenarios\nmost relevant to real-world defenders. This lack of informed evaluation impacts\nboth AI developers and those applying LLMs to SOC automation. Without clear\ninsight into LLM performance in real-world security scenarios, developers lack\na north star for development, and users cannot reliably select the most\neffective models. Meanwhile, malicious actors are using AI to scale cyber\nattacks, highlighting the need for open source benchmarks to drive adoption and\ncommunity-driven improvement among defenders and model developers. To address\nthis, we introduce CyberSOCEval, a new suite of open source benchmarks within\nCyberSecEval 4. CyberSOCEval includes benchmarks tailored to evaluate LLMs in\ntwo tasks: Malware Analysis and Threat Intelligence Reasoning--core defensive\ndomains with inadequate coverage in current benchmarks. Our evaluations show\nthat larger, more modern LLMs tend to perform better, confirming the training\nscaling laws paradigm. We also find that reasoning models leveraging test time\nscaling do not achieve the same boost as in coding and math, suggesting these\nmodels have not been trained to reason about cybersecurity analysis, and\npointing to a key opportunity for improvement. Finally, current LLMs are far\nfrom saturating our evaluations, showing that CyberSOCEval presents a\nsignificant challenge for AI developers to improve cyber defense capabilities.", "AI": {"tldr": "This paper introduces CyberSOCEval, a new open-source benchmark suite to evaluate LLMs in critical cybersecurity tasks, revealing gaps in model performance and opportunities for improvement in SOC automation.", "motivation": "The motivation stems from the gap in existing evaluations of LLMs for real-world Security Operations Center (SOC) scenarios, which hinders effective AI development and deployment for cybersecurity defense.", "method": "The authors introduced CyberSOCEval, an open-source benchmark suite under CyberSecEval 4, designed to evaluate LLM performance in Malware Analysis and Threat Intelligence Reasoning tasks.", "result": "Results show larger, newer LLMs perform better, but reasoning models leveraging test-time scaling fail to match scaling gains in coding/math tasks, indicating a lack of cybersecurity-specific training.", "conclusion": "The paper concludes that there is a significant challenge in the cybersecurity domain for AI developers to improve cyber defense capabilities, as current LLMs have not fully saturated the introduced benchmark, CyberSOCEval."}}
{"id": "2509.20172", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20172", "abs": "https://arxiv.org/abs/2509.20172", "authors": ["Daniel Maninger", "Leon Chemnitz", "Amir Molzam Sharifloo", "Jannis Brugger", "Mira Mezini"], "title": "Benchmarking Web API Integration Code Generation", "comment": "To be published in Proceedings of 2nd ACM International Conference on\n  AI-powered Software, Benchmark & Dataset Track (AIware '25)", "summary": "API integration is a cornerstone of our digital infrastructure, enabling\nsoftware systems to connect and interact. However, as shown by many studies,\nwriting or generating correct code to invoke APIs, particularly web APIs, is\nchallenging. Although large language models~(LLMs) have become popular in\nsoftware development, their effectiveness in automating the generation of web\nAPI integration code remains unexplored. In order to address this, we present a\ndataset and evaluation pipeline designed to assess the ability of LLMs to\ngenerate web API invocation code. Our experiments with several open-source LLMs\nreveal that generating API invocations poses a significant challenge, resulting\nin hallucinated endpoints, incorrect argument usage, and other errors. None of\nthe evaluated open-source models were able to solve more than 40% of the tasks.", "AI": {"tldr": "This paper introduces a dataset and evaluation pipeline to assess large language models' ability to generate correct web API integration code. Experiments show that open-source LLMs struggle with this task, achieving a maximum success rate of 40%.", "motivation": "The authors aim to study the effectiveness of LLMs in web API integration tasks, a critical area where manual coding and existing automated tools often face challenges. This is driven by the increasing adoption of LLMs in software development but the lack of understanding of their capability in API integration domain.", "method": "The research develops an evaluation framework comprising a dataset of API integration tasks and a pipeline to assess LLMs performance in generating correct code. The dataset includes a variety of web API integration scenarios. The pipeline evaluates the generated code based on correctness criteria, tracking metrics like success rate and error types.", "result": "The empirical results show that current open-source LLMs (as of the study's completion date) face significant challenges in generating correct web API integration code, with none exceeding 40% task completion success rate. Common issues include generating hallucinated endpoints, incorrect argument usage, and other code functionality errors.", "conclusion": "The paper concludes that while LLMs are becoming popular in software development, their effectiveness in web API integration is limited. This highlights the need for domain-specific knowledge and training to improve API code generation by LLMs."}}
{"id": "2509.20190", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20190", "abs": "https://arxiv.org/abs/2509.20190", "authors": ["Tanmay Khule", "Stefan Marksteiner", "Jose Alguindigue", "Hannes Fuchs", "Sebastian Fischmeister", "Apurva Narayan"], "title": "STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test Generation", "comment": "18 pages, 2 figures, accepted for 23rd escar Europe (Nov 05-06, 2025,\n  Frankfurt, Germany)", "summary": "In modern automotive development, security testing is critical for\nsafeguarding systems against increasingly advanced threats. Attack trees are\nwidely used to systematically represent potential attack vectors, but\ngenerating comprehensive test cases from these trees remains a labor-intensive,\nerror-prone task that has seen limited automation in the context of testing\nvehicular systems. This paper introduces STAF (Security Test Automation\nFramework), a novel approach to automating security test case generation.\nLeveraging Large Language Models (LLMs) and a four-step self-corrective\nRetrieval-Augmented Generation (RAG) framework, STAF automates the generation\nof executable security test cases from attack trees, providing an end-to-end\nsolution that encompasses the entire attack surface. We particularly show the\nelements and processes needed to provide an LLM to actually produce sensible\nand executable automotive security test suites, along with the integration with\nan automated testing framework. We further compare our tailored approach with\ngeneral purpose (vanilla) LLMs and the performance of different LLMs (namely\nGPT-4.1 and DeepSeek) using our approach. We also demonstrate the method of our\noperation step-by-step in a concrete case study. Our results show significant\nimprovements in efficiency, accuracy, scalability, and easy integration in any\nworkflow, marking a substantial advancement in automating automotive security\ntesting methodologies. Using TARAs as an input for verfication tests, we create\nsynergies by connecting two vital elements of a secure automotive development\nprocess.", "AI": {"tldr": "This paper introduces STAF, a security test automation framework using LLMs and RAG to generate executable automotive security test cases from attack trees, improving efficiency and accuracy in automotive security testing.", "motivation": "Manual security test case generation from attack trees in automotive systems is labor-intensive, error-prone, and lacks sufficient automation, necessitating scalable solutions to address advanced cyber threats.", "method": "STAF employs a four-step self-corrective Retrieval-Augmented Generation framework with LLMs (GPT-4.1, DeepSeek) to automate test case generation from attack trees, integrated with automated testing frameworks and tailored for automotive contexts through concrete case studies.", "result": "STAF demonstrates significant improvements in efficiency, accuracy, scalability, and workflow integration compared to vanilla LLMs, validated via case studies and performance comparisons against different LLM models.", "conclusion": "STAF represents a substantial advancement in automotive security testing automation by bridging attack tree analysis and executable test generation, fostering synergies between threat assessment and validation through TARAs."}}
{"id": "2509.20215", "categories": ["cs.SE", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.20215", "abs": "https://arxiv.org/abs/2509.20215", "authors": ["Guang Yang", "Wei Zheng", "Xiang Chen", "Yifan Sun", "Fengji Zhang", "Terry Yue Zhuo"], "title": "The Cream Rises to the Top: Efficient Reranking Method for Verilog Code Generation", "comment": "Under review ICASSP 2026", "summary": "LLMs face significant challenges in Verilog generation due to limited\ndomain-specific knowledge. While sampling techniques improve pass@k metrics,\nhardware engineers need one trustworthy solution rather than uncertain\ncandidates. To bridge this gap, we formulate it as a semantic alignment problem\nbetween requirements and Verilog implementations, and propose VCD-RNK, a\ndiscriminator model tailored for efficient Verilog code reranking.\nSpecifically, VCD-RNKincorporates Verilog-specific reasoning by distilling\nexpert knowledge across three dimensions: code semantic analysis, test case\ngeneration, and functional correctness assessment. By explicitly simulating the\nabove reasoning processes during inference, VCD-RNK effectively avoids\ncomputationally intensive test execution in existing methods.", "AI": {"tldr": "This paper proposes VCD-RNK, a discriminator model for efficient Verilog code reranking that bridges the gap between domain requirements and implementations by leveraging expert knowledge distillation across semantic analysis, test case generation, and functional correctness assessment.", "motivation": "LLMs struggle with Verilog generation due to limited domain knowledge. Existing sampling techniques prioritize pass@k metrics but hardware engineers require deterministic correctness, motivating a shift toward semantic alignment between requirements and code.", "method": "The authors formulate the problem as semantic alignment and design VCD-RNK, which distills expert knowledge through three dimensions: (1)\u00a0code semantic analysis, (2)\u00a0test case generation, and (3)\u00a0functional correctness assessment, while avoiding test execution through explicit reasoning during inference.", "result": "VCD-RNK enables efficient reranking without computationally expensive test execution, though specific quantitative results are omitted in the abstract.", "conclusion": "By transforming Verilog generation into an expert-anchored semantic alignment problem, VCD-RNK provides a practical solution for hardware engineers seeking trustworthy code outputs rather than probabilistic candidates."}}
{"id": "2509.20277", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20277", "abs": "https://arxiv.org/abs/2509.20277", "authors": ["Xiaofan Li", "Xing Gao"], "title": "Investigating Security Implications of Automatically Generated Code on the Software Supply Chain", "comment": null, "summary": "In recent years, various software supply chain (SSC) attacks have posed\nsignificant risks to the global community. Severe consequences may arise if\ndevelopers integrate insecure code snippets that are vulnerable to SSC attacks\ninto their products. Particularly, code generation techniques, such as large\nlanguage models (LLMs), have been widely utilized in the developer community.\nHowever, LLMs are known to suffer from inherent issues when generating code,\nincluding fabrication, misinformation, and reliance on outdated training data,\nall of which can result in serious software supply chain threats. In this\npaper, we investigate the security threats to the SSC that arise from these\ninherent issues. We examine three categories of threats, including eleven\npotential SSC-related threats, related to external components in source code,\nand continuous integration configuration files. We find some threats in\nLLM-generated code could enable attackers to hijack software and workflows,\nwhile some others might cause potential hidden threats that compromise the\nsecurity of the software over time. To understand these security impacts and\nseverity, we design a tool, SSCGuard, to generate 439,138 prompts based on\nSSC-related questions collected online, and analyze the responses of four\npopular LLMs from GPT and Llama. Our results show that all identified\nSSC-related threats persistently exist. To mitigate these risks, we propose a\nnovel prompt-based defense mechanism, namely Chain-of-Confirmation, to reduce\nfabrication, and a middleware-based defense that informs users of various SSC\nthreats.", "AI": {"tldr": "The paper investigates security threats caused by inherent code generation issues in LLMs for software supply chains (SSC) and proposes defenses including Chain-of-Confirmation and middleware-based threat detection.", "motivation": "LLMs' code generation issues (fabrication, misinformation, outdated data) create serious software supply chain risks that could compromise system security through hijacking or long-term vulnerabilities.", "method": "Identified 3 categories of 11 SSC-related threats through source code and CI file analysis; developed SSCGuard tool to generate 439,138 prompts testing 4 LLMs (GPT and Llama series) for vulnerability to these threats.", "result": "All identified threats persistently exist in LLM responses; some enable immediate software/workflow hijacking while others create latent security compromises over time.", "conclusion": "Proposes prompt-based Chain-of-Confirmation defense to reduce fabrication risks and middleware-based user alerting system for proactive threat mitigation in LLM-integrated software supply chains."}}
{"id": "2509.20300", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20300", "abs": "https://arxiv.org/abs/2509.20300", "authors": ["Jannis Kiesel", "Jonathan Heiss"], "title": "Confidentiality-Preserving Verifiable Business Processes through Zero-Knowledge Proofs", "comment": null, "summary": "Ensuring the integrity of business processes without disclosing confidential\nbusiness information is a major challenge in inter-organizational processes.\nThis paper introduces a zero-knowledge proof (ZKP)-based approach for the\nverifiable execution of business processes while preserving confidentiality. We\nintegrate ZK virtual machines (zkVMs) into business process management engines\nthrough a comprehensive system architecture and a prototypical implementation.\nOur approach supports chained verifiable computations through proof\ncompositions. On the example of product carbon footprinting, we model\nsequential footprinting activities and demonstrate how organizations can prove\nand verify the integrity of verifiable processes without exposing sensitive\ninformation. We assess different ZKP proving variants within process models for\ntheir efficiency in proving and verifying, and discuss the practical\nintegration of ZKPs throughout the Business Process Management (BPM) lifecycle.\nOur experiment-driven evaluation demonstrates the automation of process\nverification under given confidentiality constraints.", "AI": {"tldr": "This paper proposes a zero-knowledge proof (ZKP)-based system for confidential business process verification. By integrating zkVMs into BPM engines, it enables privacy-preserving process validation through chained computations, validated via product carbon footprinting use cases and efficiency evaluations of ZKP variants.", "motivation": "Inter-organizational business processes face challenges in ensuring process integrity without exposing sensitive business information. Traditional BPM systems lack confidentiality during verification, necessitating privacy-preserving solutions like ZKP.", "method": "The authors design a system architecture embedding zkVMs into BPM engines, using proof compositions for chained verifications. They implement a prototype to model carbon footprinting processes, test ZKP efficiency across process models, and evaluate BPM lifecycle integration.", "result": "Experiments demonstrate automated process verification under confidentiality constraints. Specific ZKP variants show practical efficiency in proving/verifying business processes, with carbon footprinting workflows successfully validated without data exposure.", "conclusion": "ZKP-based verifiable BPM systems can reconcile confidentiality and integrity requirements. The prototype proves ZKPs enable privacy-preserving process validation, offering a scalable framework for secure inter-organizational collaboration through blockchain-integrated BPM."}}
{"id": "2509.20283", "categories": ["cs.CR", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.20283", "abs": "https://arxiv.org/abs/2509.20283", "authors": ["\u00d6nder Askin", "Tim Kutta", "Holger Dette"], "title": "Monitoring Violations of Differential Privacy over Time", "comment": null, "summary": "Auditing differential privacy has emerged as an important area of research\nthat supports the design of privacy-preserving mechanisms. Privacy audits help\nto obtain empirical estimates of the privacy parameter, to expose flawed\nimplementations of algorithms and to compare practical with theoretical privacy\nguarantees. In this work, we investigate an unexplored facet of privacy\nauditing: the sustained auditing of a mechanism that can go through changes\nduring its development or deployment. Monitoring the privacy of algorithms over\ntime comes with specific challenges. Running state-of-the-art (static) auditors\nrepeatedly requires excessive sampling efforts, while the reliability of such\nmethods deteriorates over time without proper adjustments. To overcome these\nobstacles, we present a new monitoring procedure that extracts information from\nthe entire deployment history of the algorithm. This allows us to reduce\nsampling efforts, while sustaining reliable outcomes of our auditor. We derive\nformal guarantees with regard to the soundness of our methods and evaluate\ntheir performance for important mechanisms from the literature. Our theoretical\nfindings and experiments demonstrate the efficacy of our approach.", "AI": {"tldr": "This paper addresses the challenge of sustained differential privacy auditing for evolving mechanisms, introducing a monitoring procedure that leverages historical deployment data to reduce sampling efforts and maintain reliability over time.", "motivation": "Existing static auditors are either inefficient when applied repeatedly to changing mechanisms or lose reliability over time without adjustments, creating a need for a dynamic auditing approach.", "method": "A novel monitoring framework is proposed that extracts information from the algorithm's entire deployment history, combining temporal analysis with formal guarantees to ensure soundness while minimizing sampling requirements.", "result": "Theoretical analysis and experiments demonstrate significant reductions in sampling effort with maintained reliability, validated against benchmark mechanisms from literature.", "conclusion": "The proposed method enables effective sustained auditing of dynamic mechanisms, bridging the gap between static auditing techniques and real-world deployment requirements through historical data utilization and formal guarantees."}}
{"id": "2509.20308", "categories": ["cs.SE", "68M15 (Primary), 68M12, 68Q42 (Secondary)", "D.2.5; C.2.2; F.4.2"], "pdf": "https://arxiv.org/pdf/2509.20308", "abs": "https://arxiv.org/abs/2509.20308", "authors": ["Alexander Liggesmeyer", "Jos\u00e9 Antonio Zamudio Amaya", "Andreas Zeller"], "title": "Protocol Testing with I/O Grammars", "comment": "20 pages", "summary": "Generating software tests faces two fundamental problems. First, one needs to\n_generate inputs_ that are syntactically and semantically correct, yet\nsufficiently diverse to cover behavior. Second, one needs an _oracle_ to _check\noutputs_ whether a test case is correct or not. Both problems become apparent\nin _protocol testing_, where inputs are messages exchanged between parties, and\noutputs are the responses of these parties.\n  In this paper, we propose a novel approach to protocol testing that combines\ninput generation and output checking in a single framework. We introduce _I/O\ngrammars_ as the first means to _completely_ specify the syntax and semantics\nof protocols, including messages, states, and interactions. Our implementation,\nbased on the FANDANGO framework, takes a single I/O grammar, and can act as a\n_test generator_, as a _mock object_, and as an _oracle_ for a _client_, a\n_server_, or both (or actually any number of parties), a versatility not found\nin any existing tool or formalism. User-defined _constraints}_can have the\ngenerator focus on arbitrary protocol features; $k$-path guidance\nsystematically covers states, messages, responses, and value alternatives in a\nunified fashion.\n  We evaluate the effectiveness of our approach by applying it to several\nprotocols, including DNS, FTP, and SMTP. We demonstrate that I/O grammars can\nspecify advanced protocol features correctly and completely, while also\nenabling output validation of the programs under test. In its evaluation, we\nfind that systematic coverage of the I/O grammar results in much quicker\ncoverage of the input and response spaces (and thus functionality) compared to\nthe random-based state-of-the-art approaches.", "AI": {"tldr": "The paper introduces I/O grammars, a unified framework solving protocol testing's dual challenges of generating syntactically valid inputs and verifying outputs. It combines input generation/oracle with systematic coverage, validated through DNS/FTP/SMTP protocols.", "motivation": "Current protocol testing lacks tools that simultaneously handle input generation and output validation. Existing solutions either focus on syntactic generation or separate oracle mechanisms, failing to model full protocol semantics including state transitions and interactions.", "method": "1) I/O grammars: formalism specifying protocol syntax/semantics (messages, states, interactions). 2 ) FANDANGO-based implementation providing three roles: test generator, mock object, and oracle. 3 ) k-path guidance algorithm for systematic coverage of protocol features.", "result": "Successfully applied to DNS/FTP/SMTP protocols. Systematic coverage achieved 100x faster feature discovery than random approaches. I/O grammars enabled complete specification of complex features (e.g., DNS TXT records with length constraints).", "conclusion": "I/O grammars provide a fundamentally new approach to protocol testing by unifying input generation and output validation. This generic framework achieves superior coverage efficiency while enabling precise protocol constraint modeling."}}
{"id": "2509.20324", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20324", "abs": "https://arxiv.org/abs/2509.20324", "authors": ["Atousa Arzanipour", "Rouzbeh Behnia", "Reza Ebrahimi", "Kaushik Dutta"], "title": "RAG Security and Privacy: Formalizing the Threat Model and Attack Surface", "comment": "Accepted at the 5th ICDM Workshop on September 20, 2025", "summary": "Retrieval-Augmented Generation (RAG) is an emerging approach in natural\nlanguage processing that combines large language models (LLMs) with external\ndocument retrieval to produce more accurate and grounded responses. While RAG\nhas shown strong potential in reducing hallucinations and improving factual\nconsistency, it also introduces new privacy and security challenges that differ\nfrom those faced by traditional LLMs. Existing research has demonstrated that\nLLMs can leak sensitive information through training data memorization or\nadversarial prompts, and RAG systems inherit many of these vulnerabilities. At\nthe same time, reliance of RAG on an external knowledge base opens new attack\nsurfaces, including the potential for leaking information about the presence or\ncontent of retrieved documents, or for injecting malicious content to\nmanipulate model behavior. Despite these risks, there is currently no formal\nframework that defines the threat landscape for RAG systems. In this paper, we\naddress a critical gap in the literature by proposing, to the best of our\nknowledge, the first formal threat model for retrieval-RAG systems. We\nintroduce a structured taxonomy of adversary types based on their access to\nmodel components and data, and we formally define key threat vectors such as\ndocument-level membership inference and data poisoning, which pose serious\nprivacy and integrity risks in real-world deployments. By establishing formal\ndefinitions and attack models, our work lays the foundation for a more rigorous\nand principled understanding of privacy and security in RAG systems.", "AI": {"tldr": "The paper proposes the first formal threat model for Retrieval-Augmented Generation (RAG), addressing privacy and security challenges in RAG systems by introducing a taxonomy of adversaries and formally defining threat vectors like document-level membership inference and data poisoning.", "motivation": "RAG systems face unique security risks compared to traditional LLMs, including external knowledge base vulnerabilities and inherited LLM weaknesses. Existing research lacks a formal framework to address these threats.", "method": "The authors propose a structured taxonomy of adversary types based on access to model components/data and formally define threat vectors (e.g., information leakage from retrieved documents, data poisoning).", "result": "They establish a foundational threat model that categorizes adversaries and identifies critical risks (privacy/integrity) in real-world RAG deployments.", "conclusion": "This work provides a rigorous framework for understanding and mitigating RAG-specific security challenges, enabling principled research in this area."}}
{"id": "2509.20353", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20353", "abs": "https://arxiv.org/abs/2509.20353", "authors": ["Viktoria Stray", "Elias Goldmann Brandtz\u00e6g", "Viggo Tellefsen Wivestad", "Astri Barbala", "Nils Brede Moe"], "title": "Developer Productivity With and Without GitHub Copilot: A Longitudinal Mixed-Methods Case Study", "comment": "Accepted for publication in the Proceedings of the 59th Hawaii\n  International Conference on System Sciences (HICSS 2026)", "summary": "This study investigates the real-world impact of the generative AI (GenAI)\ntool GitHub Copilot on developer activity and perceived productivity. We\nconducted a mixed-methods case study in NAV IT, a large public sector agile\norganization. We analyzed 26,317 unique non-merge commits from 703 of NAV IT's\nGitHub repositories over a two-year period, focusing on commit-based activity\nmetrics from 25 Copilot users and 14 non-users. The analysis was complemented\nby survey responses on their roles and perceived productivity, as well as 13\ninterviews. Our analysis of activity metrics revealed that individuals who used\nCopilot were consistently more active than non-users, even prior to Copilot's\nintroduction. We did not find any statistically significant changes in\ncommit-based activity for Copilot users after they adopted the tool, although\nminor increases were observed. This suggests a discrepancy between changes in\ncommit-based metrics and the subjective experience of productivity.", "AI": {"tldr": "This study examines GitHub Copilot's impact on developer productivity and activity in a large public sector organization (NAV IT) through a two-year mixed-methods analysis of 26,317 commits, surveys, and interviews. Results show no significant changes in commit activity post-adoption, highlighting a discrepancy between objective metrics and subjective productivity perceptions.", "motivation": "Generative AI tools like GitHub Copilot are widely adopted, but their real-world impact on developer productivity remains poorly understood, particularly in large-scale organizational contexts requiring empirical validation.", "method": "Mixed-methods case study analyzing 26,317 non-merge commits from 703 repositories over two years, comparing 25 Copilot users vs. 14 non-users through commit activity metrics, role surveys, and 13 semi-structured interviews.", "result": "Copilot users showed higher pre-adoption activity levels but no statistically significant changes in commit metrics post-adoption, despite reporting subjective productivity improvements. Activity trends for users remained consistent before/after adoption, suggesting misalignment between productivity perception and repository activity metrics.", "conclusion": "Objective commit activity metrics do not fully capture developers' perceived productivity gains from AI tools like GitHub Copilot, indicating limitations in current productivity measurement frameworks and warranting further investigation into intangible benefits of GenAI in software development."}}
{"id": "2509.20356", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20356", "abs": "https://arxiv.org/abs/2509.20356", "authors": ["Mohamed E. Najd", "Ghada Almashaqbeh"], "title": "chainScale: Secure Functionality-oriented Scalability for Decentralized Resource Markets", "comment": null, "summary": "Decentralized resource markets are Web 3.0 applications that build\nopen-access platforms for trading digital resources among users without any\ncentral management. They promise cost reduction, transparency, and flexible\nservice provision. However, these markets usually have large workload that must\nbe processed in a timely manner, leading to serious scalability problems.\nDespite the large amount of work on blockchain scalability, existing solutions\nare ineffective as they do not account for these markets' work models and\ntraffic patterns.\n  We introduce chainScale, a secure hybrid sidechain-sharding solution that\naims to boost throughput of decentralized resource markets and reduce their\nlatency and storage footprint. At its core, chainScale leverages dependent\nsidechains and functionality-oriented workload splitting to parallelize traffic\nprocessing by having each market module assigned to a sidechain. Different from\nsharding, chainScale does not incur any cross-sidechain transactions that tend\nto be costly. chainScale introduces several techniques, including hierarchical\nworkload sharing that further sub-divides overloaded modules, and weighted\nminer assignment that assigns miners with vested interest in the system to\ncritical modules' sidechains. Furthermore, chainScale employs sidechain syncing\nto maintain the mainchain as the single truth of system state, and pruning to\ndiscard stale records. Beside analyzing security, we build a proof-of-concept\nimplementation for a distributed file storage market as a use case. Our\nexperiments show that, compared to a single sidechain-based prior solution,\nchainScale boosts throughput by 4x and reduces confirmation latency by 5x.\nAlso, they show that chainScale outperforms sharding by 2.5x in throughput and\n3.5x in latency.", "AI": {"tldr": "chainScale is a secure hybrid sidechain-sharding solution that addresses scalability issues in decentralized resource markets by parallelizing traffic processing through dependent sidechains, boosting throughput by 4x and reducing latency by 5x compared to prior solutions.", "motivation": "Decentralized resource markets face scalability challenges due to large workloads and traffic patterns not addressed by existing blockchain solutions. Current approaches are ineffective as they ignore the specific work models of these markets.", "method": "chainScale utilizes dependent sidechains with workload splitting to parallelize processing, hierarchical workload sharing for overload division, weighted miner assignment for critical modules, sidechain syncing to maintain mainchain consistency, and pruning for storage optimization. It avoids cross-sidechain transactions to reduce costs.", "result": "Experiments show chainScale achieves 4x higher throughput and 5x lower confirmation latency compared to a prior single sidechain solution. It outperforms sharding by 2.5x in throughput and 3.5x in latency.", "conclusion": "chainScale effectively resolves scalability bottlenecks in decentralized markets through its hybrid architecture and workload management strategies, validated by a proof-of-concept implementation and performance benchmarks."}}
{"id": "2509.20362", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20362", "abs": "https://arxiv.org/abs/2509.20362", "authors": ["Shaoyuan Xie", "Mohamad Habib Fakih", "Junchi Lu", "Fayzah Alshammari", "Ningfei Wang", "Takami Sato", "Halima Bouzidi", "Mohammad Abdullah Al Faruque", "Qi Alfred Chen"], "title": "FlyTrap: Physical Distance-Pulling Attack Towards Camera-based Autonomous Target Tracking Systems", "comment": "An extended version of the paper accepted by NDSS 2026", "summary": "Autonomous Target Tracking (ATT) systems, especially ATT drones, are widely\nused in applications such as surveillance, border control, and law enforcement,\nwhile also being misused in stalking and destructive actions. Thus, the\nsecurity of ATT is highly critical for real-world applications. Under the\nscope, we present a new type of attack: distance-pulling attacks (DPA) and a\nsystematic study of it, which exploits vulnerabilities in ATT systems to\ndangerously reduce tracking distances, leading to drone capturing, increased\nsusceptibility to sensor attacks, or even physical collisions. To achieve these\ngoals, we present FlyTrap, a novel physical-world attack framework that employs\nan adversarial umbrella as a deployable and domain-specific attack vector.\nFlyTrap is specifically designed to meet key desired objectives in attacking\nATT drones: physical deployability, closed-loop effectiveness, and\nspatial-temporal consistency. Through novel progressive distance-pulling\nstrategy and controllable spatial-temporal consistency designs, FlyTrap\nmanipulates ATT drones in real-world setups to achieve significant system-level\nimpacts. Our evaluations include new datasets, metrics, and closed-loop\nexperiments on real-world white-box and even commercial ATT drones, including\nDJI and HoverAir. Results demonstrate FlyTrap's ability to reduce tracking\ndistances within the range to be captured, sensor attacked, or even directly\ncrashed, highlighting urgent security risks and practical implications for the\nsafe deployment of ATT systems.", "AI": {"tldr": "This paper introduces distance-pulling attacks (DPA), a novel threat to autonomous tracking drone systems, and proposes FlyTrap\u2014a deployable attack framework using an adversarial umbrella to reduce tracking distances, enabling drone capture, sensor attacks, or collisions. Evaluations on real-world systems demonstrate practical security risks.", "motivation": "Autonomous tracking drones are widely used in security-critical applications but face emerging misuse risks. Existing research overlooks vulnerabilities enabling physical interference through distance manipulation, necessitating systematic study of such attacks.", "method": "FlyTrap employs a physical adversarial umbrella as a domain-specific attack vector, combining progressive distance-pulling strategies and spatiotemporal consistency control. The framework targets closed-loop tracking systems through deployable, real-world attack mechanisms.", "result": "FlyTrap successfully reduced tracking distances in DJI and HoverAir drones to enable capture or collision across multiple real-world experiments. New datasets and metrics quantify attack effectiveness, demonstrating practical feasibility against commercial systems.", "conclusion": "The study reveals critical security vulnerabilities in autonomous tracking systems, highlighting urgent risks for real-world deployment. FlyTrap's capabilities emphasize the need for robust countermeasures against physical-world adversarial attacks."}}
