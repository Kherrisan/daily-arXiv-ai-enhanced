<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 10]
- [cs.SE](#cs.SE) [Total: 7]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Per-sender neural network classifiers for email authorship validation](https://arxiv.org/abs/2509.00005)
*Rohit Dube*

Main category: cs.CR

TL;DR: Researchers develop and evaluate authorship validation techniques for email security, showing CNN-based models can effectively detect inauthentic messages and integrate into existing systems with low overhead.


<details>
  <summary>Details</summary>
Motivation: Modern organizations remain vulnerable to email-based attacks from compromised employee accounts because they continue to trust internal emails by default despite improved inbound phishing defenses.

Method: The paper introduces authorship validation based on per-sender writing style modeling, evaluates Naive Bayes and character-level CNN (Char-CNN) models for this task, and uses newly created Enron-based datasets with both human-written and LLM-generated simulated inauthentic emails for evaluation.

Result: Char-CNN achieves high accuracy and F1 scores across various conditions, and the study demonstrates the feasibility of deploying per-sender authorship validation in commercial systems with minimal resource requirements.

Conclusion: Authorship validation is a practical and effective real-time defense mechanism that can be integrated into commercial email systems to enhance security against internal email threats, with the character-level CNN model demonstrating high performance and low overhead.

Abstract: Business email compromise and lateral spear phishing attacks are among modern
organizations' most costly and damaging threats. While inbound phishing
defenses have improved significantly, most organizations still trust internal
emails by default, leaving themselves vulnerable to attacks from compromised
employee accounts. In this work, we define and explore the problem of
authorship validation: verifying whether a claimed sender actually authored a
given email. Authorship validation is a lightweight, real-time defense that
complements traditional detection methods by modeling per-sender writing style.
Further, the paper presents a collection of new datasets based on the Enron
corpus. These simulate inauthentic messages using both human-written and large
language model-generated emails. The paper also evaluates two classifiers -- a
Naive Bayes model and a character-level convolutional neural network (Char-CNN)
-- for the authorship validation task. Our experiments show that the Char-CNN
model achieves high accuracy and F1 scores under various circumstances.
Finally, we discuss deployment considerations and show that per-sender
authorship classifiers are practical for integrating into existing commercial
email security systems with low overhead.

</details>


### [2] [Case Studies: Effective Approaches for Navigating Cross-Border Cloud Data Transfers Amid U.S. Government Privacy and Safety Concerns](https://arxiv.org/abs/2509.00006)
*Motunrayo Adebayo*

Main category: cs.CR

TL;DR: This paper examines legal and tech challenges in international cloud data exchange and proposes frameworks for secure solutions.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of cloud technology has increased cross-border information exchange, creating a need to address legal and technological challenges to ensure secure and compliant data sharing.

Method: The paper examines existing frameworks for cross-border information exchange, identifies legal and technological challenges, and proposes solutions to address these issues.

Result: The analysis highlights vulnerabilities in existing frameworks that hinder secure cloud-based information exchange, while emphasizing the impact on domestic and international business operations.

Conclusion: The study underlines the necessity for robust frameworks to ensure secure and lawful cross-border information exchange using cloud technology, emphasizing measures to address legal and technological challenges.

Abstract: This study attempts to explain the impact of information exchange from one
country to another, as well as the legal and technological implications for
these exchanges. Due to the emergence of cloud technology, possibilities for
free exchange of information between countries have increased rapidly, as it
has become possible to save information in a country and access it in almost
any part of the world. Countries all around the world have been confronted with
developing frameworks to facilitate this process, although there are
significant challenges which must be confronted on legal and technological
fronts, as loopholes in the framework adopted by countries may hinder free
access to information stored on cloud, and also compromise data privacy. Cloud
technology is impacting a lot of issues, including domestic and international
businesses, hence the need for a study to propose measures for safe exchange of
information using cloud technology.

</details>


### [3] [Keystroke Detection by Exploiting Unintended RF Emission from Repaired USB Keyboards](https://arxiv.org/abs/2509.00043)
*Md Faizul Bari,Yi Xie,Meghna Roy Choudhury,Shreyas Sen*

Main category: cs.CR

TL;DR: Researchers discovered that improperly repaired USB cables create long-range EM side-channels, enabling perfect keystroke detection up to 12m away through walls, demanding better hardware repair security standards.


<details>
  <summary>Details</summary>
Motivation: Challenges the assumption that EM emanations from devices/cables are short-range and non-leaky, demonstrating how cable repair processes create unintended monopole antennas for long-range information exfiltration.

Method: Experimental analysis of three cable types using a detection algorithm for keystroke inference, testing at 12m distance in open spaces, corridors, and buildings while measuring interference/human-body coupling effects.

Result: 99.9% keystroke detection accuracy at 12m distance with 14cm concrete penetration, establishing the highest USB keyboard EM side-channel performance in the literature.

Conclusion: This work exposes a hardware modification vulnerability enabling long-range EM side-channel attacks, proposes multi-layer shielding as a countermeasure, and highlights the critical security risks of improperly repaired cables.

Abstract: Electronic devices and cables inadvertently emit RF emissions as a byproduct
of signal processing and/or transmission. Labeled as electromagnetic
emanations, they form an EM side-channel for data leakage. Previously, it was
believed that such leakage could be contained within a facility since they are
weak signals with a short transmission range. However, in the preliminary
version of this work [1], we found that the traditional cable repairing process
forms a tiny monopole antenna that helps emanations transmit over a long range.
Experimentation with three types of cables revealed that emanations from
repaired cables remain detectable even at >4 m and can penetrate a 14 cm thick
concrete wall. In this extended version, we show that such emanation can be
exploited at a long distance for information extraction by detecting keystrokes
typed on a repaired USB keyboard. By collecting data for 70 different
keystrokes at different distances from the target in 3 diverse environments
(open space, a corridor outside an office room, and outside a building) and
developing an efficient detection algorithm, ~100% keystroke detection accuracy
has been achieved up to 12 m distance, which is the highest reported accuracy
at such a long range for USB keyboards in the literature. The effect of two
experimental factors, interference and human-body coupling, has been
investigated thoroughly. Along with exploring the vulnerability, multi-layer
external metal shielding during the repairing process as a possible remedy has
been explored. This work exposes a new attack surface caused by hardware
modification, its exploitation, and potential countermeasures.

</details>


### [4] [Cryptographic Challenges: Masking Sensitive Data in Cyber Crimes through ASCII Art](https://arxiv.org/abs/2509.00059)
*Andres Alejandre,Kassandra Delfin,Victor Castano*

Main category: cs.CR

TL;DR: This paper explores ASCII art as a novel data masking technique for cybercrime prevention. It evaluates its strengths and limitations, recommends improved security practices, and emphasizes the need for privacy awareness to combat evolving threats.\n


<details>
  <summary>Details</summary>
Motivation: The motivation centers on addressing the growing need for innovative methods to protect personal data during its delivery and beyond, leveraging ASCII art as a novel approach to counter cybercriminal activities.\n

Method: The study examines the unique properties and historical context of ASCII art, analyzing its application in masking sensitive information during data delivery. It evaluates advantages and limitations through various cybercrime scenarios and provides recommendations for enhancing security practices.\n

Result: The results indicate that ASCII art's simplicity and ambiguity offer practical advantages for data masking, while also highlighting its limitations. The study also proposes actionable recommendations to improve data security practices and promote privacy awareness among businesses and individuals.\n

Conclusion: ASCII art, with its simplicity and ambiguity, serves as an effective tool for protecting sensitive data against cybercriminals. The study underscores the importance of robust data security measures and fostering privacy awareness in an interconnected world.\n

Abstract: The use of ASCII art as a novel approach to masking sensitive information in
cybercrime, focusing on its potential role in protecting personal data during
the delivery process and beyond, is presented. By examining the unique
properties of ASCII art and its historical context, this study discusses the
advantages and limitations of employing this technique in various cybercrime
scenarios. Additionally, providing recommendations for enhancing data security
practices and fostering a culture of privacy awareness in both businesses and
individuals. The findings suggest that ASCII art, with its simplicity and
ambiguity, can serve as an effective tool against cybercriminals, emphasizing
the need for robust data security measures and increased privacy awareness in
today's interconnected world.

</details>


### [5] [Enabling Transparent Cyber Threat Intelligence Combining Large Language Models and Domain Ontologies](https://arxiv.org/abs/2509.00081)
*Luca Cotti,Anisa Rula,Devis Bianchini,Federico Cerutti*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Effective Cyber Threat Intelligence (CTI) relies upon accurately structured
and semantically enriched information extracted from cybersecurity system logs.
However, current methodologies often struggle to identify and interpret
malicious events reliably and transparently, particularly in cases involving
unstructured or ambiguous log entries. In this work, we propose a novel
methodology that combines ontology-driven structured outputs with Large
Language Models (LLMs), to build an Artificial Intelligence (AI) agent that
improves the accuracy and explainability of information extraction from
cybersecurity logs. Central to our approach is the integration of domain
ontologies and SHACL-based constraints to guide the language model's output
structure and enforce semantic validity over the resulting graph. Extracted
information is organized into an ontology-enriched graph database, enabling
future semantic analysis and querying. The design of our methodology is
motivated by the analytical requirements associated with honeypot log data,
which typically comprises predominantly malicious activity. While our case
study illustrates the relevance of this scenario, the experimental evaluation
is conducted using publicly available datasets. Results demonstrate that our
method achieves higher accuracy in information extraction compared to
traditional prompt-only approaches, with a deliberate focus on extraction
quality rather than processing speed.

</details>


### [6] [Private, Verifiable, and Auditable AI Systems](https://arxiv.org/abs/2509.00085)
*Tobin South*

Main category: cs.CR

TL;DR: This thesis develops cryptography-based frameworks for trustworthy AI, enabling verifiable, auditable, and privacy-preserving foundation models through techniques like zero-knowledge proofs and secure computation, offering both technical implementations and policy implications.


<details>
  <summary>Details</summary>
Motivation: The thesis addresses the urgent need for security, accountability, and trustworthiness in AI systems as societal reliance grows, emphasizing the critical importance of balancing privacy requirements with transparency and auditability in foundation models.

Method: The research employs zero-knowledge cryptography, secure multi-party computation, trusted execution environments, and enhanced delegation/access control mechanisms to address privacy and verifiability challenges in AI systems.

Result: The work introduces verifiable claim frameworks, confidential deployment solutions for large language models, and secure interaction protocols for multi-agent systems, providing practical blueprints for implementing privacy-verifiability-auditability tradeoffs.

Conclusion: This dissertation concludes that integrating privacy, verifiability, and auditability in foundation model-based AI systems requires combining advanced cryptographic methods with policy-informed system design to achieve responsible AI innovation while addressing critical security risks.

Abstract: The growing societal reliance on artificial intelligence necessitates robust
frameworks for ensuring its security, accountability, and trustworthiness. This
thesis addresses the complex interplay between privacy, verifiability, and
auditability in modern AI, particularly in foundation models. It argues that
technical solutions that integrate these elements are critical for responsible
AI innovation. Drawing from international policy contributions and technical
research to identify key risks in the AI pipeline, this work introduces novel
technical solutions for critical privacy and verifiability challenges.
Specifically, the research introduces techniques for enabling verifiable and
auditable claims about AI systems using zero-knowledge cryptography; utilizing
secure multi-party computation and trusted execution environments for
auditable, confidential deployment of large language models and information
retrieval; and implementing enhanced delegation mechanisms, credentialing
systems, and access controls to secure interactions with autonomous and
multi-agent AI systems. Synthesizing these technical advancements, this
dissertation presents a cohesive perspective on balancing privacy,
verifiability, and auditability in foundation model-based AI systems, offering
practical blueprints for system designers and informing policy discussions on
AI safety and governance.

</details>


### [7] [AEGIS : Automated Co-Evolutionary Framework for Guarding Prompt Injections Schema](https://arxiv.org/abs/2509.00088)
*Ting-Chun Liu,Ching-Yu Hsu,Kuan-Yi Lee,Chi-An Fu,Hung-yi Lee*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Prompt injection attacks pose a significant challenge to the safe deployment
of Large Language Models (LLMs) in real-world applications. While prompt-based
detection offers a lightweight and interpretable defense strategy, its
effectiveness has been hindered by the need for manual prompt engineering. To
address this issue, we propose AEGIS , an Automated co-Evolutionary framework
for Guarding prompt Injections Schema. Both attack and defense prompts are
iteratively optimized against each other using a gradient-like natural language
prompt optimization technique. This framework enables both attackers and
defenders to autonomously evolve via a Textual Gradient Optimization (TGO)
module, leveraging feedback from an LLM-guided evaluation loop. We evaluate our
system on a real-world assignment grading dataset of prompt injection attacks
and demonstrate that our method consistently outperforms existing baselines,
achieving superior robustness in both attack success and detection.
Specifically, the attack success rate (ASR) reaches 1.0, representing an
improvement of 0.26 over the baseline. For detection, the true positive rate
(TPR) improves by 0.23 compared to the previous best work, reaching 0.84, and
the true negative rate (TNR) remains comparable at 0.89. Ablation studies
confirm the importance of co-evolution, gradient buffering, and multi-objective
optimization. We also confirm that this framework is effective in different
LLMs. Our results highlight the promise of adversarial training as a scalable
and effective approach for guarding prompt injections.

</details>


### [8] [Enhanced Rényi Entropy-Based Post-Quantum Key Agreement with Provable Security and Information-Theoretic Guarantees](https://arxiv.org/abs/2509.00104)
*Ruopengyu Xu,Chenglian Liu*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper presents an enhanced post-quantum key agreement protocol based on
R\'{e}nyi entropy, addressing vulnerabilities in the original construction
while preserving information-theoretic security properties. We develop a
theoretical framework leveraging entropy-preserving operations and
secret-shared verification to achieve provable security against quantum
adversaries. Through entropy amplification techniques and quantum-resistant
commitments, the protocol establishes $2^{128}$ quantum security guarantees
under the quantum random oracle model. Key innovations include a
confidentiality-preserving verification mechanism using distributed polynomial
commitments, tightened min-entropy bounds with guaranteed non-negativity, and
composable security proofs in the quantum universal composability framework.
Unlike computational approaches, our method provides information-theoretic
security without hardness assumptions while maintaining polynomial complexity.
Theoretical analysis demonstrates resilience against known quantum attack
vectors, including Grover-accelerated brute force and quantum memory attacks.
The protocol achieves parameterization for 128-bit quantum security with
efficient $\mathcal{O}(n^2)$ communication complexity. Extensions to secure
multiparty computation and quantum network applications are established,
providing a foundation for long-term cryptographic security. All security
claims are derived from mathematical proofs; this theoretical work presents no
experimental validation.

</details>


### [9] [A Whole New World: Creating a Parallel-Poisoned Web Only AI-Agents Can See](https://arxiv.org/abs/2509.00124)
*Shaked Zychlinski*

Main category: cs.CR

TL;DR: This paper reveals a stealthy cloaking attack exploiting AI agents' digital fingerprints to deliver malicious content, enabling undetected threats to agent-based systems.


<details>
  <summary>Details</summary>
Motivation: The proliferation of autonomous AI agents with detectable digital fingerprints creates a new security vulnerability, as adversaries can exploit this fingerprintability to selectively target agents with malicious content, bypassing traditional security measures.

Method: The attack employs website cloaking techniques that detect and differentiate traffic from AI agents (via their digital fingerprints) and serve malicious, cloaked content to these agents, enabling indirect prompt injections and unauthorized actions.

Result: The attack demonstrates how adversaries can stealthily hijack agent behavior for data exfiltration, malware execution, or misinformation without detection, revealing significant security implications for agentic AI systems.

Conclusion: The paper highlights the urgent need for robust defenses against cloaking attacks targeting autonomous AI agents, emphasizing the security risks associated with their distinguishable digital fingerprints.

Abstract: This paper introduces a novel attack vector that leverages website cloaking
techniques to compromise autonomous web-browsing agents powered by Large
Language Models (LLMs). As these agents become more prevalent, their unique and
often homogenous digital fingerprints - comprising browser attributes,
automation framework signatures, and network characteristics - create a new,
distinguishable class of web traffic. The attack exploits this
fingerprintability. A malicious website can identify an incoming request as
originating from an AI agent and dynamically serve a different, "cloaked"
version of its content. While human users see a benign webpage, the agent is
presented with a visually identical page embedded with hidden, malicious
instructions, such as indirect prompt injections. This mechanism allows
adversaries to hijack agent behavior, leading to data exfiltration, malware
execution, or misinformation propagation, all while remaining completely
invisible to human users and conventional security crawlers. This work
formalizes the threat model, details the mechanics of agent fingerprinting and
cloaking, and discusses the profound security implications for the future of
agentic AI, highlighting the urgent need for robust defenses against this
stealthy and scalable attack.

</details>


### [10] [A Systematic Approach to Estimate the Security Posture of a Cyber Infrastructure: A Technical Report](https://arxiv.org/abs/2509.00266)
*Qishen Sam Liang*

Main category: cs.CR

TL;DR: This paper proposes a mission-driven security framework for Cyber Infrastructures, using attack graphs and knowledge modeling to identify vulnerabilities and guide actionable security improvements.


<details>
  <summary>Details</summary>
Motivation: Existing security frameworks are too generic/complex for academic Cyber Infrastructures, which require tailored solutions for their collaborative, heterogeneous environments.

Method: A three-phase approach: (1) defining unacceptable losses and missions, (2) identifying system hazards/assets, and (3) constructing a security knowledge graph with directed attack graphs to visualize attack paths and defenses.

Result: The framework provides a comprehensive vulnerability overview, enabling operators to prioritize mitigation strategies and improve risk management through structured attack path analysis and defense visualization.

Conclusion: The proposed mission-centric framework enables proactive assessment of risks and informed decision-making for enhancing the security posture of Cyber Infrastructures, offering a structured alternative to generic security standards.

Abstract: Academic and research Cyber Infrastructures (CI) present unique security
challenges due to their collaborative nature, heterogeneous components, and the
lack of practical, tailored security assessment frameworks. Existing standards
can be too generic or complex for CI administrators to apply effectively. This
report introduces a systematic, mission-centric approach to estimate and
analyze the security posture of a CI. The framework guides administrators
through a top-down process: (1) defining unacceptable losses and security
missions, (2) identifying associated system hazards and critical assets, and
(3) modeling the CI's components and their relationships as a security
knowledge graph. The core of this methodology is the construction of directed
attack graphs, which systematically map all potential paths an adversary could
take from an entry point to a critical asset. By visualizing these attack paths
alongside defense mechanisms, the framework provides a clear, comprehensive
overview of the system's vulnerabilities and security gaps. This structured
approach enables CI operators to proactively assess risks, prioritize
mitigation strategies, and make informed, actionable decisions to strengthen
the overall security posture of the CI.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [11] [LLM-based Triplet Extraction for Automated Ontology Generation in Software Engineering Standards](https://arxiv.org/abs/2509.00140)
*Songhui Yue*

Main category: cs.SE

TL;DR: LLM-assisted AOG for SES texts improves RTE and ontology generation, matching/exceeding OpenIE benchmarks.


<details>
  <summary>Details</summary>
Motivation: Automated ontology generation (AOG) is critical for scaling knowledge representation in software engineering standards (SES), which contain unstructured, noisy, domain-specific texts that challenge traditional methods.

Method: The paper introduces an LLM-aided workflow for RTE, combining document segmentation, candidate term mining, LLM-based relation inference, term normalization, and cross-section alignment, moving beyond prompt-engineering techniques.

Result: Golden-standard benchmarks evaluated the proposed method across three granularities, showing results comparable to or outperforming the OpenIE triple extraction method.

Conclusion: The study demonstrates that the LLM-assisted approach for ontology generation in software engineering standards is comparable or superior to existing methods like OpenIE, offering a viable workflow for automated ontology creation.

Abstract: Ontologies have supported knowledge representation and whitebox reasoning for
decades; thus, the automated ontology generation (AOG) plays a crucial role in
scaling their use. Software engineering standards (SES) consist of long,
unstructured text (with high noise) and paragraphs with domain-specific terms.
In this setting, relation triple extraction (RTE), together with term
extraction, constitutes the first stage toward AOG. This work proposes an
open-source large language model (LLM)-assisted approach to RTE for SES.
Instead of solely relying on prompt-engineering-based methods, this study
promotes the use of LLMs as an aid in constructing ontologies and explores an
effective AOG workflow that includes document segmentation, candidate term
mining, LLM-based relation inference, term normalization, and cross-section
alignment. Golden-standard benchmarks at three granularities are constructed
and used to evaluate the ontology generated from the study. The results show
that it is comparable and potentially superior to the OpenIE method of triple
extraction.

</details>


### [12] [LLM-Based Program Generation for Triggering Numerical Inconsistencies Across Compilers](https://arxiv.org/abs/2509.00256)
*Yutong Wang,Cindy Rubio-González*

Main category: cs.SE

TL;DR: LLM4FP uses large language models to generate programs that expose compiler floating-point inconsistencies more effectively than state-of-the-art tools, finding twice as many issues with real-value discrepancies and broader optimization coverage.


<details>
  <summary>Details</summary>
Motivation: Floating-point compiler inconsistencies threaten numerical software reliability, but existing tools like Varity fail to expose subtle, real-world discrepancies beyond extreme values (NaNs/infinities) across diverse optimization contexts.

Method: LLM4FP integrates Grammar-Based Generation (for syntactically valid programs) and Feedback-Based Mutation (for semantic diversity) using Large Language Models to produce targeted floating-point programs that stress-test compiler behavior.

Result: LLM4FP achieves 2×+ higher inconsistency detection than Varity, reveals predominantly real-valued inconsistencies, uncovers mismatches across more optimization levels, and identifies critical host/device compiler divergences not found by prior approaches.

Conclusion: LLM4FP demonstrates that LLM-guided program generation significantly enhances the detection of compiler-specific floating-point inconsistencies compared to existing tools, particularly in identifying real-valued discrepancies and cross-optimization-level issues.

Abstract: Floating-point inconsistencies across compilers can undermine the reliability
of numerical software. We present LLM4FP, the first framework that uses Large
Language Models (LLMs) to generate floating-point programs specifically
designed to trigger such inconsistencies. LLM4FP combines Grammar-Based
Generation and Feedback-Based Mutation to produce diverse and valid programs.
We evaluate LLM4FP across multiple compilers and optimization levels, measuring
inconsistency rate, time cost, and program diversity. LLM4FP detects over twice
as many inconsistencies compared to the state-of-the-art tool, Varity. Notably,
most of the inconsistencies involve real-valued differences, rather than
extreme values like NaN or infinities. LLM4FP also uncovers inconsistencies
across a wider range of optimization levels, and finds the most mismatches
between host and device compilers. These results show that LLM-guided program
generation improves the detection of numerical inconsistencies.

</details>


### [13] [JS-TOD: Detecting Order-Dependent Flaky Tests in Jest](https://arxiv.org/abs/2509.00466)
*Negar Hashemi,Amjed Tahir,Shawn Rasheed,August Shi,Rachel Blagojevic*

Main category: cs.SE

TL;DR: JS-TOD detects test flakiness caused by order dependencies in Jest, revealing that shared files and mocking state are key culprits.


<details>
  <summary>Details</summary>
Motivation: Test order dependency is a major cause of test flakiness, where tests should ideally be isolated but often produce inconsistent results based on execution sequence.

Method: JS-TOD systematically randomizes tests, test suites, and describe blocks through customizable reordering and rerunning (default: 10 orders, 10 reruns) to expose order dependencies.

Result: Evaluation with JS-TOD identified shared files and shared mocking state between tests as the primary sources of test order dependency flakiness.

Conclusion: The paper introduces JS-TOD as an effective solution for detecting order-dependent test flakiness, concluding that shared files and mocking state are critical factors contributing to this issue.

Abstract: We present JS-TOD (JavaScript Test Order-dependency Detector), a tool that
can extract, reorder, and rerun Jest tests to reveal possible order-dependent
test flakiness. Test order dependency is one of the leading causes of test
flakiness. Ideally, each test should operate in isolation and yield consistent
results no matter the sequence in which tests are run. However, in practice,
test outcomes can vary depending on their execution order. JS-TOD employed a
systematic approach to randomising tests, test suites, and describe blocks. The
tool is highly customisable, as one can set the number of orders and reruns
required (the default setting is 10 reorder and 10 reruns for each test and
test suite). Our evaluation using JS-TOD reveals two main causes of test order
dependency flakiness: shared files and shared mocking state between tests.

</details>


### [14] [Bug Whispering: Towards Audio Bug Reporting](https://arxiv.org/abs/2509.00785)
*Elena Masserini,Daniela Micucci,Leonardo Mariani*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Bug reporting is a key feature of mobile applications, as it enables
developers to collect information about faults that escaped testing and thus
affected end-users. This paper explores the idea of allowing end-users to
immediately report the problems that they experience by recording and
submitting audio messages. Audio recording is simple to implement and has the
potential to increase the number of bug reports that development teams can
gather, thus potentially improving the rate at which bugs are identified and
fixed. However, audio bug reports exhibit specific characteristics that
challenge existing techniques for reproducing bugs. This paper discusses these
challenges based on a preliminary experiment, and motivates further research on
the collection and analysis of audio-based bug reports

</details>


### [15] [REConnect: Participatory RE that Matters](https://arxiv.org/abs/2509.01006)
*Daniela Damian,Bachan Ghimire,Ze Shi Li*

Main category: cs.SE

TL;DR: REConnect recenter requirements engineering on human connection through participatory practices


<details>
  <summary>Details</summary>
Motivation: Current practices like CrowdRE and AI elicitation risk detaching requirements work from cultural/social/political contexts

Method: Proposes REConnect framework with three principles (trust-building, co-design, empowerment) demonstrated via BloodSync, Herluma, and BridgingRoots case studies

Result: Requirements outcomes are culturally grounded, socially legitimate, and sustainable; introduces REActions practices for stakeholder engagement

Conclusion: Argues for participatory AI integration in RE to maintain human agency and ethical guardianship

Abstract: Software increasingly shapes the infrastructures of daily life, making
requirements engineering (RE) central to ensuring that systems align with human
values and lived experiences. Yet, current popular practices such as CrowdRE
and AI-assisted elicitation strategies risk detaching requirements work from
the cultural, social, and political contexts that shape lived experiences,
human values, and real user needs. In this paper, we introduce REConnect that
re-centers RE on the human connection as central to the understanding of lived
experiences where impact is sought. REConnect advocates for a human-centered
participatory approach "that matters" to the communities and beneficiaries
involved, ensuring alignment with their values and aspirations. Drawing on
three case studies of societal impact: BloodSync in rural Nepal, Herluma
supporting women at risk of homelessness in Canada, and BridgingRoots to
revitalize Indigenous languages in the Canadian Arctic. REConnect argues that
three key principles and enablers: building trusting relationships,
co-designing with and alongside stakeholders, and empowering users as agents of
change, can yield requirements that are culturally grounded, socially
legitimate, and sustainable beyond system delivery. REConnect also proposes a
set of actionable practices (REActions) that embed relationality and ongoing
stakeholder engagement throughout requirements elicitation, analysis, and
validation of solution development. Finally, we situate REConnect in the era of
Generative AI. While AI can accelerate and scale certain RE tasks, its
integration must be guided by participatory practices that not only preserve
human agency but also empower humans' roles to become guardians of values and
ethics, inclusion amplifiers, curators of AI outputs, and co-reflectors in
iterative review cycles.

</details>


### [16] [Generative Goal Modeling](https://arxiv.org/abs/2509.01048)
*Ateeq Sharfuddin,Travis Breaux*

Main category: cs.SE

TL;DR: This paper proposes using GPT-4o for automated goal extraction from interviews, achieving strong accuracy in text tracing and refinement relationships, demonstrating its potential as a tool for efficient requirements modeling.


<details>
  <summary>Details</summary>
Motivation: Manual review of interview transcripts for requirement acquisition is labor-intensive; automated techniques like textual entailment could improve efficiency and consistency in extracting goal models from textual data.

Method: The approach employs textual entailment via GPT-4o to identify goals in 15 interview transcripts across 29 domains, with performance assessed through comparison to human annotations for goal extraction, tracing, and refinement relationship generation.

Result: GPT-4o achieved 62.0% goal matching accuracy compared to human-derived goals, 98.7% accuracy in tracing goals to transcript segments, and 72.2% accuracy in generating goal refinement relationships according to human evaluators.

Conclusion: The study concludes that GPT-4o effectively supports automated goal extraction from interview transcripts with high tracing accuracy (98.7%) and acceptable refinement relationship accuracy (72.2%), although further improvements are needed for goal matching (62.0%) to better align with human performance.

Abstract: In software engineering, requirements may be acquired from stakeholders
through elicitation methods, such as interviews, observational studies, and
focus groups. When supporting acquisition from interviews, business analysts
must review transcripts to identify and document requirements. Goal modeling is
a popular technique for representing early stakeholder requirements as it lends
itself to various analyses, including refinement to map high-level goals into
software operations, and conflict and obstacle analysis. In this paper, we
describe an approach to use textual entailment to reliably extract goals from
interview transcripts and to construct goal models. The approach has been
evaluated on 15 interview transcripts across 29 application domains. The
findings show that GPT-4o can reliably extract goals from interview
transcripts, matching 62.0% of goals acquired by humans from the same
transcripts, and that GPT-4o can trace goals to originating text in the
transcript with 98.7% accuracy. In addition, when evaluated by human
annotators, GPT-4o generates goal model refinement relationships among
extracted goals with 72.2% accuracy.

</details>


### [17] [A Survey on the Techniques and Tools for Automated Requirements Elicitation and Analysis of Mobile Apps](https://arxiv.org/abs/2509.01068)
*Chong Wang,Haoning Wu,Peng Liang,Maya Daneva,Marten van Sinderen*

Main category: cs.SE

TL;DR: This systematic mapping study analyzes 73 papers to reveal that semi-automatic techniques and open-source tools are widely used for requirements analysis, mining, and classification in automated mobile app requirements elicitation.


<details>
  <summary>Details</summary>
Motivation: The paper addresses a gap in understanding the characteristics of techniques/tools and their alignment with RE tasks in mobile app requirements analysis, despite widespread adoption.

Method: The study employed a systematic mapping approach following Kitchenham et al. guidelines, analyzing 73 selected papers to map techniques, tools, and their applications.

Result: Key findings include: (1) semi-automatic techniques dominate usage, (2) open-sourced/non-self-developed tools are prevalent, and (3) requirements analysis, mining, and classification are most commonly supported tasks.

Conclusion: The paper concludes that there is a growing use of semi-automatic techniques and open-sourced tools in automated requirements elicitation for mobile apps, with requirements analysis, mining, and classification being the primary tasks supported.

Abstract: [Background:] Research on automated requirements elicitation and analysis of
mobile apps employed lots of techniques and tools proposed by RE researchers
and practitioners. However, little is known about the characteristics of these
techniques and tools as well as the RE tasks in requirements elicitation and
analysis that got supported with the help of respective techniques and tools.
[Aims:] The goal of this paper is to investigate the state-of-the-art of the
techniques and tools used in automated requirements elicitation and analysis of
mobile apps. [Method:] We carried out a systematic mapping study by following
the guidelines of Kitchenham et al. [Results:] Based on 73 selected papers, we
found the most frequently used techniques - semi-automatic techniques, and the
main characteristics of the tools - open-sourced and non-self-developed tools
for requirements analysis and text pre-processing. Plus, the most three
investigated RE tasks are requirements analysis, mining and classification.
[Conclusions:] Our most important conclusions are: (1) there is a growth in the
use of techniques and tools in automated requirements elicitation and analysis
of mobile apps, (2) semi-automatic techniques are mainly used in the
publications on this research topic, (3) requirements analysis, mining and
classification are the top three RE tasks with the support of automatic
techniques and tools, and (4) the most popular tools are open-sourced and
non-self-developed, and they are mainly used in requirements analysis and text
processing.

</details>
