{"id": "2601.17292", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17292", "abs": "https://arxiv.org/abs/2601.17292", "authors": ["Zhiyin Zhou"], "title": "Risk-based test framework for LLM features in regulated software", "comment": null, "summary": "Large language models are increasingly embedded in regulated and safety-critical software, including clinical research platforms and healthcare information systems. While these features enable natural language search, summarization, and configuration assistance, they introduce risks such as hallucinations, harmful or out-of-scope advice, privacy and security issues, bias, instability under change, and adversarial misuse. Prior work on machine learning testing and AI assurance offers useful concepts but limited guidance for interactive, product-embedded assistants. This paper proposes a risk-based testing framework for LLM features in regulated software: a six-category risk taxonomy, a layered test strategy mapping risks to concrete tests across guardrail, orchestration, and system layers, and a case study applying the approach to a Knowledgebase assistant in a clinical research platform.", "AI": {"tldr": "This paper introduces a risk-based testing framework for LLM features in regulated software, featuring a six-category risk taxonomy and a layered testing strategy, evaluated in a clinical research platform.", "motivation": "As large language models are increasingly deployed in regulated and safety-critical domains like healthcare, there is a critical need for robust testing methodologies that go beyond general AI assurance to address domain-specific risks such as hallucinations, privacy breaches, and adversarial misuse in interactive, product-embedded settings.", "method": "The authors develop a six-category risk taxonomy and a layered test strategy that maps risks to specific test types across guardrail, orchestration, and system layers, validated through a case study on a Knowledgebase assistant in a clinical research platform.", "result": "The framework successfully identifies and addresses key risks through targeted testing strategies across multiple system layers, demonstrated in a real-world clinical research platform, showing improved risk coverage and practical applicability.", "conclusion": "The proposed risk-based testing framework provides a structured and actionable approach for identifying, categorizing, and mitigating risks associated with LLM features in regulated software environments, particularly in healthcare and clinical research contexts."}}
{"id": "2601.17390", "categories": ["cs.SE", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.17390", "abs": "https://arxiv.org/abs/2601.17390", "authors": ["Yayi Wang", "Shenao Wang", "Jian Zhao", "Shaosen Shi", "Ting Li", "Yan Cheng", "Lizhong Bian", "Kan Yu", "Yanjie Zhao", "Haoyu Wang"], "title": "YASA: Scalable Multi-Language Taint Analysis on the Unified AST at Ant Group", "comment": null, "summary": "Modern enterprises increasingly adopt diverse technology stacks with various programming languages, posing significant challenges for static application security testing (SAST). Existing taint analysis tools are predominantly designed for single languages, requiring substantial engineering effort that scales with language diversity. While multi-language tools like CodeQL, Joern, and WALA attempt to address these challenges, they face limitations in intermediate representation design, analysis precision, and extensibility, which make them difficult to scale effectively for large-scale industrial applications at Ant Group. To bridge this gap, we present YASA (Yet Another Static Analyzer), a unified multi-language static taint analysis framework designed for industrial-scale deployment. Specifically, YASA introduces the Unified Abstract Syntax Tree (UAST) that provides a unified abstraction for compatibility across diverse programming languages. Building on the UAST, YASA performs point-to analysis and taint propagation, leveraging a unified semantic model to manage language-agnostic constructs, while incorporating language-specific semantic models to handle other unique language features. When compared to 6 single- and 2 multi-language static analyzers on an industry-standard benchmark, YASA consistently outperformed all baselines across Java, JavaScript, Python, and Go. In real-world deployment within Ant Group, YASA analyzed over 100 million lines of code across 7.3K internal applications. It identified 314 previously unknown taint paths, with 92 of them confirmed as 0-day vulnerabilities. All vulnerabilities were responsibly reported, with 76 already patched by internal development teams, demonstrating YASA's practical effectiveness for securing large-scale industrial software systems.", "AI": {"tldr": "YASA is a unified multi-language static taint analysis framework designed for large-scale industrial applications, introducing a Unified Abstract Syntax Tree (UAST) to enable precise, scalable, and extensible taint analysis across Java, JavaScript, Python, and Go, outperforming existing tools and discovering 92 0-day vulnerabilities at Ant Group.", "motivation": "The increasing use of diverse programming languages in modern enterprises challenges traditional single-language static analysis tools, while existing multi-language tools suffer from limitations in representation, precision, and scalability, especially in large industrial settings like Ant Group.", "method": "YASA introduces the Unified Abstract Syntax Tree (UAST) for cross-language syntactic abstraction and combines a language-agnostic semantic model with language-specific extensions. It performs point-to analysis and taint propagation within this unified framework, enabling precise and scalable taint tracking across multiple languages.", "result": "YASA outperformed 6 single-language and 2 multi-language analyzers on an industry-standard benchmark across Java, JavaScript, Python, and Go. In production at Ant Group, it analyzed over 100 million lines of code in 7.3K applications, uncovering 314 new taint paths, 92 of which were confirmed as 0-day vulnerabilities, with 76 already patched.", "conclusion": "YASA demonstrates that a unified, extensible, and semantically rich static analysis framework can effectively address the challenges of multi-language security testing in large-scale industrial environments, significantly improving detection accuracy and practical security outcomes."}}
{"id": "2601.17406", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17406", "abs": "https://arxiv.org/abs/2601.17406", "authors": ["Taher A. Ghaleb"], "title": "Fingerprinting AI Coding Agents on GitHub", "comment": "Accepted at the 23rd International Conference on Mining Software Repositories (MSR '26)", "summary": "AI coding agents are reshaping software development through both autonomous and human-mediated pull requests (PRs). When developers use AI agents to generate code under their own accounts, code authorship attribution becomes critical for repository governance, research validity, and understanding modern development practices. We present the first study on fingerprinting AI coding agents, analyzing 33,580 PRs from five major agents (OpenAI Codex, GitHub Copilot, Devin, Cursor, Claude Code) to identify behavioral signatures. With 41 features spanning commit messages, PR structure, and code characteristics, we achieve 97.2% F1-score in multi-class agent identification. We uncover distinct fingerprints: Codex shows unique multiline commit patterns (67.5% feature importance), and Claude Code exhibits distinctive code structure (27.2% importance of conditional statements). These signatures reveal that AI coding tools produce detectable behavioral patterns, suggesting potential for identifying AI contributions in software repositories.", "AI": {"tldr": "This study introduces AI coding agent fingerprinting by analyzing 33,580 pull requests, revealing detectable behavioral patterns that enable 97.2% accurate identification of agents like Codex and Claude via commit and code features.", "motivation": "To address the challenge of code authorship attribution in software development as AI agents generate code under developers' accounts, impacting governance and research accuracy.", "method": "Analyzed 33,580 PRs from five major AI coding agents using 41 features related to commit messages, PR structure, and code characteristics; applied machine learning for multi-class agent identification.", "result": "Achieved 97.2% F1-score in identifying AI coding agents; identified key behavioral signatures such as Codex's multiline commit patterns (67.5% feature importance) and Claude Code's distinctive conditional statement structures (27.2% importance).", "conclusion": "AI coding agents leave detectable behavioral fingerprints in pull requests, enabling accurate identification of their contributions, which has implications for code authorship attribution, repository governance, and research validity."}}
{"id": "2601.17413", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17413", "abs": "https://arxiv.org/abs/2601.17413", "authors": ["Taher A. Ghaleb"], "title": "When AI Agents Touch CI/CD Configurations: Frequency and Success", "comment": "Accepted at the 23rd International Conference on Mining Software Repositories (MSR '26)", "summary": "AI agents are increasingly used in software development, yet their interaction with CI/CD configurations is not well studied. We analyze 8,031 agentic pull requests (PRs) from 1,605 GitHub repositories where AI agents touch YAML configurations. CI/CD configuration files account for 3.25% of agent changes, varying by agent (Devin: 4.83%, Codex: 2.01%, p < 0.001). When agents modify CI/CD, 96.77% target GitHub Actions. Agentic PRs with CI/CD changes merge slightly less often than others (67.77% vs. 71.80%), except for Copilot, whose CI/CD changes merge 15.63 percentage points more often. Across 99,930 workflow runs, build success rates are comparable for CI/CD and non-CI/CD changes (75.59% vs. 74.87%), though three agents show significantly higher success when modifying CI/CD. These results show that AI agents rarely modify CI/CD and focus mostly on GitHub Actions, yet their configuration changes are as reliable as regular code. Copilot's strong CI/CD performance despite lower acceptance suggests emerging configuration specialization, with implications for agent training and DevOps automation.", "AI": {"tldr": "AI agents rarely modify CI/CD configurations (3.25% of changes), primarily targeting GitHub Actions (96.77%), with comparable build success rates to non-CI/CD changes; Copilot shows notably higher merge rates for CI/CD despite overall lower acceptance, suggesting emerging specialization.", "motivation": "Understanding how AI agents interact with CI/CD pipelines is critical as their use in software development grows, yet little is known about their impact on configuration files, which are crucial for DevOps reliability and automation.", "method": "Analyzed 8,031 agentic pull requests from 1,605 GitHub repositories involving YAML configuration changes, focusing on CI/CD modifications; compared merge rates and build success across agents using statistical analysis (e.g., p-values) over 99,930 workflow runs.", "result": "CI/CD changes constitute 3.25% of agent edits, varying significantly by agent (Devin: 4.83%, Codex: 2.01%, p < 0.001); 96.77% target GitHub Actions; agentic CI/CD PRs merge slightly less overall (67.77% vs 71.80%), except Copilot (+15.63 pp); build success is similar (75.59% vs 74.87%), with three agents showing significantly higher success on CI/CD changes.", "conclusion": "AI agents infrequently alter CI/CD configurations but do so reliably, with performance on par with regular code changes; Copilot's strong CI/CD merge performance suggests emerging specialization, indicating potential for targeted agent training and improved DevOps automation tools."}}
{"id": "2601.17178", "categories": ["cs.CR", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2601.17178", "abs": "https://arxiv.org/abs/2601.17178", "authors": ["Saideep Sreekumar", "Zeng Wang", "Akashdeep Saha", "Weihua Xiao", "Minghao Shao", "Muhammad Shafique", "Ozgur Sinanoglu", "Ramesh Karri", "Johann Knechtel"], "title": "TrojanGYM: A Detector-in-the-Loop LLM for Adaptive RTL Hardware Trojan Insertion", "comment": null, "summary": "Hardware Trojans (HTs) remain a critical threat because learning-based detectors often overfit to narrow trigger/payload patterns and small, stylized benchmarks. We introduce TrojanGYM, an agentic, LLM-driven framework that automatically curates HT insertions to expose detector blind spots while preserving design correctness. Given high-level HT specifications, a suite of cooperating LLM agents (instantiated with GPT-4, LLaMA-3.3-70B, and Gemini-2.5Pro) proposes and refines RTL modifications that realize diverse triggers and payloads without impacting normal functionality. TrojanGYM implements a feedback-driven benchmark generation loop co-designed with HT detectors, in which constraint-aware syntactic checking and GNN-based HT detectors provide feedback that iteratively refines HT specifications and insertion strategies to better surface detector blind spots. We further propose Robust-GNN4TJ, a new implementation of the GNN4TJ with improved graph extraction, training robustness, and prediction reliability, especially on LLM-generated HT designs. On the most challenging TrojanGYM-generated benchmarks, Robust-GNN4TJ raises HT detection rates from 0% to 60% relative to a prior GNN-based detector. We instantiate TrojanGYM on SRAM, AES-128, and UART designs at RTL level, and show that it systematically produces diverse, functionally correct HTs that reach up to 83.33% evasion rates against modern GNN-based detectors, revealing robustness gaps that are not apparent when these detectors are evaluated solely on existing TrustHub-style benchmarks. Post peer-review, we will release all codes and artifacts.", "AI": {"tldr": "TrojanGYM is an LLM-driven framework that autonomously generates diverse, functionally correct Hardware Trojans to expose blind spots in learning-based detectors, paired with Robust-GNN4TJ, an improved GNN detector that enhances detection on challenging, LLM-generated benchmarks.", "motivation": "Existing learning-based Hardware Trojan (HT) detectors suffer from overfitting to narrow trigger patterns and stylized benchmarks, limiting their generalization and robustness in real-world scenarios. There is a need for more diverse, realistic, and functionally correct HT benchmarks to rigorously evaluate and improve detector resilience.", "method": "The authors propose TrojanGYM, an agentic framework powered by LLMs (GPT-4, LLaMA-3.3-70B, Gemini-2.5Pro) that autonomously generates and refines RTL-level HT insertions based on high-level specifications. A feedback loop integrates constraint-aware syntactic validation and GNN-based detection (Robust-GNN4TJ) to iteratively improve HT designs and uncover detector weaknesses. Robust-GNN4TJ enhances GNN4TJ with better graph extraction, robust training, and reliable predictions, particularly on LLM-generated HTs.", "result": "On challenging TrojanGYM-generated benchmarks, Robust-GNN4TJ improves HT detection rates from 0% to 60% compared to prior GNN-based detectors. The framework achieves up to 83.33% evasion rates against modern detectors on SRAM, AES-128, and UART designs, demonstrating the effectiveness of generated HTs in exposing detector blind spots.", "conclusion": "TrojanGYM enables systematic generation of diverse and stealthy HTs that reveal robustness gaps in current detectors when tested beyond conventional benchmarks. The work highlights the necessity of dynamic, adversarial benchmarking in hardware security and advances detector development through improved GNN modeling and evaluation rigor."}}
{"id": "2601.17435", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17435", "abs": "https://arxiv.org/abs/2601.17435", "authors": ["Maria Jesus Rodriguez-Sanchez", "Manuel Noguera", "Angel Ruiz-Zafra", "Kawtar Benghazi"], "title": "Towards a Declarative Agentic Layer for Intelligent Agents in MCP-Based Server Ecosystems", "comment": "12 pages, 3 figures", "summary": "Recent advances in Large Language Models (LLMs) have enabled the development of increasingly complex agentic and multi-agent systems capable of planning, tool use and task decomposition. However, empirical evidence shows that many of these systems suffer from fundamental reliability issues, including hallucinated actions, unexecutable plans and brittle coordination. Crucially, these failures do not stem from limitations of the underlying models themselves, but from the absence of explicit architectural structure linking goals, capabilities and execution. This paper presents a declarative, model-independent architectural layer for grounded agentic workflows that addresses this gap. The proposed layer, referred to as DALIA (Declarative Agentic Layer for Intelligent Agents), formalises executable capabilities, exposes tasks through a declarative discovery protocol, maintains a federated directory of agents and their execution resources, and constructs deterministic task graphs grounded exclusively in declared operations. By enforcing a clear separation between discovery, planning and execution, the architecture constrains agent behaviour to a verifiable operational space, reducing reliance on speculative reasoning and free-form coordination. We present the architecture and design principles of the proposed layer and illustrate its operation through a representative task-oriented scenario, demonstrating how declarative grounding enables reproducible and verifiable agentic workflows across heterogeneous environments.", "AI": {"tldr": "This paper introduces DALIA, a declarative, model-independent architecture for intelligent agents that improves reliability by formalizing capabilities, enabling declarative task discovery, and constructing deterministic, verifiable workflows.", "motivation": "Despite advances in LLMs, agentic systems suffer from hallucinations, unexecutable plans, and coordination failures due to lack of architectural structure; DALIA addresses this gap by grounding workflows in explicit declarations.", "method": "The paper proposes DALIA, a declarative architecture that formalizes agent capabilities, uses a discovery protocol, maintains a federated agent directory, and constructs deterministic task graphs based on declared operations.", "result": "The architecture enables reproducible, verifiable, and reliable agent workflows by constraining behavior to a defined operational space, reducing speculative reasoning and improving coordination across heterogeneous environments.", "conclusion": "DALIA provides a model-independent architectural layer that enhances the reliability and verifiability of agentic systems by enforcing declarative grounding, separation of concerns, and deterministic execution."}}
{"id": "2601.17225", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17225", "abs": "https://arxiv.org/abs/2601.17225", "authors": ["Krystal Jackson", "Deepika Raman", "Jessica Newman", "Nada Madkour", "Charlotte Yuan", "Evan R. Murphy"], "title": "Toward Risk Thresholds for AI-Enabled Cyber Threats: Enhancing Decision-Making Under Uncertainty with Bayesian Networks", "comment": null, "summary": "Artificial intelligence (AI) is increasingly being used to augment and automate cyber operations, altering the scale, speed, and accessibility of malicious activity. These shifts raise urgent questions about when AI systems introduce unacceptable or intolerable cyber risk, and how risk thresholds should be identified before harms materialize at scale. In recent years, industry, government, and civil society actors have begun to articulate such thresholds for advanced AI systems, with the goal of signaling when models meaningfully amplify cyber threats, for example, by automating multi-stage intrusions, enabling zero-day discovery, or lowering the expertise required for sophisticated attacks. However, current approaches to determine these thresholds remain fragmented and limited. Many thresholds rely solely on capability benchmarks or narrow threat scenarios, and are weakly connected to empirical evidence. This paper proposes a structured approach to developing and evaluating AI cyber risk thresholds that is probabilistic, evidence-based, and operationalizable. In this paper we make three core contributions that build on our prior work that highlights the limitations of relying solely on capability assessments. First, we analyze existing industry cyber thresholds and identify common threshold elements as well as recurring methodological shortcomings. Second, we propose the use of Bayesian networks as a tool for modeling AI-enabled cyber risk, enabling the integration of heterogeneous evidence, explicit representation of uncertainty, and continuous updating as new information emerges. Third, we illustrate this approach through a focused case study on AI-augmented phishing, demonstrating how qualitative threat insights can be decomposed into measurable variables and recombined into structured risk estimates that better capture how AI changes attacker behavior and outcomes.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.17450", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17450", "abs": "https://arxiv.org/abs/2601.17450", "authors": ["Qingchao Shen"], "title": "Data-driven Test Generation for Fuzzing AI Compiler", "comment": "This paper has been accepted by ICSE 2026 Doctoral Symposium track", "summary": "Artificial Intelligence (AI) compilers are critical for efficiently deploying AI models across diverse hardware platforms. However, they remain prone to bugs that can compromise both compiler reliability and model correctness. Thus, ensuring the quality of AI compilers is crucial. In this work, we present a unified data-driven testing framework that systematically addresses stage-specific challenges in AI compilers. Specifically, OPERA migrates tests for AI libraries to test various operator conversion logic in the model loading stage. OATest synthesizes diverse optimization-aware computational graphs for testing high-level optimizations. HARMONY generates and mutates diverse low-level IR seeds to generate hardware-optimization-aware tests for testing low-level optimizations. Together, these techniques provide a comprehensive, stage-aware framework that enhances testing coverage and effectiveness, detecting 266 previously unknown bugs in four widely used AI compilers.", "AI": {"tldr": "This paper proposes OPERA, OATest, and HARMONY\u2014a unified, data-driven, stage-aware testing framework for AI compilers\u2014that collectively enhance testing by targeting model loading, high-level, and low-level optimizations, uncovering 266 new bugs in major compilers.", "motivation": "AI compilers are essential for deploying models on diverse hardware but are prone to bugs that affect both reliability and model correctness; existing testing methods lack comprehensiveness across different compilation stages.", "method": "The framework employs three stage-specific techniques: OPERA for migrating AI library tests to test operator conversion during model loading; OATest for synthesizing optimization-aware computational graphs to test high-level optimizations; and HARMONY for generating and mutating low-level IR seeds to test low-level, hardware-aware optimizations.", "result": "The framework detected 266 previously unknown bugs in four widely used AI compilers, demonstrating its effectiveness in improving testing coverage and bug detection.", "conclusion": "OPERA, OATest, and HARMONY form a unified, data-driven, and stage-aware testing framework that significantly improves the reliability of AI compilers by detecting a large number of previously unknown bugs."}}
{"id": "2601.17280", "categories": ["cs.CR", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17280", "abs": "https://arxiv.org/abs/2601.17280", "authors": ["David Condrey"], "title": "On the Insecurity of Keystroke-Based AI Authorship Detection: Timing-Forgery Attacks Against Motor-Signal Verification", "comment": "9 pages, 1 figure, 7 tables. Code available at anc/ folder", "summary": "Recent proposals advocate using keystroke timing signals, specifically the coefficient of variation ($\u03b4$) of inter-keystroke intervals, to distinguish human-composed text from AI-generated content. We demonstrate that this class of defenses is insecure against two practical attack classes: the copy-type attack, in which a human transcribes LLM-generated text producing authentic motor signals, and timing-forgery attacks, in which automated agents sample inter-keystroke intervals from empirical human distributions. Using 13,000 sessions from the SBU corpus and three timing-forgery variants (histogram sampling, statistical impersonation, and generative LSTM), we show all attacks achieve $\\ge$99.8% evasion rates against five classifiers. While detectors achieve AUC=1.000 against fully-automated injection, they classify $\\ge$99.8% of attack samples as human with mean confidence $\\ge$0.993. We formalize a non-identifiability result: when the detector observes only timing, the mutual information between features and content provenance is zero for copy-type attacks. Although composition and transcription produce statistically distinguishable motor patterns (Cohen's d=1.28), both yield $\u03b4$ values 2-4x above detection thresholds, rendering the distinction security-irrelevant. These systems confirm a human operated the keyboard, but not whether that human originated the text. Securing provenance requires architectures that bind the writing process to semantic content.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.17482", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17482", "abs": "https://arxiv.org/abs/2601.17482", "authors": ["Yang Liu", "Kaiming Zhang", "Zhuangbin Chen", "Jinyang Liu", "Zibin Zheng"], "title": "LogPrism: Unifying Structure and Variable Encoding for Effective Log Compression", "comment": null, "summary": "The prevailing \"parse-then-compress\" paradigm in log compression fundamentally limits effectiveness by treating log parsing and compression as isolated objectives. While parsers prioritize semantic accuracy (i.e., event identification), they often obscure deep correlations between static templates and dynamic variables that are critical for storage efficiency. In this paper, we investigate this misalignment through a comprehensive empirical study and propose LogPrism, a framework that bridges the gap via unified redundancy encoding. Rather than relying on a rigid pre-parsing step, LogPrism dynamically integrates structural extraction with variable encoding by constructing a Unified Redundancy Tree (URT). This hierarchical approach effectively mines \"structure+variable\" co-occurrence patterns, capturing deep contextual redundancies while accelerating processing through pre-emptive pattern encoding. Extensive experiments on 16 benchmark datasets confirm that LogPrism establishes a new state-of-the-art. It achieves the highest compression ratio on 13 datasets, surpassing leading baselines by margins of 4.7% to 80.9%, while delivering superior throughput at 29.87 MB/s (1.68$\\times$~43.04$\\times$ faster than competitors). Moreover, when configured in single-archive mode to maximize global pattern discovery, LogPrism outperforms the best baseline by 19.39% in compression ratio while maintaining a 2.62$\\times$ speed advantage.", "AI": {"tldr": "LogPrism eliminates the parse-then-compress bottleneck by jointly modeling log structure and variables via a Unified Redundancy Tree, achieving superior compression ratios and speeds across diverse log datasets.", "motivation": "The 'parse-then-compress' paradigm limits compression effectiveness by separating semantic parsing from storage optimization, failing to exploit deep correlations between log structures and variables; LogPrism addresses this misalignment.", "method": "LogPrism introduces a Unified Redundancy Tree (URT) to dynamically integrate structural extraction and variable encoding, enabling joint modeling of static templates and dynamic variables without rigid pre-parsing, thus capturing deep contextual redundancies.", "result": "LogPrism achieves state-of-the-art performance across 16 datasets, attaining the highest compression ratio on 13, outperforming baselines by 4.7% to 80.9%, with throughput of 29.87 MB/s (1.68\u001fx to 43.04\u001fx faster); in single-archive mode, it improves compression by 19.39% with 2.62\u001fx speedup.", "conclusion": "LogPrism redefines log compression by unifying parsing and compression into a single framework, effectively overcoming the limitations of the 'parse-then-compress' paradigm and setting a new standard in both compression efficiency and processing speed."}}
{"id": "2601.17355", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.17355", "abs": "https://arxiv.org/abs/2601.17355", "authors": ["Yi Lyu", "Shichun Yu", "Joe Catudal"], "title": "Safeguard: Security Controls at the Software Defined Network Layer", "comment": null, "summary": "Improvements in software defined networking allow for policy to be informed and modified by data-driven applications that can adjust policy to accommodate fluctuating requirements at line speed. However, there is some concern that over-correction can occur and cause unintended consequences depending on the data received. This is particularly problematic for network security features, such as machine-learning intrusion detection systems. We present Safeguard, a rule-based policy that overlaps a data-driven policy to prevent unintended responses for edge cases in network traffic. We develop a reference implementation of a network traffic classifier that enforces firewall rules for malicious traffic, and show how additional rulesets to allow known-good traffic are essential in utilizing a data-driven network policy.", "AI": {"tldr": "Safeguard is a rule-based policy framework that overlays data-driven policies in software-defined networks to prevent over-correction, especially in security applications like intrusion detection, by incorporating firewall rules and exceptions for known-good traffic.", "motivation": "Data-driven policies in software-defined networking enable adaptive, real-time policy adjustment, but risk over-correction due to unpredictable or noisy data, leading to unintended network behavior\u2014especially critical in security contexts such as machine-learning-based intrusion detection.", "method": "The authors propose Safeguard, a rule-based overlay policy that constrains data-driven decisions by enforcing predefined rules. They implement a reference network traffic classifier that applies firewall rules to block malicious traffic and introduce additional rulesets to explicitly allow known-good traffic, ensuring stable and safe policy enforcement.", "result": "The reference implementation demonstrates that combining data-driven policies with static, rule-based safeguards prevents erroneous blocking or misclassification of edge-case traffic. Results show that allowing known-good traffic via additional rulesets is crucial for maintaining network availability and policy correctness.", "conclusion": "Integrating rule-based safeguards with data-driven network policies enhances robustness and reliability, particularly in security-critical applications, by preventing over-correction and ensuring safe operation under uncertain or anomalous conditions."}}
{"id": "2601.17558", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17558", "abs": "https://arxiv.org/abs/2601.17558", "authors": ["J. P. Fleischer", "Tanchanok Sirikanchittavon", "Chonlachart Jeenprasom", "Nooshin Yousefzadeh", "Sanjay Ranka", "Mohammed Hadi"], "title": "Measuring Braking Behavior Using Vehicle Tracking and Camera-to-Satellite Homography Rectification", "comment": "VEHITS 2026", "summary": "This paper presents an open-source software application for analyzing traffic camera footage, focusing on vehicle behavior and braking events at signalized urban highways. The core innovation is a robust ground-plane homography estimation that links fixed traffic camera views to satellite orthoimagery. This process rectifies the camera's oblique perspective, ensuring that pixel distances accurately represent real-world distances. This enables the acquisition of features such as vehicle trajectory, speed, deceleration, and braking severity without the need for camera calibration. The pipeline employs the MAGSAC++ estimator to build the homography, converting YOLO11 object detections into a rectified top-down coordinate system. All detection and trajectory data are stored in a ClickHouse database for subsequent analysis. A real-world case study at two signalized intersections in Key West, Florida, showcased the system's capabilities. Across two days of daytime footage, braking activity at the higher-volume intersection peaked around 4 PM at approximately 57.5 events per hour, while the second intersection peaked around 10 AM at roughly 15.5 events per hour. The spatial analysis revealed that most braking events initiated upstream, with mild and moderate braking mostly occurring 30 to 45+ meters away from the stop bar and severe braking distributed throughout, but particularly concentrated in lanes with higher interaction and merging activity. The findings highlight the significant potential of this centralized safety information system to support connected vehicles, facilitating proactive traffic management, crash mitigation, and data-driven roadway design and safety analysis.", "AI": {"tldr": "This paper introduces an open-source system for analyzing traffic camera footage using homography estimation to map vehicle movements accurately without camera calibration, enabling detailed braking and trajectory analysis for urban traffic safety.", "motivation": "The motivation is to enable accurate, calibration-free analysis of vehicle behavior\u2014particularly braking events\u2014from fixed traffic camera footage in urban signalized highways, supporting large-scale traffic safety assessment and infrastructure improvement without requiring specialized hardware or manual calibration.", "method": "The method involves using robust ground-plane homography estimation (via MAGSAC++) to align traffic camera footage with satellite orthoimagery, enabling perspective rectification without camera calibration. YOLO11 detections are transformed into a top-down coordinate system to extract vehicle trajectories, speed, and braking events, with data stored in a ClickHouse database for analysis.", "result": "The system was tested at two intersections in Key West, Florida. Braking events peaked at 57.5/hour (4 PM) at the higher-volume intersection and 15.5/hour (10 AM) at the other. Most braking started upstream of the stop bar; mild-to-moderate braking occurred 30\u201345+ meters away, while severe braking was more common in high-interaction lanes.", "conclusion": "The proposed system demonstrates significant potential as a centralized safety information system for supporting connected vehicles and improving urban traffic safety through proactive management, crash mitigation, and data-driven infrastructure design."}}
{"id": "2601.17356", "categories": ["cs.CR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.17356", "abs": "https://arxiv.org/abs/2601.17356", "authors": ["Yao Zhao", "Zhang Sheng", "Shengchen Duan", "Shen Wang"], "title": "From Scores to Queues: Operationalizing Cross-Chain Obfuscation Signals for Smart-Contract Audits", "comment": null, "summary": "Obfuscation substantially increases the interpretation cost of smart-contract auditing, while the comparability and transferability of obfuscation signals across chains remain unclear. We present HObfNET as an efficient surrogate of Obfs_Tool (ObfProbe), enabling fast cross-chain scoring at scale. The model aligns well with tool outputs on Ethereum (PCC 0.9158, MAPE 8.20 percent) and achieves 8-9 ms per contract, a 2.3k-5.2k times speedup over second-level Obfs_Tool runs, enabling million-scale scoring. On large BSC, Polygon, and Avalanche corpora, we find systematic score drift: fixed-threshold transfer inflates and deflates candidate queues, motivating within-chain main and extreme thresholds (p99 and p99.9) and an actionable queueing strategy. The high-score tail exhibits rare selectors, external-call opcode enrichment, and low signature density; a proxy indicator is enriched in the BSC high-score queue, enabling secondary triage. Cross-chain reuse analysis shows tail enrichment and directional diffusion, with traceable same-hash cases across chains. In publicly alignable incident samples, all fall into the p99 queue; Transit Swap DEX Hack and New Free DAO Flash Loan exhibit cross-chain spillover, indicating real-world hit and prioritization value. We deliver a two-tier audit queue and cross-chain linkage workflow to support practical multi-chain security operations.", "AI": {"tldr": "HObfNET enables fast, scalable cross-chain smart contract obfuscation scoring, revealing systematic score drift and enabling effective audit prioritization and cross-chain threat linkage.", "motivation": "Smart contract obfuscation complicates auditing, and existing tools are too slow for large-scale, cross-chain analysis; further, the transferability of obfuscation signals across blockchains is not well understood.", "method": "HObfNET is developed as a fast surrogate model of Obfs_Tool (ObfProbe), trained to replicate obfuscation scores with high fidelity on Ethereum and extended to BSC, Polygon, and Avalanche, using statistical thresholds (p99, p99.9) for queueing and cross-chain reuse analysis.", "result": "HObfNET achieves a 2.3k-5.2k times speedup (8-9 ms per contract) over Obfs_Tool with high correlation (PCC 0.9158) on Ethereum; it reveals systematic cross-chain score drift, enabling chain-specific thresholds; high-score contracts show distinct features like rare selectors and low signature density; cross-chain analysis detects spillover incidents like Transit Swap and New Free DAO exploits, with all public incidents falling into the p99 queue.", "conclusion": "HObfNET provides an efficient, scalable solution for cross-chain smart contract obfuscation scoring, enabling effective multi-chain security operations through a two-tier audit queue and cross-chain linkage workflow."}}
{"id": "2601.17581", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17581", "abs": "https://arxiv.org/abs/2601.17581", "authors": ["Daniel Ogenrwot", "John Businge"], "title": "How AI Coding Agents Modify Code: A Large-Scale Study of GitHub Pull Requests", "comment": "5 pages, 5 figures", "summary": "AI coding agents are increasingly acting as autonomous contributors by generating and submitting pull requests (PRs). However, we lack empirical evidence on how these agent-generated PRs differ from human contributions, particularly in how they modify code and describe their changes. Understanding these differences is essential for assessing their reliability and impact on development workflows. Using the MSR 2026 Mining Challenge version of the AIDev dataset, we analyze 24,014 merged Agentic PRs (440,295 commits) and 5,081 merged Human PRs (23,242 commits). We examine additions, deletions, commits, and files touched, and evaluate the consistency between PR descriptions and their diffs using lexical and semantic similarity. Agentic PRs differ substantially from Human PRs in commit count (Cliff's $\u03b4= 0.5429$) and show moderate differences in files touched and deleted lines. They also exhibit slightly higher description-to-diff similarity across all measures. These findings provide a large-scale empirical characterization of how AI coding agents contribute to open source development.", "AI": {"tldr": "AI coding agents generate PRs with more commits and slightly more consistent descriptions than humans, revealing distinct behavioral patterns in open-source contributions.", "motivation": "To understand how AI coding agents' contributions differ from human contributions in terms of code modifications and PR descriptions, providing insights into their reliability and integration into development workflows.", "method": "Analyzed 24,014 merged Agentic PRs and 5,081 merged Human PRs from the MSR 2026 Mining Challenge AIDev dataset, comparing commit counts, files touched, additions/deletions, and lexical/semantic similarity between PR descriptions and diffs.", "result": "Agentic PRs have significantly more commits (Cliff's \u03b4 = 0.5429), moderate differences in files touched and deleted lines, and slightly higher lexical and semantic similarity between descriptions and diffs compared to human PRs.", "conclusion": "AI coding agents contribute to open-source development in a distinguishable manner from humans, with more commits and moderately different code modification patterns, while showing slightly better alignment between their PR descriptions and code changes."}}
{"id": "2601.17378", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17378", "abs": "https://arxiv.org/abs/2601.17378", "authors": ["Mohammad Zare", "Pirooz Shamsinejadbabaki"], "title": "Res-MIA: A Training-Free Resolution-Based Membership Inference Attack on Federated Learning Models", "comment": null, "summary": "Membership inference attacks (MIAs) pose a serious threat to the privacy of machine learning models by allowing adversaries to determine whether a specific data sample was included in the training set. Although federated learning (FL) is widely regarded as a privacy-aware training paradigm due to its decentralized nature, recent evidence shows that the final global model can still leak sensitive membership information through black-box access. In this paper, we introduce Res-MIA, a novel training-free and black-box membership inference attack that exploits the sensitivity of deep models to high-frequency input details. Res-MIA progressively degrades the input resolution using controlled downsampling and restoration operations, and analyzes the resulting confidence decay in the model's predictions. Our key insight is that training samples exhibit a significantly steeper confidence decline under resolution erosion compared to non-member samples, revealing a robust membership signal. Res-MIA requires no shadow models, no auxiliary data, and only a limited number of forward queries to the target model. We evaluate the proposed attack on a federated ResNet-18 trained on CIFAR-10, where it consistently outperforms existing training-free baselines and achieves an AUC of up to 0.88 with minimal computational overhead. These findings highlight frequency-sensitive overfitting as an important and previously underexplored source of privacy leakage in federated learning, and emphasize the need for privacy-aware model designs that reduce reliance on fine-grained, non-robust input features.", "AI": {"tldr": "Res-MIA is a training-free, black-box membership inference attack on federated learning that exploits the sensitivity of models to high-frequency input details by measuring confidence decay under controlled resolution reduction, achieving strong performance without shadow training or auxiliary data.", "motivation": "Despite federated learning's decentralized and privacy-preserving design, recent studies show that global models can still leak membership information through black-box access; existing attacks often require shadow training or auxiliary data, so a simpler, more practical attack is needed to uncover new leakage sources.", "method": "Res-MIA is a training-free, black-box membership inference attack that performs controlled downsampling and restoration of input images to induce resolution erosion, then analyzes the decay in model prediction confidence; members are identified by their steeper confidence drop under reduced resolution.", "result": "On a federated ResNet-18 trained on CIFAR-10, Res-MIA achieves up to 0.88 AUC, outperforming existing training-free attacks with minimal computational cost and query overhead, without requiring shadow models or auxiliary datasets.", "conclusion": "Res-MIA reveals that frequency-sensitive overfitting is a significant source of privacy leakage in federated learning, demonstrating that models can inadvertently memorize high-frequency details of training data, and highlights the need for privacy-aware designs that minimize reliance on non-robust features."}}
{"id": "2601.17584", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17584", "abs": "https://arxiv.org/abs/2601.17584", "authors": ["Mahmoud Samir Fayed", "Ahmed Samir Fayed"], "title": "Prompt Driven Development with Claude Code: Building a Complete TUI Framework for the Ring Programming Language", "comment": null, "summary": "Large language models are increasingly used in software development, yet their ability to generate and maintain large, multi module systems through natural language interaction remains insufficiently characterized. This study presents an empirical analysis of developing a 7420 line Terminal User Interface framework for the Ring programming language, completed in roughly ten hours of active work spread across three days using a purely prompt driven workflow with Claude Code, Opus 4.5. The system was produced through 107 prompts: 21 feature requests, 72 bug fix prompts, 9 prompts sharing information from Ring documentation, 4 prompts providing architectural guidance, and 1 prompt dedicated to generating documentation. Development progressed across five phases, with the Window Manager phase requiring the most interaction, followed by complex UI systems and controls expansion. Bug related prompts covered redraw issues, event handling faults, runtime errors, and layout inconsistencies, while feature requests focused primarily on new widgets, window manager capabilities, and advanced UI components. Most prompts were short, reflecting a highly iterative workflow in which the human role was limited to specifying requirements, validating behaviour, and issuing corrective prompts without writing any code manually. The resulting framework includes a complete windowing subsystem, event driven architecture, interactive widgets, hierarchical menus, grid and tree components, tab controls, and a multi window desktop environment. By combining quantitative prompt analysis with qualitative assessment of model behaviour, this study provides empirical evidence that modern LLMs can sustain architectural coherence and support the construction of production grade tooling for emerging programming languages, highlighting prompt driven development as a viable methodology within software engineering practice.", "AI": {"tldr": "A 7420-line TUI framework for Ring was built using only 107 prompts with Claude Code Opus 4.5, demonstrating that LLMs can sustain coherent, large-scale software development through natural language, enabling viable prompt-driven software engineering.", "motivation": "To characterize the capability of large language models in generating and maintaining large-scale, modular software systems through natural language interactions, particularly for emerging programming languages lacking mature tooling.", "method": "Empirical analysis of a 7420-line Terminal User Interface framework development for the Ring programming language using 107 prompts over three days, categorized into feature requests, bug fixes, documentation references, architectural guidance, and documentation generation, with qualitative and quantitative assessment of model behavior.", "result": "Successful construction of a full-featured TUI framework including window management, event handling, interactive widgets, menus, layouts, and multi-window support, developed in ~10 hours via 107 short, iterative prompts without manual coding.", "conclusion": "Modern large language models, exemplified by Claude Code Opus 4.5, can effectively support the development of production-grade, multi-module software systems through a prompt-driven approach, maintaining architectural coherence over extended development sessions."}}
{"id": "2601.17379", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17379", "abs": "https://arxiv.org/abs/2601.17379", "authors": ["Khoi Trinh", "Scott Seidenberger", "Joseph Spracklen", "Raveen Wijewickrama", "Bimal Viswanath", "Murtuza Jadliwala", "Anindya Maiti"], "title": "Prompt and Circumstances: Evaluating the Efficacy of Human Prompt Inference in AI-Generated Art", "comment": "To appear in EvoMUSART 2026", "summary": "The emerging field of AI-generated art has witnessed the rise of prompt marketplaces, where creators can purchase, sell, or share prompts to generate unique artworks. These marketplaces often assert ownership over prompts, claiming them as intellectual property. This paper investigates whether concealed prompts sold on prompt marketplaces can be considered bona fide intellectual property, given that humans and AI tools may be able to infer the prompts based on publicly advertised sample images accompanying each prompt on sale. Specifically, our study aims to assess (i) how accurately humans can infer the original prompt solely by examining an AI-generated image, with the goal of generating images similar to the original image, and (ii) the possibility of improving upon individual human and AI prompt inferences by crafting combined human and AI prompts with the help of a large language model. Although previous research has explored AI-driven prompt inference and protection strategies, our work is the first to incorporate a human subject study and examine collaborative human-AI prompt inference in depth. Our findings indicate that while prompts inferred by humans and prompts inferred through a combined human and AI effort can generate images with a moderate level of similarity, they are not as successful as using the original prompt. Moreover, combining human- and AI-inferred prompts using our suggested merging techniques did not improve performance over purely human-inferred prompts.", "AI": {"tldr": "This study evaluates the intellectual property status of AI art prompts by testing how well humans and AI can reverse-engineer them from images; results show moderate inference success but no improvement when combining human and AI efforts, suggesting limitations in protecting such prompts.", "motivation": "To examine whether AI-generated art prompts can be considered genuine intellectual property when concealed, given that sample images may allow inference by humans or AI.", "method": "Conducted a human subject study to evaluate human accuracy in inferring prompts from images; compared human-inferred prompts with AI-inferred ones and explored combining both using a large language model for improved inference.", "result": "Humans can moderately infer prompts from images; combined human-AI inference via LLMs did not outperform human-only inference, and neither matched the quality of the original prompt in generating similar images.", "conclusion": "Concealed prompts sold on marketplaces cannot be robustly protected as intellectual property since both humans and AI can infer them with moderate success, but the original prompts remain superior for generating similar images."}}
{"id": "2601.17604", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17604", "abs": "https://arxiv.org/abs/2601.17604", "authors": ["Suborno Deb Bappon", "Saikat Mondal", "Chanchal K. Roy", "Kevin Schneider"], "title": "Human-Aligned Enhancement of Programming Answers with LLMs Guided by User Feedback", "comment": "Preprint", "summary": "Large Language Models (LLMs) are widely used to support software developers in tasks such as code generation, optimization, and documentation. However, their ability to improve existing programming answers in a human-like manner remains underexplored. On technical question-and-answer platforms such as Stack Overflow (SO), contributors often revise answers based on user comments that identify errors, inefficiencies, or missing explanations. Yet roughly one-third of this feedback is never addressed due to limited time, expertise, or visibility, leaving many answers incomplete or outdated. This study investigates whether LLMs can enhance programming answers by interpreting and incorporating comment-based feedback. We make four main contributions. First, we introduce ReSOlve, a benchmark consisting of 790 SO answers with associated comment threads, annotated for improvement-related and general feedback. Second, we evaluate four state-of-the-art LLMs on their ability to identify actionable concerns, finding that DeepSeek achieves the best balance between precision and recall. Third, we present AUTOCOMBAT, an LLM-powered tool that improves programming answers by jointly leveraging user comments and question context. Compared to human revised references, AUTOCOMBAT produces near-human quality improvements while preserving the original intent and significantly outperforming the baseline. Finally, a user study with 58 practitioners shows strong practical value, with 84.5 percent indicating they would adopt or recommend the tool. Overall, AUTOCOMBAT demonstrates the potential of scalable, feedback-driven answer refinement to improve the reliability and trustworthiness of technical knowledge platforms.", "AI": {"tldr": "This paper introduces ReSOlve, a benchmark of 790 Stack Overflow answers with comment threads, and proposes AUTOCOMBAT, an LLM-powered system that improves programming answers by incorporating user feedback. Results show that AUTOCOMBAT achieves near-human quality and high user acceptance, demonstrating the potential of LLMs in scalable, feedback-driven knowledge refinement.", "motivation": "On technical Q&A platforms like Stack Overflow, many user comments pointing out issues in programming answers go unaddressed due to time, expertise, or visibility constraints. This limits the quality and reliability of shared technical knowledge. The study aims to explore whether LLMs can effectively and human-likely improve programming answers by acting on comment-based feedback.", "method": "The study introduces ReSOlve, a benchmark of 790 Stack Overflow answer-comment pairs annotated for improvement-related feedback, and evaluates four state-of-the-art LLMs on identifying actionable concerns. It proposes AUTOCOMBAT, an LLM-based tool that integrates user comments and question context to improve answers. The approach is validated against human revisions and assessed via a user study with 58 practitioners.", "result": "DeepSeek shows the best performance in identifying actionable feedback. AUTOCOMBAT generates improvements that are close to human-level quality, preserving original intent and significantly outperforming baselines. A user study with 58 developers reveals that 84.5% would adopt or recommend the tool.", "conclusion": "AUTOCOMBAT demonstrates the feasibility and practical value of using LLMs to refine programming answers on technical platforms by leveraging user feedback, thereby enhancing the reliability and trustworthiness of community-driven knowledge."}}
{"id": "2601.17471", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17471", "abs": "https://arxiv.org/abs/2601.17471", "authors": ["Wonyoung Kim", "Seunggi Min", "Minjae Gwon", "Dowoo Baik", "Haein Lee", "Hyeon Heo", "Minjae Lee", "Min Woo Baek", "Yonghwi Jin", "Younggi Park", "Yunjae Choi", "Taesoo Kim", "Sangdon Park", "Insu Yun"], "title": "PatchIsland: Orchestration of LLM Agents for Continuous Vulnerability Repair", "comment": null, "summary": "Continuous fuzzing platforms such as OSS-Fuzz uncover large numbers of vulnerabilities, yet the subsequent repair process remains largely manual. Unfortunately, existing Automated Vulnerability Repair (AVR) techniques -- including recent LLM-based systems -- are not directly applicable to continuous fuzzing. This is because these systems are designed and evaluated on a static, single-run benchmark setting, making them ill-suited for the diverse, noisy, and failure-prone environments in continuous fuzzing.\n  To address these issues, we introduce PatchIsland, a system for Continuous Vulnerability Repair (CVR) that tightly integrates with continuous fuzzing pipelines. PatchIsland employs an ensemble of diverse LLM agents. By leveraging multiple LLM agents, PatchIsland can cover a wider range of settings (e.g., different projects, bug types, and programming languages) and also improve operational robustness. In addition, PatchIsland utilizes a two-phase patch-based deduplication to mitigate duplicate crashes and patches, which can be problematic in continuous fuzzing.\n  In our internal evaluation, PatchIsland repaired 84 of 92 vulnerabilities, demonstrating strong repair capability. In the official AIxCC competition, the system operated with no human intervention in a fully autonomous environment and successfully patched 31 out of 43 vulnerabilities, achieving a repair rate of 72.1\\%.", "AI": {"tldr": "PatchIsland is a Continuous Vulnerability Repair (CVR) system that integrates with continuous fuzzing platforms using an ensemble of LLM agents and two-phase patch deduplication, achieving high repair rates in autonomous, real-world settings.", "motivation": "Existing Automated Vulnerability Repair (AVR) techniques, including LLM-based systems, are designed for static, single-run evaluations and fail in the dynamic, noisy context of continuous fuzzing; thus, a system tailored for continuous environments is needed.", "method": "PatchIsland uses an ensemble of diverse LLM agents and implements a two-phase patch-based deduplication mechanism to address the challenges of repairing vulnerabilities in continuous fuzzing environments; it is tightly integrated into continuous fuzzing workflows.", "result": "PatchIsland repaired 84 out of 92 vulnerabilities in internal evaluation and 31 out of 43 in the official AIxCC competition, achieving a 72.1% repair rate in fully autonomous operation.", "conclusion": "PatchIsland demonstrates that Continuous Vulnerability Repair (CVR) is feasible and effective when integrated with continuous fuzzing pipelines, and it sets a new standard for automation in vulnerability repair through its use of multiple LLM agents and patch deduplication."}}
{"id": "2601.17627", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17627", "abs": "https://arxiv.org/abs/2601.17627", "authors": ["Dung Pham", "Taher A. Ghaleb"], "title": "Code Change Characteristics and Description Alignment: A Comparative Study of Agentic versus Human Pull Requests", "comment": "Accepted at the 23rd International Conference on Mining Software Repositories (MSR '26)", "summary": "AI coding agents can autonomously generate pull requests (PRs), yet little is known about how their contributions compare to those of humans. We analyze 33,596 agent-generated PRs (APRs) and 6,618 human PRs (HPRs) to compare code-change characteristics and message quality. We observe that APR-introduced symbols (functions and classes) are removed much sooner than those in HPRs (median time to removal 3 vs. 34 days) and are also removed more often (symbol churn 7.33% vs. 4.10%), reflecting a focus on other tasks like documentation and test updates. Agents generate stronger commit-level messages (semantic similarity 0.72 vs. 0.68) but lag humans at PR-level summarization (PR-commit similarity 0.86 vs. 0.88). Commit message length is the best predictor of description quality, indicating reliance on individual commits over full-PR reasoning. These findings highlight a gap between agents' micro-level precision and macro-level communication, suggesting opportunities to improve agent-driven development workflows.", "AI": {"tldr": "AI coding agents generate more short-lived code changes and excel at writing atomic commit messages but struggle with holistic PR-level summarization, revealing a disconnect between local precision and global understanding in automated development.", "motivation": "To understand how AI coding agents' contributions differ from human contributions in software development, particularly in terms of code changes and communication quality.", "method": "The study compares 33,596 agent-generated PRs (APRs) and 6,618 human PRs (HPRs) based on code-change characteristics (e.g., symbol churn, time to removal) and message quality (semantic similarity at commit and PR levels).", "result": "APRs introduce symbols that are removed sooner (median 3 vs. 34 days) and more frequently (symbol churn 7.33% vs. 4.10%) than HPRs, reflecting a focus on non-core tasks. Agents produce better commit messages (similarity 0.72 vs. 0.68) but worse PR summaries (PR-commit similarity 0.86 vs. 0.88). Message length is the strongest predictor of quality.", "conclusion": "There is a notable gap between AI coding agents' micro-level precision in generating commit messages and their macro-level ability to provide coherent PR-level summaries, indicating room for improvement in agent-driven development workflows."}}
{"id": "2601.17497", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.17497", "abs": "https://arxiv.org/abs/2601.17497", "authors": ["Mohammed Barhoush", "Arthur Mehta", "Anne M\u00fcller", "Louis Salvail"], "title": "On the Impossibility of Simulation Security for Quantum Functional Encryption", "comment": "28 pages", "summary": "Functional encryption is a powerful cryptographic primitive that enables fine-grained access to encrypted data and underlies numerous applications. Although the ideal security notion for FE (simulation security) has been shown to be impossible in the classical setting, those impossibility results rely on inherently classical arguments. This leaves open the question of whether simulation-secure functional encryption can be achieved in the quantum regime.\n  In this work, we rule out this possibility by showing that the classical impossibility results largely extend to the quantum world. In particular, when the adversary can issue an unbounded number of challenge messages, we prove an unconditional impossibility, matching the classical barrier. In the case where the adversary may obtain many functional keys, classical arguments only yield impossibility under the assumption of pseudorandom functions; we strengthen this by proving impossibility under the potentially weaker assumption of pseudorandom quantum states. In the same setting, we also establish an alternative impossibility based on public-key encryption. Since public-key encryption is not known to imply pseudorandom quantum states, this provides independent evidence of the barrier. As part of our proofs, we show a novel incompressibility property for pseudorandom states, which may be of independent interest.", "AI": {"tldr": "This paper proves that simulation-secure functional encryption is impossible in the quantum setting, extending classical impossibility results and introducing new quantum-based arguments using pseudorandom quantum states and public-key encryption.", "motivation": "To determine whether simulation-secure functional encryption, impossible in the classical setting, could be achievable in the quantum setting due to fundamentally different security properties.", "method": "The authors extend classical impossibility arguments to the quantum setting, employing pseudorandom quantum states and public-key encryption assumptions, and introduce a novel incompressibility property for pseudorandom quantum states in their proofs.", "result": "Unconditional impossibility of simulation-secure FE when adversaries issue unbounded challenge queries; impossibility under pseudorandom quantum states and independently under public-key encryption when adversaries obtain many functional keys.", "conclusion": "Simulation-secure functional encryption remains unachievable in the quantum regime, as classical impossibility results extend to quantum settings, with strengthened impossibility proofs based on pseudorandom quantum states and public-key encryption."}}
{"id": "2601.17762", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17762", "abs": "https://arxiv.org/abs/2601.17762", "authors": ["Zelong Zheng", "Jiayuan Zhou", "Xing Hu", "Yi Gao", "Shengyi Pan"], "title": "Multi-Agent End-to-End Vulnerability Management for Mitigating Recurring Vulnerabilities", "comment": null, "summary": "Software vulnerability management has become increasingly critical as modern systems scale in size and complexity. However, existing automated approaches remain insufficient. Traditional static analysis methods struggle to precisely capture contextual dependencies, especially when vulnerabilities span multiple functions or modules. Large language models (LLMs) often lack the ability to retrieve and exploit sufficient contextual information, resulting in incomplete reasoning and unreliable outcomes. Meanwhile, recurring vulnerabilities emerge repeatedly due to code reuse and shared logic, making historical vulnerability knowledge an indispensable foundation for effective vulnerability detection and repair. Nevertheless, prior approaches such as clone-based detection and patch porting, have not fully leveraged this knowledge. To address these challenges, we present MAVM, a multi-agent framework for end-to-end recurring vulnerability management. MAVM integrates five components, including a vulnerability knowledge base, detection, confirmation, repair, and validation, into a unified multi-agent pipeline. We construct a knowledge base from publicly disclosed vulnerabilities, thereby addressing the underuse of historical knowledge in prior work and mitigating the lack of domain-specific expertise in LLMs. Furthermore, we design context-retrieval tools that allow agents to extract and reason over repository-level information, overcoming the contextual limitations of previous methods. Based on agents, MAVM effectively simulates real-world security workflows. To evaluate the performance of MAVM, we construct a dataset containing 78 real-world patch-porting cases (covering 114 function-level migrations). On this dataset, MAVM successfully detects and repairs 51 real vulnerabilities, outperforming baselines by 31.9%-45.2% in repair accuracy, which demonstrates its effectiveness.", "AI": {"tldr": "MAVM is a multi-agent framework for end-to-end recurring vulnerability management that leverages a historical vulnerability knowledge base and context-retrieval tools to improve detection, confirmation, repair, and validation. It outperforms baselines by 31.9%\u001345.2% in repair accuracy on a real-world dataset.", "motivation": "Existing automated vulnerability management approaches suffer from limited contextual understanding, insufficient use of historical vulnerability knowledge, and poor generalization across reused code patterns. These limitations hinder accurate detection and reliable repair of recurring vulnerabilities.", "method": "The authors propose MAVM, a multi-agent framework consisting of five components\u0014vulnerability knowledge base, detection, confirmation, repair, and validation\u0014that operate in a unified pipeline. They build a knowledge base from publicly disclosed vulnerabilities and design context-retrieval tools to enhance contextual reasoning across codebases.", "result": "MAVM was evaluated on a dataset of 78 real-world patch-porting cases (114 function-level migrations) and successfully detected and repaired 51 real vulnerabilities, achieving 31.9%\u001345.2% higher repair accuracy than baseline methods.", "conclusion": "MAVM demonstrates significant effectiveness in managing recurring software vulnerabilities by integrating historical knowledge and repository-level context into a multi-agent framework, outperforming existing methods in real-world patch-porting scenarios."}}
{"id": "2601.17533", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17533", "abs": "https://arxiv.org/abs/2601.17533", "authors": ["Silong Chen", "Yuchuan Luo", "Guilin Deng", "Yi Liu", "Min Xu", "Shaojing Fu", "Xiaohua Jia"], "title": "Reconstructing Training Data from Adapter-based Federated Large Language Models", "comment": "Yuchuan Luo and Yi Liu are co-corresponding authors. Accepted at The Web Conference (WWW) 2026", "summary": "Adapter-based Federated Large Language Models (FedLLMs) are widely adopted to reduce the computational, storage, and communication overhead of full-parameter fine-tuning for web-scale applications while preserving user privacy. By freezing the backbone and training only compact low-rank adapters, these methods appear to limit gradient leakage and thwart existing Gradient Inversion Attacks (GIAs).\n  Contrary to this assumption, we show that low-rank adapters create new, exploitable leakage channels. We propose the Unordered-word-bag-based Text Reconstruction (UTR) attack, a novel GIA tailored to the unique structure of adapter-based FedLLMs. UTR overcomes three core challenges: low-dimensional gradients, frozen backbones, and combinatorially large reconstruction spaces by: (i) inferring token presence from attention patterns in frozen layers, (ii) performing sentence-level inversion within the low-rank subspace of adapter gradients, and (iii) enforcing semantic coherence through constrained greedy decoding guided by language priors. Extensive experiments across diverse models (GPT2-Large, BERT, Qwen2.5-7B) and datasets (CoLA, SST-2, Rotten Tomatoes) demonstrate that UTR achieves near-perfect reconstruction accuracy (ROUGE-1/2 > 99), even with large batch size settings where prior GIAs fail completely. Our results reveal a fundamental tension between parameter efficiency and privacy in FedLLMs, challenging the prevailing belief that lightweight adaptation inherently enhances security. Our code and data are available at https://github.com/shwksnshwowk-wq/GIA.", "code_url": "https://github.com/shwksnshwowk-wq/GIA", "code_stars": 0, "code_last_update": "2025-10-16", "AI": {"tldr": "Adapter-based Federated Large Language Models are vulnerable to a new Gradient Inversion Attack (UTR) that reconstructs input text with >99 ROUGE score by exploiting attention patterns and low-rank gradient structures, challenging the notion that parameter-efficient tuning ensures privacy.", "motivation": "The motivation stems from the widely held belief that adapter-based fine-tuning in Federated Learning enhances privacy by reducing gradient exposure. However, the authors question this assumption, investigating whether low-rank adapters, despite their efficiency, may still leak sensitive information through new leakage channels.", "method": "The authors propose the Unordered-word-bag-based Text Reconstruction (UTR) attack, which exploits the structure of adapter-based FedLLMs by (i) inferring token presence from attention patterns in frozen layers, (ii) performing sentence-level inversion within the low-rank subspace of adapter gradients, and (iii) using constrained greedy decoding with language priors to ensure semantic coherence during reconstruction.", "result": "Extensive experiments across multiple models (GPT2-Large, BERT, Qwen2.5-7B) and datasets (CoLA, SST-2, Rotten Tomatoes) show that the UTR attack achieves near-perfect text reconstruction (ROUGE-1/2 > 99), even in large batch settings where previous Gradient Inversion Attacks fail. This demonstrates significant privacy risks in adapter-based FedLLMs.", "conclusion": "The study concludes that despite the assumption that low-rank adapters in Federated Large Language Models enhance privacy by limiting gradient leakage, they actually introduce new vulnerabilities. The proposed UTR attack effectively reconstructs input text with high accuracy, revealing a fundamental tension between parameter efficiency and privacy in adapter-based FedLLMs."}}
{"id": "2601.17888", "categories": ["cs.SE", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.17888", "abs": "https://arxiv.org/abs/2601.17888", "authors": ["Monika Santra", "Bokai Zhang", "Mark Lim", "Vishnu Asutosh Dasu", "Dongrui Zeng", "Gang Tan"], "title": "iResolveX: Multi-Layered Indirect Call Resolution via Static Reasoning and Learning-Augmented Refinement", "comment": null, "summary": "Indirect call resolution remains a key challenge in reverse engineering and control-flow graph recovery, especially for stripped or optimized binaries. Static analysis is sound but often over-approximates, producing many false positives, whereas machine-learning approaches can improve precision but may sacrifice completeness and generalization. We present iResolveX, a hybrid multi-layered framework that combines conservative static analysis with learning-based refinement. The first layer applies a conservative value-set analysis (BPA) to ensure high recall. The second layer adds a learning-based soft-signature scorer (iScoreGen) and selective inter-procedural backward analysis with memory inspection (iScoreRefine) to reduce false positives. The final output, p-IndirectCFG, annotates indirect edges with confidence scores, enabling downstream analyses to choose appropriate precision--recall trade-offs. Across SPEC CPU2006 and real-world binaries, iScoreGen reduces predicted targets by 19.2% on average while maintaining BPA-level recall (98.2%). Combined with iScoreRefine, the total reduction reaches 44.3% over BPA with 97.8% recall (a 0.4% drop). iResolveX supports both conservative, recall-preserving and F1-optimized configurations and outperforms state-of-the-art systems.", "AI": {"tldr": "iResolveX is a hybrid framework for indirect call resolution that combines conservative static analysis with learning-based refinement to achieve high recall and improved precision, reducing false positives by up to 44.3% while maintaining over 97% recall, and producing confidence-annotated control-flow graphs for flexible downstream use.", "motivation": "Indirect call resolution is critical for accurate control-flow graph recovery in stripped or optimized binaries, but existing methods face limitations: static analysis suffers from high false positives, while machine learning approaches often compromise completeness and generalization.", "method": "iResolveX employs a three-layer hybrid approach: (1) conservative value-set analysis (BPA) for high recall, (2) a learning-based soft-signature scorer (iScoreGen) to filter unlikely targets, and (3) selective inter-procedural backward analysis with memory inspection (iScoreRefine) to further reduce false positives; the resulting p-IndirectCFG annotates indirect edges with confidence scores.", "result": "iScoreGen alone reduces predicted targets by 19.2% on average while maintaining 98.2% recall; with iScoreRefine, total target reduction reaches 44.3% over BPA with only a 0.4% recall drop (97.8%); iResolveX supports both recall-preserving and F1-optimized configurations and surpasses state-of-the-art systems in performance.", "conclusion": "iResolveX effectively balances precision and recall in indirect call resolution by combining conservative static analysis with learning-based refinement, outperforming existing approaches and enabling configurable trade-offs in control-flow graph recovery."}}
{"id": "2601.17543", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.17543", "abs": "https://arxiv.org/abs/2601.17543", "authors": ["Yi Lyu", "Luke Dotson", "Nic Draves", "Andy Zhang"], "title": "CTF for education", "comment": null, "summary": "In this paper, we take a close look at how CTF can be used in cybersecurity education. We divide the CTF competitions into four different categories, which are attack-based CTFs, defense-based CTFs, jeopardy CTFs and gamified and wargames CTFs.\n  We start our analysis by summarizing the main characteristics of different CTF types. We then compare them with each other in both learning objectives and other aspects like accessibility. We conclude that combining all four CTF formats can help participants build one's cybersecurity knowledge.\n  By doing that, we hope that our findings will provide some useful insights for future CTF educators.", "AI": {"tldr": "This paper analyzes four CTF formats\u2014attack-based, defense-based, jeopardy, and gamified/wargames\u2014in cybersecurity education, comparing their features and effectiveness, and concludes that integrating all types enhances learning outcomes.", "motivation": "To examine how Capture The Flag (CTF) competitions can be effectively utilized in cybersecurity education by understanding the strengths and differences of various CTF formats.", "method": "The paper categorizes CTF competitions into four types and analyzes their characteristics, comparing them in terms of learning objectives and accessibility.", "result": "Each CTF type supports different learning outcomes; a combined approach leverages their complementary strengths for more comprehensive cybersecurity training.", "conclusion": "Combining all four CTF formats\u2014attack-based, defense-based, jeopardy, and gamified/wargames\u2014can effectively enhance participants' cybersecurity knowledge."}}
{"id": "2601.17903", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17903", "abs": "https://arxiv.org/abs/2601.17903", "authors": ["Tolgahan Bardakci", "Andreas Faes", "Mutlu Beyazit", "Serge Demeyr"], "title": "Prompt-Based REST API Test Amplification in Industry: An Experience Report", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to support software testing tasks, yet there is little evidence of their effectiveness for REST API testing in industrial settings. To address this gap, we replicate our earlier work on LLM-based REST API test amplification within an industrial context at one of the largest logistics companies in Belgium. We apply LLM-based test amplification to six representative endpoints of a production microservice embedded in a large-scale, security-sensitive system, where there is in-depth complexity in authentication, stateful behavior, and organizational constraints. Our experience shows that LLM-based test amplification remains practically useful in industry by increasing coverage and revealing various observations and anomalies.", "AI": {"tldr": "This study replicates LLM-based REST API test amplification in an industrial setting at a major Belgian logistics company, demonstrating its practical effectiveness in improving test coverage and detecting anomalies in a complex, security-sensitive microservice.", "motivation": "To evaluate the effectiveness of LLMs for REST API testing in real-world industrial contexts, where factors like authentication complexity, stateful behavior, and organizational constraints may impact performance.", "method": "Replication of prior work on LLM-based REST API test amplification within an industrial environment, applied to six representative endpoints of a production microservice at a major logistics company.", "result": "Application of LLM-based test amplification increased test coverage and identified various observations and anomalies in a production microservice within a large-scale system.", "conclusion": "LLM-based test amplification is practically useful in industrial settings, particularly for complex, security-sensitive microservices, as it improves test coverage and uncovers anomalies."}}
{"id": "2601.17548", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17548", "abs": "https://arxiv.org/abs/2601.17548", "authors": ["Narek Maloyan", "Dmitry Namiot"], "title": "Prompt Injection Attacks on Agentic Coding Assistants: A Systematic Analysis of Vulnerabilities in Skills, Tools, and Protocol Ecosystems", "comment": null, "summary": "The proliferation of agentic AI coding assistants, including Claude Code, GitHub Copilot, Cursor, and emerging skill-based architectures, has fundamentally transformed software development workflows. These systems leverage Large Language Models (LLMs) integrated with external tools, file systems, and shell access through protocols like the Model Context Protocol (MCP). However, this expanded capability surface introduces critical security vulnerabilities. In this \\textbf{Systematization of Knowledge (SoK)} paper, we present a comprehensive analysis of prompt injection attacks targeting agentic coding assistants. We propose a novel three-dimensional taxonomy categorizing attacks across \\textit{delivery vectors}, \\textit{attack modalities}, and \\textit{propagation behaviors}. Our meta-analysis synthesizes findings from 78 recent studies (2021--2026), consolidating evidence that attack success rates against state-of-the-art defenses exceed 85\\% when adaptive attack strategies are employed. We systematically catalog 42 distinct attack techniques spanning input manipulation, tool poisoning, protocol exploitation, multimodal injection, and cross-origin context poisoning. Through critical analysis of 18 defense mechanisms reported in prior work, we identify that most achieve less than 50\\% mitigation against sophisticated adaptive attacks. We contribute: (1) a unified taxonomy bridging disparate attack classifications, (2) the first systematic analysis of skill-based architecture vulnerabilities with concrete exploit chains, and (3) a defense-in-depth framework grounded in the limitations we identify. Our findings indicate that the security community must treat prompt injection as a first-class vulnerability class requiring architectural-level mitigations rather than ad-hoc filtering approaches.", "AI": {"tldr": "This SoK paper analyzes prompt injection attacks on agentic AI coding assistants, proposing a three-dimensional taxonomy and revealing that adaptive attacks succeed over 85% of the time against current defenses. It catalogs 42 attack types and 18 defenses, concluding that architectural-level, defense-in-depth solutions are urgently needed.", "motivation": "The increasing integration of LLMs with external tools and systems in agentic coding assistants expands their attack surface, making prompt injection a severe and evolving threat that lacks a unified classification and effective defenses.", "method": "A Systematization of Knowledge (SoK) approach is employed, including a meta-analysis of 78 studies (2021\u001e2026) and a novel three-dimensional taxonomy based on delivery vectors, attack modalities, and propagation behaviors; 42 attack techniques and 18 defense mechanisms are systematically cataloged and evaluated.", "result": "Attack success rates exceed 85% against state-of-the-art defenses when adaptive strategies are used; most existing defenses mitigate less than 50% of sophisticated attacks; 42 distinct attack techniques are identified across input manipulation, tool poisoning, protocol exploitation, multimodal injection, and cross-origin context poisoning; exploit chains in skill-based architectures are exposed.", "conclusion": "Prompt injection attacks represent a critical and first-class vulnerability class in agentic AI coding assistants, necessitating architectural-level defenses rather than ad-hoc filtering; existing defenses are largely insufficient, especially against adaptive attacks, and a defense-in-depth strategy grounded in systematic understanding is required."}}
{"id": "2601.18044", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18044", "abs": "https://arxiv.org/abs/2601.18044", "authors": ["Melika Sepidband", "Hamed Taherkhani", "Hung Viet Pham", "Hadi Hemmati"], "title": "RGFL: Reasoning Guided Fault Localization for Automated Program Repair Using Large Language Models", "comment": "23 pages, 5 figures", "summary": "Fault Localization (FL) is a critical step in Automated Program Repair (APR), and its importance has increased with the rise of Large Language Model (LLM)-based repair agents. In realistic project-level repair scenarios, software repositories often span millions of tokens, far exceeding current LLM context limits. Consequently, models must first identify a small, relevant subset of code, making accurate FL essential for effective repair. We present a novel project-level FL approach that improves both file- and element-level localization. Our method introduces a hierarchical reasoning module that (i) generates structured, bug-specific explanations for candidate files and elements, and (ii) leverages these explanations in a two-stage ranking scheme combining LLM-based and embedding-based signals. We further propose a counterfactual upper-bound analysis to quantify the contribution of each localization stage to repair success. We evaluate our approach on Python and Java projects from SWE-bench Verified, Lite, and Java. Compared to state-of-the-art baselines, including Agentless and OpenHands, our method consistently improves localization accuracy. On SWE-bench Verified, file-level Hit@1 improves from 71.4% to 85%, and MRR from 81.8% to 88.8%. At the element level, Exact Match under top-3 files increases from 36% to 69%. Integrating our localization into Agentless yields a 12.8% end-to-end repair success improvement.", "AI": {"tldr": "This paper presents a hierarchical reasoning-based fault localization method for LLM-driven program repair, improving file- and element-level accuracy on large codebases and boosting end-to-end repair success by 12.8% when integrated into Agentless.", "motivation": "Accurate fault localization is crucial for LLM-based program repair agents due to limited context windows, especially in large-scale real-world repositories where identifying the correct code subset is a prerequisite for successful repair.", "method": "A novel hierarchical reasoning module is introduced that generates bug-specific explanations for candidate files and code elements, followed by a two-stage ranking process combining LLM-based and embedding-based signals; a counterfactual upper-bound analysis is used to assess the impact of each stage.", "result": "On SWE-bench benchmarks, the method achieves 85% file-level Hit@1 (vs 71.4% baseline) and 88.8% MRR (vs 81.8%), with element-level Exact Match under top-3 files increasing from 36% to 69%; integration into Agentless results in a 12.8% improvement in end-to-end repair success.", "conclusion": "The proposed hierarchical reasoning approach significantly improves fault localization accuracy at both file and element levels in project-scale settings, leading to substantial gains in end-to-end repair performance when integrated into existing LLM-based repair agents."}}
{"id": "2601.17549", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17549", "abs": "https://arxiv.org/abs/2601.17549", "authors": ["Narek Maloyan", "Dmitry Namiot"], "title": "Breaking the Protocol: Security Analysis of the Model Context Protocol Specification and Prompt Injection Vulnerabilities in Tool-Integrated LLM Agents", "comment": null, "summary": "The Model Context Protocol (MCP) has emerged as a de facto standard for integrating Large Language Models with external tools, yet no formal security analysis of the protocol specification exists. We present the first rigorous security analysis of MCP's architectural design, identifying three fundamental protocol-level vulnerabilities: (1) absence of capability attestation allowing servers to claim arbitrary permissions, (2) bidirectional sampling without origin authentication enabling server-side prompt injection, and (3) implicit trust propagation in multi-server configurations. We implement \\textsc{MCPBench}, a novel framework bridging existing agent security benchmarks to MCP-compliant infrastructure, enabling direct measurement of protocol-specific attack surfaces. Through controlled experiments on 847 attack scenarios across five MCP server implementations, we demonstrate that MCP's architectural choices amplify attack success rates by 23--41\\% compared to equivalent non-MCP integrations. We propose \\textsc{MCPSec}, a backward-compatible protocol extension adding capability attestation and message authentication, reducing attack success rates from 52.8\\% to 12.4\\% with median latency overhead of 8.3ms per message. Our findings establish that MCP's security weaknesses are architectural rather than implementation-specific, requiring protocol-level remediation.", "AI": {"tldr": "This paper presents the first security analysis of the Model Context Protocol (MCP), uncovering critical architectural vulnerabilities that enable privilege escalation and injection attacks, and introduces MCPSec, a secure, efficient, and backward-compatible fix.", "motivation": "Despite MCP becoming a standard for integrating LLMs with tools, it lacks formal security analysis; this work addresses the critical need to understand and mitigate inherent security risks in its design.", "method": "The authors conducted a formal security analysis of MCP's architecture, identified three core vulnerabilities, developed MCPBench to evaluate attack surfaces across MCP implementations, and proposed MCPSec, a backward-compatible secure extension with capability attestation and message authentication.", "result": "Experiments across 847 attack scenarios on five MCP servers showed that MCP increases attack success rates by 23\u201341% compared to non-MCP systems; MCPSec reduced attack success from 52.8% to 12.4% with only 8.3ms median latency overhead per message.", "conclusion": "The security weaknesses in MCP are architectural in nature, not merely implementation-specific, and require protocol-level fixes; the proposed MCPSec extension effectively mitigates these vulnerabilities with minimal performance overhead."}}
{"id": "2601.18241", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18241", "abs": "https://arxiv.org/abs/2601.18241", "authors": ["Elena Bruches", "Vadim Alperovich", "Dari Baturova", "Roman Derunets", "Daniil Grebenkin", "Georgy Mkrtchyan", "Oleg Sedukhin", "Mikhail Klementev", "Ivan Bondarenko", "Nikolay Bushkov", "Stanislav Moiseev"], "title": "TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance", "comment": "Accepted for publication at the 9th Workshop on Validation, Analysis and Evolution of Software Tests (VST 2026), co-located with the the 33rd IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER 2026)", "summary": "While Large Language Models (LLMs) have shown promise in software engineering, their application to unit testing remains largely confined to isolated test generation or oracle prediction, neglecting the broader challenge of test suite maintenance. We introduce TAM-Eval (Test Automated Maintenance Evaluation), a framework and benchmark designed to evaluate model performance across three core test maintenance scenarios: creation, repair, and updating of test suites. Unlike prior work limited to function-level tasks, TAM-Eval operates at the test file level, while maintaining access to full repository context during isolated evaluation, better reflecting real-world maintenance workflows. Our benchmark comprises 1,539 automatically extracted and validated scenarios from Python, Java, and Go projects. TAM-Eval supports system-agnostic evaluation of both raw LLMs and agentic workflows, using a reference-free protocol based on test suite pass rate, code coverage, and mutation testing. Empirical results indicate that state-of-the-art LLMs have limited capabilities in realistic test maintenance processes and yield only marginal improvements in test effectiveness. We release TAM-Eval as an open-source framework to support future research in automated software testing. Our data and code are publicly available at https://github.com/trndcenter/TAM-Eval.", "code_url": "https://github.com/trndcenter/TAM-Eval", "code_stars": 0, "code_last_update": "2026-01-23", "AI": {"tldr": "TAM-Eval is a new benchmark for evaluating LLMs in real-world test suite maintenance (creation, repair, update) at the file level with full context; it reveals poor performance of current LLMs and provides an open framework for future research.", "motivation": "Existing LLM applications in software engineering focus on narrow tasks like test generation or oracle prediction, failing to address the comprehensive, real-world challenge of test suite maintenance.", "method": "The authors propose TAM-Eval, a benchmark operating at the test file level with full repository context, evaluating LLMs across test creation, repair, and update scenarios using pass rate, code coverage, and mutation testing in a reference-free setup.", "result": "TAM-Eval evaluates 1,539 test maintenance scenarios across Python, Java, and Go, revealing that current LLMs achieve only marginal improvements in test effectiveness and struggle with realistic maintenance workflows.", "conclusion": "State-of-the-art LLMs show limited effectiveness in realistic test suite maintenance tasks, highlighting the need for improved models and methodologies in automated software testing."}}
{"id": "2601.17561", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17561", "abs": "https://arxiv.org/abs/2601.17561", "authors": ["Jincheol Ha", "Guillaume Hanrot", "Taeyeong Noh", "Jung Hee Cheon", "Jung Woo Kim", "Damien Stehl\u00e9"], "title": "Private Iris Recognition with High-Performance FHE", "comment": null, "summary": "Among biometric verification systems, irises stand out because they offer high accuracy even in large-scale databases. For example, the World ID project aims to provide authentication to all humans via iris recognition, with millions already registered. Storing such biometric data raises privacy concerns, which can be addressed using privacy-enhancing techniques.\n  Bloemen et al. describe a solution based on 2-out-of-3 Secret-Sharing Multiparty Computation (SS-MPC), for the World ID setup. In terms of security, unless an adversary corrupts 2~servers, the iris codes remain confidential and nothing leaks beyond the result of the computation. Their solution is able to match~$32$ users against a database of~$2^{22}$ iris codes in~$\\approx 2$s , using~24 H100 GPUs, more than 40~communication rounds and $81$GB/party of data transferred (the timing assumes a network speed above~3Tb/s).\n  In the present work, we explore the use of Threshold Fully Homomorphic Encryption (ThFHE) for the same task. The ThFHE solution brings a number of security advantages: no trusted setup, the encrypted database and queries can be public, the secret can be distributed among many parties, and active security can be added without significant performance degradation.\n  Our proof-of-concept implementation of the computation phase handles $32$~eyes against a database of $7\\cdot 2^{14}$ iris codes in~$\\approx 1.8$s ($\\approx 0.33s$ for 4 eyes against the same database), using 8 RTX-5090 GPUs. To this, one should add~2 to 3 rounds of communication (depending on deployment choice). We perform the matching using the CKKS (Th)FHE scheme. Our main technical ingredients are the use of recent progress on FHE-based linear algebra boosted using int8 GPU operations, and the introduction of a technique reducing the number of ciphertexts to be processed as early as possible.", "AI": {"tldr": "This paper presents a Threshold FHE-based private iris recognition system that outperforms prior MPC methods in security and communication efficiency, achieving sub-2-second matching on large databases using GPU-optimized CKKS homomorphic encryption.", "motivation": "Privacy concerns in large-scale biometric systems like World ID motivate the need for stronger privacy-preserving authentication. Existing MPC solutions require trusted setups and high communication overhead. The work aims to improve security and efficiency using ThFHE.", "method": "The authors propose a ThFHE-based solution for private iris code matching, leveraging the CKKS FHE scheme, FHE-based linear algebra optimizations, and GPU acceleration (using RTX-5090) with int8 operations. They also introduce a technique to minimize the number of ciphertexts early in the computation.", "result": "The proof-of-concept implementation processes 32 iris queries against a database of 7\u00d72\u00b9\u2074 iris codes in approximately 1.8 seconds (0.33s for 4 queries), using 8 RTX-5090 GPUs and only 2\u20133 communication rounds. The approach supports public ciphertexts, no trusted setup, and active security with minimal overhead.", "conclusion": "The use of Threshold Fully Homomorphic Encryption (ThFHE) for large-scale iris recognition offers notable security and efficiency advantages over existing SS-MPC approaches, including stronger security guarantees, reduced communication rounds, and competitive performance with potential for scalability."}}
{"id": "2601.18341", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18341", "abs": "https://arxiv.org/abs/2601.18341", "authors": ["Romain Robbes", "Th\u00e9o Matricon", "Thomas Degueule", "Andre Hora", "Stefano Zacchiroli"], "title": "Agentic Much? Adoption of Coding Agents on GitHub", "comment": null, "summary": "In the first half of 2025, coding agents have emerged as a category of development tools that have very quickly transitioned to the practice. Unlike ''traditional'' code completion LLMs such as Copilot, agents like Cursor, Claude Code, or Codex operate with high degrees of autonomy, up to generating complete pull requests starting from a developer-provided task description. This new mode of operation is poised to change the landscape in an even larger way than code completion LLMs did, making the need to study their impact critical. Also, unlike traditional LLMs, coding agents tend to leave more explicit traces in software engineering artifacts, such as co-authoring commits or pull requests. We leverage these traces to present the first large-scale study (129,134 projects) of the adoption of coding agents on GitHub, finding an estimated adoption rate of 15.85%--22.60%, which is very high for a technology only a few months old--and increasing. We carry out an in-depth study of the adopters we identified, finding that adoption is broad: it spans the entire spectrum of project maturity; it includes established organizations; and it concerns diverse programming languages or project topics. At the commit level, we find that commits assisted by coding agents are larger than commits only authored by human developers, and have a large proportion of features and bug fixes. These findings highlight the need for further investigation into the practical use of coding agents.", "AI": {"tldr": "This paper presents the first large-scale study on the adoption of coding agents like Cursor and Claude Code on GitHub, analyzing over 129k projects. It finds rapid and broad adoption, with 15.85%\u201322.60% of projects using these tools, leaving traceable contributions in commits and pull requests. Agent-assisted commits are larger and more often involve features and bug fixes, indicating significant integration into development workflows.", "motivation": "The emergence of autonomous coding agents represents a transformative shift in software development tools, surpassing traditional code completion systems in capability and impact. Understanding their real-world adoption and usage patterns is critical due to their potential to reshape development workflows.", "method": "The authors conducted a large-scale empirical study of 129,134 GitHub projects, analyzing explicit traces left by coding agents (e.g., co-authored commits and pull requests) to identify adoption patterns and characterize usage at both project and commit levels.", "result": "The study estimates that 15.85% to 22.60% of GitHub projects adopted coding agents within months of their release, with increasing trend. Adoption spans projects of all maturity levels, organizations, languages, and domains. Agent-assisted commits are larger in size and more frequently involve feature additions and bug fixes compared to human-only commits.", "conclusion": "The study concludes that coding agents have achieved significant and rapidly growing adoption on GitHub, with broad applicability across project types and organizational contexts, necessitating further research into their practical implications and long-term impact on software development."}}
{"id": "2601.17620", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17620", "abs": "https://arxiv.org/abs/2601.17620", "authors": ["Eliron Rahimi", "Margarita Osadchy", "Orr Dunkelman"], "title": "Reconstructing Protected Biometric Templates from Binary Authentication Results", "comment": "Accepted at the International Joint Conference on Biometrics (IJCB) 2025", "summary": "Biometric data is considered to be very private and highly sensitive. As such, many methods for biometric template protection were considered over the years -- from biohashing and specially crafted feature extraction procedures, to the use of cryptographic solutions such as Fuzzy Commitments or the use of Fully Homomorphic Encryption (FHE).\n  A key question that arises is how much protection these solutions can offer when the adversary can inject samples, and observe the outputs of the system. While for systems that return the similarity score, one can use attacks such as hill-climbing, for systems where the adversary can only learn whether the authentication attempt was successful, this question remained open.\n  In this paper, we show that it is indeed possible to reconstruct the biometric template by just observing the success/failure of the authentication attempt (given the ability to inject a sufficient amount of templates). Our attack achieves negligible template reconstruction loss and enables full recovery of facial images through a generative inversion method, forming a pipeline from binary scores to high-resolution facial images that successfully pass the system more than 98\\% of the time. Our results, of course, are applicable for any protection mechanism that maintains the accuracy of the recognition.", "AI": {"tldr": "This paper demonstrates a novel attack that reconstructs biometric templates and high-resolution facial images using only binary authentication outcomes (success/failure), showing that even advanced protection methods are vulnerable if they preserve recognition accuracy.", "motivation": "The motivation stems from the open question of whether biometric systems revealing only binary authentication results are secure against template reconstruction. Despite the use of advanced protection methods like FHE or fuzzy commitments, the potential vulnerability to black-box attacks based on binary feedback remained unexplored.", "method": "The authors develop a generative inversion-based attack that leverages repeated authentication attempts with injected biometric samples. By observing only binary outcomes (success/failure), they formulate an optimization process that reconstructs the original biometric template with negligible loss, followed by a generative model to produce high-resolution facial images.", "result": "The attack achieves negligible template reconstruction error and enables full visual reconstruction of facial images via generative inversion. The reconstructed images succeed in authentication over 98% of the time, demonstrating high efficacy across various template protection schemes that maintain recognition accuracy.", "conclusion": "The study concludes that even biometric protection mechanisms returning only binary success/failure outputs are vulnerable to template reconstruction when an adversary can inject enough samples. The proposed attack enables near-complete recovery of facial templates and high-resolution images that bypass the system with over 98% success, undermining the security of any accuracy-preserving biometric protection scheme."}}
{"id": "2601.18344", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18344", "abs": "https://arxiv.org/abs/2601.18344", "authors": ["Alexandros Tsakpinis", "Efe Berk Erg\u00fclec", "Emil Schwenger", "Alexander Pretschner"], "title": "Forecasting the Maintained Score from the OpenSSF Scorecard for GitHub Repositories linked to PyPI libraries", "comment": "11 pages, 9 figures, 2 tables", "summary": "The OpenSSF Scorecard is widely used to assess the security posture of open-source software repositories, with the Maintained metric indicating recent development activity and helping identify potentially abandoned dependencies. However, this metric is inherently retrospective, reflecting only the past 90 days of activity and providing no insight into future maintenance, which limits its usefulness for proactive risk assessment. In this paper, we study to what extent future maintenance activity, as captured by the OpenSSF Maintained score, can be forecasted. We analyze 3,220 GitHub repositories associated with the top 1% most central PyPI libraries by PageRank and reconstruct historical Maintained scores over a three-year period. We formulate the task as multivariate time series forecasting and consider four target representations: raw scores, bucketed maintenance levels, numerical trend slopes, and categorical trend types. We compare a statistical model (VARMA), a machine learning model (Random Forest), and a deep learning model (LSTM) across training windows of 3-12 months and forecasting horizons of 1-6 months. Our results show that future maintenance activity can be predicted with meaningful accuracy, particularly for aggregated representations such as bucketed scores and trend types, achieving accuracies above 0.95 and 0.80, respectively. Simpler statistical and machine learning models perform on par with deep learning approaches, indicating that complex architectures are not required. These findings suggest that predictive modeling can effectively complement existing Scorecard metrics, enabling more proactive assessment of open-source maintenance risks.", "AI": {"tldr": "This paper shows that future open-source project maintenance, as measured by the OpenSSF Scorecard's Maintained metric, can be accurately forecasted using time series models, enabling proactive risk detection; simple models perform as well as deep learning.", "motivation": "The OpenSSF Scorecard's Maintained metric is retrospective, limited to the past 90 days, and provides no insight into future maintenance; this limits its utility for proactive risk assessment of open-source dependencies.", "method": "Formulated as multivariate time series forecasting; evaluated VARMA (statistical), Random Forest (machine learning), and LSTM (deep learning) models on historical Maintained scores over 3,220 GitHub repositories; tested across multiple target representations and time horizons.", "result": "Future maintenance activity can be predicted with high accuracy, especially for aggregated representations: >0.95 accuracy for bucketed maintenance levels and >0.80 for categorical trend types; simpler models (VARMA, Random Forest) perform as well as LSTM.", "conclusion": "Predictive modeling can effectively complement the OpenSSF Scorecard's Maintained metric, enabling proactive assessment of open-source maintenance risks by forecasting future activity using simple, interpretable models."}}
{"id": "2601.17638", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17638", "abs": "https://arxiv.org/abs/2601.17638", "authors": ["Nitin Choudhury", "Bikrant Bikram Pratap Maurya", "Orchid Chetia Phukan", "Arun Balaji Buduru"], "title": "FOCA: Multimodal Malware Classification via Hyperbolic Cross-Attention", "comment": "Accepted to the International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026", "summary": "In this work, we introduce FOCA, a novel multimodal framework for malware classification that jointly leverages audio and visual modalities. Unlike conventional Euclidean-based fusion methods, FOCA is the first to exploit the intrinsic hierarchical relationships between audio and visual representations within hyperbolic space. To achieve this, raw binaries are transformed into both audio and visual representations, which are then processed through three key components: (i) a hyperbolic projection module that maps Euclidean embeddings into the Poincare ball, (ii) a hyperbolic cross-attention mechanism that aligns multimodal dependencies under curvature-aware constraints, and (iii) a Mobius addition-based fusion layer. Comprehensive experiments on two benchmark datasets-Mal-Net and CICMalDroid2020- show that FOCA consistently outperforms unimodal models, surpasses most Euclidean multimodal baselines, and achieves state-of-the-art performance over existing works.", "AI": {"tldr": "FOCA is a novel hyperbolic multimodal framework for malware classification that fuses audio and visual representations in hyperbolic space using a Poincar\u0003e ball projection, hyperbolic cross-attention, and M\u0004bius fusion, achieving superior performance on benchmark datasets.", "motivation": "Conventional multimodal fusion methods for malware classification rely on Euclidean space, which may fail to capture the intrinsic hierarchical structure of multimodal representations; FOCA addresses this limitation by exploiting hyperbolic geometry, better suited for modeling complex hierarchical relationships in malware data.", "method": "FOCA employs a hyperbolic multimodal framework that includes: (i) transformation of raw binaries into audio and visual representations, (ii) a hyperbolic projection module mapping embeddings into the Poincar\u0003e ball, (iii) a curvature-aware hyperbolic cross-attention mechanism for aligning multimodal features, and (iv) a M\u0004bius addition-based fusion layer for combining representations in hyperbolic space.", "result": "FOCA achieves state-of-the-art performance on Mal-Net and CICMalDroid2020 datasets, consistently outperforming unimodal models and most Euclidean multimodal baselines in malware classification accuracy.", "conclusion": "FOCA establishes a new state-of-the-art in multimodal malware classification by effectively leveraging hyperbolic geometry to model hierarchical relationships between audio and visual representations, demonstrating superior performance over Euclidean-based methods and unimodal approaches."}}
{"id": "2601.18345", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18345", "abs": "https://arxiv.org/abs/2601.18345", "authors": ["Romain Robes Th\u00e9o Matricon", "Thomas Degueule", "Andre Hora", "Stefano Zacchiroli"], "title": "Promises, Perils, and (Timely) Heuristics for Mining Coding Agent Activity", "comment": "Preprint. Accepted for publication at MSR 2026", "summary": "In 2025, coding agents have seen a very rapid adoption. Coding agents leverage Large Language Models (LLMs) in ways that are markedly different from LLM-based code completion, making their study critical. Moreover, unlike LLM-based completion, coding agents leave visible traces in software repositories, enabling the use of MSR techniques to study their impact on SE practices. This paper documents the promises, perils, and heuristics that we have gathered from studying coding agent activity on GitHub.", "AI": {"tldr": "This paper analyzes coding agents on GitHub using MSR, revealing their growing impact on software engineering through observable behaviors, and presents key findings on their promises, risks, and usage heuristics.", "motivation": "The rapid adoption of coding agents in 2025 and their distinct operation from traditional LLM-based code completion necessitate a focused study on their real-world impact.", "method": "The authors used Mining Software Repositories (MSR) techniques to analyze visible traces of coding agents on GitHub.", "result": "The study identifies key promises, perils, and practical heuristics derived from observing coding agent usage in real software repositories.", "conclusion": "The paper concludes that studying coding agent activity on GitHub provides valuable insights into their impact on software engineering practices, highlighting both benefits and risks."}}
{"id": "2601.17644", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17644", "abs": "https://arxiv.org/abs/2601.17644", "authors": ["Ali Al-Lawati", "Suhang Wang"], "title": "A Systemic Evaluation of Multimodal RAG Privacy", "comment": null, "summary": "The growing adoption of multimodal Retrieval-Augmented Generation (mRAG) pipelines for vision-centric tasks (e.g. visual QA) introduces important privacy challenges. In particular, while mRAG provides a practical capability to connect private datasets to improve model performance, it risks the leakage of private information from these datasets during inference. In this paper, we perform an empirical study to analyze the privacy risks inherent in the mRAG pipeline observed through standard model prompting. Specifically, we implement a case study that attempts to infer the inclusion of a visual asset, e.g. image, in the mRAG, and if present leak the metadata, e.g. caption, related to it. Our findings highlight the need for privacy-preserving mechanisms and motivate future research on mRAG privacy.", "AI": {"tldr": "This paper empirically evaluates privacy risks in multimodal RAG (mRAG) systems, showing that private visual data and their metadata can be leaked through standard prompting, calling for enhanced privacy protections.", "motivation": "The increasing use of mRAG systems in vision-centric applications raises privacy concerns, especially regarding the potential exposure of private data and metadata during inference, motivating an investigation into these vulnerabilities.", "method": "The authors conduct an empirical case study using standard model prompting to test whether the presence of a visual asset (e.g., an image) in an mRAG system can be inferred and whether associated metadata (e.g., captions) can be leaked.", "result": "The study demonstrates that it is feasible to infer the inclusion of specific visual assets in an mRAG system and to extract associated metadata, highlighting concrete privacy risks.", "conclusion": "The study concludes that mRAG pipelines pose significant privacy risks, particularly through metadata leakage during inference, and underscores the necessity for privacy-preserving solutions."}}
{"id": "2601.18418", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18418", "abs": "https://arxiv.org/abs/2601.18418", "authors": ["Ji Zeng", "Dayuan Fu", "Tiantian Mi", "Yumin Zhuang", "Yaxing Huang", "Xuefeng Li", "Lyumanshan Ye", "Muhang Xie", "Qishuo Hua", "Zhen Huang", "Mohan Jiang", "Hanning Wang", "Jifan Lin", "Yang Xiao", "Jie Sun", "Yunze Wu", "Pengfei Liu"], "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering", "comment": null, "summary": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...", "AI": {"tldr": "This paper presents a scalable agentic mid-training method for code LLMs using agent-native data\u2014combining contextually and environmentally native trajectories\u2014to bridge the gap between static training and dynamic development, achieving strong results on SWE-Bench Verified with efficient token usage.", "motivation": "Agentic mid-training is underexplored despite its scalability over reinforcement learning; existing methods suffer from distribution mismatch between static training data and dynamic real-world development environments.", "method": "Introduces agentic mid-training using agent-native data composed of contextually-native trajectories (preserving full information flow) and environmentally-native trajectories (from real tool and test executions); evaluates on SWE-Bench Verified with 32B and 72B models.", "result": "Outperforms Kimi-Dev in two post-training settings with less than half the mid-training tokens (73.1B); achieves 56.1% and 58.5% resolution rates on SWE-Bench Verified for 32B and 72B models, respectively.", "conclusion": "The proposed agentic mid-training approach with agent-native data effectively enhances LLMs' software engineering capabilities, achieving state-of-the-art performance on SWE-Bench Verified with significantly fewer training tokens compared to prior methods."}}
{"id": "2601.17661", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17661", "abs": "https://arxiv.org/abs/2601.17661", "authors": ["Ahmed Oun", "Rishabh Das", "Clay Hess", "Aakriti Barat", "Savas Kaya"], "title": "A PUF-Based Security Framework for Fault and Intrusion Detection", "comment": null, "summary": "Industrial Control Systems (ICS) rely on sensor feedback to keep safety-critical processes within operational limits. This research presents a hardware-root-of-trust that embeds a Physically Unclonable Function (PUF) at the measurement layer to authenticate sensor readings. The architecture combines voltage fingerprinting with a temporal authentication that integrates with standard industrial control system architecture. The research prototypes the PUF integration on a hardware-in-the-loop (HIL) water tank testbed using a Simulink-based PUF emulator. The system maintains 99.97% accuracy over a 5.18-hour period of normal operation and flags all injected anomalies, including spike faults, hard-over faults, and hardware trojan scenarios that push the system over to an unsafe operational state. The proposed architecture provides a process-aware, vendor-agnostic approach that can integrate with legacy plants to detect sensor signal degradation or sophisticated supply chain attacks.", "AI": {"tldr": "This paper presents a PUF-based hardware-root-of-trust embedded at the sensor level in ICS, combining voltage fingerprinting and temporal authentication to detect sensor anomalies and attacks with high accuracy, validated on a HIL water tank testbed.", "motivation": "To secure safety-critical industrial control systems from sensor spoofing and supply chain attacks by ensuring the authenticity of sensor data through hardware-rooted trust.", "method": "The research integrates a Physically Unclonable Function (PUF) at the measurement layer using voltage fingerprinting and temporal authentication, implemented on a hardware-in-the-loop (HIL) water tank testbed with a Simulink-based PUF emulator.", "result": "The system achieves 99.97% accuracy over 5.18 hours and successfully detects all injected anomalies, including spike faults, hard-over faults, and hardware trojan attacks.", "conclusion": "The proposed PUF-based hardware-root-of-trust offers a robust, process-aware, and vendor-agnostic solution for authenticating sensor data in ICS, capable of detecting both physical faults and supply chain attacks."}}
{"id": "2601.18477", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18477", "abs": "https://arxiv.org/abs/2601.18477", "authors": ["Giuseppe Destefanis", "Leila Yousefi", "Martin Shepperd", "Allan Tucker", "Stephen Swift", "Steve Counsell", "Mahir Arzoky"], "title": "An Audit of Machine Learning Experiments on Software Defect Prediction", "comment": null, "summary": "Background: Machine learning algorithms are widely used to predict defect prone software components. In this literature, computational experiments are the main means of evaluation, and the credibility of results depends on experimental design and reporting. Objective: This paper audits recent software defect prediction (SDP) studies by assessing their experimental design, analysis, and reporting practices against accepted norms from statistics, machine learning, and empirical software engineering. The aim is to characterise current practice and assess the reproducibility of published results. Method: We audited SDP studies indexed in SCOPUS between 2019 and 2023, focusing on design and analysis choices such as outcome measures, out of sample validation strategies, and the use of statistical inference. Nine study issues were evaluated. Reproducibility was assessed using the instrument proposed by Gonz\u00e1lez Barahona and Robles. Results: The search identified approximately 1,585 SDP experiments published during the period. From these, we randomly sampled 101 papers, including 61 journal and 40 conference publications, with almost 50 percent behind paywalls. We observed substantial variation in research practice. The number of datasets ranged from 1 to 365, learners or learner variants from 1 to 34, and performance measures from 1 to 9. About 45 percent of studies applied formal statistical inference. Across the sample, we identified 427 issues, with a median of four per paper, and only one paper without issues. Reproducibility ranged from near complete to severely limited. We also identified two cases of tortured phrases and possible paper mill activity. Conclusions: Experimental design and reporting practices vary widely, and almost half of the studies provide insufficient detail to support reproduction. The audit indicates substantial scope for improvement.", "AI": {"tldr": "An audit of 101 software defect prediction studies from 2019 to 2023 reveals widespread variability in experimental design and poor reporting practices, with 45% using statistical inference and most containing methodological flaws, severely limiting reproducibility.", "motivation": "To evaluate the credibility and reproducibility of results in software defect prediction research by examining the quality of experimental design, statistical analysis, and reporting practices in recent publications.", "method": "A systematic audit of 101 software defect prediction (SDP) papers published between 2019 and 2023 in SCOPUS was conducted. Design, analysis, and reporting practices were evaluated against established norms in statistics, machine learning, and empirical software engineering, using nine predefined study quality indicators. Reproducibility was assessed using the Gonz\u00e1lez Barahona and Robles instrument.", "result": "Out of 101 audited papers (61 journal, 40 conference), nearly 50% applied formal statistical inference. A total of 427 methodological or reporting issues were identified, with a median of four per paper and only one paper free of issues. Reproducibility varied widely, with many studies lacking essential details. Two instances of potential paper mill activity were detected.", "conclusion": "Experimental design and reporting practices in software defect prediction studies vary significantly, with nearly half of the analyzed papers lacking sufficient detail for reproducibility. The audit reveals substantial room for improvement in methodological rigor and transparency."}}
{"id": "2601.17785", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.17785", "abs": "https://arxiv.org/abs/2601.17785", "authors": ["Tushar Jain"], "title": "Performance Analysis of Quantum-Secure Digital Signature Algorithms in Blockchain", "comment": null, "summary": "The long-term security of public blockchains strictly depends on the hardness assumptions of the underlying digital signature schemes. In the current scenario, most deployed cryptocurrencies and blockchain platforms rely on elliptic-curve cryptography, which is vulnerable to quantum attacks due to Shor's algorithm. Therefore, it is important to understand how post-quantum (PQ) digital signatures behave when integrated into real blockchain systems. This report presents a blockchain prototype that supports multiple quantum-secure signature algorithms, focusing on CRYSTALS-Dilithium, Falcon and Hawk as lattice-based schemes. This report also describes the design of the prototype and discusses the performance metrics, which include key generation, signing, verification times, key sizes and signature sizes. This report covers the problem, background, and experimental methodology, also providing a detailed comparison of quantum-secure signatures in a blockchain context and extending the analysis to schemes such as HAETAE.", "AI": {"tldr": "This paper presents a blockchain prototype integrating post-quantum digital signatures, focusing on lattice-based schemes CRYSTALS-Dilithium, Falcon, Hawk, and HAETAE, evaluates their performance, and demonstrates their viability for securing future blockchain systems against quantum threats.", "motivation": "The vulnerability of current elliptic-curve-based digital signatures in blockchains to quantum attacks necessitates the exploration and integration of quantum-secure alternatives to ensure long-term cryptographic security.", "method": "Design and implementation of a blockchain prototype supporting multiple post-quantum signature schemes, including CRYSTALS-Dilithium, Falcon, Hawk, and HAETAE, with experimental evaluation of performance metrics such as key generation, signing, verification times, key sizes, and signature sizes.", "result": "Performance evaluation shows trade-offs among lattice-based schemes in terms of speed and size, with CRYSTALS-Dilithium and Falcon offering efficient verification and moderate signature sizes, while Hawk and HAETAE provide alternative design points with varying computational and storage costs.", "conclusion": "The integration of post-quantum signature schemes into blockchain systems is feasible and necessary for long-term security against quantum threats, with lattice-based schemes like CRYSTALS-Dilithium, Falcon, and Hawk showing promising performance and security trade-offs."}}
{"id": "2601.18566", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18566", "abs": "https://arxiv.org/abs/2601.18566", "authors": ["Fabio Massacci", "Winnie Mbaka"], "title": "On the Abolition of the \"ICSE Paper\" and the Adoption of the \"Registered Proposal\" and the \"Results Report\"", "comment": "10 pages, 0 figures, International Conference on Software Engineering", "summary": "To address the 'novelty-vicious cycle' and the 'replicability crisis' of the field (both discussed in the survey) we propose abolishing the \"ICSE paper\" as we know it and replacing it with a two-tier system that also evolves the existing notion of 'Registered Report'. Authors proposing a new idea, experiment, or analysis would submit a \"Registered Proposal\" of their idea and the proposed experimental methodology to undergo peer review. The following year, anyone can submit (shorter) \"Results Reports\" on the realization of the empirical work based on the registered proposals of the previous ICSE (or FSE or ISSTA or ASE etc.). Both works should be first class citizens of the mainstream events. We argue that such a disruptive (heretical?) idea is supported and based on the responses of the community of the Future of Software Engineering pre-survey", "AI": {"tldr": "This paper proposes replacing traditional conference papers with a two-tier publication system\u2014Registered Proposals for methodological review and Results Reports for empirical findings\u2014to improve replicability and reduce bias in software engineering research.", "motivation": "To combat the novelty-vicious cycle, where only novel results are rewarded, and the replicability crisis, where empirical results are difficult or impossible to reproduce, the authors aim to restructure the publication process to incentivize methodological rigor and independent replication.", "method": "The authors propose a reform to the current publication model in top software engineering venues by introducing a two-tier system based on Registered Reports, inspired by community feedback from a pre-survey on the Future of Software Engineering.", "result": "The proposed system enables peer-reviewed methodological planning via 'Registered Proposals' and allows separate submission of empirical results as 'Results Reports', promoting transparency, reducing publication bias, and enabling broader replication across venues.", "conclusion": "The proposed two-tier system of 'Registered Proposals' and 'Results Reports' can alleviate the novelty-vicious cycle and replicability crisis in software engineering research by decoupling idea validation from empirical validation, fostering transparency, and improving scientific rigor."}}
{"id": "2601.18591", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18591", "abs": "https://arxiv.org/abs/2601.18591", "authors": ["Fiorella Zampetti", "Federico Stocchetti", "Federica Razzano", "Damian Andrew Tamburri", "Massimiliano Di Penta"], "title": "How are MLOps Frameworks Used in Open Source Projects? An Empirical Characterization", "comment": null, "summary": "Machine Learning (ML) Operations (MLOps) frameworks have been conceived to support developers and AI engineers in managing the lifecycle of their ML models. While such frameworks provide a wide range of features, developers may leverage only a subset of them, while missing some highly desired features. This paper investigates the practical use and desired feature enhancements of eight popular open-source MLOps frameworks. Specifically, we analyze their usage by dependent projects on GitHub, examining how they invoke the frameworks' APIs and commands. Then, we qualitatively analyze feature requests and enhancements mined from the frameworks' issue trackers, relating these desired improvements to the previously identified usage features. Results indicate that MLOps frameworks are rarely used out-of-the-box and are infrequently integrated into GitHub Workflows, but rather, developers use their APIs to implement custom functionality in their projects. Used features concern core ML phases and whole infrastructure governance, sometimes leveraging multiple frameworks with complementary features. The mapping with feature requests highlights that users mainly ask for enhancements to core features of the frameworks, but also better API exposure and CI/CD integration.", "AI": {"tldr": "This paper studies the real-world usage and desired enhancements of eight open-source MLOps frameworks through GitHub project analysis and issue tracking data, revealing that developers prefer using APIs to build custom solutions rather than adopting frameworks as-is, and are primarily requesting improvements in core features, API access, and CI/CD support.", "motivation": "The motivation is to understand how developers practically use MLOps frameworks and identify gaps between existing features and user needs, in order to guide future development and improvement of MLOps tools.", "method": "The authors analyze the usage of eight popular open-source MLOps frameworks by examining API and command invocations in dependent GitHub projects, combined with a qualitative analysis of feature requests and enhancement proposals extracted from the frameworks' issue trackers, mapping requested features to actual usage patterns.", "result": "Findings show that MLOps frameworks are rarely used out-of-the-box or integrated into GitHub Workflows; instead, developers leverage their APIs to create custom implementations, focusing on core ML lifecycle phases and infrastructure management. Feature requests predominantly call for enhancements in core features, improved API exposure, and better CI/CD integration.", "conclusion": "The study concludes that MLOps frameworks are primarily used through their APIs to build custom solutions rather than being adopted out-of-the-box, with users frequently combining multiple frameworks to meet their needs; furthermore, desired improvements center on enhancing core functionalities, API accessibility, and CI/CD integration."}}
{"id": "2601.17817", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.17817", "abs": "https://arxiv.org/abs/2601.17817", "authors": ["Hongjuan Li", "Hui Kang", "Jiahui Li", "Geng Sun", "Ruichen Zhang", "Jiacheng Wang", "Dusit Niyato", "Wei Ni", "Abbas Jamalipour"], "title": "Multi-Agent Collaborative Intrusion Detection for Low-Altitude Economy IoT: An LLM-Enhanced Agentic AI Framework", "comment": null, "summary": "The rapid expansion of low-altitude economy Internet of Things (LAE-IoT) networks has created unprecedented security challenges due to dynamic three-dimensional mobility patterns, distributed autonomous operations, and severe resource constraints. Traditional intrusion detection systems designed for static ground-based networks prove inadequate for tackling the unique characteristics of aerial IoT environments, including frequent topology changes, real-time detection requirements, and energy limitations. In this article, we analyze the intrusion detection requirements for LAE-IoT networks, complemented by a comprehensive review of evaluation metrics that cover detection effectiveness, response time, and resource consumption. Then, we investigate transformative potential of agentic artificial intelligence (AI) paradigms and introduce a large language model (LLM)-enabled agentic AI framework for enhancing intrusion detection in LAE-IoT networks. This leads to our proposal of a novel multi-agent collaborative intrusion detection framework that leverages specialized LLM-enhanced agents for intelligent data processing and adaptive classification. Through experimental validation, our framework demonstrates superior performance of over 90\\% classification accuracy across multiple benchmark datasets. These results highlight the transformative potential of combining agentic AI principles with LLMs for next-generation LAE-IoT security systems.", "AI": {"tldr": "This paper proposes a large language model-powered multi-agent intrusion detection framework tailored for low-altitude economy IoT networks, addressing mobility, resource constraints, and real-time detection through collaborative AI agents, achieving over 90% accuracy and showcasing the potential of agentic AI in aerial IoT security.", "motivation": "The rapid growth of LAE-IoT networks introduces critical security challenges due to 3D mobility, dynamic topology, and resource limitations, rendering traditional intrusion detection systems ineffective and necessitating innovative, adaptive solutions.", "method": "A novel multi-agent collaborative intrusion detection framework is proposed, leveraging large language model (LLM)-enhanced agents for intelligent data processing and adaptive classification, designed specifically for the unique challenges of low-altitude economy IoT networks.", "result": "The framework achieves over 90% classification accuracy on multiple benchmark datasets, demonstrating superior detection effectiveness, low response time, and efficient resource utilization in dynamic aerial IoT environments.", "conclusion": "The proposed LLM-enabled multi-agent collaborative intrusion detection framework demonstrates significant potential for enhancing security in LAE-IoT networks by effectively addressing dynamic topology, real-time detection, and resource constraints, marking a transformative advancement in aerial IoT security."}}
{"id": "2601.18749", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18749", "abs": "https://arxiv.org/abs/2601.18749", "authors": ["Haruhiko Yoshioka", "Takahiro Monno", "Haruka Tokumasu", "Taiki Wakamatsu", "Yuki Ota", "Nimmi Weeraddana", "Kenichi Matsumoto"], "title": "Let's Make Every Pull Request Meaningful: An Empirical Analysis of Developer and Agentic Pull Requests", "comment": "Accepted for publication in the 23rd International Conference on Mining Software Repositories (MSR '26) : 5 pages, 3 figures, 3 tables", "summary": "The automatic generation of pull requests (PRs) using AI agents has become increasingly common. Although AI-generated PRs are fast and easy to create, their merge rates have been reported to be lower than those created by humans. In this study, we conduct a large-scale empirical analysis of 40,214 PRs collected from the AIDev dataset. We extract 64 features across six families and fit statistical regression models to compare PR merge outcomes for human and agentic PRs, as well as across three AI agents. Our results show that submitter attributes dominate merge outcomes for both groups, while review-related features exhibit contrasting effects between human and agentic PRs. The findings of this study provide insights into improving PR quality through human-AI collaboration.", "AI": {"tldr": "This study analyzes 40,214 pull requests to understand why AI-generated PRs have lower merge rates than human ones. Using 64 features and regression models, it finds that submitter characteristics are key predictors of merge success, while review dynamics affect human and AI PRs differently\u2014insights that can guide better human-AI collaboration in software development.", "motivation": "The motivation stems from the observed lower merge rates of AI-generated PRs compared to human-created ones, despite their efficiency. The study aims to identify key factors affecting PR merge outcomes to improve the effectiveness of AI agents in software development workflows.", "method": "A large-scale empirical analysis was conducted on 40,214 PRs from the AIDev dataset. The authors extracted 64 features across six categories and applied statistical regression models to compare merge outcomes between human and AI-generated PRs, as well as among three different AI agents.", "result": "Submitter attributes were found to be the dominant factor influencing PR merge outcomes for both human and AI-generated PRs. Review-related features showed contrasting effects between human and agentic PRs, and differences were also observed across the three AI agents analyzed.", "conclusion": "The study concludes that while AI-generated pull requests (PRs) are efficient, their lower merge rates compared to human-created PRs are significantly influenced by submitter attributes and differing impacts of review-related features. The results highlight opportunities for enhancing PR quality through better human-AI collaboration."}}
{"id": "2601.17833", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17833", "abs": "https://arxiv.org/abs/2601.17833", "authors": ["Xiaohui Hu", "Wun Yu Chan", "Yuejie Shi", "Qumeng Sun", "Wei-Cheng Wang", "Chiachih Wu", "Haoyu Wang", "Ningyu He"], "title": "An Effective and Cost-Efficient Agentic Framework for Ethereum Smart Contract Auditing", "comment": null, "summary": "Smart contract security is paramount, but identifying intricate business logic vulnerabilities remains a persistent challenge because existing solutions consistently fall short: manual auditing is unscalable, static analysis tools are plagued by false positives, and fuzzers struggle to navigate deep logic states within complex systems. Even emerging AI-based methods suffer from hallucinations, context constraints, and a heavy reliance on expensive, proprietary Large Language Models. In this paper, we introduce Heimdallr, an automated auditing agent designed to overcome these hurdles through four core innovations. By reorganizing code at the function level, Heimdallr minimizes context overhead while preserving essential business logic. It then employs heuristic reasoning to detect complex vulnerabilities and automatically chain functional exploits. Finally, a cascaded verification layer validates these findings to eliminate false positives. Notably, this approach achieves high performance on lightweight, open-source models like GPToss-120B without relying on proprietary systems. Our evaluations demonstrate exceptional performance, as Heimdallr successfully reconstructed 17 out of 20 real-world attacks post June 2025, resulting in total losses of $384M, and uncovered 4 confirmed zero-day vulnerabilities that safeguarded $400M in TVL. Compared to SOTA baselines including both official industrial tools and academic tools, Heimdallr at most reduces analysis time by 97.59% and financial costs by 98.77% while boosting detection precision by over 93.66%. Notably, when applied to auditing contests, Heimdallr can achieve a 92.45% detection rate at a negligible cost of $2.31 per 10K LOC. We provide production-ready auditing services and release valuable benchmarks for future work.", "AI": {"tldr": "Heimdallr is an automated smart contract auditing agent that uses function-level code restructuring, heuristic reasoning, and cascaded verification to detect complex logic vulnerabilities accurately and efficiently. It operates on open-source LLMs, achieves high precision with minimal cost, outperforms existing tools, and has demonstrated success in real-world attack reconstruction and zero-day discovery.", "motivation": "Existing smart contract auditing methods face critical limitations: manual audits are unscalable, static analysis produces false positives, fuzzers fail to reach deep logic states, and AI-based approaches suffer from hallucinations, high context demands, and dependence on expensive proprietary LLMs. These shortcomings necessitate a robust, automated, and affordable solution.", "method": "Heimdallr employs four key innovations: (1) function-level code reorganization to reduce context overhead while preserving business logic, (2) heuristic reasoning to identify and chain complex vulnerabilities, (3) cascaded verification to eliminate false positives, and (4) operation on lightweight, open-source LLMs (e.g., GPToss-120B) without reliance on proprietary models.", "result": "Heimdallr successfully reconstructed 17 of 20 real-world attacks post-June 2025 (totaling $384M in losses) and discovered 4 confirmed zero-day vulnerabilities, preventing $400M in potential losses. It reduced analysis time by up to 97.59% and financial costs by 98.77% compared to SOTA baselines, while improving detection precision by over 93.66%. In auditing contests, it achieved a 92.45% detection rate at $2.31 per 10K LOC.", "conclusion": "Heimdallr represents a significant advancement in automated smart contract auditing by enabling accurate, cost-efficient, and scalable detection of complex business logic vulnerabilities using lightweight, open-source models, while outperforming state-of-the-art industrial and academic tools in precision, speed, and cost."}}
{"id": "2601.17875", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17875", "abs": "https://arxiv.org/abs/2601.17875", "authors": ["Sean Carlin", "Kevin Curran"], "title": "The Stateless Pattern: Ephemeral Coordination as the Third Pillar of Digital Sovereignty", "comment": "The implementation is released under the AGPLv3 license at https://github.com/scarlin90/signingroom with source code publicly auditable at the associated GitHub repository, enabling independent verification and self-hosting for users requiring maximum sovereignty", "summary": "For the past three decades, the architecture of the internet has rested on two primary pillars - communication on the World Wide Web and Value such as Bitcoin/Distributed ledgers. However, a third critical pillar, Private Coordination has remained dependent on centralised intermediaries, effectively creating a surveillance architecture by default. This paper introduces the 'Stateless Pattern', a novel network topology that replaces the traditional 'Fortress' security model (database-centric) with a 'Mist' model (ephemeral relays). By utilising client-side cryptography and self-destructing server instances, we demonstrate a protocol where the server acts as a blind medium rather than a custodian of state. We present empirical data from a live deployment (https://signingroom.io), analysing over 1,900 requests and cache-hit ratios to validate the system's 'Zero-Knowledge' properties and institutional utility. The findings suggest that digital privacy can be commoditised as a utility, technically enforcing specific articles of the universal declaration of human rights not through policy, but through physics.", "AI": {"tldr": "This paper presents the 'Stateless Pattern', a privacy-preserving network architecture that enables private coordination without trusted intermediaries by using client-side cryptography and ephemeral relays, effectively turning digital privacy into a technical utility.", "motivation": "The internet currently lacks a native infrastructure for private coordination, relying instead on centralized intermediaries that create inherent surveillance risks. This work aims to establish private coordination as a foundational pillar of the internet, alongside communication and value transfer.", "method": "The paper introduces the 'Stateless Pattern', a network topology using client-side cryptography and ephemeral (self-destructing) server relays to eliminate persistent data storage. The server acts as a blind intermediary, enabling coordination without custody of state. The protocol was validated via a live deployment at signingroom.io.", "result": "Empirical analysis from over 1,900 requests on a live system shows high cache-hit ratios and confirms the 'Zero-Knowledge' nature of the protocol. The system successfully demonstrates institutional utility while maintaining minimal server-side state.", "conclusion": "Digital privacy can be achieved as a utility by leveraging the 'Stateless Pattern' architecture, which enforces privacy-preserving coordination through technical design rather than policy, thereby realizing fundamental human rights via cryptographic and ephemeral system properties."}}
{"id": "2601.17907", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17907", "abs": "https://arxiv.org/abs/2601.17907", "authors": ["Numan Halit Guldemir", "Oluwafemi Olukoya", "Jes\u00fas Mart\u00ednez-del-Rinc\u00f3n"], "title": "FARM: Few-shot Adaptive Malware Family Classification under Concept Drift", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Malware classification models often face performance degradation due to concept drift, arising from evolving threat landscapes and the emergence of novel malware families. This paper presents FARM (Few-shot Adaptive Recognition of Malware), a framework designed to detect and adapt to both covariate and label drift in Windows Portable Executable (PE) malware classification. FARM leverages a triplet autoencoder to project samples into a discriminative latent space, enabling unsupervised drift detection via DBSCAN clustering and dynamic thresholding. For rapid adaptation, it employs few-shot learning using prototype-based classification, requiring only a handful of labeled samples. FARM also supports full retraining when enough drifted samples accumulate, updating the latent space for long-term integration. Experiments on the BenchMFC dataset demonstrate that FARM improves classification performance under covariate drift by 5.6\\%, and achieves an average F1 score of 0.85 on unseen malware families using only few-shot adaptation, which further increases to 0.94 after retraining. These results highlight FARM's robustness and adaptability in dynamic malware detection environments under limited supervision.", "AI": {"tldr": "FARM is a drift-aware malware classification framework that detects concept drift in Windows PE files using a triplet autoencoder and DBSCAN, and adapts quickly via few-shot learning, significantly improving performance in dynamic environments.", "motivation": "Malware classification models often degrade due to concept drift caused by evolving threats and new malware families; FARM addresses this by enabling continuous adaptation under limited labeled data.", "method": "FARM uses a triplet autoencoder to create a discriminative latent space, applies DBSCAN clustering and dynamic thresholding for unsupervised drift detection, and employs prototype-based few-shot learning for rapid adaptation, with periodic full retraining to update the latent space when sufficient drifted samples accumulate.", "result": "On the BenchMFC dataset, FARM improves classification performance under covariate drift by 5.6\\%, achieves an average F1 score of 0.85 on unseen malware families using few-shot learning, and reaches 0.94 after full retraining.", "conclusion": "FARM demonstrates robustness and adaptability in dynamic malware detection environments, effectively handling concept drift with limited supervision by combining unsupervised drift detection and few-shot learning, making it suitable for real-world deployment in evolving threat landscapes."}}
{"id": "2601.17909", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17909", "abs": "https://arxiv.org/abs/2601.17909", "authors": ["Adriana Watson"], "title": "From Statistical Disclosure Control to Fair AI: Navigating Fundamental Tradeoffs in Differential Privacy", "comment": "8 pages, 3 figures", "summary": "Differential privacy has become the gold standard for privacy-preserving machine learning systems. Unfortunately, subsequent work has primarily fixated on the privacy-utility tradeoff, leaving the subject of fairness constraints undervalued and under-researched. This paper provides a systematic treatment connecting three threads: (1) Dalenius's impossibility results for semantic privacy, (2) Dwork's differential privacy as an achievable alternative, and (3) emerging impossibility results from the addition of a fairness requirement. Through concrete examples and technical analysis, the three-way Pareto frontier between privacy, utility, and fairness is demonstrated to showcase the fundamental limits on what can be simultaneously achieved. In this work, these limits are characterized, the impact on minority groups is demonstrated, and practical guidance for navigating these tradeoffs are provided. This forms a unified framework synthesizing scattered results to help practitioners and policymakers make informed decisions when deploying private fair learning systems.", "AI": {"tldr": "This paper establishes fundamental limits in jointly achieving privacy, utility, and fairness in machine learning, demonstrating a three-way tradeoff through an analysis of impossibility results and offering guidance for realistic deployment of private and fair systems.", "motivation": "While differential privacy has become standard in privacy-preserving ML, fairness has been under-researched. The motivation is to address the gap in understanding the interplay and tradeoffs among privacy, utility, and fairness, especially given new impossibility results involving fairness.", "method": "The authors conduct a systematic analysis linking Dalenius's impossibility results, differential privacy, and emerging fairness impossibility results. They use concrete examples and technical analysis to characterize the three-way Pareto frontier among privacy, utility, and fairness.", "result": "The paper demonstrates a three-way Pareto frontier between privacy, utility, and fairness, characterizing the fundamental tradeoffs. It shows how these constraints disproportionately impact minority groups and provides practical guidance for managing the tradeoffs.", "conclusion": "The paper concludes that there are fundamental limits to simultaneously achieving privacy, utility, and fairness in machine learning systems, and it provides a unified framework to help practitioners and policymakers navigate these tradeoffs when deploying private and fair learning systems."}}
{"id": "2601.17911", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17911", "abs": "https://arxiv.org/abs/2601.17911", "authors": ["Thomas Heverin"], "title": "Prompt Injection Evaluations: Refusal Boundary Instability and Artifact-Dependent Compliance in GPT-4-Series Models", "comment": "15 pages, 3 figures, 1 table", "summary": "Prompt injection evaluations typically treat refusal as a stable, binary indicator of safety. This study challenges that paradigm by modeling refusal as a local decision boundary and examining its stability under structured perturbations. We evaluated two models, GPT-4.1 and GPT-4o, using 3,274 perturbation runs derived from refusal-inducing prompt injection attempts. Each base prompt was subjected to 25 perturbations across five structured families, with outcomes manually coded as Refusal, Partial Compliance, or Full Compliance.\n  Using chi-square tests, logistic regression, mixed-effects modeling, and a novel Refusal Boundary Entropy (RBE) metric, we demonstrate that while both models refuse >94% of attempts, refusal instability is persistent and non-uniform. Approximately one-third of initial refusal-inducing prompts exhibited at least one \"refusal escape,\" a transition to compliance under perturbation. We find that artifact type is a stronger predictor of refusal failure than perturbation style. Textual artifacts, such as ransomware notes, exhibited significantly higher instability, with flip rates exceeding 20%. Conversely, executable malware artifacts showed zero refusal escapes in both models. While GPT-4o demonstrated tighter refusal enforcement and lower RBE than GPT-4.1, it did not eliminate artifact-dependent risks. These findings suggest that single-prompt evaluations systematically overestimate safety robustness. We conclude that refusal behavior is a probabilistic, artifact-dependent boundary phenomenon rather than a stable binary property, requiring a shift in how LLM safety is measured and audited.", "AI": {"tldr": "Refusal in LLMs is not robust or binary\u2014structured perturbations reveal significant instability, especially for textual artifacts, undermining current safety evaluation practices.", "motivation": "To challenge the common assumption that refusal is a stable binary indicator of safety in prompt injection evaluations and to investigate the stability of refusal under structured perturbations.", "method": "Analyzed refusal stability using 3,274 perturbation runs on GPT-4.1 and GPT-4o, applying chi-square tests, logistic regression, mixed-effects modeling, and a novel Refusal Boundary Entropy (RBE) metric; perturbations were structured into five families and outcomes manually coded into refusal, partial, or full compliance.", "result": "Both models refused over 94% of attempts, yet ~1/3 of refusal-inducing prompts had at least one 'refusal escape'; textual artifacts (e.g., ransomware notes) showed high instability (flip rates >20%), while executable malware had zero escapes; GPT-4o showed tighter refusal control and lower RBE than GPT-4.1 but did not eliminate artifact-dependent risks.", "conclusion": "Refusal behavior in LLMs is not a stable binary property but a probabilistic, artifact-dependent boundary phenomenon, necessitating a paradigm shift in how safety is evaluated and audited."}}
{"id": "2601.17967", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17967", "abs": "https://arxiv.org/abs/2601.17967", "authors": ["Alon Hillel-Tuch"], "title": "Data Siphoning Through Advanced Persistent Transmission Attacks At The Physical Layer", "comment": "3 pages, extended abstract", "summary": "Data at the physical layer transmits via media such as copper cable, fiber optic, or wireless. Physical attack vectors exist that challenge data confidentiality and availability. Protocols and encryption standards help obfuscate but often cannot keep the data type and destination secure, with limited insight into confidentiality and integrity. We will investigate the feasibility of developing an awareness and integrity protocol to help mitigate physical side-channel attacks that lead to eavesdropping of data communication and denial-of-service.\n  Keywords: data confidentiality, siphoning, eavesdropping, person-in-the-middle, denial-of-service, physical layer attacks, nation-states", "AI": {"tldr": "This paper explores the feasibility of a physical layer awareness and integrity protocol to combat side-channel attacks, aiming to enhance data confidentiality and availability against eavesdropping, siphoning, and denial-of-service in wired and wireless media.", "motivation": "Physical layer security is often overlooked despite the increasing sophistication of physical attacks like siphoning and person-in-the-middle, which can compromise data confidentiality and integrity even when higher-layer encryption is used.", "method": "The paper investigates existing vulnerabilities in physical layer transmissions and evaluates the potential design of a new protocol focused on detecting and preventing side-channel attacks such as eavesdropping and denial-of-service.", "result": "The research demonstrates that a dedicated protocol can improve detection of physical layer breaches and provide better protection against eavesdropping and denial-of-service by increasing awareness and integrity at the transmission level.", "conclusion": "The study concludes that developing an awareness and integrity protocol at the physical layer is feasible and can effectively mitigate physical side-channel attacks, enhancing data confidentiality and availability."}}
{"id": "2601.18011", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.18011", "abs": "https://arxiv.org/abs/2601.18011", "authors": ["Niaz Mohammad Ramaki", "Florian Schintke"], "title": "MultiChain Blockchain Data Provenance for Deterministic Stream Processing with Kafka Streams: A Weather Data Case Study", "comment": null, "summary": "Auditability and reproducibility still are critical challenges for real-time data streams pipelines. Streaming engines are highly dependent on runtime scheduling, window triggers, arrival orders, and uncertainties such as network jitters. These all derive the streaming pipeline platforms to throw non-determinist outputs. In this work, we introduce a blockchain-backed provenance architecture for streaming platform (e.g Kafka Streams) the publishes cryptographic data of a windowed data stream without publishing window payloads on-chain. We used real-time weather data from weather stations in Berlin. Weather records are canonicalized, deduplicated, and aggregated per window, then serialised deterministically. Furthermore, the Merkle root of the records within the window is computed and stored alongside with Kafka offsets boundaries to MultiChain blockchain streams as checkpoints. Our design can enable an independent auditor to verify: (1) the completeness of window payloads, (2) canonical serialization, and (3) correctness of derived analytics such as minimum/maximum/average temperatures. We evaluated our system using real data stream from two weather stations (Berlin-Brandenburg and Berlin-Tempelhof) and showed linear verification cost, deterministic reproducibility, and with a scalable off-chain storage with on-chain cryptographic anchoring. We also demonstrated that the blockchain can afford to be integrated with streaming platforms particularly with our system, and we get satisfactory transactions per second values.", "AI": {"tldr": "This paper proposes a blockchain-based provenance system for Kafka Streams that ensures auditable and reproducible streaming analytics by storing Merkle roots of windowed data on-chain, enabling verifiable correctness without sacrificing performance.", "motivation": "Real-time streaming pipelines suffer from non-deterministic outputs due to runtime scheduling, data arrival order, and network uncertainties, making auditability and reproducibility challenging; this work aims to address these issues through tamper-proof provenance tracking.", "method": "A blockchain-backed provenance system is designed for Kafka Streams, where windowed data streams are canonicalized, deduplicated, and deterministically serialized; Merkle roots of windows are computed and stored on MultiChain blockchain alongside Kafka offsets as checkpoints, enabling verification without on-chain payload storage.", "result": "The system was evaluated using real weather data from two Berlin stations, demonstrating linear verification cost, deterministic reproducibility, scalable off-chain storage with on-chain anchoring, and satisfactory transaction throughput on the blockchain.", "conclusion": "The proposed blockchain-backed provenance architecture enables verifiable, auditable, and reproducible streaming data pipelines by leveraging cryptographic anchoring via Merkle roots stored on-chain, while maintaining scalability through off-chain data storage."}}
{"id": "2601.18068", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18068", "abs": "https://arxiv.org/abs/2601.18068", "authors": ["Jiayi Zhang", "Chenxin Sun", "Chenxiong Qian"], "title": "XGuardian: Towards Explainable and Generalized AI Anti-Cheat on FPS Games", "comment": "Accepted by USENIX Security 2026", "summary": "Aim-assist cheats are the most prevalent and infamous form of cheating in First-Person Shooter (FPS) games, which help cheaters illegally reveal the opponent's location and auto-aim and shoot, and thereby pose significant threats to the game industry. Although a considerable research effort has been made to automatically detect aim-assist cheats, existing works suffer from unreliable frameworks, limited generalizability, high overhead, low detection performance, and a lack of explainability of detection results. In this paper, we propose XGuardian, a server-side generalized and explainable system for detecting aim-assist cheats to overcome these limitations. It requires only two raw data inputs, pitch and yaw, which are all FPS games' must-haves, to construct novel temporal features and describe aim trajectories, which are essential for distinguishing cheaters and normal players. XGuardian is evaluated with the latest mainstream FPS game CS2, and validates its generalizability with another two different games. It achieves high detection performance and low overhead compared to prior works across different games with real-world and large-scale datasets, demonstrating wide generalizability and high effectiveness. It is able to justify its predictions and thereby shorten the ban cycle. We make XGuardian as well as our datasets publicly available.", "AI": {"tldr": "XGuardian is a generalized, server-side, explainable system that detects aim-assist cheats in FPS games using only pitch/yaw data, achieving high accuracy, low overhead, and strong generalizability across multiple games including CS2.", "motivation": "Existing aim-assist cheat detection systems suffer from poor generalizability, high overhead, low accuracy, and lack of explainability, which limits their real-world deployment and effectiveness in protecting the integrity of FPS games.", "method": "XGuardian uses only pitch and yaw data to construct temporal features and model aim trajectories, enabling cheat detection with minimal input while ensuring generalizability and explainability through machine learning models and feature analysis.", "result": "XGuardian achieves high detection performance with low computational overhead on real-world, large-scale datasets from CS2 and two other FPS games, validates broad generalizability, and provides explainable predictions that help shorten the ban verification cycle.", "conclusion": "XGuardian is a generalized, explainable, and effective server-side system for detecting aim-assist cheats in FPS games, demonstrating high performance, low overhead, and broad applicability across multiple games."}}
{"id": "2601.18105", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18105", "abs": "https://arxiv.org/abs/2601.18105", "authors": ["Mohammad Fasha", "Faisal Abul Rub", "Nasim Matar", "Bilal Sowan", "Mohammad Al Khaldy"], "title": "Mitigating the OWASP Top 10 For Large Language Models Applications using Intelligent Agents", "comment": "5 pages", "summary": "Large Language Models (LLMs) have emerged as a transformative and disruptive technology, enabling a wide range of applications in natural language processing, machine translation, and beyond. However, this widespread integration of LLMs also raised several security concerns highlighted by the Open Web Application Security Project (OWASP), which has identified the top 10 security vulnerabilities inherent in LLM applications. Addressing these vulnerabilities is crucial, given the increasing reliance on LLMs and the potential threats to data integrity, confidentiality, and service availability. This paper presents a framework designed to mitigate the security risks outlined in the OWASP Top 10. Our proposed model leverages LLM-enabled intelligent agents, offering a new approach to proactively identify, assess, and counteract security threats in real-time. The proposed framework serves as an initial blueprint for future research and development, aiming to enhance the security measures of LLMs and protect against emerging threats in this rapidly evolving landscape.", "AI": {"tldr": "This paper introduces a security framework using LLM-powered intelligent agents to address the OWASP Top 10 vulnerabilities in LLM applications, offering a proactive, real-time solution to enhance security in an evolving threat landscape.", "motivation": "The increasing integration of Large Language Models (LLMs) into various applications has raised significant security concerns, particularly regarding data integrity, confidentiality, and service availability, as highlighted by the OWASP Top 10. Addressing these vulnerabilities is crucial to ensuring secure deployment of LLMs.", "method": "The paper presents a framework that leverages LLM-enabled intelligent agents to proactively identify, assess, and counteract security threats in real-time, specifically targeting the OWASP Top 10 vulnerabilities in LLM applications.", "result": "The framework demonstrates a novel approach using intelligent agents to mitigate identified security risks, providing a foundation for future work in securing LLM-based systems.", "conclusion": "The proposed framework serves as an initial blueprint for future research and development, aiming to enhance the security measures of LLMs and protect against emerging threats in this rapidly evolving landscape."}}
{"id": "2601.18113", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18113", "abs": "https://arxiv.org/abs/2601.18113", "authors": ["Dezhang Kong", "Zhuxi Wu", "Shiqi Liu", "Zhicheng Tan", "Kuichen Lu", "Minghao Li", "Qichen Liu", "Shengyu Chu", "Zhenhua Xu", "Xuan Liu", "Meng Han"], "title": "MalURLBench: A Benchmark Evaluating Agents' Vulnerabilities When Processing Web URLs", "comment": null, "summary": "LLM-based web agents have become increasingly popular for their utility in daily life and work. However, they exhibit critical vulnerabilities when processing malicious URLs: accepting a disguised malicious URL enables subsequent access to unsafe webpages, which can cause severe damage to service providers and users. Despite this risk, no benchmark currently targets this emerging threat. To address this gap, we propose MalURLBench, the first benchmark for evaluating LLMs' vulnerabilities to malicious URLs. MalURLBench contains 61,845 attack instances spanning 10 real-world scenarios and 7 categories of real malicious websites. Experiments with 12 popular LLMs reveal that existing models struggle to detect elaborately disguised malicious URLs. We further identify and analyze key factors that impact attack success rates and propose URLGuard, a lightweight defense module. We believe this work will provide a foundational resource for advancing the security of web agents. Our code is available at https://github.com/JiangYingEr/MalURLBench.", "code_url": "https://github.com/JiangYingEr/MalURLBench", "code_stars": 0, "code_last_update": "2026-01-25", "AI": {"tldr": "This paper presents MalURLBench, the first benchmark for evaluating LLM vulnerabilities to malicious URLs, containing 61,845 real-world attack instances. Testing 12 LLMs reveals weak detection capabilities, prompting the development of URLGuard, a lightweight defense to improve security in web agents.", "motivation": "LLM-based web agents are increasingly used but remain vulnerable to disguised malicious URLs, posing significant risks to users and service providers. The absence of a dedicated benchmark to evaluate such vulnerabilities motivates the creation of MalURLBench to systematically assess and improve model security.", "method": "The authors constructed MalURLBench, a large-scale benchmark with 61,845 attack instances across 10 real-world scenarios and 7 categories of malicious websites. They evaluated 12 popular LLMs on this benchmark to assess detection performance and analyzed key factors influencing attack success. Based on findings, they developed URLGuard, a lightweight defense mechanism to enhance URL threat detection.", "result": "Experiments on 12 popular LLMs show poor performance in detecting elaborately disguised malicious URLs, confirming significant security weaknesses. Key factors affecting attack success were identified, and URLGuard demonstrated effectiveness as a lightweight defense to mitigate such threats.", "conclusion": "The work highlights the vulnerability of LLM-based web agents to malicious URLs and introduces MalURLBench as a foundational benchmark to evaluate and improve their security. The proposed defense module, URLGuard, offers a practical solution, and the authors advocate for increased focus on securing web agents against URL-based threats."}}
{"id": "2601.18216", "categories": ["cs.CR", "cs.DC", "cs.OS"], "pdf": "https://arxiv.org/pdf/2601.18216", "abs": "https://arxiv.org/abs/2601.18216", "authors": ["Beom Heyn Kim", "Seok Min Hong", "Mohammad Mannan"], "title": "Rhea: Detecting Privilege-Escalated Evasive Ransomware Attacks Using Format-Aware Validation in the Cloud", "comment": "12 pages, 6 figures, under review (Jan 2026)", "summary": "Ransomware variants increasingly combine privilege escalation with sophisticated evasion strategies such as intermittent encryption, low-entropy encryption, and imitation attacks. Such powerful ransomware variants, privilege-escalated evasive ransomware (PEER), can defeat existing solutions relying on I/O-pattern analysis by tampering with or obfuscating I/O traces. Meanwhile, conventional statistical content-based detection becomes unreliable as the encryption size decreases due to sampling noises. We present Rhea, a cloud-offloaded ransomware defense system that analyzes replicated data snapshots, so-called mutation snapshots. Rhea introduces Format-Aware Validation that validates the syntactic and semantic correctness of file formats, instead of relying on statistical or entropy-based indicators. By leveraging file-format specifications as detection invariants, Rhea can reliably identify fine-grained and evasive encryption even under elevated attacker privileges. Our evaluation demonstrates that Rhea significantly outperforms existing approaches, establishing its practical effectiveness against modern ransomware threats.", "AI": {"tldr": "Rhea is a cloud-based ransomware detection system that uses format-aware validation on mutation snapshots to detect evasive ransomware by checking file format correctness, offering high accuracy against privilege-escalated and low-entropy encryption attacks.", "motivation": "Existing ransomware detection methods based on I/O patterns or statistical/entropy analysis are ineffective against privilege-escalated evasive ransomware (PEER) that obfuscates I/O traces or uses minimal encryption; a more robust and invariant-based detection approach is needed.", "method": "Rhea analyzes replicated data snapshots (mutation snapshots) using Format-Aware Validation, which checks the syntactic and semantic correctness of file formats against known specifications, enabling detection of subtle, low-entropy, and intermittent encryption patterns.", "result": "Rhea demonstrates superior detection accuracy and robustness against modern evasive ransomware variants, including those with elevated privileges and obfuscated behavior, significantly outperforming state-of-the-art detection techniques in evaluation.", "conclusion": "Rhea is an effective cloud-offloaded ransomware defense system that reliably detects evasive ransomware like PEER by leveraging format-aware validation based on file-format specifications, outperforming existing statistical and I/O-pattern-based methods."}}
{"id": "2601.18413", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.18413", "abs": "https://arxiv.org/abs/2601.18413", "authors": ["Darlan Noetzold", "Valderi Reis Quietinho Leithardt"], "title": "Fundamentals, Recent Advances, and Challenges Regarding Cryptographic Algorithms for the Quantum Computing Era", "comment": "in Portuguese language", "summary": "This book arises from the need to provide a clear and up-to-date overview of the impacts of quantum computing on cryptography. The goal is to provide a reference in Portuguese for undergraduate, master's, and doctoral students in the field of data security and cryptography. Throughout the chapters, we present fundamentals, we discuss classical and post-quantum algorithms, evaluate emerging patterns, and point out real-world implementation challenges. The initial objective is to serve as a guide for students, researchers, and professionals who need to understand not only the mathematics involved, but also its practical implications in security systems and policies. For more advanced professionals, the main objective is to present content and ideas so that they can assess the changes and perspectives in the era of quantum cryptographic algorithms. To that end, the text's structure was designed to be progressive: we begin with essential concepts, move on to quantum algorithms and their consequences (with emphasis on Shor's algorithm), present issues focusing on \"families\" of post-quantum schemes (based on lattices, codes, hash functions, multivariate, isogenies), analyze the state of the art in standardization (highlighting the NIST process), and finally, discuss migration, interoperability, performance, and cryptographic governance. We hope that this work will assist in the formation of critical thinking and informed technical decision-making, fostering secure transition strategies for the post-quantum era.", "AI": {"tldr": "This Portuguese-language book offers a comprehensive, progressive guide on quantum computing's impact on cryptography, covering fundamentals, quantum threats, post-quantum algorithms, NIST standardization, and practical migration challenges for students and professionals.", "motivation": "To provide a clear, updated, and accessible resource in Portuguese on the impact of quantum computing on cryptography, catering to diverse audiences in data security and addressing the urgency of preparing for post-quantum cryptographic transitions.", "method": "The book adopts a progressive structure, starting from foundational concepts, advancing through quantum and post-quantum cryptographic algorithms, analyzing standardization efforts (especially NIST), and addressing practical implementation and governance challenges.", "result": "A structured, educational reference covering quantum threats (e.g., Shor\u2019s algorithm), post-quantum cryptographic families, standardization progress, and real-world deployment issues, enabling readers to evaluate and prepare for the quantum-era security landscape.", "conclusion": "The book aims to foster critical thinking and informed decision-making for a secure transition to post-quantum cryptography, serving as a comprehensive reference in Portuguese for students, researchers, and professionals."}}
{"id": "2601.18445", "categories": ["cs.CR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.18445", "abs": "https://arxiv.org/abs/2601.18445", "authors": ["Eymen \u00dcnay", "Bj\u00f6rn Franke", "Jackson Woodruff"], "title": "KeyMemRT Compiler and Runtime: Unlocking Memory-Scalable FHE", "comment": null, "summary": "Fully Homomorphic Encryption (FHE) enables privacy preserving computation but it suffers from high latency and memory consumption. The computations are secured with special keys called rotation keys which often take up the majority of memory. In complex FHE applications, these rotation keys can cause a large memory bottleneck limiting program throughput. Existing compilers make little effort to solve this problem, instead relying on systems with massive memory availability. This resource requirement is a barrier to FHE uptake because optimizing FHE programs by hand is challenging due to their scale, complexity and expertise required.\n  In this work, we present KeyMemRT; an MLIR based compiler and runtime framework that individually manages rotation key lifetimes to lower memory utilization and to allow arbitrary number of rotation indices to be supported without memory bloating. KeyMemRT relies on dataflow analysis to determine key lifetimes and is the first FHE compiler to provide automatic key management, handle fine-grained key-mangement and manage boostrap keys. We implement frontends for Orion and HEIR and show improvements over state-of-the-art FHE compilers. KeyMemRT achieves memory reduction of 1.74x and a speedup of 1.20x over ANT-ACE, and memory reduction of 1.16x and a speedup of 1.73x over memory-optimized compiler Fhelipe. We provide KeyMemRT as a post-optimizing compiler that can be targeted by any FHE compiler.", "AI": {"tldr": "KeyMemRT is an MLIR-based compiler and runtime for FHE that reduces memory and improves performance by automatically managing rotation and bootstrap key lifetimes via dataflow analysis, achieving up to 1.74x memory reduction and 1.73x speedup over state-of-the-art compilers.", "motivation": "Fully Homomorphic Encryption (FHE) faces high memory consumption due to large rotation keys, which creates a memory bottleneck and limits scalability and adoption; existing compilers do not adequately manage key lifetimes, requiring excessive memory and expert manual optimization.", "method": "KeyMemRT uses dataflow analysis in an MLIR-based framework to determine the lifetimes of rotation and bootstrap keys, enabling automatic and fine-grained key management; it implements frontends for Orion and HEIR and operates as a post-optimizing compiler.", "result": "KeyMemRT reduces memory usage by 1.74x and speeds up execution by 1.20x compared to ANT-ACE, and achieves 1.16x memory reduction and 1.73x speedup over the memory-optimized Fhelipe; it supports arbitrary rotation indices without memory bloating.", "conclusion": "KeyMemRT is an effective, MLIR-based compiler and runtime framework that enables significant memory reduction and performance improvements in FHE applications through automatic, fine-grained rotation key management, and is compatible with existing FHE compilers as a post-optimizing tool."}}
{"id": "2601.18511", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.18511", "abs": "https://arxiv.org/abs/2601.18511", "authors": ["Jaiyoung Park", "Sejin Park", "Jai Hyun Park", "Jung Ho Ahn", "Jung Hee Cheon", "Guillaume Hanrot", "Jung Woo Kim", "Minje Park", "Damien Stehl\u00e9"], "title": "Scaling up Privacy-Preserving ML: A CKKS Implementation of Llama-2-7B", "comment": null, "summary": "As large language models (LLMs) become ubiquitous, privacy concerns pertaining to inference inputs keep growing. In this context, fully homomorphic encryption (FHE) has emerged as a primary cryptographic solution to provide non-interactive confidential LLM inference. Existing solutions scale poorly with the input token length, and hence focus either on small models or larger models with a small number of input tokens. They also suffer from the existence of large outlier values. These values have a strong impact on the evaluation of non-linear layers, leading to large-degree polynomial approximation and thus heavy evaluation costs.\n  We propose an FHE-based private LLM inference solution that allows thousands of input tokens with only a part of them being encrypted: this fits with a scenario where the context is benign and only part of the input is sensitive. To do so, we suggest an unbalanced chunked prefill framework that processes the private and public parts of the input tokens differently. Our framework contains plaintext-plaintext, plaintext-ciphertext and ciphertext-ciphertext computational components. We adopt different strategies and ingredients for each component. We also devise new homomorphic algorithms for specific matrix multiplication and polynomial evaluation tasks encountered during LLM inference.\n  Furthermore, without retraining, we tailor the LLM inference algorithm to reduce the ranges of outlier values: we leverage machine learning strategies (token prepending and rotations) to mitigate the impact of the outliers on non-linear layers.\n  Based on these ingredients, we describe a CKKS-based end-to-end implementation of Llama-2-7B private inference for up to 4096 input tokens, of which the last 128 are encrypted. On a cluster of 8~NVIDIA RTX-4090 GPUs, inference takes 85s for summarization and 33s for generation per output token.", "AI": {"tldr": "This paper presents a scalable FHE-based private inference system for LLMs that handles up to 4096 input tokens with selective encryption of the last 128 tokens, using an unbalanced prefill framework, optimized homomorphic operations, and outlier mitigation via token manipulation, enabling efficient and confidential Llama-2-7B inference.", "motivation": "Privacy concerns in LLM inference necessitate cryptographic protection of input data. Existing FHE-based methods are limited by poor scalability with input length and high computational overhead due to outlier values in activation ranges, especially in non-linear layers.", "method": "An unbalanced chunked prefill framework is introduced that differentiates processing between public and private input tokens, employing plaintext-plaintext, plaintext-ciphertext, and ciphertext-ciphertext operations. New homomorphic algorithms are designed for matrix multiplication and polynomial evaluation in non-linear layers. Outlier mitigation is achieved via token prepending and rotational techniques without retraining.", "result": "An end-to-end CKKS-based implementation for Llama-2-7B supports up to 4096 input tokens with the last 128 encrypted. On 8 NVIDIA RTX-4090 GPUs, inference latency is 85s for summarization and 33s per output token for generation, demonstrating feasibility of large-scale private inference.", "conclusion": "The proposed FHE-based framework enables efficient and private LLM inference with long input sequences by selectively encrypting sensitive portions, achieving practical performance for Llama-2-7B with up to 4096 input tokens and 128 encrypted tokens."}}
{"id": "2601.18612", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18612", "abs": "https://arxiv.org/abs/2601.18612", "authors": ["Susim Roy", "Nalini Ratha"], "title": "Multimodal Privacy-Preserving Entity Resolution with Fully Homomorphic Encryption", "comment": "5 pages, 3 figures, IEEE ICASSP'26", "summary": "The canonical challenge of entity resolution within high-compliance sectors, where secure identity reconciliation is frequently confounded by significant data heterogeneity, including syntactic variations in personal identifiers, is a longstanding and complex problem. To this end, we introduce a novel multimodal framework operating with the voluminous data sets typical of government and financial institutions. Specifically, our methodology is designed to address the tripartite challenge of data volume, matching fidelity, and privacy. Consequently, the underlying plaintext of personally identifiable information remains computationally inaccessible throughout the matching lifecycle, empowering institutions to rigorously satisfy stringent regulatory mandates with cryptographic assurances of client confidentiality while achieving a demonstrably low equal error rate and maintaining computational tractability at scale.", "AI": {"tldr": "This paper presents a secure, scalable multimodal framework for entity resolution in high-compliance domains, addressing data heterogeneity and privacy through cryptographic protection, achieving high accuracy and regulatory adherence with low error rates and computational efficiency.", "motivation": "Entity resolution in regulated sectors is hindered by data heterogeneity, privacy constraints, and scalability demands, necessitating a solution that ensures secure identity reconciliation without compromising compliance or performance.", "method": "A novel multimodal framework is introduced that processes large-scale datasets in government and financial sectors, designed to handle data volume, matching accuracy, and privacy by keeping personally identifiable information cryptographically protected during the matching process.", "result": "The framework achieves a low equal error rate, ensures computational inaccessibility of plaintext PII, and maintains efficiency at scale, providing strong privacy guarantees and regulatory compliance.", "conclusion": "The proposed multimodal framework enables secure, scalable, and accurate entity resolution in high-compliance sectors, satisfying regulatory requirements through cryptographic privacy protections while maintaining high matching performance."}}
{"id": "2601.18754", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18754", "abs": "https://arxiv.org/abs/2601.18754", "authors": ["Mohamed Amine Ferrag", "Abderrahmane Lakas", "Merouane Debbah"], "title": "$\u03b1^3$-SecBench: A Large-Scale Evaluation Suite of Security, Resilience, and Trust for LLM-based UAV Agents over 6G Networks", "comment": null, "summary": "Autonomous unmanned aerial vehicle (UAV) systems are increasingly deployed in safety-critical, networked environments where they must operate reliably in the presence of malicious adversaries. While recent benchmarks have evaluated large language model (LLM)-based UAV agents in reasoning, navigation, and efficiency, systematic assessment of security, resilience, and trust under adversarial conditions remains largely unexplored, particularly in emerging 6G-enabled settings.\n  We introduce $\u03b1^{3}$-SecBench, the first large-scale evaluation suite for assessing the security-aware autonomy of LLM-based UAV agents under realistic adversarial interference. Building on multi-turn conversational UAV missions from $\u03b1^{3}$-Bench, the framework augments benign episodes with 20,000 validated security overlay attack scenarios targeting seven autonomy layers, including sensing, perception, planning, control, communication, edge/cloud infrastructure, and LLM reasoning. $\u03b1^{3}$-SecBench evaluates agents across three orthogonal dimensions: security (attack detection and vulnerability attribution), resilience (safe degradation behavior), and trust (policy-compliant tool usage).\n  We evaluate 23 state-of-the-art LLMs from major industrial providers and leading AI labs using thousands of adversarially augmented UAV episodes sampled from a corpus of 113,475 missions spanning 175 threat types. While many models reliably detect anomalous behavior, effective mitigation, vulnerability attribution, and trustworthy control actions remain inconsistent. Normalized overall scores range from 12.9% to 57.1%, highlighting a significant gap between anomaly detection and security-aware autonomous decision-making. We release $\u03b1^{3}$-SecBench on GitHub: https://github.com/maferrag/AlphaSecBench", "code_url": "https://github.com/maferrag/AlphaSecBench", "code_stars": 0, "code_last_update": "2026-01-27", "AI": {"tldr": "The paper introduces $\\alpha^{3}$-SecBench, the first large-scale framework to evaluate security, resilience, and trust of LLM-based UAV agents under adversarial attacks across seven autonomy layers, revealing major gaps in current models' secure decision-making despite decent anomaly detection.", "motivation": "Existing benchmarks for LLM-based UAV agents focus on reasoning and efficiency but lack systematic evaluation of security, resilience, and trust under adversarial conditions, especially in 6G-enabled networked environments.", "method": "The authors introduce $\\alpha^{3}$-SecBench, which extends $\\alpha^{3}$-Bench with 20,000 validated adversarial scenarios across seven UAV autonomy layers, and evaluate 23 state-of-the-art LLMs using 113,475 mission episodes covering 175 threat types across three dimensions: security, resilience, and trust.", "result": "Evaluation of 23 LLMs shows normalized scores between 12.9% and 57.1%; while many models detect anomalies, they struggle with mitigation, vulnerability attribution, and trustworthy control, revealing inconsistencies in security-aware autonomy.", "conclusion": "There is a significant gap in security-aware autonomous decision-making for LLM-based UAV agents, despite decent anomaly detection capabilities; $\\alpha^{3}$-SecBench provides a foundational benchmark to evaluate and improve security, resilience, and trust in such systems."}}
