<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 10]
- [cs.SE](#cs.SE) [Total: 8]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Network-Aware Differential Privacy](https://arxiv.org/abs/2509.04710)
*Zhou Li,Yu Zheng,Tianhao Wang,Sang-Woo Jun*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Differential privacy (DP) is a privacy-enhancement technology (PET) that
receives prominent attention from the academia, industry, and government. One
main development over the past decade has been the decentralization of DP,
including local DP and shuffle DP. Despite that decentralized DP heavily relies
on network communications for data collection,we found that: 1) no systematic
study has surveyed the research opportunities at the intersection of networking
and DP; 2) nor have there been significant efforts to develop DP mechanisms
that are explicitly tailored for network environments. In this paper, we seek
to address this gap by initiating a new direction of network-aware DP. We
identified two focus areas where the network research can offer substantive
contributions to the design and deployment of DP, related to network security
and topology. Through this work, we hope to encourage more research that
adapt/optimize DP's deployment in various network environments.

</details>


### [2] [Cryptographic Application of Elliptic Curve with High Rank](https://arxiv.org/abs/2509.04941)
*Xiaogang Cheng,Ren Guo,Zuxi Chen*

Main category: cs.CR

TL;DR: This paper proposes a novel cryptographic scheme using high-rank elliptic curves for hierarchical revocation, proving their cryptographic potential despite efficiency tradeoffs.


<details>
  <summary>Details</summary>
Motivation: Elliptic curve cryptography offers superior efficiency and security over RSA and finite field discrete logarithm methods. High-rank elliptic curves (previously unexplored in cryptography) present new possibilities for cryptographic protocols.

Method: Constructed a public-key signature scheme with hierarchical revocation using elliptic curves with high rank, where the curve's rank determines the revocation tree's height.

Result: The construction demonstrates the feasibility and value of high-rank elliptic curves in cryptography, even if it's not the most efficient solution. The methods and assumptions can be applied to other cryptographic schemes.

Conclusion: High-rank elliptic curves establish significant cryptographic value and the techniques open new pathways for future cryptographic constructions with wide applicability.

Abstract: Elliptic curve cryptography is better than traditional cryptography based on
RSA and discrete logarithm of finite field in terms of efficiency and security.
In this paper, we show how to exploit elliptic curve with high rank, which has
not been used in cryptography before, to construct cryptographic schemes.
Concretely we demonstrate how to construct public key signature scheme with
hierarchy revocation based on elliptic curve with high rank, where the rank
determines the height of the revocation tree. Although our construction is not
very efficient in some sense, our construction shows elliptic curve with high
rank is valuable and important for cryptographic usage. The technique and
assumption presented can surely be used for other cryptographic constructions.

</details>


### [3] [Adversarial Augmentation and Active Sampling for Robust Cyber Anomaly Detection](https://arxiv.org/abs/2509.04999)
*Sidahmed Benabderrahmane,Talal Rahwan*

Main category: cs.CR

TL;DR: Combining AutoEncoders and active learning, this paper introduces a cost-effective APT detection framework that enhances accuracy with minimal labeled data, validated on real-world imbalanced datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional supervised learning methods require abundant labeled data, which is scarce in real-world cybersecurity scenarios. Advanced Persistent Threats (APTs) are stealthy and long-term, necessitating efficient detection mechanisms that work with minimal labeled data.

Method: The paper proposes an Attention Adversarial Dual AutoEncoder-based anomaly detection framework combined with active learning. The method uses AutoEncoders for initial anomaly detection and iteratively improves performance by querying an oracle for labels on uncertain samples, reducing labeling costs and dependency on large labeled datasets.

Result: Evaluation on DARPA Transparent Computing data (0.004% APT-like attacks) across Android, Linux, BSD, and Windows shows improved detection rates during active learning. The method outperforms existing approaches in two attack scenarios, highlighting its effectiveness in imbalanced data environments.

Conclusion: The framework demonstrates substantial improvements in APT detection rates on real-world imbalanced datasets, outperforming existing methods while minimizing manual labeling efforts. This approach addresses the challenges of data scarcity and high labeling costs in APT detection.

Abstract: Advanced Persistent Threats (APTs) present a considerable challenge to
cybersecurity due to their stealthy, long-duration nature. Traditional
supervised learning methods typically require large amounts of labeled data,
which is often scarce in real-world scenarios. This paper introduces a novel
approach that combines AutoEncoders for anomaly detection with active learning
to iteratively enhance APT detection. By selectively querying an oracle for
labels on uncertain or ambiguous samples, our method reduces labeling costs
while improving detection accuracy, enabling the model to effectively learn
with minimal data and reduce reliance on extensive manual labeling. We present
a comprehensive formulation of the Attention Adversarial Dual AutoEncoder-based
anomaly detection framework and demonstrate how the active learning loop
progressively enhances the model's performance. The framework is evaluated on
real-world, imbalanced provenance trace data from the DARPA Transparent
Computing program, where APT-like attacks account for just 0.004\% of the data.
The datasets, which cover multiple operating systems including Android, Linux,
BSD, and Windows, are tested in two attack scenarios. The results show
substantial improvements in detection rates during active learning,
outperforming existing methods.

</details>


### [4] [From Protest to Power Plant: Interpreting the Role of Escalatory Hacktivism in Cyber Conflict](https://arxiv.org/abs/2509.05104)
*Richard Derbyshire,Diana Selck-Paulsson,Charl van der Walt,Joe Burton*

Main category: cs.CR

TL;DR: This paper explores the shifting landscape of hacktivism in the context of state-sponsored cyber warfare and its implications on international security policy.


<details>
  <summary>Details</summary>
Motivation: Hacktivist groups are increasingly targeting critical infrastructure in an manner that resembles cyber warfare, thereby affecting international security.

Method: The authors present a new method to interpret the roles of hacktivists by detecting their alignment with state ideology, measuring the impact of their actions, and observing the host state's involvement.

Result: The research provides insights into the motivations of hacktivists and their strategic use by states as proxies in cyber conflicts, leading to new frameworks for understanding their actions.

Conclusion: The paper concludes with an evaluation of policy and security implications, offering strategies to manage hacktivist threats and contributing to international cyber security governance discussions.

Abstract: Since 2022, hacktivist groups have escalated their tactics, expanding from
distributed denial-of-service attacks and document leaks to include targeting
operational technology (OT). By 2024, attacks on the OT of critical national
infrastructure (CNI) had been linked to partisan hacktivist efforts in ongoing
geopolitical conflicts, demonstrating a shift from protest to something more
resembling cyber warfare. This escalation raises critical questions about the
classification of these groups and the appropriate state response to their
growing role in destabilizing international security.
  This paper examines the strategic motivations behind escalatory hacktivism,
highlighting how states may tolerate, encourage, or leverage hacktivist groups
as proxies in conflicts that blur the lines between activism, cybercrime, and
state-sponsored operations. We introduce a novel method for interpreting
hacktivists based on the impact of their actions, alignment to state ideology,
and host state involvement, offering a structured approach to understanding the
phenomenon. Finally, we assess policy and security implications, particularly
for host and victim states, and propose strategies to address this evolving
threat. By doing so, this paper contributes to international discussions on
cyber security policy, governance, and the increasing intersection between
non-state cyber actors and state interests.

</details>


### [5] [Odoo-based Subcontract Inter-site Access Control Mechanism for Construction Projects](https://arxiv.org/abs/2509.05149)
*Huy Hung Ho,Nhan Le Thanh*

Main category: cs.CR

TL;DR: The paper proposes a modular Odoo ERP subsystem to manage labor elasticity in Construction 4.0, using hybrid access control, third-party integration, and role-based mapping algorithms for scalable subcontractor coordination.


<details>
  <summary>Details</summary>
Motivation: Construction 4.0â€™s reliance on specialized subcontractors creates challenges in hierarchical integration and inter-site collaboration, requiring a system to manage dynamic labor scaling while maintaining security and operational independence.

Method: The system uses a modular architecture with three-pronged synchronization, hybrid access control, cross-domain third-party integration, and a role-based mapping algorithm leveraging a tree-like index and Lagrange interpolation for efficient role mapping.

Result: Experimental results confirm scalability, performance improvements in database operations, and successful practical applications in outsourcing, integration, and scalability scenarios.

Conclusion: The system demonstrates robustness in high-user-volume scenarios and offline conditions, aligning with evolving smart construction demands through improved database performance and workflow adaptability.

Abstract: In the era of Construction 4.0, the industry is embracing a new paradigm of
labor elasticity, driven by smart and flexible outsourcing and subcontracting
strategies. The increased reliance on specialized subcontractors enables
companies to scale labor dynamically based on project demands. This adaptable
workforce model presents challenges in managing hierarchical integration and
coordinating inter-site collaboration. Our design introduces a subsystem
integrated into the Odoo ERP framework, employing a modular architecture to
streamline labor management, task tracking, and approval workflows. The system
adopts a three-pronged approach to ensure synchronized data exchange between
general contractors and subcontractors, while maintaining both security and
operational independence. The system features hybrid access control,
third-party integration for cross-domain communication, and role-based mapping
algorithm across sites. The system supports varying degrees of customization
through a unified and consolidated attribute mapping center. This center
leverages a tree-like index structure and Lagrange interpolation method to
enhance the efficiency of role mapping. Demonstrations highlight practical
application in outsourcing, integration, and scalability scenarios, confirming
the system's robustness under high user volumes and in offline conditions.
Experimental results further show improvements in database performance and
workflow adaptability to support a scalable, enterprise-level solution that
aligns with the evolving demands of smart construction management.

</details>


### [6] [Reinforcing Secure Live Migration through Verifiable State Management](https://arxiv.org/abs/2509.05150)
*Stefanos Vasileaidis,Thanassis Giannetsos,Matthias Schunter,Bruno Crispo*

Main category: cs.CR

TL;DR: TALOS is a portable, TEE-agnostic framework enabling secure live migration of trusted applications by enforcing integrity verification and minimizing trust assumptions through memory introspection and control-flow analysis.


<details>
  <summary>Details</summary>
Motivation: Live migration of trusted applications in TEEs faces critical security risks like untrusted state preservation, cloning, and integrity breaches, requiring a framework that balances portability, zero-trust verification, and execution consistency.

Method: Leverages memory introspection and control-flow graph extraction for TA verification, employs cryptographically signed snapshots for replay/rollback prevention, and enforces secure TEE bootstrapping without relying on trusted third parties.

Result: Achieved secure, verifiable migration across diverse TEEs (SGX/Keystone) with robust integrity checks and minimal performance overhead, validated through prototyping and cross-architecture evaluation.

Conclusion: TALOS successfully addresses challenges in TA migration across domain boundaries, offering efficient and portable security-critical guarantees for decentralized computing environments.

Abstract: Live migration of applications is a fundamental capability for enabling
resilient computing in modern distributed systems. However, extending this
functionality to trusted applications (TA) -- executing within Trusted
Execution Environments (TEEs) -- introduces unique challenges such as secure
state preservation, integrity verification, replay and rollback prevention, and
mitigation of unauthorized cloning of TAs. We present TALOS, a lightweight
framework for verifiable state management and trustworthy application
migration. While our implementation is prototyped and evaluated using Intel SGX
with the Gramine LibOS and RISC-V Keystone (evidencing the framework's
portability across diverse TEEs), its design is agnostic to the underlying TEE
architecture. Such agility is a necessity in today's network service mesh
(collaborative computing across the continuum) where application workloads must
be managed across domain boundaries in a harmonized fashion. TALOS is built
around the principle of minimizing trust assumptions: TAs are treated as
untrusted until explicitly verified, and the migration process does not rely on
a trusted third party. To ensure both the integrity and secure launch of the
migrated application, TALOS integrates memory introspection and control-flow
graph extraction, enabling robust verification of state continuity and
execution flow. Thereby achieving strong security guarantees while maintaining
efficiency, making it suitable for decentralized settings.

</details>


### [7] [Jamming Smarter, Not Harder: Exploiting O-RAN Y1 RAN Analytics for Efficient Interference](https://arxiv.org/abs/2509.05161)
*Abiodun Ganiyu,Dara Ron,Syed Rafiul Hussain,Vijay K Shah*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Y1 interface in O-RAN enables the sharing of RAN Analytics Information
(RAI) between the near-RT RIC and authorized Y1 consumers, which may be
internal applications within the operator's trusted domain or external systems
accessing data through a secure exposure function. While this visibility
enhances network optimization and enables advanced services, it also introduces
a potential security risk -- a malicious or compromised Y1 consumer could
misuse analytics to facilitate targeted interference. In this work, we
demonstrate how an adversary can exploit the Y1 interface to launch selective
jamming attacks by passively monitoring downlink metrics. We propose and
evaluate two Y1-aided jamming strategies: a clustering-based jammer leveraging
DBSCAN for traffic profiling and a threshold-based jammer. These are compared
against two baselines strategies -- always-on jammer and random jammer -- on an
over-the-air LTE/5G O-RAN testbed. Experimental results show that in
unconstrained jamming budget scenarios, the threshold-based jammer can closely
replicate the disruption caused by always-on jamming while reducing
transmission time by 27\%. Under constrained jamming budgets, the
clustering-based jammer proves most effective, causing up to an 18.1\% bitrate
drop while remaining active only 25\% of the time. These findings reveal a
critical trade-off between jamming stealthiness and efficiency, and illustrate
how exposure of RAN analytics via the Y1 interface can enable highly targeted,
low-overhead attacks, raising important security considerations for both
civilian and mission-critical O-RAN deployments.

</details>


### [8] [Verifiability and Privacy in Federated Learning through Context-Hiding Multi-Key Homomorphic Authenticators](https://arxiv.org/abs/2509.05162)
*Simone Bottoni,Giulio Zizzo,Stefano Braghin,Alberto Trombetta*

Main category: cs.CR

TL;DR: This paper introduces a verifiable federated learning protocol that allows clients to verify the aggregator's computations without compromising the privacy of their model updates, using secure aggregation and homomorphic authenticators.


<details>
  <summary>Details</summary>
Motivation: Existing federated learning security solutions focus on malicious clients but neglect attacks by the aggregator (e.g., model biasing or privacy leakage). The work addresses this gap by ensuring clients can detect aggregator manipulations.

Method: The method combines secure aggregation techniques with a linearly homomorphic authenticator scheme, enabling clients to verify the correctness of the global model update without learning individual contributions.

Result: The authors demonstrate that their protocol scales to large neural networks with millions of parameters, achieving efficient verification with minimal computational overhead, and confirm its practicality for real-world deployments.

Conclusion: The paper concludes that their approach effectively enables efficient, privacy-preserving verification of aggregation in federated learning, even for large-scale models, while maintaining low computational overhead.

Abstract: Federated Learning has rapidly expanded from its original inception to now
have a large body of research, several frameworks, and sold in a variety of
commercial offerings. Thus, its security and robustness is of significant
importance. There are many algorithms that provide robustness in the case of
malicious clients. However, the aggregator itself may behave maliciously, for
example, by biasing the model or tampering with the weights to weaken the
models privacy. In this work, we introduce a verifiable federated learning
protocol that enables clients to verify the correctness of the aggregators
computation without compromising the confidentiality of their updates. Our
protocol uses a standard secure aggregation technique to protect individual
model updates with a linearly homomorphic authenticator scheme that enables
efficient, privacy-preserving verification of the aggregated result. Our
construction ensures that clients can detect manipulation by the aggregator
while maintaining low computational overhead. We demonstrate that our approach
scales to large models, enabling verification over large neural networks with
millions of parameters.

</details>


### [9] [On Hyperparameters and Backdoor-Resistance in Horizontal Federated Learning](https://arxiv.org/abs/2509.05192)
*Simon Lachnit,Ghassan Karame*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Horizontal Federated Learning (HFL) is particularly vulnerable to backdoor
attacks as adversaries can easily manipulate both the training data and
processes to execute sophisticated attacks. In this work, we study the impact
of training hyperparameters on the effectiveness of backdoor attacks and
defenses in HFL. More specifically, we show both analytically and by means of
measurements that the choice of hyperparameters by benign clients does not only
influence model accuracy but also significantly impacts backdoor attack
success. This stands in sharp contrast with the multitude of contributions in
the area of HFL security, which often rely on custom ad-hoc hyperparameter
choices for benign clients$\unicode{x2013}$leading to more pronounced backdoor
attack strength and diminished impact of defenses. Our results indicate that
properly tuning benign clients' hyperparameters$\unicode{x2013}$such as
learning rate, batch size, and number of local epochs$\unicode{x2013}$can
significantly curb the effectiveness of backdoor attacks, regardless of the
malicious clients' settings. We support this claim with an extensive robustness
evaluation of state-of-the-art attack-defense combinations, showing that
carefully chosen hyperparameters yield across-the-board improvements in
robustness without sacrificing main task accuracy. For example, we show that
the 50%-lifespan of the strong A3FL attack can be reduced by 98.6%,
respectively$\unicode{x2013}$all without using any defense and while incurring
only a 2.9 percentage points drop in clean task accuracy.

</details>


### [10] [On Evaluating the Poisoning Robustness of Federated Learning under Local Differential Privacy](https://arxiv.org/abs/2509.05265)
*Zijian Wang,Wei Tong,Tingxuan Han,Haoyu Chen,Tianling Zhang,Yunlong Mao,Sheng Zhong*

Main category: cs.CR

TL;DR: This paper proposes the first attack tailored for FL with LDP by formulating MPA as a constrained optimization problem, showing that it compromises global model convergence.


<details>
  <summary>Details</summary>
Motivation: The decentralized nature of FL with LDP makes it susceptible to malicious participants. The robustness of LDPFL protocols against MPAs has not been sufficiently explored.

Method: The authors formulated the model poisoning attack as an optimization problem with global training loss maximization as the objective. They then embedded additional constraints using shadow clients to evade defenses and employed a reverse training process for implementation.

Result: Experiments on three LDPFL protocols, three benchmark datasets, and two neural network types demonstrate that the adaptive attacks effectively degrade global model performance, highlighting critical vulnerabilities of existing protocols.

Conclusion: Current LDPFL protocols are vulnerable to adaptive MPAs. The paper introduces a new attack framework and emphasizes the need for more robust defense mechanisms in LDP-based federated learning.

Abstract: Federated learning (FL) combined with local differential privacy (LDP)
enables privacy-preserving model training across decentralized data sources.
However, the decentralized data-management paradigm leaves LDPFL vulnerable to
participants with malicious intent. The robustness of LDPFL protocols,
particularly against model poisoning attacks (MPA), where adversaries inject
malicious updates to disrupt global model convergence, remains insufficiently
studied. In this paper, we propose a novel and extensible model poisoning
attack framework tailored for LDPFL settings. Our approach is driven by the
objective of maximizing the global training loss while adhering to local
privacy constraints. To counter robust aggregation mechanisms such as
Multi-Krum and trimmed mean, we develop adaptive attacks that embed carefully
crafted constraints into a reverse training process, enabling evasion of these
defenses. We evaluate our framework across three representative LDPFL
protocols, three benchmark datasets, and two types of deep neural networks.
Additionally, we investigate the influence of data heterogeneity and privacy
budgets on attack effectiveness. Experimental results demonstrate that our
adaptive attacks can significantly degrade the performance of the global model,
revealing critical vulnerabilities and highlighting the need for more robust
LDPFL defense strategies against MPA. Our code is available at
https://github.com/ZiJW/LDPFL-Attack

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [11] [Comparative Evaluation of Large Language Models for Test-Skeleton Generation](https://arxiv.org/abs/2509.04644)
*Subhang Boorlagadda,Nitya Naga Sai Atluri,Muhammet Mustafa Olmez,Edward F. Gehringer*

Main category: cs.SE

TL;DR: Four LLMs were evaluated for automated RSpec skeleton generation. DeepSeek excelled in maintainability, GPT-4 in completeness but with inconsistency. Prompt design significantly impacts results.


<details>
  <summary>Details</summary>
Motivation: Manually creating test skeletons is time-consuming and error-prone in educational and large-scale development settings. Automating this process through LLMs could improve efficiency and reduce errors in test-driven development (TDD) workflows.

Method: The study evaluates four LLMs (GPT-4, DeepSeek-Chat, Llama4-Maverick, Gemma2-9B) on generating RSpec skeletons for Ruby classes using static analysis and expert review to assess structural correctness, clarity, maintainability, and testing best practices.

Result: DeepSeek-Chat produced the most maintainable and well-structured test skeletons, while GPT-4 generated more complete but conventionally inconsistent results. Prompt design and contextual input were identified as critical factors in output quality.

Conclusion: The study concludes that while LLMs can assist in automating test skeleton generation, their effectiveness depends on model choice and prompt design. DeepSeek achieved the best balance of maintainability and structure, but challenges in consistency persist.

Abstract: This paper explores the use of Large Language Models (LLMs) to automate the
generation of test skeletons -- structural templates that outline unit test
coverage without implementing full test logic. Test skeletons are especially
important in test-driven development (TDD), where they provide an early
framework for systematic verification. Traditionally authored manually, their
creation can be time-consuming and error-prone, particularly in educational or
large-scale development settings. We evaluate four LLMs -- GPT-4,
DeepSeek-Chat, Llama4-Maverick, and Gemma2-9B -- on their ability to generate
RSpec skeletons for a real-world Ruby class developed in a university software
engineering course. Each model's output is assessed using static analysis and a
blind expert review to measure structural correctness, clarity,
maintainability, and conformance to testing best practices. The study reveals
key differences in how models interpret code structure and testing conventions,
offering insights into the practical challenges of using LLMs for automated
test scaffolding. Our results show that DeepSeek generated the most
maintainable and well-structured skeletons, while GPT-4 produced more complete
but conventionally inconsistent output. The study reveals prompt design and
contextual input as key quality factors.

</details>


### [12] [Real-Time Performance Benchmarking of TinyML Models in Embedded Systems (PICO: Performance of Inference, CPU, and Operations)](https://arxiv.org/abs/2509.04721)
*Abhishek Dey,Saurabh Srivastava,Gaurav Singh,Robert G. Pettit*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper presents PICO-TINYML-BENCHMARK, a modular and platform-agnostic
framework for benchmarking the real-time performance of TinyML models on
resource-constrained embedded systems. Evaluating key metrics such as inference
latency, CPU utilization, memory efficiency, and prediction stability, the
framework provides insights into computational trade-offs and platform-specific
optimizations. We benchmark three representative TinyML models -- Gesture
Classification, Keyword Spotting, and MobileNet V2 -- on two widely adopted
platforms, BeagleBone AI64 and Raspberry Pi 4, using real-world datasets.
Results reveal critical trade-offs: the BeagleBone AI64 demonstrates consistent
inference latency for AI-specific tasks, while the Raspberry Pi 4 excels in
resource efficiency and cost-effectiveness. These findings offer actionable
guidance for optimizing TinyML deployments, bridging the gap between
theoretical advancements and practical applications in embedded systems.

</details>


### [13] [NovaQ: Improving Quantum Program Testing through Diversity-Guided Test Case Generation](https://arxiv.org/abs/2509.04763)
*Tiancheng Jin,Shangzhou Xia,Jianjun Zhao*

Main category: cs.SE

TL;DR: NovaQ is a quantum program testing framework that uses distribution-based input generation and novelty-driven evaluation to achieve higher diversity and bug detection than existing methods.


<details>
  <summary>Details</summary>
Motivation: As quantum computing advances, ensuring program reliability becomes critical to solve classically intractable problems, necessitating more effective testing approaches for quantum programs.

Method: NovaQ employs a diversity-guided framework combining (1) a distribution-based test case generator that mutates circuit parameters to generate diverse quantum state inputs, and (2) a novelty-driven evaluation module measuring behavioral novelty through magnitude, phase, and entanglement metrics in circuit states.

Result: Experimental results demonstrate NovaQ achieves higher test input diversity and detects more bugs than existing baseline approaches across varying quantum program sizes and complexities.

Conclusion: NovaQ effectively explores under-tested program behaviors by targeting infrequently covered regions in metric space, establishing a superior testing framework for emerging quantum software.

Abstract: Quantum programs are designed to run on quantum computers, leveraging quantum
circuits to solve problems that are intractable for classical machines. As
quantum computing advances, ensuring the reliability of quantum programs has
become increasingly important. This paper introduces NovaQ, a diversity-guided
testing framework for quantum programs. NovaQ combines a distribution-based
test case generator with a novelty-driven evaluation module. The generator
produces diverse quantum state inputs by mutating circuit parameters, while the
evaluator quantifies behavioral novelty based on internal circuit state
metrics, including magnitude, phase, and entanglement. By selecting inputs that
map to infrequently covered regions in the metric space, NovaQ effectively
explores under-tested program behaviors. We evaluate NovaQ on quantum programs
of varying sizes and complexities. Experimental results show that NovaQ
consistently achieves higher test input diversity and detects more bugs than
existing baseline approaches.

</details>


### [14] [Code Review Without Borders: Evaluating Synthetic vs. Real Data for Review Recommendation](https://arxiv.org/abs/2509.04810)
*Yogev Cohen,Dudi Ohayon,Romy Somkin,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.SE

TL;DR: This paper proposes using LLMs to generate synthetic training data for automatic code review in emerging languages where labeled data is limited.


<details>
  <summary>Details</summary>
Motivation: Automated code review is critical for maintaining software quality, but many new programming languages or frameworks lack enough labeled data to train effective supervised models.

Method: The authors use LLMs to translate code changes from well-resourced languages into equivalent changes in underrepresented languages, effectively creating synthetic labeled examples. They then train classifiers using these synthetic data.

Result: Experiments on multiple GitHub language pairs show that classifiers trained on LLM-generated synthetic data can achieve performance comparable to models trained on real annotated data, significantly improving review automation in low-resource settings.

Conclusion: LLM-based synthetic data generation offers a viable and scalable solution for bootstraping automated code review systems in rapidly evolving languages and frameworks.

Abstract: Automating the decision of whether a code change requires manual review is
vital for maintaining software quality in modern development workflows.
However, the emergence of new programming languages and frameworks creates a
critical bottleneck: while large volumes of unlabelled code are readily
available, there is an insufficient amount of labelled data to train supervised
models for review classification. We address this challenge by leveraging Large
Language Models (LLMs) to translate code changes from well-resourced languages
into equivalent changes in underrepresented or emerging languages, generating
synthetic training data where labelled examples are scarce. We assume that
although LLMs have learned the syntax and semantics of new languages from
available unlabelled code, they have yet to fully grasp which code changes are
considered significant or review-worthy within these emerging ecosystems. To
overcome this, we use LLMs to generate synthetic change examples and train
supervised classifiers on them. We systematically compare the performance of
these classifiers against models trained on real labelled data. Our experiments
across multiple GitHub repositories and language pairs demonstrate that
LLM-generated synthetic data can effectively bootstrap review recommendation
systems, narrowing the performance gap even in low-resource settings. This
approach provides a scalable pathway to extend automated code review
capabilities to rapidly evolving technology stacks, even in the absence of
annotated data.

</details>


### [15] [Integrating Large Language Models in Software Engineering Education: A Pilot Study through GitHub Repositories Mining](https://arxiv.org/abs/2509.04877)
*Maryam Khan,Muhammad Azeem Akbar,Jussi Kasurinen*

Main category: cs.SE

TL;DR: This paper presents empirical validation of LLM integration motivators/demotivators in SE education, using GitHub analysis to inform framework development.


<details>
  <summary>Details</summary>
Motivation: Aims to develop a validated framework for responsible LLM integration in SE education by systematically investigating adoption factors through empirical analysis.

Method: Conducted a pilot repository mining study across 400 GitHub projects, analyzing README files and issue discussions to quantify motivator/demotivator themes from prior literature.

Result: Results show motivators like engagement (227 hits), process understanding (133 hits), and programming help (97 hits) were common. Demotivators included plagiarism/IP concerns (385 hits), security/privacy (87 hits), and over-reliance (39 hits). Certain demotivators like evaluation challenges had zero hits.

Conclusion: The study validates motivator/demotivator taxonomies, reveals research-practice gaps, and establishes a foundation for a framework to guide responsible LLM adoption in SE education.

Abstract: Context: Large Language Models (LLMs) such as ChatGPT are increasingly
adopted in software engineering (SE) education, offering both opportunities and
challenges. Their adoption requires systematic investigation to ensure
responsible integration into curricula. Objective: This doctoral research aims
to develop a validated framework for integrating LLMs into SE education through
a multi-phase process, including taxonomies development, empirical
investigation, and case studies. This paper presents the first empirical step.
Method: We conducted a pilot repository mining study of 400 GitHub projects,
analyzing README files and issues discussions to identify the presence of
motivator and demotivator previously synthesized in our literature review [ 8]
study. Results: Motivators such as engagement and motivation (227 hits),
software engineering process understanding (133 hits), and programming
assistance and debugging support (97 hits) were strongly represented.
Demotivators, including plagiarism and IP concerns (385 hits), security,
privacy and data integrity (87 hits), and over-reliance on AI in learning (39
hits), also appeared prominently. In contrast, demotivators such as challenges
in evaluating learning outcomes and difficulty in curriculum redesign recorded
no hits across the repositories. Conclusion: The study provides early empirical
validation of motivators/demotivators taxonomies with respect to their themes,
highlights research practice gaps, and lays the foundation for developing a
comprehensive framework to guide the responsible adoption of LLMs in SE
education.

</details>


### [16] [FuzzRDUCC: Fuzzing with Reconstructed Def-Use Chain Coverage](https://arxiv.org/abs/2509.04967)
*Kai Feng,Jeremy Singer,Angelos K Marnerides*

Main category: cs.SE

TL;DR: FuzzRDUCC is a novel binary fuzzing framework integrating symbolic execution for def-use chain analysis. It improves vulnerability detection by identifying critical dataflow paths with a heuristic algorithm, achieving better coverage and discovering unique crashes compared to existing fuzzers.


<details>
  <summary>Details</summary>
Motivation: Traditional grey-box fuzzers rely on control flow edge coverage, which may miss bugs not revealed through control flow analysis alone. Limited insight into program dataflows hinders thorough binary-only fuzzing.

Method: The framework reconstructs definition-use (def-use) chains directly from binary executables via symbolic execution and employs a heuristic algorithm to select relevant dataflow paths, reducing computational overhead while maintaining fuzzing thoroughness.

Result: Evaluation on binutils benchmark shows FuzzRDUCC identifies unique crashes not found by state-of-the-art fuzzers, demonstrating superior vulnerability discovery capabilities in binary analysis scenarios.

Conclusion: FuzzRDUCC establishes a feasible next-generation solution for vulnerability detection by effectively integrating dataflow analysis into binary fuzzing without excessive computational costs.

Abstract: Binary-only fuzzing often struggles with achieving thorough code coverage and
uncovering hidden vulnerabilities due to limited insight into a program's
internal dataflows. Traditional grey-box fuzzers guide test case generation
primarily using control flow edge coverage, which can overlook bugs not easily
exposed through control flow analysis alone. We argue that integrating dataflow
analysis into the fuzzing process can enhance its effectiveness by revealing
how data propagates through the program, thereby enabling the exploration of
execution paths that control flow-based methods might miss. In this context, we
introduce FuzzRDUCC, a novel fuzzing framework that employs symbolic execution
to reconstruct definition-use (def-use) chains directly from binary
executables. FuzzRDUCC identifies crucial dataflow paths and exposes security
vulnerabilities without incurring excessive computational overhead, due to a
novel heuristic algorithm that selects relevant def-use chains without
affecting the thoroughness of the fuzzing process. We evaluate FuzzRDUCC using
the binutils benchmark and demonstrate that it can identify unique crashes not
found by state-of-the-art fuzzers. Hence, establishing FuzzRDUCC as a feasible
solution for next generation vulnerability detection and discovery mechanisms.

</details>


### [17] [GenAI-based test case generation and execution in SDV platform](https://arxiv.org/abs/2509.05112)
*Denesa Zyberaj,Lukasz Mazur,Nenad Petrovic,Pankhuri Verma,Pascal Hirmer,Dirk Slama,Xiangwei Cheng,Alois Knoll*

Main category: cs.SE

TL;DR: This paper presents a GenAI-based framework for automating Gherkin test case generation in automotive systems, combining LLMs, Vision-Language Models, and digital.auto for validation while highlighting remaining manual dependencies.


<details>
  <summary>Details</summary>
Motivation: Address manual test specification effort and integration challenges in software-defined vehicles by leveraging GenAI for automation and standardization.

Method: Integration of Large Language Models and Vision-Language Models to convert requirements/diagrams into Gherkin test cases, combined with Vehicle Signal Specification modeling for standardization and compatibility.

Result: Demonstration of reduced manual effort and rapid test execution via Child Presence Detection System use case, with generated tests validated in digital.auto playground.

Conclusion: The proposed GenAI-driven approach significantly automates test case generation for automotive systems but still requires manual intervention due to current pipeline and platform limitations.

Abstract: This paper introduces a GenAI-driven approach for automated test case
generation, leveraging Large Language Models and Vision-Language Models to
translate natural language requirements and system diagrams into structured
Gherkin test cases. The methodology integrates Vehicle Signal Specification
modeling to standardize vehicle signal definitions, improve compatibility
across automotive subsystems, and streamline integration with third-party
testing tools. Generated test cases are executed within the digital.auto
playground, an open and vendor-neutral environment designed to facilitate rapid
validation of software-defined vehicle functionalities. We evaluate our
approach using the Child Presence Detection System use case, demonstrating
substantial reductions in manual test specification effort and rapid execution
of generated tests. Despite significant automation, the generation of test
cases and test scripts still requires manual intervention due to current
limitations in the GenAI pipeline and constraints of the digital.auto platform.

</details>


### [18] [AI Agents for Web Testing: A Case Study in the Wild](https://arxiv.org/abs/2509.05197)
*Naimeng Ye,Xiao Yu,Ruize Xu,Tianyi Peng,Zhou Yu*

Main category: cs.SE

TL;DR: WebProber: an AI agent-based framework that identifies complex usability issues in web testing by simulating human-like interactions, outperforming traditional tools in a case study of 120 academic websites.


<details>
  <summary>Details</summary>
Motivation: Traditional web testing focuses on code coverage and load testing, missing nuanced user behavior and common usability problems. LLMs and AI agents enable more human-like interaction, addressing this gap.

Method: The authors developed WebProber, an AI agent-based framework that autonomously explores websites, simulates real user interactions, detects bugs/usability issues, and generates human-readable reports. They evaluated it on 120 academic personal websites.

Result: WebProber uncovered 29 usability issues (e.g., accessibility barriers, navigation problems) on 120 academic websites, many missed by existing tools. The results demonstrate the effectiveness of agent-based testing in identifying user-centric issues.

Conclusion: The paper concludes that agent-based testing, exemplified by WebProber, offers a promising approach to identifying usability issues overlooked by traditional tools. It advocates for further development of user-centered testing frameworks to improve web quality.

Abstract: Automated web testing plays a critical role in ensuring high-quality user
experiences and delivering business value. Traditional approaches primarily
focus on code coverage and load testing, but often fall short of capturing
complex user behaviors, leaving many usability issues undetected. The emergence
of large language models (LLM) and AI agents opens new possibilities for web
testing by enabling human-like interaction with websites and a general
awareness of common usability problems. In this work, we present WebProber, a
prototype AI agent-based web testing framework. Given a URL, WebProber
autonomously explores the website, simulating real user interactions,
identifying bugs and usability issues, and producing a human-readable report.
We evaluate WebProber through a case study of 120 academic personal websites,
where it uncovered 29 usability issues--many of which were missed by
traditional tools. Our findings highlight agent-based testing as a promising
direction while outlining directions for developing next-generation,
user-centered testing frameworks.

</details>
