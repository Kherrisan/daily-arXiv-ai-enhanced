<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 20]
- [cs.SE](#cs.SE) [Total: 12]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [AI Propaganda factories with language models](https://arxiv.org/abs/2508.20186)
*Lukasz Olejnik*

Main category: cs.CR

TL;DR: AI-powered influence operations can be executed with small language models on regular hardware, revealing two behavioral patterns: persona design impacts output more than model identity, and counter-argument requirements increase ideological adherence and extreme content. Defenses should shift toward conversation-centric detection.


<details>
  <summary>Details</summary>
Motivation: Current AI influence operations on commodity hardware can produce compelling political messaging with small models, but previous defenses focused on model access rather than campaign mechanisms. This shifts defense strategy to address automated influence content at scale.

Method: The paper evaluates small language models' ability to generate persona-driven political messaging using automated metrics, testing how different personas affect output and analyzing behavioral shifts when models must produce counter-arguments.

Result: 1. Persona design explains model behavior better than the base model's architecture. 2. Engagement requirements (counter-argument generation) lead to stronger ideological alignment and increased extreme content production. 3. Both state and non-state actors can achieve automated influence operations.

Conclusion: Defenses must evolve to detect and disrupt conversation-level patterns in automated campaigns. The consistency of AI-generated content creates detectable signatures that can counteract its influence capabilities.

Abstract: AI-powered influence operations can now be executed end-to-end on commodity
hardware. We show that small language models produce coherent, persona-driven
political messaging and can be evaluated automatically without human raters.
Two behavioural findings emerge. First, persona-over-model: persona design
explains behaviour more than model identity. Second, engagement as a stressor:
when replies must counter-arguments, ideological adherence strengthens and the
prevalence of extreme content increases. We demonstrate that fully automated
influence-content production is within reach of both large and small actors.
Consequently, defence should shift from restricting model access towards
conversation-centric detection and disruption of campaigns and coordination
infrastructure. Paradoxically, the very consistency that enables these
operations also provides a detection signature.

</details>


### [2] [FlowMalTrans: Unsupervised Binary Code Translation for Malware Detection Using Flow-Adapter Architecture](https://arxiv.org/abs/2508.20212)
*Minghao Hu,Junzhe Wang,Weisen Zhao,Qiang Zeng,Lannan Luo*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Applying deep learning to malware detection has drawn great attention due to
its notable performance. With the increasing prevalence of cyberattacks
targeting IoT devices, there is a parallel rise in the development of malware
across various Instruction Set Architectures (ISAs). It is thus important to
extend malware detection capacity to multiple ISAs. However, training a deep
learning-based malware detection model usually requires a large number of
labeled malware samples. The process of collecting and labeling sufficient
malware samples to build datasets for each ISA is labor-intensive and
time-consuming. To reduce the burden of data collection, we propose to leverage
the ideas of Neural Machine Translation (NMT) and Normalizing Flows (NFs) for
malware detection. Specifically, when dealing with malware in a certain ISA, we
translate it to an ISA with sufficient malware samples (like X86-64). This
allows us to apply a model trained on one ISA to analyze malware from another
ISA. Our approach reduces the data collection effort by enabling malware
detection across multiple ISAs using a model trained on a single ISA.

</details>


### [3] [Robustness Assessment and Enhancement of Text Watermarking for Google's SynthID](https://arxiv.org/abs/2508.20228)
*Xia Han,Qi Li,Jianbing Ni,Mohammad Zulkernine*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advances in LLM watermarking methods such as SynthID-Text by Google
DeepMind offer promising solutions for tracing the provenance of AI-generated
text. However, our robustness assessment reveals that SynthID-Text is
vulnerable to meaning-preserving attacks, such as paraphrasing, copy-paste
modifications, and back-translation, which can significantly degrade watermark
detectability. To address these limitations, we propose SynGuard, a hybrid
framework that combines the semantic alignment strength of Semantic Information
Retrieval (SIR) with the probabilistic watermarking mechanism of SynthID-Text.
Our approach jointly embeds watermarks at both lexical and semantic levels,
enabling robust provenance tracking while preserving the original meaning.
Experimental results across multiple attack scenarios show that SynGuard
improves watermark recovery by an average of 11.1\% in F1 score compared to
SynthID-Text. These findings demonstrate the effectiveness of semantic-aware
watermarking in resisting real-world tampering. All code, datasets, and
evaluation scripts are publicly available at:
https://github.com/githshine/SynGuard.

</details>


### [4] [Network-Level Prompt and Trait Leakage in Local Research Agents](https://arxiv.org/abs/2508.20282)
*Hyejun Jeong,Mohammadreze Teymoorianfard,Abhinav Kumar,Amir Houmansadr,Eugene Badasarian*

Main category: cs.CR

TL;DR: This paper reveals that Web and Research Agents (WRAs), despite being deployed locally for privacy, are vulnerable to passive adversary inference attacks through network metadata. By creating a dataset of WRA traces and a behavioral metric (OBELS), the authors demonstrate 73% prompt knowledge recovery and 19/32 user trait inferences. They also propose mitigations that reduce attack effectiveness by 29% with negligible utility loss.


<details>
  <summary>Details</summary>
Motivation: WRAs, designed for local use to protect privacy and comply with legal/financial constraints, may still inadvertently expose sensitive information via network patterns distinguishable from human web browsing. This exposure risks privacy, especially as these agents scale in complexity and adoption.

Method: 1. Constructed a dataset of WRA traces from user queries and synthetic personas. 2. Developed the OBELS metric to assess prompt similarity. 3. Designed an inference attack requiring minimal network-level metadata (IP address timings). 4. Evaluated performance in multi-session scenarios and noisy conditions. 5. Tested mitigation strategies focused on modifying access patterns while preserving utility.

Result: 1. 73% functional/domain knowledge recovery from single-session traces. 2. 19 of 32 latent user traits inferred with high accuracy under multi-session settings. 3. Attack persistence in partial observability and noisy environments. 4. Mitigations (domain diversity reduction, trace obfuscation) reduced attack efficacy by 29% without significant utility degradation.

Conclusion: Local deployment does not inherently secure WRAs against inference attacks. Unique network traffic patterns enable trait and prompt recovery even when agents aren't accessed externally. Mitigation approaches can significantly reduce risks at low utility cost, guiding future WRA design to address these operational privacy limitations.

Abstract: We show that Web and Research Agents (WRAs) -- language model-based systems
that investigate complex topics on the Internet -- are vulnerable to inference
attacks by passive network adversaries such as ISPs. These agents could be
deployed \emph{locally} by organizations and individuals for privacy, legal, or
financial purposes. Unlike sporadic web browsing by humans, WRAs visit
$70{-}140$ domains with distinguishable timing correlations, enabling unique
fingerprinting attacks.
  Specifically, we demonstrate a novel prompt and user trait leakage attack
against WRAs that only leverages their network-level metadata (i.e., visited IP
addresses and their timings). We start by building a new dataset of WRA traces
based on user search queries and queries generated by synthetic personas. We
define a behavioral metric (called OBELS) to comprehensively assess similarity
between original and inferred prompts, showing that our attack recovers over
73\% of the functional and domain knowledge of user prompts. Extending to a
multi-session setting, we recover up to 19 of 32 latent traits with high
accuracy. Our attack remains effective under partial observability and noisy
conditions. Finally, we discuss mitigation strategies that constrain domain
diversity or obfuscate traces, showing negligible utility impact while reducing
attack effectiveness by an average of 29\%.

</details>


### [5] [Surveying the Operational Cybersecurity and Supply Chain Threat Landscape when Developing and Deploying AI Systems](https://arxiv.org/abs/2508.20307)
*Michael R Smith,Joe Ingram*

Main category: cs.CR

TL;DR: The paper discusses new cybersecurity threats introduced by AI and emphasizes the need for specialized security frameworks to address them.


<details>
  <summary>Details</summary>
Motivation: The integration of AI into software and hardware systems has changed the cybersecurity landscape, creating new attack surfaces that traditional assessments do not cover. Cyber attackers now target AI outputs to affect system performance and reliability, rather than sticking to conventional objectives.

Method: The paper analyzes operational cybersecurity and supply chain risks across the entire AI lifecycle, discusses past exploitations, and offers insights from hands-on work in the field.

Result: The analysis reveals how attackers can manipulate AI systems to degrade performance, increase false positives, and reduce model accuracy, while identifying the shortcomings of existing cybersecurity practices in relation to AI risks.

Conclusion: Organizations must develop tailored security frameworks to counter AI-specific threats and enhance the resilience and trustworthiness of AI-driven systems.

Abstract: The rise of AI has transformed the software and hardware landscape, enabling
powerful capabilities through specialized infrastructures, large-scale data
storage, and advanced hardware. However, these innovations introduce unique
attack surfaces and objectives which traditional cybersecurity assessments
often overlook. Cyber attackers are shifting their objectives from conventional
goals like privilege escalation and network pivoting to manipulating AI outputs
to achieve desired system effects, such as slowing system performance, flooding
outputs with false positives, or degrading model accuracy. This paper serves to
raise awareness of the novel cyber threats that are introduced when
incorporating AI into a software system. We explore the operational
cybersecurity and supply chain risks across the AI lifecycle, emphasizing the
need for tailored security frameworks to address evolving threats in the
AI-driven landscape. We highlight previous exploitations and provide insights
from working in this area. By understanding these risks, organizations can
better protect AI systems and ensure their reliability and resilience.

</details>


### [6] [MindGuard: Tracking, Detecting, and Attributing MCP Tool Poisoning Attack via Decision Dependence Graph](https://arxiv.org/abs/2508.20412)
*Zhiqiang Wang,Junyang Zhang,Guanquan Shi,HaoRan Cheng,Yunhao Yao,Kaiwen Guo,Haohua Du,Xiang-Yang Li*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Model Context Protocol (MCP) is increasingly adopted to standardize the
interaction between LLM agents and external tools. However, this trend
introduces a new threat: Tool Poisoning Attacks (TPA), where tool metadata is
poisoned to induce the agent to perform unauthorized operations. Existing
defenses that primarily focus on behavior-level analysis are fundamentally
ineffective against TPA, as poisoned tools need not be executed, leaving no
behavioral trace to monitor.
  Thus, we propose MindGuard, a decision-level guardrail for LLM agents,
providing provenance tracking of call decisions, policy-agnostic detection, and
poisoning source attribution against TPA. While fully explaining LLM decision
remains challenging, our empirical findings uncover a strong correlation
between LLM attention mechanisms and tool invocation decisions. Therefore, we
choose attention as an empirical signal for decision tracking and formalize
this as the Decision Dependence Graph (DDG), which models the LLM's reasoning
process as a weighted, directed graph where vertices represent logical concepts
and edges quantify the attention-based dependencies. We further design robust
DDG construction and graph-based anomaly analysis mechanisms that efficiently
detect and attribute TPA attacks. Extensive experiments on real-world datasets
demonstrate that MindGuard achieves 94\%-99\% average precision in detecting
poisoned invocations, 95\%-100\% attribution accuracy, with processing times
under one second and no additional token cost. Moreover, DDG can be viewed as
an adaptation of the classical Program Dependence Graph (PDG), providing a
solid foundation for applying traditional security policies at the decision
level.

</details>


### [7] [Federated Learning for Large Models in Medical Imaging: A Comprehensive Review](https://arxiv.org/abs/2508.20414)
*Mengyu Sun,Ziyuan Yang,Yongqiang Huang,Hui Yu,Yingyu Chen,Shuren Qi,Andrew Beng Jin Teoh,Yi Zhang*

Main category: cs.CR

TL;DR: This survey explores federated learning (FL) in medical imaging, focusing on its roles in CT/MRI reconstruction (upstream) and tumor diagnosis/segmentation (downstream).


<details>
  <summary>Details</summary>
Motivation: High-performance AI in medical imaging requires large-scale centralized data, which is challenging due to patient privacy laws and data-sharing restrictions.

Method: The study reviews FL implementations in upstream and downstream tasks, examining how FL enables collaborative training and local updates on decentralized datasets.

Result: FL helps alleviate data scarcity and maintain confidentiality by allowing multi-institutional training and local fine-tuning without centralized data storage.

Conclusion: The paper discusses innovations in FL for communication efficiency, data alignment, and secure aggregation, and highlights future research directions for the medical imaging field.

Abstract: Artificial intelligence (AI) has demonstrated considerable potential in the
realm of medical imaging. However, the development of high-performance AI
models typically necessitates training on large-scale, centralized datasets.
This approach is confronted with significant challenges due to strict patient
privacy regulations and legal restrictions on data sharing and utilization.
These limitations hinder the development of large-scale models in medical
domains and impede continuous updates and training with new data. Federated
Learning (FL), a privacy-preserving distributed training framework, offers a
new solution by enabling collaborative model development across fragmented
medical datasets. In this survey, we review FL's contributions at two stages of
the full-stack medical analysis pipeline. First, in upstream tasks such as CT
or MRI reconstruction, FL enables joint training of robust reconstruction
networks on diverse, multi-institutional datasets, alleviating data scarcity
while preserving confidentiality. Second, in downstream clinical tasks like
tumor diagnosis and segmentation, FL supports continuous model updating by
allowing local fine-tuning on new data without centralizing sensitive images.
We comprehensively analyze FL implementations across the medical imaging
pipeline, from physics-informed reconstruction networks to diagnostic AI
systems, highlighting innovations that improve communication efficiency, align
heterogeneous data, and ensure secure parameter aggregation. Meanwhile, this
paper provides an outlook on future research directions, aiming to serve as a
valuable reference for the field's development.

</details>


### [8] [Breaking Diffusion with Cache: Exploiting Approximate Caches in Diffusion Models](https://arxiv.org/abs/2508.20424)
*Desen Sun,Shuncheng Jie,Sihang Liu*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Diffusion models are a powerful class of generative models that produce
content, such as images, from user prompts, but they are computationally
intensive. To mitigate this cost, recent academic and industry work has adopted
approximate caching, which reuses intermediate states from similar prompts in a
cache. While efficient, this optimization introduces new security risks by
breaking isolation among users. This work aims to comprehensively assess new
security vulnerabilities arising from approximate caching. First, we
demonstrate a remote covert channel established with the cache, where a sender
injects prompts with special keywords into the cache and a receiver can recover
that even after days, to exchange information. Second, we introduce a prompt
stealing attack using the cache, where an attacker can recover existing cached
prompts based on cache hit prompts. Finally, we introduce a poisoning attack
that embeds the attacker's logos into the previously stolen prompt, to render
them in future user prompts that hit the cache. These attacks are all performed
remotely through the serving system, which indicates severe security
vulnerabilities in approximate caching.

</details>


### [9] [Ransomware 3.0: Self-Composing and LLM-Orchestrated](https://arxiv.org/abs/2508.20444)
*Md Raz,Meet Udeshi,P. V. Sai Charan,Prashanth Krishnamurthy,Farshad Khorrami,Ramesh Karri*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Using automated reasoning, code synthesis, and contextual decision-making, we
introduce a new threat that exploits large language models (LLMs) to
autonomously plan, adapt, and execute the ransomware attack lifecycle.
Ransomware 3.0 represents the first threat model and research prototype of
LLM-orchestrated ransomware. Unlike conventional malware, the prototype only
requires natural language prompts embedded in the binary; malicious code is
synthesized dynamically by the LLM at runtime, yielding polymorphic variants
that adapt to the execution environment. The system performs reconnaissance,
payload generation, and personalized extortion, in a closed-loop attack
campaign without human involvement. We evaluate this threat across personal,
enterprise, and embedded environments using a phase-centric methodology that
measures quantitative fidelity and qualitative coherence in each attack phase.
We show that open source LLMs can generate functional ransomware components and
sustain closed-loop execution across diverse environments. Finally, we present
behavioral signals and multi-level telemetry of Ransomware 3.0 through a case
study to motivate future development of better defenses and policy enforcements
to address novel AI-enabled ransomware attacks.

</details>


### [10] [Enhancing Resilience for IoE: A Perspective of Networking-Level Safeguard](https://arxiv.org/abs/2508.20504)
*Guan-Yan Yang,Jui-Ning Chen,Farn Wang,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: The paper introduces a Graph Structure Learning (GSL)-based safeguards framework for securing the Internet of Energy (IoE) against adversarial attacks, showcasing its robustness and reliability over traditional methods.


<details>
  <summary>Details</summary>
Motivation: The Internet of Energy (IoE) faces unique cyber threats compared to general IoT systems, with the potential to impact public safety. There is a need for resilient solutions that safeguard critical infrastructure from these evolving threats.

Method: The authors propose a framework based on Graph Structure Learning (GSL) that jointly optimizes graph topology and node representations to resist adversarial network model manipulation.

Result: The paper demonstrates the superior robustness of the GSL approach through a conceptual overview, architectural discussion, and case study using a security dataset.

Conclusion: The GSL-based safeguards framework shows promise in enhancing the resilience and reliability of IoE networks, with the paper highlighting potential applications for practitioners managing critical energy infrastructure and identifying future research directions.

Abstract: The Internet of Energy (IoE) integrates IoT-driven digital communication with
power grids to enable efficient and sustainable energy systems. Still, its
interconnectivity exposes critical infrastructure to sophisticated cyber
threats, including adversarial attacks designed to bypass traditional
safeguards. Unlike general IoT risks, IoE threats have heightened public safety
consequences, demanding resilient solutions. From the networking-level
safeguard perspective, we propose a Graph Structure Learning (GSL)-based
safeguards framework that jointly optimizes graph topology and node
representations to resist adversarial network model manipulation inherently.
Through a conceptual overview, architectural discussion, and case study on a
security dataset, we demonstrate GSL's superior robustness over representative
methods, offering practitioners a viable path to secure IoE networks against
evolving attacks. This work highlights the potential of GSL to enhance the
resilience and reliability of future IoE networks for practitioners managing
critical infrastructure. Lastly, we identify key open challenges and propose
future research directions in this novel research area.

</details>


### [11] [Characterizing Trust Boundary Vulnerabilities in TEE Containers](https://arxiv.org/abs/2508.20962)
*Weijie Liu,Hongbo Chen,Shuo Huai,Zhen Xu,Wenhao Wang,Zhi Li,Zheli Liu*

Main category: cs.CR

TL;DR: The paper analyzes TEE containers' isolation strategies and safety flaws and shares lessons for secure container documentation.


<details>
  <summary>Details</summary>
Motivation: Trusted Execution Environments (TEEs) are vital for confidential computing, but the security of applications on TEE platforms depends on the effectiveness of TEE containers as middleware solutions. There is a need to ensure that these containers properly isolate applications from potentially malicious operating systems and orchestration interfaces.

Method: The study involves an analysis of isolation strategies used by current TEE containers and the design of an automated analyzer to precisely identify and evaluate their isolation boundaries for security risks.

Result: The study found that some TEE containers have critical design and implementation flaws, which lead to information leakage, rollback attacks, denial-of-service, and Iago attacks, each posing major security risks.

Conclusion: The paper concludes by sharing key lessons to improve the development of TL container solutions and discussing emerging trends in TEE containerization design.

Abstract: Trusted Execution Environments (TEEs) have emerged as a cornerstone of
confidential computing, garnering significant attention from both academia and
industry. To enable the secure development, execution, and deployment, of
applications on TEE platforms, TEE containers have been introduced as
middleware solutions. These containers aim to shield applications from
potentially malicious operating systems and orchestration interfaces while
maintaining usability and reliability. In this paper, we analyze the isolation
strategies employed by existing TEE containers to protect secure applications.
To address the challenges in analyzing these interfaces, we designed an
automated analyzer to precisely identify and evaluate their isolation
boundaries. We observed that some TEE containers fail to achieve their intended
goals due to critical design and implementation flaws, such as information
leakage, rollback attacks, denial-of-service, and Iago attacks, which pose
significant security risks. Drawing from our findings, we share key lessons to
guide the development of more secure container solutions and discuss emerging
trends in TEE containerization design.

</details>


### [12] [BridgeShield: Enhancing Security for Cross-chain Bridge Applications via Heterogeneous Graph Mining](https://arxiv.org/abs/2508.20517)
*Dan Lin,Shunfeng Lu,Ziyan Liu,Jiajing Wu,Junyuan Fang,Kaixin Lin,Bowen Song,Zibin Zheng*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Cross-chain bridges play a vital role in enabling blockchain
interoperability. However, due to the inherent design flaws and the enormous
value they hold, they have become prime targets for hacker attacks. Existing
detection methods show progress yet remain limited, as they mainly address
single-chain behaviors and fail to capture cross-chain semantics. To address
this gap, we leverage heterogeneous graph attention networks, which are
well-suited for modeling multi-typed entities and relations, to capture the
complex execution semantics of cross-chain behaviors. We propose BridgeShield,
a detection framework that jointly models the source chain, off-chain
coordination, and destination chain within a unified heterogeneous graph
representation. BridgeShield incorporates intra-meta-path attention to learn
fine-grained dependencies within cross-chain paths and inter-meta-path
attention to highlight discriminative cross-chain patterns, thereby enabling
precise identification of attack behaviors. Extensive experiments on 51
real-world cross-chain attack events demonstrate that BridgeShield achieves an
average F1-score of 92.58%, representing a 24.39% improvement over
state-of-the-art baselines. These results validate the effectiveness of
BridgeShield as a practical solution for securing cross-chain bridges and
enhancing the resilience of multi-chain ecosystems.

</details>


### [13] [Bitcoin as an Interplanetary Monetary Standard with Proof-of-Transit Timestamping](https://arxiv.org/abs/2508.20591)
*Jose E. Puente,Carlos Puente*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We explore the feasibility of deploying Bitcoin as the shared monetary
standard between Earth and Mars, accounting for physical constraints of
interplanetary communication. We introduce a novel primitive, Proof-of-Transit
Timestamping (PoTT), to provide cryptographic, tamper-evident audit trails for
Bitcoin data across high-latency, intermittently-connected links. Leveraging
Delay/Disruption-Tolerant Networking (DTN) and optical low-Earth-orbit (LEO)
mesh constellations, we propose an architecture for header-first replication,
long-horizon Lightning channels with planetary watchtowers, and secure
settlement through federated sidechains or blind-merge-mined (BMM) commit
chains. We formalize PoTT, analyze its security model, and show how it
measurably improves reliability and accountability without altering Bitcoin
consensus or its monetary base. Near-term deployments favor strong federations
for local settlement; longer-term, blind-merge-mined commit chains (if adopted)
provide an alternative. The Earth L1 monetary base remains unchanged, while
Mars can operate a pegged commit chain or strong federation with 1:1 pegged
assets for local block production. For transparency, if both time-beacon
regimes are simultaneously compromised, PoTT-M2 (and PoTT generally) reduces to
administrative assertions rather than cryptographic time-anchoring.

</details>


### [14] [CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics](https://arxiv.org/abs/2508.20643)
*Stefano Fumero,Kai Huang,Matteo Boffa,Danilo Giordano,Marco Mellia,Zied Ben Houidi,Dario Rossi*

Main category: cs.CR

TL;DR: The paper introduces an autonomous cyber forensic agent and benchmark


<details>
  <summary>Details</summary>
Motivation: Current LLM agents in cybersecurity focus on red-team applications, with limited research on defensive uses like forensics. There's a need for systematic evaluation and design guidance for LLM agents in this area.

Method: The authors propose CyberSleuth, an autonomous agent using packet-level traces and application logs to identify targeted services, exploited vulnerabilities (CVEs), and attack success in web application attacks. They evaluate different agent architectures and LLM backends through a benchmark.

Result: CyberSleuth outperforms other agent designs in their benchmark. It correctly identifies the exact CVE in 80% of 2025 incidents. Human experts rate the agent's reports as complete, useful, and coherent, with a slight preference for the open-source DeepSeek R1 model.

Conclusion: The paper establishes a foundation for defensive LLM research by introducing CyberSleuth and a benchmark. The human study validates the quality of the agent's output, highlighting the potential of open-source models for cyber forensics.

Abstract: Large Language Model (LLM) agents are powerful tools for automating complex
tasks. In cybersecurity, researchers have primarily explored their use in
red-team operations such as vulnerability discovery and penetration tests.
Defensive uses for incident response and forensics have received comparatively
less attention and remain at an early stage. This work presents a systematic
study of LLM-agent design for the forensic investigation of realistic web
application attacks. We propose CyberSleuth, an autonomous agent that processes
packet-level traces and application logs to identify the targeted service, the
exploited vulnerability (CVE), and attack success. We evaluate the consequences
of core design decisions - spanning tool integration and agent architecture -
and provide interpretable guidance for practitioners. We benchmark four agent
architectures and six LLM backends on 20 incident scenarios of increasing
complexity, identifying CyberSleuth as the best-performing design. In a
separate set of 10 incidents from 2025, CyberSleuth correctly identifies the
exact CVE in 80% of cases. At last, we conduct a human study with 22 experts,
which rated the reports of CyberSleuth as complete, useful, and coherent. They
also expressed a slight preference for DeepSeek R1, a good news for open source
LLM. To foster progress in defensive LLM research, we release both our
benchmark and the CyberSleuth platform as a foundation for fair, reproducible
evaluation of forensic agents.

</details>


### [15] [Multi-Agent Penetration Testing AI for the Web](https://arxiv.org/abs/2508.20816)
*Isaac David,Arthur Gervais*

Main category: cs.CR

TL;DR: The paper introduces MAPTA, a multi-agent system for     autonomous web application security assessments that can     identify critical vulnerabilities in popular repositories using     LLMs, tools, and exploit validation. Despite high success rates,     some challenges remain.


<details>
  <summary>Details</summary>
Motivation:      The democratization of software development by AI platforms has led to a     surge in application development which outpaces security auditing     capabilities. Research shows that a significant percentage     (up to 40%) of AI-generated code contains vulnerabilities,     necessitating the development of a more efficient and effective     approach to ensure security without hindering development speed.

Method:      MAPTA is a multi-agent system for autonomous evaluation of web     application security. It integrates large language model     orchestration with tool-grounded execution and end-to-end exploit validation.

Result:      On the XBOW 104-challenge benchmark, MAPTA achieved 76.9% success     rate overall, with perfect performance on SSRF and misconfiguration     vulnerabilities. It had an 83% success rate on broken authorization,     85% on server-side template injection, and 83% on SQL     injection. Cross-site scripting success was 57%, with 0%     on blind SQL injection. Real-world assessments of     popular GitHub repositories (8K-70K stars) led to discovery     of critical vulnerabilities, with 10 responsibly disclosed     for CVE review. A cost analysis revealed that the     median cost for successful attempts was $0.073 compared to     $0.357 for failures.

Conclusion:      The paper's findings highlight the effectiveness of     MAPTA in identifying critical vulnerabilities in web applications.     While there is room for improvement with some     challenges, the correlation between success and low resource     usage suggests that early-stopping thresholds (around 40     tool calls or $0.30 per challenge) can be practical.     The impact of MAPTA in real-world is measured by its     discoveries in popular repositories and its low     assessment cost at $3.67 per open-source scan, although     its cost efficiency on blind SQL injection remains a challenge.

Abstract: AI-powered development platforms are making software creation accessible to a
broader audience, but this democratization has triggered a scalability crisis
in security auditing. With studies showing that up to 40% of AI-generated code
contains vulnerabilities, the pace of development now vastly outstrips the
capacity for thorough security assessment.
  We present MAPTA, a multi-agent system for autonomous web application
security assessment that combines large language model orchestration with
tool-grounded execution and end-to-end exploit validation. On the 104-challenge
XBOW benchmark, MAPTA achieves 76.9% overall success with perfect performance
on SSRF and misconfiguration vulnerabilities, 83% success on broken
authorization, and strong results on injection attacks including server-side
template injection (85%) and SQL injection (83%). Cross-site scripting (57%)
and blind SQL injection (0%) remain challenging. Our comprehensive cost
analysis across all challenges totals $21.38 with a median cost of $0.073 for
successful attempts versus $0.357 for failures. Success correlates strongly
with resource efficiency, enabling practical early-stopping thresholds at
approximately 40 tool calls or $0.30 per challenge.
  MAPTA's real-world findings are impactful given both the popularity of the
respective scanned GitHub repositories (8K-70K stars) and MAPTA's low average
operating cost of $3.67 per open-source assessment: MAPTA discovered critical
vulnerabilities including RCEs, command injections, secret exposure, and
arbitrary file write vulnerabilities. Findings are responsibly disclosed, 10
findings are under CVE review.

</details>


### [16] [JADES: A Universal Framework for Jailbreak Assessment via Decompositional Scoring](https://arxiv.org/abs/2508.20848)
*Junjie Chu,Mingjie Li,Ziqing Yang,Ye Leng,Chenhao Lin,Chao Shen,Michael Backes,Yun Shen,Yang Zhang*

Main category: cs.CR

TL;DR: This paper presents JADES, a new jailbreak evaluation framework that automatically decomposes harmful questions and scores sub-answers to assess success, outperforming existing methods with 98.5% agreement with human annotations and exposing overestimated success rates in prior attacks.


<details>
  <summary>Details</summary>
Motivation: Accurate evaluation of jailbreak success is crucial for understanding and improving the alignment of large language models, yet existing methods are inconsistent and subjective as they rely on proxy indicators or holistic judgments without proper decomposition.

Method: JADES employs a two-step process: (1) decomposition of jailbreak prompts into sub-questions with attribute-based weights and (2) automated scoring of sub-answers using a customizable rubric, including an optional fact-checking module to detect hallucinations in responses.

Result: JADES achieved 98.5% agreement with human evaluators on the JailbreakQR benchmark (400 annotated prompts/responses) in a binary success/failure setting, showing performance over 9% better than strong baselines. Re-evaluation of existing attacks showed significant success rate reductions (e.g., from 93% to 69% for LAA on GPT-3.5-Turbo).

Conclusion: JADES offers a robust, accurate framework for jailbreak evaluation through decomposition and interpretable scoring, demonstrating the need for more precise metrics in LLM safety assessments and providing a reliable foundation for future research.

Abstract: Accurately determining whether a jailbreak attempt has succeeded is a
fundamental yet unresolved challenge. Existing evaluation methods rely on
misaligned proxy indicators or naive holistic judgments. They frequently
misinterpret model responses, leading to inconsistent and subjective
assessments that misalign with human perception. To address this gap, we
introduce JADES (Jailbreak Assessment via Decompositional Scoring), a universal
jailbreak evaluation framework. Its key mechanism is to automatically decompose
an input harmful question into a set of weighted sub-questions, score each
sub-answer, and weight-aggregate the sub-scores into a final decision. JADES
also incorporates an optional fact-checking module to strengthen the detection
of hallucinations in jailbreak responses. We validate JADES on JailbreakQR, a
newly introduced benchmark proposed in this work, consisting of 400 pairs of
jailbreak prompts and responses, each meticulously annotated by humans. In a
binary setting (success/failure), JADES achieves 98.5% agreement with human
evaluators, outperforming strong baselines by over 9%. Re-evaluating five
popular attacks on four LLMs reveals substantial overestimation (e.g., LAA's
attack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results show
that JADES could deliver accurate, consistent, and interpretable evaluations,
providing a reliable basis for measuring future jailbreak attacks.

</details>


### [17] [Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review](https://arxiv.org/abs/2508.20863)
*Matteo Gioele Collu,Umberto Salviati,Roberto Confalonieri,Mauro Conti,Giovanni Apruzzese*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) are increasingly being integrated into the
scientific peer-review process, raising new questions about their reliability
and resilience to manipulation. In this work, we investigate the potential for
hidden prompt injection attacks, where authors embed adversarial text within a
paper's PDF to influence the LLM-generated review. We begin by formalising
three distinct threat models that envision attackers with different motivations
-- not all of which implying malicious intent. For each threat model, we design
adversarial prompts that remain invisible to human readers yet can steer an
LLM's output toward the author's desired outcome. Using a user study with
domain scholars, we derive four representative reviewing prompts used to elicit
peer reviews from LLMs. We then evaluate the robustness of our adversarial
prompts across (i) different reviewing prompts, (ii) different commercial
LLM-based systems, and (iii) different peer-reviewed papers. Our results show
that adversarial prompts can reliably mislead the LLM, sometimes in ways that
adversely affect a "honest-but-lazy" reviewer. Finally, we propose and
empirically assess methods to reduce detectability of adversarial prompts under
automated content checks.

</details>


### [18] [AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning](https://arxiv.org/abs/2508.20866)
*Amine Lbath,Massih-Reza Amini,Aurelien Delaitre,Vadim Okun*

Main category: cs.CR

TL;DR: This paper presents a new framework that automatically introduces realistic, category-specific vulnerabilities into secure C/C++ codebases to enhance AI-driven vulnerability detection and repair systems. It uses multiple AI agents, traditional tools, and advanced techniques like Retrieval-Augmented Generation and Low-Rank approximation for efficient model training. The experiments show high success rates in vulnerability injection.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of software and sophistication of cyber-attacks highlight the need for more effective datasets to train AI-driven vulnerability detection and repair systems.

Method: The approach uses a framework that coordinates multiple AI agents simulating expert reasoning, function agents, and traditional analysis tools. It incorporates Retrieval-Augmented Generation for context and Low-Rank weight approximation for efficient model fine-tuning.

Result: Success rates between 89% and 95% in injecting vulnerabilities at the function level across 116 code samples from three benchmarks.

Conclusion: The proposed framework effectively generates realistic dataset by automatically introducing vulnerabilities, outperforming other techniques in terms of dataset accuracy using fewer computational resources due to the Low-Rank method.

Abstract: The increasing complexity of software systems and the sophistication of
cyber-attacks have underscored the critical need for effective automated
vulnerability detection and repair systems. Traditional methods, such as static
program analysis, face significant challenges related to scalability,
adaptability, and high false-positive and false-negative rates. AI-driven
approaches, particularly those using machine learning and deep learning models,
show promise but are heavily reliant on the quality and quantity of training
data. This paper introduces a novel framework designed to automatically
introduce realistic, category-specific vulnerabilities into secure C/C++
codebases to generate datasets. The proposed approach coordinates multiple AI
agents that simulate expert reasoning, along with function agents and
traditional code analysis tools. It leverages Retrieval-Augmented Generation
for contextual grounding and employs Low-Rank approximation of weights for
efficient model fine-tuning. Our experimental study on 116 code samples from
three different benchmarks suggests that our approach outperforms other
techniques with regard to dataset accuracy, achieving between 89\% and 95\%
success rates in injecting vulnerabilities at function level.

</details>


### [19] [PromptSleuth: Detecting Prompt Injection via Semantic Intent Invariance](https://arxiv.org/abs/2508.20890)
*Mengxiao Wang,Yuxuan Zhang,Guofei Gu*

Main category: cs.CR

TL;DR: This paper proposes PromptSleuth, a semantic intent-based defense framework for mitigating prompt injection (PI) attacks in LLMs, supported by a new comprehensive benchmark demonstrating limitations of existing defenses.


<details>
  <summary>Details</summary>
Motivation: Prompt injection attacks are continually evolving with paraphrased/obfuscated inputs and multi-task strategies, undermining existing benchmarks and defenses. Current evaluation methods lack coverage of these sophisticated attack patterns.

Method: 1. Developed an extended benchmark incorporating prior frameworks plus new manipulation techniques and multi-task injection scenarios
2. Proposed PromptSleuth: a defense framework using task-level semantic intent analysis rather than superficial pattern recognition
3. Evaluated against state-of-the-art LLM benchmarks with emphasis on multi-task injection scenarios

Result: PromptSleuth outperformed 8 existing defense methods across multiple benchmarks with comparable runtime efficiency. The new benchmark revealed significant vulnerabilities in current defenses under multi-task injection scenarios.

Conclusion: Semantic intent analysis (PromptSleuth) provides a robust defense strategy against evolving prompt injection threats by addressing the invariant aspect of adversarial intent. The new benchmark highlights critical gaps in defense capabilities that intent-based approaches can resolve.

Abstract: Large Language Models (LLMs) are increasingly integrated into real-world
applications, from virtual assistants to autonomous agents. However, their
flexibility also introduces new attack vectors-particularly Prompt Injection
(PI), where adversaries manipulate model behavior through crafted inputs. As
attackers continuously evolve with paraphrased, obfuscated, and even multi-task
injection strategies, existing benchmarks are no longer sufficient to capture
the full spectrum of emerging threats.
  To address this gap, we construct a new benchmark that systematically extends
prior efforts. Our benchmark subsumes the two widely-used existing ones while
introducing new manipulation techniques and multi-task scenarios, thereby
providing a more comprehensive evaluation setting. We find that existing
defenses, though effective on their original benchmarks, show clear weaknesses
under our benchmark, underscoring the need for more robust solutions. Our key
insight is that while attack forms may vary, the adversary's intent-injecting
an unauthorized task-remains invariant. Building on this observation, we
propose PromptSleuth, a semantic-oriented defense framework that detects prompt
injection by reasoning over task-level intent rather than surface features.
Evaluated across state-of-the-art benchmarks, PromptSleuth consistently
outperforms existing defense while maintaining comparable runtime and cost
efficiency. These results demonstrate that intent-based semantic reasoning
offers a robust, efficient, and generalizable strategy for defending LLMs
against evolving prompt injection threats.

</details>


### [20] [Guarding Against Malicious Biased Threats (GAMBiT) Experiments: Revealing Cognitive Bias in Human-Subjects Red-Team Cyber Range Operations](https://arxiv.org/abs/2508.20963)
*Brandon Beltz,Jim Doty,Yvonne Fonken,Nikolos Gurney,Brett Israelsen,Nathan Lau,Stacy Marsella,Rachelle Thomas,Stoney Trent,Peggy Wu,Ya-Ting Yang,Quanyan Zhu*

Main category: cs.CR

TL;DR: The paper introduces three large-scale datasets from the GAMBiT project, capturing diverse multi-modal data from simulated cyber attacks by skilled participants over two years. These datasets aim to support research in attack analysis and security strategies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide researchers with rich, multi-modal data from real human attackers in a controlled environment. This data can help improve understanding of attacker behavior and develop better bias-aware analytic tools for cyber defense.

Method: The method involved running three experiments (July 2024-March 2025) with 19-20 skilled attackers each. Attackers used a standardized Kali Linux VM to conduct two 8-hour days of operations in a simulated enterprise network, where they attempted tasks like target discovery and data exfiltration. The researchers collected various types of data including self-reports, operational notes, terminal histories, keylogs, network packet captures (PCAP), and NIDS alerts (Suricata).

Result: The result of the study is the creation of the three large-scale datasets, each containing detailed multi-modal data from the participants' activities in the simulated enterprise network. These datasets include a variety of data types such as self-reports, notes, terminal histories, keylogs, PCAPs, and NIDS alerts, allowing for comprehensive behavioral and technical analysis.

Conclusion: The conclusion is that the combined release of these datasets from the GAMBiT project provides valuable resources for researchers to advance studies in attacker behavior modeling, bias-aware analytics, and benchmarking of methods used in cybersecurity. The data is made available via IEEE Dataport for experiments 1 to 3.

Abstract: We present three large-scale human-subjects red-team cyber range datasets
from the Guarding Against Malicious Biased Threats (GAMBiT) project. Across
Experiments 1-3 (July 2024-March 2025), 19-20 skilled attackers per experiment
conducted two 8-hour days of self-paced operations in a simulated enterprise
network (SimSpace Cyber Force Platform) while we captured multi-modal data:
self-reports (background, demographics, psychometrics), operational notes,
terminal histories, keylogs, network packet captures (PCAP), and NIDS alerts
(Suricata). Each participant began from a standardized Kali Linux VM and
pursued realistic objectives (e.g., target discovery and data exfiltration)
under controlled constraints. Derivative curated logs and labels are included.
The combined release supports research on attacker behavior modeling,
bias-aware analytics, and method benchmarking. Data are available via IEEE
Dataport entries for Experiments 1-3.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [21] [Evaluating LLMs on microservice-based applications: how complex is your specification?](https://arxiv.org/abs/2508.20119)
*Daniel M. Yellin*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper we evaluate how far LLMs have advanced in generating code for
real-world problems. Specifically, we explore code synthesis for
microservice-based applications, a widely used architecture pattern. We define
a standard template for specifying these applications, and we propose a metric
for judging the difficulty level of a specification. The higher the score, the
more difficult it is to generate code for the specification. We develop a
framework to automate the process of testing LLM-synthesized code for a
microservice using unit tests. Our experimental results show that strong LLMs
(like GPT-3o-mini) do fairly well on medium difficulty specifications but do
very poorly on those of higher difficulty levels. This is due to more intricate
business logic, a greater use of external services, database integration and
inclusion of non-functional capabilities such as authentication. We analyzed
the errors in LLM-synthesized code and report on the key challenges LLMs face
in generating code for these specifications thereby suggesting future research
directions to improve code synthesis for real-world problems.

</details>


### [22] [Towards Better Correctness and Efficiency in Code Generation](https://arxiv.org/abs/2508.20124)
*Yunlong Feng,Yang Xu,Xiao Xu,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While code large language models have demonstrated remarkable progress in
code generation, the generated code often exhibits poor runtime efficiency,
limiting its practical application in performance-sensitive scenarios. To
address this limitation, we propose an efficiency-oriented reinforcement
learning framework guided by a novel performance reward. Based on this
framework, we take a deeper dive into the code efficiency problem, identifying
then proposing methods to overcome key bottlenecks: (1) Dynamic exploration
overcomes the static data constraints of offline fine-tuning, enabling the
discovery of more efficient code implementations. (2) The error-insensitive
reinforcement learning method and high-contrast efficiency signals are crucial
for mitigating systematic errors and achieving effective optimization. (3)
Online exploration is most effective when starting from a high-correctness
baseline, as this allows for efficiency improvements without sacrificing
accuracy. With these discoveries, we finally propose a two-stage tuning method,
which achieves high and balanced performance across correctness and efficiency.
The results of experiments show the effectiveness of the method, which improves
code correctness by 10.18\% and runtime efficiency by 7.75\% on a 7B model,
achieving performance comparable to much larger model.

</details>


### [23] [Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators](https://arxiv.org/abs/2508.20340)
*Maolin Sun,Yibiao Yang,Yuming Zhou*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems
and programming languages research, providing the foundation for tasks like
symbolic execution and automated verification. Because these solvers sit on the
critical path, their correctness is essential, and high-quality test formulas
are key to uncovering bugs. However, while prior testing techniques performed
well on earlier solver versions, they struggle to keep pace with rapidly
evolving features. Recent approaches based on Large Language Models (LLMs) show
promise in exploring advanced solver capabilities, but two obstacles remain:
nearly half of the generated formulas are syntactically invalid, and iterative
interactions with the LLMs introduce substantial computational overhead. In
this study, we present Chimera, a novel LLM-assisted fuzzing framework that
addresses both issues by shifting from direct formula generation to the
synthesis of reusable term (i.e., logical expression) generators. Particularly,
Chimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for
SMT theories, including solver-specific extensions, from documentation, and (2)
synthesize composable Boolean term generators that adhere to these grammars.
During fuzzing, Chimera populates structural skeletons derived from existing
formulas with the terms iteratively produced by the LLM-synthesized generators.
This design ensures syntactic validity while promoting semantic diversity.
Notably, Chimera requires only one-time LLM interaction investment,
dramatically reducing runtime cost. We evaluated Chimera on two leading SMT
solvers: Z3 and cvc5. Our experiments show that Chimera has identified 43
confirmed bugs, 40 of which have already been fixed by developers.

</details>


### [24] [Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought](https://arxiv.org/abs/2508.20370)
*Lingzhe Zhang,Tong Jia,Kangjin Wang,Weijie Hong,Chiming Duan,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As contemporary microservice systems become increasingly popular and
complex-often comprising hundreds or even thousands of fine-grained,
interdependent subsystems-they are facing more frequent failures. Ensuring
system reliability thus demands accurate root cause localization. While traces
and metrics have proven to be effective data sources for this task, existing
methods either heavily rely on pre-defined schemas, which struggle to adapt to
evolving operational contexts, or lack interpretability in their reasoning
process, thereby leaving Site Reliability Engineers (SREs) confused. In this
paper, we conduct a comprehensive study on how SREs localize the root cause of
failures, drawing insights from multiple professional SREs across different
organizations. Our investigation reveals that human root cause analysis
exhibits three key characteristics: recursiveness, multi-dimensional expansion,
and cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,
an adaptive root cause localization method for microservice systems that
leverages a multi-agent recursion-of-thought framework. RCLAgent employs a
novel recursion-of-thought strategy to guide the LLM's reasoning process,
effectively integrating data from multiple agents and tool-assisted analysis to
accurately pinpoint the root cause. Experimental evaluations on various public
datasets demonstrate that RCLAgent achieves superior performance by localizing
the root cause using only a single request-outperforming state-of-the-art
methods that depend on aggregating multiple requests. These results underscore
the effectiveness of RCLAgent in enhancing the efficiency and precision of root
cause localization in complex microservice environments.

</details>


### [25] [AI and Agile Software Development: A Research Roadmap from the XP2025 Workshop](https://arxiv.org/abs/2508.20563)
*Zheying Zhang,Tomas Herda,Victoria Pichler,Pekka Abrahamsson,Geir K. Hanssen,Joshua Kerievsky,Alex Polyakov,Mohit Chandna,Marius Irgens,Kai-Kristian Kemell,Ayman Asad Khan,Crystal Kwok,Evan Leybourn,Munish Malik,Dorota Mleczko,Morteza Moalagh,Christopher Morales,Yuliia Pieskova,Daniel Planötscher,Mika Saari,Anastasiia Tkalich,Karl Josef Gstettner,Xiaofeng Wang*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper synthesizes the key findings from a full-day XP2025 workshop on
"AI and Agile: From Frustration to Success", held in Brugg-Windisch,
Switzerland. The workshop brought together over 30 interdisciplinary academic
researchers and industry practitioners to tackle the concrete challenges and
emerging opportunities at the intersection of Generative Artificial
Intelligence (GenAI) and agile software development. Through structured,
interactive breakout sessions, participants identified shared pain points like
tool fragmentation, governance, data quality, and critical skills gaps in AI
literacy and prompt engineering. These issues were further analyzed, revealing
underlying causes and cross-cutting concerns. The workshop concluded by
collaboratively co-creating a multi-thematic research roadmap, articulating
both short-term, implementable actions and visionary, long-term research
directions. This cohesive agenda aims to guide future investigation and drive
the responsible, human-centered integration of GenAI into agile practices.

</details>


### [26] [Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol](https://arxiv.org/abs/2508.20737)
*Wei Ma,Yixiao Yang,Qiang Hu,Shi Ying,Zhi Jin,Bo Du,Zhenchang Xing,Tianlin Li,Junjie Shi,Yang Liu,Linxiao Jiang*

Main category: cs.SE

TL;DR: The paper introduces a three-layer architecture for LLM applications (System Shell, Prompt Orchestration, LLM Inference) and proposes the Agent Interaction Communication Language (AICL) to enable testing across layers, combining traditional methods with new AI safety approaches.


<details>
  <summary>Details</summary>
Motivation: LLM applications are no longer just text generators but complex systems integrating multiple capabilities, making their non-determinism, dynamism, and context dependence problematic for quality assurance. Effective testing strategies must adapt to these new challenges.

Method: The authors decompose LLM applications into three layers (System Shell, Prompt Orchestration, LLM Inference) and evaluate how traditional software testing methods can be applied in each. They compare existing testing approaches from software engineering and AI safety communities, identifying key differences, and propose collaborative strategies and a protocol for standardized testing.

Result: The study identifies 6 core challenges due to 4 fundamental differences in traditional and AI testing approaches. It proposes four collaborative strategies and a closed-loop QA framework with pre-deployment validation and runtime monitoring. They also introduce AICL, a test-centric communication protocol for AI agents that integrates well into current architectures.

Conclusion: Quality assurance for LLM applications requires a layered approach with tailored testing methods for each layer and significant collaboration between software and AI communities. The proposed strategies, framework, and AICL protocol offer a foundation for standardization, but further research on practical implementation remains important.

Abstract: Applications of Large Language Models~(LLMs) have evolved from simple text
generators into complex software systems that integrate retrieval augmentation,
tool invocation, and multi-turn interactions. Their inherent non-determinism,
dynamism, and context dependence pose fundamental challenges for quality
assurance. This paper decomposes LLM applications into a three-layer
architecture: \textbf{\textit{System Shell Layer}}, \textbf{\textit{Prompt
Orchestration Layer}}, and \textbf{\textit{LLM Inference Core}}. We then assess
the applicability of traditional software testing methods in each layer:
directly applicable at the shell layer, requiring semantic reinterpretation at
the orchestration layer, and necessitating paradigm shifts at the inference
core. A comparative analysis of Testing AI methods from the software
engineering community and safety analysis techniques from the AI community
reveals structural disconnects in testing unit abstraction, evaluation metrics,
and lifecycle management. We identify four fundamental differences that
underlie 6 core challenges. To address these, we propose four types of
collaborative strategies (\emph{Retain}, \emph{Translate}, \emph{Integrate},
and \emph{Runtime}) and explore a closed-loop, trustworthy quality assurance
framework that combines pre-deployment validation with runtime monitoring.
Based on these strategies, we offer practical guidance and a protocol proposal
to support the standardization and tooling of LLM application testing. We
propose a protocol \textbf{\textit{Agent Interaction Communication Language}}
(AICL) that is used to communicate between AI agents. AICL has the
test-oriented features and is easily integrated in the current agent framework.

</details>


### [27] [From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of LLM-Generated Behavioural Specifications from Food-Safety Regulations](https://arxiv.org/abs/2508.20744)
*Shabnam Hassani,Mehrdad Sabetzadeh,Daniel Amyot*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Context: Laws and regulations increasingly affect software design and quality
assurance, but legal texts are written in technology-neutral language. This
creates challenges for engineers who must develop compliance artifacts such as
requirements and acceptance criteria. Manual creation is labor-intensive,
error-prone, and requires domain expertise. Advances in Generative AI (GenAI),
especially Large Language Models (LLMs), offer a way to automate deriving such
artifacts.
  Objective: We present the first systematic human-subject study of LLMs'
ability to derive behavioral specifications from legal texts using a
quasi-experimental design. These specifications translate legal requirements
into a developer-friendly form.
  Methods: Ten participants evaluated specifications generated from food-safety
regulations by Claude and Llama. Using Gherkin, a structured BDD language, 60
specifications were produced. Each participant assessed 12 across five
criteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each
specification was reviewed by two participants, yielding 120 assessments.
  Results: For Relevance, 75% of ratings were highest and 20% second-highest.
Clarity reached 90% highest. Completeness: 75% highest, 19% second.
Singularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No
lowest ratings occurred. Mann-Whitney U tests showed no significant differences
across participants or models. Llama slightly outperformed Claude in Clarity,
Completeness, and Time Savings, while Claude was stronger in Singularity.
Feedback noted hallucinations and omissions but confirmed the utility of the
specifications.
  Conclusion: LLMs can generate high-quality Gherkin specifications from legal
texts, reducing manual effort and providing structured artifacts useful for
implementation, assurance, and test generation.

</details>


### [28] [Towards an Architectural Perspective for Sustainability: Bundle the Needs from Industry](https://arxiv.org/abs/2508.20774)
*Markus Funke,Patricia Lago*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Sustainability is increasingly recognized as an emerging quality property in
software-intensive systems, yet architects lack structured guidance to address
it effectively throughout the software design phase. Architectural
perspectives-an architectural knowledge artifact composed of concerns,
activities, tactics, pitfalls, and checklists-offer a promising approach to
tackle such emerging quality properties across architectural views and are also
independent of architecture frameworks and industry contexts. In this paper, we
present a sustainability perspective vision, i.e., a revised notion of
architectural perspective meant to be filled with its own elements to target
sustainability concerns. We formulate our sustainability perspective vision
through evidence from applying snowballing to seminal literature and from
conducting a focus group with experts in the field. Our findings confirm the
relevance of the different perspective elements in practice and highlight
implications for shaping a sustainability perspective that meets industrial
needs.

</details>


### [29] [Automated Test Oracles for Flaky Cyber-Physical System Simulators: Approach and Evaluation](https://arxiv.org/abs/2508.20902)
*Baharin A. Jodat,Khouloud Gaaloul,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: Introduces assertion-based test oracles using genetic programming with Ochiai to reduce CPS testing costs and handle flaky simulators.


<details>
  <summary>Details</summary>
Motivation: CPS simulation testing is resource-intensive and prone to flakiness, creating unreliable results and high computational costs. This demands robust, execution-free test oracles to minimize retesting.

Method: Proposed assertion-based test oracles as logical predicates over system inputs, implemented via two approaches: genetic programming (GP) with SBFL formulas (Ochiai, Tarantula, Naish) as fitness functions, and decision trees/rules (DT/DR) for comparison.

Result: GP with Ochiai achieved significantly higher accuracy than Tarantula/Naish-based GP and DT/DR methods. Robustness against flakiness was demonstrated with only 4% average accuracy variation across four systems (networking, autonomous driving).

Conclusion: Assertion-based oracles using GP with Ochiai formula provide accurate, stable test outcomes for CPS, outperforming other approaches and reducing reliance on error-prone simulator executions.

Abstract: Simulation-based testing of cyber-physical systems (CPS) is costly due to the
time-consuming execution of CPS simulators. In addition, CPS simulators may be
flaky, leading to inconsistent test outcomes and requiring repeated test
re-execution for reliable test verdicts. Automated test oracles that do not
require system execution are therefore crucial for reducing testing costs.
Ideally, such test oracles should be interpretable to facilitate human
understanding of test verdicts, and they must be robust against the potential
flakiness of CPS simulators. In this article, we propose assertion-based test
oracles for CPS as sets of logical and arithmetic predicates defined over the
inputs of the system under test. Given a test input, our assertion-based test
oracle determines, without requiring test execution, whether the test passes,
fails, or if the oracle is inconclusive in predicting a verdict. We describe
two methods for generating assertion-based test oracles: one using genetic
programming~(GP) that employs well-known spectrum-based fault localization
(SBFL) ranking formulas, namely Ochiai, Tarantula, and Naish, as fitness
functions; and the other using decision trees (DT) and decision rules (DR). We
evaluate our assertion-based test oracles through case studies in the domains
of aerospace, networking and autonomous driving. We show that test oracles
generated using GP with Ochiai are significantly more accurate than those
obtained using GP with Tarantula and Naish or using DT or DR. Moreover, this
accuracy advantage remains even when accounting for the flakiness of the system
under test. We further show that the assertion-based test oracles generated by
GP with Ochiai are robust against flakiness with only 4% average variation in
their accuracy results across four different network and autonomous driving
systems with flaky behaviours.

</details>


### [30] [Deep Learning Based Concurrency Bug Detection and Localization](https://arxiv.org/abs/2508.20911)
*Zuocheng Feng,Kaiwen Zhang,Miaomiao Wang,Yiming Cheng,Yuandao Cai,Xiaofeng Li,Guanjun Liu*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Concurrency bugs, caused by improper synchronization of shared resources in
multi-threaded or distributed systems, are notoriously hard to detect and thus
compromise software reliability and security. The existing deep learning
methods face three main limitations. First, there is an absence of large and
dedicated datasets of diverse concurrency bugs for them. Second, they lack
sufficient representation of concurrency semantics. Third, binary
classification results fail to provide finer-grained debug information such as
precise bug lines. To address these problems, we propose a novel method for
effective concurrency bug detection as well as localization. We construct a
dedicated concurrency bug dataset to facilitate model training and evaluation.
We then integrate a pre-trained model with a heterogeneous graph neural network
(GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) that
concisely and effectively characterizes concurrency semantics. To further
facilitate debugging, we employ SubgraphX, a GNN-based interpretability method,
which explores the graphs to precisely localize concurrency bugs, mapping them
to specific lines of source code. On average, our method demonstrates an
improvement of 10\% in accuracy and precision and 26\% in recall compared to
state-of-the-art methods across diverse evaluation settings.

</details>


### [31] [ConfLogger: Enhance Systems' Configuration Diagnosability through Configuration Logging](https://arxiv.org/abs/2508.20977)
*Shiwen Shan,Yintong Huo,Yuxin Su,Zhining Wang,Dan Li,Zibin Zheng*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Modern configurable systems offer customization via intricate configuration
spaces, yet such flexibility introduces pervasive configuration-related issues
such as misconfigurations and latent softwarebugs. Existing diagnosability
supports focus on post-failure analysis of software behavior to identify
configuration issues, but none of these approaches look into whether the
software clue sufficient failure information for diagnosis. To fill in the
blank, we propose the idea of configuration logging to enhance existing logging
practices at the source code level. We develop ConfLogger, the first tool that
unifies configuration-aware static taint analysis with LLM-based log generation
to enhance software configuration diagnosability. Specifically, our method 1)
identifies configuration-sensitive code segments by tracing
configuration-related data flow in the whole project, and 2) generates
diagnostic log statements by analyzing configuration code contexts. Evaluation
results on eight popular software systems demonstrate the effectiveness of
ConfLogger to enhance configuration diagnosability. Specifically,
ConfLogger-enhanced logs successfully aid a log-based misconfiguration
diagnosis tool to achieve 100% accuracy on error localization in 30 silent
misconfiguration scenarios, with 80% directly resolvable through explicit
configuration information exposed. In addition, ConfLogger achieves 74%
coverage of existing logging points, outperforming baseline LLM-based loggers
by 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,
and 26.2% higher in F1 compared to the state-of-the-art baseline in terms of
variable logging while also augmenting diagnostic value. A controlled user
study on 22 cases further validated its utility, speeding up diagnostic time by
1.25x and improving troubleshooting accuracy by 251.4%.

</details>


### [32] [Dynamics of Gender Bias in Software Engineering](https://arxiv.org/abs/2508.21050)
*Thomas J. Misa*

Main category: cs.SE

TL;DR: The study explores gender biases in software engineering, focusing on the field's historical roots in engineering and computer science, and quantitatively examines women's participation in the International Conference of Software Engineering from 1976 to 2010.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the concern that software engineering, rooted in both engineering and computer science, may inherit and perpetuate gender biases common to these fields.

Method: The analysis involves surveys on the history and focus areas of software engineering, particularly profiling five field leaders and analyzing data on women's involvement as research authors at the top conference in the field between 1976 and 2010.

Result: The study identifies a dozen years with statistically significant gender exclusion in women's participation in the International Conference of Software Engineering research author listings.

Conclusion: The paper suggests policy dimensions for addressing gender bias in computing research, highlighting the need for systemic recognition and action on this issue within the software engineering field and beyond.

Abstract: The field of software engineering is embedded in both engineering and
computer science, and may embody gender biases endemic to both. This paper
surveys software engineering's origins and its long-running attention to
engineering professionalism, profiling five leaders; it then examines the
field's recent attention to gender issues and gender bias. It next
quantitatively analyzes women's participation as research authors in the
field's leading International Conference of Software Engineering (1976-2010),
finding a dozen years with statistically significant gender exclusion. Policy
dimensions of research on gender bias in computing are suggested.

</details>
