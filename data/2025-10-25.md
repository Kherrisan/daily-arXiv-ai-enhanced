<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 5]
- [cs.SE](#cs.SE) [Total: 8]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [CourtGuard: A Local, Multiagent Prompt Injection Classifier](https://arxiv.org/abs/2510.19844)
*Isaac Wu,Michael Maslowski*

Main category: cs.CR

TL;DR: The paper introduces CourtGuard, a multiagent system for detecting prompt injection attacks on Large Language Models (LLMs), where three specialized agent models evaluate the benignity or harmfulness of a prompt. The system shows a lower false positive rate than a Direct Detector but is overall less effective, highlighting the potential and challenges of multiagent systems in this context.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) are being used in sensitive applications, and prompt injection attacks represent a growing threat. These attacks can cause data leaks, misinformation, and other harmful behaviors, creating a clear need for effective detection methods.

Method: CourtGuard is a multiagent system where prompts are evaluated by three models: a 'defense attorney' (arguing innocence), a 'prosecution attorney' (arguing maliciousness), and a 'judge' (making the final classification decision). This contrasts with the 'Direct Detector' method, which uses a single model as a judge.

Result: CourtGuard achieved a lower false positive rate than the Direct Detector, but had a generally lower overall performance in detecting prompt injection attacks. The results highlight the importance of considering both adversarial and benign cases when evaluating prompts.

Conclusion: Although CourtGuard outperformed a simple LLM judge in terms of false positives, the multiagent design had worse overall effectiveness in detecting prompt injection attacks. This study encourages further research into the use of cooperative multiagent systems for LLM security, while acknowledging the challenges involved.

Abstract: As large language models (LLMs) become integrated into various sensitive
applications, prompt injection, the use of prompting to induce harmful
behaviors from LLMs, poses an ever increasing risk. Prompt injection attacks
can cause LLMs to leak sensitive data, spread misinformation, and exhibit
harmful behaviors. To defend against these attacks, we propose CourtGuard, a
locally-runnable, multiagent prompt injection classifier. In it, prompts are
evaluated in a court-like multiagent LLM system, where a "defense attorney"
model argues the prompt is benign, a "prosecution attorney" model argues the
prompt is a prompt injection, and a "judge" model gives the final
classification. CourtGuard has a lower false positive rate than the Direct
Detector, an LLM as-a-judge. However, CourtGuard is generally a worse prompt
injection detector. Nevertheless, this lower false positive rate highlights the
importance of considering both adversarial and benign scenarios for the
classification of a prompt. Additionally, the relative performance of
CourtGuard in comparison to other prompt injection classifiers advances the use
of multiagent systems as a defense against prompt injection attacks. The
implementations of CourtGuard and the Direct Detector with full prompts for
Gemma-3-12b-it, Llama-3.3-8B, and Phi-4-mini-instruct are available at
https://github.com/isaacwu2000/CourtGuard.

</details>


### [2] [Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability](https://arxiv.org/abs/2510.19851)
*Artur Zolkowski,Wen Xing,David Lindner,Florian Tram√®r,Erik Jenner*

Main category: cs.CR

TL;DR: The paper investigates whether misaligned AI models can 


<details>
  <summary>Details</summary>
Motivation: Recent findings indicate that misaligned models might exhibit deceptive

Method: Introduce a composable and quantifiable taxonomy of prompts to encourage CoT obfuscation. They evaluate both internal

Result: t that CoT monitoring is effective and efficient without obfuscation. H

Conclusion: While CoT proves valuable for oversight, robust deployment needs model-specific stress-testing for monitorability.

Abstract: Recent findings suggest that misaligned models may exhibit deceptive
behavior, raising concerns about output trustworthiness. Chain-of-thought (CoT)
is a promising tool for alignment monitoring: when models articulate their
reasoning faithfully, monitors can detect and mitigate harmful behaviors before
undesirable outcomes occur. However, a key uncertainty is: Can models obfuscate
their CoT in order to pursue hidden adversarial objectives while evading
detection? To answer this question and thus stress-test CoT monitorability, we
develop a composable and quantifiable taxonomy of prompts to elicit CoT
obfuscation. We evaluate both internal CoT (reasoning traces) and external CoT
(prompted reasoning in outputs) using toy tasks and more realistic environments
in SHADE-Arena. We show that: (i) CoT monitoring performs accurately and
efficiently without obfuscation pressure. (ii) Under strong obfuscation
pressure, some models successfully complete adversarial tasks while evading
detection. (iii) Models do not obfuscate their internal CoT as much as their
external CoT (under prompt pressure). These results suggest that while CoT
provides valuable oversight in benign settings, robust deployment requires
model-specific stress-testing of monitorability.

</details>


### [3] [Model Context Contracts - MCP-Enabled Framework to Integrate LLMs With Blockchain Smart Contracts](https://arxiv.org/abs/2510.19856)
*Eranga Bandara,Sachin Shetty,Ravi Mukkamala,Ross Gore,Peter Foytik,Safdar H. Bouk,Abdul Rahman,Xueping Liang,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.CR

TL;DR: The paper introduces MCC, a framework that connects LLMs to blockchain using a protocol similar to MCP, enabling natural language interactions with smart contracts.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in blockchain and LLMs, there is no standardized method for their seamless integration, limiting dynamic interactions. The paper aims to fill this gap.

Method: The framework, called Model Context Contracts (MCC), uses a protocol inspired by MCP to link LLMs and blockchain smart contracts. The LLM is fine-tuned with a dataset of user inputs and corresponding MCP functions.

Result: An end-to-end prototype using the MCC framework was developed on the Rahasak blockchain with a fine-tuned Llama-4 LLM, demonstrating the integration's feasibility.

Conclusion: MCC represents a novel approach to integrating LLMs and blockchain through a protocol like MCP, enabling natural language interactions with smart contracts and enhancing user experience.

Abstract: In recent years, blockchain has experienced widespread adoption across
various industries, becoming integral to numerous enterprise applications.
Concurrently, the rise of generative AI and LLMs has transformed human-computer
interactions, offering advanced capabilities in understanding and generating
human-like text. The introduction of the MCP has further enhanced AI
integration by standardizing communication between AI systems and external data
sources. Despite these advancements, there is still no standardized method for
seamlessly integrating LLM applications and blockchain. To address this
concern, we propose "MCC: Model Context Contracts" a novel framework that
enables LLMs to interact directly with blockchain smart contracts through
MCP-like protocol. This integration allows AI agents to invoke blockchain smart
contracts, facilitating more dynamic and context-aware interactions between
users and blockchain networks. Essentially, it empowers users to interact with
blockchain systems and perform transactions using queries in natural language.
Within this proposed architecture, blockchain smart contracts can function as
intelligent agents capable of recognizing user input in natural language and
executing the corresponding transactions. To ensure that the LLM accurately
interprets natural language inputs and maps them to the appropriate MCP
functions, the LLM was fine-tuned using a custom dataset comprising user inputs
paired with their corresponding MCP server functions. This fine-tuning process
significantly improved the platform's performance and accuracy. To validate the
effectiveness of MCC, we have developed an end-to-end prototype implemented on
the Rahasak blockchain with the fine-tuned Llama-4 LLM. To the best of our
knowledge, this research represents the first approach to using the concept of
Model Context Protocol to integrate LLMs with blockchain.

</details>


### [4] [Cyberattack Detection in Critical Infrastructure and Supply Chains](https://arxiv.org/abs/2510.19859)
*Smita Khapre*

Main category: cs.CR

TL;DR: This paper proposes integrating Dense Neural Networks (DNN) with Data Augmentation to improve Zero-day attack detection in IDS, addressing imbalanced network flow datasets using Synthetic Minority Oversampling Technique, but highlights limitations in real-world performance due to overfitting.


<details>
  <summary>Details</summary>
Motivation: Cyberattack detection in Critical Infrastructure and Supply Chains is challenging in Industry 4.0. Traditional IDS rely on known signatures, failing to detect Zero-day attacks. Network flow datasets are highly imbalanced, leading to detection challenges.

Method: A Dense Neural Network (DNN) is enhanced with Data Augmentation, specifically using Synthetic Minority Oversampling Technique (SMOTE) to address class imbalance. The system is tested for its ability to detect novel (Zero-day) attacks by learning from augmented data.

Result: Higher classification accuracies are achieved with balanced test data, but lower accuracies occur with original imbalanced data, indicating overfitting. A comparison with state-of-the-art methods shows poor class classification on the original dataset, suggesting SMOTE alone is insufficient for highly imbalanced data.

Conclusion: While DNN with SMOTE improves Zero-day attack detection, the method struggles with overfitting and poor performance on real-world imbalanced datasets. Novel data augmentation techniques are needed for highly imbalanced network flow data to enhance IDS effectiveness in Industry 4.0 contexts.

Abstract: Cyberattack detection in Critical Infrastructure and Supply Chains has become
challenging in Industry 4.0. Intrusion Detection Systems (IDS) are deployed to
counter the cyberattacks. However, an IDS effectively detects attacks based on
the known signatures and patterns, Zero-day attacks go undetected. To overcome
this drawback in IDS, the integration of a Dense Neural Network (DNN) with Data
Augmentation is proposed. It makes IDS intelligent and enables it to self-learn
with high accuracy when a novel attack is encountered. The network flow
captures datasets are highly imbalanced same as the real network itself. The
Data Augmentation plays a crucial role in balancing the data. The balancing of
data is challenging as the minority class is as low as 0.000004\% of the
dataset, and the abundant class is higher than 80\% of the dataset. Synthetic
Minority Oversampling Technique is used for balancing the data. However, higher
accuracies are achieved with balanced test data, lower accuracies are
noticeable with the original imbalanced test data suggesting overfitting. A
comparison with state-of-the-art research using Synthetic Minority Oversampling
Technique with Edited Nearest Neighbor shows the classification of classes
remains poor for the original dataset. This suggests highly imbalanced datasets
of network flow require a different method of data augmentation.

</details>


### [5] [Policy-Governed RAG - Research Design Study](https://arxiv.org/abs/2510.19877)
*Jean-Marie Le Ray*

Main category: cs.CR

TL;DR: This paper introduces a policy-governed RAG architecture (Contracts/Control + Manifests/Trails + Receipts/Verification) that ensures auditable, verifiable AI outputs for high-risk regulated domains, enabling compliance with EU AI Act requirements via cryptographic evidence binding and ex-ante policy enforcement.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the need for AI systems in regulated industries (pharma, healthcare, finance, legal) to meet mandatory audit requirements and mitigate error costs exceeding thousands of euros under regulations like the EU AI Act, through verifiable policy adherence mechanisms.

Method: The method involves a tripartite architecture: 1) Contracts/Control (policy enforcement via SHRDLU-like mechanisms), 2) Manifests/Trails (cryptographic anchoring of sources using Memex-like systems), and 3) Receipts/Verification (portable compliance proofs via Xanadu-like protocols). Outputs are gated by verifiable evidence rather than post-hoc explanations.

Result: Results include validated targets: 20% reduction in confident errors, 900ms latency, 2.2x serve costs. The design integrates with pre-registered pilots using NO-GO gates and offers replayable, receipt-backed compliance for auditors.

Conclusion: The paper concludes that their policy-governed RAG architecture effectively enables audit-ready, policy-compliant AI generation in high-risk regulated domains by combining cryptographic verifiability with ex-ante policy checks, addressing critical limitations in existing systems.

Abstract: A policy-governed RAG architecture is specified for audit-ready generation in
regulated workflows, organized as a triptych: (I) Contracts/Control
(SHRDLU-like), which governs output adherence to legal and internal policies;
(II) Manifests/Trails (Memex-like), which cryptographically anchors all cited
source evidence to ensure verifiable provenance; and (III)
Receipts/Verification (Xanadu-like), which provides the final, portable proof
of compliance for auditors (portable COSE/JOSE) (see Section 4 and Appendix A).
Rather than explaining model internals, outputs are gated ex-ante and bound to
cryptographically verifiable evidence for each material answer. Unvalidated
targets are stated (>=20% relative reduction in confident errors; p95 latency
<= 900 ms; <= 2.2x serve cost) together with a pre-registered (optional) pilot
using NO-GO gates. The design complements existing RAG/guardrails by making
policy checks auditable, replayable, and receipt-backed. Target domains include
back-office compliance in pharma, medical devices, finance, legal, and the
public sector where error costs may exceed thousands of euros and audit trails
are mandatory under regulations such as the EU AI Act. Future evaluations may
pre-commit to publishing negative results when any example NO-GO gate is not
met.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [E-Test: E'er-Improving Test Suites](https://arxiv.org/abs/2510.19860)
*Ketai Qiu,Luca Di Grazia,Leonardo Mariani,Mauro Pezz√®*

Main category: cs.SE

TL;DR: E-Test is a new approach that uses LLMs to identify and generate test cases for untested execution scenarios in existing test suites, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing test suites are limited in their ability to cover all execution scenarios, and manual enrichment is time-consuming. There is a need to automatically find and add test cases for scenarios that emerge in production but are untested.

Method: E-Test uses Large Language Models to analyze large sets of scenarios from production usage and identifies those not covered by existing tests. It then generates new test cases to cover these scenarios.

Result: Evaluation on 1,975 scenarios showed E-Test achieves an F1-score of 0.55, outperforming existing approaches (max 0.34) and even vanilla LLMs (max 0.39).

Conclusion: E-Test significantly improves test suite coverage by effectively targeting untested execution scenarios, reducing manual effort needed and offering a more effective solution than current state-of-the-art methods.

Abstract: Test suites are inherently imperfect, and testers can always enrich a suite
with new test cases that improve its quality and, consequently, the reliability
of the target software system. However, finding test cases that explore
execution scenarios beyond the scope of an existing suite can be extremely
challenging and labor-intensive, particularly when managing large test suites
over extended periods.
  In this paper, we propose E-Test, an approach that reduces the gap between
the execution space explored with a test suite and the executions experienced
after testing by augmenting the test suite with test cases that explore
execution scenarios that emerge in production. E-Test (i) identifies executions
that have not yet been tested from large sets of scenarios, such as those
monitored during intensive production usage, and (ii) generates new test cases
that enhance the test suite. E-Test leverages Large Language Models (LLMs) to
pinpoint scenarios that the current test suite does not adequately cover, and
augments the suite with test cases that execute these scenarios.
  Our evaluation on a dataset of 1,975 scenarios, collected from highly-starred
open-source Java projects already in production and Defects4J, demonstrates
that E-Test retrieves not-yet-tested execution scenarios significantly better
than state-of-the-art approaches. While existing regression testing and field
testing approaches for this task achieve a maximum F1-score of 0.34, and
vanilla LLMs achieve a maximum F1-score of 0.39, E-Test reaches 0.55. These
results highlight the impact of E-Test in enhancing test suites by effectively
targeting not-yet-tested execution scenarios and reducing manual effort
required for maintaining test suites.

</details>


### [7] [SODBench: A Large Language Model Approach to Documenting Spreadsheet Operations](https://arxiv.org/abs/2510.19864)
*Amila Indika,Igor Molybog*

Main category: cs.SE

TL;DR: The paper introduces Spreadsheet Operations Documentation (SOD) to improve spreadsheet automation and collaboration through AI-generated documentation. It also evaluates the performance of five large language models on this task.


<details>
  <summary>Details</summary>
Motivation: Current spreadsheet practices lack systematic documentation, leading to issues in automation, collaboration, and knowledge retention. This hinders the efficiency and reproducibility of spreadsheet-based workflows.

Method: The authors introduce SOD as a new AI task focusing on translating spreadsheet manipulation code into natural language. They create a benchmark of 111 code snippets with corresponding natural language summaries and evaluate five LLMs using BLEU, GLEU, ROUGE-L, and METEOR metrics.

Result: The evaluation results show that LLMs can generate accurate spreadsheet documentation, indicating that SOD is a viable method to enhance reproducibility and collaboration. However, challenges still remain.

Conclusion: The study demonstrates the feasibility of using LLMs for SOD, which can contribute to improving spreadsheet practices. Addressing the identified challenges is essential to fully realize the benefits of this approach.

Abstract: Numerous knowledge workers utilize spreadsheets in business, accounting, and
finance. However, a lack of systematic documentation methods for spreadsheets
hinders automation, collaboration, and knowledge transfer, which risks the loss
of crucial institutional knowledge. This paper introduces Spreadsheet
Operations Documentation (SOD), an AI task that involves generating
human-readable explanations from spreadsheet operations. Many previous studies
have utilized Large Language Models (LLMs) for generating spreadsheet
manipulation code; however, translating that code into natural language for SOD
is a less-explored area. To address this, we present a benchmark of 111
spreadsheet manipulation code snippets, each paired with a corresponding
natural language summary. We evaluate five LLMs, GPT-4o, GPT-4o-mini,
LLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B, using BLEU, GLEU, ROUGE-L, and
METEOR metrics. Our findings suggest that LLMs can generate accurate
spreadsheet documentation, making SOD a feasible prerequisite step toward
enhancing reproducibility, maintainability, and collaborative workflows in
spreadsheets, although there are challenges that need to be addressed.

</details>


### [8] [Knowledge-Guided Multi-Agent Framework for Application-Level Software Code Generation](https://arxiv.org/abs/2510.19868)
*Qian Xiong,Bo Yang,Weisong Sun,Yiran Zhang,Tianlin Li,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: KGACG is a knowledge-guided application-level code generation framework that addresses limitations in existing multi-agent methods for complex software development through a collaborative closed-loop of agents.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent frameworks struggle with large-scale application-level code generation, lacking proper code organization and maintainability. Automated code generation via LLMs remains inefficient for complex projects.

Method: KGACG introduces a closed-loop system with three agents: Code Organization & Planning Agent (COPA) for structural planning, Coding Agent (CA) for implementation, and Testing Agent (TA with feedback mechanisms) to collaborate on transforming requirements into executable code.

Result: Demonstrated in a Java Tank Battle game case study, KGACG successfully navigates organizational challenges and maintains code generation processes through its agent collaboration framework.

Conclusion: KGACG advances automation in application-level software development by combining knowledge-guided agent collaboration with systematic feedback, improving scalability and maintainability.

Abstract: Automated code generation driven by Large Lan- guage Models (LLMs) has
enhanced development efficiency, yet generating complex application-level
software code remains challenging. Multi-agent frameworks show potential, but
existing methods perform inadequately in large-scale application-level software
code generation, failing to ensure reasonable orga- nizational structures of
project code and making it difficult to maintain the code generation process.
To address this, this paper envisions a Knowledge-Guided Application-Level Code
Generation framework named KGACG, which aims to trans- form software
requirements specification and architectural design document into executable
code through a collaborative closed- loop of the Code Organization & Planning
Agent (COPA), Coding Agent (CA), and Testing Agent (TA), combined with a
feedback mechanism. We demonstrate the collaborative process of the agents in
KGACG in a Java Tank Battle game case study while facing challenges. KGACG is
dedicated to advancing the automation of application-level software
development.

</details>


### [9] [BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills](https://arxiv.org/abs/2510.19898)
*Atharv Sonwane,Isadora White,Hyunji Lee,Matheus Pereira,Lucas Caccia,Minseon Kim,Zhengyan Shi,Chinmay Singh,Alessandro Sordoni,Marc-Alexandre C√¥t√©,Xingdi Yuan*

Main category: cs.SE

TL;DR: This paper introduces a method for generating synthetic bugs that reflect realistic software development scenarios, improving training efficiency for SWE agents and achieving state-of-the-art results on SWE-bench Verified.


<details>
  <summary>Details</summary>
Motivation: High-quality bugs are crucial for training the next generation of SWE agents, but previous approaches introduce out-of-distribution effects by creating bugs intentionally. Realistic bugs better mirror actual development processes and human-edit patterns.

Method: The method involves instructing SWE agents to introduce new features into codebases, which can unintentionally break existing tests and cause bugs. This contrasts with prior intentional bug generation techniques, and uses a combination of bug generation through new feature implementation and standard bug datasets for training.

Result: The paper shows improved training efficiency: achieving the same result with 40% less data (1.2k bugs vs. 3k bugs). The model trained on these bugs achieved SOTA: FrogBoss (32B) with a pass@1 of 54.6% and FrogMini (14B) with 45.3% on SWE-bench Verified.

Conclusion: End-to-end training with synthetic bugs that simulate real development scenarios outperforms existing methods and leads to significant improvements in effectiveness and efficiency in training SWE agents.

Abstract: High quality bugs are key to training the next generation of language model
based software engineering (SWE) agents. We introduce a novel method for
synthetic generation of difficult and diverse bugs. Our method instructs SWE
Agents to introduce a feature into the codebase whereby they may
unintentionally break tests, resulting in bugs. Prior approaches often induce
an out-of-distribution effect by generating bugs intentionally (e.g. by
introducing local perturbation to existing code), which does not reflect
realistic development processes. We perform qualitative analysis to demonstrate
that our approach for generating bugs more closely reflects the patterns found
in human-authored edits. Through extensive experiments, we demonstrate that our
bugs provide more efficient training data for supervised fine-tuning,
outperforming other bug datasets by 2% with half the training data (1.2k vs. 3k
bugs). We train on our newly generated bugs in addition to existing bug
datasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench
Verified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on
SWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over
three seeds.

</details>


### [10] [On Interaction Effects in Greybox Fuzzing](https://arxiv.org/abs/2510.19984)
*Konstantinos Kitsios,Marcel B√∂hme,Alberto Bacchelli*

Main category: cs.SE

TL;DR: TL;DR Summary


<details>
  <summary>Details</summary>
Motivation: Motivation

Method: Method

Result: Result

Conclusion: Conclusion

Abstract: A greybox fuzzer is an automated software testing tool that generates new
test inputs by applying randomly chosen mutators (e.g., flipping a bit or
deleting a block of bytes) to a seed input in random order and adds all
coverage-increasing inputs to the corpus of seeds. We hypothesize that the
order in which mutators are applied to a seed input has an impact on the
effectiveness of greybox fuzzers. In our experiments, we fit a linear model to
a dataset that contains the effectiveness of all possible mutator pairs and
indeed observe the conjectured interaction effect. This points us to more
efficient fuzzing by choosing the most promising mutator sequence with a higher
likelihood. We propose MuoFuzz, a greybox fuzzer that learns and chooses the
most promising mutator sequences. MuoFuzz learns the conditional probability
that the next mutator will yield an interesting input, given the previously
selected mutator. Then, it samples from the learned probability using a random
walk to generate mutator sequences. We compare the performance of MuoFuzz to
AFL++, which uses a fixed selection probability, and MOPT, which optimizes the
selection probability of each mutator in isolation. Experimental results on the
FuzzBench and MAGMA benchmarks show that MuoFuzz achieves the highest code
coverage and finds four bugs missed by AFL++ and one missed by both AFL++ and
MOPT.

</details>


### [11] [A Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises (FAIGMOE)](https://arxiv.org/abs/2510.19997)
*Abraham Itzhak Weinberg*

Main category: cs.SE

TL;DR: Paper presents FAIGMOE, a framework for GenAI adoption in midsize and large organizations, addressing their unique challenges and combining theory with practical implementation phases.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the lack of specific frameworks for GenAI implementation in midsize and enterprise organizations, which face distinct challenges in adopting GenAI beyond what traditional technology adoption models like TAM, TOE, and DOI can address.

Method: The paper introduces FAIGMOE as a conceptual framework, built by synthesizing technology adoption theory, organizational change management, and innovation diffusion perspectives into four phases: Strategic Assessment, Planning and Use Case Development, Implementation and Integration, and Operationalization and Optimization. The framework is designed to be adaptable to different organizational scales and includes GenAI-specific considerations.

Result: The FAIGMOE framework is introduced, providing actionable implementation protocols, assessment instruments, and governance templates for GenAI adoption, tailored to the distinct needs of midsize organizations and enterprise-level businesses.

Conclusion: FAIGMOE is positioned as a comprehensive conceptual framework that addresses the adoption of GenAI in both midsize and enterprise organizations. It is presented as a contribution requiring further empirical validation but offering a new perspective to the existing literature by explicitly addressing these specific adoption contexts.

Abstract: Generative Artificial Intelligence (GenAI) presents transformative
opportunities for organizations, yet both midsize organizations and larger
enterprises face distinctive adoption challenges. Midsize organizations
encounter resource constraints and limited AI expertise, while enterprises
struggle with organizational complexity and coordination challenges. Existing
technology adoption frameworks, including TAM (Technology Acceptance Model),
TOE (Technology Organization Environment), and DOI (Diffusion of Innovations)
theory, lack the specificity required for GenAI implementation across these
diverse contexts, creating a critical gap in adoption literature. This paper
introduces FAIGMOE (Framework for the Adoption and Integration of Generative AI
in Midsize Organizations and Enterprises), a conceptual framework addressing
the unique needs of both organizational types. FAIGMOE synthesizes technology
adoption theory, organizational change management, and innovation diffusion
perspectives into four interconnected phases: Strategic Assessment, Planning
and Use Case Development, Implementation and Integration, and
Operationalization and Optimization. Each phase provides scalable guidance on
readiness assessment, strategic alignment, risk governance, technical
architecture, and change management adaptable to organizational scale and
complexity. The framework incorporates GenAI specific considerations including
prompt engineering, model orchestration, and hallucination management that
distinguish it from generic technology adoption frameworks. As a perspective
contribution, FAIGMOE provides the first comprehensive conceptual framework
explicitly addressing GenAI adoption across midsize and enterprise
organizations, offering actionable implementation protocols, assessment
instruments, and governance templates requiring empirical validation through
future research.

</details>


### [12] [The Cost of Downgrading Build Systems: A Case Study of Kubernetes](https://arxiv.org/abs/2510.20041)
*Gareema Ranjan,Mahmoud Alfadel,Gengyi Sun,Shane McIntosh*

Main category: cs.SE

TL;DR: We study the performance differences between Bazel and Go Build in the Kubernetes project and four others after downgrading. Bazel is faster but uses more memory and CPU.


<details>
  <summary>Details</summary>
Motivation: This paper aims to understand the performance implications of downgrading from artifact-based build tools like Bazel to language-specific solutions like Go Build, which are perceived as easier to maintain but may impact resource usage.

Method: The authors conduct a case study of the Kubernetes project's downgrade from Bazel to Go Build, reproducing and analyzing full and incremental builds. They replicate the study on four other projects that made similar downgrades, evaluating build time, memory, and CPU usage.

Result: Results show Bazel is faster in full builds (23.06-38.66 vs. 75.19 seconds with Go Build) and more memory-intensive (Bazel uses less memory: 81.42-351.07 MB). Bazel also has higher CPU load above certain parallelism levels. Downgrading increased CI resource costs by up to 76% in Kubernetes, with consistent memory advantages in all studied projects.

Conclusion: While language-specific build tools may offer maintainability benefits, abandoning artifact-based tools like Bazel incurs significant performance and resource costs for large projects, highlighting the trade-off between maintainability and infrastructure demands.

Abstract: Since developers invoke the build system frequently, its performance can
impact productivity. Modern artifact-based build tools accelerate builds, yet
prior work shows that teams may abandon them for alternatives that are easier
to maintain. While prior work shows why downgrades are performed, the
implications of downgrades remain largely unexplored. In this paper, we
describe a case study of the Kubernetes project, focusing on its downgrade from
an artifact-based build tool (Bazel) to a language-specific solution (Go
Build). We reproduce and analyze the full and incremental builds of change sets
during the downgrade period. On the one hand, we find that Bazel builds are
faster than Go Build, completing full builds in 23.06-38.66 up to 75.19 impose
a larger memory footprint than Go Build of 81.42-351.07 respectively. Bazel
builds also impose a greater CPU load at parallelism settings above eight for
full builds and above one for incremental builds. We estimate that downgrading
from Bazel can increase CI resource costs by up to 76 explore whether our
observations generalize by replicating our Kubernetes study on four other
projects that also downgraded from Bazel to older build tools. We observe that
while build time penalties decrease, Bazel consistently consumes more memory.
We conclude that abandoning artifact-based build tools, despite perceived
maintainability benefits, tends to incur considerable performance costs for
large projects. Our observations may help stakeholders to balance trade-offs in
build tool adoption

</details>


### [13] [Developing a Model-Driven Reengineering Approach for Migrating PL/SQL Triggers to Java: A Practical Experience](https://arxiv.org/abs/2510.20121)
*Carlos J. Fernandez-Candel,Jesus Garcia-Molina,Francisco Javier Bermudez Ruiz,Jose Ramon Hoyos Barcelo,Diego Sevilla Ruiz,Benito Jose Cuesta Viera*

Main category: cs.SE

TL;DR: Researchers developed a model-driven MDE approach using KDM models and TDD principles to systematically migrate PL/SQL code from Oracle Forms to tiered Java applications, validated through industrial collaboration and multi-stage verification processes.


<details>
  <summary>Details</summary>
Motivation: Legacy RAD platforms require modernization as enterprises seek to transition from monolithic PL/SQL systems (e.g., Oracle Forms) to tiered Java architectures, necessitating systematic migration approaches.

Method: A model-driven reengineering process integrating Test-Driven Development principles and three-tier validations, utilizing KDM metamodels to represent legacy code and implement incremental model transformations.

Result: Successful implementation of a code migration tool with validated transformation processes, detailed analysis of the reengineering workflow, and evaluation of MDE application challenges in real-world legacy system migration.

Conclusion: The application of model-driven reengineering with TDD-like methodology and multi-stage validations provides an effective approach for migrating legacy PL/SQL applications to modern Java architectures, demonstrating the viability of MDE in complex transformation scenarios.

Abstract: Model-driven software engineering (MDE) techniques are not only useful in
forward engineering scenarios, but can also be successfully applied to evolve
existing systems. RAD (Rapid Application Development) platforms emerged in the
nineties, but the success of modern software technologies motivated that a
large number of enterprises tackled the migration of their RAD applications,
such as Oracle Forms. Our research group has collaborated with a software
company in developing a solution to migrate PL/SQL monolithic code on Forms
triggers and program units to Java code separated in several tiers.
  Our research focused on the model-driven reengineering process applied to
develop the migration tool for the conversion of PL/SQL code to Java. Legacy
code is represented in form of KDM (Knowledge-Discovery Metamodel) models. In
this paper, we propose a software process to implement a model-driven
re-engineering. This process integrates a TDD-like approach to incrementally
develop model transformations with three kinds of validations for the generated
code. The implementation and validation of the re-engineering approach are
explained in detail, as well as the evaluation of some issues related with the
application of MDE.

</details>
