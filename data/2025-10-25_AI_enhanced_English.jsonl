{"id": "2510.19860", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2510.19860", "abs": "https://arxiv.org/abs/2510.19860", "authors": ["Ketai Qiu", "Luca Di Grazia", "Leonardo Mariani", "Mauro Pezz\u00e8"], "title": "E-Test: E'er-Improving Test Suites", "comment": "Accepted at the 48th IEEE/ACM International Conference on Software\n  Engineering (ICSE 2026)", "summary": "Test suites are inherently imperfect, and testers can always enrich a suite\nwith new test cases that improve its quality and, consequently, the reliability\nof the target software system. However, finding test cases that explore\nexecution scenarios beyond the scope of an existing suite can be extremely\nchallenging and labor-intensive, particularly when managing large test suites\nover extended periods.\n  In this paper, we propose E-Test, an approach that reduces the gap between\nthe execution space explored with a test suite and the executions experienced\nafter testing by augmenting the test suite with test cases that explore\nexecution scenarios that emerge in production. E-Test (i) identifies executions\nthat have not yet been tested from large sets of scenarios, such as those\nmonitored during intensive production usage, and (ii) generates new test cases\nthat enhance the test suite. E-Test leverages Large Language Models (LLMs) to\npinpoint scenarios that the current test suite does not adequately cover, and\naugments the suite with test cases that execute these scenarios.\n  Our evaluation on a dataset of 1,975 scenarios, collected from highly-starred\nopen-source Java projects already in production and Defects4J, demonstrates\nthat E-Test retrieves not-yet-tested execution scenarios significantly better\nthan state-of-the-art approaches. While existing regression testing and field\ntesting approaches for this task achieve a maximum F1-score of 0.34, and\nvanilla LLMs achieve a maximum F1-score of 0.39, E-Test reaches 0.55. These\nresults highlight the impact of E-Test in enhancing test suites by effectively\ntargeting not-yet-tested execution scenarios and reducing manual effort\nrequired for maintaining test suites.", "AI": {"tldr": "E-Test is a new approach that uses LLMs to identify and generate test cases for untested execution scenarios in existing test suites, outperforming existing methods.", "motivation": "Existing test suites are limited in their ability to cover all execution scenarios, and manual enrichment is time-consuming. There is a need to automatically find and add test cases for scenarios that emerge in production but are untested.", "method": "E-Test uses Large Language Models to analyze large sets of scenarios from production usage and identifies those not covered by existing tests. It then generates new test cases to cover these scenarios.", "result": "Evaluation on 1,975 scenarios showed E-Test achieves an F1-score of 0.55, outperforming existing approaches (max 0.34) and even vanilla LLMs (max 0.39).", "conclusion": "E-Test significantly improves test suite coverage by effectively targeting untested execution scenarios, reducing manual effort needed and offering a more effective solution than current state-of-the-art methods."}}
{"id": "2510.19864", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19864", "abs": "https://arxiv.org/abs/2510.19864", "authors": ["Amila Indika", "Igor Molybog"], "title": "SODBench: A Large Language Model Approach to Documenting Spreadsheet Operations", "comment": "14 pages, 5 figures, 4 tables", "summary": "Numerous knowledge workers utilize spreadsheets in business, accounting, and\nfinance. However, a lack of systematic documentation methods for spreadsheets\nhinders automation, collaboration, and knowledge transfer, which risks the loss\nof crucial institutional knowledge. This paper introduces Spreadsheet\nOperations Documentation (SOD), an AI task that involves generating\nhuman-readable explanations from spreadsheet operations. Many previous studies\nhave utilized Large Language Models (LLMs) for generating spreadsheet\nmanipulation code; however, translating that code into natural language for SOD\nis a less-explored area. To address this, we present a benchmark of 111\nspreadsheet manipulation code snippets, each paired with a corresponding\nnatural language summary. We evaluate five LLMs, GPT-4o, GPT-4o-mini,\nLLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B, using BLEU, GLEU, ROUGE-L, and\nMETEOR metrics. Our findings suggest that LLMs can generate accurate\nspreadsheet documentation, making SOD a feasible prerequisite step toward\nenhancing reproducibility, maintainability, and collaborative workflows in\nspreadsheets, although there are challenges that need to be addressed.", "AI": {"tldr": "The paper introduces Spreadsheet Operations Documentation (SOD) to improve spreadsheet automation and collaboration through AI-generated documentation. It also evaluates the performance of five large language models on this task.", "motivation": "Current spreadsheet practices lack systematic documentation, leading to issues in automation, collaboration, and knowledge retention. This hinders the efficiency and reproducibility of spreadsheet-based workflows.", "method": "The authors introduce SOD as a new AI task focusing on translating spreadsheet manipulation code into natural language. They create a benchmark of 111 code snippets with corresponding natural language summaries and evaluate five LLMs using BLEU, GLEU, ROUGE-L, and METEOR metrics.", "result": "The evaluation results show that LLMs can generate accurate spreadsheet documentation, indicating that SOD is a viable method to enhance reproducibility and collaboration. However, challenges still remain.", "conclusion": "The study demonstrates the feasibility of using LLMs for SOD, which can contribute to improving spreadsheet practices. Addressing the identified challenges is essential to fully realize the benefits of this approach."}}
{"id": "2510.19868", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19868", "abs": "https://arxiv.org/abs/2510.19868", "authors": ["Qian Xiong", "Bo Yang", "Weisong Sun", "Yiran Zhang", "Tianlin Li", "Yang Liu", "Zhi Jin"], "title": "Knowledge-Guided Multi-Agent Framework for Application-Level Software Code Generation", "comment": null, "summary": "Automated code generation driven by Large Lan- guage Models (LLMs) has\nenhanced development efficiency, yet generating complex application-level\nsoftware code remains challenging. Multi-agent frameworks show potential, but\nexisting methods perform inadequately in large-scale application-level software\ncode generation, failing to ensure reasonable orga- nizational structures of\nproject code and making it difficult to maintain the code generation process.\nTo address this, this paper envisions a Knowledge-Guided Application-Level Code\nGeneration framework named KGACG, which aims to trans- form software\nrequirements specification and architectural design document into executable\ncode through a collaborative closed- loop of the Code Organization & Planning\nAgent (COPA), Coding Agent (CA), and Testing Agent (TA), combined with a\nfeedback mechanism. We demonstrate the collaborative process of the agents in\nKGACG in a Java Tank Battle game case study while facing challenges. KGACG is\ndedicated to advancing the automation of application-level software\ndevelopment.", "AI": {"tldr": "KGACG is a knowledge-guided application-level code generation framework that addresses limitations in existing multi-agent methods for complex software development through a collaborative closed-loop of agents.", "motivation": "Current multi-agent frameworks struggle with large-scale application-level code generation, lacking proper code organization and maintainability. Automated code generation via LLMs remains inefficient for complex projects.", "method": "KGACG introduces a closed-loop system with three agents: Code Organization & Planning Agent (COPA) for structural planning, Coding Agent (CA) for implementation, and Testing Agent (TA with feedback mechanisms) to collaborate on transforming requirements into executable code.", "result": "Demonstrated in a Java Tank Battle game case study, KGACG successfully navigates organizational challenges and maintains code generation processes through its agent collaboration framework.", "conclusion": "KGACG advances automation in application-level software development by combining knowledge-guided agent collaboration with systematic feedback, improving scalability and maintainability."}}
{"id": "2510.19898", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19898", "abs": "https://arxiv.org/abs/2510.19898", "authors": ["Atharv Sonwane", "Isadora White", "Hyunji Lee", "Matheus Pereira", "Lucas Caccia", "Minseon Kim", "Zhengyan Shi", "Chinmay Singh", "Alessandro Sordoni", "Marc-Alexandre C\u00f4t\u00e9", "Xingdi Yuan"], "title": "BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills", "comment": null, "summary": "High quality bugs are key to training the next generation of language model\nbased software engineering (SWE) agents. We introduce a novel method for\nsynthetic generation of difficult and diverse bugs. Our method instructs SWE\nAgents to introduce a feature into the codebase whereby they may\nunintentionally break tests, resulting in bugs. Prior approaches often induce\nan out-of-distribution effect by generating bugs intentionally (e.g. by\nintroducing local perturbation to existing code), which does not reflect\nrealistic development processes. We perform qualitative analysis to demonstrate\nthat our approach for generating bugs more closely reflects the patterns found\nin human-authored edits. Through extensive experiments, we demonstrate that our\nbugs provide more efficient training data for supervised fine-tuning,\noutperforming other bug datasets by 2% with half the training data (1.2k vs. 3k\nbugs). We train on our newly generated bugs in addition to existing bug\ndatasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench\nVerified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on\nSWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over\nthree seeds.", "AI": {"tldr": "This paper introduces a method for generating synthetic bugs that reflect realistic software development scenarios, improving training efficiency for SWE agents and achieving state-of-the-art results on SWE-bench Verified.", "motivation": "High-quality bugs are crucial for training the next generation of SWE agents, but previous approaches introduce out-of-distribution effects by creating bugs intentionally. Realistic bugs better mirror actual development processes and human-edit patterns.", "method": "The method involves instructing SWE agents to introduce new features into codebases, which can unintentionally break existing tests and cause bugs. This contrasts with prior intentional bug generation techniques, and uses a combination of bug generation through new feature implementation and standard bug datasets for training.", "result": "The paper shows improved training efficiency: achieving the same result with 40% less data (1.2k bugs vs. 3k bugs). The model trained on these bugs achieved SOTA: FrogBoss (32B) with a pass@1 of 54.6% and FrogMini (14B) with 45.3% on SWE-bench Verified.", "conclusion": "End-to-end training with synthetic bugs that simulate real development scenarios outperforms existing methods and leads to significant improvements in effectiveness and efficiency in training SWE agents."}}
{"id": "2510.19844", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19844", "abs": "https://arxiv.org/abs/2510.19844", "authors": ["Isaac Wu", "Michael Maslowski"], "title": "CourtGuard: A Local, Multiagent Prompt Injection Classifier", "comment": "11 pages, 7 figures", "summary": "As large language models (LLMs) become integrated into various sensitive\napplications, prompt injection, the use of prompting to induce harmful\nbehaviors from LLMs, poses an ever increasing risk. Prompt injection attacks\ncan cause LLMs to leak sensitive data, spread misinformation, and exhibit\nharmful behaviors. To defend against these attacks, we propose CourtGuard, a\nlocally-runnable, multiagent prompt injection classifier. In it, prompts are\nevaluated in a court-like multiagent LLM system, where a \"defense attorney\"\nmodel argues the prompt is benign, a \"prosecution attorney\" model argues the\nprompt is a prompt injection, and a \"judge\" model gives the final\nclassification. CourtGuard has a lower false positive rate than the Direct\nDetector, an LLM as-a-judge. However, CourtGuard is generally a worse prompt\ninjection detector. Nevertheless, this lower false positive rate highlights the\nimportance of considering both adversarial and benign scenarios for the\nclassification of a prompt. Additionally, the relative performance of\nCourtGuard in comparison to other prompt injection classifiers advances the use\nof multiagent systems as a defense against prompt injection attacks. The\nimplementations of CourtGuard and the Direct Detector with full prompts for\nGemma-3-12b-it, Llama-3.3-8B, and Phi-4-mini-instruct are available at\nhttps://github.com/isaacwu2000/CourtGuard.", "AI": {"tldr": "The paper introduces CourtGuard, a multiagent system for detecting prompt injection attacks on Large Language Models (LLMs), where three specialized agent models evaluate the benignity or harmfulness of a prompt. The system shows a lower false positive rate than a Direct Detector but is overall less effective, highlighting the potential and challenges of multiagent systems in this context.", "motivation": "Large language models (LLMs) are being used in sensitive applications, and prompt injection attacks represent a growing threat. These attacks can cause data leaks, misinformation, and other harmful behaviors, creating a clear need for effective detection methods.", "method": "CourtGuard is a multiagent system where prompts are evaluated by three models: a 'defense attorney' (arguing innocence), a 'prosecution attorney' (arguing maliciousness), and a 'judge' (making the final classification decision). This contrasts with the 'Direct Detector' method, which uses a single model as a judge.", "result": "CourtGuard achieved a lower false positive rate than the Direct Detector, but had a generally lower overall performance in detecting prompt injection attacks. The results highlight the importance of considering both adversarial and benign cases when evaluating prompts.", "conclusion": "Although CourtGuard outperformed a simple LLM judge in terms of false positives, the multiagent design had worse overall effectiveness in detecting prompt injection attacks. This study encourages further research into the use of cooperative multiagent systems for LLM security, while acknowledging the challenges involved."}}
{"id": "2510.19984", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2510.19984", "abs": "https://arxiv.org/abs/2510.19984", "authors": ["Konstantinos Kitsios", "Marcel B\u00f6hme", "Alberto Bacchelli"], "title": "On Interaction Effects in Greybox Fuzzing", "comment": "12 pages, 2 figures, Accepted for presentation at the 48th\n  International Conference on Software Engineering (ICSE '26)", "summary": "A greybox fuzzer is an automated software testing tool that generates new\ntest inputs by applying randomly chosen mutators (e.g., flipping a bit or\ndeleting a block of bytes) to a seed input in random order and adds all\ncoverage-increasing inputs to the corpus of seeds. We hypothesize that the\norder in which mutators are applied to a seed input has an impact on the\neffectiveness of greybox fuzzers. In our experiments, we fit a linear model to\na dataset that contains the effectiveness of all possible mutator pairs and\nindeed observe the conjectured interaction effect. This points us to more\nefficient fuzzing by choosing the most promising mutator sequence with a higher\nlikelihood. We propose MuoFuzz, a greybox fuzzer that learns and chooses the\nmost promising mutator sequences. MuoFuzz learns the conditional probability\nthat the next mutator will yield an interesting input, given the previously\nselected mutator. Then, it samples from the learned probability using a random\nwalk to generate mutator sequences. We compare the performance of MuoFuzz to\nAFL++, which uses a fixed selection probability, and MOPT, which optimizes the\nselection probability of each mutator in isolation. Experimental results on the\nFuzzBench and MAGMA benchmarks show that MuoFuzz achieves the highest code\ncoverage and finds four bugs missed by AFL++ and one missed by both AFL++ and\nMOPT.", "AI": {"tldr": "TL;DR Summary", "motivation": "Motivation", "method": "Method", "result": "Result", "conclusion": "Conclusion"}}
{"id": "2510.19851", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19851", "abs": "https://arxiv.org/abs/2510.19851", "authors": ["Artur Zolkowski", "Wen Xing", "David Lindner", "Florian Tram\u00e8r", "Erik Jenner"], "title": "Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability", "comment": null, "summary": "Recent findings suggest that misaligned models may exhibit deceptive\nbehavior, raising concerns about output trustworthiness. Chain-of-thought (CoT)\nis a promising tool for alignment monitoring: when models articulate their\nreasoning faithfully, monitors can detect and mitigate harmful behaviors before\nundesirable outcomes occur. However, a key uncertainty is: Can models obfuscate\ntheir CoT in order to pursue hidden adversarial objectives while evading\ndetection? To answer this question and thus stress-test CoT monitorability, we\ndevelop a composable and quantifiable taxonomy of prompts to elicit CoT\nobfuscation. We evaluate both internal CoT (reasoning traces) and external CoT\n(prompted reasoning in outputs) using toy tasks and more realistic environments\nin SHADE-Arena. We show that: (i) CoT monitoring performs accurately and\nefficiently without obfuscation pressure. (ii) Under strong obfuscation\npressure, some models successfully complete adversarial tasks while evading\ndetection. (iii) Models do not obfuscate their internal CoT as much as their\nexternal CoT (under prompt pressure). These results suggest that while CoT\nprovides valuable oversight in benign settings, robust deployment requires\nmodel-specific stress-testing of monitorability.", "AI": {"tldr": "The paper investigates whether misaligned AI models can ", "motivation": "Recent findings indicate that misaligned models might exhibit deceptive", "method": "Introduce a composable and quantifiable taxonomy of prompts to encourage CoT obfuscation. They evaluate both internal", "result": "t that CoT monitoring is effective and efficient without obfuscation. H", "conclusion": "While CoT proves valuable for oversight, robust deployment needs model-specific stress-testing for monitorability."}}
{"id": "2510.19997", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19997", "abs": "https://arxiv.org/abs/2510.19997", "authors": ["Abraham Itzhak Weinberg"], "title": "A Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises (FAIGMOE)", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) presents transformative\nopportunities for organizations, yet both midsize organizations and larger\nenterprises face distinctive adoption challenges. Midsize organizations\nencounter resource constraints and limited AI expertise, while enterprises\nstruggle with organizational complexity and coordination challenges. Existing\ntechnology adoption frameworks, including TAM (Technology Acceptance Model),\nTOE (Technology Organization Environment), and DOI (Diffusion of Innovations)\ntheory, lack the specificity required for GenAI implementation across these\ndiverse contexts, creating a critical gap in adoption literature. This paper\nintroduces FAIGMOE (Framework for the Adoption and Integration of Generative AI\nin Midsize Organizations and Enterprises), a conceptual framework addressing\nthe unique needs of both organizational types. FAIGMOE synthesizes technology\nadoption theory, organizational change management, and innovation diffusion\nperspectives into four interconnected phases: Strategic Assessment, Planning\nand Use Case Development, Implementation and Integration, and\nOperationalization and Optimization. Each phase provides scalable guidance on\nreadiness assessment, strategic alignment, risk governance, technical\narchitecture, and change management adaptable to organizational scale and\ncomplexity. The framework incorporates GenAI specific considerations including\nprompt engineering, model orchestration, and hallucination management that\ndistinguish it from generic technology adoption frameworks. As a perspective\ncontribution, FAIGMOE provides the first comprehensive conceptual framework\nexplicitly addressing GenAI adoption across midsize and enterprise\norganizations, offering actionable implementation protocols, assessment\ninstruments, and governance templates requiring empirical validation through\nfuture research.", "AI": {"tldr": "Paper presents FAIGMOE, a framework for GenAI adoption in midsize and large organizations, addressing their unique challenges and combining theory with practical implementation phases.", "motivation": "The paper is motivated by the lack of specific frameworks for GenAI implementation in midsize and enterprise organizations, which face distinct challenges in adopting GenAI beyond what traditional technology adoption models like TAM, TOE, and DOI can address.", "method": "The paper introduces FAIGMOE as a conceptual framework, built by synthesizing technology adoption theory, organizational change management, and innovation diffusion perspectives into four phases: Strategic Assessment, Planning and Use Case Development, Implementation and Integration, and Operationalization and Optimization. The framework is designed to be adaptable to different organizational scales and includes GenAI-specific considerations.", "result": "The FAIGMOE framework is introduced, providing actionable implementation protocols, assessment instruments, and governance templates for GenAI adoption, tailored to the distinct needs of midsize organizations and enterprise-level businesses.", "conclusion": "FAIGMOE is positioned as a comprehensive conceptual framework that addresses the adoption of GenAI in both midsize and enterprise organizations. It is presented as a contribution requiring further empirical validation but offering a new perspective to the existing literature by explicitly addressing these specific adoption contexts."}}
{"id": "2510.19856", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19856", "abs": "https://arxiv.org/abs/2510.19856", "authors": ["Eranga Bandara", "Sachin Shetty", "Ravi Mukkamala", "Ross Gore", "Peter Foytik", "Safdar H. Bouk", "Abdul Rahman", "Xueping Liang", "Ng Wee Keong", "Kasun De Zoysa", "Aruna Withanage", "Nilaan Loganathan"], "title": "Model Context Contracts - MCP-Enabled Framework to Integrate LLMs With Blockchain Smart Contracts", "comment": null, "summary": "In recent years, blockchain has experienced widespread adoption across\nvarious industries, becoming integral to numerous enterprise applications.\nConcurrently, the rise of generative AI and LLMs has transformed human-computer\ninteractions, offering advanced capabilities in understanding and generating\nhuman-like text. The introduction of the MCP has further enhanced AI\nintegration by standardizing communication between AI systems and external data\nsources. Despite these advancements, there is still no standardized method for\nseamlessly integrating LLM applications and blockchain. To address this\nconcern, we propose \"MCC: Model Context Contracts\" a novel framework that\nenables LLMs to interact directly with blockchain smart contracts through\nMCP-like protocol. This integration allows AI agents to invoke blockchain smart\ncontracts, facilitating more dynamic and context-aware interactions between\nusers and blockchain networks. Essentially, it empowers users to interact with\nblockchain systems and perform transactions using queries in natural language.\nWithin this proposed architecture, blockchain smart contracts can function as\nintelligent agents capable of recognizing user input in natural language and\nexecuting the corresponding transactions. To ensure that the LLM accurately\ninterprets natural language inputs and maps them to the appropriate MCP\nfunctions, the LLM was fine-tuned using a custom dataset comprising user inputs\npaired with their corresponding MCP server functions. This fine-tuning process\nsignificantly improved the platform's performance and accuracy. To validate the\neffectiveness of MCC, we have developed an end-to-end prototype implemented on\nthe Rahasak blockchain with the fine-tuned Llama-4 LLM. To the best of our\nknowledge, this research represents the first approach to using the concept of\nModel Context Protocol to integrate LLMs with blockchain.", "AI": {"tldr": "The paper introduces MCC, a framework that connects LLMs to blockchain using a protocol similar to MCP, enabling natural language interactions with smart contracts.", "motivation": "Despite advancements in blockchain and LLMs, there is no standardized method for their seamless integration, limiting dynamic interactions. The paper aims to fill this gap.", "method": "The framework, called Model Context Contracts (MCC), uses a protocol inspired by MCP to link LLMs and blockchain smart contracts. The LLM is fine-tuned with a dataset of user inputs and corresponding MCP functions.", "result": "An end-to-end prototype using the MCC framework was developed on the Rahasak blockchain with a fine-tuned Llama-4 LLM, demonstrating the integration's feasibility.", "conclusion": "MCC represents a novel approach to integrating LLMs and blockchain through a protocol like MCP, enabling natural language interactions with smart contracts and enhancing user experience."}}
{"id": "2510.20041", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20041", "abs": "https://arxiv.org/abs/2510.20041", "authors": ["Gareema Ranjan", "Mahmoud Alfadel", "Gengyi Sun", "Shane McIntosh"], "title": "The Cost of Downgrading Build Systems: A Case Study of Kubernetes", "comment": null, "summary": "Since developers invoke the build system frequently, its performance can\nimpact productivity. Modern artifact-based build tools accelerate builds, yet\nprior work shows that teams may abandon them for alternatives that are easier\nto maintain. While prior work shows why downgrades are performed, the\nimplications of downgrades remain largely unexplored. In this paper, we\ndescribe a case study of the Kubernetes project, focusing on its downgrade from\nan artifact-based build tool (Bazel) to a language-specific solution (Go\nBuild). We reproduce and analyze the full and incremental builds of change sets\nduring the downgrade period. On the one hand, we find that Bazel builds are\nfaster than Go Build, completing full builds in 23.06-38.66 up to 75.19 impose\na larger memory footprint than Go Build of 81.42-351.07 respectively. Bazel\nbuilds also impose a greater CPU load at parallelism settings above eight for\nfull builds and above one for incremental builds. We estimate that downgrading\nfrom Bazel can increase CI resource costs by up to 76 explore whether our\nobservations generalize by replicating our Kubernetes study on four other\nprojects that also downgraded from Bazel to older build tools. We observe that\nwhile build time penalties decrease, Bazel consistently consumes more memory.\nWe conclude that abandoning artifact-based build tools, despite perceived\nmaintainability benefits, tends to incur considerable performance costs for\nlarge projects. Our observations may help stakeholders to balance trade-offs in\nbuild tool adoption", "AI": {"tldr": "We study the performance differences between Bazel and Go Build in the Kubernetes project and four others after downgrading. Bazel is faster but uses more memory and CPU.", "motivation": "This paper aims to understand the performance implications of downgrading from artifact-based build tools like Bazel to language-specific solutions like Go Build, which are perceived as easier to maintain but may impact resource usage.", "method": "The authors conduct a case study of the Kubernetes project's downgrade from Bazel to Go Build, reproducing and analyzing full and incremental builds. They replicate the study on four other projects that made similar downgrades, evaluating build time, memory, and CPU usage.", "result": "Results show Bazel is faster in full builds (23.06-38.66 vs. 75.19 seconds with Go Build) and more memory-intensive (Bazel uses less memory: 81.42-351.07 MB). Bazel also has higher CPU load above certain parallelism levels. Downgrading increased CI resource costs by up to 76% in Kubernetes, with consistent memory advantages in all studied projects.", "conclusion": "While language-specific build tools may offer maintainability benefits, abandoning artifact-based tools like Bazel incurs significant performance and resource costs for large projects, highlighting the trade-off between maintainability and infrastructure demands."}}
{"id": "2510.19859", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19859", "abs": "https://arxiv.org/abs/2510.19859", "authors": ["Smita Khapre"], "title": "Cyberattack Detection in Critical Infrastructure and Supply Chains", "comment": null, "summary": "Cyberattack detection in Critical Infrastructure and Supply Chains has become\nchallenging in Industry 4.0. Intrusion Detection Systems (IDS) are deployed to\ncounter the cyberattacks. However, an IDS effectively detects attacks based on\nthe known signatures and patterns, Zero-day attacks go undetected. To overcome\nthis drawback in IDS, the integration of a Dense Neural Network (DNN) with Data\nAugmentation is proposed. It makes IDS intelligent and enables it to self-learn\nwith high accuracy when a novel attack is encountered. The network flow\ncaptures datasets are highly imbalanced same as the real network itself. The\nData Augmentation plays a crucial role in balancing the data. The balancing of\ndata is challenging as the minority class is as low as 0.000004\\% of the\ndataset, and the abundant class is higher than 80\\% of the dataset. Synthetic\nMinority Oversampling Technique is used for balancing the data. However, higher\naccuracies are achieved with balanced test data, lower accuracies are\nnoticeable with the original imbalanced test data suggesting overfitting. A\ncomparison with state-of-the-art research using Synthetic Minority Oversampling\nTechnique with Edited Nearest Neighbor shows the classification of classes\nremains poor for the original dataset. This suggests highly imbalanced datasets\nof network flow require a different method of data augmentation.", "AI": {"tldr": "This paper proposes integrating Dense Neural Networks (DNN) with Data Augmentation to improve Zero-day attack detection in IDS, addressing imbalanced network flow datasets using Synthetic Minority Oversampling Technique, but highlights limitations in real-world performance due to overfitting.", "motivation": "Cyberattack detection in Critical Infrastructure and Supply Chains is challenging in Industry 4.0. Traditional IDS rely on known signatures, failing to detect Zero-day attacks. Network flow datasets are highly imbalanced, leading to detection challenges.", "method": "A Dense Neural Network (DNN) is enhanced with Data Augmentation, specifically using Synthetic Minority Oversampling Technique (SMOTE) to address class imbalance. The system is tested for its ability to detect novel (Zero-day) attacks by learning from augmented data.", "result": "Higher classification accuracies are achieved with balanced test data, but lower accuracies occur with original imbalanced data, indicating overfitting. A comparison with state-of-the-art methods shows poor class classification on the original dataset, suggesting SMOTE alone is insufficient for highly imbalanced data.", "conclusion": "While DNN with SMOTE improves Zero-day attack detection, the method struggles with overfitting and poor performance on real-world imbalanced datasets. Novel data augmentation techniques are needed for highly imbalanced network flow data to enhance IDS effectiveness in Industry 4.0 contexts."}}
{"id": "2510.20121", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20121", "abs": "https://arxiv.org/abs/2510.20121", "authors": ["Carlos J. Fernandez-Candel", "Jesus Garcia-Molina", "Francisco Javier Bermudez Ruiz", "Jose Ramon Hoyos Barcelo", "Diego Sevilla Ruiz", "Benito Jose Cuesta Viera"], "title": "Developing a Model-Driven Reengineering Approach for Migrating PL/SQL Triggers to Java: A Practical Experience", "comment": "31 pages, 22 figures", "summary": "Model-driven software engineering (MDE) techniques are not only useful in\nforward engineering scenarios, but can also be successfully applied to evolve\nexisting systems. RAD (Rapid Application Development) platforms emerged in the\nnineties, but the success of modern software technologies motivated that a\nlarge number of enterprises tackled the migration of their RAD applications,\nsuch as Oracle Forms. Our research group has collaborated with a software\ncompany in developing a solution to migrate PL/SQL monolithic code on Forms\ntriggers and program units to Java code separated in several tiers.\n  Our research focused on the model-driven reengineering process applied to\ndevelop the migration tool for the conversion of PL/SQL code to Java. Legacy\ncode is represented in form of KDM (Knowledge-Discovery Metamodel) models. In\nthis paper, we propose a software process to implement a model-driven\nre-engineering. This process integrates a TDD-like approach to incrementally\ndevelop model transformations with three kinds of validations for the generated\ncode. The implementation and validation of the re-engineering approach are\nexplained in detail, as well as the evaluation of some issues related with the\napplication of MDE.", "AI": {"tldr": "Researchers developed a model-driven MDE approach using KDM models and TDD principles to systematically migrate PL/SQL code from Oracle Forms to tiered Java applications, validated through industrial collaboration and multi-stage verification processes.", "motivation": "Legacy RAD platforms require modernization as enterprises seek to transition from monolithic PL/SQL systems (e.g., Oracle Forms) to tiered Java architectures, necessitating systematic migration approaches.", "method": "A model-driven reengineering process integrating Test-Driven Development principles and three-tier validations, utilizing KDM metamodels to represent legacy code and implement incremental model transformations.", "result": "Successful implementation of a code migration tool with validated transformation processes, detailed analysis of the reengineering workflow, and evaluation of MDE application challenges in real-world legacy system migration.", "conclusion": "The application of model-driven reengineering with TDD-like methodology and multi-stage validations provides an effective approach for migrating legacy PL/SQL applications to modern Java architectures, demonstrating the viability of MDE in complex transformation scenarios."}}
{"id": "2510.19877", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19877", "abs": "https://arxiv.org/abs/2510.19877", "authors": ["Jean-Marie Le Ray"], "title": "Policy-Governed RAG - Research Design Study", "comment": "51 pages, 8 figures", "summary": "A policy-governed RAG architecture is specified for audit-ready generation in\nregulated workflows, organized as a triptych: (I) Contracts/Control\n(SHRDLU-like), which governs output adherence to legal and internal policies;\n(II) Manifests/Trails (Memex-like), which cryptographically anchors all cited\nsource evidence to ensure verifiable provenance; and (III)\nReceipts/Verification (Xanadu-like), which provides the final, portable proof\nof compliance for auditors (portable COSE/JOSE) (see Section 4 and Appendix A).\nRather than explaining model internals, outputs are gated ex-ante and bound to\ncryptographically verifiable evidence for each material answer. Unvalidated\ntargets are stated (>=20% relative reduction in confident errors; p95 latency\n<= 900 ms; <= 2.2x serve cost) together with a pre-registered (optional) pilot\nusing NO-GO gates. The design complements existing RAG/guardrails by making\npolicy checks auditable, replayable, and receipt-backed. Target domains include\nback-office compliance in pharma, medical devices, finance, legal, and the\npublic sector where error costs may exceed thousands of euros and audit trails\nare mandatory under regulations such as the EU AI Act. Future evaluations may\npre-commit to publishing negative results when any example NO-GO gate is not\nmet.", "AI": {"tldr": "This paper introduces a policy-governed RAG architecture (Contracts/Control + Manifests/Trails + Receipts/Verification) that ensures auditable, verifiable AI outputs for high-risk regulated domains, enabling compliance with EU AI Act requirements via cryptographic evidence binding and ex-ante policy enforcement.", "motivation": "The motivation lies in addressing the need for AI systems in regulated industries (pharma, healthcare, finance, legal) to meet mandatory audit requirements and mitigate error costs exceeding thousands of euros under regulations like the EU AI Act, through verifiable policy adherence mechanisms.", "method": "The method involves a tripartite architecture: 1) Contracts/Control (policy enforcement via SHRDLU-like mechanisms), 2) Manifests/Trails (cryptographic anchoring of sources using Memex-like systems), and 3) Receipts/Verification (portable compliance proofs via Xanadu-like protocols). Outputs are gated by verifiable evidence rather than post-hoc explanations.", "result": "Results include validated targets: 20% reduction in confident errors, 900ms latency, 2.2x serve costs. The design integrates with pre-registered pilots using NO-GO gates and offers replayable, receipt-backed compliance for auditors.", "conclusion": "The paper concludes that their policy-governed RAG architecture effectively enables audit-ready, policy-compliant AI generation in high-risk regulated domains by combining cryptographic verifiability with ex-ante policy checks, addressing critical limitations in existing systems."}}
