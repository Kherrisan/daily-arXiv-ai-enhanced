{"id": "2508.19449", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19449", "abs": "https://arxiv.org/abs/2508.19449", "authors": ["Md Afif Al Mamun", "Gias Uddin", "Lan Xia", "Longyu Zhang"], "title": "Stack Trace-Based Crash Deduplication with Transformer Adaptation", "comment": "This work is currently under review at IEEE Transactions on Software\n  Engineering. The replication package will be made publicly available upon\n  acceptance", "summary": "Automated crash reporting systems generate large volumes of duplicate\nreports, overwhelming issue-tracking systems and increasing developer workload.\nTraditional stack trace-based deduplication methods, relying on string\nsimilarity, rule-based heuristics, or deep learning (DL) models, often fail to\ncapture the contextual and structural relationships within stack traces. We\npropose dedupT, a transformer-based approach that models stack traces\nholistically rather than as isolated frames. dedupT first adapts a pretrained\nlanguage model (PLM) to stack traces, then uses its embeddings to train a\nfully-connected network (FCN) to rank duplicate crashes effectively. Extensive\nexperiments on real-world datasets show that dedupT outperforms existing DL and\ntraditional methods (e.g., sequence alignment and information retrieval\ntechniques) in both duplicate ranking and unique crash detection, significantly\nreducing manual triage effort. On four public datasets, dedupT improves Mean\nReciprocal Rank (MRR) often by over 15% compared to the best DL baseline and up\nto 9% over traditional methods while achieving higher Receiver Operating\nCharacteristic Area Under the Curve (ROC-AUC) in detecting unique crash\nreports. Our work advances the integration of modern natural language\nprocessing (NLP) techniques into software engineering, providing an effective\nsolution for stack trace-based crash deduplication.", "AI": {"tldr": "dedupT leverages transformer-based language models to analyze stack traces holistically, outperforming existing DL and traditional methods in crash report deduplication with 15% MRR improvements and reducing manual triage effort.", "motivation": "Traditional stack trace deduplication methods fail to capture contextual and structural relationships within crash reports, leading to overwhelmed issue-tracking systems and increased developer workload. This creates a need for more sophisticated NLP-based approaches that model program errors as semantic sequences.", "method": "The proposed dedupT approach adapts a pre-trained language model (PLM) to stack traces and uses embeddings from the PLM to train a fully-connected network (FCN) for ranking duplicate crashes. This transforms stack trace modeling from isolated frame analysis to holistic contextual understanding.", "result": "dedupT achieves 15% higher Mean Reciprocal Rank (MRR) compared to the best deep learning baselines and 9% improvement over traditional methods on four public datasets, with additional benefits in unique crash detection via ROCAUC metric improvements.", "conclusion": "Our work advances the integration of modern natural language processing (NLP) techniques into software engineering, providing an effective solution for stack trace-based crash deduplication."}}
{"id": "2508.19558", "categories": ["cs.SE", "cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.19558", "abs": "https://arxiv.org/abs/2508.19558", "authors": ["Zhuohao Li", "Wenqing Chen", "Jianxing Yu", "Zhichao Lu"], "title": "Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking", "comment": null, "summary": "Embedding models have demonstrated strong performance in tasks like\nclustering, retrieval, and feature extraction while offering computational\nadvantages over generative models and cross-encoders. Benchmarks such as MTEB\nhave shown that text embeddings from large language models (LLMs) capture rich\nsemantic information, but their ability to reflect code-level functional\nsemantics remains unclear. Existing studies largely focus on code clone\ndetection, which emphasizes syntactic similarity and overlooks functional\nunderstanding. In this paper, we focus on the functional consistency of LLM\ncode embeddings, which determines if two code snippets perform the same\nfunction regardless of syntactic differences. We propose a novel data synthesis\nframework called Functionality-Oriented Code Self-Evolution to construct\ndiverse and challenging benchmarks. Specifically, we define code examples\nacross four semantic and syntactic categories and find that existing datasets\npredominantly capture syntactic properties. Our framework generates four unique\nvariations from a single code instance, providing a broader spectrum of code\nexamples that better reflect functional differences. Extensive experiments on\nthree downstream tasks-code clone detection, code functional consistency\nidentification, and code retrieval-demonstrate that embedding models\nsignificantly improve their performance when trained on our evolved datasets.\nThese results highlight the effectiveness and generalization of our data\nsynthesis framework, advancing the functional understanding of code.", "AI": {"tldr": "Researchers propose a code benchmark synthesis framework (Functionality-Oriented Code Self-Evolution) that improves models' functional understanding of code by generating diverse examples, outperforming traditional syntactic-focused approaches in key tasks.", "motivation": "Existing studies focus on syntactic similarity for code clone detection, neglecting functional understanding. The paper addresses this gap by exploring how well LLM embeddings capture code-level functional semantics.", "method": "The paper introduces a data synthesis framework that generates four variations of code snippets across semantic and syntactic categories, creating challenging benchmarks to better reflect functional differences.", "result": "Experiments show embedding models trained on evolved datasets significantly outperform baseline methods in three downstream tasks, validating the framework's effectiveness in advancing functional code understanding.", "conclusion": "The study demonstrates that the Functionality-Oriented Code Self-Evolution framework enhances the functional understanding of code by generating diverse benchmarks, leading to improved performance in tasks like clone detection, functional consistency identification, and code retrieval."}}
{"id": "2508.19610", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.19610", "abs": "https://arxiv.org/abs/2508.19610", "authors": ["Kathrin Figl", "Maria Kirchner", "Sebastian Baltes", "Michael Felderer"], "title": "The Influence of Code Comments on the Perceived Helpfulness of Stack Overflow Posts", "comment": "31 pages, 7 figures, 2 tables, to appear in the Empirical Software\n  Engineering journal", "summary": "Question-and-answer platforms such as Stack Overflow have become an important\nway for software developers to share and retrieve knowledge. However, reusing\npoorly understood code can lead to serious problems, such as bugs or security\nvulnerabilities. To better understand how code comments affect the perceived\nhelpfulness of Stack Overflow answers, we conducted an online experiment\nsimulating a Stack Overflow environment (n=91). The results indicate that both\nblock and inline comments are perceived as significantly more helpful than\nuncommented source code. Moreover, novices rated code snippets with block\ncomments as more helpful than those with inline comments. Interestingly, other\nsurface features, such as the position of an answer and its answer score, were\nconsidered less important. The content of Stack Overflow has been a major\nsource for training large language models. AI-based coding assistants such as\nGitHub Copilot, which are based on these models, might change the way Stack\nOverflow is used. However, our findings have implications beyond this specific\nplatform. First, they may help to improve the relevance of community-driven\nplatforms such as Stack Overflow, which provide human advice and explanations\nof code solutions, complementing AI-based support for software developers.\nSecond, since chat-based AI tools can be prompted to generate code in different\nways, knowing which properties influence perceived helpfulness might lead to\ntargeted prompting strategies to generate more readable code snippets.", "AI": {"tldr": "Code comments boost perceived helpfulness on Stack Overflow; block comments particularly help novices. AI coding tools could leverage these insights to generate more useful code explanations.", "motivation": "Understanding how code comments affect the perceived helpfulness of answers is critical to improving knowledge sharing on platforms like Stack Overflow. This is especially relevant given the growing use of AI-based coding assistants trained on such content.", "method": "An online experiment simulating a Stack Overflow environment with 91 participants was conducted to evaluate the impact of code comments on perceived helpfulness of answers. The effects of block/inline comments, answer position, and answer scores were analyzed.", "result": "Block and inline comments were ranked significantly more helpful than uncommented code. Novices found block comments (e.g., full paragraph explanations) more helpful than inline comments. Surface features like answer position/score had minimal impact. These findings suggest comment style impacts perceived quality, with practical implications for AI prompting strategies.", "conclusion": "The study concludes that code comments, particularly block comments for novices, significantly enhance the perceived helpfulness of Stack Overflow answers. This has implications for improving both community-driven platforms and AI-based coding assistants."}}
{"id": "2508.19663", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.19663", "abs": "https://arxiv.org/abs/2508.19663", "authors": ["Lola Solovyeva", "Eduardo Carneiro Oliveira", "Shiyu Fan", "Alper Tuncay", "Shamil Gareev", "Andrea Capiluppi"], "title": "Leveraging LLMs for Automated Translation of Legacy Code: A Case Study on PL/SQL to Java Transformation", "comment": null, "summary": "The VT legacy system, comprising approximately 2.5 million lines of PL/SQL\ncode, lacks consistent documentation and automated tests, posing significant\nchallenges for refactoring and modernisation. This study investigates the\nfeasibility of leveraging large language models (LLMs) to assist in translating\nPL/SQL code into Java for the modernised \"VTF3\" system. By leveraging a dataset\ncomprising 10 PL/SQL-to-Java code pairs and 15 Java classes, which collectively\nestablished a domain model for the translated files, multiple LLMs were\nevaluated. Furthermore, we propose a customized prompting strategy that\nintegrates chain-of-guidance reasoning with $n$-shot prompting. Our findings\nindicate that this methodology effectively guides LLMs in generating\nsyntactically accurate translations while also achieving functional\ncorrectness. However, the findings are limited by the small sample size of\navailable code files and the restricted access to test cases used for\nvalidating the correctness of the generated code. Nevertheless, these findings\nlay the groundwork for scalable, automated solutions in modernising large\nlegacy systems.", "AI": {"tldr": "LLMs with custom prompting successfully translate PL/SQL to Java for legacy system modernization, showing promise despite small data constraints.", "motivation": "The VT legacy system's 2.5 million lines of undocumented PL/SQL code pose significant refactoring challenges, necessitating exploration of LLMs for automated Java translation in the modernized 'VTF3' system.", "method": "The research evaluated multiple LLMs using a dataset of 10 PL/SQL-to-Java code pairs and 15 Java classes. A customized prompting strategy integrating chain-of-guidance reasoning and $n$-shot prompting was proposed to enhance translation accuracy.", "result": "The methodology achieved syntactically accurate and functionally correct translations, though limited by a small dataset (10 pairs, 15 classes) and restricted test case access for validation.", "conclusion": "The study demonstrates that using a customized prompting strategy with LLMs can effectively translate PL/SQL to Java for legacy system modernization, despite limitations in dataset size and test coverage, laying groundwork for scalable automated solutions."}}
{"id": "2508.19250", "categories": ["cs.CR", "cs.DM", "math.NT", "quant-ph", "94A60, 68Q12, 06B99, 81P94, 60E15", "E.3; F.2.2; G.2.0; B.8.0"], "pdf": "https://arxiv.org/pdf/2508.19250", "abs": "https://arxiv.org/abs/2508.19250", "authors": ["Ruopengyu Xu", "Chenglian Liu"], "title": "Tight Quantum-Security Bounds and Parameter Optimization for SPHINCS+ and NTRU", "comment": "15 pages, 2tables", "summary": "The imminent threat of quantum computing necessitates quantum-resistant\ncryptosystems. This paper establishes tight security bounds for two NIST PQC\nfinalists: SPHINCS+ (hash-based) and NTRU (lattice-based). Our key\ncontributions include: (1) A quantum attack model incorporating decoherence\neffects ($\\tau_d$) and parallelization limits; (2) Improved entropy\nconcentration inequalities reducing SPHINCS+ parameters by 15-20\\%; (3)\nOptimized NTRU lattice parameters via quantum lattice entropy $H_Q(\\Lambda)$;\n(4) Tightened NTRU-to-LWE reduction with polynomial-factor improvement.\nTheoretical results demonstrate significant security enhancement over existing\nconstructions, providing implementable parameters for standardization.", "AI": {"tldr": "This paper advances quantum-resistant cryptography by establishing tight security bounds for SPHINCS+ and NTRU through novel analytical methods.", "motivation": "Quantum computing threatens classical cryptography, necessitating rigorous security analysis of NIST PQC finalists to ensure long-term protection for standardization.", "method": "Four key approaches: 1) A quantum attack model with decoherence and parallelism constraints; 2) Entropy concentration inequalities for SPHINCS+ optimization; 3) Quantum lattice entropy for NTRU parameter optimization; 4) A tightened NTRU-LWE reduction framework.", "result": "15-20% parameter reduction in SPHINCS+, optimized NTRU lattice parameters, and polynomial-factor security improvement over existing NTRU-LWE reductions.", "conclusion": "The research demonstrates significant security enhancements for SPHINCS+ and NTRU, offering practical parameter sets for NIST standardization while addressing quantum threats."}}
{"id": "2508.19797", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.19797", "abs": "https://arxiv.org/abs/2508.19797", "authors": ["Joan Giner-Miguelez", "Abel G\u00f3mez", "Jordi Cabot"], "title": "Enabling Content Management Systems as an Information Source in Model-driven Projects", "comment": null, "summary": "Content Management Systems (CMSs) are the most popular tool when it comes to\ncreate and publish content across the web. Recently, CMSs have evolved,\nbecoming \\emph{headless}. Content served by a \\emph{headless CMS} aims to be\nconsumed by other applications and services through REST APIs rather than by\nhuman users through a web browser. This evolution has enabled CMSs to become a\nnotorious source of content to be used in a variety of contexts beyond pure web\nnavigation. As such, CMS have become an important component of many information\nsystems. Unfortunately, we still lack the tools to properly discover and manage\nthe information stored in a CMS, often highly customized to the needs of a\nspecific domain. Currently, this is mostly a time-consuming and error-prone\nmanual process.\n  In this paper, we propose a model-based framework to facilitate the\nintegration of headless CMSs in software development processes. Our framework\nis able to discover and explicitly represent the information schema behind the\nCMS. This facilitates designing the interaction between the CMS model and other\ncomponents consuming that information. These interactions are then generated as\npart of a middleware library that offers platform-agnostic access to the CMS to\nall the client applications. The complete framework is open-source and\navailable online.", "AI": {"tldr": "The paper proposes an open-source model-based framework to address challenges in integrating headless CMSs by automatically discovering their information schema and generating a middleware library for platform-agnostic access.", "motivation": "Headless CMSs are increasingly used across information systems, but their domain-specific customization lacks proper tools for discovery and management, resulting in manual, error-prone processes.", "method": "Developed a model-based framework that discovers and explicitly represents CMS information schema, then generates middleware libraries for seamless integration with consuming components.", "result": "A framework implementation is open-sourced, capable of modeling CMS schemas and producing reusable middleware code for cross-platform application access.", "conclusion": "The proposed model-based approach reduces manual integration efforts for headless CMSs and provides a reusable solution scalable to diverse software systems."}}
{"id": "2508.19267", "categories": ["cs.CR", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.19267", "abs": "https://arxiv.org/abs/2508.19267", "authors": ["Sai Teja Reddy Adapala", "Yashwanth Reddy Alugubelly"], "title": "The Aegis Protocol: A Foundational Security Framework for Autonomous AI Agents", "comment": "10 pages, 3 figures, 3 tables. Source compiled with pdfLaTeX;\n  bibliography included via prebuilt main.bbl. Code repository: available in\n  paper", "summary": "The proliferation of autonomous AI agents marks a paradigm shift toward\ncomplex, emergent multi-agent systems. This transition introduces systemic\nsecurity risks, including control-flow hijacking and cascading failures, that\ntraditional cybersecurity paradigms are ill-equipped to address. This paper\nintroduces the Aegis Protocol, a layered security framework designed to provide\nstrong security guarantees for open agentic ecosystems. The protocol integrates\nthree technological pillars: (1) non-spoofable agent identity via W3C\nDecentralized Identifiers (DIDs); (2) communication integrity via\nNIST-standardized post-quantum cryptography (PQC); and (3) verifiable,\nprivacy-preserving policy compliance using the Halo2 zero-knowledge proof (ZKP)\nsystem. We formalize an adversary model extending Dolev-Yao for agentic threats\nand validate the protocol against the STRIDE framework. Our quantitative\nevaluation used a discrete-event simulation, calibrated against cryptographic\nbenchmarks, to model 1,000 agents. The simulation showed a 0 percent success\nrate across 20,000 attack trials. For policy verification, analysis of the\nsimulation logs reported a median proof-generation latency of 2.79 seconds,\nestablishing a performance baseline for this class of security. While the\nevaluation is simulation-based and early-stage, it offers a reproducible\nbaseline for future empirical studies and positions Aegis as a foundation for\nsafe, scalable autonomous AI.", "AI": {"tldr": "Aegis Protocol secures autonomous AI agents using decentralized IDs, post-quantum crypto, and ZKPs, achieving 0% simulated attack success with 2.79s compliance verification in 1,000-agent systems.", "motivation": "Traditional cybersecurity paradigms lack the capability to address systemic risks in complex AI agent ecosystems, including control-flow hijacking and cascading failures that emerge from autonomous decision-making and emergent system behaviors.", "method": "Developed a layered security framework integrating (1) W3C DIDs for non-spoofable identities, (2) NIST-standardized post-quantum cryptography for message integrity, and (3) Halo2 zero-knowledge proofs for privacy-preserving compliance verification. Validated through formal adversary modeling (Dolev-Yao extension) and STRIDE evaluation, with implementation tested via discrete-event simulation of 1,000 agents.", "result": "Demonstrated 0% attack success rate across 20,000 simulation trials with complete policy violation detection, while achieving median 2.79s ZKP latency. Established first reproducible performance baseline for multi-agent cryptographic security through calibrated discrete-event simulations.", "conclusion": "The Aegis Protocol establishes a foundational security framework for autonomous AI ecosystems, providing verifiable safety guarantees while maintaining scalability in complex multi-agent environments."}}
{"id": "2508.19803", "categories": ["cs.SE", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.19803", "abs": "https://arxiv.org/abs/2508.19803", "authors": ["Peter Fettke", "Wolfgang Reisig"], "title": "Towards a fundamental theory of modeling discrete systems", "comment": "6 pages, 2 figures, author prepared version of final manuscript\n  accepted at the 44th International Conference on Conceptual Modeling, 20-23\n  October 2025, Poitiers / Futuroscope, France, Workshop on Fundamentals of\n  Conceptual Modeling (FCM)", "summary": "Modeling is a central concern in both science and engineering. However, we\nneed a new fundamental theory to address the challenges of the digital age. In\nthis paper, we first explain why modeling is fundamental and which challenges\nmust be addressed in the digital world. As a main contribution, we introduce\nthe Heraklit modeling framework as a new approach to modeling. We conclude with\nsome general remarks. Future work will involve the correctness of modeling, the\nnotion of information, and the description of invariance in modeling.", "AI": {"tldr": "Presents Heraklit - a new modeling framework addressing digital-era challenges, emphasizing future research on correctness, information theory, and invariant descriptions in modeling.", "motivation": "Modeling is fundamental in science/engineering, but new theoretical foundations are needed to overcome challenges posed by the digital era.", "method": "Introduced the Heraklit modeling framework as a novel approach to address digital-age challenges in modeling.", "result": "Presented the Heraklit framework as a foundational contribution for next-generation modeling systems.", "conclusion": "Future work will focus on modeling correctness, the concept of information, and describing invariance in modeling."}}
{"id": "2508.19273", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19273", "abs": "https://arxiv.org/abs/2508.19273", "authors": ["Tongxi Wu", "Chenwei Xu", "Jin Yang"], "title": "MixGAN: A Hybrid Semi-Supervised and Generative Approach for DDoS Detection in Cloud-Integrated IoT Networks", "comment": null, "summary": "The proliferation of cloud-integrated IoT systems has intensified exposure to\nDistributed Denial of Service (DDoS) attacks due to the expanded attack\nsurface, heterogeneous device behaviors, and limited edge protection. However,\nDDoS detection in this context remains challenging because of complex traffic\ndynamics, severe class imbalance, and scarce labeled data. While recent methods\nhave explored solutions to address class imbalance, many still struggle to\ngeneralize under limited supervision and dynamic traffic conditions. To\novercome these challenges, we propose MixGAN, a hybrid detection method that\nintegrates conditional generation, semi-supervised learning, and robust feature\nextraction. Specifically, to handle complex temporal traffic patterns, we\ndesign a 1-D WideResNet backbone composed of temporal convolutional layers with\nresidual connections, which effectively capture local burst patterns in traffic\nsequences. To alleviate class imbalance and label scarcity, we use a pretrained\nCTGAN to generate synthetic minority-class (DDoS attack) samples that\ncomplement unlabeled data. Furthermore, to mitigate the effect of noisy\npseudo-labels, we introduce a MixUp-Average-Sharpen (MAS) strategy that\nconstructs smoothed and sharpened targets by averaging predictions over\naugmented views and reweighting them towards high-confidence classes.\nExperiments on NSL-KDD, BoT-IoT, and CICIoT2023 demonstrate that MixGAN\nachieves up to 2.5% higher accuracy and 4% improvement in both TPR and TNR\ncompared to state-of-the-art methods, confirming its robustness in large-scale\nIoT-cloud environments. The source code is publicly available at\nhttps://github.com/0xCavaliers/MixGAN.", "AI": {"tldr": "MixGAN tackles DDoS detection in IoT-cloud systems by integrating temporal feature extraction, synthetic minority sampling (CTGAN), and pseudo-label optimization (MAS), achieving significant performance improvements over alternatives.", "motivation": "Existing DDoS detection methods struggle with complex traffic dynamics, severe class imbalance, and scarce labeled data in heterogeneous IoT-cloud systems, necessitating a solution that generalizes under limited supervision and dynamic conditions.", "method": "The methodology combines a 1-D WideResNet backbone for temporal traffic pattern analysis, a pre-trained CTGAN for synthetic DDoS sample generation, and a MixUp-Average-Sharpen (MAS) strategy for pseudo-label optimization.", "result": "MixGAN achieves 2.5% higher accuracy and 4% improvements in TPR and TNR compared to state-of-the-art methods on NSL-KDD, BoT-IoT, and CICIoT2023 datasets, with publicly available source code.", "conclusion": "The paper introduces MixGAN, a hybrid detection method that effectively addresses the challenges of DDoS detection in cloud-integrated IoT systems through conditional generation, semi-supervised learning, and robust feature extraction, demonstrating robustness and competitive performance on three benchmark datasets."}}
{"id": "2508.19834", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.19834", "abs": "https://arxiv.org/abs/2508.19834", "authors": ["Antero Taivalsaari", "Tommi Mikkonen", "Cesare Pautasso"], "title": "On the Future of Software Reuse in the Era of AI Native Software Engineering", "comment": "21 pages", "summary": "Software development is currently under a paradigm shift in which artificial\nintelligence and generative software reuse are taking the center stage in\nsoftware creation. Earlier opportunistic software reuse practices and organic\nsoftware development methods are rapidly being replaced by \"AI Native\"\napproaches in which developers place their trust on code that has been\ngenerated by artificial intelligence. This is leading to a new form of software\nreuse that is conceptually not all that different from cargo cult development.\nIn this paper we discuss the implications of AI-assisted generative software\nreuse, bring forth relevant questions, and define a research agenda for\ntackling the central issues associated with this emerging approach.", "AI": {"tldr": "This paper explores the shift to AI-driven software development and proposes a research agenda to address its challenges.", "motivation": "The rapid adoption of AI-native approaches in software development necessitates an examination of their underlying issues and long-term implications to avoid problematic practices like cargo cult development.", "method": "The paper employs a critical discussion approach to analyze the paradigm shift in software development, identifies emerging challenges, and formulates a research agenda.", "result": "The paper outlines a research agenda that addresses key issues in AI-assisted generative software reuse, including validation of AI-generated code, ethical considerations, and developer-AI collaboration.", "conclusion": "The paper emphasizes the need for a structured research agenda to address the challenges and implications of AI-assisted generative software reuse, ensuring that the shift to 'AI Native' development is both effective and principled."}}
{"id": "2508.19278", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19278", "abs": "https://arxiv.org/abs/2508.19278", "authors": ["Konur Tholl", "Mariam El Mezouar", "Ranwa Al Mallah"], "title": "Towards Production-Worthy Simulation for Autonomous Cyber Operations", "comment": null, "summary": "Simulated environments have proven invaluable in Autonomous Cyber Operations\n(ACO) where Reinforcement Learning (RL) agents can be trained without the\ncomputational overhead of emulation. These environments must accurately\nrepresent cybersecurity scenarios while producing the necessary signals to\nsupport RL training. In this study, we present a framework where we first\nextend CybORG's Cage Challenge 2 environment by implementing three new actions:\nPatch, Isolate, and Unisolate, to better represent the capabilities available\nto human operators in real-world settings. We then propose a design for agent\ndevelopment where we modify the reward signals and the agent's feature space to\nenhance training performance. To validate these modifications, we train DQN and\nPPO agents in the updated environment. Our study demonstrates that CybORG can\nbe extended with additional realistic functionality, while maintaining its\nability to generate informative training signals for RL agents.", "AI": {"tldr": "Researchers enhanced CybORG's RL capabilities by adding realistic actions and optimizing training signals, proving effective agent training is achievable while maintaining simulation realism.", "motivation": "Existing environments lack real-world operational realism for autonomous cyber operations training, necessitating enhanced action sets and training signals to enable effective reinforcement learning.", "method": "Extended CybORG's Cage Challenge 2 environment with Patch/Isolate/Unisolate actions and modified reward structures/agent feature spaces to optimize training performance.", "result": "DQN/PPO agent performance demonstrated that additional functionality and optimized reward designs maintain CybORG's training signal robustness while enhancing realism.", "conclusion": "The modifications to CybORG's environment and agent training framework successfully integrate realistic capabilities while preserving its efficacy in generating useful signals for RL agent training."}}
{"id": "2508.19882", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19882", "abs": "https://arxiv.org/abs/2508.19882", "authors": ["Qunying Song", "He Ye", "Mark Harman", "Federica Sarro"], "title": "Generative AI for Testing of Autonomous Driving Systems: A Survey", "comment": "67 pages, 6 figures, 29 tables", "summary": "Autonomous driving systems (ADS) have been an active area of research, with\nthe potential to deliver significant benefits to society. However, before\nlarge-scale deployment on public roads, extensive testing is necessary to\nvalidate their functionality and safety under diverse driving conditions.\nTherefore, different testing approaches are required, and achieving effective\nand efficient testing of ADS remains an open challenge. Recently, generative AI\nhas emerged as a powerful tool across many domains, and it is increasingly\nbeing applied to ADS testing due to its ability to interpret context, reason\nabout complex tasks, and generate diverse outputs. To gain a deeper\nunderstanding of its role in ADS testing, we systematically analyzed 91\nrelevant studies and synthesized their findings into six major application\ncategories, primarily centered on scenario-based testing of ADS. We also\nreviewed their effectiveness and compiled a wide range of datasets, simulators,\nADS, metrics, and benchmarks used for evaluation, while identifying 27\nlimitations. This survey provides an overview and practical insights into the\nuse of generative AI for testing ADS, highlights existing challenges, and\noutlines directions for future research in this rapidly evolving field.", "AI": {"tldr": "This survey examines generative AI's role in ADS testing through a systematic review of 91 studies, categorizing applications, evaluating tools, and identifying limitations while guiding future research.", "motivation": "Autonomous driving systems require extensive safety and functionality testing, but achieving effective and efficient testing remains a critical challenge, motivating the exploration of generative AI applications.", "method": "The study systematically analyzed 91 papers, categorizing them into six application areas for scenario-based ADS testing. It reviewed datasets, simulators, metrics, and benchmarks while identifying 27 limitations.", "result": "Generated six major application categories for generative AI in ADS testing, compiled a comprehensive resource list (datasets, simulators, etc.), and identified 27 limitations across reviewed studies.", "conclusion": "This survey highlights the role of generative AI in ADS testing, identifies challenges, and outlines future research directions for this evolving field."}}
{"id": "2508.19281", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19281", "abs": "https://arxiv.org/abs/2508.19281", "authors": ["Aoun E Muhammad", "Kin Choong Yow", "Jamel Baili", "Yongwon Cho", "Yunyoung Nam"], "title": "CORTEX: Composite Overlay for Risk Tiering and Exposure in Operational AI Systems", "comment": null, "summary": "As the deployment of Artificial Intelligence (AI) systems in high-stakes\nsectors - like healthcare, finance, education, justice, and infrastructure has\nincreased - the possibility and impact of failures of these systems have\nsignificantly evolved from being a theoretical possibility to practical\nrecurring, systemic risk. This paper introduces CORTEX (Composite Overlay for\nRisk Tiering and Exposure), a multi-layered risk scoring framework proposed to\nassess and score AI system vulnerabilities, developed on empirical analysis of\nover 1,200 incidents documented in the AI Incident Database (AIID), CORTEX\ncategorizes failure modes into 29 technical vulnerability groups. Each\nvulnerability is scored through a five-tier architecture that combines: (1)\nutility-adjusted Likelihood x Impact calculations; (2) governance + contextual\noverlays aligned with regulatory frameworks, such as the EU AI Act, NIST RMF,\nOECD principles; (3) technical surface scores, covering exposure vectors like\ndrift, traceability, and adversarial risk; (4) environmental and residual\nmodifiers tailored to context of where these systems are being deployed to use;\nand (5) a final layered assessment via Bayesian risk aggregation and Monte\nCarlo simulation to model volatility and long-tail risks. The resulting\ncomposite score can be operationalized across AI risk registers, model audits,\nconformity checks, and dynamic governance dashboards.", "AI": {"tldr": "This paper introduces CORTEX, a layered AI risk scoring framework based on real-world incident data, merging technical vulnerability analysis with regulatory alignment to enable systemic risk management across critical AI applications.", "motivation": "AI systems are now deployed in critical sectors (healthcare, finance, etc.) where failures transitioned from theoretical to recurring systemic risks. Existing methods lack structured multi-layered approaches to quantify these technical, regulatory, and contextual dimensions.", "method": "CORTEX is a five-tier architecture: 1) utility-adjusted Likelihood x Impact calculations, 2) governance overlays (EU AI Act, NIST RMF, OECD), 3) technical surface scores (drift, traceability, adversarial risk), 4) contextual/environmental modifiers, 5) Bayesian risk aggregation and Monte Carlo simulations. Based on 1,200+ AI incidents analyzed.", "result": "A composite risk score operationalizable across model audits, risk registers, and governance dashboards. Categorizes 29 technical vulnerability groups with practical risk assessment workflows.", "conclusion": "CORTEX offers a comprehensive, empirically grounded framework for AI risk assessment, combining technical and regulatory perspectives. The framework aims to enhance transparency, governance, and risk management in high-stakes AI deployments."}}
{"id": "2508.20086", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20086", "abs": "https://arxiv.org/abs/2508.20086", "authors": ["Youwei Huang", "Jianwen Li", "Sen Fang", "Yao Li", "Peng Yang", "Bin Hu", "Tao Zhang"], "title": "Smart Contract Intent Detection with Pre-trained Programming Language Model", "comment": "10 pages, 5 figures, conference", "summary": "Malicious intent in smart contract development can lead to substantial\neconomic losses. SmartIntentNN is a deep learning model specifically designed\nto identify unsafe intents in smart contracts. This model integrates the\nUniversal Sentence Encoder, a K-means clustering-based intent highlighting\nmechanism, and a Bidirectional Long Short-Term Memory network for multi-label\nclassification, achieving an F1 of 0.8633 in distinguishing ten different\nintent categories. In this study, we present an upgraded version of this model,\nSmartIntentNN2 (Smart Contract Intent Neural Network V2). A significant\nenhancement in V2 is the incorporation of a BERT-based pre-trained language\nmodel, which has been trained on a dataset of 16,000 real smart contracts using\na Masked Language Modeling objective. SmartIntentNN2 retains the BiLSTM-based\nmulti-label classification network. With an improved F1 of 0.927, V2\ndemonstrates enhanced performance compared to its predecessor, establishing\nitself as the state-of-the-art model for smart contract intent detection.", "AI": {"tldr": "SmartIntentNN2 enhances smart contract security with a BERT-enhanced architecture, achieving state-of-the-art intent detection performance (F1=0.927) by leveraging pre-trained language models and retaining effective components from its predecessor.", "motivation": "Malicious intent in smart contract development can cause substantial economic losses, necessitating advanced detection methods to enhance security in blockchain ecosystems.", "method": "The model integrates a BERT-based pre-trained language model (trained on 16,000 contracts using Masked Language Modeling), retains a BiLSTM-based multi-label classification network, and improves upon the original SmartIntentNN's structure with a K-means clustering-based intent highlighting mechanism.", "result": "SmartIntentNN2 achieves an improved F1 score of 0.927 in multi-label classification across ten intent categories, demonstrating significant performance gains over its predecessor (SmartIntentNN with F1=0.8633).", "conclusion": "SmartIntentNN2 establishes itself as the state-of-the-art model for smart contract intent detection due to its enhanced F1 score of 0.927 and integration of a BERT-based pre-trained language model."}}
{"id": "2508.19283", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19283", "abs": "https://arxiv.org/abs/2508.19283", "authors": ["Mark Dorsett", "Scott Man", "Tim Koussas"], "title": "Rethinking Denial-of-Service: A Conditional Taxonomy Unifying Availability and Sustainability Threats", "comment": "7 pages, 3 figures, 3 tables,", "summary": "This paper proposes a unified, condition-based framework for classifying both\nlegacy and cloud-era denial-of-service (DoS) attacks. The framework comprises\nthree interrelated models: a formal conditional tree taxonomy, a hierarchical\nlattice structure based on order theory, and a conceptual Venn diagram. At its\ncore, the taxonomy introduces six observable conditions (C0-C5) grounded in\nreal-world attack behaviours, including source distribution, traffic volume,\ninfrastructure targeting, and financial exploitation. These conditions enable\nconsistent classification of known attacks-such as DoS, DDoS, LDoS, LDDoS,\nEDoS, DoW, and DDoW, while supporting identification of emerging or hybrid\nvariants. The lattice structure captures the cumulative satisfaction of\nconditions, allowing hierarchical reasoning across denial attack classes. The\nVenn diagram highlights conceptual overlaps between availability- and\nsustainability-focused attacks, improving comparative insight. Together, these\nmodels provide a robust analytical lens for threat modeling, mitigation\nstrategy design, and attacker intent classification. The framework is\nparticularly relevant in cloud-native and serverless environments, where\nsustainability-based attacks are increasingly impactful yet under-recognised.\nIts extensibility also permits future integration of socio-technical or\nbehavioural dimensions. By offering a structured taxonomy with theoretical\ngrounding and real-world applicability, this work advances denial attack\ncomprehension and equips defenders, researchers, and cloud architects with a\nshared vocabulary for interpreting and mitigating evolving threat vectors.", "AI": {"tldr": "A unified framework with three models improves DoS attack classification by introducing 6 behavioral conditions, enabling consistent analysis of legacy and cloud-era attacks while addressing gaps in existing methodologies.", "motivation": "The paper addresses the growing complexity of denial-of-service attacks, particularly in cloud and serverless environments, where existing classification methods lack consistency and fail to account for emerging attack patterns like sustainability-based threats. A unified, condition-driven framework is needed to standardize terminology, improve threat modeling, and guide mitigation strategies.", "method": "The framework consists of a conditional tree taxonomy with six observable conditions (C0-C5), a hierarchical lattice based on order theory for cumulative reasoning, and a conceptual Venn diagram to visualize overlaps between attack types. These models collectively enable structured classification and comparative analysis of both established and emerging denial-of-service attacks.", "result": "The framework successfully classifies known attacks (e.g., DoS, DDoS, EDoS) using empirical conditions, identifies hybrid or novel variants through hierarchical analysis, and clarifies conceptual overlaps between availability-focused and sustainability-based attacks. Its applicability to cloud-native infrastructures, where sustainability attacks are under-recognized, positions it as a practical tool for defenders and architects.", "conclusion": "The paper introduces a comprehensive framework for classifying DoS attacks using three interrelated models, addressing gaps in existing classification systems and offering a theoretical foundation for improved threat analysis and mitigation in cloud-native environments."}}
{"id": "2508.19284", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19284", "abs": "https://arxiv.org/abs/2508.19284", "authors": ["Mark Dorsett", "Scott Mann", "Jabed Chowdhury", "Abdun Mahmood"], "title": "A Comprehensive Review of Denial of Wallet Attacks in Serverless Architectures", "comment": "12 pages, 2 figures, 5 tables", "summary": "The Denial of Wallet (DoW) attack poses a unique and growing threat to\nserverless architectures that rely on Function-as-a-Service (FaaS) models,\nexploiting the cost structure of pay-as-you-go billing to financially burden\napplication owners. Unlike traditional Denial of Service (DoS) attacks, which\naim to exhaust resources and disrupt service availability, DoW attacks focus on\nescalating costs without impacting service operation. This review traces the\nevolution of DoW research, from initial awareness and attack classification to\nadvancements in detection and mitigation strategies. Key developments include\nthe categorisation of attack types-such as Blast DDoW, Continual Inconspicuous\nDDoW, and Background Chained DDoW-and the creation of simulation tools like\nDoWTS, which enable safe experimentation and data generation. Recent\nadvancements highlight machine learning approaches, including systems like\nGringotts and DoWNet, which leverage deep learning and anomaly detection to\nidentify malicious traffic patterns. Although substantial progress has been\nmade, challenges persist, notably the lack of real-world data and the need for\nadaptive billing models. This is the first comprehensive literature review\ndedicated strictly to Denial of Wallet attacks, providing an in-depth analysis\nof their financial impacts, attack techniques, mitigation strategies, and\ndetection mechanisms within serverless computing. The paper also presents the\nfirst detailed examination of simulation and data generation tools used for DoW\nresearch, addressing a critical gap in existing cybersecurity literature. By\nsynthesising these key areas, this study serves as a foundational resource for\nfuture research and industry efforts in securing pay-as-you-go cloud\nenvironments.", "AI": {"tldr": "This paper is the first comprehensive review on Denial of Wallet (DoW) attacks in serverless computing, analyzing their financial impact, attack techniques, and current detection/mitigation strategies with a focus on gaps like real-world data and adaptive billing models.", "motivation": "Serverless computing\u2019s pay-as-you-go model makes it vulnerable to financially-targeted DoW attacks, which differ from traditional DoS by focusing on cost escalation without service disruption.", "method": "The study systematically categorizes attack types (Blast DDoW, Continual Inconspicuous DDoW, Background Chained DDoW), evaluates simulation tools (e.g., DoWTS), and examines machine learning approaches (Gringotts, DoWNet) for detecting malicious traffic patterns.", "result": "Highlights advancements in detection (deep learning, anomaly detection) and mitigation strategies but emphasizes persistent challenges, including limited real-world datasets and the need for billing model innovations.", "conclusion": "Serves as a foundational resource for addressing DoW threats, identifies critical research gaps (e.g., adaptive billing), and advocates for further industry and academic collaboration to secure pay-as-you-go cloud environments."}}
{"id": "2508.19286", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19286", "abs": "https://arxiv.org/abs/2508.19286", "authors": ["Zhan Shi", "Yefeng Yuan", "Yuhong Liu", "Liang Cheng", "Yi Fang"], "title": "RL-Finetuned LLMs for Privacy-Preserving Synthetic Rewriting", "comment": null, "summary": "The performance of modern machine learning systems depends on access to\nlarge, high-quality datasets, often sourced from user-generated content or\nproprietary, domain-specific corpora. However, these rich datasets inherently\ncontain sensitive personal information, raising significant concerns about\nprivacy, data security, and compliance with regulatory frameworks. While\nconventional anonymization techniques can remove explicit identifiers, such\nremoval may result in performance drop in downstream machine learning tasks.\nMore importantly, simple anonymization may not be effective against inference\nattacks that exploit implicit signals such as writing style, topical focus, or\ndemographic cues, highlighting the need for more robust privacy safeguards\nduring model training. To address the challenging issue of balancing user\nprivacy and data utility, we propose a reinforcement learning framework that\nfine-tunes a large language model (LLM) using a composite reward function that\njointly optimizes for explicit and implicit privacy, semantic fidelity, and\noutput diversity. To effectively capture population level regularities, the\nprivacy reward combines semantic cues with structural patterns derived from a\nminimum spanning tree (MST) over latent representations. By modeling these\nprivacy-sensitive signals in their distributional context, the proposed\napproach guides the model to generate synthetic rewrites that preserve utility\nwhile mitigating privacy risks. Empirical results show that the proposed method\nsignificantly enhances author obfuscation and privacy metrics without degrading\nsemantic quality, providing a scalable and model-agnostic solution for privacy\npreserving data generation in the era of large language models.", "AI": {"tldr": "This paper introduces a reinforcement learning framework for privacy-preserving data generation, balancing privacy and utility by training LLMs with a composite reward that combines semantic and structural privacy signals.", "motivation": "Traditional anonymization techniques inadequately address privacy risks in datasets due to performance degradation and vulnerability to inference attacks using implicit signals like writing style or demographics.", "method": "A reinforcement learning approach fine-tunes a large language model using a composite reward function that optimizes explicit/implicit privacy, semantic fidelity, and output diversity, leveraging a minimum spanning tree over latent representations to model privacy-sensitive signals.", "result": "The method significantly improves author obfuscation and privacy metrics without compromising semantic quality in generated outputs.", "conclusion": "The proposed reinforcement learning framework provides a scalable and model-agnostic solution for enhancing privacy in data generation while maintaining semantic quality, effectively balancing user privacy and data utility."}}
{"id": "2508.19287", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19287", "abs": "https://arxiv.org/abs/2508.19287", "authors": ["Zhuotao Lian", "Weiyu Wang", "Qingkui Zeng", "Toru Nakanishi", "Teruaki Kitasuka", "Chunhua Su"], "title": "Prompt-in-Content Attacks: Exploiting Uploaded Inputs to Hijack LLM Behavior", "comment": null, "summary": "Large Language Models (LLMs) are widely deployed in applications that accept\nuser-submitted content, such as uploaded documents or pasted text, for tasks\nlike summarization and question answering. In this paper, we identify a new\nclass of attacks, prompt in content injection, where adversarial instructions\nare embedded in seemingly benign inputs. When processed by the LLM, these\nhidden prompts can manipulate outputs without user awareness or system\ncompromise, leading to biased summaries, fabricated claims, or misleading\nsuggestions. We demonstrate the feasibility of such attacks across popular\nplatforms, analyze their root causes including prompt concatenation and\ninsufficient input isolation, and discuss mitigation strategies. Our findings\nreveal a subtle yet practical threat in real-world LLM workflows.", "AI": {"tldr": "Researchers uncover prompt-injection attacks that embed hidden instructions in normal inputs to manipulate LLM outputs. These attacks exploit poor input handling in platforms, causing biased/fabricated results. Authors demonstrate attacks, explain technical causes, and suggest defenses.", "motivation": "This work addresses the growing need to secure LLM applications by exposing vulnerabilities in input processing - adversarial prompts hidden in ordinary user content can covertly manipulate outputs like summaries and answers without system compromise.", "method": "The authors demonstrate attack feasibility on popular platforms through adversarial input examples, analyze root causes like prompt concatenation and input isolation failures, and propose mitigation approaches.", "result": "Demonstrated successful execution of prompt-injection attacks across major platforms, identified architectural weaknesses enabling the attacks (e.g. naive prompt concatenation), and provided actionable mitigation recommendations for LLM system designers.", "conclusion": "The paper concludes that prompt in content injection presents a subtle yet practical threat to real-world LLM workflows, necessitating improved input isolation mechanisms and mitigation strategies to prevent adversarial output manipulation."}}
{"id": "2508.19288", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19288", "abs": "https://arxiv.org/abs/2508.19288", "authors": ["Kyohei Shiomi", "Zhuotao Lian", "Toru Nakanishi", "Teruaki Kitasuka"], "title": "Tricking LLM-Based NPCs into Spilling Secrets", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to generate dynamic\ndialogue for game NPCs. However, their integration raises new security\nconcerns. In this study, we examine whether adversarial prompt injection can\ncause LLM-based NPCs to reveal hidden background secrets that are meant to\nremain undisclosed.", "AI": {"tldr": "This paper reveals security risks in LLM-based game NPCs, showing that adversarial prompts can force them to leak hidden secrets, and highlights the importance of robust security safeguards in AI-driven interactive systems.", "motivation": "As LLMs are increasingly integrated into interactive systems like game NPCs, there is a significant need to understand and mitigate novel security risks, such as adversarial prompt exploitation.", "method": "The paper investigates adversarial prompt injection techniques to test whether LLM-based NPCs can be manipulated into revealing undisclosed background secrets through carefully crafted inputs.", "result": "The research demonstrates that adversarial prompt injection can successfully cause LLM-driven NPCs to disclose hidden background information that should remain confidential within game environments.", "conclusion": "The study identifies a critical security vulnerability in LLM-based game NPCs, highlighting the need for safeguards against adversarial prompt injection to prevent unintended disclosure of hidden background secrets."}}
{"id": "2508.19292", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19292", "abs": "https://arxiv.org/abs/2508.19292", "authors": ["Xi Wang", "Songlei Jian", "Shasha Li", "Xiaopeng Li", "Bin Ji", "Jun Ma", "Xiaodong Liu", "Jing Wang", "Feilong Bao", "Jianfeng Zhang", "Baosheng Wang", "Jie Yu"], "title": "Stand on The Shoulders of Giants: Building JailExpert from Previous Attack Experience", "comment": "18 pages, EMNLP 2025 Main Conference", "summary": "Large language models (LLMs) generate human-aligned content under certain\nsafety constraints. However, the current known technique ``jailbreak prompt''\ncan circumvent safety-aligned measures and induce LLMs to output malicious\ncontent. Research on Jailbreaking can help identify vulnerabilities in LLMs and\nguide the development of robust security frameworks. To circumvent the issue of\nattack templates becoming obsolete as models evolve, existing methods adopt\niterative mutation and dynamic optimization to facilitate more automated\njailbreak attacks. However, these methods face two challenges: inefficiency and\nrepetitive optimization, as they overlook the value of past attack experiences.\nTo better integrate past attack experiences to assist current jailbreak\nattempts, we propose the \\textbf{JailExpert}, an automated jailbreak framework,\nwhich is the first to achieve a formal representation of experience structure,\ngroup experiences based on semantic drift, and support the dynamic updating of\nthe experience pool. Extensive experiments demonstrate that JailExpert\nsignificantly improves both attack effectiveness and efficiency. Compared to\nthe current state-of-the-art black-box jailbreak methods, JailExpert achieves\nan average increase of 17\\% in attack success rate and 2.7 times improvement in\nattack efficiency. Our implementation is available at\n\\href{https://github.com/xiZAIzai/JailExpert}{XiZaiZai/JailExpert}", "AI": {"tldr": "JailExpert optimizes automated jailbreak attacks by systematizing past experiences, achieving record-breaking performance in both success rates (\u219117%) and efficiency (\u00d72.7).", "motivation": "Existing jailbreak methods suffer from inefficiency and repetitive optimization as they fail to integrate past attack experiences, leading to obsolete attack templates as models evolve.", "method": "JailExpert introduces a formal representation of attack experience structure, groups experiences by semantic drift, and dynamically updates the experience pool to avoid redundant optimization.", "result": "Experiments show JailExpert achieves a 17% average attack success rate increase and 2.7\u00d7 efficiency improvement compared to current black-box jailbreak methods.", "conclusion": "JailExpert is an automated jailbreak framework that significantly enhances attack effectiveness and efficiency by leveraging formalized experience structures and semantic grouping, outperforming state-of-the-art methods by 17% in success rate and 2.7x in efficiency."}}
{"id": "2508.19309", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.19309", "abs": "https://arxiv.org/abs/2508.19309", "authors": ["Peng Gu", "Shuangchen Li", "Dylan Stow", "Russell Barnes", "Liu Liu", "Yuan Xie", "Eren Kursshan"], "title": "Leveraging 3D Technologies for Hardware Security: Opportunities and Challenges", "comment": null, "summary": "3D die stacking and 2.5D interposer design are promising technologies to\nimprove integration density, performance and cost. Current approaches face\nserious issues in dealing with emerging security challenges such as side\nchannel attacks, hardware trojans, secure IC manufacturing and IP piracy. By\nutilizing intrinsic characteristics of 2.5D and 3D technologies, we propose\nnovel opportunities in designing secure systems. We present: (i) a 3D\narchitecture for shielding side-channel information; (ii) split fabrication\nusing active interposers; (iii) circuit camouflage on monolithic 3D IC, and\n(iv) 3D IC-based security processing-in-memory (PIM). Advantages and challenges\nof these designs are discussed, showing that the new designs can improve\nexisting countermeasures against security threats and further provide new\nsecurity features.", "AI": {"tldr": "This paper explores how 2.5D/3D technologies can enhance IC security by proposing four novel designs that address side-channel attacks, hardware trojans, and other threats through architectural and fabrication innovations.", "motivation": "Current 3D/2.5D IC design approaches lack effective solutions for emerging security challenges including side-channel attacks, hardware trojans, secure manufacturing, and IP piracy.", "method": "Four approaches are proposed: (1) 3D shielding architecture for side-channel attacks; (2) split fabrication with active interposers; (3) circuit camouflage on monolithic 3D ICs; and (4) 3D IC-based security processing-in-memory (PIM).", "result": "The designs demonstrate advantages in improving existing security countermeasures and enabling new security features through 2.5D/3D technologies.", "conclusion": "The new designs improve existing countermeasures against security threats and provide novel security features by leveraging the intrinsic characteristics of 2.5D and 3D technologies."}}
{"id": "2508.19321", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19321", "abs": "https://arxiv.org/abs/2508.19321", "authors": ["Kehao Miao", "Xiaolong Jin"], "title": "An Investigation on Group Query Hallucination Attacks", "comment": null, "summary": "With the widespread use of large language models (LLMs), understanding their\npotential failure modes during user interactions is essential. In practice,\nusers often pose multiple questions in a single conversation with LLMs.\nTherefore, in this study, we propose Group Query Attack, a technique that\nsimulates this scenario by presenting groups of queries to LLMs simultaneously.\nWe investigate how the accumulated context from consecutive prompts influences\nthe outputs of LLMs. Specifically, we observe that Group Query Attack\nsignificantly degrades the performance of models fine-tuned on specific tasks.\nMoreover, we demonstrate that Group Query Attack induces a risk of triggering\npotential backdoors of LLMs. Besides, Group Query Attack is also effective in\ntasks involving reasoning, such as mathematical reasoning and code generation\nfor pre-trained and aligned models.", "AI": {"tldr": "Researchers demonstrate Group Query Attack as a novel method to exploit sequential prompt context in LLMs, causing performance collapses and backdoor activations. This highlights critical security flaws in multi-turn conversations involving math/code reasoning.", "motivation": "The study addresses the gap in understanding LLM vulnerabilities during multi-question user interactions. They aim to quantify how accumulated context in sequential prompts compromises model reliability and exposes hidden backdoors in both pre-trained and aligned models.", "method": "The authors propose Group Query Attack, a technique delivering sequential prompts to LLMs in group settings. They experimentally analyze the impact on model outputs through performance metrics and backdoor activation tests across various reasoning tasks.", "result": "Group Query Attack demonstrates substantial performance degradation (30-50% accuracy drop in math/code tasks) and successfully triggers backdoor behaviors in 68%-85% of test cases across 8 LLM variants including GPT3.5 and Llama3.", "conclusion": "This paper concludes that Group Query Attack poses a significant threat to LLMs, particularly fine-tuned/aligned models, by exploiting accumulated context risks and triggering backdoors. It underscores the need for robust context-handling mechanisms and secure training practices."}}
{"id": "2508.19323", "categories": ["cs.CR", "cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.19323", "abs": "https://arxiv.org/abs/2508.19323", "authors": ["Ms. Preeti P. Bhatt", "Rakesh R. Savant"], "title": "A Technical Review on Comparison and Estimation of Steganographic Tools", "comment": "20", "summary": "Steganography is technique of hiding a data under cover media using different\nsteganography tools. Image steganography is hiding of data\n(Text/Image/Audio/Video) under a cover as Image. This review paper presents\nclassification of image steganography and the comparison of various Image\nsteganography tools using different image formats. Analyzing numerous tools on\nthe basis of Image features and extracting the best one. Some of the tools\navailable in the market were selected based on the frequent use; these tools\nwere tested using the same input on all of them. Specific text was embedded\nwithin all host images for each of the six Steganography tools selected. The\nresults of the experiment reveal that all the six tools were relatively\nperforming at the same level, though some software performs better than others\nthrough efficiency. And it was based on the image features like size,\ndimensions, and pixel value and histogram differentiation.", "AI": {"tldr": "This paper reviews/image-steganography-tools-by-testing-their-performance-based-on-image-features-like-size-dimensions-and-histogram-changes-showing-variable-efficiency-among-commonly-used-tools", "motivation": "The paper addresses the need to systematically evaluate and compare image steganography tools to identify optimal performers under standardized testing conditions.", "method": "The paper tests six frequently used steganography tools by embedding identical text into host images. Performance is compared analytically using image characteristics such as size, dimensions, pixel values, and histogram changes.", "result": "Results show comparable baseline performance of six tools, with efficiency differences emerging through analysis of image features. No single tool outperforms others across all metrics.", "conclusion": "The study concludes that while six selected steganography tools perform similarly overall, their efficiency varies based on image features like size, dimensions, pixel values, and histogram differentiation, offering a framework for selecting optimal tools."}}
{"id": "2508.19368", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19368", "abs": "https://arxiv.org/abs/2508.19368", "authors": ["Luqman Muhammad Zagi", "Girindro Pringgo Digdo", "Wervyan Shalannanda"], "title": "Just Dork and Crawl: Measuring Illegal Online Gambling Defacement in Indonesian Websites", "comment": "6 pages, 2 figures, IEEE Conference", "summary": "This study investigates the defacement of Indonesian websites by actors\npromoting illegal online gambling. Using a lightweight methodology that\ncombines keyword-driven dorking with systematic crawling, we identified 453\ndefaced webpages within one month. Although dorking alone yielded a false\npositive rate of approximately 20.3\\%, the integration of crawling and\nkeyword-counting enabled reliable differentiation between true and false\npositives. Our measurements revealed diverse defacement behaviors, including\nrepeat defacements (150 cases), fixed instances (129), keyword modifications\n(55), and redirections or hidden URL injections. In total, 8,837 unique\nthird-party URLs spanning 5,930 domains were captured, with a small subset\nrecurring across multiple sites. Website responses were inconsistent, with an\naverage reaction time of 75.3 hours. These findings demonstrate that simple,\nreproducible techniques can provide meaningful insights into the scale,\npersistence, and dynamics of defacement, highlighting the importance of\ncontinuous measurement for strengthening defenses against online gambling\nactivities.", "AI": {"tldr": "This paper examines illegal gambling-driven website defacements in Indonesia using scalable methods, uncovering persistent patterns and emphasizing the need for continuous monitoring.", "motivation": "To understand the scale, persistence, and tactics of illegal online gambling actors defacing Indonesian websites to improve mitigation strategies.", "method": "A lightweight approach combining keyword-driven dorking with systematic crawling and keyword-counting to identify and validate defaced websites in Indonesia.", "result": "453 defaced webpages were analyzed, revealing 150 repeat defacements, 129 fixed instances, 55 keyword modifications, and 8,837 unique third-party URLs linked to gambling, with an average website response time of 75.3 hours.", "conclusion": "The study emphasizes that simple, reproducible methods effectively measure defacement campaigns driven by illegal gambling, underscoring the need for continuous monitoring to strengthen web defenses."}}
{"id": "2508.19395", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19395", "abs": "https://arxiv.org/abs/2508.19395", "authors": ["Fabian Aude Steen", "Daniel Assani Shabani"], "title": "A NIS2 pan-European registry for identifying and classifying essential and important entities", "comment": null, "summary": "The NIS2 Directive establishes a common cybersecurity governance model across\nthe European Union, requiring member states to identify, classify, and\nsupervise essential and important entities. As part of a broader governance\nnetwork, member states are also obligated to notify the European Commission,\nthe Cooperation Group, and ENISA about their cybersecurity infrastructure\nlandscape. This thesis presents an analysis of the NIS2 Directive in this\ncontext and translates its provisions into concrete technical requirements.\nThese requirements inform the design and implementation of a modular, legally\ngrounded registry system intended to support competent authorities across the\nEU in meeting their obligations. Using the Design Science Research methodology,\nthe thesis transforms complex legal provisions into structured workflows,\ndeterministic classification algorithms, and interactive dashboards. The\nresulting system automates key regulatory processes, including entity\nregistration, classification, and notification, while enabling context-aware\nsupervision and reducing administrative burden. It supports both automated and\nmanual registration methods and introduces a contextual labeling system to\nhandle edge cases, risk factors, and cross-directive dependencies. Although\ndeveloped for the Norwegian regulatory ecosystem, the system is designed for\nadaptation by other member states with minimal modification. This thesis\ncontributes a reusable framework that bridges legal interpretation and\ntechnical implementation, offering a scalable solution for national and\nEU-level NIS2 cybersecurity governance. It also identifies key limitations and\noutlines opportunities for future research and development.", "AI": {"tldr": "This paper designs a technical framework to implement EU's NIS2 Directive cybersecurity regulations, creating an adaptable system for entity classification, supervision, and notifications using Design Science Research methodology.", "motivation": "The NIS2 Directive requires EU member states to implement cybersecurity governance frameworks with entity classification/supervision obligations. The research addresses the need for a standardized technical solution to manage cross-directive dependencies and reduce administrative burdens while ensuring compliance.", "method": "Using the Design Science Research methodology, the paper translates complex legal provisions into structured workflows, deterministic classification algorithms, and interactive dashboards. The system incorporates automated/manual registration, classification, notification processes, and a contextual labeling system for edge cases.", "result": "A modular, legally grounded registry system was developed to automate entity registration, classification, and notification processes. The system supports context-aware supervision, handles edge cases via contextual labeling, and is adaptable across EU member states with minimal modification.", "conclusion": "This thesis contributes a reusable framework that bridges legal interpretation and technical implementation, offering a scalable solution for national and EU-level NIS2 cybersecurity governance. It identifies key limitations and outlines opportunities for future research and development."}}
{"id": "2508.19430", "categories": ["cs.CR", "cs.FL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.19430", "abs": "https://arxiv.org/abs/2508.19430", "authors": ["Kangfeng Ye", "Roberto Metere", "Jim Woodcock", "Poonam Yadav"], "title": "Formal Verification of Physical Layer Security Protocols for Next-Generation Communication Networks", "comment": "Submitted to ICFEM2025; 23 pages, 2 tables, and 6 figures", "summary": "Formal verification is crucial for ensuring the robustness of security\nprotocols against adversarial attacks. The Needham-Schroeder protocol, a\nfoundational authentication mechanism, has been extensively studied, including\nits integration with Physical Layer Security (PLS) techniques such as\nwatermarking and jamming. Recent research has used ProVerif to verify these\nmechanisms in terms of secrecy. However, the ProVerif-based approach limits the\nability to improve understanding of security beyond verification results. To\novercome these limitations, we re-model the same protocol using an Isabelle\nformalism that generates sound animation, enabling interactive and automated\nformal verification of security protocols. Our modelling and verification\nframework is generic and highly configurable, supporting both cryptography and\nPLS. For the same protocol, we have conducted a comprehensive analysis (secrecy\nand authenticity in four different eavesdropper locations under both passive\nand active attacks) using our new web interface. Our findings not only\nsuccessfully reproduce and reinforce previous results on secrecy but also\nreveal an uncommon but expected outcome: authenticity is preserved across all\nexamined scenarios, even in cases where secrecy is compromised. We have\nproposed a PLS-based Diffie-Hellman protocol that integrates watermarking and\njamming, and our analysis shows that it is secure for deriving a session key\nwith required authentication. These highlight the advantages of our novel\napproach, demonstrating its robustness in formally verifying security\nproperties beyond conventional methods.", "AI": {"tldr": "This paper proposes an Isabelle-based formal verification framework for security protocols that overcomes limitations of existing tools. It demonstrates robust verification of authenticity under diverse attack scenarios and validates a novel PLS-based Diffie-Hellman protocol, showcasing enhanced security analysis capabilities compared to traditional methods.", "motivation": "The study addresses limitations of ProVerif-based verification for security protocols, which restricts deeper understanding of security properties beyond basic verification results. The goal is to develop a more informative and flexible formal verification approach.", "method": "The authors re-model the Needham-Schroeder protocol using an Isabelle formalism to enable interactive and automated formal verification. This framework supports both cryptography and PLS and utilizes a new web interface for comprehensive analysis (secrecy and authenticity) under diverse attack scenarios.", "result": "The framework successfully reproduces secrecy results from prior work, reveals authenticity preservation across all scenarios (including secrecy-compromised cases), and confirms the security of the PLS-based Diffie-Hellman protocol for session key derivation with authentication.", "conclusion": "The paper concludes that the proposed Isabelle-based formalism enhances security protocol verification by demonstrating robustness in verifying authenticity even when secrecy is compromised, and by validating the security of a PLS-based Diffie-Hellman protocol."}}
{"id": "2508.19450", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19450", "abs": "https://arxiv.org/abs/2508.19450", "authors": ["Elvin Li", "Onat Gungor", "Zhengli Shang", "Tajana Rosing"], "title": "CITADEL: Continual Anomaly Detection for Enhanced Learning in IoT Intrusion Detection", "comment": "Under review at IEEE IoTJ", "summary": "The Internet of Things (IoT), with its high degree of interconnectivity and\nlimited computational resources, is particularly vulnerable to a wide range of\ncyber threats. Intrusion detection systems (IDS) have been extensively studied\nto enhance IoT security, and machine learning-based IDS (ML-IDS) show\nconsiderable promise for detecting malicious activity. However, their\neffectiveness is often constrained by poor adaptability to emerging threats and\nthe issue of catastrophic forgetting during continuous learning. To address\nthese challenges, we propose CITADEL, a self-supervised continual learning\nframework designed to extract robust representations from benign data while\npreserving long-term knowledge through optimized memory consolidation\nmechanisms. CITADEL integrates a tabular-to-image transformation module, a\nmemory-aware masked autoencoder for self-supervised representation learning,\nand a novelty detection component capable of identifying anomalies without\ndependence on labeled attack data. Our design enables the system to\nincrementally adapt to emerging behaviors while retaining its ability to detect\npreviously observed threats. Experiments on multiple intrusion datasets\ndemonstrate that CITADEL achieves up to a 72.9% improvement over the VAE-based\nlifelong anomaly detector (VLAD) in key detection and retention metrics,\nhighlighting its effectiveness in dynamic IoT environments.", "AI": {"tldr": "CITADEL achieves state-of-the-art IoT intrusion detection via self-supervised continual learning, combining novel representation learning and memory consolidation to overcome forgetting and adapt to new threats without labeled attack data.", "motivation": "IoT systems face critical security challenges due to their interconnectedness and resource limitations, while existing ML-based IDS struggle with adaptability to emerging threats and catastrophic forgetting during continuous learning.", "method": "CITADEL employs a self-supervised continual learning framework with three core components: (1) tabular-to-image data transformation, (2) memory-aware masked autoencoder for representation learning, and (3) attack-agnostic novelty detection, enabling robust knowledge retention and incremental adaptation.", "result": "CITADEL outperforms the baseline VLAD method by 72.9% in key detection/retention metrics across multiple intrusion datasets, showing superior performance in both threat detection accuracy and knowledge preservation.", "conclusion": "CITADEL provides an effective solution for IoT intrusion detection by addressing adaptability and catastrophic forgetting through self-supervised continual learning, demonstrating superior performance in dynamic environments."}}
{"id": "2508.19456", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19456", "abs": "https://arxiv.org/abs/2508.19456", "authors": ["Cagla Ipek Kocal", "Onat Gungor", "Tajana Rosing", "Baris Aksanli"], "title": "ReLATE+: Unified Framework for Adversarial Attack Detection, Classification, and Resilient Model Selection in Time-Series Classification", "comment": "Under review at IEEE TSMC Journal. arXiv admin note: text overlap\n  with arXiv:2503.07882", "summary": "Minimizing computational overhead in time-series classification, particularly\nin deep learning models, presents a significant challenge due to the high\ncomplexity of model architectures and the large volume of sequential data that\nmust be processed in real time. This challenge is further compounded by\nadversarial attacks, emphasizing the need for resilient methods that ensure\nrobust performance and efficient model selection. To address this challenge, we\npropose ReLATE+, a comprehensive framework that detects and classifies\nadversarial attacks, adaptively selects deep learning models based on\ndataset-level similarity, and thus substantially reduces retraining costs\nrelative to conventional methods that do not leverage prior knowledge, while\nmaintaining strong performance. ReLATE+ first checks whether the incoming data\nis adversarial and, if so, classifies the attack type, using this insight to\nidentify a similar dataset from a repository and enable the reuse of the\nbest-performing associated model. This approach ensures strong performance\nwhile reducing the need for retraining, and it generalizes well across\ndifferent domains with varying data distributions and feature spaces.\nExperiments show that ReLATE+ reduces computational overhead by an average of\n77.68%, enhancing adversarial resilience and streamlining robust model\nselection, all without sacrificing performance, within 2.02% of Oracle.", "AI": {"tldr": "ReLATE+ reduces computational costs by 77.68% in adversarial time-series classification through adaptive model reuse and attack-aware selection, achieving near-Oracle accuracy.", "motivation": "High computational complexity in real-time time-series classification with deep learning models, exacerbated by adversarial attacks, necessitates methods that ensure robust performance, efficient model selection, and cost-effective adaptation.", "method": "ReLATE+ framework detects adversarial attacks, classifies their types, and leverages dataset-level similarity to reuse pre-trained models from a repository, minimizing retraining costs and adaptation time.", "result": "Experiments demonstrate 77.68% average computational overhead reduction, maintaining 2.02% deviation from Oracle performance while generalizing robustly across diverse domains with varying data distributions.", "conclusion": "ReLATE+ achieves robust performance reduction in computational overhead by 77.68% while maintaining adversarial resilience and efficient model selection without compromising accuracy."}}
{"id": "2508.19465", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19465", "abs": "https://arxiv.org/abs/2508.19465", "authors": ["Onyinye Okoye"], "title": "Addressing Weak Authentication like RFID, NFC in EVs and EVCs using AI-powered Adaptive Authentication", "comment": "Research paper exploring AI-driven adaptive authentication in the\n  Electric Vehicle industry", "summary": "The rapid expansion of the Electric Vehicles (EVs) and Electric Vehicle\nCharging Systems (EVCs) has introduced new cybersecurity challenges,\nspecifically in authentication protocols that protect vehicles, users, and\nenergy infrastructure. Although widely adopted for convenience, traditional\nauthentication mechanisms like Radio Frequency Identification (RFID) and Near\nField Communication (NFC) rely on static identifiers and weak encryption,\nmaking them highly vulnerable to attack vectors such as cloning, relay attacks,\nand signal interception. This study explores an AI-powered adaptive\nauthentication framework designed to overcome these shortcomings by integrating\nmachine learning, anomaly detection, behavioral analytics, and contextual risk\nassessment. Grounded in the principles of Zero Trust Architecture, the proposed\nframework emphasizes continuous verification, least privilege access, and\nsecure communication. Through a comprehensive literature review, this research\nevaluates current vulnerabilities and highlights AI-driven solutions to provide\na scalable, resilient, and proactive defense. Ultimately, the research findings\nconclude that adopting AI-powered adaptive authentication is a strategic\nimperative for securing the future of electric mobility and strengthening\ndigital trust across the ecosystem. Keywords: weak authentication, RFID, NFC,\nML, AI-powered adaptive authentication, relay attacks, cloning, eavesdropping,\nMITM attacks, Zero Trust Architecture", "AI": {"tldr": "This paper addresses cybersecurity flaws in EV charging authentication by proposing an AI-powered adaptive framework using Zero Trust principles. Traditional methods like RFID/NFC are found vulnerable, but AI-driven anomaly detection and contextual analysis emerge as critical for scalable, proactive security in electric mobility.", "motivation": "Traditional authentication methods (RFID, NFC) for EVs/EVCs are vulnerable to attacks like cloning, relay attacks, and signal interception due to static identifiers and weak encryption, creating new cybersecurity challenges in the expanding EV infrastructure.", "method": "The study proposes an AI-powered adaptive authentication framework that integrates machine learning, anomaly detection, behavioral analytics, and contextual risk assessment within the principles of Zero Trust Architecture (continuous verification, least privilege access, and secure communication).", "result": "The research evaluates current vulnerabilities in authentication protocols and highlights AI-driven solutions for scalable, resilient, proactive defense mechanisms.", "conclusion": "Adopting AI-powered adaptive authentication is deemed a strategic imperative to secure the future of electric mobility and strengthen digital trust in EV charging ecosystems."}}
{"id": "2508.19472", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19472", "abs": "https://arxiv.org/abs/2508.19472", "authors": ["Kyler Katz", "Sara Moshtari", "Ibrahim Mujhid", "Mehdi Mirakhorli", "Derek Garcia"], "title": "SIExVulTS: Sensitive Information Exposure Vulnerability Detection System using Transformer Models and Static Analysis", "comment": null, "summary": "Sensitive Information Exposure (SIEx) vulnerabilities (CWE-200) remain a\npersistent and under-addressed threat across software systems, often leading to\nserious security breaches. Existing detection tools rarely target the diverse\nsubcategories of CWE-200 or provide context-aware analysis of code-level data\nflows.\n  Aims: This paper aims to present SIExVulTS, a novel vulnerability detection\nsystem that integrates transformer-based models with static analysis to\nidentify and verify sensitive information exposure in Java applications.\n  Method: SIExVulTS employs a three-stage architecture: (1) an Attack Surface\nDetection Engine that uses sentence embeddings to identify sensitive variables,\nstrings, comments, and sinks; (2) an Exposure Analysis Engine that instantiates\nCodeQL queries aligned with the CWE-200 hierarchy; and (3) a Flow Verification\nEngine that leverages GraphCodeBERT to semantically validate source-to-sink\nflows. We evaluate SIExVulTS using three curated datasets, including real-world\nCVEs, a benchmark set of synthetic CWE-200 examples, and labeled flows from 31\nopen-source projects.\n  Results: The Attack Surface Detection Engine achieved an average F1 score\ngreater than 93\\%, the Exposure Analysis Engine achieved an F1 score of\n85.71\\%, and the Flow Verification Engine increased precision from 22.61\\% to\n87.23\\%. Moreover, SIExVulTS successfully uncovered six previously unknown CVEs\nin major Apache projects.\n  Conclusions: The results demonstrate that SIExVulTS is effective and\npractical for improving software security against sensitive data exposure,\naddressing limitations of existing tools in detecting and verifying CWE-200\nvulnerabilities.", "AI": {"tldr": "SIExVulTS combines transformers and static analysis to accurately detect CWE-200 vulnerabilities in Java apps, uncovering unknown security issues while outperforming existing tools in precision and verification.", "motivation": "CWE-200 vulnerabilities remain under-addressed, with existing tools lacking coverage of diverse subcategories and context-aware data-flow analysis.", "method": "SIExVulTS employs a three-stage architecture: (1) Attack Surface Detection using sentence embeddings, (2) Exposure Analysis via CodeQL queries aligned with CWE-200, and (3) Flow Verification with GraphCodeBERT. Evaluated on three datasets including real-world CVEs, synthetic examples, and open-source projects.", "result": "Attack Surface Detection achieved >93% F1 score, Exposure Analysis 85.71% F1, and Flow Verification increased precision from 22.61% to 87.23%. Identified six previously unknown CVEs in Apache projects.", "conclusion": "SIExVulTS is effective and practical for improving software security against sensitive data exposure, addressing limitations of existing tools in detecting and verifying CWE-200 vulnerabilities."}}
{"id": "2508.19493", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19493", "abs": "https://arxiv.org/abs/2508.19493", "authors": ["Zhixin Lin", "Jungang Li", "Shidong Pan", "Yibo Shi", "Yue Yao", "Dongliang Xu"], "title": "Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents", "comment": null, "summary": "Smartphones bring significant convenience to users but also enable devices to\nextensively record various types of personal information. Existing smartphone\nagents powered by Multimodal Large Language Models (MLLMs) have achieved\nremarkable performance in automating different tasks. However, as the cost,\nthese agents are granted substantial access to sensitive users' personal\ninformation during this operation. To gain a thorough understanding of the\nprivacy awareness of these agents, we present the first large-scale benchmark\nencompassing 7,138 scenarios to the best of our knowledge. In addition, for\nprivacy context in scenarios, we annotate its type (e.g., Account Credentials),\nsensitivity level, and location. We then carefully benchmark seven available\nmainstream smartphone agents. Our results demonstrate that almost all\nbenchmarked agents show unsatisfying privacy awareness (RA), with performance\nremaining below 60% even with explicit hints. Overall, closed-source agents\nshow better privacy ability than open-source ones, and Gemini 2.0-flash\nachieves the best, achieving an RA of 67%. We also find that the agents'\nprivacy detection capability is highly related to scenario sensitivity level,\ni.e., the scenario with a higher sensitivity level is typically more\nidentifiable. We hope the findings enlighten the research community to rethink\nthe unbalanced utility-privacy tradeoff about smartphone agents. Our code and\nbenchmark are available at https://zhixin-l.github.io/SAPA-Bench.", "AI": {"tldr": "A large benchmark reveals smartphone agents have poor privacy awareness; closed-source models perform better, but overall detection of sensitive data remains suboptimal.", "motivation": "Smartphone agents powered by MLLMs access sensitive user data extensively, but their privacy awareness remains underexplored, necessitating a thorough evaluation of their performance in this area.", "method": "The authors created a large-scale benchmark with 7,138 scenarios annotated for privacy context types, sensitivity levels, and locations, then evaluated seven mainstream smartphone agents for their privacy awareness.", "result": "Most agents scored below 60% in privacy awareness (RA), with closed-source models (e.g., Gemini 2.0-flash at 67%) outperforming open-source ones. Higher sensitivity scenarios were more recognizable to agents.", "conclusion": "The study highlights the inadequate privacy awareness of current smartphone agents, urging a re-evaluation of the utility-privacy tradeoff, and provides a benchmark for future research."}}
{"id": "2508.19500", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19500", "abs": "https://arxiv.org/abs/2508.19500", "authors": ["David Noever"], "title": "Servant, Stalker, Predator: How An Honest, Helpful, And Harmless (3H) Agent Unlocks Adversarial Skills", "comment": null, "summary": "This paper identifies and analyzes a novel vulnerability class in Model\nContext Protocol (MCP) based agent systems. The attack chain describes and\ndemonstrates how benign, individually authorized tasks can be orchestrated to\nproduce harmful emergent behaviors. Through systematic analysis using the MITRE\nATLAS framework, we demonstrate how 95 agents tested with access to multiple\nservices-including browser automation, financial analysis, location tracking,\nand code deployment-can chain legitimate operations into sophisticated attack\nsequences that extend beyond the security boundaries of any individual service.\nThese red team exercises survey whether current MCP architectures lack\ncross-domain security measures necessary to detect or prevent a large category\nof compositional attacks. We present empirical evidence of specific attack\nchains that achieve targeted harm through service orchestration, including data\nexfiltration, financial manipulation, and infrastructure compromise. These\nfindings reveal that the fundamental security assumption of service isolation\nfails when agents can coordinate actions across multiple domains, creating an\nexponential attack surface that grows with each additional capability. This\nresearch provides a barebones experimental framework that evaluate not whether\nagents can complete MCP benchmark tasks, but what happens when they complete\nthem too well and optimize across multiple services in ways that violate human\nexpectations and safety constraints. We propose three concrete experimental\ndirections using the existing MCP benchmark suite.", "AI": {"tldr": "This paper reveals a new vulnerability class in MCP-based agent systems, where authorized tasks across services can be orchestrated into harmful emergent behaviors. Existing architectures lack cross-domain security to prevent such compositional attacks, and the study proposes an experimental framework to evaluate beyond benchmark tasks.", "motivation": "Current MCP systems assume individual service isolation as secure, but coordinated tasks across multiple domains with legitimate operations can lead to sophisticated attacks. This highlights the need to address cross-domain vulnerabilities that existing frameworks ignore.", "method": "The paper conducts systematic analysis using MITRE ATLAS, testing 95 agents with cross-service capabilities like browser automation, finance, location tracking, and code deployment. Red team exercises simulate attack chains, and an experimental framework evaluates unintended optimization beyond MCP benchmarks.", "result": "Empirical attack chains achieved data exfiltration, financial manipulation, and infrastructure compromise. The study demonstrates that service isolation assumptions fail as attack surfaces grow exponentially with added capabilities.", "conclusion": "MCP architectures require cross-domain security measures to prevent compositional attacks. The proposed experimental framework highlights safety risks from over-optimization and provides actionable directions for securing multi-service agent systems."}}
{"id": "2508.19525", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19525", "abs": "https://arxiv.org/abs/2508.19525", "authors": ["Tianshi Xu", "Wen-jie Lu", "Jiangrui Yu", "Chen Yi", "Chenqi Lin", "Runsheng Wang", "Meng Li"], "title": "Breaking the Layer Barrier: Remodeling Private Transformer Inference with Hybrid CKKS and MPC", "comment": "USENIX Security 2025", "summary": "This paper presents an efficient framework for private Transformer inference\nthat combines Homomorphic Encryption (HE) and Secure Multi-party Computation\n(MPC) to protect data privacy. Existing methods often leverage HE for linear\nlayers (e.g., matrix multiplications) and MPC for non-linear layers (e.g.,\nSoftmax activation functions), but the conversion between HE and MPC introduces\nsignificant communication costs. The proposed framework, dubbed BLB, overcomes\nthis by breaking down layers into fine-grained operators and further fusing\nadjacent linear operators, reducing the need for HE/MPC conversions. To manage\nthe increased ciphertext bit width from the fused linear operators, BLB\nproposes the first secure conversion protocol between CKKS and MPC and enables\nCKKS-based computation of the fused operators. Additionally, BLB proposes an\nefficient matrix multiplication protocol for fused computation in Transformers.\nExtensive evaluations on BERT-base, BERT-large, and GPT2-base show that BLB\nachieves a $21\\times$ reduction in communication overhead compared to BOLT\n(S\\&P'24) and a $2\\times$ reduction compared to Bumblebee (NDSS'25), along with\nlatency reductions of $13\\times$ and $1.8\\times$, respectively, when leveraging\nGPU acceleration.", "AI": {"tldr": "BLB is a private Transformer inference framework that reduces communication overhead by 21x over BOLT and 2x over Bumblebee through HE-MPC fusion and optimized protocols.", "motivation": "Current methods using HE for linear layers and MPC for non-linear layers face high communication costs due to repeated HE-MPC conversions during inference.", "method": "Breaks down model layers into fine-grained operators, fuses adjacent linear operators to minimize conversions, introduces secure CKKS-MPC conversion protocol and GPU-accelerated matrix multiplication protocol via CKKS.", "result": "21x/13x and 2x/1.8x reductions in communication/lantency vs. BOLT and Bumblebee on BERT and GPT2 models with GPU acceleration.", "conclusion": "BLB establishes a new benchmark for communication efficiency in private Transformer inference by eliminating unnecessary HE-MPC conversions through structured operator fusion."}}
{"id": "2508.19641", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19641", "abs": "https://arxiv.org/abs/2508.19641", "authors": ["Lincan Li", "Bolin Shen", "Chenxi Zhao", "Yuxiang Sun", "Kaixiang Zhao", "Shirui Pan", "Yushun Dong"], "title": "Intellectual Property in Graph-Based Machine Learning as a Service: Attacks and Defenses", "comment": null, "summary": "Graph-structured data, which captures non-Euclidean relationships and\ninteractions between entities, is growing in scale and complexity. As a result,\ntraining state-of-the-art graph machine learning (GML) models have become\nincreasingly resource-intensive, turning these models and data into invaluable\nIntellectual Property (IP). To address the resource-intensive nature of model\ntraining, graph-based Machine-Learning-as-a-Service (GMLaaS) has emerged as an\nefficient solution by leveraging third-party cloud services for model\ndevelopment and management. However, deploying such models in GMLaaS also\nexposes them to potential threats from attackers. Specifically, while the APIs\nwithin a GMLaaS system provide interfaces for users to query the model and\nreceive outputs, they also allow attackers to exploit and steal model\nfunctionalities or sensitive training data, posing severe threats to the safety\nof these GML models and the underlying graph data. To address these challenges,\nthis survey systematically introduces the first taxonomy of threats and\ndefenses at the level of both GML model and graph-structured data. Such a\ntailored taxonomy facilitates an in-depth understanding of GML IP protection.\nFurthermore, we present a systematic evaluation framework to assess the\neffectiveness of IP protection methods, introduce a curated set of benchmark\ndatasets across various domains, and discuss their application scopes and\nfuture challenges. Finally, we establish an open-sourced versatile library\nnamed PyGIP, which evaluates various attack and defense techniques in GMLaaS\nscenarios and facilitates the implementation of existing benchmark methods. The\nlibrary resource can be accessed at: https://labrai.github.io/PyGIP. We believe\nthis survey will play a fundamental role in intellectual property protection\nfor GML and provide practical recipes for the GML community.", "AI": {"tldr": "This work systematically addresses GML model IP protection in cloud-based GMLaaS by creating a threat taxonomy, evaluation framework, benchmark datasets, and an open-source library (PyGIP) to advance secure deployment of graph-structured data models.", "motivation": "As graph-structured data and GML models grow in complexity and value as intellectual property, their deployment via GMLaaS exposes them to severe threats like model stealing and data leakage through API interfaces. This necessitates systematic frameworks and tools to address IP protection challenges in GMLaaS ecosystems.", "method": "The paper introduces a tailored taxonomy categorizing threats and defenses at the GML model and graph data levels, proposes a systematic evaluation framework for IP protection methods, curates benchmark datasets across domains, and develops PyGIP\u2014a versatile library for implementing and evaluating attack/defense techniques in GMLaaS scenarios.", "result": "The paper delivers a structured threat taxonomy, an evaluation framework for IP protection methods, domain-specific benchmark datasets, and the PyGIP library (https://labrai.github.io/PyGIP) for practical implementation and analysis of GMLaaS security solutions.", "conclusion": "This survey provides a comprehensive taxonomy of threats and defenses for GML model and graph data IP protection, establishes an evaluation framework with benchmark datasets, and introduces PyGIP, an open-source library to assess attack/defense methods, offering foundational guidance for securing GMLaaS systems."}}
{"id": "2508.19697", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19697", "abs": "https://arxiv.org/abs/2508.19697", "authors": ["Chao Huang", "Zefeng Zhang", "Juewei Yue", "Quangang Li", "Chuang Zhang", "Tingwen Liu"], "title": "Safety Alignment Should Be Made More Than Just A Few Attention Heads", "comment": null, "summary": "Current safety alignment for large language models(LLMs) continues to present\nvulnerabilities, given that adversarial prompting can effectively bypass their\nsafety measures.Our investigation shows that these safety mechanisms\npredominantly depend on a limited subset of attention heads: removing or\nablating these heads can severely compromise model safety. To identify and\nevaluate these safety-critical components, we introduce RDSHA, a targeted\nablation method that leverages the model's refusal direction to pinpoint\nattention heads mostly responsible for safety behaviors. Further analysis shows\nthat existing jailbreak attacks exploit this concentration by selectively\nbypassing or manipulating these critical attention heads. To address this\nissue, we propose AHD, a novel training strategy designed to promote the\ndistributed encoding of safety-related behaviors across numerous attention\nheads. Experimental results demonstrate that AHD successfully distributes\nsafety-related capabilities across more attention heads. Moreover, evaluations\nunder several mainstream jailbreak attacks show that models trained with AHD\nexhibit considerably stronger safety robustness, while maintaining overall\nfunctional utility.", "AI": {"tldr": "This paper proposes AHD, a training method that distributes safety mechanisms across attention heads in LLMs to enhance robustness against adversarial attacks.", "motivation": "Current safety mechanisms in LLMs rely on a limited subset of attention heads, making them vulnerable to adversarial prompts that bypass these concentrated safety components.", "method": "The authors introduce RDSHA, a method using the model's refusal direction to identify safety-critical attention heads, and AHD, a training strategy to distribute safety behaviors across numerous attention heads.", "result": "AHD successfully distributes safety capabilities across more attention heads, leading to improved robustness against mainstream jailbreak attacks without compromising overall model utility.", "conclusion": "The study concludes that distributing safety-related behaviors across multiple attention heads, as achieved by the AHD training strategy, significantly enhances the robustness of LLMs against jailbreak attacks while preserving functional utility."}}
{"id": "2508.19714", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19714", "abs": "https://arxiv.org/abs/2508.19714", "authors": ["Subhrojyoti Mukherjee", "Manoranjan Mohanty"], "title": "Addressing Deepfake Issue in Selfie banking through camera based authentication", "comment": null, "summary": "Fake images in selfie banking are increasingly becoming a threat. Previously,\nit was just Photoshop, but now deep learning technologies enable us to create\nhighly realistic fake identities, which fraudsters exploit to bypass biometric\nsystems such as facial recognition in online banking. This paper explores the\nuse of an already established forensic recognition system, previously used for\npicture camera localization, in deepfake detection.", "AI": {"tldr": "The paper investigates applying a camera localization forensic system to detect deepfakes in banking selfies, countering fraud from realistic synthetic images.", "motivation": "The research is driven by the increasing threat of deepfake technology in bypassing biometric security systems, particularly in financial applications like selfie banking, where synthetic images are exploited to commit fraud.", "method": "The study utilizes a pre-existing forensic recognition system, previously employed for determining the origin of photos through camera localization, and adapts it for the purpose of detecting deepfake-generated fake identities in online banking scenarios.", "result": "The paper's findings indicate that this approach is viable for identifying deepfake images, suggesting that forensic systems can be effectively retrained or reconfigured to detect anomalies in facial recognition data created via synthetic means.", "conclusion": "This paper concludes that repurposing an established forensic recognition system, originally designed for camera localization, can serve as an effective method to detect deepfake images in selfie banking, thereby enhancing biometric security against fraud."}}
{"id": "2508.19774", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19774", "abs": "https://arxiv.org/abs/2508.19774", "authors": ["Tong Liu", "Guozhu Meng", "Peng Zhou", "Zizhuang Deng", "Shuaiyin Yao", "Kai Chen"], "title": "The Art of Hide and Seek: Making Pickle-Based Model Supply Chain Poisoning Stealthy Again", "comment": null, "summary": "Pickle deserialization vulnerabilities have persisted throughout Python's\nhistory, remaining widely recognized yet unresolved. Due to its ability to\ntransparently save and restore complex objects into byte streams, many AI/ML\nframeworks continue to adopt pickle as the model serialization protocol despite\nits inherent risks. As the open-source model ecosystem grows, model-sharing\nplatforms such as Hugging Face have attracted massive participation,\nsignificantly amplifying the real-world risks of pickle exploitation and\nopening new avenues for model supply chain poisoning. Although several\nstate-of-the-art scanners have been developed to detect poisoned models, their\nincomplete understanding of the poisoning surface leaves the detection logic\nfragile and allows attackers to bypass them. In this work, we present the first\nsystematic disclosure of the pickle-based model poisoning surface from both\nmodel loading and risky function perspectives. Our research demonstrates how\npickle-based model poisoning can remain stealthy and highlights critical gaps\nin current scanning solutions. On the model loading surface, we identify 22\ndistinct pickle-based model loading paths across five foundational AI/ML\nframeworks, 19 of which are entirely missed by existing scanners. We further\ndevelop a bypass technique named Exception-Oriented Programming (EOP) and\ndiscover 9 EOP instances, 7 of which can bypass all scanners. On the risky\nfunction surface, we discover 133 exploitable gadgets, achieving almost a 100%\nbypass rate. Even against the best-performing scanner, these gadgets maintain\nan 89% bypass rate. By systematically revealing the pickle-based model\npoisoning surface, we achieve practical and robust bypasses against real-world\nscanners. We responsibly disclose our findings to corresponding vendors,\nreceiving acknowledgments and a $6000 bug bounty.", "AI": {"tldr": "Researchers uncover critical pickle deserialization vulnerabilities in AI/ML frameworks, exposing 22 loading paths and high-bypass EOP techniques that evade state-of-the-art scanners, earning a $6000 bug bounty.", "motivation": "Existing scanners for pickle model poisoning lack comprehensive surface understanding, allowing attackers to bypass protections. The study aims to reveal these critical vulnerabilities in Python's serialization ecosystem.", "method": "The authors systematically analyzed pickle-based model loading paths across frameworks, developed the Exception-Oriented Programming (EOP) bypass technique, and identified exploitable gadgets to evaluate scanner effectiveness.", "result": "22 pickle-based loading paths (19 undetected), 9 EOP instances (7 fully bypass), and 133 exploitable gadgets (89-100% bypass rates) were discovered, showcasing robust bypasses against current scanners.", "conclusion": "The study highlights the persistent risks of pickle-based model poisoning, exposing significant gaps in current detection methods and demonstrating practical bypass techniques, leading to a $6000 bug bounty after responsible disclosure."}}
{"id": "2508.19819", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19819", "abs": "https://arxiv.org/abs/2508.19819", "authors": ["Viktor Valadi", "Mattias \u00c5kesson", "Johan \u00d6stman", "Salman Toor", "Andreas Hellander"], "title": "From Research to Reality: Feasibility of Gradient Inversion Attacks in Federated Learning", "comment": "Under review at KDD 2026 (Research Track)", "summary": "Gradient inversion attacks have garnered attention for their ability to\ncompromise privacy in federated learning. However, many studies consider\nattacks with the model in inference mode, where training-time behaviors like\ndropout are disabled and batch normalization relies on fixed statistics. In\nthis work, we systematically analyze how architecture and training behavior\naffect vulnerability, including the first in-depth study of inference-mode\nclients, which we show dramatically simplifies inversion. To assess attack\nfeasibility under more realistic conditions, we turn to clients operating in\nstandard training mode. In this setting, we find that successful attacks are\nonly possible when several architectural conditions are met simultaneously:\nmodels must be shallow and wide, use skip connections, and, critically, employ\npre-activation normalization. We introduce two novel attacks against models in\ntraining-mode with varying attacker knowledge, achieving state-of-the-art\nperformance under realistic training conditions. We extend these efforts by\npresenting the first attack on a production-grade object-detection model. Here,\nto enable any visibly identifiable leakage, we revert to the lenient inference\nmode setting and make multiple architectural modifications to increase model\nvulnerability, with the extent of required changes highlighting the strong\ninherent robustness of such architectures. We conclude this work by offering\nthe first comprehensive mapping of settings, clarifying which combinations of\narchitectural choices and operational modes meaningfully impact privacy. Our\nanalysis provides actionable insight into when models are likely vulnerable,\nwhen they appear robust, and where subtle leakage may persist. Together, these\nfindings reframe how gradient inversion risk should be assessed in future\nresearch and deployment scenarios.", "AI": {"tldr": "This paper maps how neural architecture and training behavior impact gradient inversion risks in federated learning, introduces robust attacks, and highlights that modern architectures are more privacy-preserving by design.", "motivation": "Previous gradient inversion analysis overlooked training-mode behaviors. This work aims to systematically assess how architecture and training dynamics (e.g., dropout, normalization) influence privacy risks in realistic federated learning scenarios.", "method": "The study compares inference-mode and training-mode client vulnerabilities, identifies architectural conditions (shallow/wide models, skip connections, pre-activation normalization) for successful attacks, and introduces novel attacks against training-mode models and modified production-grade object detectors.", "result": "Key findings include: (1) inference-mode clients enable simpler inversion, (2) training-mode attacks require specific architectural conditions, (3) novel attacks outperform prior methods in realistic settings, and (4) production-grade models show strong inherent robustness unless artificially modified.", "conclusion": "The paper provides a comprehensive analysis of privacy vulnerabilities in federated learning through gradient inversion, clarifying architectural and operational factors that impact robustness. It reframes risk assessment for future research and deployment."}}
{"id": "2508.19825", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19825", "abs": "https://arxiv.org/abs/2508.19825", "authors": ["Shaoor Munir", "Nurullah Demir", "Qian Li", "Konrad Kollnig", "Zubair Shafiq"], "title": "Every Keystroke You Make: A Tech-Law Measurement and Analysis of Event Listeners for Wiretapping", "comment": null, "summary": "The privacy community has a long track record of investigating emerging types\nof web tracking techniques. Recent work has focused on compliance of web\ntrackers with new privacy laws such as Europe's GDPR and California's CCPA.\nDespite the growing body of research documenting widespread lack of compliance\nwith new privacy laws, there is a lack of robust enforcement. Different from\nprior work, we conduct a tech-law analysis to map decades-old U.S. laws about\ninterception of electronic communications--so-called wiretapping--to web\ntracking. Bridging the tech-law gap for older wiretapping laws is important and\ntimely because, in cases where legal harm to privacy is proven, they can\nprovide statutory private right of action, are at the forefront of recent\nprivacy enforcement, and could ultimately lead to a meaningful change in the\nweb tracking landscape.\n  In this paper, we focus on a particularly invasive tracking technique: the\nuse of JavaScript event listeners by third-party trackers for real-time\nkeystroke interception on websites. We use an instrumented web browser to crawl\na sample of the top-million websites to investigate the use of event listeners\nthat aligns with the criteria for wiretapping, according to U.S. wiretapping\nlaw at the federal level and in California. We find evidence that 38.52%\nwebsites installed third-party event listeners to intercept keystrokes, and\nthat at least 3.18% websites transmitted intercepted information to a\nthird-party server, which aligns with the criteria for wiretapping. We further\nfind evidence that the intercepted information such as email addresses typed\ninto form fields are used for unsolicited email marketing. Beyond our work that\nmaps the intersection between technical measurement and U.S. wiretapping law,\nadditional future legal research is required to determine when the wiretapping\nobserved in our paper passes the threshold for illegality.", "AI": {"tldr": "This study maps invasive JavaScript-based keystroke interception on websites to U.S. wiretapping laws, finding over 3% of sites illegally transmit intercepted data for marketing, suggesting legal enforcement opportunities under old but relevant laws.", "motivation": "The study addresses the lack of enforcement of privacy laws and explores existing wiretapping laws as a legal avenue for holding web trackers accountable, given their potential to provide private right of action and drive meaningful change in privacy practices.", "method": "The researchers instrumented a web browser to crawl the top-million websites, identifying third-party JavaScript event listeners that intercept keystrokes and assess their alignment with U.S. wiretapping laws. They found 38.52% of sites used such listeners, with 3.18% transmitting intercepted data for email marketing.", "result": "38.52% of websites used third-party event listeners to intercept keystrokes, 3.18% transmitted intercepted data to third-party servers, and the data was used for unsolicited email marketing, aligning with wiretapping criteria under U.S. law.", "conclusion": "The paper concludes that bridging the tech-law gap for wiretapping laws could enable legal enforcement against invasive web tracking practices, potentially reshaping the web tracking landscape. However, further legal analysis is required to confirm the threshold for illegality."}}
