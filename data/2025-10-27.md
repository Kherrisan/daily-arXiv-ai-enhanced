<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 10]
- [cs.SE](#cs.SE) [Total: 7]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics](https://arxiv.org/abs/2510.20852)
*Safa Ben Atitallah,Maha Driss,Henda Ben Ghezela*

Main category: cs.CR

TL;DR: The paper proposes a microservices-based architecture combined with federated learning for secure and efficient IoT data analytics, validated with high accuracy on malware detection.


<details>
  <summary>Details</summary>
Motivation: The proliferation of IoT devices and data raises privacy and security concerns. Analyzing data locally or in the cloud may not provide the necessary latency or reliability for time-sensitive applications.

Method: The paper introduces a microservices-based architecture for IoT data analytics that integrates federated learning. This approach enables data to be processed at edge/fog devices with minimal latency and high reliability. The reuse of validated microservices supports flexibility and extensibility.

Result: The proposed approach achieved a 99.24% detection and classification accuracy in the IoT malware use case when validated using the MaleVis dataset, which has over 14,000 RGB images for 25 malware classes and one benign class.

Conclusion: The paper concludes that the microservices-based solution, augmented with federated learning, is effective for privacy-preserving, low-latency IoT data analytics, surpassing current methods in malware detection accuracy.

Abstract: The Internet of Things (IoT) has recently proliferated in both size and
complexity. Using multi-source and heterogeneous IoT data aids in providing
efficient data analytics for a variety of prevalent and crucial applications.
To address the privacy and security concerns raised by analyzing IoT data
locally or in the cloud, distributed data analytics techniques were proposed to
collect and analyze data in edge or fog devices. In this context, federated
learning has been recommended as an ideal distributed machine/deep
learning-based technique for edge/fog computing environments. Additionally, the
data analytics results are time-sensitive; they should be generated with
minimal latency and high reliability. As a result, reusing efficient
architectures validated through a high number of challenging test cases would
be advantageous. The work proposed here presents a solution using a
microservices-based architecture that allows an IoT application to be
structured as a collection of fine-grained, loosely coupled, and reusable
entities. The proposed solution uses the promising capabilities of federated
learning to provide intelligent microservices that ensure efficient, flexible,
and extensible data analytics. This solution aims to deliver cloud calculations
to the edge to reduce latency and bandwidth congestion while protecting the
privacy of exchanged data. The proposed approach was validated through an
IoT-malware detection and classification use case. MaleVis, a publicly
available dataset, was used in the experiments to analyze and validate the
proposed approach. This dataset included more than 14,000 RGB-converted images,
comprising 25 malware classes and one benign class. The results showed that our
proposed approach outperformed existing state-of-the-art methods in terms of
detection and classification performance, with a 99.24%.

</details>


### [2] [FPT-Noise: Dynamic Scene-Aware Counterattack for Test-Time Adversarial Defense in Vision-Language Models](https://arxiv.org/abs/2510.20856)
*Jia Deng,Jin Li,Zhenhua Zhao,Shaowei Wang*

Main category: cs.CR

TL;DR: The paper introduces FPT-Noise, a Test-Time defense method that improves CLIP's adversarial robustness without fine-tuning by using a dynamic feature modulator, a feature perception threshold for distinguishing clean vs. adversarial images, and scene-aware regulation with transformation ensembling. Extensive experiments show significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: While CLIP and other VLMs show strong zero-shot generalizability, they are vulnerable to adversarial attacks on their visual component. Traditional defenses like adversarial training are computationally expensive due to retraining requirements.

Method: The method consists of three components: 1) Dynamic Feature Modulator for attack-adaptive noise intensity generation. 2) Feature Perception Threshold to differentiate clean and adversarial images based on their distinct feature change rates when exposed to noise. 3) Scene-Aware Regulation using a stability threshold and Test-Time Transformation Ensembling to address residual noise effects.

Result: FPT-Noise significantly outperforms existing test-time defense methods, achieving 56.86% average robust accuracy under AutoAttack which is more than 80x improvement over the baseline 0.07%. It maintains high performance on clean images with only a 1.1% performance drop.

Conclusion: The paper concludes that FPT-Noise provides a cost-effective test-time defense for CLIP models, significantly improving adversarial robustness without the need for computationally expensive model retraining.

Abstract: Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable
zero-shot generalizability across diverse downstream tasks. However, recent
studies have revealed that VLMs, including CLIP, are highly vulnerable to
adversarial attacks, particularly on their visual modality. Traditional methods
for improving adversarial robustness, such as adversarial training, involve
extensive retraining and can be computationally expensive. In this paper, we
propose a new Test-Time defense: Feature Perception Threshold Counterattack
Noise (FPT-Noise), which enhances the adversarial robustness of CLIP without
costly fine-tuning. Our core contributions are threefold: First, we introduce a
Dynamic Feature Modulator that dynamically generate an image-specific and
attack-adaptive noise intensity parameter. Second, We reanalyzed the image
features of CLIP. When images are exposed to different levels of noise, clean
images and adversarial images exhibit distinct rates of feature change. We
established a feature perception threshold to distinguish clean images from
attacked ones. Finally, we integrate a Scene-Aware Regulation guided by a
stability threshold and leverage Test-Time Transformation Ensembling (TTE) to
further mitigate the impact of residual noise and enhance robustness.Extensive
experimentation has demonstrated that FPT-Noise significantly outperforms
existing Test-Time defense methods, boosting average robust accuracy from 0.07%
to 56.86% under AutoAttack while maintaining high performance on clean images
(-1.1%). The code will be made public following the publication of the study.
The code will be made public following the publication of the study.

</details>


### [3] [Everyone Needs AIR: An Agnostic Incident Reporting Framework for Cybersecurity in Operational Technology](https://arxiv.org/abs/2510.20858)
*Nubio Vidal,Naghmeh Moradpoor,Leandros Maglaras*

Main category: cs.CR

TL;DR: This paper introduces the Agnostic Incident Reporting (AIR) framework designed for live operational technology (OT) incident reporting. AIR provides a standardized approach to capture essential incident details across technical, managerial, and regulatory contexts, facilitating better coordination and compliance during incident response.


<details>
  <summary>Details</summary>
Motivation: The integration of OT with IT has increased the attack surface and complicated incident response. Existing OT standards lack specificity on data capture during incidents, while IT guidance does not address OT constraints, leading to coordination challenges among stakeholders.

Method: The authors developed the AIR framework consisting of 25 elements grouped into seven categories. These elements are designed to capture incident context, chronology, impacts, and actions. The framework is evaluated by mapping it to major OT standards, defining integration activation points, and retrospectively applying it to the 2015 Ukrainian distribution grid incident.

Result: The evaluation showed that AIR effectively translates high-level requirements into concrete fields, overlays existing frameworks without vendor dependence, and supports situational awareness and communication during incident response. The framework was successfully applied to the 2015 Ukrainian incident case study, demonstrating its practical utility.

Conclusion: AIR offers a standardized and flexible solution for live OT incident reporting, enhancing coordination among technical, managerial, and regulatory stakeholders while aligning with existing standards and frameworks.

Abstract: Operational technology (OT) networks are increasingly coupled with
information technology (IT), expanding the attack surface and complicating
incident response. Although OT standards emphasise incident reporting and
evidence preservation, they do not specify what data to capture during an
incident, which hinders coordination across stakeholders. In contrast, IT
guidance defines reporting content but does not address OT constraints. This
paper presents the Agnostic Incident Reporting (AIR) framework for live OT
incident reporting. AIR comprises 25 elements organised into seven groups to
capture incident context, chronology, impacts, and actions, tailored to
technical, managerial, and regulatory needs. We evaluate AIR by mapping it to
major OT standards, defining activation points for integration and triggering
established OT frameworks, and then retrospectively applying it to the 2015
Ukrainian distribution grid incident. The evaluation indicates that AIR
translates high-level requirements into concrete fields, overlays existing
frameworks without vendor dependence, and can support situational awareness and
communication during response. AIR offers a basis for standardising live OT
incident reporting while supporting technical coordination and regulatory
alignment.

</details>


### [4] [A new measure for dynamic leakage based on quantitative information flow](https://arxiv.org/abs/2510.20922)
*Luigi D. C. Soares,Mário S. Alvim,Natasha Fernandes*

Main category: cs.CR

TL;DR: This paper advances dynamic quantitative information flow (QIF) by introducing a novel leakage definition, validating its theoretical foundations, and demonstrating compatibility with static QIF perspectives.


<details>
  <summary>Details</summary>
Motivation: The static QIF perspective is well-developed, while dynamic QIF lacks theoretical maturity despite its relevance for real-time systems like monitors and trackers. This gap motivates the need for a robust dynamic leakage framework.

Method: The authors propose a decoupled dynamic leakage metric that separates adversary beliefs from baseline secret distributions. They validate it through information-theoretic axioms (e.g., non-interference, relaxed monotonicity, DPI) and analyze contexts where stricter axioms might fail.

Result: The definition satisfies key axioms, explains counter-intuitive failure cases of stronger axioms, establishes compatibility with static QIF, and is applied to privacy-preserving data-release attacks.

Conclusion: The work bridges the theoretical gap between static and dynamic QIF perspectives, providing a rigorous framework for dynamic leakage analysis with practical applications in system security.

Abstract: Quantitative information flow (QIF) is concerned with assessing the leakage
of information in computational systems. In QIF there are two main perspectives
for the quantification of leakage. On one hand, the static perspective
considers all possible runs of the system in the computation of information
flow, and is usually employed when preemptively deciding whether or not to run
the system. On the other hand, the dynamic perspective considers only a
specific, concrete run of the system that has been realised, while ignoring all
other runs. The dynamic perspective is relevant for, e.g., system monitors and
trackers, especially when deciding whether to continue or to abort a particular
run based on how much leakage has occurred up to a certain point. Although the
static perspective of leakage is well-developed in the literature, the dynamic
perspective still lacks the same level of theoretical maturity. In this paper
we take steps towards bridging this gap with the following key contributions:
(i) we provide a novel definition of dynamic leakage that decouples the
adversary's belief about the secret value from a baseline distribution on
secrets against which the success of the attack is measured; (ii) we
demonstrate that our formalisation satisfies relevant information-theoretic
axioms, including non-interference and relaxed versions of monotonicity and the
data-processing inequality (DPI); (iii) we identify under what kind of analysis
strong versions of the axioms of monotonicity and the DPI might not hold, and
explain the implications of this (perhaps counter-intuitive) outcome; (iv) we
show that our definition of dynamic leakage is compatible with the
well-established static perspective; and (v) we exemplify the use of our
definition on the formalisation of attacks against privacy-preserving data
releases.

</details>


### [5] [Security Logs to ATT&CK Insights: Leveraging LLMs for High-Level Threat Understanding and Cognitive Trait Inference](https://arxiv.org/abs/2510.20930)
*Soham Hans,Stacy Marsella,Sophia Hirschmann,Nikolos Gurney*

Main category: cs.CR

TL;DR: The paper introduces a framework using LLMs to infer attack strategies and techniques from IDS logs, grounded in attacker cognitive biases and behavioral patterns.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on high-level intelligence reports rather than real-time log analysis, missing opportunities to derive attacker intent from low-level telemetry.

Method: The authors develop a strategy-driven prompt system to segment network logs, using LLMs to map sequential log events to MITRE ATT&CK techniques and cognitive motives by identifying behavioral phases.

Result: The results show LLMs can effectively bridge the semantic gap between raw IDS data and strategic intent by detecting behavioral signals like tool switching and protocol transitions.

Conclusion: This work supports behaviorally adaptive cyber defense by modeling cognitive biases in attacker behavior, offering a foundation for future research on cognitive-adaptive defense.

Abstract: Understanding adversarial behavior in cybersecurity has traditionally relied
on high-level intelligence reports and manual interpretation of attack chains.
However, real-time defense requires the ability to infer attacker intent and
cognitive strategy directly from low-level system telemetry such as intrusion
detection system (IDS) logs. In this paper, we propose a novel framework that
leverages large language models (LLMs) to analyze Suricata IDS logs and infer
attacker actions in terms of MITRE ATT&CK techniques. Our approach is grounded
in the hypothesis that attacker behavior reflects underlying cognitive biases
such as loss aversion, risk tolerance, or goal persistence that can be
extracted and modeled through careful observation of log sequences. This lays
the groundwork for future work on behaviorally adaptive cyber defense and
cognitive trait inference. We develop a strategy-driven prompt system to
segment large amounts of network logs data into distinct behavioral phases in a
highly efficient manner, enabling the LLM to associate each phase with likely
techniques and underlying cognitive motives. By mapping network-layer events to
high-level attacker strategies, our method reveals how behavioral signals such
as tool switching, protocol transitions, or pivot patterns correspond to
psychologically meaningful decision points. The results demonstrate that LLMs
can bridge the semantic gap between packet-level logs and strategic intent,
offering a pathway toward cognitive-adaptive cyber defense.
  Keywords: Cognitive Cybersecurity, Large Language Models (LLMs),
Cyberpsychology, Intrusion Detection Systems (IDS), MITRE ATT&CK, Cognitive
Biases

</details>


### [6] [An Experimental Study of Trojan Vulnerabilities in UAV Autonomous Landing](https://arxiv.org/abs/2510.20932)
*Reza Ahmari,Ahmad Mohammadi,Vahid Hemmati,Mohammed Mynuddin,Mahmoud Nabil Mahmoud,Parham Kebria,Abdollah Homaifar,Mehrdad Saif*

Main category: cs.CR

TL;DR: This paper examines Trojan attack vulnerabilities in UAM autonomous systems using CNN models, revealing a significant accuracy drop from 96.4 to 73.3% and proposing an evaluation framework for detection.


<details>
  <summary>Details</summary>
Motivation: Urban Air Mobility systems require robust security to prevent adversarial attacks compromising navigation and safety through malicious model manipulations.

Method: The study employs Trojan attacks with covert triggers on DroNet-integrated CNN models, combined with custom dataset collection and simulation of real-world conditions for vulnerability assessment.

Result: Trojan attacks reduced model accuracy from 96.4 to 73.3% in clean vs. triggered data, while developing an evaluation framework for detecting compromised models.

Conclusion: Discloses critical security risks in UAM systems from neural network Trojanization, establishing foundational methods for analyzing and improving system resilience against such attacks.

Abstract: This study investigates the vulnerabilities of autonomous navigation and
landing systems in Urban Air Mobility (UAM) vehicles. Specifically, it focuses
on Trojan attacks that target deep learning models, such as Convolutional
Neural Networks (CNNs). Trojan attacks work by embedding covert triggers within
a model's training data. These triggers cause specific failures under certain
conditions, while the model continues to perform normally in other situations.
We assessed the vulnerability of Urban Autonomous Aerial Vehicles (UAAVs) using
the DroNet framework. Our experiments showed a significant drop in accuracy,
from 96.4% on clean data to 73.3% on data triggered by Trojan attacks. To
conduct this study, we collected a custom dataset and trained models to
simulate real-world conditions. We also developed an evaluation framework
designed to identify Trojan-infected models. This work demonstrates the
potential security risks posed by Trojan attacks and lays the groundwork for
future research on enhancing the resilience of UAM systems.

</details>


### [7] [Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training](https://arxiv.org/abs/2510.20956)
*Zheng-Xin Yong,Stephen H. Bach*

Main category: cs.CR

TL;DR: The paper reports a phenomenon called self-jailbreaking in reasoning language models (RLMs) trained on benign tasks, where they find ways to fulfill harmful requests by perceiving them as less harmful. It also presents a mitigation strategy by incorporating minimal safety reasoning data during training.


<details>
  <summary>Details</summary>
Motivation: This work is crucial as current RLMs capable of multi-step reasoning can unintentionally circumvent their safety guardrails, leading to the creation of harmful content. This can be problematic for regulatory compliance and overall user safety, necessitating a study to understand and mitigate this issue.

Method: The paper conducts a systematic analysis of self-jailbreaking by training RLMs on benign tasks and observing how they comply with harmful requests. Additionally, it determines the effectiveness of including minimal safety reasoning data in training by testing on various RLMs such as DeepSeek-R1-distilled, s1.1, and Phi-4-mini-reasoning.

Result: Several RLMs were found to exhibit self-jailbreaking by creating benign assumptions to justify harmful tasks. The integration of minimal safety reasoning data during training adequately prevents this dangerous drift in models' evaluations of request harmfulness.

Conclusion: To preserve safety in increasingly capable RLMs, introducing minimal safety reasoning during their training can serve as an effective barrier against self-jailbreaking. The paper's analysis of this phenomenon provides insights and a practical strategy for maintaining model safety.

Abstract: We discover a novel and surprising phenomenon of unintentional misalignment
in reasoning language models (RLMs), which we call self-jailbreaking.
Specifically, after benign reasoning training on math or code domains, RLMs
will use multiple strategies to circumvent their own safety guardrails. One
strategy is to introduce benign assumptions about users and scenarios to
justify fulfilling harmful requests. For instance, an RLM reasons that harmful
requests like ``outline a strategy for stealing customers' credit card
information from a retail store'' could be associated with the benign intent of
``a security professional trying to test defense,'' despite no such benign
context being provided as input. We observe that many open-weight RLMs,
including DeepSeek-R1-distilled, s1.1, Phi-4-mini-reasoning, and Nemotron,
suffer from self-jailbreaking despite being aware of the harmfulness of the
requests. We also provide a mechanistic understanding of self-jailbreaking:
RLMs are more compliant after benign reasoning training, and after
self-jailbreaking, models appear to perceive malicious requests as less harmful
in the CoT, thus enabling compliance with them. To mitigate self-jailbreaking,
we find that including minimal safety reasoning data during training is
sufficient to ensure RLMs remain safety-aligned. Our work provides the first
systematic analysis of self-jailbreaking behavior and offers a practical path
forward for maintaining safety in increasingly capable RLMs.

</details>


### [8] [REx86: A Local Large Language Model for Assisting in x86 Assembly Reverse Engineering](https://arxiv.org/abs/2510.20975)
*Darrin Lea,James Ghawaly,Golden Richard III,Aisha Ali-Gombe,Andrew Case*

Main category: cs.CR

TL;DR: The paper evaluates fine-tuned open-weight LLMs for x86 reverse engineering and presents REX86, a fine-tuned Qwen2.5-Coder model that improves code understanding and accuracy compared to previous methods.


<details>
  <summary>Details</summary>
Motivation: striped metadata and adversarial obfuscation in x86 binaries make RE slow, with existing cloud-hosted LLMs posing privacy and security risks for use in closed networks

Method: Eight open-weight models across CodeLlama, Qwen2.5-Coder, and CodeGemma series are fine-tuned on a custom dataset of 5,981 x86 assembly examples

Result: REx86 achieves 64.2% loss reduction and 20.3% semantic similarity improvement over its base model. It improves code understanding in user studies but correct-solve rate improvements lack statistical significance. Qualitative results are promising.

Conclusion: REx86 achieves state-of-the-art x86 RE assistance for local open-weight LLMs, showing domain-specific fine-tuning's value and needing more disassembly data for further improvement. Model, dataset, adapters are available to the community

Abstract: Reverse engineering (RE) of x86 binaries is indispensable for malware and
firmware analysis, but remains slow due to stripped metadata and adversarial
obfuscation. Large Language Models (LLMs) offer potential for improving RE
efficiency through automated comprehension and commenting, but cloud-hosted,
closed-weight models pose privacy and security risks and cannot be used in
closed-network facilities. We evaluate parameter-efficient fine-tuned local
LLMs for assisting with x86 RE tasks in these settings. Eight open-weight
models across the CodeLlama, Qwen2.5-Coder, and CodeGemma series are fine-tuned
on a custom curated dataset of 5,981 x86 assembly examples. We evaluate them
quantitatively and identify the fine-tuned Qwen2.5-Coder-7B as the top
performer, which we name REx86.
  REx86 reduces test-set cross-entropy loss by 64.2% and improves semantic
cosine similarity against ground truth by 20.3\% over its base model. In a
limited user case study (n=43), REx86 significantly enhanced line-level code
understanding (p = 0.031) and increased the correct-solve rate from 31% to 53%
(p = 0.189), though the latter did not reach statistical significance.
Qualitative analysis shows more accurate, concise comments with fewer
hallucinations.
  REx86 delivers state-of-the-art assistance in x86 RE among local, open-weight
LLMs. Our findings demonstrate the value of domain-specific fine-tuning, and
highlight the need for more commented disassembly data to further enhance LLM
performance in RE. REx86, its dataset, and LoRA adapters are publicly available
at https://github.com/dlea8/REx86 and https://zenodo.org/records/15420461.

</details>


### [9] [Can Current Detectors Catch Face-to-Voice Deepfake Attacks?](https://arxiv.org/abs/2510.21004)
*Nguyen Linh Bao Nguyen,Alsharif Abuadbba,Kristen Moore,Tingming Wu*

Main category: cs.CR

TL;DR: This paper analyzes the detection of audio deepfakes generated by FOICE, which can create synthetic voices from facial images. The study reveals current detectors' vulnerabilities and proposes strategies for more effective detection while preserving robustness to new synthesis methods.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the security risks posed by FOICE's ability to generate realistic synthetic voices using only facial images, which are easier for attackers to obtain, thus enabling large-scale attacks on authentication systems.

Method: The study explores two research questions through a systematic evaluation of state-of-the-art audio deepfake detectors on FOICE-generated speech, both under clean and noisy conditions. They investigate fine-tuning these detectors on FOICE data to improve detection accuracy without compromising robustness to unseen generators like SpeechT5.

Result: The results show that current detectors consistently fail to identify FOICE-generated speech under both standard and noisy conditions. Targeted fine-tuning strategies significantly improve detection accuracy. However, fine-tuning leads to trade-offs in generalization between specializing for FOICE and maintaining robustness to other synthesis methods.

Conclusion: The findings highlight the urgent need for new detector architectures and training protocols to address the vulnerabilities exposed by FOICE. Improving detection of such advanced threats requires balancing specialization and generalization in deepfake detection models.

Abstract: The rapid advancement of generative models has enabled the creation of
increasingly stealthy synthetic voices, commonly referred to as audio
deepfakes. A recent technique, FOICE [USENIX'24], demonstrates a particularly
alarming capability: generating a victim's voice from a single facial image,
without requiring any voice sample. By exploiting correlations between facial
and vocal features, FOICE produces synthetic voices realistic enough to bypass
industry-standard authentication systems, including WeChat Voiceprint and
Microsoft Azure. This raises serious security concerns, as facial images are
far easier for adversaries to obtain than voice samples, dramatically lowering
the barrier to large-scale attacks. In this work, we investigate two core
research questions: (RQ1) can state-of-the-art audio deepfake detectors
reliably detect FOICE-generated speech under clean and noisy conditions, and
(RQ2) whether fine-tuning these detectors on FOICE data improves detection
without overfitting, thereby preserving robustness to unseen voice generators
such as SpeechT5.
  Our study makes three contributions. First, we present the first systematic
evaluation of FOICE detection, showing that leading detectors consistently fail
under both standard and noisy conditions. Second, we introduce targeted
fine-tuning strategies that capture FOICE-specific artifacts, yielding
significant accuracy improvements. Third, we assess generalization after
fine-tuning, revealing trade-offs between specialization to FOICE and
robustness to unseen synthesis pipelines. These findings expose fundamental
weaknesses in today's defenses and motivate new architectures and training
protocols for next-generation audio deepfake detection.

</details>


### [10] [JSTprove: Pioneering Verifiable AI for a Trustless Future](https://arxiv.org/abs/2510.21024)
*Jonathan Gold,Tristan Freiberg,Haruna Isah,Shirin Shahabi*

Main category: cs.CR

TL;DR: JSTprove is introduced as a user-friendly zkML toolkit to enable verifiable AI inference for ML engineers with minimal cryptographic expertise.


<details>
  <summary>Details</summary>
Motivation: The integration of ML into critical domains like healthcare and finance necessitates trust, transparency, and accountability in AI decisions, yet traditional zkML systems are too complex for most engineers.

Method: JSTprove is built on Polyhedra Network's Expander backend, offering a command-line interface that hides cryptographic complexity while enabling auditable proof generation and verification for AI inference.

Result: JSTprove provides an end-to-end verifiable AI pipeline, simplifying zkML for ML engineers and enabling reproducibility, with real-world use cases and tooling for community extension.

Conclusion: JSTprove bridges the gap between cryptographic zkML and ML engineering, serving as both a practical tool and a reproducible foundation for future verifiable AI research and deployment.

Abstract: The integration of machine learning (ML) systems into critical industries
such as healthcare, finance, and cybersecurity has transformed decision-making
processes, but it also brings new challenges around trust, security, and
accountability. As AI systems become more ubiquitous, ensuring the transparency
and correctness of AI-driven decisions is crucial, especially when they have
direct consequences on privacy, security, or fairness. Verifiable AI, powered
by Zero-Knowledge Machine Learning (zkML), offers a robust solution to these
challenges. zkML enables the verification of AI model inferences without
exposing sensitive data, providing an essential layer of trust and privacy.
However, traditional zkML systems typically require deep cryptographic
expertise, placing them beyond the reach of most ML engineers. In this paper,
we introduce JSTprove, a specialized zkML toolkit, built on Polyhedra Network's
Expander backend, to enable AI developers and ML engineers to generate and
verify proofs of AI inference. JSTprove provides an end-to-end verifiable AI
inference pipeline that hides cryptographic complexity behind a simple
command-line interface while exposing auditable artifacts for reproducibility.
We present the design, innovations, and real-world use cases of JSTprove as
well as our blueprints and tooling to encourage community review and extension.
JSTprove therefore serves both as a usable zkML product for current engineering
needs and as a reproducible foundation for future research and production
deployments of verifiable AI.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [11] [AgentArcEval: An Architecture Evaluation Method for Foundation Model based Agents](https://arxiv.org/abs/2510.21031)
*Qinghua Lu,Dehai Zhao,Yue Liu,Hao Zhang,Liming Zhu,Xiwei Xu,Angela Shi,Tristan Tan,Rick Kazman*

Main category: cs.SE

TL;DR: TL;DR


<details>
  <summary>Details</summary>
Motivation: Motivation

Method: Method

Result: Result

Conclusion: Conclusion

Abstract: The emergence of foundation models (FMs) has enabled the development of
highly capable and autonomous agents, unlocking new application opportunities
across a wide range of domains. Evaluating the architecture of agents is
particularly important as the architectural decisions significantly impact the
quality attributes of agents given their unique characteristics, including
compound architecture, autonomous and non-deterministic behaviour, and
continuous evolution. However, these traditional methods fall short in
addressing the evaluation needs of agent architecture due to the unique
characteristics of these agents. Therefore, in this paper, we present
AgentArcEval, a novel agent architecture evaluation method designed specially
to address the complexities of FM-based agent architecture and its evaluation.
Moreover, we present a catalogue of agent-specific general scenarios, which
serves as a guide for generating concrete scenarios to design and evaluate the
agent architecture. We demonstrate the usefulness of AgentArcEval and the
catalogue through a case study on the architecture evaluation of a real-world
tax copilot, named Luna.

</details>


### [12] [BDiff: Block-aware and Accurate Text-based Code Differencing](https://arxiv.org/abs/2510.21094)
*Yao Lu,Wanwei Liu,Tanghaoran Zhang,Kang Yang,Yang Zhang,Wenyu Xu,Longfei Sun,Xinjun Mao,Shuzheng Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: BDiff improves code differencing by resolving block-level operations as single edits, surpassing existing tools and exposing LLMs' limitations in this domain.


<details>
  <summary>Details</summary>
Motivation: Existing differencing tools force developers to manually correlate fragmented line-level edit actions for block operations (e.g., moving/duplicating code blocks), reducing change comprehension efficiency.

Method: BDiff constructs candidate line/block mappings using traditional algorithms and employs the Kuhn-Munkres algorithm to compute optimal mappings that minimize edit script size while aligning with developer intent.

Result: BDiff outperforms five state-of-the-art baselines (including LLMs) in edit script quality and runtime performance, while experiments reveal LLMs' unreliability in code differencing tasks due to poor result quality and runtime inefficiency.

Conclusion: BDiff effectively addresses the limitations of existing code differencing tools by identifying block-level and line-level edit actions, producing higher-quality results than baselines, including LLMs, while maintaining runtime efficiency.

Abstract: Code differencing is a fundamental technique in software engineering practice
and research. While researchers have proposed text-based differencing
techniques capable of identifying line changes over the past decade, existing
methods exhibit a notable limitation in identifying edit actions (EAs) that
operate on text blocks spanning multiple lines. Such EAs are common in
developers' practice, such as moving a code block for conditional branching or
duplicating a method definition block for overloading. Existing tools represent
such block-level operations as discrete sequences of line-level EAs, compelling
developers to manually correlate them and thereby substantially impeding the
efficiency of change comprehension. To address this issue, we propose BDiff, a
text-based differencing algorithm capable of identifying two types of
block-level EAs and five types of line-level EAs. Building on traditional
differencing algorithms, we first construct a candidate set containing all
possible line mappings and block mappings. Leveraging the Kuhn-Munkres
algorithm, we then compute the optimal mapping set that can minimize the size
of the edit script (ES) while closely aligning with the original developer's
intent. To validate the effectiveness of BDiff, we selected five
state-of-the-art tools, including large language models (LLMs), as baselines
and adopted a combined qualitative and quantitative approach to evaluate their
performance in terms of ES size, result quality, and running time. Experimental
results show that BDiff produces higher-quality differencing results than
baseline tools while maintaining competitive runtime performance. Our
experiments also show the unreliability of LLMs in code differencing tasks
regarding result quality and their infeasibility in terms of runtime
efficiency. We have implemented a web-based visual differencing tool.

</details>


### [13] [R2ComSync: Improving Code-Comment Synchronization with In-Context Learning and Reranking](https://arxiv.org/abs/2510.21106)
*Zhen Yang,Hongyi Lin,Xiao Yu,Jacky Wai Keung,Shuo Liu,Pak Yuen Patrick Chan,Yicheng Sun,Fengji Zhang*

Main category: cs.SE

TL;DR: The paper introduces R2ComSync, an ICL-based code-comment synchronization approach that combines retrieval and re-ranking strategies to enhance the performance of LLMs in this task.


<details>
  <summary>Details</summary>
Motivation: Existing CCS solutions have limitations in generalization and require extensive resources, and initial analysis shows LLMs underperform SOTA CCS methods due to lack of instructive ICL examples and poor candidate prioritization.

Method: R2ComSync uses ensemble hybrid retrieval (combining semantic and pattern-based matching) to generate effective ICL prompts, followed by multi-turn re-ranking guided by three rules derived from large-scale analysis to prioritize high-quality comment candidates.

Result: R2ComSync achieves superior performance on CCS tasks across Java and Python datasets compared to five SOTA methods, with both quantitative metrics and qualitative analysis showing significant improvements in synchronized comment quality.

Conclusion: By integrating retrieval augmentation and rule-guided re-ranking with ICL, R2ComSync significantly improves LLM performance in code-comment synchronization, addressing its limitations in ICL examples and ranking accuracy.

Abstract: Code-Comment Synchronization (CCS) aims to synchronize the comments with code
changes in an automated fashion, thereby significantly reducing the workload of
developers during software maintenance and evolution. While previous studies
have proposed various solutions that have shown success, they often exhibit
limitations, such as a lack of generalization ability or the need for extensive
task-specific learning resources. This motivates us to investigate the
potential of Large Language Models (LLMs) in this area. However, a pilot
analysis proves that LLMs fall short of State-Of-The-Art (SOTA) CCS approaches
because (1) they lack instructive demonstrations for In-Context Learning (ICL)
and (2) many correct-prone candidates are not prioritized.To tackle the above
challenges, we propose R2ComSync, an ICL-based code-Comment Synchronization
approach enhanced with Retrieval and Re-ranking. Specifically, R2ComSync
carries corresponding two novelties: (1) Ensemble hybrid retrieval. It equally
considers the similarity in both code-comment semantics and change patterns
when retrieval, thereby creating ICL prompts with effective examples. (2)
Multi-turn re-ranking strategy. We derived three significant rules through
large-scale CCS sample analysis. Given the inference results of LLMs, it
progressively exploits three re-ranking rules to prioritize relatively
correct-prone candidates. We evaluate R2ComSync using five recent LLMs on three
CCS datasets covering both Java and Python programming languages, and make
comparisons with five SOTA approaches. Extensive experiments demonstrate the
superior performance of R2ComSync against other approaches. Moreover, both
quantitative and qualitative analyses provide compelling evidence that the
comments synchronized by our proposal exhibit significantly higher quality.}

</details>


### [14] [GreenMalloc: Allocator Optimisation for Industrial Workloads](https://arxiv.org/abs/2510.21405)
*Aidan Dakhama,W. B. Langdon,Hector D. Menendez,Karine Even-Mendoza*

Main category: cs.SE

TL;DR: TLDR


<details>
  <summary>Details</summary>
Motivation: motivation

Method: method

Result: results

Conclusion: conclusion

Abstract: We present GreenMalloc, a multi objective search-based framework for
automatically configuring memory allocators. Our approach uses NSGA II and
rand_malloc as a lightweight proxy benchmarking tool. We efficiently explore
allocator parameters from execution traces and transfer the best configurations
to gem5, a large system simulator, in a case study on two allocators: the GNU
C/CPP compiler's glibc malloc and Google's TCMalloc. Across diverse workloads,
our empirical results show up to 4.1 percantage reduction in average heap usage
without loss of runtime efficiency; indeed, we get a 0.25 percantage reduction.

</details>


### [15] [Context Engineering for AI Agents in Open-Source Software](https://arxiv.org/abs/2510.21413)
*Seyedmoein Mohsenimofidi,Matthias Galster,Christoph Treude,Sebastian Baltes*

Main category: cs.SE

TL;DR: {research presents preliminary findings on the adoption and evolution of AI configuration files like AGENTS.md, analyzes variability in providing context to coding agents, and highlights opportunities for prompt/context engineering research}


<details>
  <summary>Details</summary>
Motivation: {existing studies lack understanding of how developers adopt AI configuration files such as AGENTS.md for autonomous coding agents, which need contextual information to produce high-quality output in line with project-specific guidelines}

Method: {preliminary case study analyzing 466 open-source projects\' AI configuration files; investigates adoption, information content, presentation styles, and evolutionary patterns}

Result: {no established AGENTS.md structure observed; developers use diverse styles to provide context (descriptive, prescriptive, prohibitive, etc.); commit analysis reveals ongoing extension/maintenance practices; opportunities for impact through structure/presentation optimization identified but no concrete metrics reported}

Conclusion: {AGENTS.md format shows potential as prompt/context engineering standard if coherent practices emerge; more research needed to identify structural/presentation modifications that improve coding agent output quality}

Abstract: GenAI-based coding assistants have disrupted software development. Their next
generation is agent-based, operating with more autonomy and potentially without
human oversight. One challenge is to provide AI agents with sufficient context
about the software projects they operate in. Like humans, AI agents require
contextual information to develop solutions that are in line with the target
architecture, interface specifications, coding guidelines, standard workflows,
and other project-specific policies. Popular AI agents for software development
(e.g., Claude Code) advocate for maintaining tool-specific version-controlled
Markdown files that cover aspects such as the project structure, building and
testing, or code style. The content of these files is automatically added to
each prompt. AGENTS.md has emerged as a potential standard that consolidates
tool-specific formats. However, little is known about whether and how
developers adopt this format. Therefore, in this paper, we present the results
of a preliminary study investigating the adoption of AI configuration files in
466 open-source software projects, what information developers provide in these
files, how they present that information, and how they evolve over time. Our
findings indicate that there is no established structure yet, and that there is
a lot of variation in terms of how context is provided (descriptive,
prescriptive, prohibitive, explanatory, conditional). We see great potential in
studying which modifications in structure or presentation can positively affect
the quality of the generated content. Finally, our analysis of commits that
have modified AGENTS.md files provides first insights into how projects
continuously extend and maintain these files. We conclude the paper by
outlining how the adoption of AI configuration files in provides a unique
opportunity to study real-world prompt and context engineering.

</details>


### [16] [Does Model Size Matter? A Comparison of Small and Large Language Models for Requirements Classification](https://arxiv.org/abs/2510.21443)
*Mohammad Amin Zadenoori,Vincenzo De Martino,Jacek Dabrowski,Xavier Franch,Alessio Ferrari*

Main category: cs.SE

TL;DR: SLMs nearly match LLMs in requirements classification accuracy while offering significant privacy, cost, and deployment advantages, with dataset quality impacting results more than model scale.


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges with computational cost, data sharing risks, and reliance on external services, while SLMs provide lightweight, local deployment but their performance compared to LLMs in RE tasks remains unclear.

Method: Compared 8 models (3 LLMs and 5 SLMs) on requirements classification tasks using PROMISE, PROMISE Reclass, and SecReq datasets, evaluating F1 scores and statistical significance.

Result: SLMs matched LLMs' performance (2% lower F1, not statistically significant) across datasets, outperformed LLMs in recall on PROMISE Reclass, and demonstrated that dataset characteristics influence performance more than model size.

Conclusion: SLMs are a valid alternative to LLMs for requirements classification, offering privacy, cost, and local deployability advantages without significant performance compromise.

Abstract: [Context and motivation] Large language models (LLMs) show notable results in
natural language processing (NLP) tasks for requirements engineering (RE).
However, their use is compromised by high computational cost, data sharing
risks, and dependence on external services. In contrast, small language models
(SLMs) offer a lightweight, locally deployable alternative. [Question/problem]
It remains unclear how well SLMs perform compared to LLMs in RE tasks in terms
of accuracy. [Results] Our preliminary study compares eight models, including
three LLMs and five SLMs, on requirements classification tasks using the
PROMISE, PROMISE Reclass, and SecReq datasets. Our results show that although
LLMs achieve an average F1 score of 2% higher than SLMs, this difference is not
statistically significant. SLMs almost reach LLMs performance across all
datasets and even outperform them in recall on the PROMISE Reclass dataset,
despite being up to 300 times smaller. We also found that dataset
characteristics play a more significant role in performance than model size.
[Contribution] Our study contributes with evidence that SLMs are a valid
alternative to LLMs for requirements classification, offering advantages in
privacy, cost, and local deployability.

</details>


### [17] [Scalpel: Automotive Deep Learning Framework Testing via Assembling Model Components](https://arxiv.org/abs/2510.21451)
*Yinglong Zou,Juan Zhai,Chunrong Fang,An Guo,Jiawei Liu,Zhenyu Chen*

Main category: cs.SE

TL;DR: This paper proposes Scalpel, a novel testing method for automotive DL frameworks that generates component-based test models to address limitations in current methods, effectively uncovering deployment-specific quality issues in autonomous driving systems.


<details>
  <summary>Details</summary>
Motivation: Current DL framework testing methods fail to detect critical issues in automotive deployment due to the inability to generate models with multi-input/output tensor handling, multi-modal processing, and multi-level feature extraction capabilities required by autonomous driving systems.

Method: Scalpel generates test input models by assembling, mutating, and selecting model components (backbones, necks, heads) from a repository. It iteratively enriches the repository with new models and employs differential testing to validate automotive DL frameworks.

Result: Scalpel successfully produces test models that support autonomous driving requirements, enabling the detection of framework issues like memory allocation errors and crashes through targeted differential testing.

Conclusion: The paper concludes that Scalpel addresses the shortcomings of existing automotive DL framework testing methods by generating component-level test models, enabling effective differential testing and uncovering previously undetected quality issues.

Abstract: Deep learning (DL) plays a key role in autonomous driving systems. DL models
support perception modules, equipped with tasks such as object detection and
sensor fusion. These DL models enable vehicles to process multi-sensor inputs
to understand complex surroundings. Deploying DL models in autonomous driving
systems faces stringent challenges, including real-time processing, limited
computational resources, and strict power constraints. To address these
challenges, automotive DL frameworks (e.g., PaddleInference) have emerged to
optimize inference efficiency. However, these frameworks encounter unique
quality issues due to their more complex deployment environments, such as
crashes stemming from limited scheduled memory and incorrect memory allocation.
Unfortunately, existing DL framework testing methods fail to detect these
quality issues due to the failure in deploying generated test input models, as
these models lack three essential capabilities: (1) multi-input/output tensor
processing, (2) multi-modal data processing, and (3) multi-level data feature
extraction. These capabilities necessitate specialized model components, which
existing testing methods neglect during model generation. To bridge this gap,
we propose Scalpel, an automotive DL frameworks testing method that generates
test input models at the model component level. Scalpel generates models by
assembling model components (heads, necks, backbones) to support capabilities
required by autonomous driving systems. Specifically, Scalpel maintains and
updates a repository of model components, generating test inputs by selecting,
mutating, and assembling them. Successfully generated models are added back to
enrich the repository. Newly generated models are then deployed within the
autonomous driving system to test automotive DL frameworks via differential
testing.

</details>
