{"id": "2508.18370", "categories": ["cs.SE", "cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18370", "abs": "https://arxiv.org/abs/2508.18370", "authors": ["Terry Yue Zhuo", "Dingmin Wang", "Hantian Ding", "Varun Kumar", "Zijian Wang"], "title": "Training Language Model Agents to Find Vulnerabilities with CTF-Dojo", "comment": null, "summary": "Large language models (LLMs) have demonstrated exceptional capabilities when\ntrained within executable runtime environments, notably excelling at software\nengineering tasks through verified feedback loops. Yet, scalable and\ngeneralizable execution-grounded environments remain scarce, limiting progress\nin training more capable ML agents. We introduce CTF-Dojo, the first\nlarge-scale executable runtime tailored for training LLMs with verifiable\nfeedback, featuring 658 fully functional Capture-The-Flag (CTF)-style\nchallenges containerized in Docker with guaranteed reproducibility. To enable\nrapid scaling without manual intervention, we develop CTF-Forge, an automated\npipeline that transforms publicly available artifacts into ready-to-use\nexecution environments in minutes, eliminating weeks of expert configuration\ntraditionally required. We trained LLM-based agents on just 486 high-quality,\nexecution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute\ngains over strong baselines across three competitive benchmarks: InterCode-CTF,\nNYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1,\nestablishing a new open-weight state-of-the-art that rivals frontier models\nlike DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a\nbenchmark for executable-agent learning, CTF-Dojo demonstrates that\nexecution-grounded training signals are not only effective but pivotal in\nadvancing high-performance ML agents without dependence on costly proprietary\nsystems.", "AI": {"tldr": "CTF-Dojo & CTF-Forge boost LLM training via verifiable execution environments, achieving SOTA results without proprietary systems.", "motivation": "Scalable, generalizable execution-grounded environments are scarce, hindering progress in training capable ML agents.", "method": "The paper introduces CTF-Dojo, a large-scale executable runtime with 658 CTF-style challenges containerized in Docker, and CTF-Forge, an automated pipeline for creating environments from public artifacts.", "result": "Models trained on CTF-Dojo achieved up to 11.6% absolute gains over baselines, with the 32B model reaching 31.9% Pass@1, setting a new open-weight state-of-the-art.", "conclusion": "CTF-Dojo demonstrates that execution-grounded training with verifiable feedback is pivotal for advancing high-performance ML agents, enabling competitive results against frontier models without relying on proprietary systems."}}
{"id": "2508.18431", "categories": ["cs.SE", "cs.ET", "cs.HC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.18431", "abs": "https://arxiv.org/abs/2508.18431", "authors": ["K\u00e9rian Fiter", "Louis Malassign\u00e9-Onfroy", "Bentley Oakes"], "title": "DTInsight: A Tool for Explicit, Interactive, and Continuous Digital Twin Reporting", "comment": null, "summary": "With Digital Twin (DT) construction and evolution occurring over time,\nstakeholders require tools to understand the current characteristics and\nconceptual architecture of the system at any time. We introduce DTInsight, a\nsystematic and automated tool and methodology for producing continuous\nreporting for DTs. DTInsight offers three key features: (a) an interactive\nconceptual architecture visualization of DTs; (b) generation of summaries of DT\ncharacteristics based on ontological data; and (c) integration of these outputs\ninto a reporting page within a continuous integration and continuous deployment\n(CI/CD) pipeline. Given a modeled description of the DT aligning to our DT\nDescription Framework (DTDF), DTInsight enables up-to-date and detailed reports\nfor enhanced stakeholder understanding.", "AI": {"tldr": "DTInsight is a CI/CD-integrated tool that automates Digital Twin reporting with architecture visualizations and ontology-based summaries to improve stakeholder understanding over time.", "motivation": "Stakeholders need tools to dynamically understand evolving Digital Twins' architectures and characteristics in real-time for informed decision-making.", "method": "The authors developed DTInsight, a systematic methodology and tool that offers (a) interactive conceptual architecture visualization, (b) ontology-based DT characteristic summaries, and (c) CI/CD pipeline integration for continuous reporting.", "result": "DTInsight generates up-to-date reports for Digital Twins modeled with their DTDF framework, enabling enhanced stakeholder insights through automated visualization and ontological data processing.", "conclusion": "DTInsight enhances stakeholder understanding of Digital Twins by providing automated, continuous reporting through visualization, ontological summaries, and CI/CD pipeline integration."}}
{"id": "2508.18452", "categories": ["cs.SE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.18452", "abs": "https://arxiv.org/abs/2508.18452", "authors": ["Pierre-Emmanuel Goffi", "Rapha\u00ebl Tremblay", "Bentley Oakes"], "title": "Engineering a Digital Twin for the Monitoring and Control of Beer Fermentation Sampling", "comment": "Accepted for EDTconf 2025", "summary": "Successfully engineering interactive industrial DTs is a complex task,\nespecially when implementing services beyond passive monitoring. We present\nhere an experience report on engineering a safety-critical digital twin (DT)\nfor beer fermentation monitoring, which provides continual sampling and reduces\nmanual sampling time by 91%. We document our systematic methodology and\npractical solutions for implementing bidirectional DTs in industrial\nenvironments. This includes our three-phase engineering approach that\ntransforms a passive monitoring system into an interactive Type 2 DT with\nreal-time control capabilities for pressurized systems operating at seven bar.\nWe contribute details of multi-layered safety protocols, hardware-software\nintegration strategies across Arduino controllers and Unity visualization, and\nreal-time synchronization solutions. We document specific engineering\nchallenges and solutions spanning interdisciplinary integration, demonstrating\nhow our use of the constellation reporting framework facilitates cross-domain\ncollaboration. Key findings include the critical importance of safety-first\ndesign, simulation-driven development, and progressive implementation\nstrategies. Our work thus provides actionable guidance for practitioners\ndeveloping DTs requiring bidirectional control in safety-critical applications.", "AI": {"tldr": "This paper presents a safety-critical beer fermentation DT reducing manual sampling by 91%. A three-phase methodology enables bidirectional control via Arduino-Unity integration and multi-layered safety protocols, offering practical guidance for industrial DT implementation.", "motivation": "Engineering bidirectional industrial DTs beyond passive monitoring poses significant complexity, particularly for safety-critical systems requiring real-time control. Traditional approaches lack systematic methodologies for such implementations.", "method": "A three-phase engineering approach transformed passive monitoring into interactive Type 2 DTs, combining multi-layered safety protocols, Arduino-Unity integration, and real-time synchronization solutions. The constellation reporting framework enabled interdisciplinary collaboration.", "result": "Achieved 91% reduction in manual sampling time through continual DT monitoring of beer fermentation. Successfully implemented real-time control for pressurized systems at seven bar, demonstrating viable bidirectional DT capabilities in industrial environments.", "conclusion": "The study emphasizes the importance of safety-first design, simulation-driven development, and progressive implementation strategies for bidirectional Type 2 DTs in safety-critical industrial applications. It provides actionable guidance for achieving real-time control and cross-domain collaboration."}}
{"id": "2508.18547", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18547", "abs": "https://arxiv.org/abs/2508.18547", "authors": ["Youssef Abdelsalam", "Norman Peitek", "Anna-Maria Maurer", "Mariya Toneva", "Sven Apel"], "title": "How do Humans and LLMs Process Confusing Code?", "comment": null, "summary": "Already today, humans and programming assistants based on large language\nmodels (LLMs) collaborate in everyday programming tasks. Clearly, a\nmisalignment between how LLMs and programmers comprehend code can lead to\nmisunderstandings, inefficiencies, low code quality, and bugs.\n  A key question in this space is whether humans and LLMs are confused by the\nsame kind of code. This would not only guide our choices of integrating LLMs in\nsoftware engineering workflows, but also inform about possible improvements of\nLLMs.\n  To this end, we conducted an empirical study comparing an LLM to human\nprogrammers comprehending clean and confusing code. We operationalized\ncomprehension for the LLM by using LLM perplexity, and for human programmers\nusing neurophysiological responses (in particular, EEG-based fixation-related\npotentials).\n  We found that LLM perplexity spikes correlate both in terms of location and\namplitude with human neurophysiological responses that indicate confusion. This\nresult suggests that LLMs and humans are similarly confused about the code.\nBased on these findings, we devised a data-driven, LLM-based approach to\nidentify regions of confusion in code that elicit confusion in human\nprogrammers.", "AI": {"tldr": "LLMs and programmers share similar confusion in code comprehension; this insight enables identifying confusing code for humans via LLM analysis.", "motivation": "The paper investigates whether humans and LLMs are confused by similar code to improve LLM integration into programming workflows and guide LLM improvements, addressing potential misunderstandings and code quality issues.", "method": "An empirical study comparing LLM perplexity (for LLM comprehension) and human neurophysiological responses (via EEG-based fixation-related potentials) was conducted on clean and confusing code examples.", "result": "LLM perplexity spikes correlated with human neurophysiological confusion signals in terms of location and amplitude. This correlation led to a data-driven approach to identify confusing code regions for human programmers using LLM outputs.", "conclusion": "The study concludes that LLMs and human programmers are similarly confused by the same code regions, aligning their comprehension patterns. This alignment suggests opportunities to improve LLM integration in software engineering workflows and refine LLM capabilities."}}
{"id": "2508.18439", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18439", "abs": "https://arxiv.org/abs/2508.18439", "authors": ["Anders M\u00f8lmen H\u00f8st", "Pierre Lison", "Leon Moonen"], "title": "A Systematic Approach to Predict the Impact of Cybersecurity Vulnerabilities Using LLMs", "comment": null, "summary": "Vulnerability databases, such as the National Vulnerability Database (NVD),\noffer detailed descriptions of Common Vulnerabilities and Exposures (CVEs), but\noften lack information on their real-world impact, such as the tactics,\ntechniques, and procedures (TTPs) that adversaries may use to exploit the\nvulnerability. However, manually linking CVEs to their corresponding TTPs is a\nchallenging and time-consuming task, and the high volume of new vulnerabilities\npublished annually makes automated support desirable.\n  This paper introduces TRIAGE, a two-pronged automated approach that uses\nLarge Language Models (LLMs) to map CVEs to relevant techniques from the ATT&CK\nknowledge base. We first prompt an LLM with instructions based on MITRE's CVE\nMapping Methodology to predict an initial list of techniques. This list is then\ncombined with the results from a second LLM-based module that uses in-context\nlearning to map a CVE to relevant techniques. This hybrid approach\nstrategically combines rule-based reasoning with data-driven inference. Our\nevaluation reveals that in-context learning outperforms the individual mapping\nmethods, and the hybrid approach improves recall of exploitation techniques. We\nalso find that GPT-4o-mini performs better than Llama3.3-70B on this task.\nOverall, our results show that LLMs can be used to automatically predict the\nimpact of cybersecurity vulnerabilities and TRIAGE makes the process of mapping\nCVEs to ATT&CK more efficient.\n  Keywords: vulnerability impact, CVE, ATT&CK techniques, large language\nmodels, automated mapping.", "AI": {"tldr": "Researchers developed TRIAGE, a hybrid LLM system linking CVEs to ATT&CK techniques, demonstrating that combining rule-based and data-driven approaches improves mapping accuracy and efficiency in vulnerability impact analysis.", "motivation": "Vulnerability databases lack actionable TTPs information, and manual CVE-ATT&CK mapping is impractical due to high vulnerability volumes. Automated solutions are urgently needed for efficient cybersecurity impact assessment.", "method": "TRIAGE combines two LLM modules: 1) an LLM prompted with MITRE's methodology to generate initial technique mappings, and 2) an in-context learning module for data-driven inference. The hybrid approach merges rule-based reasoning with data-driven methods.", "result": "Evaluation found: 1) in-context learning outperforms standalone methods, 2) hybrid approach improves exploitation technique recall, and 3) GPT-4o-mini surpasses Llama3.3-70B in performance. The system successfully automates predicting vulnerability impacts.", "conclusion": "The paper concludes that TRIAGE, a hybrid LLM-based approach, effectively automates mapping CVEs to ATT&CK techniques, improving recall and efficiency in vulnerability impact analysis. LLMs demonstrate significant potential in this domain."}}
{"id": "2508.18636", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18636", "abs": "https://arxiv.org/abs/2508.18636", "authors": ["Yan Wang", "Xinyi Hou", "Yanjie Zhao", "Weiguo Lin", "Haoyu Wang", "Junjun Si"], "title": "LaQual: A Novel Framework for Automated Evaluation of LLM App Quality", "comment": null, "summary": "LLM app stores are quickly emerging as platforms that gather a wide range of\nintelligent applications based on LLMs, giving users many choices for content\ncreation, coding support, education, and more. However, the current methods for\nranking and recommending apps in these stores mostly rely on static metrics\nlike user activity and favorites, which makes it hard for users to efficiently\nfind high-quality apps. To address these challenges, we propose LaQual, an\nautomated framework for evaluating the quality of LLM apps. LaQual consists of\nthree main stages: first, it labels and classifies LLM apps in a hierarchical\nway to accurately match them to different scenarios; second, it uses static\nindicators, such as time-weighted user engagement and functional capability\nmetrics, to filter out low-quality apps; and third, it conducts a dynamic,\nscenario-adaptive evaluation, where the LLM itself generates scenario-specific\nevaluation metrics, scoring rules, and tasks for a thorough quality assessment.\nExperiments on a popular LLM app store show that LaQual is effective. Its\nautomated scores are highly consistent with human judgments (with Spearman's\nrho of 0.62 and p=0.006 in legal consulting, and rho of 0.60 and p=0.009 in\ntravel planning). By effectively screening, LaQual can reduce the pool of\ncandidate LLM apps by 66.7% to 81.3%. User studies further confirm that LaQual\nsignificantly outperforms baseline systems in decision confidence, comparison\nefficiency (with average scores of 5.45 compared to 3.30), and the perceived\nvalue of its evaluation reports (4.75 versus 2.25). Overall, these results\ndemonstrate that LaQual offers a scalable, objective, and user-centered\nsolution for finding and recommending high-quality LLM apps in real-world use\ncases.", "AI": {"tldr": "LaQual improves LLM app discovery by filtering low-quality apps and providing dynamic scenario-aware evaluations, validated through strong correlation with human judgment and superior user performance metrics.", "motivation": "Current LLM app store ranking methods rely on static metrics like user activity, making it difficult for users to efficiently discover high-quality apps. This creates a need for an automated, objective, and scalable framework to address these limitations.", "method": "The framework employs three stages: hierarchical labeling/classification of LLM apps, static filtering using time-weighted engagement and functional metrics, and dynamic scenario-adaptive evaluation where an LLM generates scenario-specific metrics and tasks for quality assessment.", "result": "LaQual achieves Spearman's rho correlation of 0.62-0.60 with human judgments in specific scenarios, reduces the candidate app pool by 66.7%-81.3%, and outperforms baselines in user studies (decision confidence: 5.45 vs 3.30; evaluation report value: 4.75 vs 2.25).", "conclusion": "LaQual demonstrates a scalable, objective, and user-centered solution for evaluating and recommending high-quality LLM apps by combining hierarchical classification, static filtering, and dynamic scenario-adaptive evaluation. It effectively reduces suboptimal app selection and outperforms baselines in user confidence and efficiency."}}
{"id": "2508.18453", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.18453", "abs": "https://arxiv.org/abs/2508.18453", "authors": ["Yaser Baseri", "Abdelhakim Senhaji Hafid", "Dimitrios Makrakis", "Hamidreza Fereidouni"], "title": "Privacy-Preserving Federated Learning Framework for Risk-Based Adaptive Authentication", "comment": null, "summary": "Balancing robust security with strong privacy guarantees is critical for\nRisk-Based Adaptive Authentication (RBA), particularly in decentralized\nsettings. Federated Learning (FL) offers a promising solution by enabling\ncollaborative risk assessment without centralizing user data. However, existing\nFL approaches struggle with Non-Independent and Identically Distributed\n(Non-IID) user features, resulting in biased, unstable, and poorly generalized\nglobal models. This paper introduces FL-RBA2, a novel Federated Learning\nframework for Risk-Based Adaptive Authentication that addresses Non-IID\nchallenges through a mathematically grounded similarity transformation. By\nconverting heterogeneous user features (including behavioral, biometric,\ncontextual, interaction-based, and knowledge-based modalities) into IID\nsimilarity vectors, FL-RBA2 supports unbiased aggregation and personalized risk\nmodeling across distributed clients. The framework mitigates cold-start\nlimitations via clustering-based risk labeling, incorporates Differential\nPrivacy (DP) to safeguard sensitive information, and employs Message\nAuthentication Codes (MACs) to ensure model integrity and authenticity.\nFederated updates are securely aggregated into a global model, achieving strong\nbalance between user privacy, scalability, and adaptive authentication\nrobustness. Rigorous game-based security proofs in the Random Oracle Model\nformally establish privacy, correctness, and adaptive security guarantees.\nExtensive experiments on keystroke, mouse, and contextual datasets validate\nFL-RBA2's effectiveness in high-risk user detection and its resilience to model\ninversion and inference attacks, even under strong DP constraints.", "AI": {"tldr": "FL-RBA2 is a privacy-preserving Federated Learning framework that addresses Non-IID data challenges in Risk-Based Adaptive Authentication through similarity-driven aggregation, DP safeguards, and secure MACs, achieving robust decentralized authentication.", "motivation": "Existing Federated Learning (FL) approaches for Risk-Based Adaptive Authentication (RBA) fail to handle non-IID user feature distributions, leading to biased global models, poor generalization, and security vulnerabilities. Decentralized systems require balancing privacy, scalability, and robust authentication.", "method": "FL-RBA2 employs a similarity transformation to convert non-IID heterogeneous user features into IID similarity vectors, enabling unbiased global model aggregation. It integrates clustering-based cold-start mitigation, Differential Privacy (DP) for data protection, and Message Authentication Codes (MACs) for model integrity.", "result": "FL-RBA2 demonstrates superior high-risk user detection accuracy and resilience against model inversion and inference attacks on real-world datasets under strong DP constraints. Game-based security proofs validate its privacy and adaptive security guarantees.", "conclusion": "FL-RBA2 provides a robust solution for decentralized risk-based authentication by addressing Non-IID data challenges, ensuring privacy-preserving, scalable, and secure adaptive authentication through mathematical similarity transformations and cryptographic techniques."}}
{"id": "2508.18675", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18675", "abs": "https://arxiv.org/abs/2508.18675", "authors": ["Xu Lu", "Weisong Sun", "Yiran Zhang", "Ming Hu", "Cong Tian", "Zhi Jin", "Yang Liu"], "title": "Requirements Development and Formalization for Reliable Code Generation: A Multi-Agent Vision", "comment": null, "summary": "Automated code generation has long been considered the holy grail of software\nengineering. The emergence of Large Language Models (LLMs) has catalyzed a\nrevolutionary breakthrough in this area. However, existing methods that only\nrely on LLMs remain inadequate in the quality of generated code, offering no\nguarantees of satisfying practical requirements. They lack a systematic\nstrategy for requirements development and modeling. Recently, LLM-based agents\ntypically possess powerful abilities and play an essential role in facilitating\nthe alignment of LLM outputs with user requirements. In this paper, we envision\nthe first multi-agent framework for reliable code generation based on\n\\textsc{re}quirements \\textsc{de}velopment and \\textsc{fo}rmalization, named\n\\textsc{ReDeFo}. This framework incorporates three agents, highlighting their\naugmentation with knowledge and techniques of formal methods, into the\nrequirements-to-code generation pipeline to strengthen quality assurance. The\ncore of \\textsc{ReDeFo} is the use of formal specifications to bridge the gap\nbetween potentially ambiguous natural language requirements and precise\nexecutable code. \\textsc{ReDeFo} enables rigorous reasoning about correctness,\nuncovering hidden bugs, and enforcing critical properties throughout the\ndevelopment process. In general, our framework aims to take a promising step\ntoward realizing the long-standing vision of reliable, auto-generated software.", "AI": {"tldr": "ReDeFo introduces a multi-agent code generation framework that uses formal specifications to transform natural language requirements into reliable code, achieving quality assurance through systematic formal verification.", "motivation": "Existing LLM-based code generation methods lack systematic requirements validation and quality guarantees. While LLM agents excel at alignment, there remains a critical gap in ensuring generated code satisfies practical requirements through formal verification.", "method": "The framework incorporates three agents integrated with formal methods techniques into a requirements-to-code pipeline. It utilizes formal specifications as a bridge between natural language requirements and executable code, enabling correctness reasoning, bug detection, and property enforcement.", "result": "ReDeFo enables rigorous reasoning about correctness proofs (including hidden bug detection), systematic property enforcement during code generation, and establishes a formal bridge between natural language requirements and executable specifications.", "conclusion": "This paper proposes ReDeFo, the first multi-agent framework for reliable code generation based on formal requirements development and specification, offering a promising step toward achieving trustworthy auto-generated software through rigorous quality assurance. "}}
{"id": "2508.18485", "categories": ["cs.CR", "cs.DS", "cs.SE", "E.3; F.2.1"], "pdf": "https://arxiv.org/pdf/2508.18485", "abs": "https://arxiv.org/abs/2508.18485", "authors": ["Peter T. Breuer"], "title": "An 8- and 12-bit block AES cipher", "comment": "This \"research note\" of mine from 2013 has been requested so often\n  from me over the years, along with requests for a way to cite it properly,\n  that I think it's appropriate to put it out on the web in a citeable archive.\n  Arxiv, step up", "summary": "Because it is so unusual, or hard to find, or expository, a truly tiny 8- or\n12-bit block AES (Rijndael) cipher is documented here, along with Java source\ncode.", "AI": {"tldr": "Authors created and shared a tiny 8/12-bit AES cipher in Java for educational/experimental use.", "motivation": "To provide an accessible, minimal AES variant for research/experimental purposes due to rarity of such implementations.", "method": "Documenting a reduced 8/12-bit block AES cipher with Java implementation.", "result": "Published documentation and Java source code for 8/12-bit block size AES.", "conclusion": "The creation of a small AES variant is feasible and useful for educational or constrained environments."}}
{"id": "2508.18721", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18721", "abs": "https://arxiv.org/abs/2508.18721", "authors": ["Yunrui Pei", "Hongshu Wang", "Wenjie Zhang", "Yun Lin", "Weiyu Kong", "Jin song Dong"], "title": "LLM as an Execution Estimator: Recovering Missing Dependency for Practical Time-travelling Debugging", "comment": null, "summary": "Dynamic data dependency, answering \"why a variable has this value?\", is\ncritical for debugging. Given a program step `s` reading a variable `v`,\nfinding the dynamic definition of `v` is challenging. Traditional methods\nrequire either (1) exhaustive instrumentation of all possible definitions of\n`v` in one run or (2) replicating the run to re-examine reads/writes - both\ncostly. If `v` is defined in a library, instrumentation becomes expensive; for\nnon-deterministic programs, replication is infeasible.\n  We propose RecovSlicing, which computes dynamic data dependency in a single\nrun with partial instrumentation. We leverage LLMs to infer program behavior\nfrom a partially recorded trace and code context. Given a trace and a slicing\ncriterion (step `s` and variable `v`), RecovSlicing estimates the runtime\ndefinition of `v` by recovering the missing execution.It also supports implicit\nvariables, such as those in `list.get(i)`. Technically, RecovSlicing tackles:\n(1) recovering runtime values and structures, and (2) aligning recovered\nvariables with recorded memory to analyze definitions.\n  We evaluate RecovSlicing on 8,300 data dependencies across three slicing\nbenchmarks, comparing it with Slicer4J, ND-Slicer, LLM Slicer, and re-execution\nSlicer. RecovSlicing achieves accuracy of 80.3%, 91.1%, and 98.3%,\noutperforming the best baseline (39.0%, 82.0%, 59.9%), and also leads in recall\n(91.1%, 91.1%, 98.3% vs. 53.4%, 79.1%, 87.1%). Integrated into a regression bug\nlocalizer, it enables finding 16% more regressions.", "AI": {"tldr": "RecovSlicing uses LLMs and partial traces for fast dynamic data dependency analysis, outperforming existing tools and enabling better debugging with 16% more regressions detected.", "motivation": "Traditional dependency analysis methods either require exhaustive instrumentation (costly for library variables) or program replication (infeasible for non-deterministic programs). This creates a need for efficient, accurate dynamic analysis without these limitations.", "method": "RecovSlicing combines partial instrumentation with LLM-based inference of program behavior from traces and code context. It tackles runtime value recovery and variable alignment in memory to estimate dynamic definitions while supporting implicit variables like collection accesses.", "result": "On 8,300 dependencies, RecovSlicing achieves 80.3%-98.3% accuracy vs. 39.0%-87.1% for best baselines and 91.1%-98.3% recall vs. 53.4%-87.1%. Integration into a bug localizer found 16% more regressions.", "conclusion": "RecovSlicing effectively computes dynamic data dependencies in a single run with partial instrumentation, leveraging LLMs to infer program behavior from partial traces. It outperforms existing methods in accuracy and recall, addressing non-deterministic programs and library variables where traditional approaches fail."}}
{"id": "2508.18488", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18488", "abs": "https://arxiv.org/abs/2508.18488", "authors": ["Martin Lochner", "Keegan Keplinger"], "title": "Collaborative Intelligence: Topic Modelling of Large Language Model use in Live Cybersecurity Operations", "comment": null, "summary": "Objective: This work describes the topic modelling of Security Operations\nCentre (SOC) use of a large language model (LLM), during live security\noperations. The goal is to better understand how these specialists voluntarily\nuse this tool.\n  Background: Human-automation teams have been extensively studied, but\ntransformer-based language models have sparked a new wave of collaboration. SOC\npersonnel at a major cybersecurity provider used an LLM to support live\nsecurity operations. This study examines how these specialists incorporated the\nLLM into their work.\n  Method: Our data set is the result of 10 months of SOC operators accessing\nGPT-4 over an internally deployed HTTP-based chat application. We performed two\ntopic modelling exercises, first using the established BERTopic model\n(Grootendorst, 2022), and second, using a novel topic modeling workflow.\n  Results: Both the BERTopic analysis and novel modelling approach revealed\nthat SOC operators primarily used the LLM to facilitate their understanding of\ncomplex text strings. Variations on this use-case accounted for ~40% of SOC LLM\nusage.\n  Conclusion: SOC operators are required to rapidly interpret complex commands\nand similar information. Their natural tendency to leverage LLMs to support\nthis activity indicates that their workflow can be supported and augmented by\ndesigning collaborative LLM tools for use in the SOC.\n  Application: This work can aid in creating next-generation tools for Security\nOperations Centres. By understanding common use-cases, we can develop workflows\nsupporting SOC task flow. One example is a right-click context menu for\nexecuting a command line analysis LLM call directly in the SOC environment.", "AI": {"tldr": "This paper analyzes how SOC personnel use LLMs in real-time security operations, finding they primarily leverage these tools to interpret complex text. The study advocates for collaborative LLM tool design to enhance SOC workflows.", "motivation": "To understand how SOC specialists voluntarily integrate LLMs into live security operations, as human-automation collaboration is evolving with transformer-based models.", "method": "Data was collected from 10 months of SOC operators using GPT-4 via an internally deployed chat application. Topic modeling was performed using the BERTopic model and a novel workflow to analyze LLM usage patterns.", "result": "Both modeling approaches revealed SOC operators primarily used the LLM (~40% of usage) to comprehend complex text strings, such as interpreting commands and similar information during operations.", "conclusion": "SOC operators can be supported and augmented by designing collaborative LLM tools tailored to their workflow. This includes creating next-generation tools like a right-click context menu for executing command line analysis LLM calls directly within the SOC environment, improving their ability to interpret complex information rapidly."}}
{"id": "2508.18771", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18771", "abs": "https://arxiv.org/abs/2508.18771", "authors": ["Kexin Sun", "Hongyu Kuang", "Sebastian Baltes", "Xin Zhou", "He Zhang", "Xiaoxing Ma", "Guoping Rong", "Dong Shao", "Christoph Treude"], "title": "Does AI Code Review Lead to Code Changes? A Case Study of GitHub Actions", "comment": null, "summary": "AI-based code review tools automatically review and comment on pull requests\nto improve code quality. Despite their growing presence, little is known about\ntheir actual impact. We present a large-scale empirical study of 16 popular\nAI-based code review actions for GitHub workflows, analyzing more than 22,000\nreview comments in 178 repositories. We investigate (1) how these tools are\nadopted and configured, (2) whether their comments lead to code changes, and\n(3) which factors influence their effectiveness. We develop a two-stage\nLLM-assisted framework to determine whether review comments are addressed, and\nuse interpretable machine learning to identify influencing factors. Our\nfindings show that, while adoption is growing, effectiveness varies widely.\nComments that are concise, contain code snippets, and are manually triggered,\nparticularly those from hunk-level review tools, are more likely to result in\ncode changes. These results highlight the importance of careful tool design and\nsuggest directions for improving AI-based code review systems.", "AI": {"tldr": "This study empirically evaluates 16 AI code review tools, finding that concise, manually triggered comments with code snippets most effectively drive code changes, underscoring the need for improved tool design.", "motivation": "The paper addresses the lack of understanding about the actual impact of AI-based code review tools despite their growing adoption, aiming to investigate their effectiveness and influencing factors.", "method": "The research uses a two-stage LLM-assisted framework to determine comment addressing and interpretable machine learning to identify effectiveness factors, analyzing over 22,000 review comments across 178 repositories.", "result": "Key results show that concise, code-snippet containing, and manually triggered comments (especially from hunk-level tools) are more likely to result in code changes, indicating adoption is growing but effectiveness varies.", "conclusion": "The study highlights the importance of careful tool design and suggests directions for improving AI-based code review systems, emphasizing that effectiveness varies widely and factors like concise comments and manual triggering enhance impact."}}
{"id": "2508.18649", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18649", "abs": "https://arxiv.org/abs/2508.18649", "authors": ["Nanxi Li", "Zhengyue Zhao", "Chaowei Xiao"], "title": "PRISM: Robust VLM Alignment with Principled Reasoning for Integrated Safety in Multimodality", "comment": null, "summary": "Safeguarding vision-language models (VLMs) is a critical challenge, as\nexisting methods often suffer from over-defense, which harms utility, or rely\non shallow alignment, failing to detect complex threats that require deep\nreasoning. To this end, we introduce PRISM (Principled Reasoning for Integrated\nSafety in Multimodality), a system2-like framework that aligns VLMs by\nembedding a structured, safety-aware reasoning process. Our framework consists\nof two key components: PRISM-CoT, a dataset that teaches safety-aware\nchain-of-thought reasoning, and PRISM-DPO, generated via Monte Carlo Tree\nSearch (MCTS) to further refine this reasoning through Direct Preference\nOptimization to help obtain a delicate safety boundary. Comprehensive\nevaluations demonstrate PRISM's effectiveness, achieving remarkably low attack\nsuccess rates including 0.15% on JailbreakV-28K for Qwen2-VL and 90%\nimprovement over the previous best method on VLBreak for LLaVA-1.5. PRISM also\nexhibits strong robustness against adaptive attacks, significantly increasing\ncomputational costs for adversaries, and generalizes effectively to\nout-of-distribution challenges, reducing attack success rates to just 8.70% on\nthe challenging multi-image MIS benchmark. Remarkably, this robust defense is\nachieved while preserving, and in some cases enhancing, model utility. To\npromote reproducibility, we have made our code, data, and model weights\navailable at https://github.com/SaFoLab-WISC/PRISM.", "AI": {"tldr": "PRISM is a vision-language model safety framework combining structured reasoning datasets and search-based optimization to achieve state-of-the-art attack resistance with minimal utility tradeoffs.", "motivation": "Existing VLM safeguard methods either over-defend (reducing utility) or use shallow alignment that fails against complex threats. A principled approach for robust safety-aware reasoning is needed to address these limitations.", "method": "PRISM employs a system2-like framework with two components: PRISM-CoT (a safety-aware reasoning dataset) and PRISM-DPO (Monte Carlo Tree Search-based Direct Preference Optimization). This structured approach aligns models through deep reasoning processes.", "result": "PRISM achieves 0.15% attack success rate on JailbreakV-28K, 90% improvement over prior methods on VLBreak, and 8.70% success rate on the MIS benchmark. It maintains model utility while increasing adversarial computational costs.", "conclusion": "PRISM effectively balances safety and utility in vision-language models, offering a robust defense against various attacks while maintaining or enhancing model performance."}}
{"id": "2508.18816", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18816", "abs": "https://arxiv.org/abs/2508.18816", "authors": ["Sabato Nocera", "Davide Fucci", "Giuseppe Scanniello"], "title": "Dealing with SonarQube Cloud: Initial Results from a Mining Software Repository Study", "comment": "Accepted for ESEM25 NIER track", "summary": "Background: Static Code Analysis (SCA) tools are widely adopted to enforce\ncode quality standards. However, little is known about how open-source projects\nuse and customize these tools. Aims: This paper investigates how GitHub\nprojects use and customize a popular SCA tool, namely SonarQube Cloud. Method:\nWe conducted a mining study of GitHub projects that are linked through GitHub\nActions to SonarQube Cloud projects. Results: Among 321 GitHub projects using\nSonarQube Cloud, 81% of them are correctly connected to SonarQube Cloud\nprojects, while others exhibit misconfigurations or restricted access. Among\n265 accessible SonarQube Cloud projects, 75% use the organization's default\nquality gate, i.e., a set of conditions that deployed source code must meet to\npass automated checks. While 55% of the projects use the built-in quality gate\nprovided by SonarQube Cloud, 45% of them customize their quality gate with\ndifferent conditions. Overall, the most common quality conditions align with\nSonarQube Cloud's \"Clean as You Code\" principle and enforce security,\nmaintainability, reliability, coverage, and a few duplicates on newly added or\nmodified source code. Conclusions: Many projects rely on predefined\nconfigurations, yet a significant portion customize their configurations to\nmeet specific quality goals. Building on our initial results, we envision a\nfuture research agenda linking quality gate configurations to actual software\noutcomes (e.g., improvement of software security). This would enable\nevidence-based recommendations for configuring SCA tools like SonarQube Cloud\nin various contexts.", "AI": {"tldr": "A study of 321 GitHub projects using SonarQube Cloud found 81% had valid connections, with 75% relying on default quality gates (45% customized). Most configurations align with SonarQube's 'Clean as You Code' principles focusing on security and maintainability. Future research will link these configurations to software outcomes.", "motivation": "This paper investigates how open-source projects use and customize SonarQube Cloud, an SCA tool, given the lack of prior knowledge about such practices.", "method": "The authors conducted a mining study of 321 GitHub projects linked to SonarQube Cloud via GitHub Actions. They analyzed connection validity (81% correctly connected), quality gate configurations (75% use organizational defaults, 45% customize), and aligned conditions with SonarQube's 'Clean as You Code' principles.", "result": "Among 321 projects, 81% correctly connected to SonarQube Cloud. 75% of 265 accessible projects used default quality gates; 45% customized them. Common conditions emphasized security, maintainability, reliability, coverage, and duplicate code management for new/modified code.", "conclusion": "Many projects rely on predefined configurations for SonarQube Cloud, but a significant portion customize quality gates to meet specific quality goals. The study suggests future research linking quality gate configurations to actual software outcomes (e.g., security improvements) to enable evidence-based recommendations."}}
{"id": "2508.18652", "categories": ["cs.CR", "cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.18652", "abs": "https://arxiv.org/abs/2508.18652", "authors": ["Runpeng Geng", "Yanting Wang", "Ying Chen", "Jinyuan Jia"], "title": "UniC-RAG: Universal Knowledge Corruption Attacks to Retrieval-Augmented Generation", "comment": "21 pages, 4 figures", "summary": "Retrieval-augmented generation (RAG) systems are widely deployed in\nreal-world applications in diverse domains such as finance, healthcare, and\ncybersecurity. However, many studies showed that they are vulnerable to\nknowledge corruption attacks, where an attacker can inject adversarial texts\ninto the knowledge database of a RAG system to induce the LLM to generate\nattacker-desired outputs. Existing studies mainly focus on attacking specific\nqueries or queries with similar topics (or keywords). In this work, we propose\nUniC-RAG, a universal knowledge corruption attack against RAG systems. Unlike\nprior work, UniC-RAG jointly optimizes a small number of adversarial texts that\ncan simultaneously attack a large number of user queries with diverse topics\nand domains, enabling an attacker to achieve various malicious objectives, such\nas directing users to malicious websites, triggering harmful command execution,\nor launching denial-of-service attacks. We formulate UniC-RAG as an\noptimization problem and further design an effective solution to solve it,\nincluding a balanced similarity-based clustering method to enhance the attack's\neffectiveness. Our extensive evaluations demonstrate that UniC-RAG is highly\neffective and significantly outperforms baselines. For instance, UniC-RAG could\nachieve over 90% attack success rate by injecting 100 adversarial texts into a\nknowledge database with millions of texts to simultaneously attack a large set\nof user queries (e.g., 2,000). Additionally, we evaluate existing defenses and\nshow that they are insufficient to defend against UniC-RAG, highlighting the\nneed for new defense mechanisms in RAG systems.", "AI": {"tldr": "UniC-RAG introduces a universal, highly effective method for corrupting RAG systems by injecting adversarial texts, achieving >90% success rates against 2k+ diverse queries, while exposing flaws in current defense mechanisms.", "motivation": "Existing attacks on RAG systems are limited to specific queries or topics. This work addresses the practical gap by proposing a universal attack capable of targeting a wide range of user queries for versatile malicious outcomes.", "method": "UniC-RAG is formulated as an optimization problem, employing a balanced similarity-based clustering method to optimize a small set of adversarial texts. These texts are designed to simultaneously attack diverse user queries across multiple domains.", "result": "UniC-RAG achieves over 90% attack success rate with 100 adversarial texts injected into databases with millions of texts, attacking up to 2,000 diverse queries. It significantly outperforms baselines and demonstrates existing defenses' ineffectiveness.", "conclusion": "UniC-RAG demonstrates a highly effective universal knowledge corruption attack on RAG systems, achieving high success rates while exposing vulnerabilities in existing defenses. The findings emphasize the urgent need for developing robust defensive mechanisms."}}
{"id": "2508.18955", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18955", "abs": "https://arxiv.org/abs/2508.18955", "authors": ["Yunbo Ni", "Shaohua Li"], "title": "Interleaving Large Language Models for Compiler Testing", "comment": null, "summary": "Testing compilers with AI models, especially large language models (LLMs),\nhas shown great promise. However, current approaches struggle with two key\nproblems: The generated programs for testing compilers are often too simple,\nand extensive testing with the LLMs is computationally expensive. In this\npaper, we propose a novel compiler testing framework that decouples the testing\nprocess into two distinct phases: an offline phase and an online phase. In the\noffline phase, we use LLMs to generate a collection of small but feature-rich\ncode pieces. In the online phase, we reuse these code pieces by strategically\ncombining them to build high-quality and valid test programs, which are then\nused to test compilers.\n  We implement this idea in a tool, LegoFuzz, for testing C compilers. The\nresults are striking: we found 66 bugs in GCC and LLVM, the most widely used C\ncompilers. Almost half of the bugs are miscompilation bugs, which are serious\nand hard-to-find bugs that none of the existing LLM-based tools could find. We\nbelieve this efficient design opens up new possibilities for using AI models in\nsoftware testing beyond just C compilers.", "AI": {"tldr": "LegoFuzz uses a two-phase AI-driven approach to find 66 C compiler bugs efficiently, overcoming prior limitations in complexity and cost.", "motivation": "Current LLM-based compiler testing methods generate overly simple programs or incur high computational costs, limiting their effectiveness.", "method": "A two-phase testing framework (offline and online) using LLMs: offline phase generates small, rich code snippets, while the online phase strategically combines them into valid test programs for compiler testing.", "result": "LegoFuzz detected 66 bugs (including 33 critical miscompilation bugs) in GCC and LLVM, outperforming existing LLM-based tools due to its efficient design.", "conclusion": "The proposed two-phase framework effectively addresses computational costs and program complexity issues in compiler testing, enabling efficient bug detection with AI models and suggesting broader applications in software testing."}}
{"id": "2508.18684", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.18684", "abs": "https://arxiv.org/abs/2508.18684", "authors": ["Shaswata Mitra", "Azim Bazarov", "Martin Duclos", "Sudip Mittal", "Aritran Piplai", "Md Rayhanur Rahman", "Edward Zieglar", "Shahram Rahimi"], "title": "FALCON: Autonomous Cyber Threat Intelligence Mining with LLMs for IDS Rule Generation", "comment": "11 pages, 5 figures, 4 tables", "summary": "Signature-based Intrusion Detection Systems (IDS) detect malicious activities\nby matching network or host activity against predefined rules. These rules are\nderived from extensive Cyber Threat Intelligence (CTI), which includes attack\nsignatures and behavioral patterns obtained through automated tools and manual\nthreat analysis, such as sandboxing. The CTI is then transformed into\nactionable rules for the IDS engine, enabling real-time detection and\nprevention. However, the constant evolution of cyber threats necessitates\nfrequent rule updates, which delay deployment time and weaken overall security\nreadiness. Recent advancements in agentic systems powered by Large Language\nModels (LLMs) offer the potential for autonomous IDS rule generation with\ninternal evaluation. We introduce FALCON, an autonomous agentic framework that\ngenerates deployable IDS rules from CTI data in real-time and evaluates them\nusing built-in multi-phased validators. To demonstrate versatility, we target\nboth network (Snort) and host-based (YARA) mediums and construct a\ncomprehensive dataset of IDS rules with their corresponding CTIs. Our\nevaluations indicate FALCON excels in automatic rule generation, with an\naverage of 95% accuracy validated by qualitative evaluation with 84%\ninter-rater agreement among multiple cybersecurity analysts across all metrics.\nThese results underscore the feasibility and effectiveness of LLM-driven data\nmining for real-time cyber threat mitigation.", "AI": {"tldr": "FALCON is an autonomous LLM-powered IDS rule generator that reduces update delays and improves real-time threat detection using validated CTI data, achieving 95% accuracy and high analyst agreement.", "motivation": "Evolving cyber threats require frequent IDS rule updates, which traditional CTI-driven methods delay due to manual processes, weakening security readiness.", "method": "FALCON utilizes agentic systems powered by LLMs to autonomously generate and validate IDS rules from CTI data, targeting both network (Snort) and host-based (YARA) systems with multi-phased internal validation.", "result": "FALCON achieved 95% accuracy in rule generation, with 84% inter-rater agreement among analysts, validating its effectiveness across network and host-based detection systems.", "conclusion": "The study demonstrates that LLM-driven frameworks like FALCON can autonomously generate accurate IDS rules, improving real-time cybersecurity readiness and reducing reliance on manual updates."}}
{"id": "2508.18993", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18993", "abs": "https://arxiv.org/abs/2508.18993", "authors": ["Ziyi Ni", "Huacan Wang", "Shuo Zhang", "Shuo Lu", "Ziyang He", "Wang You", "Zhenheng Tang", "Yuntao Du", "Bill Sun", "Hongzhang Liu", "Sen Hu", "Ronghao Chen", "Bo Li", "Xin Li", "Chen Hu", "Binxing Jiao", "Daxin Jiang", "Pin Lyu"], "title": "GitTaskBench: A Benchmark for Code Agents Solving Real-World Tasks Through Code Repository Leveraging", "comment": "Highly practical, Well-motivated, Actionable", "summary": "Beyond scratch coding, exploiting large-scale code repositories (e.g.,\nGitHub) for practical tasks is vital in real-world software development, yet\ncurrent benchmarks rarely evaluate code agents in such authentic,\nworkflow-driven scenarios. To bridge this gap, we introduce GitTaskBench, a\nbenchmark designed to systematically assess this capability via 54 realistic\ntasks across 7 modalities and 7 domains. Each task pairs a relevant repository\nwith an automated, human-curated evaluation harness specifying practical\nsuccess criteria. Beyond measuring execution and task success, we also propose\nthe alpha-value metric to quantify the economic benefit of agent performance,\nwhich integrates task success rates, token cost, and average developer\nsalaries. Experiments across three state-of-the-art agent frameworks with\nmultiple advanced LLMs show that leveraging code repositories for complex task\nsolving remains challenging: even the best-performing system, OpenHands+Claude\n3.7, solves only 48.15% of tasks. Error analysis attributes over half of\nfailures to seemingly mundane yet critical steps like environment setup and\ndependency resolution, highlighting the need for more robust workflow\nmanagement and increased timeout preparedness. By releasing GitTaskBench, we\naim to drive progress and attention toward repository-aware code reasoning,\nexecution, and deployment -- moving agents closer to solving complex,\nend-to-end real-world tasks. The benchmark and code are open-sourced at\nhttps://github.com/QuantaAlpha/GitTaskBench.", "AI": {"tldr": "GitTaskBench benchmarks code agents on real GitHub tasks: best system solves 48%, with failure roots in setup workflows. New metric quantifies economic value. Challenges remain for real-world code agent deployment.", "motivation": "Current benchmarks lack evaluation of code agents in authentic, workflow-driven scenarios using real-world repositories like GitHub. This gap hinders progress in developing practical code agents for complex software tasks.", "method": "The authors created GitTaskBench, a benchmark with 54 realistic tasks across 7 modalities and 7 domains, paired with human-curated evaluation harnesses. They introduced the alpha-value metric to quantify economic benefits and conducted experiments across three frameworks with multiple advanced LLMs.", "result": "The best-performing system (OpenHands+Claude 3.7) solved 48.15% of tasks. Error analysis revealed 50%+ failures stemmed from environment setup and dependency resolution, highlighting workflow management shortcomings. The alpha-value metric revealed performance-cost tradeoffs.", "conclusion": "The paper concludes that leveraging code repositories for complex task solving remains challenging, with even the best system solving only 48.15% of tasks. By releasing GitTaskBench, the authors aim to advance repository-aware code agents and address bottlenecks in workflow management for real-world software development."}}
{"id": "2508.18750", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.18750", "abs": "https://arxiv.org/abs/2508.18750", "authors": ["Zeng Zhang", "Xiaoqi Li"], "title": "Immutable Digital Recognition via Blockchain", "comment": null, "summary": "The process integrates the decentralised management and centralised operation\nmodels, aligning them with the national policy directives. The developed\nsolution enables the full utilisation of blockchain technology's advantages\nwhile also fostering community participation. Consequently, it establishes a\nsecure, legal, reliable, and dynamic electronic certification system.", "AI": {"tldr": "Hybrid blockchain system aligns decentralized/centralized models with policy for secure community-driven certification.", "motivation": "To leverage blockchain's benefits while ensuring compliance with regulations and fostering community involvement.", "method": "Combined decentralized management and centralized operation aligned with national policy using blockchain technology.", "result": "A secure, legal, reliable, and dynamic electronic certification system was established.", "conclusion": "The integration of decentralized and centralized models with blockchain creates a compliant, secure certification system."}}
{"id": "2508.19056", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.19056", "abs": "https://arxiv.org/abs/2508.19056", "authors": ["S. Panda", "D. Munjal", "D. P. Mohapatra"], "title": "A Slice-Based Change Impact Analysis for Regression Test Case Prioritization of Object-Oriented Programs", "comment": null, "summary": "Test case prioritization focuses on finding a suitable order of execution of\nthe test cases in a test suite to meet some performance goals like detecting\nfaults early. It is likely that some test cases execute the program parts that\nare more prone to errors and will detect more errors if executed early during\nthe testing process. Finding an optimal order of execution for the selected\nregression test cases saves time and cost of retesting. This paper presents a\nstatic approach to prioritizing the test cases by computing the affected\ncomponent coupling (ACC) of the affected parts of object-oriented programs. We\nconstruct a graph named affected slice graph (ASG) to represent these affected\nprogram parts.We determine the fault-proneness of the nodes of ASG by computing\ntheir respective ACC values. We assign higher priority to those test cases that\ncover the nodes with higher ACC values. Our analysis with mutation faults shows\nthat the test cases executing the fault-prone program parts have a higher\nchance to reveal faults earlier than other test cases in the test suite. The\nresult obtained from seven case studies justifies that our approach is feasible\nand gives acceptable performance in comparison to some existing techniques.", "AI": {"tldr": "This paper proposes a static test prioritization method using affected component coupling (ACC) in object-oriented systems. By constructing an affected slice graph (ASG) and prioritizing test cases based on ACC values, it achieves early fault detection. Seven case studies and mutation testing validate its effectiveness compared to existing approaches.", "motivation": "The need to optimize test case execution order for early fault detection and cost-effective regression testing motivates this work, as early fault exposure reduces retesting time and costs.", "method": "The paper introduces a static approach using affected component coupling (ACC) to prioritize test cases. It constructs an affected slice graph (ASG) to represent program parts, computes ACC values to determine fault-proneness, and assigns higher priority to test cases covering high-ACC nodes.", "result": "Seven case studies and mutation fault analysis demonstrate the approach's feasibility and acceptable performance compared to existing techniques, with test cases covering high-ACC nodes showing higher fault detection rates early.", "conclusion": "The proposed ACC-based static approach for test case prioritization is feasible and effective in detecting faults earlier, as validated by seven case studies and mutation fault analysis, showing comparable performance to existing techniques."}}
{"id": "2508.18805", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18805", "abs": "https://arxiv.org/abs/2508.18805", "authors": ["Rui Zhang", "Zihan Wang", "Tianli Yang", "Hongwei Li", "Wenbo Jiang", "Qingchuan Zhao", "Yang Liu", "Guowen Xu"], "title": "Hidden Tail: Adversarial Image Causing Stealthy Resource Consumption in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) are increasingly deployed in real-world\napplications, but their high inference cost makes them vulnerable to resource\nconsumption attacks. Prior attacks attempt to extend VLM output sequences by\noptimizing adversarial images, thereby increasing inference costs. However,\nthese extended outputs often introduce irrelevant abnormal content,\ncompromising attack stealthiness. This trade-off between effectiveness and\nstealthiness poses a major limitation for existing attacks. To address this\nchallenge, we propose \\textit{Hidden Tail}, a stealthy resource consumption\nattack that crafts prompt-agnostic adversarial images, inducing VLMs to\ngenerate maximum-length outputs by appending special tokens invisible to users.\nOur method employs a composite loss function that balances semantic\npreservation, repetitive special token induction, and suppression of the\nend-of-sequence (EOS) token, optimized via a dynamic weighting strategy.\nExtensive experiments show that \\textit{Hidden Tail} outperforms existing\nattacks, increasing output length by up to 19.2$\\times$ and reaching the\nmaximum token limit, while preserving attack stealthiness. These results\nhighlight the urgent need to improve the robustness of VLMs against\nefficiency-oriented adversarial threats. Our code is available at\nhttps://github.com/zhangrui4041/Hidden_Tail.", "AI": {"tldr": "Introduces 'Hidden Tail', a stealthy attack on Vision-Language Models that uses invisible special tokens to achieve 19.2\u00d7 longer outputs without detectable content anomalies, revealing critical robustness gaps in VLM efficiency defenses.", "motivation": "Existing resource consumption attacks on VLMs face a trade-off between increasing output length (effectiveness) and maintaining stealthiness due to irrelevant content in extended outputs. This limitation motivates the development of a more balanced attack approach.", "method": "The authors propose 'Hidden Tail', a method that crafts prompt-agnostic adversarial images using a composite loss function with dynamic weighting. This function simultaneously preserves semantic content, induces repetitive invisible special tokens, and suppresses the end-of-sequence (EOS) token to maximize output length.", "result": "Experiments show that 'Hidden Tail' achieves up to 19.2\u00d7 output length extension, reaches maximum token limits, and maintains attack stealthiness without introducing noticeable abnormalities in the output.", "conclusion": "This paper highlights the need to enhance the robustness of Vision-Language Models against stealthy resource consumption attacks by demonstrating a novel method that effectively balances effectiveness and stealthiness."}}
{"id": "2508.18832", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.18832", "abs": "https://arxiv.org/abs/2508.18832", "authors": ["Sara Saeidian", "Ata Yavuzy\u0131lmaz", "Leonhard Grosse", "Georg Schuppe", "Tobias J. Oechtering"], "title": "A Tight Context-aware Privacy Bound for Histogram Publication", "comment": "Submitted to IEEE Signal Processing Letters", "summary": "We analyze the privacy guarantees of the Laplace mechanism releasing the\nhistogram of a dataset through the lens of pointwise maximal leakage (PML).\nWhile differential privacy is commonly used to quantify the privacy loss, it is\na context-free definition that does not depend on the data distribution. In\ncontrast, PML enables a more refined analysis by incorporating assumptions\nabout the data distribution. We show that when the probability of each\nhistogram bin is bounded away from zero, stronger privacy protection can be\nachieved for a fixed level of noise. Our results demonstrate the advantage of\ncontext-aware privacy measures and show that incorporating assumptions about\nthe data can improve privacy-utility tradeoffs.", "AI": {"tldr": "Using pointwise maximal leakage instead of differential privacy, the paper shows that incorporating data distribution assumptions allows stronger privacy guarantees for the Laplace mechanism under certain distributional conditions.", "motivation": "Differential privacy is context-free and ignores data distribution, limiting its ability to refine privacy guarantees based on statistical properties of datasets.", "method": "The authors analyze the Laplace mechanism for histogram release using pointwise maximal leakage (PML), contrasting it with differential privacy by incorporating data distribution assumptions into privacy analysis.", "result": "When histogram bins have probabilities bounded from zero, the Laplace mechanism achieves stronger privacy under PML for equivalent noise levels compared to differential privacy.", "conclusion": "The paper concludes that context-aware privacy measures like PML offer stronger privacy protection compared to context-free measures like differential privacy, given reasonable assumptions about data distributions."}}
{"id": "2508.18942", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.18942", "abs": "https://arxiv.org/abs/2508.18942", "authors": ["Ahmed Mounsf Rafik Bendada", "Yacine Ghamri-Doudane"], "title": "EnerSwap: Large-Scale, Privacy-First Automated Market Maker for V2G Energy Trading", "comment": "11 pages, 7 figures, 1 table, 1 algorithm, Paper accepted in 27th\n  MSWiM Conference", "summary": "With the rapid growth of Electric Vehicle (EV) technology, EVs are destined\nto shape the future of transportation. The large number of EVs facilitates the\ndevelopment of the emerging vehicle-to-grid (V2G) technology, which realizes\nbidirectional energy exchanges between EVs and the power grid. This has led to\nthe setting up of electricity markets that are usually confined to a small\ngeographical location, often with a small number of participants. Usually,\nthese markets are manipulated by intermediaries responsible for collecting bids\nfrom prosumers, determining the market-clearing price, incorporating grid\nconstraints, and accounting for network losses. While centralized models can be\nhighly efficient, they grant excessive power to the intermediary by allowing\nthem to gain exclusive access to prosumers \\textquotesingle price preferences.\nThis opens the door to potential market manipulation and raises significant\nprivacy concerns for users, such as the location of energy providers. This lack\nof protection exposes users to potential risks, as untrustworthy servers and\nmalicious adversaries can exploit this information to infer trading activities\nand real identities. This work proposes a secure, decentralized exchange market\nbuilt on blockchain technology, utilizing a privacy-preserving Automated Market\nMaker (AMM) model to offer open and fair, and equal access to traders, and\nmitigates the most common trading-manipulation attacks. Additionally, it\nincorporates a scalable architecture based on geographical dynamic sharding,\nallowing for efficient resource allocation and improved performance as the\nmarket grows.", "AI": {"tldr": "This paper addresses flaws in centralized EV electricity markets by proposing a blockchain-based decentralized exchange with privacy-preserving AMM and scalable sharding to prevent manipulation, protect user privacy, and handle market growth.", "motivation": "Centralized EV electricity markets suffer from intermediary manipulation, privacy vulnerabilities (e.g., exposure of user locations and identities), and scalability limitations, necessitating a decentralized, secure alternative.", "method": "The authors propose a decentralized blockchain-based exchange using a privacy-preserving Automated Market Maker (AMM) model and a scalable architecture with geographical dynamic sharding to ensure fair trading and efficient resource allocation.", "result": "The solution enables open, fair, and tamper-proof trading while mitigating manipulation attacks and achieving scalability through dynamic sharding, making it suitable for growing EV networks.", "conclusion": "The paper concludes that a blockchain-based decentralized exchange with a privacy-preserving AMM and geographical sharding effectively addresses market manipulation, privacy risks, and scalability challenges in EV-driven electricity markets."}}
{"id": "2508.19219", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.19219", "abs": "https://arxiv.org/abs/2508.19219", "authors": ["Faezeh Dehghan Tarzjani", "Mostafa Salehi"], "title": "An Efficient Lightweight Blockchain for Decentralized IoT", "comment": null, "summary": "The Internet of Things (IoT) is applied in various fields, and the number of\nphysical devices connected to the IoT is increasingly growing. There are\nsignificant challenges to the IoT's growth and development, mainly due to the\ncentralized nature and large-scale IoT networks. The emphasis on the\ndecentralization of IoT's architecture can overcome challenges to IoT's\ncapabilities. A promising decentralized platform for IoT is blockchain. Owing\nto IoT devices' limited resources, traditional consensus algorithms such as PoW\nand PoS in the blockchain are computationally expensive. Therefore, the PoA\nconsensus algorithm is proposed in the blockchain consensus network for IoT.\nThe PoA selects the validator as Turn-based selection (TBS) that needs\noptimization and faces system reliability, energy consumption, latency, and low\nscalability. We propose an efficient, lightweight blockchain for decentralizing\nIoT architecture by using virtualization and clustering to increase\nproductivity and scalability to address these issues. We also introduce a novel\nPoA based on the Weight-Based-Selection (WBS) method for validators to validate\ntransactions and add them to the blockchain. By simulation, we evaluated the\nperformance of our proposed WBS method as opposed to TBS. The results show\nreduced energy consumption, and response time, and increased throughput.", "AI": {"tldr": "This paper proposes a lightweight blockchain solution for IoT using a Weight-Based-Selection consensus algorithm and virtualization to improve scalability and reduce energy consumption.", "motivation": "The study addresses the limitations of traditional blockchain consensus algorithms (e.g., PoW, PoS) for resource-constrained IoT devices and the shortcomings of existing TBS-based PoA (e.g., high energy consumption, latency, and poor scalability).", "method": "The authors propose a blockchain-based decentralized IoT architecture using (1) a novel Weight-Based-Selection (WBS) Proof-of-Authority (PoA) consensus algorithm for validator selection and (2) virtualization/clustering techniques to improve productivity and scalability. Simulations were conducted to evaluate performance.", "result": "Simulation results demonstrate the WBS method reduces energy consumption and response time while increasing throughput compared to existing TBS approaches.", "conclusion": "The paper concludes that the proposed WBS-based PoA and virtualization/clustering techniques effectively enhance blockchain efficiency and scalability for IoT, addressing the limitations of existing methods like TBS."}}
{"id": "2508.18947", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.18947", "abs": "https://arxiv.org/abs/2508.18947", "authors": ["Ronal Singh", "Shahroz Tariq", "Fatemeh Jalalvand", "Mohan Baruwal Chhetri", "Surya Nepal", "Cecile Paris", "Martin Lochner"], "title": "LLMs in the SOC: An Empirical Study of Human-AI Collaboration in Security Operations Centres", "comment": "22 pages, 9 figures, under review", "summary": "The integration of Large Language Models (LLMs) into Security Operations\nCentres (SOCs) presents a transformative, yet still evolving, opportunity to\nreduce analyst workload through human-AI collaboration. However, their\nreal-world application in SOCs remains underexplored. To address this gap, we\npresent a longitudinal study of 3,090 analyst queries from 45 SOC analysts over\n10 months. Our analysis reveals that analysts use LLMs as on-demand aids for\nsensemaking and context-building, rather than for making high-stakes\ndeterminations, preserving analyst decision authority. The majority of queries\nare related to interpreting low-level telemetry (e.g., commands) and refining\ntechnical communication through short (1-3 turn) interactions. Notably, 93% of\nqueries align with established cybersecurity competencies (NICE Framework),\nunderscoring the relevance of LLM use for SOC-related tasks. Despite variations\nin tasks and engagement, usage trends indicate a shift from occasional\nexploration to routine integration, with growing adoption and sustained use\namong a subset of analysts. We find that LLMs function as flexible, on-demand\ncognitive aids that augment, rather than replace, SOC expertise. Our study\nprovides actionable guidance for designing context-aware, human-centred AI\nassistance in security operations, highlighting the need for further\nin-the-wild research on real-world analyst-LLM collaboration, challenges, and\nimpacts.", "AI": {"tldr": "SOC analysts use LLMs as on-demand cognitive aids for task efficiency and context-building, preserving human decision-making while highlighting design opportunities for human-centric AI tools.", "motivation": "To address the underexplored real-world application of LLMs in SOCs and understand their practical role in analyst workflows.", "method": "Longitudinal analysis of 3,090 queries from 45 SOC analysts over 10 months, examining query patterns, NICE framework alignment, and usage trends.", "result": "93% of LLM queries align with NICE cybersecurity competencies; usage shifts from exploration to routine integration, focusing on telemetry interpretation and technical communication refinement.", "conclusion": "LLMs serve as augmentative cognitive aids in SOCs, maintaining analyst decision authority while offering actionable design insights for human-AI collaboration."}}
{"id": "2508.18976", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18976", "abs": "https://arxiv.org/abs/2508.18976", "authors": ["Stephen Meisenbacher", "Alexandra Klymenko", "Andreea-Elena Bodea", "Florian Matthes"], "title": "The Double-edged Sword of LLM-based Data Reconstruction: Understanding and Mitigating Contextual Vulnerability in Word-level Differential Privacy Text Sanitization", "comment": "15 pages, 4 figures, 8 tables. Accepted to WPES @ CCS 2025", "summary": "Differentially private text sanitization refers to the process of privatizing\ntexts under the framework of Differential Privacy (DP), providing provable\nprivacy guarantees while also empirically defending against adversaries seeking\nto harm privacy. Despite their simplicity, DP text sanitization methods\noperating at the word level exhibit a number of shortcomings, among them the\ntendency to leave contextual clues from the original texts due to randomization\nduring sanitization $\\unicode{x2013}$ this we refer to as $\\textit{contextual\nvulnerability}$. Given the powerful contextual understanding and inference\ncapabilities of Large Language Models (LLMs), we explore to what extent LLMs\ncan be leveraged to exploit the contextual vulnerability of DP-sanitized texts.\nWe expand on previous work not only in the use of advanced LLMs, but also in\ntesting a broader range of sanitization mechanisms at various privacy levels.\nOur experiments uncover a double-edged sword effect of LLM-based data\nreconstruction attacks on privacy and utility: while LLMs can indeed infer\noriginal semantics and sometimes degrade empirical privacy protections, they\ncan also be used for good, to improve the quality and privacy of DP-sanitized\ntexts. Based on our findings, we propose recommendations for using LLM data\nreconstruction as a post-processing step, serving to increase privacy\nprotection by thinking adversarially.", "AI": {"tldr": "This research demonstrates that Large Language Models can both weaken and strengthen differentially private text sanitization, proposing adversarial use of LLMs for improved privacy protections.", "motivation": "Current word-level DP text sanitization methods suffer from 'contextual vulnerabilities' that leave exploitable clues in the sanitized output. This work investigates how LLMs' contextual inference capabilities can both threat and enhance DP protections.", "method": "The paper employs advanced Large Language Models (LLMs) to conduct data reconstruction attacks on DP-sanitized texts. It evaluates these attacks across diverse sanitization mechanisms and privacy levels, extending prior work by incorporating more sophisticated LLMs and broader testing scenarios.", "result": "Experiments reveal a dual impact: LLM-based attacks degrade empirical privacy protections by reconstructing semantics but simultaneously enable improved sanitization outcomes when leveraged for post-processing. LLMs demonstrate both destructive and constructive potential for differential privacy in text data.", "conclusion": "The study concludes that while LLMs can exploit contextual vulnerabilities in DP-sanitized texts, they can also be harnessed to improve both privacy protections and text utility through adversarial post-processing techniques."}}
{"id": "2508.19072", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19072", "abs": "https://arxiv.org/abs/2508.19072", "authors": ["Sidahmed Benabderrahmane", "Talal Rahwan"], "title": "Attackers Strike Back? Not Anymore -- An Ensemble of RL Defenders Awakens for APT Detection", "comment": null, "summary": "Advanced Persistent Threats (APTs) represent a growing menace to modern\ndigital infrastructure. Unlike traditional cyberattacks, APTs are stealthy,\nadaptive, and long-lasting, often bypassing signature-based detection systems.\nThis paper introduces a novel framework for APT detection that unites deep\nlearning, reinforcement learning (RL), and active learning into a cohesive,\nadaptive defense system. Our system combines auto-encoders for latent\nbehavioral encoding with a multi-agent ensemble of RL-based defenders, each\ntrained to distinguish between benign and malicious process behaviors. We\nidentify a critical challenge in existing detection systems: their static\nnature and inability to adapt to evolving attack strategies. To this end, our\narchitecture includes multiple RL agents (Q-Learning, PPO, DQN, adversarial\ndefenders), each analyzing latent vectors generated by an auto-encoder. When\nany agent is uncertain about its decision, the system triggers an active\nlearning loop to simulate expert feedback, thus refining decision boundaries.\nAn ensemble voting mechanism, weighted by each agent's performance, ensures\nrobust final predictions.", "AI": {"tldr": "A novel APT detection system uses RL and active learning to adapt to evolving threats, overcoming static detection limitations. Auto-encoders and ensemble methods enhance accuracy and robustness.", "motivation": "Existing APT detection systems are static and fail to adapt to evolving attacks. Signature-based methods are inadequate for stealthy, adaptive threats, necessitating a dynamic, learning-based approach.", "method": "The framework uses auto-encoders for latent behavior encoding, multi-agent RL defenders (Q-Learning, PPO, DQN, adversarial agents), and active learning loops for decision refinement. Ensemble voting, weighted by agent performance, ensures final predictions.", "result": "The system effectively handles evolving APT strategies through adaptive agent training and active learning, while the ensemble mechanism improves decision robustness and accuracy.", "conclusion": "The paper presents an adaptive framework combining deep learning, reinforcement learning, and active learning to improve APT detection, addressing the limitations of static systems and enhancing robustness through ensemble methods."}}
{"id": "2508.19115", "categories": ["cs.CR", "cs.AI", "E.3; I.2.6; I.5.1; F.1.2"], "pdf": "https://arxiv.org/pdf/2508.19115", "abs": "https://arxiv.org/abs/2508.19115", "authors": ["Joshua Lee", "Ali Arastehfard", "Weiran Liu", "Xuegang Ban", "Yuan Hong"], "title": "SecureV2X: An Efficient and Privacy-Preserving System for Vehicle-to-Everything (V2X) Applications", "comment": "10 pages, 3 figures", "summary": "Autonomous driving and V2X technologies have developed rapidly in the past\ndecade, leading to improved safety and efficiency in modern transportation.\nThese systems interact with extensive networks of vehicles, roadside\ninfrastructure, and cloud resources to support their machine learning\ncapabilities. However, the widespread use of machine learning in V2X systems\nraises issues over the privacy of the data involved. This is particularly\nconcerning for smart-transit and driver safety applications which can\nimplicitly reveal user locations or explicitly disclose medical data such as\nEEG signals. To resolve these issues, we propose SecureV2X, a scalable,\nmulti-agent system for secure neural network inferences deployed between the\nserver and each vehicle. Under this setting, we study two multi-agent V2X\napplications: secure drowsiness detection, and secure red-light violation\ndetection. Our system achieves strong performance relative to baselines, and\nscales efficiently to support a large number of secure computation interactions\nsimultaneously. For instance, SecureV2X is $9.4 \\times$ faster, requires\n$143\\times$ fewer computational rounds, and involves $16.6\\times$ less\ncommunication on drowsiness detection compared to other secure systems.\nMoreover, it achieves a runtime nearly $100\\times$ faster than state-of-the-art\nbenchmarks in object detection tasks for red light violation detection.", "AI": {"tldr": "SecureV2X is a privacy-preserving multi-agent framework for V2X systems that enables fast, secure neural inference for drowsiness and red-light violation detection with 10-100\u00d7 efficiency gains over existing solutions.", "motivation": "The proliferation of machine learning in vehicular networks introduces critical privacy risks, particularly for safety-critical applications that handle sensitive data like location information and brainwave signals (EEG). Existing security solutions lack scalability and efficiency for real-time V2X operations.", "method": "We propose SecureV2X, a multi-agent system architecture that performs secure neural network inferences between servers and vehicles. The approach focuses on two applications: secure drowsiness detection using EEG signals and secure red-light violation detection through object identification, implementing privacy-preserving computation protocols.", "result": "SecureV2X demonstrates 9.4\u00d7 speed improvement and 16.6\u00d7 lower communication costs for drowsiness detection compared to existing secure systems. For red-light violation detection, it achieves nearly 100\u00d7 faster runtime than state-of-the-art methods, with 143\u00d7 fewer computational rounds while maintaining accuracy.", "conclusion": "SecureV2X effectively addresses privacy challenges in V2X systems by providing a scalable, efficient framework for secure neural network inferences, achieving significant performance gains while maintaining data privacy in safety-critical applications."}}
