<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 8]
- [cs.SE](#cs.SE) [Total: 5]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [A Novel APVD Steganography Technique Incorporating Pseudorandom Pixel Selection for Robust Image Security](https://arxiv.org/abs/2507.13367)
*Mehrab Hosain,Rajiv Kapoor*

Main category: cs.CR

TL;DR: The paper proposes a steganography method combining Adaptive Pixel Value Differencing (APVD) with pseudorandom pixel selection, addressing the 'unused blocks' issue to enhance security, capacity, and image quality metrics (PSNR, UIQ, SSIM) across various image types.


<details>
  <summary>Details</summary>
Motivation: The existing APVD method suffers from 'unused blocks' which reduce security, embedding capacity, and visual quality of steganographic images.

Method: Integrates APVD with a pseudorandom pixel selection mechanism to optimize block utilization and balance data embedding requirements.

Result: Experimental results show improved PSNR, UIQ, and SSIM values compared to APVD and other contemporary methods, demonstrating enhanced performance in security, capacity, and image fidelity.

Conclusion: The proposed APVD-pseudorandom integration offers a versatile, effective solution for secure data hiding in both color and grayscale images without compromising visual quality.

Abstract: Steganography is the process of embedding secret information discreetly
within a carrier, ensuring secure exchange of confidential data. The Adaptive
Pixel Value Differencing (APVD) steganography method, while effective,
encounters certain challenges like the "unused blocks" issue. This problem can
cause a decrease in security, compromise the embedding capacity, and lead to
lower visual quality. This research presents a novel steganographic strategy
that integrates APVD with pseudorandom pixel selection to effectively mitigate
these issues. The results indicate that the new method outperforms existing
techniques in aspects of security, data hiding capacity, and the preservation
of image quality. Empirical results reveal that the combination of APVD with
pseudorandom pixel selection significantly enhances key image quality metrics
such as Peak Signal-to-Noise Ratio (PSNR), Universal Image Quality Index (UIQ),
and Structural Similarity Index (SSIM), surpassing other contemporary methods
in performance. The newly proposed method is versatile, able to handle a
variety of cover and secret images in both color and grayscale, thereby
ensuring secure data transmission without compromising the aesthetic quality of
the image.

</details>


### [2] [PHASE: Passive Human Activity Simulation Evaluation](https://arxiv.org/abs/2507.13505)
*Steven Lamp,Jason D. Hiser,Anh Nguyen-Tuong,Jack W. Davidson*

Main category: cs.CR

TL;DR: The paper introduces PHASE, a machine learning framework for passive evaluation of human-like behavior in synthetic user personas within cybersecurity simulations, achieving 90% accuracy.


<details>
  <summary>Details</summary>
Motivation: Cybersecurity simulations need realistic human behavior to be effective, but existing methods lack quantitative ways to assess behavioral fidelity of synthetic personas.

Method: PHASE analyzes Zeek connection logs passively using DNS-based traffic classification and SHAP analysis to identify temporal/behavioral signatures distinguishing human and non-human activity.

Result: Demonstrated accurate detection of non-human behavioral patterns in synthetic personas, enabling a revised configuration with significantly improved human-likeness through behavioral analysis.

Conclusion: Active simulation environments can now achieve higher realism by leveraging PHASE's passive behavioral fidelity evaluation to refine synthetic user personas effectively.

Abstract: Cybersecurity simulation environments, such as cyber ranges, honeypots, and
sandboxes, require realistic human behavior to be effective, yet no
quantitative method exists to assess the behavioral fidelity of synthetic user
personas. This paper presents PHASE (Passive Human Activity Simulation
Evaluation), a machine learning framework that analyzes Zeek connection logs
and distinguishes human from non-human activity with over 90\% accuracy. PHASE
operates entirely passively, relying on standard network monitoring without any
user-side instrumentation or visible signs of surveillance. All network
activity used for machine learning is collected via a Zeek network appliance to
avoid introducing unnecessary network traffic or artifacts that could disrupt
the fidelity of the simulation environment. The paper also proposes a novel
labeling approach that utilizes local DNS records to classify network traffic,
thereby enabling machine learning analysis. Furthermore, we apply SHAP (SHapley
Additive exPlanations) analysis to uncover temporal and behavioral signatures
indicative of genuine human users. In a case study, we evaluate a synthetic
user persona and identify distinct non-human patterns that undermine behavioral
realism. Based on these insights, we develop a revised behavioral configuration
that significantly improves the human-likeness of synthetic activity yielding a
more realistic and effective synthetic user persona.

</details>


### [3] [FuSeFL: Fully Secure and Scalable Cross-Silo Federated Learning](https://arxiv.org/abs/2507.13591)
*Sahar Ghoflsaz Ghinani,Elaheh Sadredini*

Main category: cs.CR

TL;DR: FuSeFL is a fully secure, scalable Federated Learning (FL) framework for cross-silo environments that uses decentralized client-pair training with lightweight MPC and secure aggregation to maintain data, model, and update confidentiality while reducing communication latency (95% lower) and server memory (50% lower) without compromising accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing secure FL solutions using cryptography suffer from high computational, communication, or memory overheads, limiting practicality. Additionally, global model confidentiality is often overlooked in sensitive domains.

Method: FuSeFL decentralizes training across client pairs via lightweight secure multiparty computation (MPC) and restricts the server to only secure aggregation. This eliminates server bottlenecks, avoids data sharing, and maintains full confidentiality of all elements through training protocol design.

Result: Achieved 95% reduction in communication latency, 50% reduction in server memory usage, resistance to inference attacks (model inversion, membership inference, gradient leakage), and higher accuracy compared to prior secure FL methods in cross-silo scenarios.

Conclusion: FuSeFL enables practical secure FL at scale by simultaneously addressing data confidentiality, model secrecy, and system efficiency through its decentralized architecture and optimized MPC aggregation approach, making it suitable for large-scale privacy-sensitive deployments.

Abstract: Federated Learning (FL) enables collaborative model training without
centralizing client data, making it attractive for privacy-sensitive domains.
While existing approaches employ cryptographic techniques such as homomorphic
encryption, differential privacy, or secure multiparty computation to mitigate
inference attacks-including model inversion, membership inference, and gradient
leakage-they often suffer from high computational, communication, or memory
overheads. Moreover, many methods overlook the confidentiality of the global
model itself, which may be proprietary and sensitive. These challenges limit
the practicality of secure FL, especially in cross-silo deployments involving
large datasets and strict compliance requirements.
  We present FuSeFL, a fully secure and scalable FL scheme designed for
cross-silo settings. FuSeFL decentralizes training across client pairs using
lightweight secure multiparty computation (MPC), while confining the server's
role to secure aggregation. This design eliminates server bottlenecks, avoids
data offloading, and preserves full confidentiality of data, model, and updates
throughout training. FuSeFL defends against inference threats, achieves up to
95% lower communication latency and 50% lower server memory usage, and improves
accuracy over prior secure FL solutions, demonstrating strong security and
efficiency at scale.

</details>


### [4] [GIFT: Gradient-aware Immunization of diffusion models against malicious Fine-Tuning with safe concepts retention](https://arxiv.org/abs/2507.13598)
*Amro Abdalla,Ismail Shaheen,Dan DeGenaro,Rupayan Mallick,Bogdan Raita,Sarah Adel Bargal*

Main category: cs.CR

TL;DR: GIFT is a gradient-aware immunization technique that defends diffusion models against adversarial fine-tuning while maintaining safe content generation. It uses bi-level optimization to degrade harmful concept representation and preserve performance on benign data.


<details>
  <summary>Details</summary>
Motivation: Current safety mechanisms like safety checkers are easily bypassed, and concept erasure methods fail when an attacker applies adversarial fine-tuning. There's a need for robust protection that prevents models from re-learning harmful concepts post-safety interventions.

Method: GIFT frames immunization as a bi-level optimization problem: the upper-level objective disrupts harmful concept representation via representation noising and maximization, while the lower-level objective ensures the model retains proficiency in generating non-malicious content through optimization constraints.

Result: Experiments demonstrate GIFT significantly impairs the model's ability to re-learn adversarial harmful concepts during malicious fine-tuning while maintaining generation quality on safe data, making it resistant to such attacks without compromising utility.

Conclusion: GIFT provides an effective framework for creating inherently safer diffusion models by integrating proactive immunization into their design. This approach shifts safety from reactive post-hoc measures to structural robustness, opening new research directions for secure generative AI.

Abstract: We present GIFT: a {G}radient-aware {I}mmunization technique to defend
diffusion models against malicious {F}ine-{T}uning while preserving their
ability to generate safe content. Existing safety mechanisms like safety
checkers are easily bypassed, and concept erasure methods fail under
adversarial fine-tuning. GIFT addresses this by framing immunization as a
bi-level optimization problem: the upper-level objective degrades the model's
ability to represent harmful concepts using representation noising and
maximization, while the lower-level objective preserves performance on safe
data. GIFT achieves robust resistance to malicious fine-tuning while
maintaining safe generative quality. Experimental results show that our method
significantly impairs the model's ability to re-learn harmful concepts while
maintaining performance on safe content, offering a promising direction for
creating inherently safer generative models resistant to adversarial
fine-tuning attacks.

</details>


### [5] [Large Language Models in Cybersecurity: Applications, Vulnerabilities, and Defense Techniques](https://arxiv.org/abs/2507.13629)
*Niveen O. Jaffal,Mohammed Alkhanafseh,David Mohaisen*

Main category: cs.CR

TL;DR: This survey explores how Large Language Models (LLMs) are applied in cybersecurity for threat detection, vulnerability assessment, incident response across IoT, blockchain, and hardware domains, while systematically analyzing their own vulnerabilities and proposing mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of LLMs in cybersecurity applications, combined with their domain versatility and contextual reasoning capabilities, necessitates a thorough examination of both their integrative potential and inherent security risks to guide future deployment.

Method: The paper conducts a comprehensive review of recent advancements in LLM cybersecurity applications, categorizes case studies into integration and security risks, and synthesizes limitations through thematic analysis of existing research.

Result: Identified key cybersecurity domains where LLMs outperform traditional approaches, mapped common vulnerabilities in LLM architectures, and provided actionable strategies for building secure cyber defense systems while addressing current technology limitations.

Conclusion: LLMs represent transformative potential for cybersecurity but require careful implementation with robust mitigation strategies to address their dual role as both solution and security target, ensuring scalable and future-ready defense architectures.

Abstract: Large Language Models (LLMs) are transforming cybersecurity by enabling
intelligent, adaptive, and automated approaches to threat detection,
vulnerability assessment, and incident response. With their advanced language
understanding and contextual reasoning, LLMs surpass traditional methods in
tackling challenges across domains such as IoT, blockchain, and hardware
security. This survey provides a comprehensive overview of LLM applications in
cybersecurity, focusing on two core areas: (1) the integration of LLMs into key
cybersecurity domains, and (2) the vulnerabilities of LLMs themselves, along
with mitigation strategies. By synthesizing recent advancements and identifying
key limitations, this work offers practical insights and strategic
recommendations for leveraging LLMs to build secure, scalable, and future-ready
cyber defense systems.

</details>


### [6] [TopicAttack: An Indirect Prompt Injection Attack via Topic Transition](https://arxiv.org/abs/2507.13686)
*Yulin Chen,Haoran Li,Yuexin Li,Yue Liu,Yangqiu Song,Bryan Hooi*

Main category: cs.CR

TL;DR: This paper introduces TopicAttack, a method for indirect prompt injection attacks in LLMs, which uses fabricated conversational transitions to smoothly shift topics toward malicious instructions, achieving over 90% attack success rates even under defenses.


<details>
  <summary>Details</summary>
Motivation: LLMs are vulnerable to indirect prompt injection attacks due to their inability to distinguish instructions from data content. Existing methods' abrupt injections reduce their effectiveness, motivating the need for smoother transitions.

Method: TopicAttack generates a gradual, plausible conversational transition prompt by introducing fabricated context, incrementally steering the LLM toward the malicious instruction embedded in external data sources.

Result: TopicAttack achieves state-of-the-art performance with an attack success rate (ASR) over 90% in most cases, even when multiple defenses are applied, and demonstrates a significantly higher injected-to-original attention ratio compared to baseline methods.

Conclusion: TopicAttack effectively bypasses LLM defenses against indirect prompt injection attacks through its smooth topic transition mechanism, supported by high ASR and attention score analysis, indicating a critical threat requiring improved mitigation strategies.

Abstract: Large language models (LLMs) have shown remarkable performance across a range
of NLP tasks. However, their strong instruction-following capabilities and
inability to distinguish instructions from data content make them vulnerable to
indirect prompt injection attacks. In such attacks, instructions with malicious
purposes are injected into external data sources, such as web documents. When
LLMs retrieve this injected data through tools, such as a search engine and
execute the injected instructions, they provide misled responses. Recent attack
methods have demonstrated potential, but their abrupt instruction injection
often undermines their effectiveness. Motivated by the limitations of existing
attack methods, we propose TopicAttack, which prompts the LLM to generate a
fabricated conversational transition prompt that gradually shifts the topic
toward the injected instruction, making the injection smoother and enhancing
the plausibility and success of the attack. Through comprehensive experiments,
TopicAttack achieves state-of-the-art performance, with an attack success rate
(ASR) over 90\% in most cases, even when various defense methods are applied.
We further analyze its effectiveness by examining attention scores. We find
that a higher injected-to-original attention ratio leads to a greater success
probability, and our method achieves a much higher ratio than the baseline
methods.

</details>


### [7] [Quantum Blockchain Survey: Foundations, Trends, and Gaps](https://arxiv.org/abs/2507.13720)
*Saurav Ghosh*

Main category: cs.CR

TL;DR: This paper surveys post-quantum and quantum blockchain systems, comparing their cryptographic foundations, architectural designs, implementation challenges, trade-offs (security/scalability/deployment), and open research problems.


<details>
  <summary>Details</summary>
Motivation: Quantum computing threatens classical blockchain cryptography, necessitating evaluation of emerging quantum-resistant solutions to guide secure system development in the quantum era.

Method: The authors conduct a comprehensive review and comparative analysis of technical proposals in post-quantum blockchains (quantum-resistant algorithms) and quantum blockchains (quantum property implementation), addressing security assumptions and scalability implications.

Result: Key findings include trade-off patterns between security guarantees and system performance, deployment feasibility of post-quantum algorithms, and unresolved challenges in quantum network infrastructure and consensus mechanism redesign.

Conclusion: The analysis establishes a reference framework for prioritizing research efforts and technical upgrades required to prepare blockchain systems for quantum computing threats.

Abstract: Quantum computing poses fundamental risks to classical blockchain systems by
undermining widely used cryptographic primitives. In response, two major
research directions have emerged: post-quantum blockchains, which integrate
quantum-resistant algorithms, and quantum blockchains, which leverage quantum
properties such as entanglement and quantum key distribution. This survey
reviews key developments in both areas, analyzing their cryptographic
foundations, architectural designs, and implementation challenges. This work
provides a comparative overview of technical proposals, highlight trade-offs in
security, scalability, and deployment, and identify open research problems
across hardware, consensus, and network design. The goal is to offer a
structured and comprehensive reference for advancing secure blockchain systems
in the quantum era.

</details>


### [8] [Developers Insight On Manifest v3 Privacy and Security Webextensions](https://arxiv.org/abs/2507.13926)
*Libor Polčák,Giorgio Maone,Michael McMahon,Martin Bednář*

Main category: cs.CR

TL;DR: This paper examines the challenges and opportunities in Chrome's Manifest v3 API transition for webextensions, revealing mixed responses with critical API limitations affecting functionality migration.


<details>
  <summary>Details</summary>
Motivation: Browser APIs influence webextension capabilities. Chrome's shift to Manifest v3 necessitates understanding its impact on privacy, security, and user experience.

Method: In-depth structured qualitative research analyzing responses from projects adopting, adapting, or resisting Manifest v3 changes.

Result: Manifest v3 migration results in differential outcomes: some extensions maintain functionality, while others lose features or stop updating due to missing APIs like reliable content injection and secure storage.

Conclusion: The Manifest v3 transition offers benefits but also significant limitations for webextensions, requiring improved browser APIs to address identified critical gaps.

Abstract: Webextensions can improve web browser privacy, security, and user experience.
The APIs offered by the browser to webextensions affect possible functionality.
Currently, Chrome transitions to a modified set of APIs called Manifest v3.
This paper studies the challenges and opportunities of Manifest v3 with an
in-depth structured qualitative research. Even though some projects observed
positive effects, a majority expresses concerns over limited benefits to users,
removal of crucial APIs, or the need to find workarounds. Our findings indicate
that the transition affects different types of webextensions differently; some
can migrate without losing functionality, while other projects remove
functionality or decline to update. The respondents identified several critical
missing APIs, including reliable APIs to inject content scripts, APIs for
storing confidential content, and others.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [9] [Socio-Technical Smell Dynamics in Code Samples: A Multivocal Review on Emergence, Evolution, and Co-Occurrence](https://arxiv.org/abs/2507.13481)
*Arthur Bueno,Bruno Cafeo,Maria Cagnin,Awdren Fontão*

Main category: cs.SE

TL;DR: This study examines the co-occurrence and evolution of code and community smells in open-source code samples, revealing that community-level dysfunctions often anticipate or worsen technical degradation, requiring socio-technical governance strategies.


<details>
  <summary>Details</summary>
Motivation: Code samples in open-source ecosystems are crucial for knowledge transfer but face socio-technical degradation due to informal governance. Understanding the relationship between community smells (e.g., fragmented communication) and code smells (e.g., poor modularity) is essential to mitigating this decay.

Method: A Multivocal Literature Review synthesized 30 peer-reviewed papers and 17 practitioner-oriented sources (2013-2024), focusing on thematic patterns of smell co-occurrence and their socio-technical interplay.

Result: Nine recurring socio-technical patterns were identified, demonstrating that community smells like radio silence and centralized ownership often precede or amplify code smells. Limited onboarding and lack of refactoring further enable smell accumulation.

Conclusion: Community dysfunctions in OSSECOs are early indicators of maintainability decay in code samples. The findings emphasize developing tailored socio-technical quality metrics and lightweight governance to preserve their instructional value.

Abstract: Code samples play a pivotal role in open-source ecosystems (OSSECO), serving
as lightweight artifacts that support knowledge transfer, onboarding, and
framework adoption. Despite their instructional relevance, these samples are
often governed informally, with minimal review and unclear ownership, which
increases their exposure to socio-technical degradation. In this context, the
co-occurrence and longitudinal interplay of code smells (e.g., large classes,
poor modularity) and community smells (e.g., lone contributors, fragmented
communication) become particularly critical. While each type of smell has been
studied in isolation, little is known about how community-level dysfunctions
anticipate or exacerbate technical anomalies in code samples over time. This
study investigates how code and community smells emerge, co-occur, and evolve
within code samples maintained in OSSECOs. A Multivocal Literature Review
protocol was applied, encompassing 30 peer-reviewed papers and 17
practitioner-oriented sources (2013-2024). Thematic synthesis was conducted to
identify recurring socio-technical patterns related to smell dynamics. Nine
patterns were identified, showing that community smells often precede or
reinforce technical degradation in code samples. Symptoms such as "radio
silence" and centralized ownership were frequently associated with persistent
structural anomalies. Additionally, limited onboarding, the absence of
continuous refactoring, and informal collaboration emerged as recurring
conditions for smell accumulation. Conclusion: In OSSECOs, particularly within
code samples, community-level dysfunctions not only correlate with but often
signal maintainability decay. These findings underscore the need for
socio-technical quality indicators and lightweight governance mechanisms
tailored to shared instructional artifacts.

</details>


### [10] [AI-Assisted Fixes to Code Review Comments at Scale](https://arxiv.org/abs/2507.13499)
*Chandra Maddila,Negar Ghorbani,James Saindon,Parth Thakkar,Vijayaraghavan Murali,Rui Abreu,Jingyue Shen,Brian Zhou,Nachiappan Nagappan,Peter C. Rigby*

Main category: cs.SE

TL;DR: Meta developed MetaMateCR, an AI-assisted code review system, achieving 19.7% ActionableToApplied rate post-production rollout and outperforming GPT-4o by 9.2pp. Safety trials revealed initial UX issues but a resolved implementation with 9.7% efficiency improvement over baselines.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient AI-assisted code review fixes at Meta's scale on 10k+ weekly code review comments.

Method: Created an internal benchmark of 64k <review comment, patch> data points to fine-tune Llama models, followed by offline evaluation, production safety trials with randomized UX controls, and full-scale deployment testing.

Result: 1. Offline: LargeLSFT model achieved 68% exact match patch accuracy (9pp better than GPT-4o) and used modern Hack functions more effectively than PHP-based GPT-4o suggestions. 2. Safety trial: Initial UX caused 5% review time regression; resolved by changing UX to hide AI suggestions from reviewers. 3. Production: ActionableToApplied rate of 19.7% with 9.2pp improvement over GPT-4o.

Conclusion: MetaMateCR demonstrates AI's potential in code review when combined with safety trials. Shows successful enterprise code review system implementation where UX design significantly impacts AI effectiveness, with direct implications for large-scale code review automation.

Abstract: Aim. There are 10s of thousands of code review comments each week at Meta. We
developed Metamate for Code Review (MetaMateCR) that provides AI-assisted fixes
for reviewer comments in production at scale.
  Method. We developed an internal benchmark of 64k <review comment, patch>
data points to fine-tune Llama models. Once our models achieve reasonable
offline results, we roll them into production. To ensure that our AI-assisted
fixes do not negatively impact the time it takes to do code reviews, we conduct
randomized controlled safety trials as well as full production experiments.
  Offline Results. As a baseline, we compare GPT-4o to our small and large
Llama models. In offline results, our LargeLSFT model creates an exact match
patch 68% of the time outperforming GPT-4o by 9 percentage points (pp). The
internal models also use more modern Hack functions when compared to the PHP
functions suggested by GPT-4o.
  Safety Trial. When we roll MetaMateCR into production in a safety trial that
compares no AI patches with AI patch suggestions, we see a large regression
with reviewers taking over 5% longer to conduct reviews. After investigation,
we modify the UX to only show authors the AI patches, and see no regressions in
the time for reviews.
  Production. When we roll LargeLSFT into production, we see an
ActionableToApplied rate of 19.7%, which is a 9.2pp improvement over GPT-4o.
Our results illustrate the importance of safety trials in ensuring that AI does
not inadvertently slow down engineers, and a successful review comment to AI
patch product running at scale.

</details>


### [11] [Towards Better Requirements from the Crowd: Developer Engagement with Feature Requests in Open Source Software](https://arxiv.org/abs/2507.13553)
*Pragyan K C,Rambod Ghandiparsi,Thomas Herron,John Heaps,Mitra Bokaei Hosseini*

Main category: cs.SE

TL;DR: Analyzes how open-source developers handle ambiguous/incomplete feature requests, revealing rare explicit clarification and focus on aligning with project goals rather than resolving text ambiguity.


<details>
  <summary>Details</summary>
Motivation: Ambiguous or incomplete natural language feature requests cause misinterpretation and implementation issues, yet developer clarification practices are poorly understood.

Method: Investigated NL defects in feature requests and conversational dynamics of clarification using open-source software issue tracker analysis.

Result: Found feature requests contain ambiguity/incompleteness, explicit clarification is uncommon, and developer focus lies in understanding user intent/goal and feasibility over textual details.

Conclusion: Identified patterns in clarification workflows to inform best practices for improving user-developer collaboration in handling feature requests.

Abstract: As user demands evolve, effectively incorporating feature requests is crucial
for maintaining software relevance and user satisfaction. Feature requests,
typically expressed in natural language, often suffer from ambiguity or
incomplete information due to communication gaps or the requester's limited
technical expertise. These issues can lead to misinterpretation, faulty
implementation, and reduced software quality. While seeking clarification from
requesters is a common strategy to mitigate these risks, little is known about
how developers engage in this clarification process in practice-how they
formulate clarifying questions, seek technical or contextual details, align on
goals and use cases, or decide to close requests without attempting
clarification. This study investigates how feature requests are prone to NL
defects (i.e. ambiguous or incomplete) and the conversational dynamics of
clarification in open-source software (OSS) development, aiming to understand
how developers handle ambiguous or incomplete feature requests. Our findings
suggest that feature requests published on the OSS platforms do possess
ambiguity and incompleteness, and in some cases, both. We also find that
explicit clarification for the resolution of these defects is uncommon;
developers usually focus on aligning with project goals rather than resolving
unclear text. When clarification occurs, it emphasizes understanding user
intent/goal and feasibility, rather than technical details. By characterizing
the dynamics of clarification in open-source issue trackers, this work
identifies patterns that can improve user-developer collaboration and inform
best practices for handling feature requests effectively.

</details>


### [12] [Demystifying Feature Requests: Leveraging LLMs to Refine Feature Requests in Open-Source Software](https://arxiv.org/abs/2507.13555)
*Pragyan K C,Rambod Ghandiparsi,Thomas Herron,John Heaps,Mitra Bokaei Hosseini*

Main category: cs.SE

TL;DR: This paper proposes an LLM-based method to automate detection and refinement of natural language defects in feature requests, addressing limitations of traditional validation methods in decentralized environments like OSS.


<details>
  <summary>Details</summary>
Motivation: Rapid evolution of software requirements through user-submitted natural language feature requests introduces ambiguities and incompleteness that hinder SE tasks. Traditional validation methods are impractical for large-scale decentralized OSS ecosystems.

Method: Developed a large language model approach to automatically identify NL defects in feature requests and generate targeted clarification questions, evaluated through real-world OSS data analysis, human annotation comparisons, and developer interviews.

Result: Demonstrated LLM effectiveness in detecting defects and generating CQs comparable to human annotations, with developer interviews validating practical relevance and highlighting improvements in downstream SE task efficiency.

Conclusion: LLM-based automation of NL defect refinement significantly enhances feature request utility in decentralized software development, offering scalable solutions to improve requirement quality and developer productivity.

Abstract: The growing popularity and widespread use of software applications (apps)
across various domains have driven rapid industry growth. Along with this
growth, fast-paced market changes have led to constantly evolving software
requirements. Such requirements are often grounded in feature requests and
enhancement suggestions, typically provided by users in natural language (NL).
However, these requests often suffer from defects such as ambiguity and
incompleteness, making them challenging to interpret. Traditional validation
methods (e.g., interviews and workshops) help clarify such defects but are
impractical in decentralized environments like open-source software (OSS),
where change requests originate from diverse users on platforms like GitHub.
This paper proposes a novel approach leveraging Large Language Models (LLMs) to
detect and refine NL defects in feature requests. Our approach automates the
identification of ambiguous and incomplete requests and generates clarification
questions (CQs) to enhance their usefulness for developers. To evaluate its
effectiveness, we apply our method to real-world OSS feature requests and
compare its performance against human annotations. In addition, we conduct
interviews with GitHub developers to gain deeper insights into their
perceptions of NL defects, the strategies they use to address these defects,
and the impact of defects on downstream software engineering (SE) tasks.

</details>


### [13] [Testing Autonomous Driving Systems -- What Really Matters and What Doesn't](https://arxiv.org/abs/2507.13661)
*Changwen Li,Joseph Sifakis,Rongjie Yan,Jian Zhang*

Main category: cs.SE

TL;DR: This paper addresses the fragmented testing of autonomous driving systems by proposing a framework to evaluate test methods and highlighting the lack of rationality and determinacy in current autopilot designs, which limits effective testing.


<details>
  <summary>Details</summary>
Motivation: The lack of a standardized technical assessment basis for autonomous driving systems (ADS) testing methods necessitates a structured approach to evaluate their effectiveness, validity, and alignment with operational capabilities.

Method: The study introduces a framework comparing existing test methods based on intrinsic effectiveness and validity, while analyzing eight open autopilots to demonstrate how design principles (policy selection and reproducibility) impact these metrics.

Result: Most test methods fail to meet both effectiveness and validity criteria, and existing autopilots largely lack rationality and determinacy, undermining the reliability of testing outcomes.

Conclusion: Current ADS testing lacks sufficient guarantees for critical autopilot properties; future development must prioritize rational, deterministic designs to improve testing efficacy and validity.

Abstract: Despite extensive research, the testing of autonomous driving systems (ADS)
landscape remains fragmented, and there is currently no basis for an informed
technical assessment of the importance and contribution of the current state of
the art. This paper attempts to address this problem by exploring two
complementary aspects.
  First, it proposes a framework for comparing existing test methods in terms
of their intrinsic effectiveness and validity. It shows that many methods do
not meet both of these requirements. Either because they are based on criteria
that do not allow for rapid, inexpensive, and comprehensive detection of
failures, or because the degree of validity of the properties tested cannot be
accurately estimated. In particular, it is shown that most critical test
methods do not take into account the nominal operational capabilities of
autopilots and generate scenarios that are impossible for the tested vehicles
to handle, resulting in unjustified rejections.
  Secondly, the paper shows that test effectiveness and validity are highly
dependent on how autopilots are designed: how they choose between different
control policies to perform maneuvers, as well as on the reproducibility of the
results. In fact, most test methods take for granted two principles underlying
traditional methods, but do not generally apply to ADS. We maintain that the
absence of rationality and determinacy significantly impairs the effectiveness
and validity of test methods, and provide test results on eight open
autopilots, in which most do not satisfy these properties, thereby illustrating
this fact.
  We conclude that under the current state of the art, it is impossible to
obtain strong enough guarantees for essential autopilot properties and
recommend that autopilots be developed with a view to both rationality and
determinacy.

</details>
