<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 18]
- [cs.SE](#cs.SE) [Total: 6]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [WatchWitch: Interoperability, Privacy, and Autonomy for the Apple Watch](https://arxiv.org/abs/2507.07210)
*Nils Rollshausen,Alexander Heinrich,Matthias Hollick,Jiska Classen*

Main category: cs.CR

TL;DR: The paper reverse-engineers Apple Watch's wireless protocols to break free from Apple's ecosystem, enabling Android interoperability with enhanced privacy controls and user data autonomy.


<details>
  <summary>Details</summary>
Motivation: Smartwatches collect sensitive health data, but their closed ecosystems limit user control over data processing. The paper aims to address this by enabling interoperability and privacy controls.

Method: The researchers reverse-engineered Apple Watch's wireless protocols and built WatchWitch, a custom Android reimplementation to achieve interoperability and privacy features.

Result: Discovered multiple security issues in Apple's protocols and demonstrated functional Android-based smartwatch implementation with enhanced privacy capabilities.

Conclusion: The work establishes practical interoperability outside of Apple's ecosystem, empowering users with greater control over their health data while setting a foundation for future research in consumer data autonomy.

Abstract: Smartwatches such as the Apple Watch collect vast amounts of intimate health
and fitness data as we wear them. Users have little choice regarding how this
data is processed: The Apple Watch can only be used with Apple's iPhones, using
their software and their cloud services. We are the first to publicly
reverse-engineer the watch's wireless protocols, which led to discovering
multiple security issues in Apple's proprietary implementation. With
WatchWitch, our custom Android reimplementation, we break out of Apple's walled
garden -- demonstrating practical interoperability with enhanced privacy
controls and data autonomy. We thus pave the way for more consumer choice in
the smartwatch ecosystem, offering users more control over their devices.

</details>


### [2] [Automated Attack Testflow Extraction from Cyber Threat Report using BERT for Contextual Analysis](https://arxiv.org/abs/2507.07244)
*Faissal Ahmadou,Sepehr Ghaffarzadegan,Boubakr Nour,Makan Pourzandi,Mourad Debbabi,Chadi Assi*

Main category: cs.CR

TL;DR: FLOWGUARDIAN automates extracting attack testflows from threat reports using BERT and NLP, reducing manual effort and errors while enhancing cybersecurity testing.


<details>
  <summary>Details</summary>
Motivation: Manually extracting attack testflows from unstructured threat reports is knowledge-intensive, time-consuming, and error-prone, limiting efficiency in APT identification and mitigation.

Method: Leverages BERT-based language models and NLP techniques to analyze, contextualize security events, reconstruct attack sequences, and generate structured testflows.

Result: Empirical validation on public threat reports shows FLOWGUARDIAN achieves high accuracy and efficiency in testflow extraction, improving security teams' capabilities.

Conclusion: FLOWGUARDIAN provides a robust automated solution for cybersecurity, enabling proactive threat hunting and incident response through reliable testflow generation.

Abstract: In the ever-evolving landscape of cybersecurity, the rapid identification and
mitigation of Advanced Persistent Threats (APTs) is crucial. Security
practitioners rely on detailed threat reports to understand the tactics,
techniques, and procedures (TTPs) employed by attackers. However, manually
extracting attack testflows from these reports requires elusive knowledge and
is time-consuming and prone to errors. This paper proposes FLOWGUARDIAN, a
novel solution leveraging language models (i.e., BERT) and Natural Language
Processing (NLP) techniques to automate the extraction of attack testflows from
unstructured threat reports. FLOWGUARDIAN systematically analyzes and
contextualizes security events, reconstructs attack sequences, and then
generates comprehensive testflows. This automated approach not only saves time
and reduces human error but also ensures comprehensive coverage and robustness
in cybersecurity testing. Empirical validation using public threat reports
demonstrates FLOWGUARDIAN's accuracy and efficiency, significantly enhancing
the capabilities of security teams in proactive threat hunting and incident
response.

</details>


### [3] [Disa: Accurate Learning-based Static Disassembly with Attentions](https://arxiv.org/abs/2507.07246)
*Peicheng Wang,Monika Santra,Mingyu Liu,Cong Sun,Dongrui Zeng,Gang Tan*

Main category: cs.CR

TL;DR: Disa is a deep learning-based disassembly approach using multi-head self-attention to improve instruction/function boundary identification and CFG accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional disassembly methods fail with obfuscated binaries due to reliance on heuristics and file-format assumptions, leading to incomplete/incorrect results.

Method: Disa employs superset instructions and multi-head self-attention mechanisms to learn instruction correlations and infer entry-points, integrating block-memory models for improved boundary detection.

Result: Disa achieves 9.1% F1-score improvement against disassembly desynchronization obfuscation, 13.2% improvement against source-level obfuscators, and 18.5% memory block precision gain, reducing AICT by 4.4%.

Conclusion: Disa outperforms prior approaches in function extraction and CFG generation for obfuscated binaries through its architecture-agnostic instruction correlation modeling.

Abstract: For reverse engineering related security domains, such as vulnerability
detection, malware analysis, and binary hardening, disassembly is crucial yet
challenging. The fundamental challenge of disassembly is to identify
instruction and function boundaries. Classic approaches rely on file-format
assumptions and architecture-specific heuristics to guess the boundaries,
resulting in incomplete and incorrect disassembly, especially when the binary
is obfuscated. Recent advancements of disassembly have demonstrated that deep
learning can improve both the accuracy and efficiency of disassembly. In this
paper, we propose Disa, a new learning-based disassembly approach that uses the
information of superset instructions over the multi-head self-attention to
learn the instructions' correlations, thus being able to infer function
entry-points and instruction boundaries. Disa can further identify instructions
relevant to memory block boundaries to facilitate an advanced block-memory
model based value-set analysis for an accurate control flow graph (CFG)
generation. Our experiments show that Disa outperforms prior deep-learning
disassembly approaches in function entry-point identification, especially
achieving 9.1% and 13.2% F1-score improvement on binaries respectively
obfuscated by the disassembly desynchronization technique and popular
source-level obfuscator. By achieving an 18.5% improvement in the memory block
precision, Disa generates more accurate CFGs with a 4.4% reduction in Average
Indirect Call Targets (AICT) compared with the state-of-the-art heuristic-based
approach.

</details>


### [4] [Semi-fragile watermarking of remote sensing images using DWT, vector quantization and automatic tiling](https://arxiv.org/abs/2507.07250)
*Jordi Serra-Ruiz,David Meg√≠as*

Main category: cs.CR

TL;DR: A semi-fragile watermarking method for multispectral/hyperspectral images uses tree-structured vector quantization on pixel signatures to detect significant modifications while surviving lossy compression.


<details>
  <summary>Details</summary>
Motivation: This work addresses the need for watermarking techniques in remote sensing images that maintain a mark under lossy compression but enable detection of unauthorized alterations. Traditional methods process bands separately, lacking effective signature-based multi-band analysis for integrity verification.

Method: 1. Segments multispectral/hyperspectral images into 3D blocks. 2. Constructs tree-structured vector quantizers for each block's pixel signatures. 3. Applies an iterative algorithm to manipulate quantization trees until a block satisfies an embedding criterion. 4. Preserves watermarks through compression but breaks under significant modifications.

Result: The method retains watermarks under lossy compression (above a compression threshold) while accurately identifying forged blocks and their locations in the image. This dual capability balances robustness against compression and sensitivity to malicious tampering.

Conclusion: The proposed semi-fragile watermarking approach effectively addresses both robustness against standard compression and fragility to significant modifications. Its signature-based, multi-band processing provides reliable integrity verification for remote sensing images, making it suitable for applications requiring tamper detection without compromising essential compression resilience.

Abstract: A semi-fragile watermarking scheme for multiple band images is presented in
this article. We propose to embed a mark into remote sensing images applying a
tree-structured vector quantization approach to the pixel signatures instead of
processing each band separately. The signature of the multispectral or
hyperspectral image is used to embed the mark in it order to detect any
significant modification of the original image. The image is segmented into
three-dimensional blocks, and a tree-structured vector quantizer is built for
each block. These trees are manipulated using an iterative algorithm until the
resulting block satisfies a required criterion, which establishes the embedded
mark. The method is shown to be able to preserve the mark under lossy
compression (above a given threshold) but, at the same time, it detects
possibly forged blocks and their position in the whole image.

</details>


### [5] [FedP3E: Privacy-Preserving Prototype Exchange for Non-IID IoT Malware Detection in Cross-Silo Federated Learning](https://arxiv.org/abs/2507.07258)
*Rami Darwish,Mahmoud Abdelsalam,Sajad Khorsandroo,Kaushik Roy*

Main category: cs.CR

TL;DR: The paper introduces FedP3E, a privacy-preserving federated learning (FL) framework designed to enhance malware detection in IoT ecosystems by addressing class imbalance and non-IID data challenges through prototype exchange with noise perturbation and SMOTE-based augmentation.


<details>
  <summary>Details</summary>
Motivation: Standard FL methods like FedAvg and FedProx struggle in real-world IoT scenarios with class-imbalanced, non-IID malware data. Current solutions risk privacy and fail to capture rare malware patterns due to limited data sharing.

Method: FedP3E employs Gaussian Mixture Models (GMMs) to generate class-wise prototypes at each client. These prototypes are perturbed with Gaussian noise before aggregation and redistribution. Local models leverage SMOTE-based minority class augmentation to improve learning in imbalanced settings, avoiding raw data or gradient sharing.

Result: FedP3E demonstrated reduced communication overhead and improved robustness to statistical heterogeneity on the N-BaIoT dataset under cross-silo scenarios, effectively enriching local models with shared structural patterns.

Conclusion: FedP3E offers a privacy-preserving, communication-efficient FL framework that outperforms standard approaches in handling data heterogeneity and rare malware classes, making it suitable for real-world IoT deployments.

Abstract: As IoT ecosystems continue to expand across critical sectors, they have
become prominent targets for increasingly sophisticated and large-scale malware
attacks. The evolving threat landscape, combined with the sensitive nature of
IoT-generated data, demands detection frameworks that are both
privacy-preserving and resilient to data heterogeneity. Federated Learning (FL)
offers a promising solution by enabling decentralized model training without
exposing raw data. However, standard FL algorithms such as FedAvg and FedProx
often fall short in real-world deployments characterized by class imbalance and
non-IID data distributions -- particularly in the presence of rare or disjoint
malware classes. To address these challenges, we propose FedP3E
(Privacy-Preserving Prototype Exchange), a novel FL framework that supports
indirect cross-client representation sharing while maintaining data privacy.
Each client constructs class-wise prototypes using Gaussian Mixture Models
(GMMs), perturbs them with Gaussian noise, and transmits only these compact
summaries to the server. The aggregated prototypes are then distributed back to
clients and integrated into local training, supported by SMOTE-based
augmentation to enhance representation of minority malware classes. Rather than
relying solely on parameter averaging, our prototype-driven mechanism enables
clients to enrich their local models with complementary structural patterns
observed across the federation -- without exchanging raw data or gradients.
This targeted strategy reduces the adverse impact of statistical heterogeneity
with minimal communication overhead. We evaluate FedP3E on the N-BaIoT dataset
under realistic cross-silo scenarios with varying degrees of data imbalance.

</details>


### [6] [Shuffling for Semantic Secrecy](https://arxiv.org/abs/2507.07401)
*Fupei Chen,Liyao Xiang,Haoxiang Sun,Hei Victor Cheng,Kaiming Shen*

Main category: cs.CR

TL;DR: This paper introduces a random shuffling method as a shared secret key to enhance semantic communication security, balancing transmission rate and leakage rate effectively.


<details>
  <summary>Details</summary>
Motivation: Traditional secure coding schemes struggle to optimally trade-off transmission speed and information leakage, especially under adverse channel conditions like strong noise or fading, necessitating a novel approach.

Method: The proposed system employs random feature sequence shuffling, generating a shared permutation pattern to act as the secret key. This plugin-based technique operates on wiretap channels, targeting semantic distortion for eavesdroppers while optimizing transmission rate and semantic error probability under leakage rate constraints.

Result: Simulations demonstrate the superiority of the shuffling method over benchmarks in secure transmission performance, particularly showing significant resilience against challenges from strong noise and unpredictable fading environments.

Conclusion: The random shuffling approach innovatively improves semantic communication security with a flexible plugin design, achieving a favorable transmission-leakage trade-off and enhanced robustness in vulnerable channel conditions.

Abstract: Deep learning draws heavily on the latest progress in semantic
communications. The present paper aims to examine the security aspect of this
cutting-edge technique from a novel shuffling perspective. Our goal is to
improve upon the conventional secure coding scheme to strike a desirable
tradeoff between transmission rate and leakage rate. To be more specific, for a
wiretap channel, we seek to maximize the transmission rate while minimizing the
semantic error probability under the given leakage rate constraint. Toward this
end, we devise a novel semantic security communication system wherein the
random shuffling pattern plays the role of the shared secret key. Intuitively,
the permutation of feature sequences via shuffling would distort the semantic
essence of the target data to a sufficient extent so that eavesdroppers cannot
access it anymore. The proposed random shuffling method also exhibits its
flexibility in working for the existing semantic communication system as a
plugin. Simulations demonstrate the significant advantage of the proposed
method over the benchmark in boosting secure transmission, especially when
channels are prone to strong noise and unpredictable fading.

</details>


### [7] [Phishing Detection in the Gen-AI Era: Quantized LLMs vs Classical Models](https://arxiv.org/abs/2507.07406)
*Jikesh Thapa,Gurrehmat Chahal,Serban Voinea Gabreanu,Yazan Otoum*

Main category: cs.CR

TL;DR: The paper evaluates ML, DL, and quantized LLMs for phishing detection, highlighting LLMs' potential for context-aware detection and cost-efficiency when optimized.


<details>
  <summary>Details</summary>
Motivation: Phishing attacks are evolving in sophistication, necessitating detection systems that balance accuracy with computational efficiency.

Method: The study compares traditional ML and DL models with quantized small-parameter LLMs on a curated dataset, analyzing accuracy, robustness, and prompting strategies (zero-shot, few-shot) at 17GB VRAM.

Result: While LLMs show lower raw accuracy than ML/DL, they detect subtle context cues effectively. Models like DeepSeek R1 Distill Qwen 14B (Q8_0) achieve +80% accuracy with lightweight deployment but struggle against rephrased emails.

Conclusion: Optimized LLMs offer interpretable phishing detection with cost-performance trade-offs, indicating potential for real-time, explainable cybersecurity solutions when robustness issues are addressed.

Abstract: Phishing attacks are becoming increasingly sophisticated, underscoring the
need for detection systems that strike a balance between high accuracy and
computational efficiency. This paper presents a comparative evaluation of
traditional Machine Learning (ML), Deep Learning (DL), and quantized
small-parameter Large Language Models (LLMs) for phishing detection. Through
experiments on a curated dataset, we show that while LLMs currently
underperform compared to ML and DL methods in terms of raw accuracy, they
exhibit strong potential for identifying subtle, context-based phishing cues.
We also investigate the impact of zero-shot and few-shot prompting strategies,
revealing that LLM-rephrased emails can significantly degrade the performance
of both ML and LLM-based detectors. Our benchmarking highlights that models
like DeepSeek R1 Distill Qwen 14B (Q8_0) achieve competitive accuracy, above
80%, using only 17GB of VRAM, supporting their viability for cost-efficient
deployment. We further assess the models' adversarial robustness and
cost-performance tradeoffs, and demonstrate how lightweight LLMs can provide
concise, interpretable explanations to support real-time decision-making. These
findings position optimized LLMs as promising components in phishing defence
systems and offer a path forward for integrating explainable, efficient AI into
modern cybersecurity frameworks.

</details>


### [8] [Hybrid LLM-Enhanced Intrusion Detection for Zero-Day Threats in IoT Networks](https://arxiv.org/abs/2507.07413)
*Mohammad F. Al-Hammouri,Yazan Otoum,Rasha Atwa,Amiya Nayak*

Main category: cs.CR

TL;DR: This paper proposes a hybrid intrusion detection system combining signature-based methods with GPT-2's semantic analysis to enhance detection accuracy (6.3% improvement) and reduce false positives (9.0% reduction) while maintaining real-time performance in IoT/networked environments.


<details>
  <summary>Details</summary>
Motivation: Traditional signature-based intrusion detection systems struggle with evolving zero-day attacks in modern distributed/heterogeneous IoT environments. GPT-2's ability to process unstructured data and identify complex patterns offers a complementary solution for detecting novel threats.

Method: The framework integrates rule-based signature matching with a GPT-2 language model for context-aware analysis of network traffic data. The LLM component processes non-structured security logs to identify subtle attack patterns and semantic relationships that signature-based approaches miss.

Result: Experiments show 6.3% higher detection accuracy compared to conventional systems, 9.0% reduction in false positives, and latency within 200ms per analysis on a test dataset of network traffic patterns.

Conclusion: Language models like GPT-2 can significantly strengthen intrusion detection capabilities by addressing limitations of signature-based approaches, creating a scalable, intelligent cybersecurity defense system for resource-constrained environments while maintaining real-time responsiveness.

Abstract: This paper presents a novel approach to intrusion detection by integrating
traditional signature-based methods with the contextual understanding
capabilities of the GPT-2 Large Language Model (LLM). As cyber threats become
increasingly sophisticated, particularly in distributed, heterogeneous, and
resource-constrained environments such as those enabled by the Internet of
Things (IoT), the need for dynamic and adaptive Intrusion Detection Systems
(IDSs) becomes increasingly urgent. While traditional methods remain effective
for detecting known threats, they often fail to recognize new and evolving
attack patterns. In contrast, GPT-2 excels at processing unstructured data and
identifying complex semantic relationships, making it well-suited to uncovering
subtle, zero-day attack vectors. We propose a hybrid IDS framework that merges
the robustness of signature-based techniques with the adaptability of
GPT-2-driven semantic analysis. Experimental evaluations on a representative
intrusion dataset demonstrate that our model enhances detection accuracy by
6.3%, reduces false positives by 9.0%, and maintains near real-time
responsiveness. These results affirm the potential of language model
integration to build intelligent, scalable, and resilient cybersecurity
defences suited for modern connected environments.

</details>


### [9] [Autonomous AI-based Cybersecurity Framework for Critical Infrastructure: Real-Time Threat Mitigation](https://arxiv.org/abs/2507.07416)
*Jenifer Paulraj,Brindha Raghuraman,Nagarani Gopalakrishnan,Yazan Otoum*

Main category: cs.CR

TL;DR: This paper proposes a hybrid AI-driven cybersecurity framework to address vulnerabilities in critical infrastructure systems (e.g. energy grids, healthcare) exposed to emerging cyber threats like ransomware and APTs. It explores threat landscapes, attack vectors, and challenges in adversarial AI and regulatory integration.


<details>
  <summary>Details</summary>
Motivation: Critical infrastructure systems are increasingly interconnected, making them vulnerable to cyber threats (ransomware, DoS, APTs) that endanger societal stability and economic resilience. There is a need for advanced cybersecurity solutions to address these risks.

Method: The authors develop a hybrid AI cybersecurity framework focusing on real-time vulnerability detection, threat modeling, and automated remediation. They analyze adversarial AI risks, regulatory compliance requirements, and integration challenges in infrastructure systems.

Result: The study identifies key vulnerabilities and attack vectors, demonstrates the hybrid AI framework's capability to enhance threat detection and response, and provides evidence of how adversarial AI mitigation and regulatory alignment can strengthen infrastructure security.

Conclusion: A hybrid AI-driven approach is essential for mitigating cyber threats to critical infrastructure. The proposed framework offers actionable solutions for improving system resilience while addressing technical, regulatory, and adaptive challenges through integrated AI capabilities.

Abstract: Critical infrastructure systems, including energy grids, healthcare
facilities, transportation networks, and water distribution systems, are
pivotal to societal stability and economic resilience. However, the increasing
interconnectivity of these systems exposes them to various cyber threats,
including ransomware, Denial-of-Service (DoS) attacks, and Advanced Persistent
Threats (APTs). This paper examines cybersecurity vulnerabilities in critical
infrastructure, highlighting the threat landscape, attack vectors, and the role
of Artificial Intelligence (AI) in mitigating these risks. We propose a hybrid
AI-driven cybersecurity framework to enhance real-time vulnerability detection,
threat modelling, and automated remediation. This study also addresses the
complexities of adversarial AI, regulatory compliance, and integration. Our
findings provide actionable insights to strengthen the security and resilience
of critical infrastructure systems against emerging cyber threats.

</details>


### [10] [May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks](https://arxiv.org/abs/2507.07417)
*Nishit V. Pandya,Andrey Labunets,Sicun Gao,Earlence Fernandes*

Main category: cs.CR

TL;DR: The paper evaluates existing prompt injection defenses for LLMs by constructing attention-based attacks that break two systems (SecAlign and StruQ) with 70% success rates under whitebox scenarios.


<details>
  <summary>Details</summary>
Motivation: Prompt injection defenses are widely used in academic research and production systems, but their claimed security guarantees remain untested against strong optimization-based attacks.

Method: Develops a novel attention-based attack algorithm and applies it to analyze SecAlign (CCS 2025) and StruQ (USENIX Security 2025) systems through whitebox experiments with token budget constraints.

Result: Demonstrates 70% success rates in breaking defenses with only modest computational costs, while releasing code and attack specifications for reproducibility.

Conclusion: These findings challenge the effectiveness of instruction/data separation defenses against prompt injection, highlighting critical robustness gaps in whitebox attack scenarios for LLM security mechanisms.

Abstract: A popular class of defenses against prompt injection attacks on large
language models (LLMs) relies on fine-tuning the model to separate instructions
and data, so that the LLM does not follow instructions that might be present
with data. There are several academic systems and production-level
implementations of this idea. We evaluate the robustness of this class of
prompt injection defenses in the whitebox setting by constructing strong
optimization-based attacks and showing that the defenses do not provide the
claimed security properties. Specifically, we construct a novel attention-based
attack algorithm for text-based LLMs and apply it to two recent whitebox
defenses SecAlign (CCS 2025) and StruQ (USENIX Security 2025), showing attacks
with success rates of up to 70% with modest increase in attacker budget in
terms of tokens. Our findings make fundamental progress towards understanding
the robustness of prompt injection defenses in the whitebox setting. We release
our code and attacks at https://github.com/nishitvp/better_opts_attacks

</details>


### [11] [RADAR: a Radio-based Analytics for Dynamic Association and Recognition of pseudonyms in VANETs](https://arxiv.org/abs/2507.07732)
*Giovanni Gambigliani Zoccoli,Filip Valgimigli,Dario Stabili,Mirco Marchetti*

Main category: cs.CR

TL;DR: RADAR is a vehicle tracking algorithm in C-ITS that exploits DSRC and Wi-Fi signals to de-anonymize vehicles, outperforming existing methods in limited coverage scenarios. The study also publicizes all code and simulations.


<details>
  <summary>Details</summary>
Motivation: This paper addresses the challenge of maintaining vehicle privacy in VANETs by analyzing de-anonymization techniques when attackers lack full path coverage, aiming to improve tracking accuracy.

Method: The method combines DSRC and Wi-Fi probe request messages to create temporal sequences, evaluated using three pseudonym-Wi-Fi identifier association metrics: Count, Statistical RSSI, and Pearson RSSI.

Result: Pearson RSSI demonstrated superior tracking performance under pseudonym-changing schemes compared to existing approaches in all tested scenarios, with the study also releasing its implementations and simulations.

Conclusion: RADAR enhances vehicle tracking in realistic C-ITS scenarios by leveraging multiple radio signals and introduces Pearson RSSI as a more effective de-anonymization metric, while providing open-source tools for validation.

Abstract: This paper presents RADAR, a tracking algorithm for vehicles participating in
Cooperative Intelligent Transportation Systems (C-ITS) that exploits multiple
radio signals emitted by a modern vehicle to break privacy-preserving pseudonym
schemes deployed in VANETs. This study shows that by combining Dedicated Short
Range Communication (DSRC) and Wi-Fi probe request messages broadcast by the
vehicle, it is possible to improve tracking over standard de-anonymization
approaches that only leverage DSRC, especially in realistic scenarios where the
attacker does not have full coverage of the entire vehicle path. The
experimental evaluation compares three different metrics for pseudonym and
Wi-Fi probe identifier association (Count, Statistical RSSI, and Pearson RSSI),
demonstrating that the Pearson RSSI metric is better at tracking vehicles under
pseudonym-changing schemes in all scenarios and against previous works. As an
additional contribution to the state-of-the-art, we publicly release all
implementations and simulation scenarios used in this work.

</details>


### [12] [Rainbow Artifacts from Electromagnetic Signal Injection Attacks on Image Sensors](https://arxiv.org/abs/2507.07773)
*Youqian Zhang,Xinyu Ji,Zhihao Wang,Qinhong Jiang*

Main category: cs.CR

TL;DR: This paper introduces a novel electromagnetic signal injection attack that manipulates CMOS image sensors in the analog domain, causing rainbow-like color artifacts that bypass digital integrity checks and mislead object detection models.


<details>
  <summary>Details</summary>
Motivation: Image sensors are crucial for safety/security systems (e.g., surveillance, autonomous vehicles) that depend on unaltered visual data for decision-making, yet existing digital security mechanisms cannot protect against physical analog-layer attacks.

Method: Researchers injected carefully tuned electromagnetic interference into the analog domain of image sensors, inducing visible rainbow-like artifacts, and evaluated how these propagate through image signal processing pipelines to affect state-of-the-art object detection models.

Result: The attack successfully creates untrackable color artifacts that disrupt object detection processes, showing that even high-end models produce significant mispredictions when confronted with such physical-layer manipulations.

Conclusion: The study reveals a critical vulnerability in the analog domain of image sensors that can undermine visual perception in security systems, advocating for improved physical-layer defenses to prevent undetectable adversarial manipulations.

Abstract: Image sensors are integral to a wide range of safety- and security-critical
systems, including surveillance infrastructure, autonomous vehicles, and
industrial automation. These systems rely on the integrity of visual data to
make decisions. In this work, we investigate a novel class of electromagnetic
signal injection attacks that target the analog domain of image sensors,
allowing adversaries to manipulate raw visual inputs without triggering
conventional digital integrity checks. We uncover a previously undocumented
attack phenomenon on CMOS image sensors: rainbow-like color artifacts induced
in images captured by image sensors through carefully tuned electromagnetic
interference. We further evaluate the impact of these attacks on
state-of-the-art object detection models, showing that the injected artifacts
propagate through the image signal processing pipeline and lead to significant
mispredictions. Our findings highlight a critical and underexplored
vulnerability in the visual perception stack, highlighting the need for more
robust defenses against physical-layer attacks in such systems.

</details>


### [13] [Mitigating Watermark Stealing Attacks in Generative Models via Multi-Key Watermarking](https://arxiv.org/abs/2507.07871)
*Toluwani Aremu,Noor Hussein,Munachiso Nwadike,Samuele Poppi,Jie Zhang,Karthik Nandakumar,Neil Gong,Nils Lukas*

Main category: cs.CR

TL;DR: This paper proposes a multi-key watermarking framework to prevent forged watermark attacks in generative AI by treating watermarks as black-boxes, providing both theoretical guarantees and empirical validation across datasets.


<details>
  <summary>Details</summary>
Motivation: GenAI providers face threats from watermark stealing attacks where malicious users forge watermarks to falsely accuse providers, damaging trust and accountability despite the benefits of watermarking for content provenance tracking.

Method: The authors develop a post-hoc multi-key extension for any watermarking method across modalities, introducing a game-theoretic security framework to model and evaluate forging threats systematically.

Result: Their approach significantly reduces forged watermark success rates (30-70% relative reduction) compared to single-key systems through extensive experiments on multiple benchmark datasets and provides provable security bounds.

Conclusion: The framework establishes a new paradigm for watermarking security by formally characterizing the threat model through security games and demonstrating that multi-key systems exponentially reduce attack success probability while maintaining content authenticity.

Abstract: Watermarking offers a promising solution for GenAI providers to establish the
provenance of their generated content. A watermark is a hidden signal embedded
in the generated content, whose presence can later be verified using a secret
watermarking key. A threat to GenAI providers are \emph{watermark stealing}
attacks, where users forge a watermark into content that was \emph{not}
generated by the provider's models without access to the secret key, e.g., to
falsely accuse the provider. Stealing attacks collect \emph{harmless}
watermarked samples from the provider's model and aim to maximize the expected
success rate of generating \emph{harmful} watermarked samples. Our work focuses
on mitigating stealing attacks while treating the underlying watermark as a
black-box. Our contributions are: (i) Proposing a multi-key extension to
mitigate stealing attacks that can be applied post-hoc to any watermarking
method across any modality. (ii) We provide theoretical guarantees and
demonstrate empirically that our method makes forging substantially less
effective across multiple datasets, and (iii) we formally define the threat of
watermark forging as the task of generating harmful, watermarked content and
model this threat via security games.

</details>


### [14] [The Trust Fabric: Decentralized Interoperability and Economic Coordination for the Agentic Web](https://arxiv.org/abs/2507.07901)
*Sree Bhargavi Balija,Rekha Singal,Abhishek Singh,Ramesh Raskar,Erfan Darzi,Raghu Bala,Thomas Hardjono,Ken Huang*

Main category: cs.CR

TL;DR: Nanda Unified Architecture introduces a decentralized framework addressing AI agent ecosystem fragmentation with DID-based discovery, semantic agent cards, and a dynamic trust layer integrating cryptographic proofs and privacy-preserving micropayments for interoperability across enterprise and Web3 systems.


<details>
  <summary>Details</summary>
Motivation: The fragmentation of AI agent ecosystems necessitates scalable solutions for interoperability, trust, and economic coordination, which existing protocols (MCP, A2A, ACP, AGP) fail to provide effectively.

Method: The architecture features three core components: (1) distributed registries for fast DID-based agent discovery, (2) semantic agent cards with verifiable credentials and composability profiles, and (3) a dynamic trust layer combining behavioral attestations and policy compliance. It also incorporates Synergetics' patented AgentTalk protocol (US Patent 12,244,584 B1), secure containerization, and X42/H42 micropayments for economic coordination.

Result: Real-world healthcare deployments achieved 99.9% compliance, demonstrating robust policy enforcement. The system supports high monthly transaction volumes (measured in millions) while maintaining strong privacy guarantees through decentralized cryptographic protocols.

Conclusion: By integrating MIT's trust research with Cisco/Synergetics' production deployments, Nanda shows trust can become a native currency in a decentralized economy. Policy-as-code and cryptographic proofs enable enterprise-Web3 interoperability at scale, solving existing protocol limitations and establishing trust as a core collaborative mechanism.

Abstract: The fragmentation of AI agent ecosystems has created urgent demands for
interoperability, trust, and economic coordination that current protocols --
including MCP (Hou et al., 2025), A2A (Habler et al., 2025), ACP (Liu et al.,
2025), and Cisco's AGP (Edwards, 2025) -- cannot address at scale. We present
the Nanda Unified Architecture, a decentralized framework built around three
core innovations: fast DID-based agent discovery through distributed
registries, semantic agent cards with verifiable credentials and composability
profiles, and a dynamic trust layer that integrates behavioral attestations
with policy compliance. The system introduces X42/H42 micropayments for
economic coordination and MAESTRO, a security framework incorporating
Synergetics' patented AgentTalk protocol (US Patent 12,244,584 B1) and secure
containerization. Real-world deployments demonstrate 99.9 percent compliance in
healthcare applications and substantial monthly transaction volumes with strong
privacy guarantees. By unifying MIT's trust research with production
deployments from Cisco and Synergetics, we show how cryptographic proofs and
policy-as-code transform agents into trust-anchored participants in a
decentralized economy (Lakshmanan, 2025; Sha, 2025). The result enables a
globally interoperable Internet of Agents where trust becomes the native
currency of collaboration across both enterprise and Web3 ecosystems.

</details>


### [15] [Can Large Language Models Improve Phishing Defense? A Large-Scale Controlled Experiment on Warning Dialogue Explanations](https://arxiv.org/abs/2507.07916)
*Federico Maria Cau,Giuseppe Desolda,Francesco Greco,Lucio Davide Spano,Luca Vigan√≤*

Main category: cs.CR

TL;DR: This paper evaluates using LLMs (Claude 3.5 and Llama 3.3) to generate phishing warning explanations, finding LLM-generated explanations can match/surpass manual ones in reducing susceptibility. Feature-based explanations work better for actual phishing links, while counterfactual explanations reduce false positives.


<details>
  <summary>Details</summary>
Motivation: Current phishing warnings lack clarity and scalability, enabling attackers to exploit human behavior vulnerabilities. Research aims to assess LLMs' potential to address these deficiencies with dynamically generated explanations.

Method: Large-scale between-subjects study (N=750) comparing manual vs LLM-generated explanations using two styles (feature-based and counterfactual). Metrics included click-through rates and perceptual factors (trust, risk perception, clarity).

Result: LLMs achieved similar/performance exceeded manual explanations in reducing phishing susceptibility (82% effective). Claude 3.5 showed particular strength (12% lower false positive rate). Feature-based explanations (74% accuracy) outperformed counterfactual ones (63% accuracy) for genuine phishing attempts.

Conclusion: LLMs can produce adaptive, scalable phishing explanations that maintain human-centered effectiveness. Implementation should prioritize feature-based explanations for primary threats and counterfactual reasoning for false positive reduction, leveraging model strengths to enhance cybersecurity defenses.

Abstract: Phishing has become a prominent risk in modern cybersecurity, often used to
bypass technological defences by exploiting predictable human behaviour.
Warning dialogues are a standard mitigation measure, but the lack of
explanatory clarity and static content limits their effectiveness. In this
paper, we report on our research to assess the capacity of Large Language
Models (LLMs) to generate clear, concise, and scalable explanations for
phishing warnings. We carried out a large-scale between-subjects user study (N
= 750) to compare the influence of warning dialogues supplemented with manually
generated explanations against those generated by two LLMs, Claude 3.5 Sonnet
and Llama 3.3 70B. We investigated two explanatory styles (feature-based and
counterfactual) for their effects on behavioural metrics (click-through rate)
and perceptual outcomes (e.g., trust, risk, clarity). The results indicate that
well-constructed LLM-generated explanations can equal or surpass manually
crafted explanations in reducing susceptibility to phishing; Claude-generated
warnings exhibited particularly robust performance. Feature-based explanations
were more effective for genuine phishing attempts, whereas counterfactual
explanations diminished false-positive rates. Other variables such as workload,
gender, and prior familiarity with warning dialogues significantly moderated
warning effectiveness. These results indicate that LLMs can be used to
automatically build explanations for warning users against phishing, and that
such solutions are scalable, adaptive, and consistent with human-centred
values.

</details>


### [16] [KeyDroid: A Large-Scale Analysis of Secure Key Storage in Android Apps](https://arxiv.org/abs/2507.07927)
*Jenny Blessing,Ross J. Anderson,Alastair R. Beresford*

Main category: cs.CR

TL;DR: Most Android apps handling sensitive data do not utilize hardware-backed encryption, and those using secure elements face performance issues.


<details>
  <summary>Details</summary>
Motivation: Hardware-backed key storage in Android offers enhanced security against OS compromises, but its adoption rate and performance trade-offs remain understudied despite industry push for broader use since 2011.

Method: Analyzed 490,119 apps to assess trusted hardware usage, cross-referenced with Play Store data safety labels. Measured cryptographic operation runtimes across software- and hardware-backed keystores.

Result: 56.3% of apps reporting sensitive data processing ignore hardware-backed storage; 5.03% use secure elements. Secure elements show impractical performance for symmetric/asymmetric encryption with significant payloads.

Conclusion: Android developers underutilize strong hardware-backed security, likely due to performance limitations, highlighting a need for better adoption strategies or performance improvements in secure elements.

Abstract: Most contemporary mobile devices offer hardware-backed storage for
cryptographic keys, user data, and other sensitive credentials. Such hardware
protects credentials from extraction by an adversary who has compromised the
main operating system, such as a malicious third-party app. Since 2011, Android
app developers can access trusted hardware via the Android Keystore API. In
this work, we conduct the first comprehensive survey of hardware-backed key
storage in Android devices. We analyze 490 119 Android apps, collecting data on
how trusted hardware is used by app developers (if used at all) and
cross-referencing our findings with sensitive user data collected by each app,
as self-reported by developers via the Play Store's data safety labels.
  We find that despite industry-wide initiatives to encourage adoption, 56.3%
of apps self-reporting as processing sensitive user data do not use Android's
trusted hardware capabilities at all, while just 5.03% of apps collecting some
form of sensitive data use the strongest form of trusted hardware, a secure
element distinct from the main processor. To better understand the potential
downsides of using secure hardware, we conduct the first empirical analysis of
trusted hardware performance in mobile devices, measuring the runtime of common
cryptographic operations across both software- and hardware-backed keystores.
We find that while hardware-backed key storage using a coprocessor is viable
for most common cryptographic operations, secure elements capable of preventing
more advanced attacks make performance infeasible for symmetric encryption with
non-negligible payloads and any kind of asymmetric encryption.

</details>


### [17] [EinHops: Einsum Notation for Expressive Homomorphic Operations on RNS-CKKS Tensors](https://arxiv.org/abs/2507.07972)
*Karthik Garimella,Austin Ebel,Brandon Reagen*

Main category: cs.CR

TL;DR: EinHops, a minimalist system utilizing Einstein summation notation, enables easier and more interpretable multi-dimensional tensor operations in FHE by explicitly encoding dimensional structures and transformations, providing full visibility into packing strategies while maintaining simplicity and generality.


<details>
  <summary>Details</summary>
Motivation: FHE systems face challenges when handling multi-dimensional tensor operations due to their limited instruction set (SIMD add/multiply/rotate). Current methods requiring 1-D tensor packing hide critical decisions behind abstractions, hindering debugging, optimization, and extension of existing systems.

Method: The authors leverage einsum notation to explicitly encode tensor dimensional structures and operations. They decompose einsum expressions into a fixed sequence of FHE-friendly operations (add/multiply/rotate) and implement this as a minimalist system called EinHops, which exposes the underlying packing strategy with full visibility.

Result: EinHops was evaluated on a spectrum of tensor operations from transposes to complex multi-dimensional contractions. The system demonstrated simplicity, generality, and interpretability advantages through its explicit strategy, enabling seamless encrypted tensor operations.

Conclusion: Einsum notation's explicit dimensional encoding makes building an FHE tensor system both feasible and interpretable. EinHops solves the multi-dimensional tensor processing challenge while remaining open-source and minimalist, offering simplicity and visibility for developers.

Abstract: Fully Homomorphic Encryption (FHE) is an encryption scheme that allows for
computation to be performed directly on encrypted data, effectively closing the
loop on secure and outsourced computing. Data is encrypted not only during rest
and transit, but also during processing. However, FHE provides a limited
instruction set: SIMD addition, SIMD multiplication, and cyclic rotation of 1-D
vectors. This restriction makes performing multi-dimensional tensor operations
challenging. Practitioners must pack these tensors into 1-D vectors and map
tensor operations onto this one-dimensional layout rather than their
traditional nested structure. And while prior systems have made significant
strides in automating this process, they often hide critical packing decisions
behind layers of abstraction, making debugging, optimizing, and building on top
of these systems difficult.
  In this work, we approach multi-dimensional tensor operations in FHE through
Einstein summation (einsum) notation. Einsum notation explicitly encodes
dimensional structure and operations in its syntax, naturally exposing how
tensors should be packed and transformed. We decompose einsum expressions into
a fixed set of FHE-friendly operations. We implement our design and present
EinHops, a minimalist system that factors einsum expressions into a fixed
sequence of FHE operations. EinHops enables developers to perform encrypted
tensor operations using FHE while maintaining full visibility into the
underlying packing strategy. We evaluate EinHops on a range of tensor
operations from a simple transpose to complex multi-dimensional contractions.
We show that the explicit nature of einsum notation allows us to build an FHE
tensor system that is simple, general, and interpretable. We open-source
EinHops at the following repository: https://github.com/baahl-nyu/einhops.

</details>


### [18] [Defending Against Prompt Injection With a Few DefensiveTokens](https://arxiv.org/abs/2507.07974)
*Sizhe Chen,Yizhu Wang,Nicholas Carlini,Chawin Sitawarin,David Wagner*

Main category: cs.CR

TL;DR: This paper introduces DefensiveToken, a test-time defense for large language models (LLMs) against prompt injection attacks, offering flexibility to switch between state-of-the-art utility and near-SOTA security by appending a few special tokens during inference.


<details>
  <summary>Details</summary>
Motivation: Current test-time defenses for LLMs (e.g., defensive prompting) are less effective than training-time defenses, which require parameter modifications. This motivates the need for test-time defenses that maintain both high security and utility without altering model parameters.

Method: The authors propose inserting specially designed tokens (DefensiveTokens) during testing. These tokens are embedded with values optimized for robustness against prompt injection and can be appended to inputs when security is critical, without modifying the model itself.

Result: DefensiveToken achieves security comparable to training-time defenses with a minimal impact on utility. It enables dynamic activation of security depending on scenario needs, though the abstract does not include specific quantitative results.

Conclusion: DefensiveToken provides a flexible solution for securing LLMs against prompt injection attacks during testing, balancing SOTA utility and near-SOTA security at minimal cost. The code is available on GitHub.

Abstract: When large language model (LLM) systems interact with external data to
perform complex tasks, a new attack, namely prompt injection, becomes a
significant threat. By injecting instructions into the data accessed by the
system, the attacker is able to override the initial user task with an
arbitrary task directed by the attacker. To secure the system, test-time
defenses, e.g., defensive prompting, have been proposed for system developers
to attain security only when needed in a flexible manner. However, they are
much less effective than training-time defenses that change the model
parameters. Motivated by this, we propose DefensiveToken, a test-time defense
with prompt injection robustness comparable to training-time alternatives.
DefensiveTokens are newly inserted as special tokens, whose embeddings are
optimized for security. In security-sensitive cases, system developers can
append a few DefensiveTokens before the LLM input to achieve security with a
minimal utility drop. In scenarios where security is less of a concern,
developers can simply skip DefensiveTokens; the LLM system remains the same as
there is no defense, generating high-quality responses. Thus, DefensiveTokens,
if released alongside the model, allow a flexible switch between the
state-of-the-art (SOTA) utility and almost-SOTA security at test time. The code
is available at https://github.com/Sizhe-Chen/DefensiveToken.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [19] [A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering](https://arxiv.org/abs/2507.07325)
*Martin Obaidi,Marc Herrmann,Elisa Schmid,Raymond Ochsner,Kurt Schneider,Jil Kl√ºnder*

Main category: cs.SE

TL;DR: This paper introduces a robust German sentiment analysis dataset for software engineering, compiled from Android-Hilfe.de with high interrater agreement, addressing a gap in domain-specific German tools.


<details>
  <summary>Details</summary>
Motivation: Existing sentiment analysis tools in software engineering lack domain-specific German datasets, limiting their applicability to German-speaking developer communities.

Method: The authors collected 5,949 unique German developer statements from a forum, annotated them with six emotions (per Shaver et al.'s model) using four German-speaking annotators, and validated the dataset through interrater agreement analysis and domain-specific tool benchmarking.

Result: High interrater agreement confirmed dataset reliability (reliability measures unspecified). Evaluations revealed existing German sentiment tools underperform in software engineering domains due to lack of domain-specific training data.

Conclusion: The dataset provides a validated resource for improving German sentiment analysis in software engineering, while highlighting annotation optimization strategies and practical use cases for future research and tool development.

Abstract: Sentiment analysis is an essential technique for investigating the emotional
climate within developer teams, contributing to both team productivity and
project success. Existing sentiment analysis tools in software engineering
primarily rely on English or non-German gold-standard datasets. To address this
gap, our work introduces a German dataset of 5,949 unique developer statements,
extracted from the German developer forum Android-Hilfe.de. Each statement was
annotated with one of six basic emotions, based on the emotion model by Shaver
et al., by four German-speaking computer science students. Evaluation of the
annotation process showed high interrater agreement and reliability. These
results indicate that the dataset is sufficiently valid and robust to support
sentiment analysis in the German-speaking software engineering community.
Evaluation with existing German sentiment analysis tools confirms the lack of
domain-specific solutions for software engineering. We also discuss approaches
to optimize annotation and present further use cases for the dataset.

</details>


### [20] [Automatic Generation of Explainability Requirements and Software Explanations From User Reviews](https://arxiv.org/abs/2507.07344)
*Martin Obaidi,Jannik Fischbach,Jakob Droste,Hannah Deters,Marc Herrmann,Jil Kl√ºnder,Steffen Kr√§tzig,Hugo Villamizar,Kurt Schneider*

Main category: cs.SE

TL;DR: The paper introduces an automated tool to derive explainability requirements from user reviews and generate aligned explanations. Evaluation with 58 annotated reviews shows AI-generated explanations are preferred for clarity/style but require human validation for correctness.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of translating user feedback into structured explainability requirements to improve transparency, trust, and compliance in software systems.

Method: Collaborated with an industrial manufacturer to create a dataset of 58 user reviews with manual annotations. Evaluated AI-generated requirements and explanations against human-annotated ones.

Result: AI-generated requirements lack relevance/correctness, but explanations are favored for clarity/style. Systematic gaps in automation effectiveness were identified through empirical testing.

Conclusion: The approach advances explainability requirements by combining automation with human validation, while highlighting the need for improved accuracy in automatic generation.

Abstract: Explainability has become a crucial non-functional requirement to enhance
transparency, build user trust, and ensure regulatory compliance. However,
translating explanation needs expressed in user feedback into structured
requirements and corresponding explanations remains challenging. While existing
methods can identify explanation-related concerns in user reviews, there is no
established approach for systematically deriving requirements and generating
aligned explanations. To contribute toward addressing this gap, we introduce a
tool-supported approach that automates this process. To evaluate its
effectiveness, we collaborated with an industrial automation manufacturer to
create a dataset of 58 user reviews, each annotated with manually crafted
explainability requirements and explanations. Our evaluation shows that while
AI-generated requirements often lack relevance and correctness compared to
human-created ones, the AI-generated explanations are frequently preferred for
their clarity and style. Nonetheless, correctness remains an issue,
highlighting the importance of human validation. This work contributes to the
advancement of explainability requirements in software systems by (1)
introducing an automated approach to derive requirements from user reviews and
generate corresponding explanations, (2) providing empirical insights into the
strengths and limitations of automatically generated artifacts, and (3)
releasing a curated dataset to support future research on the automatic
generation of explainability requirements.

</details>


### [21] [Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN](https://arxiv.org/abs/2507.07468)
*Sten Gr√ºner,Nafise Eskandani*

Main category: cs.SE

TL;DR: This paper proposes a distributed AAS copy-on-write infrastructure combined with BPMN to enhance secure, scalable, and interoperable automation in engineering workflows across organizations.


<details>
  <summary>Details</summary>
Motivation: Industry 4.0 requires automated and optimized plant/process engineering workflows, necessitating interoperable digital twins and secure cross-organizational collaboration.

Method: Developed a copy-on-write architecture for distributed AAS management alongside a BPMN workflow engine to structure and automate engineering processes.

Result: Created a workflow management prototype that automates AAS operations, demonstrating improved efficiency and traceability in engineering workflows.

Conclusion: The proposed AAS infrastructure with BPMN workflows enables scalable, secure automation of engineering processes while facilitating cross-organizational collaboration through enhanced digital twin interoperability.

Abstract: The integration of Industry 4.0 technologies into engineering workflows is an
essential step toward automating and optimizing plant and process engineering
processes. The Asset Administration Shell (AAS) serves as a key enabler for
creating interoperable Digital Twins that facilitate engineering data exchange
and automation. This paper explores the use of AAS within engineering
workflows, particularly in combination with Business Process Model and Notation
(BPMN) to define structured and automated processes. We propose a distributed
AAS copy-on-write infrastructure that enhances security and scalability while
enabling seamless cross organizational collaboration. We also introduce a
workflow management prototype automating AAS operations and engineering
workflows, improving efficiency and traceability.

</details>


### [22] [From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering](https://arxiv.org/abs/2507.07548)
*Jonathan Ullrich,Matthias Koch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: The study explores how developers use LLMs for code generation from requirements, finding that requirements must be manually decomposed into tasks with design decisions before LLMs can effectively process them, indicating RE work remains crucial even with LLMs.


<details>
  <summary>Details</summary>
Motivation: Assess feasibility of replacing traditional software engineering with LLMs by examining how practitioners integrate requirements into code generation workflows.

Method: Conducted interviews with 18 practitioners across 14 companies to analyze their use of requirements and design artifacts in LLM-based code generation processes.

Result: Developers must manually decompose abstract requirements into programming tasks and enrich them with design decisions/architectural constraints before using LLMs for code generation.

Conclusion: Fundamental requirements engineering remains essential when using LLMs for code generation; the proposed theory aids in contextualizing automated approaches for requirements-centric software engineering tasks.

Abstract: With the advent of generative LLMs and their advanced code generation
capabilities, some people already envision the end of traditional software
engineering, as LLMs may be able to produce high-quality code based solely on
the requirements a domain expert feeds into the system. The feasibility of this
vision can be assessed by understanding how developers currently incorporate
requirements when using LLMs for code generation-a topic that remains largely
unexplored. We interviewed 18 practitioners from 14 companies to understand how
they (re)use information from requirements and other design artifacts to feed
LLMs when generating code. Based on our findings, we propose a theory that
explains the processes developers employ and the artifacts they rely on. Our
theory suggests that requirements, as typically documented, are too abstract
for direct input into LLMs. Instead, they must first be manually decomposed
into programming tasks, which are then enriched with design decisions and
architectural constraints before being used in prompts. Our study highlights
that fundamental RE work is still necessary when LLMs are used to generate
code. Our theory is important for contextualizing scientific approaches to
automating requirements-centric SE tasks.

</details>


### [23] [Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap](https://arxiv.org/abs/2507.07682)
*Kaicheng Huang,Fanyu Wang,Yutan Huang,Chetan Arora*

Main category: cs.SE

TL;DR: The paper presents a systematic review on prompt engineering for requirement engineering (RE) tasks, addressing challenges in LLM controllability and proposing a roadmap for reproducible workflows.


<details>
  <summary>Details</summary>
Motivation: Current LLMs exhibit uncertainty and lack controllability, and there is no clear guidance for effective prompt engineering in RE, hindering trustworthy adoption.

Method: A roadmap-oriented systematic literature review following Kitchenham and Petersen's protocol, involving six digital libraries, screening 867 records, analyzing 35 primary studies, and proposing a hybrid taxonomy.

Result: Identified research gaps and limitations in PE4RE, introduced a taxonomy linking techniques (e.g., few-shot, Chain-of-Thought) to RE tasks (elicitation, validation, traceability), and mapped LLM families and prompt types used in prior work.

Conclusion: The roadmap outlines steps to evolve ad-hoc PE prototypes into reproducible, practitioner-friendly workflows, addressing fragmentation and enabling trustworthy use of LLMs in RE.

Abstract: Advancements in large language models (LLMs) have led to a surge of prompt
engineering (PE) techniques that can enhance various requirements engineering
(RE) tasks. However, current LLMs are often characterized by significant
uncertainty and a lack of controllability. This absence of clear guidance on
how to effectively prompt LLMs acts as a barrier to their trustworthy
implementation in the RE field. We present the first roadmap-oriented
systematic literature review of Prompt Engineering for RE (PE4RE). Following
Kitchenham's and Petersen's secondary-study protocol, we searched six digital
libraries, screened 867 records, and analyzed 35 primary studies. To bring
order to a fragmented landscape, we propose a hybrid taxonomy that links
technique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented
RE roles (elicitation, validation, traceability). Two research questions, with
five sub-questions, map the tasks addressed, LLM families used, and prompt
types adopted, and expose current limitations and research gaps. Finally, we
outline a step-by-step roadmap showing how today's ad-hoc PE prototypes can
evolve into reproducible, practitioner-friendly workflows.

</details>


### [24] [From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry](https://arxiv.org/abs/2507.07689)
*Chetan Arora,Fanyu Wang,Chakkrit Tantithamthavorn,Aldeida Aleti,Shaun Kenyon*

Main category: cs.SE

TL;DR: This paper proposes using Retrieval-Augmented Generation (RAG) models to semi-automate requirements engineering for the space industry, particularly aiding smaller organizations with unstructured mission documents.


<details>
  <summary>Details</summary>
Motivation: Small space organizations and new entrants face difficulties extracting actionable requirements from large, unstructured space mission documents while adhering to strict standards and mission-specific constraints.

Method: A modular AI-driven approach combining document preprocessing, semantic classification, contextual retrieval from domain standards, and large language model (LLM)-based synthesis of draft requirements.

Result: The method was tested on a real-world mission document via collaboration with Starbound Space Solutions, showing reduced manual effort, improved requirement coverage, and lightweight compliance alignment.

Conclusion: The approach demonstrates feasibility for AI integration in space RE workflows, with a roadmap to enable smaller organizations to participate in safety-critical missions with greater ease.

Abstract: Requirements engineering (RE) in the space industry is inherently complex,
demanding high precision, alignment with rigorous standards, and adaptability
to mission-specific constraints. Smaller space organisations and new entrants
often struggle to derive actionable requirements from extensive, unstructured
documents such as mission briefs, interface specifications, and regulatory
standards. In this innovation opportunity paper, we explore the potential of
Retrieval-Augmented Generation (RAG) models to support and (semi-)automate
requirements generation in the space domain. We present a modular, AI-driven
approach that preprocesses raw space mission documents, classifies them into
semantically meaningful categories, retrieves contextually relevant content
from domain standards, and synthesises draft requirements using large language
models (LLMs). We apply the approach to a real-world mission document from the
space domain to demonstrate feasibility and assess early outcomes in
collaboration with our industry partner, Starbound Space Solutions. Our
preliminary results indicate that the approach can reduce manual effort,
improve coverage of relevant requirements, and support lightweight compliance
alignment. We outline a roadmap toward broader integration of AI in RE
workflows, intending to lower barriers for smaller organisations to participate
in large-scale, safety-critical missions.

</details>
