{"id": "2507.07325", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07325", "abs": "https://arxiv.org/abs/2507.07325", "authors": ["Martin Obaidi", "Marc Herrmann", "Elisa Schmid", "Raymond Ochsner", "Kurt Schneider", "Jil Kl\u00fcnder"], "title": "A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering", "comment": "This paper has been accepted at the 33rd IEEE International\n  Requirements Engineering Workshop (REW 2025)", "summary": "Sentiment analysis is an essential technique for investigating the emotional\nclimate within developer teams, contributing to both team productivity and\nproject success. Existing sentiment analysis tools in software engineering\nprimarily rely on English or non-German gold-standard datasets. To address this\ngap, our work introduces a German dataset of 5,949 unique developer statements,\nextracted from the German developer forum Android-Hilfe.de. Each statement was\nannotated with one of six basic emotions, based on the emotion model by Shaver\net al., by four German-speaking computer science students. Evaluation of the\nannotation process showed high interrater agreement and reliability. These\nresults indicate that the dataset is sufficiently valid and robust to support\nsentiment analysis in the German-speaking software engineering community.\nEvaluation with existing German sentiment analysis tools confirms the lack of\ndomain-specific solutions for software engineering. We also discuss approaches\nto optimize annotation and present further use cases for the dataset.", "AI": {"tldr": "This paper introduces a robust German sentiment analysis dataset for software engineering, compiled from Android-Hilfe.de with high interrater agreement, addressing a gap in domain-specific German tools.", "motivation": "Existing sentiment analysis tools in software engineering lack domain-specific German datasets, limiting their applicability to German-speaking developer communities.", "method": "The authors collected 5,949 unique German developer statements from a forum, annotated them with six emotions (per Shaver et al.'s model) using four German-speaking annotators, and validated the dataset through interrater agreement analysis and domain-specific tool benchmarking.", "result": "High interrater agreement confirmed dataset reliability (reliability measures unspecified). Evaluations revealed existing German sentiment tools underperform in software engineering domains due to lack of domain-specific training data.", "conclusion": "The dataset provides a validated resource for improving German sentiment analysis in software engineering, while highlighting annotation optimization strategies and practical use cases for future research and tool development."}}
{"id": "2507.07344", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07344", "abs": "https://arxiv.org/abs/2507.07344", "authors": ["Martin Obaidi", "Jannik Fischbach", "Jakob Droste", "Hannah Deters", "Marc Herrmann", "Jil Kl\u00fcnder", "Steffen Kr\u00e4tzig", "Hugo Villamizar", "Kurt Schneider"], "title": "Automatic Generation of Explainability Requirements and Software Explanations From User Reviews", "comment": "This paper has been accepted at the 33rd IEEE International\n  Requirements Engineering Workshop (REW 2025)", "summary": "Explainability has become a crucial non-functional requirement to enhance\ntransparency, build user trust, and ensure regulatory compliance. However,\ntranslating explanation needs expressed in user feedback into structured\nrequirements and corresponding explanations remains challenging. While existing\nmethods can identify explanation-related concerns in user reviews, there is no\nestablished approach for systematically deriving requirements and generating\naligned explanations. To contribute toward addressing this gap, we introduce a\ntool-supported approach that automates this process. To evaluate its\neffectiveness, we collaborated with an industrial automation manufacturer to\ncreate a dataset of 58 user reviews, each annotated with manually crafted\nexplainability requirements and explanations. Our evaluation shows that while\nAI-generated requirements often lack relevance and correctness compared to\nhuman-created ones, the AI-generated explanations are frequently preferred for\ntheir clarity and style. Nonetheless, correctness remains an issue,\nhighlighting the importance of human validation. This work contributes to the\nadvancement of explainability requirements in software systems by (1)\nintroducing an automated approach to derive requirements from user reviews and\ngenerate corresponding explanations, (2) providing empirical insights into the\nstrengths and limitations of automatically generated artifacts, and (3)\nreleasing a curated dataset to support future research on the automatic\ngeneration of explainability requirements.", "AI": {"tldr": "The paper introduces an automated tool to derive explainability requirements from user reviews and generate aligned explanations. Evaluation with 58 annotated reviews shows AI-generated explanations are preferred for clarity/style but require human validation for correctness.", "motivation": "Addressing the challenge of translating user feedback into structured explainability requirements to improve transparency, trust, and compliance in software systems.", "method": "Collaborated with an industrial manufacturer to create a dataset of 58 user reviews with manual annotations. Evaluated AI-generated requirements and explanations against human-annotated ones.", "result": "AI-generated requirements lack relevance/correctness, but explanations are favored for clarity/style. Systematic gaps in automation effectiveness were identified through empirical testing.", "conclusion": "The approach advances explainability requirements by combining automation with human validation, while highlighting the need for improved accuracy in automatic generation."}}
{"id": "2507.07468", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07468", "abs": "https://arxiv.org/abs/2507.07468", "authors": ["Sten Gr\u00fcner", "Nafise Eskandani"], "title": "Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN", "comment": "7 pages, 7 figures, Accepted at IFAC EAAS 2025\n  (https://j3c.org/eaas.php)", "summary": "The integration of Industry 4.0 technologies into engineering workflows is an\nessential step toward automating and optimizing plant and process engineering\nprocesses. The Asset Administration Shell (AAS) serves as a key enabler for\ncreating interoperable Digital Twins that facilitate engineering data exchange\nand automation. This paper explores the use of AAS within engineering\nworkflows, particularly in combination with Business Process Model and Notation\n(BPMN) to define structured and automated processes. We propose a distributed\nAAS copy-on-write infrastructure that enhances security and scalability while\nenabling seamless cross organizational collaboration. We also introduce a\nworkflow management prototype automating AAS operations and engineering\nworkflows, improving efficiency and traceability.", "AI": {"tldr": "This paper proposes a distributed AAS copy-on-write infrastructure combined with BPMN to enhance secure, scalable, and interoperable automation in engineering workflows across organizations.", "motivation": "Industry 4.0 requires automated and optimized plant/process engineering workflows, necessitating interoperable digital twins and secure cross-organizational collaboration.", "method": "Developed a copy-on-write architecture for distributed AAS management alongside a BPMN workflow engine to structure and automate engineering processes.", "result": "Created a workflow management prototype that automates AAS operations, demonstrating improved efficiency and traceability in engineering workflows.", "conclusion": "The proposed AAS infrastructure with BPMN workflows enables scalable, secure automation of engineering processes while facilitating cross-organizational collaboration through enhanced digital twin interoperability."}}
{"id": "2507.07548", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07548", "abs": "https://arxiv.org/abs/2507.07548", "authors": ["Jonathan Ullrich", "Matthias Koch", "Andreas Vogelsang"], "title": "From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering", "comment": "This paper has been accepted for publication at the 33rd IEEE\n  International Requirements Engineering (RE) conference", "summary": "With the advent of generative LLMs and their advanced code generation\ncapabilities, some people already envision the end of traditional software\nengineering, as LLMs may be able to produce high-quality code based solely on\nthe requirements a domain expert feeds into the system. The feasibility of this\nvision can be assessed by understanding how developers currently incorporate\nrequirements when using LLMs for code generation-a topic that remains largely\nunexplored. We interviewed 18 practitioners from 14 companies to understand how\nthey (re)use information from requirements and other design artifacts to feed\nLLMs when generating code. Based on our findings, we propose a theory that\nexplains the processes developers employ and the artifacts they rely on. Our\ntheory suggests that requirements, as typically documented, are too abstract\nfor direct input into LLMs. Instead, they must first be manually decomposed\ninto programming tasks, which are then enriched with design decisions and\narchitectural constraints before being used in prompts. Our study highlights\nthat fundamental RE work is still necessary when LLMs are used to generate\ncode. Our theory is important for contextualizing scientific approaches to\nautomating requirements-centric SE tasks.", "AI": {"tldr": "The study explores how developers use LLMs for code generation from requirements, finding that requirements must be manually decomposed into tasks with design decisions before LLMs can effectively process them, indicating RE work remains crucial even with LLMs.", "motivation": "Assess feasibility of replacing traditional software engineering with LLMs by examining how practitioners integrate requirements into code generation workflows.", "method": "Conducted interviews with 18 practitioners across 14 companies to analyze their use of requirements and design artifacts in LLM-based code generation processes.", "result": "Developers must manually decompose abstract requirements into programming tasks and enrich them with design decisions/architectural constraints before using LLMs for code generation.", "conclusion": "Fundamental requirements engineering remains essential when using LLMs for code generation; the proposed theory aids in contextualizing automated approaches for requirements-centric software engineering tasks."}}
{"id": "2507.07210", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07210", "abs": "https://arxiv.org/abs/2507.07210", "authors": ["Nils Rollshausen", "Alexander Heinrich", "Matthias Hollick", "Jiska Classen"], "title": "WatchWitch: Interoperability, Privacy, and Autonomy for the Apple Watch", "comment": "To appear in \"Proceedings on Privacy Enhancing Technologies\"", "summary": "Smartwatches such as the Apple Watch collect vast amounts of intimate health\nand fitness data as we wear them. Users have little choice regarding how this\ndata is processed: The Apple Watch can only be used with Apple's iPhones, using\ntheir software and their cloud services. We are the first to publicly\nreverse-engineer the watch's wireless protocols, which led to discovering\nmultiple security issues in Apple's proprietary implementation. With\nWatchWitch, our custom Android reimplementation, we break out of Apple's walled\ngarden -- demonstrating practical interoperability with enhanced privacy\ncontrols and data autonomy. We thus pave the way for more consumer choice in\nthe smartwatch ecosystem, offering users more control over their devices.", "AI": {"tldr": "The paper reverse-engineers Apple Watch's wireless protocols to break free from Apple's ecosystem, enabling Android interoperability with enhanced privacy controls and user data autonomy.", "motivation": "Smartwatches collect sensitive health data, but their closed ecosystems limit user control over data processing. The paper aims to address this by enabling interoperability and privacy controls.", "method": "The researchers reverse-engineered Apple Watch's wireless protocols and built WatchWitch, a custom Android reimplementation to achieve interoperability and privacy features.", "result": "Discovered multiple security issues in Apple's protocols and demonstrated functional Android-based smartwatch implementation with enhanced privacy capabilities.", "conclusion": "The work establishes practical interoperability outside of Apple's ecosystem, empowering users with greater control over their health data while setting a foundation for future research in consumer data autonomy."}}
{"id": "2507.07682", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07682", "abs": "https://arxiv.org/abs/2507.07682", "authors": ["Kaicheng Huang", "Fanyu Wang", "Yutan Huang", "Chetan Arora"], "title": "Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap", "comment": null, "summary": "Advancements in large language models (LLMs) have led to a surge of prompt\nengineering (PE) techniques that can enhance various requirements engineering\n(RE) tasks. However, current LLMs are often characterized by significant\nuncertainty and a lack of controllability. This absence of clear guidance on\nhow to effectively prompt LLMs acts as a barrier to their trustworthy\nimplementation in the RE field. We present the first roadmap-oriented\nsystematic literature review of Prompt Engineering for RE (PE4RE). Following\nKitchenham's and Petersen's secondary-study protocol, we searched six digital\nlibraries, screened 867 records, and analyzed 35 primary studies. To bring\norder to a fragmented landscape, we propose a hybrid taxonomy that links\ntechnique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented\nRE roles (elicitation, validation, traceability). Two research questions, with\nfive sub-questions, map the tasks addressed, LLM families used, and prompt\ntypes adopted, and expose current limitations and research gaps. Finally, we\noutline a step-by-step roadmap showing how today's ad-hoc PE prototypes can\nevolve into reproducible, practitioner-friendly workflows.", "AI": {"tldr": "The paper presents a systematic review on prompt engineering for requirement engineering (RE) tasks, addressing challenges in LLM controllability and proposing a roadmap for reproducible workflows.", "motivation": "Current LLMs exhibit uncertainty and lack controllability, and there is no clear guidance for effective prompt engineering in RE, hindering trustworthy adoption.", "method": "A roadmap-oriented systematic literature review following Kitchenham and Petersen's protocol, involving six digital libraries, screening 867 records, analyzing 35 primary studies, and proposing a hybrid taxonomy.", "result": "Identified research gaps and limitations in PE4RE, introduced a taxonomy linking techniques (e.g., few-shot, Chain-of-Thought) to RE tasks (elicitation, validation, traceability), and mapped LLM families and prompt types used in prior work.", "conclusion": "The roadmap outlines steps to evolve ad-hoc PE prototypes into reproducible, practitioner-friendly workflows, addressing fragmentation and enabling trustworthy use of LLMs in RE."}}
{"id": "2507.07244", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07244", "abs": "https://arxiv.org/abs/2507.07244", "authors": ["Faissal Ahmadou", "Sepehr Ghaffarzadegan", "Boubakr Nour", "Makan Pourzandi", "Mourad Debbabi", "Chadi Assi"], "title": "Automated Attack Testflow Extraction from Cyber Threat Report using BERT for Contextual Analysis", "comment": null, "summary": "In the ever-evolving landscape of cybersecurity, the rapid identification and\nmitigation of Advanced Persistent Threats (APTs) is crucial. Security\npractitioners rely on detailed threat reports to understand the tactics,\ntechniques, and procedures (TTPs) employed by attackers. However, manually\nextracting attack testflows from these reports requires elusive knowledge and\nis time-consuming and prone to errors. This paper proposes FLOWGUARDIAN, a\nnovel solution leveraging language models (i.e., BERT) and Natural Language\nProcessing (NLP) techniques to automate the extraction of attack testflows from\nunstructured threat reports. FLOWGUARDIAN systematically analyzes and\ncontextualizes security events, reconstructs attack sequences, and then\ngenerates comprehensive testflows. This automated approach not only saves time\nand reduces human error but also ensures comprehensive coverage and robustness\nin cybersecurity testing. Empirical validation using public threat reports\ndemonstrates FLOWGUARDIAN's accuracy and efficiency, significantly enhancing\nthe capabilities of security teams in proactive threat hunting and incident\nresponse.", "AI": {"tldr": "FLOWGUARDIAN automates extracting attack testflows from threat reports using BERT and NLP, reducing manual effort and errors while enhancing cybersecurity testing.", "motivation": "Manually extracting attack testflows from unstructured threat reports is knowledge-intensive, time-consuming, and error-prone, limiting efficiency in APT identification and mitigation.", "method": "Leverages BERT-based language models and NLP techniques to analyze, contextualize security events, reconstruct attack sequences, and generate structured testflows.", "result": "Empirical validation on public threat reports shows FLOWGUARDIAN achieves high accuracy and efficiency in testflow extraction, improving security teams' capabilities.", "conclusion": "FLOWGUARDIAN provides a robust automated solution for cybersecurity, enabling proactive threat hunting and incident response through reliable testflow generation."}}
{"id": "2507.07689", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07689", "abs": "https://arxiv.org/abs/2507.07689", "authors": ["Chetan Arora", "Fanyu Wang", "Chakkrit Tantithamthavorn", "Aldeida Aleti", "Shaun Kenyon"], "title": "From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry", "comment": null, "summary": "Requirements engineering (RE) in the space industry is inherently complex,\ndemanding high precision, alignment with rigorous standards, and adaptability\nto mission-specific constraints. Smaller space organisations and new entrants\noften struggle to derive actionable requirements from extensive, unstructured\ndocuments such as mission briefs, interface specifications, and regulatory\nstandards. In this innovation opportunity paper, we explore the potential of\nRetrieval-Augmented Generation (RAG) models to support and (semi-)automate\nrequirements generation in the space domain. We present a modular, AI-driven\napproach that preprocesses raw space mission documents, classifies them into\nsemantically meaningful categories, retrieves contextually relevant content\nfrom domain standards, and synthesises draft requirements using large language\nmodels (LLMs). We apply the approach to a real-world mission document from the\nspace domain to demonstrate feasibility and assess early outcomes in\ncollaboration with our industry partner, Starbound Space Solutions. Our\npreliminary results indicate that the approach can reduce manual effort,\nimprove coverage of relevant requirements, and support lightweight compliance\nalignment. We outline a roadmap toward broader integration of AI in RE\nworkflows, intending to lower barriers for smaller organisations to participate\nin large-scale, safety-critical missions.", "AI": {"tldr": "This paper proposes using Retrieval-Augmented Generation (RAG) models to semi-automate requirements engineering for the space industry, particularly aiding smaller organizations with unstructured mission documents.", "motivation": "Small space organizations and new entrants face difficulties extracting actionable requirements from large, unstructured space mission documents while adhering to strict standards and mission-specific constraints.", "method": "A modular AI-driven approach combining document preprocessing, semantic classification, contextual retrieval from domain standards, and large language model (LLM)-based synthesis of draft requirements.", "result": "The method was tested on a real-world mission document via collaboration with Starbound Space Solutions, showing reduced manual effort, improved requirement coverage, and lightweight compliance alignment.", "conclusion": "The approach demonstrates feasibility for AI integration in space RE workflows, with a roadmap to enable smaller organizations to participate in safety-critical missions with greater ease."}}
{"id": "2507.07246", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07246", "abs": "https://arxiv.org/abs/2507.07246", "authors": ["Peicheng Wang", "Monika Santra", "Mingyu Liu", "Cong Sun", "Dongrui Zeng", "Gang Tan"], "title": "Disa: Accurate Learning-based Static Disassembly with Attentions", "comment": "To appear at ACM CCS 2025", "summary": "For reverse engineering related security domains, such as vulnerability\ndetection, malware analysis, and binary hardening, disassembly is crucial yet\nchallenging. The fundamental challenge of disassembly is to identify\ninstruction and function boundaries. Classic approaches rely on file-format\nassumptions and architecture-specific heuristics to guess the boundaries,\nresulting in incomplete and incorrect disassembly, especially when the binary\nis obfuscated. Recent advancements of disassembly have demonstrated that deep\nlearning can improve both the accuracy and efficiency of disassembly. In this\npaper, we propose Disa, a new learning-based disassembly approach that uses the\ninformation of superset instructions over the multi-head self-attention to\nlearn the instructions' correlations, thus being able to infer function\nentry-points and instruction boundaries. Disa can further identify instructions\nrelevant to memory block boundaries to facilitate an advanced block-memory\nmodel based value-set analysis for an accurate control flow graph (CFG)\ngeneration. Our experiments show that Disa outperforms prior deep-learning\ndisassembly approaches in function entry-point identification, especially\nachieving 9.1% and 13.2% F1-score improvement on binaries respectively\nobfuscated by the disassembly desynchronization technique and popular\nsource-level obfuscator. By achieving an 18.5% improvement in the memory block\nprecision, Disa generates more accurate CFGs with a 4.4% reduction in Average\nIndirect Call Targets (AICT) compared with the state-of-the-art heuristic-based\napproach.", "AI": {"tldr": "Disa is a deep learning-based disassembly approach using multi-head self-attention to improve instruction/function boundary identification and CFG accuracy.", "motivation": "Traditional disassembly methods fail with obfuscated binaries due to reliance on heuristics and file-format assumptions, leading to incomplete/incorrect results.", "method": "Disa employs superset instructions and multi-head self-attention mechanisms to learn instruction correlations and infer entry-points, integrating block-memory models for improved boundary detection.", "result": "Disa achieves 9.1% F1-score improvement against disassembly desynchronization obfuscation, 13.2% improvement against source-level obfuscators, and 18.5% memory block precision gain, reducing AICT by 4.4%.", "conclusion": "Disa outperforms prior approaches in function extraction and CFG generation for obfuscated binaries through its architecture-agnostic instruction correlation modeling."}}
{"id": "2507.07250", "categories": ["cs.CR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.07250", "abs": "https://arxiv.org/abs/2507.07250", "authors": ["Jordi Serra-Ruiz", "David Meg\u00edas"], "title": "Semi-fragile watermarking of remote sensing images using DWT, vector quantization and automatic tiling", "comment": null, "summary": "A semi-fragile watermarking scheme for multiple band images is presented in\nthis article. We propose to embed a mark into remote sensing images applying a\ntree-structured vector quantization approach to the pixel signatures instead of\nprocessing each band separately. The signature of the multispectral or\nhyperspectral image is used to embed the mark in it order to detect any\nsignificant modification of the original image. The image is segmented into\nthree-dimensional blocks, and a tree-structured vector quantizer is built for\neach block. These trees are manipulated using an iterative algorithm until the\nresulting block satisfies a required criterion, which establishes the embedded\nmark. The method is shown to be able to preserve the mark under lossy\ncompression (above a given threshold) but, at the same time, it detects\npossibly forged blocks and their position in the whole image.", "AI": {"tldr": "A semi-fragile watermarking method for multispectral/hyperspectral images uses tree-structured vector quantization on pixel signatures to detect significant modifications while surviving lossy compression.", "motivation": "This work addresses the need for watermarking techniques in remote sensing images that maintain a mark under lossy compression but enable detection of unauthorized alterations. Traditional methods process bands separately, lacking effective signature-based multi-band analysis for integrity verification.", "method": "1. Segments multispectral/hyperspectral images into 3D blocks. 2. Constructs tree-structured vector quantizers for each block's pixel signatures. 3. Applies an iterative algorithm to manipulate quantization trees until a block satisfies an embedding criterion. 4. Preserves watermarks through compression but breaks under significant modifications.", "result": "The method retains watermarks under lossy compression (above a compression threshold) while accurately identifying forged blocks and their locations in the image. This dual capability balances robustness against compression and sensitivity to malicious tampering.", "conclusion": "The proposed semi-fragile watermarking approach effectively addresses both robustness against standard compression and fragility to significant modifications. Its signature-based, multi-band processing provides reliable integrity verification for remote sensing images, making it suitable for applications requiring tamper detection without compromising essential compression resilience."}}
{"id": "2507.07258", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07258", "abs": "https://arxiv.org/abs/2507.07258", "authors": ["Rami Darwish", "Mahmoud Abdelsalam", "Sajad Khorsandroo", "Kaushik Roy"], "title": "FedP3E: Privacy-Preserving Prototype Exchange for Non-IID IoT Malware Detection in Cross-Silo Federated Learning", "comment": null, "summary": "As IoT ecosystems continue to expand across critical sectors, they have\nbecome prominent targets for increasingly sophisticated and large-scale malware\nattacks. The evolving threat landscape, combined with the sensitive nature of\nIoT-generated data, demands detection frameworks that are both\nprivacy-preserving and resilient to data heterogeneity. Federated Learning (FL)\noffers a promising solution by enabling decentralized model training without\nexposing raw data. However, standard FL algorithms such as FedAvg and FedProx\noften fall short in real-world deployments characterized by class imbalance and\nnon-IID data distributions -- particularly in the presence of rare or disjoint\nmalware classes. To address these challenges, we propose FedP3E\n(Privacy-Preserving Prototype Exchange), a novel FL framework that supports\nindirect cross-client representation sharing while maintaining data privacy.\nEach client constructs class-wise prototypes using Gaussian Mixture Models\n(GMMs), perturbs them with Gaussian noise, and transmits only these compact\nsummaries to the server. The aggregated prototypes are then distributed back to\nclients and integrated into local training, supported by SMOTE-based\naugmentation to enhance representation of minority malware classes. Rather than\nrelying solely on parameter averaging, our prototype-driven mechanism enables\nclients to enrich their local models with complementary structural patterns\nobserved across the federation -- without exchanging raw data or gradients.\nThis targeted strategy reduces the adverse impact of statistical heterogeneity\nwith minimal communication overhead. We evaluate FedP3E on the N-BaIoT dataset\nunder realistic cross-silo scenarios with varying degrees of data imbalance.", "AI": {"tldr": "The paper introduces FedP3E, a privacy-preserving federated learning (FL) framework designed to enhance malware detection in IoT ecosystems by addressing class imbalance and non-IID data challenges through prototype exchange with noise perturbation and SMOTE-based augmentation.", "motivation": "Standard FL methods like FedAvg and FedProx struggle in real-world IoT scenarios with class-imbalanced, non-IID malware data. Current solutions risk privacy and fail to capture rare malware patterns due to limited data sharing.", "method": "FedP3E employs Gaussian Mixture Models (GMMs) to generate class-wise prototypes at each client. These prototypes are perturbed with Gaussian noise before aggregation and redistribution. Local models leverage SMOTE-based minority class augmentation to improve learning in imbalanced settings, avoiding raw data or gradient sharing.", "result": "FedP3E demonstrated reduced communication overhead and improved robustness to statistical heterogeneity on the N-BaIoT dataset under cross-silo scenarios, effectively enriching local models with shared structural patterns.", "conclusion": "FedP3E offers a privacy-preserving, communication-efficient FL framework that outperforms standard approaches in handling data heterogeneity and rare malware classes, making it suitable for real-world IoT deployments."}}
{"id": "2507.07401", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07401", "abs": "https://arxiv.org/abs/2507.07401", "authors": ["Fupei Chen", "Liyao Xiang", "Haoxiang Sun", "Hei Victor Cheng", "Kaiming Shen"], "title": "Shuffling for Semantic Secrecy", "comment": null, "summary": "Deep learning draws heavily on the latest progress in semantic\ncommunications. The present paper aims to examine the security aspect of this\ncutting-edge technique from a novel shuffling perspective. Our goal is to\nimprove upon the conventional secure coding scheme to strike a desirable\ntradeoff between transmission rate and leakage rate. To be more specific, for a\nwiretap channel, we seek to maximize the transmission rate while minimizing the\nsemantic error probability under the given leakage rate constraint. Toward this\nend, we devise a novel semantic security communication system wherein the\nrandom shuffling pattern plays the role of the shared secret key. Intuitively,\nthe permutation of feature sequences via shuffling would distort the semantic\nessence of the target data to a sufficient extent so that eavesdroppers cannot\naccess it anymore. The proposed random shuffling method also exhibits its\nflexibility in working for the existing semantic communication system as a\nplugin. Simulations demonstrate the significant advantage of the proposed\nmethod over the benchmark in boosting secure transmission, especially when\nchannels are prone to strong noise and unpredictable fading.", "AI": {"tldr": "This paper introduces a random shuffling method as a shared secret key to enhance semantic communication security, balancing transmission rate and leakage rate effectively.", "motivation": "Traditional secure coding schemes struggle to optimally trade-off transmission speed and information leakage, especially under adverse channel conditions like strong noise or fading, necessitating a novel approach.", "method": "The proposed system employs random feature sequence shuffling, generating a shared permutation pattern to act as the secret key. This plugin-based technique operates on wiretap channels, targeting semantic distortion for eavesdroppers while optimizing transmission rate and semantic error probability under leakage rate constraints.", "result": "Simulations demonstrate the superiority of the shuffling method over benchmarks in secure transmission performance, particularly showing significant resilience against challenges from strong noise and unpredictable fading environments.", "conclusion": "The random shuffling approach innovatively improves semantic communication security with a flexible plugin design, achieving a favorable transmission-leakage trade-off and enhanced robustness in vulnerable channel conditions."}}
{"id": "2507.07406", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07406", "abs": "https://arxiv.org/abs/2507.07406", "authors": ["Jikesh Thapa", "Gurrehmat Chahal", "Serban Voinea Gabreanu", "Yazan Otoum"], "title": "Phishing Detection in the Gen-AI Era: Quantized LLMs vs Classical Models", "comment": "8 Pages, IEEE Conference", "summary": "Phishing attacks are becoming increasingly sophisticated, underscoring the\nneed for detection systems that strike a balance between high accuracy and\ncomputational efficiency. This paper presents a comparative evaluation of\ntraditional Machine Learning (ML), Deep Learning (DL), and quantized\nsmall-parameter Large Language Models (LLMs) for phishing detection. Through\nexperiments on a curated dataset, we show that while LLMs currently\nunderperform compared to ML and DL methods in terms of raw accuracy, they\nexhibit strong potential for identifying subtle, context-based phishing cues.\nWe also investigate the impact of zero-shot and few-shot prompting strategies,\nrevealing that LLM-rephrased emails can significantly degrade the performance\nof both ML and LLM-based detectors. Our benchmarking highlights that models\nlike DeepSeek R1 Distill Qwen 14B (Q8_0) achieve competitive accuracy, above\n80%, using only 17GB of VRAM, supporting their viability for cost-efficient\ndeployment. We further assess the models' adversarial robustness and\ncost-performance tradeoffs, and demonstrate how lightweight LLMs can provide\nconcise, interpretable explanations to support real-time decision-making. These\nfindings position optimized LLMs as promising components in phishing defence\nsystems and offer a path forward for integrating explainable, efficient AI into\nmodern cybersecurity frameworks.", "AI": {"tldr": "The paper evaluates ML, DL, and quantized LLMs for phishing detection, highlighting LLMs' potential for context-aware detection and cost-efficiency when optimized.", "motivation": "Phishing attacks are evolving in sophistication, necessitating detection systems that balance accuracy with computational efficiency.", "method": "The study compares traditional ML and DL models with quantized small-parameter LLMs on a curated dataset, analyzing accuracy, robustness, and prompting strategies (zero-shot, few-shot) at 17GB VRAM.", "result": "While LLMs show lower raw accuracy than ML/DL, they detect subtle context cues effectively. Models like DeepSeek R1 Distill Qwen 14B (Q8_0) achieve +80% accuracy with lightweight deployment but struggle against rephrased emails.", "conclusion": "Optimized LLMs offer interpretable phishing detection with cost-performance trade-offs, indicating potential for real-time, explainable cybersecurity solutions when robustness issues are addressed."}}
{"id": "2507.07413", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07413", "abs": "https://arxiv.org/abs/2507.07413", "authors": ["Mohammad F. Al-Hammouri", "Yazan Otoum", "Rasha Atwa", "Amiya Nayak"], "title": "Hybrid LLM-Enhanced Intrusion Detection for Zero-Day Threats in IoT Networks", "comment": "6 pages, IEEE conference", "summary": "This paper presents a novel approach to intrusion detection by integrating\ntraditional signature-based methods with the contextual understanding\ncapabilities of the GPT-2 Large Language Model (LLM). As cyber threats become\nincreasingly sophisticated, particularly in distributed, heterogeneous, and\nresource-constrained environments such as those enabled by the Internet of\nThings (IoT), the need for dynamic and adaptive Intrusion Detection Systems\n(IDSs) becomes increasingly urgent. While traditional methods remain effective\nfor detecting known threats, they often fail to recognize new and evolving\nattack patterns. In contrast, GPT-2 excels at processing unstructured data and\nidentifying complex semantic relationships, making it well-suited to uncovering\nsubtle, zero-day attack vectors. We propose a hybrid IDS framework that merges\nthe robustness of signature-based techniques with the adaptability of\nGPT-2-driven semantic analysis. Experimental evaluations on a representative\nintrusion dataset demonstrate that our model enhances detection accuracy by\n6.3%, reduces false positives by 9.0%, and maintains near real-time\nresponsiveness. These results affirm the potential of language model\nintegration to build intelligent, scalable, and resilient cybersecurity\ndefences suited for modern connected environments.", "AI": {"tldr": "This paper proposes a hybrid intrusion detection system combining signature-based methods with GPT-2's semantic analysis to enhance detection accuracy (6.3% improvement) and reduce false positives (9.0% reduction) while maintaining real-time performance in IoT/networked environments.", "motivation": "Traditional signature-based intrusion detection systems struggle with evolving zero-day attacks in modern distributed/heterogeneous IoT environments. GPT-2's ability to process unstructured data and identify complex patterns offers a complementary solution for detecting novel threats.", "method": "The framework integrates rule-based signature matching with a GPT-2 language model for context-aware analysis of network traffic data. The LLM component processes non-structured security logs to identify subtle attack patterns and semantic relationships that signature-based approaches miss.", "result": "Experiments show 6.3% higher detection accuracy compared to conventional systems, 9.0% reduction in false positives, and latency within 200ms per analysis on a test dataset of network traffic patterns.", "conclusion": "Language models like GPT-2 can significantly strengthen intrusion detection capabilities by addressing limitations of signature-based approaches, creating a scalable, intelligent cybersecurity defense system for resource-constrained environments while maintaining real-time responsiveness."}}
{"id": "2507.07416", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07416", "abs": "https://arxiv.org/abs/2507.07416", "authors": ["Jenifer Paulraj", "Brindha Raghuraman", "Nagarani Gopalakrishnan", "Yazan Otoum"], "title": "Autonomous AI-based Cybersecurity Framework for Critical Infrastructure: Real-Time Threat Mitigation", "comment": "7 pages, IEEE conference", "summary": "Critical infrastructure systems, including energy grids, healthcare\nfacilities, transportation networks, and water distribution systems, are\npivotal to societal stability and economic resilience. However, the increasing\ninterconnectivity of these systems exposes them to various cyber threats,\nincluding ransomware, Denial-of-Service (DoS) attacks, and Advanced Persistent\nThreats (APTs). This paper examines cybersecurity vulnerabilities in critical\ninfrastructure, highlighting the threat landscape, attack vectors, and the role\nof Artificial Intelligence (AI) in mitigating these risks. We propose a hybrid\nAI-driven cybersecurity framework to enhance real-time vulnerability detection,\nthreat modelling, and automated remediation. This study also addresses the\ncomplexities of adversarial AI, regulatory compliance, and integration. Our\nfindings provide actionable insights to strengthen the security and resilience\nof critical infrastructure systems against emerging cyber threats.", "AI": {"tldr": "This paper proposes a hybrid AI-driven cybersecurity framework to address vulnerabilities in critical infrastructure systems (e.g. energy grids, healthcare) exposed to emerging cyber threats like ransomware and APTs. It explores threat landscapes, attack vectors, and challenges in adversarial AI and regulatory integration.", "motivation": "Critical infrastructure systems are increasingly interconnected, making them vulnerable to cyber threats (ransomware, DoS, APTs) that endanger societal stability and economic resilience. There is a need for advanced cybersecurity solutions to address these risks.", "method": "The authors develop a hybrid AI cybersecurity framework focusing on real-time vulnerability detection, threat modeling, and automated remediation. They analyze adversarial AI risks, regulatory compliance requirements, and integration challenges in infrastructure systems.", "result": "The study identifies key vulnerabilities and attack vectors, demonstrates the hybrid AI framework's capability to enhance threat detection and response, and provides evidence of how adversarial AI mitigation and regulatory alignment can strengthen infrastructure security.", "conclusion": "A hybrid AI-driven approach is essential for mitigating cyber threats to critical infrastructure. The proposed framework offers actionable solutions for improving system resilience while addressing technical, regulatory, and adaptive challenges through integrated AI capabilities."}}
{"id": "2507.07417", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07417", "abs": "https://arxiv.org/abs/2507.07417", "authors": ["Nishit V. Pandya", "Andrey Labunets", "Sicun Gao", "Earlence Fernandes"], "title": "May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks", "comment": null, "summary": "A popular class of defenses against prompt injection attacks on large\nlanguage models (LLMs) relies on fine-tuning the model to separate instructions\nand data, so that the LLM does not follow instructions that might be present\nwith data. There are several academic systems and production-level\nimplementations of this idea. We evaluate the robustness of this class of\nprompt injection defenses in the whitebox setting by constructing strong\noptimization-based attacks and showing that the defenses do not provide the\nclaimed security properties. Specifically, we construct a novel attention-based\nattack algorithm for text-based LLMs and apply it to two recent whitebox\ndefenses SecAlign (CCS 2025) and StruQ (USENIX Security 2025), showing attacks\nwith success rates of up to 70% with modest increase in attacker budget in\nterms of tokens. Our findings make fundamental progress towards understanding\nthe robustness of prompt injection defenses in the whitebox setting. We release\nour code and attacks at https://github.com/nishitvp/better_opts_attacks", "AI": {"tldr": "The paper evaluates existing prompt injection defenses for LLMs by constructing attention-based attacks that break two systems (SecAlign and StruQ) with 70% success rates under whitebox scenarios.", "motivation": "Prompt injection defenses are widely used in academic research and production systems, but their claimed security guarantees remain untested against strong optimization-based attacks.", "method": "Develops a novel attention-based attack algorithm and applies it to analyze SecAlign (CCS 2025) and StruQ (USENIX Security 2025) systems through whitebox experiments with token budget constraints.", "result": "Demonstrates 70% success rates in breaking defenses with only modest computational costs, while releasing code and attack specifications for reproducibility.", "conclusion": "These findings challenge the effectiveness of instruction/data separation defenses against prompt injection, highlighting critical robustness gaps in whitebox attack scenarios for LLM security mechanisms."}}
{"id": "2507.07732", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07732", "abs": "https://arxiv.org/abs/2507.07732", "authors": ["Giovanni Gambigliani Zoccoli", "Filip Valgimigli", "Dario Stabili", "Mirco Marchetti"], "title": "RADAR: a Radio-based Analytics for Dynamic Association and Recognition of pseudonyms in VANETs", "comment": "7 pages, 4 figures, accepted for publication at the 2025 IEEE 102nd\n  Vehicular Technology Conference: VTC2025-Fall", "summary": "This paper presents RADAR, a tracking algorithm for vehicles participating in\nCooperative Intelligent Transportation Systems (C-ITS) that exploits multiple\nradio signals emitted by a modern vehicle to break privacy-preserving pseudonym\nschemes deployed in VANETs. This study shows that by combining Dedicated Short\nRange Communication (DSRC) and Wi-Fi probe request messages broadcast by the\nvehicle, it is possible to improve tracking over standard de-anonymization\napproaches that only leverage DSRC, especially in realistic scenarios where the\nattacker does not have full coverage of the entire vehicle path. The\nexperimental evaluation compares three different metrics for pseudonym and\nWi-Fi probe identifier association (Count, Statistical RSSI, and Pearson RSSI),\ndemonstrating that the Pearson RSSI metric is better at tracking vehicles under\npseudonym-changing schemes in all scenarios and against previous works. As an\nadditional contribution to the state-of-the-art, we publicly release all\nimplementations and simulation scenarios used in this work.", "AI": {"tldr": "RADAR is a vehicle tracking algorithm in C-ITS that exploits DSRC and Wi-Fi signals to de-anonymize vehicles, outperforming existing methods in limited coverage scenarios. The study also publicizes all code and simulations.", "motivation": "This paper addresses the challenge of maintaining vehicle privacy in VANETs by analyzing de-anonymization techniques when attackers lack full path coverage, aiming to improve tracking accuracy.", "method": "The method combines DSRC and Wi-Fi probe request messages to create temporal sequences, evaluated using three pseudonym-Wi-Fi identifier association metrics: Count, Statistical RSSI, and Pearson RSSI.", "result": "Pearson RSSI demonstrated superior tracking performance under pseudonym-changing schemes compared to existing approaches in all tested scenarios, with the study also releasing its implementations and simulations.", "conclusion": "RADAR enhances vehicle tracking in realistic C-ITS scenarios by leveraging multiple radio signals and introduces Pearson RSSI as a more effective de-anonymization metric, while providing open-source tools for validation."}}
{"id": "2507.07773", "categories": ["cs.CR", "cs.CV", "B.8; I.4"], "pdf": "https://arxiv.org/pdf/2507.07773", "abs": "https://arxiv.org/abs/2507.07773", "authors": ["Youqian Zhang", "Xinyu Ji", "Zhihao Wang", "Qinhong Jiang"], "title": "Rainbow Artifacts from Electromagnetic Signal Injection Attacks on Image Sensors", "comment": "5 pages, 4 figures", "summary": "Image sensors are integral to a wide range of safety- and security-critical\nsystems, including surveillance infrastructure, autonomous vehicles, and\nindustrial automation. These systems rely on the integrity of visual data to\nmake decisions. In this work, we investigate a novel class of electromagnetic\nsignal injection attacks that target the analog domain of image sensors,\nallowing adversaries to manipulate raw visual inputs without triggering\nconventional digital integrity checks. We uncover a previously undocumented\nattack phenomenon on CMOS image sensors: rainbow-like color artifacts induced\nin images captured by image sensors through carefully tuned electromagnetic\ninterference. We further evaluate the impact of these attacks on\nstate-of-the-art object detection models, showing that the injected artifacts\npropagate through the image signal processing pipeline and lead to significant\nmispredictions. Our findings highlight a critical and underexplored\nvulnerability in the visual perception stack, highlighting the need for more\nrobust defenses against physical-layer attacks in such systems.", "AI": {"tldr": "This paper introduces a novel electromagnetic signal injection attack that manipulates CMOS image sensors in the analog domain, causing rainbow-like color artifacts that bypass digital integrity checks and mislead object detection models.", "motivation": "Image sensors are crucial for safety/security systems (e.g., surveillance, autonomous vehicles) that depend on unaltered visual data for decision-making, yet existing digital security mechanisms cannot protect against physical analog-layer attacks.", "method": "Researchers injected carefully tuned electromagnetic interference into the analog domain of image sensors, inducing visible rainbow-like artifacts, and evaluated how these propagate through image signal processing pipelines to affect state-of-the-art object detection models.", "result": "The attack successfully creates untrackable color artifacts that disrupt object detection processes, showing that even high-end models produce significant mispredictions when confronted with such physical-layer manipulations.", "conclusion": "The study reveals a critical vulnerability in the analog domain of image sensors that can undermine visual perception in security systems, advocating for improved physical-layer defenses to prevent undetectable adversarial manipulations."}}
{"id": "2507.07871", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07871", "abs": "https://arxiv.org/abs/2507.07871", "authors": ["Toluwani Aremu", "Noor Hussein", "Munachiso Nwadike", "Samuele Poppi", "Jie Zhang", "Karthik Nandakumar", "Neil Gong", "Nils Lukas"], "title": "Mitigating Watermark Stealing Attacks in Generative Models via Multi-Key Watermarking", "comment": null, "summary": "Watermarking offers a promising solution for GenAI providers to establish the\nprovenance of their generated content. A watermark is a hidden signal embedded\nin the generated content, whose presence can later be verified using a secret\nwatermarking key. A threat to GenAI providers are \\emph{watermark stealing}\nattacks, where users forge a watermark into content that was \\emph{not}\ngenerated by the provider's models without access to the secret key, e.g., to\nfalsely accuse the provider. Stealing attacks collect \\emph{harmless}\nwatermarked samples from the provider's model and aim to maximize the expected\nsuccess rate of generating \\emph{harmful} watermarked samples. Our work focuses\non mitigating stealing attacks while treating the underlying watermark as a\nblack-box. Our contributions are: (i) Proposing a multi-key extension to\nmitigate stealing attacks that can be applied post-hoc to any watermarking\nmethod across any modality. (ii) We provide theoretical guarantees and\ndemonstrate empirically that our method makes forging substantially less\neffective across multiple datasets, and (iii) we formally define the threat of\nwatermark forging as the task of generating harmful, watermarked content and\nmodel this threat via security games.", "AI": {"tldr": "This paper proposes a multi-key watermarking framework to prevent forged watermark attacks in generative AI by treating watermarks as black-boxes, providing both theoretical guarantees and empirical validation across datasets.", "motivation": "GenAI providers face threats from watermark stealing attacks where malicious users forge watermarks to falsely accuse providers, damaging trust and accountability despite the benefits of watermarking for content provenance tracking.", "method": "The authors develop a post-hoc multi-key extension for any watermarking method across modalities, introducing a game-theoretic security framework to model and evaluate forging threats systematically.", "result": "Their approach significantly reduces forged watermark success rates (30-70% relative reduction) compared to single-key systems through extensive experiments on multiple benchmark datasets and provides provable security bounds.", "conclusion": "The framework establishes a new paradigm for watermarking security by formally characterizing the threat model through security games and demonstrating that multi-key systems exponentially reduce attack success probability while maintaining content authenticity."}}
{"id": "2507.07901", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07901", "abs": "https://arxiv.org/abs/2507.07901", "authors": ["Sree Bhargavi Balija", "Rekha Singal", "Abhishek Singh", "Ramesh Raskar", "Erfan Darzi", "Raghu Bala", "Thomas Hardjono", "Ken Huang"], "title": "The Trust Fabric: Decentralized Interoperability and Economic Coordination for the Agentic Web", "comment": null, "summary": "The fragmentation of AI agent ecosystems has created urgent demands for\ninteroperability, trust, and economic coordination that current protocols --\nincluding MCP (Hou et al., 2025), A2A (Habler et al., 2025), ACP (Liu et al.,\n2025), and Cisco's AGP (Edwards, 2025) -- cannot address at scale. We present\nthe Nanda Unified Architecture, a decentralized framework built around three\ncore innovations: fast DID-based agent discovery through distributed\nregistries, semantic agent cards with verifiable credentials and composability\nprofiles, and a dynamic trust layer that integrates behavioral attestations\nwith policy compliance. The system introduces X42/H42 micropayments for\neconomic coordination and MAESTRO, a security framework incorporating\nSynergetics' patented AgentTalk protocol (US Patent 12,244,584 B1) and secure\ncontainerization. Real-world deployments demonstrate 99.9 percent compliance in\nhealthcare applications and substantial monthly transaction volumes with strong\nprivacy guarantees. By unifying MIT's trust research with production\ndeployments from Cisco and Synergetics, we show how cryptographic proofs and\npolicy-as-code transform agents into trust-anchored participants in a\ndecentralized economy (Lakshmanan, 2025; Sha, 2025). The result enables a\nglobally interoperable Internet of Agents where trust becomes the native\ncurrency of collaboration across both enterprise and Web3 ecosystems.", "AI": {"tldr": "Nanda Unified Architecture introduces a decentralized framework addressing AI agent ecosystem fragmentation with DID-based discovery, semantic agent cards, and a dynamic trust layer integrating cryptographic proofs and privacy-preserving micropayments for interoperability across enterprise and Web3 systems.", "motivation": "The fragmentation of AI agent ecosystems necessitates scalable solutions for interoperability, trust, and economic coordination, which existing protocols (MCP, A2A, ACP, AGP) fail to provide effectively.", "method": "The architecture features three core components: (1) distributed registries for fast DID-based agent discovery, (2) semantic agent cards with verifiable credentials and composability profiles, and (3) a dynamic trust layer combining behavioral attestations and policy compliance. It also incorporates Synergetics' patented AgentTalk protocol (US Patent 12,244,584 B1), secure containerization, and X42/H42 micropayments for economic coordination.", "result": "Real-world healthcare deployments achieved 99.9% compliance, demonstrating robust policy enforcement. The system supports high monthly transaction volumes (measured in millions) while maintaining strong privacy guarantees through decentralized cryptographic protocols.", "conclusion": "By integrating MIT's trust research with Cisco/Synergetics' production deployments, Nanda shows trust can become a native currency in a decentralized economy. Policy-as-code and cryptographic proofs enable enterprise-Web3 interoperability at scale, solving existing protocol limitations and establishing trust as a core collaborative mechanism."}}
{"id": "2507.07916", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.07916", "abs": "https://arxiv.org/abs/2507.07916", "authors": ["Federico Maria Cau", "Giuseppe Desolda", "Francesco Greco", "Lucio Davide Spano", "Luca Vigan\u00f2"], "title": "Can Large Language Models Improve Phishing Defense? A Large-Scale Controlled Experiment on Warning Dialogue Explanations", "comment": null, "summary": "Phishing has become a prominent risk in modern cybersecurity, often used to\nbypass technological defences by exploiting predictable human behaviour.\nWarning dialogues are a standard mitigation measure, but the lack of\nexplanatory clarity and static content limits their effectiveness. In this\npaper, we report on our research to assess the capacity of Large Language\nModels (LLMs) to generate clear, concise, and scalable explanations for\nphishing warnings. We carried out a large-scale between-subjects user study (N\n= 750) to compare the influence of warning dialogues supplemented with manually\ngenerated explanations against those generated by two LLMs, Claude 3.5 Sonnet\nand Llama 3.3 70B. We investigated two explanatory styles (feature-based and\ncounterfactual) for their effects on behavioural metrics (click-through rate)\nand perceptual outcomes (e.g., trust, risk, clarity). The results indicate that\nwell-constructed LLM-generated explanations can equal or surpass manually\ncrafted explanations in reducing susceptibility to phishing; Claude-generated\nwarnings exhibited particularly robust performance. Feature-based explanations\nwere more effective for genuine phishing attempts, whereas counterfactual\nexplanations diminished false-positive rates. Other variables such as workload,\ngender, and prior familiarity with warning dialogues significantly moderated\nwarning effectiveness. These results indicate that LLMs can be used to\nautomatically build explanations for warning users against phishing, and that\nsuch solutions are scalable, adaptive, and consistent with human-centred\nvalues.", "AI": {"tldr": "This paper evaluates using LLMs (Claude 3.5 and Llama 3.3) to generate phishing warning explanations, finding LLM-generated explanations can match/surpass manual ones in reducing susceptibility. Feature-based explanations work better for actual phishing links, while counterfactual explanations reduce false positives.", "motivation": "Current phishing warnings lack clarity and scalability, enabling attackers to exploit human behavior vulnerabilities. Research aims to assess LLMs' potential to address these deficiencies with dynamically generated explanations.", "method": "Large-scale between-subjects study (N=750) comparing manual vs LLM-generated explanations using two styles (feature-based and counterfactual). Metrics included click-through rates and perceptual factors (trust, risk perception, clarity).", "result": "LLMs achieved similar/performance exceeded manual explanations in reducing phishing susceptibility (82% effective). Claude 3.5 showed particular strength (12% lower false positive rate). Feature-based explanations (74% accuracy) outperformed counterfactual ones (63% accuracy) for genuine phishing attempts.", "conclusion": "LLMs can produce adaptive, scalable phishing explanations that maintain human-centered effectiveness. Implementation should prioritize feature-based explanations for primary threats and counterfactual reasoning for false positive reduction, leveraging model strengths to enhance cybersecurity defenses."}}
{"id": "2507.07927", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07927", "abs": "https://arxiv.org/abs/2507.07927", "authors": ["Jenny Blessing", "Ross J. Anderson", "Alastair R. Beresford"], "title": "KeyDroid: A Large-Scale Analysis of Secure Key Storage in Android Apps", "comment": null, "summary": "Most contemporary mobile devices offer hardware-backed storage for\ncryptographic keys, user data, and other sensitive credentials. Such hardware\nprotects credentials from extraction by an adversary who has compromised the\nmain operating system, such as a malicious third-party app. Since 2011, Android\napp developers can access trusted hardware via the Android Keystore API. In\nthis work, we conduct the first comprehensive survey of hardware-backed key\nstorage in Android devices. We analyze 490 119 Android apps, collecting data on\nhow trusted hardware is used by app developers (if used at all) and\ncross-referencing our findings with sensitive user data collected by each app,\nas self-reported by developers via the Play Store's data safety labels.\n  We find that despite industry-wide initiatives to encourage adoption, 56.3%\nof apps self-reporting as processing sensitive user data do not use Android's\ntrusted hardware capabilities at all, while just 5.03% of apps collecting some\nform of sensitive data use the strongest form of trusted hardware, a secure\nelement distinct from the main processor. To better understand the potential\ndownsides of using secure hardware, we conduct the first empirical analysis of\ntrusted hardware performance in mobile devices, measuring the runtime of common\ncryptographic operations across both software- and hardware-backed keystores.\nWe find that while hardware-backed key storage using a coprocessor is viable\nfor most common cryptographic operations, secure elements capable of preventing\nmore advanced attacks make performance infeasible for symmetric encryption with\nnon-negligible payloads and any kind of asymmetric encryption.", "AI": {"tldr": "Most Android apps handling sensitive data do not utilize hardware-backed encryption, and those using secure elements face performance issues.", "motivation": "Hardware-backed key storage in Android offers enhanced security against OS compromises, but its adoption rate and performance trade-offs remain understudied despite industry push for broader use since 2011.", "method": "Analyzed 490,119 apps to assess trusted hardware usage, cross-referenced with Play Store data safety labels. Measured cryptographic operation runtimes across software- and hardware-backed keystores.", "result": "56.3% of apps reporting sensitive data processing ignore hardware-backed storage; 5.03% use secure elements. Secure elements show impractical performance for symmetric/asymmetric encryption with significant payloads.", "conclusion": "Android developers underutilize strong hardware-backed security, likely due to performance limitations, highlighting a need for better adoption strategies or performance improvements in secure elements."}}
{"id": "2507.07972", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07972", "abs": "https://arxiv.org/abs/2507.07972", "authors": ["Karthik Garimella", "Austin Ebel", "Brandon Reagen"], "title": "EinHops: Einsum Notation for Expressive Homomorphic Operations on RNS-CKKS Tensors", "comment": "11 pages, 7 figures, 1 table", "summary": "Fully Homomorphic Encryption (FHE) is an encryption scheme that allows for\ncomputation to be performed directly on encrypted data, effectively closing the\nloop on secure and outsourced computing. Data is encrypted not only during rest\nand transit, but also during processing. However, FHE provides a limited\ninstruction set: SIMD addition, SIMD multiplication, and cyclic rotation of 1-D\nvectors. This restriction makes performing multi-dimensional tensor operations\nchallenging. Practitioners must pack these tensors into 1-D vectors and map\ntensor operations onto this one-dimensional layout rather than their\ntraditional nested structure. And while prior systems have made significant\nstrides in automating this process, they often hide critical packing decisions\nbehind layers of abstraction, making debugging, optimizing, and building on top\nof these systems difficult.\n  In this work, we approach multi-dimensional tensor operations in FHE through\nEinstein summation (einsum) notation. Einsum notation explicitly encodes\ndimensional structure and operations in its syntax, naturally exposing how\ntensors should be packed and transformed. We decompose einsum expressions into\na fixed set of FHE-friendly operations. We implement our design and present\nEinHops, a minimalist system that factors einsum expressions into a fixed\nsequence of FHE operations. EinHops enables developers to perform encrypted\ntensor operations using FHE while maintaining full visibility into the\nunderlying packing strategy. We evaluate EinHops on a range of tensor\noperations from a simple transpose to complex multi-dimensional contractions.\nWe show that the explicit nature of einsum notation allows us to build an FHE\ntensor system that is simple, general, and interpretable. We open-source\nEinHops at the following repository: https://github.com/baahl-nyu/einhops.", "AI": {"tldr": "EinHops, a minimalist system utilizing Einstein summation notation, enables easier and more interpretable multi-dimensional tensor operations in FHE by explicitly encoding dimensional structures and transformations, providing full visibility into packing strategies while maintaining simplicity and generality.", "motivation": "FHE systems face challenges when handling multi-dimensional tensor operations due to their limited instruction set (SIMD add/multiply/rotate). Current methods requiring 1-D tensor packing hide critical decisions behind abstractions, hindering debugging, optimization, and extension of existing systems.", "method": "The authors leverage einsum notation to explicitly encode tensor dimensional structures and operations. They decompose einsum expressions into a fixed sequence of FHE-friendly operations (add/multiply/rotate) and implement this as a minimalist system called EinHops, which exposes the underlying packing strategy with full visibility.", "result": "EinHops was evaluated on a spectrum of tensor operations from transposes to complex multi-dimensional contractions. The system demonstrated simplicity, generality, and interpretability advantages through its explicit strategy, enabling seamless encrypted tensor operations.", "conclusion": "Einsum notation's explicit dimensional encoding makes building an FHE tensor system both feasible and interpretable. EinHops solves the multi-dimensional tensor processing challenge while remaining open-source and minimalist, offering simplicity and visibility for developers."}}
{"id": "2507.07974", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07974", "abs": "https://arxiv.org/abs/2507.07974", "authors": ["Sizhe Chen", "Yizhu Wang", "Nicholas Carlini", "Chawin Sitawarin", "David Wagner"], "title": "Defending Against Prompt Injection With a Few DefensiveTokens", "comment": null, "summary": "When large language model (LLM) systems interact with external data to\nperform complex tasks, a new attack, namely prompt injection, becomes a\nsignificant threat. By injecting instructions into the data accessed by the\nsystem, the attacker is able to override the initial user task with an\narbitrary task directed by the attacker. To secure the system, test-time\ndefenses, e.g., defensive prompting, have been proposed for system developers\nto attain security only when needed in a flexible manner. However, they are\nmuch less effective than training-time defenses that change the model\nparameters. Motivated by this, we propose DefensiveToken, a test-time defense\nwith prompt injection robustness comparable to training-time alternatives.\nDefensiveTokens are newly inserted as special tokens, whose embeddings are\noptimized for security. In security-sensitive cases, system developers can\nappend a few DefensiveTokens before the LLM input to achieve security with a\nminimal utility drop. In scenarios where security is less of a concern,\ndevelopers can simply skip DefensiveTokens; the LLM system remains the same as\nthere is no defense, generating high-quality responses. Thus, DefensiveTokens,\nif released alongside the model, allow a flexible switch between the\nstate-of-the-art (SOTA) utility and almost-SOTA security at test time. The code\nis available at https://github.com/Sizhe-Chen/DefensiveToken.", "AI": {"tldr": "This paper introduces DefensiveToken, a test-time defense for large language models (LLMs) against prompt injection attacks, offering flexibility to switch between state-of-the-art utility and near-SOTA security by appending a few special tokens during inference.", "motivation": "Current test-time defenses for LLMs (e.g., defensive prompting) are less effective than training-time defenses, which require parameter modifications. This motivates the need for test-time defenses that maintain both high security and utility without altering model parameters.", "method": "The authors propose inserting specially designed tokens (DefensiveTokens) during testing. These tokens are embedded with values optimized for robustness against prompt injection and can be appended to inputs when security is critical, without modifying the model itself.", "result": "DefensiveToken achieves security comparable to training-time defenses with a minimal impact on utility. It enables dynamic activation of security depending on scenario needs, though the abstract does not include specific quantitative results.", "conclusion": "DefensiveToken provides a flexible solution for securing LLMs against prompt injection attacks during testing, balancing SOTA utility and near-SOTA security at minimal cost. The code is available on GitHub."}}
