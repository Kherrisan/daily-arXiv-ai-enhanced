{"id": "2509.00005", "categories": ["cs.CR", "cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.00005", "abs": "https://arxiv.org/abs/2509.00005", "authors": ["Rohit Dube"], "title": "Per-sender neural network classifiers for email authorship validation", "comment": "11 pages, 5 figures, 8 tables", "summary": "Business email compromise and lateral spear phishing attacks are among modern\norganizations' most costly and damaging threats. While inbound phishing\ndefenses have improved significantly, most organizations still trust internal\nemails by default, leaving themselves vulnerable to attacks from compromised\nemployee accounts. In this work, we define and explore the problem of\nauthorship validation: verifying whether a claimed sender actually authored a\ngiven email. Authorship validation is a lightweight, real-time defense that\ncomplements traditional detection methods by modeling per-sender writing style.\nFurther, the paper presents a collection of new datasets based on the Enron\ncorpus. These simulate inauthentic messages using both human-written and large\nlanguage model-generated emails. The paper also evaluates two classifiers -- a\nNaive Bayes model and a character-level convolutional neural network (Char-CNN)\n-- for the authorship validation task. Our experiments show that the Char-CNN\nmodel achieves high accuracy and F1 scores under various circumstances.\nFinally, we discuss deployment considerations and show that per-sender\nauthorship classifiers are practical for integrating into existing commercial\nemail security systems with low overhead."}
{"id": "2509.00006", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.00006", "abs": "https://arxiv.org/abs/2509.00006", "authors": ["Motunrayo Adebayo"], "title": "Case Studies: Effective Approaches for Navigating Cross-Border Cloud Data Transfers Amid U.S. Government Privacy and Safety Concerns", "comment": "Privacy, Security", "summary": "This study attempts to explain the impact of information exchange from one\ncountry to another, as well as the legal and technological implications for\nthese exchanges. Due to the emergence of cloud technology, possibilities for\nfree exchange of information between countries have increased rapidly, as it\nhas become possible to save information in a country and access it in almost\nany part of the world. Countries all around the world have been confronted with\ndeveloping frameworks to facilitate this process, although there are\nsignificant challenges which must be confronted on legal and technological\nfronts, as loopholes in the framework adopted by countries may hinder free\naccess to information stored on cloud, and also compromise data privacy. Cloud\ntechnology is impacting a lot of issues, including domestic and international\nbusinesses, hence the need for a study to propose measures for safe exchange of\ninformation using cloud technology."}
{"id": "2509.00043", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.00043", "abs": "https://arxiv.org/abs/2509.00043", "authors": ["Md Faizul Bari", "Yi Xie", "Meghna Roy Choudhury", "Shreyas Sen"], "title": "Keystroke Detection by Exploiting Unintended RF Emission from Repaired USB Keyboards", "comment": "This journal version is an extended version of a previously published\n  conference paper which can be found here:\n  https://ieeexplore.ieee.org/abstract/document/10181751", "summary": "Electronic devices and cables inadvertently emit RF emissions as a byproduct\nof signal processing and/or transmission. Labeled as electromagnetic\nemanations, they form an EM side-channel for data leakage. Previously, it was\nbelieved that such leakage could be contained within a facility since they are\nweak signals with a short transmission range. However, in the preliminary\nversion of this work [1], we found that the traditional cable repairing process\nforms a tiny monopole antenna that helps emanations transmit over a long range.\nExperimentation with three types of cables revealed that emanations from\nrepaired cables remain detectable even at >4 m and can penetrate a 14 cm thick\nconcrete wall. In this extended version, we show that such emanation can be\nexploited at a long distance for information extraction by detecting keystrokes\ntyped on a repaired USB keyboard. By collecting data for 70 different\nkeystrokes at different distances from the target in 3 diverse environments\n(open space, a corridor outside an office room, and outside a building) and\ndeveloping an efficient detection algorithm, ~100% keystroke detection accuracy\nhas been achieved up to 12 m distance, which is the highest reported accuracy\nat such a long range for USB keyboards in the literature. The effect of two\nexperimental factors, interference and human-body coupling, has been\ninvestigated thoroughly. Along with exploring the vulnerability, multi-layer\nexternal metal shielding during the repairing process as a possible remedy has\nbeen explored. This work exposes a new attack surface caused by hardware\nmodification, its exploitation, and potential countermeasures."}
{"id": "2509.00059", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.00059", "abs": "https://arxiv.org/abs/2509.00059", "authors": ["Andres Alejandre", "Kassandra Delfin", "Victor Castano"], "title": "Cryptographic Challenges: Masking Sensitive Data in Cyber Crimes through ASCII Art", "comment": "11 pages, 4 figures", "summary": "The use of ASCII art as a novel approach to masking sensitive information in\ncybercrime, focusing on its potential role in protecting personal data during\nthe delivery process and beyond, is presented. By examining the unique\nproperties of ASCII art and its historical context, this study discusses the\nadvantages and limitations of employing this technique in various cybercrime\nscenarios. Additionally, providing recommendations for enhancing data security\npractices and fostering a culture of privacy awareness in both businesses and\nindividuals. The findings suggest that ASCII art, with its simplicity and\nambiguity, can serve as an effective tool against cybercriminals, emphasizing\nthe need for robust data security measures and increased privacy awareness in\ntoday's interconnected world."}
{"id": "2509.00140", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00140", "abs": "https://arxiv.org/abs/2509.00140", "authors": ["Songhui Yue"], "title": "LLM-based Triplet Extraction for Automated Ontology Generation in Software Engineering Standards", "comment": null, "summary": "Ontologies have supported knowledge representation and whitebox reasoning for\ndecades; thus, the automated ontology generation (AOG) plays a crucial role in\nscaling their use. Software engineering standards (SES) consist of long,\nunstructured text (with high noise) and paragraphs with domain-specific terms.\nIn this setting, relation triple extraction (RTE), together with term\nextraction, constitutes the first stage toward AOG. This work proposes an\nopen-source large language model (LLM)-assisted approach to RTE for SES.\nInstead of solely relying on prompt-engineering-based methods, this study\npromotes the use of LLMs as an aid in constructing ontologies and explores an\neffective AOG workflow that includes document segmentation, candidate term\nmining, LLM-based relation inference, term normalization, and cross-section\nalignment. Golden-standard benchmarks at three granularities are constructed\nand used to evaluate the ontology generated from the study. The results show\nthat it is comparable and potentially superior to the OpenIE method of triple\nextraction."}
{"id": "2509.00081", "categories": ["cs.CR", "cs.AI", "I.2.7; I.2.6; I.2.4"], "pdf": "https://arxiv.org/pdf/2509.00081", "abs": "https://arxiv.org/abs/2509.00081", "authors": ["Luca Cotti", "Anisa Rula", "Devis Bianchini", "Federico Cerutti"], "title": "Enabling Transparent Cyber Threat Intelligence Combining Large Language Models and Domain Ontologies", "comment": "14 pages, 3 figures, 6 tables, accepted at XAI-KRKG@ECAI25: First\n  International ECAI Workshop on eXplainable AI, Knowledge Representation and\n  Knowledge Graphs, October 25-30, 2025, Bologna, Italy", "summary": "Effective Cyber Threat Intelligence (CTI) relies upon accurately structured\nand semantically enriched information extracted from cybersecurity system logs.\nHowever, current methodologies often struggle to identify and interpret\nmalicious events reliably and transparently, particularly in cases involving\nunstructured or ambiguous log entries. In this work, we propose a novel\nmethodology that combines ontology-driven structured outputs with Large\nLanguage Models (LLMs), to build an Artificial Intelligence (AI) agent that\nimproves the accuracy and explainability of information extraction from\ncybersecurity logs. Central to our approach is the integration of domain\nontologies and SHACL-based constraints to guide the language model's output\nstructure and enforce semantic validity over the resulting graph. Extracted\ninformation is organized into an ontology-enriched graph database, enabling\nfuture semantic analysis and querying. The design of our methodology is\nmotivated by the analytical requirements associated with honeypot log data,\nwhich typically comprises predominantly malicious activity. While our case\nstudy illustrates the relevance of this scenario, the experimental evaluation\nis conducted using publicly available datasets. Results demonstrate that our\nmethod achieves higher accuracy in information extraction compared to\ntraditional prompt-only approaches, with a deliberate focus on extraction\nquality rather than processing speed."}
{"id": "2509.00256", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.00256", "abs": "https://arxiv.org/abs/2509.00256", "authors": ["Yutong Wang", "Cindy Rubio-Gonz√°lez"], "title": "LLM-Based Program Generation for Triggering Numerical Inconsistencies Across Compilers", "comment": null, "summary": "Floating-point inconsistencies across compilers can undermine the reliability\nof numerical software. We present LLM4FP, the first framework that uses Large\nLanguage Models (LLMs) to generate floating-point programs specifically\ndesigned to trigger such inconsistencies. LLM4FP combines Grammar-Based\nGeneration and Feedback-Based Mutation to produce diverse and valid programs.\nWe evaluate LLM4FP across multiple compilers and optimization levels, measuring\ninconsistency rate, time cost, and program diversity. LLM4FP detects over twice\nas many inconsistencies compared to the state-of-the-art tool, Varity. Notably,\nmost of the inconsistencies involve real-valued differences, rather than\nextreme values like NaN or infinities. LLM4FP also uncovers inconsistencies\nacross a wider range of optimization levels, and finds the most mismatches\nbetween host and device compilers. These results show that LLM-guided program\ngeneration improves the detection of numerical inconsistencies."}
{"id": "2509.00085", "categories": ["cs.CR", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.00085", "abs": "https://arxiv.org/abs/2509.00085", "authors": ["Tobin South"], "title": "Private, Verifiable, and Auditable AI Systems", "comment": "PhD thesis", "summary": "The growing societal reliance on artificial intelligence necessitates robust\nframeworks for ensuring its security, accountability, and trustworthiness. This\nthesis addresses the complex interplay between privacy, verifiability, and\nauditability in modern AI, particularly in foundation models. It argues that\ntechnical solutions that integrate these elements are critical for responsible\nAI innovation. Drawing from international policy contributions and technical\nresearch to identify key risks in the AI pipeline, this work introduces novel\ntechnical solutions for critical privacy and verifiability challenges.\nSpecifically, the research introduces techniques for enabling verifiable and\nauditable claims about AI systems using zero-knowledge cryptography; utilizing\nsecure multi-party computation and trusted execution environments for\nauditable, confidential deployment of large language models and information\nretrieval; and implementing enhanced delegation mechanisms, credentialing\nsystems, and access controls to secure interactions with autonomous and\nmulti-agent AI systems. Synthesizing these technical advancements, this\ndissertation presents a cohesive perspective on balancing privacy,\nverifiability, and auditability in foundation model-based AI systems, offering\npractical blueprints for system designers and informing policy discussions on\nAI safety and governance."}
{"id": "2509.00466", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.00466", "abs": "https://arxiv.org/abs/2509.00466", "authors": ["Negar Hashemi", "Amjed Tahir", "Shawn Rasheed", "August Shi", "Rachel Blagojevic"], "title": "JS-TOD: Detecting Order-Dependent Flaky Tests in Jest", "comment": null, "summary": "We present JS-TOD (JavaScript Test Order-dependency Detector), a tool that\ncan extract, reorder, and rerun Jest tests to reveal possible order-dependent\ntest flakiness. Test order dependency is one of the leading causes of test\nflakiness. Ideally, each test should operate in isolation and yield consistent\nresults no matter the sequence in which tests are run. However, in practice,\ntest outcomes can vary depending on their execution order. JS-TOD employed a\nsystematic approach to randomising tests, test suites, and describe blocks. The\ntool is highly customisable, as one can set the number of orders and reruns\nrequired (the default setting is 10 reorder and 10 reruns for each test and\ntest suite). Our evaluation using JS-TOD reveals two main causes of test order\ndependency flakiness: shared files and shared mocking state between tests."}
{"id": "2509.00088", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00088", "abs": "https://arxiv.org/abs/2509.00088", "authors": ["Ting-Chun Liu", "Ching-Yu Hsu", "Kuan-Yi Lee", "Chi-An Fu", "Hung-yi Lee"], "title": "AEGIS : Automated Co-Evolutionary Framework for Guarding Prompt Injections Schema", "comment": null, "summary": "Prompt injection attacks pose a significant challenge to the safe deployment\nof Large Language Models (LLMs) in real-world applications. While prompt-based\ndetection offers a lightweight and interpretable defense strategy, its\neffectiveness has been hindered by the need for manual prompt engineering. To\naddress this issue, we propose AEGIS , an Automated co-Evolutionary framework\nfor Guarding prompt Injections Schema. Both attack and defense prompts are\niteratively optimized against each other using a gradient-like natural language\nprompt optimization technique. This framework enables both attackers and\ndefenders to autonomously evolve via a Textual Gradient Optimization (TGO)\nmodule, leveraging feedback from an LLM-guided evaluation loop. We evaluate our\nsystem on a real-world assignment grading dataset of prompt injection attacks\nand demonstrate that our method consistently outperforms existing baselines,\nachieving superior robustness in both attack success and detection.\nSpecifically, the attack success rate (ASR) reaches 1.0, representing an\nimprovement of 0.26 over the baseline. For detection, the true positive rate\n(TPR) improves by 0.23 compared to the previous best work, reaching 0.84, and\nthe true negative rate (TNR) remains comparable at 0.89. Ablation studies\nconfirm the importance of co-evolution, gradient buffering, and multi-objective\noptimization. We also confirm that this framework is effective in different\nLLMs. Our results highlight the promise of adversarial training as a scalable\nand effective approach for guarding prompt injections."}
{"id": "2509.00785", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.00785", "abs": "https://arxiv.org/abs/2509.00785", "authors": ["Elena Masserini", "Daniela Micucci", "Leonardo Mariani"], "title": "Bug Whispering: Towards Audio Bug Reporting", "comment": "2 pages, 1 figure, IEEE International Symposium on Software\n  Reliability Engineering (ISSRE), 2025, Fast Abstracts Session", "summary": "Bug reporting is a key feature of mobile applications, as it enables\ndevelopers to collect information about faults that escaped testing and thus\naffected end-users. This paper explores the idea of allowing end-users to\nimmediately report the problems that they experience by recording and\nsubmitting audio messages. Audio recording is simple to implement and has the\npotential to increase the number of bug reports that development teams can\ngather, thus potentially improving the rate at which bugs are identified and\nfixed. However, audio bug reports exhibit specific characteristics that\nchallenge existing techniques for reproducing bugs. This paper discusses these\nchallenges based on a preliminary experiment, and motivates further research on\nthe collection and analysis of audio-based bug reports"}
{"id": "2509.00104", "categories": ["cs.CR", "cs.IT", "math.IT", "quant-ph", "94A60, 81P94, 94A17, 68Q12", "E.3; K.6.5; F.1.2; F.2.1"], "pdf": "https://arxiv.org/pdf/2509.00104", "abs": "https://arxiv.org/abs/2509.00104", "authors": ["Ruopengyu Xu", "Chenglian Liu"], "title": "Enhanced R√©nyi Entropy-Based Post-Quantum Key Agreement with Provable Security and Information-Theoretic Guarantees", "comment": "11 pages, 3 tables", "summary": "This paper presents an enhanced post-quantum key agreement protocol based on\nR\\'{e}nyi entropy, addressing vulnerabilities in the original construction\nwhile preserving information-theoretic security properties. We develop a\ntheoretical framework leveraging entropy-preserving operations and\nsecret-shared verification to achieve provable security against quantum\nadversaries. Through entropy amplification techniques and quantum-resistant\ncommitments, the protocol establishes $2^{128}$ quantum security guarantees\nunder the quantum random oracle model. Key innovations include a\nconfidentiality-preserving verification mechanism using distributed polynomial\ncommitments, tightened min-entropy bounds with guaranteed non-negativity, and\ncomposable security proofs in the quantum universal composability framework.\nUnlike computational approaches, our method provides information-theoretic\nsecurity without hardness assumptions while maintaining polynomial complexity.\nTheoretical analysis demonstrates resilience against known quantum attack\nvectors, including Grover-accelerated brute force and quantum memory attacks.\nThe protocol achieves parameterization for 128-bit quantum security with\nefficient $\\mathcal{O}(n^2)$ communication complexity. Extensions to secure\nmultiparty computation and quantum network applications are established,\nproviding a foundation for long-term cryptographic security. All security\nclaims are derived from mathematical proofs; this theoretical work presents no\nexperimental validation."}
{"id": "2509.01006", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01006", "abs": "https://arxiv.org/abs/2509.01006", "authors": ["Daniela Damian", "Bachan Ghimire", "Ze Shi Li"], "title": "REConnect: Participatory RE that Matters", "comment": "23 pages", "summary": "Software increasingly shapes the infrastructures of daily life, making\nrequirements engineering (RE) central to ensuring that systems align with human\nvalues and lived experiences. Yet, current popular practices such as CrowdRE\nand AI-assisted elicitation strategies risk detaching requirements work from\nthe cultural, social, and political contexts that shape lived experiences,\nhuman values, and real user needs. In this paper, we introduce REConnect that\nre-centers RE on the human connection as central to the understanding of lived\nexperiences where impact is sought. REConnect advocates for a human-centered\nparticipatory approach \"that matters\" to the communities and beneficiaries\ninvolved, ensuring alignment with their values and aspirations. Drawing on\nthree case studies of societal impact: BloodSync in rural Nepal, Herluma\nsupporting women at risk of homelessness in Canada, and BridgingRoots to\nrevitalize Indigenous languages in the Canadian Arctic. REConnect argues that\nthree key principles and enablers: building trusting relationships,\nco-designing with and alongside stakeholders, and empowering users as agents of\nchange, can yield requirements that are culturally grounded, socially\nlegitimate, and sustainable beyond system delivery. REConnect also proposes a\nset of actionable practices (REActions) that embed relationality and ongoing\nstakeholder engagement throughout requirements elicitation, analysis, and\nvalidation of solution development. Finally, we situate REConnect in the era of\nGenerative AI. While AI can accelerate and scale certain RE tasks, its\nintegration must be guided by participatory practices that not only preserve\nhuman agency but also empower humans' roles to become guardians of values and\nethics, inclusion amplifiers, curators of AI outputs, and co-reflectors in\niterative review cycles."}
{"id": "2509.00124", "categories": ["cs.CR", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.00124", "abs": "https://arxiv.org/abs/2509.00124", "authors": ["Shaked Zychlinski"], "title": "A Whole New World: Creating a Parallel-Poisoned Web Only AI-Agents Can See", "comment": "10 pages, 1 figure", "summary": "This paper introduces a novel attack vector that leverages website cloaking\ntechniques to compromise autonomous web-browsing agents powered by Large\nLanguage Models (LLMs). As these agents become more prevalent, their unique and\noften homogenous digital fingerprints - comprising browser attributes,\nautomation framework signatures, and network characteristics - create a new,\ndistinguishable class of web traffic. The attack exploits this\nfingerprintability. A malicious website can identify an incoming request as\noriginating from an AI agent and dynamically serve a different, \"cloaked\"\nversion of its content. While human users see a benign webpage, the agent is\npresented with a visually identical page embedded with hidden, malicious\ninstructions, such as indirect prompt injections. This mechanism allows\nadversaries to hijack agent behavior, leading to data exfiltration, malware\nexecution, or misinformation propagation, all while remaining completely\ninvisible to human users and conventional security crawlers. This work\nformalizes the threat model, details the mechanics of agent fingerprinting and\ncloaking, and discusses the profound security implications for the future of\nagentic AI, highlighting the urgent need for robust defenses against this\nstealthy and scalable attack."}
{"id": "2509.01048", "categories": ["cs.SE", "D.2.1"], "pdf": "https://arxiv.org/pdf/2509.01048", "abs": "https://arxiv.org/abs/2509.01048", "authors": ["Ateeq Sharfuddin", "Travis Breaux"], "title": "Generative Goal Modeling", "comment": "11 pages,", "summary": "In software engineering, requirements may be acquired from stakeholders\nthrough elicitation methods, such as interviews, observational studies, and\nfocus groups. When supporting acquisition from interviews, business analysts\nmust review transcripts to identify and document requirements. Goal modeling is\na popular technique for representing early stakeholder requirements as it lends\nitself to various analyses, including refinement to map high-level goals into\nsoftware operations, and conflict and obstacle analysis. In this paper, we\ndescribe an approach to use textual entailment to reliably extract goals from\ninterview transcripts and to construct goal models. The approach has been\nevaluated on 15 interview transcripts across 29 application domains. The\nfindings show that GPT-4o can reliably extract goals from interview\ntranscripts, matching 62.0% of goals acquired by humans from the same\ntranscripts, and that GPT-4o can trace goals to originating text in the\ntranscript with 98.7% accuracy. In addition, when evaluated by human\nannotators, GPT-4o generates goal model refinement relationships among\nextracted goals with 72.2% accuracy."}
{"id": "2509.00266", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.00266", "abs": "https://arxiv.org/abs/2509.00266", "authors": ["Qishen Sam Liang"], "title": "A Systematic Approach to Estimate the Security Posture of a Cyber Infrastructure: A Technical Report", "comment": "11 pages, 5 figures, technical report", "summary": "Academic and research Cyber Infrastructures (CI) present unique security\nchallenges due to their collaborative nature, heterogeneous components, and the\nlack of practical, tailored security assessment frameworks. Existing standards\ncan be too generic or complex for CI administrators to apply effectively. This\nreport introduces a systematic, mission-centric approach to estimate and\nanalyze the security posture of a CI. The framework guides administrators\nthrough a top-down process: (1) defining unacceptable losses and security\nmissions, (2) identifying associated system hazards and critical assets, and\n(3) modeling the CI's components and their relationships as a security\nknowledge graph. The core of this methodology is the construction of directed\nattack graphs, which systematically map all potential paths an adversary could\ntake from an entry point to a critical asset. By visualizing these attack paths\nalongside defense mechanisms, the framework provides a clear, comprehensive\noverview of the system's vulnerabilities and security gaps. This structured\napproach enables CI operators to proactively assess risks, prioritize\nmitigation strategies, and make informed, actionable decisions to strengthen\nthe overall security posture of the CI."}
{"id": "2509.01068", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01068", "abs": "https://arxiv.org/abs/2509.01068", "authors": ["Chong Wang", "Haoning Wu", "Peng Liang", "Maya Daneva", "Marten van Sinderen"], "title": "A Survey on the Techniques and Tools for Automated Requirements Elicitation and Analysis of Mobile Apps", "comment": null, "summary": "[Background:] Research on automated requirements elicitation and analysis of\nmobile apps employed lots of techniques and tools proposed by RE researchers\nand practitioners. However, little is known about the characteristics of these\ntechniques and tools as well as the RE tasks in requirements elicitation and\nanalysis that got supported with the help of respective techniques and tools.\n[Aims:] The goal of this paper is to investigate the state-of-the-art of the\ntechniques and tools used in automated requirements elicitation and analysis of\nmobile apps. [Method:] We carried out a systematic mapping study by following\nthe guidelines of Kitchenham et al. [Results:] Based on 73 selected papers, we\nfound the most frequently used techniques - semi-automatic techniques, and the\nmain characteristics of the tools - open-sourced and non-self-developed tools\nfor requirements analysis and text pre-processing. Plus, the most three\ninvestigated RE tasks are requirements analysis, mining and classification.\n[Conclusions:] Our most important conclusions are: (1) there is a growth in the\nuse of techniques and tools in automated requirements elicitation and analysis\nof mobile apps, (2) semi-automatic techniques are mainly used in the\npublications on this research topic, (3) requirements analysis, mining and\nclassification are the top three RE tasks with the support of automatic\ntechniques and tools, and (4) the most popular tools are open-sourced and\nnon-self-developed, and they are mainly used in requirements analysis and text\nprocessing."}
{"id": "2509.00300", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.00300", "abs": "https://arxiv.org/abs/2509.00300", "authors": ["Ghadeer Almusaddar", "Yicheng Zhang", "Saber Ganjisaffar", "Barry Williams", "Yu David Liu", "Dmitry Ponomare", "Nael Abu-Ghazaleh"], "title": "ShadowScope: GPU Monitoring and Validation via Composable Side Channel Signals", "comment": null, "summary": "As modern systems increasingly rely on GPUs for computationally intensive\ntasks such as machine learning acceleration, ensuring the integrity of GPU\ncomputation has become critically important. Recent studies have shown that GPU\nkernels are vulnerable to both traditional memory safety issues (e.g., buffer\noverflow attacks) and emerging microarchitectural threats (e.g., Rowhammer\nattacks), many of which manifest as anomalous execution behaviors observable\nthrough side-channel signals. However, existing golden model based validation\napproaches that rely on such signals are fragile, highly sensitive to\ninterference, and do not scale well across GPU workloads with diverse\nscheduling behaviors. To address these challenges, we propose ShadowScope, a\nmonitoring and validation framework that leverages a composable golden model.\nInstead of building a single monolithic reference, ShadowScope decomposes\ntrusted kernel execution into modular, repeatable functions that encode key\nbehavioral features. This composable design captures execution patterns at\nfiner granularity, enabling robust validation that is resilient to noise,\nworkload variation, and interference across GPU workloads. To further reduce\nreliance on noisy software-only monitoring, we introduce ShadowScope+, a\nhardware-assisted validation mechanism that integrates lightweight on-chip\nchecks into the GPU pipeline. ShadowScope+ achieves high validation accuracy\nwith an average runtime overhead of just 4.6%, while incurring minimal hardware\nand design complexity. Together, these contributions demonstrate that\nside-channel observability can be systematically repurposed into a practical\ndefense for GPU kernel integrity."}
{"id": "2509.01149", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01149", "abs": "https://arxiv.org/abs/2509.01149", "authors": ["Hui Zeng", "Zhihao Xu", "Hui Li", "Siwen Wang", "Qian Ma"], "title": "Compiler Bugs Detection in Logic Synthesis Tools via Linear Upper Confidence Bound", "comment": null, "summary": "Field-Programmable Gate Arrays (FPGAs) play an indispensable role in\nElectronic Design Automation (EDA), translating Register-Transfer Level (RTL)\ndesigns into gate-level netlists. The correctness and reliability of FPGA logic\nsynthesis tools are critically important, as unnoticed bugs in these tools may\ninfect the final hardware implementations. However, recent approaches often\nrely heavily on random selection strategies, limiting the structural diversity\nof the generated HDL test cases and resulting in inadequate exploration of the\ntool's feature space. To address this limitation, we propose Lin-Hunter, a\nnovel testing framework designed to systematically enhance the diversity of HDL\ntest cases and the efficiency of FPGA logic synthesis tool validation.\nSpecifically, Lin-Hunter introduces a principled set of metamorphic\ntransformation rules to generate functionally equivalent yet structurally\ndiverse HDL test case variants, effectively addressing the limited diversity of\nexisting test inputs. To further enhance bug discovery efficiency, Lin-Hunter\nintegrates an adaptive strategy selection mechanism based on the Linear Upper\nConfidence Bound (LinUCB) method. This method leverages feedback from synthesis\nlogs of previously executed test cases to dynamically prioritize transformation\nstrategies that have empirically demonstrated a higher likelihood of triggering\nsynthesis bugs. Comprehensive experiments conducted over a three-month period\ndemonstrate the practical effectiveness of Lin-Hunter. Our method has\ndiscovered 18 unique bugs, including 10 previously unreported defects, which\nhave been confirmed by official developers. Moreover, our method outperforms\nstate-of-the-art testing methods in both test-case diversity and bug-discovery\nefficiency."}
{"id": "2509.00437", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.00437", "abs": "https://arxiv.org/abs/2509.00437", "authors": ["Hamideh Haghiri", "Rajesh Baidya", "Stefan Dvoretskii", "Klaus H. Maier-Hein", "Marco Nolden"], "title": "A Hybrid AI-based and Rule-based Approach to DICOM De-identification: A Solution for the MIDI-B Challenge", "comment": null, "summary": "Ensuring the de-identification of medical imaging data is a critical step in\nenabling safe data sharing. This paper presents a hybrid de-identification\nframework designed to process Digital Imaging and Communications in Medicine\n(DICOM) files. Our framework adopts a modified, pre-built rule-based component,\nupdated with The Cancer Imaging Archive (TCIA)'s best practices guidelines, as\noutlined in DICOM PS 3.15, for improved performance. It incorporates PaddleOCR,\na robust Optical Character Recognition (OCR) system for extracting text from\nimages, and RoBERTa, a fine-tuned transformer-based model for identifying and\nremoving Personally Identifiable Information (PII) and Protected Health\nInformation (PHI). Initially, the transformer-based model and the rule-based\ncomponent were integrated to process for both structured data and free text.\nHowever, this coarse-grained approach did not yield optimal results. To improve\nperformance, we refined our approach by applying the transformer model\nexclusively to free text, while structured data was handled only by rule-based\nmethods. In this framework the DICOM validator dciodvfy was leveraged to ensure\nthe integrity of DICOM files after the deID process. Through iterative\nrefinement, including the incorporation of custom rules and private tag\nhandling, the framework achieved a de-identification accuracy of 99.91% on the\nMIDI-B test dataset. The results demonstrate the effectiveness of combining\nrule-based compliance with AI-enabled adaptability in addressing the complex\nchallenges of DICOM de-identification."}
{"id": "2509.01255", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01255", "abs": "https://arxiv.org/abs/2509.01255", "authors": ["Oleksii Novikov", "Davide Fucci", "Oleksandr Adamov", "Daniel Mendez"], "title": "Policy-driven Software Bill of Materials on GitHub: An Empirical Study", "comment": "To be published in the proceedings of PROFES2025", "summary": "Background. The Software Bill of Materials (SBOM) is a machine-readable list\nof all the software dependencies included in a software. SBOM emerged as way to\nassist securing the software supply chain. However, despite mandates from\ngovernments to use SBOM, research on this artifact is still in its early\nstages. Aims. We want to understand the current state of SBOM in open-source\nprojects, focusing specifically on policy-driven SBOMs, i.e., SBOM created to\nachieve security goals, such as enhancing project transparency and ensuring\ncompliance, rather than being used as fixtures for tools or artificially\ngenerated for benchmarking or academic research purposes. Method. We performed\na mining software repository study to collect and carefully select SBOM files\nhosted on GitHub. We analyzed the information reported in policy-driven SBOMs\nand the vulnerabilities associated with the declared dependencies by means of\ndescriptive statistics. Results. We show that only 0.56% of popular GitHub\nrepositories contain policy-driven SBOM. The declared dependencies contain\n2,202 unique vulnerabilities, while 22% of them do not report licensing\ninformation. Conclusion. Our findings provide insights for SBOM usage to\nsupport security assessment and licensing."}
{"id": "2509.00476", "categories": ["cs.CR", "cs.AI", "68T10 (Primary) 68T05, 68M25 (Secondary)", "I.2.6; I.5.2; K.6.5"], "pdf": "https://arxiv.org/pdf/2509.00476", "abs": "https://arxiv.org/abs/2509.00476", "authors": ["Omar Khalid Ali Mohamed"], "title": "Cross-Domain Malware Detection via Probability-Level Fusion of Lightweight Gradient Boosting Models", "comment": "5 pages, 3 figures, 3 tables. Conference-style formatting (IEEEtran)", "summary": "The escalating sophistication of malware necessitates robust detection\nmechanisms that generalize across diverse data sources. Traditional\nsingle-dataset models struggle with cross-domain generalization and often incur\nhigh computational costs. This paper presents a novel, lightweight framework\nfor malware detection that employs probability-level fusion across three\ndistinct datasets: EMBER (static features), API Call Sequences (behavioral\nfeatures), and CIC Obfuscated Memory (memory patterns). Our method trains\nindividual LightGBM classifiers on each dataset, selects top predictive\nfeatures to ensure efficiency, and fuses their prediction probabilities using\noptimized weights determined via grid search. Extensive experiments demonstrate\nthat our fusion approach achieves a macro F1-score of 0.823 on a cross-domain\nvalidation set, significantly outperforming individual models and providing\nsuperior generalization. The framework maintains low computational overhead,\nmaking it suitable for real-time deployment, and all code and data are provided\nfor full reproducibility."}
{"id": "2509.01294", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01294", "abs": "https://arxiv.org/abs/2509.01294", "authors": ["Helge Spieker", "Nadjib Lazaar", "Arnaud Gotlieb", "Nassim Belmecheri"], "title": "Metamorphic Testing of Multimodal Human Trajectory Prediction", "comment": "Information and Software Technology", "summary": "Context: Predicting human trajectories is crucial for the safety and\nreliability of autonomous systems, such as automated vehicles and mobile\nrobots. However, rigorously testing the underlying multimodal Human Trajectory\nPrediction (HTP) models, which typically use multiple input sources (e.g.,\ntrajectory history and environment maps) and produce stochastic outputs\n(multiple possible future paths), presents significant challenges. The primary\ndifficulty lies in the absence of a definitive test oracle, as numerous future\ntrajectories might be plausible for any given scenario. Objectives: This\nresearch presents the application of Metamorphic Testing (MT) as a systematic\nmethodology for testing multimodal HTP systems. We address the oracle problem\nthrough metamorphic relations (MRs) adapted for the complexities and stochastic\nnature of HTP. Methods: We present five MRs, targeting transformations of both\nhistorical trajectory data and semantic segmentation maps used as an\nenvironmental context. These MRs encompass: 1) label-preserving geometric\ntransformations (mirroring, rotation, rescaling) applied to both trajectory and\nmap inputs, where outputs are expected to transform correspondingly. 2)\nMap-altering transformations (changing semantic class labels, introducing\nobstacles) with predictable changes in trajectory distributions. We propose\nprobabilistic violation criteria based on distance metrics between probability\ndistributions, such as the Wasserstein or Hellinger distance. Conclusion: This\nstudy introduces tool, a MT framework for the oracle-less testing of\nmultimodal, stochastic HTP systems. It allows for assessment of model\nrobustness against input transformations and contextual changes without\nreliance on ground-truth trajectories."}
{"id": "2509.00561", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.00561", "abs": "https://arxiv.org/abs/2509.00561", "authors": ["Yuwen Pu", "Zhou Feng", "Chunyi Zhou", "Jiahao Chen", "Chunqiang Hu", "Haibo Hu", "Shouling Ji"], "title": "FreeTalk:A plug-and-play and black-box defense against speech synthesis attacks", "comment": "under review", "summary": "Recently, speech assistant and speech verification have been used in many\nfields, which brings much benefit and convenience for us. However, when we\nenjoy these speech applications, our speech may be collected by attackers for\nspeech synthesis. For example, an attacker generates some inappropriate\npolitical opinions with the characteristic of the victim's voice by obtaining a\npiece of the victim's speech, which will greatly influence the victim's\nreputation. Specifically, with the appearance of some zero-shot voice\nconversion methods, the cost of speech synthesis attacks has been further\nreduced, which also brings greater challenges to user voice security and\nprivacy. Some researchers have proposed the corresponding privacy-preserving\nmethods. However, the existing approaches have some non-negligible drawbacks:\nlow transferability and robustness, high computational overhead. These\ndeficiencies seriously limit the existing method deployed in practical\nscenarios. Therefore, in this paper, we propose a lightweight, robust,\nplug-and-play privacy preservation method against speech synthesis attacks in a\nblack-box setting. Our method generates and adds a frequency-domain\nperturbation to the original speech to achieve privacy protection and high\nspeech quality. Then, we present a data augmentation strategy and noise\nsmoothing mechanism to improve the robustness of the proposed method. Besides,\nto reduce the user's defense overhead, we also propose a novel identity-wise\nprotection mechanism. It can generate a universal perturbation for one speaker\nand support privacy preservation for speech of any length. Finally, we conduct\nextensive experiments on 5 speech synthesis models, 5 speech verification\nmodels, 1 speech recognition model, and 2 datasets. The experimental results\ndemonstrate that our method has satisfying privacy-preserving performance, high\nspeech quality, and utility."}
{"id": "2509.01313", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01313", "abs": "https://arxiv.org/abs/2509.01313", "authors": ["Zhao Tian", "Junjie Chen"], "title": "Aligning Requirement for Large Language Model's Code Generation", "comment": "Accepted by ICSE 2026", "summary": "Code generation refers to the automatic generation of source code based on a\ngiven programming specification, which has garnered significant attention\nparticularly with the advancement of large language models (LLMs). However, due\nto the inherent complexity of real-world problems, the LLM-generated code often\nfails to fully align with the provided specification. While state-of-the-art\nagent-based techniques have been proposed to enhance LLM code generation, they\noverlook the critical issue of specification perception, resulting in\npersistent misalignment issues. Given that accurate perception of programming\nspecifications serves as the foundation of the LLM-based code generation\nparadigm, ensuring specification alignment is particularly crucial. In this\nwork, we draw on software requirements engineering to propose Specine, a novel\nspecification alignment technique for LLM code generation. Its key idea is to\nidentify misaligned input specifications, lift LLM-perceived specifications,\nand align them to enhance the code generation performance of LLMs. Our\ncomprehensive experiments on four state-of-the-art LLMs across five challenging\ncompetitive benchmarks by comparing with ten state-of-the-art baselines,\ndemonstrate the effectiveness of Specine. For example, Specine outperforms the\nmost effective baseline, achieving an average improvement of 29.60\\% across all\nsubjects in terms of Pass@1."}
{"id": "2509.00615", "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00615", "abs": "https://arxiv.org/abs/2509.00615", "authors": ["Narasimha Raghavan Veeraragavan", "Jan Franz Nyg√•rd"], "title": "Federated Survival Analysis with Node-Level Differential Privacy: Private Kaplan-Meier Curves", "comment": "This is the author's accepted version of the paper in IEEE FLTA 2025.\n  The final version of record will appear in Proceedings of the IEEE\n  International Conference on Federated Learning Technologies and Applications\n  (FLTA 2025)", "summary": "We investigate how to calculate Kaplan-Meier survival curves across multiple\nhealth-care jurisdictions while protecting patient privacy with node-level\ndifferential privacy. Each site discloses its curve only once, adding Laplace\nnoise whose scale is determined by the length of the common time grid; the\nserver then averages the noisy curves, so the overall privacy budget remains\nunchanged. We benchmark four one-shot smoothing techniques: Discrete Cosine\nTransform, Haar Wavelet shrinkage, adaptive Total-Variation denoising, and a\nparametric Weibull fit on the NCCTG lung-cancer cohort under five privacy\nlevels and three partition scenarios (uniform, moderately skewed, highly\nimbalanced). Total-Variation gives the best mean accuracy, whereas the\nfrequency-domain smoothers offer stronger worst-case robustness and the Weibull\nmodel shows the most stable behaviour at the strictest privacy setting. Across\nall methods the released curves keep the empirical log-rank type-I error below\nfifteen percent for privacy budgets of 0.5 and higher, demonstrating that\nclinically useful survival information can be shared without iterative training\nor heavy cryptography."}
{"id": "2509.01318", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01318", "abs": "https://arxiv.org/abs/2509.01318", "authors": ["Chiara Ghinami", "Jonas Winzer", "Nils Bosbach", "Lennart M. Reimann", "Lukas J√ºnger", "Simon W√∂rner", "Rainer Leupers"], "title": "Leveraging SystemC-TLM-based Virtual Prototypes for Embedded Software Fuzzing", "comment": null, "summary": "SystemC-based virtual prototypes have emerged as widely adopted tools to test\nsoftware ahead of hardware availability, reducing the time-to-market and\nimproving software reliability. Recently, fuzzing has become a popular method\nfor automated software testing due to its ability to quickly identify\ncorner-case errors. However, its application to embedded software is still\nlimited. Simulator tools can help bridge this gap by providing a more powerful\nand controlled execution environment for testing. Existing solutions, however,\noften tightly couple fuzzers with built-in simulators that lack support for\nhardware peripherals and of- fer limited flexibility, restricting their ability\nto test embedded software. To address these limitations, we present a framework\nthat allows the integration of American-Fuzzy-Lop-based fuzzers and\nSystemC-based simulators. The framework provides a harness to decouple the\nadopted fuzzer and simulator. In addition, it intercepts peripheral accesses\nand queries the fuzzer for values, effectively linking peripheral behavior to\nthe fuzzer. This solution enables flexible interchangeability of peripher- als\nwithin the simulation environment and supports the interfacing of different\nSystemC-based virtual prototypes. The flexibility of the pro- posed solution is\ndemonstrated by integrating the harness with different simulators and by\ntesting various softwares."}
{"id": "2509.00634", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00634", "abs": "https://arxiv.org/abs/2509.00634", "authors": ["Chaoyu Zhang", "Heng Jin", "Shanghao Shi", "Hexuan Yu", "Sydney Johns", "Y. Thomas Hou", "Wenjing Lou"], "title": "Enabling Trustworthy Federated Learning via Remote Attestation for Mitigating Byzantine Threats", "comment": null, "summary": "Federated Learning (FL) has gained significant attention for its\nprivacy-preserving capabilities, enabling distributed devices to\ncollaboratively train a global model without sharing raw data. However, its\ndistributed nature forces the central server to blindly trust the local\ntraining process and aggregate uncertain model updates, making it susceptible\nto Byzantine attacks from malicious participants, especially in\nmission-critical scenarios. Detecting such attacks is challenging due to the\ndiverse knowledge across clients, where variations in model updates may stem\nfrom benign factors, such as non-IID data, rather than adversarial behavior.\nExisting data-driven defenses struggle to distinguish malicious updates from\nnatural variations, leading to high false positive rates and poor filtering\nperformance.\n  To address this challenge, we propose Sentinel, a remote attestation\n(RA)-based scheme for FL systems that regains client-side transparency and\nmitigates Byzantine attacks from a system security perspective. Our system\nemploys code instrumentation to track control-flow and monitor critical\nvariables in the local training process. Additionally, we utilize a trusted\ntraining recorder within a Trusted Execution Environment (TEE) to generate an\nattestation report, which is cryptographically signed and securely transmitted\nto the server. Upon verification, the server ensures that legitimate client\ntraining processes remain free from program behavior violation or data\nmanipulation, allowing only trusted model updates to be aggregated into the\nglobal model. Experimental results on IoT devices demonstrate that Sentinel\nensures the trustworthiness of the local training integrity with low runtime\nand memory overhead."}
{"id": "2509.01389", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01389", "abs": "https://arxiv.org/abs/2509.01389", "authors": ["Diego Clerissi", "Elena Masserini", "Daniela Micucci", "Leonardo Mariani"], "title": "Towards Multi-Platform Mutation Testing of Task-based Chatbots", "comment": "4 pages, 1 figure, Accepted at 9th International Workshop on Software\n  Faults 2025", "summary": "Chatbots, also known as conversational agents, have become ubiquitous,\noffering services for a multitude of domains. Unlike general-purpose chatbots,\ntask-based chatbots are software designed to prioritize the completion of tasks\nof the domain they handle (e.g., flight booking). Given the growing popularity\nof chatbots, testing techniques that can generate full conversations as test\ncases have emerged. Still, thoroughly testing all the possible conversational\nscenarios implemented by a task-based chatbot is challenging, resulting in\nincorrect behaviors that may remain unnoticed. To address this challenge, we\nproposed MUTABOT, a mutation testing approach for injecting faults in\nconversations and producing faulty chatbots that emulate defects that may\naffect the conversational aspects. In this paper, we present our extension of\nMUTABOT to multiple platforms (Dialogflow and Rasa), and present experiments\nthat show how mutation testing can be used to reveal weaknesses in test suites\ngenerated by the Botium state-of-the-art test generator."}
{"id": "2509.00647", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00647", "abs": "https://arxiv.org/abs/2509.00647", "authors": ["Yu-Zheng Lin", "Sujan Ghimire", "Abhiram Nandimandalam", "Jonah Michael Camacho", "Unnati Tripathi", "Rony Macwan", "Sicong Shao", "Setareh Rafatirad", "Rozhin Yasaei", "Pratik Satam", "Soheil Salehi"], "title": "LLM-HyPZ: Hardware Vulnerability Discovery using an LLM-Assisted Hybrid Platform for Zero-Shot Knowledge Extraction and Refinement", "comment": "10 pages, 6 figures", "summary": "The rapid growth of hardware vulnerabilities has created an urgent need for\nsystematic and scalable analysis methods. Unlike software flaws, which are\noften patchable post-deployment, hardware weaknesses remain embedded across\nproduct lifecycles, posing persistent risks to processors, embedded devices,\nand IoT platforms. Existing efforts such as the MITRE CWE Hardware List (2021)\nrelied on expert-driven Delphi surveys, which lack statistical rigor and\nintroduce subjective bias, while large-scale data-driven foundations for\nhardware weaknesses have been largely absent. In this work, we propose\nLLM-HyPZ, an LLM-assisted hybrid framework for zero-shot knowledge extraction\nand refinement from vulnerability corpora. Our approach integrates zero-shot\nLLM classification, contextualized embeddings, unsupervised clustering, and\nprompt-driven summarization to mine hardware-related CVEs at scale. Applying\nLLM-HyPZ to the 2021-2024 CVE corpus (114,836 entries), we identified 1,742\nhardware-related vulnerabilities. We distilled them into five recurring themes,\nincluding privilege escalation via firmware and BIOS, memory corruption in\nmobile and IoT systems, and physical access exploits. Benchmarking across seven\nLLMs shows that LLaMA 3.3 70B achieves near-perfect classification accuracy\n(99.5%) on a curated validation set. Beyond methodological contributions, our\nframework directly supported the MITRE CWE Most Important Hardware Weaknesses\n(MIHW) 2025 update by narrowing the candidate search space. Specifically, our\npipeline surfaced 411 of the 1,026 CVEs used for downstream MIHW analysis,\nthereby reducing expert workload and accelerating evidence gathering. These\nresults establish LLM-HyPZ as the first data-driven, scalable approach for\nsystematically discovering hardware vulnerabilities, thereby bridging the gap\nbetween expert knowledge and real-world vulnerability evidence."}
{"id": "2509.01445", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01445", "abs": "https://arxiv.org/abs/2509.01445", "authors": ["Muhammad Ovais Ahmad", "Tomas Gustavsson"], "title": "Non Technical Debt in Agile Software Development", "comment": null, "summary": "NonTechnical Debt (NTD) is a common challenge in agile software development,\nmanifesting in four critical forms, Process Debt, Social Debt, People Debt,\nOrganizational debt. NODLA project is a collaboration between Karlstad\nUniversity and four leading Swedish industrial partners, reveals how various\ndebt types disrupt large scale Agile Software Development (ASD) environments.\nThrough extensive surveys, indepth interviews, and statistical analyses\ninvolving a diverse group of software professionals, we identified key drivers\nof NTD and their impacts. Our findings emphasize (1) Well structured, highly\ncohesive teams learn faster, adapt more effectively, and innovate consistently.\n(2) Psychological safety, fostered by proactive leadership, is essential for\ninnovation, experimentation, and keeping employees. (3) Inefficient processes\nand unclear roles contribute significantly to drops in job satisfaction,\nproductivity and team morale. (4) Social fragmentation, particularly in remote\nand hybrid settings, breeds rework, delays, and increased costs. (5) Neglected\nhuman resource needs, such as delayed hiring or insufficient training, limit an\norganization ability to meet growing demands. This white paper distils these\ninsights into practical, evidence based strategies, such as refining team\ncomposition, clarifying roles, fostering psychological safety, streamlining\nworkflows, and embracing failure as a learning tool. By implementing these\nstrategies, organizations can reduce NTD, reclaim agility, and unlock their\nteams full potential."}
{"id": "2509.00662", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.00662", "abs": "https://arxiv.org/abs/2509.00662", "authors": ["Vamsi Shankar Simhadri", "Yichang Xiong", "Habiba Farrukh", "Xiaokuan Zhang"], "title": "Virtual Reality, Real Problems: A Longitudinal Security Analysis of VR Firmware", "comment": "17 pages, 25 figures, 6 tables, To appear on ACM CCS 2025", "summary": "Virtual Reality (VR) technology is rapidly growing in recent years. VR\ndevices such as Meta Quest 3 utilize numerous sensors to collect users' data to\nprovide an immersive experience. Due to the extensive data collection and the\nimmersive nature, the security of VR devices is paramount. Leading VR devices\noften adopt and customize Android systems, which makes them susceptible to both\nAndroid-based vulnerabilities and new issues introduced by VR-specific\ncustomizations (e.g., system services to support continuous head and hand\ntracking). While prior work has extensively examined the security properties of\nthe Android software stack, how these security properties hold for VR systems\nremains unexplored. In this paper, we present the first comprehensive security\nanalysis of VR firmware. We collect over 300 versions of VR firmware from two\nmajor vendors, Quest and Pico, and perform a longitudinal analysis across the\nkernel layer, the system binary and library layer, and the application layer.\nWe have identified several security issues in these VR firmware, including\nmissing kernel-level security features, insufficient binary hardening,\ninconsistent permission enforcement, and inadequate SELinux policy enforcement.\nBased on our findings, we synthesize recommendations for VR vendors to improve\nsecurity and trust for VR devices. This paper will act as an important security\nresource for VR developers, users, and vendors, and will also direct future\nadvancements in secure VR ecosystem."}
{"id": "2509.01494", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01494", "abs": "https://arxiv.org/abs/2509.01494", "authors": ["Zhengran Zeng", "Ruikai Shi", "Keke Han", "Yixin Li", "Kaicheng Sun", "Yidong Wang", "Zhuohao Yu", "Rui Xie", "Wei Ye", "Shikun Zhang"], "title": "Benchmarking and Studying the LLM-based Code Review", "comment": null, "summary": "Automated Code Review (ACR) is crucial for software quality, yet existing\nbenchmarks often fail to reflect real-world complexities, hindering the\nevaluation of modern Large Language Models (LLMs). Current benchmarks\nfrequently focus on fine-grained code units, lack complete project context, and\nuse inadequate evaluation metrics. To address these limitations, we introduce\nSWRBench , a new benchmark comprising 1000 manually verified Pull Requests\n(PRs) from GitHub, offering PR-centric review with full project context.\nSWRBench employs an objective LLM-based evaluation method that aligns strongly\nwith human judgment (~90 agreement) by verifying if issues from a structured\nground truth are covered in generated reviews. Our systematic evaluation of\nmainstream ACR tools and LLMs on SWRBench reveals that current systems\nunderperform, and ACR tools are more adept at detecting functional errors.\nSubsequently, we propose and validate a simple multi-review aggregation\nstrategy that significantly boosts ACR performance, increasing F1 scores by up\nto 43.67%. Our contributions include the SWRBench benchmark, its objective\nevaluation method, a comprehensive study of current ACR capabilities, and an\neffective enhancement approach, offering valuable insights for advancing ACR\nresearch."}
{"id": "2509.00706", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.00706", "abs": "https://arxiv.org/abs/2509.00706", "authors": ["YuKun Zhu", "ManYuan Hua", "Hai Huang", "YongZhao Zhang", "Jie Yang", "FengHua Xu", "RuiDong Chen", "XiaoSong Zhang", "JiGuo Yu", "Yong Ma"], "title": "X-PRINT:Platform-Agnostic and Scalable Fine-Grained Encrypted Traffic Fingerprinting", "comment": null, "summary": "Although encryption protocols such as TLS are widely de-ployed,side-channel\nmetadata in encrypted traffic still reveals patterns that allow application and\nbehavior inference.How-ever,existing fine-grained fingerprinting approaches\nface two key limitations:(i)reliance on platform-dependent\ncharac-teristics,which restricts generalization across heterogeneous\nplatforms,and(ii)poor scalability for fine-grained behavior identification in\nopen-world settings.\n  In this paper,we present X-PRINT,the first server-centric,URI-based framework\nfor cross-platform fine-grained encrypted-traffic fingerprinting.X-PRINT\nsystematically demonstrates that backend URI invocation patterns can serve as\nplatform-agnostic invariants and are effective for mod-eling fine-grained\nbehaviors.To achieve robust identifica-tion,X-PRINT further leverages\ntemporally structured URI maps for behavior inference and emphasizes the\nexclusion of platform-or application-specific private URIs to handle unseen\ncases,thereby improving reliability in open-world and cross-platform\nsettings.Extensive experiments across diverse cross-platform and open-world\nsettings show that X-PRINT achieves state-of-the-art accuracy in fine-grained\nfingerprint-ing and exhibits strong scalability and robustness."}
{"id": "2509.01527", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01527", "abs": "https://arxiv.org/abs/2509.01527", "authors": ["Amirreza Nayyeri", "Abbas Rasoolzadegan"], "title": "A Privacy-Preserving Recommender for Filling Web Forms Using a Local Large Language Model", "comment": null, "summary": "Web applications are increasingly used in critical domains such as education,\nfinance, and e-commerce. This highlights the need to ensure their failure-free\nperformance. One effective method for evaluating failure-free performance is\nweb form testing, where defining effective test scenarios is key to a complete\nand accurate evaluation. A core aspect of this process involves filling form\nfields with suitable values to create effective test cases. However, manually\ngenerating these values is time-consuming and prone to errors. To address this,\nvarious tools have been developed to assist testers. With the appearance of\nlarge language models (LLMs), a new generation of tools seeks to handle this\ntask more intelligently. Although many LLM-based tools have been introduced, as\nthese models typically rely on cloud infrastructure, their use in testing\nconfidential web forms raises concerns about unintended data leakage and\nbreaches of confidentiality. This paper introduces a privacy-preserving\nrecommender that operates locally using a large language model. The tool\nassists testers in web form testing by suggesting effective field values. This\ntool analyzes the HTML structure of forms, detects input types, and extracts\nconstraints based on each field's type and contextual content, guiding proper\nfield filling."}
{"id": "2509.00770", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.00770", "abs": "https://arxiv.org/abs/2509.00770", "authors": ["Shaofei Huang", "Christopher M. Poskitt", "Lwin Khin Shar"], "title": "Bayesian and Multi-Objective Decision Support for Real-Time Cyber-Physical Incident Mitigation", "comment": null, "summary": "This research proposes a real-time, adaptive decision-support framework for\nmitigating cyber incidents in cyber-physical systems, developed in response to\nan increasing reliance on these systems within critical infrastructure and\nevolving adversarial tactics. Existing decision-support systems often fall\nshort in accounting for multi-agent, multi-path attacks and trade-offs between\nsafety and operational continuity. To address this, our framework integrates\nhierarchical system modelling with Bayesian probabilistic reasoning,\nconstructing Bayesian Network Graphs from system architecture and vulnerability\ndata. Models are encoded using a Domain Specific Language to enhance\ncomputational efficiency and support dynamic updates. In our approach, we use a\nhybrid exposure probability estimation framework, which combines Exploit\nPrediction Scoring System and Common Vulnerability Scoring System scores via\nBayesian confidence calibration to handle epistemic uncertainty caused by\nincomplete or heterogeneous vulnerability metadata. Mitigation recommendations\nare generated as countermeasure portfolios, refined using multi-objective\noptimisation to identify Pareto-optimal strategies balancing attack likelihood,\nimpact severity, and system availability. To accommodate time- and\nresource-constrained incident response, frequency-based heuristics are applied\nto prioritise countermeasures across the optimised portfolios. The framework\nwas evaluated through three representative cyber-physical attack scenarios,\ndemonstrating its versatility in handling complex adversarial behaviours under\nreal-time response constraints. The results affirm its utility in operational\ncontexts and highlight the robustness of our proposed approach across diverse\nthreat environments."}
{"id": "2509.01612", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01612", "abs": "https://arxiv.org/abs/2509.01612", "authors": ["Omur Sahin", "Man Zhang", "Andrea Arcuri"], "title": "WFC/WFD: Web Fuzzing Commons, Dataset and Guidelines to Support Experimentation in REST API Fuzzing", "comment": null, "summary": "Fuzzing REST APIs is an important research problem, with practical\napplications and impact in industry. As such, a lot of research work has been\ncarried out on this topic in the last few years. However, there are three major\nissues that hinder further progress: how to deal with API authentication; how\nto catalog and compare different fault types found by different fuzzers; and\nwhat to use as case study to facilitate fair comparisons among fuzzers. To\naddress these important challenges, we present Web Fuzzing Commons (WFC) and\nWeb Fuzzing Dataset (WFD). WFC is a set of open-source libraries and schema\ndefinitions to declaratively specify authentication info and catalog different\ntypes of faults that fuzzers can automatically detect. WFD is a collection of\n36 open-source APIs with all necessary scaffolding to easily run experiments\nwith fuzzers, supported by WFC. To show the usefulness of WFC/WFD, a set of\nexperiments is carried out with EvoMaster, a state-of-the-art fuzzer for Web\nAPIs. However, any fuzzer can benefit from WFC and WFD. We compare EvoMaster\nwith other state-of-the-art tools such as ARAT-RL, EmRest, LLamaRestTest,\nRESTler, and Schemathesis. We discuss common pitfalls in tool comparisons, as\nwell as providing guidelines with support of WFC/WFD to avoid them."}
{"id": "2509.00811", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.00811", "abs": "https://arxiv.org/abs/2509.00811", "authors": ["Samuel Punch", "Krishnendu Guha"], "title": "MAESTROCUT: Dynamic, Noise-Adaptive, and Secure Quantum Circuit Cutting on Near-Term Hardware", "comment": "14 Pages", "summary": "We present MaestroCut, a closed-loop framework for quantum circuit cutting\nthat adapts partitioning and shot allocation to device drift and workload\nvariation. MaestroCut tracks a variance proxy in real time, triggers re-cutting\nwhen accuracy degrades, and routes shots using topology-aware priors. An online\nestimator cascade (MLE, Bayesian, GP-assisted) selects the lowest-error\nreconstruction within a fixed budget. Tier-1 simulations show consistent\nvariance contraction and reduced mean-squared error versus uniform and\nproportional baselines. Tier-2 emulation with realistic queueing and noise\ndemonstrates stable latency targets, high reliability, and ~1% software\noverhead under stress scenarios. These results indicate that adaptive circuit\ncutting can provide accuracy and efficiency improvements with minimal\noperational cost on near-term hardware."}
{"id": "2509.01616", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2509.01616", "abs": "https://arxiv.org/abs/2509.01616", "authors": ["Konstantinos Kitsios", "Marco Castelluccio", "Alberto Bacchelli"], "title": "Automated Generation of Issue-Reproducing Tests by Combining LLMs and Search-Based Testing", "comment": "13 pages, 8 figures, accepted for publication (to appear) in the 40th\n  IEEE/ACM International Conference on Automated Software Engineering, ASE 2025", "summary": "Issue-reproducing tests fail on buggy code and pass once a patch is applied,\nthus increasing developers' confidence that the issue has been resolved and\nwill not be re-introduced. However, past research has shown that developers\noften commit patches without such tests, making the automated generation of\nissue-reproducing tests an area of interest. We propose BLAST, a tool for\nautomatically generating issue-reproducing tests from issue-patch pairs by\ncombining LLMs and search-based software testing (SBST). For the LLM part, we\ncomplement the issue description and the patch by extracting relevant context\nthrough git history analysis, static analysis, and SBST-generated tests. For\nthe SBST part, we adapt SBST for generating issue-reproducing tests; the issue\ndescription and the patch are fed into the SBST optimization through an\nintermediate LLM-generated seed, which we deserialize into SBST-compatible\nform. BLAST successfully generates issue-reproducing tests for 151/426 (35.4%)\nof the issues from a curated Python benchmark, outperforming the\nstate-of-the-art (23.5%). Additionally, to measure the real-world impact of\nBLAST, we built a GitHub bot that runs BLAST whenever a new pull request (PR)\nlinked to an issue is opened, and if BLAST generates an issue-reproducing test,\nthe bot proposes it as a comment in the PR. We deployed the bot in three\nopen-source repositories for three months, gathering data from 32 PRs-issue\npairs. BLAST generated an issue-reproducing test in 11 of these cases, which we\nproposed to the developers. By analyzing the developers' feedback, we discuss\nchallenges and opportunities for researchers and tool builders. Data and\nmaterial: https://doi.org/10.5281/zenodo.16949042"}
{"id": "2509.00812", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.00812", "abs": "https://arxiv.org/abs/2509.00812", "authors": ["Samuel Punch", "Krishnendu Guha"], "title": "Adaptive t Design Dummy-Gate Obfuscation for Cryogenic Scale Enforcement", "comment": "6", "summary": "Cloud quantum services can reveal circuit structure and timing through\nscheduler metadata, latency patterns, and co-tenant interference. We introduce\nNADGO (Noise-Adaptive Dummy-Gate Obfuscation), a scheduling and obfuscation\nstack that enforces operational privacy for gate-model workloads by applying\nper-interval limits on observable information leakage. To support\nconfidentiality and fair multi-tenancy, operators require a method to audit\ncompliance at acceptable overheads. NADGO combines: (i) hardware-aware t-design\npadding for structured cover traffic, (ii) particle-filter timing randomization\nto mask queue patterns, (iii) CASQUE subcircuit routing across heterogeneous\nbackends, and (iv) a per-interval leakage estimator with locked calibration\nartifacts and a dual-threshold kill-switch. We prototype the approach on a\n4-qubit superconducting tile with cryo-CMOS control and evaluate both\ndepth-varied local-random circuits and small QAOA instances. Monitoring runs at\na 6.3 microsecond control interval, and per-interval decisions are recorded in\nan append-only, hash-chained audit log. Across Monte Carlo (Tier 1) and\ncloud-hardware emulation (Tier 2) evaluations, NADGO maintains leakage within\nbudget in nominal operation (interval-abort rate below 1 percent) and under\nattack yields high separation with concentrated aborts. At matched leakage\ntargets, microbenchmarks indicate lower latency and cryogenic power consumption\nthan static padding, while end-to-end workloads maintain competitive cost\nenvelopes."}
{"id": "2509.01946", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01946", "abs": "https://arxiv.org/abs/2509.01946", "authors": ["Aarsh Shah", "Cleyton Magalhaes", "Kiev Gama", "Ronnie de Souza Santos"], "title": "Tether: A Personalized Support Assistant for Software Engineers with ADHD", "comment": null, "summary": "Equity, diversity, and inclusion in software engineering often overlook\nneurodiversity, particularly the experiences of developers with Attention\nDeficit Hyperactivity Disorder (ADHD). Despite the growing awareness about that\npopulation in SE, few tools are designed to support their cognitive challenges\n(e.g., sustained attention, task initiation, self-regulation) within\ndevelopment workflows. We present Tether, an LLM-powered desktop application\ndesigned to support software engineers with ADHD by delivering adaptive,\ncontext-aware assistance. Drawing from engineering research methodology, Tether\ncombines local activity monitoring, retrieval-augmented generation (RAG), and\ngamification to offer real-time focus support and personalized dialogue. The\nsystem integrates operating system level system tracking to prompt engagement\nand its chatbot leverages ADHD-specific resources to offer relevant responses.\nPreliminary validation through self-use revealed improved contextual accuracy\nfollowing iterative prompt refinements and RAG enhancements. Tether\ndifferentiates itself from generic tools by being adaptable and aligned with\nsoftware-specific workflows and ADHD-related challenges. While not yet\nevaluated by target users, this work lays the foundation for future\nneurodiversity-aware tools in SE and highlights the potential of LLMs as\npersonalized support systems for underrepresented cognitive needs."}
{"id": "2509.00820", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.00820", "abs": "https://arxiv.org/abs/2509.00820", "authors": ["Zhenhua Xu", "Zhaokun Yan", "Binhan Xu", "Xin Tong", "Haitao Xu", "Yourong Chen", "Meng Han"], "title": "Unlocking the Effectiveness of LoRA-FP for Seamless Transfer Implantation of Fingerprints in Downstream Models", "comment": "Accepted By EMNLP2025", "summary": "With the rapid advancement of large language models (LLMs), safeguarding\nintellectual property (IP) has become increasingly critical. To address the\nchallenges of high costs and potential contamination in fingerprint\nintegration, we propose LoRA-FP, a lightweight, plug-and-play framework that\nembeds backdoor fingerprints into LoRA adapters through constrained\nfine-tuning. This design enables seamless fingerprint transplantation via\nparameter fusion, eliminating the need for full-parameter updates while\npreserving model integrity. Experimental results demonstrate that LoRA-FP not\nonly significantly reduces computational overhead compared to conventional\napproaches but also achieves superior robustness across diverse scenarios,\nincluding incremental training and model fusion. Our code and datasets are\npublicly available at https://github.com/Xuzhenhua55/LoRA-FP."}
{"id": "2509.01947", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01947", "abs": "https://arxiv.org/abs/2509.01947", "authors": ["Mahdi Farzandway", "Fatemeh Ghassemi"], "title": "Automated Repair of C Programs Using Large Language Models", "comment": null, "summary": "This study explores the potential of Large Language Models (LLMs) in\nautomating the repair of C programs. We present a framework that integrates\nspectrum-based fault localization (SBFL), runtime feedback, and\nChain-of-Thought-structured prompting into an autonomous repair loop. Unlike\nprior approaches, our method explicitly combines statistical program analysis\nwith LLM reasoning. The iterative repair cycle leverages a structured\nChain-of-Thought (CoT) prompting approach, where the model reasons over failing\ntests, suspicious code regions, and prior patch outcomes, before generating new\ncandidate patches. The model iteratively changes the code, evaluates the\nresults, and incorporates reasoning from previous attempts into subsequent\nmodifications, reducing repeated errors and clarifying why some bugs remain\nunresolved. Our evaluation spans 3,902 bugs from the Codeflaws benchmark, where\nour approach achieves 44.93% repair accuracy, representing a 3.61% absolute\nimprovement over strong state-of-the-art APR baselines such as GPT-4 with CoT.\nThis outcome highlights a practical pathway toward integrating statistical\nprogram analysis with generative AI in automated debugging."}
{"id": "2509.00882", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.00882", "abs": "https://arxiv.org/abs/2509.00882", "authors": ["Xiang Li", "Yueci Su", "Jiahao Liu", "Zhiwei Lin", "Yuebing Hou", "Peiming Gao", "Yuanchao Zhang"], "title": "VULSOVER: Vulnerability Detection via LLM-Driven Constraint Solving", "comment": null, "summary": "Traditional vulnerability detection methods rely heavily on predefined rule\nmatching, which often fails to capture vulnerabilities accurately. With the\nrise of large language models (LLMs), leveraging their ability to understand\ncode semantics has emerged as a promising direction for achieving more accurate\nand efficient vulnerability detection. However, current LLM-based approaches\nface significant challenges: instability in model outputs, limitations in\ncontext length, and hallucination. As a result, many existing solutions either\nuse LLMs merely to enrich predefined rule sets, thereby keeping the detection\nprocess fundamentally rule-based, or over-rely on them, leading to poor\nrobustness. To address these challenges, we propose a constraint-solving\napproach powered by LLMs named VULSOLVER. By modeling vulnerability detection\nas a constraint-solving problem, and by integrating static application security\ntesting (SAST) with the semantic reasoning capabilities of LLMs, our method\nenables the LLM to act like a professional human security expert. We assess\nVULSOLVER on the OWASP Benchmark (1,023 labeled samples), achieving 96.29%\naccuracy, 96.55% F1-score, and 100% recall. Applied to popular GitHub\nrepositories, VULSOLVER also identified 15 previously unknown high-severity\nvulnerabilities (CVSS 7.5-9.8), demonstrating its effectiveness in real-world\nsecurity analysis."}
{"id": "2509.02012", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.02012", "abs": "https://arxiv.org/abs/2509.02012", "authors": ["Katrine Christensen", "Mahsa Varshosaz", "Ra√∫l Pardo"], "title": "ProbTest: Unit Testing for Probabilistic Programs (Extended Version)", "comment": "Pre-print of paper to appear in the proceedings of the 23nd edition\n  of the International Conference on Software Engineering and Formal Methods\n  (SEFM'25)", "summary": "Testing probabilistic programs is non-trivial due to their stochastic nature.\nGiven an input, the program may produce different outcomes depending on the\nunderlying stochastic choices in the program. This means testing the expected\noutcomes of probabilistic programs requires repeated test executions unlike\ndeterministic programs where a single execution may suffice for each test\ninput. This raises the following question: how many times should we run a\nprobabilistic program to effectively test it? This work proposes a novel\nblack-box unit testing method, ProbTest, for testing the outcomes of\nprobabilistic programs. Our method is founded on the theory surrounding a\nwell-known combinatorial problem, the coupon collector's problem. Using this\nmethod, developers can write unit tests as usual without extra effort while the\nnumber of required test executions is determined automatically with statistical\nguarantees for the results. We implement ProbTest as a plug-in for PyTest, a\nwell-known unit testing tool for python programs. Using this plug-in,\ndevelopers can write unit tests similar to any other Python program and the\nnecessary test executions are handled automatically. We evaluate the method on\ncase studies from the Gymnasium reinforcement learning library and a randomized\ndata structure."}
{"id": "2509.00896", "categories": ["cs.CR", "cs.SY", "eess.SY", "D.2.0"], "pdf": "https://arxiv.org/pdf/2509.00896", "abs": "https://arxiv.org/abs/2509.00896", "authors": ["Maryam Mahdi Alhusseini", "Mohammad Reza Feizi Derakhshi"], "title": "Hybrid AI-Driven Intrusion Detection: Framework Leveraging Novel Feature Selection for Enhanced Network Security", "comment": "16 pages, 12 figures", "summary": "In today's rapidly evolving digital landscape, safeguarding network\ninfrastructures against cyberattacks has become a critical priority. This\nresearch presents an innovative AI-driven real-time intrusion detection\nframework designed to enhance network security, particularly in Wireless Sensor\nNetworks (WSNs) and Cloud Computing (CC) environments. The system employs\nclassical machine learning models, Logistic Regression, Decision Tree, and\nK-Nearest Neighbors, optimized through the novel Energy Valley Optimization\n(EVO) method using the NSL-KDD dataset. Feature selection significantly reduced\nthe number of input features from 42 to 18 while maintaining strong detection\ncapabilities. The proposed system achieved 98.95 percent accuracy with Decision\nTree, 98.47 percent with K-Nearest Neighbors, and 88.84 percent with Logistic\nRegression. Moreover, high precision, recall, and F1-scores were attained\nacross all classifiers while substantially reducing training and testing times,\nmaking the framework highly suitable for real-time applications. To ensure fair\ndetection across diverse attack types, dataset balancing via downsampling was\napplied to address class imbalance challenges. This investigation focuses on\nthe significance of advancing intrusion detection systems in cloud computing\nand WSNs. Overall, this work advances secure communications by delivering a\nscalable, low-latency, and high-accuracy intrusion detection solution aligned\nwith the latest trends in artificial intelligence, cybersecurity, and real-time\ndigital networks"}
{"id": "2509.02022", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.02022", "abs": "https://arxiv.org/abs/2509.02022", "authors": ["Bj√∏rnar Haugstad J√•tten", "Simon Boye J√∏rgensen", "Rasmus Petersen", "Ra√∫l Pardo"], "title": "Scalable Thread-Safety Analysis of Java Classes with CodeQL", "comment": null, "summary": "In object-oriented languages software developers rely on thread-safe classes\nto implement concurrent applications. However, determining whether a class is\nthread-safe is a challenging task. This paper presents a highly scalable method\nto analyze thread-safety in Java classes. We provide a definition of\nthread-safety for Java classes founded on the correctness principle of the Java\nmemory model, data race freedom. We devise a set of properties for Java classes\nthat are proven to ensure thread-safety. We encode these properties in the\nstatic analysis tool CodeQL to automatically analyze Java source code. We\nperform an evaluation on the top 1000 GitHub repositories. The evaluation\ncomprises 3632865 Java classes; with 1992 classes annotated as @ThreadSafe from\n71 repositories. These repositories include highly popular software such as\nApache Flink (24.6k stars), Facebook Fresco (17.1k stars), PrestoDB (16.2k\nstarts), and gRPC (11.6k starts). Our queries detected thousands of\nthread-safety errors. The running time of our queries is below 2 minutes for\nrepositories up to 200k lines of code, 20k methods, 6000 fields, and 1200\nclasses. We have submitted a selection of detected concurrency errors as PRs,\nand developers positively reacted to these PRs. We have submitted our CodeQL\nqueries to the main CodeQL repository, and they are currently in the process of\nbecoming available as part of GitHub actions. The results demonstrate the\napplicability and scalability of our method to analyze thread-safety in\nreal-world code bases."}
{"id": "2509.00918", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.00918", "abs": "https://arxiv.org/abs/2509.00918", "authors": ["Xubin Yue", "Zhenhua Xu", "Wenpeng Xing", "Jiahui Yu", "Mohan Li", "Meng Han"], "title": "PREE: Towards Harmless and Adaptive Fingerprint Editing in Large Language Models via Knowledge Prefix Enhancement", "comment": null, "summary": "Addressing the intellectual property protection challenges in commercial\ndeployment of large language models (LLMs), existing black-box fingerprinting\ntechniques face dual challenges from incremental fine-tuning erasure and\nfeature-space defense due to their reliance on overfitting high-perplexity\ntrigger patterns. Recent work has revealed that model editing in the\nfingerprinting domain offers distinct advantages, including significantly lower\nfalse positive rates, enhanced harmlessness, and superior robustness. Building\non this foundation, this paper innovatively proposes a\n$\\textbf{Pr}$efix-$\\textbf{e}$nhanced Fingerprint $\\textbf{E}$diting Framework\n(PREE), which encodes copyright information into parameter offsets through\ndual-channel knowledge edit to achieve covert embedding of fingerprint\nfeatures. Experimental results demonstrate that the proposed solution achieves\nthe 90\\% trigger precision in mainstream architectures including LLaMA-3 and\nQwen-2.5. The minimal parameter offset (change rate < 0.03) effectively\npreserves original knowledge representation while demonstrating strong\nrobustness against incremental fine-tuning and multi-dimensional defense\nstrategies, maintaining zero false positive rate throughout evaluations."}
{"id": "2509.02025", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.02025", "abs": "https://arxiv.org/abs/2509.02025", "authors": ["Junda He", "Zhou Yang", "Jieke Shi", "Chengran Yang", "Kisub Kim", "Bowen Xu", "Xin Zhou", "David Lo"], "title": "Curiosity-Driven Testing for Sequential Decision-Making Process", "comment": "Update the Replication Package URL", "summary": "Sequential decision-making processes (SDPs) are fundamental for complex\nreal-world challenges, such as autonomous driving, robotic control, and traffic\nmanagement. While recent advances in Deep Learning (DL) have led to mature\nsolutions for solving these complex problems, SDMs remain vulnerable to\nlearning unsafe behaviors, posing significant risks in safety-critical\napplications. However, developing a testing framework for SDMs that can\nidentify a diverse set of crash-triggering scenarios remains an open challenge.\nTo address this, we propose CureFuzz, a novel curiosity-driven black-box fuzz\ntesting approach for SDMs. CureFuzz proposes a curiosity mechanism that allows\na fuzzer to effectively explore novel and diverse scenarios, leading to\nimproved detection of crashtriggering scenarios. Additionally, we introduce a\nmulti-objective seed selection technique to balance the exploration of novel\nscenarios and the generation of crash-triggering scenarios, thereby optimizing\nthe fuzzing process. We evaluate CureFuzz on various SDMs and experimental\nresults demonstrate that CureFuzz outperforms the state-of-the-art method by a\nsubstantial margin in the total number of faults and distinct types of\ncrash-triggering scenarios. We also demonstrate that the crash-triggering\nscenarios found by CureFuzz can repair SDMs, highlighting CureFuzz as a\nvaluable tool for testing SDMs and optimizing their performance."}
{"id": "2509.00973", "categories": ["cs.CR", "cs.AI", "68T05, 68Q32, 94A60,", "I.2.6; I.2.3; I.2.0; D.4.6"], "pdf": "https://arxiv.org/pdf/2509.00973", "abs": "https://arxiv.org/abs/2509.00973", "authors": ["Kanchon Gharami", "Hansaka Aluvihare", "Shafika Showkat Moni", "Berker Pek√∂z"], "title": "Clone What You Can't Steal: Black-Box LLM Replication via Logit Leakage and Distillation", "comment": "8 pages. Accepted for publication in the proceedings of 7th IEEE\n  International Conference on Trust, Privacy and Security in Intelligent\n  Systems, and Applications (IEEE TPS 2025)", "summary": "Large Language Models (LLMs) are increasingly deployed in mission-critical\nsystems, facilitating tasks such as satellite operations, command-and-control,\nmilitary decision support, and cyber defense. Many of these systems are\naccessed through application programming interfaces (APIs). When such APIs lack\nrobust access controls, they can expose full or top-k logits, creating a\nsignificant and often overlooked attack surface. Prior art has mainly focused\non reconstructing the output projection layer or distilling surface-level\nbehaviors. However, regenerating a black-box model under tight query\nconstraints remains underexplored. We address that gap by introducing a\nconstrained replication pipeline that transforms partial logit leakage into a\nfunctional deployable substitute model clone. Our two-stage approach (i)\nreconstructs the output projection matrix by collecting top-k logits from under\n10k black-box queries via singular value decomposition (SVD) over the logits,\nthen (ii) distills the remaining architecture into compact student models with\nvarying transformer depths, trained on an open source dataset. A 6-layer\nstudent recreates 97.6% of the 6-layer teacher model's hidden-state geometry,\nwith only a 7.31% perplexity increase, and a 7.58 Negative Log-Likelihood\n(NLL). A 4-layer variant achieves 17.1% faster inference and 18.1% parameter\nreduction with comparable performance. The entire attack completes in under 24\ngraphics processing unit (GPU) hours and avoids triggering API rate-limit\ndefenses. These results demonstrate how quickly a cost-limited adversary can\nclone an LLM, underscoring the urgent need for hardened inference APIs and\nsecure on-premise defense deployments."}
{"id": "2509.02150", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.02150", "abs": "https://arxiv.org/abs/2509.02150", "authors": ["Pin Ji", "Yang Feng", "Zongtai Li", "Xiangchi Zhou", "Jia Liu", "Jun Sun", "Zhihong Zhao"], "title": "Txt2Sce: Scenario Generation for Autonomous Driving System Testing Based on Textual Reports", "comment": null, "summary": "With the rapid advancement of deep learning and related technologies,\nAutonomous Driving Systems (ADSs) have made significant progress and are\ngradually being widely applied in safety-critical fields. However, numerous\naccident reports show that ADSs still encounter challenges in complex\nscenarios. As a result, scenario-based testing has become essential for\nidentifying defects and ensuring reliable performance. In particular,\nreal-world accident reports offer valuable high-risk scenarios for more\ntargeted ADS testing. Despite their potential, existing methods often rely on\nvisual data, which demands large memory and manual annotation. Additionally,\nsince existing methods do not adopt standardized scenario formats (e.g.,\nOpenSCENARIO), the generated scenarios are often tied to specific platforms and\nADS implementations, limiting their scalability and portability. To address\nthese challenges, we propose Txt2Sce, a method for generating test scenarios in\nOpenSCENARIO format based on textual accident reports. Txt2Sce first uses a LLM\nto convert textual accident reports into corresponding OpenSCENARIO scenario\nfiles. It then generates a derivation-based scenario file tree through scenario\ndisassembly, scenario block mutation, and scenario assembly. By utilizing the\nderivation relationships between nodes in the scenario tree, Txt2Sce helps\ndevelopers identify the scenario conditions that trigger unexpected behaviors\nof ADSs. In the experiments, we employ Txt2Sce to generate 33 scenario file\ntrees, resulting in a total of 4,373 scenario files for testing the open-source\nADS, Autoware. The experimental results show that Txt2Sce successfully converts\ntextual reports into valid OpenSCENARIO files, enhances scenario diversity\nthrough mutation, and effectively detects unexpected behaviors of Autoware in\nterms of safety, smartness, and smoothness."}
{"id": "2509.01046", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.01046", "abs": "https://arxiv.org/abs/2509.01046", "authors": ["Khashayar Khajavi", "Tao Wang"], "title": "Lightening the Load: A Cluster-Based Framework for A Lower-Overhead, Provable Website Fingerprinting Defense", "comment": null, "summary": "Website fingerprinting (WF) attacks remain a significant threat to encrypted\ntraffic, prompting the development of a wide range of defenses. Among these,\ntwo prominent classes are regularization-based defenses, which shape traffic\nusing fixed padding rules, and supersequence-based approaches, which conceal\ntraces among predefined patterns. In this work, we present a unified framework\nfor designing an adaptive WF defense that combines the effectiveness of\nregularization with the provable security of supersequence-style grouping. The\nscheme first extracts behavioural patterns from traces and clusters them into\n(k,l)-diverse anonymity sets; an early-time-series classifier (adapted from\nECDIRE) then switches from a conservative global set of regularization\nparameters to the lighter, set-specific parameters. We instantiate the design\nas Adaptive Tamaraw, a variant of Tamaraw that assigns padding parameters on a\nper-cluster basis while retaining its original information-theoretic guarantee.\nComprehensive experiments on public real-world datasets confirm the benefits.\nBy tuning k, operators can trade privacy for efficiency: in its high-privacy\nmode Adaptive Tamaraw pushes the bound on any attacker's accuracy below 30%,\nwhereas in efficiency-centred settings it cuts total overhead by 99% compared\nwith classic Tamaraw."}
{"id": "2509.02221", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.02221", "abs": "https://arxiv.org/abs/2509.02221", "authors": ["Martin Skoglund", "Fredrik Warg", "Anders Thors√©n", "Sasikumar Punnekkat", "Hans Hansson"], "title": "Formalizing Operational Design Domains with the Pkl Language", "comment": "8 pages, 9 figures, IV 2025", "summary": "The deployment of automated functions that can operate without direct human\nsupervision has changed safety evaluation in domains seeking higher levels of\nautomation. Unlike conventional systems that rely on human operators, these\nfunctions require new assessment frameworks to demonstrate that they do not\nintroduce unacceptable risks under real-world conditions. To make a convincing\nsafety claim, the developer must present a thorough justification argument,\nsupported by evidence, that a function is free from unreasonable risk when\noperated in its intended context. The key concept relevant to the presented\nwork is the intended context, often captured by an Operational Design Domain\nspecification (ODD). ODD formalization is challenging due to the need to\nmaintain flexibility in adopting diverse specification formats while preserving\nconsistency and traceability and integrating seamlessly into the development,\nvalidation, and assessment. This paper presents a way to formalize an ODD in\nthe Pkl language, addressing central challenges in specifying ODDs while\nimproving usability through specialized configuration language features. The\napproach is illustrated with an automotive example but can be broadly applied\nto ensure rigorous assessments of operational contexts."}
{"id": "2509.01178", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.01178", "abs": "https://arxiv.org/abs/2509.01178", "authors": ["Hao Guo", "Zhaoqian Liu", "Liqiang Peng", "Shuaishuai Li", "Ximing Fu", "Weiran Liu", "Lin Qu"], "title": "Efficient and High-Accuracy Secure Two-Party Protocols for a Class of Functions with Real-number Inputs", "comment": "17 pages, 3 figures", "summary": "In two-party secret sharing scheme, values are typically encoded as unsigned\nintegers $\\mathsf{uint}(x)$, whereas real-world applications often require\ncomputations on signed real numbers $\\mathsf{Real}(x)$. To enable secure\nevaluation of practical functions, it is essential to computing\n$\\mathsf{Real}(x)$ from shared inputs, as protocols take shares as input. At\nUSENIX'25, Guo et al. proposed an efficient method for computing signed integer\nvalues $\\mathsf{int}(x)$ from shares, which can be extended to compute\n$\\mathsf{Real}(x)$. However, their approach imposes a restrictive input\nconstraint $|x| < \\frac{L}{3}$ for $x \\in \\mathbb{Z}_L$, limiting its\napplicability in real-world scenarios. In this work, we significantly relax\nthis constraint to $|x| < B$ for any $B \\leq \\frac{L}{2}$, where $B =\n\\frac{L}{2}$ corresponding to the natural representable range in $x \\in\n\\mathbb{Z}_L$. This relaxes the restrictions and enables the computation of\n$\\mathsf{Real}(x)$ with loose or no input constraints. Building upon this\nfoundation, we present a generalized framework for designing secure protocols\nfor a broad class of functions, including integer division ($\\lfloor\n\\frac{x}{d} \\rfloor$), trigonometric ($\\sin(x)$) and exponential ($e^{-x}$)\nfunctions. Our experimental evaluation demonstrates that the proposed protocols\nachieve both high efficiency and high accuracy. Notably, our protocol for\nevaluating $e^{-x}$ reduces communication costs to approximately 31% of those\nin SirNN (S&P 21) and Bolt (S&P 24), with runtime speedups of up to $5.53\n\\times$ and $3.09 \\times$, respectively. In terms of accuracy, our protocol\nachieves a maximum ULP error of $1.435$, compared to $2.64$ for SirNN and\n$8.681$ for Bolt."}
{"id": "2509.02311", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.02311", "abs": "https://arxiv.org/abs/2509.02311", "authors": ["Martin Skoglund", "Fredrik Warg", "Anders Thoren", "Sasikumar Punnekkat", "Hans Hansson"], "title": "Methodology for Test Case Allocation based on a Formalized ODD", "comment": "12 pages, 8 figures, DECSoS, SAFECOMP 2025", "summary": "The emergence of Connected, Cooperative, and Automated Mobility (CCAM)\nsystems has significantly transformed the safety assessment landscape. Because\nthey integrate automated vehicle functions beyond those managed by a human\ndriver, new methods are required to evaluate their safety. Approaches that\ncompile evidence from multiple test environments have been proposed for\ntype-approval and similar evaluations, emphasizing scenario coverage within the\nsystems Operational Design Domain (ODD). However, aligning diverse test\nenvironment requirements with distinct testing capabilities remains\nchallenging. This paper presents a method for evaluating the suitability of\ntest case allocation to various test environments by drawing on and extending\nan existing ODD formalization with key testing attributes. The resulting\nconstruct integrates ODD parameters and additional test attributes to capture a\ngiven test environments relevant capabilities. This approach supports automatic\nsuitability evaluation and is demonstrated through a case study on an automated\nreversing truck function. The system's implementation fidelity is tied to ODD\nparameters, facilitating automated test case allocation based on each\nenvironments capacity for object-detection sensor assessment."}
{"id": "2509.01211", "categories": ["cs.CR", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.01211", "abs": "https://arxiv.org/abs/2509.01211", "authors": ["Dezhang Kong", "Hujin Peng", "Yilun Zhang", "Lele Zhao", "Zhenhua Xu", "Shi Lin", "Changting Lin", "Meng Han"], "title": "Web Fraud Attacks Against LLM-Driven Multi-Agent Systems", "comment": null, "summary": "With the proliferation of applications built upon LLM-driven multi-agent\nsystems (MAS), the security of Web links has become a critical concern in\nensuring system reliability. Once an agent is induced to visit a malicious\nwebsite, attackers can use it as a springboard to conduct diverse subsequent\nattacks, which will drastically expand the attack surface. In this paper, we\npropose Web Fraud Attacks, a novel type of attack aiming at inducing MAS to\nvisit malicious websites. We design 11 representative attack variants that\nencompass domain name tampering (homoglyph deception, character substitution,\netc.), link structure camouflage (sub-directory nesting, sub-domain grafting,\nparameter obfuscation, etc.), and other deceptive techniques tailored to\nexploit MAS's vulnerabilities in link validation. Through extensive experiments\non these crafted attack vectors, we demonstrate that Web fraud attacks not only\nexhibit significant destructive potential across different MAS architectures\nbut also possess a distinct advantage in evasion: they circumvent the need for\ncomplex input formats such as jailbreaking, which inherently carry higher\nexposure risks. These results underscore the importance of addressing Web fraud\nattacks in LLM-driven MAS, as their stealthiness and destructiveness pose\nnon-negligible threats to system security and user safety."}
{"id": "2509.02330", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02330", "abs": "https://arxiv.org/abs/2509.02330", "authors": ["Yicong Zhao", "Shisong Chen", "Jiacheng Zhang", "Zhixu Li"], "title": "ReCode: Improving LLM-based Code Repair with Fine-Grained Retrieval-Augmented Generation", "comment": "Accepted by CIKM 2025", "summary": "Recent advances in large language models (LLMs) have demonstrated impressive\ncapabilities in code-related tasks, such as code generation and automated\nprogram repair. Despite their promising performance, most existing approaches\nfor code repair suffer from high training costs or computationally expensive\ninference. Retrieval-augmented generation (RAG), with its efficient in-context\nlearning paradigm, offers a more scalable alternative. However, conventional\nretrieval strategies, which are often based on holistic code-text embeddings,\nfail to capture the structural intricacies of code, resulting in suboptimal\nretrieval quality. To address the above limitations, we propose ReCode, a\nfine-grained retrieval-augmented in-context learning framework designed for\naccurate and efficient code repair. Specifically, ReCode introduces two key\ninnovations: (1) an algorithm-aware retrieval strategy that narrows the search\nspace using preliminary algorithm type predictions; and (2) a modular\ndual-encoder architecture that separately processes code and textual inputs,\nenabling fine-grained semantic matching between input and retrieved contexts.\nFurthermore, we propose RACodeBench, a new benchmark constructed from\nreal-world user-submitted buggy code, which addresses the limitations of\nsynthetic benchmarks and supports realistic evaluation. Experimental results on\nRACodeBench and competitive programming datasets demonstrate that ReCode\nachieves higher repair accuracy with significantly reduced inference cost,\nhighlighting its practical value for real-world code repair scenarios."}
{"id": "2509.01253", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.01253", "abs": "https://arxiv.org/abs/2509.01253", "authors": ["Sayan Biswas", "Philippe Chartier", "Akash Dhasade", "Tom Jurien", "David Kerriou", "Anne-Marie Kerrmarec", "Mohammed Lemou", "Franklin Tranie", "Martijn de Vos", "Milos Vujasinovic"], "title": "Practical and Private Hybrid ML Inference with Fully Homomorphic Encryption", "comment": null, "summary": "In contemporary cloud-based services, protecting users' sensitive data and\nensuring the confidentiality of the server's model are critical. Fully\nhomomorphic encryption (FHE) enables inference directly on encrypted inputs,\nbut its practicality is hindered by expensive bootstrapping and inefficient\napproximations of non-linear activations. We introduce Safhire, a hybrid\ninference framework that executes linear layers under encryption on the server\nwhile offloading non-linearities to the client in plaintext. This design\neliminates bootstrapping, supports exact activations, and significantly reduces\ncomputation. To safeguard model confidentiality despite client access to\nintermediate outputs, Safhire applies randomized shuffling, which obfuscates\nintermediate values and makes it practically impossible to reconstruct the\nmodel. To further reduce latency, Safhire incorporates advanced optimizations\nsuch as fast ciphertext packing and partial extraction. Evaluations on multiple\nstandard models and datasets show that Safhire achieves 1.5X - 10.5X lower\ninference latency than Orion, a state-of-the-art baseline, with manageable\ncommunication overhead and comparable accuracy, thereby establishing the\npracticality of hybrid FHE inference."}
{"id": "2509.02372", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.02372", "abs": "https://arxiv.org/abs/2509.02372", "authors": ["Zhiyang Chen", "Tara Saba", "Xun Deng", "Xujie Si", "Fan Long"], "title": "Poisoned at Scale: A Scalable Audit Uncovers Hidden Scam Endpoints in Production LLMs", "comment": "10 pages, 4 figures", "summary": "Large Language Models (LLMs) have become critical to modern software\ndevelopment, but their reliance on internet datasets for training introduces a\nsignificant security risk: the absorption and reproduction of malicious\ncontent. To evaluate this threat, this paper introduces a scalable, automated\naudit framework that synthesizes innocuous, developer-style prompts from known\nscam databases to query production LLMs and determine if they generate code\ncontaining harmful URLs. We conducted a large-scale evaluation across four\nproduction LLMs (GPT-4o, GPT-4o-mini, Llama-4-Scout, and DeepSeek-V3), and\nfound a systemic vulnerability, with all tested models generating malicious\ncode at a non-negligible rate. On average, 4.2\\% of programs generated in our\nexperiments contained malicious URLs. Crucially, this malicious code is often\ngenerated in response to benign prompts. We manually validate the prompts which\ncause all four LLMs to generate malicious code, and resulting in 177 innocuous\nprompts that trigger all models to produce harmful outputs. These results\nprovide strong empirical evidence that the training data of production LLMs has\nbeen successfully poisoned at scale, underscoring the urgent need for more\nrobust defense mechanisms and post-generation safety checks to mitigate the\npropagation of hidden security threats."}
{"id": "2509.01271", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.01271", "abs": "https://arxiv.org/abs/2509.01271", "authors": ["Rujie Dai", "Peizhuo Lv", "Yujiang Gui", "Qiujian Lv", "Yuanyuan Qiao", "Yan Wang", "Degang Sun", "Weiqing Huang", "Yingjiu Li", "XiaoFeng Wang"], "title": "An Automated Attack Investigation Approach Leveraging Threat-Knowledge-Augmented Large Language Models", "comment": null, "summary": "Advanced Persistent Threats (APTs) are prolonged, stealthy intrusions by\nskilled adversaries that compromise high-value systems to steal data or disrupt\noperations. Reconstructing complete attack chains from massive, heterogeneous\nlogs is essential for effective attack investigation, yet existing methods\nsuffer from poor platform generality, limited generalization to evolving\ntactics, and an inability to produce analyst-ready reports. Large Language\nModels (LLMs) offer strong semantic understanding and summarization\ncapabilities, but in this domain they struggle to capture the long-range,\ncross-log dependencies critical for accurate reconstruction.\n  To solve these problems, we present an LLM-empowered attack investigation\nframework augmented with a dynamically adaptable Kill-Chain-aligned threat\nknowledge base. We organizes attack-relevant behaviors into stage-aware\nknowledge units enriched with semantic annotations, enabling the LLM to\niteratively retrieve relevant intelligence, perform causal reasoning, and\nprogressively expand the investigation context. This process reconstructs\nmulti-phase attack scenarios and generates coherent, human-readable\ninvestigation reports. Evaluated on 15 attack scenarios spanning single-host\nand multi-host environments across Windows and Linux (over 4.3M log events, 7.2\nGB of data), the system achieves an average True Positive Rate (TPR) of 97.1%\nand an average False Positive Rate (FPR) of 0.2%, significantly outperforming\nthe SOTA method ATLAS, which achieves an average TPR of 79.2% and an average\nFPR of 29.1%."}
{"id": "2509.01375", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.01375", "abs": "https://arxiv.org/abs/2509.01375", "authors": ["Alberto Miguel-Diez", "Adri√°n Campazas-Vega", "√Ångel Manuel Guerrero-Higueras", "Claudia √Ålvarez-Aparicio", "Vicente Matell√°n-Olivera"], "title": "Anomaly detection in network flows using unsupervised online machine learning", "comment": "14 pages, 3 figures, 6 tables", "summary": "Nowadays, the volume of network traffic continues to grow, along with the\nfrequency and sophistication of attacks. This scenario highlights the need for\nsolutions capable of continuously adapting, since network behavior is dynamic\nand changes over time. This work presents an anomaly detection model for\nnetwork flows using unsupervised machine learning with online learning\ncapabilities. This approach allows the system to dynamically learn the normal\nbehavior of the network and detect deviations without requiring labeled data,\nwhich is particularly useful in real-world environments where traffic is\nconstantly changing and labeled data is scarce. The model was implemented using\nthe River library with a One-Class SVM and evaluated on the NF-UNSW-NB15\ndataset and its extended version v2, which contain network flows labeled with\ndifferent attack categories. The results show an accuracy above 98%, a false\npositive rate below 3.1%, and a recall of 100% in the most advanced version of\nthe dataset. In addition, the low processing time per flow (<0.033 ms)\ndemonstrates the feasibility of the approach for real-time applications."}
{"id": "2509.01434", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.01434", "abs": "https://arxiv.org/abs/2509.01434", "authors": ["Handi Chen", "Jing Deng", "Xiuzhe Wu", "Zhihan Jiang", "Xinchen Zhang", "Xianhao Chen", "Edith C. H. Ngai"], "title": "LiFeChain: Lightweight Blockchain for Secure and Efficient Federated Lifelong Learning in IoT", "comment": null, "summary": "The expansion of Internet of Things (IoT) devices constantly generates\nheterogeneous data streams, driving demand for continuous, decentralized\nintelligence. Federated Lifelong Learning (FLL) provides an ideal solution by\nincorporating federated and lifelong learning to overcome catastrophic\nforgetting. The extended lifecycle of FLL in IoT systems increases their\nvulnerability to persistent attacks, and these risks may be obscured by\nperformance degradation caused by spatial-temporal data heterogeneity.\nMoreover, this problem is exacerbated by the standard single-server\narchitecture, as its single point of failure makes it difficult to maintain a\nreliable audit trail for long-term threats. Blockchain provides a tamper-proof\nfoundation for trustworthy FLL systems. Nevertheless, directly applying\nblockchain to FLL significantly increases computational and retrieval costs\nwith the expansion of the knowledge base, slowing down the training on IoT\ndevices. To address these challenges, we propose LiFeChain, a lightweight\nblockchain for secure and efficient federated lifelong learning by providing a\ntamper-resistant ledger with minimal on-chain disclosure and bidirectional\nverification. To the best of our knowledge, LiFeChain is the first blockchain\ntailored for FLL. LiFeChain incorporates two complementary mechanisms: the\nproof-of-model-correlation (PoMC) consensus on the server, which couples\nlearning and unlearning mechanisms to mitigate negative transfer, and segmented\nzero-knowledge arbitration (Seg-ZA) on the client, which detects and arbitrates\nabnormal committee behavior without compromising privacy. LiFeChain is designed\nas a plug-and-play component that can be seamlessly integrated into existing\nFLL algorithms. Experimental results demonstrate that LiFeChain not only\nenhances model performance against two long-term attacks but also sustains high\nefficiency and scalability."}
{"id": "2509.01463", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.01463", "abs": "https://arxiv.org/abs/2509.01463", "authors": ["Pranjay Malhotra"], "title": "LLMHoney: A Real-Time SSH Honeypot with Large Language Model-Driven Dynamic Response Generation", "comment": "7 Pages", "summary": "Cybersecurity honeypots are deception tools for engaging attackers and gather\nintelligence, but traditional low or medium-interaction honeypots often rely on\nstatic, pre-scripted interactions that can be easily identified by skilled\nadversaries. This Report presents LLMHoney, an SSH honeypot that leverages\nLarge Language Models (LLMs) to generate realistic, dynamic command outputs in\nreal time. LLMHoney integrates a dictionary-based virtual file system to handle\ncommon commands with low latency while using LLMs for novel inputs, achieving a\nbalance between authenticity and performance. We implemented LLMHoney using\nopen-source LLMs and evaluated it on a testbed with 138 representative Linux\ncommands. We report comprehensive metrics including accuracy (exact-match,\nCosine Similarity, Jaro-Winkler Similarity, Levenshtein Similarity and BLEU\nscore), response latency and memory overhead. We evaluate LLMHoney using\nmultiple LLM backends ranging from 0.36B to 3.8B parameters, including both\nopen-source models and a proprietary model(Gemini). Our experiments compare 13\ndifferent LLM variants; results show that Gemini-2.0 and moderately-sized\nmodels Qwen2.5:1.5B and Phi3:3.8B provide the most reliable and accurate\nresponses, with mean latencies around 3 seconds, whereas smaller models often\nproduce incorrect or out-of-character outputs. We also discuss how LLM\nintegration improves honeypot realism and adaptability compared to traditional\nhoneypots, as well as challenges such as occasional hallucinated outputs and\nincreased resource usage. Our findings demonstrate that LLM-driven honeypots\nare a promising approach to enhance attacker engagement and collect richer\nthreat intelligence."}
{"id": "2509.01470", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.01470", "abs": "https://arxiv.org/abs/2509.01470", "authors": ["I. D. Lutz", "A. M. Hill", "M. C. Valenti"], "title": "Privacy-preserving authentication for military 5G networks", "comment": "To appear in Proc. IEEE Military Commun. Conf. (MILCOM), (Los\n  Angeles, CA), Oct. 2025", "summary": "As 5G networks gain traction in defense applications, ensuring the privacy\nand integrity of the Authentication and Key Agreement (AKA) protocol is\ncritical. While 5G AKA improves upon previous generations by concealing\nsubscriber identities, it remains vulnerable to replay-based synchronization\nand linkability threats under realistic adversary models. This paper provides a\nunified analysis of the standardized 5G AKA flow, identifying several\nvulnerabilities and highlighting how each exploits protocol behavior to\ncompromise user privacy. To address these risks, we present five lightweight\nmitigation strategies. We demonstrate through prototype implementation and\ntesting that these enhancements strengthen resilience against linkability\nattacks with minimal computational and signaling overhead. Among the solutions\nstudied, those introducing a UE-generated nonce emerge as the most promising,\neffectively neutralizing the identified tracking and correlation attacks with\nnegligible additional overhead. Integrating this extension as an optional\nfeature to the standard 5G AKA protocol offers a backward-compatible,\nlow-overhead path toward a more privacy-preserving authentication framework for\nboth commercial and military 5G deployments."}
{"id": "2509.01509", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.01509", "abs": "https://arxiv.org/abs/2509.01509", "authors": ["Chengyu Song", "Jianming Zheng"], "title": "Insight-LLM: LLM-enhanced Multi-view Fusion in Insider Threat Detection", "comment": null, "summary": "Insider threat detection (ITD) requires analyzing sparse, heterogeneous user\nbehavior. Existing ITD methods predominantly rely on single-view modeling,\nresulting in limited coverage and missed anomalies. While multi-view learning\nhas shown promise in other domains, its direct application to ITD introduces\nsignificant challenges: scalability bottlenecks from independently trained\nsub-models, semantic misalignment across disparate feature spaces, and view\nimbalance that causes high-signal modalities to overshadow weaker ones. In this\nwork, we present Insight-LLM, the first modular multi-view fusion framework\nspecifically tailored for insider threat detection. Insight-LLM employs frozen,\npre-nes, achieving state-of-the-art detection with low latency and parameter\noverhead."}
{"id": "2509.01592", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SY", "eess.SY", "68T05, 93C65, 90C35", "K.6.5; C.2.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.01592", "abs": "https://arxiv.org/abs/2509.01592", "authors": ["Einstein Rivas Pizarro", "Wajiha Zaheer", "Li Yang", "Khalil El-Khatib", "Glenn Harvel"], "title": "Securing Radiation Detection Systems with an Efficient TinyML-Based IDS for Edge Devices", "comment": "Preprint author original pre review. Accepted and Presented at NPIC &\n  HMIT 2025. The official proceedings version is available in the ANS Digital\n  Library", "summary": "Radiation Detection Systems (RDSs) play a vital role in ensuring public\nsafety across various settings, from nuclear facilities to medical\nenvironments. However, these systems are increasingly vulnerable to\ncyber-attacks such as data injection, man-in-the-middle (MITM) attacks, ICMP\nfloods, botnet attacks, privilege escalation, and distributed denial-of-service\n(DDoS) attacks. Such threats could compromise the integrity and reliability of\nradiation measurements, posing significant public health and safety risks. This\npaper presents a new synthetic radiation dataset and an Intrusion Detection\nSystem (IDS) tailored for resource-constrained environments, bringing Machine\nLearning (ML) predictive capabilities closer to the sensing edge layer of\ncritical infrastructure. Leveraging TinyML techniques, the proposed IDS employs\nan optimized XGBoost model enhanced with pruning, quantization, feature\nselection, and sampling. These TinyML techniques significantly reduce the size\nof the model and computational demands, enabling real-time intrusion detection\non low-resource devices while maintaining a reasonable balance between\nefficiency and accuracy."}
{"id": "2509.01597", "categories": ["cs.CR", "cs.DS", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.01597", "abs": "https://arxiv.org/abs/2509.01597", "authors": ["Kaitlyn Webb", "Prottay Protivash", "John Durrell", "Daniell Toth", "Aleksandra Slavkoviƒá", "Daniel Kifer"], "title": "Statistics-Friendly Confidentiality Protection for Establishment Data, with Applications to the QCEW", "comment": "37 pages (14 main text and 24 appendix pages), 7 figures", "summary": "Confidentiality for business data is an understudied area of disclosure\navoidance, where legacy methods struggle to provide acceptable results. Modern\nformal privacy techniques designed for person-level data do not provide\nsuitable confidentiality/utility trade-offs due to the highly skewed nature of\nbusiness data and because extreme outlier records are often important\ncontributors to query answers. In this paper, inspired by Gaussian Differential\nPrivacy, we propose a novel confidentiality framework for business data with a\nfocus on interpretability for policy makers. We propose two query-answering\nmechanisms and analyze new challenges that arise when noisy query answers are\nconverted into confidentiality-preserving microdata. We evaluate our mechanisms\non confidential Quarterly Census of Employment and Wages (QCEW) microdata and a\npublic substitute dataset."}
{"id": "2509.01599", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SY", "eess.SY", "68T05, 93C65, 90C35", "K.6.5; C.2.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.01599", "abs": "https://arxiv.org/abs/2509.01599", "authors": ["Nathanael Coolidge", "Jaime Gonz√°lez Sanz", "Li Yang", "Khalil El Khatib", "Glenn Harvel", "Nelson Agbemava", "I Putu Susila", "Mehmet Yavuz Yagci"], "title": "An Efficient Intrusion Detection System for Safeguarding Radiation Detection Systems", "comment": "Preprint author original pre review. Accepted and Presented at ISOFIC\n  2024. The official proceedings version is available on the conference site", "summary": "Radiation Detection Systems (RDSs) are used to measure and detect abnormal\nlevels of radioactive material in the environment. These systems are used in\nmany applications to mitigate threats posed by high levels of radioactive\nmaterial. However, these systems lack protection against malicious external\nattacks to modify the data. The novelty of applying Intrusion Detection Systems\n(IDS) in RDSs is a crucial element in safeguarding these critical\ninfrastructures. While IDSs are widely used in networking environments to\nsafeguard against various attacks, their application in RDSs is novel. A common\nattack on RDSs is Denial of Service (DoS), where the attacker aims to overwhelm\nthe system, causing malfunctioning RDSs. This paper proposes an efficient\nMachine Learning (ML)-based IDS to detect anomalies in radiation data, focusing\non DoS attacks. This work explores the use of sampling methods to create a\nsimulated DoS attack based on a real radiation dataset, followed by an\nevaluation of various ML algorithms, including Random Forest, Support Vector\nMachine (SVM), logistic regression, and Light Gradient-Boosting Machine\n(LightGBM), to detect DoS attacks on RDSs. LightGBM is emphasized for its\nsuperior accuracy and low computational resource consumption, making it\nparticularly suitable for real-time intrusion detection. Additionally, model\noptimization and TinyML techniques, including feature selection, parallel\nexecution, and random search methods, are used to improve the efficiency of the\nproposed IDS. Finally, an optimized and efficient LightGBM-based IDS is\ndeveloped to achieve accurate intrusion detection for RDSs."}
{"id": "2509.01701", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.01701", "abs": "https://arxiv.org/abs/2509.01701", "authors": ["Kazi Hassan Shakib", "Muhammad Asfand Hafeez", "Arslan Munir"], "title": "AmphiKey: A Dual-Mode Secure Authenticated Key Encapsulation Protocol for Smart Grid", "comment": null, "summary": "AmphiKey, a dual-mode post-quantum/traditional (PQ/T) hybrid authenticated\nkey exchange mechanism (AKEM) has been designed to secure smart grid\ncommunications against both classical and quantum threats. AmphiKey offers two\ndistinct operational modes within a single framework: an Authenticated Mode and\na Deniable Mode. The Authenticated Mode employs a blackbox approach, combining\nephemeral ML-KEM-768 and X25519 with long-term Raccoon DSA keys to provide\nforward secrecy and strong, non-repudiable authenticity. This design achieves\n\"OR\" confidentiality, where security holds if either of the KEMs is unbroken,\nand robust \"AND\" authenticity. For the signature operation, it leverages the\n'masking-friendly' Raccoon digital signature (DSA), which is specifically\ndesigned for side-channel attack resistance, though this protection is\nlocalized to the signing key and does not provide deniability. In contrast,\nDeniable Mode provides deniable authentication, preserving privacy. The\nprotocol used ML-KEM-768 (AKEM-1), Ephemeral X25519 (AKEM-2), Raccoon-based DSA\n(Rac) (compared performance to ML-DSA-65), and the Ascon cipher to deliver its\nsecurity guarantees. Key contributions include providing a flexible protocol\nwith enhanced security, optional deniability, and efficiency adapted to the\ndiverse needs of the smart grid infrastructure. We present a comprehensive\nperformance evaluation on a heterogeneous testbed featuring a powerful server\nand client (AMD Ryzen 5) and a resource-constrained client (Raspberry Pi). In\nefficient Deniable mode, the full handshake completes in 0.15 ms on the server\nand 0.41 ms on the Raspberry Pi client. In contrast, the Authenticated Mode is\nbottlenecked by the client-side signature generation; the handshake takes 4.8\nms for the Raspberry Pi client to initiate and 0.84 ms for the server to\nverify."}
{"id": "2509.01717", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.01717", "abs": "https://arxiv.org/abs/2509.01717", "authors": ["Hojjat Farshadinia", "Ali Barati", "Hamid Barati"], "title": "Designing a Layered Framework to Secure Data via Improved Multi Stage Lightweight Cryptography in IoT Cloud Systems", "comment": null, "summary": "This paper presents a novel multi-layered hybrid security approach aimed at\nenhancing lightweight encryption for IoT-Cloud systems. The primary goal is to\novercome limitations inherent in conventional solutions such as TPA,\nBlockchain, ECDSA and ZSS which often fall short in terms of data protection,\ncomputational efficiency and scalability. Our proposed method strategically\nrefines and integrates these technologies to address their shortcomings while\nmaximizing their individual strengths. By doing so we create a more reliable\nand high-performance framework for secure data exchange across heterogeneous\nenvironments. The model leverages the combined potential of emerging\ntechnologies, particularly Blockchain, IoT and Cloud computing which when\neffectively coordinated offer significant advancements in security\narchitecture. The proposed framework consists of three core layers: (1) the\nH.E.EZ Layer which integrates improved versions of Hyperledger Fabric,\nEnc-Block and a hybrid ECDSA-ZSS scheme to improve encryption speed,\nscalability and reduce computational cost; (2) the Credential Management Layer\nindependently verifying data integrity and authenticity; and (3) the Time and\nAuditing Layer designed to reduce traffic overhead and optimize performance\nacross dynamic workloads. Evaluation results highlight that the proposed\nsolution not only strengthens security but also significantly improves\nexecution time, communication efficiency and system responsiveness, offering a\nrobust path forward for next-generation IoT-Cloud infrastructures."}
{"id": "2509.01731", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.01731", "abs": "https://arxiv.org/abs/2509.01731", "authors": ["Tran Duc Le", "Phuc Hao Do", "Truong Duy Dinh", "Van Dai Pham"], "title": "Are Enterprises Ready for Quantum-Safe Cybersecurity?", "comment": "Are Enterprises Ready for Quantum-Safe Cybersecurity?", "summary": "Quantum computing threatens to undermine classical cryptography by breaking\nwidely deployed encryption and signature schemes. This paper examines\nenterprise readiness for quantum-safe cybersecurity through three perspectives:\n(i) the technologist view, assessing the maturity of post-quantum cryptography\n(PQC) and quantum key distribution (QKD); (ii) the enterprise (CISO/CIO) view,\nanalyzing organizational awareness, risk management, and operational barriers;\nand (iii) the threat actor view, evaluating the evolving quantum threat and the\nurgency of migration. Using recent standards (e.g., NIST's 2024 PQC\nalgorithms), industry surveys, and threat intelligence, we synthesize findings\nvia a SWOT analysis to map strengths, weaknesses, opportunities, and threats.\nResults indicate uneven and generally insufficient preparedness: while PQC\nstandards and niche QKD deployments signal technical progress, fewer than 5\\%\nof enterprises have formal quantum-transition plans, and many underestimate\n\"harvest now, decrypt later\" risks. Financial, telecom, and government sectors\nhave begun migration, but most industries remain exploratory or stalled by\ncosts, complexity, and skills gaps. Expert consensus places cryptanalytically\nrelevant quantum computers in the 2030s, yet delayed preparation could leave\ntoday's data vulnerable for decades. We recommend immediate steps: establishing\ncrypto-agility, creating quantum transition roadmaps, prioritizing PQC\ndeployment in high-value systems, and upskilling cybersecurity teams. A\ncoordinated, proactive approach is essential to secure current and future\ndigital assets in the quantum era."}
{"id": "2509.01742", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.01742", "abs": "https://arxiv.org/abs/2509.01742", "authors": ["Yitong Guo", "Hongbo Chen", "Haobin Hiroki Chen", "Yukui Luo", "XiaoFeng Wang", "Chenghong Wang"], "title": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure HBM Accelerators", "comment": "Accepted by CCS 2025", "summary": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO((log log N)^2) bandwidth overhead. BOLT introduces three key innovations: (i)\na new OMAP algorithm that leverages isolated HBM as an unobservable cache to\naccelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook."}
{"id": "2509.01791", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.01791", "abs": "https://arxiv.org/abs/2509.01791", "authors": ["Luca Pajola", "Eugenio Caripoti", "Simeone Pizzi", "Mauro Conti", "Stefan Banzer", "Giovanni Apruzzese"], "title": "E-PhishGen: Unlocking Novel Research in Phishing Email Detection", "comment": "Accepted to ACM AISec '26", "summary": "Every day, our inboxes are flooded with unsolicited emails, ranging between\nannoying spam to more subtle phishing scams. Unfortunately, despite abundant\nprior efforts proposing solutions achieving near-perfect accuracy, the reality\nis that countering malicious emails still remains an unsolved dilemma.\n  This \"open problem\" paper carries out a critical assessment of scientific\nworks in the context of phishing email detection. First, we focus on the\nbenchmark datasets that have been used to assess the methods proposed in\nresearch. We find that most prior work relied on datasets containing emails\nthat -- we argue -- are not representative of current trends, and mostly\nencompass the English language. Based on this finding, we then re-implement and\nre-assess a variety of detection methods reliant on machine learning (ML),\nincluding large-language models (LLM), and release all of our codebase -- an\n(unfortunately) uncommon practice in related research. We show that most such\nmethods achieve near-perfect performance when trained and tested on the same\ndataset -- a result which intrinsically hinders development (how can future\nresearch outperform methods that are already near perfect?). To foster the\ncreation of \"more challenging benchmarks\" that reflect current phishing trends,\nwe propose E-PhishGEN, an LLM-based (and privacy-savvy) framework to generate\nnovel phishing-email datasets. We use our E-PhishGEN to create E-PhishLLM, a\nnovel phishing-email detection dataset containing 16616 emails in three\nlanguages. We use E-PhishLLM to test the detectors we considered, showing a\nmuch lower performance than that achieved on existing benchmarks -- indicating\na larger room for improvement. We also validate the quality of E-PhishLLM with\na user study (n=30). To sum up, we show that phishing email detection is still\nan open problem -- and provide the means to tackle such a problem by future\nresearch."}
{"id": "2509.01835", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.01835", "abs": "https://arxiv.org/abs/2509.01835", "authors": ["Saad Ullah", "Praneeth Balasubramanian", "Wenbo Guo", "Amanda Burnett", "Hammond Pearce", "Christopher Kruegel", "Giovanni Vigna", "Gianluca Stringhini"], "title": "From CVE Entries to Verifiable Exploits: An Automated Multi-Agent Framework for Reproducing CVEs", "comment": null, "summary": "High-quality datasets of real-world vulnerabilities and their corresponding\nverifiable exploits are crucial resources in software security research. Yet\nsuch resources remain scarce, as their creation demands intensive manual effort\nand deep security expertise. In this paper, we present CVE-GENIE, an automated,\nlarge language model (LLM)-based multi-agent framework designed to reproduce\nreal-world vulnerabilities, provided in Common Vulnerabilities and Exposures\n(CVE) format, to enable creation of high-quality vulnerability datasets. Given\na CVE entry as input, CVE-GENIE gathers the relevant resources of the CVE,\nautomatically reconstructs the vulnerable environment, and (re)produces a\nverifiable exploit. Our systematic evaluation highlights the efficiency and\nrobustness of CVE-GENIE's design and successfully reproduces approximately 51%\n(428 of 841) CVEs published in 2024-2025, complete with their verifiable\nexploits, at an average cost of $2.77 per CVE. Our pipeline offers a robust\nmethod to generate reproducible CVE benchmarks, valuable for diverse\napplications such as fuzzer evaluation, vulnerability patching, and assessing\nAI's security capabilities."}
{"id": "2509.02004", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.02004", "abs": "https://arxiv.org/abs/2509.02004", "authors": ["Takao Murakami", "Yuichi Sei", "Reo Eriguchi"], "title": "Augmented Shuffle Differential Privacy Protocols for Large-Domain Categorical and Key-Value Data", "comment": "Full version of the paper accepted at NDSS 2026", "summary": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols."}
{"id": "2509.02042", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.02042", "abs": "https://arxiv.org/abs/2509.02042", "authors": ["Pascal Zimmer", "Simon Lachnit", "Alexander Jan Zielinski", "Ghassan Karame"], "title": "Targeted Physical Evasion Attacks in the Near-Infrared Domain", "comment": "To appear in the Proceedings of the Network and Distributed Systems\n  Security Symposium (NDSS) 2026", "summary": "A number of attacks rely on infrared light sources or heat-absorbing material\nto imperceptibly fool systems into misinterpreting visual input in various\nimage recognition applications. However, almost all existing approaches can\nonly mount untargeted attacks and require heavy optimizations due to the\nuse-case-specific constraints, such as location and shape. In this paper, we\npropose a novel, stealthy, and cost-effective attack to generate both targeted\nand untargeted adversarial infrared perturbations. By projecting perturbations\nfrom a transparent film onto the target object with an off-the-shelf infrared\nflashlight, our approach is the first to reliably mount laser-free targeted\nattacks in the infrared domain. Extensive experiments on traffic signs in the\ndigital and physical domains show that our approach is robust and yields higher\nattack success rates in various attack scenarios across bright lighting\nconditions, distances, and angles compared to prior work. Equally important,\nour attack is highly cost-effective, requiring less than US\\$50 and a few tens\nof seconds for deployment. Finally, we propose a novel segmentation-based\ndetection that thwarts our attack with an F1-score of up to 99%."}
{"id": "2509.02076", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02076", "abs": "https://arxiv.org/abs/2509.02076", "authors": ["Kong Mun Yeen", "Rafidah Md Noor", "Wahidah Md Shah", "Aslinda Hassan", "Muhammad Umair Munir"], "title": "Forecasting Future DDoS Attacks Using Long Short Term Memory (LSTM) Model", "comment": "18 pages", "summary": "This paper forecasts future Distributed Denial of Service (DDoS) attacks\nusing deep learning models. Although several studies address forecasting DDoS\nattacks, they remain relatively limited compared to detection-focused research.\nBy studying the current trends and forecasting based on newer and updated\ndatasets, mitigation plans against the attacks can be planned and formulated.\nThe methodology used in this research work conforms to the Cross Industry\nStandard Process for Data Mining (CRISP-DM) model."}
{"id": "2509.02077", "categories": ["cs.CR", "cs.CL", "cs.LG", "68T50 Natural language processing", "D.4.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.02077", "abs": "https://arxiv.org/abs/2509.02077", "authors": ["Refat Othman", "Diaeddin Rimawi", "Bruno Rossi", "Barbara Russo"], "title": "From Attack Descriptions to Vulnerabilities: A Sentence Transformer-Based Approach", "comment": "Accepted in The Journal of Systems and Software (2025)", "summary": "In the domain of security, vulnerabilities frequently remain undetected even\nafter their exploitation. In this work, vulnerabilities refer to publicly\ndisclosed flaws documented in Common Vulnerabilities and Exposures (CVE)\nreports. Establishing a connection between attacks and vulnerabilities is\nessential for enabling timely incident response, as it provides defenders with\nimmediate, actionable insights. However, manually mapping attacks to CVEs is\ninfeasible, thereby motivating the need for automation. This paper evaluates 14\nstate-of-the-art (SOTA) sentence transformers for automatically identifying\nvulnerabilities from textual descriptions of attacks. Our results demonstrate\nthat the multi-qa-mpnet-base-dot-v1 (MMPNet) model achieves superior\nclassification performance when using attack Technique descriptions, with an\nF1-score of 89.0, precision of 84.0, and recall of 94.7. Furthermore, it was\nobserved that, on average, 56% of the vulnerabilities identified by the MMPNet\nmodel are also represented within the CVE repository in conjunction with an\nattack, while 61% of the vulnerabilities detected by the model correspond to\nthose cataloged in the CVE repository. A manual inspection of the results\nrevealed the existence of 275 predicted links that were not documented in the\nMITRE repositories. Consequently, the automation of linking attack techniques\nto vulnerabilities not only enhances the detection and response capabilities\nrelated to software security incidents but also diminishes the duration during\nwhich vulnerabilities remain exploitable, thereby contributing to the\ndevelopment of more secure systems."}
{"id": "2509.02083", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.02083", "abs": "https://arxiv.org/abs/2509.02083", "authors": ["Dmitry Tanana"], "title": "Performance analysis of common browser extensions for cryptojacking detection", "comment": null, "summary": "This paper considers five extensions for Chromium-based browsers in order to\ndetermine how effective can browser-based defenses against cryptojacking\navailable to regular users be. We've examined most popular extensions -\nMinerBlock, AdGuard AdBlocker, Easy Redirect && Prevent Cryptojacking,\nCoinEater and Miners Shield, which claim to be designed specifically to\nidentify and stop illegal cryptocurrency mining. An empirically confirmed\ndataset of 373 distinct cryptojacking-infected websites which was assembled\nduring multi-stage procedure, was used to test those extensions. The results\nshowed that all plugins in question had significant performance limits. Easy\nRedirect and Miners Shield only blocked 6 and 5 websites respectively, while\nMinerBlock had the greatest detection rate at only 27% (101/373 sites blocked).\nMost concerningly, despite promises of cryptojacking prevention, AdGuard (which\nhas over 13 million users) and CoinEater were unable to identify any of the\ncompromised websites. These results demonstrate serious flaws in cryptojacking\ndetection products targeted for regular users, since even the best-performing\nspecimen failed to detect 73% of attacks. The obvious difference between\nadvertised capabilities and real performance highlights the urgent need for\neither accessibility improvements for laboratory-grade detection technologies\nthat show 90%+ efficiency in controlled environment or fundamental upgrades to\ncurrent commonly used extensions."}
{"id": "2509.02189", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.02189", "abs": "https://arxiv.org/abs/2509.02189", "authors": ["Aditya Bhardwaj", "P√©ter Kutas"], "title": "A Gentle Introduction to Blind signatures: From RSA to Lattice-based Cryptography", "comment": null, "summary": "Blind signatures were first introduced by David Chaum. They allow a user to\nhave a message signed by a signer without revealing the message itself. This\nproperty is particularly useful in applications such as electronic voting and\ndigital cash, where user anonymity is important. In a blind signature scheme,\nthe user blinds their message before sending it to the signer, who signs the\nblinded message. The user then unblinds the signed message to obtain a valid\nsignature that can be verified publicly, ensuring that the signer cannot trace\nthe signed message back to the original unblinded version. A good analogy is\nplacing the message inside an envelope and having the envelope signed. Once the\nenvelope is opened, the signature remains valid for the enclosed message,\nensuring that the content remains confidential.\n  Such constructions provide anonymity and privacy to the user but given a\npractical quantum computer, the security of traditional crypto-systems\nproviding such features will be broken. To address this, the development of\nquantum-resistant cryptographic protocols is essential for maintaining the\nsecurity of digital transactions and data. Aligning with the same goal, this\nwork aims to thoroughly review the background of lattice-based blind\nsignatures. We start with the foundations of digital signatures in the\nclassical settings and then move on to lattice-based constructions."}
{"id": "2509.02289", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.02289", "abs": "https://arxiv.org/abs/2509.02289", "authors": ["Anuj Gautam", "Tarun Yadav", "Garrett Smith", "Kent Seamons", "Scott Ruoti"], "title": "Passwords and FIDO2 Are Meant To Be Secret: A Practical Secure Authentication Channel for Web Browsers", "comment": "Extended version of paper published at CCS 2025:\n  https://doi.org/10.1145/3719027.3765195", "summary": "Password managers provide significant security benefits to users. However,\nmalicious client-side scripts and browser extensions can steal passwords after\nthe manager has autofilled them into the web page. In this paper, we extend\nprior work by Stock and Johns, showing how password autofill can be hardened to\nprevent these local attacks. We implement our design in the Firefox browser and\nconduct experiments demonstrating that our defense successfully protects\npasswords from XSS attacks and malicious extensions. We also show that our\nimplementation is compatible with 97% of the Alexa top 1000 websites. Next, we\ngeneralize our design, creating a second defense that prevents recently\ndiscovered local attacks against the FIDO2 protocols. We implement this second\ndefense into Firefox, demonstrating that it protects the FIDO2 protocol against\nXSS attacks and malicious extensions. This defense is compatible with all\nwebsites, though it does require a small change (2-3 lines) to web servers\nimplementing FIDO2."}
{"id": "2509.02372", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.02372", "abs": "https://arxiv.org/abs/2509.02372", "authors": ["Zhiyang Chen", "Tara Saba", "Xun Deng", "Xujie Si", "Fan Long"], "title": "Poisoned at Scale: A Scalable Audit Uncovers Hidden Scam Endpoints in Production LLMs", "comment": "10 pages, 4 figures", "summary": "Large Language Models (LLMs) have become critical to modern software\ndevelopment, but their reliance on internet datasets for training introduces a\nsignificant security risk: the absorption and reproduction of malicious\ncontent. To evaluate this threat, this paper introduces a scalable, automated\naudit framework that synthesizes innocuous, developer-style prompts from known\nscam databases to query production LLMs and determine if they generate code\ncontaining harmful URLs. We conducted a large-scale evaluation across four\nproduction LLMs (GPT-4o, GPT-4o-mini, Llama-4-Scout, and DeepSeek-V3), and\nfound a systemic vulnerability, with all tested models generating malicious\ncode at a non-negligible rate. On average, 4.2\\% of programs generated in our\nexperiments contained malicious URLs. Crucially, this malicious code is often\ngenerated in response to benign prompts. We manually validate the prompts which\ncause all four LLMs to generate malicious code, and resulting in 177 innocuous\nprompts that trigger all models to produce harmful outputs. These results\nprovide strong empirical evidence that the training data of production LLMs has\nbeen successfully poisoned at scale, underscoring the urgent need for more\nrobust defense mechanisms and post-generation safety checks to mitigate the\npropagation of hidden security threats."}
{"id": "2509.02387", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02387", "abs": "https://arxiv.org/abs/2509.02387", "authors": ["Rye Stahle-Smith", "Rasha Karakchi"], "title": "Real-time ML-based Defense Against Malicious Payload in Reconfigurable Embedded Systems", "comment": "This paper is submitted at Supercomputing (SC'25)", "summary": "The growing use of FPGAs in reconfigurable systems introducessecurity risks\nthrough malicious bitstreams that could cause denial-of-service (DoS), data\nleakage, or covert attacks. We investigated chip-level hardware malicious\npayload in embedded systems and proposed a supervised machine learning method\nto detect malicious bitstreams via static byte-level features. Our approach\ndiverges from existing methods by analyzing bitstreams directly at the binary\nlevel, enabling real-time detection without requiring access to source code or\nnetlists. Bitstreams were sourced from state-of-the-art (SOTA) benchmarks and\nre-engineered to target the Xilinx PYNQ-Z1 FPGA Development Board. Our dataset\nincluded 122 samples of benign and malicious configurations. The data were\nvectorized using byte frequency analysis, compressed using TSVD, and balanced\nusing SMOTE to address class imbalance. The evaluated classifiers demonstrated\nthat Random Forest achieved a macro F1-score of 0.97, underscoring the\nviability of real-time Trojan detection on resource-constrained systems. The\nfinal model was serialized and successfully deployed via PYNQ to enable\nintegrated bitstream analysis."}
{"id": "2509.02411", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02411", "abs": "https://arxiv.org/abs/2509.02411", "authors": ["Honghui Xu", "Kaiyang Li", "Wei Chen", "Danyang Zheng", "Zhiyuan Li", "Zhipeng Cai"], "title": "A Survey: Towards Privacy and Security in Mobile Large Language Models", "comment": null, "summary": "Mobile Large Language Models (LLMs) are revolutionizing diverse fields such\nas healthcare, finance, and education with their ability to perform advanced\nnatural language processing tasks on-the-go. However, the deployment of these\nmodels in mobile and edge environments introduces significant challenges\nrelated to privacy and security due to their resource-intensive nature and the\nsensitivity of the data they process. This survey provides a comprehensive\noverview of privacy and security issues associated with mobile LLMs,\nsystematically categorizing existing solutions such as differential privacy,\nfederated learning, and prompt encryption. Furthermore, we analyze\nvulnerabilities unique to mobile LLMs, including adversarial attacks,\nmembership inference, and side-channel attacks, offering an in-depth comparison\nof their effectiveness and limitations. Despite recent advancements, mobile\nLLMs face unique hurdles in achieving robust security while maintaining\nefficiency in resource-constrained environments. To bridge this gap, we propose\npotential applications, discuss open challenges, and suggest future research\ndirections, paving the way for the development of trustworthy,\nprivacy-compliant, and scalable mobile LLM systems."}
{"id": "2509.02412", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.02412", "abs": "https://arxiv.org/abs/2509.02412", "authors": ["Wenhao Chen", "Morris Chang", "Witawas Srisa-an", "Yong Guan"], "title": "APEX: Automatic Event Sequence Generation for Android Applications", "comment": null, "summary": "Due to the event driven nature and the versatility of GUI designs in Android\nprograms, it is challenging to generate event sequences with adequate code\ncoverage within a reasonable time. A common approach to handle this issue is to\nrely on GUI models to generate event sequences. These sequences can be\neffective in covering GUI states, but inconsistent in exposing program\nbehaviors that require specific inputs. A major obstacle to generate such\nspecific inputs is the lack of a systematic GUI exploration process to\naccommodate the analysis requirements. In this paper, we introduce Android Path\nExplorer (APEX), a systematic input generation framework using concolic\nexecution. APEX addresses the limitations of model-based sequence generation by\nusing concolic execution to discover the data dependencies of GUI state\ntransitions. Moreover, concolic execution is also used to prioritize events\nduring the exploration of GUI, which leads to a more robust model and accurate\ninput generation. The key novelty of APEX is that concolic execution is not\nonly used to construct event sequences, but also used to traverse the GUI more\nsystematically. As such, our experimental results show that APEX can be used to\ngenerate a set of event sequences that achieve high code coverage, as well as\nevent sequences that reach specific targets."}
{"id": "2509.02413", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.02413", "abs": "https://arxiv.org/abs/2509.02413", "authors": ["Edoardo Marangone", "Eugenio Nerio Nemmi", "Daniele Friolo", "Giuseppe Ateniese", "Ingo Weber", "Claudio Di Ciccio"], "title": "Enabling decision support over confidential data", "comment": null, "summary": "Enabling automated decision-making processes by leveraging data-driven\nanalysis is a core goal of Decision Support Systems (DSSs). In multi-party\nscenarios where decisions rely on distributed and sensitive data, though,\nensuring confidentiality, verifiability, transparency, integrity, and\nconsistency at once remains an open challenge for DSSs. To tackle this\nmulti-faceted problem, we propose the Secure Platform for Automated decision\nRules via Trusted Applications (SPARTA) approach. By leveraging Trusted\nExecution Environments (TEEs) at its core, SPARTA ensures that the decision\nlogic and the data remain protected. To guarantee transparency and consistency\nof the decision process, SPARTA encodes decision rules into verifiable software\nobjects deployed within TEEs. To maintain the confidentiality of the outcomes\nwhile keeping the information integrity, SPARTA employs cryptography techniques\non notarized data based on user-definable access policies. Based on experiments\nconducted on public benchmarks and synthetic data, we find our approach to be\npractically applicable and scalable."}
