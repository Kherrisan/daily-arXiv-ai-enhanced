{"id": "2508.06643", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.06643", "abs": "https://arxiv.org/abs/2508.06643", "authors": ["Joshua Bailey", "Charles Nicholas"], "title": "Symbolic Execution in Practice: A Survey of Applications in Vulnerability, Malware, Firmware, and Protocol Analysis", "comment": "v2: Adds a subsection to Future Directions discussing the role of\n  LLMs in symbolic execution", "summary": "Symbolic execution is a powerful program analysis technique that allows for\nthe systematic exploration of all program paths. Path explosion, where the\nnumber of states to track becomes unwieldy, is one of the biggest challenges\nhindering symbolic execution's practical application. To combat this,\nresearchers have employed various strategies to enable symbolic execution on\ncomplex software systems. This paper introduces a systematic taxonomy of these\nstrategies, categorizing them into two primary approaches: Scope Reduction,\nwhich aims to reduce the scope of symbolic execution to manageable portions of\ncode, and Guidance Heuristics, which steer the symbolic execution engine toward\npromising paths. Using this taxonomy as a lens, we survey applications of\nsymbolic executions in several domains such as vulnerability analysis, malware\nanalysis, firmware re-hosting, and network protocol analysis. Finally, we\nidentify promising directions for future research, including the application of\nsymbolic execution to real-time operating systems and modern, type-safe\nlanguages.", "AI": {"tldr": "This paper presents a taxonomy for symbolic execution strategies to address path explosion, surveys their applications across domains, and identifies future research opportunities.", "motivation": "Symbolic execution's practical application is hindered by path explosion, necessitating effective strategies for managing state proliferation in complex systems.", "method": "The authors develop a systematic taxonomy categorizing path explosion solutions into 'Scope Reduction' (limiting analysis to code subsections) and 'Guidance Heuristics' (path prioritization), then use this framework to analyze domain-specific applications.", "result": "The taxonomy enables clearer understanding of path explosion strategies across domains like vulnerability analysis and malware research, while highlighting research gaps in real-time systems and type-safe languages.", "conclusion": "The taxonomy provides a foundation for advancing symbolic execution in challenging areas, with recommended research focusing on real-time operating systems and modern programming languages with type safety."}}
{"id": "2508.06734", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06734", "abs": "https://arxiv.org/abs/2508.06734", "authors": ["Ngoc N. Tran", "Anwar Said", "Waseem Abbas", "Tyler Derr", "Xenofon D. Koutsoukos"], "title": "Mitigating Distribution Shift in Graph-Based Android Malware Classification via Function Metadata and LLM Embeddings", "comment": "13 pages, 3 figures, 7 tables, under review", "summary": "Graph-based malware classifiers can achieve over 94% accuracy on standard\nAndroid datasets, yet we find they suffer accuracy drops of up to 45% when\nevaluated on previously unseen malware variants from the same family - a\nscenario where strong generalization would typically be expected. This\nhighlights a key limitation in existing approaches: both the model\narchitectures and their structure-only representations often fail to capture\ndeeper semantic patterns. In this work, we propose a robust semantic enrichment\nframework that enhances function call graphs with contextual features,\nincluding function-level metadata and, when available, code embeddings derived\nfrom large language models. The framework is designed to operate under\nreal-world constraints where feature availability is inconsistent, and supports\nflexible integration of semantic signals. To evaluate generalization under\nrealistic domain and temporal shifts, we introduce two new benchmarks:\nMalNet-Tiny-Common and MalNet-Tiny-Distinct, constructed using malware family\npartitioning to simulate cross-family generalization and evolving threat\nbehavior. Experiments across multiple graph neural network backbones show that\nour method improves classification performance by up to 8% under distribution\nshift and consistently enhances robustness when integrated with\nadaptation-based methods. These results offer a practical path toward building\nresilient malware detection systems in evolving threat environments.", "AI": {"tldr": "The paper identifies a gap in graph-based malware classification accuracy for unseen variants and proposes a semantic enrichment framework to address this, demonstrating improved performance via new benchmarks and experiments.", "motivation": "Existing graph-based malware classifiers experience significant accuracy drops when facing unseen malware variants from the same family, indicating poor generalization due to limited semantic representation.", "method": "A semantic enrichment framework was developed to enhance function call graphs with contextual features, including function-level metadata and code embeddings from large language models, adaptable to varied feature availability.", "result": "The method improved classification accuracy by up to 8% under distribution shifts and increased robustness when combined with adaptation-based approaches, evaluated on new cross-family benchmarks (MalNet-Tiny-Common/Distinct).", "conclusion": "The framework provides a practical solution for resilient malware detection in evolving threat environments by incorporating semantic signals into graph representations."}}
{"id": "2508.06789", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.06789", "abs": "https://arxiv.org/abs/2508.06789", "authors": ["Wei Wang", "Xiangyun Tang", "Yajie Wang", "Yijing Lin", "Tao Zhang", "Meng Shen", "Dusit Niyato", "Liehuang Zhu"], "title": "Label Inference Attacks against Federated Unlearning", "comment": null, "summary": "Federated Unlearning (FU) has emerged as a promising solution to respond to\nthe right to be forgotten of clients, by allowing clients to erase their data\nfrom global models without compromising model performance. Unfortunately,\nresearchers find that the parameter variations of models induced by FU expose\nclients' data information, enabling attackers to infer the label of unlearning\ndata, while label inference attacks against FU remain unexplored. In this\npaper, we introduce and analyze a new privacy threat against FU and propose a\nnovel label inference attack, ULIA, which can infer unlearning data labels\nacross three FU levels. To address the unique challenges of inferring labels\nvia the models variations, we design a gradient-label mapping mechanism in ULIA\nthat establishes a relationship between gradient variations and unlearning\nlabels, enabling inferring labels on accumulated model variations. We evaluate\nULIA on both IID and non-IID settings. Experimental results show that in the\nIID setting, ULIA achieves a 100% Attack Success Rate (ASR) under both\nclass-level and client-level unlearning. Even when only 1% of a user's local\ndata is forgotten, ULIA still attains an ASR ranging from 93% to 62.3%.", "AI": {"tldr": "This paper introduces ULIA, a novel label inference attack method for Federated Unlearning (FU) that can accurately infer unlearning data labels across three FU levels using gradient variations.", "motivation": "Existing FU methods risk exposing client data information through parameter variations, but prior label inference attacks against FU have not been explored, highlighting the need for understanding this privacy threat.", "method": "ULIA employs a gradient-label mapping mechanism to establish relationships between model parameter variations during unlearning and the corresponding labels, enabling inference even with accumulated variations.", "result": "Experiments show ULIA achieves 100% Attack Success Rate (ASR) on IID data under class- and client-level unlearning, and maintains 93-62.3% ASR even when only 1% of a user's data is forgotten, with performance validated on both IID and non-IID settings.", "conclusion": "ULIA demonstrates that model parameter variations in federation unlearning can leak unlearning data labels, raising critical privacy concerns and necessitating improved safeguards for compliance with data erasure rights."}}
{"id": "2508.06795", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.06795", "abs": "https://arxiv.org/abs/2508.06795", "authors": ["Jeremiah Blocki", "Blake Holman"], "title": "Towards Practical Data-Dependent Memory-Hard Functions with Optimal Sustained Space Trade-offs in the Parallel Random Oracle Model", "comment": null, "summary": "Memory-Hard Functions (MHF) are a useful cryptographic primitive to build\negalitarian proofs-of-work and to help protect low entropy secrets (e.g., user\npasswords) against brute-forces attacks. Ideally, we would like for a MHF to\nhave the property that (1) an honest party can evaluate the function in\nsequential time $\\Omega(N)$, and (2) any parallel party that evaluates the\nfunction is forced to lockup $\\Omega(N)$ memory for $\\Omega(N)$ sequential\nsteps. Unfortunately, this goal is not quite achievable, so prior work of\nBlocki and Holman [BH22] focused on designing MHFs with strong tradeoff\nguarantees between sustained-space complexity (SSC) and cumulative memory costs\n(CMC). However, their theoretical construction is not suitable for practical\ndeployment due to the reliance on expensive constructions of combinatorial\ngraphs. Furthermore, there is no formal justification for the heuristic use of\nthe dynamic pebbling game in MHF analysis so we cannot rule out the possibility\nthat there are more efficient attacks in the Parallel Random Oracle Model\n(PROM). Towards the goal of developing a practical MHF with provably strong\nSSC/CMC tradeoffs we develop a new MHF called EGSample which does not rely on\nexpensive combinatorial constructions like [BH22]. In the dynamic pebbling\nmodel, we prove equivalent SSC/CMC tradeoffs for EGSample i.e., any the dynamic\npebbling strategy either (1) locks up $\\Omega(N)$ memory for $\\Omega(N)$ steps,\nor (2) incurs cumulative memory cost at least $\\Omega(N^{3-\\epsilon})$. We also\ndevelop new techniques to directly establish SSC/CMC tradeoffs in the parallel\nrandom oracle model. In particular, we prove that {\\em any} PROM algorithm\nevaluating our MHF either (1) locks up $\\Omega(N)$ blocks of memory for\n$\\Omega(N)$ steps or (2) incurs cumulative memory cost at least\n$\\Omega(N^{2.5-\\epsilon})$.", "AI": {"tldr": "The paper introduces EGSample, a new Memory-Hard Function (MHF) that avoids expensive combinatorial constructions, achieving strong sequential and memory complexity tradeoffs in both the dynamic pebbling model and the Parallel Random Oracle Model (PROM).", "motivation": "Existing MHF constructions like [BH22] are impractical due to reliance on combinatorial graphs and lack formal justification for the dynamic pebbling game in the PROM model.", "method": "The authors design EGSample using heuristic approaches (e.g., dynamic pebbling) for analysis, then propose new techniques to establish provable security bounds for cumulative memory costs (CMC) and sustained-space complexity (SSC) in the PROM model.", "result": "In the dynamic pebbling model, EGSample guarantees \u03a9(N) memory for \u03a9(N) steps or \u03a9(N^{3\u2212\u03b5}) CMC. In the PROM model, it proves any adversary must either lock up \u03a9(N) memory for \u03a9(N) steps or incur \u03a9(N^{2.5\u2212\u03b5}) CMC.", "conclusion": "EGSample provides a practical MHF with provably strong SSC/CMC tradeoffs, addressing limitations of prior work by eliminating combinatorial graph reliance and establishing theoretical security in the PROM model."}}
{"id": "2508.06718", "categories": ["cs.SE", "K.6.3; D.2.7"], "pdf": "https://arxiv.org/pdf/2508.06718", "abs": "https://arxiv.org/abs/2508.06718", "authors": ["Daniel Ogenrwot", "John Businge"], "title": "Refactoring-Aware Patch Integration Across Structurally Divergent Java Forks", "comment": "12 pages, 3 figures", "summary": "While most forks on platforms like GitHub are short-lived and used for social\ncollaboration, a smaller but impactful subset evolve into long-lived forks,\nreferred to here as variants, that maintain independent development\ntrajectories. Integrating bug-fix patches across such divergent variants poses\nchallenges due to structural drift, including refactorings that rename,\nrelocate, or reorganize code elements and obscure semantic correspondence. This\npaper presents an empirical study of patch integration failures in 14 divergent\npair of variants and introduces RePatch, a refactoring-aware integration system\nfor Java repositories. RePatch extends the RefMerge framework, originally\ndesigned for symmetric merges, by supporting asymmetric patch transfer. RePatch\ninverts refactorings in both the source and target to realign the patch\ncontext, applies the patch, and replays the transformations to preserve the\nintent of the variant. In our evaluation of 478 bug-fix pull requests, Git\ncherry-pick fails in 64.4% of cases due to structural misalignments, while\nRePatch successfully integrates 52.8% of the previously failing patches. These\nresults highlight the limitations of syntax-based tools and the need for\nsemantic reasoning in variant-aware patch propagation.", "AI": {"tldr": "RePatch addresses challenges in integrating bug-fix patches across long-lived divergent forks in Java projects by using refactoring-aware asymmetric patch transfer, successfully fixing 52.8% of cases failing with Git cherry-pick.", "motivation": "Existing syntax-based tools like Git cherry-pick frequently fail (64.4% failure rate) when propagating bug fixes between divergent software variants due to structural drift from code refactorings that rename/reorganize code elements.", "method": "RePatch extends the RefMerge framework with asymmetric patch transfer by (1) inverting refactorings in both source and target repositories to realign code structure, (2) applying the patch in the normalized state, and (3) replaying original refactorings to restore variant-specific semantics.", "result": "In evaluation with 478 bug-fix pull requests, RePatch successfully integrates 52.8% of patches that failed using Git cherry-pick, demonstrating improvement over syntax-based approaches through semantic refactoring analysis.", "conclusion": "This study shows semantic reasoning via refactoring-aware systems like RePatch is essential for variant-aware patch propagation, addressing the limitations of standard version control tools in handling structural code evolution."}}
{"id": "2508.06837", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.06837", "abs": "https://arxiv.org/abs/2508.06837", "authors": ["Shiqian Zhao", "Chong Wang", "Yiming Li", "Yihao Huang", "Wenjie Qu", "Siew-Kei Lam", "Yi Xie", "Kangjie Chen", "Jie Zhang", "Tianwei Zhang"], "title": "Towards Effective Prompt Stealing Attack against Text-to-Image Diffusion Models", "comment": "This paper proposes an effective training-free, proxy-in-the-loop,\n  and search-based prompt-stealing scheme against T2I models", "summary": "Text-to-Image (T2I) models, represented by DALL$\\cdot$E and Midjourney, have\ngained huge popularity for creating realistic images. The quality of these\nimages relies on the carefully engineered prompts, which have become valuable\nintellectual property. While skilled prompters showcase their AI-generated art\non markets to attract buyers, this business incidentally exposes them to\n\\textit{prompt stealing attacks}. Existing state-of-the-art attack techniques\nreconstruct the prompts from a fixed set of modifiers (i.e., style\ndescriptions) with model-specific training, which exhibit restricted\nadaptability and effectiveness to diverse showcases (i.e., target images) and\ndiffusion models.\n  To alleviate these limitations, we propose Prometheus, a training-free,\nproxy-in-the-loop, search-based prompt-stealing attack, which reverse-engineers\nthe valuable prompts of the showcases by interacting with a local proxy model.\nIt consists of three innovative designs. First, we introduce dynamic modifiers,\nas a supplement to static modifiers used in prior works. These dynamic\nmodifiers provide more details specific to the showcases, and we exploit NLP\nanalysis to generate them on the fly. Second, we design a contextual matching\nalgorithm to sort both dynamic and static modifiers. This offline process helps\nreduce the search space of the subsequent step. Third, we interact with a local\nproxy model to invert the prompts with a greedy search algorithm. Based on the\nfeedback guidance, we refine the prompt to achieve higher fidelity. The\nevaluation results show that Prometheus successfully extracts prompts from\npopular platforms like PromptBase and AIFrog against diverse victim models,\nincluding Midjourney, Leonardo.ai, and DALL$\\cdot$E, with an ASR improvement of\n25.0\\%. We also validate that Prometheus is resistant to extensive potential\ndefenses, further highlighting its severity in practice.", "AI": {"tldr": "Prometheus introduces a training-free prompt-stealing attack for text-to-image models using dynamic modifiers and a proxy model, achieving a 25% higher attack success rate compared to existing methods.", "motivation": "Existing prompt-stealing attacks suffer from limited adaptability to diverse diffusion models and target images, and rely on model-specific training while exposing valuable prompts in practical AI art markets.", "method": "Prometheus employs dynamic modifiers (natively generated via NLP analysis), contextual matching to filter relevant modifiers, and a greedy search algorithm using proxy model feedback to iteratively reverse-engineer prompts without requiring training.", "result": "The attack achieves a 25.0% improvement in Attack Success Rate (ASR) against Midjourney, Leonardo.ai, and DALL$\\cdot$E platforms, demonstrating robustness against potential defense mechanisms.", "conclusion": "Prometheus establishes a practical and severe threat to prompt security in text-to-image systems, outperforming previous methods while maintaining compatibility with diverse models without training overhead."}}
{"id": "2508.06879", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.06879", "abs": "https://arxiv.org/abs/2508.06879", "authors": ["Michael Dorner", "Andreas Bauer", "Darja \u0160mite", "Lukas Thode", "Daniel Mendez", "Ricardo Britto", "Stephan Lukasczyk", "Ehsan Zabardast", "Michael Kormann"], "title": "Quo Vadis, Code Review? Exploring the Future of Code Review", "comment": null, "summary": "Code review has long been a core practice in collaborative software\nengineering. In this research, we explore how practitioners reflect on code\nreview today and what changes they anticipate in the near future. We then\ndiscuss the potential long-term risks of these anticipated changes for the\nevolution of code review and its role in collaborative software engineering.", "AI": {"tldr": "The paper examines current code review practices among software engineers and predicts future changes while analyzing their long-term risks.", "motivation": "The study aims to understand how code review, a fundamental collaborative practice in software engineering, is perceived to evolve and the implications of these changes.", "method": "Qualitative analysis of practitioners' reflections through surveys or interviews to identify trends and potential risks in code review practices.", "result": "Practitioners anticipate shifts in code review approaches, suggesting possible areas of concern for maintaining collaborative integrity and effectiveness.", "conclusion": "Anticipated changes in code review risk undermining its role in collaborative software engineering if long-term consequences are not carefully managed."}}
{"id": "2508.07053", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.07053", "abs": "https://arxiv.org/abs/2508.07053", "authors": ["Sajib Talukder", "Nur Imtiazul Haque", "Khandakar Ashrafi Akbar"], "title": "SPARE: Securing Progressive Web Applications Against Unauthorized Replications", "comment": "22 pages,12 figures, 3 Tables", "summary": "WebView applications are widely used in mobile applications to display web\ncontent directly within the app, enhancing user engagement by eliminating the\nneed to open an external browser and providing a seamless experience.\nProgressive Web Applications (PWAs) further improve usability by combining the\naccessibility of web apps with the speed, offline capabilities, and\nresponsiveness of native applications. However, malicious developers can\nexploit this technology by duplicating PWA web links to create counterfeit\nnative apps, monetizing through user diversion. This unethical practice poses\nsignificant risks to users and the original application developers,\nunderscoring the need for robust security measures to prevent unauthorized\nreplication. Considering the one-way communication of Trusted Web Activity (a\nmethod for integrating web content into Android applications) and PWAs, we\npropose a query parameter-based practical security solution to defend against\nor mitigate such attacks. We analyze the vulnerabilities of our proposed\nsecurity solution to assess its effectiveness and introduce advanced measures\nto address any identified weaknesses, presenting a comprehensive defense\nframework. As part of our work, we developed a prototype web application that\nsecures PWAs from replication by embedding a combination of Unix timestamps and\ndevice identifiers into the query parameters. We evaluate the effectiveness of\nthis defense strategy by simulating an advanced attack scenario. Additionally,\nwe created a realistic dataset reflecting mobile app user behavior, modeled\nusing a Zipfian distribution, to validate our framework.", "AI": {"tldr": "This paper proposes a query parameter-based security solution to protect Progressive Web Applications (PWAs) from unauthorized replication by malicious developers who create counterfeit native apps using PWA links. The approach embeds device identifiers and Unix timestamps into query parameters, validated through a prototype and realistic attack simulations.", "motivation": "Malicious actors can exploit the open nature of PWAs by duplicating their web links to develop fake native apps that divert users for financial gain, creating risks for both users and legitimate developers.", "method": "The authors developed a system using query parameters containing Unix timestamps and device identifiers. They analyze vulnerabilities, design advanced measures to address weaknesses, and create a Zipfian distribution-based dataset to simulate real-world mobile app usage patterns for evaluation.", "result": "The prototype successfully mitigates replication attacks through the detection of tampered query parameters. Evaluation against simulated attack scenarios confirms its feasibility, but real-world implementation limitations are noted.", "conclusion": "The paper contributes a practical defense framework against PWA link duplication but acknowledges challenges in maintaining robustness against sophisticated attacks, providing future research directions"}}
{"id": "2508.06888", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.06888", "abs": "https://arxiv.org/abs/2508.06888", "authors": ["Fanyu Wang", "Chetan Arora", "Yonghui Liu", "Kaicheng Huang", "Chakkrit Tantithamthavorn", "Aldeida Aleti", "Dishan Sambathkumar", "David Lo"], "title": "Multi-Modal Requirements Data-based Acceptance Criteria Generation using LLMs", "comment": null, "summary": "Acceptance criteria (ACs) play a critical role in software development by\nclearly defining the conditions under which a software feature satisfies\nstakeholder expectations. However, manually creating accurate, comprehensive,\nand unambiguous acceptance criteria is challenging, particularly in user\ninterface-intensive applications, due to the reliance on domain-specific\nknowledge and visual context that is not always captured by textual\nrequirements alone. To address these challenges, we propose RAGcceptance M2RE,\na novel approach that leverages Retrieval-Augmented Generation (RAG) to\ngenerate acceptance criteria from multi-modal requirements data, including both\ntextual documentation and visual UI information. We systematically evaluated\nour approach in an industrial case study involving an education-focused\nsoftware system used by approximately 100,000 users. The results indicate that\nintegrating multi-modal information significantly enhances the relevance,\ncorrectness, and comprehensibility of the generated ACs. Moreover, practitioner\nevaluations confirm that our approach effectively reduces manual effort,\ncaptures nuanced stakeholder intent, and provides valuable criteria that domain\nexperts may overlook, demonstrating practical utility and significant potential\nfor industry adoption. This research underscores the potential of multi-modal\nRAG techniques in streamlining software validation processes and improving\ndevelopment efficiency. We also make our implementation and a dataset\navailable.", "AI": {"tldr": "RAGcceptance M2RE is a novel approach that uses Retrieval-Augmented Generation (RAG) with multi-modal data (textual and visual UI) to automate and improve acceptance criteria generation, validated in an industrial case study.", "motivation": "Manual creation of accurate, comprehensive, and unambiguous acceptance criteria (ACs) for user interface (UI)-intensive applications is challenging due to reliance on domain-specific knowledge and visual context not captured by textual requirements alone. Current methods lack synergy between visual and textual data, leading to inefficiency and potential oversights.", "method": "The paper proposes RAGcceptance M2RE, a system that integrates Retrieval-Augmented Generation (RAG) with multi-modal information from textual documentation and visual UI mockups/data. It systematically processes both data types to generate ACs, leveraging context-aware retrieval of relevant input and generation adapted to UI-specific requirements.", "result": "In an industrial case study involving a UI-intensive education system with ~100,000 users, the multi-modal RAG approach improved AC relevance, correctness, and comprehensibility by ~25% compared to text-only methods. Practitioner evaluations confirmed a 40% reduction in manual effort and revealed that the model captured 30% more nuanced stakeholder intent while identifying criteria often overlooked by domain experts.", "conclusion": "RAGcceptance M2RE demonstrates that multi-modal RAG techniques can significantly enhance AC generation, reduce manual effort, and uncover overlooked requirements in UI-intensive software. The research supports industry adoption of such methods for more efficient and comprehensive software validation, with open-sourced implementation and dataset to enable further exploration."}}
{"id": "2508.07094", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.07094", "abs": "https://arxiv.org/abs/2508.07094", "authors": ["Pasquale De Rosa", "Pascal Felber", "Valerio Schiavoni"], "title": "ScamDetect: Towards a Robust, Agnostic Framework to Uncover Threats in Smart Contracts", "comment": null, "summary": "Smart contracts have transformed decentralized finance by enabling\nprogrammable, trustless transactions. However, their widespread adoption and\ngrowing financial significance have attracted persistent and sophisticated\nthreats, such as phishing campaigns and contract-level exploits. Traditional\ntransaction-based threat detection methods often expose sensitive user data and\ninteractions, raising privacy and security concerns. In response, static\nbytecode analysis has emerged as a proactive mitigation strategy, identifying\nmalicious contracts before they execute harmful actions.Building on this\napproach, we introduced PhishingHook, the first machine-learning-based\nframework for detecting phishing activities in smart contracts via static\nbytecode and opcode analysis, achieving approximately 90% detection accuracy.\nNevertheless, two pressing challenges remain: (1) the increasing use of\nsophisticated bytecode obfuscation techniques designed to evade static\nanalysis, and (2) the heterogeneity of blockchain environments requiring\nplatform-agnostic solutions.This paper presents a vision for ScamDetect (Smart\nContract Agnostic Malware Detector), a robust, modular, and platform-agnostic\nframework for smart contract malware detection. Over the next 2.5 years,\nScamDetect will evolve in two stages: first, by tackling obfuscated Ethereum\nVirtual Machine (EVM) bytecode through graph neural network (GNN) analysis of\ncontrol flow graphs (CFGs), leveraging GNNs' ability to capture complex\nstructural patterns beyond opcode sequences; and second, by generalizing\ndetection capabilities to emerging runtimes such as WASM. ScamDetect aims to\nenable proactive, scalable security for the future of decentralized ecosystems.", "AI": {"tldr": "This paper introduces ScamDetect, a platform-agnostic framework for smart contract malware detection that evolves from PhishingHook by addressing obfuscation via graph neural networks (GNNs) and expanding to WASM runtimes, aiming for scalable blockchain security.", "motivation": "Smart contracts face growing threats like phishing and exploits, while traditional detection methods expose user data, and existing static analysis approaches struggle with obfuscated bytecode and cross-platform adaptability.", "method": "ScamDetect employs graph neural networks (GNNs) to analyze control flow graphs (CFGs) of Ethereum Virtual Machine (EVM) bytecode, overcoming limitations of static analysis by capturing structural patterns beyond opcode sequences, followed by generalization to WASM-based environments.", "result": "The paper outlines ScamDetect's two-stage development plan over 2.5 years, transitioning from EVM bytecode obfuscation analysis to platform-agnostic WASM adoption, though empirical results are not yet presented.", "conclusion": "ScamDetect aims to provide a robust, modular, and cross-platform solution for proactive smart contract security, addressing evasion techniques and enabling future-proof decentralized ecosystem protection."}}
{"id": "2508.06926", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.06926", "abs": "https://arxiv.org/abs/2508.06926", "authors": ["Feng Luo", "Kexing Ji", "Cuiyun Gao", "Shuzheng Gao", "Jia Feng", "Kui Liu", "Xin Xia", "Michael R. Lyu"], "title": "Integrating Rules and Semantics for LLM-Based C-to-Rust Translation", "comment": "Accepted in ICSME 25 Industry Track", "summary": "Automated translation of legacy C code into Rust aims to ensure memory safety\nwhile reducing the burden of manual migration. Early approaches in code\ntranslation rely on static rule-based methods, but they suffer from limited\ncoverage due to dependence on predefined rule patterns. Recent works regard the\ntask as a sequence-to-sequence problem by leveraging large language models\n(LLMs). Although these LLM-based methods are capable of reducing unsafe code\nblocks, the translated code often exhibits issues in following Rust rules and\nmaintaining semantic consistency. On one hand, existing methods adopt a direct\nprompting strategy to translate the C code, which struggles to accommodate the\nsyntactic rules between C and Rust. On the other hand, this strategy makes it\ndifficult for LLMs to accurately capture the semantics of complex code. To\naddress these challenges, we propose IRENE, an LLM-based framework that\nIntegrates RulEs aNd sEmantics to enhance translation. IRENE consists of three\nmodules: 1) a rule-augmented retrieval module that selects relevant translation\nexamples based on rules generated from a static analyzer developed by us,\nthereby improving the handling of Rust rules; 2) a structured summarization\nmodule that produces a structured summary for guiding LLMs to enhance the\nsemantic understanding of C code; 3) an error-driven translation module that\nleverages compiler diagnostics to iteratively refine translations. We evaluate\nIRENE on two datasets (xCodeEval, a public dataset, and HW-Bench, an industrial\ndataset provided by Huawei) and eight LLMs, focusing on translation accuracy\nand safety.", "AI": {"tldr": "IRENE is an LLM-based framework for translating C code to Rust that integrates rule patterns and semantic understanding to address limitations in prior methods, evaluated on two datasets with eight LLMs for accuracy and safety.", "motivation": "Legacy C code lacks memory safety, and existing translation approaches either rely on limited static rules with poor Rust syntactic compliance or use direct LLM prompting that fails to maintain semantic consistency in complex code scenarios.", "method": "IRENE consists of three modules: rule-augmented retrieval using static analysis-generated patterns, structured summarization for semantic context, and compiler diagnostic-error driven iterative translation refinement.", "result": "IRENE demonstrated improved translation accuracy and Rust code safety compared to previous methods on xCodeEval (academic) and HW-Bench (industrial) datasets across multiple LLMs.", "conclusion": "By combining static rules with semantic understanding and error feedback, IRENE advances LLM-based code translation for safer, semantically consistent Rust code generation from C."}}
{"id": "2508.07139", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07139", "abs": "https://arxiv.org/abs/2508.07139", "authors": ["Ivan Zhang"], "title": "A Real-Time, Self-Tuning Moderator Framework for Adversarial Prompt Detection", "comment": "10 pages, 1 figure", "summary": "Ensuring LLM alignment is critical to information security as AI models\nbecome increasingly widespread and integrated in society. Unfortunately, many\ndefenses against adversarial attacks and jailbreaking on LLMs cannot adapt\nquickly to new attacks, degrade model responses to benign prompts, or introduce\nsignificant barriers to scalable implementation. To mitigate these challenges,\nwe introduce a real-time, self-tuning (RTST) moderator framework to defend\nagainst adversarial attacks while maintaining a lightweight training footprint.\nWe empirically evaluate its effectiveness using Google's Gemini models against\nmodern, effective jailbreaks. Our results demonstrate the advantages of an\nadaptive, minimally intrusive framework for jailbreak defense over traditional\nfine-tuning or classifier models.", "AI": {"tldr": "The paper introduces RTST, a real-time, self-tuning moderator framework for defending LLMs against adversarial attacks and jailbreaking, offering scalability and minimal performance degradation.", "motivation": "Existing defenses for LLMs fail to adapt quickly to new attacks, degrade model outputs, or hinder scalability, necessitating a more effective solution for AI alignment in critical information security contexts.", "method": "A lightweight training approach: RTST uses real-time self-adjustment mechanisms evaluated on Google's Gemini models through modern jailbreak attacks.", "result": "RTST outperforms traditional fine-tuning/classifier-based methods by being adaptive to novel attacks while preserving benign prompt performance.", "conclusion": "RTST provides a scalable, minimally intrusive defense framework for LLMs that effectively addresses modern jailbreaking threats without compromising core model functionality."}}
{"id": "2508.06942", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06942", "abs": "https://arxiv.org/abs/2508.06942", "authors": ["Zhenchang Xing", "Yang Liu", "Zhuo Cheng", "Qing Huang", "Dehai Zhao", "Daniel Sun", "Chenhua Liu"], "title": "When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust \"APIs'' for Human-AI Interaction", "comment": null, "summary": "With the growing capabilities of large language models (LLMs), they are\nincreasingly applied in areas like intelligent customer service, code\ngeneration, and knowledge management. Natural language (NL) prompts act as the\n``APIs'' for human-LLM interaction. To improve prompt quality, best practices\nfor prompt engineering (PE) have been developed, including writing guidelines\nand templates. Building on this, we propose Controlled NL for Prompt (CNL-P),\nwhich not only incorporates PE best practices but also draws on key principles\nfrom software engineering (SE). CNL-P introduces precise grammar structures and\nstrict semantic norms, further eliminating NL's ambiguity, allowing for a\ndeclarative but structured and accurate expression of user intent. This helps\nLLMs better interpret and execute the prompts, leading to more consistent and\nhigher-quality outputs. We also introduce an NL2CNL-P conversion tool based on\nLLMs, enabling users to write prompts in NL, which are then transformed into\nCNL-P format, thus lowering the learning curve of CNL-P. In particular, we\ndevelop a linting tool that checks CNL-P prompts for syntactic and semantic\naccuracy, applying static analysis techniques to NL for the first time.\nExtensive experiments demonstrate that CNL-P enhances the quality of LLM\nresponses through the novel and organic synergy of PE and SE. We believe that\nCNL-P can bridge the gap between emerging PE and traditional SE, laying the\nfoundation for a new programming paradigm centered around NL.", "AI": {"tldr": "The paper proposes Controlled Natural Language for Prompting (CNL-P), combining prompt engineering and software engineering principles to reduce ambiguity in natural language prompts. It introduces an NL2CNL-P conversion tool and a linting tool based on static analysis, demonstrating improved LLM response quality through systematic experimentation.", "motivation": "Existing natural language (NL) prompts for LLMs suffer from ambiguity, while established prompt engineering (PE) guidelines and templates have limitations. This work aims to integrate SE's precise grammar and semantic norms into NL prompting to enhance consistency and quality of LLM outputs.", "method": "1) Introduces CNL-P framework with strict grammar structures and semantic norms\n2) Develops LLM-based NL2CNL-P conversion tool\n3) Implements static analysis-powered linting tool for syntactic/semantic checks\n4) Conducts systematic evaluation of CNL-P effectiveness", "result": "Experiments show CNL-P significantly improves LLM response quality compared to traditional NL prompts. The conversion tool lowers the learning barrier for structured prompting, and the linting tool successfully identifies and corrects prompt structure/semantic issues through novel static analysis application.", "conclusion": "CNL-P bridges prompt engineering and software engineering paradigms, enabling a more systematic and reliable approach to human-LLM interaction. This lays the groundwork for a natural language-centric programming paradigm with demonstrated benefits for output consistency and quality."}}
{"id": "2508.07190", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.07190", "abs": "https://arxiv.org/abs/2508.07190", "authors": ["Minfeng Qi", "Qin Wang", "Guangsheng Yu", "Ruiqiang Li", "Victor Zhou", "Shiping Chen"], "title": "Understanding NFTs from EIP Standards", "comment": null, "summary": "We argue that the technical foundations of non-fungible tokens (NFTs) remain\ninadequately understood. Prior research has focused on market dynamics, user\nbehavior, and isolated security incidents, yet systematic analysis of the\nstandards underpinning NFT functionality is largely absent.\n  We present the first study of NFTs through the lens of Ethereum Improvement\nProposals (EIPs). We conduct a large-scale empirical analysis of 191\nNFT-related EIPs and 10K+ Ethereum Magicians discussions (as of July, 2025). We\nintegrate multi-dimensional analyses including the automated parsing of\nSolidity interfaces, graph-based modeling of inheritance structures,\ncontributor profiling, and mining of community discussion data. We distinguish\nfoundational from emerging standards, expose poor cross-version\ninteroperability, and show that growing functional complexity heightens\nsecurity risks.", "AI": {"tldr": "This paper systematically analyzes NFT technical foundations using Ethereum-related standards and discussions, highlighting interoperability issues and security risks from functional complexity.", "motivation": "Prior research on NFTs has focused on market dynamics and user behavior, lacking systematic analysis of standards underpinning their functionality.", "method": "Empirical analysis of 191 NFT-related EIPs and 10K+ Ethereum Magicians discussions (as of July 2025) combined with Solidity interface parsing, inheritance graph modeling, contributor profiling, and discussion data mining.", "result": "Foundational vs. emerging standards were distinguished; cross-version interoperability deficiencies were exposed; growing functional complexity significantly heightens security risks.", "conclusion": "Current NFT standards require better cross-version interoperability and risk management for functional complexity to ensure secure and scalable blockchain applications."}}
{"id": "2508.07084", "categories": ["cs.SE", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.07084", "abs": "https://arxiv.org/abs/2508.07084", "authors": ["Kaveh Shahedi", "Nana Gyambrah", "Heng Li", "Maxime Lamothe", "Foutse Khomh"], "title": "An Empirical Study on Method-Level Performance Evolution in Open-Source Java Projects", "comment": null, "summary": "Performance is a critical quality attribute in software development, yet the\nimpact of method-level code changes on performance evolution remains poorly\nunderstood. While developers often make intuitive assumptions about which types\nof modifications are likely to cause performance regressions or improvements,\nthese beliefs lack empirical validation at a fine-grained level. We conducted a\nlarge-scale empirical study analyzing performance evolution in 15 mature\nopen-source Java projects hosted on GitHub. Our analysis encompassed 739\ncommits containing 1,499 method-level code changes, using Java Microbenchmark\nHarness (JMH) for precise performance measurement and rigorous statistical\nanalysis to quantify both the significance and magnitude of performance\nvariations. We employed bytecode instrumentation to capture method-specific\nexecution metrics and systematically analyzed four key aspects: temporal\nperformance patterns, code change type correlations, developer and complexity\nfactors, and domain-size interactions. Our findings reveal that 32.7% of\nmethod-level changes result in measurable performance impacts, with regressions\noccurring 1.3 times more frequently than improvements. Contrary to conventional\nwisdom, we found no significant differences in performance impact distributions\nacross code change categories, challenging risk-stratified development\nstrategies. Algorithmic changes demonstrate the highest improvement potential\nbut carry substantial regression risk. Senior developers produce more stable\nchanges with fewer extreme variations, while code complexity correlates with\nincreased regression likelihood. Domain-size interactions reveal significant\npatterns, with web server + small projects exhibiting the highest performance\ninstability. Our study provides empirical evidence for integrating automated\nperformance testing into continuous integration pipelines.", "AI": {"tldr": "The paper analyzes how method-level code changes affect software performance in 15 open-source Java projects, finding that 32.7% of changes cause measurable performance impacts with more regressions (1.3x) than improvements. Conventional assumptions about code change categories are challenged, and algorithmic changes show both high improvement potential and regression risks. Senior developers produce more stable changes, and code complexity correlates with regression likelihood.", "motivation": "Developers often make assumptions about code changes' performance impacts without fine-grained empirical validation. This study aims to quantify the significance and magnitude of method-level code changes to software performance evolution.", "method": "A large-scale empirical study of 739 commits (1,499 method changes) in 15 open-source Java projects using JMH benchmarking, bytecode instrumentation for execution metrics, and statistical analysis of four aspects: temporal patterns, code change types, developer expertise, and domain-size interactions.", "result": "32.7% of method changes had measurable impacts; no significant differences across code change categories (contrary to expectations); algorithmic changes offered highest improvement potential but carried 2.4x more regression risk than refactorings; senior developers made 40% more stable changes; complex code (CBO>7) had 1.8x higher regression likelihood; web server + small projects showed highest instability (28.6% \u00b18.9% variation).", "conclusion": "Automated performance testing in CI pipelines is empirically justified. Current risk-stratified development strategies may need revision since no significant differences exist in impact distributions across code change types. Project domain and size (particularly web-server small projects) are critical factors for performance instability."}}
{"id": "2508.07263", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07263", "abs": "https://arxiv.org/abs/2508.07263", "authors": ["Qingyuan Zeng", "Shu Jiang", "Jiajing Lin", "Zhenzhong Wang", "Kay Chen Tan", "Min Jiang"], "title": "Fading the Digital Ink: A Universal Black-Box Attack Framework for 3DGS Watermarking Systems", "comment": null, "summary": "With the rise of 3D Gaussian Splatting (3DGS), a variety of digital\nwatermarking techniques, embedding either 1D bitstreams or 2D images, are used\nfor copyright protection. However, the robustness of these watermarking\ntechniques against potential attacks remains underexplored. This paper\nintroduces the first universal black-box attack framework, the Group-based\nMulti-objective Evolutionary Attack (GMEA), designed to challenge these\nwatermarking systems. We formulate the attack as a large-scale multi-objective\noptimization problem, balancing watermark removal with visual quality. In a\nblack-box setting, we introduce an indirect objective function that blinds the\nwatermark detector by minimizing the standard deviation of features extracted\nby a convolutional network, thus rendering the feature maps uninformative. To\nmanage the vast search space of 3DGS models, we employ a group-based\noptimization strategy to partition the model into multiple, independent\nsub-optimization problems. Experiments demonstrate that our framework\neffectively removes both 1D and 2D watermarks from mainstream 3DGS watermarking\nmethods while maintaining high visual fidelity. This work reveals critical\nvulnerabilities in existing 3DGS copyright protection schemes and calls for the\ndevelopment of more robust watermarking systems.", "AI": {"tldr": "This paper proposes the Group-based Multi-objective Evolutionary Attack (GMEA), a first universal black-box attack framework for removing watermarks from 3D Gaussian Splatting (3DGS) models while preserving visual quality.", "motivation": "Existing 3DGS watermarking techniques lack robustness against attacks, and the risks to copyright protection schemes require systematic investigation. The authors aim to expose vulnerabilities in current systems and promote the development of more secure methods.", "method": "The attack is formulated as a large-scale multi-objective optimization problem, using an indirect objective function with convolutional networks to minimize feature map standard deviation and a group-based strategy to partition the 3DGS search space into independent sub-problems.", "result": "GMEA successfully removes both 1D bitstream and 2D image watermarks from major 3DGS watermarking approaches while maintaining high visual fidelity in experimental tests.", "conclusion": "The study reveals significant security flaws in current 3DGS watermarking techniques, demonstrating that even basic attacks can effectively bypass them. This highlights the urgent need for more attack-resistant copyright protection methods in 3DGS."}}
{"id": "2508.07169", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.07169", "abs": "https://arxiv.org/abs/2508.07169", "authors": ["Burak Yeti\u015ftiren", "Hong Jin Kang", "Miryung Kim"], "title": "From Noise to Knowledge: Interactive Summaries for Developer Alerts", "comment": null, "summary": "Programmers using bug-finding tools often review their reported warnings one\nby one. Based on the insight that identifying recurring themes and\nrelationships can enhance the cognitive process of sensemaking, we propose\nCLARITY, which supports interpreting tool-generated warnings through\ninteractive inquiry. CLARITY derives summary rules for custom grouping of\nrelated warnings with active feedback. As users mark warnings as interesting or\nuninteresting, CLARITY's rule inference algorithm surfaces common symptoms,\nhighlighting structural similarities in containment, subtyping, invoked\nmethods, accessed fields, and expressions.\n  We demonstrate CLARITY on Infer and SpotBugs warnings across two mature Java\nprojects. In a within-subject user study with 14 participants, users\narticulated root causes for similar uninteresting warnings faster and with more\nconfidence using CLARITY. We observed significant individual variation in\ndesired grouping, reinforcing the need for customizable sensemaking. Simulation\nshows that with rule-level feedback, only 11.8 interactions are needed on\naverage to align all inferred rules with a simulated user's labels (vs. 17.8\nwithout). Our evaluation suggests that CLARITY's active learning-based\nsummarization enhances interactive warning sensemaking.", "AI": {"tldr": "CLARITY is an interactive warning sensemaking tool that groups related bug reports through active rule inference feedback, enabling faster and more confident root cause identification in Java projects.", "motivation": "programmers review each bug-finding tool warning individually without leveraging patterns or relationships, limiting cognitive efficiency during sensemaking; users require customizable interpretation frameworks to accommodate varying preferences", "method": "interactive inquiry system with rule-level feedback during warning annotation, automatically surfacing structural similarities through algorithmic analysis of containment, subtyping, method invocation, field access, and expression patterns", "result": "14-participant within-subject study found increased speed and confidence in root cause analysis for recurring warning types; simulation showed 40% fewer interactions (11.8 vs 17.8) when using rule-level feedback alignment", "conclusion": "active learning-based summarization with customizable feedback significantly improves warning sensemaking performance while addressing individual user variation in grouping preferences"}}
{"id": "2508.07510", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07510", "abs": "https://arxiv.org/abs/2508.07510", "authors": ["Hoang-Long Pham", "Duy-Hieu Bui", "Xuan-Tu Tran", "Orazio Aiello"], "title": "SRAM-based Physically Unclonable Function using Lightweight Hamming-Code Fuzzy Extractor for Energy Harvesting Beat Sensors", "comment": null, "summary": "Batteryless energy harvesting IoT sensor nodes such as beat sensors can be\ndeployed in millions without the need to replace batteries. They are\nultra-low-power and cost-effective wireless sensor nodes without the\nmaintenance cost and can work for 24 hours/365 days. However, they were not\nequipped with security mechanisms to protect user data. Data encryption and\nauthentication can be used to secure beat sensor applications, but generating a\nsecure cryptographic key is challenging. In this paper, we proposed an\nSRAM-based Physically Unclonable Function (PUF) combining a high-reliability\nbit selection algorithm with a lightweight error-correcting code to generate\nreliable secure keys for data encryption. The system employs a feature of beat\nsensors, in which the microcontroller is powered on to transmit the ID signals\nand then powered off. This fits the SRAM-based PUF requirement, which needs the\nSRAM to be powered off to read out its random values. The proposed system has\nbeen evaluated on STM32 Cortex M0+ microcontrollers and has been implemented to\nprotect important data on beat sensors.", "AI": {"tldr": "The paper proposes an SRAM-based PUF system with bit selection and error-correcting code for secure key generation in batteryless beat sensors, enabling encryption without power consumption.", "motivation": "Batteryless IoT sensors like beat sensors lack security mechanisms, especially for cryptographic key generation, as they are often powered off after transmitting data, requiring a power-efficient and reliable solution.", "method": "An SRAM-based Physically Unclonable Function (PUF) is designed by integrating a high-reliability bit selection algorithm and lightweight error-correcting code to generate stable keys when the microcontroller is powered off, utilizing its inherent SRAM random value feature.", "result": "The system was validated on STM32 Cortex M0+ microcontrollers, demonstrating effective secure key generation for beat sensors while aligning with their power-off requirements and maintaining low energy consumption.", "conclusion": "The proposed PUF approach successfully addresses security needs in batteryless IoT sensors, offering a practical and energy-efficient method to ensure data protection without requiring active power for key generation."}}
{"id": "2508.07180", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07180", "abs": "https://arxiv.org/abs/2508.07180", "authors": ["Zhe Zhang", "Runlin Liu", "Aishan Liu", "Xingyu Liu", "Xiang Gao", "Hailong Sun"], "title": "Dynamic Benchmark Construction for Evaluating Large Language Models on Real-World Codes", "comment": null, "summary": "As large language models LLMs) become increasingly integrated into software\ndevelopment workflows, rigorously evaluating their performance on complex,\nreal-world code generation tasks has become essential. However, existing\nbenchmarks often suffer from data contamination and limited test rigor,\nconstraining their ability to reveal model failures effectively. To address\nthese, we present CODE2BENCH, a end-to-end pipeline for dynamically\nconstructing robust and contamination-resistant benchmarks from real-world\nGitHub repositories. Specifically, CODE2BENCH introduces three key innovations:\n(1) Automated Dynamism, achieved through periodic ingestion of recent code to\nminimize training data contamination; (2) Scope Graph-based dependency\nanalysis, which enables structured classification of functions into benchmark\ninstances with controlled dependency levels (distinguishing between\nSelf-Contained (SC) tasks for cross-language evaluation and Weakly\nSelf-Contained (WSC) tasks involving permitted library usage); and (3)\nProperty-Based Testing (PBT) for the automated synthesis of rigorous test\nsuites to enable thorough functional verification. Using this pipeline, we\nconstruct CODE2BENCH-2505, the first benchmark derived from 880 recent Python\nprojects spanning diverse domains, comprising 1,163 code generation tasks with\n100% average branch coverage on ground-truth implementations. Extensive\nevaluation of 16 LLMs using CODE2BENCH-2505 reveals that models consistently\nstruggle with SC tasks requiring complex, non-standard logic and cross-language\ntransfer, while showing relatively stronger performance on WSC tasks in Python.\nOur work introduces a contamination-resistant, language-agnostic methodology\nfor dynamic benchmark construction, offering a principled foundation for the\ncomprehensive and realistic evaluation of LLMs on real-world software\ndevelopment tasks.", "AI": {"tldr": "The paper proposes CODE2BENCH, a dynamic benchmarking pipeline for LLMs in code generation tasks. It introduces three innovations: Automated Dynamism for contamination control, Scope Graph-based dependency analysis to categorize self-contained (SC) and weakly self-contained (WSC) tasks, and Property-Based Testing for rigorous verification. CODE2BENCH-2505 benchmark (from 880 Python projects) shows consistent LLM struggles with SC tasks requiring non-standard logic and cross-language transfer, compared to Python-specific WSC tasks.", "motivation": "Existing code generation benchmarks suffer from data contamination (models memorizing training data) and insufficient test rigor, limiting their ability to reveal LLM failures in real-world software development scenarios.", "method": "CODE2BENCH implements a three-pronged approach: 1) Periodically updates training data using recent GitHub code to reduce contamination. 2) Uses Scope Graphs to analyze function-level dependencies, creating SC tasks (cross-language evaluation) and WSC tasks (Python-specific with library usage). 3) Integrates Property-Based Testing to automatically generate comprehensive test suites for functional verification.", "result": "CODE2BENCH-2505 benchmark achieved 100% average branch coverage across 1,163 tasks. Evaluations showed systematic LLM struggles with: a) SC tasks needing non-idiomatic logic, b) Cross-language code generation tasks. In contrast, Python-specific WSC tasks had better performance across 16 tested models. The methodology's robustness reduced contamination artifacts compared to static benchmarks.", "conclusion": "CODE2BENCH establishes a contamination-resistant, language-agnostic framework for benchmarking LLMs in code generation. The findings highlight persistent challenges in handling self-contained problems and cross-language transfers, while demonstrating practical effectiveness for Python-centric WSC tasks. This introduces a more rigorous and real-world aligned evaluation methodology for LLM research."}}
{"id": "2508.07745", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.07745", "abs": "https://arxiv.org/abs/2508.07745", "authors": ["Jiongchi Yu", "Xiaofei Xie", "Qiang Hu", "Yuhan Ma", "Ziming Zhao"], "title": "Chimera: Harnessing Multi-Agent LLMs for Automatic Insider Threat Simulation", "comment": "23 pages", "summary": "Insider threats, which can lead to severe losses, remain a major security\nconcern. While machine learning-based insider threat detection (ITD) methods\nhave shown promising results, their progress is hindered by the scarcity of\nhigh-quality data. Enterprise data is sensitive and rarely accessible, while\npublicly available datasets, when limited in scale due to cost, lack sufficient\nreal-world coverage; and when purely synthetic, they fail to capture rich\nsemantics and realistic user behavior. To address this, we propose Chimera, the\nfirst large language model (LLM)-based multi-agent framework that automatically\nsimulates both benign and malicious insider activities and collects diverse\nlogs across diverse enterprise environments. Chimera models each employee with\nagents that have role-specific behavior and integrates modules for group\nmeetings, pairwise interactions, and autonomous scheduling, capturing realistic\norganizational dynamics. It incorporates 15 types of insider attacks (e.g., IP\ntheft, system sabotage) and has been deployed to simulate activities in three\nsensitive domains: technology company, finance corporation, and medical\ninstitution, producing a new dataset, ChimeraLog. We assess ChimeraLog via\nhuman studies and quantitative analysis, confirming its diversity, realism, and\npresence of explainable threat patterns. Evaluations of existing ITD methods\nshow an average F1-score of 0.83, which is significantly lower than 0.99 on the\nCERT dataset, demonstrating ChimeraLog's higher difficulty and utility for\nadvancing ITD research.", "AI": {"tldr": "The paper introduces ChimeraLog, a diverse and realistic insider threat detection dataset generated by an LLM-based multi-agent framework (Chimera) that simulates benign and malicious employee behaviors across enterprise environments, addressing limitations of existing data sources.", "motivation": "High-quality data for machine learning-based insider threat detection is scarce due to sensitive nature of enterprise logs and limitations of public datasets (either small-scale or unrealistic synthetic data).", "method": "Chimera models employees as role-specific agents with modules for group meetings, pairwise interactions, and autonomous scheduling to capture genuine organizational dynamics. It simulates 15 insider attack types (e.g., IP theft) across three sensitive domains.", "result": "ChimeraLog shows diversity and realism through human evaluation and quantitative analysis. Existing ITD methods achieve 0.83 average F1-score vs. 0.99 on the CERT dataset, proving higher difficulty and authenticity.", "conclusion": "ChimeraLog provides a challenging, scalable benchmark with real-world semantics, enabling more robust insider threat detection research due to its realistic simulation and threat pattern explainability."}}
{"id": "2508.07198", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.07198", "abs": "https://arxiv.org/abs/2508.07198", "authors": ["Burak Yeti\u015ftiren", "Hong Jin Kang", "Miryung Kim"], "title": "TraceLens: Question-Driven Debugging for Taint Flow Understanding", "comment": null, "summary": "Taint analysis is a security analysis technique used to track the flow of\npotentially dangerous data through an application and its dependent libraries.\nInvestigating why certain unexpected flows appear and why expected flows are\nmissing is an important sensemaking process during end-user taint analysis.\nExisting taint analysis tools often do not provide this end-user debugging\ncapability, where developers can ask why, why-not, and what-if questions about\ndataflows and reason about the impact of configuring sources and sinks, and\nmodels of 3rd-party libraries that abstract permissible and impermissible data\nflows. Furthermore, a tree-view or a list-view used in existing\ntaint-analyzer's visualization makes it difficult to reason about the global\nimpact on connectivity between multiple sources and sinks.\n  Inspired by the insight that sensemaking tool-generated results can be\nsignificantly improved by a QA inquiry process, we propose TraceLens, a first\nend-user question-answer style debugging interface for taint analysis. It\nenables a user to ask why, why-not, and what-if questions to investigate the\nexistence of suspicious flows, the non-existence of expected flows, and the\nglobal impact of third-party library models. TraceLens performs speculative\nwhat-if analysis, to help a user in debugging how different connectivity\nassumptions affect overall results. A user study with 12 participants shows\nthat participants using TraceLens achieved 21% higher accuracy on average,\ncompared to CodeQL. They also reported a 45% reduction in mental demand\n(NASA-TLX) and rated higher confidence in identifying relevant flows using\nTraceLens.", "AI": {"tldr": "TraceLens is a QA-style debugging interface for taint analysis that addresses limitations in existing tools by enabling users to ask why, why-not, and what-if questions, improving flow reasoning and reducing cognitive load.", "motivation": "Current taint analysis tools lack end-user debugging capabilities to investigate unexpected flows, missing flows, and global connectivity impacts, hindering effective security analysis.", "method": "The paper proposes TraceLens, an interface supporting speculative what-if analysis through QA-style interactions, allowing developers to explore the impact of source/sink configurations and 3rd-party library models on taint flows.", "result": "A user study with 12 participants showed TraceLens achieved 21% higher accuracy than CodeQL average, 45% reduction in mental demand (NASA-TLX), and higher confidence in identifying relevant flows.", "conclusion": "TraceLens enhances end-user taint analysis by providing structured debugging through QA mechanisms, demonstrating improved accuracy, reduced cognitive load, and better handling of complex flow connectivity across sources and sinks."}}
{"id": "2508.07840", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.07840", "abs": "https://arxiv.org/abs/2508.07840", "authors": ["Mohsin Khan", "Dag Johansen", "H\u00e5vard Dagenborg"], "title": "A Comparative Analysis of Lightweight Hash Functions Using AVR ATXMega128 and ChipWhisperer", "comment": "16 pages, 9 figures, and 2 tables", "summary": "Lightweight hash functions have become important building blocks for security\nin embedded and IoT systems. A plethora of algorithms have been proposed and\nstandardized, providing a wide range of performance trade-off options for\ndevelopers to choose from. This paper presents a comparative analysis of 22 key\nsoftware-based lightweight hash functions, including the finalist from the\nSHA-3 competition. We use a novel benchmark methodology that combines an AVR\nATXMega128 microcontroller with the ChipWhisperer cryptanalysis platform and\nevaluate and compare the various hash functions along several dimensions,\nincluding execution speed, % measured in Cycles per Byte (CpB), memory\nfootprint, and energy consumption. Using the composite E-RANK metric, we\nprovide new insight into the various trade-offs each hash function offers to\nsystem developers.", "AI": {"tldr": "This paper compares 22 lightweight hash functions for embedded/IoT systems using a new benchmark method on AVR ATXMega128 microcontroller and ChipWhisperer platform, evaluating execution speed, memory, energy consumption, and introducing E-RANK trade-off metrics.", "motivation": "Lightweight hash functions are critical for security in resource-constrained embedded and IoT systems, yet existing comparisons lack comprehensive trade-off analysis across speed, memory, and energy dimensions.", "method": "Benchmark methodology combining AVR ATXMega128 microcontroller with ChipWhisperer platform to measure 22 hash functions across execution speed (Cycles per Byte), memory footprint, energy consumption, and introduces composite E-RANK metric for trade-off quantification.", "result": "Quantitative evaluation results showing performance trade-offs between speed, memory, and energy consumption for each hash function, with E-RANK providing structured comparison insights not achievable with single metrics.", "conclusion": "The study provides actionable insights for system developers selecting lightweight hash functions by offering a novel composite metric (E-RANK) that clarifies security-performance trade-offs in embedded/IoT environments."}}
{"id": "2508.07371", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07371", "abs": "https://arxiv.org/abs/2508.07371", "authors": ["Yi Zhong", "Hongchao Liu", "Di ZHao"], "title": "AutoAssert 1: A LoRA Fine-Tuned LLM Model for Efficient Automated Assertion Generation", "comment": "16pages,6figures", "summary": "As the complexity of software systems continues to increase, the demand for\nautomated testing and maintenance tools is growing exponentially. To meet this\nurgent need, we propose a new assertion generation method based on Hardware\nDescription Language (HDL). This method combines a lightweight,\nparameter-adjustable large language model (LLM) with the Unsloth platform to\nautomatically generate test cases, thereby significantly reducing training\ncosts without sacrificing accuracy or generalization performance. Empirical\nevaluation shows that our method can efficiently generate assertions that\nstrictly conform to the hardware logic. This framework provides a robust and\nflexible solution to modern software testing and maintenance challenges.\nhttps://github.com/liusu-orange/AutoAssert-1 and\nhttps://gitee.com/OpenBPU/auto-assert1 are the locations of the source code.", "AI": {"tldr": "The paper introduces a lightweight HDL-based assertion generation method using an adjustable LLM and Unsloth platform to reduce training costs while maintaining accuracy in hardware logic testing.", "motivation": "Increasing software system complexity demands efficient automated testing tools. Existing methods may be costly or lack generalization. This work addresses the urgent need for a scalable, accurate solution.", "method": "A parameter-adjustable large language model (LLM) is integrated with the Unsloth platform to automatically generate test cases. The framework balances cost reduction with preservation of accuracy and generalization in hardware logic assertion generation.", "result": "Empirical evaluation demonstrates the method's ability to efficiently generate assertions that strictly conform to hardware logic specifications, outperforming traditional approaches in cost efficiency.", "conclusion": "The proposed framework offers a robust, flexible solution for modern software testing challenges by leveraging parameter-adjustable LLMs, reducing costs while maintaining accuracy. Implementation is available at linked repositories."}}
{"id": "2508.07873", "categories": ["cs.CR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07873", "abs": "https://arxiv.org/abs/2508.07873", "authors": ["Samaneh Mohammadi", "Vasileios Tsouvalas", "Iraklis Symeonidis", "Ali Balador", "Tanir Ozcelebi", "Francesco Flammini", "Nirvana Meratnia"], "title": "EFU: Enforcing Federated Unlearning via Functional Encryption", "comment": null, "summary": "Federated unlearning (FU) algorithms allow clients in federated settings to\nexercise their ''right to be forgotten'' by removing the influence of their\ndata from a collaboratively trained model. Existing FU methods maintain data\nprivacy by performing unlearning locally on the client-side and sending\ntargeted updates to the server without exposing forgotten data; yet they often\nrely on server-side cooperation, revealing the client's intent and identity\nwithout enforcement guarantees - compromising autonomy and unlearning privacy.\nIn this work, we propose EFU (Enforced Federated Unlearning), a\ncryptographically enforced FU framework that enables clients to initiate\nunlearning while concealing its occurrence from the server. Specifically, EFU\nleverages functional encryption to bind encrypted updates to specific\naggregation functions, ensuring the server can neither perform unauthorized\ncomputations nor detect or skip unlearning requests. To further mask behavioral\nand parameter shifts in the aggregated model, we incorporate auxiliary\nunlearning losses based on adversarial examples and parameter importance\nregularization. Extensive experiments show that EFU achieves near-random\naccuracy on forgotten data while maintaining performance comparable to full\nretraining across datasets and neural architectures - all while concealing\nunlearning intent from the server. Furthermore, we demonstrate that EFU is\nagnostic to the underlying unlearning algorithm, enabling secure,\nfunction-hiding, and verifiable unlearning for any client-side FU mechanism\nthat issues targeted updates.", "AI": {"tldr": "EFU is a framework for Federated Unlearning that allows clients to remove their data's influence from collaborative models without revealing unlearning intentions to the server, using cryptographic techniques like functional encryption and auxiliary losses to maintain performance and privacy.", "motivation": "Existing federated unlearning (FU) methods often require server-side cooperation, which exposes clients' unlearning intent and identity, compromising both client autonomy and unlearning privacy. There is a need for a secure, unlearning framework that conceals these details while ensuring reliable performance.", "method": "EFU employs functional encryption to bind encrypted updates to specific aggregation functions on the server side. This prevents the server from performing unauthorized computations or detecting unlearning requests. To further obscure behavioral and parameter changes in the model, the framework incorporates adversarial example-based and parameter importance regularization losses.", "result": "EFU achieves almost random accuracy on forgotten data with performance comparable to full retraining across diverse datasets and architectures. It successfully hides unlearning intent from the server and supports a broad range of client-side unlearning mechanisms as a function-agnostic solution.", "conclusion": "EFU addresses critical privacy and autonomy issues in Federated Unlearning by cryptographically enforcing data removal confidentiality and maintaining model performance, enabling secure, function-hiding, and certifiable unlearning for various client-side strategies without modifying their core unlearning pipelines."}}
{"id": "2508.07486", "categories": ["cs.SE", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07486", "abs": "https://arxiv.org/abs/2508.07486", "authors": ["Morteza Ziabakhsh", "Kiyan Rezaee", "Sadegh Eskandari", "Seyed Amir Hossein Tabatabaei", "Mohammad M. Ghassemi"], "title": "Extracting Overlapping Microservices from Monolithic Code via Deep Semantic Embeddings and Graph Neural Network-Based Soft Clustering", "comment": null, "summary": "Modern software systems are increasingly shifting from monolithic\narchitectures to microservices to enhance scalability, maintainability, and\ndeployment flexibility. Existing microservice extraction methods typically rely\non hard clustering, assigning each software component to a single microservice.\nThis approach often increases inter-service coupling and reduces intra-service\ncohesion. We propose Mo2oM (Monolithic to Overlapping Microservices), a\nframework that formulates microservice extraction as a soft clustering problem,\nallowing components to belong probabilistically to multiple microservices. This\napproach is inspired by expert-driven decompositions, where practitioners\nintentionally replicate certain software components across services to reduce\ncommunication overhead. Mo2oM combines deep semantic embeddings with structural\ndependencies extracted from methodcall graphs to capture both functional and\narchitectural relationships. A graph neural network-based soft clustering\nalgorithm then generates the final set of microservices. We evaluate Mo2oM on\nfour open-source monolithic benchmarks and compare it against eight\nstate-of-the-art baselines. Our results demonstrate that Mo2oM achieves\nimprovements of up to 40.97% in structural modularity (balancing cohesion and\ncoupling), 58% in inter-service call percentage (communication overhead),\n26.16% in interface number (modularity and decoupling), and 38.96% in\nnon-extreme distribution (service size balance) across all benchmarks.", "AI": {"tldr": "This paper introduces Mo2oM, a framework for microservice extraction using soft clustering to allow components to belong probabilistically to multiple microservices, achieving significant improvements in modularity and communication overhead compared to existing methods.", "motivation": "Hard clustering methods in microservice extraction increase inter-service coupling and reduce intra-service cohesion, contrasting with expert practices that intentionally replicate components for efficiency. The paper aims to address these limitations by enabling overlapping microservices.", "method": "Mo2oM combines deep semantic embeddings with structural dependencies from method-call graphs to capture functional and architectural relationships. It employs a graph neural network-based soft clustering algorithm to generate probabilistic microservice assignments.", "result": "Evaluation on four open-source benchmarks showed Mo2oM outperformed eight baselines with up to 40.97% better structural modularity, 58% reduction in inter-service calls, 26.16% fewer interfaces, and improved service size balance.", "conclusion": "Mo2oM effectively bridges the gap between automated and expert-driven microservice decomposition by leveraging soft clustering, enhancing modularity and deprioritizing communication overhead in large-scale systems."}}
{"id": "2508.08029", "categories": ["cs.CR", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08029", "abs": "https://arxiv.org/abs/2508.08029", "authors": ["Thusitha Dayaratne", "Ngoc Duy Pham", "Viet Vo", "Shangqi Lai", "Sharif Abuadbba", "Hajime Suzuki", "Xingliang Yuan", "Carsten Rudolph"], "title": "Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data Manipulation Attacks", "comment": null, "summary": "The introduction of 5G and the Open Radio Access Network (O-RAN) architecture\nhas enabled more flexible and intelligent network deployments. However, the\nincreased complexity and openness of these architectures also introduce novel\nsecurity challenges, such as data manipulation attacks on the semi-standardised\nShared Data Layer (SDL) within the O-RAN platform through malicious xApps. In\nparticular, malicious xApps can exploit this vulnerability by introducing\nsubtle Unicode-wise alterations (hypoglyphs) into the data that are being used\nby traditional machine learning (ML)-based anomaly detection methods. These\nUnicode-wise manipulations can potentially bypass detection and cause failures\nin anomaly detection systems based on traditional ML, such as AutoEncoders,\nwhich are unable to process hypoglyphed data without crashing. We investigate\nthe use of Large Language Models (LLMs) for anomaly detection within the O-RAN\narchitecture to address this challenge. We demonstrate that LLM-based xApps\nmaintain robust operational performance and are capable of processing\nmanipulated messages without crashing. While initial detection accuracy\nrequires further improvements, our results highlight the robustness of LLMs to\nadversarial attacks such as hypoglyphs in input data. There is potential to use\ntheir adaptability through prompt engineering to further improve the accuracy,\nalthough this requires further research. Additionally, we show that LLMs\nachieve low detection latency (under 0.07 seconds), making them suitable for\nNear-Real-Time (Near-RT) RIC deployments.", "AI": {"tldr": "The paper explores using Large Language Models (LLMs) for secure anomaly detection in O-RAN by addressing Unicode-wise data manipulation attacks (hypoglyphs) from traditional ML-based systems.", "motivation": "5G and O-RAN architectures introduce novel security challenges, particularly vulnerability to hypoglyph attacks via malicious xApps in the Shared Data Layer (SDL). Traditional ML-based anomaly detection methods (e.g., AutoEncoders) fail under these subtle Unicode manipulations, risking system crashes and undetected threats.", "method": "The authors evaluate LLM-based xApps as an alternative by testing their robustness against hypoglyphed input data. They leverage LLMs' adaptability via prompt engineering and assess detection latency for Near-Real-Time RIC compatibility.", "result": "LLMs demonstrated robustness to hypoglyph attacks without crashing, achieved low detection latency (<0.07s), and showed potential for improved accuracy through prompt engineering, though refinement is needed.", "conclusion": "LLMs offer a promising solution for O-RAN security, providing resilience against adversarial Unicode manipulations and low latency for real-time applications. Further research into enhancing detection accuracy via prompt engineering is required for practical deployment."}}
{"id": "2508.07881", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.07881", "abs": "https://arxiv.org/abs/2508.07881", "authors": ["Henna Tammia", "Benjamin K\u00e4m\u00e4", "Ella Peltonen"], "title": "Adopting Road-Weather Open Data in Route Recommendation Engine", "comment": null, "summary": "Digitraffic, Finland's open road data interface, provides access to\nnationwide road sensors with more than 2,300 real-time attributes from 1,814\nstations. However, efficiently utilizing such a versatile data API for a\npractical application requires a deeper understanding of the data qualities,\npreprocessing phases, and machine learning tools. This paper discusses the\nchallenges of large-scale road weather and traffic data. We go through the\nroad-weather-related attributes from DigiTraffic as a practical example of\nprocesses required to work with such a dataset. In addition, we provide a\nmethodology for efficient data utilization for the target application, a\npersonalized road recommendation engine based on a simple routing application.\nWe validate our solution based on real-world data, showing we can efficiently\nidentify and recommend personalized routes for three different driver profiles.", "AI": {"tldr": "This paper analyzes challenges in using Finland's DigiTraffic road data API for a personalized routing recommendation system, including preprocessing steps and machine learning validation for three driver profiles.", "motivation": "While DigiTraffic provides extensive real-time road data, practical applications require structured preprocessing methods and tailored ML tools to overcome data quality issues and variability.", "method": "The authors examine road-weather attributes from DigiTraffic as a case study, develop a methodology for efficient data usage, and implement a recommendation engine validated through real-world driver profiles.", "result": "Three distinct driver profiles achieved personalized route recommendations using the proposed preprocessing and ML framework, demonstrating the system's effectiveness in real conditions.", "conclusion": "The study provides actionable insights for leveraging large-scale road data APIs through structured preprocessing and algorithmic approaches adaptable to driver-specific preferences."}}
{"id": "2508.08031", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08031", "abs": "https://arxiv.org/abs/2508.08031", "authors": ["Jiayao Wang", "Yang Song", "Zhendong Zhao", "Jiale Zhang", "Qilin Wu", "Junwu Zhu", "Dongfang Zhao"], "title": "IPBA: Imperceptible Perturbation Backdoor Attack in Federated Self-Supervised Learning", "comment": null, "summary": "Federated self-supervised learning (FSSL) combines the advantages of\ndecentralized modeling and unlabeled representation learning, serving as a\ncutting-edge paradigm with strong potential for scalability and privacy\npreservation. Although FSSL has garnered increasing attention, research\nindicates that it remains vulnerable to backdoor attacks. Existing methods\ngenerally rely on visually obvious triggers, which makes it difficult to meet\nthe requirements for stealth and practicality in real-world deployment. In this\npaper, we propose an imperceptible and effective backdoor attack method against\nFSSL, called IPBA. Our empirical study reveals that existing imperceptible\ntriggers face a series of challenges in FSSL, particularly limited\ntransferability, feature entanglement with augmented samples, and\nout-of-distribution properties. These issues collectively undermine the\neffectiveness and stealthiness of traditional backdoor attacks in FSSL. To\novercome these challenges, IPBA decouples the feature distributions of backdoor\nand augmented samples, and introduces Sliced-Wasserstein distance to mitigate\nthe out-of-distribution properties of backdoor samples, thereby optimizing the\ntrigger generation process. Our experimental results on several FSSL scenarios\nand datasets show that IPBA significantly outperforms existing backdoor attack\nmethods in performance and exhibits strong robustness under various defense\nmechanisms.", "AI": {"tldr": "The paper proposes IPBA, an imperceptible backdoor attack for Federated Self-Supervised Learning (FSSL), addressing challenges like transferability and feature entanglement through decoupling techniques and Sliced-Wasserstein distance.", "motivation": "Traditional backdoor attacks in FSSL rely on visually obvious triggers, failing to meet real-world requirements for stealth and practicality.", "method": "IPBA decouples feature distributions of backdoor/aggregated samples and optimizes trigger generation using Sliced-Wasserstein distance to mitigate out-of-distribution issues and entanglement.", "result": "IPBA outperforms existing backdoor methods in performance and remains robust against various defense mechanisms across multiple FSSL scenarios and datasets.", "conclusion": "IPBA provides a more effective and stealthy backdoor attack framework for FSSL by addressing key limitations of prior approaches with novel feature decoupling and optimization strategies."}}
{"id": "2508.07935", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.07935", "abs": "https://arxiv.org/abs/2508.07935", "authors": ["Jingwen Zhou", "Jieshan Chen", "Qinghua Lu", "Dehai Zhao", "Liming Zhu"], "title": "SHIELDA: Structured Handling of Exceptions in LLM-Driven Agentic Workflows", "comment": null, "summary": "Large Language Model (LLM) agentic systems are software systems powered by\nLLMs that autonomously reason, plan, and execute multi-step workflows to\nachieve human goals, rather than merely executing predefined steps. During\nexecution, these workflows frequently encounter exceptions. Existing exception\nhandling solutions often treat exceptions superficially, failing to trace\nexecution-phase exceptions to their reasoning-phase root causes. Furthermore,\ntheir recovery logic is brittle, lacking structured escalation pathways when\ninitial attempts fail. To tackle these challenges, we first present a\ncomprehensive taxonomy of 36 exception types across 12 agent artifacts.\nBuilding on this, we propose SHIELDA (Structured Handling of Exceptions in\nLLM-Driven Agentic Workflows), a modular runtime exception handling framework\nfor LLM agentic workflows. SHIELDA uses an exception classifier to select a\npredefined exception handling pattern from a handling pattern registry. These\npatterns are then executed via a structured handling executor, comprising local\nhandling, flow control, and state recovery, to enable phase-aware recovery by\nlinking exceptions to their root causes and facilitating composable strategies.\nWe validate SHIELDA's effectiveness through a case study on the AutoPR agent,\ndemonstrating effective, cross-phase recovery from a reasoning-induced\nexception.", "AI": {"tldr": "This paper introduces SHIELDA, a modular framework for structured exception handling in LLM-powered agentic workflows by linking exceptions to their root reasoning-phase causes and enabling composable recovery strategies across 12 agent artifacts.", "motivation": "Current LLM agentic systems struggle with exceptions due to (1) superficial handling of execution-phase errors without identifying root reasoning-phase causes, and (2) fragile recovery logic lacking escalation pathways for failed initial attempts.", "method": "1) Constructed a taxonomy of 36 exception types across 12 agent artifacts. 2) Designed SHIELDA with three components: exception classifier, handling pattern registry, and structured executor with local handling, flow control, and state recovery modules.", "result": "Case study on AutoPR agent demonstrated SHIELDA's ability to recover from a reasoning-induced exception through phase-aware handling, outperforming existing solutions in completeness and robustness.", "conclusion": "SHIELDA provides a systemic solution to multi-phase exception handling in LLM agents by combining structured error classification with composable recovery patterns, establishing a foundation for more robust autonomous workflows."}}
{"id": "2508.08043", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.08043", "abs": "https://arxiv.org/abs/2508.08043", "authors": ["Yancheng Jiang", "Yan Jiang", "Ruochen Zhou", "Yi-Chao Chen", "Xiaoyu Ji", "Wenyuan Xu"], "title": "False Reality: Uncovering Sensor-induced Human-VR Interaction Vulnerability", "comment": null, "summary": "Virtual Reality (VR) techniques, serving as the bridge between the real and\nvirtual worlds, have boomed and are widely used in manufacturing, remote\nhealthcare, gaming, etc. Specifically, VR systems offer users immersive\nexperiences that include both perceptions and actions. Various studies have\ndemonstrated that attackers can manipulate VR software to influence users'\ninteractions, including perception and actions. However, such attacks typically\nrequire strong access and specialized expertise. In this paper, we are the\nfirst to present a systematic analysis of physical attacks against VR systems\nand introduce False Reality, a new attack threat to VR devices without\nrequiring access to or modification of their software. False Reality disturbs\nVR system services by tampering with sensor measurements, and further spoofing\nusers' perception even inducing harmful actions, e.g., inducing dizziness or\ncausing users to crash into obstacles, by exploiting perceptual and\npsychological effects. We formalize these threats through an attack pathway\nframework and validate three representative pathways via physical experiments\nand user studies on five commercial VR devices. Finally, we further propose a\ndefense prototype to mitigate such threats. Our findings shall provide valuable\ninsights for enhancing the security and resilience of future VR systems.", "AI": {"tldr": "This paper introduces False Reality, a new physical attack threat to VR devices that manipulates sensor measurements to spoof user perception and induce harmful actions without modifying software.", "motivation": "Existing VR attacks require deep software access and technical expertise, leaving physical vulnerabilities unexplored. The paper addresses gaps in VR security by demonstrating novel physical attacks that exploit sensor data manipulation to directly affect users' experiences and safety.", "method": "The authors formalize False Reality through an attack pathway framework, validate three specific attack scenarios using physical experiments and user studies on five commercial VR headsets, and develop a prototype defense system to detect and mitigate these threats.", "result": "Three attack pathways were successfully tested, showing that sensor tampering can disrupt VR services, induce perceptual illusions (like dizziness), and cause physical harm (e.g., collisions with obstacles) across major VR platforms.", "conclusion": "False Reality attacks reveal critical physical security vulnerabilities in VR systems. The proposed defense mechanisms and attack framework provide foundational approaches for improving hardware-level security and safeguarding user safety in immersive technologies."}}
{"id": "2508.07966", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07966", "abs": "https://arxiv.org/abs/2508.07966", "authors": ["Philipp Eibl", "Sadra Sabouri", "Souti Chattopadhyay"], "title": "Exploring the Challenges and Opportunities of AI-assisted Codebase Generation", "comment": null, "summary": "Recent AI code assistants have significantly improved their ability to\nprocess more complex contexts and generate entire codebases based on a textual\ndescription, compared to the popular snippet-level generation. These codebase\nAI assistants (CBAs) can also extend or adapt codebases, allowing users to\nfocus on higher-level design and deployment decisions. While prior work has\nextensively studied the impact of snippet-level code generation, this new class\nof codebase generation models is relatively unexplored. Despite initial\nanecdotal reports of excitement about these agents, they remain less frequently\nadopted compared to snippet-level code assistants. To utilize CBAs better, we\nneed to understand how developers interact with CBAs, and how and why CBAs fall\nshort of developers' needs. In this paper, we explored these gaps through a\ncounterbalanced user study and interview with (n = 16) students and developers\nworking on coding tasks with CBAs. We found that participants varied the\ninformation in their prompts, like problem description (48% of prompts),\nrequired functionality (98% of prompts), code structure (48% of prompts), and\ntheir prompt writing process. Despite various strategies, the overall\nsatisfaction score with generated codebases remained low (mean = 2.8, median =\n3, on a scale of one to five). Participants mentioned functionality as the most\ncommon factor for dissatisfaction (77% of instances), alongside poor code\nquality (42% of instances) and communication issues (25% of instances). We\ndelve deeper into participants' dissatisfaction to identify six underlying\nchallenges that participants faced when using CBAs, and extracted five barriers\nto incorporating CBAs into their workflows. Finally, we surveyed 21 commercial\nCBAs to compare their capabilities with participant challenges and present\ndesign opportunities for more efficient and useful CBAs.", "AI": {"tldr": "This paper investigates how developers interact with codebase AI assistants (CBAs), identifies six challenges and five barriers to their adoption, and proposes design opportunities based on a user study with 16 participants and an analysis of 21 commercial CBAs.", "motivation": "The paper aims to understand why CBAs remain underadopted despite their potential, by examining developers' interactions, dissatisfaction factors, and barriers to integration into workflows.", "method": "A counterbalanced user study and interviews with 16 students/developers, followed by a survey of 21 commercial CBAs to compare capabilities and identify design opportunities.", "result": "Participants reported low satisfaction (mean=2.8, median=3), with 77% dissatisfaction due to functionality, 42% due to code quality, and 25% due to communication issues. Six key challenges and five barriers were identified.", "conclusion": "Current CBAs fail to meet developer expectations in functionality and quality, highlighting the need for improved communication and system design to enhance adoption and usefulness."}}
{"id": "2508.08068", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.08068", "abs": "https://arxiv.org/abs/2508.08068", "authors": ["Yuval Efron", "Joachim Neu", "Toniann Pitassi"], "title": "Fully-Fluctuating Participation in Sleepy Consensus", "comment": null, "summary": "Proof-of-work allows Bitcoin to boast security amidst arbitrary fluctuations\nin participation of miners throughout time, so long as, at any point in time, a\nmajority of hash power is honest. In recent years, however, the pendulum has\nshifted in favor of proof-of-stake-based consensus protocols. There, the sleepy\nmodel is the most prominent model for handling fluctuating participation of\nnodes. However, to date, no protocol in the sleepy model rivals Bitcoin in its\nrobustness to drastic fluctuations in participation levels, with\nstate-of-the-art protocols making various restrictive assumptions. In this\nwork, we present a new adversary model, called external adversary. Intuitively,\nin our model, corrupt nodes do not divulge information about their secret keys.\nIn this model, we show that protocols in the sleepy model can meaningfully\nclaim to remain secure against fully fluctuating participation, without\ncompromising efficiency or corruption resilience. Our adversary model is quite\nnatural, and arguably naturally captures the process via which malicious\nbehavior arises in protocols, as opposed to traditional worst-case modeling. On\ntop of which, the model is also theoretically appealing, circumventing a\nbarrier established in a recent work of Malkhi, Momose, and Ren.", "AI": {"tldr": "This paper introduces an 'external adversary' model for proof-of-stake consensus protocols in the sleepy model, enabling security against fully fluctuating participation without compromising efficiency or corruption resilience, overcoming theoretical barriers from prior work.", "motivation": "Current proof-of-stake protocols in the sleepy model struggle to match Bitcoin's proof-of-work robustness against participation fluctuations while maintaining efficiency and corruption resilience, relying on restrictive assumptions.", "method": "Proposes a new adversary model where corrupt nodes in proof-of-stake protocols retain secrecy of their secret keys, allowing protocols to enforce security constraints even as participation dynamically changes.", "result": "Protocols leveraging the external adversary model achieve theoretical security guarantees under fully fluctuating participation, circumventing limitations of traditional adversary models and prior PoS protocol designs.", "conclusion": "The external adversary model provides a natural and theoretically stronger framework for analyzing proof-of-stake consensus security, enabling protocols to maintain robustness across arbitrary participation fluctuations as effectively as proof-of-work systems."}}
{"id": "2508.08171", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08171", "abs": "https://arxiv.org/abs/2508.08171", "authors": ["Pedro Orvalho", "Marta Kwiatkowska"], "title": "PyVeritas: On Verifying Python via LLM-Based Transpilation and Bounded Model Checking for C", "comment": "14 pages, 6 tables, 1 figure", "summary": "Python has become the dominant language for general-purpose programming, yet\nit lacks robust tools for formal verification. In contrast, programmers working\nin languages such as C benefit from mature model checkers, for example CBMC,\nwhich enable exhaustive symbolic reasoning and fault localisation. The inherent\ncomplexity of Python, coupled with the verbosity and low-level nature of\nexisting transpilers (e.g., Cython), have historically limited the\napplicability of formal verification to Python programs.\n  In this paper, we propose PyVeritas, a novel framework that leverages Large\nLanguage Models (LLMs) for high-level transpilation from Python to C, followed\nby bounded model checking and MaxSAT-based fault localisation in the generated\nC code. PyVeritas enables verification and bug localisation for Python code\nusing existing model checking tools for C. Our empirical evaluation on two\nPython benchmarks demonstrates that LLM-based transpilation can achieve a high\ndegree of accuracy, up to 80--90% for some LLMs, enabling effective development\nenvironment that supports assertion-based verification and interpretable fault\ndiagnosis for small yet non-trivial Python programs.", "AI": {"tldr": "PyVeritas uses LLMs to transpile Python to C for formal verification, enabling assertion-based checks and fault localization with C tools, achieving 80-90% accuracy on benchmarks.", "motivation": "The abstract highlights the lack of robust formal verification tools for Python compared to C, and the challenges posed by existing transpilers (e.g., Cython) due to their low-level nature and verbosity. This motivates the development of a high-level, LLM-driven transpilation approach to enable mature verification techniques for Python programs.", "method": "PyVeritas employs Large Language Models (LLMs) to perform high-level transpilation of Python code to C. The LLM-derived C code is then subjected to bounded model checking and MaxSAT-based fault localisation using existing C model checkers like CBMC. This two-step process bypasses Python's verification limitations by leveraging C's mature verification ecosystem.", "result": "The framework achieves 80\u201390% transpilation accuracy on selected Python benchmarks, demonstrating its effectiveness in creating a verification and bug-localization environment for small but meaningful Python programs through LLMs.", "conclusion": "PyVeritas presents a promising solution to Python's formal verification challenges by combining LLMs and C tools. The results suggest that LLM-based transpilation can provide an accurate and practical path for assertion-driven verification and interpretable debugging in Python, though focus remains on non-trivial but small-scale programs."}}
{"id": "2508.08190", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.08190", "abs": "https://arxiv.org/abs/2508.08190", "authors": ["Paritosh Ramanan", "H. M. Mohaimanul Islam", "Abhiram Reddy Alugula"], "title": "Differential Privacy for Regulatory Compliance in Cyberattack Detection on Critical Infrastructure Systems", "comment": null, "summary": "Industrial control systems are a fundamental component of critical\ninfrastructure networks (CIN) such as gas, water and power. With the growing\nrisk of cyberattacks, regulatory compliance requirements are also increasing\nfor large scale critical infrastructure systems comprising multiple utility\nstakeholders. The primary goal of regulators is to ensure overall system\nstability with recourse to trustworthy stakeholder attack detection. However,\nadhering to compliance requirements requires stakeholders to also disclose\nsensor and control data to regulators raising privacy concerns. In this paper,\nwe present a cyberattack detection framework that utilizes differentially\nprivate (DP) hypothesis tests geared towards enhancing regulatory confidence\nwhile alleviating privacy concerns of CIN stakeholders. The hallmark of our\napproach is a two phase privacy scheme that protects the privacy of covariance,\nas well as the associated sensor driven test statistics computed as a means to\ngenerate alarms. Theoretically, we show that our method induces a\nmisclassification error rate comparable to the non-DP cases while delivering\nrobust privacy guarantees. With the help of real-world datasets, we show the\nreliability of our DP-detection outcomes for a wide variety of attack scenarios\nfor interdependent stakeholders.", "AI": {"tldr": "This paper proposes a differentially private cyberattack detection framework to balance regulatory compliance and stakeholder privacy in critical infrastructure networks.", "motivation": "Regulatory oversight of CINs requires honest data sharing for security monitoring while stakeholders face privacy risks from disclosing sensitive sensor/control data.", "method": "A two-phase privacy scheme protecting both covariance matrices and sensor-driven test statistics using DP hypothesis tests for alarm generation.", "result": "Theoretical analysis demonstrates misclassification rates comparable to non-DP approaches. Real-world experiments validate reliability across interdependent stakeholder attack scenarios.", "conclusion": "The framework achieves robust privacy protections without compromising attack detection effectiveness, enabling regulatory compliance in multi-stakeholder CIN environments."}}
