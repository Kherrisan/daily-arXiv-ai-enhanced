<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 14]
- [cs.SE](#cs.SE) [Total: 8]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM Inference](https://arxiv.org/abs/2508.08438)
*Kexin Chu,Zecheng Lin,Dawei Xiang,Zixu Shen,Jianchang Su,Cheng Chu,Yiwei Yang,Wenhui Zhang,Wenfei Wu,Wei Zhang*

Main category: cs.CR

TL;DR: SafeKV is a privacy-aware KV-cache management framework that enables secure global sharing of non-sensitive entries while confining sensitive content to private caches, reducing side-channel attack leakage by 94%-97% with performance improvements over prior defenses.


<details>
  <summary>Details</summary>
Motivation: Global KV-cache sharing in LLM inference creates timing side-channel vulnerabilities by exposing sensitive user inputs through shared cache entries. Existing per-user isolation defenses cause significant 38.9% TTFT performance degradation, limiting their practicality for high-throughput applications. This gap in secure yet efficient LLM inference motivates the need for a better solution that preserves both privacy and performance.

Method: The SafeKV framework employs three core components: (1) a hybrid multi-tier detection system combining rule-based pattern matching, general-purpose privacy detection, and context-aware validation; (2) a unified radix-tree indexing structure managing public/private entries across heterogeneous memory (HBM/DRAM/SSD); and (3) entropy-based access monitoring to detect and mitigate residual leakage. These components work together to identify and isolate sensitive content while maintaining cache reuse efficiency for non-sensitive data.

Result: SafeKV achieves 94%-97% mitigation of timing-based side-channel attacks while outperforming per-user isolation by up to 40.58% in TTFT and 2.66X in throughput. For Qwen3-235B models, it reduces cache-induced TTFT overhead from 50.41% to 11.74%. These results demonstrate effective detection and isolation of sensitive content without compromising cache reuse benefits across diverse LLMs and workloads.

Conclusion: SafeKV successfully addresses the privacy-performance tradeoff in global KV-cache sharing for LLMs. By combining fine-grained privacy control with high cache efficiency, it provides robust runtime privacy guarantees while reclaiming the original performance advantages, making secure LLM inference at scale both practical and efficient. The framework's multi-tier detection and hybrid memory management approach establishes a new paradigm for balancing data privacy and hardware resource optimization in large-scale AI deployments.

Abstract: Global KV-cache sharing has emerged as a key optimization for accelerating
large language model (LLM) inference. However, it exposes a new class of timing
side-channel attacks, enabling adversaries to infer sensitive user inputs via
shared cache entries. Existing defenses, such as per-user isolation, eliminate
leakage but degrade performance by up to 38.9% in time-to-first-token (TTFT),
making them impractical for high-throughput deployment. To address this gap, we
introduce SafeKV (Secure and Flexible KV Cache Sharing), a privacy-aware
KV-cache management framework that selectively shares non-sensitive entries
while confining sensitive content to private caches. SafeKV comprises three
components: (i) a hybrid, multi-tier detection pipeline that integrates
rule-based pattern matching, a general-purpose privacy detector, and
context-aware validation; (ii) a unified radix-tree index that manages public
and private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and
(iii) entropy-based access monitoring to detect and mitigate residual
information leakage. Our evaluation shows that SafeKV mitigates 94% - 97% of
timing-based side-channel attacks. Compared to per-user isolation method,
SafeKV improves TTFT by up to 40.58% and throughput by up to 2.66X across
diverse LLMs and workloads. SafeKV reduces cache-induced TTFT overhead from
50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with
high cache reuse efficiency, SafeKV reclaims the performance advantages of
global sharing while providing robust runtime privacy guarantees for LLM
inference.

</details>


### [2] [Designing with Deception: ML- and Covert Gate-Enhanced Camouflaging to Thwart IC Reverse Engineering](https://arxiv.org/abs/2508.08462)
*Junling Fan,David Koblah,Domenic Forte*

Main category: cs.CR

TL;DR: This paper proposes an ML-driven IC security method integrating cryptic and mimetic deception via a new AIG-VAE model and covert gate variants to protect against reverse engineering.


<details>
  <summary>Details</summary>
Motivation: Current IC camouflage techniques focus on localized gate modifications and lack comprehensive deception strategies, leaving hardware vulnerable to IP theft and security breaches through reverse engineering attacks.

Method: The approach uses a Variational Autoencoder (AIG-VAE) for circuit representation encoding, combined with three covert gate types (Fake Inverters, Fake Buffers, Universal Transmitters) to achieve dual-layered protection through functional preservation and appearance mimicry.

Result: Experimental validation shows effective functional maintenance (100% pre- and post-layout functionality) with high camouflage (99.7%) and similarity scores (99.9% for 3P4R) while introducing minimal structural overhead.

Conclusion: The work establishes a new IC camouflage standard by bridging mimetic deception in hardware security, demonstrating practical resistance to AI-enhanced RE attacks and advancing cyber deception applications in critical system protection.

Abstract: Integrated circuits (ICs) are essential to modern electronic systems, yet
they face significant risks from physical reverse engineering (RE) attacks that
compromise intellectual property (IP) and overall system security. While IC
camouflage techniques have emerged to mitigate these risks, existing approaches
largely focus on localized gate modifications, neglecting comprehensive
deception strategies. To address this gap, we present a machine learning
(ML)-driven methodology that integrates cryptic and mimetic cyber deception
principles to enhance IC security against RE. Our approach leverages a novel
And-Inverter Graph Variational Autoencoder (AIG-VAE) to encode circuit
representations, enabling dual-layered camouflage through functional
preservation and appearance mimicry. By introducing new variants of covert
gates -- Fake Inverters, Fake Buffers, and Universal Transmitters -- our
methodology achieves robust protection by obscuring circuit functionality while
presenting misleading appearances. Experimental results demonstrate the
effectiveness of our strategy in maintaining circuit functionality while
achieving high camouflage and similarity scores with minimal structural
overhead. Additionally, we validate the robustness of our method against
advanced artificial intelligence (AI)-enhanced RE attacks, highlighting its
practical applicability in securing IC designs. By bridging the gap in mimetic
deception for hardware security, our work sets a new standard for IC
camouflage, advancing the application of cyber deception principles to protect
critical systems from adversarial threats.

</details>


### [3] [AI Security Map: Holistic Organization of AI Security Technologies and Impacts on Stakeholders](https://arxiv.org/abs/2508.08583)
*Hiroya Kato,Kentaro Kita,Kento Hasegawa,Seira Hidano*

Main category: cs.CR

TL;DR: The paper proposes an AI security map to holistically organize AI security elements and their negative impacts on information systems and stakeholders. The map includes two aspects: information system aspect (ISA) and external influence aspect (EIA), with findings, recommendations, and open problems for future AI security research.


<details>
  <summary>Details</summary>
Motivation: Current AI security research is fragmented across domains/elements, making it difficult to understand relationships and stakeholder impacts. There's a need for a unified framework to map interrelationships and consequences.

Method: Development of an AI security map with (1) information system aspect (ISA) classifying AI elements within systems and (2) external influence aspect (EIA) capturing stakeholder impacts from AI attacks/misuse. Each element's negative impacts and countermeasures are systematically identified and visualized.

Result: The AI security map (ISA+EIA) clarifies potential negative impacts, their causes, and countermeasures. Demonstrates how impacts on AI-based systems correlate with stakeholder consequences. Derives new findings through map analysis and provides actionable recommendations for the field.

Conclusion: The proposed AI security map enhances understanding of security interrelationships and stakeholder risks, offering a framework to analyze negative impacts. The work addresses knowledge gaps in AI security by systematically organizing threats and suggesting future research directions through identified open problems.

Abstract: As the social implementation of AI has been steadily progressing, research
and development related to AI security has also been increasing. However,
existing studies have been limited to organizing related techniques, attacks,
defenses, and risks in terms of specific domains or AI elements. Thus, it
extremely difficult to understand the relationships among them and how negative
impacts on stakeholders are brought about. In this paper, we argue that the
knowledge, technologies, and social impacts related to AI security should be
holistically organized to help understand relationships among them. To this
end, we first develop an AI security map that holistically organizes
interrelationships among elements related to AI security as well as negative
impacts on information systems and stakeholders. This map consists of the two
aspects, namely the information system aspect (ISA) and the external influence
aspect (EIA). The elements that AI should fulfill within information systems
are classified under the ISA. The EIA includes elements that affect
stakeholders as a result of AI being attacked or misused. For each element,
corresponding negative impacts are identified. By referring to the AI security
map, one can understand the potential negative impacts, along with their causes
and countermeasures. Additionally, our map helps clarify how the negative
impacts on AI-based systems relate to those on stakeholders. We show some
findings newly obtained by referring to our map. We also provide several
recommendations and open problems to guide future AI security communities.

</details>


### [4] [Generative AI for Critical Infrastructure in Smart Grids: A Unified Framework for Synthetic Data Generation and Anomaly Detection](https://arxiv.org/abs/2508.08593)
*Aydin Zaboli,Junho Hong*

Main category: cs.CR

TL;DR: This paper proposes a GenAI-enhanced approach for detecting cyber threats in IEC61850-based digital substations, addressing data scarcity with AATM-generated GOOSE datasets and demonstrating superior performance over ML-based systems.


<details>
  <summary>Details</summary>
Motivation: Traditional anomaly detection systems (ADSs) struggle with data scarcity and evolving cyber threats in IEC61850-based smart grids using protocols like GOOSE.

Method: Leverages generative AI to design advanced adversarial traffic mutation (AATM) for synthetic dataset generation, combined with task-oriented dialogue (ToD) processes in ADS implementation.

Result: GenAI-based ADS outperforms ML-based systems on AATM-generated GOOSE datasets using standard/advanced performance metrics.

Conclusion: The GenAI-driven framework with AATM provides a protocol-compliant, scalable solution for robust threat detection in digital substations while addressing data generation and zero-day attack challenges.

Abstract: In digital substations, security events pose significant challenges to the
sustained operation of power systems. To mitigate these challenges, the
implementation of robust defense strategies is critically important. A thorough
process of anomaly identification and detection in information and
communication technology (ICT) frameworks is crucial to ensure secure and
reliable communication and coordination between interconnected devices within
digital substations. Hence, this paper addresses the critical cybersecurity
challenges confronting IEC61850-based digital substations within modern smart
grids, where the integration of advanced communication protocols, e.g., generic
object-oriented substation event (GOOSE), has enhanced energy management and
introduced significant vulnerabilities to cyberattacks. Focusing on the
limitations of traditional anomaly detection systems (ADSs) in detecting
threats, this research proposes a transformative approach by leveraging
generative AI (GenAI) to develop robust ADSs. The primary contributions include
the suggested advanced adversarial traffic mutation (AATM) technique to
generate synthesized and balanced datasets for GOOSE messages, ensuring
protocol compliance and enabling realistic zero-day attack pattern creation to
address data scarcity. Then, the implementation of GenAI-based ADSs
incorporating the task-oriented dialogue (ToD) processes has been explored for
improved detection of attack patterns. Finally, a comparison of the GenAI-based
ADS with machine learning (ML)-based ADSs has been implemented to showcase the
outperformance of the GenAI-based frameworks considering the AATM-generated
GOOSE datasets and standard/advanced performance evaluation metrics.

</details>


### [5] [Hypervisor-based Double Extortion Ransomware Detection Method Using Kitsune Network Features](https://arxiv.org/abs/2508.08655)
*Manabu Hirano,Ryotaro Kobayashi*

Main category: cs.CR

TL;DR: The paper addresses double extortion ransomware attacks by introducing a hypervisor-based detection method combining low-level storage/memory behaviors and Kitsune NIDS for network traffic, achieving a 0.166 improvement in macro F score for data exfiltration detection despite attackers bypassing OS-level defenses.


<details>
  <summary>Details</summary>
Motivation: Organizations' improved data backup strategies against conventional crypto-ransomware have shifted attacks to double extortion methods, which combine data encryption with exfiltration threats.

Method: A novel defense-in-depth approach using low-level storage/memory behavioral features and Kitsune NIDS's network traffic analysis, monitored via a thin hypervisor to bypass OS-level protections.

Result: The method achieved a macro F score improvement of 0.166 in detecting data exfiltration phases of double extortion ransomware attacks.

Conclusion: While the method demonstrates effectiveness, limitations are acknowledged, with ongoing research on future work to strengthen its application against evolving ransomware techniques.

Abstract: Double extortion ransomware attacks have become mainstream since many
organizations adopt more robust and resilient data backup strategies against
conventional crypto-ransomware. This paper presents detailed attack stages,
tactics, procedures, and tools used in the double extortion ransomware attacks.
We then present a novel detection method using low-level storage and memory
behavioral features and network traffic features obtained from a thin
hypervisor to establish a defense-in-depth strategy for when attackers
compromise OS-level protection. We employed the lightweight \emph{Kitsune}
Network Intrusion Detection System (NIDS)'s network feature to detect the data
exfiltration phase in double extortion ransomware attacks. Our experimental
results showed that the presented method improved by 0.166 in the macro F score
of the data exfiltration phase detection rate. Lastly, we discuss the
limitations of the presented method and future work.

</details>


### [6] [Evasive Ransomware Attacks Using Low-level Behavioral Adversarial Examples](https://arxiv.org/abs/2508.08656)
*Manabu Hirano,Ryotaro Kobayashi*

Main category: cs.CR

TL;DR: This paper explores low-level behavioral adversarial examples in evasive ransomware, presenting a threat model and method to optimize malware source code using a micro-behavior control function. Evaluation with Conti ransomware source code demonstrates how attackers can manipulate behavioral features like thread count and encryption ratios to reduce detection rates by ransomware detectors.


<details>
  <summary>Details</summary>
Motivation: The rising use of deep learning in cybersecurity defenses necessitates understanding adversarial evasion techniques. Attackers exploit vulnerabilities through small perturbations to features, highlighting critical flaws in AI-based malware detection systems that require targeted analysis and mitigation strategies.

Method: The authors 1) define low-level behavioral adversarial examples specifically for ransomware evasion, 2) develop a formalized threat model to generate optimized malware code, and 3) implement a micro-behavior control function leveraging Conti ransomware's source code to test evasive behavior through modifiable parameters like threads, encryption ratios, and timing delays during execution.

Result: The micro-behavior control function demonstrated measurable decreases in Conti ransomware detection rates by modifying behavioral parameters. The method showed how attackers can systematically control multiple ransomware features to evade AI-based detectors while maintaining operational effectiveness.

Conclusion: This work establishes low-level behavioral adversarial examples as a novel and realistic threat model for evasive malware, revealing limitations in current ransomware detection systems. The findings emphasize the need for robust detection mechanisms capable of resisting behavioral manipulation through multi-parameter optimization techniques, with implications for advancing AI-driven cybersecurity defenses.

Abstract: Protecting state-of-the-art AI-based cybersecurity defense systems from cyber
attacks is crucial. Attackers create adversarial examples by adding small
changes (i.e., perturbations) to the attack features to evade or fool the deep
learning model. This paper introduces the concept of low-level behavioral
adversarial examples and its threat model of evasive ransomware. We formulate
the method and the threat model to generate the optimal source code of evasive
malware. We then examine the method using the leaked source code of Conti
ransomware with the micro-behavior control function. The micro-behavior control
function is our test component to simulate changing source code in ransomware;
ransomware's behavior can be changed by specifying the number of threads, file
encryption ratio, and delay after file encryption at the boot time. We
evaluated how much an attacker can control the behavioral features of
ransomware using the micro-behavior control function to decrease the detection
rate of a ransomware detector.

</details>


### [7] [Approximate DBSCAN under Differential Privacy](https://arxiv.org/abs/2508.08749)
*Yuan Qiu,Ke Yi*

Main category: cs.CR

TL;DR: This paper proposes a new DP-DBSCAN algorithm based on spans, which achieves linear-time complexity with sandwich quality guarantees and practical validation.


<details>
  <summary>Details</summary>
Motivation: Existing DP-DBSCAN algorithms that publish cluster labels are shown to lack utility, both empirically and theoretically. The need for better data representation under privacy constraints drives the new approach.

Method: The authors redefine DP-DBSCAN using 'spans' for label-free visualization/classification and develop a linear-time algorithm with pure-DP histogram construction as a key component.

Result: They achieve sandwich quality guarantees for approximation in constant dimensions, establish matching lower bounds, and validate performance on synthetic and real datasets.

Conclusion: The new DP-DBSCAN approach addresses prior limitations by focusing on span-based outputs and provides efficient, provably good clustering under differential privacy.

Abstract: This paper revisits the DBSCAN problem under differential privacy (DP).
Existing DP-DBSCAN algorithms aim at publishing the cluster labels of the input
points. However, we show that both empirically and theoretically, this approach
cannot offer any utility in the published results. We therefore propose an
alternative definition of DP-DBSCAN based on the notion of spans. We argue that
publishing the spans actually better serves the purposes of visualization and
classification of DBSCAN. Then we present a linear-time DP-DBSCAN algorithm
achieving the sandwich quality guarantee in any constant dimensions, as well as
matching lower bounds on the approximation ratio. A key building block in our
algorithm is a linear-time algorithm for constructing a histogram under
pure-DP, which is of independent interest. Finally, we conducted experiments on
both synthetic and real-world datasets to verify the practical performance of
our DP-DBSCAN algorithm.

</details>


### [8] [Never Compromise to Vulnerabilities: A Comprehensive Survey on AI Governance](https://arxiv.org/abs/2508.08789)
*Yuchu Jiang,Jian Zhao,Yuchen Yuan,Tianle Zhang,Yao Huang,Yanghao Zhang,Yan Wang,Yanshu Li,Xizhong Guo,Yusheng Zhao,Jun Zhang,Zhi Zhang,Xiaojian Lin,Yixiu Zou,Haoxuan Ma,Yuhu Shang,Yuzhi Hu,Keshu Cai,Ruochen Zhang,Boyuan Chen,Yilan Gao,Ziheng Jiao,Yi Qin,Shuangjun Du,Xiao Tong,Zhekun Liu,Yu Chen,Xuankun Rong,Rui Wang,Yejie Zheng,Zhaoxin Fan,Hongyuan Zhang,Pan Zhou,Lei Jin,Hao Zhao,Xu Yang,Jiaojiao Zhao,Jianshu Li,Joey Tianyi Zhou,Zhi-Qi Cheng,Longtao Huang,Zhiyi Liu,Zheng Zhu,Jianan Li,Gang Wang,Qi Li,Xu-Yao Zhang,Yaodong Yang,Mang Ye,Wenqi Ren,Zhaofeng He,Hang Su,Rongrong Ni,Liping Jing,Xingxing Wei,Junliang Xing,Massimo Alioto,Shengmei Shen,Petia Radeva,Dacheng Tao,Ya-Qin Zhang,Shuicheng Yan,Chi Zhang,Zhongjiang He,Xuelong Li*

Main category: cs.CR

TL;DR: The paper proposes a three-pillar AI governance framework integrating technical and societal dimensions, identifies three core challenges through a systematic review of 300+ studies, and offers actionable guidance for developing trustworthy AI systems.


<details>
  <summary>Details</summary>
Motivation: AI's rapid advancements have introduced technical vulnerabilities (algorithmic bias, adversarial sensitivity) leading to societal risks (misinformation, inequity, security breaches) and eroded public trust, necessitating robust governance solutions.

Method: Developed a comprehensive framework around three pillars (Intrinsic Security, Derivative Security, Social Ethics) through systematic review of over 300 studies, analyzing technical methods, evaluation benchmarks, and policy insights.

Result: Identified three critical governance challenges: 1) Generalization gap in defenses, 2) Overlooked real-world risks in evaluation protocols, 3) Fragmented regulations. Demonstrated limitations of reactive, siloed governance approaches.

Conclusion: Proposing an integrated research agenda that unifies technical security mechanisms with social ethics principles can enhance AI reliability, mitigate real-world harms, and strengthen accountability. The framework provides practical guidance for stakeholders across disciplines.

Abstract: The rapid advancement of AI has expanded its capabilities across domains, yet
introduced critical technical vulnerabilities, such as algorithmic bias and
adversarial sensitivity, that pose significant societal risks, including
misinformation, inequity, security breaches, physical harm, and eroded public
trust. These challenges highlight the urgent need for robust AI governance. We
propose a comprehensive framework integrating technical and societal
dimensions, structured around three interconnected pillars: Intrinsic Security
(system reliability), Derivative Security (real-world harm mitigation), and
Social Ethics (value alignment and accountability). Uniquely, our approach
unifies technical methods, emerging evaluation benchmarks, and policy insights
to promote transparency, accountability, and trust in AI systems. Through a
systematic review of over 300 studies, we identify three core challenges: (1)
the generalization gap, where defenses fail against evolving threats; (2)
inadequate evaluation protocols that overlook real-world risks; and (3)
fragmented regulations leading to inconsistent oversight. These shortcomings
stem from treating governance as an afterthought, rather than a foundational
design principle, resulting in reactive, siloed efforts that fail to address
the interdependence of technical integrity and societal trust. To overcome
this, we present an integrated research agenda that bridges technical rigor
with social responsibility. Our framework offers actionable guidance for
researchers, engineers, and policymakers to develop AI systems that are not
only robust and secure but also ethically aligned and publicly trustworthy. The
accompanying repository is available at
https://github.com/ZTianle/Awesome-AI-SG.

</details>


### [9] [Image selective encryption analysis using mutual information in CNN based embedding space](https://arxiv.org/abs/2508.08832)
*Ikram Messadi,Giulia Cervia,Vincent Itier*

Main category: cs.CR

TL;DR: This paper explores using mutual information (MI) estimators, such as empirical and MINE, to detect information leakage in selectively encrypted images, aiming to address the lack of quantitative privacy measures.


<details>
  <summary>Details</summary>
Motivation: With increasing data transmission and evolving notions of privacy, there is an urgent need for quantitative methods to assess information leakage in encrypted image data, which lacks robust information-theoretic analysis.

Method: The study employs mutual information (MI) estimation techniques, specifically empirical MI and the MINE framework, within a probabilistic model to capture spatial dependencies and residual structures in encrypted images.

Result: The work demonstrates a promising approach for image information leakage estimation by leveraging MI estimators to analyze encrypted representations.

Conclusion: The integration of MI estimators into encrypted image analysis offers a novel pathway for quantitatively addressing information leakage, highlighting potential advancements in privacy-secure data transmission methods.

Abstract: As digital data transmission continues to scale, concerns about privacy grow
increasingly urgent - yet privacy remains a socially constructed and
ambiguously defined concept, lacking a universally accepted quantitative
measure. This work examines information leakage in image data, a domain where
information-theoretic guarantees are still underexplored. At the intersection
of deep learning, information theory, and cryptography, we investigate the use
of mutual information (MI) estimators - in particular, the empirical estimator
and the MINE framework - to detect leakage from selectively encrypted images.
Motivated by the intuition that a robust estimator would require a
probabilistic frameworks that can capture spatial dependencies and residual
structures, even within encrypted representations - our work represent a
promising direction for image information leakage estimation.

</details>


### [10] [EditMF: Drawing an Invisible Fingerprint for Your Large Language Models](https://arxiv.org/abs/2508.08836)
*Jiaxuan Wu,Yinghan Zhou,Wanli Peng,Yiming Xue,Juan Wen,Ping Zhong*

Main category: cs.CR

TL;DR: EditMF is a training-free fingerprinting method for LLMs that achieves imperceptible IP protection with minimal overhead using causally traced triplets from an encrypted knowledge base and zero-space updates.


<details>
  <summary>Details</summary>
Motivation: Large language models require significant resources to train, necessitating intellectual property protection. Current fingerprinting methods using back-doors lack stealth and efficiency.

Method: 1. Encrypted artificial knowledge base with coherent triplets (virtual author-novel-protagonist facts) for embedding ownership bits
2. Causal tracing localizes minimal affected layers per triple
3. Zero-space injection method avoids unrelated knowledge perturbation

Result: Outperforms LoRA-based approaches with imperceptible fingerprints, maintains model performance on LLaMA and Qwen families, achieves single-query verification success (requires black-box query to retrieve exactly pre-embedded protagonist)

Conclusion: EditMF provides effective secure ownership verification for LLMs with significantly lower overhead than training-based methods, balancing stealth, efficiency and robustness.

Abstract: Training large language models (LLMs) is resource-intensive and expensive,
making protecting intellectual property (IP) for LLMs crucial. Recently,
embedding fingerprints into LLMs has emerged as a prevalent method for
establishing model ownership. However, existing back-door-based methods suffer
from limited stealth and efficiency. To simultaneously address these issues, we
propose EditMF, a training-free fingerprinting paradigm that achieves highly
imperceptible fingerprint embedding with minimal computational overhead.
Ownership bits are mapped to compact, semantically coherent triples drawn from
an encrypted artificial knowledge base (e.g., virtual author-novel-protagonist
facts). Causal tracing localizes the minimal set of layers influencing each
triple, and a zero-space update injects the fingerprint without perturbing
unrelated knowledge. Verification requires only a single black-box query and
succeeds when the model returns the exact pre-embedded protagonist. Empirical
results on LLaMA and Qwen families show that EditMF combines high
imperceptibility with negligible model's performance loss, while delivering
robustness far beyond LoRA-based fingerprinting and approaching that of SFT
embeddings. Extensive experiments demonstrate that EditMF is an effective and
low-overhead solution for secure LLM ownership verification.

</details>


### [11] [Redactable Blockchains: An Overview](https://arxiv.org/abs/2508.08898)
*Federico Calandra,Marco Bernardo,Andrea Esposito,Francesco Fabris*

Main category: cs.CR

TL;DR: This paper explores redactable blockchains which allow controlled modifications to overcome immutability issues in real-world applications, analyzing cryptographic methods and practical deployment scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional blockchain immutability creates conflicts with regulatory compliance, data correction, and privacy requirements, necessitating a framework that enables law-compliant and secure data edits.

Method: The analysis uses cryptographic primitives like chameleon hash functions and alternative redaction schemes to enable secure edits, evaluates competing approaches, and examines practical applications in healthcare, finance, Internet of drones, and federated learning.

Result: Identified shortcomings in existing redaction methods and highlighted the feasibility of redactable blockchains in private settings through use-case analysis across various industries.

Conclusion: Redactable blockchains show potential for creating legally compliant, trustworthy digital infrastructures but require further research to address technical limitations, regulatory alignment, and scalability challenges, including those related to reversible computing.

Abstract: Blockchains are widely recognized for their immutability, which provides
robust guarantees of data integrity and transparency. However, this same
feature poses significant challenges in real-world situations that require
regulatory compliance, correction of erroneous data, or removal of sensitive
information. Redactable blockchains address the limitations of traditional ones
by enabling controlled, auditable modifications to blockchain data, primarily
through cryptographic mechanisms such as chameleon hash functions and
alternative redaction schemes. This report examines the motivations for
introducing redactability, surveys the cryptographic primitives that enable
secure edits, and analyzes competing approaches and their shortcomings. Special
attention is paid to the practical deployment of redactable blockchains in
private settings, with discussions of use cases in healthcare, finance,
Internet of drones, and federated learning. Finally, the report outlines
further challenges, also in connection with reversible computing, and the
future potential of redactable blockchains in building law-compliant,
trustworthy, and scalable digital infrastructures.

</details>


### [12] [Load-Altering Attacks Against Power Grids: A Case Study Using the GB-36 Bus System Open Dataset](https://arxiv.org/abs/2508.08945)
*Syed Irtiza Maksud,Subhash Lakshminarayana*

Main category: cs.CR

TL;DR: This paper investigates the impact of Load Altering Attacks (LAAs) on a real-world power grid model using open-source data and examines how Battery Energy Storage Systems (BESS) can mitigate these threats through simulation.


<details>
  <summary>Details</summary>
Motivation: Digitalization and IoT-enabled devices in power grids have increased vulnerability to LAAs, which can destabilize the grid through rapid frequency fluctuations. Bridging academic research and practical grid operation insights is critical.

Method: The study uses the Great Britain (GB)-36 Zone transmission network model from NESO, evaluates LAA severity thresholds, and explores BESS-based fast frequency response services via DIgSILENT PowerFactory simulations.

Result: Identifies grid-tolerance limits for LAAs and analyzes BESS deployment impacts (location and attack delay timing) on system response, demonstrating how these delays and placements affect stability.

Conclusion: BESS deployment in strategic locations and timely response to LAAs can effectively prevent cascading grid failures, highlighting practical guidelines for grid operators to enhance system resilience against cyber threats.

Abstract: The growing digitalization and the rapid adoption of high-powered
Internet-of-Things (IoT)-enabled devices (e.g., EV charging stations) have
increased the vulnerability of power grids to cyber threats. In particular, the
so-called Load Altering Attacks (LAAs) can trigger rapid frequency fluctuations
and potentially destabilize the power grid. This paper aims to bridge the gap
between academic research and practical application by using open-source
datasets released by grid operators. It investigates various LAA scenarios on a
real-world transmission network, namely the Great Britain (GB)-36 Zone model
released by the UK's National Electricity System Operator (NESO). It evaluates
the threshold of LAA severity that the grid can tolerate before triggering
cascading effects. Additionally, it explores how Battery Energy Storage Systems
(BESS) based fast frequency response services can mitigate or prevent such
impacts. Simulations are conducted using DIgSILENT PowerFactory to ensure
realistic system representation. The analysis provides several useful insights
to grid operators on the LAA impact, such as the influence of the relative
locations of BESS and LAA, as well as how delays in attack execution can
influence the overall system response.

</details>


### [13] [Attacks and Defenses Against LLM Fingerprinting](https://arxiv.org/abs/2508.09021)
*Kevin Kurian,Ethan Holland,Sean Oesch*

Main category: cs.CR

TL;DR: The paper presents an offensive LLM fingerprinting attack using reinforcement learning to optimize query selection (3 queries vs. random selection), and a defensive approach employing semantic-preserving secondary LLM filtering to obfuscate model identity while maintaining output quality.


<details>
  <summary>Details</summary>
Motivation: Large language models deployed in sensitive environments face privacy/security risks from fingerprinting attacks, necessitating both enhanced attack methodologies and effective mitigation strategies.

Method: 1) Reinforcement learning-based query optimization for fingerprinting attacks. 2) Semantic-preserving output filtering via secondary LLM for defense.

Result: Attack achieves 3-query efficiency vs. random methods. Defense reduces fingerprinting accuracy across models without sacrificing output quality.

Conclusion: Provides dual contributions: improved fingerprinting tool capabilities through query optimization and practical LLM filtering techniques that mitigate fingerprinting risks while preserving semantic integrity.

Abstract: As large language models are increasingly deployed in sensitive environments,
fingerprinting attacks pose significant privacy and security risks. We present
a study of LLM fingerprinting from both offensive and defensive perspectives.
Our attack methodology uses reinforcement learning to automatically optimize
query selection, achieving better fingerprinting accuracy with only 3 queries
compared to randomly selecting 3 queries from the same pool. Our defensive
approach employs semantic-preserving output filtering through a secondary LLM
to obfuscate model identity while maintaining semantic integrity. The defensive
method reduces fingerprinting accuracy across tested models while preserving
output quality. These contributions show the potential to improve
fingerprinting tools capabilities while providing practical mitigation
strategies against fingerprinting attacks.

</details>


### [14] [Developing a Transferable Federated Network Intrusion Detection System](https://arxiv.org/abs/2508.09060)
*Abu Shafin Mohammad Mahdee Jameel,Shreya Ghosh,Aly El Gamal*

Main category: cs.CR

TL;DR: This paper proposes a deep learning-based distributed intrusion detection system (IDS) that improves transferability against unknown attacks and maintains local detection rates through a CNN model and two algorithms.


<details>
  <summary>Details</summary>
Motivation: Traditional IDS models struggle with unknown attacks. The authors aim to enhance transferability, allowing models to detect new attacks by leveraging knowledge from known ones.

Method: The method involves a CNN model with a two-step data pre-processing stage and a Block-Based Smart Aggregation (BBSA) algorithm to maximize transferability relationships across devices.

Result: The system achieves superior transferability performance while maintaining high local detection rates, and demonstrates generalizability across datasets and model backbones.

Conclusion: The proposed approach effectively combines distributed deployment with transferability optimization, offering practical ID solutions for evolving threats with publicly available code.

Abstract: Intrusion Detection Systems (IDS) are a vital part of a network-connected
device. In this paper, we develop a deep learning based intrusion detection
system that is deployed in a distributed setup across devices connected to a
network. Our aim is to better equip deep learning models against unknown
attacks using knowledge from known attacks. To this end, we develop algorithms
to maximize the number of transferability relationships. We propose a
Convolutional Neural Network (CNN) model, along with two algorithms that
maximize the number of relationships observed. One is a two step data
pre-processing stage, and the other is a Block-Based Smart Aggregation (BBSA)
algorithm. The proposed system succeeds in achieving superior transferability
performance while maintaining impressive local detection rates. We also show
that our method is generalizable, exhibiting transferability potential across
datasets and even with different backbones. The code for this work can be found
at https://github.com/ghosh64/tabfidsv2.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [15] [Context Engineering for Multi-Agent LLM Code Assistants Using Elicit, NotebookLM, ChatGPT, and Claude Code](https://arxiv.org/abs/2508.08322)
*Muhammad Haseeb*

Main category: cs.SE

TL;DR: This paper proposes a context-engineering workflow integrating multiple AI components (GPT-5, Elicit, NotebookLM, Claude Code agents) to enhance code generation accuracy in complex, multi-file software projects by addressing contextual limitations and knowledge gaps.


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges in complex code projects due to context limitations and knowledge gaps, necessitating improved workflows for intent clarification, domain knowledge injection, and contextual understanding during code generation.

Method: A four-component system combining: (1) GPT-5 as Intent Translator for requirement clarification, (2) Elicit-based semantic literature retrieval for knowledge injection, (3) NotebookLM for contextual document synthesis, and (4) Claude Code multi-agent framework with specialized sub-agents for orchestration and validation.

Result: The method achieves higher single-shot success rates and better context adherence than baseline models. Qualitative testing on a Next.js codebase shows effective feature planning, editing, and testing with minimal human intervention, outperforming frameworks like CodePlan and MASAI in targeted context implementation.

Conclusion: Multi-agent systems with strategic context engineering significantly improve coding assistants' capabilities in production environments, demonstrating that role-decomposed agents and focused knowledge injection can address critical LLM limitations in complex software projects.

Abstract: Large Language Models (LLMs) have shown promise in automating code generation
and software engineering tasks, yet they often struggle with complex,
multi-file projects due to context limitations and knowledge gaps. We propose a
novel context engineering workflow that combines multiple AI components: an
Intent Translator (GPT-5) for clarifying user requirements, an Elicit-powered
semantic literature retrieval for injecting domain knowledge, NotebookLM-based
document synthesis for contextual understanding, and a Claude Code multi-agent
system for code generation and validation. Our integrated approach leverages
intent clarification, retrieval-augmented generation, and specialized
sub-agents orchestrated via Claude's agent framework. We demonstrate that this
method significantly improves the accuracy and reliability of code assistants
in real-world repositories, yielding higher single-shot success rates and
better adherence to project context than baseline single-agent approaches.
Qualitative results on a large Next.js codebase show the multi-agent system
effectively plans, edits, and tests complex features with minimal human
intervention. We compare our system with recent frameworks like CodePlan,
MASAI, and HyperAgent, highlighting how targeted context injection and agent
role decomposition lead to state-of-the-art performance. Finally, we discuss
the implications for deploying LLM-based coding assistants in production, along
with lessons learned on context management and future research directions.

</details>


### [16] [Energy-Aware Code Generation with LLMs: Benchmarking Small vs. Large Language Models for Sustainable AI Programming](https://arxiv.org/abs/2508.08332)
*Humza Ashraf,Syed Muhammad Danish,Aris Leivadeas,Yazan Otoum,Zeeshan Sattar*

Main category: cs.SE

TL;DR: This paper compares open-source small language models (SLMs) with large commercial LLMs (GPT-4.0, DeepSeek-Reasoner) for code generation tasks, finding that SLMs often match or exceed LLMs in energy efficiency while maintaining correctness on over 52% of LeetCode problems across three difficulty levels.


<details>
  <summary>Details</summary>
Motivation: Large commercial LLMs like ChatGPT produce high energy use and carbon emissions despite prevalent use for code generation; the study aims to evaluate if energy-efficient SLMs can achieve similar performance.

Method: Evaluated 150 LeetCode problems (easy, medium, hard) using three SLMs (StableCode-3B, StarCoderBase-3B, Qwen2.5-Coder-3B-Instruct) and two LLMs, comparing model-generated code against efficient human-written solutions using run-time, memory, energy consumption, and correctness metrics.

Result: LLMs outperformed in correctness across all problem difficulties, but correct SLM outputs were more energy efficient than LLMs in over 52% of evaluated problems.

Conclusion: SLMs demonstrate potential to reduce computational energy use compared to LLMs when their code is correct, suggesting they could be viable alternatives for energy-sensitive code generation applications.

Abstract: Large Language Models (LLMs) are widely used for code generation. However,
commercial models like ChatGPT require significant computing power, which leads
to high energy use and carbon emissions. This has raised concerns about their
environmental impact. In this study, we evaluate open-source Small Language
Models (SLMs) trained explicitly for code generation and compare their
performance and energy efficiency against large LLMs and efficient
human-written Python code. The goal is to investigate whether SLMs can match
the performance of LLMs on certain types of programming problems while
producing more energy-efficient code. We evaluate 150 coding problems from
LeetCode, evenly distributed across three difficulty levels: easy, medium, and
hard. Our comparison includes three small open-source models, StableCode-3B,
StarCoderBase-3B, and Qwen2.5-Coder-3B-Instruct, and two large commercial
models, GPT-4.0 and DeepSeek-Reasoner. The generated code is evaluated using
four key metrics: run-time, memory usage, energy consumption, and correctness.
We use human-written solutions as a baseline to assess the quality and
efficiency of the model-generated code. Results indicate that LLMs achieve the
highest correctness across all difficulty levels, but SLMs are often more
energy-efficient when their outputs are correct. In over 52% of the evaluated
problems, SLMs consumed the same or less energy than LLMs.

</details>


### [17] [Improving Merge Pipeline Throughput in Continuous Integration via Pull Request Prioritization](https://arxiv.org/abs/2508.08342)
*Maximilian Jungwirth,Martin Gruber,Gordon Fraser*

Main category: cs.SE

TL;DR: The paper proposes a build system-agnostic method to optimize merge pipeline order using historical data and prediction models to prioritize PRs likely to pass, improving throughput during peak hours.


<details>
  <summary>Details</summary>
Motivation: High load on merge pipelines creates bottlenecks despite parallelization, while existing optimizations rely on specific build systems and lack general applicability.

Method: Leverages historical build data, PR metadata, and contextual information to predict build success probabilities, dynamically reordering PRs in merge pipelines using a learning-based approach for prediction.

Result: Predictive ordering achieved higher throughput than FIFO and non-learning strategies in large-scale experiments, demonstrating effectiveness during peak hours and cross-build system applicability.

Conclusion: Prediction-based PR prioritization provides a scalable, build system-agnostic solution to merge pipeline bottlenecks while maintaining easy integration into existing workflows.

Abstract: Integrating changes into large monolithic software repositories is a critical
step in modern software development that substantially impacts the speed of
feature delivery, the stability of the codebase, and the overall productivity
of development teams. To ensure the stability of the main branch, many
organizations use merge pipelines that test software versions before the
changes are permanently integrated. However, the load on merge pipelines is
often so high that they become bottlenecks, despite the use of parallelization.
Existing optimizations frequently rely on specific build systems, limiting
their generalizability and applicability. In this paper we propose to optimize
the order of PRs in merge pipelines using practical build predictions utilizing
only historical build data, PR metadata, and contextual information to estimate
the likelihood of successful builds in the merge pipeline. By dynamically
prioritizing likely passing PRs during peak hours, this approach maximizes
throughput when it matters most. Experiments conducted on a real-world,
large-scale project demonstrate that predictive ordering significantly
outperforms traditional first-in-first-out (FIFO), as well as
non-learning-based ordering strategies. Unlike alternative optimizations, this
approach is agnostic to the underlying build system and thus easily integrable
into existing automated merge pipelines.

</details>


### [18] [OmniLLP: Enhancing LLM-based Log Level Prediction with Context-Aware Retrieval](https://arxiv.org/abs/2508.08545)
*Youssef Esseddiq Ouatiti,Mohammed Sayagh,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: OmniLLP enhances ML-based log level prediction by structuring in-context examples through semantic and developer ownership clustering, achieving 0.88-0.96 AUC performance.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based log level prediction (LLP) methods use random in-context examples, ignoring code semantic structures and developer ownership patterns that could improve prediction accuracy.

Method: OmniLLP clusters source files using (1) semantic similarity reflecting functional purpose and (2) developer ownership cohesion. It then retrieves in-context learning examples exclusively from these clusters to provide better-structured prompts for LLPs.

Result: OmniLLP improves LLP accuracy by up to 8% AUC over random methods. The combined semantic+ownership approach achieves 0.88-0.96 AUC across projects, outperforming baseline approaches.

Conclusion: Integrating code semantics and developer ownership context into LLM-based LLPs significantly enhances accuracy and provides a more reliable logging solution, improving system maintainability and observability.

Abstract: Developers insert logging statements in source code to capture relevant
runtime information essential for maintenance and debugging activities. Log
level choice is an integral, yet tricky part of the logging activity as it
controls log verbosity and therefore influences systems' observability and
performance. Recent advances in ML-based log level prediction have leveraged
large language models (LLMs) to propose log level predictors (LLPs) that
demonstrated promising performance improvements (AUC between 0.64 and 0.8).
Nevertheless, current LLM-based LLPs rely on randomly selected in-context
examples, overlooking the structure and the diverse logging practices within
modern software projects. In this paper, we propose OmniLLP, a novel LLP
enhancement framework that clusters source files based on (1) semantic
similarity reflecting the code's functional purpose, and (2) developer
ownership cohesion. By retrieving in-context learning examples exclusively from
these semantic and ownership aware clusters, we aim to provide more coherent
prompts to LLPs leveraging LLMs, thereby improving their predictive accuracy.
Our results show that both semantic and ownership-aware clusterings
statistically significantly improve the accuracy (by up to 8\% AUC) of the
evaluated LLM-based LLPs compared to random predictors (i.e., leveraging
randomly selected in-context examples from the whole project). Additionally,
our approach that combines the semantic and ownership signal for in-context
prediction achieves an impressive 0.88 to 0.96 AUC across our evaluated
projects. Our findings highlight the value of integrating software
engineering-specific context, such as code semantic and developer ownership
signals into LLM-LLPs, offering developers a more accurate, contextually-aware
approach to logging and therefore, enhancing system maintainability and
observability.

</details>


### [19] [Hallucinations in Code Change to Natural Language Generation: Prevalence and Evaluation of Detection Metrics](https://arxiv.org/abs/2508.08661)
*Chunhua Liu,Hong Yi Lin,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: This paper analyzes hallucinations in language models for commit message and code review generation, finding that 50% of code reviews and 20% of commit messages involve hallucinations. Combining metrics like model confidence and feature attribution improves detection effectiveness.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in software engineering tasks involving code changes remain understudied, despite their potential to harm project integrity. Previous detection methods lack efficacy for structurally complex code contexts.

Method: The study systematically quantifies hallucination rates through evaluation of recent language models and compares multiple metric-based detection approaches, including model confidence and feature attribution analysis.

Result: 50\% of generated code reviews and 20\% of commit messages contain hallucinations. Combined metrics outperform single measures, with confidence-based and attribution-based methods showing highest utility for real-time detection.

Conclusion: Structurally-aware detection approaches using combined metrics are essential for reliably identifying hallucinations in code modification tasks, offering practical solutions for improving model trustworthiness during inference.

Abstract: Language models have shown strong capabilities across a wide range of tasks
in software engineering, such as code generation, yet they suffer from
hallucinations. While hallucinations have been studied independently in natural
language and code generation, their occurrence in tasks involving code changes
which have a structurally complex and context-dependent format of code remains
largely unexplored. This paper presents the first comprehensive analysis of
hallucinations in two critical tasks involving code change to natural language
generation: commit message generation and code review comment generation. We
quantify the prevalence of hallucinations in recent language models and explore
a range of metric-based approaches to automatically detect them. Our findings
reveal that approximately 50\% of generated code reviews and 20\% of generated
commit messages contain hallucinations. Whilst commonly used metrics are weak
detectors on their own, combining multiple metrics substantially improves
performance. Notably, model confidence and feature attribution metrics
effectively contribute to hallucination detection, showing promise for
inference-time detection.\footnote{All code and data will be released upon
acceptance.

</details>


### [20] [Description and Comparative Analysis of QuRE: A New Industrial Requirements Quality Dataset](https://arxiv.org/abs/2508.08868)
*Henning Femmer,Frank Houdek,Max Unterbusch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: This paper introduces QuRE, a new dataset of 2,111 industrial requirements annotated via real-world reviews, aiming to establish a common gold standard for research on requirements quality.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for requirements quality research are often inaccessible, small, or lack contextual and label detail, hindering empirical rigor and comparability in the field.

Method: QuRE was created by compiling industrial requirements from an active five-year contract, annotated through systematic reviews over nearly a decade. Descriptive statistics (lexical diversity, readability) were generated and compared to synthetic datasets and other existing data.

Result: QuRE is released as a linguistically realistic dataset with detailed contextual descriptions and industrially validated labels, offering a resource for evaluating and improving requirements defect detection.

Conclusion: QuRE enables more collaborative, transparent, and empirically robust requirements quality research by providing a standardized, real-world benchmark with both quality annotations and contextual metadata.

Abstract: Requirements quality is central to successful software and systems
engineering. Empirical research on quality defects in natural language
requirements relies heavily on datasets, ideally as realistic and
representative as possible. However, such datasets are often inaccessible,
small, or lack sufficient detail. This paper introduces QuRE (Quality in
Requirements), a new dataset comprising 2,111 industrial requirements that have
been annotated through a real-world review process. Previously used for over
five years as part of an industrial contract, this dataset is now being
released to the research community. In this work, we furthermore provide
descriptive statistics on the dataset, including measures such as lexical
diversity and readability, and compare it to existing requirements datasets and
synthetically generated requirements. In contrast to synthetic datasets, QuRE
is linguistically similar to existing ones. However, this dataset comes with a
detailed context description, and its labels have been created and used
systematically and extensively in an industrial context over a period of close
to a decade. Our goal is to foster transparency, comparability, and empirical
rigor by supporting the development of a common gold standard for requirements
quality datasets. This, in turn, will enable more sound and collaborative
research efforts in the field.

</details>


### [21] [Empirical Analysis of Temporal and Spatial Fault Characteristics in Multi-Fault Bug Repositories](https://arxiv.org/abs/2508.08872)
*Dylan Callaghan,Alexandra van der Spuy,Bernd Fischer*

Main category: cs.SE

TL;DR: This paper challenges the assumption that software versions have a single fault by empirically analyzing 16 open-source Java and Python projects, showing that faults are long-lived and coexist in multiple versions, while being evenly distributed across a small subset of code without hotspots.


<details>
  <summary>Details</summary>
Motivation: Software fault fixing incurs high maintenance costs, requiring datasets and understanding of fault patterns for effective testing and evaluation. The study aims to address limitations in existing datasets (Defects4J, BugsInPy) by analyzing real-world fault characteristics.

Method: Empirical analysis of temporal and spatial fault patterns in 16 open-source Java/Python projects using Defects4J and BugsInPy datasets.

Result: 1) Many software faults are long-lived, resulting in multiple coexisting faults per version (contrary to dataset assumptions). 2) Faults are evenly distributed across a small code subset, with few hotspots.

Conclusion: Dataset assumptions (single-fault versions, sparse distributions with hotspots) may be inaccurate. Long-lived, concurrent faults and even distribution patterns demand revised testing strategies for realistic evaluation.

Abstract: Fixing software faults contributes significantly to the cost of software
maintenance and evolution. Techniques for reducing these costs require datasets
of software faults, as well as an understanding of the faults, for optimal
testing and evaluation. In this paper, we present an empirical analysis of the
temporal and spatial characteristics of faults existing in 16 open-source Java
and Python projects, which form part of the Defects4J and BugsInPy datasets,
respectively. Our findings show that many faults in these software systems are
long-lived, leading to the majority of software versions having multiple
coexisting faults. This is in contrast to the assumptions of the original
datasets, where the majority of versions only identify a single fault. In
addition, we show that although the faults are found in only a small subset of
the systems, these faults are often evenly distributed amongst this subset,
leading to relatively few bug hotspots.

</details>


### [22] [Toward Automated Hypervisor Scenario Generation Based on VM Workload Profiling for Resource-Constrained Environments](https://arxiv.org/abs/2508.08952)
*Hyunwoo Kim,Jaeseong Lee,Sunpyo Hong,Changmin Han*

Main category: cs.SE

TL;DR: The paper introduces an automated scenario generation framework for efficient hardware resource allocation in software-defined vehicles (SDVs) by combining runtime profiling, theoretical models, and vendor heuristics, validated through real-world deployments.


<details>
  <summary>Details</summary>
Motivation: Chipset vendors provide static board support packages and hypervisor setups unable to dynamically adapt to varying system requirements and workloads, challenging Tier 1 integrators to optimize resource allocation in SDVs.

Method: An automated framework is proposed, integrating domain-guided parametric modeling and deep learning-based QoS modeling to profile runtime behavior and optimize hypervisor configurations tailored to system constraints.

Result: The framework demonstrates improvements in integration efficiency and reduces development time for Tier 1 integrators, validated through real-world deployments in resource-constrained environment scenarios.

Conclusion: The proposed framework addresses the limitations of static configurations in SDVs by automating resource allocation optimization, showing practical benefits in reducing development costs and improving adaptability in complex automotive virtualization scenarios.

Abstract: In the automotive industry, the rise of software-defined vehicles (SDVs) has
  driven a shift toward virtualization-based architectures that consolidate
  diverse automotive workloads on a shared hardware platform. To support this
  evolution, chipset vendors provide board support packages (BSPs), hypervisor
  setups, and resource allocation guidelines. However, adapting these static
  configurations to varying system requirements and workloads remain a
  significant challenge for Tier 1 integrators.
  This paper presents an automated scenario generation framework, which helps
  automotive vendors to allocate hardware resources efficiently across multiple
  VMs. By profiling runtime behavior and integrating both theoretical models
and
  vendor heuristics, the proposed tool generates optimized hypervisor
  configurations tailored to system constraints.
  We compare two main approaches for modeling target QoS based on profiled data
  and resource allocation: domain-guided parametric modeling and deep
  learning-based modeling. We further describe our optimization strategy using
  the selected QoS model to derive efficient resource allocations. Finally, we
  report on real-world deployments to demonstrate the effectiveness of our
  framework in improving integration efficiency and reducing development time
in
  resource-constrained environments.

</details>
