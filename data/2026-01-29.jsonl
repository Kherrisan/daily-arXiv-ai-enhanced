{"id": "2601.19968", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.19968", "abs": "https://arxiv.org/abs/2601.19968", "authors": ["Youngwoong Cho"], "title": "What is the AGI in Offensive Security ?", "comment": null, "summary": "What is the AGI in Offensive Security? One can break it down into two questions : (1) any offensive security tasks could be reduced into symbolic language manipulation (language representation + reasoning), (2) powerful language model (LLM) are enough to \"deal with\" any symbolic language manipulation. This paper can formally model a target system as a state machine and a hacker as an interactive symbolic agent. And it shows that every interaction in an offensive engagement can be encoded as a finite string. This paper provides definitions, short lemmas, and open discussion."}
{"id": "2601.19970", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19970", "abs": "https://arxiv.org/abs/2601.19970", "authors": ["Nourin Shahin", "Izzat Alsmadi"], "title": "Benchmarking LLAMA Model Security Against OWASP Top 10 For LLM Applications", "comment": null, "summary": "As large language models (LLMs) move from research prototypes to enterprise systems, their security vulnerabilities pose serious risks to data privacy and system integrity. This study benchmarks various Llama model variants against the OWASP Top 10 for LLM Applications framework, evaluating threat detection accuracy, response safety, and computational overhead. Using the FABRIC testbed with NVIDIA A30 GPUs, we tested five standard Llama models and five Llama Guard variants on 100 adversarial prompts covering ten vulnerability categories. Our results reveal significant differences in security performance: the compact Llama-Guard-3-1B model achieved the highest detection rate of 76% with minimal latency (0.165s per test), whereas base models such as Llama-3.1-8B failed to detect threats (0% accuracy) despite longer inference times (0.754s). We observe an inverse relationship between model size and security effectiveness, suggesting that smaller, specialized models often outperform larger general-purpose ones in security tasks. Additionally, we provide an open-source benchmark dataset including adversarial prompts, threat labels, and attack metadata to support reproducible research in AI security, [1]."}
{"id": "2601.20163", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.20163", "abs": "https://arxiv.org/abs/2601.20163", "authors": ["Mahsa Tahghigh", "Hassan Salmani"], "title": "Reference-Free Spectral Analysis of EM Side-Channels for Always-on Hardware Trojan Detection", "comment": "Accepted at GOMACTech 2026", "summary": "Always-on hardware Trojans (HTs) pose a critical risk to trusted microelectronics, yet most side-channel detection methods rely on unavailable golden references. We present a reference-free approach that combines time-frequency EM analysis with Gaussian Mixture Models (GMMs). By applying Short-Time Fourier Transform (STFT) at multiple window sizes, we show that HT-free circuits exhibit fluctuating statistical structure, while always-on HTs leave persistent footprints with fewer, more consistent mixture components. Results on AES-128 demonstrate feasibility without requiring reference models."}
{"id": "2601.20184", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.20184", "abs": "https://arxiv.org/abs/2601.20184", "authors": ["Mohsen Hatami", "Van Tuan Pham", "Hozefa Lakadawala", "Yu Chen"], "title": "Securing AI Agents in Cyber-Physical Systems: A Survey of Environmental Interactions, Deepfake Threats, and Defenses", "comment": null, "summary": "The increasing integration of AI agents into cyber-physical systems (CPS) introduces new security risks that extend beyond traditional cyber or physical threat models. Recent advances in generative AI enable deepfake and semantic manipulation attacks that can compromise agent perception, reasoning, and interaction with the physical environment, while emerging protocols such as the Model Context Protocol (MCP) further expand the attack surface through dynamic tool use and cross-domain context sharing. This survey provides a comprehensive review of security threats targeting AI agents in CPS, with a particular focus on environmental interactions, deepfake-driven attacks, and MCP-mediated vulnerabilities. We organize the literature using the SENTINEL framework, a lifecycle-aware methodology that integrates threat characterization, feasibility analysis under CPS constraints, defense selection, and continuous validation. Through an end-to-end case study grounded in a real-world smart grid deployment, we quantitatively illustrate how timing, noise, and false-positive costs constrain deployable defenses, and why detection mechanisms alone are insufficient as decision authorities in safety-critical CPS. The survey highlights the role of provenance- and physics-grounded trust mechanisms and defense-in-depth architectures, and outlines open challenges toward trustworthy AI-enabled CPS."}
{"id": "2601.19964", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.19964", "abs": "https://arxiv.org/abs/2601.19964", "authors": ["Maxim Tabachnyk", "Xu Shu", "Alexander Frömmgen", "Pavel Sychev", "Vahid Meimand", "Ilia Krets", "Stanislav Pyatykh", "Abner Araujo", "Kristóf Molnár", "Satish Chandra"], "title": "Achieving Productivity Gains with AI-based IDE features: A Journey at Google", "comment": "Accepted for publication at the 3rd International Workshop on Large Language Models For Code (LLM4Code '26 workshop at ICSE '26)", "summary": "We discuss Google's journey in developing and refining two internal AI-based IDE features: code completion and natural-language-driven code transformation (Transform Code). We address challenges in latency, user experience and suggestion quality, all backed by rigorous experimentation. The article serves as an example of how to refine AI developer tools across the user interface, backend, and model layers, to deliver tangible productivity improvements in an enterprise setting."}
{"id": "2601.20270", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20270", "abs": "https://arxiv.org/abs/2601.20270", "authors": ["Holly Trikilis", "Pasindu Marasinghe", "Fariza Rashid", "Suranga Seneviratne"], "title": "Eliciting Least-to-Most Reasoning for Phishing URL Detection", "comment": null, "summary": "Phishing continues to be one of the most prevalent attack vectors, making accurate classification of phishing URLs essential. Recently, large language models (LLMs) have demonstrated promising results in phishing URL detection. However, their reasoning capabilities that enabled such performance remain underexplored. To this end, in this paper, we propose a Least-to-Most prompting framework for phishing URL detection. In particular, we introduce an \"answer sensitivity\" mechanism that guides Least-to-Most's iterative approach to enhance reasoning and yield higher prediction accuracy. We evaluate our framework using three URL datasets and four state-of-the-art LLMs, comparing against a one-shot approach and a supervised model. We demonstrate that our framework outperforms the one-shot baseline while achieving performance comparable to that of the supervised model, despite requiring significantly less training data. Furthermore, our in-depth analysis highlights how the iterative reasoning enabled by Least-to-Most, and reinforced by our answer sensitivity mechanism, drives these performance gains. Overall, we show that this simple yet powerful prompting strategy consistently outperforms both one-shot and supervised approaches, despite requiring minimal training or few-shot guidance. Our experimental setup can be found in our Github repository github.sydney.edu.au/htri0928/least-to-most-phishing-detection."}
{"id": "2601.20103", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20103", "abs": "https://arxiv.org/abs/2601.20103", "authors": ["Darshan Deshpande", "Anand Kannappan", "Rebecca Qian"], "title": "Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis", "comment": "Dataset: https://huggingface.co/datasets/PatronusAI/trace-dataset", "summary": "Recent advances in reinforcement learning for code generation have made robust environments essential to prevent reward hacking. As LLMs increasingly serve as evaluators in code-based RL, their ability to detect reward hacking remains understudied. In this paper, we propose a novel taxonomy of reward exploits spanning across 54 categories and introduce TRACE (Testing Reward Anomalies in Code Environments), a synthetically curated and human-verified benchmark containing 517 testing trajectories. Unlike prior work that evaluates reward hack detection in isolated classification scenarios, we contrast these evaluations with a more realistic, contrastive anomaly detection setup on TRACE. Our experiments reveal that models capture reward hacks more effectively in contrastive settings than in isolated classification settings, with GPT-5.2 with highest reasoning mode achieving the best detection rate at 63%, up from 45% in isolated settings on TRACE. Building on this insight, we demonstrate that state-of-the-art models struggle significantly more with semantically contextualized reward hacks compared to syntactically contextualized ones. We further conduct qualitative analyses of model behaviors, as well as ablation studies showing that the ratio of benign to hacked trajectories and analysis cluster sizes substantially impact detection performance. We release the benchmark and evaluation harness to enable the community to expand TRACE and evaluate their models."}
{"id": "2601.20310", "categories": ["cs.CR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20310", "abs": "https://arxiv.org/abs/2601.20310", "authors": ["Xin Zhang", "Zijin Yang", "Kejiang Chen", "Linfeng Ma", "Weiming Zhang", "Nenghai Yu"], "title": "SemBind: Binding Diffusion Watermarks to Semantics Against Black-Box Forgery Attacks", "comment": null, "summary": "Latent-based watermarks, integrated into the generation process of latent diffusion models (LDMs), simplify detection and attribution of generated images. However, recent black-box forgery attacks, where an attacker needs at least one watermarked image and black-box access to the provider's model, can embed the provider's watermark into images not produced by the provider, posing outsized risk to provenance and trust. We propose SemBind, the first defense framework for latent-based watermarks that resists black-box forgery by binding latent signals to image semantics via a learned semantic masker. Trained with contrastive learning, the masker yields near-invariant codes for the same prompt and near-orthogonal codes across prompts; these codes are reshaped and permuted to modulate the target latent before any standard latent-based watermark. SemBind is generally compatible with existing latent-based watermarking schemes and keeps image quality essentially unchanged, while a simple mask-ratio parameter offers a tunable trade-off between anti-forgery strength and robustness. Across four mainstream latent-based watermark methods, our SemBind-enabled anti-forgery variants markedly reduce false acceptance under black-box forgery while providing a controllable robustness-security balance."}
{"id": "2601.20106", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20106", "abs": "https://arxiv.org/abs/2601.20106", "authors": ["Shamse Tasnim Cynthia", "Joy Krishan Das", "Banani Roy"], "title": "Are We All Using Agents the Same Way? An Empirical Study of Core and Peripheral Developers Use of Coding Agents", "comment": null, "summary": "Autonomous AI agents are transforming software development and redefining how developers collaborate with AI. Prior research shows that the adoption and use of AI-powered tools differ between core and peripheral developers. However, it remains unclear how this dynamic unfolds in the emerging era of autonomous coding agents. In this paper, we present the first empirical study of 9,427 agentic PRs, examining how core and peripheral developers use, review, modify, and verify agent-generated contributions prior to acceptance. Through a mix of qualitative and quantitative analysis, we make four key contributions. First, a subset of peripheral developers use agents more often, delegating tasks evenly across bug fixing, feature addition, documentation, and testing. In contrast, core developers focus more on documentation and testing, yet their agentic PRs are frequently merged into the main/master branch. Second, core developers engage slightly more in review discussions than peripheral developers, and both groups focus on evolvability issues. Third, agentic PRs are less likely to be modified, but when they are, both groups commonly perform refactoring. Finally, peripheral developers are more likely to merge without running CI checks, whereas core developers more consistently require passing verification before acceptance. Our analysis offers a comprehensive view of how developer experience shapes integration offer insights for both peripheral and core developers on how to effectively collaborate with coding agents."}
{"id": "2601.20325", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.20325", "abs": "https://arxiv.org/abs/2601.20325", "authors": ["Lulu Xue", "Shengshan Hu", "Wei Lu", "Ziqi Zhou", "Yufei Song", "Jianhong Cheng", "Minghui Li", "Yanjun Zhang", "Leo Yu Zhang"], "title": "UnlearnShield: Shielding Forgotten Privacy against Unlearning Inversion", "comment": "This work has been accepted by ICASSP 2026", "summary": "Machine unlearning is an emerging technique that aims to remove the influence of specific data from trained models, thereby enhancing privacy protection. However, recent research has uncovered critical privacy vulnerabilities, showing that adversaries can exploit unlearning inversion to reconstruct data that was intended to be erased. Despite the severity of this threat, dedicated defenses remain lacking. To address this gap, we propose UnlearnShield, the first defense specifically tailored to counter unlearning inversion. UnlearnShield introduces directional perturbations in the cosine representation space and regulates them through a constraint module to jointly preserve model accuracy and forgetting efficacy, thereby reducing inversion risk while maintaining utility. Experiments demonstrate that it achieves a good trade-off among privacy protection, accuracy, and forgetting."}
{"id": "2601.20109", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20109", "abs": "https://arxiv.org/abs/2601.20109", "authors": ["Shamse Tasnim Cynthia", "Al Muttakin", "Banani Roy"], "title": "Beyond Bug Fixes: An Empirical Investigation of Post-Merge Code Quality Issues in Agent-Generated Pull Requests", "comment": null, "summary": "The increasing adoption of AI coding agents has increased the number of agent-generated pull requests (PRs) merged with little or no human intervention. Although such PRs promise productivity gains, their post-merge code quality remains underexplored, as prior work has largely relied on benchmarks and controlled tasks rather than large-scale post-merge analyses. To address this gap, we analyze 1,210 merged agent-generated bug-fix PRs from Python repositories in the AIDev dataset. Using SonarQube, we perform a differential analysis between base and merged commits to identify code quality issues newly introduced by PR changes. We examine issue frequency, density, severity, and rule-level prevalence across five agents. Our results show that apparent differences in raw issue counts across agents largely disappear after normalizing by code churn, indicating that higher issue counts are primarily driven by larger PRs. Across all agents, code smells dominate, particularly at critical and major severities, while bugs are less frequent but often severe. Overall, our findings show that merge success does not reliably reflect post-merge code quality, highlighting the need for systematic quality checks for agent-generated bug-fix PRs."}
{"id": "2601.20346", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20346", "abs": "https://arxiv.org/abs/2601.20346", "authors": ["Asifullah Khan", "Aimen Wadood", "Mubashar Iqbal", "Umme Zahoora"], "title": "Multimodal Multi-Agent Ransomware Analysis Using AutoGen", "comment": "45 pages, 11 figures and 10 tables", "summary": "Ransomware has become one of the most serious cybersecurity threats causing major financial losses and operational disruptions worldwide.Traditional detection methods such as static analysis, heuristic scanning and behavioral analysis often fall short when used alone. To address these limitations, this paper presents multimodal multi agent ransomware analysis framework designed for ransomware classification. Proposed multimodal multiagent architecture combines information from static, dynamic and network sources. Each data type is handled by specialized agent that uses auto encoder based feature extraction. These representations are then integrated through a fusion agent. After that fused representation are used by transformer based classifier. It identifies the specific ransomware family. The agents interact through an interagent feedback mechanism that iteratively refines feature representations by suppressing low confidence information. The framework was evaluated on large scale datasets containing thousands of ransomware and benign samples. Multiple experiments were conducted on ransomware dataset. It outperforms single modality and nonadaptive fusion baseline achieving improvement of up to 0.936 in Macro-F1 for family classification and reducing calibration error. Over 100 epochs, the agentic feedback loop displays a stable monotonic convergence leading to over +0.75 absolute improvement in terms of agent quality and a final composite score of around 0.88 without fine tuning of the language models. Zeroday ransomware detection remains family dependent on polymorphism and modality disruptions. Confidence aware abstention enables reliable real world deployment by favoring conservativeand trustworthy decisions over forced classification. The findings indicate that proposed approach provides a practical andeffective path toward improving real world ransomware defense systems."}
{"id": "2601.20112", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20112", "abs": "https://arxiv.org/abs/2601.20112", "authors": ["Maja Vukovic", "Rangeet Pan", "Tin Kam Ho", "Rahul Krishna", "Raju Pavuluri", "Michele Merler"], "title": "Usage, Effects and Requirements for AI Coding Assistants in the Enterprise: An Empirical Study", "comment": "To appear in the 3rd International Workshop on Large Language Models For Code, co-located at ICSE, Rio de Janeiro, Brazil, 2026", "summary": "The rise of large language models (LLMs) has accelerated the development of automated techniques and tools for supporting various software engineering tasks, e.g., program understanding, code generation, software testing, and program repair. As CodeLLMs are being employed toward automating these tasks, one question that arises, especially in enterprise settings, is whether these coding assistants and the code LLMs that power them are ready for real-world projects and enterprise use cases, and how do they impact the existing software engineering process and user experience. In this paper we survey 57 developers from different domains and with varying software engineering skill about their experience with AI coding assistants and CodeLLMs. We also reviewed 35 user surveys on the usage, experience and expectations of professionals and students using AI coding assistants and CodeLLMs. Based on our study findings and analysis of existing surveys, we discuss the requirements for AI-powered coding assistants."}
{"id": "2601.20368", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.20368", "abs": "https://arxiv.org/abs/2601.20368", "authors": ["Mohamed Amine Legheraba", "Nour Rachdi", "Maria Gradinariu Potop-Butucaru", "Sébastien Tixeuil"], "title": "LIFT: Byzantine Resilient Hub-Sampling", "comment": null, "summary": "Recently, a novel peer sampling protocol, Elevator, was introduced to construct network topologies tailored for emerging decentralized applications such as federated learning and blockchain. Elevator builds hub-based topologies in a fully decentralized manner, randomly selecting hubs among participating nodes. These hubs, acting as central nodes connected to the entire network, can be leveraged to accelerate message dissemination. Simulation results have shown that Elevator converges rapidly (within 3--4 cycles) and exhibits robustness against crash failures and churn. However, its resilience to Byzantine adversaries has not been investigated. In this work, we provide the first evaluation of Elevator under Byzantine adversaries and show that even a small fraction (2%) of Byzantine nodes is sufficient to subvert the network. As a result, we introduce LIFT, a new protocol that extends Elevator by employing a cryptographically secure pseudo-random number generator (PRNG) for hub selection, thereby mitigating Byzantine manipulation. In contrast, LIFT withstands adversarial infiltration and remains robust with up to 10% Byzantine nodes. These results highlight the necessity of secure randomness in decentralized hub formation and position LIFT as a more reliable building block for Byzantine-resilient decentralized systems."}
{"id": "2601.20147", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20147", "abs": "https://arxiv.org/abs/2601.20147", "authors": ["Saima Afrin", "Zaiyu Cheng", "Tushar Sharma", "Alexander Serebrenik", "Massimiliano Di Penta", "Antonio Mastropaolo"], "title": "Not All Tokens Matter: Data-Centric Optimization for Efficient Code Summarization", "comment": null, "summary": "Instruction-tuned Language Models ILMs have become essential components of modern AI systems, demonstrating exceptional versatility across a wide range of natural language and reasoning tasks. Among their most impactful applications is code generation, where ILMs--commonly referred to as Code Language Models CLMs--have demonstrated remarkable capability. This strength stems from their defining feature: the use of explicit task instructions during fine-tuning, which enables them to bridge natural language and code by translating human intent into executable code. While much of their progress has been driven by advances in scaling laws and training methodologies, one critical aspect remains underexplored--the impact of system prompts on the performance of both general-purpose ILMs and specialized CLMs when instantiated to assist users with code generation activities. In this study, we take a first step toward bridging this gap by systematically evaluating how system prompts of varying instructional detail, along with model scale, prompting strategy, and programming language, affect ILMs and CLMs in code generation tasks. Our evaluation framework, spanning 120 model configurations, reveals that (1) the influence of system prompts increases with model scale; (2) few-shot prompting reduces this effect compared to zero-shot; and (3) programming language matters, with Java showing greater sensitivity to system prompt variations than Python."}
{"id": "2601.20374", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.20374", "abs": "https://arxiv.org/abs/2601.20374", "authors": ["Sura Khalid Salsal", "Eman Shaker Mahmood", "Farah Tawfiq Abdul Hussien", "Maryam Mahdi Alhusseini", "Azhar Naji Alyahya", "Nikolai Safiullin"], "title": "A High-Performance Fractal Encryption Framework and Modern Innovations for Secure Image Transmission", "comment": null, "summary": "The current digital era, driven by growing threats to data security, requires a robust image encryption technique. Classical encryption algorithms suffer from a trade-off among security, image fidelity, and computational efficiency. This paper aims to enhance the performance and efficiency of image encryption. This is done by proposing Fractal encryption based on Fourier transforms as a new method of image encryption, leveraging state-of-the-art technology. The new approach considered here intends to enhance both security and efficiency in image encryption by comparing Fractal Encryption with basic methods. The suggested system also aims to optimise encryption/ decryption times and preserve image quality. This paper provides an introduction to Image Encryption using the fractal-based method, its mathematical formulation, and its comparative efficiency against publicly known traditional encryption methods. As a result, after filling the gaps identified in previous research, it has significantly improved both its encryption/decryption time and image fidelity compared to other techniques. In this paper, directions for future research and possible improvements are outlined for attention."}
{"id": "2601.20148", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20148", "abs": "https://arxiv.org/abs/2601.20148", "authors": ["Marcus Emmanuel Barnes", "Taher A. Ghaleb", "Safwat Hassan"], "title": "LogSieve: Task-Aware CI Log Reduction for Sustainable LLM-Based Analysis", "comment": "Preprint. Accepted for presentation at Mining Software Repositories (MSR'26), co-located ICSE 2026. The final version will appear in the ACM Digital Library as part of the MSR'26 conference proceedings", "summary": "Logs are essential for understanding Continuous Integration (CI) behavior, particularly for diagnosing build failures and performance regressions. Yet their growing volume and verbosity make both manual inspection and automated analysis increasingly costly, time-consuming, and environmentally costly. While prior work has explored log compression, anomaly detection, and LLM-based log analysis, most efforts target structured system logs rather than the unstructured, noisy, and verbose logs typical of CI workflows.\n  We present LogSieve, a lightweight, RCA-aware and semantics-preserving log reduction technique that filters low-information lines while retaining content relevant to downstream reasoning. Evaluated on CI logs from 20 open-source Android projects using GitHub Actions, LogSieve achieves an average 42% reduction in lines and 40% reduction in tokens with minimal semantic loss. This pre-inference reduction lowers computational cost and can proportionally reduce energy use (and associated emissions) by decreasing the volume of data processed during LLM inference.\n  Compared with structure-first baselines (LogZip and random-line removal), LogSieve preserves much higher semantic and categorical fidelity (Cosine = 0.93, GPTScore = 0.93, 80% exact-match accuracy). Embedding-based classifiers automate relevance detection with near-human accuracy (97%), enabling scalable and sustainable integration of semantics-aware filtering into CI workflows. LogSieve thus bridges log management and LLM reasoning, offering a practical path toward greener and more interpretable CI automation."}
{"id": "2601.20378", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.20378", "abs": "https://arxiv.org/abs/2601.20378", "authors": ["Mario Perera", "Michael Mackay", "Max Hashem Eiza", "Alessandro Raschellà", "Nathan Shone", "Mukesh Kumar Maheshwari"], "title": "Towards Quantum-Safe O-RAN -- Experimental Evaluation of ML-KEM-Based IPsec on the E2 Interface", "comment": "Please note this is a draft and will be revised soon. However, all the simulations, experiments, and findings are final", "summary": "As Open Radio Access Network (O-RAN) deployments expand and adversaries adopt 'store-now, decrypt-later' strategies, operators need empirical data on the cost of migrating critical control interfaces to post-quantum cryptography (PQC). This paper experimentally evaluates the impact of integrating a NIST-aligned module-lattice KEM (ML-KEM, CRYSTALS-Kyber) into IKEv2/IPsec protecting the E2 interface between the 5G Node B (gNB) and the Near-Real-Time RAN Intelligent Controller (Near-RT RIC). Using an open-source testbed built from srsRAN, Open5GS, FlexRIC and strongSwan (with liboqs), we compare three configurations: no IPsec, classical ECDH-based IPsec, and ML-KEM-based IPsec. The study focuses on IPsec tunnel-setup latency and the runtime behaviour of Near-RT RIC xApps under realistic signalling workloads. Results from repeated, automated runs show that ML-KEM integration adds a small overhead to tunnel establishment, which is approximately 3~5 ms in comparison to classical IPsec, while xApp operation and RIC control loops remain stable in our experiments. These findings indicate that ML-KEM based IPsec on the E2 interface is practically feasible and inform quantum-safe migration strategies for O-RAN deployments."}
{"id": "2601.20158", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20158", "abs": "https://arxiv.org/abs/2601.20158", "authors": ["Laura Baird", "Armin Moin"], "title": "Cascaded Vulnerability Attacks in Software Supply Chains", "comment": "IEEE/ACM International Conference on Software Engineering (ICSE) 2026 Extended Abstract", "summary": "Most of the current software security analysis tools assess vulnerabilities in isolation. However, sophisticated software supply chain security threats often stem from cascaded vulnerability and security weakness chains that span dependent components. Moreover, although the adoption of Software Bills of Materials (SBOMs) has been accelerating, downstream vulnerability findings vary substantially across SBOM generators and analysis tools. We propose a novel approach to SBOM-driven security analysis methods and tools. We model vulnerability relationships over dependency structure rather than treating scanner outputs as independent records. We represent enriched SBOMs as heterogeneous graphs with nodes being the SBOM components and dependencies, the known software vulnerabilities, and the known software security weaknesses. We then train a Heterogeneous Graph Attention Network (HGAT) to predict whether a component is associated with at least one known vulnerability. Since documented multi-vulnerability chains are scarce, we model cascade discovery as a link prediction problem over CVE pairs using a multi-layer perceptron neural network. This way, we produce ranked candidate links that can be composed into multi-step paths. The HGAT component classifier achieves an Accuracy of 91.03% and an F1-score of 74.02%."}
{"id": "2601.20400", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.20400", "abs": "https://arxiv.org/abs/2601.20400", "authors": ["Jean-Guillaume Dumas", "Aude Maignan", "Luiza Soezima"], "title": "Fuzzy Private Set Union via Oblivious Key Homomorphic Encryption Retrieval", "comment": null, "summary": "Private Set Multi-Party Computations are protocols that allow parties to jointly and securely compute functions: apart from what is deducible from the output of the function, the input sets are kept private. Then, a Private Set Union (PSU), resp. Intersection (PSI), is a protocol that allows parties to jointly compute the union, resp. the intersection, between their private sets. Now a structured PSI, is a PSI where some structure of the sets can allow for more efficient protocols. For instance in Fuzzy PSI, elements only need to be close enough, instead of equal, to be part of the intersection. We present in this paper, Fuzzy PSU protocols (FPSU), able to efficiently take into account approximations in the union. For this, we introduce a new efficient sub-protocol, called Oblivious Key Homomorphic Encryption Retrieval (OKHER), improving on Oblivious Key-Value Retrieval (OKVR) techniques in our setting. In the fuzzy context, the receiver set $X=\\{x_i\\}_{1..n}$ is replaced by ${\\mathcal B}_δ(X)$, the union of $n$ balls of dimension $d$ with radius $δ$, centered at the $x_i$. The sender set is just its $m$ points of dimension $d$. Then the FPSU functionality corresponds to $X \\sqcup \\{y \\in Y, y \\notin {\\mathcal B}_δ(X)\\}$. Thus, we formally define the FPSU functionality and security properties, and propose several protocols tuned to the patterns of the balls using the $l_\\infty$ distance. Using our OKHER routine and homomorphic encryption, we are for instance able to obtain a FPSU protocols with an asymptotic communication volume bound ranging from $O(dm\\log(δ{n}))$ to $O(d^2m\\log(δ^2n))$, depending on the receiver data set structure."}
{"id": "2601.20160", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20160", "abs": "https://arxiv.org/abs/2601.20160", "authors": ["Lukas Ottenhof", "Daniel Penner", "Abram Hindle", "Thibaud Lutellier"], "title": "How do Agents Refactor: An Empirical Study", "comment": "Accepted for publication in 23rd International Mining Software Repositories Conference (MSR 2026) : 5 pages, 4 tables", "summary": "Software development agents such as Claude Code, GitHub Copilot, Cursor Agent, Devin, and OpenAI Codex are being increasingly integrated into developer workflows. While prior work has evaluated agent capabilities for code completion and task automation, there is little work investigating how these agents perform Java refactoring in practice, the types of changes they make, and their impact on code quality. In this study, we present the first analysis of agentic refactoring pull requests in Java, comparing them to developer refactorings across 86 projects per group. Using RefactoringMiner and DesigniteJava 3.0, we identify refactoring types and detect code smells before and after refactoring commits. Our results show that agent refactorings are dominated by annotation changes (the 5 most common refactoring types done by agents are annotation related), in contrast to the diverse structural improvements typical of developers. Despite these differences in refactoring types, we find Cursor to be the only model to show a statistically significant increase in refactoring smells."}
{"id": "2601.20507", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.20507", "abs": "https://arxiv.org/abs/2601.20507", "authors": ["Philipp Mao", "Li Shi", "Marcel Busch", "Mathias Payer"], "title": "TÄMU: Emulating Trusted Applications at the (GlobalPlatform)-API Layer", "comment": null, "summary": "Mobile devices rely on Trusted Execution Environments (TEEs) to execute security-critical code and protect sensitive assets. This security-critical code is modularized in components known as Trusted Applications (TAs). Vulnerabilities in TAs can compromise the TEE and, thus, the entire system. However, the closed-source nature and fragmentation of mobile TEEs severely hinder dynamic analysis of TAs, limiting testing efforts to mostly static analyses. This paper presents TÄMU, a rehosting platform enabling dynamic analysis of TAs, specifically fuzzing and debugging, by interposing their execution at the API layer. To scale to many TAs across different TEEs, TÄMU leverages the standardization of TEE APIs, driven by the GlobalPlatform specifications. For the remaining TEE-specific APIs not shared across different TEEs, TÄMU introduces the notion of greedy high-level emulation, a technique that allows prioritizing manual rehosting efforts based on the potential coverage gain during fuzzing. We implement TÄMU and use it to emulate 67 TAs across four TEEs. Our fuzzing campaigns yielded 17 zero-day vulnerabilities across 11 TAs. These results indicate a deficit of dynamic analysis capabilities across the TEE ecosystem, where not even vendors with source code unlocked these capabilities for themselves. TÄMU promises to close this gap by bringing effective and practical dynamic analysis to the mobile TEE domain."}
{"id": "2601.20171", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20171", "abs": "https://arxiv.org/abs/2601.20171", "authors": ["Kazuma Yamasaki", "Joseph Ayobami Joshua", "Tasha Settewong", "Mahmoud Alfadel", "Kazumasa Shimari", "Kenichi Matsumoto"], "title": "Who Writes the Docs in SE 3.0? Agent vs. Human Documentation Pull Requests", "comment": "Comments: 5 pages, 5 figures. To appear in MSR 2026 Mining Challenge (April 2026). Code available at https://github.com/NAIST-SE/msr2026-docs-prs-replication", "summary": "As software engineering moves toward SE3.0, AI agents are increasingly used to carry out development tasks and contribute changes to software projects. It is therefore important to understand the extent of these contributions and how human developers review and intervene, since these factors shape the risks of delegating work to AI agents. While recent studies have examined how AI agents support software development tasks (e.g., code generation, issue resolution, and PR automation), their role in documentation tasks remains underexplored-even though documentation is widely consumed and shapes how developers understand and use software.\n  Using the AIDev, we analyze 1,997 documentation-related pull requests (PRs) authored by AI agents and human developers, where documentation PRs are those that create or modify project documentation artifacts. We find that AI agents submit substantially more documentation-related PRs than humans in the studied repositories. We further observe that agent-authored documentation edits are typically integrated with little follow-up modification from humans, raising concerns about review practices and the reliability of agent-generated documentation. Overall, while AI agents already contribute substantially to documentation workflows, our results suggest concerns for emerging challenges for documentation quality assurance and human-AI collaboration in SE3.0."}
{"id": "2601.20548", "categories": ["cs.CR", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.20548", "abs": "https://arxiv.org/abs/2601.20548", "authors": ["Kahraman Kostas", "Rabia Yasa Kostas"], "title": "IoT Device Identification with Machine Learning: Common Pitfalls and Best Practices", "comment": "4 pages", "summary": "This paper critically examines the device identification process using machine learning, addressing common pitfalls in existing literature. We analyze the trade-offs between identification methods (unique vs. class based), data heterogeneity, feature extraction challenges, and evaluation metrics. By highlighting specific errors, such as improper data augmentation and misleading session identifiers, we provide a robust guideline for researchers to enhance the reproducibility and generalizability of IoT security models."}
{"id": "2601.20223", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20223", "abs": "https://arxiv.org/abs/2601.20223", "authors": ["Aral de Moor", "Yana Hrynevich", "Hleb Badzeika", "Vladyslav Furda", "Marko Kojic", "Artem Savelev", "Kostadin Cvejoski", "Darya Rovdo", "Ekaterina Garanina"], "title": "Control Models for In-IDE Code Completion", "comment": "6 pages; accepted at IDE'26 co-located with ICSE'26", "summary": "We introduce control models for LLM-powered code completion in JetBrains IDEs: ML classifiers which trigger inference and filter the generated suggestions to better align them with users and reduce unnecessary requests. To this end, we evaluate boosting- and transformer-based architectures on an offline dataset of real code completions with n=98 users. We further evaluate the offline classification performance of our boosting-based approach on a range of syntactically diverse languages; and perform an A/B study in a production environment where they improve completion efficiency and quality metrics. With this study, we hope to demonstrate the potential in using auxiliary models for smarter in-IDE integration of LLM-driven features, highlight fruitful future directions, and open problems."}
{"id": "2601.20629", "categories": ["cs.CR", "cs.ET", "cs.OS"], "pdf": "https://arxiv.org/pdf/2601.20629", "abs": "https://arxiv.org/abs/2601.20629", "authors": ["Aditya Mitra", "Hamza Haroon", "Amaan Rais Shah", "Mohammad Elham Rasooli", "Bogdan Itsam Dorantes Nikolaev", "Tuğçe Ballı"], "title": "/dev/SDB: Software Defined Boot -- A novel standard for diskless booting anywhere and everywhere", "comment": null, "summary": "A computer is nothing but a device that processes the instructions supplied to it. However, as computers evolved, the instructions or codes started to be more complicated. As computers started to be used by non-technical people, it became imperative that the users be able to use the machine without having underlying knowledge of the code or the hardware. And operating system became the backbone for translating the inputs from the user to actual operation on the hardware. With the increasing complexity and the choices of operating system, it became clear that different groups of people, especially in an enterprise scenario, required different operating systems. Installing them all on a single machine, for shared computers became a difficult task, giving rise to network-based booting. But network-based booting was confined to only wired connectivity, keeping it restricted to very small geographical areas. The proposed system, /dev/SDB, is aimed at creating a standard where any user, anyone on the globe, can access the operating system authorized to them without having to be on the corporate network. It aims to offer the same over Wi-Fi as well as cellular connectivity, ensuring employees can truly work from anywhere, while following the policies for operating systems and without redundant hardware."}
{"id": "2601.20240", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20240", "abs": "https://arxiv.org/abs/2601.20240", "authors": ["Anthony Peruma", "Truman Choy", "Gerald Lee", "Italo De Oliveira Santos"], "title": "Understanding npm Developers' Practices, Challenges, and Recommendations for Secure Package Development", "comment": "The 19th IEEE/ACM International Conference on Cooperative and Human Aspects of Software Engineering - Research Track", "summary": "Background: The Node Package Manager (npm) ecosystem plays a vital role in modern software development by providing a vast repository of packages and tools that developers can use to implement their software systems. However, recent vulnerabilities in third-party packages have led to serious security breaches, compromising the integrity of applications that depend on them. Objective: This study investigates how npm package developers perceive and handle security in their work. We examined developers' understanding of security risks, the practices and tools they use, the barriers to stronger security measures, and their suggestions for improving the npm ecosystem's security. Method: We conducted an online survey with 75 npm package developers and undertook a mixed-methods approach to analyzing their responses. Results: While developers prioritize security, they perceive their packages as only moderately secure, with concerns about supply chain attacks, dependency vulnerabilities, and malicious code. Only 40% are satisfied with the current npm security tools due to issues such as alert fatigue. Automated methods such as two-factor authentication and npm audit are favored over code reviews. Many drop dependencies due to abandonment or vulnerabilities, and typically respond to vulnerabilities in their packages by quickly releasing patches. Key barriers include time constraints and high false-positive rates. To improve npm security, developers seek better detection tools, clearer documentation, stronger account protections, and more education initiatives. Conclusion: Our findings will benefit npm package contributors and maintainers by highlighting prevalent security challenges and promoting discussions on best practices to strengthen security and trustworthiness within the npm landscape."}
{"id": "2601.20638", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.20638", "abs": "https://arxiv.org/abs/2601.20638", "authors": ["David Schmidt", "Sebastian Schrittwieser", "Edgar Weippl"], "title": "Supply Chain Insecurity: Exposing Vulnerabilities in iOS Dependency Management Systems", "comment": null, "summary": "Dependency management systems are a critical component in software development, enabling projects to incorporate existing functionality efficiently. However, misconfigurations and malicious actors in these systems pose severe security risks, leading to supply chain attacks. Despite the widespread use of smartphone apps, the security of dependency management systems in the iOS software supply chain has received limited attention. In this paper, we focus on CocoaPods, one of the most widely used dependency management systems for iOS app development, but also examine the security of Carthage and Swift Package Manager (SwiftPM). We demonstrate that iOS apps expose internal package names and versions. Attackers can exploit this leakage to register previously unclaimed dependencies in CocoaPods, enabling remote code execution (RCE) on developer machines and build servers. Additionally, we show that attackers can compromise dependencies by reclaiming abandoned domains and GitHub URLs. Analyzing a dataset of 9,212 apps, we quantify how many apps are susceptible to these vulnerabilities. Further, we inspect the use of vulnerable dependencies within public GitHub repositories. Our findings reveal that popular apps disclose internal dependency information, enabling dependency confusion attacks. Furthermore, we show that hijacking a single CocoaPod library through an abandoned domain could compromise 63 iOS apps, affecting millions of users. Finally, we compare iOS dependency management systems with Cargo, Go modules, Maven, npm, and pip to discuss mitigation strategies for the identified threats."}
{"id": "2601.20382", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20382", "abs": "https://arxiv.org/abs/2601.20382", "authors": ["Klara Borowa", "Andrzej Zalewski", "Lech Madeyski"], "title": "How Software Engineering Research Overlooks Local Industry: A Smaller Economy Perspective", "comment": "Accepted for ICSE - FOSE 2026 (International Conference on Software Engineering: Future of Software Engineering)", "summary": "The software engineering researchers from countries with smaller economies, particularly non-English speaking ones, represent valuable minorities within the software engineering community. As researchers from Poland, we represent such a country. We analyzed the ICSE FOSE (Future of Software Engineering) community survey through reflexive thematic analysis to show our viewpoint on key software community issues. We believe that the main problem is the growing research-industry gap, which particularly impacts smaller communities and small local companies. Based on this analysis and our experiences, we present a set of recommendations for improvements that would enhance software engineering research and industrial collaborations in smaller economies."}
{"id": "2601.20716", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.20716", "abs": "https://arxiv.org/abs/2601.20716", "authors": ["Abylay Satybaldy", "Kamil Tylinski", "Jiahua Xu"], "title": "Decentralized Identity in Practice: Benchmarking Latency, Cost, and Privacy", "comment": null, "summary": "Decentralized Identifiers (DIDs) are increasingly deployed on distributed ledgers, yet systematic cross-platform evidence on their operational behavior remains limited. We present an empirical benchmarking study of three prominent ledger-based DID methods - Ethereum, Hedera, and XRP Ledger - using reference Software Development Kits (SDKs) under a unified experimental setup. We measure latency, transaction cost, and on-chain metadata exposure, normalizing latency by each platform's block or consensus interval and cost by its native value transfer fee. Privacy leakage is quantified using a Metadata-Leakage Score (MLS), an entropy-based measure expressed in bits per operation.\n  Our results reveal distinct architectural trade-offs. Ethereum enables near-instant, off-chain DID creation, but incurs the highest latency and cost for on-chain lifecycle operations. XRPL delivers deterministic and stable latency with fixed, low fees, yet exhibits higher metadata leakage due to more verbose transaction payloads. Hedera achieves the lowest on-chain latency and low fees with minimal metadata leakage, while occasional variance arises from SDK-side processing and confirmation pipelines.\n  Overall, the findings show that ledger architecture and SDK workflows play a major role in shaping DID latency, cost, and metadata exposure, complementing the effects of the underlying consensus mechanism. These results provide evidence-based insights to support informed selection and configuration of DID systems under performance and privacy constraints."}
{"id": "2601.20394", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20394", "abs": "https://arxiv.org/abs/2601.20394", "authors": ["Giovanna Broccia", "Maurice H. ter Beek", "Walter Cazzola", "Luca Favalli", "Francesco Bertolotti", "Alessio Ferrari"], "title": "Comprehension vs. Adoption: Evaluating a Language Workbench Through a Family of Experiments", "comment": null, "summary": "Language workbenches are tools that enable the definition, reuse, and composition of programming languages and their ecosystems, aiming to streamline language development. To facilitate their adoption by language designers, the comprehensibility of the language used to define other languages is an important aspect to evaluate. Moreover, considering that language workbenches are relatively new tools, user acceptance emerges as a crucial factor to be accounted for during their assessment. Current literature often neglects user-centred aspects like comprehensibility and acceptance in the assessment of this breed of tools. This paper addresses this gap through a family of experiments assessing Neverlang, a modular language workbench. The study adopts a tailored version of the Method Evaluation Model (MEM) to evaluate the comprehensibility of Neverlang's meta-language and programs, as well as user acceptance in terms of perceived ease of use, perceived usefulness, and intention to use. It also investigates the relationships among these dimensions. The experiments were conducted in three iterations involving participants from academia. The results reveal that users demonstrate sufficient comprehension of Neverlang's meta-language, particularly concerning its syntax, express a favourable perception of its usefulness, and indicate their intention to use it. However, the results also indicate that Neverlang's ease of use remains a challenge. Additionally, variations in the perceived ease of use and perceived usefulness, whether low or high, influence the users' intention to use the tool. Surprisingly, no significant correlation is found between comprehensibility and user acceptance. Notably, higher comprehensibility of the meta-language does not necessarily translate into greater acceptance, underscoring the complex interplay between comprehension and adoption."}
{"id": "2601.20404", "categories": ["cs.SE", "cs.AI", "cs.ET", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.20404", "abs": "https://arxiv.org/abs/2601.20404", "authors": ["Jai Lal Lulla", "Seyedmoein Mohsenimofidi", "Matthias Galster", "Jie M. Zhang", "Sebastian Baltes", "Christoph Treude"], "title": "On the Impact of AGENTS.md Files on the Efficiency of AI Coding Agents", "comment": "5 pages, 1 figure, 1 table", "summary": "AI coding agents such as Codex and Claude Code are increasingly used to autonomously contribute to software repositories. However, little is known about how repository-level configuration artifacts affect operational efficiency of the agents. In this paper, we study the impact of AGENTS.md files on the runtime and token consumption of AI coding agents operating on GitHub pull requests. We analyze 10 repositories and 124 pull requests, executing agents under two conditions: with and without an AGENTS.md file. We measure wall-clock execution time and token usage during agent execution. Our results show that the presence of AGENTS.md is associated with a lower median runtime ($Δ28.64$%) and reduced output token consumption ($Δ16.58$%), while maintaining a comparable task completion behavior. Based on these results, we discuss immediate implications for the configuration and deployment of AI coding agents in practice, and outline a broader research agenda on the role of repository-level instructions in shaping the behavior, efficiency, and integration of AI coding agents in software development workflows."}
{"id": "2601.20415", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20415", "abs": "https://arxiv.org/abs/2601.20415", "authors": ["Jon Marcos-Mercadé", "Unai Lopez-Novoa", "Mikel Egaña Aranguren"], "title": "An Empirical Evaluation of Modern MLOps Frameworks", "comment": "Supplementary code is available in the following GitHub repository: https://github.com/Jonmaa/MLOps", "summary": "Given the increasing adoption of AI solutions in professional environments, it is necessary for developers to be able to make informed decisions about the current tool landscape. This work empirically evaluates various MLOps (Machine Learning Operations) tools to facilitate the management of the ML model lifecycle: MLflow, Metaflow, Apache Airflow, and Kubeflow Pipelines. The tools are evaluated by assessing the criteria of Ease of installation, Configuration flexibility, Interoperability, Code instrumentation complexity, result interpretability, and Documentation when implementing two common ML scenarios: Digit classifier with MNIST and Sentiment classifier with IMDB and BERT. The evaluation is completed by providing weighted results that lead to practical conclusions on which tools are best suited for different scenarios."}
{"id": "2601.20459", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20459", "abs": "https://arxiv.org/abs/2601.20459", "authors": ["Mugdha Khedkar", "Michael Schlichtig", "Mohamed Soliman", "Eric Bodden"], "title": "Challenges in Android Data Disclosure: An Empirical Study", "comment": "Accepted at MOBILESoft 2026 Research Track", "summary": "Current legal frameworks enforce that Android developers accurately report the data their apps collect. However, large codebases can make this reporting challenging. This paper employs an empirical approach to understand developers' experience with Google Play Store's Data Safety Section (DSS) form.\n  We first survey 41 Android developers to understand how they categorize privacy-related data into DSS categories and how confident they feel when completing the DSS form. To gain a broader and more detailed view of the challenges developers encounter during the process, we complement the survey with an analysis of 172 online developer discussions, capturing the perspectives of 642 additional developers. Together, these two data sources represent insights from 683 developers.\n  Our findings reveal that developers often manually classify the privacy-related data their apps collect into the data categories defined by Google-or, in some cases, omit classification entirely-and rely heavily on existing online resources when completing the form. Moreover, developers are generally confident in recognizing the data their apps collect, yet they lack confidence in translating this knowledge into DSS-compliant disclosures. Key challenges include issues in identifying privacy-relevant data to complete the form, limited understanding of the form, and concerns about app rejection due to discrepancies with Google's privacy requirements.\n  These results underscore the need for clearer guidance and more accessible tooling to support developers in meeting privacy-aware reporting obligations."}
{"id": "2601.20615", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20615", "abs": "https://arxiv.org/abs/2601.20615", "authors": ["Yanlin Wang", "Jiadong Wu", "Tianyue Jiang", "Mingwei Liu", "Jiachi Chen", "Chong Wang", "Ensheng Shi", "Xilin Liu", "Yuchi Ma", "Zibin Zheng"], "title": "DRAINCODE: Stealthy Energy Consumption Attacks on Retrieval-Augmented Code Generation via Context Poisoning", "comment": "12 pages, 4 figures", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in code generation by leveraging retrieval-augmented generation (RAG) methods. However, the computational costs associated with LLM inference, particularly in terms of latency and energy consumption, have received limited attention in the security context. This paper introduces DrainCode, the first adversarial attack targeting the computational efficiency of RAG-based code generation systems. By strategically poisoning retrieval contexts through a mutation-based approach, DrainCode forces LLMs to produce significantly longer outputs, thereby increasing GPU latency and energy consumption. We evaluate the effectiveness of DrainCode across multiple models. Our experiments show that DrainCode achieves up to an 85% increase in latency, a 49% increase in energy consumption, and more than a 3x increase in output length compared to the baseline. Furthermore, we demonstrate the generalizability of the attack across different prompting strategies and its effectiveness compared to different defenses. The results highlight DrainCode as a potential method for increasing the computational overhead of LLMs, making it useful for evaluating LLM security in resource-constrained environments. We provide code and data at https://github.com/DeepSoftwareAnalytics/DrainCode."}
{"id": "2601.20662", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20662", "abs": "https://arxiv.org/abs/2601.20662", "authors": ["Julien Malka", "Arnout Engelen"], "title": "Lila: Decentralized Build Reproducibility Monitoring for the Functional Package Management Model", "comment": null, "summary": "Ensuring the integrity of software build artifacts is an increasingly important concern for modern software engineering, driven by increasingly sophisticated attacks on build systems, distribution channels, and development infrastructures. Reproducible builds $\\unicode{x2013}$ where binaries built independently from the same source code can be verified to be bit-for-bit identical to the distributed artifacts $\\unicode{x2013}$ provide a principled foundation for transparency and trust in software distribution.\n  Despite their potential, the large-scale adoption of reproducible builds faces two significant challenges: achieving high reproducibility rates across vast software collections and establishing reproducibility monitoring infrastructure that can operate at very large scale. While recent studies have shown that high reproducibility rates are achievable at scale $\\unicode{x2013}$ demonstrated by the Nix ecosystem achieving over 90% reproducibility on more than 80,000 packages $\\unicode{x2013}$ the problem of effective reproducibility monitoring remains largely unsolved.\n  In this work, we address the reproducibility monitoring challenge by introducing Lila, a decentralized system for reproducibility assessment tailored to the functional package management model. Lila enables distributed reporting of build results and aggregation into a reproducibility database, benefiting both practitioners and future empirical build reproducibility studies."}
{"id": "2601.20755", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20755", "abs": "https://arxiv.org/abs/2601.20755", "authors": ["Bohua Zou", "Debayan Roy", "Dhimankumar Yogesh Airao", "Weihao Xu", "Binqi Sun", "Yutao Liu", "Haibo Chen"], "title": "ProfInfer: An eBPF-based Fine-Grained LLM Inference Profiler", "comment": "Accepted in the 9th Annual Conference on Machine Learning and Systems (MLSys 2026)", "summary": "As large language models (LLMs) move from research to production, understanding how inference engines behave in real time has become both essential and elusive. Unlike general-purpose engines such as ONNX Runtime, today's LLM inference systems offer little operator-level visibility, leaving developers blind to where time and resources go. Even basic questions -- is this workload memory-bound or compute-bound? -- often remain unanswered. To close this gap, we develop a fine-grained, non-intrusive profiling framework for modern LLM inference engines, exemplified by llama.cpp but applicable to similar runtime architectures. Built on extended Berkeley Packet Filter (eBPF) technology, our system dynamically attaches probes to runtime functions across multiple layers -- without modifying or recompiling the source. It transforms collected traces into rich visualizations of operators, graphs, timelines, and hardware counter trends, exposing how dense inference, Mixture-of-Experts routing, and operator offloading behave in practice. With less than 4% runtime overhead and high profiling fidelity, our framework makes LLM inference both transparent and diagnosable, turning performance profiling into a practical tool for optimization, scheduling, and resource-aware deployment."}
{"id": "2601.20810", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20810", "abs": "https://arxiv.org/abs/2601.20810", "authors": ["Shahd Seddik", "Fahd Seddik", "Iman Saberi", "Fatemeh Fard", "Minh Hieu Huynh", "Patanamon Thongtanunam"], "title": "Context-Augmented Code Generation Using Programming Knowledge Graphs", "comment": null, "summary": "Large Language Models (LLMs) excel at code generation but struggle with complex problems. Retrieval-Augmented Generation (RAG) mitigates this issue by integrating external knowledge, yet retrieval models often miss relevant context, and generation models hallucinate with irrelevant data. We propose Programming Knowledge Graph (PKG) for semantic representation and fine-grained retrieval of code and text. Our approach enhances retrieval precision through tree pruning and mitigates hallucinations via a re-ranking mechanism that integrates non-RAG solutions. Structuring external data into finer-grained nodes improves retrieval granularity. Evaluations on HumanEval and MBPP show up to 20% pass@1 accuracy gains and a 34% improvement over baselines on MBPP. Our findings demonstrate that our proposed PKG approach along with re-ranker effectively address complex problems while maintaining minimal negative impact on solutions that are already correct without RAG. The replication package is published at https://github.com/iamshahd/ProgrammingKnowledgeGraph"}
