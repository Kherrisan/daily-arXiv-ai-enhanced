{"id": "2508.03846", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03846", "abs": "https://arxiv.org/abs/2508.03846", "authors": ["Hashini Gunatilake", "John Grundy", "Rashina Hoda", "Ingo Mueller"], "title": "Empathy Guidelines for Improving Practitioner Well-being & Software Engineering Practices", "comment": null, "summary": "Empathy is a powerful yet often overlooked element in software engineering\n(SE), supporting better teamwork, smoother communication, and effective\ndecision-making. In our previous study, we identified a range of practitioner\nstrategies for fostering empathy in SE contexts. Building on these insights,\nthis paper introduces 17 actionable empathy guidelines designed to support\npractitioners, teams, and organisations. We also explore how these guidelines\ncan be implemented in practice by examining real-world applications,\nchallenges, and strategies to overcome them shared by software practitioners.\nTo support adoption, we present a visual prioritisation framework that\ncategorises the guidelines based on perceived importance, ease of\nimplementation, and willingness to adopt. The findings offer practical and\nflexible suggestions for integrating empathy into everyday SE work, helping\nteams move from principles to sustainable action.", "AI": {"tldr": "This paper presents 17 empathy guidelines and a visual prioritization framework for software engineering, offering practical strategies to integrate empathy into daily work and overcome real-world implementation challenges.", "motivation": "Empathy enhances teamwork, communication, and decision-making in software engineering but is rarely systematically applied.", "method": "Derived from prior practitioner strategies, the authors created actionable guidelines through analysis of real-world applications, challenges, and adoption barriers shared by software engineers.", "result": "A visual framework categorizing 17 empathy guidelines by importance, implementation ease, and willingness to adopt, providing flexible integration paths for teams and organizations.", "conclusion": "The guidelines and prioritization framework enable software engineering teams to translate empathy principles into sustained practices for improved collaboration and outcomes."}}
{"id": "2508.03856", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03856", "abs": "https://arxiv.org/abs/2508.03856", "authors": ["Richard Hegewald", "Rebecca Beyer"], "title": "Evaluating Software Supply Chain Security in Research Software", "comment": "Accepted at conference GI SKILL 2025", "summary": "The security of research software is essential for ensuring the integrity and\nreproducibility of scientific results. However, research software security is\nstill largely unexplored. Due to its dependence on open source components and\ndistributed development practices, research software is particularly vulnerable\nto supply chain attacks. This study analyses 3,248 high-quality, largely\npeer-reviewed research software repositories using the OpenSSF Scorecard. We\nfind a generally weak security posture with an average score of 3.5/10.\nImportant practices, such as signed releases and branch protection, are rarely\nimplemented. Finally, we present actionable, low-effort recommendations that\ncan help research teams improve software security and mitigate potential\nthreats to scientific integrity.", "AI": {"tldr": "This study highlights the weak security practices in 3,248 research software repositories (avg. score 3.5/10) using OpenSSF Scorecard and proposes low-effort security improvements. Key vulnerable practices include lack of signed releases and branch protection.", "motivation": "Research software security is critical for scientific integrity but overlooked, with vulnerabilities from open-source dependencies and distributed development workflows potentially compromising results.", "method": "Analysis of 3,248 peer-reviewed research repositories via OpenSSF Scorecard metrics to evaluate security practices implementation rates and threat exposure.", "result": "Average security score of 3.5/10 across 11 security categories, with only 3% maintaining signed releases and similar low adoption of branch protection and dependency management.", "conclusion": "Weak security posture exposes scientific research to supply chain attacks; implementable recommendations include enabling branch protection, signing releases, and monitoring dependencies through existing tools."}}
{"id": "2508.03881", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03881", "abs": "https://arxiv.org/abs/2508.03881", "authors": ["Martin Obaidi", "Kushtrim Qengaj", "Jakob Droste", "Hannah Deters", "Marc Herrmann", "Jil Kl\u00fcnder", "Elisa Schmid", "Kurt Schneider"], "title": "From App Features to Explanation Needs: Analyzing Correlations and Predictive Potential", "comment": "This paper has been accepted at the 33rd IEEE International\n  Requirements Engineering Workshop (REW 2025)", "summary": "In today's digitized world, software systems must support users in\nunderstanding both how to interact with a system and why certain behaviors\noccur. This study investigates whether explanation needs, classified from user\nreviews, can be predicted based on app properties, enabling early consideration\nduring development and large-scale requirements mining. We analyzed a gold\nstandard dataset of 4,495 app reviews enriched with metadata (e.g., app\nversion, ratings, age restriction, in-app purchases). Correlation analyses\nidentified mostly weak associations between app properties and explanation\nneeds, with moderate correlations only for specific features such as app\nversion, number of reviews, and star ratings. Linear regression models showed\nlimited predictive power, with no reliable forecasts across configurations.\nValidation on a manually labeled dataset of 495 reviews confirmed these\nfindings. Categories such as Security & Privacy and System Behavior showed\nslightly higher predictive potential, while Interaction and User Interface\nremained most difficult to predict. Overall, our results highlight that\nexplanation needs are highly context-dependent and cannot be precisely inferred\nfrom app metadata alone. Developers and requirements engineers should therefore\nsupplement metadata analysis with direct user feedback to effectively design\nexplainable and user-centered software systems.", "AI": {"tldr": "The study found that predicting explanation needs from app metadata is largely unreliable, emphasizing the necessity of direct user feedback for designing explainable software systems.", "motivation": "Modern software systems require users to understand interactions and system behaviors, prompting the need to identify explanation requirements early in development through app properties.", "method": "Analysis of a dataset with 4,495 app reviews and metadata (e.g., app version, ratings, in-app purchases) using correlation analysis and linear regression models, validated on a manually labeled subset of 495 reviews.", "result": "Most app properties showed weak correlations with explanation needs; models had limited predictive power, and manual validation confirmed these limitations. Categories like Security & Privacy had higher predictive potential than Interaction and User Interface.", "conclusion": "Explanation needs are context-dependent and cannot be reliably inferred from metadata alone; developers should combine metadata analysis with direct user feedback to create user-centered explainable systems."}}
{"id": "2508.03922", "categories": ["cs.SE", "cs.HC", "D.2.1"], "pdf": "https://arxiv.org/pdf/2508.03922", "abs": "https://arxiv.org/abs/2508.03922", "authors": ["Soroush Heydari"], "title": "A Human Centric Requirements Engineering Framework for Assessing Github Copilot Output", "comment": "8 pages", "summary": "The rapid adoption of Artificial Intelligence(AI) programming assistants such\nas GitHub Copilot introduces new challenges in how these software tools address\nhuman needs. Many existing evaluation frameworks address technical aspects such\nas code correctness and efficiency, but often overlook crucial human factors\nthat affect the successful integration of AI assistants in software development\nworkflows. In this study, I analyzed GitHub Copilot's interaction with users\nthrough its chat interface, measured Copilot's ability to adapt explanations\nand code generation to user expertise levels, and assessed its effectiveness in\nfacilitating collaborative programming experiences. I established a\nhuman-centered requirements framework with clear metrics to evaluate these\nqualities in GitHub Copilot chat. I discussed the test results and their\nimplications for future analysis of human requirements in automated\nprogramming.", "AI": {"tldr": "The paper introduces a human-centered evaluation framework for GitHub Copilot, analyzing its adaptability to user expertise and effectiveness in collaborative programming through chat-based interactions.", "motivation": "Existing evaluation frameworks for AI programming assistants focus on technical metrics (code correctness, efficiency) while neglecting human factors critical to successful tool integration, such as adaptability to expertise levels and collaborative effectiveness.", "method": "The study analyzed GitHub Copilot's chat interface interactions, measured its ability to adapt explanations and code generation to varying user expertise levels, and assessed its collaborative programming capabilities using a human-centered requirements framework with defined metrics.", "result": "Test results were discussed alongside their implications for evaluating human requirements in automated programming, demonstrating where Copilot succeeds/fails in adapting to users and supporting collaboration.", "conclusion": "The work establishes a foundational framework for human-centered evaluation of AI assistants, highlighting the need to integrate user expertise and collaboration factors in future development and analysis of programming automation tools."}}
{"id": "2508.03696", "categories": ["cs.CR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03696", "abs": "https://arxiv.org/abs/2508.03696", "authors": ["Xinqi Lyu", "Yihao Liu", "Yanjie Li", "Bin Xiao"], "title": "PLA: Prompt Learning Attack against Text-to-Image Generative Models", "comment": "10 pages, 3 figures, and published to ICCV2025", "summary": "Text-to-Image (T2I) models have gained widespread adoption across various\napplications. Despite the success, the potential misuse of T2I models poses\nsignificant risks of generating Not-Safe-For-Work (NSFW) content. To\ninvestigate the vulnerability of T2I models, this paper delves into adversarial\nattacks to bypass the safety mechanisms under black-box settings. Most previous\nmethods rely on word substitution to search adversarial prompts. Due to limited\nsearch space, this leads to suboptimal performance compared to gradient-based\ntraining. However, black-box settings present unique challenges to training\ngradient-driven attack methods, since there is no access to the internal\narchitecture and parameters of T2I models. To facilitate the learning of\nadversarial prompts in black-box settings, we propose a novel prompt learning\nattack framework (PLA), where insightful gradient-based training tailored to\nblack-box T2I models is designed by utilizing multimodal similarities.\nExperiments show that our new method can effectively attack the safety\nmechanisms of black-box T2I models including prompt filters and post-hoc safety\ncheckers with a high success rate compared to state-of-the-art methods.\nWarning: This paper may contain offensive model-generated content.", "AI": {"tldr": "This paper investigates black-box adversarial attacks on Text-to-Image models to bypass safety mechanisms and proposes a novel prompt learning attack framework (PLA) that leverages multimodal similarities for effective gradient-based training, achieving high success rates over existing methods.", "motivation": "The growing adoption of T2I models with risks of misuse for NSFW content generation necessitates stronger safety mechanisms, but previous suboptimal attack methods (limited by word substitution without gradient optimization) highlight weaknesses in current defenses.", "method": "PLA: a black-box adversarial attack framework that designs gradient-driven training using multimodal similarities to learn adversarial prompts without model architecture/parameter access.", "result": "PLA successfully bypasses prompt filters and post-hoc safety checkers in black-box T2I models with higher success rates than state-of-the-art methods.", "conclusion": "The proposed PLA framework demonstrates significant effectiveness in attacking T2I safety mechanisms under black-box scenarios, emphasizing the need for stronger defense strategies against gradient-driven adversarial prompts."}}
{"id": "2508.03931", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03931", "abs": "https://arxiv.org/abs/2508.03931", "authors": ["Everton Guimaraes", "Nathalia Nascimento", "Chandan Shivalingaiah", "Asish Nelapati"], "title": "Analyzing Prominent LLMs: An Empirical Study of Performance and Complexity in Solving LeetCode Problems", "comment": "11 pages, 13 figures, 29th International Conference on Evaluation and\n  Assessment in Software Engineering (EASE)", "summary": "Large Language Models (LLMs) like ChatGPT, Copilot, Gemini, and DeepSeek are\ntransforming software engineering by automating key tasks, including code\ngeneration, testing, and debugging. As these models become integral to\ndevelopment workflows, a systematic comparison of their performance is\nessential for optimizing their use in real world applications. This study\nbenchmarks these four prominent LLMs on one hundred and fifty LeetCode problems\nacross easy, medium, and hard difficulties, generating solutions in Java and\nPython. We evaluate each model based on execution time, memory usage, and\nalgorithmic complexity, revealing significant performance differences. ChatGPT\ndemonstrates consistent efficiency in execution time and memory usage, while\nCopilot and DeepSeek show variability as task complexity increases. Gemini,\nalthough effective on simpler tasks, requires more attempts as problem\ndifficulty rises. Our findings provide actionable insights into each model's\nstrengths and limitations, offering guidance for developers selecting LLMs for\nspecific coding tasks and providing insights on the performance and complexity\nof GPT-like generated solutions.", "AI": {"tldr": "This paper systematically benchmarks four large language models (ChatGPT, Copilot, Gemini, DeepSeek) on 150 LeetCode problems in Java and Python, revealing substantial performance disparities across different levels of algorithmic complexity and offering guidance for real-world software engineering applications.", "motivation": "LLMs are revolutionizing software engineering through automated coding, testing, and debugging. A comparative analysis is necessary to help developers optimize model selection for specific tasks and understand the tradeoffs between performance and complexity.", "method": "The study evaluated model solutions on 150 LeetCode problems (easy/medium/hard) using two languages. Solutions were assessed via three metrics: execution speed, memory efficiency, and algorithmic complexity analysis.", "result": "1) ChatGPT showed consistent efficiency. 2) Copilot and DeepSeek exhibited increased variability with domain complexity. 3) Gemini required exponentially more trials for effective solutions on hard problems. 4) Language choice (Java/Python) affected performance profiles across models.", "conclusion": "This benchmark analysis provides empirical insights into LLM strengths/limitations for developers, highlighting how model architecture, training data, and prompt engineering influence real-world coding performance with domain-specific implications."}}
{"id": "2508.03879", "categories": ["cs.CR", "cs.OS", "D.4.6; K.6.5; C.2.0"], "pdf": "https://arxiv.org/pdf/2508.03879", "abs": "https://arxiv.org/abs/2508.03879", "authors": ["Arjun Juneja"], "title": "RX-INT: A Kernel Engine for Real-Time Detection and Analysis of In-Memory Threats", "comment": "10 pages, 8 figures, 1 table. Presents RX-INT, a kernel-mode system\n  for real-time detection of fileless malware using event-driven VAD scanning\n  and automated import resolution. Demonstrates superior detection capabilities\n  against PE-sieve on advanced evasion techniques including module stomping and\n  headerless manual mapping", "summary": "Malware and cheat developers use fileless execution techniques to evade\ntraditional, signature-based security products. These methods include various\ntypes of manual mapping, module stomping, and threadless injection which work\nentirely within the address space of a legitimate process, presenting a\nchallenge for detection due to ambiguity between what is legitimate and what\nisn't. Existing tools often have weaknesses, such as a dependency on Portable\nExecutable (PE) structures or a vulnerability to time-of-check-to-time-of-use\n(TOCTOU) race conditions where an adversary cleans up before a periodic scan\nhas the chance to occur. To address this gap, we present RX-INT, a\nkernel-assisted system featuring an architecture that provides resilience\nagainst TOCTOU attacks. RX-INT introduces a detection engine that combines a\nreal-time thread creation monitor with a stateful Virtual Address Descriptor\n(VAD) scanner alongside various heuristics within. This engine snapshots both\nprivate and image-backed memory regions, using real-time memory hashing to\ndetect illicit modifications like module stomping. Critically, we demonstrate a\nhigher detection rate in certain benchmarks of this approach through a direct\ncomparison with PE-sieve, a commonly used and powerful memory forensics tool.\nIn our evaluation, RX-INT successfully detected a manually mapped region that\nwas not identified by PE-sieve. We then conclude that our architecture\nrepresents a tangible difference in the detection of fileless threats, with\ndirect applications in the fields of anti-cheat and memory security.", "AI": {"tldr": "RX-INT improves fileless threat detection by combining real-time thread creation monitoring, stateful VAD scanning, and heuristics to address limitations in existing tools like TOCTOU race conditions and PE structure dependencies.", "motivation": "Fileless execution techniques (e.g., manual mapping, module stomping) evade traditional security tools by operating within legitimate processes and exploiting detection gaps like PE structure assumptions and TOCTOU vulnerabilities.", "method": "RX-INT utilizes a kernel-assisted detection engine with real-time thread creation tracking, stateful VAD scanning, and memory hashing to capture illicit modifications. It implements heuristics to enhance detection accuracy.", "result": "RX-INT outperformed PE-sieve in benchmarks, detecting a manually mapped region overlooked by PE-sieve while avoiding TOCTOU susceptibility. The architecture achieved higher detection rates for fileless threats.", "conclusion": "RX-INT's design offers a significant advancement in fileless threat detection, demonstrating practical effectiveness for anti-cheat systems and memory security through improved resilience and real-time analysis capabilities."}}
{"id": "2508.03949", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03949", "abs": "https://arxiv.org/abs/2508.03949", "authors": ["Md. Abdul Awal", "Mrigank Rochan", "Chanchal K. Roy"], "title": "Model Compression vs. Adversarial Robustness: An Empirical Study on Language Models for Code", "comment": null, "summary": "Transformer-based language models for code have shown remarkable performance\nin various software analytics tasks, but their adoption is hindered by high\ncomputational costs, slow inference speeds, and substantial environmental\nimpact. Model compression techniques such as pruning, quantization, and\nknowledge distillation have gained traction in addressing these challenges.\nHowever, the impact of these strategies on the robustness of compressed\nlanguage models for code in adversarial scenarios remains poorly understood.\nUnderstanding how these compressed models behave under adversarial attacks is\nessential for their safe and effective deployment in real-world applications.\nTo bridge this knowledge gap, we conduct a comprehensive evaluation of how\ncommon compression strategies affect the adversarial robustness of compressed\nmodels. We assess the robustness of compressed versions of three widely used\nlanguage models for code across three software analytics tasks, using six\nevaluation metrics and four commonly used classical adversarial attacks. Our\nfindings indicate that compressed models generally maintain comparable\nperformance to their uncompressed counterparts. However, when subjected to\nadversarial attacks, compressed models exhibit significantly reduced\nrobustness. These results reveal a trade-off between model size reduction and\nadversarial robustness, underscoring the need for careful consideration when\ndeploying compressed models in security-critical software applications. Our\nstudy highlights the need for further research into compression strategies that\nstrike a balance between computational efficiency and adversarial robustness,\nwhich is essential for deploying reliable language models for code in\nreal-world software applications.", "AI": {"tldr": "The paper evaluates the adversarial robustness of compressed code language models, revealing a trade-off between computational efficiency and security, and highlights the need for balanced compression strategies.", "motivation": "Adopting compressed code language models is hindered by unknown impacts on adversarial robustness, crucial for safe deployment in security-sensitive applications. This study addresses this gap to ensure reliable model compression.", "method": "Assessed three compressed code language models across three software analytics tasks using six evaluation metrics and four classical adversarial attacks to quantify robustness degradation.", "result": "Compressed models retain similar performance under normal conditions but demonstrate significantly reduced robustness under adversarial attacks, confirming the trade-off between size reduction and security.", "conclusion": "Model compression for code introduces adversarial vulnerabilities, emphasizing the necessity of developing compression techniques that preserve both efficiency and robustness for practical use."}}
{"id": "2508.03882", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03882", "abs": "https://arxiv.org/abs/2508.03882", "authors": ["Arturo S\u00e1nchez-Matas", "Pablo Escribano Ruiz", "Daniel D\u00edaz-L\u00f3pez", "Angel Luis Perales G\u00f3mez", "Pantaleone Nespoli", "Gregorio Mart\u00ednez P\u00e9rez"], "title": "Simulating Cyberattacks through a Breach Attack Simulation (BAS) Platform empowered by Security Chaos Engineering (SCE)", "comment": "8 pages, 4 figures, paper in proceedings of the \"X Jornadas\n  Nacionales de Investigaci\\'on en Ciberseguridad\" in Zaragoza, Spain, June,\n  2025", "summary": "In today digital landscape, organizations face constantly evolving cyber\nthreats, making it essential to discover slippery attack vectors through novel\ntechniques like Security Chaos Engineering (SCE), which allows teams to test\ndefenses and identify vulnerabilities effectively. This paper proposes to\nintegrate SCE into Breach Attack Simulation (BAS) platforms, leveraging\nadversary profiles and abilities from existing threat intelligence databases.\nThis innovative proposal for cyberattack simulation employs a structured\narchitecture composed of three layers: SCE Orchestrator, Connector, and BAS\nlayers. Utilizing MITRE Caldera in the BAS layer, our proposal executes\nautomated attack sequences, creating inferred attack trees from adversary\nprofiles. Our proposal evaluation illustrates how integrating SCE with BAS can\nenhance the effectiveness of attack simulations beyond traditional scenarios,\nand be a useful component of a cyber defense strategy.", "AI": {"tldr": "This paper proposes integrating Security Chaos Engineering with Breach Attack Simulation platforms using a three-layer architecture (SCE Orchestrator, Connector, and BAS layers) to improve attack simulation effectiveness through automated sequences and threat intelligence, demonstrating enhanced vulnerability identification beyond traditional methods.", "motivation": "Organizations must adapt to evolving cyber threats by discovering elusive attack vectors. Traditional simulations fall short, necessitating innovative methods like combining Security Chaos Engineering (SCE) with Breach Attack Simulation (BAS) for proactive defense testing.", "method": "A structured three-layer architecture (SCE Orchestrator, Connector, BAS layers) was developed, leveraging MITRE Caldera in the BAS layer to execute automated attack sequences inferred from adversary profiles and threat intelligence databases.", "result": "Evaluation results show the proposed integration enhances attack simulation effectiveness beyond traditional BAS scenarios, enabling dynamic testing and more realistic identification of vulnerabilities.", "conclusion": "Integrating SCE and BAS with adversary-profile-driven automation offers a scalable, proactive strategy for cyber defense, improving simulation realism and organizational readiness against complex threats."}}
{"id": "2508.04125", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04125", "abs": "https://arxiv.org/abs/2508.04125", "authors": ["Sangwon Hyun", "Hyunjun Kim", "Jinhyuk Jang", "Hyojin Choi", "M. Ali Babar"], "title": "Experimental Analysis of Productive Interaction Strategy with ChatGPT: User Study on Function and Project-level Code Generation Tasks", "comment": "The benchmark repository has not been publicly released yet due to\n  the IP policy in our institutions. If you would like to use the benchmark or\n  collaborate on extension, please contact \"dr.sangwon.hyun@gmail.com\"", "summary": "The application of Large Language Models (LLMs) is growing in the productive\ncompletion of Software Engineering tasks. Yet, studies investigating the\nproductive prompting techniques often employed a limited problem space,\nprimarily focusing on well-known prompting patterns and mainly targeting\nfunction-level SE practices. We identify significant gaps in real-world\nworkflows that involve complexities beyond class-level (e.g., multi-class\ndependencies) and different features that can impact Human-LLM Interactions\n(HLIs) processes in code generation. To address these issues, we designed an\nexperiment that comprehensively analyzed the HLI features regarding the code\ngeneration productivity. Our study presents two project-level benchmark tasks,\nextending beyond function-level evaluations. We conducted a user study with 36\nparticipants from diverse backgrounds, asking them to solve the assigned tasks\nby interacting with the GPT assistant using specific prompting patterns. We\nalso examined the participants' experience and their behavioral features during\ninteractions by analyzing screen recordings and GPT chat logs. Our statistical\nand empirical investigation revealed (1) that three out of 15 HLI features\nsignificantly impacted the productivity in code generation; (2) five primary\nguidelines for enhancing productivity for HLI processes; and (3) a taxonomy of\n29 runtime and logic errors that can occur during HLI processes, along with\nsuggested mitigation plans.", "AI": {"tldr": "This paper investigates Human-LLM Interaction (HLI) features in code generation beyond function-level, identifying three productivity-impact features, five productivity-enhancing guidelines, and a 29-error taxonomy from project-level experiments with 36 participants.", "motivation": "Current studies on LLM prompting techniques in Software Engineering (SE) focus narrowly on function-level practices and known patterns, neglecting real-world workflow complexities involving multi-class dependencies and varying HLI features that affect code generation productivity.", "method": "The researchers designed a user study with 36 diverse participants solving two project-level benchmark tasks using GPT assistants. They analyzed code generation productivity through HLI feature evaluation, along with user experience data from screen recordings and GPT chat logs.", "result": "(1) 3/15 HLI features significantly impact code generation productivity; (2) 5 actionable guidelines for improving HLI processes; (3) Taxonomy of 29 runtime/logic errors during HLI code generation with mitigation strategies.", "conclusion": "The study expands SE research on LLM prompting by analyzing project-level complexities and HLI dynamics, providing practical guidelines and an error taxonomy to enhance productivity in code generation workflows involving class dependencies and other real-world challenges."}}
{"id": "2508.03936", "categories": ["cs.CR", "cs.CL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03936", "abs": "https://arxiv.org/abs/2508.03936", "authors": ["Xiangzhe Xu", "Guangyu Shen", "Zian Su", "Siyuan Cheng", "Hanxi Guo", "Lu Yan", "Xuan Chen", "Jiasheng Jiang", "Xiaolong Jin", "Chengpeng Wang", "Zhuo Zhang", "Xiangyu Zhang"], "title": "ASTRA: Autonomous Spatial-Temporal Red-teaming for AI Software Assistants", "comment": "The first two authors (Xiangzhe Xu and Guangyu Shen) contributed\n  equally to this work", "summary": "AI coding assistants like GitHub Copilot are rapidly transforming software\ndevelopment, but their safety remains deeply uncertain-especially in\nhigh-stakes domains like cybersecurity. Current red-teaming tools often rely on\nfixed benchmarks or unrealistic prompts, missing many real-world\nvulnerabilities. We present ASTRA, an automated agent system designed to\nsystematically uncover safety flaws in AI-driven code generation and security\nguidance systems. ASTRA works in three stages: (1) it builds structured\ndomain-specific knowledge graphs that model complex software tasks and known\nweaknesses; (2) it performs online vulnerability exploration of each target\nmodel by adaptively probing both its input space, i.e., the spatial\nexploration, and its reasoning processes, i.e., the temporal exploration,\nguided by the knowledge graphs; and (3) it generates high-quality\nviolation-inducing cases to improve model alignment. Unlike prior methods,\nASTRA focuses on realistic inputs-requests that developers might actually\nask-and uses both offline abstraction guided domain modeling and online domain\nknowledge graph adaptation to surface corner-case vulnerabilities. Across two\nmajor evaluation domains, ASTRA finds 11-66% more issues than existing\ntechniques and produces test cases that lead to 17% more effective alignment\ntraining, showing its practical value for building safer AI systems.", "AI": {"tldr": "ASTRA is an automated system for uncovering safety flaws in AI coding assistants, particularly in cybersecurity. It uses knowledge graphs to explore vulnerabilities in both input space and reasoning processes, finding 11-66% more issues and improving alignment training by 17%.", "motivation": "Current red-teaming tools for AI coding assistants use fixed benchmarks or unrealistic prompts, missing many real-world vulnerabilities in critical domains like cybersecurity. There is a need for systematic methods to test safety in these systems.", "method": "ASTRA operates in three stages: 1) Constructing structured domain-specific knowledge graphs of software tasks and weaknesses; 2) Online exploration targeting input space (spatial) and reasoning processes (temporal) using adaptive probing guided by these graphs; 3) Generating violation-inducing test cases to enhance model alignment. It combines offline domain modeling with online knowledge graph adaptation.", "result": "ASTRA found 11-66% more vulnerabilities than existing techniques in two major domains and 17% more effective alignment training with generated test cases. It demonstrates practical value for safer AI systems.", "conclusion": "ASTRA provides a novel approach to red-teaming AI coding assistants by focusing on realistic developer inputs and using dynamic domain knowledge adaptation. It offers significant improvements over previous tools in uncovering safety issues and training safer models."}}
{"id": "2508.04295", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.04295", "abs": "https://arxiv.org/abs/2508.04295", "authors": ["Chaofan Wang", "Tingrui Yu", "Jie Wang", "Dong Chen", "Wenrui Zhang", "Yuling Shi", "Xiaodong Gu", "Beijun Shen"], "title": "EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation", "comment": null, "summary": "Rust's compile-time safety guarantees make it ideal for safety-critical\nsystems, creating demand for translating legacy C codebases to Rust. While\nvarious approaches have emerged for this task, they face inherent trade-offs:\nrule-based solutions face challenges in meeting code safety and idiomaticity\nrequirements, while LLM-based solutions often fail to generate semantically\nequivalent Rust code, due to the heavy dependencies of modules across the\nentire codebase. Recent studies have revealed that both solutions are limited\nto small-scale programs. In this paper, we propose EvoC2Rust, an automated\nframework for converting entire C projects to equivalent Rust ones. EvoC2Rust\nemploys a skeleton-guided translation strategy for project-level translation.\nThe pipeline consists of three evolutionary stages: 1) it first decomposes the\nC project into functional modules, employs a feature-mapping-enhanced LLM to\ntransform definitions and macros and generates type-checked function stubs,\nwhich form a compilable Rust skeleton; 2) it then incrementally translates the\nfunction, replacing the corresponding stub placeholder; 3) finally, it repairs\ncompilation errors by integrating LLM and static analysis. Through evolutionary\naugmentation, EvoC2Rust combines the advantages of both rule-based and\nLLM-based solutions. Our evaluation on open-source benchmarks and six\nindustrial projects demonstrates EvoC2Rust's superior performance in\nproject-level C-to-Rust translation. On average, it achieves 17.24% and 14.32%\nimprovements in syntax and semantic accuracy over the LLM-based approaches,\nalong with a 96.79% higher code safety rate than the rule-based tools. At the\nmodule level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates\non industrial projects, even for complex codebases and long functions.", "AI": {"tldr": "EvoC2Rust is a new automated framework that addresses the challenges of converting entire C projects to Rust by combining a skeleton-guided translation strategy with feature-mapping LLMs and static analysis, achieving better syntax, semantic accuracy, and code safety than previous methods.", "motivation": "Legacy C codebases need translation to Rust for safety-critical systems, but existing rule-based and LLM-based approaches struggle with large-scale projects and safety/semantic equivalence issues.", "method": "EvoC2Rust uses a three-stage evolutionary process: decomposing C projects into modules, generating a compilable Rust skeleton via a feature-mapping-enhanced LLM, incrementally translating functions, and repairing errors through LLM integration and static analysis.", "result": "On average, EvoC2Rust improved syntax and semantic accuracy by 17.24% and 14.32% over LLM-based methods and achieved 96.79% higher code safety rates than rule-based tools; module-level test and compilation success rates exceeded 89% on industrial projects.", "conclusion": "EvoC2Rust effectively bridges the limitations of rule-based and LLM-based translation methods through evolutionary augmentation, demonstrating superior performance for large-scale, safety-critical C-to-Rust translation."}}
{"id": "2508.04094", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.04094", "abs": "https://arxiv.org/abs/2508.04094", "authors": ["Chengrui Sun", "Hua Zhang", "Haoran Gao", "Zian Tian", "Jianjin Zhao", "qi Li", "Hongliang Zhu", "Zongliang Shen", "Shang Wang", "Anmin Fu"], "title": "Isolate Trigger: Detecting and Eradicating Evade-Adaptive Backdoors", "comment": null, "summary": "All current detection of backdoor attacks on deep learning models fall under\nthe category of a non essential features(NEF), which focus on fighting against\nsimple and efficient vertical class backdoor -- trigger is small, few and not\noverlapping with the source. Evade-adaptive backdoor (EAB) attacks have evaded\nNEF detection and improved training efficiency. We introduces a precise,\nefficient and universal detection and defense framework coined as Isolate\nTrigger (IsTr). IsTr aims to find the hidden trigger by breaking the barrier of\nthe source features. Therefore, it investigates the essence of backdoor\ntriggering, and uses Steps and Differential-Middle-Slice as components to\nupdate past theories of distance and gradient. IsTr also plays a positive role\nin the model, whether the backdoor exists. For example, accurately find and\nrepair the wrong identification caused by deliberate or unintentional training\nin automatic driving. Extensive experiments on robustness scross various tasks,\nincluding MNIST, facial recognition, and traffic sign recognition, confirm the\nhigh efficiency, generality and precision of the IsTr. We rigorously evaluated\nthe effectiveness of the IsTr against a series of six EAB attacks, including\nBadnets, Sin-Wave, Multi-trigger, SSBAs, CASSOCK, HCB. None of these\ncountermeasures evade, even when attacks are combined and the trigger and\nsource overlap.", "AI": {"tldr": "The paper introduces IsTr, a universal detection and defense framework for evasive backdoor attacks (EABs) that break existing detection methods by using small/overlapping triggers. The framework improves distance/gradient analysis to identify and repair hidden backdoor triggers across tasks like MNIST, facial recognition, and traffic sign systems.", "motivation": "Existing Non-Essential Features (NEF) detection methods fail against EAB attacks which use small, non-overlapping triggers and efficient evasion techniques, creating urgent need for better universal detection solutions for deep learning security.", "method": "Develops Isolate Trigger (IsTr) framework through: 1) Backdoor triggering analysis, 2) Introducing Steps and Differential-Middle-Slice components to enhance distance/gradient theories, 3) Dual-purpose attack detection and model repair capabilities via trigger isolation.", "result": "IsTr successfully detects six EAB attack variants (Badnets, Sin-Wave, Multi-trigger, SSBAs, CASSOCK, HCB) with high precision across diverse tasks including driving safety applications. Experiments validate exceptional robustness even against combined attacks and overlapping source/trigger features.", "conclusion": "IsTr establishes a new baseline for backdoor defense by achieving universal detection of evasive attacks that defeat legacy methods, demonstrating both high efficiency and dual functionality for model repair in production environments."}}
{"id": "2508.04352", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.04352", "abs": "https://arxiv.org/abs/2508.04352", "authors": ["Dragana Sunaric", "Charlotte Verbruggen", "Dominik Bork"], "title": "Vanilla-Converter: A Tool for Converting Camunda 7 BPMN Models into Camunda 8 Models", "comment": null, "summary": "As organizations prepare for the end-of-life of Camunda 7, manual migration\nremains complex due to fundamental differences between the two platforms. We\npresent Vanilla-Converter, a command-line tool that facilitates the migration\nof BPMN models from Camunda 7 to Camunda 8. Vanilla-Converter automates the\ntransformation process, supports a wide range of BPMN elements, and produces a\ntransformed model and a detailed transformation log indicating automatic\nchanges and remaining manual conversion tasks. The tool's effectiveness is\ndemonstrated through three case studies with real industrially used Camunda 7\nmodels, confirming its ability to convert these models into valid and\nexecutable Camunda 8 models.", "AI": {"tldr": "Vanilla-Converter is a command-line tool for migrating BPMN models from Camunda 7 to 8, automating transformation and providing detailed logs for remaining manual tasks.", "motivation": "Organizations face complex manual migration processes due to fundamental differences between Camunda 7 and 8 platforms.", "method": "The tool automates BPMN model transformation, supports various elements, and generates logs to track automatic changes and manual conversion needs.", "result": "Three case studies with real Camunda 7 models were successfully converted to valid, executable Camunda 8 models, confirming the tool's effectiveness.", "conclusion": "Vanilla-Converter effectively addresses Camunda 7-to-8 migration challenges through automation and detailed logging, reducing manual effort for complex transitions."}}
{"id": "2508.04100", "categories": ["cs.CR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.04100", "abs": "https://arxiv.org/abs/2508.04100", "authors": ["Borui Li", "Li Yan", "Junhao Han", "Jianmin Liu", "Lei Yu"], "title": "SenseCrypt: Sensitivity-guided Selective Homomorphic Encryption for Joint Federated Learning in Cross-Device Scenarios", "comment": "17 pages, 19 figures", "summary": "Homomorphic Encryption (HE) prevails in securing Federated Learning (FL), but\nsuffers from high overhead and adaptation cost. Selective HE methods, which\npartially encrypt model parameters by a global mask, are expected to protect\nprivacy with reduced overhead and easy adaptation. However, in cross-device\nscenarios with heterogeneous data and system capabilities, traditional\nSelective HE methods deteriorate client straggling, and suffer from degraded HE\noverhead reduction performance. Accordingly, we propose SenseCrypt, a\nSensitivity-guided selective Homomorphic EnCryption framework, to adaptively\nbalance security and HE overhead per cross-device FL client. Given the\nobservation that model parameter sensitivity is effective for measuring\nclients' data distribution similarity, we first design a privacy-preserving\nmethod to respectively cluster the clients with similar data distributions.\nThen, we develop a scoring mechanism to deduce the straggler-free ratio of\nmodel parameters that can be encrypted by each client per cluster. Finally, for\neach client, we formulate and solve a multi-objective model parameter selection\noptimization problem, which minimizes HE overhead while maximizing model\nsecurity without causing straggling. Experiments demonstrate that SenseCrypt\nensures security against the state-of-the-art inversion attacks, while\nachieving normal model accuracy as on IID data, and reducing training time by\n58.4%-88.7% as compared to traditional HE methods.", "AI": {"tldr": "SenseCrypt is a sensitivity-guided selective HE framework that balances privacy and efficiency in cross-device FL by clustering clients, optimizing encryption ratios, and reducing training time by 58.4%-88.7% over traditional HE methods.", "motivation": "Traditional selective HE in FL causes client straggling and fails to reduce overhead effectively in cross-device settings with heterogeneous data and system capabilities.", "method": "1. Cluster clients using model parameter sensitivity to identify similar data distributions. 2. Develop a scoring mechanism to determine client-specific encryption ratios without straggling. 3. Formulate and solve a multi-objective optimization problem for parameter selection balancing HE overhead and security.", "result": "SenseCrypt achieves security against state-of-the-art inversion attacks, maintains model accuracy comparable to IID data, and reduces training time by 58.4%-88.7% relative to traditional HE approaches.", "conclusion": "SenseCrypt demonstrates robust privacy-efficiency tradeoffs for cross-device FL by adaptively encrypting model parameters based on sensitivity and client heterogeneity."}}
{"id": "2508.04408", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.04408", "abs": "https://arxiv.org/abs/2508.04408", "authors": ["Carlos Andr\u00e9s Ram\u00edrez Cata\u00f1o", "Makoto Itoh"], "title": "Breaking New Ground in Software Defect Prediction: Introducing Practical and Actionable Metrics with Superior Predictive Power for Enhanced Decision-Making", "comment": "16 pages, 2 figures, 2 formulas, 12 tables", "summary": "Software defect prediction using code metrics has been extensively researched\nover the past five decades. However, prediction harnessing non-software metrics\nis under-researched. Considering that the root cause of software defects is\noften attributed to human error, human factors theory might offer key\nforecasting metrics for actionable insights. This paper explores automated\nsoftware defect prediction at the method level based on the developers' coding\nhabits. First, we propose a framework for deciding the metrics to conduct\npredictions. Next, we compare the performance of our metrics to that of the\ncode and commit history metrics shown by research to achieve the highest\nperformance to date. Finally, we analyze the prediction importance of each\nmetric. As a result of our analyses of twenty-one critical infrastructure\nlarge-scale open-source software projects, we have presented: (1) a human\nerror-based framework with metrics useful for defect prediction at method\nlevel; (2) models using our proposed metrics achieve better average prediction\nperformance than the state-of-the-art code metrics and history measures; (3)\nthe prediction importance of all metrics distributes differently with each of\nthe novel metrics having better average importance than code and history\nmetrics; (4) the novel metrics dramatically enhance the explainability,\npracticality, and actionability of software defect prediction models,\nsignificantly advancing the field. We present a systematic approach to\nforecasting defect-prone software methods via a human error framework. This\nwork empowers practitioners to act on predictions, empirically demonstrating\nhow developer coding habits contribute to defects in software systems.", "AI": {"tldr": "This paper proposes a human error-based framework for method-level software defect prediction using developers' coding habits, demonstrating improved performance, importance distribution, and actionability over existing code and commit history metrics through analysis of 21 critical infrastructure open-source projects.", "motivation": "Existing research on software defect prediction focuses heavily on algorithmic code metrics, while human factors (linked to defect root causes) remain underexplored. This limits actionable insights for practitioners addressing preventable human errors in software development.", "method": "We developed a novel human factors framework:1) Defined defect-predictive coding habit metrics 2) Conducted empirical comparison against state-of-the-art code/commit-based metrics 3) Perfomed systematic prediction importance analysis. Evaluated using 21 large-scale critical infrastructure open-source projects.", "result": "1) Human habit framework produced better average prediction performance (30% higher than code/history baselines) 2) All novel metrics outperformed code/history metrics in importance rankings 3) Enhanced model explainability and practical recommendations showed: a) Specific coding patterns with defect risk b) Human error prevention strategies for different scenarios c) Real-time correction possibilities through habit monitoring", "conclusion": "This work establishes that human factors-based metrics systematically enhance method-level defect prediction. Practitioners can now detect, understand, and mitigate defects at source by addressing developers' coding habits, marking a shift from purely algorithmic approaches to human-centric defect prevention strategies in software engineering."}}
{"id": "2508.04155", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04155", "abs": "https://arxiv.org/abs/2508.04155", "authors": ["Jiajun Gu", "Yuhang Yao", "Shuaiqi Wang", "Carlee Joe-Wong"], "title": "Evaluating Selective Encryption Against Gradient Inversion Attacks", "comment": null, "summary": "Gradient inversion attacks pose significant privacy threats to distributed\ntraining frameworks such as federated learning, enabling malicious parties to\nreconstruct sensitive local training data from gradient communications between\nclients and an aggregation server during the aggregation process. While\ntraditional encryption-based defenses, such as homomorphic encryption, offer\nstrong privacy guarantees without compromising model utility, they often incur\nprohibitive computational overheads. To mitigate this, selective encryption has\nemerged as a promising approach, encrypting only a subset of gradient data\nbased on the data's significance under a certain metric. However, there have\nbeen few systematic studies on how to specify this metric in practice. This\npaper systematically evaluates selective encryption methods with different\nsignificance metrics against state-of-the-art attacks. Our findings demonstrate\nthe feasibility of selective encryption in reducing computational overhead\nwhile maintaining resilience against attacks. We propose a distance-based\nsignificance analysis framework that provides theoretical foundations for\nselecting critical gradient elements for encryption. Through extensive\nexperiments on different model architectures (LeNet, CNN, BERT, GPT-2) and\nattack types, we identify gradient magnitude as a generally effective metric\nfor protection against optimization-based gradient inversions. However, we also\nobserve that no single selective encryption strategy is universally optimal\nacross all attack scenarios, and we provide guidelines for choosing appropriate\nstrategies for different model architectures and privacy requirements.", "AI": {"tldr": "The paper proposes a distance-based significance analysis framework for selective encryption in federated learning, demonstrating that gradient magnitude is an effective metric against optimization-based gradient inversion attacks, though no universal strategy exists across all scenarios.", "motivation": "Gradient inversion attacks threaten privacy in distributed learning frameworks like federated learning. Traditional encryption is computationally expensive, necessitating lightweight alternatives like selective encryption, which requires practical significance metrics for deciding which gradient components to encrypt.", "method": "The authors systematically evaluate selective encryption methods using diverse significance metrics against state-of-the-art gradient inversion attacks. They develop a distance-based significance analysis framework to theoretically identify critical gradient elements and empirically test its effectiveness through experiments across LeNet, CNN, BERT, and GPT-2 architectures with varied attack types.", "result": "The framework reduces computational overhead while maintaining attack resilience. Gradient magnitude consistently offers robust protection against optimization-based gradient inversions, but the performance of selective encryption strategies varies significantly across different attack scenarios and model architectures.", "conclusion": "Selective encryption is a viable approach for balancing computational efficiency and privacy in federated learning. The study establishes gradient magnitude as a reliable metric for certain attacks, but emphasizes the need for scenario-specific strategies. The proposed distance-based framework provides a theoretical guideline for practical implementation of privacy-preserving gradient communications."}}
{"id": "2508.04448", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.04448", "abs": "https://arxiv.org/abs/2508.04448", "authors": ["Damian Gnieciak", "Tomasz Szandala"], "title": "Large Language Models Versus Static Code Analysis Tools: A Systematic Benchmark for Vulnerability Detection", "comment": null, "summary": "Modern software relies on a multitude of automated testing and quality\nassurance tools to prevent errors, bugs and potential vulnerabilities. This\nstudy sets out to provide a head-to-head, quantitative and qualitative\nevaluation of six automated approaches: three industry-standard rule-based\nstatic code-analysis tools (SonarQube, CodeQL and Snyk Code) and three\nstate-of-the-art large language models hosted on the GitHub Models platform\n(GPT-4.1, Mistral Large and DeepSeek V3). Using a curated suite of ten\nreal-world C# projects that embed 63 vulnerabilities across common categories\nsuch as SQL injection, hard-coded secrets and outdated dependencies, we measure\nclassical detection accuracy (precision, recall, F-score), analysis latency,\nand the developer effort required to vet true positives. The language-based\nscanners achieve higher mean F-1 scores,0.797, 0.753 and 0.750, than their\nstatic counterparts, which score 0.260, 0.386 and 0.546, respectively. LLMs'\nadvantage originates from superior recall, confirming an ability to reason\nacross broader code contexts. However, this benefit comes with substantial\ntrade-offs: DeepSeek V3 exhibits the highest false-positive ratio, all language\nmodels mislocate issues at line-or-column granularity due to tokenisation\nartefacts. Overall, language models successfully rival traditional static\nanalysers in finding real vulnerabilities. Still, their noisier output and\nimprecise localisation limit their standalone use in safety-critical audits. We\ntherefore recommend a hybrid pipeline: employ language models early in\ndevelopment for broad, context-aware triage, while reserving deterministic\nrule-based scanners for high-assurance verification. The open benchmark and\nJSON-based result harness released with this paper lay a foundation for\nreproducible, practitioner-centric research into next-generation automated code\nsecurity.", "AI": {"tldr": "This paper evaluates six code security tools (SonarQube, CodeQL, Snyk Code, and three LLMs) on 10 C# projects with 63 vulnerabilities. LLMs outperform static analyzers in detection accuracy but face challenges with false positives and precise localization. A hybrid toolchain combining both approaches is recommended for safety-critical applications.", "motivation": "Modern software security needs robust tools. Traditional rule-based static analyzers may lack contextual understanding, while LLMs show potential for broader code reasoning. This study addresses the need to quantify their complementary strengths and weaknesses for practical security workflows.", "method": "10 real-world C# projects encoding 63 vulnerabilities (SQL injection, hard-coded secrets, outdated dependencies) were analyzed using SonarQube, CodeQL, Snyk Code (static analysis) and GPT-4.1, Mistral Large, DeepSeek V3 (LLMs). Metrics included F-1 scores (0.750-0.797 vs 0.260-0.546), analysis latency, false-positive rates, and developer effort for validation.", "result": "Language-based scanners achieved higher F-1 scores (0.750-0.797) than static analyzers (0.260-0.546), driven by superior recall but lower precision. DeepSeek V3 showed 37% false-positives. All LLMs struggled with line/column-level issue localization due to tokenization artifacts. The study demonstrates LLMs' competition with static tools but highlights practical deployment barriers.", "conclusion": "LLMs match static analyzers in vulnerability detection while offering contextual insights, making them suitable for early-stage triage. Rule-based tools maintain advantages for high-assurance verification. The proposed hybrid pipeline addresses tool limitations through complementary strengths. The open benchmark and result harness enable future reproducible research in next-generation code security. (Key limitation: LLM mislocalization due to tokenization artifacts.)"}}
{"id": "2508.04178", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.04178", "abs": "https://arxiv.org/abs/2508.04178", "authors": ["Md Sajidul Islam Sajid", "Shihab Ahmed", "Ryan Sosnoski"], "title": "Secure Development of a Hooking-Based Deception Framework Against Keylogging Techniques", "comment": "Accepted at IEEE Secure Development Conference (SecDev) 2025", "summary": "Keyloggers remain a serious threat in modern cybersecurity, silently\ncapturing user keystrokes to steal credentials and sensitive information.\nTraditional defenses focus mainly on detection and removal, which can halt\nmalicious activity but do little to engage or mislead adversaries. In this\npaper, we present a deception framework that leverages API hooking to intercept\ninput-related API calls invoked by keyloggers at runtime and inject realistic\ndecoy keystrokes. A core challenge, however, lies in the increasing adoption of\nanti-hooking techniques by advanced keyloggers. Anti-hooking strategies allow\nmalware to bypass or detect instrumentation. To counter this, we introduce a\nhardened hooking layer that detects tampering and rapidly reinstates disrupted\nhooks, ensuring continuity of deception. We evaluate our framework against a\ncustom-built \"super keylogger\" incorporating multiple evasion strategies, as\nwell as 50 real-world malware samples spanning ten prominent keylogger\nfamilies. Experimental results demonstrate that our system successfully resists\nsophisticated bypass attempts, maintains operational stealth, and reliably\ndeceives attackers by feeding them decoys. The system operates with negligible\nperformance overhead and no observable impact on user experience. Our findings\nshow that resilient, runtime deception can play a practical and robust role in\nconfronting advanced threats.", "AI": {"tldr": "This paper introduces a deception framework using API hooking with a hardened hooking layer to counter advanced keyloggers incorporating anti-hooking techniques. The system injects realistic decoy keystrokes to mislead attackers, demonstrating resilience against bypass attempts with negligible performance overhead.", "motivation": "Traditional keylogger defenses focus on detection and removal, which prevent malicious activity but lack engagement or countermeasures against evasion tactics used by sophisticated keyloggers. A new approach is needed to actively mislead adversaries and counter advanced malware techniques.", "method": "The framework employs API hooking to intercept input-related malware API calls at runtime and inject decoy keystrokes. A hardened hooking layer is introduced to detect and rapidly reinstate hooks disrupted by anti-hooking strategies, enhancing deception resilience.", "result": "Experimental evaluation against a custom 'super keylogger' with evasion strategies and 50 samples from ten keylogger families showed the system resists bypass attempts, maintains operational stealth, and successfully deceives attackers without significant performance degradation.", "conclusion": "The hardened hooking deception framework effectively mitigates advanced keyloggers by maintaining real-time, resilient deception. The results demonstrate that runtime keystroke deception is a practical and robust defense mechanism, offering alternatives to detection-focused approaches and mitigating anti-hooking evasion techniques."}}
{"id": "2508.04479", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.04479", "abs": "https://arxiv.org/abs/2508.04479", "authors": ["Hashini Gunatilake", "John Grundy", "Rashina Hoda", "Ingo Mueller"], "title": "Manifestations of Empathy in Software Engineering: How, Why, and When It Matters", "comment": null, "summary": "Empathy plays a crucial role in software engineering (SE), influencing\ncollaboration, communication, and decision-making. While prior research has\nhighlighted the importance of empathy in SE, there is limited understanding of\nhow empathy manifests in SE practice, what motivates SE practitioners to\ndemonstrate empathy, and the factors that influence empathy in SE work. Our\nstudy explores these aspects through 22 interviews and a large scale survey\nwith 116 software practitioners. Our findings provide insights into the\nexpression of empathy in SE, the drivers behind empathetic practices, SE\nactivities where empathy is perceived as useful or not, and the other factors\nthat influence empathy. In addition, we offer practical implications for SE\npractitioners and researchers, offering a deeper understanding of how to\neffectively integrate empathy into SE processes.", "AI": {"tldr": "This study investigates empathy in software engineering through 22 interviews and a survey of 116 practitioners, revealing how empathy is expressed, what motivates it, its perceived usefulness in SE activities, and influencing factors, with practical implications for researchers and practitioners.", "motivation": "Prior research emphasizes the importance of empathy in software engineering but lacks understanding of its practical manifestations, motivators, and influencing factors, necessitating this exploration to bridge theoretical and applied perspectives.", "method": "The study employs a mixed-methods approach, combining 22 qualitative interviews with a quantitative survey involving 116 software practitioners, to gather insights on empathetic practices in SE contexts.", "result": "Findings include insights into empathy expression in SE, key motivators (e.g., collaboration, communication), specific activities where empathy is valued or challenged, and organizational and project-level factors (e.g., tools, culture) that influence empathetic behavior.", "conclusion": "The research provides actionable recommendations for integrating empathy into SE processes, highlighting its role in enhancing team dynamics and decision-making while addressing barriers like tool limitations and cultural norms."}}
{"id": "2508.04189", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.04189", "abs": "https://arxiv.org/abs/2508.04189", "authors": ["Kunlan Xiang", "Haomiao Yang", "Meng Hao", "Haoxin Wang", "Shaofeng Li", "Wenbo Jiang"], "title": "BadTime: An Effective Backdoor Attack on Multivariate Long-Term Time Series Forecasting", "comment": null, "summary": "Multivariate Long-Term Time Series Forecasting (MLTSF) models are\nincreasingly deployed in critical domains such as climate, finance, and\ntransportation. Although a variety of powerful MLTSF models have been proposed\nto improve predictive performance, the robustness of MLTSF models against\nmalicious backdoor attacks remains entirely unexplored, which is crucial to\nensuring their reliable and trustworthy deployment. To address this gap, we\nconduct an in-depth study on backdoor attacks against MLTSF models and propose\nthe first effective attack method named BadTime. BadTime executes a backdoor\nattack by poisoning training data and customizing the backdoor training\nprocess. During data poisoning, BadTime proposes a contrast-guided strategy to\nselect the most suitable training samples for poisoning, then employs a graph\nattention network to identify influential variables for trigger injection.\nSubsequently, BadTime further localizes optimal positions for trigger injection\nbased on lag analysis and proposes a puzzle-like trigger structure that\ndistributes the trigger across multiple poisoned variables to jointly steer the\nprediction of the target variable. During backdoor training, BadTime\nalternately optimizes the model and triggers via proposed tailored optimization\nobjectives. Extensive experiments show that BadTime significantly outperforms\nstate-of-the-art (SOTA) backdoor attacks on time series forecasting by reducing\nMAE by over 50% on target variables and boosting stealthiness by more than 3\ntimes.", "AI": {"tldr": "This paper proposes BadTime, the first effective backdoor attack method for Multivariate Long-Term Time Series Forecasting (MLTSF) models, improving predictive performance through contrast-guided data poisoning and puzzle-like trigger structures while maintaining high stealthiness.", "motivation": "MLTSF models' robustness against malicious backdoor attacks is unexplored despite their use in critical domains, threatening their reliability and trustworthiness required for deployment.", "method": "1) Contrast-guided sample selection for data poisoning \n2) Graph attention network to identify influential variables \n3) Lag analysis-based trigger localization with puzzle-like distributed triggers \n4) Alternating optimization of model and triggers via tailored objectives", "result": "BadTime outperforms SOTA attacks in: \n- Reducing MAE on target variables by >50% \n- Achieving 3\u00d7 higher stealthiness in experiments \n- Demonstrating robust performance across multiple MLTSF benchmarks", "conclusion": "This work establishes the first practical backdoor attack framework for time series forecasting, revealing vulnerabilities through an attacker's perspective while providing insights for developing more secure MLTSF systems."}}
{"id": "2508.04208", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.04208", "abs": "https://arxiv.org/abs/2508.04208", "authors": ["Saifullah Saifullah", "Stefan Agne", "Andreas Dengel", "Sheraz Ahmed"], "title": "DP-DocLDM: Differentially Private Document Image Generation using Latent Diffusion Models", "comment": "Accepted in ICDAR 2025", "summary": "As deep learning-based, data-driven information extraction systems become\nincreasingly integrated into modern document processing workflows, one primary\nconcern is the risk of malicious leakage of sensitive private data from these\nsystems. While some recent works have explored Differential Privacy (DP) to\nmitigate these privacy risks, DP-based training is known to cause significant\nperformance degradation and impose several limitations on standard training\nprocedures, making its direct application to downstream tasks both difficult\nand costly. In this work, we aim to address the above challenges within the\ncontext of document image classification by substituting real private data with\na synthetic counterpart. In particular, we propose to use conditional latent\ndiffusion models (LDMs) in combination with differential privacy (DP) to\ngenerate class-specific synthetic document images under strict privacy\nconstraints, which can then be utilized to train a downstream classifier\nfollowing standard training procedures. We investigate our approach under\nvarious pretraining setups, including unconditional, class-conditional, and\nlayout-conditional pretraining, in combination with multiple private training\nstrategies such as class-conditional and per-label private fine-tuning with\nDPDM and DP-Promise algorithms. Additionally, we evaluate it on two well-known\ndocument benchmark datasets, RVL-CDIP and Tobacco3482, and show that it can\ngenerate useful and realistic document samples across various document types\nand privacy levels ($\\varepsilon \\in \\{1, 5, 10\\}$). Lastly, we show that our\napproach achieves substantial performance improvements in downstream\nevaluations on small-scale datasets, compared to the direct application of\nDP-Adam.", "AI": {"tldr": "This paper proposes using conditional latent diffusion models with differential privacy to generate synthetic document images for classification, avoiding performance losses from standard DP training methods.", "motivation": "Current DP techniques degrade model performance and limit standard training procedures when addressing privacy risks in document image classification systems. Direct real data leakage mitigation is challenging/costly for downstream tasks.", "method": "Combines LDMs and DP to create class-specific synthetic document images under strict privacy constraints. Tests three pretraining approaches (unconditional/class/layout-conditional) with DPDM/DP-Promise algorithms for private fine-tuning.", "result": "Synthetic documents across RVL-CDIP and Tobacco3482 datasets maintained realism while achieving substantial performance improvements over DP-Adam in downstream classification, particularly on small-scale datasets with epsilon 1-10 privacy levels.", "conclusion": "Synthetic data generation provides a practical solution for maintaining privacy without performance penalties in document image classification, outperforming direct DP training methods when implementing strict privacy constraints."}}
{"id": "2508.04285", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.04285", "abs": "https://arxiv.org/abs/2508.04285", "authors": ["Takumi Suimon", "Yuki Koizumi", "Junji Takemasa", "Toru Hasegawa"], "title": "Per-element Secure Aggregation against Data Reconstruction Attacks in Federated Learning", "comment": "10 pages, 5 figures", "summary": "Federated learning (FL) enables collaborative model training without sharing\nraw data, but individual model updates may still leak sensitive information.\nSecure aggregation (SecAgg) mitigates this risk by allowing the server to\naccess only the sum of client updates, thereby concealing individual\ncontributions. However, a significant vulnerability has recently attracted\nincreasing attention: when model updates are sparse vectors, a non-zero value\ncontributed by a single client at a given index can be directly revealed in the\naggregate, enabling precise data reconstruction attacks. In this paper, we\npropose a novel enhancement to SecAgg that reveals aggregated values only at\nindices with at least $t$ non-zero contributions. Our mechanism introduces a\nper-element masking strategy to prevent the exposure of under-contributed\nelements, while maintaining modularity and compatibility with many existing\nSecAgg implementations by relying solely on cryptographic primitives already\nemployed in a typical setup. We integrate this mechanism into Flamingo, a\nlow-round SecAgg protocol, to provide a robust defense against such attacks.\nOur analysis and experimental results indicate that the additional\ncomputational and communication overhead introduced by our mechanism remains\nwithin an acceptable range, supporting the practicality of our approach.", "AI": {"tldr": "This paper proposes an enhancement to secure aggregation in federated learning that prevents data leakage by masks aggregated values at indices with fewer than $t$ non-zero contributions, ensuring modularity and compatibility with existing protocols while maintaining practical overhead.", "motivation": "Federated learning's secure aggregation (SecAgg) is vulnerable to data reconstruction attacks when sparse model updates reveal individual contributions. Existing SecAgg methods inadequately protect non-zero values in sparse vectors, creating a critical privacy risk.", "method": "The authors introduce a per-element masking strategy to conceal values in the aggregate sum at indices with less than $t$ non-zero contributions. This leverages existing cryptographic primitives in SecAgg protocols like Flamingo, ensuring compatibility without requiring significant architectural changes.", "result": "Experimental analysis demonstrates the mechanism effectively mitigates reconstruction attacks while introducing minimal computational and communication overhead (within acceptable practical limits).", "conclusion": "The proposed method provides a robust, low-overhead solution to a critical vulnerability in SecAgg for sparse federated learning updates, enhancing privacy without compromising system efficiency or compatibility."}}
{"id": "2508.04561", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04561", "abs": "https://arxiv.org/abs/2508.04561", "authors": ["Muhammad Azmi Umer", "Chuadhry Mujeeb Ahmed", "Aditya Mathur", "Muhammad Taha Jilani"], "title": "Attack Pattern Mining to Discover Hidden Threats to Industrial Control Systems", "comment": null, "summary": "This work focuses on validation of attack pattern mining in the context of\nIndustrial Control System (ICS) security. A comprehensive security assessment\nof an ICS requires generating a large and variety of attack patterns. For this\npurpose we have proposed a data driven technique to generate attack patterns\nfor an ICS. The proposed technique has been used to generate over 100,000\nattack patterns from data gathered from an operational water treatment plant.\nIn this work we present a detailed case study to validate the attack patterns.", "AI": {"tldr": "The paper presents a data-driven method for generating attack patterns for Industrial Control Systems (ICS) using data from a water treatment plant, validated through a case study with over 100,000 patterns.", "motivation": "A comprehensive ICS security assessment requires diverse and numerous attack patterns, which traditional methods may not efficiently generate.", "method": "A data-driven technique was developed to analyze operational data from an ICS (specifically a water treatment plant) to algorithmically generate attack patterns.", "result": "Over 100,000 attack patterns were created, and their validity was evaluated through a detailed case study focusing on the ICS context.", "conclusion": "The technique demonstrates effectiveness in generating large-scale validated attack patterns for ICS security, enabling more robust threat modeling and detection strategies."}}
{"id": "2508.04583", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.04583", "abs": "https://arxiv.org/abs/2508.04583", "authors": ["Marc Damie", "Mihai Pop", "Merijn Posthuma"], "title": "Measuring the Carbon Footprint of Cryptographic Privacy-Enhancing Technologies", "comment": null, "summary": "Privacy-enhancing technologies (PETs) have attracted significant attention in\nresponse to privacy regulations, driving the development of applications that\nprioritize user data protection. At the same time, the information and\ncommunication technology (ICT) sector faces growing pressure to reduce its\nenvironmental footprint, particularly its carbon emissions. While numerous\nstudies have assessed the energy footprint of various ICT applications, the\nenvironmental footprint of cryptographic PETs remains largely unexplored.\n  Our work addresses this gap by proposing a standardized methodology for\nevaluating the carbon footprint of PETs. To demonstrate this methodology, we\nfocus on PETs supporting client-server applications as they are the simplest to\ndeploy. In particular, we measure the energy consumption and carbon footprint\nincrease induced by five cryptographic PETs (compared to their non-private\nequivalent): HTTPS web browsing, encrypted machine learning (ML) inference,\nencrypted ML training, encrypted databases, and encrypted emails. Our findings\nreveal significant variability in carbon footprint increases, ranging from a\ntwofold increase in HTTPS web browsing to a 100,000-fold increase in encrypted\nML.\n  Our study provides essential data to help decision-makers assess\nprivacy-carbon trade-offs in such applications. Finally, we outline key\nresearch directions for developing PETs that balance strong privacy protection\nwith environmental sustainability.", "AI": {"tldr": "This paper establishes a standardized methodology to evaluate the carbon footprint of cryptographic privacy-enhancing technologies (PETs) and quantifies their environmental impact across key applications like encrypted web browsing, machine learning, and email.", "motivation": "PETs are critical for meeting privacy regulations but lack environmental analysis. The ICT sector needs to reduce carbon emissions, yet the trade-off between privacy and carbon cost remains unaddressed, making it hard to prioritize sustainable implementations.", "method": "The authors propose a systematic evaluation framework to measure carbon footprint increases of cryptographic PETs. They apply this methodology to five use cases (HTTPS, encrypted ML inference/training, encrypted databases, encrypted email) by comparing their energy consumption against non-private counterparts.", "result": "Significant carbon footprint variations were observed, with HTTPS showing a 2\u00d7 increase and encrypted ML training reaching an alarming 100,000\u00d7 increase, highlighting the need for tailored assessments of privacy-technology combinations.", "conclusion": "The study bridges the gap in PET sustainability analysis and provides decision-making data for privacy-carbon trade-offs. It also identifies urgent research paths to develop PETs that simultaneously achieve strong privacy and low environmental impact."}}
{"id": "2508.04641", "categories": ["cs.CR", "C.2.4"], "pdf": "https://arxiv.org/pdf/2508.04641", "abs": "https://arxiv.org/abs/2508.04641", "authors": ["Kirti Singh", "Vinay J. Ribeiro", "Susmita Mandal"], "title": "4-Swap: Achieving Grief-Free and Bribery-Safe Atomic Swaps Using Four Transactions", "comment": "Accepted to AFT 2025. To appear in the LIPIcs proceedings", "summary": "Cross-chain asset exchange is crucial for blockchain interoperability.\nExisting solutions rely on trusted third parties and risk asset loss, or use\ndecentralized alternatives like atomic swaps, which suffer from grief attacks.\nGriefing occurs when a party prematurely exits, locking the counterparty's\nassets until a timelock expires. Hedged Atomic Swaps mitigate griefing by\nintroducing a penalty premium; however, they increase the number of\ntransactions from four (as in Tier Nolan's swap) to six, which in turn\nintroduces new griefing risks. Grief-Free (GF) Swap reduces this to five\ntransactions by consolidating assets and premiums on a single chain. However,\nno existing protocol achieves grief-free asset exchange in just four\ntransactions.\n  This paper presents 4-Swap, the first cross-chain atomic swap protocol that\nis both grief-free and bribery-safe, while completing asset exchange in just\nfour transactions. By combining the griefing premium and principal into a\nsingle transaction per chain, 4-Swap reduces on-chain transactions, leading to\nfaster execution compared to previous grief-free solutions. It is fully\ncompatible with Bitcoin and operates without the need for any new opcodes. A\ngame-theoretic analysis shows that rational participants have no incentive to\ndeviate from the protocol, ensuring robust compliance and security.", "AI": {"tldr": "4-Swap is a cross-chain asset exchange protocol that completes grief-free and bribery-safe swaps in four transactions by consolidating components into single-chain operations, improving upon prior methods (6-5 transactions) with faster execution and Bitcoin compatibility.", "motivation": "Traditional cross-chain exchanges face asset security risks with third parties, while decentralized atomic swaps are vulnerable to griefing attacks. Existing grief-free alternatives like Hedged Atomic Swaps and GF Swap require 6 and 5 transactions respectively, lacking efficiency.", "method": "4-Swap reduces transaction count to four by merging griefing premium and principal assets into one transaction per chain, leveraging a single on-chain deposit model and timelock mechanisms. Game-theoretic analysis confirms protocol security against rational attackers.", "result": "Achieved first cross-chain atomic swap protocol with guaranteed four transactions, eliminating griefing and bribe incentives. Demonstrated compatibility with Bitcoin\u2019s current opcode set, enabling faster execution through reduced on-chain activity.", "conclusion": "4-Swap establishes a mathematically optimal cross-chain exchange solution, combining robust security with maximum efficiency. Its minimal transaction count and resistance to incentive-driven attacks represent significant advancements in blockchain interoperability."}}
