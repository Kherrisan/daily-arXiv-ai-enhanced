<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 6]
- [cs.SE](#cs.SE) [Total: 2]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [SoK: A Beginner-Friendly Introduction to Fault Injection Attacks](https://arxiv.org/abs/2509.18341)
*Christopher Simon Liu,Fan Wang,Patrick Gould,Carter Yagemann*

Main category: cs.CR

TL;DR: Analyzes fault injection techniques through taxonomy, cost-benefit analysis, and replication of detection tools to advance cyber-physical security research.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need to understand system vulnerabilities under stress and enhance security by systematically analyzing fault injection methods and their practical implications.

Method: The study employs a three-pronged approach: (1) creating a taxonomy of fault injection techniques, (2) conducting a state-of-the-art analysis with cost-benefit comparisons, and (3) replicating a vulnerability detection tool to inform future work.

Result: The paper delivers a beginner-friendly taxonomy, a cost-benefit analysis of state-of-the-art techniques, and a replication analysis identifying new research directions for vulnerability detection.

Conclusion: The paper emphasizes the importance of fault injection research for improving cyber-physical security, providing a structured analysis of techniques, cost-benefit evaluations, and a replication study to guide future research.

Abstract: Fault Injection is the study of observing how systems behave under unusual
stress, environmental or otherwise. In practice, fault injection involves
testing the limits of computer systems and finding novel ways to potentially
break cyber-physical security.
  The contributions of this paper are three-fold. First, we provide a
beginner-friendly introduction to this research topic and an in-depth taxonomy
of fault injection techniques. Second, we highlight the current
state-of-the-art and provide a cost-benefit analysis of each attack method.
Third, for those interested in doing fault injection research, we provide a
replication analysis of an existing vulnerability detection tool and identify a
research focus for future work.

</details>


### [2] [Turning Hearsay into Discovery: Industrial 3D Printer Side Channel Information Translated to Stealing the Object Design](https://arxiv.org/abs/2509.18366)
*Aleksandr Dolgavin,Jacob Gatlin,Moti Yung,Mark Yampolskiy*

Main category: cs.CR

TL;DR: This paper exposes a critical security vulnerability in industrial 3D printers (PBF): attackers can reconstruct 3D printed designs by analyzing power side-channel signals, bypassing encryption. Mitigation requires securing physical signal emissions, not just digital files.


<details>
  <summary>Details</summary>
Motivation: Outsourced 3D printing relies on protecting design files against unauthorized access. However, prior work showed side-channel threats on basic FDM printers, but not for industrially relevant PBF systems. This paper addresses the concrete risk of side-channel attacks on PBF processes, which dominate high-value manufacturing.

Method: The authors collected power side-channel data from individual PBF printer actuators during operation and developed a reconstruction approach inspired by Differential Power Analysis. They used voxel-based volumetric comparisons to evaluate reconstruction accuracy.

Result: The method achieved up to 90.29% true positives and â‰¤9.71% false positives/negatives in reconstructing two 3D printed design models using power side-channel data, demonstrating the feasibility of the attack.

Conclusion: The study demonstrates that side-channel attacks on industrial 3D printers, particularly PBF systems, pose a critical threat to design security. Traditional protections (e.g., encryption) are insufficient, and mitigating physical signal leaks (power, noise) is essential to prevent model reconstruction.

Abstract: The central security issue of outsourced 3D printing (aka AM: Additive
Manufacturing), an industry that is expected to dominate manufacturing, is the
protection of the digital design (containing the designers' model, which is
their intellectual property) shared with the manufacturer. Here, we show, for
the first time, that side-channel attacks are, in fact, a concrete serious
threat to existing industrial grade 3D printers, enabling the reconstruction of
the model printed (regardless of employing ways to directly conceal the design,
e.g. by encrypting it in transit and before loading it into the printer).
Previously, such attacks were demonstrated only on fairly simple FDM desktop 3D
printers, which play a negligible role in manufacturing of valuable designs. We
focus on the Powder Bed Fusion (PBF) AM process, which is popular for
manufacturing net-shaped parts with both polymers and metals. We demonstrate
how its individual actuators can be instrumented for the collection of power
side-channel information during the printing process. We then present our
approach to reconstruct the 3D printed model solely from the collected power
side-channel data. Further, inspired by Differential Power Analysis, we
developed a method to improve the quality of the reconstruction based on
multiple traces. We tested our approach on two design models with different
degrees of complexity. For different models, we achieved as high as 90.29~\% of
True Positives and as low as 7.02~\% and 9.71~\% of False Positives and False
Negatives by voxel-based volumetric comparison between reconstructed and
original designs. The lesson learned from our attack is that the security of
design files cannot solely rely on protecting the files themselves in an
industrial environment, but must instead also rely on assuring no leakage of
power, noise and similar signals to potential eavesdroppers in the printer's
vicinity.

</details>


### [3] [VoxGuard: Evaluating User and Attribute Privacy in Speech via Membership Inference Attacks](https://arxiv.org/abs/2509.18413)
*Efthymios Tsaprazlis,Thanathai Lertpetchpun,Tiantian Feng,Sai Praneeth Karimireddy,Shrikanth Narayanan*

Main category: cs.CR

TL;DR: The paper critiques current voice anonymization evaluations that rely on Equal Error Rate (EER) and proposes VoxGuard, a differential privacy-based framework to assess privacy at low false-positive rates (FPR) where even minor re-identification risks are critical. Their findings show existing methods significantly underperform in low-FPR scenarios, with adversaries achieving near-perfect attribute recovery post-anonymization.


<details>
  <summary>Details</summary>
Motivation: Current evaluation metrics (EER) fail to capture meaningful privacy breaches in low false-positive rate regimes, where even a small number of successful speaker re-identifications or attribute inferences (e.g., gender, accent accuracy) constitute significant risks, especially against informed adversaries using advanced techniques.

Method: Introduced VoxGuard, a differential privacy-motivated framework formalizing two privacy metrics: User Privacy (speaker re-identification prevention) and Attribute Privacy (sensitive traits protection). Evaluated on synthetic/real datasets with attacks using fine-tuned models, max-similarity scoring, and transparent inference to compare low-FPR performance against traditional EER benchmarks.

Result: Adversaries outperformed by orders of magnitude at low-FPR despite similar EER values. Specific attacks achieved near-perfect gender/accent recovery post-anonymization. EER misleadingly underrepresented privacy leakage in high-precision attack scenarios.

Conclusion: Recommend shifting privacy evaluation to low-FPR regimes. VoxGuard provides a more realistic benchmark for assessing privacy risks in voice anonymization, highlighting critical gaps in current methods exposed by advanced adversarial techniques.

Abstract: Voice anonymization aims to conceal speaker identity and attributes while
preserving intelligibility, but current evaluations rely almost exclusively on
Equal Error Rate (EER) that obscures whether adversaries can mount
high-precision attacks. We argue that privacy should instead be evaluated in
the low false-positive rate (FPR) regime, where even a small number of
successful identifications constitutes a meaningful breach. To this end, we
introduce VoxGuard, a framework grounded in differential privacy and membership
inference that formalizes two complementary notions: User Privacy, preventing
speaker re-identification, and Attribute Privacy, protecting sensitive traits
such as gender and accent. Across synthetic and real datasets, we find that
informed adversaries, especially those using fine-tuned models and
max-similarity scoring, achieve orders-of-magnitude stronger attacks at low-FPR
despite similar EER. For attributes, we show that simple transparent attacks
recover gender and accent with near-perfect accuracy even after anonymization.
Our results demonstrate that EER substantially underestimates leakage,
highlighting the need for low-FPR evaluation, and recommend VoxGuard as a
benchmark for evaluating privacy leakage.

</details>


### [4] [Context Lineage Assurance for Non-Human Identities in Critical Multi-Agent Systems](https://arxiv.org/abs/2509.18415)
*Sumana Malkapuram,Sameera Gangavarapu,Kailashnath Reddy Kavalakuntla,Ananya Gangavarapu*

Main category: cs.CR

TL;DR: The paper proposes a cryptographically grounded framework for secure agent-to-agent interactions using Merkle trees and federated proof servers to enable lineage verification, identity attestation, and multi-hop auditability for non-human identities (NHIs).


<details>
  <summary>Details</summary>
Motivation: Autonomous agents requiring secure interactions in regulated environments (e.g., FedRAMP) need verifiable provenance tracking and standardized identity validation mechanisms to ensure trustworthiness across multi-hop communications.

Method: 1) Append-only Merkle trees based on Certificate Transparency (CT) logs for lineage anchoring
2)Lederated proof server aggregating inclusion proofs and consistency checks into compressed attestations
3) Enhanced A2A agent cards with identity verification primitives for standardized NHI authentication

Result: Establishes COhesive model with cryptographically secure multi-hop verification capabilities, enabling external auditors to validate call chain integrity without full execution traces while maintaining standardized identity attestation workflows.

Conclusion: Advances security of inter-agent ecosystems through integrated lineage verification and auditing infrastructure, providing foundational tools for NHI governance in compliance-sensitive environments via cryptographic attestation mechanisms.

Abstract: The proliferation of autonomous software agents necessitates rigorous
frameworks for establishing secure and verifiable agent-to-agent (A2A)
interactions, particularly when such agents are instantiated as non-human
identities(NHIs). We extend the A2A paradigm [1 , 2] by introducing a
cryptographically grounded mechanism for lineage verification, wherein the
provenance and evolution of NHIs are anchored in append-only Merkle tree
structures modeled after Certificate Transparency (CT) logs. Unlike traditional
A2A models that primarily secure point-to-point interactions, our approach
enables both agents and external verifiers to cryptographically validate
multi-hop provenance, thereby ensuring the integrity of the entire call chain.
  A federated proof server acts as an auditor across one or more Merkle logs,
aggregating inclusion proofs and consistency checks into compact, signed
attestations that external parties can verify without access to the full
execution trace. In parallel, we augment the A2A agent card to incorporate
explicit identity verification primitives, enabling both peer agents and human
approvers to authenticate the legitimacy of NHI representations in a
standardized manner. Together, these contributions establish a cohesive model
that integrates identity attestation, lineage verification, and independent
proof auditing, thereby advancing the security posture of inter-agent
ecosystems and providing a foundation for robust governance of NHIs in
regulated environments such as FedRAMP.

</details>


### [5] [Coherence-driven inference for cybersecurity](https://arxiv.org/abs/2509.18520)
*Steve Huntsman*

Main category: cs.CR

TL;DR: LLMs enable coherence-driven inference for cybersecurity team operations, showcasing promise for future autonomous decision-making applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore early applications of automatic CDI in cybersecurity, particularly for enhancing decision-making in red and blue team operations.

Method: The method involves utilizing large language models to compile weighted graphs from natural language data, enabling coherence-driven inference for cybersecurity tasks.

Result: The result presents an initial demonstration of automatic CDI's potential, indicating its feasibility for future cybersecurity applications.

Conclusion: The study concludes that the application of LLMs for CDI is a promising approach for advancing cybersecurity decision-making processes and potentially automating blue team operations in the future.

Abstract: Large language models (LLMs) can compile weighted graphs on natural language
data to enable automatic coherence-driven inference (CDI) relevant to red and
blue team operations in cybersecurity. This represents an early application of
automatic CDI that holds near- to medium-term promise for decision-making in
cybersecurity and eventually also for autonomous blue team operations.

</details>


### [6] [Examining I2P Resilience: Effect of Centrality-based Attack](https://arxiv.org/abs/2509.18572)
*Kemi Akanbi,Sunkanmi Oluwadare,Jess Kropczynski,Jacques Bou Abdo*

Main category: cs.CR

TL;DR: This study identifies structural vulnerabilities in the I2P decentralized network under targeted adversarial attacks, showing efficiency declines (10.6\% density reduction, 33\% longer path lengths) that challenge assumptions about decentralization ensuring robustness.


<details>
  <summary>Details</summary>
Motivation: Despite its popularity as a censorship-avoidance tool, I2P's resilience to attacks has been understudied compared to TOR, yet understanding such vulnerabilities is critical for designing secure decentralized systems.

Method: Network analysis using adversarial percolation simulations, measuring density (0.01065443 pre-attack) and average path length (6.842194 pre-attack), with degree centrality to prioritize node removal.

Result: Post-percolation metrics show 10.6\%! decrease in network density and 33\%! increase in average path length, demonstrating significant degradation in connectivity and performance under targeted attacks.

Conclusion: The findings reveal that even decentralized anonymity networks like I2P have structural fragility against strategic node disruption, necessitating improved network design strategies to enhance attack resilience.

Abstract: This study examines the robustness of I2P, a well-regarded anonymous and
decentralized peer-to-peer network designed to ensure anonymity,
confidentiality, and circumvention of censorship. Unlike its more widely
researched counterpart, TOR, I2P's resilience has received less scholarly
attention. Employing network analysis, this research evaluates I2P's
susceptibility to adversarial percolation. By utilizing the degree centrality
as a measure of nodes' influence in the network, the finding suggests the
network is vulnerable to targeted disruptions. Before percolation, the network
exhibited a density of 0.01065443 and an average path length of 6.842194. At
the end of the percolation process, the density decreased by approximately 10%,
and the average path length increased by 33%, indicating a decline in
efficiency and connectivity. These results highlight that even decentralized
networks, such as I2P, exhibit structural fragility under targeted attacks,
emphasizing the need for improved design strategies to enhance resilience
against adversarial disruptions.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [7] [CoRaCMG: Contextual Retrieval-Augmented Framework for Commit Message Generation](https://arxiv.org/abs/2509.18337)
*Bo Xiong,Linghao Zhang,Chong Wang,Peng Liang*

Main category: cs.SE

TL;DR: CoRaCMG boosts LLM commit message quality by retrieving and integrating similar examples, significantly improving metrics like BLEU and CIDEr with marginal gains after three examples.


<details>
  <summary>Details</summary>
Motivation: Current commit messages are often low-quality and CMG using LLMs remains limited; requires better methods to reduce manual effort and improve message precision.

Method: Proposes a three-phase framework: (1) Retrieve similar diff-message pairs, (2) Augment prompts with these pairs, and (3) Generate messages via LLMs, enabling project-specific terminology and style learning.

Result: Achieves 76% BLEU and 71% CIDEr improvements for DeepSeek-R1 with one retrieved example; GPT-4o shows 89% BLEU increase. Gains plateau beyond three examples due to diminishing returns.

Conclusion: CoRaCMG enhances commit message generation by leveraging retrieved diff-message pairs to improve LLM output quality, demonstrating significant performance gains with diminishing returns after three examples.

Abstract: Commit messages play a key role in documenting the intent behind code
changes. However, they are often low-quality, vague, or incomplete, limiting
their usefulness. Commit Message Generation (CMG) aims to automatically
generate descriptive commit messages from code diffs to reduce developers'
effort and improve message quality. Although recent advances in LLMs have shown
promise in automating CMG, their performance remains limited. This paper aims
to enhance CMG performance by retrieving similar diff-message pairs to guide
LLMs to generate commit messages that are more precise and informative. We
proposed CoRaCMG, a Contextual Retrieval-augmented framework for Commit Message
Generation, structured in three phases: (1) Retrieve: retrieving the similar
diff-message pairs; (2) Augment: combining them with the query diff into a
structured prompt; and (3) Generate: generating commit messages corresponding
to the query diff via LLMs. CoRaCMG enables LLMs to learn project-specific
terminologies and writing styles from the retrieved diff-message pairs, thereby
producing high-quality commit messages. We evaluated our method on various
LLMs, including closed-source GPT models and open-source DeepSeek models.
Experimental results show that CoRaCMG significantly boosts LLM performance
across four metrics (BLEU, Rouge-L, METEOR, and CIDEr). Specifically,
DeepSeek-R1 achieves relative improvements of 76% in BLEU and 71% in CIDEr when
augmented with a single retrieved example pair. After incorporating the single
example pair, GPT-4o achieves the highest improvement rate, with BLEU
increasing by 89%. Moreover, performance gains plateau after more than three
examples are used, indicating diminishing returns. Further analysis shows that
the improvements are attributed to the model's ability to capture the
terminologies and writing styles of human-written commit messages from the
retrieved example pairs.

</details>


### [8] [Reading Between the Lines: Scalable User Feedback via Implicit Sentiment in Developer Prompts](https://arxiv.org/abs/2509.18361)
*Daye Nam,Malgorzata Salawa,Satish Chandra*

Main category: cs.SE

TL;DR: The paper introduces a scalable method for evaluating developer satisfaction with conversational AI by analyzing sentiment in developer prompts, achieving a 13x higher signal detection rate than explicit feedback using industrial logs of 372 developers.


<details>
  <summary>Details</summary>
Motivation: Traditional unscalable user studies and shallow quantitative signals from logs inhibit accurate developer satisfaction evaluation, necessitating a scalable solution for reliable insights at scale.

Method: Proposes and evaluates sentiment analysis of developer prompts using 372 professionals' industrial usage logs to identify implicit satisfaction signals, contrasting with explicit feedback mechanisms.

Result: Sentiment analysis identified satisfaction signals in ~8% of interactions (13x more frequent than explicit feedback) with reasonable accuracy using off-the-shelf tools.

Conclusion: This practical approach complements existing feedback channels, enabling scalable, comprehensive understanding of developer experiences with AI assistants and opening new research directions.

Abstract: Evaluating developer satisfaction with conversational AI assistants at scale
is critical but challenging. User studies provide rich insights, but are
unscalable, while large-scale quantitative signals from logs or in-product
ratings are often too shallow or sparse to be reliable. To address this gap, we
propose and evaluate a new approach: using sentiment analysis of developer
prompts to identify implicit signals of user satisfaction. With an analysis of
industrial usage logs of 372 professional developers, we show that this
approach can identify a signal in ~8% of all interactions, a rate more than 13
times higher than explicit user feedback, with reasonable accuracy even with an
off-the-shelf sentiment analysis approach. This new practical approach to
complement existing feedback channels would open up new directions for building
a more comprehensive understanding of the developer experience at scale.

</details>
