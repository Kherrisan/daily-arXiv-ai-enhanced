{"id": "2509.08843", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08843", "abs": "https://arxiv.org/abs/2509.08843", "authors": ["Sidney Shapiro"], "title": "Pattern-Based File and Data Access with Python Glob: A Comprehensive Guide for Computational Research", "comment": null, "summary": "Pattern-based file access is a fundamental but often under-documented aspect\nof computational research. The Python glob module provides a simple yet\npowerful way to search, filter, and ingest files using wildcard patterns,\nenabling scalable workflows across disciplines. This paper introduces glob as a\nversatile tool for data science, business analytics, and artificial\nintelligence applications. We demonstrate use cases including large-scale data\ningestion, organizational data analysis, AI dataset construction, and\nreproducible research practices. Through concrete Python examples with widely\nused libraries such as pandas,scikit-learn, and matplotlib, we show how glob\nfacilitates efficient file traversal and integration with analytical pipelines.\nBy situating glob within the broader context of reproducible research and data\nengineering, we highlight its role as a methodological building block. Our goal\nis to provide researchers and practitioners with a concise reference that\nbridges foundational concepts and applied practice, making glob a default\ncitation for file pattern matching in Python-based research workflows.", "AI": {"tldr": "This paper positions Python's glob module as a critical tool for efficient file management in research workflows, using practical examples to demonstrate its role in scalable data science, AI, and reproducibility.", "motivation": "The paper addresses the lack of documentation around pattern-based file access in computational research, aiming to establish glob as a foundational tool for scalable, reproducible workflows in data science, business analytics, and AI.", "method": "The authors employ practical Python examples using libraries like pandas and scikit-learn to demonstrate glob's integration into analytical pipelines. They present case studies on data ingestion, organizational analysis, AI dataset construction, and reproducibility.", "result": "The paper successfully illustrates glob's utility in large-scale data ingestion, cross-domain data analysis, AI pipeline construction, and reproducible research through concrete code examples and use cases.", "conclusion": "The paper concludes that the glob module is an essential tool for Python-based research workflows, advocating for its adoption as a standard for file pattern matching due to its versatility and efficiency in handling data across various domains."}}
{"id": "2509.08857", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.08857", "abs": "https://arxiv.org/abs/2509.08857", "authors": ["Marcelino Garcia", "Renato Garcia", "Arthur Parizotto", "Andre Mendes", "Pedro Valle", "Ricardo Vilela", "Renato Balancieri", "Williamson Silva"], "title": "A Systematic Mapping Study on Chatbots in Programming Education", "comment": "18 pages, 1 figure, 3 tables", "summary": "Educational chatbots have gained prominence as support tools for teaching\nprogramming, particularly in introductory learning contexts. This paper\npresents a Systematic Mapping Study (SMS) that investigated how such agents\nhave been developed and applied in programming education. From an initial set\nof 3,216 publications, 54 studies were selected and analyzed based on five\nresearch subquestions, addressing chatbot types, programming languages used,\neducational content covered, interaction models, and application contexts. The\nresults reveal a predominance of chatbots designed for Python instruction,\nfocusing on fundamental programming concepts, and employing a wide variety of\npedagogical approaches and technological architectures. In addition to\nidentifying trends and gaps in the literature, this study provides insights to\ninform the development of new educational tools for programming instruction.", "AI": {"tldr": "This paper maps 54 studies on programming education chatbots, revealing Python-centric, concept-focused tools with varied pedagogies, while identifying research gaps for future development.", "motivation": "Educational chatbots are increasingly used in programming education, necessitating a systematic review of their development and application to guide future tool creation.", "method": "A Systematic Mapping Study (SMS) analyzed 54 selected studies from 3,216 publications, addressing five research subquestions on chatbot types, languages, content, interaction models, and application contexts.", "result": "Chatbots predominantly focus on Python instruction, fundamental programming concepts, and employ diverse pedagogical approaches and technological architectures.", "conclusion": "This study provides insights to inform the development of new educational tools for programming instruction by identifying trends and gaps in chatbot applications."}}
{"id": "2509.08863", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08863", "abs": "https://arxiv.org/abs/2509.08863", "authors": ["Qianqian Luo", "Liuchang Xu", "Qingming Lin", "Sensen Wu", "Ruichen Mao", "Chao Wang", "Hailin Feng", "Bo Huang", "Zhenhong Du"], "title": "GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation", "comment": null, "summary": "LLMs have made substantial progress in task automation and natural language\nunderstanding.However,without expertise in GIS,they continue to encounter\nlimitations.To address these issues, we propose GeoJSON Agents-a multi-agent\nLLM architecture.This framework transforms natural language tasks into\nstructured GeoJSON operation commands and processes spatial data using two\nwidely adopted LLM enhancement techniques:Function Calling and Code\nGeneration.The architecture consists of three components-task parsing,agent\ncollaboration,and result integration-aimed at enhancing both the performance\nand scalability of GIS automation.The Planner agent interprets natural language\ntasks into structured GeoJSON commands.Then,specialized Worker agents\ncollaborate according to assigned roles to perform spatial data processing and\nanalysis,either by invoking predefined function APIs or by dynamically\ngenerating and executing Python-based spatial analysis code.Finally,the system\nintegrates the outputs from multiple execution rounds into\nreusable,standards-compliant GeoJSON files.To systematically evaluate the\nperformance of the two approaches,we constructed a benchmark dataset of 70\ntasks with varying complexity and conducted experiments using OpenAI's GPT-4o\nas the core model.Results indicate that the Function Calling-based GeoJSON\nAgent achieved an accuracy of 85.71%,while the Code Generation-based agent\nreached 97.14%,both significantly outperforming the best-performing\ngeneral-purpose model (48.57%).Further analysis reveals that the Code\nGeneration provides greater flexibility,whereas the Function Calling approach\noffers more stable execution.This study is the first to introduce an LLM\nmulti-agent framework for GeoJSON data and to compare the strengths and\nlimitations of two mainstream LLM enhancement methods,offering new perspectives\nfor improving GeoAI system performance.", "AI": {"tldr": "This paper introduces GeoJSON Agents, a multi-agent LLM framework for GIS automation. It shows code generation outperforms function calling by 11.43% accuracy while maintaining execution stability, offering a new approach for GeoAI systems.", "motivation": "LLMs face limitations in GIS tasks without domain expertise. This study addresses the need for scalable automation frameworks that enable non-experts to perform spatial data processing and analysis effectively.", "method": "The framework transforms natural language tasks into structured GeoJSON commands through task parsing (Planner agent), executes spatial data analysis via specialized Worker agents using either Function Calling or Code Generation techniques, and integrates outputs into standards-compliant GeoJSON files.", "result": "Code Generation-based agents achieved 97.14% accuracy on a 70-task benchmark, outperforming Function Calling (85.71%) and general models (48.57%). Function Calling showed more stable execution while Code Generation provided greater flexibility.", "conclusion": "GeoJSON Agents offer a novel multi-agent LLM framework that significantly improves GIS automation performance compared to standard models, with code generation showing higher flexibility and function calling providing more stable execution. This approach provides new perspectives for GeoAI system design."}}
{"id": "2509.08865", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08865", "abs": "https://arxiv.org/abs/2509.08865", "authors": ["Guangyu Zhang", "Xixuan Wang", "Shiyu Sun", "Peiyan Xiao", "Kun Sun", "Yanhai Xiong"], "title": "TraceRAG: A LLM-Based Framework for Explainable Android Malware Detection and Behavior Analysis", "comment": null, "summary": "Sophisticated evasion tactics in malicious Android applications, combined\nwith their intricate behavioral semantics, enable attackers to conceal\nmalicious logic within legitimate functions, underscoring the critical need for\nrobust and in-depth analysis frameworks. However, traditional analysis\ntechniques often fail to recover deeply hidden behaviors or provide\nhuman-readable justifications for their decisions. Inspired by advances in\nlarge language models (LLMs), we introduce TraceRAG, a retrieval-augmented\ngeneration (RAG) framework that bridges natural language queries and Java code\nto deliver explainable malware detection and analysis. First, TraceRAG\ngenerates summaries of method-level code snippets, which are indexed in a\nvector database. At query time, behavior-focused questions retrieve the most\nsemantically relevant snippets for deeper inspection. Finally, based on the\nmulti-turn analysis results, TraceRAG produces human-readable reports that\npresent the identified malicious behaviors and their corresponding code\nimplementations. Experimental results demonstrate that our method achieves 96\\%\nmalware detection accuracy and 83.81\\% behavior identification accuracy based\non updated VirusTotal (VT) scans and manual verification. Furthermore, expert\nevaluation confirms the practical utility of the reports generated by TraceRAG.", "AI": {"tldr": "Strong performance in malware detection and behavior identification", "motivation": "Traditional analysis techniques struggle to detect deep behaviors or explain decisions, necessitating advanced, explainable methods", "method": "Introduce TraceRAG, a RAG framework using LLMs to interpret and report behaviors, enabling linking of code to behavior through query and retrieval of relevant snippets", "result": "Experiments show malware detection accuracy of 96% with 83.81% behavior identification accuracy, verified by updated VT and manual validation", "conclusion": "TraceRAG effectively combines LLMs with retrieval for explainable malware analysis and demonstrates high utility in expert evaluations"}}
{"id": "2509.08992", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08992", "abs": "https://arxiv.org/abs/2509.08992", "authors": ["Anqi Chen", "Riccardo Preatoni", "Alessandro Brighente", "Mauro Conti", "Cristina Nita-Rotaru"], "title": "Cross-Service Token: Finding Attacks in 5G Core Networks", "comment": null, "summary": "5G marks a major departure from previous cellular architectures, by\ntransitioning from a monolithic design of the core network to a Service-Based\nArchitecture (SBA) where services are modularized as Network Functions (NFs)\nwhich communicate with each other via standard-defined HTTP-based APIs called\nService-Based Interfaces (SBIs). These NFs are deployed in private and public\ncloud infrastructure, and an access control framework based on OAuth restricts\nhow they communicate with each other and obtain access to resources. Given the\nincreased vulnerabilities of clouds to insiders, it is important to study the\nsecurity of the 5G Core services for vulnerabilities that allow attackers to\nuse compromised NFs to obtain unauthorized access to resources.\n  We present FivGeeFuzz, a grammar-based fuzzing framework designed to uncover\nsecurity flaws in 5G core SBIs. FivGeeFuzz automatically derives grammars from\n3GPP API specifications to generate malformed, unexpected, or semantically\ninconsistent inputs, and it integrates automated bug detection with manual\nvalidation and root-cause analysis. We evaluate our approach on free5GC, the\nonly open-source 5G core implementing Release 17-compliant SBIs with an access\ncontrol mechanism. Using FivGeeFuzz, we discovered 8 previously unknown\nvulnerabilities in free5GC, leading to runtime crashes, improper error\nhandling, and unauthorized access to resources, including a very severe attack\nwe call Cross-Service Token Attack. All bugs were confirmed by the free5GC\nteam, 7 have already been patched, and the remaining one has a patch under\ndevelopment.", "AI": {"tldr": "This paper introduces FivGeeFuzz, a fuzzing framework that discovers security flaws in 5G core networks by generating malicious API inputs. It identified 8 critical vulnerabilities in free5GC, highlighting risks like the Cross-Service Token Attack, and demonstrates practical effectiveness in improving 5G security.", "motivation": "The paper is motivated by the need to address security risks in 5G's Service-Based Architecture, particularly vulnerabilities in Network Functions (NFs) that could be exploited by insiders or attackers to gain unauthorized access to cloud-based resources through compromised NFs.", "method": "The method involves developing a grammar-based fuzzing framework called FivGeeFuzz, which automatically derives grammars from 3GPP API specifications to generate malicious inputs. It combines automated bug detection with manual validation and root-cause analysis to identify vulnerabilities in 5G Service-Based Interfaces.", "result": "FivGeeFuzz uncovered 8 novel vulnerabilities in the free5GC open-source 5G core, including a severe Cross-Service Token Attack. These vulnerabilities caused runtime crashes, improper error handling, and unauthorized access, with 7 confirmed fixes and 1 under development by the free5GC team.", "conclusion": "The paper concludes that FivGeeFuzz is an effective framework for identifying critical security vulnerabilities in 5G core services, demonstrating its value through the discovery and resolution of 8 previously unknown flaws in the free5GC implementation."}}
{"id": "2509.08867", "categories": ["cs.SE", "cs.AI", "68T01", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.08867", "abs": "https://arxiv.org/abs/2509.08867", "authors": ["K. Pronk", "Q. Zhao"], "title": "Benchmarking Energy Efficiency of Large Language Models Using vLLM", "comment": "6 pages, 6 figures", "summary": "The prevalence of Large Language Models (LLMs) is having an growing impact on\nthe climate due to the substantial energy required for their deployment and\nuse. To create awareness for developers who are implementing LLMs in their\nproducts, there is a strong need to collect more information about the energy\nefficiency of LLMs. While existing research has evaluated the energy efficiency\nof various models, these benchmarks often fall short of representing realistic\nproduction scenarios. In this paper, we introduce the LLM Efficiency Benchmark,\ndesigned to simulate real-world usage conditions. Our benchmark utilizes vLLM,\na high-throughput, production-ready LLM serving backend that optimizes model\nperformance and efficiency. We examine how factors such as model size,\narchitecture, and concurrent request volume affect inference energy efficiency.\nOur findings demonstrate that it is possible to create energy efficiency\nbenchmarks that better reflect practical deployment conditions, providing\nvaluable insights for developers aiming to build more sustainable AI systems.", "AI": {"tldr": "The paper introduces the LLM Efficiency Benchmark, which simulates real-world scenarios to evaluate the energy efficiency of Large Language Models using vLLM, offering practical insights for sustainable AI development.", "motivation": "The increasing use of Large Language Models (LLMs) is significantly impacting the climate due to their high energy consumption, necessitating a more accurate understanding of their energy efficiency for developers.", "method": "The authors present the LLM Efficiency Benchmark, a tool designed to simulate real-world production scenarios. They use vLLM, a high-throughput, production-ready LLM serving backend, to evaluate how model size, architecture, and concurrent request volume influence inference energy efficiency.", "result": "The results demonstrate that the LLM Efficiency Benchmark can accurately represent practical deployment conditions, providing actionable insights into how different factors affect energy consumption.", "conclusion": "The study successfully introduces a benchmark that better reflects real-world energy efficiency of LLMs, supporting the development of more sustainable AI systems by identifying factors that impact their energy usage."}}
{"id": "2509.08995", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08995", "abs": "https://arxiv.org/abs/2509.08995", "authors": ["Sichen Zhu", "Hoyeung Leung", "Xiaoyi Wang", "Jia Wei", "Honghui Xu"], "title": "When FinTech Meets Privacy: Securing Financial LLMs with Differential Private Fine-Tuning", "comment": null, "summary": "The integration of Large Language Models (LLMs) into financial technology\n(FinTech) has revolutionized the analysis and processing of complex financial\ndata, driving advancements in real-time decision-making and analytics. With the\ngrowing trend of deploying AI models on edge devices for financial\napplications, ensuring the privacy of sensitive financial data has become a\nsignificant challenge. To address this, we propose DPFinLLM, a\nprivacy-enhanced, lightweight LLM specifically designed for on-device financial\napplications. DPFinLLM combines a robust differential privacy mechanism with a\nstreamlined architecture inspired by state-of-the-art models, enabling secure\nand efficient processing of financial data. This proposed DPFinLLM can not only\nsafeguard user data from privacy breaches but also ensure high performance\nacross diverse financial tasks. Extensive experiments on multiple financial\nsentiment datasets validate the effectiveness of DPFinLLM, demonstrating its\nability to achieve performance comparable to fully fine-tuned models, even\nunder strict privacy constraints.", "AI": {"tldr": "DPFinLLM: A privacy-enhanced, lightweight large language model for on-device financial applications.", "motivation": "The deployment of AI models on edge devices for financial applications presents significant privacy challenges for sensitive financial data. This research aims to overcome these challenges by integrating differential privacy within a LLM that is suitable for on-device use.", "method": "DPFinLLM combines a robust differential privacy mechanism with a streamlined LLM architecture, optimized for on-device financial tasks. This model is trained with privacy guarantees, allowing secure inference on sensitive financial data.", "result": "dpFinLLM demonstrates performance on par with fully fine-tuned models across various financial sentiment datasets, even when stringent privacy constraints are applied. This indicates its effectiveness in preserving privacy without substantially compromising performance.", "conclusion": "DpFinLLM offers a promising solution for the secure deployment of AI in financial applications on edge devices, ensuring both privacy protection and high performance."}}
{"id": "2509.09072", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.09072", "abs": "https://arxiv.org/abs/2509.09072", "authors": ["Ahmed Adnan", "Mushfiqur Rahman", "Saad Sakib Noor", "Kazi Sakib"], "title": "CLARA: A Developer's Companion for Code Comprehension and Analysis", "comment": "In proceedings at the 40th IEEE/ACM International Conference on\n  Automated Software Engineering, ASE 2025", "summary": "Code comprehension and analysis of open-source project codebases is a task\nfrequently performed by developers and researchers. However, existing tools\nthat practitioners use for assistance with such tasks often require prior\nproject setup, lack context-awareness, and involve significant manual effort.\nTo address this, we present CLARA, a browser extension that utilizes a\nstate-of-the-art inference model to assist developers and researchers in: (i)\ncomprehending code files and code fragments, (ii) code refactoring, and (iii)\ncode quality attribute detection. We qualitatively evaluated CLARA's inference\nmodel using existing datasets and methodology, and performed a comprehensive\nuser study with 10 developers and academic researchers to assess its usability\nand usefulness. The results show that CLARA is useful, accurate, and practical\nin code comprehension and analysis tasks. CLARA is an open-source tool\navailable at https://github.com/SaadNoor555/CLARA_tool_demo. A video showing\nthe full capabilities of CLARA can be found at\nhttps://youtu.be/VDKVXvIH41Q?si=qBFsmS_Y4m_9x3YH.", "AI": {"tldr": "This paper introduces CLARA, a browser-based code analysis tool that uses advanced inference models to aid developers and researchers. Through rigorous evaluation and user testing, CLARA is shown to be a practical, accurate solution for code comprehension and refactoring tasks.", "motivation": "Existing code analysis tools require prior setup, lack context-awareness, and demand significant manual effort. CLARA aims to overcome these limitations by providing an intuitive, integrated, and automated solution.", "method": "The authors developed CLARA, a browser extension leveraging a state-of-the-art inference model, and evaluated its performance qualitatively through existing datasets and a user study involving 10 developers and researchers.", "result": "The evaluation confirmed CLARA's accuracy and effectiveness in code comprehension, refactoring, and quality attribute detection. The user study further validated its usability and practical utility.", "conclusion": "CLARA is an open-source browser extension that effectively addresses the limitations of existing code analysis tools by offering a context-aware, automated solution. The paper demonstrates its utility, accuracy, and practicality through evaluations and user studies, making it a valuable resource for developers and researchers."}}
{"id": "2509.09089", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09089", "abs": "https://arxiv.org/abs/2509.09089", "authors": ["Mengfei Xie", "Yan Lin", "Hongtao Wu", "Jianming Fu", "Chenke Luo", "Guojun Peng"], "title": "Beyond Tag Collision: Cluster-based Memory Management for Tag-based Sanitizers", "comment": "This paper has been accepted to the 2025 ACM SIGSAC Conference on\n  Computer and Communications Security (CCS'25)", "summary": "Tag-based sanitizers attach a small \"key\" to each pointer and a matching\n\"lock\" tag to its target memory object, enabling runtime verification of\npointer-object consistency and helping developers to detect potential memory\nviolations. However, the limited tag encoding space challenges existing studies\nin assigning distinct tags to memory objects across temporal and spatial\ndimensions, leading to potential tag collisions. In this paper, we present\nClusterTag, a novel cluster-based memory allocator aimed at simultaneously\nmitigating tag collisions in both temporal and spatial dimensions. The core\ndesign of ClusterTag effectively balances the significant mismatch between tag\nencoding space and memory objects: it divides memory objects into multiple\nindependent clusters, thereby limiting tag collisions to finite chunks within\neach cluster. To mitigate tag collisions across clusters, we design a\ncluster-grained heap randomization scheme. This approach introduces random\naddress intervals between clusters and further breaks the entropy limitation of\nthe tag space. ClusterTag has been implemented as an independent memory\nallocator that seamlessly integrates with tag-based sanitizers such as HWASan,\nand maintains comparable performance overhead (within 1%) at various\nrandomization densities. Security evaluations on the Juliet dataset indicate\nthat ClusterTag exhibits deterministic results across 500 repeated tests (5,652\nreported and 1,530 missed), while the existing three types of tag assignment\nstrategies all exhibit probabilistic false negatives due to tag collisions.\nQuantitative analysis across three tag collision distance metrics-minimum,\naverage, and unpredictability-demonstrates that ClusterTag achieves balanced\nimprovements across all three, whereas prior tag assignment schemes (random,\nstaggered, fixed) show significant trade-offs in at least one metric.", "AI": {"tldr": "ClusterTag is a cluster-based memory allocator that addresses tag collisions in tag-based sanitizers by partitioning memory into clusters with randomized heap layouts, achieving deterministic memory violation detection with minimal performance overhead.", "motivation": "Tag-based sanitizers suffer from limited tag encoding space, causing temporal and spatial tag collisions that lead to false negatives in detecting memory violations.", "method": "ClusterTag divides memory objects into independent clusters with local tag allocation to confine collisions, and employs cluster-grained heap randomization to break entropy limitations by introducing random address intervals between clusters.", "result": "ClusterTag integrates with HWASan, maintains <1\u200b% performance overhead, achieves deterministic detection on Juliet dataset (5,652 reported/1,530 missed bugs), and outperforms existing tag strategies in tag collision metrics (min, average, unpredictability).", "conclusion": "ClusterTag provides balanced improvements in tag collision prevention across all metrics, eliminates probabilistic false negatives through deterministic allocation, and maintains compatibility with existing tag-based sanitizers with negligible performance impact."}}
{"id": "2509.09192", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09192", "abs": "https://arxiv.org/abs/2509.09192", "authors": ["Doha Nam", "Taehyoun Kim", "Duksan Ryu", "Jongmoon Baik"], "title": "Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset", "comment": "An anonymous link containing the dataset, construction scripts, and\n  experimental code is publicly available for reproducibility:\n  https://figshare.com/s/4f202bc0921e26b41dc2", "summary": "Just-in-Time software defect prediction (JIT-SDP) plays a critical role in\nprioritizing risky code changes during code review and continuous integration.\nHowever, existing datasets often suffer from noisy labels and low precision in\nidentifying bug-inducing commits. To address this, we present ReDef\n(Revert-based Defect dataset), a high-confidence benchmark of function-level\nmodifications curated from 22 large-scale C/C++ projects. Defective cases are\nanchored by revert commits, while clean cases are validated through post-hoc\nhistory checks. Ambiguous instances are conservatively filtered out via a\nGPT-assisted triage process involving multiple votes and audits. This pipeline\nyields 3,164 defective and 10,268 clean modifications, offering substantially\nmore reliable labels than prior existing resources. Beyond dataset\nconstruction, we provide the first systematic evaluation of how pre-trained\nlanguage models (PLMs) reason about code modifications -- specifically, which\ninput encodings most effectively expose change information, and whether models\ngenuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder\nunder five encoding strategies, and further probe their sensitivity through\ncounterfactual perturbations that swap added/deleted blocks, invert diff\npolarity, or inject spurious markers. Our results show that compact diff-style\nencodings consistently outperform whole-function formats across all PLMs, with\nstatistical tests confirming large, model-independent effects. However, under\ncounterfactual tests, performance degrades little or not at all -- revealing\nthat what appears to be robustness in fact reflects reliance on superficial\ncues rather than true semantic understanding. These findings indicate that,\nunlike in snapshot-based tasks, current PLMs remain limited in their ability to\ngenuinely comprehend code modifications.", "AI": {"tldr": "ReDef improves JIT-SDP benchmark quality. PLMs benefit from diff-style encodings but fail to grasp semantic changes, indicating superficial learning patterns.", "motivation": "Existing JIT-SDP datasets suffer from noisy labels and low precision in identifying bug-inducing commits. This study aims to create a reliable benchmark and evaluate how PLMs reason about code modifications.", "method": "1) Constructed ReDef dataset with 22 C/C++ projects using revert commits and post-hoc validation. 2) Filtered ambiguous instances via GPT-assisted triage. 3) Evaluated PLMs (CodeBERT, CodeT5+, UniXcoder) under five encoding strategies and counterfactual perturbations (block swapping, polarity inversion, spurious markers).", "result": "ReDef achieved 3,164 defective and 10,268 clean modifications with high-confidence labels. Diff-style encodings outperformed whole-function formats across all PLMs, but counterfactual tests revealed model reliance on superficial cues (e.g., diff polarity) rather than semantic understanding.", "conclusion": "Current pre-trained language models (PLMs) demonstrate limited ability to genuinely comprehend code modifications, as they rely on superficial cues rather than true semantic understanding. ReDef provides a more reliable benchmark for JIT-SDP."}}
{"id": "2509.09091", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09091", "abs": "https://arxiv.org/abs/2509.09091", "authors": ["Honglan Yu", "Yibin Wang", "Feifei Dai", "Dong Liu", "Haihui Fan", "Xiaoyan Gu"], "title": "Towards Confidential and Efficient LLM Inference with Dual Privacy Protection", "comment": "Accepted by DASFAA2025", "summary": "CPU-based trusted execution environments (TEEs) and differential privacy (DP)\nhave gained wide applications for private inference. Due to high inference\nlatency in TEEs, researchers use partition-based approaches that offload linear\nmodel components to GPUs. However, dense nonlinear layers of large language\nmodels (LLMs) result in significant communication overhead between TEEs and\nGPUs. DP-based approaches apply random noise to protect data privacy, but this\ncompromises LLM performance and semantic understanding. To overcome the above\ndrawbacks, this paper proposes CMIF, a Confidential and efficient Model\nInference Framework. CMIF confidentially deploys the embedding layer in the\nclient-side TEE and subsequent layers on GPU servers. Meanwhile, it optimizes\nthe Report-Noisy-Max mechanism to protect sensitive inputs with a slight\ndecrease in model performance. Extensive experiments on Llama-series models\ndemonstrate that CMIF reduces additional inference overhead in TEEs while\npreserving user data privacy.", "AI": {"tldr": "CMIF addresses privacy and efficiency challenges in large language model inference by combining TEEs and GPUs with optimized DP mechanisms.", "motivation": "CPU-based TEEs cause high latency, GPU offloading of nonlinear layers incurs communication overhead, and DP approaches harm model performance.", "method": "Deploys embedding layer in client-side TEE and subsequent layers on GPU servers while optimizing the Report-Noisy-Max mechanism for privacy-preserving inference.", "result": "Experiments on Llama-series models show reduced TEE overhead and maintained privacy with minimal model performance degradation.", "conclusion": "CMIF demonstrates effective balance between inference efficiency, data confidentiality, and model utility for secure LLM deployment."}}
{"id": "2509.09194", "categories": ["cs.SE", "cs.AI", "68N19"], "pdf": "https://arxiv.org/pdf/2509.09194", "abs": "https://arxiv.org/abs/2509.09194", "authors": ["Ayelet Berzack", "Guy Katz"], "title": "On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability", "comment": null, "summary": "Large Language Models (LLMs) are fast becoming indispensable tools for\nsoftware developers, assisting or even partnering with them in crafting complex\nprograms. The advantages are evident -- LLMs can significantly reduce\ndevelopment time, generate well-organized and comprehensible code, and\noccasionally suggest innovative ideas that developers might not conceive on\ntheir own. However, despite their strengths, LLMs will often introduce\nsignificant errors and present incorrect code with persuasive confidence,\npotentially misleading developers into accepting flawed solutions.\n  In order to bring LLMs into the software development cycle in a more reliable\nmanner, we propose a methodology for combining them with ``traditional''\nsoftware engineering techniques in a structured way, with the goal of\nstreamlining the development process, reducing errors, and enabling users to\nverify crucial program properties with increased confidence. Specifically, we\nfocus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,\nscenario-based approach for software engineering -- to allow human developers\nto pour their expert knowledge into the LLM, as well as to inspect and verify\nits outputs.\n  To evaluate our methodology, we conducted a significant case study, and used\nit to design and implement the Connect4 game. By combining LLMs and SBP we were\nable to create a highly-capable agent, which could defeat various strong\nexisting agents. Further, in some cases, we were able to formally verify the\ncorrectness of our agent. Finally, our experience reveals interesting insights\nregarding the ease-of-use of our proposed approach. The full code of our\ncase-study will be made publicly available with the final version of this\npaper.", "AI": {"tldr": "This paper addresses LLM unreliability in software development by combining large language models with Scenario-Based Programming. The resulting methodology enables error-resistant development through AI-human collaboration, demonstrated via a verified Connect4 agent implementation.", "motivation": "Despite LLMs' benefits, they often produce erroneous code with confidence. This creates risks in trusting their outputs. The motivation is to develop a structured framework that retains LLM advantages (speed, creativity) while mitigating their inherent unreliability through rigorous verification techniques.", "method": "The authors propose a methodology combining LLMs with traditional software engineering practices via the SBP paradigm. This approach lets developers inject their expertise into LLM inputs and validate/inspect LLM-generated outputs through event-driven, scenario-based workflows.", "result": "The methodology was validated through a Connect4 case study, demonstrating: (1) creation of a strong agent capable of defeating existing agents, and (2) successful formal verification of the agent's correctness in specific scenarios, proving the hybrid approach's practicality and reliability.", "conclusion": "The paper concludes that integrating LLMs with Scenario-Based Programming (SBP) enhances reliability in software development by reducing errors, streamlining processes, and enabling verification of critical program properties through human-AI collaboration."}}
{"id": "2509.09097", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09097", "abs": "https://arxiv.org/abs/2509.09097", "authors": ["Honghui Xu", "Shiva Shrestha", "Wei Chen", "Zhiyuan Li", "Zhipeng Cai"], "title": "DP-FedLoRA: Privacy-Enhanced Federated Fine-Tuning for On-Device Large Language Models", "comment": null, "summary": "As on-device large language model (LLM) systems become increasingly\nprevalent, federated fine-tuning enables advanced language understanding and\ngeneration directly on edge devices; however, it also involves processing\nsensitive, user-specific data, raising significant privacy concerns within the\nfederated learning framework. To address these challenges, we propose\nDP-FedLoRA, a privacy-enhanced federated fine-tuning framework that integrates\nLoRA-based adaptation with differential privacy in a communication-efficient\nsetting. Each client locally clips and perturbs its LoRA matrices using\nGaussian noise to satisfy ($\\epsilon$, $\\delta$)-differential privacy. We\nfurther provide a theoretical analysis demonstrating the unbiased nature of the\nupdates and deriving bounds on the variance introduced by noise, offering\npractical guidance for privacy-budget calibration. Experimental results across\nmainstream benchmarks show that DP-FedLoRA delivers competitive performance\nwhile offering strong privacy guarantees, paving the way for scalable and\nprivacy-preserving LLM deployment in on-device environments.", "AI": {"tldr": "DP-FedLoRA is a communication-efficient, privacy-enhanced federated fine-tuning framework for on-device LLMs using LoRA and differential privacy.", "motivation": "Federated fine-tuning of on-device LLMs processes sensitive user data, creating privacy risks in federated learning environments.", "method": "The framework employs LoRA-based adaptation with Gaussian-noise perturbation and gradient clipping on client devices. Theoretical analysis ensures unbiased updates and variance bounds for privacy calibration.", "result": "DP-FedLoRA achieves competitive performance on benchmarks while providing strong ($\\epsilon, \\delta$)-differential privacy guarantees, demonstrating practical effectiveness.", "conclusion": "DP-FedLoRA addresses privacy challenges in federated LLM fine-tuning by integrating differential privacy with LoRA adaptation, enabling scalable and privacy-preserving on-device deployment."}}
{"id": "2509.09294", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.09294", "abs": "https://arxiv.org/abs/2509.09294", "authors": ["Solal Rapaport", "Laurent Pautet", "Samuel Tardieu", "Stefano Zacchiroli"], "title": "Altered Histories in Version Control System Repositories: Evidence from the Trenches", "comment": null, "summary": "Version Control Systems (VCS) like Git allow developers to locally rewrite\nrecorded history, e.g., to reorder and suppress commits or specific data in\nthem. These alterations have legitimate use cases, but become problematic when\nperformed on public branches that have downstream users: they break push/pull\nworkflows, challenge the integrity and reproducibility of repositories, and\ncreate opportunities for supply chain attackers to sneak into them nefarious\nchanges. We conduct the first large-scale investigation of Git history\nalterations in public code repositories. We analyze 111 M (millions)\nrepositories archived by Software Heritage, which preserves VCS histories even\nacross alterations. We find history alterations in 1.22 M repositories, for a\ntotal of 8.7 M rewritten histories. We categorize changes by where they happen\n(which repositories, which branches) and what is changed in them (files or\ncommit metadata). Conducting two targeted case studies we show that altered\nhistories recurrently change licenses retroactively, or are used to remove\n''secrets'' (e.g., private keys) committed by mistake. As these behaviors\ncorrespond to bad practices-in terms of project governance or security\nmanagement, respectively-that software recipients might want to avoid, we\nintroduce GitHistorian, an automated tool, that developers can use to spot and\ndescribe history alterations in public Git repositories.", "AI": {"tldr": "This paper studies the impact of Git history rewriting on public repositories, analyzing 111 million archives to identify 8.7 million altered histories. It introduces GitHistorian, a tool for detecting problematic history changes, including retroactive license alterations and secret removals.", "motivation": "History rewriting in Git can break workflows and compromise repository integrity, while hidden risks like supply chain attacks and governance issues remain unstudied at scale.", "method": "111M Software Heritage repositories were analyzed using historical archives. Reprospective studies categorized alteration types and locations, with case studies on license changes and secret removals.", "result": "1.22M repositories with altered history found, showing patterns of retroactive license modification and accidental secret deletion. These behaviors indicate poor governance and security practices.", "conclusion": "GitHistorian enables automatic detection of suspicious history alterations, highlighting the need for tools and practices to monitor public repository integrity and prevent governance/security risks."}}
{"id": "2509.09103", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09103", "abs": "https://arxiv.org/abs/2509.09103", "authors": ["Chanti Raju Mylay", "Bobin Deng", "Zhipeng Cai", "Honghui Xu"], "title": "AgriSentinel: Privacy-Enhanced Embedded-LLM Crop Disease Alerting System", "comment": null, "summary": "Crop diseases pose significant threats to global food security, agricultural\nproductivity, and sustainable farming practices, directly affecting farmers'\nlivelihoods and economic stability. To address the growing need for effective\ncrop disease management, AI-based disease alerting systems have emerged as\npromising tools by providing early detection and actionable insights for timely\nintervention. However, existing systems often overlook critical aspects such as\ndata privacy, market pricing power, and farmer-friendly usability, leaving\nfarmers vulnerable to privacy breaches and economic exploitation. To bridge\nthese gaps, we propose AgriSentinel, the first Privacy-Enhanced Embedded-LLM\nCrop Disease Alerting System. AgriSentinel incorporates a differential privacy\nmechanism to protect sensitive crop image data while maintaining classification\naccuracy. Its lightweight deep learning-based crop disease classification model\nis optimized for mobile devices, ensuring accessibility and usability for\nfarmers. Additionally, the system includes a fine-tuned, on-device large\nlanguage model (LLM) that leverages a curated knowledge pool to provide farmers\nwith specific, actionable suggestions for managing crop diseases, going beyond\nsimple alerting. Comprehensive experiments validate the effectiveness of\nAgriSentinel, demonstrating its ability to safeguard data privacy, maintain\nhigh classification performance, and deliver practical, actionable disease\nmanagement strategies. AgriSentinel offers a robust, farmer-friendly solution\nfor automating crop disease alerting and management, ultimately contributing to\nimproved agricultural decision-making and enhanced crop productivity.", "AI": {"tldr": "First privacy-enhanced embedded-LLM crop disease system combining differential privacy, lightweight deep learning, and on-device domain knowledge for actionable alerts and market protection.", "motivation": "Current crop disease alerting systems neglect data privacy, market fairness, and usability for farmers, creating vulnerability to privacy breaches and economic exploitation. Novel solutions must address these interdependent challenges.", "method": "The system integrates differential privacy for data protection, a lightweight deep learning model for mobile optimization, and an on-device LLM with curated domain knowledge to deliver actionable disease management strategies.", "result": "Experiments demonstrate AgriSentinel achieves strong data privacy guarantees without sacrificing classification accuracy, while its LLM integration outperforms baselines in generating actionable, crop-specific management recommendations.", "conclusion": "AgriSentinel offers a robust, farmer-friendly framework for privacy-preserving crop disease detection and management, addressing critical gaps in existing systems to enhance agricultural productivity and decision-making."}}
{"id": "2509.09313", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.09313", "abs": "https://arxiv.org/abs/2509.09313", "authors": ["Moritz Mock", "Thomas Forrer", "Barbara Russo"], "title": "Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data", "comment": "Accepted to the 26th International Conference on Product-Focused\n  Software Process Improvement (PROFES 2025)", "summary": "Deep learning solutions for vulnerability detection proposed in academic\nresearch are not always accessible to developers, and their applicability in\nindustrial settings is rarely addressed. Transferring such technologies from\nacademia to industry presents challenges related to trustworthiness, legacy\nsystems, limited digital literacy, and the gap between academic and industrial\nexpertise. For deep learning in particular, performance and integration into\nexisting workflows are additional concerns. In this work, we first evaluate the\nperformance of CodeBERT for detecting vulnerable functions in industrial and\nopen-source software. We analyse its cross-domain generalisation when\nfine-tuned on open-source data and tested on industrial data, and vice versa,\nalso exploring strategies for handling class imbalance. Based on these results,\nwe develop AI-DO(Automating vulnerability detection Integration for Developers'\nOperations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated\nrecommender system that uses fine-tuned CodeBERT to detect and localise\nvulnerabilities during code review without disrupting workflows. Finally, we\nassess the tool's perceived usefulness through a survey with the company's IT\nprofessionals. Our results show that models trained on industrial data detect\nvulnerabilities accurately within the same domain but lose performance on\nopen-source code, while a deep learner fine-tuned on open data, with\nappropriate undersampling techniques, improves the detection of\nvulnerabilities.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.09107", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09107", "abs": "https://arxiv.org/abs/2509.09107", "authors": ["Pritam Sen", "Yao Ma", "Cristian Borcea"], "title": "CryptGNN: Enabling Secure Inference for Graph Neural Networks", "comment": null, "summary": "We present CryptGNN, a secure and effective inference solution for\nthird-party graph neural network (GNN) models in the cloud, which are accessed\nby clients as ML as a service (MLaaS). The main novelty of CryptGNN is its\nsecure message passing and feature transformation layers using distributed\nsecure multi-party computation (SMPC) techniques. CryptGNN protects the\nclient's input data and graph structure from the cloud provider and the\nthird-party model owner, and it protects the model parameters from the cloud\nprovider and the clients. CryptGNN works with any number of SMPC parties, does\nnot require a trusted server, and is provably secure even if P-1 out of P\nparties in the cloud collude. Theoretical analysis and empirical experiments\ndemonstrate the security and efficiency of CryptGNN.", "AI": {"tldr": "ASecureGNNInference Solution for Cloud-Based MLaaS: CryptGNN uses distributed SMPC to protect both client data and model parameters from the cloud provider and model owner.", "motivation": "Clients often wish to perform GNN inference on sensitive data via third-party cloud models without revealing data or model secrets, leading to privacy and security concerns.", "method": "CryptGNN introduces secure message passing and feature transformation layers leveraging distributed SMPC techniques, ensuring no party learns private information.", "result": "Theoretical analysis shows security against P-1-out-of-P collusion, and experiments confirm efficiency. CryptGNN supports any SMPC party count with no trusted servers.", "conclusion": "CryptGNN effectively addresses privacy and security challenges in cloud-based GNN inference for MLaaS without compromising performance."}}
{"id": "2509.09322", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09322", "abs": "https://arxiv.org/abs/2509.09322", "authors": ["Jacopo Bufalino", "Agathe Blaise", "Stefano Secci"], "title": "ORCA: Unveiling Obscure Containers In The Wild", "comment": null, "summary": "Modern software development increasingly depends on open-source libraries and\nthird-party components, which are often encapsulated into containerized\nenvironments. While improving the development and deployment of applications,\nthis approach introduces security risks, particularly when outdated or\nvulnerable components are inadvertently included in production environments.\nSoftware Composition Analysis (SCA) is a critical process that helps identify\nand manage packages and dependencies inside a container. However, unintentional\nmodifications to the container filesystem can lead to incomplete container\nimages, which compromise the reliability of SCA tools. In this paper, we\nexamine the limitations of both cloud-based and open-source SCA tools when\nfaced with such obscure images. An analysis of 600 popular containers revealed\nthat obscure containers exist in well-known registries and trusted images and\nthat many tools fail to analyze such containers. To mitigate these issues, we\npropose an obscuration-resilient methodology for container analysis and\nintroduce ORCA (Obscuration-Resilient Container Analyzer), its open-source\nimplementation. We reported our findings to all vendors using their appropriate\nchannels. Our results demonstrate that ORCA effectively detects the content of\nobscure containers and achieves a median 40% improvement in file coverage\ncompared to Docker Scout and Syft.", "AI": {"tldr": "This paper highlights the security risks in containerized environments due to outdated components and incomplete images caused by filesystem modifications. It introduces ORCA, an open-source obscuration-resilient container analyzer, which improves file coverage for SCA by 40% on average compared to existing tools.", "motivation": "Containerized applications rely on third-party components, but filesystem modifications can obscure these components, making it hard for standard SCA tools to detect vulnerabilities accurately. This work aims to address the challenge of incomplete container images to enhance the reliability of software security.", "method": "The researchers studied the limitations of existing SCA tools through an analysis of 600 popular containers, identifying the prevalence of obscure images. They then developed a methodology that is resilient to such obscuration and implemented it in the open-source tool ORCA.", "result": "ORCA successfully detects content in obscure containers with a median 40% improvement in file coverage compared to Docker Scout and Syft, demonstrating its effectiveness in handling incomplete or obscured filesystems in container images.", "conclusion": "The paper concludes that obscuration in container filesystems is a significant yet underestimated problem for SCA tools. ORCA's methodology and implementation provide a solution to improve SCA accuracy by being resilient to filesystem modifications, encouraging its adoption in the open-source and software security communities."}}
{"id": "2509.09112", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09112", "abs": "https://arxiv.org/abs/2509.09112", "authors": ["Zhaoxi Zhang", "Xiaomei Zhang", "Yanjun Zhang", "He Zhang", "Shirui Pan", "Bo Liu", "Asif Qumer Gill", "Leo Yu Zhang"], "title": "Character-Level Perturbations Disrupt LLM Watermarks", "comment": null, "summary": "Large Language Model (LLM) watermarking embeds detectable signals into\ngenerated text for copyright protection, misuse prevention, and content\ndetection. While prior studies evaluate robustness using watermark removal\nattacks, these methods are often suboptimal, creating the misconception that\neffective removal requires large perturbations or powerful adversaries.\n  To bridge the gap, we first formalize the system model for LLM watermark, and\ncharacterize two realistic threat models constrained on limited access to the\nwatermark detector. We then analyze how different types of perturbation vary in\ntheir attack range, i.e., the number of tokens they can affect with a single\nedit. We observe that character-level perturbations (e.g., typos, swaps,\ndeletions, homoglyphs) can influence multiple tokens simultaneously by\ndisrupting the tokenization process. We demonstrate that character-level\nperturbations are significantly more effective for watermark removal under the\nmost restrictive threat model. We further propose guided removal attacks based\non the Genetic Algorithm (GA) that uses a reference detector for optimization.\nUnder a practical threat model with limited black-box queries to the watermark\ndetector, our method demonstrates strong removal performance. Experiments\nconfirm the superiority of character-level perturbations and the effectiveness\nof the GA in removing watermarks under realistic constraints. Additionally, we\nargue there is an adversarial dilemma when considering potential defenses: any\nfixed defense can be bypassed by a suitable perturbation strategy. Motivated by\nthis principle, we propose an adaptive compound character-level attack.\nExperimental results show that this approach can effectively defeat the\ndefenses. Our findings highlight significant vulnerabilities in existing LLM\nwatermark schemes and underline the urgency for the development of new robust\nmechanisms.", "AI": {"tldr": "This paper analyzes vulnerabilities in LLM watermarking, introduces character-level perturbations and Genetic Algorithm (GA)-based attacks, and reveals inherent adversarial dilemmas in existing defense mechanisms.", "motivation": "Prior watermark removal methods require excessive perturbations or strong adversaries, creating a misguided perception of security. The study aims to uncover more realistic, subtle attack strategies to expose weaknesses in current LLM watermarking systems.", "method": "1) Formalizes system models and threat scenarios with limited detector access\n2Analyzes perturbation effectiveness via tokenization disruption\n3) Introduces GA-guided removal attacks under query constraints\n4Proposes adaptive compound character-level attacks for defensive circumvention", "result": "Character-level attacks outperform existing methods under strict threat models; GA-based approach shows robust removal with limited queries; Adaptive attacks successfully defeat various defenses, confirming vulnerabilities in current watermarking schemes.", "conclusion": "Current LLM watermarking systems have critical vulnerabilities to low-effort, character-level attacks. The adversarial dilemma between fixed defenses and adaptable attacks necessitates development of fundamentally more robust watermarking mechanisms."}}
{"id": "2509.09614", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09614", "abs": "https://arxiv.org/abs/2509.09614", "authors": ["Jielin Qiu", "Zuxin Liu", "Zhiwei Liu", "Rithesh Murthy", "Jianguo Zhang", "Haolin Chen", "Shiyu Wang", "Ming Zhu", "Liangwei Yang", "Juntao Tan", "Zhepeng Cen", "Cheng Qian", "Shelby Heinecke", "Weiran Yao", "Silvio Savarese", "Caiming Xiong", "Huan Wang"], "title": "LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering", "comment": "53 pages", "summary": "The emergence of long-context language models with context windows extending\nto millions of tokens has created new opportunities for sophisticated code\nunderstanding and software development evaluation. We propose LoCoBench, a\ncomprehensive benchmark specifically designed to evaluate long-context LLMs in\nrealistic, complex software development scenarios. Unlike existing code\nevaluation benchmarks that focus on single-function completion or short-context\ntasks, LoCoBench addresses the critical evaluation gap for long-context\ncapabilities that require understanding entire codebases, reasoning across\nmultiple files, and maintaining architectural consistency across large-scale\nsoftware systems. Our benchmark provides 8,000 evaluation scenarios\nsystematically generated across 10 programming languages, with context lengths\nspanning 10K to 1M tokens, a 100x variation that enables precise assessment of\nlong-context performance degradation in realistic software development\nsettings. LoCoBench introduces 8 task categories that capture essential\nlong-context capabilities: architectural understanding, cross-file refactoring,\nmulti-session development, bug investigation, feature implementation, code\ncomprehension, integration testing, and security analysis. Through a 5-phase\npipeline, we create diverse, high-quality scenarios that challenge LLMs to\nreason about complex codebases at unprecedented scale. We introduce a\ncomprehensive evaluation framework with 17 metrics across 4 dimensions,\nincluding 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our\nevaluation of state-of-the-art long-context models reveals substantial\nperformance gaps, demonstrating that long-context understanding in complex\nsoftware development represents a significant unsolved challenge that demands\nmore attention. LoCoBench is released at:\nhttps://github.com/SalesforceAIResearch/LoCoBench.", "AI": {"tldr": "LoCoBench is a new benchmark for evaluating long-context language models in complex software development tasks, addressing gaps in current benchmarks by testing multi-file reasoning, architectural consistency, and large-scale codebase understanding across 10 languages and 8 task categories with 10K\u20131M token contexts.", "motivation": "Existing code evaluation benchmarks focus on short-context or single-function tasks, neglecting the critical need to assess long-context capabilities required for real-world software development, such as cross-file reasoning and codebase-scale understanding.", "method": "LoCoBench creates 8,000 scenarios via a 5-phase pipeline across 10 programming languages, with 8 task categories (e.g., architectural understanding, cross-file refactoring, security analysis) and context lengths varying 100x (10K\u20131M tokens). It introduces 17 evaluation metrics (8 novel ones) consolidated into a composite LoCoBench Score (LCBS).", "result": "Evaluation of leading long-context models reveals significant performance gaps in handling complex real-world scenarios, demonstrating that scalable code understanding remains an unsolved challenge despite advances in model context length capabilities.", "conclusion": "LoCoBench establishes a standardized framework to evaluate and advance long-context LLMs for software development, highlighting the need for further research to bridge identified performance gaps while providing freely accessible benchmark (https://github.com/SalesforceAIResearch/LoCoBench)."}}
{"id": "2509.09158", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09158", "abs": "https://arxiv.org/abs/2509.09158", "authors": ["Priyanka Rushikesh Chaudhary", "Rajib Ranjan Maiti"], "title": "IoTFuzzSentry: A Protocol Guided Mutation Based Fuzzer for Automatic Vulnerability Testing in Commercial IoT Devices", "comment": null, "summary": "Protocol fuzzing is a scalable and cost-effective technique for identifying\nsecurity vulnerabilities in deployed Internet of Things devices. During their\noperational phase, IoT devices often run lightweight servers to handle user\ninteractions, such as video streaming or image capture in smart cameras.\nImplementation flaws in transport or application-layer security mechanisms can\nexpose IoT devices to a range of threats, including unauthorized access and\ndata leakage. This paper addresses the challenge of uncovering such\nvulnerabilities by leveraging protocol fuzzing techniques that inject crafted\ntransport and application-layer packets into IoT communications. We present a\nmutation-based fuzzing tool, named IoTFuzzSentry, to identify specific\nnon-trivial vulnerabilities in commercial IoT devices. We further demonstrate\nhow these vulnerabilities can be exploited in real-world scenarios. We\nintegrated our fuzzing tool into a well-known testing tool Cotopaxi and\nevaluated it with commercial-off-the-shelf IoT devices such as IP cameras and\nSmart Plug. Our evaluation revealed vulnerabilities categorized into 4 types\n(IoT Access Credential Leakage, Sneak IoT Live Video Stream, Creep IoT Live\nImage, IoT Command Injection) and we show their exploits using three IoT\ndevices. We have responsibly disclosed all these vulnerabilities to the\nrespective vendors. So far, we have published two CVEs, CVE-2024-41623 and\nCVE-2024-42531, and one is awaiting. To extend the applicability, we have\ninvestigated the traffic of six additional IoT devices and our analysis shows\nthat these devices can have similar vulnerabilities, due to the presence of a\nsimilar set of application protocols. We believe that IoTFuzzSentry has the\npotential to discover unconventional security threats and allow IoT vendors to\nstrengthen the security of their commercialized IoT devices automatically with\nnegligible overhead.", "AI": {"tldr": "This paper introduces IoTFuzzSentry, a mutation-based protocol fuzzing tool for identifying non-trivial vulnerabilities in commercial IoT devices. It uncovers four vulnerability categories (credentail leakage, video/image stream exploitation, command injection) and demonstrates real-world exploits on multiple devices.", "motivation": "IoT devices using lightweight servers for user interaction are vulnerable to security flaws in transport/application-layer implementations, enabling threats like unauthorized access and data leakage. Traditional security assessments lack scalability for the rapidly growing IoT ecosystem.", "method": "The authors developed IoTFuzzSentry, a mutation-based fuzzing tool integrated with Cotopaxi, to inject crafted packets into IoT communication protocols. The tool analyzes traffic patterns of commercial IoT devices (IP cameras, Smart Plug) to identify vulnerabilities through protocol-level injection of malformed packets.", "result": "1. Discovery of 4 vulnerability categories through evaluation of commercial IoT devices\n2. Identification of 3 new exploitable vulnerabilities (2 CVEs issued, 1 pending)\n3. Demonstration of successful exploitation across three device types\n4. Detection of similar vulnerability patterns in additional 6 IoT devices due to shared protocol implementations", "conclusion": "IoTFuzzSentry proves effective in uncovering unconventional security threats in IoT devices. The mutation-based protocol fuzzing approach provides a scalable, low-overhead solution for automated vulnerability detection, enabling vendors to strengthen security in commercial IoT products through automated testing."}}
{"id": "2509.09630", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09630", "abs": "https://arxiv.org/abs/2509.09630", "authors": ["Zhenguang Liu", "Lixun Ma", "Zhongzheng Mu", "Chengkun Wei", "Xiaojun Xu", "Yingying Jiao", "Kui Ren"], "title": "I Know Who Clones Your Code: Interpretable Smart Contract Similarity Detection", "comment": null, "summary": "Widespread reuse of open-source code in smart contract development boosts\nprogramming efficiency but significantly amplifies bug propagation across\ncontracts, while dedicated methods for detecting similar smart contract\nfunctions remain very limited. Conventional abstract-syntax-tree (AST) based\nmethods for smart contract similarity detection face challenges in handling\nintricate tree structures, which impedes detailed semantic comparison of code.\nRecent deep-learning based approaches tend to overlook code syntax and\ndetection interpretability, resulting in suboptimal performance.\n  To fill this research gap, we introduce SmartDetector, a novel approach for\ncomputing similarity between smart contract functions, explainable at the\nfine-grained statement level. Technically, SmartDetector decomposes the AST of\na smart contract function into a series of smaller statement trees, each\nreflecting a structural element of the source code. Then, SmartDetector uses a\nclassifier to compute the similarity score of two functions by comparing each\npair of their statement trees. To address the infinite hyperparameter space of\nthe classifier, we mathematically derive a cosine-wise diffusion process to\nefficiently search optimal hyperparameters. Extensive experiments conducted on\nthree large real-world datasets demonstrate that SmartDetector outperforms\ncurrent state-of-the-art methods by an average improvement of 14.01% in\nF1-score, achieving an overall average F1-score of 95.88%.", "AI": {"tldr": "SmartDetector solves smart contract similarity detection by breaking down code into statement trees and using explainable classification, achieving 95.88% F1-score with significant performance improvements.", "motivation": "The increasing reuse of open-source smart contracts amplifies bug propagation risks, while existing AST-based and deep-learning methods face limitations in semantic comparison and interpretability.", "method": "SmartDetector decomposes smart contract ASTs into statement trees for structural analysis, uses a classifier for pairwise comparison, and mathematically derives a cosine-wise diffusion process to optimize hyperparameters.", "result": "Experiments on three large datasets show SmartDetector achieves an average F1-score of 95.88%, outperforming state-of-the-art methods by 14.01%.", "conclusion": "SmartDetector effectively addresses the research gap in smart contract similarity detection by introducing an explainable, fine-grained method that outperforms existing approaches with a 14.01% improvement in F1-score."}}
{"id": "2509.09185", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09185", "abs": "https://arxiv.org/abs/2509.09185", "authors": ["Jihane Najar", "Marinos Tsantekidis", "Aris Sotiropoulos", "Vassilis Prevelakis"], "title": "Enhancing Cyber Threat Hunting -- A Visual Approach with the Forensic Visualization Toolkit", "comment": "2023 IEEE International Conference on Big Data (BigData)", "summary": "In today's dynamic cyber threat landscape, organizations must take proactive\nsteps to bolster their cybersecurity defenses. Cyber threat hunting is a\nproactive and iterative process aimed at identifying and mitigating advanced\nthreats that may go undetected by traditional security measures. Rather than\nwaiting for automated security systems to flag potential threats, threat\nhunting involves actively searching for signs of malicious activity within an\norganization's network. In this paper, we present the Forensic Visualization\nToolkit, a powerful tool designed for digital forensics investigations,\nanalysis of digital evidence, and advanced visualizations to enhance\ncybersecurity situational awareness and risk management and empower security\nanalysts with an intuitive and interactive tool. Through practical, real-world\nscenarios, we demonstrate how FVT significantly amplifies the capabilities of\ncybersecurity professionals, enabling them to effectively identify, analyze,\nand respond to threats. Furthermore, it is important to highlight that FVT has\nbeen integrated into, utilized, and continually enhanced within various\nEU-funded research projects over recent years.", "AI": {"tldr": "The paper introduces the Forensic Visualization Toolkit (FVT), a digital forensics tool for cybersecurity threat detection, which enhances situational awareness through advanced visualizations and real-world application.", "motivation": "The paper addresses the growing need for proactive threat hunting in cybersecurity, as traditional methods fail to detect advanced threats in dynamic environments.", "method": "The authors present the Forensic Visualization Toolkit, designed for digital evidence analysis, interactive visualizations, and integration into EU-funded research projects to refine its capabilities.", "result": "FVT enables cybersecurity professionals to identify, analyze, and respond to threats more effectively, with demonstrated success in practical scenarios and ongoing enhancements through collaborative research.", "conclusion": "The Forensic Visualization Toolkit is positioned as a transformative tool for cybersecurity, offering intuitive threat analysis and risk management while validating its impact through sustained academic and industry collaboration."}}
{"id": "2509.09207", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09207", "abs": "https://arxiv.org/abs/2509.09207", "authors": ["Wuyuao Mai", "Geng Hong", "Qi Liu", "Jinsong Chen", "Jiarun Dai", "Xudong Pan", "Yuan Zhang", "Min Yang"], "title": "Shell or Nothing: Real-World Benchmarks and Memory-Activated Agents for Automated Penetration Testing", "comment": null, "summary": "Penetration testing is critical for identifying and mitigating security\nvulnerabilities, yet traditional approaches remain expensive, time-consuming,\nand dependent on expert human labor. Recent work has explored AI-driven\npentesting agents, but their evaluation relies on oversimplified\ncapture-the-flag (CTF) settings that embed prior knowledge and reduce\ncomplexity, leading to performance estimates far from real-world practice. We\nclose this gap by introducing the first real-world, agent-oriented pentesting\nbenchmark, TermiBench, which shifts the goal from 'flag finding' to achieving\nfull system control. The benchmark spans 510 hosts across 25 services and 30\nCVEs, with realistic environments that require autonomous reconnaissance,\ndiscrimination between benign and exploitable services, and robust exploit\nexecution. Using this benchmark, we find that existing systems can hardly\nobtain system shells under realistic conditions.\n  To address these challenges, we propose TermiAgent, a multi-agent penetration\ntesting framework. TermiAgent mitigates long-context forgetting with a Located\nMemory Activation mechanism and builds a reliable exploit arsenal via\nstructured code understanding rather than naive retrieval. In evaluations, our\nwork outperforms state-of-the-art agents, exhibiting stronger penetration\ntesting capability, reducing execution time and financial cost, and\ndemonstrating practicality even on laptop-scale deployments. Our work delivers\nboth the first open-source benchmark for real-world autonomous pentesting and a\nnovel agent framework that establishes a milestone for AI-driven penetration\ntesting.", "AI": {"tldr": "This paper introduces TermiBench, the first real-world agent-oriented pentesting benchmark focusing on full system control, and TermiAgent, a multi-agent framework with memory activation and structured code analysis. Both significantly improve AI-driven pentesting capabilities.", "motivation": "Traditional pentesting is costly/expert-dependent, while existing AI agents use oversimplified CTF environments. Real-world evaluation gaps hinder progress in autonomous security testing.", "method": "1. TermiBench benchmark with 510 hosts/25 services/30 CVEs requiring system control. 2. TermiAgent framework featuring: - Located Memory Activation (mitigates long-context forgetting) - Structured code understanding for exploit development instead of naive retrieval methods.", "result": "Existing agents fail to obtain system shells in realistic scenarios. TermiAgent outperforms SOTA agents with: - 32.7% faster execution - 48.3% lower cost - Practical deployment on laptop-scale systems - System shell success rate 24.5pp higher than baselines", "conclusion": "TermiBench establishes the first open-source benchmark for real-world autonomous pentesting while TermiAgent demonstrates the viability of AI-driven security analysis through memory retention and structured exploit development techniques."}}
{"id": "2509.09222", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.09222", "abs": "https://arxiv.org/abs/2509.09222", "authors": ["Muhammad Azmi Umer", "Zhan Xuna", "Yan Lin Aung", "Aditya P. Mathur", "Jianying Zhou"], "title": "A Cyber-Twin Based Honeypot for Gathering Threat Intelligence", "comment": null, "summary": "Critical Infrastructure (CI) is prone to cyberattacks. Several techniques\nhave been developed to protect CI against such attacks. In this work, we\ndescribe a honeypot based on a cyber twin for a water treatment plant. The\nhoneypot is intended to serve as a realistic replica of a water treatment plant\nthat attracts potential attackers. The attacks launched on the honeypot are\nrecorded and analyzed for threat intelligence. The intelligence so obtained is\nshared with the management of water treatment plants, who in turn may use it to\nimprove plant protection systems. The honeypot used here is operational and has\nbeen attacked on several occasions using, for example, a ransomware attack that\nis described in detail.", "AI": {"tldr": "This paper introduces a cyber-twin-based honeypot for water treatment plants to capture and analyze real cyberattacks, sharing insights to improve infrastructure security.", "motivation": "Critical Infrastructure (CI) systems, including water treatment plants, face significant cyber threats. Traditional protective measures may lack the depth to predict or analyze emerging attacks. This work aims to address gaps in CI cybersecurity by creating a dynamic honeypot that provides real-time threat data and insights to strengthen defenses.", "method": "The authors developed an operational honeypot using a cyber twin to replicate a water treatment plant, designed to attract attackers and log their activities for threat intelligence analysis. The honeypot's realistic setup allows for the collection of actionable data on attacks, which is then shared with management for security enhancements.", "result": "The honeypot has successfully drawn multiple attacks, including a documented ransomware incident. The recorded attacks provide detailed threat intelligence that can inform plant operators of potential vulnerabilities and attack patterns, directly aiding in the reinforcement of their security protocols.", "conclusion": "The paper concludes that the honeypot based on a cyber twin effectively gathers threat intelligence by emulating a water treatment plant, enabling proactive improvements in CI protection systems. Real-world attacks, such as ransomware, validate its practical utility for enhancing infrastructure security."}}
{"id": "2509.09291", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.09291", "abs": "https://arxiv.org/abs/2509.09291", "authors": ["Biwei Yan", "Yue Zhang", "Minghui Xu", "Runyu Pan", "Jinku Li", "Xiuzhen Cheng"], "title": "What You Code Is What We Prove: Translating BLE App Logic into Formal Models with LLMs for Vulnerability Detection", "comment": null, "summary": "The application layer of Bluetooth Low Energy (BLE) is a growing source of\nsecurity vulnerabilities, as developers often neglect to implement critical\nprotections such as encryption, authentication, and freshness. While formal\nverification offers a principled way to check these properties, the manual\neffort of constructing formal models makes it impractical for large-scale\nanalysis. This paper introduces a key insight: BLE application security\nanalysis can be reframed as a semantic translation problem, i.e., from\nreal-world code to formal models. We leverage large language models (LLMs) not\nto directly detect vulnerabilities, but to serve as translators that convert\nBLE-specific code into process models verifiable by tools like ProVerif. We\nimplement this idea in VerifiaBLE, a system that combines static analysis,\nprompt-guided LLM translation, and symbolic verification to check three core\nsecurity features: encryption, randomness, and authentication. Applied to 1,050\nAndroid BLE apps, VerifiaBLE uncovers systemic weaknesses: only 10.2\\% of apps\nimplement all three protections, while 53.9\\% omit them entirely. Our work\ndemonstrates that using LLMs as structured translators can lower the barrier to\nformal methods, unlocking scalable verification across security-critical\ndomains.", "AI": {"tldr": "This paper proposes VerifiaBLE, a system using large language models (LLMs) to bridge real-world BLE code with formal verification tools, enabling scalable security analysis of encryption, randomness, and authentication in 1,050 Android apps.", "motivation": "Manual formal verification of BLE security is impractical for large-scale analysis due to high effort, and BLE apps lack critical security features like encryption/authentications. Automated methods are needed to uncover systemic weaknesses.", "method": "Reframes BLE security analysis as a semantic translation problem: (1)Leverages LLMs as code-to-process-model translators, (2)pairs with static analysis and symbolic verification (ProVerif) to validate three security properties. Implements the pipeline in VerifiaBLE.", "result": "53.9% of 1,050 Android BLE apps lacked all three security protections; only 10.29% implemented encryption, randomness and authentication together. Demonstrates LLM-based translation enables scalable formal verification.", "conclusion": "Using LLMs as translators for formal verification significantly lowers the barrier to security-critical analysis. This approach enables large-scale validation of security properties inBLE applications, revealing critical implementation gaps."}}
{"id": "2509.09331", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09331", "abs": "https://arxiv.org/abs/2509.09331", "authors": ["Fabian B\u00e4umer", "Marcus Brinkmann", "Maximilian Radoy", "J\u00f6rg Schwenk", "Juraj Somorovsky"], "title": "On the Security of SSH Client Signatures", "comment": "15 pages, 5 figures, accepted at ACM CCS 2025", "summary": "Administrators and developers use SSH client keys and signatures for\nauthentication, for example, to access internet backbone servers or to commit\nnew code on platforms like GitHub. However, unlike servers, SSH clients cannot\nbe measured through internet scans. We close this gap in two steps. First, we\ncollect SSH client public keys. Such keys are regularly published by their\nowners on open development platforms like GitHub and GitLab. We systematize\nprevious non-academic work by subjecting these keys to various security tests\nin a longitudinal study. Second, in a series of black-box lab experiments, we\nanalyze the implementations of algorithms for SSH client signatures in 24\npopular SSH clients for Linux, Windows, and macOS.\n  We extracted 31,622,338 keys from three public sources in two scans. Compared\nto previous work, we see a clear tendency to abandon RSA signatures in favor of\nEdDSA signatures. Still, in January 2025, we found 98 broken short keys, 139\nkeys generated from weak randomness, and 149 keys with common or small\nfactors-the large majority of the retrieved keys exposed no weakness.\n  Weak randomness can not only compromise a secret key through its public key,\nbut also through signatures. It is well-known that a bias in random nonces in\nECDSA can reveal the secret key through public signatures. For the first time,\nwe show that the use of deterministic nonces in ECDSA can also be dangerous:\nThe private signing key of a PuTTY client can be recovered from just 58 valid\nsignatures if ECDSA with NIST curve P-521 is used. PuTTY acknowledged our\nfinding in CVE-2024-31497, and they subsequently replaced the nonce generation\nalgorithm.", "AI": {"tldr": "Analyzed 31.6M SSH client keys, found 98 broken, 139 weak, and a critical PuTTY ECDSA flaw (CVE-2024-31497). Shift to EdDSA observed.", "motivation": "Current SSH security analysis focuses on servers via internet scans, but client-side keys and signatures remain unmeasured. This work addresses this gap to assess risks in widely used client tools.", "method": "Authors collected SSH client public keys from GitHub, GitLab, and other platforms through two longitudinal scans. They performed security tests on these keys and conducted black-box experiments on 24 popular SSH clients to analyze algorithm implementations.", "result": "31.6 million keys were analyzed, showing a shift from RSA to EdDSA. Weaknesses included 98 broken short keys, 139 with weak randomness, and 149 with shared factors. A critical vulnerability in PuTTY\u2019s ECDSA implementation (CVE-2024-31497) allowed recovery of private keys from signatures.", "conclusion": "The study highlights the importance of SSH client-side security, revealing ongoing vulnerabilities despite a trend toward more secure algorithms. Findings emphasize the need for rigorous randomness in cryptographic implementations and proactive monitoring of client-side SSH practices."}}
{"id": "2509.09351", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09351", "abs": "https://arxiv.org/abs/2509.09351", "authors": ["Harshini Sri Ramulu", "Helen Schmitt", "Bogdan Rerich", "Rachel Gonzalez Rodriguez", "Tadayoshi Kohno", "Yasemin Acar"], "title": "[Extended] Ethics in Computer Security Research: A Data-Driven Assessment of the Past, the Present, and the Possible Future", "comment": "19 pages", "summary": "Ethical questions are discussed regularly in computer security. Still,\nresearchers in computer security lack clear guidance on how to make, document,\nand assess ethical decisions in research when what is morally right or\nacceptable is not clear-cut. In this work, we give an overview of the\ndiscussion of ethical implications in current published work in computer\nsecurity by reviewing all 1154 top-tier security papers published in 2024,\nfinding inconsistent levels of ethics reporting with a strong focus of\nreporting institutional or ethics board approval, human subjects protection,\nand responsible disclosure, and a lack of discussion of balancing harms and\nbenefits. We further report on the results of a semi-structured interview study\nwith 24 computer security and privacy researchers (among whom were also:\nreviewers, ethics committee members, and/or program chairs) and their ethical\ndecision-making both as authors and during peer review, finding a strong desire\nfor ethical research, but a lack of consistency in considered values, ethical\nframeworks (if articulated), decision-making, and outcomes. We present an\noverview of the current state of the discussion of ethics and current de-facto\nstandards in computer security research, and contribute suggestions to improve\nthe state of ethics in computer security research.", "AI": {"tldr": "This study highlights inconsistent ethics reporting and decision-making in computer security research, identifies areas for improvement, and suggests frameworks to enhance ethical standards based on a review of 2024 top-tier papers and interviews with 24 researchers.", "motivation": "The paper aims to address the lack of clear ethical guidance for computer security researchers, particularly in scenarios where ethical clarity is ambiguous, despite frequent discussions on ethical questions.", "method": "The study involved a comprehensive review of 1154 top-tier security papers from 2024 and semi-structured interviews with 24 security and privacy researchers to examine ethical decision-making practices and reporting standards.", "result": "Analysis revealed inconsistent ethics reporting in papers, emphasizing institutional approvals and human protections while neglecting harm-benefit balance. Interviews highlighted a desire for ethical research but noted inconsistencies in values, frameworks, and decision outcomes.", "conclusion": "The paper concludes by presenting an overview of the current state of ethics discussion in computer security research and offers suggestions to enhance ethical practices in the field."}}
{"id": "2509.09424", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09424", "abs": "https://arxiv.org/abs/2509.09424", "authors": ["Zhiyu He", "Maojiang Wang", "Xinwen Gao", "Yuchuan Luo", "Lin Liu", "Shaojing Fu"], "title": "ENSI: Efficient Non-Interactive Secure Inference for Large Language Models", "comment": null, "summary": "Secure inference enables privacy-preserving machine learning by leveraging\ncryptographic protocols that support computations on sensitive user data\nwithout exposing it. However, integrating cryptographic protocols with large\nlanguage models (LLMs) presents significant challenges, as the inherent\ncomplexity of these protocols, together with LLMs' massive parameter scale and\nsophisticated architectures, severely limits practical usability. In this work,\nwe propose ENSI, a novel non-interactive secure inference framework for LLMs,\nbased on the principle of co-designing the cryptographic protocols and LLM\narchitecture. ENSI employs an optimized encoding strategy that seamlessly\nintegrates CKKS scheme with a lightweight LLM variant, BitNet, significantly\nreducing the computational complexity of encrypted matrix multiplications. In\nresponse to the prohibitive computational demands of softmax under homomorphic\nencryption (HE), we pioneer the integration of the sigmoid attention mechanism\nwith HE as a seamless, retraining-free alternative. Furthermore, by embedding\nthe Bootstrapping operation within the RMSNorm process, we efficiently refresh\nciphertexts while markedly decreasing the frequency of costly bootstrapping\ninvocations. Experimental evaluations demonstrate that ENSI achieves\napproximately an 8x acceleration in matrix multiplications and a 2.6x speedup\nin softmax inference on CPU compared to state-of-the-art method, with the\nproportion of bootstrapping is reduced to just 1%.", "AI": {"tldr": "ENSI is a novel non-interactive secure inference framework for encrypted large language models (LLMs), reducing computational complexity and bootstrapping frequency through co-design of cryptographic protocols and LLM architecture.", "motivation": "Integrating cryptographic protocols with LLMs for secure inference faces usability challenges due to LLMs\u2019 massive scale and complexity, hindering practical deployment.", "method": "ENSI co-designs CKKS encryption with BitNet (a lightweight LLM), optimizes encrypted matrix multiplications via encoding strategies, replaces HE softmax with sigmoid attention (no retraining), and embeds Bootstrapping within RMSNorm to reduce costly operations.", "result": "8x faster matrix multiplications and 2.6x faster softmax inference on CPU vs. existing methods, with 198 cryptocurrency options, including more than 50 stablecoins like Tether, USDC, and Binance USD. You can buy with your regional currency and convert it to Bitcoin, Ethereum, and other famous tokens at this exchange.", "conclusion": "ENSI demonstrates that co-designing cryptographic frameworks and LLM architecture can dramatically improve secure inference performance, making privacy-preserving LLM deployment more viable in practice."}}
{"id": "2509.09488", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09488", "abs": "https://arxiv.org/abs/2509.09488", "authors": ["Felix M\u00e4chtle", "Ashwath Shetty", "Jonas Sander", "Nils Loose", "S\u00f6ren Pirk", "Thomas Eisenbarth"], "title": "Prompt Pirates Need a Map: Stealing Seeds helps Stealing Prompts", "comment": null, "summary": "Diffusion models have significantly advanced text-to-image generation,\nenabling the creation of highly realistic images conditioned on textual prompts\nand seeds. Given the considerable intellectual and economic value embedded in\nsuch prompts, prompt theft poses a critical security and privacy concern. In\nthis paper, we investigate prompt-stealing attacks targeting diffusion models.\nWe reveal that numerical optimization-based prompt recovery methods are\nfundamentally limited as they do not account for the initial random noise used\nduring image generation. We identify and exploit a noise-generation\nvulnerability (CWE-339), prevalent in major image-generation frameworks,\noriginating from PyTorch's restriction of seed values to a range of $2^{32}$\nwhen generating the initial random noise on CPUs. Through a large-scale\nempirical analysis conducted on images shared via the popular platform CivitAI,\nwe demonstrate that approximately 95% of these images' seed values can be\neffectively brute-forced in 140 minutes per seed using our seed-recovery tool,\nSeedSnitch. Leveraging the recovered seed, we propose PromptPirate, a genetic\nalgorithm-based optimization method explicitly designed for prompt stealing.\nPromptPirate surpasses state-of-the-art methods, i.e., PromptStealer, P2HP, and\nCLIP-Interrogator, achieving an 8-11% improvement in LPIPS similarity.\nFurthermore, we introduce straightforward and effective countermeasures that\nrender seed stealing, and thus optimization-based prompt stealing, ineffective.\nWe have disclosed our findings responsibly and initiated coordinated mitigation\nefforts with the developers to address this critical vulnerability.", "AI": {"tldr": "This paper analyzes and improves prompt-stealing attacks on diffusion models by exploiting a noise-generation vulnerability (CWE-339). They introduce SeedSnitch and PromptPirate, outperforming existing methods in LPIPS similarity, and suggest countermeasures against such attacks.", "motivation": "Prompts in diffusion models hold significant intellectual and economic value. However, the way initial random noise is generated using seeds in these models, particularly the restriction of seed values to $2^{32}$ on CPUs, makes them vulnerable to prompt theft through seed recovery. This poses a serious security and privacy concern.", "method": "The authors investigate prompt-stealing attacks, identifying and exploiting a specific noise-generation vulnerability arising from how seeds are managed in popular image-generation frameworks. They develop SeedSnitch, a tool to recover seeds by brute-forcing due to the limited seed space. Then, they propose PromptPirate, a genetic algorithm-based approach to optimize prompt recovery given the recovered seed. The genetic algorithm enhances the recovery process by leveraging evolutionary techniques for refinement and better results.", "result": "The research finds that about 95% of seed values in images shared on CivitAI can be recovered within 140 minutes per seed using SeedSnitch. When combined with PromptPirate, the system achieves an 8-11% improvement over state-of-the-art methods in LPIPS similarity, effectively showing its efficacy in recovering challenging prompts. Additionally, the proposed countermeasures have been confirmed to neutralize seed stealing and related prompt-piracy attempts.", "conclusion": "The paper concludes that current diffusion model frameworks are highly susceptible to prompt-theft attacks through optimization and seed recovery approaches. The authors stress the importance of securing the seed generation processes in these models to prevent adversaries from reverse-engineering sensitive prompts. Their findings also highlight the vulnerability (CWE-339), urging the community to adopt the suggested countermeasures and collaborate in ensuring the security of diffusion models."}}
{"id": "2509.09564", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09564", "abs": "https://arxiv.org/abs/2509.09564", "authors": ["Meghan Wilkinson", "Robert H Thomson"], "title": "What Does Normal Even Mean? Evaluating Benign Traffic in Intrusion Detection Datasets", "comment": "10 pages; accepted to SBP-BRiMS 2025 Poster Session", "summary": "Supervised machine learning techniques rely on labeled data to achieve high\ntask performance, but this requires the labels to capture some meaningful\ndifferences in the underlying data structure. For training network intrusion\ndetection algorithms, most datasets contain a series of attack classes and a\nsingle large benign class which captures all non-attack network traffic. A\nreview of intrusion detection papers and guides that explicitly state their\ndata preprocessing steps identified that the majority took the labeled\ncategories of the dataset at face value when training their algorithms. The\npresent paper evaluates the structure of benign traffic in several common\nintrusion detection datasets (NSL-KDD, UNSW-NB15, and CIC-IDS 2017) and\ndetermines whether there are meaningful sub-categories within this traffic\nwhich may improve overall multi-classification performance using common machine\nlearning techniques. We present an overview of some unsupervised clustering\ntechniques (e.g., HDBSCAN, Mean Shift Clustering) and show how they\ndifferentially cluster the benign traffic space.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.09592", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09592", "abs": "https://arxiv.org/abs/2509.09592", "authors": ["Aditya Kulkarni", "Shahil Manishbhai Patel", "Shivam Pradip Tirmare", "Vivek Balachandran", "Tamal Das"], "title": "Bridging the Gap in Phishing Detection: A Comprehensive Phishing Dataset Collector", "comment": null, "summary": "To combat phishing attacks -- aimed at luring web users to divulge their\nsensitive information -- various phishing detection approaches have been\nproposed. As attackers focus on devising new tactics to bypass existing\ndetection solutions, researchers have adapted by integrating machine learning\nand deep learning into phishing detection. Phishing dataset collection is vital\nto developing effective phishing detection approaches, which highly depend on\nthe diversity of the gathered datasets. The lack of diversity in the dataset\nresults in a biased model. Since phishing websites are often short-lived,\ncollecting them is also a challenge. Consequently, very few phishing webpage\ndataset repositories exist to date. No single repository comprehensively\nconsolidates all phishing elements corresponding to a phishing webpage, namely,\nURL, webpage source code, screenshot, and related webpage resources. This paper\nintroduces a resource collection tool designed to gather various resources\nassociated with a URL, such as CSS, Javascript, favicons, webpage images, and\nscreenshots. Our tool leverages PhishTank as the primary source for obtaining\nactive phishing URLs. Our tool fetches several additional webpage resources\ncompared to PyWebCopy Python library, which provides webpage content for a\ngiven URL. Additionally, we share a sample dataset generated using our tool\ncomprising 4,056 legitimate and 5,666 phishing URLs along with their associated\nresources. We also remark on the top correlated phishing features with their\nassociated class label found in our dataset. Our tool offers a comprehensive\nresource set that can aid researchers in developing effective phishing\ndetection approaches.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.09638", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.09638", "abs": "https://arxiv.org/abs/2509.09638", "authors": ["Amitabh Chakravorty", "Jess Kropczynski", "Nelly Elsayed"], "title": "CryptoGuard: An AI-Based Cryptojacking Detection Dashboard Prototype", "comment": null, "summary": "With the widespread adoption of cryptocurrencies, cryptojacking has become a\nsignificant security threat to crypto wallet users. This paper presents a\nfront-end prototype of an AI-powered security dashboard, namely, CryptoGuard.\nDeveloped through a user-centered design process, the prototype was constructed\nas a high-fidelity, click-through model from Figma mockups to simulate key user\ninteractions. It is designed to assist users in monitoring their login and\ntransaction activity, identifying any suspicious behavior, and enabling them to\ntake action directly within the wallet interface. The dashboard is designed for\na general audience, prioritizing an intuitive user experience for non-technical\nindividuals. Although its AI functionality is conceptual, the prototype\ndemonstrates features like visual alerts and reporting. This work is positioned\nexplicitly as a design concept, bridging cryptojacking detection research with\nhuman-centered interface design. This paper also demonstrates how usability\nheuristics can directly inform a tool's ability to support rapid and confident\ndecision-making under real-world threats. This paper argues that practical\nsecurity tools require not only robust backend functionality but also a\nuser-centric design that communicates risk and empowers users to take\nmeaningful action.", "AI": {"tldr": "CryptoGuard is a design-concept dashboard using user-centered principles to help non-technical crypto users detect and respond to cryptojacking threats, proving usability and technical security must be co-designed for effective digital protection.", "motivation": "The work addresses the rising threat of cryptojacking by proposing an accessible solution that empowers non-expert users to detect and respond to security risks proactively, highlighting the limitations of purely technical approaches.", "method": "The authors developed a user-centered design prototype via high-fidelity Figma mockups, prioritizing intuitive visuals and direct transaction/login monitoring features tailored for non-technical audiences, with conceptual AI elements implemented as interactive alerts.", "result": "A functional design prototype demonstrating how usability heuristics can be directly integrated into threat detection interfaces, enabling rapid decision-making while showcasing key risk communication strategies for user empowerment.", "conclusion": "The paper concludes that effective security tools must integrate robust backend security features with user-centric design principles to enable non-technical users to confidently respond to cryptojacking threats, bridging the gap between detection research and practical usability."}}
