<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 9]
- [cs.SE](#cs.SE) [Total: 6]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [A Comprehensive Survey of Website Fingerprinting Attacks and Defenses in Tor: Advances and Open Challenges](https://arxiv.org/abs/2510.11804)
*Yuwen Cui,Guangjing Wang,Khanh Vu,Kai Wei,Kehan Shen,Zhengyuan Jiang,Xiao Han,Ning Wang,Zhuo Lu,Yao Liu*

Main category: cs.CR

TL;DR: This paper provides a comprehensive survey of website fingerprinting (WF) research in the Tor network, categorizing existing work into datasets, attack models, and defense mechanisms while identifying trade-offs and emerging challenges.


<details>
  <summary>Details</summary>
Motivation: Tor's anonymity is threatened by WF attacks, yet prior defenses lack systematic evaluation and unified analysis. Existing research lacks a holistic survey consolidating methodologies and challenges.

Method: Systematic categorization of WF research into three domains (datasets, attack models, defense mechanisms), followed by comparative analysis of techniques and evaluation of strengths/limitations under diverse threat models.

Result: In-depth comparative analysis of WF techniques, highlighting trade-offs between privacy/usability/performance, and identification of emerging challenges like multi-tab browsing and coarse-grained traffic features.

Conclusion: This survey establishes a foundational framework for advancing Tor privacy by consolidating prior work, clarifying open research directions, and emphasizing the need for balanced defense strategies.

Abstract: The Tor network provides users with strong anonymity by routing their
internet traffic through multiple relays. While Tor encrypts traffic and hides
IP addresses, it remains vulnerable to traffic analysis attacks such as the
website fingerprinting (WF) attack, achieving increasingly high fingerprinting
accuracy even under open-world conditions. In response, researchers have
proposed a variety of defenses, ranging from adaptive padding, traffic
regularization, and traffic morphing to adversarial perturbation, that seek to
obfuscate or reshape traffic traces. However, these defenses often entail
trade-offs between privacy, usability, and system performance. Despite
extensive research, a comprehensive survey unifying WF datasets, attack
methodologies, and defense strategies remains absent. This paper fills that gap
by systematically categorizing existing WF research into three key domains:
datasets, attack models, and defense mechanisms. We provide an in-depth
comparative analysis of techniques, highlight their strengths and limitations
under diverse threat models, and discuss emerging challenges such as multi-tab
browsing and coarse-grained traffic features. By consolidating prior work and
identifying open research directions, this survey serves as a foundation for
advancing stronger privacy protection in Tor.

</details>


### [2] [BlackIce: A Containerized Red Teaming Toolkit for AI Security Testing](https://arxiv.org/abs/2510.11823)
*Caelin Kaplan,Alexander Warnecke,Neil Archibald*

Main category: cs.CR

TL;DR: BlackIce is an open-source containerized toolkit for AI red teaming inspired by Kali Linux, bundling 14 tools for LLM/ML model security testing via a unified Docker image.


<details>
  <summary>Details</summary>
Motivation: Existing AI red teaming tools face usability challenges due to fragmented tooling, conflicting dependencies, and low adoption of dedicated red teams, necessitating standardized, modular solutions to lower entry barriers.

Method: The authors designed BlackIce as a version-pinned Docker image with 14 selected open-source tools for Responsible AI/Security, modular architecture enabling community extensions, and a CLI interface simplifying local/cloud deployment.

Result: BlackIce provides a reproducible, scalable environment for AI red teaming assessments, with a documented architecture, tool selection rationale, and supported evaluation types.

Conclusion: BlackIce addresses critical gaps in AI red teaming infrastructure by combining reproducibility, modularity, and ease-of-use, enabling standardized security assessments of AI systems.

Abstract: AI models are being increasingly integrated into real-world systems, raising
significant concerns about their safety and security. Consequently, AI red
teaming has become essential for organizations to proactively identify and
address vulnerabilities before they can be exploited by adversaries. While
numerous AI red teaming tools currently exist, practitioners face challenges in
selecting the most appropriate tools from a rapidly expanding landscape, as
well as managing complex and frequently conflicting software dependencies
across isolated projects. Given these challenges and the relatively small
number of organizations with dedicated AI red teams, there is a strong need to
lower barriers to entry and establish a standardized environment that
simplifies the setup and execution of comprehensive AI model assessments.
  Inspired by Kali Linux's role in traditional penetration testing, we
introduce BlackIce, an open-source containerized toolkit designed for red
teaming Large Language Models (LLMs) and classical machine learning (ML)
models. BlackIce provides a reproducible, version-pinned Docker image that
bundles 14 carefully selected open-source tools for Responsible AI and Security
testing, all accessible via a unified command-line interface. With this setup,
initiating red team assessments is as straightforward as launching a container,
either locally or using a cloud platform. Additionally, the image's modular
architecture facilitates community-driven extensions, allowing users to easily
adapt or expand the toolkit as new threats emerge. In this paper, we describe
the architecture of the container image, the process used for selecting tools,
and the types of evaluations they support.

</details>


### [3] [Countermind: A Multi-Layered Security Architecture for Large Language Models](https://arxiv.org/abs/2510.11837)
*Dominik Schwarz*

Main category: cs.CR

TL;DR: This paper introduces Countermind, a proactive security architecture for LLMs, to combat form-first attacks by implementing pre-inference input validation, parameter-space restrictions, self-adaptive defense mechanisms, and multimodal threat mitigation.


<details>
  <summary>Details</summary>
Motivation: Current LLM security defenses relying on post-hoc output filtering are ineffective against 'form-first' attacks like prompt injection and jailbreaking, which exploit the model's inability to distinguish trusted instructions from untrusted data. This creates a need for proactive, pre-inference security mechanisms.

Method: The paper proposes Countermind, a multi-layered security architecture with four core components: (1) Semantic Boundary Logic (SBL) and Text Crypter, (2) Parameter-Space Restriction (PSR) mechanism, (3) Secure, Self-Regulating Core with OODA loop, and (4) Multimodal Input Sandbox. These components enforce input validation, restrict semantic drift, adapt defenses via audit logs, and mitigate non-textual threats.

Result: The paper establishes a conceptual framework for Countermind but does not provide quantitative results. It emphasizes designing evaluation metrics to measure ASR reduction and latency overhead for the proposed architecture in future work.

Conclusion: The paper concludes by outlining an evaluation plan to quantify the proposed architecture's effectiveness in reducing the Attack Success Rate (ASR) for form-first attacks and measuring its latency overhead, while presenting a proactive security framework for LLM applications.

Abstract: The security of Large Language Model (LLM) applications is fundamentally
challenged by "form-first" attacks like prompt injection and jailbreaking,
where malicious instructions are embedded within user inputs. Conventional
defenses, which rely on post hoc output filtering, are often brittle and fail
to address the root cause: the model's inability to distinguish trusted
instructions from untrusted data. This paper proposes Countermind, a
multi-layered security architecture intended to shift defenses from a reactive,
post hoc posture to a proactive, pre-inference, and intra-inference enforcement
model. The architecture proposes a fortified perimeter designed to structurally
validate and transform all inputs, and an internal governance mechanism
intended to constrain the model's semantic processing pathways before an output
is generated. The primary contributions of this work are conceptual designs
for: (1) A Semantic Boundary Logic (SBL) with a mandatory, time-coupled Text
Crypter intended to reduce the plaintext prompt injection attack surface,
provided all ingestion paths are enforced. (2) A Parameter-Space Restriction
(PSR) mechanism, leveraging principles from representation engineering, to
dynamically control the LLM's access to internal semantic clusters, with the
goal of mitigating semantic drift and dangerous emergent behaviors. (3) A
Secure, Self-Regulating Core that uses an OODA loop and a learning security
module to adapt its defenses based on an immutable audit log. (4) A Multimodal
Input Sandbox and Context-Defense mechanisms to address threats from
non-textual data and long-term semantic poisoning. This paper outlines an
evaluation plan designed to quantify the proposed architecture's effectiveness
in reducing the Attack Success Rate (ASR) for form-first attacks and to measure
its potential latency overhead.

</details>


### [4] [Deep Research Brings Deeper Harm](https://arxiv.org/abs/2510.11851)
*Shuo Chen,Zonggen Li,Zhen Han,Bailan He,Tong Liu,Haokun Chen,Georg Groh,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CR

TL;DR: The paper analyzes the security risks of Deep Research (DR) agents based on Large Language Models (LLMs), identifying how they can produce detailed, dangerous reports even when a standalone LLM would reject the input. It introduces two jailbreak strategies to expose these risks and presents three key findings on alignment failures and vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: The paper aims to highlight the heightened risks associated with using DR agents in sensitive domains such as biosecurity, where these agents can generate dangerous content even if the original query would be rejected by a standalone LLM. This calls for a deeper safety analysis given the multi-step planning and execution capabilities of DR agents.

Method: The researchers propose two original jailbreak strategies: Plan Injection, which involves inserting malicious sub-goals into the agent's task plan; and Intent Hijack, which reframes harmful queries as academic research questions. These approaches are tested across different LLMs and safety benchmarks, including those related to biosecurity.

Result: The experiments demonstrate three main insights: (1) the alignment of LLM within DR agents can break under academic framing of harmful prompts; (2) multi-step planning weakens alignment, exposing systemic vulnerabilities not addressed by existing prompt-level safeguards; (3) DR agents are not only able to bypass refusals but also produce more coherent and dangerous content compared to single LLM outputs.

Conclusion: The paper concludes that DR agents have deep-rooted safety issues, especially due to their multi-step reasoning and synthesis capabilities, and that these represent a significant risk in high-stakes domains. It advocates for more effective alignment techniques that are specifically adapted for DR agents to prevent misuse.

Abstract: Deep Research (DR) agents built on Large Language Models (LLMs) can perform
complex, multi-step research by decomposing tasks, retrieving online
information, and synthesizing detailed reports. However, the misuse of LLMs
with such powerful capabilities can lead to even greater risks. This is
especially concerning in high-stakes and knowledge-intensive domains such as
biosecurity, where DR can generate a professional report containing detailed
forbidden knowledge. Unfortunately, we have found such risks in practice:
simply submitting a harmful query, which a standalone LLM directly rejects, can
elicit a detailed and dangerous report from DR agents. This highlights the
elevated risks and underscores the need for a deeper safety analysis. Yet,
jailbreak methods designed for LLMs fall short in exposing such unique risks,
as they do not target the research ability of DR agents. To address this gap,
we propose two novel jailbreak strategies: Plan Injection, which injects
malicious sub-goals into the agent's plan; and Intent Hijack, which reframes
harmful queries as academic research questions. We conducted extensive
experiments across different LLMs and various safety benchmarks, including
general and biosecurity forbidden prompts. These experiments reveal 3 key
findings: (1) Alignment of the LLMs often fail in DR agents, where harmful
prompts framed in academic terms can hijack agent intent; (2) Multi-step
planning and execution weaken the alignment, revealing systemic vulnerabilities
that prompt-level safeguards cannot address; (3) DR agents not only bypass
refusals but also produce more coherent, professional, and dangerous content,
compared with standalone LLMs. These results demonstrate a fundamental
misalignment in DR agents and call for better alignment techniques tailored to
DR agents. Code and datasets are available at
https://chenxshuo.github.io/deeper-harm.

</details>


### [5] [Lightweight CNN-Based Wi-Fi Intrusion Detection Using 2D Traffic Representations](https://arxiv.org/abs/2510.11898)
*Rayed Suhail Ahmad,Rehan Ahmad,Quamar Niyaz*

Main category: cs.CR

TL;DR: The paper proposes a lightweight deep learning-based NIDS for Wi-Fi environments using 2D data representations and CNNs, showing competitive performance with low latency.


<details>
  <summary>Details</summary>
Motivation: Wi-Fi networks are widely used but vulnerable to attacks due to their inherent weaknesses and widespread deployment, necessitating effective intrusion detection systems.

Method: The authors convert network traffic into 2D representations and train lightweight CNN models for intrusion detection, using five techniques for data generation and the AWID3 dataset for training.

Result: The proposed NIDS demonstrates competitive detection performance with low inference time, suitable for real-world Wi-Fi deployments.

Conclusion: The approach is effective and efficient for Wi-Fi intrusion detection, paving the way for practical deployment of deep learning methods in this domain.

Abstract: Wi-Fi networks are ubiquitous in both home and enterprise environments,
serving as a primary medium for Internet access and forming the backbone of
modern IoT ecosystems. However, their inherent vulnerabilities, combined with
widespread adoption, create opportunities for malicious actors to gain
unauthorized access or compromise sensitive data stored on connected devices.
To address these challenges, we propose a deep learning based network intrusion
detection system (NIDS) for Wi-Fi environments. Building on our previous work,
we convert network traffic into two-dimensional data representations and use
them to train DL models based on convolutional neural network (CNN)
architectures. We implement five distinct techniques for generating the
two-dimensional representations, and to ensure low detection latency, we adopt
lightweight CNN architectures in our NIDS. The models are trained using the
AWID3 dataset, a publicly available benchmark for Wi-Fi NIDS research, and are
evaluated for both binary and multi-class classification tasks. Experimental
results demonstrate that the proposed approach achieves competitive detection
performance with low inference time, making it suitable for real-world Wi-Fi
deployment scenarios.

</details>


### [6] [Robust ML-based Detection of Conventional, LLM-Generated, and Adversarial Phishing Emails Using Advanced Text Preprocessing](https://arxiv.org/abs/2510.11915)
*Deeksha Hareesha Kulal,Chidozie Princewill Arannonu,Afsah Anwar,Nidhi Rastogi,Quamar Niyaz*

Main category: cs.CR

TL;DR: The paper presents a robust phishing email detection system addressing challenges from LLM-generated and adversarial phishing emails through enhanced preprocessing and machine learning, achieving 94.26% detection accuracy and resilience against AI-powered threats.


<details>
  <summary>Details</summary>
Motivation: Phishing emails generated by large language models (LLMs) and modified via adversarial techniques evade traditional detection systems, necessitating improved defenses to combat increasingly sophisticated cyber threats.

Method: An enhanced text preprocessing pipeline (spelling correction, word splitting) counters adversarial perturbations, combined with NLP feature extraction and machine learning algorithms to detect phishing emails.

Result: Achieved 94.26% detection accuracy and 84.39% F1-score on public datasets; models demonstrated robustness against adversarial attacks (via Python TextAttack framework) and LLM-generated phishing samples (ChatGPT, Llama).

Conclusion: The proposed system effectively detects both traditional and AI-powered phishing emails, offering resilience to adversarial techniques and evolving LLM-driven threats through robust preprocessing and adaptive machine learning.

Abstract: Phishing remains a critical cybersecurity threat, especially with the advent
of large language models (LLMs) capable of generating highly convincing
malicious content. Unlike earlier phishing attempts which are identifiable by
grammatical errors, misspellings, incorrect phrasing, and inconsistent
formatting, LLM generated emails are grammatically sound, contextually
relevant, and linguistically natural. These advancements make phishing emails
increasingly difficult to distinguish from legitimate ones, challenging
traditional detection mechanisms. Conventional phishing detection systems often
fail when faced with emails crafted by LLMs or manipulated using adversarial
perturbation techniques. To address this challenge, we propose a robust
phishing email detection system featuring an enhanced text preprocessing
pipeline. This pipeline includes spelling correction and word splitting to
counteract adversarial modifications and improve detection accuracy. Our
approach integrates widely adopted natural language processing (NLP) feature
extraction techniques and machine learning algorithms. We evaluate our models
on publicly available datasets comprising both phishing and legitimate emails,
achieving a detection accuracy of 94.26% and F1-score of 84.39% in model
deployment setting. To assess robustness, we further evaluate our models using
adversarial phishing samples generated by four attack methods in Python
TextAttack framework. Additionally, we evaluate models' performance against
phishing emails generated by LLMs including ChatGPT and Llama. Results
highlight the resilience of models against evolving AI-powered phishing
threats.

</details>


### [7] [CTIArena: Benchmarking LLM Knowledge and Reasoning Across Heterogeneous Cyber Threat Intelligence](https://arxiv.org/abs/2510.11974)
*Yutong Cheng,Yang Liu,Changze Li,Dawn Song,Peng Gao*

Main category: cs.CR

TL;DR: CTIArena benchmark shows LLMs benefit from security knowledge augmentation for multi-source threat intelligence analysis.


<details>
  <summary>Details</summary>
Motivation: Existing LLM evaluations for CTI are limited to closed-book settings, narrow task coverage, and single-source analysis, despite real-world CTI requiring multi-source reasoning and domain-specific knowledge.

Method: CTIArena is introduced as a benchmark evaluating LLMs in knowledge-augmented settings for multi-source CTI analysis, spanning three categories (structured, unstructured, hybrid) and nine tasks, with retrieval-augmented techniques to enhance performance.

Result: Most evaluated LLMs improved significantly when augmented with security-specific knowledge via retrieval-augmented techniques, revealing underperformance in closed-book setups and emphasizing the need for domain-adaptation.

Conclusion: The paper highlights the necessity of domain-specific techniques to enhance LLMs for CTI, demonstrating that knowledge-augmented approaches address their limitations in multi-source, heterogeneous threat analysis.

Abstract: Cyber threat intelligence (CTI) is central to modern cybersecurity, providing
critical insights for detecting and mitigating evolving threats. With the
natural language understanding and reasoning capabilities of large language
models (LLMs), there is increasing interest in applying them to CTI, which
calls for benchmarks that can rigorously evaluate their performance. Several
early efforts have studied LLMs on some CTI tasks but remain limited: (i) they
adopt only closed-book settings, relying on parametric knowledge without
leveraging CTI knowledge bases; (ii) they cover only a narrow set of tasks,
lacking a systematic view of the CTI landscape; and (iii) they restrict
evaluation to single-source analysis, unlike realistic scenarios that require
reasoning across multiple sources. To fill these gaps, we present CTIArena, the
first benchmark for evaluating LLM performance on heterogeneous, multi-source
CTI under knowledge-augmented settings. CTIArena spans three categories,
structured, unstructured, and hybrid, further divided into nine tasks that
capture the breadth of CTI analysis in modern security operations. We evaluate
ten widely used LLMs and find that most struggle in closed-book setups but show
noticeable gains when augmented with security-specific knowledge through our
designed retrieval-augmented techniques. These findings highlight the
limitations of general-purpose LLMs and the need for domain-tailored techniques
to fully unlock their potential for CTI.

</details>


### [8] [Security and Privacy Assessment of U.S. and Non-U.S. Android E-Commerce Applications](https://arxiv.org/abs/2510.12031)
*Urvashi Kishnani,Sanchari Das*

Main category: cs.CR

TL;DR: High security and privacy risks in top-grossing e-commerce apps


<details>
  <summary>Details</summary>
Motivation: E-commerce mobile apps are used for major financial transactions, so ensuring their security and privacy is vital. This study investigates the existing issues in such apps.

Method: The team analyzed 92 Android e-commerce apps from two categories (U.S. and international) using MobSF, AndroBugs, and RiskInDroid tools.

Result: Most apps used unsecured HTTP connections with a poor MobSF security score; 77 had excessive permissions, and both groups had similar network vulnerabilities but U.S. apps were better at manifest, code, and certificate issues.

Conclusion: The research highlights the need for stronger, standardized, and user-focused security approaches for e-commerce mobile apps across all regions.

Abstract: E-commerce mobile applications are central to global financial transactions,
making their security and privacy crucial. In this study, we analyze 92
top-grossing Android e-commerce apps (58 U.S.-based and 34 international) using
MobSF, AndroBugs, and RiskInDroid. Our analysis shows widespread SSL and
certificate weaknesses, with approximately 92% using unsecured HTTP connections
and an average MobSF security score of 40.92/100. Over-privileged permissions
were identified in 77 apps. While U.S. apps exhibited fewer manifest, code, and
certificate vulnerabilities, both groups showed similar network-related issues.
We advocate for the adoption of stronger, standardized, and user-focused
security practices across regions.

</details>


### [9] [Over-Threshold Multiparty Private Set Intersection for Collaborative Network Intrusion Detection](https://arxiv.org/abs/2510.12045)
*Onur Eren Arpaci,Raouf Boutaba,Florian Kerschbaum*

Main category: cs.CR

TL;DR: This paper introduces a privacy-preserving method for collaborative network intrusion detection.


<details>
  <summary>Details</summary>
Motivation: Sharing IP addresses directly among collaborative institutions is a privacy risk due to their personally identifiable nature.

Method: The authors designed a protocol where multiple participants jointly identify over-represented IP addresses using a novel hashing scheme.

Result: The new protocol reduces computational complexity compared to existing solutions and was tested successfully on network logs from multiple institutions.

Conclusion: The paper offers two deployment strategies for the protocol, each optimized for different security and cost trade-offs.

Abstract: An important function of collaborative network intrusion detection is to
analyze the network logs of the collaborators for joint IP addresses. However,
sharing IP addresses in plain is sensitive and may be even subject to privacy
legislation as it is personally identifiable information. In this paper, we
present the privacy-preserving collection of IP addresses. We propose a single
collector, over-threshold private set intersection protocol. In this protocol
$N$ participants identify the IP addresses that appear in at least $t$
participant's sets without revealing any information about other IP addresses.
Using a novel hashing scheme, we reduce the computational complexity of the
previous state-of-the-art solution from $O(M(N \log{M}/t)^{2t})$ to
$O(t^2M\binom{N}{t})$, where $M$ denotes the dataset size. This reduction makes
it practically feasible to apply our protocol to real network logs. We test our
protocol using joint networks logs of multiple institutions. Additionally, we
present two deployment options: a collusion-safe deployment, which provides
stronger security guarantees at the cost of increased communication overhead,
and a non-interactive deployment, which assumes a non-colluding collector but
offers significantly lower communication costs and applicable to many use cases
of collaborative network intrusion detection similar to ours.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [10] [eye2vec: Learning Distributed Representations of Eye Movement for Program Comprehension Analysis](https://arxiv.org/abs/2510.11722)
*Haruhiko Yoshioka,Kazumasa Shimari,Hidetake Uwano,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: eye2vec is an infrastructure for analyzing developers' eye movements during code reading by modeling fixations as transitions between syntactic elements using distributed representations, enabling flexible analysis.


<details>
  <summary>Details</summary>
Motivation: Traditional eye-tracking analysis in programming requires manual preselection of targets (e.g., control flow/elements) and analysis methods, leading to time-consuming workflows and inconsistent results due to arbitrary AOI definitions.

Method: eye2vec represents consecutive fixations as distributed syntactic element transitions, enabling diverse analysis methods through rich semantic vector representations of code navigation patterns.

Result: Facilitates analysis of code comprehension with reduced manual effort, supports multiple AOI definitions, and provides semantically interpretable metrics through distributed vector analysis.

Conclusion: eye2vec addresses limitations of traditional eye-tracking studies by offering an automated, flexible infrastructure for analyzing developers' code-reading behaviors through a distributed representation framework.

Abstract: This paper presents eye2vec, an infrastructure for analyzing software
developers' eye movements while reading source code. In common eye-tracking
studies in program comprehension, researchers must preselect analysis targets
such as control flow or syntactic elements, and then develop analysis methods
to extract appropriate metrics from the fixation for source code. Here,
researchers can define various levels of AOIs like words, lines, or code
blocks, and the difference leads to different results. Moreover, the
interpretation of fixation for word/line can vary across the purposes of the
analyses. Hence, the eye-tracking analysis is a difficult task that depends on
the time-consuming manual work of the researchers. eye2vec represents
continuous two fixations as transitions between syntactic elements using
distributed representations. The distributed representation facilitates the
adoption of diverse data analysis methods with rich semantic interpretations.

</details>


### [11] [Task-Aware Reduction for Scalable LLM-Database Systems](https://arxiv.org/abs/2510.11813)
*Marcus Emmanuel Barnes,Taher A. Ghaleb,Safwat Hassan*

Main category: cs.SE

TL;DR: This paper advocates treating LLM token budgets as 'attention budgets' and proposes task-aware text reduction as a core principle for efficient LLM-data integration, emphasizing adaptive preprocessing to prioritize task-relevant information in noisy data workflows.


<details>
  <summary>Details</summary>
Motivation: Real-world data like logs and telemetry are verbose/noisy, directly ingesting them into LLMs is costly and misaligned with task goals. Prior work focuses on model optimizations, neglecting upstream input reduction challenges.

Method: Reframing input reduction as 'attention allocation' rather than compression. Outlines research challenges: developing benchmarks, adaptive reduction pipelines, and token-budget-aware preprocessing integration into database/retrieval systems.

Result: Proposes a design framework for task-driven input reduction, emphasizing sustainable LLM usage through intelligent signal prioritization in data-intensive workflows.

Conclusion: 呼吁将注意力资源集中于关键信号，通过上游文本优化实现可扩展、精准且环保的LLM-数据融合系统，推动数据库与检索系统架构的根本性创新。

Abstract: Large Language Models (LLMs) are increasingly applied to data-intensive
workflows, from database querying to developer observability. Yet the
effectiveness of these systems is constrained by the volume, verbosity, and
noise of real-world text-rich data such as logs, telemetry, and monitoring
streams. Feeding such data directly into LLMs is costly, environmentally
unsustainable, and often misaligned with task objectives. Parallel efforts in
LLM efficiency have focused on model- or architecture-level optimizations, but
the challenge of reducing upstream input verbosity remains underexplored. In
this paper, we argue for treating the token budget of an LLM as an attention
budget and elevating task-aware text reduction as a first-class design
principle for language -- data systems. We position input-side reduction not as
compression, but as attention allocation: prioritizing information most
relevant to downstream tasks. We outline open research challenges for building
benchmarks, designing adaptive reduction pipelines, and integrating
token-budget--aware preprocessing into database and retrieval systems. Our
vision is to channel scarce attention resources toward meaningful signals in
noisy, data-intensive workflows, enabling scalable, accurate, and sustainable
LLM--data integration.

</details>


### [12] [Lingxi: Repository-Level Issue Resolution Framework Enhanced by Procedural Knowledge Guided Scaling](https://arxiv.org/abs/2510.11838)
*Xu Yang,Jiayuan Zhou,Michael Pacheco,Wenhan Zhu,Pengfei He,Shaowei Wang,Kui Liu,Ruiqi Pan*

Main category: cs.SE

TL;DR: Lingxi is a framework that utilizes procedural knowledge from historical data to enhance LLMs in solving complex, repository-level software engineering issues.


<details>
  <summary>Details</summary>
Motivation: Despite significant advancements in LLM-powered agents for software engineering, they struggle with complex, repository-level issues due to lack of procedural knowledge and the need for excessive computational resources in brute-force exploration.

Method: Lingxi constructs procedural knowledge offline using a hierarchical abstraction mechanism, and then applies a knowledge-driven scaling method during online issue resolution to guide agents based on patterns from similar past issues.

Result: Lingxi resolves 74.6% of bugs in the SWE-bench Verified benchmark, outperforming five existing state-of-the-art methods by 5.4% to 14.9% in the Past@1 setting.

Conclusion: The framework demonstrates that using procedural knowledge significantly improves the performance of agents in complex issue resolution, with the most impactful aspect being design patterns and coding practices, and the importance of knowledge varying across different resolution stages.

Abstract: Driven by the advancements of Large Language Models (LLMs), LLM-powered
agents are making significant improvements in software engineering tasks, yet
struggle with complex, repository-level issue resolution. Existing agent-based
methods have two key limitations. First, they lack of procedural knowledge
(i.e., how an issue is fixed step-by-step and rationales behind it) to learn
and leverage for issue resolution. Second, they rely on massive computational
power to blindly explore the solution space. % To address those limitations, we
propose Lingxi, an issue resolution framework that leverages procedural
knowledge extracted from historical issue-fixing data to guide agents in
solving repository-level issues. \ourTool first constructs this knowledge
offline through a hierarchical abstraction mechanism, enabling agents to learn
the how and why behind a fix, not just the final solution. During online
application, it employs a knowledge-driven scaling method that leverages the
procedural knowledge of similar issues to intelligently analyze the target
issue from multiple perspectives, in sharp contrast to undirected, brute-force
exploration. % Lingxi successfully resolves 74.6\% of bugs on the SWE-bench
Verified benchmark in Past@1 setting, outperforming five state-of-the-art
techniques by a significant margin (5.4\% to 14.9\%). Our comprehensive
ablation study confirmed that the success of Lingxi comes directly from its use
of procedural knowledge. Without it, the performance gains from scaling alone
is negligible. Our qualitative study further shows that the ``design patterns
$\&$ coding practices'' is the most critical knowledge aspect, and that the
roles of different knowledge aspects switch across different stages (i.e.,
analysis, planning, and fixing).

</details>


### [13] [DMAS-Forge: A Framework for Transparent Deployment of AI Applications as Distributed Systems](https://arxiv.org/abs/2510.11872)
*Alessandro Cornacchia,Vaastav Anand,Muhammad Bilal,Zafar Qazi,Marco Canini*

Main category: cs.SE

TL;DR: DMAS-Forge is a framework that simplifies deploying distributed multi-agent AI applications by decoupling application logic from deployment specifics and automating glue code/configuration generation.


<details>
  <summary>Details</summary>
Motivation: Current AI agent deployment is labor-intensive and fragmented across evolving frameworks. Service-oriented architectures emulate complex agent interactions, but orchestrating these systems requires significant manual effort.

Method: DMAS-Forge abstracts deployment complexities through automated code generation, separating application logic from deployment choices. The framework supports diverse deployment scenarios with minimal manual intervention.

Result: Prototype implementation demonstrates framework's ability to generate deployment artifacts. Design principles and vision are presented alongside discussion of opportunities for future work.

Conclusion: DMAS-Forge reduces deployment overhead for multi-agent systems while maintaining flexibility across architectures. Future work includes expanding deployment targets and refining automation capabilities.

Abstract: Agentic AI applications increasingly rely on multiple agents with distinct
roles, specialized tools, and access to memory layers to solve complex tasks --
closely resembling service-oriented architectures. Yet, in the rapid evolving
landscape of programming frameworks and new protocols, deploying and testing AI
agents as distributed systems remains a daunting and labor-intensive task. We
present DMAS-Forge, a framework designed to close this gap. DMAS-Forge
decouples application logic from specific deployment choices, and aims at
transparently generating the necessary glue code and configurations to spawn
distributed multi-agent applications across diverse deployment scenarios with
minimal manual effort. We present our vision, design principles, and a
prototype of DMAS-Forge. Finally, we discuss the opportunities and future work
for our approach.

</details>


### [14] [TorchCor: High-Performance Cardiac Electrophysiology Simulations with the Finite Element Method on GPUs](https://arxiv.org/abs/2510.12011)
*Bei Zhou,Maximilian Balmus,Cesare Corrado,Ludovica Cicci,Shuang Qian,Steven A. Niederer*

Main category: cs.SE

TL;DR: TorchCor is a free, PyTorch-based GPU-accelerated Python library that makes cardiac electrophysiology simulations both faster and more widely accessible.


<details>
  <summary>Details</summary>
Motivation: Traditional cardiac electrophysiology simulations require high-core CPU systems that are often inaccessible to researchers and clinicians.

Method: TorchCor utilizes the finite element method on general-purpose GPUs built with PyTorch to accelerate simulations.

Result: TorchCor's accuracy was validated through manufactured analytical solutions and N-version benchmarks, demonstrating significant performance improvements especially for large 3D meshes.

Conclusion: TorchCor, a Python library for CEP simulations using GPUs, addresses the inaccessible computing resources issue by providing a freely available, high-performance solution.

Abstract: Cardiac electrophysiology (CEP) simulations are increasingly used for
understanding cardiac arrhythmias and guiding clinical decisions. However,
these simulations typically require high-performance computing resources with
numerous CPU cores, which are often inaccessible to many research groups and
clinicians. To address this, we present TorchCor, a high-performance Python
library for CEP simulations using the finite element method on general-purpose
GPUs. Built on PyTorch, TorchCor significantly accelerates CEP simulations,
particularly for large 3D meshes. The accuracy of the solver is verified
against manufactured analytical solutions and the $N$-version benchmark
problem. TorchCor is freely available for both academic and commercial use
without restrictions.

</details>


### [15] [Enhancing Neural Code Representation with Additional Context](https://arxiv.org/abs/2510.12082)
*Huy Nguyen,Christoph Treude,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Automated program comprehension underpins many software engineering tasks,
from code summarisation to clone detection. Recent deep learning models achieve
strong results but typically rely on source code alone, overlooking contextual
information such as version history or structural relationships. This limits
their ability to capture how code evolves and operates. We conduct an empirical
study on how enriching code representations with such contextual signals
affects neural model performance on key comprehension tasks. Two downstream
tasks, code clone detection and code summarisation, are evaluated using SeSaMe
(1,679 Java methods) and CodeSearchNet (63,259 methods). Five representative
models (CodeBERT, GraphCodeBERT, CodeT5, PLBART, ASTNN) are fine-tuned under
code-only and context-augmented settings. Results show that context generally
improves performance: version history consistently boosts clone detection
(e.g., CodeT5 +15.92% F1) and summarisation (e.g., GraphCodeBERT +5.56%
METEOR), while call-graph effects vary by model and task. Combining multiple
contexts yields further gains (up to +21.48% macro-F1). Human evaluation on 100
Java snippets confirms that context-augmented summaries are significantly
preferred for Accuracy and Content Adequacy (p <= 0.026; |delta| up to 0.55).
These findings highlight the potential of contextual signals to enhance code
comprehension and open new directions for optimising contextual encoding in
neural SE models.

</details>
