{"id": "2509.04644", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.04644", "abs": "https://arxiv.org/abs/2509.04644", "authors": ["Subhang Boorlagadda", "Nitya Naga Sai Atluri", "Muhammet Mustafa Olmez", "Edward F. Gehringer"], "title": "Comparative Evaluation of Large Language Models for Test-Skeleton Generation", "comment": "Forthcoming in Frontiers in Education (FIE 2025), Nashville,\n  Tennessee, USA, Nov 2-5, 2025", "summary": "This paper explores the use of Large Language Models (LLMs) to automate the\ngeneration of test skeletons -- structural templates that outline unit test\ncoverage without implementing full test logic. Test skeletons are especially\nimportant in test-driven development (TDD), where they provide an early\nframework for systematic verification. Traditionally authored manually, their\ncreation can be time-consuming and error-prone, particularly in educational or\nlarge-scale development settings. We evaluate four LLMs -- GPT-4,\nDeepSeek-Chat, Llama4-Maverick, and Gemma2-9B -- on their ability to generate\nRSpec skeletons for a real-world Ruby class developed in a university software\nengineering course. Each model's output is assessed using static analysis and a\nblind expert review to measure structural correctness, clarity,\nmaintainability, and conformance to testing best practices. The study reveals\nkey differences in how models interpret code structure and testing conventions,\noffering insights into the practical challenges of using LLMs for automated\ntest scaffolding. Our results show that DeepSeek generated the most\nmaintainable and well-structured skeletons, while GPT-4 produced more complete\nbut conventionally inconsistent output. The study reveals prompt design and\ncontextual input as key quality factors.", "AI": {"tldr": "Four LLMs were evaluated for automated RSpec skeleton generation. DeepSeek excelled in maintainability, GPT-4 in completeness but with inconsistency. Prompt design significantly impacts results.", "motivation": "Manually creating test skeletons is time-consuming and error-prone in educational and large-scale development settings. Automating this process through LLMs could improve efficiency and reduce errors in test-driven development (TDD) workflows.", "method": "The study evaluates four LLMs (GPT-4, DeepSeek-Chat, Llama4-Maverick, Gemma2-9B) on generating RSpec skeletons for Ruby classes using static analysis and expert review to assess structural correctness, clarity, maintainability, and testing best practices.", "result": "DeepSeek-Chat produced the most maintainable and well-structured test skeletons, while GPT-4 generated more complete but conventionally inconsistent results. Prompt design and contextual input were identified as critical factors in output quality.", "conclusion": "The study concludes that while LLMs can assist in automating test skeleton generation, their effectiveness depends on model choice and prompt design. DeepSeek achieved the best balance of maintainability and structure, but challenges in consistency persist."}}
{"id": "2509.04721", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04721", "abs": "https://arxiv.org/abs/2509.04721", "authors": ["Abhishek Dey", "Saurabh Srivastava", "Gaurav Singh", "Robert G. Pettit"], "title": "Real-Time Performance Benchmarking of TinyML Models in Embedded Systems (PICO: Performance of Inference, CPU, and Operations)", "comment": null, "summary": "This paper presents PICO-TINYML-BENCHMARK, a modular and platform-agnostic\nframework for benchmarking the real-time performance of TinyML models on\nresource-constrained embedded systems. Evaluating key metrics such as inference\nlatency, CPU utilization, memory efficiency, and prediction stability, the\nframework provides insights into computational trade-offs and platform-specific\noptimizations. We benchmark three representative TinyML models -- Gesture\nClassification, Keyword Spotting, and MobileNet V2 -- on two widely adopted\nplatforms, BeagleBone AI64 and Raspberry Pi 4, using real-world datasets.\nResults reveal critical trade-offs: the BeagleBone AI64 demonstrates consistent\ninference latency for AI-specific tasks, while the Raspberry Pi 4 excels in\nresource efficiency and cost-effectiveness. These findings offer actionable\nguidance for optimizing TinyML deployments, bridging the gap between\ntheoretical advancements and practical applications in embedded systems.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.04763", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.04763", "abs": "https://arxiv.org/abs/2509.04763", "authors": ["Tiancheng Jin", "Shangzhou Xia", "Jianjun Zhao"], "title": "NovaQ: Improving Quantum Program Testing through Diversity-Guided Test Case Generation", "comment": "5 pages", "summary": "Quantum programs are designed to run on quantum computers, leveraging quantum\ncircuits to solve problems that are intractable for classical machines. As\nquantum computing advances, ensuring the reliability of quantum programs has\nbecome increasingly important. This paper introduces NovaQ, a diversity-guided\ntesting framework for quantum programs. NovaQ combines a distribution-based\ntest case generator with a novelty-driven evaluation module. The generator\nproduces diverse quantum state inputs by mutating circuit parameters, while the\nevaluator quantifies behavioral novelty based on internal circuit state\nmetrics, including magnitude, phase, and entanglement. By selecting inputs that\nmap to infrequently covered regions in the metric space, NovaQ effectively\nexplores under-tested program behaviors. We evaluate NovaQ on quantum programs\nof varying sizes and complexities. Experimental results show that NovaQ\nconsistently achieves higher test input diversity and detects more bugs than\nexisting baseline approaches.", "AI": {"tldr": "NovaQ is a quantum program testing framework that uses distribution-based input generation and novelty-driven evaluation to achieve higher diversity and bug detection than existing methods.", "motivation": "As quantum computing advances, ensuring program reliability becomes critical to solve classically intractable problems, necessitating more effective testing approaches for quantum programs.", "method": "NovaQ employs a diversity-guided framework combining (1) a distribution-based test case generator that mutates circuit parameters to generate diverse quantum state inputs, and (2) a novelty-driven evaluation module measuring behavioral novelty through magnitude, phase, and entanglement metrics in circuit states.", "result": "Experimental results demonstrate NovaQ achieves higher test input diversity and detects more bugs than existing baseline approaches across varying quantum program sizes and complexities.", "conclusion": "NovaQ effectively explores under-tested program behaviors by targeting infrequently covered regions in metric space, establishing a superior testing framework for emerging quantum software."}}
{"id": "2509.04810", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04810", "abs": "https://arxiv.org/abs/2509.04810", "authors": ["Yogev Cohen", "Dudi Ohayon", "Romy Somkin", "Yehudit Aperstein", "Alexander Apartsin"], "title": "Code Review Without Borders: Evaluating Synthetic vs. Real Data for Review Recommendation", "comment": "4 pages, 1 figure", "summary": "Automating the decision of whether a code change requires manual review is\nvital for maintaining software quality in modern development workflows.\nHowever, the emergence of new programming languages and frameworks creates a\ncritical bottleneck: while large volumes of unlabelled code are readily\navailable, there is an insufficient amount of labelled data to train supervised\nmodels for review classification. We address this challenge by leveraging Large\nLanguage Models (LLMs) to translate code changes from well-resourced languages\ninto equivalent changes in underrepresented or emerging languages, generating\nsynthetic training data where labelled examples are scarce. We assume that\nalthough LLMs have learned the syntax and semantics of new languages from\navailable unlabelled code, they have yet to fully grasp which code changes are\nconsidered significant or review-worthy within these emerging ecosystems. To\novercome this, we use LLMs to generate synthetic change examples and train\nsupervised classifiers on them. We systematically compare the performance of\nthese classifiers against models trained on real labelled data. Our experiments\nacross multiple GitHub repositories and language pairs demonstrate that\nLLM-generated synthetic data can effectively bootstrap review recommendation\nsystems, narrowing the performance gap even in low-resource settings. This\napproach provides a scalable pathway to extend automated code review\ncapabilities to rapidly evolving technology stacks, even in the absence of\nannotated data.", "AI": {"tldr": "This paper proposes using LLMs to generate synthetic training data for automatic code review in emerging languages where labeled data is limited.", "motivation": "Automated code review is critical for maintaining software quality, but many new programming languages or frameworks lack enough labeled data to train effective supervised models.", "method": "The authors use LLMs to translate code changes from well-resourced languages into equivalent changes in underrepresented languages, effectively creating synthetic labeled examples. They then train classifiers using these synthetic data.", "result": "Experiments on multiple GitHub language pairs show that classifiers trained on LLM-generated synthetic data can achieve performance comparable to models trained on real annotated data, significantly improving review automation in low-resource settings.", "conclusion": "LLM-based synthetic data generation offers a viable and scalable solution for bootstraping automated code review systems in rapidly evolving languages and frameworks."}}
{"id": "2509.04710", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.04710", "abs": "https://arxiv.org/abs/2509.04710", "authors": ["Zhou Li", "Yu Zheng", "Tianhao Wang", "Sang-Woo Jun"], "title": "Network-Aware Differential Privacy", "comment": null, "summary": "Differential privacy (DP) is a privacy-enhancement technology (PET) that\nreceives prominent attention from the academia, industry, and government. One\nmain development over the past decade has been the decentralization of DP,\nincluding local DP and shuffle DP. Despite that decentralized DP heavily relies\non network communications for data collection,we found that: 1) no systematic\nstudy has surveyed the research opportunities at the intersection of networking\nand DP; 2) nor have there been significant efforts to develop DP mechanisms\nthat are explicitly tailored for network environments. In this paper, we seek\nto address this gap by initiating a new direction of network-aware DP. We\nidentified two focus areas where the network research can offer substantive\ncontributions to the design and deployment of DP, related to network security\nand topology. Through this work, we hope to encourage more research that\nadapt/optimize DP's deployment in various network environments.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.04877", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.04877", "abs": "https://arxiv.org/abs/2509.04877", "authors": ["Maryam Khan", "Muhammad Azeem Akbar", "Jussi Kasurinen"], "title": "Integrating Large Language Models in Software Engineering Education: A Pilot Study through GitHub Repositories Mining", "comment": null, "summary": "Context: Large Language Models (LLMs) such as ChatGPT are increasingly\nadopted in software engineering (SE) education, offering both opportunities and\nchallenges. Their adoption requires systematic investigation to ensure\nresponsible integration into curricula. Objective: This doctoral research aims\nto develop a validated framework for integrating LLMs into SE education through\na multi-phase process, including taxonomies development, empirical\ninvestigation, and case studies. This paper presents the first empirical step.\nMethod: We conducted a pilot repository mining study of 400 GitHub projects,\nanalyzing README files and issues discussions to identify the presence of\nmotivator and demotivator previously synthesized in our literature review [ 8]\nstudy. Results: Motivators such as engagement and motivation (227 hits),\nsoftware engineering process understanding (133 hits), and programming\nassistance and debugging support (97 hits) were strongly represented.\nDemotivators, including plagiarism and IP concerns (385 hits), security,\nprivacy and data integrity (87 hits), and over-reliance on AI in learning (39\nhits), also appeared prominently. In contrast, demotivators such as challenges\nin evaluating learning outcomes and difficulty in curriculum redesign recorded\nno hits across the repositories. Conclusion: The study provides early empirical\nvalidation of motivators/demotivators taxonomies with respect to their themes,\nhighlights research practice gaps, and lays the foundation for developing a\ncomprehensive framework to guide the responsible adoption of LLMs in SE\neducation.", "AI": {"tldr": "This paper presents empirical validation of LLM integration motivators/demotivators in SE education, using GitHub analysis to inform framework development.", "motivation": "Aims to develop a validated framework for responsible LLM integration in SE education by systematically investigating adoption factors through empirical analysis.", "method": "Conducted a pilot repository mining study across 400 GitHub projects, analyzing README files and issue discussions to quantify motivator/demotivator themes from prior literature.", "result": "Results show motivators like engagement (227 hits), process understanding (133 hits), and programming help (97 hits) were common. Demotivators included plagiarism/IP concerns (385 hits), security/privacy (87 hits), and over-reliance (39 hits). Certain demotivators like evaluation challenges had zero hits.", "conclusion": "The study validates motivator/demotivator taxonomies, reveals research-practice gaps, and establishes a foundation for a framework to guide responsible LLM adoption in SE education."}}
{"id": "2509.04941", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.04941", "abs": "https://arxiv.org/abs/2509.04941", "authors": ["Xiaogang Cheng", "Ren Guo", "Zuxi Chen"], "title": "Cryptographic Application of Elliptic Curve with High Rank", "comment": null, "summary": "Elliptic curve cryptography is better than traditional cryptography based on\nRSA and discrete logarithm of finite field in terms of efficiency and security.\nIn this paper, we show how to exploit elliptic curve with high rank, which has\nnot been used in cryptography before, to construct cryptographic schemes.\nConcretely we demonstrate how to construct public key signature scheme with\nhierarchy revocation based on elliptic curve with high rank, where the rank\ndetermines the height of the revocation tree. Although our construction is not\nvery efficient in some sense, our construction shows elliptic curve with high\nrank is valuable and important for cryptographic usage. The technique and\nassumption presented can surely be used for other cryptographic constructions.", "AI": {"tldr": "This paper proposes a novel cryptographic scheme using high-rank elliptic curves for hierarchical revocation, proving their cryptographic potential despite efficiency tradeoffs.", "motivation": "Elliptic curve cryptography offers superior efficiency and security over RSA and finite field discrete logarithm methods. High-rank elliptic curves (previously unexplored in cryptography) present new possibilities for cryptographic protocols.", "method": "Constructed a public-key signature scheme with hierarchical revocation using elliptic curves with high rank, where the curve's rank determines the revocation tree's height.", "result": "The construction demonstrates the feasibility and value of high-rank elliptic curves in cryptography, even if it's not the most efficient solution. The methods and assumptions can be applied to other cryptographic schemes.", "conclusion": "High-rank elliptic curves establish significant cryptographic value and the techniques open new pathways for future cryptographic constructions with wide applicability."}}
{"id": "2509.04967", "categories": ["cs.SE", "cs.CR", "D.2.5"], "pdf": "https://arxiv.org/pdf/2509.04967", "abs": "https://arxiv.org/abs/2509.04967", "authors": ["Kai Feng", "Jeremy Singer", "Angelos K Marnerides"], "title": "FuzzRDUCC: Fuzzing with Reconstructed Def-Use Chain Coverage", "comment": null, "summary": "Binary-only fuzzing often struggles with achieving thorough code coverage and\nuncovering hidden vulnerabilities due to limited insight into a program's\ninternal dataflows. Traditional grey-box fuzzers guide test case generation\nprimarily using control flow edge coverage, which can overlook bugs not easily\nexposed through control flow analysis alone. We argue that integrating dataflow\nanalysis into the fuzzing process can enhance its effectiveness by revealing\nhow data propagates through the program, thereby enabling the exploration of\nexecution paths that control flow-based methods might miss. In this context, we\nintroduce FuzzRDUCC, a novel fuzzing framework that employs symbolic execution\nto reconstruct definition-use (def-use) chains directly from binary\nexecutables. FuzzRDUCC identifies crucial dataflow paths and exposes security\nvulnerabilities without incurring excessive computational overhead, due to a\nnovel heuristic algorithm that selects relevant def-use chains without\naffecting the thoroughness of the fuzzing process. We evaluate FuzzRDUCC using\nthe binutils benchmark and demonstrate that it can identify unique crashes not\nfound by state-of-the-art fuzzers. Hence, establishing FuzzRDUCC as a feasible\nsolution for next generation vulnerability detection and discovery mechanisms.", "AI": {"tldr": "FuzzRDUCC is a novel binary fuzzing framework integrating symbolic execution for def-use chain analysis. It improves vulnerability detection by identifying critical dataflow paths with a heuristic algorithm, achieving better coverage and discovering unique crashes compared to existing fuzzers.", "motivation": "Traditional grey-box fuzzers rely on control flow edge coverage, which may miss bugs not revealed through control flow analysis alone. Limited insight into program dataflows hinders thorough binary-only fuzzing.", "method": "The framework reconstructs definition-use (def-use) chains directly from binary executables via symbolic execution and employs a heuristic algorithm to select relevant dataflow paths, reducing computational overhead while maintaining fuzzing thoroughness.", "result": "Evaluation on binutils benchmark shows FuzzRDUCC identifies unique crashes not found by state-of-the-art fuzzers, demonstrating superior vulnerability discovery capabilities in binary analysis scenarios.", "conclusion": "FuzzRDUCC establishes a feasible next-generation solution for vulnerability detection by effectively integrating dataflow analysis into binary fuzzing without excessive computational costs."}}
{"id": "2509.04999", "categories": ["cs.CR", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04999", "abs": "https://arxiv.org/abs/2509.04999", "authors": ["Sidahmed Benabderrahmane", "Talal Rahwan"], "title": "Adversarial Augmentation and Active Sampling for Robust Cyber Anomaly Detection", "comment": null, "summary": "Advanced Persistent Threats (APTs) present a considerable challenge to\ncybersecurity due to their stealthy, long-duration nature. Traditional\nsupervised learning methods typically require large amounts of labeled data,\nwhich is often scarce in real-world scenarios. This paper introduces a novel\napproach that combines AutoEncoders for anomaly detection with active learning\nto iteratively enhance APT detection. By selectively querying an oracle for\nlabels on uncertain or ambiguous samples, our method reduces labeling costs\nwhile improving detection accuracy, enabling the model to effectively learn\nwith minimal data and reduce reliance on extensive manual labeling. We present\na comprehensive formulation of the Attention Adversarial Dual AutoEncoder-based\nanomaly detection framework and demonstrate how the active learning loop\nprogressively enhances the model's performance. The framework is evaluated on\nreal-world, imbalanced provenance trace data from the DARPA Transparent\nComputing program, where APT-like attacks account for just 0.004\\% of the data.\nThe datasets, which cover multiple operating systems including Android, Linux,\nBSD, and Windows, are tested in two attack scenarios. The results show\nsubstantial improvements in detection rates during active learning,\noutperforming existing methods.", "AI": {"tldr": "Combining AutoEncoders and active learning, this paper introduces a cost-effective APT detection framework that enhances accuracy with minimal labeled data, validated on real-world imbalanced datasets.", "motivation": "Traditional supervised learning methods require abundant labeled data, which is scarce in real-world cybersecurity scenarios. Advanced Persistent Threats (APTs) are stealthy and long-term, necessitating efficient detection mechanisms that work with minimal labeled data.", "method": "The paper proposes an Attention Adversarial Dual AutoEncoder-based anomaly detection framework combined with active learning. The method uses AutoEncoders for initial anomaly detection and iteratively improves performance by querying an oracle for labels on uncertain samples, reducing labeling costs and dependency on large labeled datasets.", "result": "Evaluation on DARPA Transparent Computing data (0.004% APT-like attacks) across Android, Linux, BSD, and Windows shows improved detection rates during active learning. The method outperforms existing approaches in two attack scenarios, highlighting its effectiveness in imbalanced data environments.", "conclusion": "The framework demonstrates substantial improvements in APT detection rates on real-world imbalanced datasets, outperforming existing methods while minimizing manual labeling efforts. This approach addresses the challenges of data scarcity and high labeling costs in APT detection."}}
{"id": "2509.05112", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05112", "abs": "https://arxiv.org/abs/2509.05112", "authors": ["Denesa Zyberaj", "Lukasz Mazur", "Nenad Petrovic", "Pankhuri Verma", "Pascal Hirmer", "Dirk Slama", "Xiangwei Cheng", "Alois Knoll"], "title": "GenAI-based test case generation and execution in SDV platform", "comment": null, "summary": "This paper introduces a GenAI-driven approach for automated test case\ngeneration, leveraging Large Language Models and Vision-Language Models to\ntranslate natural language requirements and system diagrams into structured\nGherkin test cases. The methodology integrates Vehicle Signal Specification\nmodeling to standardize vehicle signal definitions, improve compatibility\nacross automotive subsystems, and streamline integration with third-party\ntesting tools. Generated test cases are executed within the digital.auto\nplayground, an open and vendor-neutral environment designed to facilitate rapid\nvalidation of software-defined vehicle functionalities. We evaluate our\napproach using the Child Presence Detection System use case, demonstrating\nsubstantial reductions in manual test specification effort and rapid execution\nof generated tests. Despite significant automation, the generation of test\ncases and test scripts still requires manual intervention due to current\nlimitations in the GenAI pipeline and constraints of the digital.auto platform.", "AI": {"tldr": "This paper presents a GenAI-based framework for automating Gherkin test case generation in automotive systems, combining LLMs, Vision-Language Models, and digital.auto for validation while highlighting remaining manual dependencies.", "motivation": "Address manual test specification effort and integration challenges in software-defined vehicles by leveraging GenAI for automation and standardization.", "method": "Integration of Large Language Models and Vision-Language Models to convert requirements/diagrams into Gherkin test cases, combined with Vehicle Signal Specification modeling for standardization and compatibility.", "result": "Demonstration of reduced manual effort and rapid test execution via Child Presence Detection System use case, with generated tests validated in digital.auto playground.", "conclusion": "The proposed GenAI-driven approach significantly automates test case generation for automotive systems but still requires manual intervention due to current pipeline and platform limitations."}}
{"id": "2509.05104", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.05104", "abs": "https://arxiv.org/abs/2509.05104", "authors": ["Richard Derbyshire", "Diana Selck-Paulsson", "Charl van der Walt", "Joe Burton"], "title": "From Protest to Power Plant: Interpreting the Role of Escalatory Hacktivism in Cyber Conflict", "comment": null, "summary": "Since 2022, hacktivist groups have escalated their tactics, expanding from\ndistributed denial-of-service attacks and document leaks to include targeting\noperational technology (OT). By 2024, attacks on the OT of critical national\ninfrastructure (CNI) had been linked to partisan hacktivist efforts in ongoing\ngeopolitical conflicts, demonstrating a shift from protest to something more\nresembling cyber warfare. This escalation raises critical questions about the\nclassification of these groups and the appropriate state response to their\ngrowing role in destabilizing international security.\n  This paper examines the strategic motivations behind escalatory hacktivism,\nhighlighting how states may tolerate, encourage, or leverage hacktivist groups\nas proxies in conflicts that blur the lines between activism, cybercrime, and\nstate-sponsored operations. We introduce a novel method for interpreting\nhacktivists based on the impact of their actions, alignment to state ideology,\nand host state involvement, offering a structured approach to understanding the\nphenomenon. Finally, we assess policy and security implications, particularly\nfor host and victim states, and propose strategies to address this evolving\nthreat. By doing so, this paper contributes to international discussions on\ncyber security policy, governance, and the increasing intersection between\nnon-state cyber actors and state interests.", "AI": {"tldr": "This paper explores the shifting landscape of hacktivism in the context of state-sponsored cyber warfare and its implications on international security policy.", "motivation": "Hacktivist groups are increasingly targeting critical infrastructure in an manner that resembles cyber warfare, thereby affecting international security.", "method": "The authors present a new method to interpret the roles of hacktivists by detecting their alignment with state ideology, measuring the impact of their actions, and observing the host state's involvement.", "result": "The research provides insights into the motivations of hacktivists and their strategic use by states as proxies in cyber conflicts, leading to new frameworks for understanding their actions.", "conclusion": "The paper concludes with an evaluation of policy and security implications, offering strategies to manage hacktivist threats and contributing to international cyber security governance discussions."}}
{"id": "2509.05197", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.05197", "abs": "https://arxiv.org/abs/2509.05197", "authors": ["Naimeng Ye", "Xiao Yu", "Ruize Xu", "Tianyi Peng", "Zhou Yu"], "title": "AI Agents for Web Testing: A Case Study in the Wild", "comment": null, "summary": "Automated web testing plays a critical role in ensuring high-quality user\nexperiences and delivering business value. Traditional approaches primarily\nfocus on code coverage and load testing, but often fall short of capturing\ncomplex user behaviors, leaving many usability issues undetected. The emergence\nof large language models (LLM) and AI agents opens new possibilities for web\ntesting by enabling human-like interaction with websites and a general\nawareness of common usability problems. In this work, we present WebProber, a\nprototype AI agent-based web testing framework. Given a URL, WebProber\nautonomously explores the website, simulating real user interactions,\nidentifying bugs and usability issues, and producing a human-readable report.\nWe evaluate WebProber through a case study of 120 academic personal websites,\nwhere it uncovered 29 usability issues--many of which were missed by\ntraditional tools. Our findings highlight agent-based testing as a promising\ndirection while outlining directions for developing next-generation,\nuser-centered testing frameworks.", "AI": {"tldr": "WebProber: an AI agent-based framework that identifies complex usability issues in web testing by simulating human-like interactions, outperforming traditional tools in a case study of 120 academic websites.", "motivation": "Traditional web testing focuses on code coverage and load testing, missing nuanced user behavior and common usability problems. LLMs and AI agents enable more human-like interaction, addressing this gap.", "method": "The authors developed WebProber, an AI agent-based framework that autonomously explores websites, simulates real user interactions, detects bugs/usability issues, and generates human-readable reports. They evaluated it on 120 academic personal websites.", "result": "WebProber uncovered 29 usability issues (e.g., accessibility barriers, navigation problems) on 120 academic websites, many missed by existing tools. The results demonstrate the effectiveness of agent-based testing in identifying user-centric issues.", "conclusion": "The paper concludes that agent-based testing, exemplified by WebProber, offers a promising approach to identifying usability issues overlooked by traditional tools. It advocates for further development of user-centered testing frameworks to improve web quality."}}
{"id": "2509.05149", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05149", "abs": "https://arxiv.org/abs/2509.05149", "authors": ["Huy Hung Ho", "Nhan Le Thanh"], "title": "Odoo-based Subcontract Inter-site Access Control Mechanism for Construction Projects", "comment": "7 pages, 2 figures, The 9th OISP Science and Technology Symposium", "summary": "In the era of Construction 4.0, the industry is embracing a new paradigm of\nlabor elasticity, driven by smart and flexible outsourcing and subcontracting\nstrategies. The increased reliance on specialized subcontractors enables\ncompanies to scale labor dynamically based on project demands. This adaptable\nworkforce model presents challenges in managing hierarchical integration and\ncoordinating inter-site collaboration. Our design introduces a subsystem\nintegrated into the Odoo ERP framework, employing a modular architecture to\nstreamline labor management, task tracking, and approval workflows. The system\nadopts a three-pronged approach to ensure synchronized data exchange between\ngeneral contractors and subcontractors, while maintaining both security and\noperational independence. The system features hybrid access control,\nthird-party integration for cross-domain communication, and role-based mapping\nalgorithm across sites. The system supports varying degrees of customization\nthrough a unified and consolidated attribute mapping center. This center\nleverages a tree-like index structure and Lagrange interpolation method to\nenhance the efficiency of role mapping. Demonstrations highlight practical\napplication in outsourcing, integration, and scalability scenarios, confirming\nthe system's robustness under high user volumes and in offline conditions.\nExperimental results further show improvements in database performance and\nworkflow adaptability to support a scalable, enterprise-level solution that\naligns with the evolving demands of smart construction management.", "AI": {"tldr": "The paper proposes a modular Odoo ERP subsystem to manage labor elasticity in Construction 4.0, using hybrid access control, third-party integration, and role-based mapping algorithms for scalable subcontractor coordination.", "motivation": "Construction 4.0\u2019s reliance on specialized subcontractors creates challenges in hierarchical integration and inter-site collaboration, requiring a system to manage dynamic labor scaling while maintaining security and operational independence.", "method": "The system uses a modular architecture with three-pronged synchronization, hybrid access control, cross-domain third-party integration, and a role-based mapping algorithm leveraging a tree-like index and Lagrange interpolation for efficient role mapping.", "result": "Experimental results confirm scalability, performance improvements in database operations, and successful practical applications in outsourcing, integration, and scalability scenarios.", "conclusion": "The system demonstrates robustness in high-user-volume scenarios and offline conditions, aligning with evolving smart construction demands through improved database performance and workflow adaptability."}}
{"id": "2509.05150", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05150", "abs": "https://arxiv.org/abs/2509.05150", "authors": ["Stefanos Vasileaidis", "Thanassis Giannetsos", "Matthias Schunter", "Bruno Crispo"], "title": "Reinforcing Secure Live Migration through Verifiable State Management", "comment": null, "summary": "Live migration of applications is a fundamental capability for enabling\nresilient computing in modern distributed systems. However, extending this\nfunctionality to trusted applications (TA) -- executing within Trusted\nExecution Environments (TEEs) -- introduces unique challenges such as secure\nstate preservation, integrity verification, replay and rollback prevention, and\nmitigation of unauthorized cloning of TAs. We present TALOS, a lightweight\nframework for verifiable state management and trustworthy application\nmigration. While our implementation is prototyped and evaluated using Intel SGX\nwith the Gramine LibOS and RISC-V Keystone (evidencing the framework's\nportability across diverse TEEs), its design is agnostic to the underlying TEE\narchitecture. Such agility is a necessity in today's network service mesh\n(collaborative computing across the continuum) where application workloads must\nbe managed across domain boundaries in a harmonized fashion. TALOS is built\naround the principle of minimizing trust assumptions: TAs are treated as\nuntrusted until explicitly verified, and the migration process does not rely on\na trusted third party. To ensure both the integrity and secure launch of the\nmigrated application, TALOS integrates memory introspection and control-flow\ngraph extraction, enabling robust verification of state continuity and\nexecution flow. Thereby achieving strong security guarantees while maintaining\nefficiency, making it suitable for decentralized settings.", "AI": {"tldr": "TALOS is a portable, TEE-agnostic framework enabling secure live migration of trusted applications by enforcing integrity verification and minimizing trust assumptions through memory introspection and control-flow analysis.", "motivation": "Live migration of trusted applications in TEEs faces critical security risks like untrusted state preservation, cloning, and integrity breaches, requiring a framework that balances portability, zero-trust verification, and execution consistency.", "method": "Leverages memory introspection and control-flow graph extraction for TA verification, employs cryptographically signed snapshots for replay/rollback prevention, and enforces secure TEE bootstrapping without relying on trusted third parties.", "result": "Achieved secure, verifiable migration across diverse TEEs (SGX/Keystone) with robust integrity checks and minimal performance overhead, validated through prototyping and cross-architecture evaluation.", "conclusion": "TALOS successfully addresses challenges in TA migration across domain boundaries, offering efficient and portable security-critical guarantees for decentralized computing environments."}}
{"id": "2509.05161", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05161", "abs": "https://arxiv.org/abs/2509.05161", "authors": ["Abiodun Ganiyu", "Dara Ron", "Syed Rafiul Hussain", "Vijay K Shah"], "title": "Jamming Smarter, Not Harder: Exploiting O-RAN Y1 RAN Analytics for Efficient Interference", "comment": "8 pages, 7 figures", "summary": "The Y1 interface in O-RAN enables the sharing of RAN Analytics Information\n(RAI) between the near-RT RIC and authorized Y1 consumers, which may be\ninternal applications within the operator's trusted domain or external systems\naccessing data through a secure exposure function. While this visibility\nenhances network optimization and enables advanced services, it also introduces\na potential security risk -- a malicious or compromised Y1 consumer could\nmisuse analytics to facilitate targeted interference. In this work, we\ndemonstrate how an adversary can exploit the Y1 interface to launch selective\njamming attacks by passively monitoring downlink metrics. We propose and\nevaluate two Y1-aided jamming strategies: a clustering-based jammer leveraging\nDBSCAN for traffic profiling and a threshold-based jammer. These are compared\nagainst two baselines strategies -- always-on jammer and random jammer -- on an\nover-the-air LTE/5G O-RAN testbed. Experimental results show that in\nunconstrained jamming budget scenarios, the threshold-based jammer can closely\nreplicate the disruption caused by always-on jamming while reducing\ntransmission time by 27\\%. Under constrained jamming budgets, the\nclustering-based jammer proves most effective, causing up to an 18.1\\% bitrate\ndrop while remaining active only 25\\% of the time. These findings reveal a\ncritical trade-off between jamming stealthiness and efficiency, and illustrate\nhow exposure of RAN analytics via the Y1 interface can enable highly targeted,\nlow-overhead attacks, raising important security considerations for both\ncivilian and mission-critical O-RAN deployments.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.05162", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05162", "abs": "https://arxiv.org/abs/2509.05162", "authors": ["Simone Bottoni", "Giulio Zizzo", "Stefano Braghin", "Alberto Trombetta"], "title": "Verifiability and Privacy in Federated Learning through Context-Hiding Multi-Key Homomorphic Authenticators", "comment": null, "summary": "Federated Learning has rapidly expanded from its original inception to now\nhave a large body of research, several frameworks, and sold in a variety of\ncommercial offerings. Thus, its security and robustness is of significant\nimportance. There are many algorithms that provide robustness in the case of\nmalicious clients. However, the aggregator itself may behave maliciously, for\nexample, by biasing the model or tampering with the weights to weaken the\nmodels privacy. In this work, we introduce a verifiable federated learning\nprotocol that enables clients to verify the correctness of the aggregators\ncomputation without compromising the confidentiality of their updates. Our\nprotocol uses a standard secure aggregation technique to protect individual\nmodel updates with a linearly homomorphic authenticator scheme that enables\nefficient, privacy-preserving verification of the aggregated result. Our\nconstruction ensures that clients can detect manipulation by the aggregator\nwhile maintaining low computational overhead. We demonstrate that our approach\nscales to large models, enabling verification over large neural networks with\nmillions of parameters.", "AI": {"tldr": "This paper introduces a verifiable federated learning protocol that allows clients to verify the aggregator's computations without compromising the privacy of their model updates, using secure aggregation and homomorphic authenticators.", "motivation": "Existing federated learning security solutions focus on malicious clients but neglect attacks by the aggregator (e.g., model biasing or privacy leakage). The work addresses this gap by ensuring clients can detect aggregator manipulations.", "method": "The method combines secure aggregation techniques with a linearly homomorphic authenticator scheme, enabling clients to verify the correctness of the global model update without learning individual contributions.", "result": "The authors demonstrate that their protocol scales to large neural networks with millions of parameters, achieving efficient verification with minimal computational overhead, and confirm its practicality for real-world deployments.", "conclusion": "The paper concludes that their approach effectively enables efficient, privacy-preserving verification of aggregation in federated learning, even for large-scale models, while maintaining low computational overhead."}}
{"id": "2509.05192", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05192", "abs": "https://arxiv.org/abs/2509.05192", "authors": ["Simon Lachnit", "Ghassan Karame"], "title": "On Hyperparameters and Backdoor-Resistance in Horizontal Federated Learning", "comment": "To appear in the Proceedings of the ACM Conference on Computer and\n  Communications Security (CCS) 2025", "summary": "Horizontal Federated Learning (HFL) is particularly vulnerable to backdoor\nattacks as adversaries can easily manipulate both the training data and\nprocesses to execute sophisticated attacks. In this work, we study the impact\nof training hyperparameters on the effectiveness of backdoor attacks and\ndefenses in HFL. More specifically, we show both analytically and by means of\nmeasurements that the choice of hyperparameters by benign clients does not only\ninfluence model accuracy but also significantly impacts backdoor attack\nsuccess. This stands in sharp contrast with the multitude of contributions in\nthe area of HFL security, which often rely on custom ad-hoc hyperparameter\nchoices for benign clients$\\unicode{x2013}$leading to more pronounced backdoor\nattack strength and diminished impact of defenses. Our results indicate that\nproperly tuning benign clients' hyperparameters$\\unicode{x2013}$such as\nlearning rate, batch size, and number of local epochs$\\unicode{x2013}$can\nsignificantly curb the effectiveness of backdoor attacks, regardless of the\nmalicious clients' settings. We support this claim with an extensive robustness\nevaluation of state-of-the-art attack-defense combinations, showing that\ncarefully chosen hyperparameters yield across-the-board improvements in\nrobustness without sacrificing main task accuracy. For example, we show that\nthe 50%-lifespan of the strong A3FL attack can be reduced by 98.6%,\nrespectively$\\unicode{x2013}$all without using any defense and while incurring\nonly a 2.9 percentage points drop in clean task accuracy.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.05265", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05265", "abs": "https://arxiv.org/abs/2509.05265", "authors": ["Zijian Wang", "Wei Tong", "Tingxuan Han", "Haoyu Chen", "Tianling Zhang", "Yunlong Mao", "Sheng Zhong"], "title": "On Evaluating the Poisoning Robustness of Federated Learning under Local Differential Privacy", "comment": null, "summary": "Federated learning (FL) combined with local differential privacy (LDP)\nenables privacy-preserving model training across decentralized data sources.\nHowever, the decentralized data-management paradigm leaves LDPFL vulnerable to\nparticipants with malicious intent. The robustness of LDPFL protocols,\nparticularly against model poisoning attacks (MPA), where adversaries inject\nmalicious updates to disrupt global model convergence, remains insufficiently\nstudied. In this paper, we propose a novel and extensible model poisoning\nattack framework tailored for LDPFL settings. Our approach is driven by the\nobjective of maximizing the global training loss while adhering to local\nprivacy constraints. To counter robust aggregation mechanisms such as\nMulti-Krum and trimmed mean, we develop adaptive attacks that embed carefully\ncrafted constraints into a reverse training process, enabling evasion of these\ndefenses. We evaluate our framework across three representative LDPFL\nprotocols, three benchmark datasets, and two types of deep neural networks.\nAdditionally, we investigate the influence of data heterogeneity and privacy\nbudgets on attack effectiveness. Experimental results demonstrate that our\nadaptive attacks can significantly degrade the performance of the global model,\nrevealing critical vulnerabilities and highlighting the need for more robust\nLDPFL defense strategies against MPA. Our code is available at\nhttps://github.com/ZiJW/LDPFL-Attack", "AI": {"tldr": "This paper proposes the first attack tailored for FL with LDP by formulating MPA as a constrained optimization problem, showing that it compromises global model convergence.", "motivation": "The decentralized nature of FL with LDP makes it susceptible to malicious participants. The robustness of LDPFL protocols against MPAs has not been sufficiently explored.", "method": "The authors formulated the model poisoning attack as an optimization problem with global training loss maximization as the objective. They then embedded additional constraints using shadow clients to evade defenses and employed a reverse training process for implementation.", "result": "Experiments on three LDPFL protocols, three benchmark datasets, and two neural network types demonstrate that the adaptive attacks effectively degrade global model performance, highlighting critical vulnerabilities of existing protocols.", "conclusion": "Current LDPFL protocols are vulnerable to adaptive MPAs. The paper introduces a new attack framework and emphasizes the need for more robust defense mechanisms in LDP-based federated learning."}}
