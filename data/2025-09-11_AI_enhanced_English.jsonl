{"id": "2509.08090", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08090", "abs": "https://arxiv.org/abs/2509.08090", "authors": ["Eman Abdullah AlOmar", "Luo Xu", "Sofia Martinez", "Anthony Peruma", "Mohamed Wiem Mkaouer", "Christian D. Newman", "Ali Ouni"], "title": "ChatGPT for Code Refactoring: Analyzing Topics, Interaction, and Effective Prompts", "comment": null, "summary": "Large Language Models (LLMs), such as ChatGPT, have become widely popular and\nwidely used in various software engineering tasks such as refactoring, testing,\ncode review, and program comprehension. Although recent studies have examined\nthe effectiveness of LLMs in recommending and suggesting refactoring, there is\na limited understanding of how developers express their refactoring needs when\ninteracting with ChatGPT. In this paper, our goal is to explore interactions\nrelated to refactoring between developers and ChatGPT to better understand how\ndevelopers identify areas for improvement in code, and how ChatGPT addresses\ndevelopers' needs. Our approach involves text mining 715 refactoring-related\ninteractions from 29,778 ChatGPT prompts and responses, as well as the analysis\nof developers' explicit refactoring intentions.", "AI": {"tldr": "This paper examines developer-chats with ChatGPT for refactoring, analyzing 715 interactions to uncover how needs are expressed and met, enhancing LLMs' role in code improvement.", "motivation": "Developers' interaction patterns with ChatGPT for refactoring remain underexplored, despite LLMs being widely used in software engineering tasks like refactoring. This study aims to bridge this gap.", "method": "The researchers conducted text mining of 715 refactoring-related interactions from 29,778 ChatGPT prompts/responses, analyzing developers' explicit refactoring intentions and how ChatGPT addresses them.", "result": "The analysis reveals key patterns in how developers articulate refactoring needs and how ChatGPT interprets and satisfies these requests, offering practical implications for refining LLM support in code improvement tasks.", "conclusion": "The study highlights the importance of understanding how developers and ChatGPT interact during refactoring tasks, providing actionable insights for improving LLM-based tools in software engineering."}}
{"id": "2509.08285", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08285", "abs": "https://arxiv.org/abs/2509.08285", "authors": ["Carmen C\u00e2rlan", "Daniel Ratiu", "Michael Wagner"], "title": "Safety Factories -- a Manifesto", "comment": "Presented at The 44th International Conference on Computer Safety,\n  Reliability and Security (SafeComp 2025)", "summary": "Modern cyber-physical systems are operated by complex software that\nincreasingly takes over safety-critical functions. Software enables rapid\niterations and continuous delivery of new functionality that meets the\never-changing expectations of users. As high-speed development requires\ndiscipline, rigor, and automation, software factories are used. These entail\nmethods and tools used for software development, such as build systems and\npipelines. To keep up with the rapid evolution of software, we need to bridge\nthe disconnect in methods and tools between software development and safety\nengineering today. We need to invest more in formality upfront - capturing\nsafety work products in semantically rich models that are machine-processable,\ndefining automatic consistency checks, and automating the generation of\ndocumentation - to benefit later. Transferring best practices from software to\nsafety engineering is worth exploring. We advocate for safety factories, which\nintegrate safety tooling and methods into software development pipelines.", "AI": {"tldr": "Bridging software-safety gap via automated, formal safety engineering (\"safety factories\").", "motivation": "Modern cyber-physical systems require rapid software iterations for competitive advantage, but existing methods create a disconnect between software development and safety engineering, necessitating disciplined, automated solutions for safety-critical systems.", "method": "The paper proposes capturing safety work products in semantically rich, machine-processable models, implementing automatic consistency checks, automating documentation generation, and adopting best practices from software development for safety engineering.", "result": "The proposed 'safety factories' framework integrates safety tooling with software pipelines, enabling automated safety analysis and documentation generation during development.", "conclusion": "The paper advocates for integrating safety engineering tools and methods into software development pipelines to address the gap between rapid development and safety-critical requirements, emphasizing the need for formal models and automation."}}
{"id": "2509.08389", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.08389", "abs": "https://arxiv.org/abs/2509.08389", "authors": ["Marco Torchiano", "Riccardo Coppola", "Antonio Vetro'", "Xhoi Musaj"], "title": "The Impact of Team Diversity in Agile Development Education", "comment": "Post-print of paper published at FSE Companion '25: Proceedings of\n  the 33rd ACM International Conference on the Foundations of Software\n  Engineering", "summary": "Software Engineering is mostly a male-dominated sector, where gender\ndiversity is a key feature for improving equality of opportunities,\nproductivity, and innovation. Other diversity aspects, including but not\nlimited to nationality and ethnicity, are often understudied.In this work we\naim to assess the impact of team diversity, focusing mainly on gender and\nnationality, in the context of an agile software development project-based\ncourse. We analyzed 51 teams over three academic years, measuring three\ndifferent Diversity indexes - regarding Gender, Nationality and their\nco-presence - to examine how different aspects of diversity impact the quality\nof team project outcomes.Statistical analysis revealed a moderate,\nstatistically significant correlation between gender diversity and project\nsuccess, aligning with existing literature. Diversity in nationality showed a\nnegative but negligible effect on project results, indicating that promoting\nthese aspects does not harm students' performance. Analyzing their co-presence\nwithin a team, gender and nationality combined had a negative impact, likely\ndue to increased communication barriers and differing cultural norms.This study\nunderscores the importance of considering multiple diversity dimensions and\ntheir interactions in educational settings. Our findings, overall, show that\npromoting diversity in teams does not negatively impact their performance and\nachievement of educational goals.", "AI": {"tldr": "Gender diversity improves project success in software engineering education, nationality diversity has minimal negative impact, and teams with both gender and nationality diversity may face slight performance challenges due to communication barriers\u2014promoting diversity remains educationally beneficial.", "motivation": "This work addresses the understudied impact of nationality diversity and its interaction with gender diversity in software engineering education, aiming to clarify whether promoting diversity affects educational outcomes and project quality in agile teams.", "method": "The study analyzed 51 teams across three academic years in an agile software development course. Three diversity indexes (gender, nationality, and their co-presence) were calculated for each team and correlated with project outcomes using statistical analysis.", "result": "Gender diversity showed a moderate positive correlation with project success, nationality diversity had a negligible negative effect, and combined diversity (gender+nationality) had a slight negative impact likely due to communication barriers and cultural differences.", "conclusion": "Promoting diversity in software engineering education, particularly in gender and nationality, does not negatively impact team performance and is important for achieving educational goals. However, combined diversity (gender and nationality) may introduce communication barriers and cultural differences that hinder project success."}}
{"id": "2509.08524", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08524", "abs": "https://arxiv.org/abs/2509.08524", "authors": ["Felix M\u00e4chtle", "Nils Loose", "Jan-Niclas Serr", "Jonas Sander", "Thomas Eisenbarth"], "title": "AutoStub: Genetic Programming-Based Stub Creation for Symbolic Execution", "comment": "2025 HUMIES finalist", "summary": "Symbolic execution is a powerful technique for software testing, but suffers\nfrom limitations when encountering external functions, such as native methods\nor third-party libraries. Existing solutions often require additional context,\nexpensive SMT solvers, or manual intervention to approximate these functions\nthrough symbolic stubs. In this work, we propose a novel approach to\nautomatically generate symbolic stubs for external functions during symbolic\nexecution that leverages Genetic Programming. When the symbolic executor\nencounters an external function, AutoStub generates training data by executing\nthe function on randomly generated inputs and collecting the outputs. Genetic\nProgramming then derives expressions that approximate the behavior of the\nfunction, serving as symbolic stubs. These automatically generated stubs allow\nthe symbolic executor to continue the analysis without manual intervention,\nenabling the exploration of program paths that were previously intractable. We\ndemonstrate that AutoStub can automatically approximate external functions with\nover 90% accuracy for 55% of the functions evaluated, and can infer\nlanguage-specific behaviors that reveal edge cases crucial for software\ntesting.", "AI": {"tldr": "This paper proposes a GP-based method to generate symbolic stubs for external functions during symbolic execution without manual intervention, achieving high accuracy in function approximation.", "motivation": "Symbolic execution is limited by the need for context, expensive solvers, or manual intervention when dealing with external functions like native methods or third-party libraries.", "method": "AutoStub automatically generates symbolic stubs by executing external functions on random inputs, collecting outputs, and using Genetic Programming to derive expressions that approximate their behavior.", "result": "The method achieved over 90% accuracy for 55% of evaluated functions and successfully inferred language-specific behaviors to reveal edge cases important for testing.", "conclusion": "AutoStub provides an effective, automated solution to approximate external functions during symbolic execution, improving test coverage and reducing manual effort."}}
{"id": "2509.08083", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08083", "abs": "https://arxiv.org/abs/2509.08083", "authors": ["Laurie Williams", "Sammy Migues"], "title": "Establishing a Baseline of Software Supply Chain Security Task Adoption by Software Organizations", "comment": null, "summary": "Software supply chain attacks have increased exponentially since 2020. The\nprimary attack vectors for supply chain attacks are through: (1) software\ncomponents; (2) the build infrastructure; and (3) humans (a.k.a software\npractitioners). Software supply chain risk management frameworks provide a list\nof tasks that an organization can adopt to reduce software supply chain risk.\nExhaustively adopting all the tasks of these frameworks is infeasible,\nnecessitating the prioritized adoption of tasks. Software organizations can\nbenefit from being guided in this prioritization by learning what tasks other\nteams have adopted. The goal of this study is to aid software development\norganizations in understanding the adoption of security tasks that reduce\nsoftware supply chain risk through an interview study of software practitioners\nengaged in software supply chain risk management efforts. An interview study\nwas conducted with 61 practitioners at nine software development organizations\nthat have focused efforts on reducing software supply chain risk. The results\nof the interviews indicate that organizations had implemented the most adopted\nsoftware tasks before the focus on software supply chain security. Therefore,\ntheir implementation in organizations is more mature. The tasks that mitigate\nthe novel attack vectors through software components and the build\ninfrastructure are in the early stages of adoption. Adoption of these tasks\nshould be prioritized.", "AI": {"tldr": "The study identifies that prioritizing adoption of security tasks targeting novel software supply chain attack vectors (components and build infrastructure) is critical as these are less implemented despite existing frameworks having more mature but already widely adopted tasks.", "motivation": "The exponential increase in software supply chain attacks since 2020 and the infeasibility of exhaustively adopting all risk management framework tasks necessitate prioritization guidance for organizations.", "method": "An interview study with 61 software practitioners across nine organizations focused on their implementation of software supply chain risk management tasks.", "result": "Organizations predominantly implemented tasks that were already adopted beforehand (e.g., human-focused tasks), while tasks addressing newer attack vectors through software components and build infrastructure remain in early adoption stages.", "conclusion": "Software organizations should prioritize adopting security tasks that mitigate novel software-based attack vectors in supply chains to address immature implementation areas."}}
{"id": "2509.08546", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08546", "abs": "https://arxiv.org/abs/2509.08546", "authors": ["Yu Zhu", "Jiyuan Ye"], "title": "Beyond the Binary: The System of All-round Evaluation of Research and Its Practices in China", "comment": "STI-ENID 2025 Conference Paer", "summary": "The lack of a macro-level, systematic evaluation theory to guide the\nimplementation of evaluation practices has become a key bottleneck in the\nreform of global research evaluation systems. By reviewing the historical\ndevelopment of research evaluation, this paper highlights the current binary\nopposition between qualitative and quantitative methods in evaluation\npractices. This paper introduces the System of All-round Evaluation of Research\n(SAER), a framework that integrates form, content, and utility evaluations with\nsix key elements. SAER offers a theoretical breakthrough by transcending the\nbinary, providing a comprehensive foundation for global evaluation reforms. The\ncomprehensive system proposes a trinity of three evaluation dimensions,\ncombined with six evaluation elements, which would help academic evaluators and\nresearchers reconcile binary oppositions in evaluation methods. The system\nhighlights the dialectical wisdom and experience embedded in Chinese research\nevaluation theory, offering valuable insights and references for the reform and\nadvancement of global research evaluation systems.", "AI": {"tldr": "The paper addresses the lack of a systematic evaluation theory in global research reform by introducing SAER, a framework integrating three evaluation dimensions and six elements. It bridges qualitative-quantitative divides, rooted in Chinese dialectical theory, to advance global evaluation practices.", "motivation": "The study responds to stagnant research evaluation reforms due to fragmented binary approaches (qualitative vs. quantitative methods) and the absence of a unifying macro-level theory to guide systematic implementation.", "method": "The authors review research evaluation history to reveal binary oppositions, then propose the SAER system\u2014combining form, content, and utility evaluations through six elements\u2014to transcending methodological divides via a trinity of evaluation dimensions.", "result": "SAER provides a theoretical breakthrough by harmonizing evaluation dimensions and elements, offering a practical foundation for academic evaluators to resolve methodological contradictions and advance holistic research assessment.", "conclusion": "By embedding Chinese dialectical principles, SAER demonstrates how to transcend qualitative-quantitative binaries, delivering a globally relevant framework for research evaluation reform while preserving cultural epistemological contributions."}}
{"id": "2509.08091", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08091", "abs": "https://arxiv.org/abs/2509.08091", "authors": ["Jing Chen", "Onat Gungor", "Zhengli Shang", "Tajana Rosing"], "title": "SAGE: Sample-Aware Guarding Engine for Robust Intrusion Detection Against Adversarial Attacks", "comment": "Under review at IEEE TIFS", "summary": "The rapid proliferation of the Internet of Things (IoT) continues to expose\ncritical security vulnerabilities, necessitating the development of efficient\nand robust intrusion detection systems (IDS). Machine learning-based intrusion\ndetection systems (ML-IDS) have significantly improved threat detection\ncapabilities; however, they remain highly susceptible to adversarial attacks.\nWhile numerous defense mechanisms have been proposed to enhance ML-IDS\nresilience, a systematic approach for selecting the most effective defense\nagainst a specific adversarial attack remains absent. To address this\nchallenge, we previously proposed DYNAMITE, a dynamic defense selection\napproach that identifies the most suitable defense against adversarial attacks\nthrough an ML-driven selection mechanism. Building on this foundation, we\npropose SAGE (Sample-Aware Guarding Engine), a substantially improved defense\nalgorithm that integrates active learning with targeted data reduction. It\nemploys an active learning mechanism to selectively identify the most\ninformative input samples and their corresponding optimal defense labels, which\nare then used to train a second-level learner responsible for selecting the\nmost effective defense. This targeted sampling improves computational\nefficiency, exposes the model to diverse adversarial strategies during\ntraining, and enhances robustness, stability, and generalizability. As a\nresult, SAGE demonstrates strong predictive performance across multiple\nintrusion detection datasets, achieving an average F1-score improvement of 201%\nover the state-of-the-art defenses. Notably, SAGE narrows the performance gap\nto the Oracle to just 3.8%, while reducing computational overhead by up to 29x.", "AI": {"tldr": "SAGE, a sample-aware defense engine for IoT intrusion detection, uses active learning and data reduction to achieve robust adversarial resilience. It outperforms existing methods by 201% in F1-score, minimizes the Oracle gap, and reduces computational costs by 29\u00d7.", "motivation": "Current ML-based intrusion detection systems (ML-IDS) struggle with adversarial attacks due to the absence of a systematic defense selection approach. This creates vulnerabilities in IoT ecosystems, necessitating more efficient and resilient defense mechanisms.", "method": "SAGE employs an active learning mechanism to identify the most informative input samples and their optimal defense labels, which are then used to train a second-level learner for defense selection. This integrates targeted data reduction, enabling efficient learning of robust defense strategies.", "result": "SAGE achieves a 201% average F1-score improvement over state-of-the-art defenses, reduces the performance gap to the Oracle by 3.8%, and decreases computational overhead by up to 29\u00d7 while maintaining robustness and generalizability across datasets.", "conclusion": "SAGE effectively enhances intrusion detection by combining active learning with targeted data reduction, significantly improving performance and efficiency compared to existing methods, while narrowing the gap to an ideal Oracle model."}}
{"id": "2509.08667", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08667", "abs": "https://arxiv.org/abs/2509.08667", "authors": ["Amirali Rayegan", "Tim Menzies"], "title": "Minimal Data, Maximum Clarity: A Heuristic for Explaining Optimization", "comment": null, "summary": "Efficient, interpretable optimization is a critical but underexplored\nchallenge in software engineering, where practitioners routinely face vast\nconfiguration spaces and costly, error-prone labeling processes. This paper\nintroduces EZR, a novel and modular framework for multi-objective optimization\nthat unifies active sampling, learning, and explanation within a single,\nlightweight pipeline. Departing from conventional wisdom, our Maximum Clarity\nHeuristic demonstrates that using less (but more informative) data can yield\noptimization models that are both effective and deeply understandable. EZR\nemploys an active learning strategy based on Naive Bayes sampling to\nefficiently identify high-quality configurations with a fraction of the labels\nrequired by fully supervised approaches. It then distills optimization logic\ninto concise decision trees, offering transparent, actionable explanations for\nboth global and local decision-making. Extensive experiments across 60\nreal-world datasets establish that EZR reliably achieves over 90% of the\nbest-known optimization performance in most cases, while providing clear,\ncohort-based rationales that surpass standard attribution-based explainable AI\n(XAI) methods (LIME, SHAP, BreakDown) in clarity and utility. These results\nendorse \"less but better\"; it is both possible and often preferable to use\nfewer (but more informative) examples to generate label-efficient optimization\nand explanations in software systems. To support transparency and\nreproducibility, all code and experimental materials are publicly available at\nhttps://github.com/amiiralii/Minimal-Data-Maximum-Clarity.", "AI": {"tldr": "EZR is a label-efficient, interpretable multi-objective optimization framework combining active learning and decision trees to achieve 90+% of state-of-the-art performance with minimal data.", "motivation": "Software engineering faces challenges of vast configuration spaces and costly labeling, requiring optimization methods that balance effectiveness with interpretability.", "method": "EZR uses Naive Bayes-based active learning to prioritize informative samples, then distills optimization policies into decision trees for transparent explanations.", "result": "Experiments on 60 datasets show EZR matches 90+% of best-known performance while outperforming LIME/SHAP in explanation clarity with 80-95\u2013% label reduction.", "conclusion": "Prioritizing informative data through active learning enables high-performance optimization with minimal labels, advancing 'minimum data maximum clarity' in software systems."}}
{"id": "2509.08200", "categories": ["cs.CR", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.08200", "abs": "https://arxiv.org/abs/2509.08200", "authors": ["William Cashman", "Chasen Milner", "Michael Houle", "Michael Jones", "Hayden Jananthan", "Jeremy Kepner", "Peter Michaleas", "Alex Pentland"], "title": "Accelerating AI Development with Cyber Arenas", "comment": "2 pages, 1 figure, 7 references, accepted to IEEE HPEC 2025", "summary": "AI development requires high fidelity testing environments to effectively\ntransition from the laboratory to operations. The flexibility offered by cyber\narenas presents a novel opportunity to test new artificial intelligence (AI)\ncapabilities with users. Cyber arenas are designed to expose end-users to\nreal-world situations and must rapidly incorporate evolving capabilities to\nmeet their core objectives. To explore this concept the MIT/IEEE/Amazon Graph\nChallenge Anonymized Network Sensor was deployed in a cyber arena during a\nNational Guard exercise.", "AI": {"tldr": "This paper evaluates the use of cyber arenas for AI testing by deploying a specialized network sensor during a National Guard exercise, demonstrating their potential to simulate real-world AI performance and adaptability.", "motivation": "AI development requires high-fidelity testing environments to bridge the gap between laboratory settings and operational deployment. Cyber arenas provide flexible, evolving frameworks to expose AI capabilities to real-world complexities and user interactions.", "method": "The paper leverages the MIT/IEEE/Amazon Graph Challenge Anonymized Network Sensor, deploying it within a cyber arena during a National Guard exercise to evaluate its performance and adaptability in real-world operational contexts.", "result": "The experiment successfully integrated the anonymized network sensor into a dynamic, mission-driven exercise, validating the feasibility of using cyber arenas to test and refine AI systems under realistic conditions.", "conclusion": "The deployment of the MIT/IEEE/Amazon Graph Challenge Anonymized Network Sensor in a cyber arena during a National Guard exercise demonstrates the effectiveness of cyber arenas for rapidly testing and integrating AI capabilities in real-world scenarios."}}
{"id": "2509.08724", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08724", "abs": "https://arxiv.org/abs/2509.08724", "authors": ["Junhao Wang", "Daoguang Zan", "Shulin Xin", "Siyao Liu", "Yurong Wu", "Kai Shen"], "title": "SWE-Mirror: Scaling Issue-Resolving Datasets by Mirroring Issues Across Repositories", "comment": null, "summary": "Creating large-scale verifiable training datasets for issue-resolving tasks\nis a critical yet notoriously difficult challenge. Existing methods on\nautomating the Gym environment setup process for real-world issues suffer from\nlow success rates and high overhead. Meanwhile, synthesizing new tasks within\nexisting Gym environments leaves the vast pool of authentic, human-reported\nproblems untapped. To maximize the utilization of existing Gym environments and\nalso the rich data of issue-resolving history on GitHub, we introduce\nSWE-Mirror, a pipeline that distills a real-world issue's semantic essence,\nmirrors it into another repository with a configured Gym environment, and\nre-animates it as a verifiable issue-resolving task. SWE-Mirror reuses existing\nGym environments along with the vast pool of issue-resolving history hosted on\nGitHub to construct a large-scale dataset of mirrored authentic and verifiable\ntasks. Applying SWE-Mirror to 40 repositories across 4 languages, we have\ncurated a dataset with 60,671 issue-resolving tasks and demonstrated the value\nof our dataset by training and evaluating coding agents at various scale.\nPost-training experiments show that models trained with the dataset exhibit\nimprovements in issue-resolving capabilities. Furthermore, by extending the\ndataset size to over 12,000 high-quality trajectories, we established a new\nstate-of-the-art (SOTA) among Qwen2.5-Coder-Instruct based LLMs on the\nOpenHands agent framework, which increases the resolve rate on\nSWE-Bench-Verified by +21.8% for the 7B model and +46.0% for the 32B model and\nvalidates the effectiveness of our approach.", "AI": {"tldr": "This paper introduces SWE-Mirror, a pipeline that transforms real GitHub issues into verifiable Gym environments, creating a large-scale dataset that improves coding agent performance by over 46% for large models.", "motivation": "Existing methods for creating verifiable training datasets for issue-resolution face limitations in success rates, computational overhead, and failure to utilize authentic human-reported issues from repositories like GitHub, necessitating a scalable and effective solution.", "method": "The SWE-Mirror pipeline distills semantic essence from GitHub issues, mirrors them into configurable Gym environments, and generates verifiable tasks by reusing existing infrastructure. This approach bridges the gap between synthetic environment setups and real-world problem data.", "result": "Generated 60,671 verified tasks across 40 repositories and 4 languages, achieving +21.8% resolve rate improvement for 7B models and +46.0% for 32B models on SWE-Bench-Verified datasets, establishing a new SOTA with 12,000 high-quality trajectories.", "conclusion": "SWE-Mirror effectively constructs a large-scale verifiable dataset by leveraging real GitHub issues and Gym environments, significantly improving issue-resolution capabilities of coding agents with substantial performance gains over existing methods."}}
{"id": "2509.08204", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08204", "abs": "https://arxiv.org/abs/2509.08204", "authors": ["Behnaz Hassanshahi", "Trong Nhan Mai", "Benjamin Selwyn Smith", "Nicholas Allen"], "title": "Unlocking Reproducibility: Automating re-Build Process for Open-Source Software", "comment": null, "summary": "Software ecosystems like Maven Central play a crucial role in modern software\nsupply chains by providing repositories for libraries and build plugins.\nHowever, the separation between binaries and their corresponding source code in\nMaven Central presents a significant challenge, particularly when it comes to\nlinking binaries back to their original build environment. This lack of\ntransparency poses security risks, as approximately 84% of the top 1200\ncommonly used artifacts are not built using a transparent CI/CD pipeline.\nConsequently, users must place a significant amount of trust not only in the\nsource code but also in the environment in which these artifacts are built.\n  Rebuilding software artifacts from source provides a robust solution to\nimprove supply chain security. This approach allows for a deeper review of\ncode, verification of binary-source equivalence, and control over dependencies.\nHowever, challenges arise due to variations in build environments, such as JDK\nversions and build commands, which can lead to build failures. Additionally,\nensuring that all dependencies are rebuilt from source across large and complex\ndependency graphs further complicates the process. In this paper, we introduce\nan extension to Macaron, an industry-grade open-source supply chain security\nframework, to automate the rebuilding of Maven artifacts from source. Our\napproach improves upon existing tools, by offering better performance in source\ncode detection and automating the extraction of build specifications from\nGitHub Actions workflows. We also present a comprehensive root cause analysis\nof build failures in Java projects and propose a scalable solution to automate\nthe rebuilding of artifacts, ultimately enhancing security and transparency in\nthe open-source supply chain.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
