{"id": "2508.08438", "categories": ["cs.CR", "cs.LG", "cs.OS"], "pdf": "https://arxiv.org/pdf/2508.08438", "abs": "https://arxiv.org/abs/2508.08438", "authors": ["Kexin Chu", "Zecheng Lin", "Dawei Xiang", "Zixu Shen", "Jianchang Su", "Cheng Chu", "Yiwei Yang", "Wenhui Zhang", "Wenfei Wu", "Wei Zhang"], "title": "Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM Inference", "comment": "17 pages,17 figures", "summary": "Global KV-cache sharing has emerged as a key optimization for accelerating\nlarge language model (LLM) inference. However, it exposes a new class of timing\nside-channel attacks, enabling adversaries to infer sensitive user inputs via\nshared cache entries. Existing defenses, such as per-user isolation, eliminate\nleakage but degrade performance by up to 38.9% in time-to-first-token (TTFT),\nmaking them impractical for high-throughput deployment. To address this gap, we\nintroduce SafeKV (Secure and Flexible KV Cache Sharing), a privacy-aware\nKV-cache management framework that selectively shares non-sensitive entries\nwhile confining sensitive content to private caches. SafeKV comprises three\ncomponents: (i) a hybrid, multi-tier detection pipeline that integrates\nrule-based pattern matching, a general-purpose privacy detector, and\ncontext-aware validation; (ii) a unified radix-tree index that manages public\nand private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and\n(iii) entropy-based access monitoring to detect and mitigate residual\ninformation leakage. Our evaluation shows that SafeKV mitigates 94% - 97% of\ntiming-based side-channel attacks. Compared to per-user isolation method,\nSafeKV improves TTFT by up to 40.58% and throughput by up to 2.66X across\ndiverse LLMs and workloads. SafeKV reduces cache-induced TTFT overhead from\n50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with\nhigh cache reuse efficiency, SafeKV reclaims the performance advantages of\nglobal sharing while providing robust runtime privacy guarantees for LLM\ninference."}
{"id": "2508.08462", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.08462", "abs": "https://arxiv.org/abs/2508.08462", "authors": ["Junling Fan", "David Koblah", "Domenic Forte"], "title": "Designing with Deception: ML- and Covert Gate-Enhanced Camouflaging to Thwart IC Reverse Engineering", "comment": null, "summary": "Integrated circuits (ICs) are essential to modern electronic systems, yet\nthey face significant risks from physical reverse engineering (RE) attacks that\ncompromise intellectual property (IP) and overall system security. While IC\ncamouflage techniques have emerged to mitigate these risks, existing approaches\nlargely focus on localized gate modifications, neglecting comprehensive\ndeception strategies. To address this gap, we present a machine learning\n(ML)-driven methodology that integrates cryptic and mimetic cyber deception\nprinciples to enhance IC security against RE. Our approach leverages a novel\nAnd-Inverter Graph Variational Autoencoder (AIG-VAE) to encode circuit\nrepresentations, enabling dual-layered camouflage through functional\npreservation and appearance mimicry. By introducing new variants of covert\ngates -- Fake Inverters, Fake Buffers, and Universal Transmitters -- our\nmethodology achieves robust protection by obscuring circuit functionality while\npresenting misleading appearances. Experimental results demonstrate the\neffectiveness of our strategy in maintaining circuit functionality while\nachieving high camouflage and similarity scores with minimal structural\noverhead. Additionally, we validate the robustness of our method against\nadvanced artificial intelligence (AI)-enhanced RE attacks, highlighting its\npractical applicability in securing IC designs. By bridging the gap in mimetic\ndeception for hardware security, our work sets a new standard for IC\ncamouflage, advancing the application of cyber deception principles to protect\ncritical systems from adversarial threats."}
{"id": "2508.08583", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08583", "abs": "https://arxiv.org/abs/2508.08583", "authors": ["Hiroya Kato", "Kentaro Kita", "Kento Hasegawa", "Seira Hidano"], "title": "AI Security Map: Holistic Organization of AI Security Technologies and Impacts on Stakeholders", "comment": null, "summary": "As the social implementation of AI has been steadily progressing, research\nand development related to AI security has also been increasing. However,\nexisting studies have been limited to organizing related techniques, attacks,\ndefenses, and risks in terms of specific domains or AI elements. Thus, it\nextremely difficult to understand the relationships among them and how negative\nimpacts on stakeholders are brought about. In this paper, we argue that the\nknowledge, technologies, and social impacts related to AI security should be\nholistically organized to help understand relationships among them. To this\nend, we first develop an AI security map that holistically organizes\ninterrelationships among elements related to AI security as well as negative\nimpacts on information systems and stakeholders. This map consists of the two\naspects, namely the information system aspect (ISA) and the external influence\naspect (EIA). The elements that AI should fulfill within information systems\nare classified under the ISA. The EIA includes elements that affect\nstakeholders as a result of AI being attacked or misused. For each element,\ncorresponding negative impacts are identified. By referring to the AI security\nmap, one can understand the potential negative impacts, along with their causes\nand countermeasures. Additionally, our map helps clarify how the negative\nimpacts on AI-based systems relate to those on stakeholders. We show some\nfindings newly obtained by referring to our map. We also provide several\nrecommendations and open problems to guide future AI security communities."}
{"id": "2508.08593", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08593", "abs": "https://arxiv.org/abs/2508.08593", "authors": ["Aydin Zaboli", "Junho Hong"], "title": "Generative AI for Critical Infrastructure in Smart Grids: A Unified Framework for Synthetic Data Generation and Anomaly Detection", "comment": "28 pages, 12 figures", "summary": "In digital substations, security events pose significant challenges to the\nsustained operation of power systems. To mitigate these challenges, the\nimplementation of robust defense strategies is critically important. A thorough\nprocess of anomaly identification and detection in information and\ncommunication technology (ICT) frameworks is crucial to ensure secure and\nreliable communication and coordination between interconnected devices within\ndigital substations. Hence, this paper addresses the critical cybersecurity\nchallenges confronting IEC61850-based digital substations within modern smart\ngrids, where the integration of advanced communication protocols, e.g., generic\nobject-oriented substation event (GOOSE), has enhanced energy management and\nintroduced significant vulnerabilities to cyberattacks. Focusing on the\nlimitations of traditional anomaly detection systems (ADSs) in detecting\nthreats, this research proposes a transformative approach by leveraging\ngenerative AI (GenAI) to develop robust ADSs. The primary contributions include\nthe suggested advanced adversarial traffic mutation (AATM) technique to\ngenerate synthesized and balanced datasets for GOOSE messages, ensuring\nprotocol compliance and enabling realistic zero-day attack pattern creation to\naddress data scarcity. Then, the implementation of GenAI-based ADSs\nincorporating the task-oriented dialogue (ToD) processes has been explored for\nimproved detection of attack patterns. Finally, a comparison of the GenAI-based\nADS with machine learning (ML)-based ADSs has been implemented to showcase the\noutperformance of the GenAI-based frameworks considering the AATM-generated\nGOOSE datasets and standard/advanced performance evaluation metrics."}
{"id": "2508.08322", "categories": ["cs.SE", "cs.AI", "68T07, 68N01", "D.2.2; I.2.6; D.2.5; I.2.8"], "pdf": "https://arxiv.org/pdf/2508.08322", "abs": "https://arxiv.org/abs/2508.08322", "authors": ["Muhammad Haseeb"], "title": "Context Engineering for Multi-Agent LLM Code Assistants Using Elicit, NotebookLM, ChatGPT, and Claude Code", "comment": "15 pages, 5 figures, research paper on multi-agent LLM systems for\n  code generation", "summary": "Large Language Models (LLMs) have shown promise in automating code generation\nand software engineering tasks, yet they often struggle with complex,\nmulti-file projects due to context limitations and knowledge gaps. We propose a\nnovel context engineering workflow that combines multiple AI components: an\nIntent Translator (GPT-5) for clarifying user requirements, an Elicit-powered\nsemantic literature retrieval for injecting domain knowledge, NotebookLM-based\ndocument synthesis for contextual understanding, and a Claude Code multi-agent\nsystem for code generation and validation. Our integrated approach leverages\nintent clarification, retrieval-augmented generation, and specialized\nsub-agents orchestrated via Claude's agent framework. We demonstrate that this\nmethod significantly improves the accuracy and reliability of code assistants\nin real-world repositories, yielding higher single-shot success rates and\nbetter adherence to project context than baseline single-agent approaches.\nQualitative results on a large Next.js codebase show the multi-agent system\neffectively plans, edits, and tests complex features with minimal human\nintervention. We compare our system with recent frameworks like CodePlan,\nMASAI, and HyperAgent, highlighting how targeted context injection and agent\nrole decomposition lead to state-of-the-art performance. Finally, we discuss\nthe implications for deploying LLM-based coding assistants in production, along\nwith lessons learned on context management and future research directions."}
{"id": "2508.08655", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.08655", "abs": "https://arxiv.org/abs/2508.08655", "authors": ["Manabu Hirano", "Ryotaro Kobayashi"], "title": "Hypervisor-based Double Extortion Ransomware Detection Method Using Kitsune Network Features", "comment": "\\copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Double extortion ransomware attacks have become mainstream since many\norganizations adopt more robust and resilient data backup strategies against\nconventional crypto-ransomware. This paper presents detailed attack stages,\ntactics, procedures, and tools used in the double extortion ransomware attacks.\nWe then present a novel detection method using low-level storage and memory\nbehavioral features and network traffic features obtained from a thin\nhypervisor to establish a defense-in-depth strategy for when attackers\ncompromise OS-level protection. We employed the lightweight \\emph{Kitsune}\nNetwork Intrusion Detection System (NIDS)'s network feature to detect the data\nexfiltration phase in double extortion ransomware attacks. Our experimental\nresults showed that the presented method improved by 0.166 in the macro F score\nof the data exfiltration phase detection rate. Lastly, we discuss the\nlimitations of the presented method and future work."}
{"id": "2508.08332", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08332", "abs": "https://arxiv.org/abs/2508.08332", "authors": ["Humza Ashraf", "Syed Muhammad Danish", "Aris Leivadeas", "Yazan Otoum", "Zeeshan Sattar"], "title": "Energy-Aware Code Generation with LLMs: Benchmarking Small vs. Large Language Models for Sustainable AI Programming", "comment": null, "summary": "Large Language Models (LLMs) are widely used for code generation. However,\ncommercial models like ChatGPT require significant computing power, which leads\nto high energy use and carbon emissions. This has raised concerns about their\nenvironmental impact. In this study, we evaluate open-source Small Language\nModels (SLMs) trained explicitly for code generation and compare their\nperformance and energy efficiency against large LLMs and efficient\nhuman-written Python code. The goal is to investigate whether SLMs can match\nthe performance of LLMs on certain types of programming problems while\nproducing more energy-efficient code. We evaluate 150 coding problems from\nLeetCode, evenly distributed across three difficulty levels: easy, medium, and\nhard. Our comparison includes three small open-source models, StableCode-3B,\nStarCoderBase-3B, and Qwen2.5-Coder-3B-Instruct, and two large commercial\nmodels, GPT-4.0 and DeepSeek-Reasoner. The generated code is evaluated using\nfour key metrics: run-time, memory usage, energy consumption, and correctness.\nWe use human-written solutions as a baseline to assess the quality and\nefficiency of the model-generated code. Results indicate that LLMs achieve the\nhighest correctness across all difficulty levels, but SLMs are often more\nenergy-efficient when their outputs are correct. In over 52% of the evaluated\nproblems, SLMs consumed the same or less energy than LLMs."}
{"id": "2508.08656", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.08656", "abs": "https://arxiv.org/abs/2508.08656", "authors": ["Manabu Hirano", "Ryotaro Kobayashi"], "title": "Evasive Ransomware Attacks Using Low-level Behavioral Adversarial Examples", "comment": "\\copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Protecting state-of-the-art AI-based cybersecurity defense systems from cyber\nattacks is crucial. Attackers create adversarial examples by adding small\nchanges (i.e., perturbations) to the attack features to evade or fool the deep\nlearning model. This paper introduces the concept of low-level behavioral\nadversarial examples and its threat model of evasive ransomware. We formulate\nthe method and the threat model to generate the optimal source code of evasive\nmalware. We then examine the method using the leaked source code of Conti\nransomware with the micro-behavior control function. The micro-behavior control\nfunction is our test component to simulate changing source code in ransomware;\nransomware's behavior can be changed by specifying the number of threads, file\nencryption ratio, and delay after file encryption at the boot time. We\nevaluated how much an attacker can control the behavioral features of\nransomware using the micro-behavior control function to decrease the detection\nrate of a ransomware detector."}
{"id": "2508.08342", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.08342", "abs": "https://arxiv.org/abs/2508.08342", "authors": ["Maximilian Jungwirth", "Martin Gruber", "Gordon Fraser"], "title": "Improving Merge Pipeline Throughput in Continuous Integration via Pull Request Prioritization", "comment": "This paper is accepted on the Industry Track of the 41st\n  International Conference on Software Maintenance and Evolution (ICSME 2025)", "summary": "Integrating changes into large monolithic software repositories is a critical\nstep in modern software development that substantially impacts the speed of\nfeature delivery, the stability of the codebase, and the overall productivity\nof development teams. To ensure the stability of the main branch, many\norganizations use merge pipelines that test software versions before the\nchanges are permanently integrated. However, the load on merge pipelines is\noften so high that they become bottlenecks, despite the use of parallelization.\nExisting optimizations frequently rely on specific build systems, limiting\ntheir generalizability and applicability. In this paper we propose to optimize\nthe order of PRs in merge pipelines using practical build predictions utilizing\nonly historical build data, PR metadata, and contextual information to estimate\nthe likelihood of successful builds in the merge pipeline. By dynamically\nprioritizing likely passing PRs during peak hours, this approach maximizes\nthroughput when it matters most. Experiments conducted on a real-world,\nlarge-scale project demonstrate that predictive ordering significantly\noutperforms traditional first-in-first-out (FIFO), as well as\nnon-learning-based ordering strategies. Unlike alternative optimizations, this\napproach is agnostic to the underlying build system and thus easily integrable\ninto existing automated merge pipelines."}
{"id": "2508.08749", "categories": ["cs.CR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.08749", "abs": "https://arxiv.org/abs/2508.08749", "authors": ["Yuan Qiu", "Ke Yi"], "title": "Approximate DBSCAN under Differential Privacy", "comment": null, "summary": "This paper revisits the DBSCAN problem under differential privacy (DP).\nExisting DP-DBSCAN algorithms aim at publishing the cluster labels of the input\npoints. However, we show that both empirically and theoretically, this approach\ncannot offer any utility in the published results. We therefore propose an\nalternative definition of DP-DBSCAN based on the notion of spans. We argue that\npublishing the spans actually better serves the purposes of visualization and\nclassification of DBSCAN. Then we present a linear-time DP-DBSCAN algorithm\nachieving the sandwich quality guarantee in any constant dimensions, as well as\nmatching lower bounds on the approximation ratio. A key building block in our\nalgorithm is a linear-time algorithm for constructing a histogram under\npure-DP, which is of independent interest. Finally, we conducted experiments on\nboth synthetic and real-world datasets to verify the practical performance of\nour DP-DBSCAN algorithm."}
{"id": "2508.08545", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08545", "abs": "https://arxiv.org/abs/2508.08545", "authors": ["Youssef Esseddiq Ouatiti", "Mohammed Sayagh", "Bram Adams", "Ahmed E. Hassan"], "title": "OmniLLP: Enhancing LLM-based Log Level Prediction with Context-Aware Retrieval", "comment": null, "summary": "Developers insert logging statements in source code to capture relevant\nruntime information essential for maintenance and debugging activities. Log\nlevel choice is an integral, yet tricky part of the logging activity as it\ncontrols log verbosity and therefore influences systems' observability and\nperformance. Recent advances in ML-based log level prediction have leveraged\nlarge language models (LLMs) to propose log level predictors (LLPs) that\ndemonstrated promising performance improvements (AUC between 0.64 and 0.8).\nNevertheless, current LLM-based LLPs rely on randomly selected in-context\nexamples, overlooking the structure and the diverse logging practices within\nmodern software projects. In this paper, we propose OmniLLP, a novel LLP\nenhancement framework that clusters source files based on (1) semantic\nsimilarity reflecting the code's functional purpose, and (2) developer\nownership cohesion. By retrieving in-context learning examples exclusively from\nthese semantic and ownership aware clusters, we aim to provide more coherent\nprompts to LLPs leveraging LLMs, thereby improving their predictive accuracy.\nOur results show that both semantic and ownership-aware clusterings\nstatistically significantly improve the accuracy (by up to 8\\% AUC) of the\nevaluated LLM-based LLPs compared to random predictors (i.e., leveraging\nrandomly selected in-context examples from the whole project). Additionally,\nour approach that combines the semantic and ownership signal for in-context\nprediction achieves an impressive 0.88 to 0.96 AUC across our evaluated\nprojects. Our findings highlight the value of integrating software\nengineering-specific context, such as code semantic and developer ownership\nsignals into LLM-LLPs, offering developers a more accurate, contextually-aware\napproach to logging and therefore, enhancing system maintainability and\nobservability."}
{"id": "2508.08789", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.08789", "abs": "https://arxiv.org/abs/2508.08789", "authors": ["Yuchu Jiang", "Jian Zhao", "Yuchen Yuan", "Tianle Zhang", "Yao Huang", "Yanghao Zhang", "Yan Wang", "Yanshu Li", "Xizhong Guo", "Yusheng Zhao", "Jun Zhang", "Zhi Zhang", "Xiaojian Lin", "Yixiu Zou", "Haoxuan Ma", "Yuhu Shang", "Yuzhi Hu", "Keshu Cai", "Ruochen Zhang", "Boyuan Chen", "Yilan Gao", "Ziheng Jiao", "Yi Qin", "Shuangjun Du", "Xiao Tong", "Zhekun Liu", "Yu Chen", "Xuankun Rong", "Rui Wang", "Yejie Zheng", "Zhaoxin Fan", "Hongyuan Zhang", "Pan Zhou", "Lei Jin", "Hao Zhao", "Xu Yang", "Jiaojiao Zhao", "Jianshu Li", "Joey Tianyi Zhou", "Zhi-Qi Cheng", "Longtao Huang", "Zhiyi Liu", "Zheng Zhu", "Jianan Li", "Gang Wang", "Qi Li", "Xu-Yao Zhang", "Yaodong Yang", "Mang Ye", "Wenqi Ren", "Zhaofeng He", "Hang Su", "Rongrong Ni", "Liping Jing", "Xingxing Wei", "Junliang Xing", "Massimo Alioto", "Shengmei Shen", "Petia Radeva", "Dacheng Tao", "Ya-Qin Zhang", "Shuicheng Yan", "Chi Zhang", "Zhongjiang He", "Xuelong Li"], "title": "Never Compromise to Vulnerabilities: A Comprehensive Survey on AI Governance", "comment": "24 pages, 3 figures", "summary": "The rapid advancement of AI has expanded its capabilities across domains, yet\nintroduced critical technical vulnerabilities, such as algorithmic bias and\nadversarial sensitivity, that pose significant societal risks, including\nmisinformation, inequity, security breaches, physical harm, and eroded public\ntrust. These challenges highlight the urgent need for robust AI governance. We\npropose a comprehensive framework integrating technical and societal\ndimensions, structured around three interconnected pillars: Intrinsic Security\n(system reliability), Derivative Security (real-world harm mitigation), and\nSocial Ethics (value alignment and accountability). Uniquely, our approach\nunifies technical methods, emerging evaluation benchmarks, and policy insights\nto promote transparency, accountability, and trust in AI systems. Through a\nsystematic review of over 300 studies, we identify three core challenges: (1)\nthe generalization gap, where defenses fail against evolving threats; (2)\ninadequate evaluation protocols that overlook real-world risks; and (3)\nfragmented regulations leading to inconsistent oversight. These shortcomings\nstem from treating governance as an afterthought, rather than a foundational\ndesign principle, resulting in reactive, siloed efforts that fail to address\nthe interdependence of technical integrity and societal trust. To overcome\nthis, we present an integrated research agenda that bridges technical rigor\nwith social responsibility. Our framework offers actionable guidance for\nresearchers, engineers, and policymakers to develop AI systems that are not\nonly robust and secure but also ethically aligned and publicly trustworthy. The\naccompanying repository is available at\nhttps://github.com/ZTianle/Awesome-AI-SG."}
{"id": "2508.08661", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08661", "abs": "https://arxiv.org/abs/2508.08661", "authors": ["Chunhua Liu", "Hong Yi Lin", "Patanamon Thongtanunam"], "title": "Hallucinations in Code Change to Natural Language Generation: Prevalence and Evaluation of Detection Metrics", "comment": "8 main pages, 5 figures", "summary": "Language models have shown strong capabilities across a wide range of tasks\nin software engineering, such as code generation, yet they suffer from\nhallucinations. While hallucinations have been studied independently in natural\nlanguage and code generation, their occurrence in tasks involving code changes\nwhich have a structurally complex and context-dependent format of code remains\nlargely unexplored. This paper presents the first comprehensive analysis of\nhallucinations in two critical tasks involving code change to natural language\ngeneration: commit message generation and code review comment generation. We\nquantify the prevalence of hallucinations in recent language models and explore\na range of metric-based approaches to automatically detect them. Our findings\nreveal that approximately 50\\% of generated code reviews and 20\\% of generated\ncommit messages contain hallucinations. Whilst commonly used metrics are weak\ndetectors on their own, combining multiple metrics substantially improves\nperformance. Notably, model confidence and feature attribution metrics\neffectively contribute to hallucination detection, showing promise for\ninference-time detection.\\footnote{All code and data will be released upon\nacceptance."}
{"id": "2508.08832", "categories": ["cs.CR", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.08832", "abs": "https://arxiv.org/abs/2508.08832", "authors": ["Ikram Messadi", "Giulia Cervia", "Vincent Itier"], "title": "Image selective encryption analysis using mutual information in CNN based embedding space", "comment": "Accepted for presentation at the 13th European Workshop on Visual\n  Information Processing (EUVIP), Oct 2025, Valetta, Malta", "summary": "As digital data transmission continues to scale, concerns about privacy grow\nincreasingly urgent - yet privacy remains a socially constructed and\nambiguously defined concept, lacking a universally accepted quantitative\nmeasure. This work examines information leakage in image data, a domain where\ninformation-theoretic guarantees are still underexplored. At the intersection\nof deep learning, information theory, and cryptography, we investigate the use\nof mutual information (MI) estimators - in particular, the empirical estimator\nand the MINE framework - to detect leakage from selectively encrypted images.\nMotivated by the intuition that a robust estimator would require a\nprobabilistic frameworks that can capture spatial dependencies and residual\nstructures, even within encrypted representations - our work represent a\npromising direction for image information leakage estimation."}
{"id": "2508.08868", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.08868", "abs": "https://arxiv.org/abs/2508.08868", "authors": ["Henning Femmer", "Frank Houdek", "Max Unterbusch", "Andreas Vogelsang"], "title": "Description and Comparative Analysis of QuRE: A New Industrial Requirements Quality Dataset", "comment": null, "summary": "Requirements quality is central to successful software and systems\nengineering. Empirical research on quality defects in natural language\nrequirements relies heavily on datasets, ideally as realistic and\nrepresentative as possible. However, such datasets are often inaccessible,\nsmall, or lack sufficient detail. This paper introduces QuRE (Quality in\nRequirements), a new dataset comprising 2,111 industrial requirements that have\nbeen annotated through a real-world review process. Previously used for over\nfive years as part of an industrial contract, this dataset is now being\nreleased to the research community. In this work, we furthermore provide\ndescriptive statistics on the dataset, including measures such as lexical\ndiversity and readability, and compare it to existing requirements datasets and\nsynthetically generated requirements. In contrast to synthetic datasets, QuRE\nis linguistically similar to existing ones. However, this dataset comes with a\ndetailed context description, and its labels have been created and used\nsystematically and extensively in an industrial context over a period of close\nto a decade. Our goal is to foster transparency, comparability, and empirical\nrigor by supporting the development of a common gold standard for requirements\nquality datasets. This, in turn, will enable more sound and collaborative\nresearch efforts in the field."}
{"id": "2508.08836", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08836", "abs": "https://arxiv.org/abs/2508.08836", "authors": ["Jiaxuan Wu", "Yinghan Zhou", "Wanli Peng", "Yiming Xue", "Juan Wen", "Ping Zhong"], "title": "EditMF: Drawing an Invisible Fingerprint for Your Large Language Models", "comment": "8 pages, 2 figures", "summary": "Training large language models (LLMs) is resource-intensive and expensive,\nmaking protecting intellectual property (IP) for LLMs crucial. Recently,\nembedding fingerprints into LLMs has emerged as a prevalent method for\nestablishing model ownership. However, existing back-door-based methods suffer\nfrom limited stealth and efficiency. To simultaneously address these issues, we\npropose EditMF, a training-free fingerprinting paradigm that achieves highly\nimperceptible fingerprint embedding with minimal computational overhead.\nOwnership bits are mapped to compact, semantically coherent triples drawn from\nan encrypted artificial knowledge base (e.g., virtual author-novel-protagonist\nfacts). Causal tracing localizes the minimal set of layers influencing each\ntriple, and a zero-space update injects the fingerprint without perturbing\nunrelated knowledge. Verification requires only a single black-box query and\nsucceeds when the model returns the exact pre-embedded protagonist. Empirical\nresults on LLaMA and Qwen families show that EditMF combines high\nimperceptibility with negligible model's performance loss, while delivering\nrobustness far beyond LoRA-based fingerprinting and approaching that of SFT\nembeddings. Extensive experiments demonstrate that EditMF is an effective and\nlow-overhead solution for secure LLM ownership verification."}
{"id": "2508.08872", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2508.08872", "abs": "https://arxiv.org/abs/2508.08872", "authors": ["Dylan Callaghan", "Alexandra van der Spuy", "Bernd Fischer"], "title": "Empirical Analysis of Temporal and Spatial Fault Characteristics in Multi-Fault Bug Repositories", "comment": null, "summary": "Fixing software faults contributes significantly to the cost of software\nmaintenance and evolution. Techniques for reducing these costs require datasets\nof software faults, as well as an understanding of the faults, for optimal\ntesting and evaluation. In this paper, we present an empirical analysis of the\ntemporal and spatial characteristics of faults existing in 16 open-source Java\nand Python projects, which form part of the Defects4J and BugsInPy datasets,\nrespectively. Our findings show that many faults in these software systems are\nlong-lived, leading to the majority of software versions having multiple\ncoexisting faults. This is in contrast to the assumptions of the original\ndatasets, where the majority of versions only identify a single fault. In\naddition, we show that although the faults are found in only a small subset of\nthe systems, these faults are often evenly distributed amongst this subset,\nleading to relatively few bug hotspots."}
{"id": "2508.08898", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.08898", "abs": "https://arxiv.org/abs/2508.08898", "authors": ["Federico Calandra", "Marco Bernardo", "Andrea Esposito", "Francesco Fabris"], "title": "Redactable Blockchains: An Overview", "comment": null, "summary": "Blockchains are widely recognized for their immutability, which provides\nrobust guarantees of data integrity and transparency. However, this same\nfeature poses significant challenges in real-world situations that require\nregulatory compliance, correction of erroneous data, or removal of sensitive\ninformation. Redactable blockchains address the limitations of traditional ones\nby enabling controlled, auditable modifications to blockchain data, primarily\nthrough cryptographic mechanisms such as chameleon hash functions and\nalternative redaction schemes. This report examines the motivations for\nintroducing redactability, surveys the cryptographic primitives that enable\nsecure edits, and analyzes competing approaches and their shortcomings. Special\nattention is paid to the practical deployment of redactable blockchains in\nprivate settings, with discussions of use cases in healthcare, finance,\nInternet of drones, and federated learning. Finally, the report outlines\nfurther challenges, also in connection with reversible computing, and the\nfuture potential of redactable blockchains in building law-compliant,\ntrustworthy, and scalable digital infrastructures."}
{"id": "2508.08952", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.08952", "abs": "https://arxiv.org/abs/2508.08952", "authors": ["Hyunwoo Kim", "Jaeseong Lee", "Sunpyo Hong", "Changmin Han"], "title": "Toward Automated Hypervisor Scenario Generation Based on VM Workload Profiling for Resource-Constrained Environments", "comment": null, "summary": "In the automotive industry, the rise of software-defined vehicles (SDVs) has\n  driven a shift toward virtualization-based architectures that consolidate\n  diverse automotive workloads on a shared hardware platform. To support this\n  evolution, chipset vendors provide board support packages (BSPs), hypervisor\n  setups, and resource allocation guidelines. However, adapting these static\n  configurations to varying system requirements and workloads remain a\n  significant challenge for Tier 1 integrators.\n  This paper presents an automated scenario generation framework, which helps\n  automotive vendors to allocate hardware resources efficiently across multiple\n  VMs. By profiling runtime behavior and integrating both theoretical models\nand\n  vendor heuristics, the proposed tool generates optimized hypervisor\n  configurations tailored to system constraints.\n  We compare two main approaches for modeling target QoS based on profiled data\n  and resource allocation: domain-guided parametric modeling and deep\n  learning-based modeling. We further describe our optimization strategy using\n  the selected QoS model to derive efficient resource allocations. Finally, we\n  report on real-world deployments to demonstrate the effectiveness of our\n  framework in improving integration efficiency and reducing development time\nin\n  resource-constrained environments."}
{"id": "2508.08945", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.08945", "abs": "https://arxiv.org/abs/2508.08945", "authors": ["Syed Irtiza Maksud", "Subhash Lakshminarayana"], "title": "Load-Altering Attacks Against Power Grids: A Case Study Using the GB-36 Bus System Open Dataset", "comment": null, "summary": "The growing digitalization and the rapid adoption of high-powered\nInternet-of-Things (IoT)-enabled devices (e.g., EV charging stations) have\nincreased the vulnerability of power grids to cyber threats. In particular, the\nso-called Load Altering Attacks (LAAs) can trigger rapid frequency fluctuations\nand potentially destabilize the power grid. This paper aims to bridge the gap\nbetween academic research and practical application by using open-source\ndatasets released by grid operators. It investigates various LAA scenarios on a\nreal-world transmission network, namely the Great Britain (GB)-36 Zone model\nreleased by the UK's National Electricity System Operator (NESO). It evaluates\nthe threshold of LAA severity that the grid can tolerate before triggering\ncascading effects. Additionally, it explores how Battery Energy Storage Systems\n(BESS) based fast frequency response services can mitigate or prevent such\nimpacts. Simulations are conducted using DIgSILENT PowerFactory to ensure\nrealistic system representation. The analysis provides several useful insights\nto grid operators on the LAA impact, such as the influence of the relative\nlocations of BESS and LAA, as well as how delays in attack execution can\ninfluence the overall system response."}
{"id": "2508.09021", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09021", "abs": "https://arxiv.org/abs/2508.09021", "authors": ["Kevin Kurian", "Ethan Holland", "Sean Oesch"], "title": "Attacks and Defenses Against LLM Fingerprinting", "comment": null, "summary": "As large language models are increasingly deployed in sensitive environments,\nfingerprinting attacks pose significant privacy and security risks. We present\na study of LLM fingerprinting from both offensive and defensive perspectives.\nOur attack methodology uses reinforcement learning to automatically optimize\nquery selection, achieving better fingerprinting accuracy with only 3 queries\ncompared to randomly selecting 3 queries from the same pool. Our defensive\napproach employs semantic-preserving output filtering through a secondary LLM\nto obfuscate model identity while maintaining semantic integrity. The defensive\nmethod reduces fingerprinting accuracy across tested models while preserving\noutput quality. These contributions show the potential to improve\nfingerprinting tools capabilities while providing practical mitigation\nstrategies against fingerprinting attacks."}
{"id": "2508.09060", "categories": ["cs.CR", "cs.LG", "cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.09060", "abs": "https://arxiv.org/abs/2508.09060", "authors": ["Abu Shafin Mohammad Mahdee Jameel", "Shreya Ghosh", "Aly El Gamal"], "title": "Developing a Transferable Federated Network Intrusion Detection System", "comment": "Currently under review", "summary": "Intrusion Detection Systems (IDS) are a vital part of a network-connected\ndevice. In this paper, we develop a deep learning based intrusion detection\nsystem that is deployed in a distributed setup across devices connected to a\nnetwork. Our aim is to better equip deep learning models against unknown\nattacks using knowledge from known attacks. To this end, we develop algorithms\nto maximize the number of transferability relationships. We propose a\nConvolutional Neural Network (CNN) model, along with two algorithms that\nmaximize the number of relationships observed. One is a two step data\npre-processing stage, and the other is a Block-Based Smart Aggregation (BBSA)\nalgorithm. The proposed system succeeds in achieving superior transferability\nperformance while maintaining impressive local detection rates. We also show\nthat our method is generalizable, exhibiting transferability potential across\ndatasets and even with different backbones. The code for this work can be found\nat https://github.com/ghosh64/tabfidsv2."}
