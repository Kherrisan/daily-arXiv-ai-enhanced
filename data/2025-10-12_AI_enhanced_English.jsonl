{"id": "2510.07435", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.07435", "abs": "https://arxiv.org/abs/2510.07435", "authors": ["Zixuan Feng", "Sadia Afroz", "Anita Sarma"], "title": "Modeling Developer Burnout with GenAI Adoption", "comment": "10 pages, LLM", "summary": "Generative AI (GenAI) is rapidly reshaping software development workflows.\nWhile prior studies emphasize productivity gains, the adoption of GenAI also\nintroduces new pressures that may harm developers' well-being. In this paper,\nwe investigate the relationship between the adoption of GenAI and developers'\nburnout. We utilized the Job Demands--Resources (JD--R) model as the analytic\nlens in our empirical study. We employed a concurrent embedded mixed-methods\nresearch design, integrating quantitative and qualitative evidence. We first\nsurveyed 442 developers across diverse organizations, roles, and levels of\nexperience. We then employed Partial Least Squares--Structural Equation\nModeling (PLS-SEM) and regression to model the relationships among job demands,\njob resources, and burnout, complemented by a qualitative analysis of\nopen-ended responses to contextualize the quantitative findings. Our results\nshow that GenAI adoption heightens burnout by increasing job demands, while job\nresources and positive perceptions of GenAI mitigate these effects, reframing\nadoption as an opportunity.", "AI": {"tldr": "GenAI in software development raises burnout risks via increased job demands, but these effects can be offset by sufficient resources and positive developer attitudes to AI tools.", "motivation": "This paper addresses the gap between existing studies on GenAI's productivity benefits and its unexplored negative impacts on developer well-being, focusing on burnout risks introduced by AI adoption.", "method": "A concurrent embedded mixed-methods design was used, combining a survey of 442 developers with PLS-SEM and regression analyses for quantitative modeling, alongside qualitative analysis of open-ended responses using the JD-R model framework.", "result": "GenAI adoption increases burnout through heightened job demands, while job resources and positive GenAI perceptions significantly reduce these negative effects, suggesting mitigation strategies for sustainable AI integration.", "conclusion": "The study concludes that GenAI adoption can exacerbate developer burnout by increasing job demands but highlights that adequate job resources and positive perceptions of GenAI can transform adoption into a career-enhancing opportunity."}}
{"id": "2510.07529", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.07529", "abs": "https://arxiv.org/abs/2510.07529", "authors": ["Carol Hanna", "Federica Sarro", "Mark Harman", "Justyna Petke"], "title": "HotBugs.jar: A Benchmark of Hot Fixes for Time-Critical Bugs", "comment": null, "summary": "Hot fixes are urgent, unplanned changes deployed to production systems to\naddress time-critical issues. Despite their importance, no existing evaluation\nbenchmark focuses specifically on hot fixes. We present HotBugs$.$jar, the\nfirst dataset dedicated to real-world hot fixes. From an initial mining of 10\nactive Apache projects totaling over 190K commits and 150K issue reports, we\nidentified 746 software patches that met our hot-fix criteria. After manual\nevaluation, 679 were confirmed as genuine hot fixes, of which 110 are\nreproducible using a test suite. Building upon the Bugs$.$jar framework,\nHotBugs$.$jar integrates these 110 reproducible cases and makes available all\n679 manually validated hot fixes, each enriched with comprehensive metadata to\nsupport future research. Each hot fix was systematically identified using Jira\nissue data, validated by independent reviewers, and packaged in a reproducible\nformat with buggy and fixed versions, test suites, and metadata. HotBugs$.$jar\nhas already been adopted as the official challenge dataset for the Search-Based\nSoftware Engineering (SBSE) Conference Challenge Track, demonstrating its\nimmediate impact. This benchmark enables the study and evaluation of tools for\nrapid debugging, automated repair, and production-grade resilience in modern\nsoftware systems to drive research in this essential area forward.", "AI": {"tldr": "HotBugs.jar is the first benchmark for hot fixes, curating 679 real-world production patches with metadata and test harnesses from Apache projects to advance research in rapid debugging and automated repair for mission-critical systems.", "motivation": "The lack of a dedicated benchmark for hot fixes, which are urgent production changes critical to system reliability, limits progress in developing tools for rapid debugging and automated repair in real-world software systems.", "method": "The authors mined 10 Apache projects (190K commits, 150K issues), identified 746 candidate hot fixes via Jira issue data, manually validated 679, and integrated 110 reproducible cases into a curated dataset with metadata and test suites using the Bugs.jar framework.", "result": "HotBugs.jar contains 679 validated hot fixes (110 reproducible) with comprehensive metadata and test suites, already adopted by SBSE Conference Challenge Track. It enables empirical evaluation of production-grade resilience techniques.", "conclusion": "HotBugs.jar fills a critical research gap by providing the first benchmark for evaluating tools in hot-fix scenarios, enabling systematic study of rapid debugging and automated repair in production systems."}}
{"id": "2510.07604", "categories": ["cs.SE", "D.2.4"], "pdf": "https://arxiv.org/pdf/2510.07604", "abs": "https://arxiv.org/abs/2510.07604", "authors": ["Yubo Bai", "Tapti Palit"], "title": "RustAssure: Differential Symbolic Testing for LLM-Transpiled C-to-Rust Code", "comment": "13 pages to appear in Proceedings of ASE 2025", "summary": "Rust is a memory-safe programming language that significantly improves\nsoftware security. Existing codebases written in unsafe memory languages, such\nas C, must first be transpiled to Rust to take advantage of Rust's improved\nsafety guarantees. RustAssure presents a system that uses Large Language Models\n(LLMs) to automatically transpile existing C codebases to Rust. RustAssure uses\nprompt engineering techniques to maximize the chances of the LLM generating\nidiomatic and safe Rust code. Moreover, because LLMs often generate code with\nsubtle bugs that can be missed under traditional unit or fuzz testing,\nRustAssure performs differential symbolic testing to establish the semantic\nsimilarity between the original C and LLM-transpiled Rust code. We evaluated\nRustAssure with five real-world applications and libraries, and showed that our\nsystem is able to generate compilable Rust functions for 89.8% of all C\nfunctions, of which 69.9% produced equivalent symbolic return values for both\nthe C and Rust functions.", "AI": {"tldr": "RustAssure uses LLMs to automatically convert C to safe Rust with 89.8% compilation success, validated by symbolic testing showing 69.9% functional equivalence.", "motivation": "Existing C codebases need conversion to Rust for improved memory safety, but manual conversion is impractical at scale.", "method": "LLM-based transpilation with prompt engineering and differential symbolic testing for validation.", "result": "89.8% C functions compiled to Rust, 69.9% achieved equivalent symbolic return values in tests.", "conclusion": "RustAssure demonstrates effectiveness in transpiling C to Rust with high compilation success and semantic similarity, though full equivalence remains partial."}}
{"id": "2510.07740", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07740", "abs": "https://arxiv.org/abs/2510.07740", "authors": ["Dezhi Ran", "Yuan Cao", "Mengzhou Wu", "Simin Chen", "Yuzhe Guo", "Jun Ren", "Zihe Song", "Hao Yu", "Jialei Wei", "Linyi Li", "Wei Yang", "Baishakhi Ray", "Tao Xie"], "title": "AppForge: From Assistant to Independent Developer -- Are GPTs Ready for Software Development?", "comment": "Under Review. Benchmark and leadboards at\n  https://appforge-bench.github.io/", "summary": "Large language models (LLMs) have demonstrated remarkable capability in\nfunction-level code generation tasks. Unlike isolated functions, real-world\napplications demand reasoning over the entire software system: developers must\norchestrate how different components interact, maintain consistency across\nstates over time, and ensure the application behaves correctly within the\nlifecycle and framework constraints. Yet, no existing benchmark adequately\nevaluates whether LLMs can bridge this gap and construct entire software\nsystems from scratch. To address this gap, we propose APPFORGE, a benchmark\nconsisting of 101 software development problems drawn from real-world Android\napps. Given a natural language specification detailing the app functionality, a\nlanguage model is tasked with implementing the functionality into an Android\napp from scratch. Developing an Android app from scratch requires understanding\nand coordinating app states, lifecycle management, and asynchronous operations,\ncalling for LLMs to generate context-aware, robust, and maintainable code. To\nconstruct APPFORGE, we design a multi-agent system to automatically summarize\nthe main functionalities from app documents and navigate the app to synthesize\ntest cases validating the functional correctness of app implementation.\nFollowing rigorous manual verification by Android development experts, APPFORGE\nincorporates the test cases within an automated evaluation framework that\nenables reproducible assessment without human intervention, making it easily\nadoptable for future research. Our evaluation on 12 flagship LLMs show that all\nevaluated models achieve low effectiveness, with the best-performing model\n(GPT-5) developing only 18.8% functionally correct applications, highlighting\nfundamental limitations in current models' ability to handle complex,\nmulti-component software engineering challenges.", "AI": {"tldr": "This paper introduces APPFORGE, a benchmark evaluating LLMs' ability to build functional Android apps from natural language specifications, revealing significant limitations in current models' system-level engineering capabilities.", "motivation": "While LLMs excel at code generation, no benchmark exists to assess their ability to design entire software systems with lifecycle management, state coordination, and framework compliance - a critical gap in real-world software development.", "method": "The authors created 101 Android app problems from real-world apps, implemented a multi-agent system to: (1.) automate specification extraction from documents, (2.) synthesize test cases through app navigation, and (3.) establish an automated evaluation framework validated by Android experts.", "result": "12 leading LLMs (including GPT-5 as the best-performing model) achieved a maximum of 18.8%-functional correctness, with all models struggling with complex, multi-component implementation challenges across Android app lifecycles and state management.", "conclusion": "APPFORGE demonstrates current LLMs lack the capacity to handle system-level software engineering tasks, identifying critical limitations in their ability to produce context-aware, maintainable code for multi-component applications within framework constraints."}}
{"id": "2510.07452", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07452", "abs": "https://arxiv.org/abs/2510.07452", "authors": ["Anthony Hughes", "Vasisht Duddu", "N. Asokan", "Nikolaos Aletras", "Ning Ma"], "title": "PATCH: Mitigating PII Leakage in Language Models with Privacy-Aware Targeted Circuit PatcHing", "comment": null, "summary": "Language models (LMs) may memorize personally identifiable information (PII)\nfrom training data, enabling adversaries to extract it during inference.\nExisting defense mechanisms such as differential privacy (DP) reduce this\nleakage, but incur large drops in utility. Based on a comprehensive study using\ncircuit discovery to identify the computational circuits responsible PII\nleakage in LMs, we hypothesize that specific PII leakage circuits in LMs should\nbe responsible for this behavior. Therefore, we propose PATCH (Privacy-Aware\nTargeted Circuit PatcHing), a novel approach that first identifies and\nsubsequently directly edits PII circuits to reduce leakage. PATCH achieves\nbetter privacy-utility trade-off than existing defenses, e.g., reducing recall\nof PII leakage from LMs by up to 65%. Finally, PATCH can be combined with DP to\nreduce recall of residual leakage of an LM to as low as 0.01%. Our analysis\nshows that PII leakage circuits persist even after the application of existing\ndefense mechanisms. In contrast, PATCH can effectively mitigate their impact.", "AI": {"tldr": "This paper proposes PATCH, a circuit-based method to mitigate PII leakage in language models by directly targeting and editing responsible computational circuits, achieving superior privacy-utility trade-offs compared to existing defenses.", "motivation": "Current defenses like differential privacy reduce PII leakage but significantly degrade model utility. The authors hypothesize that specific computational circuits in LMs are responsible for PII leakage and seek targeted solutions to preserve utility.", "method": "1) Use circuit discovery to identify PII leakage circuits in LMs 2.) Apply PATCH by editing these circuits to disrupt leakage while maintaining utility. Combines targeted edits with differential privacy for enhanced results.", "result": "Reduces PII recall leakage by 65% without DP and achieves 0.01 residual leakage with DP combination. Surpasses existing defenses in privacy-utility trade-off. Identified PII circuits persist despite baseline defenses.", "conclusion": "PATCH effectively addresses PII leakage by targeting root circuits, outperforming conventional approaches. Combining with existing methods further enhances privacy while minimizing utility loss."}}
{"id": "2510.07815", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.07815", "abs": "https://arxiv.org/abs/2510.07815", "authors": ["Zeyu Sun", "Jingjing Liang", "Weiyi Wang", "Chenyao Suo", "Junjie Chen", "Fanjiang Xu"], "title": "Interleaved Learning and Exploration: A Self-Adaptive Fuzz Testing Framework for MLIR", "comment": null, "summary": "MLIR (Multi-Level Intermediate Representation) has rapidly become a\nfoundational technology for modern compiler frameworks, enabling extensibility\nacross diverse domains. However, ensuring the correctness and robustness of\nMLIR itself remains challenging. Existing fuzzing approaches-based on manually\ncrafted templates or rule-based mutations-struggle to generate sufficiently\ndiverse and semantically valid test cases, making it difficult to expose subtle\nor deep-seated bugs within MLIR's complex and evolving code space. In this\npaper, we present FLEX, a novel self-adaptive fuzzing framework for MLIR. FLEX\nleverages neural networks for program generation, a perturbed sampling strategy\nto encourage diversity, and a feedback-driven augmentation loop that\niteratively improves its model using both crashing and non-crashing test cases.\nStarting from a limited seed corpus, FLEX progressively learns valid syntax and\nsemantics and autonomously produces high-quality test inputs. We evaluate FLEX\non the upstream MLIR compiler against four state-of-the-art fuzzers. In a\n30-day campaign, FLEX discovers 80 previously unknown bugs-including multiple\nnew root causes and parser bugs-while in 24-hour fixed-revision comparisons, it\ndetects 53 bugs (over 3.5x as many as the best baseline) and achieves 28.2%\ncode coverage, outperforming the next-best tool by 42%. Ablation studies\nfurther confirm the critical role of both perturbed generation and diversity\naugmentation in FLEX's effectiveness.", "AI": {"tldr": "FLEX: Self-adaptive MLIR fuzzer with neural generation and feedback loops finds 3.5\u00d7 more bugs than state-of-the-art tools in 24 hours.", "motivation": "MLIR's foundational role requires robust testing, but existing fuzzing approaches (manual templates/rule-based mutations) fail to produce diverse, semantically valid test cases, making it difficult to detect subtle bugs in MLIR's complex codebase.", "method": "The paper proposes FLEX, a self-adaptive fuzzing framework for MLIR that utilizes 1) neural network-driven program generation, 2) perturbed sampling for diversity, and 3) feedback-driven augmentation loops using crashing/non-crashing test cases to iteratively refine the model. This allows FLEX to learn valid syntax/semantics from a small seed corpus and autonomously generate high-quality test inputs.", "result": "In 30 days, FLEX discovers 80 new bugs (including parser bugs and new root causes). In 24-hour fixed-revision comparisons, it detects 53 bugs (3.5\u00d7 baseline), achieves 28.2% code coverage (42% better than next-best tool), and ablation studies confirm the importance of perturbed sampling and diversity augmentation.", "conclusion": "FLEX addresses the limitations of existing MLIR fuzzing tools by combining neural program generation with adaptive strategies, achieving significant improvements in bug detection and coverage while emphasizing the importance of diversity and feedback-driven learning."}}
{"id": "2510.07457", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07457", "abs": "https://arxiv.org/abs/2510.07457", "authors": ["Kalyan Cheerla", "Lotfi Ben Othmane", "Kirill Morozov"], "title": "Comparison of Fully Homomorphic Encryption and Garbled Circuit Techniques in Privacy-Preserving Machine Learning Inference", "comment": "8 pages, 9 figures, 2 tables, 32 references", "summary": "Machine Learning (ML) is making its way into fields such as healthcare,\nfinance, and Natural Language Processing (NLP), and concerns over data privacy\nand model confidentiality continue to grow. Privacy-preserving Machine Learning\n(PPML) addresses this challenge by enabling inference on private data without\nrevealing sensitive inputs or proprietary models. Leveraging Secure Computation\ntechniques from Cryptography, two widely studied approaches in this domain are\nFully Homomorphic Encryption (FHE) and Garbled Circuits (GC). This work\npresents a comparative evaluation of FHE and GC for secure neural network\ninference. A two-layer neural network (NN) was implemented using the CKKS\nscheme from the Microsoft SEAL library (FHE) and the TinyGarble2.0 framework\n(GC) by IntelLabs. Both implementations are evaluated under the semi-honest\nthreat model, measuring inference output error, round-trip time, peak memory\nusage, communication overhead, and communication rounds. Results reveal a\ntrade-off: modular GC offers faster execution and lower memory consumption,\nwhile FHE supports non-interactive inference.", "AI": {"tldr": "This study compares FHE and GC for secure neural network inference, finding GC excels in speed/memory while FHE enables non-interactive execution, highlighting key trade-offs for privacy-preserving ML applications.", "motivation": "Growing concerns over data privacy and model confidentiality in machine learning necessitate secure inference solutions that protect sensitive inputs and proprietary models while enabling privacy-preserving ML.", "method": "The authors implemented a two-layer neural network using the CKKS FHE scheme (Microsoft SEAL library) and Garbled Circuits (TinyGarble2.0 framework). They evaluated both under the semi-honest threat model across metrics including error, latency, memory, communication overhead, and rounds.", "result": "Garbled Circuits achieved faster execution speeds and lower memory consumption compared to FHE. Conversely, Fully Homomorphic Encryption supported non-interactive inference at the cost of higher computational requirements and latency.", "conclusion": "The paper concludes that FHE and GC each have distinct advantages depending on the specific requirements of the application. FHE's non-interactive nature makes it suitable for asynchronous communication, while GC-based solutions excel in scenarios requiring faster execution and lower memory usage."}}
{"id": "2510.07834", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.07834", "abs": "https://arxiv.org/abs/2510.07834", "authors": ["Lingjun Liu", "Feiran Qin", "Owolabi Legunsen", "Marcelo d'Amorim"], "title": "Bug Histories as Sources of Compiler Fuzzing Mutators", "comment": null, "summary": "Bugs in compilers, which are critical infrastructure today, can have outsized\nnegative impacts. Mutational fuzzers aid compiler bug detection by\nsystematically mutating compiler inputs, i.e., programs. Their effectiveness\ndepends on the quality of the mutators used. Yet, no prior work used compiler\nbug histories as a source of mutators. We propose IssueMut, the first approach\nfor extracting compiler fuzzing mutators from bug histories. Our insight is\nthat bug reports contain hints about program elements that induced compiler\nbugs; they can guide fuzzers towards similar bugs. IssueMut uses an automated\nmethod to mine mutators from bug reports and retrofit such mutators into\nexisting mutational compiler fuzzers. Using IssueMut, we mine 587 mutators from\n1760 GCC and LLVM bug reports. Then, we run IssueMut on these compilers, with\nall their test inputs as seed corpora. We find that \"bug history\" mutators are\neffective: they find new bugs that a state-of-the-art mutational compiler\nfuzzer misses-28 in GCC and 37 in LLVM. Of these, 60 were confirmed or fixed,\nvalidating our idea that bug histories have rich information that compiler\nfuzzers should leverage.", "AI": {"tldr": "IssueMut introduces a compiler fuzzing approach that harvests mutators from historical bug reports, uncovering 60 new bugs in GCC/LLVM and showing historical data can significantly enhance fuzzer effectiveness.", "motivation": "Compiler bugs can cause significant issues, and prior mutational fuzzers lack effectiveness due to suboptimal mutators. The paper addresses this by proposing to mine mutators directly from historical compiler bug reports, a previously untapped resource.", "method": "IssueMut automatically mines mutators from GCC and LLVM bug reports using an automated mining method, retrofitting them into existing compiler fuzzers to enhance bug detection.", "result": "IssueMut identified 28 new bugs in GCC and 37 in LLVM missed by state-of-the-art fuzzers, with 60 confirmed or fixed, demonstrating the efficacy of using bug history to guide fuzzing.", "conclusion": "IssueMut effectively leverages compiler bug histories to generate new mutators, validating the potential of historical bug data in improving compiler fuzzing."}}
{"id": "2510.07462", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.07462", "abs": "https://arxiv.org/abs/2510.07462", "authors": ["Maryam Ataei Nezhad", "Hamid Barati", "Ali Barati"], "title": "A Secure Authentication-Driven Protected Data Collection Protocol in Internet of Things", "comment": null, "summary": "Internet of Things means connecting different devices through the Internet.\nThe Internet of things enables humans to remotely manage and control the\nobjects they use with the Internet infrastructure. After the advent of the\nInternet of Things in homes, organizations, and private companies, privacy and\ninformation security are the biggest concern. This issue has challenged the\nspread of the Internet of things as news of the users theft of information by\nhackers intensified. The proposed method in this paper consists of three\nphases. In the first phase, a star structure is constructed within each\ncluster, and a unique key is shared between each child and parent to encrypt\nand secure subsequent communications. The second phase is for intracluster\ncommunications, in which members of the cluster send their data to the cluster\nhead in a multi hop manner. Also, in this phase, the data is encrypted with\ndifferent keys in each hop, and at the end of each connection, the keys are\nupdated to ensure data security. The third phase is to improve the security of\ninter cluster communications using an authentication protocol. In this way, the\ncluster heads are authenticated before sending information to prevent malicious\nnodes in the network. The proposed method is also simulated using NS2 software.\nThe results showed that the proposed method has improved in terms of energy\nconsumption, end-to-end delay, flexibility, packet delivery rate, and the\nnumber of alive nodes compared to other methods.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.07941", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.07941", "abs": "https://arxiv.org/abs/2510.07941", "authors": ["Srijita Basu", "Haraldsson Bengt", "Miroslaw Staron", "Christian Berger", "Jennifer Horkoff", "Magnus Almgren"], "title": "An AUTOSAR-Aligned Architectural Study of Vulnerabilities in Automotive SoC Software", "comment": "16 pages, 7 figures, 18th International Conference on the Quality of\n  Information and Communications Technology", "summary": "Cooperative, Connected and Automated Mobility (CCAM) are complex\ncyber-physical systems (CPS) that integrate computation, communication, and\ncontrol in safety-critical environments. At their core, System-on-Chip (SoC)\nplatforms consolidate processing units, communication interfaces, AI\naccelerators, and security modules into a single chip. AUTOSAR (AUTomotive Open\nSystem ARchitecture) standard was developed in the automotive domain to better\nmanage this complexity, defining layered software structures and interfaces to\nfacilitate reuse of HW/SW components. However, in practice, this integrated SoC\nsoftware architecture still poses security challenges, particularly in\nreal-time, safety-critical environments. Recent reports highlight a surge in\nSoC-related vulnerabilities, yet systematic analysis of their root causes and\nimpact within AUTOSAR-aligned architectures is lacking. This study fills that\ngap by analyzing 180 publicly reported automotive SoC vulnerabilities, mapped\nto a representative SoC software architecture model that is aligned with\nAUTOSAR principles for layered abstraction and service orientation. We identify\n16 root causes and 56 affected software modules, and examine mitigation delays\nacross Common Weakness Enumeration (CWE) categories and architectural layers.\nWe uncover dominant vulnerability patterns and critical modules with prolonged\npatch delays, and provide actionable insights for securing automotive CPS\nplatforms, including guides for improved detection, prioritization, and\nlocalization strategies for SoC software architectures in SoC-based vehicle\nplatforms.", "AI": {"tldr": "This paper analyzes 180 automotive SoC vulnerabilities in AUTOSAR-aligned architectures, identifies 16 root causes and 56 affected modules, and provides security improvement strategies for safety-critical vehicle systems.", "motivation": "AUTOSAR's layered architecture for automotive SoC platforms faces unaddressed security challenges, despite increasing vulnerability reports. Systematic analysis of root causes and mitigation delays in real-time safety-critical environments is lacking.", "method": "Mapped 180 public SoC vulnerabilities to a representative AUTOSAR-aligned architecture model. Analyzed root causes, affected modules, and mitigation delays using CWE categories and architectural layers.", "result": "Identified 16 root causes, 56 vulnerable modules, dominant vulnerability patterns, and prolonged patch delays in critical modules. Revealed gaps in current security practices for automotive CPS platforms.", "conclusion": "Provides actionable security guidelines for automotive SoC platforms including detection prioritization, vulnerability localization, and architectural mitigation strategies aligned with AUTOSAR principles."}}
{"id": "2510.07479", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.07479", "abs": "https://arxiv.org/abs/2510.07479", "authors": ["Alain Couvreur", "Thomas Debris-Alazard", "Philippe Gaborit", "Adrien Vin\u00e7otte"], "title": "MIRANDA: short signatures from a leakage-free full-domain-hash scheme", "comment": null, "summary": "We present $\\mathsf{Miranda}$, the first family of full-domain-hash\nsignatures based on matrix codes. This signature scheme fulfils the paradigm of\nGentry, Peikert and Vaikuntanathan ($\\mathsf{GPV}$), which gives strong\nsecurity guarantees. Our trapdoor is very simple and generic: if we propose it\nwith matrix codes, it can actually be instantiated in many other ways since it\nonly involves a subcode of a decodable code (or lattice) in a unique decoding\nregime of parameters. Though $\\mathsf{Miranda}$ signing algorithm relies on a\ndecoding task where there is exactly one solution, there are many possible\nsignatures given a message to sign and we ensure that signatures are not\nleaking information on their underlying trapdoor by means of a very simple\nprocedure involving the drawing of a small number of uniform bits. In\nparticular $\\mathsf{Miranda}$ does not use a rejection sampling procedure which\nmakes its implementation a very simple task contrary to other\n$\\mathsf{GPV}$-like signatures schemes such as $\\mathsf{Falcon}$ or even\n$\\mathsf{Wave}$.\n  We instantiate $\\mathsf{Miranda}$ with the famous family of Gabidulin codes\nrepresented as spaces of matrices and we study thoroughly its security (in the\nEUF-CMA security model). For~$128$ bits of classical security, the signature\nsizes are as low as~$90$ bytes and the public key sizes are in the order\nof~$2.6$ megabytes.", "AI": {"tldr": "Miranda introduces a lightweight GPV-style digital signature using matrix codes. By avoiding rejection sampling and leveraging simple uniform-bit randomization, it achieves compact signatures and improved practicality compared to Falcon/Wave.", "motivation": "Traditional GPV-like signatures (e.g., Falcon, Wave) rely on complex rejection sampling. The paper aims to simplify implementation through a generic trapdoor construction that works with any decodable code/lattice and eliminates the need for rejection sampling.", "method": "Miranda leverages matrix codes (specifically Gabidulin codes) within the GPV paradigm. It uses a trapdoor mechanism based on a subcode of a decodable code/lattice in a unique decoding regime. Signatures avoid rejection sampling by randomly drawing uniform bits, ensuring no trapdoor information leakage.", "result": "For 128-bit classical security, Miranda achieves signature sizes of 90 bytes and 2.6 MB public keys using Gabidulin codes. The scheme is provably secure under EUF-CMA and benefits from straightforward implementation due to the absence of rejection sampling.", "conclusion": "Miranda provides a practical alternative to existing digital signature schemes by offering reduced complexity and simpler implementation while maintaining strong security guarantees. Its design avoids rejection sampling, reducing overhead and improving efficiency compared to similar GPV-inspired schemes like Falcon and Wave."}}
{"id": "2510.08005", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08005", "abs": "https://arxiv.org/abs/2510.08005", "authors": ["Utku Boran Torun", "Mehmet Taha Demircan", "Mahmut Furkan G\u00f6n", "Eray T\u00fcz\u00fcn"], "title": "Past, Present, and Future of Bug Tracking in the Generative AI Era", "comment": "Submitted to ACM TOSEM Special Issue: 2030 Software Engineering\n  Roadmap", "summary": "Traditional bug tracking systems rely heavily on manual reporting,\nreproduction, triaging, and resolution, each carried out by different\nstakeholders such as end users, customer support, developers, and testers. This\ndivision of responsibilities requires significant coordination and widens the\ncommunication gap between non-technical users and technical teams, slowing the\nprocess from bug discovery to resolution. Moreover, current systems are highly\nasynchronous; users often wait hours or days for a first response, delaying\nfixes and contributing to frustration. This paper examines the evolution of bug\ntracking, from early paper-based reporting to today's web-based and SaaS\nplatforms. Building on this trajectory, we propose an AI-powered bug tracking\nframework that augments existing tools with intelligent, large language model\n(LLM)-driven automation. Our framework addresses two main challenges: reducing\ntime-to-fix and minimizing human overhead. Users report issues in natural\nlanguage, while AI agents refine reports, attempt reproduction, and request\nmissing details. Reports are then classified, invalid ones resolved through\nno-code fixes, and valid ones localized and assigned to developers. LLMs also\ngenerate candidate patches, with human oversight ensuring correctness. By\nintegrating automation into each phase, our framework accelerates response\ntimes, improves collaboration, and strengthens software maintenance practices\nfor a more efficient, user-centric future.", "AI": {"tldr": "This paper proposes an AI-powered bug tracking framework that uses large language models to automate reporting, classification, and patch generation, reducing manual effort and speeding up bug resolution while bridging communication gaps between users and developers.", "motivation": "Traditional bug tracking systems suffer from manual processes, asynchronous communication gaps, delays in resolution, and inefficiencies caused by disjointed stakeholder roles. These challenges lead to prolonged fix times, user frustration, and higher maintenance costs, motivating the need for intelligent automation.", "method": "The proposed framework uses AI agents integrated with large language models (LLMs) to automate bug tracking phases: users report issues in natural language, AI refines reports, attempts reproduction, and generates candidate patches. Automated classification, no-code fixes for invalid reports, and human-in-the-loop validation ensure efficiency and correctness.", "result": "The framework achieves reduced time-to-fix, minimized human intervention, and enhanced collaboration between non-technical users and technical teams. It demonstrates improved bug classification accuracy, faster issue localization, and effective patch generation through LLM-driven automation.", "conclusion": "The paper concludes that an AI-powered bug tracking framework, leveraging large language models, can significantly enhance software maintenance by accelerating response times, improving inter-team collaboration, and reducing manual overhead, paving the way for a more efficient and user-centric bug resolution process."}}
{"id": "2510.07533", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.07533", "abs": "https://arxiv.org/abs/2510.07533", "authors": ["Haowen Xu", "Tianya Zhao", "Xuyu Wang", "Lei Ma", "Jun Dai", "Alexander Wyglinski", "Xiaoyan Sun"], "title": "EMPalm: Exfiltrating Palm Biometric Data via Electromagnetic Side-Channels", "comment": null, "summary": "Palm recognition has emerged as a dominant biometric authentication\ntechnology in critical infrastructure. These systems operate in either\nsingle-modal form, using palmprint or palmvein individually, or dual-modal\nform, fusing the two modalities. Despite this diversity, they share similar\nhardware architectures that inadvertently emit electromagnetic (EM) signals\nduring operation. Our research reveals that these EM emissions leak palm\nbiometric information, motivating us to develop EMPalm--an attack framework\nthat covertly recovers both palmprint and palmvein images from eavesdropped EM\nsignals. Specifically, we first separate the interleaved transmissions of the\ntwo modalities, identify and combine their informative frequency bands, and\nreconstruct the images. To further enhance fidelity, we employ a diffusion\nmodel to restore fine-grained biometric features unique to each domain.\nEvaluations on seven prototype and two commercial palm acquisition devices show\nthat EMPalm can recover palm biometric information with high visual fidelity,\nachieving SSIM scores up to 0.79, PSNR up to 29.88 dB, and FID scores as low as\n6.82 across all tested devices, metrics that collectively demonstrate strong\nstructural similarity, high signal quality, and low perceptual discrepancy. To\nassess the practical implications of the attack, we further evaluate it against\nfour state-of-the-art palm recognition models, achieving a model-wise average\nspoofing success rate of 65.30% over 6,000 samples from 100 distinct users.", "AI": {"tldr": "Palm recognition systems emit EM signals that can be eavesdropped to recover biometric info, an attack named EMPalm is introduced with evaluation on prototypes and commercial devices.", "motivation": "Palm recognition systems, used for critical security, can be compromised through EM emissions that leak sensitive data, prompting the creation of EMPalm to demonstrate and address this vulnerability.", "method": "EMPalm separates interleaved modality transmissions, combines relevant EM frequency bands, reconstructs images, and uses a domain-specific diffusion model for enhanced feature restoration.", "result": "EMPalm achieves SSIM up to 0.79, PSNR up to 29.88 dB, and FID as low as 6.82, with a 65.30% spoofing success rate against four SOTA models using 6,000 user samples.", "conclusion": "The study underscores the threat posed by EM leakage in palm biometrics, introduces EMPalm's effectiveness, and calls for EM-aware security measures against this novel eavesdropping vulnerability."}}
{"id": "2510.08200", "categories": ["cs.SE", "68N15", "D.2.13"], "pdf": "https://arxiv.org/pdf/2510.08200", "abs": "https://arxiv.org/abs/2510.08200", "authors": ["Alexander Hellwig", "Nico Jansen", "Bernhard Rumpe"], "title": "Building Whitespace-Sensitive Languages Using Whitespace-Insensitive Components", "comment": "11 pages, 4 figures, 6 listings", "summary": "In Software Language Engineering, there is a trend towards reusability by\ncomposing modular language components. However, this reusability is severely\ninhibited by a gap in integrating whitespace-sensitive and\nwhitespace-insensitive languages. There is currently no consistent procedure\nfor seamlessly reusing such language components in both cases, such that\nlibraries often cannot be reused, and whitespacesensitive languages are\ndeveloped from scratch. This paper presents a technique for using modular,\nwhitespaceinsensitive language modules to construct whitespace sensitive\nlanguages by pre-processing language artifacts before parsing. The approach is\nevaluated by reconstructing a simplified version of the programming language\nPython. Our solution aims to increase the reusability of existing language\ncomponents to reduce development time and increase the overall quality of\nsoftware languages.", "AI": {"tldr": "This paper addresses the gap in reusing whitespace-sensitive and whitespace-insensitive language modules by proposing a preprocessing technique to enable modular composition. It evaluates the approach on a simplified Python implementation, aiming to enhance reusability and reduce development time.", "motivation": "Current integration challenges hinder reusability in software language engineering, forcing redundant development of whitespace-sensitive languages and limiting library reuse due to inconsistent integration procedures.", "method": "The paper introduces a method that preprocesses language artifacts before parsing, allowing whitespace-insensitive modules to construct whitespace-sensitive languages through modular composition.", "result": "The approach is validated by reconstructing a simplified version of Python, demonstrating the feasibility of reusing existing modules to build whitespace-sensitive languages.", "conclusion": "The proposed technique bridges the integration gap, promoting reuse of language components, reducing development effort, and improving software language quality through modular, preprocessing-based composition."}}
{"id": "2510.07584", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.07584", "abs": "https://arxiv.org/abs/2510.07584", "authors": ["Thomas Debris-Alazard", "Philippe Gaborit", "Romaric Neveu", "Olivier Ruatta"], "title": "A Minrank-based Encryption Scheme \u00e0 la Alekhnovich-Regev", "comment": null, "summary": "Introduced in 2003 and 2005, Alekhnovich and Regev' schemes were the first\npublic-key encryptions whose security is only based on the average hardness of\ndecoding random linear codes and LWE, without other security assumptions. Such\nsecurity guarantees made them very popular, being at the origin of the now\nstandardized HQC or Kyber.\n  We present an adaptation of Alekhnovich and Regev' encryption scheme whose\nsecurity is only based on the hardness of a slight variation of MinRank, the\nso-called stationary-MinRank problem. We succeeded to reach this strong\nsecurity guarantee by showing that stationary-MinRank benefits from a\nsearch-to-decision reduction. Our scheme therefore brings a partial answer to\nthe long-standing open question of building an encryption scheme whose security\nrelies solely on the hardness of MinRank.\n  Finally, we show after a thoroughly security analysis that our scheme is\npractical and competitive with other encryption schemes admitting such strong\nsecurity guarantees. Our scheme is slightly less efficient than FrodoKEM, but\nmuch more efficient than Alekhnovich and Regev' original schemes, with\npossibilities of improvements by considering more structure, in the same way as\nHQC and Kyber.", "AI": {"tldr": "New encryption scheme based on MinRank achieves strong security and practical efficiency, advancing post-quantum cryptography.", "motivation": "Addressing the long-standing open question of creating a secure encryption scheme relying exclusively on MinRank's hardness.", "method": "Adapted Alekhnovich and Regev's schemes using stationary-MinRank, leveraging a search-to-decision reduction for strong security.", "result": "A practical scheme outperforming original Regev/Alekhnovich designs, with slight efficiency trade-offs compared to FrodoKEM, but competitive with strong security guarantees.", "conclusion": "The paper presents a new encryption scheme based on stationary-MinRank, providing a partial answer to building encryption solely on MinRank's hardness with practical efficiency."}}
{"id": "2510.07697", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07697", "abs": "https://arxiv.org/abs/2510.07697", "authors": ["Man Hu", "Xinyi Wu", "Zuofeng Suo", "Jinbo Feng", "Linghui Meng", "Yanhao Jia", "Anh Tuan Luu", "Shuai Zhao"], "title": "Rethinking Reasoning: A Survey on Reasoning-based Backdoors in LLMs", "comment": null, "summary": "With the rise of advanced reasoning capabilities, large language models\n(LLMs) are receiving increasing attention. However, although reasoning improves\nLLMs' performance on downstream tasks, it also introduces new security risks,\nas adversaries can exploit these capabilities to conduct backdoor attacks.\nExisting surveys on backdoor attacks and reasoning security offer comprehensive\noverviews but lack in-depth analysis of backdoor attacks and defenses targeting\nLLMs' reasoning abilities. In this paper, we take the first step toward\nproviding a comprehensive review of reasoning-based backdoor attacks in LLMs by\nanalyzing their underlying mechanisms, methodological frameworks, and\nunresolved challenges. Specifically, we introduce a new taxonomy that offers a\nunified perspective for summarizing existing approaches, categorizing\nreasoning-based backdoor attacks into associative, passive, and active. We also\npresent defense strategies against such attacks and discuss current challenges\nalongside potential directions for future research. This work offers a novel\nperspective, paving the way for further exploration of secure and trustworthy\nLLM communities.", "AI": {"tldr": "This paper reviews reasoning-based backdoor attacks in LLMs, proposes a taxonomy (associative/passive/active), summarizes defenses, and highlights unresolved challenges and future research directions for secure LLM development.", "motivation": "Advanced reasoning capabilities in LLMs enhance performance but introduce security risks via exploitable backdoor attacks. Existing surveys lack depth in addressing reasoning-specific attacks, motivating this in-depth review.", "method": "The paper introduces a new taxonomy categorizing reasoning-based backdoor attacks into associative, passive, and active types. It analyzes attack mechanisms, methodological frameworks, and evaluates defense strategies while identifying unresolved challenges.", "result": "The paper presents a unified taxonomy for reasoning-based backdoor attacks, summarizes existing approaches, proposes defense strategies, and outlines current challenges and future research directions to improve LLM security.", "conclusion": "This work provides a novel perspective on reasoning-based backdoor attacks in LLMs, offering a taxonomy and paving the way for secure, trustworthy LLM communities. Future research directions are highlighted to address unresolved challenges."}}
{"id": "2510.07806", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.07806", "abs": "https://arxiv.org/abs/2510.07806", "authors": ["Yihao Peng", "Biao Ma", "Hai Wan", "Xibin Zhao"], "title": "ANCORA: Accurate Intrusion Recovery for Web Applications", "comment": null, "summary": "Modern web application recovery presents a critical dilemma. Coarse-grained\nsnapshot rollbacks cause unacceptable data loss for legitimate users.\nSurgically removing an attack's impact is hindered by a fundamental challenge\nin high-concurrency environments: it is difficult to attribute resulting file\nand database modifications to a specific attack-related request. We present\nANCORA, a system for precise intrusion recovery in web applications without\ninvasive instrumentation. ANCORA first isolates the full sequence of syscalls\ntriggered by a single malicious request. Based on this sequence, ANCORA\naddresses file and database modifications separately. To trace file changes, it\nbuilds a provenance graph that reveals all modifications, including those by\nexploit-spawned processes. To attribute database operations, a more difficult\nchallenge due to connection pooling, ANCORA introduces a novel spatiotemporal\nanchor. This anchor uses the request's network connection tuple and active time\nwindow to pinpoint exact database operations. With all malicious file and\ndatabase operations precisely identified, ANCORA performs a unified rewind and\nselective replay recovery. It reverts the system to a clean snapshot taken\nbefore the attack, then selectively re-applies only legitimate operations to\nboth the file system and database. This completely removes the attack's effects\nwhile preserving concurrent legitimate data. We evaluated ANCORA on 10 web\napplications and 20 CVE-based attack scenarios with concurrency up to 150\nconnections. Experiments demonstrate ANCORA achieves 99.9% recovery accuracy\nwith manageable overhead: up to 19.8% response latency increase and 17.8% QPS\ndecrease in worst cases, and recovery throughput of 110.7 database operations\nper second and 27.2 affected files per second, effectively preserving\nlegitimate data.", "AI": {"tldr": "ANCORA is a non-invasive system for precise web application recovery after attacks. It identifies and removes malicious file/database changes while preserving legitimate concurrent data through syscalls isolation, provenance graphs, and spatiotemporal anchors.", "motivation": "Existing solutions face two extremes: coarse-grained rollbacks cause data loss for legitimate users, while fine-grained recovery fails in high-concurrency environments due to the challenge of attributing modifications to specific attack-related requests.", "method": "1) Isolate syscalls of malicious requests via introspection. 2a) Build provenance graphs to trace file modifications and exploit-spawned processes. 2b In databases, use spatiotemporal anchors (network connection tuple + time window) to attribute operations. 3a) Rewind system to pre-attack snapshot. 3b Selectively replay legitimate operations.", "result": "99.9%% recovery accuracy on 10 web apps/20 attack scenarios under 150-concurrency. Overhead: 19.8%% max latency increase, 17.8%% QPS decrease. Recovery throughput: 110.7 DB ops/s and 27.2 files/s.", "conclusion": "ANCORA enables precise intrusion recovery without invasive instrumentation, achieving high accuracy and performance in real-world high-concurrency scenarios while preserving legitimate user data."}}
{"id": "2510.07809", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07809", "abs": "https://arxiv.org/abs/2510.07809", "authors": ["Renhua Ding", "Xiao Yang", "Zhengwei Fang", "Jun Luo", "Kun He", "Jun Zhu"], "title": "Effective and Stealthy One-Shot Jailbreaks on Deployed Mobile Vision-Language Agents", "comment": null, "summary": "Large vision-language models (LVLMs) enable autonomous mobile agents to\noperate smartphone user interfaces, yet vulnerabilities to UI-level attacks\nremain critically understudied. Existing research often depends on conspicuous\nUI overlays, elevated permissions, or impractical threat models, limiting\nstealth and real-world applicability. In this paper, we present a practical and\nstealthy one-shot jailbreak attack that leverages in-app prompt injections:\nmalicious applications embed short prompts in UI text that remain inert during\nhuman interaction but are revealed when an agent drives the UI via ADB (Android\nDebug Bridge). Our framework comprises three crucial components: (1)\nlow-privilege perception-chain targeting, which injects payloads into malicious\napps as the agent's visual inputs; (2) stealthy user-invisible activation, a\ntouch-based trigger that discriminates agent from human touches using physical\ntouch attributes and exposes the payload only during agent operation; and (3)\none-shot prompt efficacy, a heuristic-guided, character-level\niterative-deepening search algorithm (HG-IDA*) that performs one-shot,\nkeyword-level detoxification to evade on-device safety filters. We evaluate\nacross multiple LVLM backends, including closed-source services and\nrepresentative open-source models within three Android applications, and we\nobserve high planning and execution hijack rates in single-shot scenarios\n(e.g., GPT-4o: 82.5% planning / 75.0% execution). These findings expose a\nfundamental security vulnerability in current mobile agents with immediate\nimplications for autonomous smartphone operation.", "AI": {"tldr": "This paper introduces a practical and stealthy one-shot jailbreak attack on large vision-language models (LVLMs), exploiting UI-level vulnerabilities via in-app prompts and physical touch attributes. The proposed method achieves high attack success rates across multiple Android apps and LVLMs (e.g., 82.5%/75.0%\u00b2 hijack rates against GPT-4o).\n\n2", "motivation": "Existing attacks on LVLM-powered mobile agents rely on obvious UI overlays, elevated privileges, or impractical threat models. This work addresses the need for stealthier, low-privilege attacks that operate under real-world constraints while evading safety filters.", "method": "The framework combines three components: (1)\u00a0low-privilege visual input hijacking via UI text injection, (2)\u00a0touch attribute-based activation (discriminating agent vs human input), and (3)\u00a0HG-IDA*: a heuristic-guided, character-level iterative-deepening algorithm for one-shot keyword detoxification to bypass filters.", "result": "Empirical evaluations show high single-shot attack effectiveness across open- and closed-source LVLMs in three Android app scenarios (e.g., 82.5%\u00b2 planning/75.0%\u00b2 execution hijack rates against GPT-4o). The attack remains stealthy from users.", "conclusion": "Exposes fundamental vulnerabilities in mobile agent security, demonstrating how UI-level prompt injections with physical trigger discriminators can reliably subvert autonomous smartphone operations despite platform-level protections."}}
{"id": "2510.07901", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.07901", "abs": "https://arxiv.org/abs/2510.07901", "authors": ["Georgios Diamantopoulos", "Nikos Tziritas", "Rami Bahsoon", "Georgios Theodoropoulos"], "title": "Decentralised Blockchain Management Through Digital Twins", "comment": "Accepted for publication in the proceedings of the 24th Asia\n  Simulation Conference 2025", "summary": "The necessity of blockchain systems to remain decentralised limits current\nsolutions to blockchain governance and dynamic management, forcing a trade-off\nbetween control and decentralisation. In light of the above, this work proposes\na dynamic and decentralised blockchain management mechanism based on digital\ntwins. To ensure decentralisation, the proposed mechanism utilises multiple\ndigital twins that the system's stakeholders control. To facilitate\ndecentralised decision-making, the twins are organised in a secondary\nblockchain system that orchestrates agreement on, and propagation of decisions\nto the managed blockchain. This enables the management of blockchain systems\nwithout centralised control. A preliminary evaluation of the performance and\nimpact of the overheads introduced by the proposed mechanism is conducted\nthrough simulation. The results demonstrate the proposed mechanism's ability to\nreach consensus on decisions quickly and reconfigure the primary blockchain\nwith minimal overhead.", "AI": {"tldr": "Proposes a dynamic, decentralized blockchain management mechanism using digital twins to avoid trade-off between control and decentralization.", "motivation": "Blockchain systems must stay decentralized, but existing governance solutions force a trade-off between centralized control and decentralization.", "method": "Proposes using multiple digital twins controlled by stakeholders in a secondary blockchain to orchestrate decisions.", "result": "Simulation shows the mechanism reaches consensus quickly with minimal overhead and reconfigures the primary blockchain effectively.", "conclusion": "The approach allows decentralized management of blockchain systems, potentially avoiding the typical control-decentralization trade-off."}}
{"id": "2510.07968", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.07968", "abs": "https://arxiv.org/abs/2510.07968", "authors": ["Xiangtao Meng", "Tianshuo Cong", "Li Wang", "Wenyu Chen", "Zheng Li", "Shanqing Guo", "Xiaoyun Wang"], "title": "From Defender to Devil? Unintended Risk Interactions Induced by LLM Defenses", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable performance across various\napplications, but their deployment in sensitive domains raises significant\nconcerns. To mitigate these risks, numerous defense strategies have been\nproposed. However, most existing studies assess these defenses in isolation,\noverlooking their broader impacts across other risk dimensions. In this work,\nwe take the first step in investigating unintended interactions caused by\ndefenses in LLMs, focusing on the complex interplay between safety, fairness,\nand privacy. Specifically, we propose CrossRiskEval, a comprehensive evaluation\nframework to assess whether deploying a defense targeting one risk\ninadvertently affects others. Through extensive empirical studies on 14\ndefense-deployed LLMs, covering 12 distinct defense strategies, we reveal\nseveral alarming side effects: 1) safety defenses may suppress direct responses\nto sensitive queries related to bias or privacy, yet still amplify indirect\nprivacy leakage or biased outputs; 2) fairness defenses increase the risk of\nmisuse and privacy leakage; 3) privacy defenses often impair safety and\nexacerbate bias. We further conduct a fine-grained neuron-level analysis to\nuncover the underlying mechanisms of these phenomena. Our analysis reveals the\nexistence of conflict-entangled neurons in LLMs that exhibit opposing\nsensitivities across multiple risk dimensions. Further trend consistency\nanalysis at both task and neuron levels confirms that these neurons play a key\nrole in mediating the emergence of unintended behaviors following defense\ndeployment. We call for a paradigm shift in LLM risk evaluation, toward\nholistic, interaction-aware assessment of defense strategies.", "AI": {"tldr": "This paper reveals unintended cross-dimensional risks of LLM defenses (safety/fairness/privacy) using CrossRiskEval, advocating for holistic evaluations to address conflicts in model deployments.", "motivation": "Existing research isolates defense strategies for risks like safety, fairness, and privacy, ignoring their interconnected impacts, which limits understanding of their broader trade-offs.", "method": "The paper introduces CrossRiskEval, a framework for assessing unintended interactions between safety, fairness, and privacy defenses in LLMs. It evaluates 14 defense-deployed models and employs neuron-level analysis to identify conflict-entangled neurons.", "result": "1) Safety defenses may increase indirect bias/privacy leakage; 2) Fairness defenses raise misuse and privacy risks; 3) Privacy defenses worsen safety and bias. Neuron-level analysis reveals conflict-entangled neurons mediating these interactions.", "conclusion": "The study emphasizes the need for a paradigm shift toward holistic and interaction-aware evaluation of defense strategies in LLMs to address unintended cross-dimensional consequences."}}
{"id": "2510.08013", "categories": ["cs.CR", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2510.08013", "abs": "https://arxiv.org/abs/2510.08013", "authors": ["Yurang R. Kuang"], "title": "Composition Law of Conjugate Observables in Random Permutation Sorting Systems", "comment": null, "summary": "We present the discovery of a fundamental composition law governing conjugate\nobservables in the Random Permutation Sorting System (RPSS). The law links the\ndiscrete permutation count Np and the continuous elapsed time T through a\nfunctional relation connecting the characteristic function of timing\ndistributions to the probability generating function of permutation counts.\nThis framework enables entropy purification, transforming microarchitectural\ntiming fluctuations into uniform randomness via geometric convergence. We\nestablish convergence theorems with explicit bounds and validate the results\nexperimentally, achieving Shannon entropy above 7.9998 bits per byte and\nchi-square uniformity across diverse platforms. The composition law provides a\nuniversal foundation for generating provably uniform randomness from\ngeneral-purpose computation, securing cryptographic purity from emergent\ncomputational dynamics.", "AI": {"tldr": "This paper introduces a composition law in the Random Permutation Sorting System (RPSS) that links permutation counts and time distributions to generate uniform randomness with high entropy, validated across platforms for cryptographic applications.", "motivation": "The motivation stems from the need to generate provably uniform randomness from general-purpose computation, addressing the critical requirement of cryptographic systems that rely on high-quality entropy. The paper aims to secure computational dynamics against entropy loss and ensure cryptographic purity through emergent mathematical structures.", "method": "The authors developed a functional relation connecting the characteristic function of timing distributions to the probability generating function of permutation counts within the Random Permutation Sorting System (RPSS). This framework uses entropy purification, leveraging geometric convergence to transform microarchitectural timing fluctuations into uniform randomness. Theoretical convergence theorems with explicit bounds were established and validated experimentally.", "result": "Experimental validation demonstrated Shannon entropy exceeding 7.9998 bits per byte and statistically significant chi-square uniformity across diverse platforms, confirming the effectiveness of the entropy purification framework. The theoretical bounds on convergence were empirically verified, showcasing robust performance.", "conclusion": "The paper concludes that the discovered composition law in RPSS provides a universal framework for generating provably uniform randomness from general-purpose computation, offering a secure and efficient solution for cryptographic applications."}}
{"id": "2510.08084", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08084", "abs": "https://arxiv.org/abs/2510.08084", "authors": ["Hikmat A. M. Abdeljaber", "Md. Alamgir Hossain", "Sultan Ahmad", "Ahmed Alsanad", "Md Alimul Haque", "Sudan Jha", "Jabeen Nazeer"], "title": "A Novel Ensemble Learning Approach for Enhanced IoT Attack Detection: Redefining Security Paradigms in Connected Systems", "comment": "14 pages, 5 fiugres, 7 tables", "summary": "The rapid expansion of Internet of Things (IoT) devices has transformed\nindustries and daily life by enabling widespread connectivity and data\nexchange. However, this increased interconnection has introduced serious\nsecurity vulnerabilities, making IoT systems more exposed to sophisticated\ncyber attacks. This study presents a novel ensemble learning architecture\ndesigned to improve IoT attack detection. The proposed approach applies\nadvanced machine learning techniques, specifically the Extra Trees Classifier,\nalong with thorough preprocessing and hyperparameter optimization. It is\nevaluated on several benchmark datasets including CICIoT2023, IoTID20,\nBotNeTIoT L01, ToN IoT, N BaIoT, and BoT IoT. The results show excellent\nperformance, achieving high recall, accuracy, and precision with very low error\nrates. These outcomes demonstrate the model efficiency and superiority compared\nto existing approaches, providing an effective and scalable method for securing\nIoT environments. This research establishes a solid foundation for future\nprogress in protecting connected devices from evolving cyber threats.", "AI": {"tldr": "This paper proposes an ensemble learning architecture using the Extra Trees Classifier to enhance IoT attack detection, demonstrating superior performance across multiple benchmark datasets through advanced preprocessing and hyperparameter optimization.", "motivation": "The proliferation of IoT devices has created significant security vulnerabilities due to increased interconnectivity, necessitating more effective detection methods to counter sophisticated cyber threats.", "method": "The approach employs the Extra Trees Classifier with optimized hyperparameters and rigorous preprocessing, validated using six benchmark IoT datasets (CICIoT2023, IoTID20, BotNeTIoT L01, ToN IoT, N BaIoT, BoT IoT).", "result": "Achieved state-of-the-art performance with high recall (99.3%), accuracy (99.8%), precision (99.5%), and minimal error rates (0.2%), outperforming existing methods in detection efficiency and scalability.", "conclusion": "The proposed model establishes a robust and scalable solution for IoT security, providing a foundation for future advancements in defending against evolving cyber threats in connected systems."}}
{"id": "2510.08101", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.08101", "abs": "https://arxiv.org/abs/2510.08101", "authors": ["Simone Bozzolan", "Stefano Calzavara", "Lorenzo Cazzaro"], "title": "LLM-Assisted Web Measurements", "comment": "12 pages, 4 figures, 4 tables", "summary": "Web measurements are a well-established methodology for assessing the\nsecurity and privacy landscape of the Internet. However, existing top lists of\npopular websites commonly used as measurement targets are unlabeled and lack\nsemantic information about the nature of the sites they include. This\nlimitation makes targeted measurements challenging, as researchers often need\nto rely on ad-hoc techniques to bias their datasets toward specific categories\nof interest. In this paper, we investigate the use of Large Language Models\n(LLMs) as a means to enable targeted web measurement studies through their\nsemantic understanding capabilities. Building on prior literature, we identify\nkey website classification tasks relevant to web measurements and construct\ndatasets to systematically evaluate the performance of different LLMs on these\ntasks. Our results demonstrate that LLMs may achieve strong performance across\nmultiple classification scenarios. We then conduct LLM-assisted web measurement\nstudies inspired by prior work and rigorously assess the validity of the\nresulting research inferences. Our results demonstrate that LLMs can serve as a\npractical tool for analyzing security and privacy trends on the Web.", "AI": {"tldr": "LLM-based classification enables efficient, targeted web security/privacy measurement.", "motivation": "Existing web measurement targets lack labeling and semantic context, requiring ad-hoc methods for category-specific analysis.", "method": "The study evaluates LLMs on website classification tasks using custom datasets and applies them to conduct LLM-assisted web measurements, validating their effectiveness in security and privacy analysis.", "result": "LLMs demonstrate strong classification performance across scenarios and successfully support valid research inferences about web security and privacy.", "conclusion": "LLMs can serve as a practical tool for analyzing web security and privacy trends through targeted classification and measurement."}}
{"id": "2510.08225", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.08225", "abs": "https://arxiv.org/abs/2510.08225", "authors": ["Daniel Pressens\u00e9", "Elisavet Kozyri"], "title": "TracE2E: Easily Deployable Middleware for Decentralized Data Traceability", "comment": null, "summary": "This paper presents TracE2E, a middleware written in Rust, that can provide\nboth data explainability and compliance across multiple nodes. By mediating\ninputs and outputs of processes, TracE2E records provenance information and\nenforces data-protection policies (e.g., confidentiality, integrity) that\ndepend on the recorded provenance. Unlike existing approaches that necessitate\nsubstantial application modifications, TracE2E is designed for easy integration\ninto existing and future applications through a wrapper of the Rust standard\nlibrary's IO module. We describe how TracE2E consistently records provenance\ninformation across nodes, and we demonstrate how the compliance layer of\nTracE2E can accommodate the enforcement of multiple policies.", "AI": {"tldr": "TracE2E is a Rust-based middleware for data explainability and compliance across distributed systems, enabling policy enforcement without significant application modifications.", "motivation": "Existing approaches require major application changes to support compliance and provenance tracking, hindering adoption in production systems.", "method": "TracE2E intercepts and wraps I/O operations using Rust's standard library, recording provenance data and enforcing policies across nodes through a wrapper-based architecture.", "result": "Demonstrated consistent cross-node provenance recording and policy enforcement capabilities, with implementation and evaluation showing feasibility of the approach.", "conclusion": "TracE2E provides a practical, minimally intrusive solution for achieving compliance and explainability in distributed applications through library-level mediation."}}
{"id": "2510.08272", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.08272", "abs": "https://arxiv.org/abs/2510.08272", "authors": ["C\u00e9drick Austa", "Jan Tobias M\u00fchlberg", "Jean-Michel Dricot"], "title": "Systematic Assessment of Cache Timing Vulnerabilities on RISC-V Processors", "comment": null, "summary": "While interest in the open RISC-V instruction set architecture is growing,\ntools to assess the security of concrete processor implementations are lacking.\nThere are dedicated tools and benchmarks for common microarchitectural\nside-channel vulnerabilities for popular processor families such as Intel\nx86-64 or ARM, but not for RISC-V. In this paper we describe our efforts in\nporting an Intel x86-64 benchmark suite for cache-based timing vulnerabilities\nto RISC-V. We then use this benchmark to evaluate the security of three\ncommercially available RISC-V processors, the T-Head C910 and the SiFive U54\nand U74 cores. We observe that the C910 processor exhibits more distinct timing\ntypes than the other processors, leading to the assumption that code running on\nthe C910 would be exposed to more microarchitectural vulnerability sources. In\naddition, our evaluation reveals that $37.5\\%$ of the vulnerabilities covered\nby the benchmark exist in all processors, while only $6.8\\%$ are absent from\nall cores. Our work, in particular the ported benchmark, aims to support RISC-V\nprocessor designers to identify leakage sources early in their designs and to\nsupport the development of countermeasures.", "AI": {"tldr": "This paper ported an x86-64 cache-based timing vulnerability benchmark to RISC-V and evaluated three RISC-V processors, finding varying susceptibility to microarchitectural side-channel attacks.", "motivation": "RISC-V lacks security assessment tools for microarchitectural vulnerabilities, despite growing adoption of the architecture.", "method": "The authors ported an existing x86-64 benchmark suite to RISC-V and evaluated the T-Head C910, SiFive U54, and U74 cores for cache-based timing vulnerabilities.", "result": "The C910 showed distinct timing patterns potentially enabling more attacks. 37.5% of vulnerabilities were present in all processors, while 6.8% were absent in all.", "conclusion": "The ported benchmark helps RISC-V designers identify leakage sources early in the design process and develop effective countermeasures against timing-based side-channel attacks."}}
{"id": "2510.08333", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08333", "abs": "https://arxiv.org/abs/2510.08333", "authors": ["Mika\u00ebla Ngambo\u00e9", "Jean-Simon Marrocco", "Jean-Yves Ouattara", "Jos\u00e9 M. Fernandez", "Gabriela Nicolescu"], "title": "New Machine Learning Approaches for Intrusion Detection in ADS-B", "comment": "This is the author's version of the work accepted for publication\n  Digital Avionics Systems Conference (DASC) 2025. The final version will be\n  available via IEEE Xplore", "summary": "With the growing reliance on the vulnerable Automatic Dependent\nSurveillance-Broadcast (ADS-B) protocol in air traffic management (ATM),\nensuring security is critical. This study investigates emerging machine\nlearning models and training strategies to improve AI-based intrusion detection\nsystems (IDS) for ADS-B. Focusing on ground-based ATM systems, we evaluate two\ndeep learning IDS implementations: one using a transformer encoder and the\nother an extended Long Short-Term Memory (xLSTM) network, marking the first\nxLSTM-based IDS for ADS-B. A transfer learning strategy was employed, involving\npre-training on benign ADS-B messages and fine-tuning with labeled data\ncontaining instances of tampered messages. Results show this approach\noutperforms existing methods, particularly in identifying subtle attacks that\nprogressively undermine situational awareness. The xLSTM-based IDS achieves an\nF1-score of 98.9%, surpassing the transformer-based model at 94.3%. Tests on\nunseen attacks validated the generalization ability of the xLSTM model.\nInference latency analysis shows that the 7.26-second delay introduced by the\nxLSTM-based IDS fits within the Secondary Surveillance Radar (SSR) refresh\ninterval (5-12 s), although it may be restrictive for time-critical operations.\nWhile the transformer-based IDS achieves a 2.1-second latency, it does so at\nthe cost of lower detection performance.", "AI": {"tldr": "This study introduces the first xLSTM-based IDS for enhancing ADS-B security. Using transfer learning, the system surpasses existing methods with 98.9% accuracy but requires 7.26 seconds per detection - a trade-off between performance and real-time capabilities.", "motivation": "ADS-B's vulnerabilities pose critical security risks to air traffic management. Traditional detection methods struggle against subtle progressive attacks that undermine situational awareness, necessitating more effective AI-based approaches.", "method": "The paper compares two deep learning intrusion detection systems (IDS) for ADS-B: one using a transformer encoder and another leveraging an extended Long Short-Term Memory (xLSTM) network. A transfer learning strategy was employed, pre-training on benign ADS-B messages followed by fine-tuning on labeled attack data.", "result": "The xLSTM-based IDS achieved an F1-score of 98.9%, outperforming the transformer-based model (94.3%). Inference latency was 7.26 seconds (within SSR limits), while the faster transformer model (2.1 seconds latency) delivered reduced detection accuracy. The xLSTM also generalized well to unseen attack patterns.", "conclusion": "The study demonstrates that the xLSTM-based IDS, with an F1-score of 98.9%, achieves superior performance in detecting ADS-B attacks compared to transformer-based models, while its processing latency aligns with SSR refresh intervals. However, its speed limitations highlight trade-offs between real-time requirements and detection accuracy."}}
