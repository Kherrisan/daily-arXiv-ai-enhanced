{"id": "2507.13367", "categories": ["cs.CR", "cs.CV", "cs.MM", "eess.IV", "68Q80", "I.4.2"], "pdf": "https://arxiv.org/pdf/2507.13367", "abs": "https://arxiv.org/abs/2507.13367", "authors": ["Mehrab Hosain", "Rajiv Kapoor"], "title": "A Novel APVD Steganography Technique Incorporating Pseudorandom Pixel Selection for Robust Image Security", "comment": "Accepted COMITCON 2023. Lecture Notes in Electrical Engineering, vol\n  1191. Springer", "summary": "Steganography is the process of embedding secret information discreetly\nwithin a carrier, ensuring secure exchange of confidential data. The Adaptive\nPixel Value Differencing (APVD) steganography method, while effective,\nencounters certain challenges like the \"unused blocks\" issue. This problem can\ncause a decrease in security, compromise the embedding capacity, and lead to\nlower visual quality. This research presents a novel steganographic strategy\nthat integrates APVD with pseudorandom pixel selection to effectively mitigate\nthese issues. The results indicate that the new method outperforms existing\ntechniques in aspects of security, data hiding capacity, and the preservation\nof image quality. Empirical results reveal that the combination of APVD with\npseudorandom pixel selection significantly enhances key image quality metrics\nsuch as Peak Signal-to-Noise Ratio (PSNR), Universal Image Quality Index (UIQ),\nand Structural Similarity Index (SSIM), surpassing other contemporary methods\nin performance. The newly proposed method is versatile, able to handle a\nvariety of cover and secret images in both color and grayscale, thereby\nensuring secure data transmission without compromising the aesthetic quality of\nthe image.", "AI": {"tldr": "The paper proposes a steganography method combining Adaptive Pixel Value Differencing (APVD) with pseudorandom pixel selection, addressing the 'unused blocks' issue to enhance security, capacity, and image quality metrics (PSNR, UIQ, SSIM) across various image types.", "motivation": "The existing APVD method suffers from 'unused blocks' which reduce security, embedding capacity, and visual quality of steganographic images.", "method": "Integrates APVD with a pseudorandom pixel selection mechanism to optimize block utilization and balance data embedding requirements.", "result": "Experimental results show improved PSNR, UIQ, and SSIM values compared to APVD and other contemporary methods, demonstrating enhanced performance in security, capacity, and image fidelity.", "conclusion": "The proposed APVD-pseudorandom integration offers a versatile, effective solution for secure data hiding in both color and grayscale images without compromising visual quality."}}
{"id": "2507.13505", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.13505", "abs": "https://arxiv.org/abs/2507.13505", "authors": ["Steven Lamp", "Jason D. Hiser", "Anh Nguyen-Tuong", "Jack W. Davidson"], "title": "PHASE: Passive Human Activity Simulation Evaluation", "comment": null, "summary": "Cybersecurity simulation environments, such as cyber ranges, honeypots, and\nsandboxes, require realistic human behavior to be effective, yet no\nquantitative method exists to assess the behavioral fidelity of synthetic user\npersonas. This paper presents PHASE (Passive Human Activity Simulation\nEvaluation), a machine learning framework that analyzes Zeek connection logs\nand distinguishes human from non-human activity with over 90\\% accuracy. PHASE\noperates entirely passively, relying on standard network monitoring without any\nuser-side instrumentation or visible signs of surveillance. All network\nactivity used for machine learning is collected via a Zeek network appliance to\navoid introducing unnecessary network traffic or artifacts that could disrupt\nthe fidelity of the simulation environment. The paper also proposes a novel\nlabeling approach that utilizes local DNS records to classify network traffic,\nthereby enabling machine learning analysis. Furthermore, we apply SHAP (SHapley\nAdditive exPlanations) analysis to uncover temporal and behavioral signatures\nindicative of genuine human users. In a case study, we evaluate a synthetic\nuser persona and identify distinct non-human patterns that undermine behavioral\nrealism. Based on these insights, we develop a revised behavioral configuration\nthat significantly improves the human-likeness of synthetic activity yielding a\nmore realistic and effective synthetic user persona.", "AI": {"tldr": "The paper introduces PHASE, a machine learning framework for passive evaluation of human-like behavior in synthetic user personas within cybersecurity simulations, achieving 90% accuracy.", "motivation": "Cybersecurity simulations need realistic human behavior to be effective, but existing methods lack quantitative ways to assess behavioral fidelity of synthetic personas.", "method": "PHASE analyzes Zeek connection logs passively using DNS-based traffic classification and SHAP analysis to identify temporal/behavioral signatures distinguishing human and non-human activity.", "result": "Demonstrated accurate detection of non-human behavioral patterns in synthetic personas, enabling a revised configuration with significantly improved human-likeness through behavioral analysis.", "conclusion": "Active simulation environments can now achieve higher realism by leveraging PHASE's passive behavioral fidelity evaluation to refine synthetic user personas effectively."}}
{"id": "2507.13591", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13591", "abs": "https://arxiv.org/abs/2507.13591", "authors": ["Sahar Ghoflsaz Ghinani", "Elaheh Sadredini"], "title": "FuSeFL: Fully Secure and Scalable Cross-Silo Federated Learning", "comment": "15 Pages, 12 Figures", "summary": "Federated Learning (FL) enables collaborative model training without\ncentralizing client data, making it attractive for privacy-sensitive domains.\nWhile existing approaches employ cryptographic techniques such as homomorphic\nencryption, differential privacy, or secure multiparty computation to mitigate\ninference attacks-including model inversion, membership inference, and gradient\nleakage-they often suffer from high computational, communication, or memory\noverheads. Moreover, many methods overlook the confidentiality of the global\nmodel itself, which may be proprietary and sensitive. These challenges limit\nthe practicality of secure FL, especially in cross-silo deployments involving\nlarge datasets and strict compliance requirements.\n  We present FuSeFL, a fully secure and scalable FL scheme designed for\ncross-silo settings. FuSeFL decentralizes training across client pairs using\nlightweight secure multiparty computation (MPC), while confining the server's\nrole to secure aggregation. This design eliminates server bottlenecks, avoids\ndata offloading, and preserves full confidentiality of data, model, and updates\nthroughout training. FuSeFL defends against inference threats, achieves up to\n95% lower communication latency and 50% lower server memory usage, and improves\naccuracy over prior secure FL solutions, demonstrating strong security and\nefficiency at scale.", "AI": {"tldr": "FuSeFL is a fully secure, scalable Federated Learning (FL) framework for cross-silo environments that uses decentralized client-pair training with lightweight MPC and secure aggregation to maintain data, model, and update confidentiality while reducing communication latency (95% lower) and server memory (50% lower) without compromising accuracy.", "motivation": "Existing secure FL solutions using cryptography suffer from high computational, communication, or memory overheads, limiting practicality. Additionally, global model confidentiality is often overlooked in sensitive domains.", "method": "FuSeFL decentralizes training across client pairs via lightweight secure multiparty computation (MPC) and restricts the server to only secure aggregation. This eliminates server bottlenecks, avoids data sharing, and maintains full confidentiality of all elements through training protocol design.", "result": "Achieved 95% reduction in communication latency, 50% reduction in server memory usage, resistance to inference attacks (model inversion, membership inference, gradient leakage), and higher accuracy compared to prior secure FL methods in cross-silo scenarios.", "conclusion": "FuSeFL enables practical secure FL at scale by simultaneously addressing data confidentiality, model secrecy, and system efficiency through its decentralized architecture and optimized MPC aggregation approach, making it suitable for large-scale privacy-sensitive deployments."}}
{"id": "2507.13598", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13598", "abs": "https://arxiv.org/abs/2507.13598", "authors": ["Amro Abdalla", "Ismail Shaheen", "Dan DeGenaro", "Rupayan Mallick", "Bogdan Raita", "Sarah Adel Bargal"], "title": "GIFT: Gradient-aware Immunization of diffusion models against malicious Fine-Tuning with safe concepts retention", "comment": "Warning: This paper contains NSFW content. Reader discretion is\n  advised", "summary": "We present GIFT: a {G}radient-aware {I}mmunization technique to defend\ndiffusion models against malicious {F}ine-{T}uning while preserving their\nability to generate safe content. Existing safety mechanisms like safety\ncheckers are easily bypassed, and concept erasure methods fail under\nadversarial fine-tuning. GIFT addresses this by framing immunization as a\nbi-level optimization problem: the upper-level objective degrades the model's\nability to represent harmful concepts using representation noising and\nmaximization, while the lower-level objective preserves performance on safe\ndata. GIFT achieves robust resistance to malicious fine-tuning while\nmaintaining safe generative quality. Experimental results show that our method\nsignificantly impairs the model's ability to re-learn harmful concepts while\nmaintaining performance on safe content, offering a promising direction for\ncreating inherently safer generative models resistant to adversarial\nfine-tuning attacks.", "AI": {"tldr": "GIFT is a gradient-aware immunization technique that defends diffusion models against adversarial fine-tuning while maintaining safe content generation. It uses bi-level optimization to degrade harmful concept representation and preserve performance on benign data.", "motivation": "Current safety mechanisms like safety checkers are easily bypassed, and concept erasure methods fail when an attacker applies adversarial fine-tuning. There's a need for robust protection that prevents models from re-learning harmful concepts post-safety interventions.", "method": "GIFT frames immunization as a bi-level optimization problem: the upper-level objective disrupts harmful concept representation via representation noising and maximization, while the lower-level objective ensures the model retains proficiency in generating non-malicious content through optimization constraints.", "result": "Experiments demonstrate GIFT significantly impairs the model's ability to re-learn adversarial harmful concepts during malicious fine-tuning while maintaining generation quality on safe data, making it resistant to such attacks without compromising utility.", "conclusion": "GIFT provides an effective framework for creating inherently safer diffusion models by integrating proactive immunization into their design. This approach shifts safety from reactive post-hoc measures to structural robustness, opening new research directions for secure generative AI."}}
{"id": "2507.13481", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.13481", "abs": "https://arxiv.org/abs/2507.13481", "authors": ["Arthur Bueno", "Bruno Cafeo", "Maria Cagnin", "Awdren Font\u00e3o"], "title": "Socio-Technical Smell Dynamics in Code Samples: A Multivocal Review on Emergence, Evolution, and Co-Occurrence", "comment": "12 pages; 2 figures; Preprint with the original submission accepted\n  for publication at 39th Brazilian Symposium on Software Engineering (SBES)", "summary": "Code samples play a pivotal role in open-source ecosystems (OSSECO), serving\nas lightweight artifacts that support knowledge transfer, onboarding, and\nframework adoption. Despite their instructional relevance, these samples are\noften governed informally, with minimal review and unclear ownership, which\nincreases their exposure to socio-technical degradation. In this context, the\nco-occurrence and longitudinal interplay of code smells (e.g., large classes,\npoor modularity) and community smells (e.g., lone contributors, fragmented\ncommunication) become particularly critical. While each type of smell has been\nstudied in isolation, little is known about how community-level dysfunctions\nanticipate or exacerbate technical anomalies in code samples over time. This\nstudy investigates how code and community smells emerge, co-occur, and evolve\nwithin code samples maintained in OSSECOs. A Multivocal Literature Review\nprotocol was applied, encompassing 30 peer-reviewed papers and 17\npractitioner-oriented sources (2013-2024). Thematic synthesis was conducted to\nidentify recurring socio-technical patterns related to smell dynamics. Nine\npatterns were identified, showing that community smells often precede or\nreinforce technical degradation in code samples. Symptoms such as \"radio\nsilence\" and centralized ownership were frequently associated with persistent\nstructural anomalies. Additionally, limited onboarding, the absence of\ncontinuous refactoring, and informal collaboration emerged as recurring\nconditions for smell accumulation. Conclusion: In OSSECOs, particularly within\ncode samples, community-level dysfunctions not only correlate with but often\nsignal maintainability decay. These findings underscore the need for\nsocio-technical quality indicators and lightweight governance mechanisms\ntailored to shared instructional artifacts.", "AI": {"tldr": "This study examines the co-occurrence and evolution of code and community smells in open-source code samples, revealing that community-level dysfunctions often anticipate or worsen technical degradation, requiring socio-technical governance strategies.", "motivation": "Code samples in open-source ecosystems are crucial for knowledge transfer but face socio-technical degradation due to informal governance. Understanding the relationship between community smells (e.g., fragmented communication) and code smells (e.g., poor modularity) is essential to mitigating this decay.", "method": "A Multivocal Literature Review synthesized 30 peer-reviewed papers and 17 practitioner-oriented sources (2013-2024), focusing on thematic patterns of smell co-occurrence and their socio-technical interplay.", "result": "Nine recurring socio-technical patterns were identified, demonstrating that community smells like radio silence and centralized ownership often precede or amplify code smells. Limited onboarding and lack of refactoring further enable smell accumulation.", "conclusion": "Community dysfunctions in OSSECOs are early indicators of maintainability decay in code samples. The findings emphasize developing tailored socio-technical quality metrics and lightweight governance to preserve their instructional value."}}
{"id": "2507.13629", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13629", "abs": "https://arxiv.org/abs/2507.13629", "authors": ["Niveen O. Jaffal", "Mohammed Alkhanafseh", "David Mohaisen"], "title": "Large Language Models in Cybersecurity: Applications, Vulnerabilities, and Defense Techniques", "comment": "21 pages", "summary": "Large Language Models (LLMs) are transforming cybersecurity by enabling\nintelligent, adaptive, and automated approaches to threat detection,\nvulnerability assessment, and incident response. With their advanced language\nunderstanding and contextual reasoning, LLMs surpass traditional methods in\ntackling challenges across domains such as IoT, blockchain, and hardware\nsecurity. This survey provides a comprehensive overview of LLM applications in\ncybersecurity, focusing on two core areas: (1) the integration of LLMs into key\ncybersecurity domains, and (2) the vulnerabilities of LLMs themselves, along\nwith mitigation strategies. By synthesizing recent advancements and identifying\nkey limitations, this work offers practical insights and strategic\nrecommendations for leveraging LLMs to build secure, scalable, and future-ready\ncyber defense systems.", "AI": {"tldr": "This survey explores how Large Language Models (LLMs) are applied in cybersecurity for threat detection, vulnerability assessment, incident response across IoT, blockchain, and hardware domains, while systematically analyzing their own vulnerabilities and proposing mitigation strategies.", "motivation": "The rapid advancement of LLMs in cybersecurity applications, combined with their domain versatility and contextual reasoning capabilities, necessitates a thorough examination of both their integrative potential and inherent security risks to guide future deployment.", "method": "The paper conducts a comprehensive review of recent advancements in LLM cybersecurity applications, categorizes case studies into integration and security risks, and synthesizes limitations through thematic analysis of existing research.", "result": "Identified key cybersecurity domains where LLMs outperform traditional approaches, mapped common vulnerabilities in LLM architectures, and provided actionable strategies for building secure cyber defense systems while addressing current technology limitations.", "conclusion": "LLMs represent transformative potential for cybersecurity but require careful implementation with robust mitigation strategies to address their dual role as both solution and security target, ensuring scalable and future-ready defense architectures."}}
{"id": "2507.13499", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.13499", "abs": "https://arxiv.org/abs/2507.13499", "authors": ["Chandra Maddila", "Negar Ghorbani", "James Saindon", "Parth Thakkar", "Vijayaraghavan Murali", "Rui Abreu", "Jingyue Shen", "Brian Zhou", "Nachiappan Nagappan", "Peter C. Rigby"], "title": "AI-Assisted Fixes to Code Review Comments at Scale", "comment": null, "summary": "Aim. There are 10s of thousands of code review comments each week at Meta. We\ndeveloped Metamate for Code Review (MetaMateCR) that provides AI-assisted fixes\nfor reviewer comments in production at scale.\n  Method. We developed an internal benchmark of 64k <review comment, patch>\ndata points to fine-tune Llama models. Once our models achieve reasonable\noffline results, we roll them into production. To ensure that our AI-assisted\nfixes do not negatively impact the time it takes to do code reviews, we conduct\nrandomized controlled safety trials as well as full production experiments.\n  Offline Results. As a baseline, we compare GPT-4o to our small and large\nLlama models. In offline results, our LargeLSFT model creates an exact match\npatch 68% of the time outperforming GPT-4o by 9 percentage points (pp). The\ninternal models also use more modern Hack functions when compared to the PHP\nfunctions suggested by GPT-4o.\n  Safety Trial. When we roll MetaMateCR into production in a safety trial that\ncompares no AI patches with AI patch suggestions, we see a large regression\nwith reviewers taking over 5% longer to conduct reviews. After investigation,\nwe modify the UX to only show authors the AI patches, and see no regressions in\nthe time for reviews.\n  Production. When we roll LargeLSFT into production, we see an\nActionableToApplied rate of 19.7%, which is a 9.2pp improvement over GPT-4o.\nOur results illustrate the importance of safety trials in ensuring that AI does\nnot inadvertently slow down engineers, and a successful review comment to AI\npatch product running at scale.", "AI": {"tldr": "Meta developed MetaMateCR, an AI-assisted code review system, achieving 19.7% ActionableToApplied rate post-production rollout and outperforming GPT-4o by 9.2pp. Safety trials revealed initial UX issues but a resolved implementation with 9.7% efficiency improvement over baselines.", "motivation": "To address the need for efficient AI-assisted code review fixes at Meta's scale on 10k+ weekly code review comments.", "method": "Created an internal benchmark of 64k <review comment, patch> data points to fine-tune Llama models, followed by offline evaluation, production safety trials with randomized UX controls, and full-scale deployment testing.", "result": "1. Offline: LargeLSFT model achieved 68% exact match patch accuracy (9pp better than GPT-4o) and used modern Hack functions more effectively than PHP-based GPT-4o suggestions. 2. Safety trial: Initial UX caused 5% review time regression; resolved by changing UX to hide AI suggestions from reviewers. 3. Production: ActionableToApplied rate of 19.7% with 9.2pp improvement over GPT-4o.", "conclusion": "MetaMateCR demonstrates AI's potential in code review when combined with safety trials. Shows successful enterprise code review system implementation where UX design significantly impacts AI effectiveness, with direct implications for large-scale code review automation."}}
{"id": "2507.13686", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.13686", "abs": "https://arxiv.org/abs/2507.13686", "authors": ["Yulin Chen", "Haoran Li", "Yuexin Li", "Yue Liu", "Yangqiu Song", "Bryan Hooi"], "title": "TopicAttack: An Indirect Prompt Injection Attack via Topic Transition", "comment": "19 pages", "summary": "Large language models (LLMs) have shown remarkable performance across a range\nof NLP tasks. However, their strong instruction-following capabilities and\ninability to distinguish instructions from data content make them vulnerable to\nindirect prompt injection attacks. In such attacks, instructions with malicious\npurposes are injected into external data sources, such as web documents. When\nLLMs retrieve this injected data through tools, such as a search engine and\nexecute the injected instructions, they provide misled responses. Recent attack\nmethods have demonstrated potential, but their abrupt instruction injection\noften undermines their effectiveness. Motivated by the limitations of existing\nattack methods, we propose TopicAttack, which prompts the LLM to generate a\nfabricated conversational transition prompt that gradually shifts the topic\ntoward the injected instruction, making the injection smoother and enhancing\nthe plausibility and success of the attack. Through comprehensive experiments,\nTopicAttack achieves state-of-the-art performance, with an attack success rate\n(ASR) over 90\\% in most cases, even when various defense methods are applied.\nWe further analyze its effectiveness by examining attention scores. We find\nthat a higher injected-to-original attention ratio leads to a greater success\nprobability, and our method achieves a much higher ratio than the baseline\nmethods.", "AI": {"tldr": "This paper introduces TopicAttack, a method for indirect prompt injection attacks in LLMs, which uses fabricated conversational transitions to smoothly shift topics toward malicious instructions, achieving over 90% attack success rates even under defenses.", "motivation": "LLMs are vulnerable to indirect prompt injection attacks due to their inability to distinguish instructions from data content. Existing methods' abrupt injections reduce their effectiveness, motivating the need for smoother transitions.", "method": "TopicAttack generates a gradual, plausible conversational transition prompt by introducing fabricated context, incrementally steering the LLM toward the malicious instruction embedded in external data sources.", "result": "TopicAttack achieves state-of-the-art performance with an attack success rate (ASR) over 90% in most cases, even when multiple defenses are applied, and demonstrates a significantly higher injected-to-original attention ratio compared to baseline methods.", "conclusion": "TopicAttack effectively bypasses LLM defenses against indirect prompt injection attacks through its smooth topic transition mechanism, supported by high ASR and attention score analysis, indicating a critical threat requiring improved mitigation strategies."}}
{"id": "2507.13553", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13553", "abs": "https://arxiv.org/abs/2507.13553", "authors": ["Pragyan K C", "Rambod Ghandiparsi", "Thomas Herron", "John Heaps", "Mitra Bokaei Hosseini"], "title": "Towards Better Requirements from the Crowd: Developer Engagement with Feature Requests in Open Source Software", "comment": "Accepted at the 9th International Workshop on Crowd-Based\n  Requirements Engineering (CrowdRE'25)", "summary": "As user demands evolve, effectively incorporating feature requests is crucial\nfor maintaining software relevance and user satisfaction. Feature requests,\ntypically expressed in natural language, often suffer from ambiguity or\nincomplete information due to communication gaps or the requester's limited\ntechnical expertise. These issues can lead to misinterpretation, faulty\nimplementation, and reduced software quality. While seeking clarification from\nrequesters is a common strategy to mitigate these risks, little is known about\nhow developers engage in this clarification process in practice-how they\nformulate clarifying questions, seek technical or contextual details, align on\ngoals and use cases, or decide to close requests without attempting\nclarification. This study investigates how feature requests are prone to NL\ndefects (i.e. ambiguous or incomplete) and the conversational dynamics of\nclarification in open-source software (OSS) development, aiming to understand\nhow developers handle ambiguous or incomplete feature requests. Our findings\nsuggest that feature requests published on the OSS platforms do possess\nambiguity and incompleteness, and in some cases, both. We also find that\nexplicit clarification for the resolution of these defects is uncommon;\ndevelopers usually focus on aligning with project goals rather than resolving\nunclear text. When clarification occurs, it emphasizes understanding user\nintent/goal and feasibility, rather than technical details. By characterizing\nthe dynamics of clarification in open-source issue trackers, this work\nidentifies patterns that can improve user-developer collaboration and inform\nbest practices for handling feature requests effectively.", "AI": {"tldr": "Analyzes how open-source developers handle ambiguous/incomplete feature requests, revealing rare explicit clarification and focus on aligning with project goals rather than resolving text ambiguity.", "motivation": "Ambiguous or incomplete natural language feature requests cause misinterpretation and implementation issues, yet developer clarification practices are poorly understood.", "method": "Investigated NL defects in feature requests and conversational dynamics of clarification using open-source software issue tracker analysis.", "result": "Found feature requests contain ambiguity/incompleteness, explicit clarification is uncommon, and developer focus lies in understanding user intent/goal and feasibility over textual details.", "conclusion": "Identified patterns in clarification workflows to inform best practices for improving user-developer collaboration in handling feature requests."}}
{"id": "2507.13720", "categories": ["cs.CR", "cs.DC", "cs.ET", "cs.NI", "68M10, 81P94, 94A60 68M10, 81P94, 94A60 68M10, 81P94, 94A60", "C.2.1; E.3; K.6.5"], "pdf": "https://arxiv.org/pdf/2507.13720", "abs": "https://arxiv.org/abs/2507.13720", "authors": ["Saurav Ghosh"], "title": "Quantum Blockchain Survey: Foundations, Trends, and Gaps", "comment": "12 Pages, 4 figures", "summary": "Quantum computing poses fundamental risks to classical blockchain systems by\nundermining widely used cryptographic primitives. In response, two major\nresearch directions have emerged: post-quantum blockchains, which integrate\nquantum-resistant algorithms, and quantum blockchains, which leverage quantum\nproperties such as entanglement and quantum key distribution. This survey\nreviews key developments in both areas, analyzing their cryptographic\nfoundations, architectural designs, and implementation challenges. This work\nprovides a comparative overview of technical proposals, highlight trade-offs in\nsecurity, scalability, and deployment, and identify open research problems\nacross hardware, consensus, and network design. The goal is to offer a\nstructured and comprehensive reference for advancing secure blockchain systems\nin the quantum era.", "AI": {"tldr": "This paper surveys post-quantum and quantum blockchain systems, comparing their cryptographic foundations, architectural designs, implementation challenges, trade-offs (security/scalability/deployment), and open research problems.", "motivation": "Quantum computing threatens classical blockchain cryptography, necessitating evaluation of emerging quantum-resistant solutions to guide secure system development in the quantum era.", "method": "The authors conduct a comprehensive review and comparative analysis of technical proposals in post-quantum blockchains (quantum-resistant algorithms) and quantum blockchains (quantum property implementation), addressing security assumptions and scalability implications.", "result": "Key findings include trade-off patterns between security guarantees and system performance, deployment feasibility of post-quantum algorithms, and unresolved challenges in quantum network infrastructure and consensus mechanism redesign.", "conclusion": "The analysis establishes a reference framework for prioritizing research efforts and technical upgrades required to prepare blockchain systems for quantum computing threats."}}
{"id": "2507.13555", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13555", "abs": "https://arxiv.org/abs/2507.13555", "authors": ["Pragyan K C", "Rambod Ghandiparsi", "Thomas Herron", "John Heaps", "Mitra Bokaei Hosseini"], "title": "Demystifying Feature Requests: Leveraging LLMs to Refine Feature Requests in Open-Source Software", "comment": "Accepted at the 33rd IEEE International Requirements Engineering 2025", "summary": "The growing popularity and widespread use of software applications (apps)\nacross various domains have driven rapid industry growth. Along with this\ngrowth, fast-paced market changes have led to constantly evolving software\nrequirements. Such requirements are often grounded in feature requests and\nenhancement suggestions, typically provided by users in natural language (NL).\nHowever, these requests often suffer from defects such as ambiguity and\nincompleteness, making them challenging to interpret. Traditional validation\nmethods (e.g., interviews and workshops) help clarify such defects but are\nimpractical in decentralized environments like open-source software (OSS),\nwhere change requests originate from diverse users on platforms like GitHub.\nThis paper proposes a novel approach leveraging Large Language Models (LLMs) to\ndetect and refine NL defects in feature requests. Our approach automates the\nidentification of ambiguous and incomplete requests and generates clarification\nquestions (CQs) to enhance their usefulness for developers. To evaluate its\neffectiveness, we apply our method to real-world OSS feature requests and\ncompare its performance against human annotations. In addition, we conduct\ninterviews with GitHub developers to gain deeper insights into their\nperceptions of NL defects, the strategies they use to address these defects,\nand the impact of defects on downstream software engineering (SE) tasks.", "AI": {"tldr": "This paper proposes an LLM-based method to automate detection and refinement of natural language defects in feature requests, addressing limitations of traditional validation methods in decentralized environments like OSS.", "motivation": "Rapid evolution of software requirements through user-submitted natural language feature requests introduces ambiguities and incompleteness that hinder SE tasks. Traditional validation methods are impractical for large-scale decentralized OSS ecosystems.", "method": "Developed a large language model approach to automatically identify NL defects in feature requests and generate targeted clarification questions, evaluated through real-world OSS data analysis, human annotation comparisons, and developer interviews.", "result": "Demonstrated LLM effectiveness in detecting defects and generating CQs comparable to human annotations, with developer interviews validating practical relevance and highlighting improvements in downstream SE task efficiency.", "conclusion": "LLM-based automation of NL defect refinement significantly enhances feature request utility in decentralized software development, offering scalable solutions to improve requirement quality and developer productivity."}}
{"id": "2507.13926", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.13926", "abs": "https://arxiv.org/abs/2507.13926", "authors": ["Libor Pol\u010d\u00e1k", "Giorgio Maone", "Michael McMahon", "Martin Bedn\u00e1\u0159"], "title": "Developers Insight On Manifest v3 Privacy and Security Webextensions", "comment": "WEBIST'25, Marbella, Spain", "summary": "Webextensions can improve web browser privacy, security, and user experience.\nThe APIs offered by the browser to webextensions affect possible functionality.\nCurrently, Chrome transitions to a modified set of APIs called Manifest v3.\nThis paper studies the challenges and opportunities of Manifest v3 with an\nin-depth structured qualitative research. Even though some projects observed\npositive effects, a majority expresses concerns over limited benefits to users,\nremoval of crucial APIs, or the need to find workarounds. Our findings indicate\nthat the transition affects different types of webextensions differently; some\ncan migrate without losing functionality, while other projects remove\nfunctionality or decline to update. The respondents identified several critical\nmissing APIs, including reliable APIs to inject content scripts, APIs for\nstoring confidential content, and others.", "AI": {"tldr": "This paper examines the challenges and opportunities in Chrome's Manifest v3 API transition for webextensions, revealing mixed responses with critical API limitations affecting functionality migration.", "motivation": "Browser APIs influence webextension capabilities. Chrome's shift to Manifest v3 necessitates understanding its impact on privacy, security, and user experience.", "method": "In-depth structured qualitative research analyzing responses from projects adopting, adapting, or resisting Manifest v3 changes.", "result": "Manifest v3 migration results in differential outcomes: some extensions maintain functionality, while others lose features or stop updating due to missing APIs like reliable content injection and secure storage.", "conclusion": "The Manifest v3 transition offers benefits but also significant limitations for webextensions, requiring improved browser APIs to address identified critical gaps."}}
{"id": "2507.13661", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13661", "abs": "https://arxiv.org/abs/2507.13661", "authors": ["Changwen Li", "Joseph Sifakis", "Rongjie Yan", "Jian Zhang"], "title": "Testing Autonomous Driving Systems -- What Really Matters and What Doesn't", "comment": null, "summary": "Despite extensive research, the testing of autonomous driving systems (ADS)\nlandscape remains fragmented, and there is currently no basis for an informed\ntechnical assessment of the importance and contribution of the current state of\nthe art. This paper attempts to address this problem by exploring two\ncomplementary aspects.\n  First, it proposes a framework for comparing existing test methods in terms\nof their intrinsic effectiveness and validity. It shows that many methods do\nnot meet both of these requirements. Either because they are based on criteria\nthat do not allow for rapid, inexpensive, and comprehensive detection of\nfailures, or because the degree of validity of the properties tested cannot be\naccurately estimated. In particular, it is shown that most critical test\nmethods do not take into account the nominal operational capabilities of\nautopilots and generate scenarios that are impossible for the tested vehicles\nto handle, resulting in unjustified rejections.\n  Secondly, the paper shows that test effectiveness and validity are highly\ndependent on how autopilots are designed: how they choose between different\ncontrol policies to perform maneuvers, as well as on the reproducibility of the\nresults. In fact, most test methods take for granted two principles underlying\ntraditional methods, but do not generally apply to ADS. We maintain that the\nabsence of rationality and determinacy significantly impairs the effectiveness\nand validity of test methods, and provide test results on eight open\nautopilots, in which most do not satisfy these properties, thereby illustrating\nthis fact.\n  We conclude that under the current state of the art, it is impossible to\nobtain strong enough guarantees for essential autopilot properties and\nrecommend that autopilots be developed with a view to both rationality and\ndeterminacy.", "AI": {"tldr": "This paper addresses the fragmented testing of autonomous driving systems by proposing a framework to evaluate test methods and highlighting the lack of rationality and determinacy in current autopilot designs, which limits effective testing.", "motivation": "The lack of a standardized technical assessment basis for autonomous driving systems (ADS) testing methods necessitates a structured approach to evaluate their effectiveness, validity, and alignment with operational capabilities.", "method": "The study introduces a framework comparing existing test methods based on intrinsic effectiveness and validity, while analyzing eight open autopilots to demonstrate how design principles (policy selection and reproducibility) impact these metrics.", "result": "Most test methods fail to meet both effectiveness and validity criteria, and existing autopilots largely lack rationality and determinacy, undermining the reliability of testing outcomes.", "conclusion": "Current ADS testing lacks sufficient guarantees for critical autopilot properties; future development must prioritize rational, deterministic designs to improve testing efficacy and validity."}}
