<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 17]
- [cs.SE](#cs.SE) [Total: 21]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [AegisShield: Democratizing Cyber Threat Modeling with Generative AI](https://arxiv.org/abs/2509.10482)
*Matthew Grofsky*

Main category: cs.CR

TL;DR: AegisShield, an AI-driven threat modeling tool, streamlines security assessments for small organizations by automating STRIDE/MITRE ATT&CK analysis with real-time threat data, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional threat modeling is inefficient and resource-intensive for small organizations, necessitating scalable, automated solutions to improve risk management.

Method: Developed AegisShield, a generative AI tool combining STRIDE and MITRE ATT&CK frameworks with real-time threat intelligence from NVD and AlienVault to automate threat generation and assessment.

Result: AegisShield reduced complexity (p < 0.001), produced semantically aligned threats (p < 0.05), and achieved 85.4% MITRE ATT&CK mapping accuracy (p < 0.001) across 15 case studies and 8000+ AI-generated threats.

Conclusion: AegisShield automates threat modeling, enabling resource-limited organizations to efficiently manage risks and promoting secure-by-design practices through integration with MITRE ATT&CK and real-time threat intelligence.

Abstract: The increasing sophistication of technology systems makes traditional threat
modeling hard to scale, especially for small organizations with limited
resources. This paper develops and evaluates AegisShield, a generative AI
enhanced threat modeling tool that implements STRIDE and MITRE ATT&CK to
automate threat generation and provide systematic assessments. By integrating
real time threat intelligence from the National Vulnerability Database and
AlienVault Open Threat Exchange, AegisShield produces streamlined and
accessible threat descriptions. Our assessment of 243 threats from 15 case
studies and over 8000 AI generated threats shows that AegisShield reduces
complexity (p less than 0.001), yields outputs semantically aligned with expert
developed threats (p less than 0.05), and achieves an 85.4 percent success rate
in mapping threats to MITRE ATT&CK techniques (p less than 0.001). Automating
and standardizing threat modeling helps under resourced organizations address
risk earlier and supports wider adoption of secure by design practices.

</details>


### [2] [Turning CVEs into Educational Labs:Insights and Challenges](https://arxiv.org/abs/2509.10488)
*Trueye Tafese*

Main category: cs.CR

TL;DR: This paper proposes a framework for converting CVEs into Docker-based educational labs to enhance cybersecurity training through hands-on vulnerability analysis.


<details>
  <summary>Details</summary>
Motivation: Bridging the gap between theoretical cybersecurity knowledge and practical skills by simulating real-world vulnerabilities in a controlled environment.

Method: Developed containerized labs using Docker to replicate vulnerabilities like SQL Injection and SSL certificate issues, combined with structured tutorials, pre/post surveys, and remediation steps.

Result: Students demonstrated improved comprehension of cybersecurity principles, threat mitigation, and secure coding practices through lab-based learning.

Conclusion: The approach provides a scalable, reproducible model for integrating CVEs into education, enhancing real-world security understanding safely.

Abstract: This research focuses on transforming CVEs to hands-on educational lab for
cybersecurity training. The study shows the practical application of CVEs by
developing containerized lab environments- Docker to simulate real-world
vulnerabilities like SQL Injection, arbitrary code execution, and improper SSL
certificate validation. These labs has structured tutorials, pre- and
post-surveys to evaluate learning outcomes, and remediation steps.Key
challenges included interpreting limited CVE data, resolving technical
complexities in lab design, and ensuring accessibility for diverse learners.
Despite these difficulties, the findings highlight the use of educational
benefits of vulnerability analysis, bridging theoretical concepts with hands-on
experience. The results indicate that students improved comprehension of
cybersecurity principles, threat mitigation techniques, and secure coding
practices. This innovative approach provides a scalable and reproducible model
for integrating CVEs into cybersecurity education, fostering a deeper
understanding of real-world security challenges in a controlled and safe
environment.

</details>


### [3] [Investigation Of The Distinguishability Of Giraud-Verneuil Atomic Blocks](https://arxiv.org/abs/2509.10492)
*Philip Laryea Doku*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this work, we investigate the security of Elliptic Curve Cryptosystem
(ECC) implementations against Side-Channel Analysis (SCA). ECC is well known
for its efficiency and strong security, yet vulnerable to SCA which exploits
physical information leaked during scalar multiplication (kP). Countermeasures
such as regularity and atomicity exist; this thesis focuses on atomicity. In
this work, we study the Giraud and Verneuil atomic pattern for kP, implementing
it using the right-to-left kP algorithm on the NIST EC P-256 curve. We use the
FLECC library with constant-time operations and execute on the Texas
Instruments LAUNCHXLF28379D MCU. We measure Electromagnetic (EM) emissions
during kP using a Lecroy WavePro 604HD Oscilloscope, a Langer ICS 105
Integrated Circuit Scanner, and a Langer MFA-R 0.2-75 Near Field Probe. We
investigate whether the Giraud and Verneuil atomic blocks are distinguishable
in EM traces. Our findings show that, when additional clock cycle processes are
present, the atomic blocks can be visually distinguished; after removing these
processes, they become more synchronised and harder to distinguish, reducing
the risk of a successful SCA attack. These results show that, although the
atomic pattern is correctly implemented with dummy operations, resistance to
SCA can still be affected by additional processes inserted at hardware or
software level.This means atomicity alone may not fully protect ECC from SCA.
More research is needed to investigate the causes of the additional clock cycle
processes and how intermediate operations are addressed in memory registers.
This will help to understand the processes that lead to the insertion of these
additional clock cycles. This thesis is the first to experimentally implement
and investigate Giraud and Verneuil's atomic pattern on hardware, and it offers
useful results to improve countermeasures against SCA.

</details>


### [4] [EchoLeak: The First Real-World Zero-Click Prompt Injection Exploit in a Production LLM System](https://arxiv.org/abs/2509.10540)
*Pavan Reddy,Aditya Sanjay Gujral*

Main category: cs.CR

TL;DR: EchoLeak (CVE-2025-32711), a zero-click prompt injection vulnerability in Microsoft 365 Copilot, enabling unauthenticated data exfiltration via a crafted email by chaining multiple bypasses. The study proposes mitigations and security principles for AI systems.


<details>
  <summary>Details</summary>
Motivation: Enterprise adoption of LLMs introduces novel security risks as they interface internal and external data. This paper addresses the lack of understanding about practical threat models and defensive strategies for AI-native vulnerabilities.

Method: Chained exploit techniques including: (1)​⦈ bypassing Microsoft's XPIA (Cross Prompt Injection Attempt) classifier, (2)​⦈ circumventing link redaction using reference-style Markdown, (3)​⦈ exploiting auto-fetched images, and (4)​⦈ leveraging a Microsoft Teams proxy under content security policies to achieve cross-LLM privilege escalation.

Result: Demonstrated full data exfiltration without user interaction. Proposed mitigations include: (1)​⦈ Prompt partitioning, (2)​⦈ Enhanced input/output filtering, (3)​⦈ Provenance-based access control, and (4)​⦈ Strict content security policies. Established prompt injection as a critical vulnerability class.

Conclusion: Highlights the need for least privilege principles, defense-in-depth architectures, and adversarial testing in AI copilots. Provides a blueprint for securing AI systems against future attacks by operationalizing security patterns like input hardening, provenance tracking, and policy enforcement.

Abstract: Large language model (LLM) assistants are increasingly integrated into
enterprise workflows, raising new security concerns as they bridge internal and
external data sources. This paper presents an in-depth case study of EchoLeak
(CVE-2025-32711), a zero-click prompt injection vulnerability in Microsoft 365
Copilot that enabled remote, unauthenticated data exfiltration via a single
crafted email. By chaining multiple bypasses-evading Microsofts XPIA (Cross
Prompt Injection Attempt) classifier, circumventing link redaction with
reference-style Markdown, exploiting auto-fetched images, and abusing a
Microsoft Teams proxy allowed by the content security policy-EchoLeak achieved
full privilege escalation across LLM trust boundaries without user interaction.
We analyze why existing defenses failed, and outline a set of engineering
mitigations including prompt partitioning, enhanced input/output filtering,
provenance-based access control, and strict content security policies. Beyond
the specific exploit, we derive generalizable lessons for building secure AI
copilots, emphasizing the principle of least privilege, defense-in-depth
architectures, and continuous adversarial testing. Our findings establish
prompt injection as a practical, high-severity vulnerability class in
production AI systems and provide a blueprint for defending against future
AI-native threats.

</details>


### [5] [Robust DDoS-Attack Classification with 3D CNNs Against Adversarial Methods](https://arxiv.org/abs/2509.10543)
*Landon Bragg,Nathan Dorsey,Josh Prior,John Ajit,Ben Kim,Nate Willis,Pablo Rivas*

Main category: cs.CR

TL;DR: This paper introduces a 3D CNN with hive-plot encodings and adversarial training for accurate, early-stage DDoS detection, achieving robustness against evasion tactics.


<details>
  <summary>Details</summary>
Motivation: DDoS attacks evade detection by camouflaging traffic. Traditional methods lack robustness against adversarial manipulations, necessitating a more effective classification technique that can identify subtle attack patterns early and resist adversarial examples.

Method: The method combines spatio-temporal hive-plot encodings with a 3D CNN trained via adversarial techniques (FGSM, PGD) and data augmentation. This architecture focuses on extracting temporal patterns from network traffic and improving robustness through early signal analysis.

Result: The model achieves 93% adversarial accuracy on benchmark datasets, exceeding prior methods (50-55%), while maintaining strong clean-sample performance. It identifies frames 3-4 as critical for early classification.

Conclusion: The paper effectively addresses the challenge of detecting subtle DDoS attacks by introducing a hybrid approach of hive-plot sequences and adversarially trained 3D CNNs. It demonstrates robust performance against adversarial examples and enables early-stage classification, offering a practical advancement for network security systems.

Abstract: Distributed Denial-of-Service (DDoS) attacks remain a serious threat to
online infrastructure, often bypassing detection by altering traffic in subtle
ways. We present a method using hive-plot sequences of network data and a 3D
convolutional neural network (3D CNN) to classify DDoS traffic with high
accuracy. Our system relies on three main ideas: (1) using spatio-temporal
hive-plot encodings to set a pattern-recognition baseline, (2) applying
adversarial training with FGSM and PGD alongside spatial noise and image
shifts, and (3) analyzing frame-wise predictions to find early signals. On a
benchmark dataset, our method lifts adversarial accuracy from 50-55% to over
93% while maintaining clean-sample performance. Frames 3-4 offer strong
predictive signals, showing early-stage classification is possible.

</details>


### [6] [Decentralized Identity Management on Ripple: A Conceptual Framework for High-Speed, Low-Cost Identity Transactions in Attestation-Based Attribute-Based Identity](https://arxiv.org/abs/2509.10545)
*Ruwanga Konara,Kasun De Zoysa,Asanka Sayakkara*

Main category: cs.CR

TL;DR: This paper identifies the research gap in ABABDIDM on Ripple and proposes a conceptual framework to utilize Ripple's blockchain for decentralized identity management.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the lack of attention given to ABABDIDM compared to other decentralized identity management systems and the underutilized potential of Ripple's blockchain features for such implementations.

Method: The paper conceptualizes an Attestation-Based Attribute-Based Decentralized Identity Management System (ABABDIDM) using the Ripple blockchain platform.

Result: A conceptual framework for ABABDIDM on Ripple is proposed, highlighting how Ripple's characteristics can be leveraged for improved decentralized identity management.

Conclusion: The authors conclude that there is a significant gap in research regarding ABABDIDM on Ripple, despite its advantages. They propose a conceptual framework for ABABDIDM on Ripple to address these gaps.

Abstract: Recent years have seen many industrial implementations and much scholastic
research, i.e., prototypes and theoretical frameworks, in Decentralized
Identity Management Systems (DIDMS). It is safe to say that Attestation-Based
Attribute-Based Decentralized IDM (ABABDIDM) has not received anywhere near the
same level of attention in the literature as general Attribute-Based DIDMs
(ABDIDM), i.e, decentralized Attribute-Based Access Control (ABAC). The use of
decentralization, i.e., DIDM, is to improve upon the security and
privacy-related issues of centralized Identity Management Systems (IDM) and
Attribute-Based IDMs (ABIDM). And blockchain is the framework used for
decentralization in all these schemes. Many DIDMs - even ABDIDMs - have been
defined on popular blockchains such as Hyperledger, Ethereum, and Bitcoin.
However, despite the characteristics of Ripple that makes it appealing for an
ABIDM, there is a lack of research to develop an Identity Management System
(IDMS) on Ripple in literature. We have attempted to conceptualize an ABABDIDM
on Ripple.

</details>


### [7] [Auditable Early Stopping for Agentic Routing: Ledger-Verified Run-Wise Certificates under Local DP](https://arxiv.org/abs/2509.10550)
*Shivam Akhauri*

Main category: cs.CR

TL;DR: The paper introduces run-wise early-stopping certificates for differentially private best-first search in tool-use pipelines to maintain local differential privacy and generate auditable trails, with methods for exact and surrogate modes and a compiler for DAGs, validated through experiments and reproducible results.


<details>
  <summary>Details</summary>
Motivation: The motivation behind the paper is to address the challenge of early stopping in differentially private tool-use pipelines where agents sequentially carry out actions like retrieval, summarization, and calculation. Routers need reliable stopping mechanisms to preserve local DP while ensuring audit trails are maintained for transparent tracking of the decision-making process.

Method: The method proposed in the paper includes a technique for early stopping in probabilistic best-first search, introducing certificates for exact and surrogate modes to enhance privacy guarantees. The approach involves coupling realized path scores with pruning keys using an exponential race realized through offset propagation. It presents a compiler from shared-node DAGs to prefix DAGs, routines for maintaining count guarantees, and a deterministic ledger system for audit trails.

Result: The results demonstrate the effectiveness of the two-stop modes (Exact and Surrogate) in maintaining differentially private (DP) guarantees, along with the practical utilities of the compiler, validator, and control mechanisms. The paper provides a small real tool-use pipeline and auditable trails, offering evidence of DP preservation through logging and logging of all behaviors with results that are reproducible and verifiable using commodity hardware.

Conclusion: The conclusion highlights that the introduced method successfully enables private and early stopping best-first search in tool-use systems through two modes of operation and a validation system. It showcases the robustness of the DP guarantees, the comprehensibility of audit trails, and the effectiveness of the compiler and control mechanisms in real-world scenarios.

Abstract: In production tool-use agents (e.g., retrieval $\to$ summarization $\to$
calculator), routers must know when to stop exploring while preserving local DP
and leaving an auditable trail. We present run-wise early-stopping certificates
for perturb-and-MAP (PaM) best-first search on context-indexed prefix DAGs
whose children partition the leaves. We couple realized path scores and pruning
keys to a single exponential race realized lazily via offset propagation. With
exact leaf counts $N(v)$, lazy reuse at winners and independent residuals yield
an Exact mode with a sound halting rule based on Key$(v) = M_tau(v) - \log
t(v)$, where $t(v)$ is the minimum arrival time among leaves under $v$. With
only upper bounds $N_{ub} \ge N$, a Surrogate mode uses a parent-anchored
surrogate race without winner reuse; because $-\log \hat t \ge -\log t$, the
frontier invariant holds and stopping remains sound. We add a compiler from
shared-node DAGs to prefix DAGs, local finiteness checks, a SuffixCountDP
routine for exact counts with safe downgrades, a validator-side tightening term
$\kappa = \log(N/N_{ub})$, and an auditable ledger/validator that replays runs
deterministically. We also give an absolute LogSumExp tail bound, an acyclicity
certificate, and a fallback PRF-per-leaf scheme (NoCert) whose work matches a
realized-score best-first baseline up to a small per-node overhead. Finally, we
integrate a price/latency/$(\epsilon, \delta)$-aware multi-LLM controller and
DP-trained LoRA adapters chosen at runtime; these choices do not affect the
two-mode frontier invariants. We report Mac/commodity-hardware reproducible
results, a small real tool-use pipeline, and validator-checked audit trails,
with code and ledgers provided.

</details>


### [8] [A Hybrid Encryption Framework Combining Classical, Post-Quantum, and QKD Methods](https://arxiv.org/abs/2509.10551)
*Amal Raj,Vivek Balachandran*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper introduces a hybrid encryption framework combining classical
cryptography (EdDSA, ECDH), post-quantum cryptography (ML-DSA-6x5, ML-KEM-768),
and Quantum Key Distribution (QKD) via Guardian to counter quantum computing
threats. Our prototype implements this integration, using a key derivation
function to generate secure symmetric and HMAC keys, and evaluates its
performance across execution time and network metrics. The approach improves
data protection by merging classical efficiency with PQC's quantum resilience
and QKD's key security, offering a practical transition path for cryptographic
systems. This research lays the foundation for future adoption of PQC in
securing digital communication.

</details>


### [9] [AVEC: Bootstrapping Privacy for Local LLMs](https://arxiv.org/abs/2509.10561)
*Madhava Gaikwad*

Main category: cs.CR

TL;DR: AVEC proposes a privacy framework for local LLMs using adaptive differential privacy budgets, verifiable edge checks, and theoretical guarantees, validated through simulations but requiring future empirical deployment testing.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of enforcing privacy for local language models at the edge, particularly when handling delegated queries, through explicit verifiability and adaptive privacy mechanisms to prevent over- or under-protecting sensitive data.

Method: AVEC introduces an adaptive budgeting algorithm allocating differential privacy parameters based on query sensitivity, local confidence, and historical usage, combined with verifiable transformations and on-device integrity checks. Theoretical guarantees are formalized using R\'enyi differential privacy, odometer-based accounting, and bounds on delegation-leakage and deterministic gating limitations.

Result: Simulation-based evaluations validate the behavior of AVEC's mechanisms and accounting systems, though deployment readiness or task-level utility with live LLMs is not claimed. Theoretical contributions include utility ceilings, leakage bounds, and impossibility results.

Conclusion: The paper establishes AVEC as a conceptual framework and theoretical foundation for future empirical research on bootstrapping privacy in local LLMs, highlighting pathways for follow-up studies despite not claiming deployment readiness.

Abstract: This position paper presents AVEC (Adaptive Verifiable Edge Control), a
framework for bootstrapping privacy for local language models by enforcing
privacy at the edge with explicit verifiability for delegated queries. AVEC
introduces an adaptive budgeting algorithm that allocates per-query
differential privacy parameters based on sensitivity, local confidence, and
historical usage, and uses verifiable transformation with on-device integrity
checks. We formalize guarantees using R\'enyi differential privacy with
odometer-based accounting, and establish utility ceilings, delegation-leakage
bounds, and impossibility results for deterministic gating and hash-only
certification. Our evaluation is simulation-based by design to study mechanism
behavior and accounting; we do not claim deployment readiness or task-level
utility with live LLMs. The contribution is a conceptual architecture and
theoretical foundation that chart a pathway for empirical follow-up on
privately bootstrapping local LLMs.

</details>


### [10] [Enhancing IoMT Security with Explainable Machine Learning: A Case Study on the CICIOMT2024 Dataset](https://arxiv.org/abs/2509.10563)
*Mohammed Yacoubi,Omar Moussaoui,C. Drocourt*

Main category: cs.CR

TL;DR: This study evaluates bagging (Random Forests) and boosting (CatBoost)


<details>
  <summary>Details</summary>
Motivation: The opacity of AI models in cybersecurity, especially for IoMT devices, hinders trust and accountability despite their effectiveness in detecting threats.

Method: Compared ensemble learners (Random Forest for bagging, CatBoost for boosting), then applied SHAP and LIME XAI techniques to generate local/global explanations for model decisions.

Result: Ensemble methods showed strong performance for cyber-attack classification, but their complexity reduced transparency; XAI techniques provided feature importance insights to address this limitation.

Conclusion: XAI integration with ensemble learning offers a balanced approach for effective and interpretable cyber threat detection in medical IoT environments, though further optimization is needed.

Abstract: Explainable Artificial Intelligence (XAI) enhances the transparency and
interpretability of AI models, addressing their inherent opacity. In
cybersecurity, particularly within the Internet of Medical Things (IoMT), the
black-box nature of AI-driven threat detection poses a significant challenge.
Cybersecurity professionals must not only detect attacks but also understand
the reasoning behind AI decisions to ensure trust and accountability. The rapid
increase in cyberattacks targeting connected medical devices threatens patient
safety and data privacy, necessitating advanced AI-driven solutions. This study
compares two ensemble learning techniques, bagging and boosting, for
cyber-attack classification in IoMT environments. We selected Random Forest for
bagging and CatBoost for boosting. Random Forest helps reduce variance, while
CatBoost improves bias by combining weak classifiers into a strong ensemble
model, making them effective for detecting sophisticated attacks. However,
their complexity often reduces transparency, making it difficult for
cybersecurity professionals to interpret and trust their decisions. To address
this issue, we apply XAI models to generate local and global explanations,
providing insights into AI decision-making. Using techniques like SHAP (Shapley
Additive Explanations) and LIME (Local Interpretable Model-agnostic
Explanations), we highlight feature importance to help stakeholders understand
the key factors driving cyber threat detection.

</details>


### [11] [SG-ML: Smart Grid Cyber Range Modelling Language](https://arxiv.org/abs/2509.10568)
*Muhammad M. Roomi,Suhail S. M. Hussain,Daisuke Mashima*

Main category: cs.CR

TL;DR: SG-ML is an XML-based modeling language for automated smart grid cyber range generation, integrating power/cyber system standards with customizable extensions to enable reproducible, scalable deployments.


<details>
  <summary>Details</summary>
Motivation: Existing ad-hoc cyber range approaches lack unified integration of power system and cyber network modeling, requiring a standardized solution for automated deployment while reusing existing infrastructure standards like IEC 61850 and PLCopen XML.

Method: Defined via XML schemas combining IEC 61850 SCL for power system topology, PLCopen XML for industrial automation, and proprietary schemas addressing security requirements (attack parameters, network constraints). An SG-ML Processor compiles these into executable environments.

Result: Demonstrated that SG-ML enables environment creation through standard asset reuse (IEC examples) and custom extensions for security scenarios, achieving reproducible cyber range generation with reduced manual effort.

Conclusion: SG-ML fills critical gaps in smart grid security assessment by providing a domain-specific language that harmonizes physical/cyber modeling standards with security requirements, enabling efficient, scalable, and standardized cyber range generation for research/training.

Abstract: This work provides a detailed specification of the Smart Grid Modelling
Language (SG-ML), which is designed for the automated generation of smart grid
cyber ranges. SG-ML is defined as a set of XML schemas that describe a smart
grid's configuration in both machine-readable and human-friendly ways, thereby
bridging the gap between system modelling and automated deployment. Unlike
prior ad-hoc approaches to cyber range design, SG-ML provides a unified
methodology that integrates both power system and cyber network
representations. The SG-ML model can be customized by users to meet specific
requirements, such as emulating physical or cyber topologies and configuring
network devices. An SG-ML Processor then parses this configured model to
instantiate the cyber range environment. The modelling language leverages
established standards like the IEC 61850 Substation Configuration Language
(SCL) and IEC 61131 PLCopen XML to define power system topology, cyber network
topology, and device configurations. This approach allows for the reuse of
existing assets, reducing the effort needed to create the SG-ML model. To
address gaps not covered by these standards such as attack injection
parameters, scenario-specific metadata, and additional network constraints,
SG-ML introduces proprietary schemas that complement standard models. Overall,
SG-ML enables reproducible, scalable, and automated generation of realistic
smart grid cyber ranges for research, training, and security assessment.

</details>


### [12] [MarkDiffusion: An Open-Source Toolkit for Generative Watermarking of Latent Diffusion Models](https://arxiv.org/abs/2509.10569)
*Leyi Pan,Sheng Guan,Zheyu Fu,Luyang Si,Zian Wang,Xuming Hu,Irwin King,Philip S. Yu,Aiwei Liu,Lijie Wen*

Main category: cs.CR

TL;DR: MarkDiffusion is an open-source toolkit for watermarking latent diffusion models with unified implementation, visualization, and standardized evaluation across 24 tools and 8 automated pipelines.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking methods lack standardized evaluation frameworks and visualization tools, hindering research progress and public understanding of watermarking effectiveness in latent diffusion models.

Method: The toolkit features: 1) a unified implementation framework for watermarking algorithms and user interfaces 2) visualization tools for watermark patterns and 3) an evaluation module with 24 tools across detectability, robustness, and quality, plus 8 automated pipelines.

Result: MarkDiffusion provides researchers with 1) a flexible platform for watermarking algorithm development, 2) intuitive visualization mechanisms for public engagement, and 3) standardized evaluation pipelines across 24 tools to benchmark watermarking performance.

Conclusion: MarkDiffusion advances generative watermarking research and applications by providing a unified, open-source toolkit that fosters collaboration and evaluates watermarking effectiveness through standardized metrics.

Abstract: We introduce MarkDiffusion, an open-source Python toolkit for generative
watermarking of latent diffusion models. It comprises three key components: a
unified implementation framework for streamlined watermarking algorithm
integrations and user-friendly interfaces; a mechanism visualization suite that
intuitively showcases added and extracted watermark patterns to aid public
understanding; and a comprehensive evaluation module offering standard
implementations of 24 tools across three essential aspects - detectability,
robustness, and output quality - plus 8 automated evaluation pipelines. Through
MarkDiffusion, we seek to assist researchers, enhance public awareness and
engagement in generative watermarking, and promote consensus while advancing
research and applications.

</details>


### [13] [Directionality of the Voynich Script](https://arxiv.org/abs/2509.10573)
*Christophe Parisel*

Main category: cs.CR

TL;DR: Authors present a statistical n-gram perplexity method to analyze the Voynich Manuscript's script direction, offering a quantitative approach to a historically unresolved problem.


<details>
  <summary>Details</summary>
Motivation: The Voynich Manuscript's script/cipher direction remains debated, and prior studies lack quantitative methods to address this question systematically.

Method: The authors developed a statistical approach that calculates n-gram perplexity asymmetry in character sequences to detect directional reading bias (LTR vs RTL) in the manuscript.

Result: The method demonstrates the ability to identify directional bias in character sequences, potentially resolving the ambiguity of whether the underlying system reads LTR or RTL.

Conclusion: This paper concludes that the proposed statistical method provides a viable approach to determining the directional bias of the Voynich Manuscript's script or cipher, offering a new analytical tool for such historical cryptographic problems.

Abstract: While the Voynich Manuscript was almost certainly written left-to-right
(LTR), the question whether the underlying script or cipher reads LTR or
right-to-left (RTL) has received little quantitative attention. We introduce a
statistical method that leverages n-gram perplexity asymmetry to determine
directional bias in character sequences.

</details>


### [14] [The Coding Limits of Robust Watermarking for Generative Models](https://arxiv.org/abs/2509.10577)
*Danilo Francati,Yevin Nikhel Goonatilake,Shubham Pawar,Daniele Venturi,Giuseppe Ateniese*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We prove a sharp threshold for the robustness of cryptographic watermarking
for generative models. This is achieved by introducing a coding abstraction,
which we call messageless secret-key codes, that formalizes sufficient and
necessary requirements of robust watermarking: soundness, tamper detection, and
pseudorandomness. Thus, we establish that robustness has a precise limit: For
binary outputs no scheme can survive if more than half of the encoded bits are
modified, and for an alphabet of size q the corresponding threshold is
$(1-1/q)$ of the symbols.
  Complementing this impossibility, we give explicit constructions that meet
the bound up to a constant slack. For every ${\delta} > 0$, assuming
pseudorandom functions and access to a public counter, we build linear-time
codes that tolerate up to $(1/2)(1-{\delta})$ errors in the binary case and
$(1-1/q)(1-{\delta})$ errors in the $q$-ary case. Together with the lower
bound, these yield the maximum robustness achievable under standard
cryptographic assumptions.
  We then test experimentally whether this limit appears in practice by looking
at the recent watermarking for images of Gunn, Zhao, and Song (ICLR 2025). We
show that a simple crop and resize operation reliably flipped about half of the
latent signs and consistently prevented belief-propagation decoding from
recovering the codeword, erasing the watermark while leaving the image visually
intact.
  These results provide a complete characterization of robust watermarking,
identifying the threshold at which robustness fails, constructions that achieve
it, and an experimental confirmation that the threshold is already reached in
practice.

</details>


### [15] [Multi-channel secure communication framework for wireless IoT (MCSC-WoT): enhancing security in Internet of Things](https://arxiv.org/abs/2509.10581)
*Prokash Barman,Ratul Chowdhury,Banani Saha*

Main category: cs.CR

TL;DR: This paper proposes MCSC, a framework combining dynamic channel-hopping and advanced cryptography to secure IoT/WoT networks while optimizing performance metrics.


<details>
  <summary>Details</summary>
Motivation: Existing security solutions for IoT/WoT face limitations against evolving attacks (MITM, jamming) and suffer from synchronization challenges that increase latency and energy consumption in resource-constrained devices.

Method: MCSC integrates advanced cryptographic protocols with dynamic channel-hopping strategies to reduce synchronization overhead and enhance security metrics like packet delivery ratio and energy efficiency.

Result: MCSC demonstrates lower error rates, stronger resilience to diverse cyber threats, and outperforms Frequency Hop Spread Spectrum, AES, and ECC schemes in critical performance metrics through comprehensive comparisons.

Conclusion: The proposed MCSC framework effectively secures IoT and WoT networks without compromising operational performance, validated under various interference conditions.

Abstract: In modern smart systems, the convergence of the Internet of Things (IoT) and
Wireless of Things (WoT) have been revolutionized by offering a broad level of
wireless connectivity and communication among various devices. Hitherto, this
greater interconnectivity poses important security problems, including the
question of how to securely interconnect different networks, preserve secure
communication channels, and maintain data integrity. However, the traditional
cryptographic method and frequency hopping technique, although they provide
some protection, are not sufficient to defend against Man-In-The-Middle,
jamming, and replay attacks. In addition, synchronization issues in
multi-channel communication systems result in increased latency and energy
consumption, which make them unsuitable for resource-constrained IoT and WoT
devices. This work presents the Multi-Channel Secure Communication (MCSC)
framework, which integrates advanced cryptographic protocols with dynamic
channel-hopping strategies to enhance security with reduced synchronization
overhead. The MCSC framework maximizes the critical performance metrics, such
as packet delivery ratio, latency, throughput, and energy efficiency, and
fulfills the specific requirements of the IoT and WoT networks. A comprehensive
comparison of MCSC with well-established methods, including Frequency Hop
Spread Spectrum, single channel Advanced Encryption Standard, and various
Elliptic Curve Cryptography-based schemes, indicates that MCSC has lower error
rates and is more resilient to a wider range of cyber attacks. The efficiency
of the proposed solution to secure IoT and WoT networks without compromising
the operational performance is validated under various interference conditions.

</details>


### [16] [Safety and Security Analysis of Large Language Models: Risk Profile and Harm Potential](https://arxiv.org/abs/2509.10655)
*Charankumar Akiri,Harrison Simpson,Kshitiz Aryal,Aarav Khanna,Maanak Gupta*

Main category: cs.CR

TL;DR: The paper assesses the safety and security of various LLMs against adversarial prompts, introduces a Risk Severity Index, and identifies vulnerabilities calling for improved governance.


<details>
  <summary>Details</summary>
Motivation: The motivation revolves around understanding the safety and security risks of widely used LLMs as adversarial techniques evolve, emphasizing the potential societal threats from these vulnerabilities.

Method: An empirical analysis is conducted by evaluating nine prominent LLMs across 24 security and safety categories, using adversarial prompts to test their generation of harmful outputs.

Result: Observed vulnerabilities in safety filters and a preliminary evaluation of the Risk Severity Index (RSI) as a scalable metric to quantify security postures and risk levels of LLMs.

Conclusion: There is an urgent need for enhanced alignment, responsible deployment practices, and model governance in LLM development to mitigate the identified risks.

Abstract: While the widespread deployment of Large Language Models (LLMs) holds great
potential for society, their vulnerabilities to adversarial manipulation and
exploitation can pose serious safety, security, and ethical risks. As new
threats continue to emerge, it becomes critically necessary to assess the
landscape of LLMs' safety and security against evolving adversarial prompt
techniques. To understand the behavior of LLMs, this research provides an
empirical analysis and risk profile of nine prominent LLMs, Claude Opus 4,
DeepSeek V3 (both open-source and online), Gemini 2.5 Flash, GPT-4o, Grok 3,
Llama 4 Scout, Mistral 7B, and Qwen 3 1.7B, against 24 different security and
safety categories. These LLMs are evaluated on their ability to produce harmful
responses for adversarially crafted prompts (dataset has been made public) for
a broad range of safety and security topics, such as promotion of violent
criminal behavior, promotion of non-violent criminal activity, societal harms
related to safety, illegal sexual content, dangerous code generation, and
cybersecurity threats beyond code. Our study introduces the Risk Severity Index
(RSI), an agile and scalable evaluation score, to quantify and compare the
security posture and creating a risk profile of LLMs. As the LLM development
landscape progresses, the RSI is intended to be a valuable metric for comparing
the risks of LLMs across evolving threats. This research finds widespread
vulnerabilities in the safety filters of the LLMs tested and highlights the
urgent need for stronger alignment, responsible deployment practices, and model
governance, particularly for open-access and rapidly iterated models.

</details>


### [17] [LLM in the Middle: A Systematic Review of Threats and Mitigations to Real-World LLM-based Systems](https://arxiv.org/abs/2509.10682)
*Vitor Hugo Galhardo Moia,Igor Jochem Sanz,Gabriel Antonio Fontes Rebello,Rodrigo Duarte de Meneses,Briland Hitaj,Ulf Lindqvist*

Main category: cs.CR

TL;DR: This survey systematically categorizes security threats and defenses for LLM-based systems across their life cycles, aiding stakeholders in risk mitigation and highlighting open challenges for secure AI adoption.


<details>
  <summary>Details</summary>
Motivation: The growing adoption of LLMs has drawn cybercriminals seeking to exploit them, necessitating effective security measures against both traditional and novel threats targeting LLMs and their integration.

Method: Systematic review and categorization of threats and defenses across the software and LLM life cycles, analyzing real-world scenarios, classifying threats by severity and context, and mapping defense strategies to life cycle phases and attack strategies.

Result: A structured analysis of threats, their classification by severity and scenario, and systematically categorized defense strategies mapped to life cycle phases and corresponding attack attenuation.

Conclusion: This survey provides a comprehensive categorization of security and privacy threats in LLM-based systems, offering defense strategies mapped to life cycles and attack scenarios. It aims to assist stakeholders in mitigating risks and addressing open challenges for secure LLM adoption.

Abstract: The success and wide adoption of generative AI (GenAI), particularly large
language models (LLMs), has attracted the attention of cybercriminals seeking
to abuse models, steal sensitive data, or disrupt services. Moreover, providing
security to LLM-based systems is a great challenge, as both traditional threats
to software applications and threats targeting LLMs and their integration must
be mitigated. In this survey, we shed light on security and privacy concerns of
such LLM-based systems by performing a systematic review and comprehensive
categorization of threats and defensive strategies considering the entire
software and LLM life cycles. We analyze real-world scenarios with distinct
characteristics of LLM usage, spanning from development to operation. In
addition, threats are classified according to their severity level and to which
scenarios they pertain, facilitating the identification of the most relevant
threats. Recommended defense strategies are systematically categorized and
mapped to the corresponding life cycle phase and possible attack strategies
they attenuate. This work paves the way for consumers and vendors to understand
and efficiently mitigate risks during integration of LLMs in their respective
solutions or organizations. It also enables the research community to benefit
from the discussion of open challenges and edge cases that may hinder the
secure and privacy-preserving adoption of LLM-based systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [18] [Quality Assessment of Tabular Data using Large Language Models and Code Generation](https://arxiv.org/abs/2509.10572)
*Ashlesha Akella,Akshar Kaul,Krishnasuri Narayanam,Sameep Mehta*

Main category: cs.SE

TL;DR: The paper proposes a three-stage framework combining statistical inliner detection and LLM-driven rule/code generation to improve tabular data quality validation, addressing inefficiencies of traditional rule-based methods.


<details>
  <summary>Details</summary>
Motivation: Rule-based validation for tabular data is inefficient, requires human intervention, and is computationally expensive, motivating the need for automated, scalable solutions.

Method: The framework first filters data via traditional clustering, then iteratively leverages LLMs (augmented with RAG and domain-specific examples) to generate semantically valid rules and executable code, enforced by guardrails for accuracy and consistency.

Result: Extensive evaluations on benchmark datasets demonstrate the approach's effectiveness in generating reliable data quality rules and validations.

Conclusion: The proposed method successfully addresses limitations of existing approaches by integrating statistical techniques with LLMs, achieving efficient and accurate data quality validation.

Abstract: Reliable data quality is crucial for downstream analysis of tabular datasets,
yet rule-based validation often struggles with inefficiency, human
intervention, and high computational costs. We present a three-stage framework
that combines statistical inliner detection with LLM-driven rule and code
generation. After filtering data samples through traditional clustering, we
iteratively prompt LLMs to produce semantically valid quality rules and
synthesize their executable validators through code-generating LLMs. To
generate reliable quality rules, we aid LLMs with retrieval-augmented
generation (RAG) by leveraging external knowledge sources and domain-specific
few-shot examples. Robust guardrails ensure the accuracy and consistency of
both rules and code snippets. Extensive evaluations on benchmark datasets
confirm the effectiveness of our approach.

</details>


### [19] [Reasonable Experiments in Model-Based Systems Engineering](https://arxiv.org/abs/2509.10649)
*Johan Cederbladh,Loek Cleophas,Eduard Kamburjan,Lucas Lima,Rakshit Mittal,Hans Vangheluwe*

Main category: cs.SE

TL;DR: This paper introduces an experiment-management framework that leverages case-based reasoning and domain knowledge to intelligently reuse experimental data, reducing redundant experiments and accelerating system design in digital engineering contexts.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the inefficiency and high resource demand of conducting repeated experiments in digital engineering and early validation & verification. Efficient reuse of experimental data is critical to accelerate system design efforts and avoid unnecessary resource expenditure.

Method: A framework is developed for managing experiments (both digital and physical) using case-based reasoning with domain knowledge. It includes an experiment manager architecture that determines if existing experiments can answer new queries without conducting them afresh. The approach is validated using an industrial vehicular energy system-design case study.

Result: Experimental validation through an industrial vehicular energy system-design case study demonstrates the feasibility and effectiveness of the proposed framework in reusing experimental data and reducing the need for redundant experiments.

Conclusion: The framework presented in this paper aims to reduce redundant experiments in Model-Based Systems Engineering by efficiently reusing experimental data through case-based reasoning and domain knowledge, thereby saving time and resources in the design process.

Abstract: With the current trend in Model-Based Systems Engineering towards Digital
Engineering and early Validation & Verification, experiments are increasingly
used to estimate system parameters and explore design decisions. Managing such
experimental configuration metadata and results is of utmost importance in
accelerating overall design effort. In particular, we observe it is important
to 'intelligent-ly' reuse experiment-related data to save time and effort by
not performing potentially superfluous, time-consuming, and resource-intensive
experiments. In this work, we present a framework for managing experiments on
digital and/or physical assets with a focus on case-based reasoning with domain
knowledge to reuse experimental data efficiently by deciding whether an
already-performed experiment (or associated answer) can be reused to answer a
new (potentially different) question from the engineer/user without having to
set up and perform a new experiment. We provide the general architecture for
such an experiment manager and validate our approach using an industrial
vehicular energy system-design case study.

</details>


### [20] [Arguzz: Testing zkVMs for Soundness and Completeness Bugs](https://arxiv.org/abs/2509.10819)
*Christoph Hochrainer,Valentin Wüstholz,Maria Christakis*

Main category: cs.SE

TL;DR: Arguzz is the first automated tool for testing zero-knowledge virtual machines (zkVMs) to detect soundness and completeness bugs. It combines metamorphic testing with fault injection to generate equivalent program pairs, execute them in zkVMs, and identify vulnerabilities, uncovering 11 bugs in three real-world zkVMs (e.g., RISC Zero).


<details>
  <summary>Details</summary>
Motivation: ZKVMs underpin blockchain scalability but are error-prone due to complex constraint systems, risking invalid execution acceptance (soundness flaws) or rejection of valid ones (completeness flaws). Current manual audits fail to catch all issues, necessitating systematic testing tools.

Method: Arguzz creates semantically equivalent Rust program pairs, merges them into a single test case with a known output, and executes it in a zkVM. Faults are injected to simulate malicious provers, revealing overly weak constraints by observing incorrect proof validation.

Result: Arguzz tested six major zkVMs and found 11 critical bugs in three (RISC Zero, Jolt, SP1), including a $50,000 bounty-winning vulnerability in RISC Zero. This demonstrates the high error rate in existing zkVMs and the effectiveness of Arguzz’s multifaceted approach.

Conclusion: Automated testing tools like Arguzz are critical for securing zkVMs, as manual audits and informal testing inadequately address their inherent complexity. The results highlight systemic issues in zkVM design and the need for robust, injection-based evaluation methods to ensure soundness and completeness.

Abstract: Zero-knowledge virtual machines (zkVMs) are increasingly deployed in
decentralized applications and blockchain rollups since they enable verifiable
off-chain computation. These VMs execute general-purpose programs, frequently
written in Rust, and produce succinct cryptographic proofs. However, zkVMs are
complex, and bugs in their constraint systems or execution logic can cause
critical soundness (accepting invalid executions) or completeness (rejecting
valid ones) issues.
  We present Arguzz, the first automated tool for testing zkVMs for soundness
and completeness bugs. To detect such bugs, Arguzz combines a novel variant of
metamorphic testing with fault injection. In particular, it generates
semantically equivalent program pairs, merges them into a single Rust program
with a known output, and runs it inside a zkVM. By injecting faults into the
VM, Arguzz mimics malicious or buggy provers to uncover overly weak
constraints.
  We used Arguzz to test six real-world zkVMs (RISC Zero, Nexus, Jolt, SP1,
OpenVM, and Pico) and found eleven bugs in three of them. One RISC Zero bug
resulted in a $50,000 bounty, despite prior audits, demonstrating the critical
need for systematic testing of zkVMs.

</details>


### [21] [TPSQLi: Test Prioritization for SQL Injection Vulnerability Detection in Web Applications](https://arxiv.org/abs/2509.10920)
*Guan-Yan Yang,Farn Wang,You-Zong Gu,Ya-Wen Teng,Kuo-Hui Yeh,Ping-Hsueh Ho,Wei-Ling Wen*

Main category: cs.SE

TL;DR: This paper introduces a dynamic SQL injection testing framework that prioritizes tests based on historical data and temporal vulnerability patterns, enhancing testing efficiency and defense adaptability in agile environments.


<details>
  <summary>Details</summary>
Motivation: The proliferation of network applications and the prevalence of injection attacks (ranked top 3 in OWASP Top 10 2021) have created a pressing need for more efficient vulnerability testing in agile development cycles.

Method: The proposed method employs a test prioritization framework that adjusts defense strength vectors based on previous test outcomes, incorporating dynamic adjustments and temporal aspects of vulnerability exposure to optimize testing workflows.

Result: The approach aims to improve vulnerability detection efficiency and mitigation effectiveness through a flexible framework that optimizes testing workflows with context-aware prioritization and adaptive defense mechanisms.

Conclusion: The paper introduces a novel test prioritization method for SQL injection vulnerabilities that enhances testing efficiency and effectiveness through dynamic adjustments and temporal considerations, contributing to more adaptive and tailored defense mechanisms in agile development cycles.

Abstract: The rapid proliferation of network applications has led to a significant
increase in network attacks. According to the OWASP Top 10 Projects report
released in 2021, injection attacks rank among the top three vulnerabilities in
software projects. This growing threat landscape has increased the complexity
and workload of software testing, necessitating advanced tools to support agile
development cycles. This paper introduces a novel test prioritization method
for SQL injection vulnerabilities to enhance testing efficiency. By leveraging
previous test outcomes, our method adjusts defense strength vectors for
subsequent tests, optimizing the testing workflow and tailoring defense
mechanisms to specific software needs. This approach aims to improve the
effectiveness and efficiency of vulnerability detection and mitigation through
a flexible framework that incorporates dynamic adjustments and considers the
temporal aspects of vulnerability exposure.

</details>


### [22] [When the Code Autopilot Breaks: Why LLMs Falter in Embedded Machine Learning](https://arxiv.org/abs/2509.10946)
*Roberto Morabito,Guanghan Wu*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) are increasingly used to automate software
generation in embedded machine learning workflows, yet their outputs often fail
silently or behave unpredictably. This article presents an empirical
investigation of failure modes in LLM-powered ML pipelines, based on an
autopilot framework that orchestrates data preprocessing, model conversion, and
on-device inference code generation. We show how prompt format, model behavior,
and structural assumptions influence both success rates and failure
characteristics, often in ways that standard validation pipelines fail to
detect. Our analysis reveals a diverse set of error-prone behaviors, including
format-induced misinterpretations and runtime-disruptive code that compiles but
breaks downstream. We derive a taxonomy of failure categories and analyze
errors across multiple LLMs, highlighting common root causes and systemic
fragilities. Though grounded in specific devices, our study reveals broader
challenges in LLM-based code generation. We conclude by discussing directions
for improving reliability and traceability in LLM-powered embedded ML systems.

</details>


### [23] [Hardness, Structural Knowledge, and Opportunity: An Analytical Framework for Modular Performance Modeling](https://arxiv.org/abs/2509.11000)
*Omid Gheibi,Christian Kästner,Pooyan Jamshidi*

Main category: cs.SE

TL;DR: This paper systematically analyzes how structural aspects (modules, options) and available structural knowledge affect performance modeling. It shows modeling hardness increases with system complexity, with structural knowledge and hardness jointly enhancing opportunities. The impact varies between ranking vs. prediction tasks.


<details>
  <summary>Details</summary>
Motivation: The gap lies in understanding how structural knowledge and system-specific aspects (e.g., module counts) influence model improvements. This work investigates how these factors interact with modeling hardness and opportunities for enhanced performance modeling.

Method: The authors introduced a concept of 'modeling hardness' and used controlled experiments with synthetic system models to create an 'analytical matrix' measuring how structural aspects (e.g., modules and options per module) and structural knowledge influence modeling opportunities. Experiments quantified the impact on performance metrics (ranking accuracy vs. prediction accuracy).

Result: Modeling hardness is driven by module counts and configuration options per module. Higher structural knowledge and modeling hardness increase improvement opportunities. Structural knowledge dominates for ranking accuracy (debugging tasks), while hardness dominates for prediction accuracy (resource management tasks).

Conclusion: The paper concludes that understanding structural aspects and structural knowledge is crucial for improving modular performance modeling. It provides actionable insights for system designers to strategically allocate resources and choose modeling approaches based on system characteristics and task objectives.

Abstract: Performance-influence models are beneficial for understanding how
configurations affect system performance, but their creation is challenging due
to the exponential growth of configuration spaces. While gray-box approaches
leverage selective "structural knowledge" (like the module execution graph of
the system) to improve modeling, the relationship between this knowledge, a
system's characteristics (we call them "structural aspects"), and potential
model improvements is not well understood. This paper addresses this gap by
formally investigating how variations in structural aspects (e.g., the number
of modules and options per module) and the level of structural knowledge impact
the creation of "opportunities" for improved "modular performance modeling". We
introduce and quantify the concept of modeling "hardness", defined as the
inherent difficulty of performance modeling. Through controlled experiments
with synthetic system models, we establish an "analytical matrix" to measure
these concepts. Our findings show that modeling hardness is primarily driven by
the number of modules and configuration options per module. More importantly,
we demonstrate that both higher levels of structural knowledge and increased
modeling hardness significantly enhance the opportunity for improvement. The
impact of these factors varies by performance metric; for ranking accuracy
(e.g., in debugging task), structural knowledge is more dominant, while for
prediction accuracy (e.g., in resource management task), hardness plays a
stronger role. These results provide actionable insights for system designers,
guiding them to strategically allocate time and select appropriate modeling
approaches based on a system's characteristics and a given task's objectives.

</details>


### [24] [ViScratch: Using Large Language Models and Gameplay Videos for Automated Feedback in Scratch](https://arxiv.org/abs/2509.11065)
*Yuan Si,Daming Li,Hanyuan Shi,Jialu Zhang*

Main category: cs.SE

TL;DR: ViScratch is a first-of-its-kind multimodal Scratch debugger that combines gameplay video and block code analysis in a two-stage pipeline, outperforming existing tools by leveraging video as a primary debugging signal.


<details>
  <summary>Details</summary>
Motivation: Block-based programming environments like Scratch suffer from semantic bugs despite syntax protection. Existing tools rely on predefined rules or manual input and ignore the platform’s visual nature, creating a need for visual-aware debugging solutions.

Method: ViScratch uses a two-stage pipeline: a vision-language model aligns visual symptoms from gameplay video with code structure to identify critical issues, then proposes minimal abstract syntax tree (AST) repairs verified via execution in the Scratch virtual machine.

Result: ViScratch outperforms prior tools in bug identification and repair quality when evaluated on real-world projects, even without access to project descriptions or goals, showing gameplay video is a crucial debugging signal.

Conclusion: This work demonstrates that video can serve as a first-class specification in visual programming environments, opening new directions for LLM-based debugging beyond symbolic code alone.

Abstract: Block-based programming environments such as Scratch are increasingly popular
in programming education, in particular for young learners. While the use of
blocks helps prevent syntax errors, semantic bugs remain common and difficult
to debug. Existing tools for Scratch debugging rely heavily on predefined rules
or user manual inputs, and crucially, they ignore the platform's inherently
visual nature.
  We introduce ViScratch, the first multimodal feedback generation system for
Scratch that leverages both the project's block code and its generated gameplay
video to diagnose and repair bugs. ViScratch uses a two-stage pipeline: a
vision-language model first aligns visual symptoms with code structure to
identify a single critical issue, then proposes minimal, abstract syntax tree
level repairs that are verified via execution in the Scratch virtual machine.
  We evaluate ViScratch on a set of real-world Scratch projects against
state-of-the-art LLM-based tools and human testers. Results show that gameplay
video is a crucial debugging signal: ViScratch substantially outperforms prior
tools in both bug identification and repair quality, even without access to
project descriptions or goals. This work demonstrates that video can serve as a
first-class specification in visual programming environments, opening new
directions for LLM-based debugging beyond symbolic code alone.

</details>


### [25] [Rethinking Technology Stack Selection with AI Coding Proficiency](https://arxiv.org/abs/2509.11132)
*Xiaoyu Zhang,Weipeng Jiang,Juan Zhai,Shiqing Ma,Qingshuang Bao,Chenhao Lin,Chao Shen,Tianlin Li,Yang Liu*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) are now an integral part of software development
workflows and are reshaping the whole process. Traditional technology stack
selection has not caught up. Most of the existing selection methods focus
solely on the inherent attributes of the technology, overlooking whether the
LLM can effectively leverage the chosen technology. For example, when
generating code snippets using popular libraries like Selenium (one of the most
widely used test automation tools with over 33k GitHub stars), existing LLMs
frequently generate low-quality code snippets (e.g., using deprecated APIs and
methods, or containing syntax errors). As such, teams using LLM assistants risk
choosing technologies that cannot be used effectively by LLMs, yielding high
debugging effort and mounting technical debt. We foresee a practical question
in the LLM era, is a technology ready for AI-assisted development? In this
paper, we first propose the concept, AI coding proficiency, the degree to which
LLMs can utilize a given technology to generate high-quality code snippets. We
conduct the first comprehensive empirical study examining AI proficiency across
170 third-party libraries and 61 task scenarios, evaluating six widely used
LLMs. Our findings reveal that libraries with similar functionalities can
exhibit up to 84% differences in the quality score of LLM-generated code, while
different models also exhibit quality gaps among their generation results using
the same library. These gaps translate into real engineering costs and can
steer developer choices toward a narrow set of libraries with high AI coding
proficiency, threatening technological diversity in the ecosystem. We call on
the community to integrate AI proficiency assessments into technology selection
frameworks and develop mitigation strategies, preserving competitive balance in
AI-driven development.

</details>


### [26] [UserTrace: User-Level Requirements Generation and Traceability Recovery from Software Project Repositories](https://arxiv.org/abs/2509.11238)
*Dongming Jin,Zhi Jin,Yiran Zhang,Zheng Fang,Linyu Li,Yuanpeng He,Xiaohong Chen,Weisong Sun*

Main category: cs.SE

TL;DR: UserTrace is a multi-agent system that automates user-level requirement generation and trace link recovery, improving software maintainability and enabling user validation of AI-generated software.


<details>
  <summary>Details</summary>
Motivation: Existing automated code summarization (ACS) and requirements traceability (RT) techniques primarily focus on developer-oriented requirements and lack consideration for project evolution, leaving user-level requirements (URs) and live trace links underexplored despite their critical role in software maintainability and user intent validation.

Method: UserTrace employs a multi-agent system with four specialized agents (Code Reviewer, Searcher, Writer, and Verifier) operating through three phases: structuring repository dependencies, deriving implementation-level requirements (IRs), and synthesizing user-level requirements (URs) with domain-specific context.

Result: UserTrace outperforms an established baseline in UR completeness, correctness, and helpfulness, achieves higher precision in trace link recovery than five state-of-the-art RT approaches, and demonstrates practical utility through a user study showing its effectiveness in validating AI-generated software against user intent.

Conclusion: UserTrace effectively addresses the gap in generating user-level requirements and live trace links, enhancing software maintainability and enabling end users to validate AI-generated software alignment with their intent.

Abstract: Software maintainability critically depends on high-quality requirements
descriptions and explicit traceability between requirements and code. Although
automated code summarization (ACS) and requirements traceability (RT)
techniques have been widely studied, existing ACS methods mainly generate
implementation-level (i.e., developer-oriented) requirements (IRs) for
fine-grained units (e.g., methods), while RT techniques often overlook the
impact of project evolution. As a result, user-level (i.e., end user-oriented)
requirements (URs) and live trace links remain underexplored, despite their
importance for supporting user understanding and for validating whether
AI-generated software aligns with user intent. To address this gap, we propose
UserTrace, a multi-agent system that automatically generates URs and recovers
live trace links (from URs to IRs to code) from software repositories.
UserTrace coordinates four specialized agents (i.e., Code Reviewer, Searcher,
Writer, and Verifier) through a three-phase process: structuring repository
dependencies, deriving IRs for code units, and synthesizing URs with
domain-specific context. Our comparative evaluation shows that UserTrace
produces URs with higher completeness, correctness, and helpfulness than an
established baseline, and achieves superior precision in trace link recovery
compared to five state-of-the-art RT approaches. A user study further
demonstrates that UserTrace helps end users validate whether the AI-generated
repositories align with their intent.

</details>


### [27] [Beyond Autoregression: An Empirical Study of Diffusion Large Language Models for Code Generation](https://arxiv.org/abs/2509.11252)
*Chengze li,Yitong Zhang,Jia Li,Liyi Cai,Ge Li*

Main category: cs.SE

TL;DR: This paper investigates diffusion LLMs for code generation, addressing limitations of autoregressive models (low efficiency, fixed left-to-right order). It compares 9 diffusion LLMs with autoregressive counterparts through empirical studies on 4 benchmarks, revealing their competitiveness, strong extrapolation, and practical optimizations.


<details>
  <summary>Details</summary>
Motivation: Autoregressive LLMs for code generation struggle with efficiency (token-by-token generation) and the inherently non-sequential nature of programming. Diffusion LLMs propose multi-token prediction and flexible generation order but lack systematic study in code generation.

Method: The authors conduct the first empirical study on diffusion LLMs for code generation, evaluating 9 models across 4 benchmarks. They analyze effectiveness, efficiency, and length extrapolation, exploring factors affecting performance and proposing practical guidance.

Result: Key findings: (1)
diffusion LLMs match autoregressive models with similar sizes; (2)
stronger length extrapolation and long code comprehension; (3)
practical optimizations based on effectiveness/efficiency factors; (4)
directions for model improvement. Open-source resources are provided.

Conclusion: Diffusion LLMs demonstrate promising potential for code generation, overcoming autoregressive limitations. The study establishes foundational benchmarks and insights for future research, with open-source tools to accelerate development in this area.

Abstract: LLMs have become the mainstream approaches to code generation. Existing LLMs
mainly employ autoregressive generation, i.e. generating code token-by-token
from left to right. However, the underlying autoregressive generation has two
limitations in code generation. First, autoregressive LLMs only generate a
token at each step, showing low efficiency in practice. Second, programming is
a non-sequential process involving back-and-forth editing, while autoregressive
LLMs only employ the left-to-right generation order. These two intrinsic
limitations hinder the further development of LLMs in code generation.
Recently, diffusion LLMs have emerged as a promising alternative. Diffusion
LLMs address the above limitations with two advances, including multi-token
prediction (i.e. generating multiple tokens at each step) and flexible
generation order (i.e. flexibly determining which positions to generate
tokens). However, there is no systematic study exploring diffusion LLMs in code
generation. To bridge the knowledge gap, we present the first empirical study
of diffusion LLMs for code generation. Our study involves 9 representative
diffusion LLMs and conduct experiments on 4 widely used benchmarks. Based on
the results, we summarize the following findings. (1) Existing diffusion LLMs
are competitive with autoregressive LLMs with similar sizes. (2) Diffusion LLMs
have a stronger length extrapolation ability than autoregressive LLMs and
perform better in long code understanding. (3) We explore factors impacting the
effectiveness and efficiency of diffusion LLMs, and provide practical guidance.
(4) We discuss several promising further directions to improve diffusion LLMs
on code generation. We open-source all source code, data, and results to
facilitate the following research. The code is publicly available at
https://github.com/zhangyitonggg/dllm4code.

</details>


### [28] [A Web-Based Environment for the Specification and Generation of Smart Legal Contracts](https://arxiv.org/abs/2509.11258)
*Regan Meloche,Durga Sivakumar,Amal A. Anda,Sofana Alfuhaid,Daniel Amyot,Luigi Logrippo,John Mylopoulos*

Main category: cs.SE

TL;DR: This paper presents a Web-based tool for converting legal contracts into smart contracts via Symboleo refinement, enabling automated compliance monitoring on Hyperledger Fabric.


<details>
  <summary>Details</summary>
Motivation: Monitoring contract compliance against legal obligations is critical for detecting violations promptly. However, a significant gap exists between natural language contracts and their smart contract implementations, hindering automation and compliance enforcement.

Method: A Web-based environment is introduced to support user-assisted refinement of Symboleo specifications derived from legal contract templates, followed by automated generation of monitoring smart contracts deployable on Hyperledger Fabric.

Result: The environment is illustrated using a transactive energy domain sample contract, showing its effectiveness in streamlining smart contract development while maintaining alignment with legal compliance requirements.

Conclusion: The proposed environment addresses the gap between natural language contracts and smart contracts, demonstrating significant potential for accelerating smart contract development in legal compliance contexts, particularly through its application in the transactive energy domain.

Abstract: Monitoring the compliance of contract performance against legal obligations
is important in order to detect violations, ideally, as soon as they occur.
Such monitoring can nowadays be achieved through the use of smart contracts,
which provide protection against tampering as well as some level of automation
in handling violations. However, there exists a large gap between natural
language contracts and smart contract implementations. This paper introduces a
Web-based environment that partly fills that gap by supporting the
user-assisted refinement of Symboleo specifications corresponding to legal
contract templates, followed by the automated generation of monitoring smart
contracts deployable on the Hyperledger Fabric platform. This environment,
illustrated using a sample contract from the transactive energy domain, shows
much potential in accelerating the development of smart contracts in a legal
compliance context.

</details>


### [29] [Weakly Supervised Vulnerability Localization via Multiple Instance Learning](https://arxiv.org/abs/2509.11312)
*Wenchao Gu,Yupan Chen,Yanlin Wang,Hongyu Zhang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: WAVES enables effective vulnerability detection and localization using only function-level labels, reducing reliance on expensive statement-level annotations while achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods require costly expert-labeled statement-level data for vulnerability localization. The need for a label-efficient approach to address this bottleneck drives the proposed weakly supervised solution.

Method: WAVES employs multiple instance learning to convert function-level labels into pseudo-statement-level labels, training classifiers using these pseudo labels to detect vulnerabilities and localize statements without manual annotation.

Result: Extensive experiments on three benchmarks demonstrate comparable vulnerability detection accuracy and superior statement-level localization performance compared to prior baselines.

Conclusion: The paper introduces WAVES, a weakly supervised approach for vulnerability localization that eliminates the need for statement-level labels. It achieves comparable vulnerability detection performance and state-of-the-art localization results on benchmark datasets.

Abstract: Software vulnerability detection has emerged as a significant concern in the
field of software security recently, capturing the attention of numerous
researchers and developers. Most previous approaches focus on coarse-grained
vulnerability detection, such as at the function or file level. However, the
developers would still encounter the challenge of manually inspecting a large
volume of code inside the vulnerable function to identify the specific
vulnerable statements for modification, indicating the importance of
vulnerability localization. Training the model for vulnerability localization
usually requires ground-truth labels at the statement-level, and labeling
vulnerable statements demands expert knowledge, which incurs high costs. Hence,
the demand for an approach that eliminates the need for additional labeling at
the statement-level is on the rise. To tackle this problem, we propose a novel
approach called WAVES for WeAkly supervised Vulnerability Localization via
multiplE inStance learning, which does not need the additional statement-level
labels during the training. WAVES has the capability to determine whether a
function is vulnerable (i.e., vulnerability detection) and pinpoint the
vulnerable statements (i.e., vulnerability localization). Specifically,
inspired by the concept of multiple instance learning, WAVES converts the
ground-truth label at the function-level into pseudo labels for individual
statements, eliminating the need for additional statement-level labeling. These
pseudo labels are utilized to train the classifiers for the function-level
representation vectors. Extensive experimentation on three popular benchmark
datasets demonstrates that, in comparison to previous baselines, our approach
achieves comparable performance in vulnerability detection and state-of-the-art
performance in statement-level vulnerability localization.

</details>


### [30] [Large Language Models (LLMs) for Requirements Engineering (RE): A Systematic Literature Review](https://arxiv.org/abs/2509.11446)
*Mohammad Amin Zadenoori,Jacek Dąbrowski,Waad Alhoshan,Liping Zhao,Alessio Ferrari*

Main category: cs.SE

TL;DR: This paper systematically reviews 74 studies (2023-2024) on applying Large Language Models (LLMs) to Requirements Engineering (RE), finding recent trends in usage patterns, prompting strategies, and task focus differences compared to earlier NLP-based work.


<details>
  <summary>Details</summary>
Motivation: The growing adoption of LLMs in RE warrants understanding new application patterns and gaps compared to traditional NLP approaches to guide researchers and practitioners.

Method: Systematic literature review of 74 primary studies from 2023-2024, categorizing research by RE activities, prompting strategies, evaluation methods, and technical artifacts.

Result: LLMs are increasingly used for requirements elicitation/validation over defect detection; novel tasks like test generation are emerging; GPT-based models with zero-/few-shot prompting dominate; industry application remains limited.

Conclusion: The review highlights expanding RE-LLM integration opportunities while identifying gaps in industrial deployment, complex workflow integration, and under-studied tasks, providing a curated list of tools and datasets for future work.

Abstract: Large Language Models (LLMs) are finding applications in numerous domains,
and Requirements Engineering (RE) is increasingly benefiting from their
capabilities to assist with complex, language-intensive tasks. This paper
presents a systematic literature review of 74 primary studies published between
2023 and 2024, examining how LLMs are being applied in RE. The study
categorizes the literature according to several dimensions, including
publication trends, RE activities, prompting strategies, and evaluation
methods. Our findings indicate notable patterns, among which we observe
substantial differences compared to previous works leveraging standard Natural
Language Processing (NLP) techniques. Most of the studies focus on using LLMs
for requirements elicitation and validation, rather than defect detection and
classification, which were dominant in the past. Researchers have also
broadened their focus and addressed novel tasks, e.g., test generation,
exploring the integration of RE with other software engineering (SE)
disciplines. Although requirements specifications remain the primary focus,
other artifacts are increasingly considered, including issues from issue
tracking systems, regulations, and technical manuals. The studies mostly rely
on GPT-based models, and often use Zero-shot or Few-shot prompting. They are
usually evaluated in controlled environments, with limited use in industry
settings and limited integration in complex workflows. Our study outlines
important future directions, such as leveraging the potential to expand the
influence of RE in SE, exploring less-studied tasks, improving prompting
methods, and testing in real-world environments. Our contribution also helps
researchers and practitioners use LLMs more effectively in RE, by providing a
list of identified tools leveraging LLMs for RE, as well as datasets.

</details>


### [31] [VulAgent: Hypothesis-Validation based Multi-Agent Vulnerability Detection](https://arxiv.org/abs/2509.11523)
*Ziliang Wang,Ge Li,Jia Li,Hao Zhu,Zhi Jin*

Main category: cs.SE

TL;DR: VulAgent, a multi-agent hypothesis-validation framework, enhances LLM-based vulnerability detection by structuring audits around context-aware hypotheses, significantly improving accuracy (6.6%+) and fixing false positives (36%↓).


<details>
  <summary>Details</summary>
Motivation: Language models struggle with project-level vulnerability detection due to the dual challenge of localizing security-sensitive code and reasoning over complex program context. Human auditors’ hypothesis-driven workflow inspired this solution.

Method: VulAgent is a multi-agent framework that aligns specialized agents with analysis perspectives (e.g., memory, authorization) to collaboratively detect vulnerabilities. It uses a hypothesis-validation paradigm where hypotheses are formed and verified against relevant program context via LLM interactions, reducing false positives.

Result: VulAgent achieves a 6.6% higher overall accuracy, up to 450% improvement in identifying vulnerable-fixed code pairs (246% average), and a 36% reduction in false positives compared to state-of-the-art LLM-based baselines across two datasets.

Conclusion: VulAgent effectively addresses the challenges of project-level vulnerability detection by employing a hypothesis-validation approach, leading to significant improvements in accuracy and a reduction in false positives, as demonstrated by the empirical results on two datasets.

Abstract: The application of language models to project-level vulnerability detection
remains challenging, owing to the dual requirement of accurately localizing
security-sensitive code and correctly correlating and reasoning over complex
program context. We present VulAgent, a multi-agent vulnerability detection
framework based on hypothesis validation. Our design is inspired by how human
auditors review code: when noticing a sensitive operation, they form a
hypothesis about a possible vulnerability, consider potential trigger paths,
and then verify the hypothesis against the surrounding context. VulAgent
implements a semantics-sensitive, multi-view detection pipeline: specialized
agents, each aligned to a specific analysis perspective (e.g., memory,
authorization), collaboratively surface and precisely localize sensitive code
sites with higher coverage. Building on this, VulAgent adopts a
hypothesis-validation paradigm: for each vulnerability report, it builds
hypothesis conditions and a trigger path, steering the LLM to target the
relevant program context and defensive checks during verification, which
reduces false positives. On average across the two datasets, VulAgent improves
overall accuracy by 6.6%, increases the correct identification rate of
vulnerable--fixed code pairs by up to 450% (246% on average), and reduces the
false positive rate by about 36% compared with state-of-the-art LLM-based
baselines.

</details>


### [32] [Sedeve-Kit, a Specification-Driven Development Framework for Building Distributed Systems](https://arxiv.org/abs/2509.11566)
*Hua Guo,Yunhong Ji,Xuan Zhou*

Main category: cs.SE

TL;DR: This paper proposes a three-stage specification-driven framework for distributed systems, using TLA+ to bridge design and implementation through continuous verification, addressing challenges in concurrency and faults.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges of non-deterministic concurrency and faults in distributed systems, necessitating a robust development approach to ensure correctness and reliability.

Method: The method involves three stages: (1) defining system specifications and invariants using TLA+ for model checking and test case generation, (2) writing code to ensure consistency with specifications, and (3) rigorous system testing using generated test cases.

Result: The framework enables systematic verification of distributed systems through specification alignment, model checking, and iterative testing, enhancing system quality and consistency.

Conclusion: The paper concludes that the proposed specification-driven development framework maintains a strong connection between abstract design and concrete implementation through continuous verification, ensuring system quality in distributed systems development.

Abstract: Developing distributed systems presents significant challenges, primarily due
to the complexity introduced by non-deterministic concurrency and faults. To
address these, we propose a specification-driven development framework. Our
method encompasses three key stages. The first stage defines system
specifications and invariants using TLA${^+}$. It allows us to perform model
checking on the algorithm's correctness and generate test cases for subsequent
development phases. In the second stage, based on the established
specifications, we write code to ensure consistency and accuracy in the
implementation. Finally, after completing the coding process, we rigorously
test the system using the test cases generated in the initial stage. This
process ensures system quality by maintaining a strong connection between the
abstract design and the concrete implementation through continuous
verification.

</details>


### [33] [Automated Creation and Enrichment Framework for Improved Invocation of Enterprise APIs as Tools](https://arxiv.org/abs/2509.11626)
*Prerna Agarwal,Himanshu Gupta,Soujanya Soni,Rohith Vallam,Renuka Sindhgatta,Sameep Mehta*

Main category: cs.SE

TL;DR: ACE is a novel framework that transforms enterprise APIs into LLM-compatible tools through automated documentation enrichment and dynamic selection, significantly improving agent performance in complex enterprise workflows.


<details>
  <summary>Details</summary>
Motivation: Poor API documentation, complex schemas, and extensive operations hinder accurate tool selection and payload formation in enterprise LLM applications, reducing accuracy by up to 25%.

Method: ACE employs two core mechanisms: (1) generating enriched tool specifications with parameter descriptions and examples, and (2) a dynamic runtime shortlisting mechanism to filter relevant tools, reducing complexity while maintaining scalability.

Result: Validation on proprietary and open-source APIs demonstrated ACE's integration with agentic frameworks, confirming its effectiveness as the first end-to-end solution for this problem.

Conclusion: The ACE framework effectively automates the creation, enrichment, and dynamic selection of enterprise API tools, enhancing LLM agents' efficiency and accuracy in enterprise environments.

Abstract: Recent advancements in Large Language Models (LLMs) has lead to the
development of agents capable of complex reasoning and interaction with
external tools. In enterprise contexts, the effective use of such tools that
are often enabled by application programming interfaces (APIs), is hindered by
poor documentation, complex input or output schema, and large number of
operations. These challenges make tool selection difficult and reduce the
accuracy of payload formation by up to 25%. We propose ACE, an automated tool
creation and enrichment framework that transforms enterprise APIs into
LLM-compatible tools. ACE, (i) generates enriched tool specifications with
parameter descriptions and examples to improve selection and invocation
accuracy, and (ii) incorporates a dynamic shortlisting mechanism that filters
relevant tools at runtime, reducing prompt complexity while maintaining
scalability. We validate our framework on both proprietary and open-source APIs
and demonstrate its integration with agentic frameworks. To the best of our
knowledge, ACE is the first end-to-end framework that automates the creation,
enrichment, and dynamic selection of enterprise API tools for LLM agents.

</details>


### [34] [Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based Information for Code Large Language Models](https://arxiv.org/abs/2509.11686)
*Jian Wang,Xiaofei Xie,Qiang Hu,Shangqing Liu,Yi Li*

Main category: cs.SE

TL;DR: This study found that integrating semantic execution traces into Code LLMs provides minimal improvement in reasoning capabilities during training and testing, suggesting new approaches are needed.


<details>
  <summary>Details</summary>
Motivation: Current Code LLMs struggle with runtime behavior reasoning and fragmented semantic representations, creating challenges for practical deployment and limiting generalization.

Method: A generic framework was developed to integrate trace-based semantic information into code task-relevant prompts, combined with a comprehensive analysis of semantic information's role in strengthening reasoning in Code LLMs.

Result: Experiments revealed that trace-based semantic information provided limited benefits for supervised fine-tuning and test-time scaling of Code LLMs, contradicting prior assumptions about its utility.

Conclusion: The study highlights the need for improved frameworks to enhance Code LLMs' reasoning capabilities, as current semantic information integration shows limited effectiveness in supervised fine-tuning and test-time scaling.

Abstract: Code Large Language Models (Code LLMs) have opened a new era in programming
with their impressive capabilities. However, recent research has revealed
critical limitations in their ability to reason about runtime behavior and
understand the actual functionality of programs, which poses significant
challenges for their post-training and practical deployment. Specifically, Code
LLMs encounter two principal issues: (1) a lack of proficiency in reasoning
about program execution behavior, as they struggle to interpret what programs
actually do during runtime, and (2) the inconsistent and fragmented
representation of semantic information, such as execution traces, across
existing methods, which hinders their ability to generalize and reason
effectively. These challenges underscore the necessity for more systematic
approaches to enhance the reasoning capabilities of Code LLMs. To address these
issues, we introduce a generic framework to support integrating semantic
information~(e.g., execution trace) to code task-relevant prompts, and conduct
a comprehensive study to explore the role of semantic information in enhancing
the reasoning ability of Code LLMs accordingly. Specifically, we focus on
investigating the usefulness of trace-based semantic information in boosting
supervised fine-tuning~(SFT) and post-phase inference of Code LLMs. The
experimental results surprisingly disagree with previous works and demonstrate
that semantic information has limited usefulness for SFT and test time scaling
of Code LLM.

</details>


### [35] [AI Asset Management for Manufacturing (AIM4M): Development of a Process Model for Operationalization](https://arxiv.org/abs/2509.11691)
*Lukas Rauh,Mel-Rick Süner,Daniel Schel,Thomas Bauernhansl*

Main category: cs.SE

TL;DR: Proposes an AI lifecycle process model for manufacturing integrating MLOps and CPPS requirements


<details>
  <summary>Details</summary>
Motivation: Despite AI benefits in manufacturing, operationalizing AI beyond prototypes remains challenging due to technical complexity, lack of standards, and fragmented organizational processes in cyber-physical production systems (CPPS).

Method: Developed a process model based on MLOps principles refined for CPPS domain-specific requirements, focusing on three key aspects for lifecycle management of AI assets.

Result: A theoretical framework enabling systematic development, deployment, and management of AI assets aligned with CPPS constraints and regulatory demands.

Conclusion: The process model addresses operationalization barriers in manufacturing by providing structured AI lifecycle management tailored to CPPS environments through MLOps adaptation.

Abstract: The benefits of adopting artificial intelligence (AI) in manufacturing are
undeniable. However, operationalizing AI beyond the prototype, especially when
involved with cyber-physical production systems (CPPS), remains a significant
challenge due to the technical system complexity, a lack of implementation
standards and fragmented organizational processes. To this end, this paper
proposes a new process model for the lifecycle management of AI assets designed
to address challenges in manufacturing and facilitate effective
operationalization throughout the entire AI lifecycle. The process model, as a
theoretical contribution, builds on machine learning operations (MLOps)
principles and refines three aspects to address the domain-specific
requirements from the CPPS context. As a result, the proposed process model
aims to support organizations in practice to systematically develop, deploy and
manage AI assets across their full lifecycle while aligning with CPPS-specific
constraints and regulatory demands.

</details>


### [36] [From Evaluation to Enhancement: Large Language Models for Zero-Knowledge Proof Code Generation](https://arxiv.org/abs/2509.11708)
*Zhantong Xue,Pingchuan Ma,Zhaoyu Wang,Shuai Wang*

Main category: cs.SE

TL;DR: This paper evaluates LLMs for zero-knowledge programming, identifies gaps in gadget reasoning, and introduces a framework (ZK-Coder) to boost code correctness by 50–60 percentage points.


<details>
  <summary>Details</summary>
Motivation: While LLMs excel in general programming, their effectiveness in ZK programming—requiring finite field arithmetic and gadget reasoning—remains unexplored, with significant practical implications for privacy and security domains.

Method: The authors propose ZK-Eval, a three-level evaluation pipeline (language knowledge, gadget competence, end-to-end generation), and ZK-Coder, an agentic framework with constraint sketching, guided retrieval, and interactive repair.

Result: On Circom and Noir, ZK-Coder improves success rates from 17.35% to 83.38% and 32.21% to 90.05%, respectively, demonstrating substantial gains over baseline LLMs.

Conclusion: The study establishes ZK-Eval and ZK-Coder as a framework for evaluating and enhancing LLMs in ZK code generation, aiming to reduce entry barriers for practitioners and advance trustworthy computation.

Abstract: Zero-knowledge proofs (ZKPs) are increasingly deployed in domains such as
privacy-preserving authentication, blockchain scalability, and secure finance.
However, authoring ZK programs remains challenging: unlike mainstream
programming, ZK development requires reasoning about finite field arithmetic,
constraint systems, and gadgets, making it knowledge-intensive and error-prone.
While large language models (LLMs) have demonstrated strong code generation
capabilities in general-purpose languages, their effectiveness for ZK
programming, where correctness hinges on both language mastery and gadget-level
reasoning, remains unexplored. To address this gap, we propose
\textsc{ZK-Eval}, a domain-specific evaluation pipeline that probes LLM
capabilities at three levels: language knowledge, gadget competence, and
end-to-end program generation. Our evaluation of four state-of-the-art LLMs
reveals that models excel at surface-level syntax but struggle with gadget
usage and semantic correctness, often yielding incorrect programs. Based on
these insights, we introduce \textsc{ZK-Coder}, an agentic framework that
augments LLMs with constraint sketching, guided retrieval, and interactive
repair. Experiments on Circom and Noir show substantial gains, with success
rates improving from 17.35\% to 83.38\% and from 32.21\% to 90.05\%,
respectively. With \textsc{ZK-Eval} and \textsc{ZK-Coder}, we establish a
foundation for systematically measuring and augmenting LLMs in ZK code
generation to lower barriers for practitioners and advance trustworthy
computation.

</details>


### [37] [Toward Greener Background Processes -- Measuring Energy Cost of Autosave Feature](https://arxiv.org/abs/2509.11738)
*Maria Küüsvek,Hina Anwar*

Main category: cs.SE

TL;DR: This paper proposes a reusable process to evaluate energy consumption of background processes in desktop apps, identifies design factors in autosave features, and offers four recommendations for sustainable Python-based software development.


<details>
  <summary>Details</summary>
Motivation: Background processes in desktop applications are often overlooked in energy studies despite their significant cumulative impact. The paper aims to address this gap by evaluating energy behavior at the operational design level.

Method: The method involves a three-phase process: decomposing background functionality into core operations, operational isolation, and controlled measurements for comparative profiling. This was applied to autosave features in three open-source Python text editors with 900 empirical energy measurements.

Result: Key design factors affecting energy use include save frequency, buffering strategy, and auxiliary logic like change detection. Four actionable recommendations for greener autosave implementations in Python were provided.

Conclusion: The paper concludes that by identifying key design factors and providing actionable recommendations, developers can create more energy-efficient autosave features in Python, promoting sustainable software practices.

Abstract: Background processes in desktop applications are often overlooked in energy
consumption studies, yet they represent continuous, automated workloads with
significant cumulative impact. This paper introduces a reusable process for
evaluating the energy behavior of such features at the level of operational
design. The process works in three phases: 1) decomposing background
functionality into core operations, 2) operational isolation, and 3) controlled
measurements enabling comparative profiling. We instantiate the process in a
case study of autosave implementations across three open-source Python-based
text editors. Using 900 empirical software-based energy measurements, we
identify key design factors affecting energy use, including save frequency,
buffering strategy, and auxiliary logic such as change detection. We give four
actionable recommendations for greener implementations of autosave features in
Python to support sustainable software practices.

</details>


### [38] [Analysing Python Machine Learning Notebooks with Moose](https://arxiv.org/abs/2509.11748)
*Marius Mignard,Steven Costiou,Nicolas Anquetil,Anne Etien*

Main category: cs.SE

TL;DR: Vespucci Linter is a multi-level static analysis tool for improving ML code quality in notebooks by addressing general Python, notebook structure, and ML-specific issues.


<details>
  <summary>Details</summary>
Motivation: Machine Learning code in notebooks often has lower quality due to bad practices at three levels: Python coding conventions, notebook organization, and ML-specific aspects like reproducibility and API usage. Existing tools only focus on one level and miss ML semantics.

Method: Vespucci Linter uses a metamodeling approach to unify notebook structure with Python code entities, enabling multi-level static analysis. Implemented 22 rules based on the literature.

Result: Applied to 5,000 Kaggle notebooks, Vespucci found violations at all three levels, showing its effectiveness and highlighting widespread issues in ML notebooks.

Conclusion: Vespucci Linter's multi-level approach successfully addresses multiple ML code quality aspects in notebooks, offering potential to enhance quality and reliability in data science workflows.

Abstract: Machine Learning (ML) code, particularly within notebooks, often exhibits
lower quality compared to traditional software. Bad practices arise at three
distinct levels: general Python coding conventions, the organizational
structure of the notebook itself, and ML-specific aspects such as
reproducibility and correct API usage. However, existing analysis tools
typically focus on only one of these levels and struggle to capture ML-specific
semantics, limiting their ability to detect issues. This paper introduces
Vespucci Linter, a static analysis tool with multi-level capabilities, built on
Moose and designed to address this challenge. Leveraging a metamodeling
approach that unifies the notebook's structural elements with Python code
entities, our linter enables a more contextualized analysis to identify issues
across all three levels. We implemented 22 linting rules derived from the
literature and applied our tool to a corpus of 5,000 notebooks from the Kaggle
platform. The results reveal violations at all levels, validating the relevance
of our multi-level approach and demonstrating Vespucci Linter's potential to
improve the quality and reliability of ML development in notebook environments.

</details>
