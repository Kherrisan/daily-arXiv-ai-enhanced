{"id": "2509.15283", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.PL", "I.2.7; F.2.2; I.2.2"], "pdf": "https://arxiv.org/pdf/2509.15283", "abs": "https://arxiv.org/abs/2509.15283", "authors": ["Kadin Matotek", "Heather Cassel", "Md Amiruzzaman", "Linh B. Ngo"], "title": "Evaluating the Limitations of Local LLMs in Solving Complex Programming Challenges", "comment": "Comments: 16 pages, 3 figures, 8 tables, accepted to CCSC Eastern\n  2025", "summary": "This study examines the performance of today's open-source, locally hosted\nlarge-language models (LLMs) in handling complex competitive programming tasks\nwith extended problem descriptions and contexts. Building on the original\nFramework for AI-driven Code Generation Evaluation (FACE), the authors retrofit\nthe pipeline to work entirely offline through the Ollama runtime, collapsing\nFACE's sprawling per-problem directory tree into a handful of consolidated JSON\nfiles, and adding robust checkpointing so multi-day runs can resume after\nfailures. The enhanced framework generates, submits, and records solutions for\nthe full Kattis corpus of 3,589 problems across eight code-oriented models\nranging from 6.7-9 billion parameters. The submission results show that the\noverall pass@1 accuracy is modest for the local models, with the best models\nperforming at approximately half the acceptance rate of the proprietary models,\nGemini 1.5 and ChatGPT-4. These findings expose a persistent gap between\nprivate, cost-controlled LLM deployments and state-of-the-art proprietary\nservices, yet also highlight the rapid progress of open models and the\npractical benefits of an evaluation workflow that organizations can replicate\non in-house hardware.", "AI": {"tldr": "Open-source local LLMs solve only about half as many programming challenges as proprietary models, but the paper shows how to evaluate them using an improved offline framework that streamlines testing across thousands of programming tasks.", "motivation": "This work addresses the need to compare performance between open-source local LLM deployments (with cost control) and state-of-the-art proprietary models in solving complex, context-rich programming tasks. This provides practical insights for organizations choosing between open vs. closed solutions.", "method": "The authors enhanced the FACE framework by retrofitting it to operate offline using the Ollama runtime, consolidating the file structure into JSON files, and adding checkpointing. They evaluated eight open-source LLMs with 6.7-9 billion parameters across 3,589 Kattis programming problems.", "result": "The open-source models achieved a pass@1 accuracy of approximately 50% compared to the acceptance rates of the proprietary models. This highlights a persistent performance gap but also shows the feasibility of evaluating large LLM performance using the enhanced offline framework.", "conclusion": "The study concludes that while there is a performance gap between open-source local LLMs and proprietary models like Gemini 1.5 and ChatGPT-4 in competitive programming tasks, the evaluation framework demonstrates the practical benefits of open models and shows their rapid progress. This allows organizations to replicate the evaluation on in-house hardware."}}
{"id": "2509.15397", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15397", "abs": "https://arxiv.org/abs/2509.15397", "authors": ["Simantika Bhattacharjee Dristi", "Matthew B. Dwyer"], "title": "LoCaL: Countering Surface Bias in Code Evaluation Metrics", "comment": null, "summary": "With the increasing popularity of large language models (LLMs) and LLM-based\nagents, reliable and effective code evaluation metrics (CEMs) have become\ncrucial for progress across several software engineering tasks. While popular\nbenchmarks often provide test cases to assess the correctness of generated\ncode, crafting and executing test cases is expensive. Reference-based CEMs\nprovide a cheaper alternative by scoring a candidate program based on its\nfunctional similarity to a reference. Although prior research has focused on\nreporting the weak correlation between these CEMs and functional correctness,\nthe causes are only assumed, and plausible solutions remain unexplored. In this\nwork, we critically evaluate four state-of-the-art reference-based CEMs,\nrevealing their strong bias towards surface-level features rather than code\nfunctionality. Despite this surface bias, current evaluation datasets for these\nCEMs rarely include code pairs that are surface-similar yet functionally\ndissimilar, or functionally similar yet surface-dissimilar. To mitigate this\ngap, we propose LoCaL (Looks Can Lie), a CEM evaluation benchmark, with 3117\ncode pairs at both the method and program levels. Each pair is labeled with a\nfunctional similarity score and aims to target regions where CEMs are likely to\nperform poorly. The functional similarity scores are calculated through\ndifferential fuzzing, which eliminates the need for predefined test cases and,\nat the same time, improves the reliability of the scores by executing an order\nof magnitude more tests than prior work. We find that all four CEMs show\nsignificant performance degradation on LoCaL, compared to the baselines.\nFinally, based on our findings, we draw the implication that exposing CEMs to\nLoCaL-like data might facilitate the development of metrics that are robust to\nsurface bias.", "AI": {"tldr": "This paper introduces LoCaL, a benchmark to evaluate reference-based code evaluation metrics (CEMs) by exposing their surface-level bias. The authors highlight existing CEMs tend to prioritize superficial similarity over functional correctness and demonstrate their performance degradation on LoCaL's diverse code pairs.", "motivation": "Existing CEMs rely on either expensive test case execution or flawed surface-based comparisons with reference implementations. Current evaluations lack scenarios that distinguish between surface and functional similarity, hindering progress in developing reliable metrics for LLM-generated code.", "method": "The authors: (1\uff09Analyzed four state-of-the-art reference-based CEMs for surface bias (2\uff09Created LoCaL - a benchmark with 3,117 code pairs labeled with functional similarity scores derived from differential fuzzing (3\uff09Compared CEM performance on LoCaL against baseline datasets", "result": "All tested CEMs exhibited significant performance degradation on LoCaL compared to baseline datasets. The benchmark successfully identified surface-biased evaluation gaps through code pairs with conflicting surface/functional similarity patterns.", "conclusion": "Exposing CEMs to surface-discrepant scenarios in LoCaL-style benchmarks can help develop more functionally robust metrics. The study emphasizes the need for evaluation frameworks that explicitly address the surface vs. functional similarity tradeoff in code generation assessment."}}
{"id": "2509.15567", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15567", "abs": "https://arxiv.org/abs/2509.15567", "authors": ["Hongyu Kuang", "Ning Zhang", "Hui Gao", "Xin Zhou", "Wesley K. G. Assun\u00e7\u00e3o", "Xiaoxing Ma", "Dong Shao", "Guoping Rong", "He Zhang"], "title": "Brevity is the Soul of Wit: Condensing Code Changes to Improve Commit Message Generation", "comment": null, "summary": "Commit messages are valuable resources for describing why code changes are\ncommitted to repositories in version control systems (e.g., Git). They\neffectively help developers understand code changes and better perform software\nmaintenance tasks. Unfortunately, developers often neglect to write\nhigh-quality commit messages in practice. Therefore, a growing body of work is\nproposed to generate commit messages automatically. These works all\ndemonstrated that how to organize and represent code changes is vital in\ngenerating good commit messages, including the use of fine-grained graphs or\nembeddings to better represent code changes. In this study, we choose an\nalternative way to condense code changes before generation, i.e., proposing\nbrief yet concise text templates consisting of the following three parts: (1)\nsummarized code changes, (2) elicited comments, and (3) emphasized code\nidentifiers. Specifically, we first condense code changes by using our proposed\ntemplates with the help of a heuristic-based tool named ChangeScribe, and then\nfine-tune CodeLlama-7B on the pairs of our proposed templates and corresponding\ncommit messages. Our proposed templates better utilize pre-trained language\nmodels, while being naturally brief and readable to complement generated commit\nmessages for developers. Our evaluation based on a widely used dataset showed\nthat our approach can outperform six baselines in terms of BLEU-Norm, METEOR,\nand ROUGE-L, with average improvements of 51.7%, 78.7%, and 62.5%,\nrespectively. The ablation study and human evaluation also provide further\ninsights into the effectiveness of our approach.", "AI": {"tldr": "The paper proposes using concise text templates to condense code changes (summarized changes, elicited comments, emphasized identifiers) to improve commit message generation via CodeLlama-7B fine-tuning, achieving significant improvements over six baselines.", "motivation": "Developers often neglect high-quality commit messages despite their importance for software maintenance. Existing methods require complex representations (graphs/embeddings); this approach explores simpler, human-readable templates to enhance pre-trained models.", "method": "1) Templates condense code changes into summarized edits, code comments, and key identifiers using ChangeScribe. 2) Fine-tune CodeLlama-7B on template-commit message pairs. 3) Leverage templates' brevity while maintaining readability as input to language models.", "result": "Outperformed six baselines by 51.7% (BLEU-Norm), 78.7% (METEOR), and 62.5% (ROUGE-L). Ablation studies and human evaluations confirmed the effectiveness of all template components.", "conclusion": "Text templates provide a concise, effective alternative to complex representations for commit message generation, improving model performance while maintaining readability and interpretability for developers."}}
{"id": "2509.15777", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15777", "abs": "https://arxiv.org/abs/2509.15777", "authors": ["Haoran Xu", "Zhi Chen", "Junxiao Han", "Xinkui Zhao", "Jianwei Yin", "Shuiguang Deng"], "title": "How Far Are We? An Empirical Analysis of Current Vulnerability Localization Approaches", "comment": null, "summary": "Open-source software vulnerability patch detection is a critical component\nfor maintaining software security and ensuring software supply chain integrity.\nTraditional manual detection methods face significant scalability challenges\nwhen processing large volumes of commit histories, while being prone to human\nerrors and omissions. Existing automated approaches, including heuristic-based\nmethods and pre-trained model solutions, suffer from limited accuracy, poor\ngeneralization capabilities, and inherent methodological constraints that\nhinder their practical deployment. To address these fundamental challenges,\nthis paper conducts a comprehensive empirical study of existing vulnerability\npatch detection methods, revealing four key insights that guide the design of\neffective solutions: the critical impact of search space reduction, the\nsuperiority of pre-trained semantic understanding over architectural\ncomplexity, the temporal limitations of web crawling approaches, and the\nadvantages of knowledge-driven methods. Based on these insights, we propose a\nnovel two-stage framework that combines version-driven candidate filtering with\nlarge language model-based multi-round dialogue voting to achieve accurate and\nefficient vulnerability patch identification. Extensive experiments on a\ndataset containing 750 real vulnerabilities demonstrate that our method\noutperforms current approaches.", "AI": {"tldr": "This paper addresses software vulnerability detection by introducing a two-stage framework combining version filtering and LLM-based dialogue voting, validated on 750 real vulnerabilities to show superior performance over existing methods.", "motivation": "Current methods for vulnerability patch detection suffer from scalability, accuracy, and generalization limitations. Manual approaches are error-prone, while automated solutions lack robustness and practical deployment viability, necessitating a paradigm shift.", "method": "The authors introduce a two-stage framework combining (1) version-driven candidate filtering to reduce search space and (2) a large language model-based multi-round dialogue voting system for precise patch identification. This integrates semantic understanding with iterative refinement.", "result": "Experiments on a dataset of 750 real vulnerabilities show the proposed method outperforms state-of-the-art approaches in accuracy and efficiency, validating its four key empirical insights.", "conclusion": "This paper proposes a novel two-stage framework for vulnerability patch detection, demonstrating superior performance over existing methods through empirical validation on a real-world dataset. The insights derived from the study provide actionable guidelines for future research and practical deployment in software security."}}
{"id": "2509.15433", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.15433", "abs": "https://arxiv.org/abs/2509.15433", "authors": ["Vaibhav Agrawal", "Kiarash Ahi"], "title": "Synergizing Static Analysis with Large Language Models for Vulnerability Discovery and beyond", "comment": null, "summary": "This report examines the synergy between Large Language Models (LLMs) and\nStatic Application Security Testing (SAST) to improve vulnerability discovery.\nTraditional SAST tools, while effective for proactive security, are limited by\nhigh false-positive rates and a lack of contextual understanding. Conversely,\nLLMs excel at code analysis and pattern recognition but can be prone to\ninconsistencies and hallucinations. By integrating these two technologies, a\nmore intelligent and efficient system is created. This combination moves beyond\nmere vulnerability detection optimization, transforming security into a deeply\nintegrated, contextual process that provides tangible benefits like improved\ntriage, dynamic bug descriptions, bug validation via exploit generation and\nenhanced analysis of complex codebases. The result is a more effective security\napproach that leverages the strengths of both technologies while mitigating\ntheir weaknesses. SAST-Genius reduced false positives by about 91 % (225 to 20)\ncompared to Semgrep alone.", "AI": {"tldr": "This paper proposes integrating Large Language Models (LLMs) with Static Application Security Testing (SAST) to improve vulnerability discovery by combining their strengths while mitigating weaknesses.", "motivation": "Traditional SAST tools suffer from high false-positive rates and limited contextual understanding, while LLMs face issues with consistency and hallucinations in code analysis.", "method": "A hybrid system combining LLMs and SAST tools (e.g., Semgrep) is developed to leverage contextual analysis and pattern recognition for triage, dynamic bug descriptions, exploit validation, and complex code analysis.", "result": "SAST-Genius reduced false positives by 91% (225 to 20) compared to Semgrep alone, demonstrating significant improvements in accuracy and efficiency.", "conclusion": "The integration of LLMs and SAST transforms security into a context-aware, workflow-enhancing process that outperforms standalone tools in vulnerability discovery and validation."}}
{"id": "2509.15893", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15893", "abs": "https://arxiv.org/abs/2509.15893", "authors": ["Andrea Bombarda", "Federico Conti", "Marcello Minervini", "Aurora Zanenga", "Claudio Menghi"], "title": "Failure Modes and Effects Analysis: An Experience from the E-Bike Domain", "comment": "12 pages", "summary": "Software failures can have catastrophic and costly consequences. Functional\nFailure Mode and Effects Analysis (FMEA) is a standard technique used within\nCyber-Physical Systems (CPS) to identify software failures and assess their\nconsequences. Simulation-driven approaches have recently been shown to be\neffective in supporting FMEA. However, industries need evidence of the\neffectiveness of these approaches to increase practical adoption. This\nindustrial paper presents our experience with using FMEA to analyze the safety\nof a CPS from the e-Bike domain. We used Simulink Fault Analyzer, an industrial\ntool that supports engineers with FMEA. We identified 13 realistic faults,\nmodeled them, and analyzed their effects. We sought expert feedback to analyze\nthe appropriateness of our models and the effectiveness of the faults in\ndetecting safety breaches. Our results reveal that for the faults we\nidentified, our models were accurate or contained minor imprecision that we\nsubsequently corrected. They also confirm that FMEA helps engineers improve\ntheir models. Specifically, the output provided by the simulation-driven\nsupport for 38.4% (5 out of 13) of the faults did not match the engineers'\nexpectations, helping them discover unexpected effects of the faults. We\npresent a thorough discussion of our results and ten lessons learned. Our\nfindings are useful for software engineers who work as Simulink engineers, use\nthe Simulink Fault Analyzer, or work as safety analysts.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.15499", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.15499", "abs": "https://arxiv.org/abs/2509.15499", "authors": ["Shijia Li", "Jiang Ming", "Lanqing Liu", "Longwei Yang", "Ni Zhang", "Chunfu Jia"], "title": "Adversarially Robust Assembly Language Model for Packed Executables Detection", "comment": "Accepted by ACM CCS 2025", "summary": "Detecting packed executables is a critical component of large-scale malware\nanalysis and antivirus engine workflows, as it identifies samples that warrant\ncomputationally intensive dynamic unpacking to reveal concealed malicious\nbehavior. Traditionally, packer detection techniques have relied on empirical\nfeatures, such as high entropy or specific binary patterns. However, these\nempirical, feature-based methods are increasingly vulnerable to evasion by\nadversarial samples or unknown packers (e.g., low-entropy packers).\nFurthermore, the dependence on expert-crafted features poses challenges in\nsustaining and evolving these methods over time.\n  In this paper, we examine the limitations of existing packer detection\nmethods and propose Pack-ALM, a novel deep-learning-based approach for\ndetecting packed executables. Inspired by the linguistic concept of\ndistinguishing between real and pseudo words, we reformulate packer detection\nas a task of differentiating between legitimate and \"pseudo\" instructions. To\nachieve this, we preprocess native data and packed data into \"pseudo\"\ninstructions and design a pre-trained assembly language model that recognizes\nfeatures indicative of packed data. We evaluate Pack-ALM against leading\nindustrial packer detection tools and state-of-the-art assembly language\nmodels. Extensive experiments on over 37,000 samples demonstrate that Pack-ALM\neffectively identifies packed binaries, including samples created with\nadversarial or previously unseen packing techniques. Moreover, Pack-ALM\noutperforms traditional entropy-based methods and advanced assembly language\nmodels in both detection accuracy and adversarial robustness.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.15971", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15971", "abs": "https://arxiv.org/abs/2509.15971", "authors": ["Owen Truong", "Terrence Zhang", "Arnav Marchareddy", "Ryan Lee", "Jeffery Busold", "Michael Socas", "Eman Abdullah AlOmar"], "title": "LeakageDetector 2.0: Analyzing Data Leakage in Jupyter-Driven Machine Learning Pipelines", "comment": null, "summary": "In software development environments, code quality is crucial. This study\naims to assist Machine Learning (ML) engineers in enhancing their code by\nidentifying and correcting Data Leakage issues within their models. Data\nLeakage occurs when information from the test dataset is inadvertently included\nin the training data when preparing a data science model, resulting in\nmisleading performance evaluations. ML developers must carefully separate their\ndata into training, evaluation, and test sets to avoid introducing Data Leakage\ninto their code. In this paper, we develop a new Visual Studio Code (VS Code)\nextension, called LeakageDetector, that detects Data Leakage, mainly Overlap,\nPreprocessing and Multi-test leakage, from Jupyter Notebook files. Beyond\ndetection, we included two correction mechanisms: a conventional approach,\nknown as a quick fix, which manually fixes the leakage, and an LLM-driven\napproach that guides ML developers toward best practices for building ML\npipelines.", "AI": {"tldr": "The paper introduces LeakageDetector, a VS Code extension to detect and fix three types of Data Leakage (Overlap, Preprocessing, Multi-test) in Jupyter Notebook ML code, offering automated correction via quick fixes and LLM-guided best practices.", "motivation": "Data Leakage in ML model evaluation leads to overestimated performance, yet manual detection is error-prone. Existing tools lack integration directly in ML engineers' development environments.", "method": "Developed a VS Code extension that analyzes Jupyter Notebooks for leakage patterns using static code analysis and AST parsing. Implements two correction strategies: manual quick fixes and LLM-powered suggestions for pipeline restructuring.", "result": "LeakageDetector successfully identifies three major leakage types, with validation showing effectiveness in detecting real-world leakage scenarios and providing actionable fixes.", "conclusion": "Automated leakage detection in development workflows improves ML code quality and reduces human error through immediate feedback and education on proper pipeline practices."}}
{"id": "2509.15547", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.15547", "abs": "https://arxiv.org/abs/2509.15547", "authors": ["Zhiyu Huang", "Guyue Li", "Hao Xu", "Derrick Wing Kwan Ng"], "title": "Fluid Antenna System-assisted Physical Layer Secret Key Generation", "comment": null, "summary": "This paper investigates physical-layer key generation (PLKG) in multi-antenna\nbase station systems, by leveraging a fluid antenna system (FAS) to dynamically\ncustomize radio environments. Without requiring additional nodes or extensive\nradio frequency chains, the FAS effectively enables adaptive antenna port\nselection by exploiting channel spatial correlation to enhance the key\ngeneration rate (KGR) at legitimate nodes. To comprehensively evaluate the\nefficiency of the FAS in PLKG, we propose an FAS-assisted PLKG model that\nintegrates transmit beamforming and sparse port selection under independent and\nidentically distributed and spatially correlated channel models, respectively.\nSpecifically, the PLKG utilizes reciprocal channel probing to derive a\nclosed-form KGR expression based on the mutual information between legitimate\nchannel estimates. Nonconvex optimization problems for these scenarios are\nformulated to maximize the KGR subject to transmit power constraints and sparse\nport activation. We propose an iterative algorithm by capitalizing on\nsuccessive convex approximation and Cauchy-Schwarz inequality to obtain a\nlocally optimal solution. A reweighted $\\ell_1$-norm-based algorithm is applied\nto advocate for the sparse port activation of FAS-assisted PLKG. Furthermore, a\nlow-complexity sliding window-based port selection is proposed to substitute\nreweighted $\\ell_1$-norm method based on Rayleigh-quotient analysis. Simulation\nresults demonstrate that the FAS-PLKG scheme significantly outperforms the\nFA-PLKG scheme in both independent and spatially correlated environments. The\nsliding window-based port selection method introduced in this paper has been\nshown to yield superior KGR, compared to the reweighted $\\ell_1$-norm method.\nIt is shown that the FAS achieves higher KGR with fewer RF chains through\ndynamic sparse port selection.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.16081", "categories": ["cs.SE", "cs.MS", "G.1.3; D.2.11"], "pdf": "https://arxiv.org/pdf/2509.16081", "abs": "https://arxiv.org/abs/2509.16081", "authors": ["Marcel Koch", "Tobias Ribizel", "Pratik Nayak", "Fritz G\u00f6bel", "Gregor Olenik", "Terry Cojean"], "title": "Software Development Aspects of Integrating Linear Algebra Libraries", "comment": "16 pages, 2 figures", "summary": "Many scientific discoveries are made through, or aided by, the use of\nsimulation software. These sophisticated software applications are not built\nfrom the ground up, instead they rely on smaller parts for specific use cases,\nusually from domains unfamiliar to the application scientists. The software\nlibrary Ginkgo is one of these building blocks to handle sparse numerical\nlinear algebra on different platforms. By using Ginkgo, applications are able\nto ease the transition to modern systems, and speed up their simulations\nthrough faster numerical linear algebra routines. This paper discusses the\nchallenges and benefits for application software in adopting Ginkgo. It will\npresent examples from different domains, such as CFD, power grid simulation, as\nwell as electro-cardiophysiology. For these cases, the impact of the\nintegrations on the application code is discussed from a software engineering\nstandpoint, and in particular, the approaches taken by Ginkgo and the\napplications to enable sustainable software development are highlighted.", "AI": {"tldr": "This paper analyzes the use and influence of the Ginkgo software library in various application domains.", "motivation": "Simulation software drives scientific discovery and productivity, yet they often have to incorporate specialized components from unfamiliar domains to manage sparse numerical linear algebra tasks.", "method": "The authors have focused on examining the experiences and impacts of integrating Ginkgo into different applications.", "result": "Successful adaptations of Ginkgo have reduced the burden on developers to manage cross-domain software components and enhanced the applications' performance.", "conclusion": "The integration of the Ginkgo library exemplifies a sustainable approach to using specialized software components in broader simulation contexts, making maintenance and performance improvements more feasible."}}
{"id": "2509.15555", "categories": ["cs.CR", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.15555", "abs": "https://arxiv.org/abs/2509.15555", "authors": ["Rasil Baidar", "Sasa Maric", "Robert Abbas"], "title": "Hybrid Deep Learning-Federated Learning Powered Intrusion Detection System for IoT/5G Advanced Edge Computing Network", "comment": null, "summary": "The exponential expansion of IoT and 5G-Advanced applications has enlarged\nthe attack surface for DDoS, malware, and zero-day intrusions. We propose an\nintrusion detection system that fuses a convolutional neural network (CNN), a\nbidirectional LSTM (BiLSTM), and an autoencoder (AE) bottleneck within a\nprivacy-preserving federated learning (FL) framework. The CNN-BiLSTM branch\ncaptures local and gated cross-feature interactions, while the AE emphasizes\nreconstruction-based anomaly sensitivity. Training occurs across edge devices\nwithout sharing raw data. On UNSW-NB15 (binary), the fused model attains AUC\n99.59 percent and F1 97.36 percent; confusion-matrix analysis shows balanced\nerror rates with high precision and recall. Average inference time is\napproximately 0.0476 ms per sample on our test hardware, which is well within\nthe less than 10 ms URLLC budget, supporting edge deployment. We also discuss\nexplainability, drift tolerance, and FL considerations for compliant, scalable\n5G-Advanced IoT security.", "AI": {"tldr": "The paper introduces a CNN-BiLSTM-AE based intrusion detection system in a privacy-preserving FL framework for 5G-Advanced IoT, achieving high AUC and F1 scores with fast inference.", "motivation": "The growing IoT and 5G-Advanced applications have increased the risks of cyber threats like DDoS, malware, and zero-day intrusions, necessitating effective and privacy-preserving intrusion detection solutions.", "method": "The proposed system combines a CNN for local feature interactions, a BiLSTM for sequence analysis, and an AE for anomaly detection, all integrated within a federated learning framework to preserve privacy during distributed training.", "result": "Evaluations on the UNSW-NB15 dataset show the fused model achieves an AUC of 99.59% and an F1 score of 97.36%, with low inference latency of ~0.0476 ms per sample, fitting URLLC's 10 ms requirement.", "conclusion": "The model offers a secure, efficient, and explainable solution for 5G-Advanced IoT environments, addressing scalability and drift tolerance challenges in FL-based intrusion detection."}}
{"id": "2509.16140", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16140", "abs": "https://arxiv.org/abs/2509.16140", "authors": ["Avinash Patil"], "title": "When Bugs Linger: A Study of Anomalous Resolution Time Outliers and Their Themes", "comment": "7 pages, 2 tables, 21 figures", "summary": "Efficient bug resolution is critical for maintaining software quality and\nuser satisfaction. However, specific bug reports experience unusually long\nresolution times, which may indicate underlying process inefficiencies or\ncomplex issues. This study presents a comprehensive analysis of bug resolution\nanomalies across seven prominent open-source repositories: Cassandra, Firefox,\nHadoop, HBase, SeaMonkey, Spark, and Thunderbird. Utilizing statistical methods\nsuch as Z-score and Interquartile Range (IQR), we identify anomalies in bug\nresolution durations. To understand the thematic nature of these anomalies, we\napply Term Frequency-Inverse Document Frequency (TF-IDF) for textual feature\nextraction and KMeans clustering to group similar bug summaries. Our findings\nreveal consistent patterns across projects, with anomalies often clustering\naround test failures, enhancement requests, and user interface issues. This\napproach provides actionable insights for project maintainers to prioritize and\neffectively address long-standing bugs.", "AI": {"tldr": "The study analyzes bug resolution anomalies in seven open-source projects, identifying patterns around test failures, enhancement requests, and UI issues using statistical and clustering techniques.", "motivation": "To improve software quality and user satisfaction by understanding and addressing the causes of unusually long bug resolution times through systematic analysis.", "method": "The authors employed statistical methods (Z-score, IQR) to detect anomalies in resolution durations and used TF-IDF and KMeans clustering for thematic analysis of bug summaries.", "result": "Anomalies in bug resolution times were found across multiple projects, with frequent clustering around test failures, enhancement requests, and user interface-related issues.", "conclusion": "The proposed method offers actionable insights for project maintainers to prioritize and resolve long-standing bugs effectively, contributing to better software maintenance practices."}}
{"id": "2509.15572", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.15572", "abs": "https://arxiv.org/abs/2509.15572", "authors": ["Xinpeng Liu", "Junming Liu", "Peiyu Liu", "Han Zheng", "Qinying Wang", "Mathias Payer", "Shouling Ji", "Wenhai Wang"], "title": "Cuckoo Attack: Stealthy and Persistent Attacks Against AI-IDE", "comment": null, "summary": "Modern AI-powered Integrated Development Environments (AI-IDEs) are\nincreasingly defined by an Agent-centric architecture, where an LLM-powered\nAgent is deeply integrated to autonomously execute complex tasks. This tight\nintegration, however, also introduces a new and critical attack surface.\nAttackers can exploit these components by injecting malicious instructions into\nuntrusted external sources, effectively hijacking the Agent to perform harmful\noperations beyond the user's intention or awareness. This emerging threat has\nquickly attracted research attention, leading to various proposed attack\nvectors, such as hijacking Model Context Protocol (MCP) Servers to access\nprivate data. However, most existing approaches lack stealth and persistence,\nlimiting their practical impact.\n  We propose the Cuckoo Attack, a novel attack that achieves stealthy and\npersistent command execution by embedding malicious payloads into configuration\nfiles. These files, commonly used in AI-IDEs, execute system commands during\nroutine operations, without displaying execution details to the user. Once\nconfigured, such files are rarely revisited unless an obvious runtime error\noccurs, creating a blind spot for attackers to exploit. We formalize our attack\nparadigm into two stages, including initial infection and persistence. Based on\nthese stages, we analyze the practicality of the attack execution process and\nidentify the relevant exploitation techniques. Furthermore, we analyze the\nimpact of Cuckoo Attack, which can not only invade the developer's local\ncomputer but also achieve supply chain attacks through the spread of\nconfiguration files. We contribute seven actionable checkpoints for vendors to\nevaluate their product security. The critical need for these checks is\ndemonstrated by our end-to-end Proof of Concept, which validated the proposed\nattack across nine mainstream Agent and AI-IDE pairs.", "AI": {"tldr": "The paper proposes Cuckoo Attack, a method to stealthily and persistently hijack LLM-powered agents in AI-IDEs by injecting malicious payloads into configuration files. The study demonstrates the attack's impact and presents seven security checkpoints for vendors.", "motivation": "Despite the growing use of AI-IDEs with agent-centric architecture, current attack methods lack stealth and persistence, limiting their real-world effectiveness. The paper aims to address this by introducing a more impactful attack methodology.", "method": "The Cuckoo Attack embeds malicious payloads into configuration files commonly used in AI-IDEs. These files execute system commands during routine operations without user visibility, allowing attackers to maintain persistence and stealth. The attack is formalized into two stages: initial infection and persistence, with end-to-end testing on nine mainstream AI-IDE and agent pairs.", "result": "The Cuckoo Attack can successfully execute commands within AI-IDEs without user detection. It poses significant risks, including local computer compromises and supply chain attacks through configuration file propagation. Additionally, the researchers have demonstrated the feasibility and impact through a Proof of Concept.", "conclusion": "Cuckoo Attack highlights the security vulnerabilities in AI-IDE agent integrations due to their complex architectures and ubiquitous configuration usage. The paper's contribution includes raising awareness of these vulnerabilities, confirming the attack's practicality through testing, and providing seven actionable checkpoints to improve vendor security."}}
{"id": "2509.16187", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16187", "abs": "https://arxiv.org/abs/2509.16187", "authors": ["Ali Reza Ibrahimzada", "Brandon Paulsen", "Reyhaneh Jabbarvand", "Joey Dodds", "Daniel Kroening"], "title": "MatchFixAgent: Language-Agnostic Autonomous Repository-Level Code Translation Validation and Repair", "comment": null, "summary": "Code translation transforms source code from one programming language (PL) to\nanother. Validating the functional equivalence of translation and repairing, if\nnecessary, are critical steps in code translation. Existing automated\nvalidation and repair approaches struggle to generalize to many PLs due to high\nengineering overhead, and they rely on existing and often inadequate test\nsuites, which results in false claims of equivalence and ineffective\ntranslation repair. We develop MatchFixAgent, a large language model\n(LLM)-based, PL-agnostic framework for equivalence validation and repair of\ntranslations. MatchFixAgent features a multi-agent architecture that divides\nequivalence validation into several sub-tasks to ensure thorough and consistent\nsemantic analysis of the translation. Then it feeds this analysis to test agent\nto write and execute tests. Upon observing a test failure, the repair agent\nattempts to fix the translation bug. The final (in)equivalence decision is made\nby the verdict agent, considering semantic analyses and test execution results.\n  We compare MatchFixAgent's validation and repair results with four\nrepository-level code translation techniques. We use 2,219 translation pairs\nfrom their artifacts, which cover 6 PL pairs, and are collected from 24 GitHub\nprojects totaling over 900K lines of code. Our results demonstrate that\nMatchFixAgent produces (in)equivalence verdicts for 99.2% of translation pairs,\nwith the same equivalence validation result as prior work on 72.8% of them.\nWhen MatchFixAgent's result disagrees with prior work, we find that 60.7% of\nthe time MatchFixAgent's result is actually correct. In addition, we show that\nMatchFixAgent can repair 50.6% of inequivalent translation, compared to prior\nwork's 18.5%. This demonstrates that MatchFixAgent is far more adaptable to\nmany PL pairs than prior work, while producing highly accurate validation\nresults.", "AI": {"tldr": "This paper introduces MatchFixAgent, a multi-agent framework using LLMs for code translation validation and repair. It outperforms existing methods in cross-language generalization and repair effectiveness through semantic analysis, test generation, and automated bug fixing.", "motivation": "Current code translation validation approaches have poor generalization across multiple programming languages due to high engineering overhead and reliance on insufficient test suites, leading to unreliable equivalence verification and ineffective repairs.", "method": "The framework employs four specialized agents: a multi-agent architecture divides validation into semantic analysis subtasks, a test agent writes/execute tests, a repair agent fixes translation bugs based on test failures, and a verdict agent synthesizes analysis to declare (in)equivalence decisions.", "result": "Evaluations on 2,219 translation pairs across 6 PL pairs show 99.2% equivalence validation accuracy (72.8% agreement with prior work, but 60.7% correct when disagreeing) and 50.6% repair success rate (vs 18.5% in prior work), demonstrating superior adaptability and accuracy.", "conclusion": "MatchFixAgent establishes a more reliable and PL-agnostic approach to code translation validation and repair, significantly improving cross-language generalization and correction capabilities through its multi-agent architecture and LLM-driven semantic analysis framework."}}
{"id": "2509.15653", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.15653", "abs": "https://arxiv.org/abs/2509.15653", "authors": ["Yaser Baseri", "Abdelhakim Hafid", "Arash Habibi Lashkari"], "title": "Future-Proofing Cloud Security Against Quantum Attacks: Risk, Transition, and Mitigation Strategies", "comment": null, "summary": "Quantum Computing (QC) introduces a transformative threat to digital\nsecurity, with the potential to compromise widely deployed classical\ncryptographic systems. This survey offers a comprehensive and systematic\nexamination of quantumsafe security for Cloud Computing (CC), focusing on the\nvulnerabilities, transition strategies, and mitigation mechanisms required to\nsecure cloud infrastructures in the quantum era. We evaluated the landscape of\nquantum threats across the entire CC stack, demonstrating how quantum\nalgorithms can undermine classical encryption and compromise cloud security at\nmultiple architectural layers. Using a structured risk assessment methodology\nbased on the STRIDE model, we evaluate quantum-induced attack vectors and their\nimpact on cloud environments. To address these challenges, we propose a layered\nsecurity framework that integrates hybrid cryptographic transition strategies,\ncryptographic agility, and proactive risk mitigation. We analyze the\npreparation and implementation approaches of the major Cloud Service Providers\n(CSPs), including AWS, Azure and GCP, synthesizing platform-specific\ninitiatives toward Post-Quantum Cryptography (PQC). Furthermore, we provide a\ndetailed evaluation of standardized PQC algorithms, exploring their resilience\nto side-channel and active attacks within cloud-native deployments. This survey\nserves as a strategic reference for cloud architects, policymakers, and\nresearchers, offering actionable insights for navigating the complex transition\nto quantum-resilient cloud systems. We conclude by identifying six key future\nresearch directions: standardization and interoperability, performance and\nscalability, implementation security, integration with emerging technologies,\nsystemic preparedness, and crypto-agile migration frameworks.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.15754", "categories": ["cs.CR", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15754", "abs": "https://arxiv.org/abs/2509.15754", "authors": ["Toby Sharp"], "title": "Hornet Node and the Hornet DSL: A Minimal, Executable Specification for Bitcoin Consensus", "comment": null, "summary": "Bitcoin's consensus rules are encoded in the implementation of its reference\nclient: \"The code is the spec.\" Yet this code is unsuitable for formal\nverification due to side effects, mutable state, concurrency, and legacy\ndesign. A standalone formal specification would enable verification both across\nversions of the reference client and against new client implementations,\nstrengthening decentralization by reducing the risk of consensus-splitting\nbugs. Yet such a specification has long been considered intractable given the\ncomplexity of Bitcoin's consensus logic. We demonstrate a compact, executable,\ndeclarative C++ specification of Bitcoin consensus rules that syncs mainnet to\ntip in a few hours on a single thread. We also introduce the Hornet\nDomain-Specific Language (DSL) specifically designed to encode these rules\nunambiguously for execution, enabling formal reasoning, consensus code\ngeneration, and AI-driven adversarial testing. Our spec-driven client Hornet\nNode offers a modern and modular complement to the reference client. Its clear,\nidiomatic style makes it suitable for education, while its performance makes it\nideal for experimentation. We highlight architectural contributions such as its\nlayered design, efficient data structures, and strong separation of concerns,\nsupported by production-quality code examples. We argue that Hornet Node and\nHornet DSL together provide the first credible path toward a pure, formal,\nexecutable specification of Bitcoin consensus.", "AI": {"tldr": "This work creates a verifiable Bitcoin consensus spec (Hornet DSL) and client (Hornet Node) that enable formal verification, reduce consensus risks, and offer a modern, educational alternative to the reference implementation.", "motivation": "Bitcoin's consensus rules are embedded in a complex, unverifiable reference client codebase, creating risks of consensus-splitting bugs. A standalone formal specification would enable cross-version and cross-implementation verification to strengthen decentralization.", "method": "Developed a compact, executable, declarative C++ specification of Bitcoin consensus rules and introduced the Hornet DSL. This DSL facilitates unambiguous encoding, formal reasoning, code generation, and adversarial testing.", "result": "The Hornet Node client syncs mainnet efficiently with production-ready architecture, demonstrating layered design, optimized data structures, and clear separation of concerns. The DSL supports formal reasoning and code generation.", "conclusion": "Hornet Node and Hornet DSL provide the first credible path toward a pure, formal, executable Bitcoin consensus specification, enhancing decentralization through verifiable compliance and modern design."}}
{"id": "2509.15694", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.15694", "abs": "https://arxiv.org/abs/2509.15694", "authors": ["Anastasiia Belousova", "Francesco Marchiori", "Mauro Conti"], "title": "Inference Attacks on Encrypted Online Voting via Traffic Analysis", "comment": "Accepted at ISC 2025", "summary": "Online voting enables individuals to participate in elections remotely,\noffering greater efficiency and accessibility in both governmental and\norganizational settings. As this method gains popularity, ensuring the security\nof online voting systems becomes increasingly vital, as the systems supporting\nit must satisfy a demanding set of security requirements. Most research in this\narea emphasizes the design and verification of cryptographic protocols to\nprotect voter integrity and system confidentiality. However, other vectors,\nsuch as network traffic analysis, remain relatively understudied, even though\nthey may pose significant threats to voter privacy and the overall\ntrustworthiness of the system.\n  In this paper, we examine how adversaries can exploit metadata from encrypted\nnetwork traffic to uncover sensitive information during online voting. Our\nanalysis reveals that, even without accessing the encrypted content, it is\npossible to infer critical voter actions, such as whether a person votes, the\nexact moment a ballot is submitted, and whether the ballot is valid or spoiled.\nWe test these attacks with both rule-based techniques and machine learning\nmethods. We evaluate our attacks on two widely used online voting platforms,\none proprietary and one partially open source, achieving classification\naccuracy as high as 99.5%. These results expose a significant privacy\nvulnerability that threatens key properties of secure elections, including\nvoter secrecy and protection against coercion or vote-buying. We explore\nmitigations to our attacks, demonstrating that countermeasures such as payload\npadding and timestamp equalization can substantially limit their effectiveness.", "AI": {"tldr": "This paper exposes how encrypted traffic metadata in online voting systems can be exploited to breach privacy, even without decrypting content, and proposes practical countermeasures like timestamp equalization and payload padding.", "motivation": "The rapid adoption of online voting necessitates addressing understudied threats like network traffic analysis, which can compromise voter privacy and election trustworthiness despite robust cryptographic protocols.", "method": "The paper employs rule-based and machine learning techniques to analyze metadata from encrypted network traffic, evaluating attacks on two online voting platforms.", "result": "The attacks achieved classification accuracy up to 99.5%, revealing vulnerabilities in ballot submission timing and validity. Mitigations reduced attack effectiveness but underscored system weaknesses.", "conclusion": "The study highlights significant privacy vulnerabilities in online voting systems through network traffic analysis, demonstrating that even encrypted metadata can be exploited. Countermeasures like payload padding and timestamp equalization effectively mitigate these risks."}}
{"id": "2509.15756", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15756", "abs": "https://arxiv.org/abs/2509.15756", "authors": ["Dongyang Zhan", "Kai Tan", "Lin Ye", "Xiangzhan Yu", "Hongli Zhang", "Zheng He"], "title": "An Adversarial Robust Behavior Sequence Anomaly Detection Approach Based on Critical Behavior Unit Learning", "comment": null, "summary": "Sequential deep learning models (e.g., RNN and LSTM) can learn the sequence\nfeatures of software behaviors, such as API or syscall sequences. However,\nrecent studies have shown that these deep learning-based approaches are\nvulnerable to adversarial samples. Attackers can use adversarial samples to\nchange the sequential characteristics of behavior sequences and mislead malware\nclassifiers. In this paper, an adversarial robustness anomaly detection method\nbased on the analysis of behavior units is proposed to overcome this problem.\nWe extract related behaviors that usually perform a behavior intention as a\nbehavior unit, which contains the representative semantic information of local\nbehaviors and can be used to improve the robustness of behavior analysis. By\nlearning the overall semantics of each behavior unit and the contextual\nrelationships among behavior units based on a multilevel deep learning model,\nour approach can mitigate perturbation attacks that target local and\nlarge-scale behaviors. In addition, our approach can be applied to both\nlow-level and high-level behavior logs (e.g., API and syscall logs). The\nexperimental results show that our approach outperforms all the compared\nmethods, which indicates that our approach has better performance against\nobfuscation attacks.", "AI": {"tldr": "This paper introduces a robust malware detection framework using behavior-unit analysis with multilevel deep learning, effectively mitigating adversarial attacks by capturing contextual semantics while outperforming baselines in obfuscation attack resistance.", "motivation": "Sequential deep learning models (e.g., RNN/LSTM) fail to defend against adversarial attacks that manipulate behavior sequences, creating vulnerabilities in malware detection. Existing approaches do not address obfuscation attacks that perturb sequential characteristics.", "method": "The method involves: (1) identifying behavior units with representative semantic information; (2) using a multilevel deep learning model to analyze behavior unit semantics and contextual relationships; (3) mitigating perturbation attacks targeting both local and large-scale behaviors.", "result": "Experimental validation demonstrates the proposed method outperforms existing techniques in detecting obfuscation attacks, showing improved robustness against perturbation attacks targeting both local and large-scale behavior patterns.", "conclusion": "The proposed approach enhances malware classification robustness against adversarial attacks by leveraging behavior units, achieving superior performance across both low-level and high-level behavior logs."}}
{"id": "2509.15725", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.15725", "abs": "https://arxiv.org/abs/2509.15725", "authors": ["Matteo Repetto", "Enrico Cambiaso", "Fabio Patrone", "Sandro Zappatore"], "title": "Flying Drones to Locate Cyber-Attackers in LoRaWAN Metropolitan Networks", "comment": "12 pages", "summary": "Today, many critical services and industrial systems rely on wireless\nnetworks for interaction with the IoT, hence becoming vulnerable to a broad\nnumber of cyber-threats. While detecting this kind of attacks is not difficult\nwith common cyber-security tools, and even trivial for jamming, finding their\norigin and identifying culprits is almost impossible today, yet indispensable\nto stop them, especially when attacks are generated with portable or self-made\ndevices that continuously move around. To address this open challenge, the\nFOLLOWME project investigates the feasibility of using UAV to locate and even\nchase attackers during illicit usage of the radio spectrum. The main objective\nis to develop a cyber-physical security framework that integrates network\ntelemetry with wireless localization. The former triggers alarms in case of\nanomalies or known attack patterns and provides a coarse-grained indication of\nthe physical area (i.e., the position of affected access gateways), whereas the\nlatter systematically scans such area to identify the exact location of the\nattacker. The project will specifically address long-range metropolitan area\nnetworks and focus on the LoRaWAN protocol, which is the typical scenario for\nSmart City services.", "AI": {"tldr": "The paper proposes a cyber-physical security framework using UAVs (FOLLOWME project)\n                      to locate attackers in wireless IoT networks (particularly LoRaWAN), addressing the challenge\n                      of identifying mobile adversaries in critical infrastructure.", "motivation": "Critical IoT services face vulnerabilities due to wireless networks, but\n                      tracing mobile attackers (e.g., with portable jamming devices)\n                      is currently infeasible despite detectable attacks\u2014hindering\n                      effective threat mitigation in smart city environments.", "method": "Integrates network telemetry (anomaly detection, attack pattern recognition with\n                      gateway localization) with UAV-based wireless localization (systematic scanning of\n                      high-risk areas). Focuses on LoRaWAN protocol for Smart City applications.", "result": "Develops a framework that combines coarse-grained network alerts with UAV-driven\n                      fine-grained localization to trace and intercept attackers in\n                      long-range metropolitan IoT networks.", "conclusion": "The project demonstrates feasible use of UAVs to overcome limitations of existing\n                      tools in tracking mobile radio attacks, offering a scalable solution\n                      for secure Smart City radio spectrum management."}}
{"id": "2509.16030", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.16030", "abs": "https://arxiv.org/abs/2509.16030", "authors": ["Kai Tan", "Dongyang Zhan", "Lin Ye", "Hongli Zhang", "Binxing Fang", "Zhihong Tian"], "title": "A High-performance Real-time Container File Monitoring Approach Based on Virtual Machine Introspection", "comment": null, "summary": "As cloud computing continues to advance and become an integral part of modern\nIT infrastructure, container security has emerged as a critical factor in\nensuring the smooth operation of cloud-native applications. An attacker can\nattack the service in the container or even perform the container escape attack\nby tampering with the files. Monitoring container files is important for APT\ndetection and cyberspace security. Existing file monitoring methods are usually\nbased on host operating system or virtual machine introspection to protect file\nsecurity in real time. The methods based on the host operating system usually\nmonitor file operations in the host operating system. However, when the\ncontainer escapes to the host, the host operating system will no longer be\nsecure, so these methods face the problem of weak security. Aiming at the\nproblems of low security and high overload introduced in existing container\nfile monitoring, a high-performance container file monitoring method based on\nvirtual machine introspection is proposed. The experimental results show that\nthe proposed approach can effectively monitor the container files and introduce\nan acceptable monitoring overload.", "AI": {"tldr": "This paper proposes a VM-introspection-based container file monitoring solution that overcomes security weaknesses and performance limitations of existing host-based methods, achieving effective file monitoring with low overhead.", "motivation": "Existing container file monitoring methods based on host OS or virtual machine introspection face security risks (e.g., container escape vulnerabilities) and high computational overhead, necessitating a more secure and efficient solution.", "method": "The authors propose a container file monitoring approach utilizing virtual machine introspection instead of relying on the host operating system, enhancing security and reducing monitoring overhead.", "result": "Experiments demonstrate effective container file monitoring with acceptable performance overhead, validating the approach's feasibility.", "conclusion": "The proposed high-performance container file monitoring method based on virtual machine introspection effectively addresses security and performance issues in existing solutions."}}
{"id": "2509.16038", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.16038", "abs": "https://arxiv.org/abs/2509.16038", "authors": ["Miel Verkerken", "Laurens D'hooge", "Bruno Volckaert", "Filip De Turck", "Giovanni Apruzzese"], "title": "ConCap: Practical Network Traffic Generation for Flow-based Intrusion Detection Systems", "comment": "20 pages", "summary": "Network Intrusion Detection Systems (NIDS) have been studied in research for\nalmost four decades. Yet, despite thousands of papers claiming scientific\nadvances, a non-negligible number of recent works suggest that the findings of\nprior literature may be questionable. At the root of such a disagreement is the\nwell-known challenge of obtaining data representative of a real-world\nnetwork-and, hence, usable for security assessments. We tackle such a challenge\nin this paper. We propose ConCap, a practical tool meant to facilitate\nexperimental research on NIDS. Through ConCap, a researcher can set up an\nisolated and lightweight network environment and configure it to produce\nnetwork-related data, such as packets or NetFlows, that are automatically\nlabeled, hence ready for fine-grained experiments. ConCap is rooted on\nopen-source software and is designed to foster experimental reproducibility\nacross the scientific community by sharing just one configuration file. Through\ncomprehensive experiments on 10 different network activities, further expanded\nvia in-depth analyses of 21 variants of two specific activities and of 100\nrepetitions of four other ones, we empirically verify that ConCap produces\nnetwork data resembling that of a real-world network. We also carry out\nexperiments on well-known benchmark datasets as well as on a real \"smart-home\"\nnetwork, showing that, from a cyber-detection viewpoint, ConCap's\nautomatically-labeled NetFlows are functionally equivalent to those collected\nin other environments. Finally, we show that ConCap enables to safely reproduce\nsophisticated attack chains (e.g., to test/enhance existing NIDS). Altogether,\nConCap is a solution to the \"data problem\" that is plaguing NIDS research.", "AI": {"tldr": "ConCap solves NIDS research's data problem by enabling realistic, labeled network data generation in a reproducible, open-source framework", "motivation": "Prior NIDS research suffers from questionable data representativeness and reproducibility due to challenges in obtaining real-world network data, undermining scientific validity.", "method": "Proposes ConCap, an open-source tool that creates isolated network environments to generate automatically-labeled network data (e.g., packets, NetFlows) through customizable configurations, ensuring research reproducibility with shared setup files.", "result": "Empirical validation across 10 network activities, 21 activity variants, and 100 repetitions shows ConCap data mirrors real-world networks; benchmark comparisons confirm functional equivalence of ConCap NetFlows to real datasets; tool safely replicates attack chains for testing.", "conclusion": "ConCap addresses the critical issue of data reliability and reproducibility in NIDS research by enabling the generation of realistic, labeled network data in a controlled environment, thereby enhancing the validity and replicability of experiments."}}
{"id": "2509.16052", "categories": ["cs.CR", "cs.DC", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2509.16052", "abs": "https://arxiv.org/abs/2509.16052", "authors": ["Vabuk Pahari", "Andrea Canidio"], "title": "How Exclusive are Ethereum Transactions? Evidence from non-winning blocks", "comment": "arXiv admin note: text overlap with arXiv:2506.04940", "summary": "We analyze 15,097 blocks proposed for inclusion in Ethereum's blockchain over\nan 8-minute window on December 3, 2024, during which 38 blocks were added to\nthe chain. We classify transactions as exclusive -- present only in blocks from\na single builder -- or private -- absent from the public mempool but included\nin blocks from multiple builders. We find that exclusive transactions account\nfor 84% of the total fees paid by transactions in winning blocks. Furthermore,\nwe show that exclusivity cannot be fully explained by exclusive relationships\nbetween senders and builders: about 7% of all exclusive transactions included\non-chain, by value, come from senders who route exclusively to a single\nbuilder. Analyzing transaction logs shows that some exclusive transactions are\nduplicates or variations of the same strategy, but even accounting for that,\nthe share of the total fees paid by transactions in winning blocks is at least\n77.2%. Taken together, our findings highlight that exclusive transactions are\nthe dominant source of builder revenues.", "AI": {"tldr": "The study examines Ethereum block proposers' exclusive/ private transactions, revealing 77.2-84%\u00a0of builder revenues from exclusive transactions unexplained by sender-builder monopolies.", "motivation": "This research investigates mechanisms driving builder revenues in Ethereum's MEV ecosystem to assess decentralization effects and market monopolization risks from transaction exclusivity patterns.", "method": "Analyzed 15,097 blocks in an 8-minute window through transaction classification (exclusive/private), sender-routing analysis, and blockchain log pattern detection using Ethereum's December 3, 2024 block data.", "result": "84% of total transaction fees in winning blocks stem from exclusive transactions; 77.2%\u00a0remain after duplicates, with only 7%\u00a0linked to sender-builder monopolies. Private transactions contribute 16%\u00a0of blockspace revenue.", "conclusion": "Exclusive transaction dominance in builder revenues far exceeds sender-builder natural monopolies, suggesting structural market advantages over standard MEV extraction in Ethereum's current design."}}
