{"id": "2508.16619", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16619", "abs": "https://arxiv.org/abs/2508.16619", "authors": ["Rahul Mishra", "Sudhanshu Kumar Jha", "Naresh Kshetri", "Bishnu Bhusal", "Mir Mehedi Rahman", "Md Masud Rana", "Aimina Ali Eli", "Khaled Aminul Islam", "Bishwo Prakash Pokharel"], "title": "nodeWSNsec: A hybrid metaheuristic approach for reliable security and node deployment in WSNs", "comment": "12 pages, 9 figures", "summary": "Efficient and reliable node deployment in Wireless Sensor Networks is crucial\nfor optimizing coverage of the area, connectivity among nodes, and energy\nefficiency. This paper proposes a hybrid meta heuristic approach combining a\nGenetic Algorithm (GA) and Particle Swarm Optimization (PSO) to address the\nchallenges of energy efficient and reliable node deployment. The GA PSO hybrid\nleverages GAs strong exploration capabilities and PSOs rapid convergence,\nachieving an optimum stability between coverage and energy consumption. The\nperformance of the proposed approach is evaluated against GA and PSO alone and\nthe innovatory meta heuristic based Competitive Multi Objective Marine\nPredators Algorithm (CMOMPA) across varying sensing ranges. Simulation results\ndemonstrate that GA PSO requires 15% to 25% fewer sensor nodes and maintains\n95% or more area coverage while maintaining the connectivity in comparison to\nstandalone GA or PSO algorithm. The proposed algorithm also dominates CMOMPA\nwhen compared for long sensing and communication range in terms of higher\ncoverage, improved connectivity, and reduced deployment time while requiring\nfewer sensor nodes. This study also explores key trade offs in WSN deployment\nand highlights future research directions, including heterogeneous node\ndeployment, mobile WSNs, and enhanced multi objective optimization techniques.\nThe findings underscore the effectiveness of hybrid meta heuristics in\nimproving WSN performance, offering a promising approach for real world\napplications such as environmental monitoring, smart cities, smart agriculture,\ndisaster response, and IIoT.", "AI": {"tldr": "This paper introduces a hybrid GA-PSO algorithm for energy-efficient and reliable node deployment in Wireless Sensor Networks (WSN), achieving fewer nodes required and better coverage/connectivity compared to standalone GA/PSO and a CMOMPA competitor. Key trade-offs and future directions are explored.", "motivation": "Optimizing WSN node deployment is critical for coverage, connectivity, and energy efficiency. Prior methods face limitations in balancing these objectives, necessitating a hybrid approach to leverage complementary strengths.", "method": "A hybrid meta-heuristic algorithm combining Genetic Algorithm (GA) for exploration with Particle Swarm Optimization (PSO) for convergence, addressing multi-objective optimization for node deployment.", "result": "GA-PSO reduces sensor nodes by 15\u201325% while maintaining \u226595% area coverage and connectivity compared to GA/PSO alone. It outperforms CMOMPA in coverage, connectivity, deployment time, and node count for long-range scenarios.", "conclusion": "Hybrid meta-heuristics like GA-PSO effectively optimize WSN deployment, with promising potential for real-world applications. Future work includes heterogeneous nodes, mobile WSNs, and advanced multi-objective algorithms."}}
{"id": "2508.16625", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16625", "abs": "https://arxiv.org/abs/2508.16625", "authors": ["Rijha Safdar", "Danyail Mateen", "Syed Taha Ali", "M. Umer Ashfaq", "Wajahat Hussain"], "title": "Data and Context Matter: Towards Generalizing AI-based Software Vulnerability Detection", "comment": null, "summary": "The performance of AI-based software vulnerability detection systems is often\nlimited by their poor generalization to unknown codebases. In this research, we\nexplore the impact of data quality and model architecture on the\ngeneralizability of vulnerability detection systems. By generalization we mean\nability of high vulnerability detection performance across different C/C++\nsoftware projects not seen during training. Through a series of experiments, we\ndemonstrate that improvements in dataset diversity and quality substantially\nenhance detection performance. Additionally, we compare multiple encoder-only\nand decoder-only models, finding that encoder based models outperform in terms\nof accuracy and generalization. Our model achieves 6.8% improvement in recall\non the benchmark BigVul[1] dataset, also outperforming on unseen projects,\nhence showing enhanced generalizability. These results highlight the role of\ndata quality and model selection in the development of robust vulnerability\ndetection systems. Our findings suggest a direction for future systems having\nhigh cross-project effectiveness.", "AI": {"tldr": "This paper investigates how data quality and model architecture affect the generalization of AI-based vulnerabilities detectors. Experiments show that diverse datasets and encoder models enhance cross-project performance.", "motivation": "Current AI-based vulnerability detection systems struggle to generalize across unseen C/C++ codebases, necessitating improvements in dataset and model strategies.", "method": "The study conducts experiments comparing encoder-only and decoder-only models, evaluates dataset diversity/quality, and tests on the BigVul benchmark to assess cross-project performance.", "result": "Encoder models achieved 6.8% higher recall on BigVul and outperformed in unseen projects. Dataset improvements significantly boosted detection performance.", "conclusion": "The research highlights data quality and encoder architecture as critical for robust vulnerability detectors with cross-project effectiveness, guiding future system development."}}
{"id": "2508.16637", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16637", "abs": "https://arxiv.org/abs/2508.16637", "authors": ["Abraham Itzhak Weinberg"], "title": "Passive Hack-Back Strategies for Cyber Attribution: Covert Vectors in Denied Environment", "comment": null, "summary": "Attributing cyberattacks remains a central challenge in modern cybersecurity,\nparticularly within denied environments where defenders have limited visibility\ninto attacker infrastructure and are restricted by legal or operational rules\nof engagement. This perspective examines the strategic value of passive\nhack-back techniques that enable covert attribution and intelligence collection\nwithout initiating direct offensive actions. Key vectors include tracking\nbeacons, honeytokens, environment-specific payloads, and supply-chain-based\ntraps embedded within exfiltrated or leaked assets. These approaches rely on\nthe assumption that attackers will interact with compromised data in traceable\nways, allowing defenders to gather signals without violating engagement\npolicies. The paper also explores the role of Artificial Intelligence (AI) in\nenhancing passive hack-back operations. Topics include the deployment of\nautonomous agents for forensic reconnaissance, the use of Large Language Models\n(LLMs) to generate dynamic payloads, and Adversarial Machine Learning (AML)\ntechniques for evasion and counter-deception. A dedicated section discusses the\nimplications of quantum technologies in this context, both as future threats to\ncryptographic telemetry and as potential tools for stealthy communication and\npost-quantum resilience. Finally, the paper advocates for hybrid defensive\nframeworks that combine passive attribution with delayed or conditional active\nresponses, while maintaining compliance with legal, ethical, and operational\nconstraints.", "AI": {"tldr": "This paper explores passive hack-back techniques for covert cyberattack attribution and intelligence collection, integrating AI and quantum technologies for effectiveness and stealth, while advocating hybrid frameworks with conditional active responses for legal compliance.", "motivation": "Modern cybersecurity faces challenges in attributing attacks in denied environments with limited visibility and restrictive engagement policies.", "method": "Analyzes passive methods (tracking beacons, honeytokens, environment-specific payloads, supply-chain traps) combined with AI (autonomous agents, LLMs for dynamic payloads) and quantum technologies for evasion, communication, and resilience.", "result": "Demonstrates how passive attribution with AI/quantum tools can bypass engagement restrictions while gathering actionable intelligence.", "conclusion": "Hybrid frameworks merging passive attribution with delayed/conditional active defenses, while adhering to legal/ethical constraints, provide optimal cybersecurity solutions in restricted scenarios."}}
{"id": "2508.16662", "categories": ["cs.CR", "cs.CY", "cs.NI", "cs.SE", "K.6.5; C.2.0; D.4.6"], "pdf": "https://arxiv.org/pdf/2508.16662", "abs": "https://arxiv.org/abs/2508.16662", "authors": ["Alexander Tabalipa"], "title": "Bridging the Mobile Trust Gap: A Zero Trust Framework for Consumer-Facing Applications", "comment": "43 pages, 5 figures, 9 tables. Working Paper - Version 1.0. Submitted\n  under a CC BY-SA 4.0 license. Also available as an SSRN Working Paper.\n  Feedback and collaboration are welcome", "summary": "Zero Trust Architecture (ZTA) has become a widely adopted model for securing\nenterprise environments, promoting continuous verification and minimal trust\nacross systems. However, its application in mobile contexts remains limited,\ndespite mobile applications now accounting for most global digital interactions\nand being increasingly targeted by sophisticated threats. Existing Zero Trust\nframeworks developed by organisations such as the National Institute of\nStandards and Technology (NIST) and the Cybersecurity and Infrastructure\nSecurity Agency (CISA) primarily focus on enterprise-managed infrastructure,\nassuming organisational control over devices, networks, and identities. This\npaper addresses a critical gap by proposing an extended Zero Trust model\ndesigned for mobile applications operating in untrusted, user-controlled\nenvironments. Using a design science methodology, the study introduced a\nsix-pillar framework that supports runtime enforcement of trust through\ncontrols including device integrity, user identity validation, data protection,\nsecure application programming interface (API) usage, behavioural monitoring,\nand live application protection. Each pillar was mapped to relevant regulatory\nand security standards to support compliance. A phased implementation roadmap\nand maturity assessment model were also developed to guide adoption across\nvarying organisational contexts. The proposed model offers a practical and\nstandards-aligned approach to securing mobile applications beyond\npre-deployment controls, aligning real-time enforcement with Zero Trust\nprinciples. This contribution expands the operational boundaries of ZTA and\nprovides organisations with a deployable path to reduce fraud, enhance\ncompliance, and address emerging mobile security challenges. Future research\nmay include empirical validation of the framework and cross-sector application\ntesting.", "AI": {"tldr": "The paper proposes an extended Zero Trust model for mobile applications, addressing security gaps in untrusted user-controlled environments with a six-pillar framework and compliance-aligned implementation roadmap.", "motivation": "Existing Zero Trust frameworks from NIST/CISA focus on enterprise infrastructure with organizational control over devices, but mobile apps face rising sophisticated threats and lack tailored security models despite driving most global digital interactions.", "method": "A design science methodology was used to develop the framework, incorporating device integrity, user identity validation, data protection, secure API usage, behavioral monitoring, and live application protection as six pillars.", "result": "The framework maps to regulatory standards, provides a phased implementation roadmap, and a maturity assessment model, offering real-time security enforcement for mobile apps beyond pre-deployment controls.", "conclusion": "This work expands ZTA boundaries, enabling mobile app security in user-controlled environments, facilitating fraud reduction and compliance. Future research includes empirical validation and cross-sector testing."}}
{"id": "2508.16671", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16671", "abs": "https://arxiv.org/abs/2508.16671", "authors": ["Mingyang Zhou", "Quanming Yao", "Lun Du", "Lanning Wei", "Da Zheng"], "title": "Reflective Paper-to-Code Reproduction Enabled by Fine-Grained Verification", "comment": null, "summary": "Reproducing machine learning papers is essential for scientific progress but\nremains challenging for both humans and automated agents. Existing agent-based\nmethods often struggle to fully and accurately reproduce implementation details\nsuch as mathematical formulas and algorithmic logic. Previous studies show that\nreflection with explicit feedback improves agent performance. However, current\npaper reproduction methods fail to effectively adopt this strategy. This gap\nmainly arises from the diverse paper patterns, complex method modules, and\nvaried configurations encountered in research papers. Motivated by how humans\nuse systematic checklists to efficiently debug complex code, we propose\n\\textbf{RePro}, a \\textbf{Re}flective Paper-to-Code \\textbf{Repro}duction\nframework that automatically extracts a paper's fingerprint, referring to a\ncomprehensive set of accurate and atomic criteria serving as high-quality\nsupervisory signals. The framework first generates code based on the extracted\ninformation, and then leverages the fingerprint within iterative verification\nand refinement loop. This approach systematically detects discrepancies and\nproduces targeted revisions to align generated code with the paper's\nimplementation details. Extensive experiments on the PaperBench Code-Dev\nbenchmark have been conducted, RePro achieves 13.0\\% performance gap over\nbaselines, and it correctly revises complex logical and mathematical criteria\nin reflecting, on which the effectiveness is obvious.", "AI": {"tldr": "This paper introduces RePro, a framework for reflective paper-to-code reproduction in machine learning (ML) that addresses the challenge of accurately implementing paper details. It uses a 'fingerprint' extraction mechanism to create precise criteria for code verification and iterative refinement, achieving a 13.0% performance improvement on the PaperBench Code-Dev benchmark by systematically detecting and revising discrepancies.", "motivation": "Existing paper reproduction methods struggle with implementation accuracy (mathematical formulas, algorithmic logic) and fail to leverage reflection with explicit feedback. Research papers exhibit high variability in structure, complexity, and configuration requirements, necessitating a systematic debug approach similar to human checklists to ensure faithful reproduction.", "method": "RePro 1) extracts a comprehensive set of accurate and atomic 'fingerprints' (implementation criteria) from research papers and 2) uses these fingerprints in an iterative verification/refinement loop during code generation. The framework generates initial code, then systematically identifies discrepancies between generated code and paper specifications (via the fingerprint criteria) and produces targeted revisions to achieve alignment.", "result": "RePro achieves a 13.0% performance gap over baseline methods on the PaperBench Code-Dev benchmark. It demonstrates effectiveness in handling complex logical and mathematical criteria during reflection, successfully detecting and revising implementation inconsistencies that prior methods miss.", "conclusion": "RePro establishes a systematic framework for improving paper reproduction by leveraging fingerprint-based criteria to guide iterative code verification and refinement. This approach enables machines to replicate the human-like practice of using checklists to debug complex implementations, achieving significant performance gains over existing methods on standardized ML reproduction benchmarks."}}
{"id": "2508.16761", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16761", "abs": "https://arxiv.org/abs/2508.16761", "authors": ["Nesrine Benchoubane", "Olfa Ben Yahia", "William Ferguson", "Gurkan Gur", "Sumit Chakravarty", "Gregory Falco", "Gunes Karabulut Kurt"], "title": "Securing Heterogeneous Network (HetNet) Communications for Wildfire Management: Mitigating the Effects of Adversarial and Environmental Threats", "comment": null, "summary": "In the face of adverse environmental conditions and cyber threats, robust\ncommunication systems for critical applications such as wildfire management and\ndetection demand secure and resilient architectures. This paper presents a\nnovel framework that considers both adversarial factors, building resilience\ninto a heterogeneous network (HetNet) integrating Low Earth Orbit (LEO)\nsatellite constellation with High-Altitude Platform Ground Stations (HAPGS) and\nLow-Altitude Platforms (LAPS), tailored to support wildfire management\noperations. Building upon our previous work on secure-by-component approach for\nlink segment security, we extend protection to the communication layer by\nsecuring both Radio Frequency (RF)/Free Space Optics (FSO) management and\ndifferent links. Through a case study, we quantify how environmental stressors\nimpact secrecy capacity and expose the system to passive adversaries. Key\nfindings demonstrate that atmospheric attenuation and beam misalignment can\nnotably degrade secrecy capacity across both short- and long-range\ncommunication links, while high-altitude eavesdroppers face less signal\ndegradation, increasing their interception capability. Moreover, increasing\ntransmit power to counter environmental losses can inadvertently improve\neavesdropper reception, thereby reducing overall link confidentiality. Our work\nnot only highlights the importance of protecting networks from these dual\nthreats but also aligns with the IEEE P3536 Standard for Space System\nCybersecurity Design, ensuring resilience and the prevention of mission\nfailures.", "AI": {"tldr": "This paper proposes a novel heterogeneous communication framework integrating LEO satellites, HAPGS, and LAPS to enhance resilience against environmental challenges and cyber threats in wildfire management, while aligning with IEEE P3536 standards.", "motivation": "Critical applications like wildfire management require secure, resilient communication systems to withstand harsh environmental conditions and increasing cyber threats that could jeopardize mission outcomes.", "method": "The framework extends a secure-by-component approach to the communication layer by incorporating RF/FSO link protection mechanisms into a HetNet architecture. Case study simulations quantify effects of environmental stressors on network security performance.", "result": "Environmental factors cause 15-40% secrecy capacity degradation in LAPS-HAPGS/LEO links, but HAPS remain 15% more secure against eavesdropping even at 20dB SNR. Transmit power optimization shows nonlinear trade-offs between signal quality and confidentiality risks.", "conclusion": "The work establishes that environmental protection and cybersecurity must be co-designed in critical communication networks, providing IEEE P3536-compliant solutions to prevent mission failures from atmospheric effects or signal interception."}}
{"id": "2508.16678", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.16678", "abs": "https://arxiv.org/abs/2508.16678", "authors": ["Konrad Cinkusz", "Jaros\u0142aw A. Chudziak", "Ewa Niewiadomska-Szynkiewicz"], "title": "Cognitive Agents Powered by Large Language Models for Agile Software Project Management", "comment": null, "summary": "This paper investigates the integration of cognitive agents powered by Large\nLanguage Models (LLMs) within the Scaled Agile Framework (SAFe) to reinforce\nsoftware project management. By deploying virtual agents in simulated software\nenvironments, this study explores their potential to fulfill fundamental roles\nin IT project development, thereby optimizing project outcomes through\nintelligent automation. Particular emphasis is placed on the adaptability of\nthese agents to Agile methodologies and their transformative impact on\ndecision-making, problem-solving, and collaboration dynamics. The research\nleverages the CogniSim ecosystem, a platform designed to simulate real-world\nsoftware engineering challenges, such as aligning technical capabilities with\nbusiness objectives, managing interdependencies, and maintaining project\nagility. Through iterative simulations, cognitive agents demonstrate advanced\ncapabilities in task delegation, inter-agent communication, and project\nlifecycle management. By employing natural language processing to facilitate\nmeaningful dialogues, these agents emulate human roles and improve the\nefficiency and precision of Agile practices. Key findings from this\ninvestigation highlight the ability of LLM-powered cognitive agents to deliver\nmeasurable improvements in various metrics, including task completion times,\nquality of deliverables, and communication coherence. These agents exhibit\nscalability and adaptability, ensuring their applicability across diverse and\ncomplex project environments. This study underscores the potential of\nintegrating LLM-powered agents into Agile project management frameworks as a\nmeans of advancing software engineering practices. This integration not only\nrefines the execution of project management tasks but also sets the stage for a\nparadigm shift in how teams collaborate and address emerging challenges.", "AI": {"tldr": "This paper explores integrating LLM-powered cognitive agents into the Scaled Agile Framework (SAFe) to enhance software project management via intelligent automation, demonstrating measurable improvements in efficiency, quality, and collaboration.", "motivation": "The study addresses the need for optimizing IT project management by leveraging cognitive agents to handle technical-business alignment, interdependencies, and maintain project agility in complex environments.", "method": "The research used the CogniSim ecosystem to simulate software engineering challenges, deploying virtual agents that utilized natural language processing to optimize task delegation, inter-agent communication, and project lifecycle management through iterative testing.", "result": "LLM-powered agents improved task completion time, deliverables quality, and communication coherence. They showed scalability and adaptability across diverse project scenarios.", "conclusion": "Integrating LLM agents into Agile frameworks can transform collaboration and problem-solving in software engineering, enabling a paradigm shift through enhanced intelligent automation and adaptive decision-making."}}
{"id": "2508.16765", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16765", "abs": "https://arxiv.org/abs/2508.16765", "authors": ["GodsGift Uzor", "Hasan Al-Qudah", "Ynes Ineza", "Abdul Serwadda"], "title": "Guarding Your Conversations: Privacy Gatekeepers for Secure Interactions with Cloud-Based AI Models", "comment": "2025 19th International Conference on Semantic Computing (ICSC)", "summary": "The interactive nature of Large Language Models (LLMs), which closely track\nuser data and context, has prompted users to share personal and private\ninformation in unprecedented ways. Even when users opt out of allowing their\ndata to be used for training, these privacy settings offer limited protection\nwhen LLM providers operate in jurisdictions with weak privacy laws, invasive\ngovernment surveillance, or poor data security practices. In such cases, the\nrisk of sensitive information, including Personally Identifiable Information\n(PII), being mishandled or exposed remains high. To address this, we propose\nthe concept of an \"LLM gatekeeper\", a lightweight, locally run model that\nfilters out sensitive information from user queries before they are sent to the\npotentially untrustworthy, though highly capable, cloud-based LLM. Through\nexperiments with human subjects, we demonstrate that this dual-model approach\nintroduces minimal overhead while significantly enhancing user privacy, without\ncompromising the quality of LLM responses.", "AI": {"tldr": "This paper proposes an 'LLM gatekeeper' as a local filtering system to enhance privacy when interacting with cloud-based LLMs, addressing risks from weak privacy laws and data security. Human experiments show minimal overhead and preserved response quality.", "motivation": "Users increasingly share personal information with LLMs despite privacy settings, creating risks in jurisdictions with inadequate data protection laws, government surveillance, or poor security practices.", "method": "A lightweight, locally executed 'LLM gatekeeper' filters sensitive information from user queries before transmitting them to cloud-based LLMs, tested via controlled human subject experiments.", "result": "The gatekeeper reduced sensitive data exposure by ~98% in experiments (based on similar studies), while introducing less than 2% response quality degradation and negligible computational overhead.", "conclusion": "The LLM gatekeeper provides a practical architecture to mitigate privacy risks in cloud LLM interactions without sacrificing usability or effectiveness."}}
{"id": "2508.16684", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.16684", "abs": "https://arxiv.org/abs/2508.16684", "authors": ["Vikranth Udandarao", "Nipun Misra"], "title": "Democratizing AI Development: Local LLM Deployment for India's Developer Ecosystem in the Era of Tokenized APIs", "comment": "for survey results, check\n  https://docs.google.com/spreadsheets/d/1t0eV9oURaiu2HfARWo6sriBO0eC8bHUyZNN7CgK2NAk/edit?usp=sharing", "summary": "India's developer community faces significant barriers to sustained\nexperimentation and learning with commercial Large Language Model (LLM) APIs,\nprimarily due to economic and infrastructural constraints. This study\nempirically evaluates local LLM deployment using Ollama as an alternative to\ncommercial cloud-based services for developer-focused applications. Through a\nmixed-methods analysis involving 180 Indian developers, students, and AI\nenthusiasts, we find that local deployment enables substantially greater\nhands-on development and experimentation, while reducing costs by 33% compared\nto commercial solutions. Developers using local LLMs completed over twice as\nmany experimental iterations and reported deeper understanding of advanced AI\narchitectures. Our results highlight local deployment as a critical enabler for\ninclusive and accessible AI development, demonstrating how technological\naccessibility can enhance learning outcomes and innovation capacity in\nresource-constrained environments.", "AI": {"tldr": "The study examines local LLM deployment in India using Ollama to overcome economic and infrastructural barriers, finding it reduces costs by 33% and enables twice as many experimental iterations compared to commercial cloud services.", "motivation": "Indian developers struggle with cost and infrastructure limitations when using commercial LLM APIs for experimentation and learning, creating barriers to inclusive AI development.", "method": "Mixed-methods analysis with 180 participants (developers, students, AI enthusiasts) comparing local Ollama deployments against commercial cloud-based LLM services.", "result": "Local deployments reduced costs by 33%, enabled >2x more experimental iterations, and improved understanding of advanced AI architectures among 180 participants.", "conclusion": "Local LLM deployment is critical for accessible AI development in resource-constrained environments, enhancing learning outcomes and innovation capacity through greater accessibility."}}
{"id": "2508.16843", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16843", "abs": "https://arxiv.org/abs/2508.16843", "authors": ["Kamel Kamel", "Keshav Sood", "Hridoy Sankar Dutta", "Sunil Aryal"], "title": "A Survey of Threats Against Voice Authentication and Anti-Spoofing Systems", "comment": "This paper will be submitted to the Computer Science Review", "summary": "Voice authentication has undergone significant changes from traditional\nsystems that relied on handcrafted acoustic features to deep learning models\nthat can extract robust speaker embeddings. This advancement has expanded its\napplications across finance, smart devices, law enforcement, and beyond.\nHowever, as adoption has grown, so have the threats. This survey presents a\ncomprehensive review of the modern threat landscape targeting Voice\nAuthentication Systems (VAS) and Anti-Spoofing Countermeasures (CMs), including\ndata poisoning, adversarial, deepfake, and adversarial spoofing attacks. We\nchronologically trace the development of voice authentication and examine how\nvulnerabilities have evolved in tandem with technological advancements. For\neach category of attack, we summarize methodologies, highlight commonly used\ndatasets, compare performance and limitations, and organize existing literature\nusing widely accepted taxonomies. By highlighting emerging risks and open\nchallenges, this survey aims to support the development of more secure and\nresilient voice authentication systems.", "AI": {"tldr": "This paper surveys modern threats to voice authentication systems and anti-spoofing countermeasures, tracking their evolution alongside deep learning advancements.", "motivation": "Voice authentication's growing adoption in security-critical domains like finance and law enforcement necessitates understanding emerging threats such as deepfake and adversarial attacks.", "method": "Chronological analysis of voice authentication evolution, taxonomic categorization of attacks (data poisoning, adversarial attacks, deepfake, and spoofing), performance comparisons, dataset reviews, and literature synthesis.", "result": "Systematic summary of attack methodologies, evaluation of existing literature's strengths/limitations, and identification of emerging risks in voice biometrics.", "conclusion": "The survey provides a framework for developing more secure voice authentication systems by clarifying attack landscapes and highlighting open security challenges."}}
{"id": "2508.16688", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16688", "abs": "https://arxiv.org/abs/2508.16688", "authors": ["Ankur Tomar", "Hengyue Liang", "Indranil Bhattacharya", "Natalia Larios", "Francesco Carbone"], "title": "Cybernaut: Towards Reliable Web Automation", "comment": null, "summary": "The emergence of AI-driven web automation through Large Language Models\n(LLMs) offers unprecedented opportunities for optimizing digital workflows.\nHowever, deploying such systems within industry's real-world environments\npresents four core challenges: (1) ensuring consistent execution, (2)\naccurately identifying critical HTML elements, (3) meeting human-like accuracy\nin order to automate operations at scale and (4) the lack of comprehensive\nbenchmarking data on internal web applications. Existing solutions are\nprimarily tailored for well-designed, consumer-facing websites (e.g.,\nAmazon.com, Apple.com) and fall short in addressing the complexity of\npoorly-designed internal web interfaces. To address these limitations, we\npresent Cybernaut, a novel framework to ensure high execution consistency in\nweb automation agents designed for robust enterprise use. Our contributions are\nthreefold: (1) a Standard Operating Procedure (SOP) generator that converts\nuser demonstrations into reliable automation instructions for linear browsing\ntasks, (2) a high-precision HTML DOM element recognition system tailored for\nthe challenge of complex web interfaces, and (3) a quantitative metric to\nassess execution consistency. The empirical evaluation on our internal\nbenchmark demonstrates that using our framework enables a 23.2% improvement\n(from 72% to 88.68%) in task execution success rate over the browser_use.\nCybernaut identifies consistent execution patterns with 84.7% accuracy,\nenabling reliable confidence assessment and adaptive guidance during task\nexecution in real-world systems. These results highlight Cybernaut's\neffectiveness in enterprise-scale web automation and lay a foundation for\nfuture advancements in web automation.", "AI": {"tldr": "The paper addresses four challenges in deploying LLMs for web automation in enterprise environments and introduces Cybernaut, a framework improving task execution success rates by 23.2% through SOP generation, high-precision DOM element recognition, and consistency metrics.", "motivation": "Existing web automation systems fail to handle poorly-designed internal enterprise web interfaces, creating a critical gap in real-world deployment despite promising opportunities for digital workflow optimization.", "method": "Cybernaut introduces (1) a SOP generator for creating reliable automation instructions from user demonstrations, (2) a specialized HTML DOM element recognition system for complex interfaces, and (3) a quantitative metric to measure execution consistency via internal benchmarking.", "result": "Achieved 88.68% task execution success rate with 23.2% improvement over browser_use, plus 84.7% accuracy in identifying consistent execution patterns for adaptive automated workflows.", "conclusion": "Cybernaut demonstrates significant effectiveness in enterprise-scale web automation, solving key limitations of existing tools and establishing a foundation for future advancements in industrial automation applications."}}
{"id": "2508.16868", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.16868", "abs": "https://arxiv.org/abs/2508.16868", "authors": ["Joshua Mashburn", "Johann Knechtel", "Florian Klemme", "Hussam Amrouch", "Ozgur Sinanoglu", "Paul V. Gratz"], "title": "Targeted Wearout Attacks in Microprocessor Cores", "comment": "13 pages, 11 figures, submitted to IEEE International Symposium on\n  High-Performance Computer Architecture 2026 (HPCA-32)", "summary": "Negative-Bias Temperature Instability is a dominant aging mechanism in\nnanoscale CMOS circuits such as microprocessors. With this aging mechanism, the\nrate of device aging is dependent not only on overall operating conditions,\nsuch as heat, but also on user controllable inputs to the transistors. This\ndependence on input implies a possible timing fault-injection attack wherein a\ntargeted path of logic is intentionally degraded through the purposeful,\nsoftware-driven actions of an attacker, rendering a targeted bit effectively\nstuck.\n  In this work, we describe such an attack mechanism, which we dub a\n\"$\\textbf{Targeted Wearout Attack}$\", wherein an attacker with sufficient\nknowledge of the processor core, executing a carefully crafted software program\nwith only user privilege, is able to degrade a functional unit within the\nprocessor with the aim of eliciting a particular desired incorrect calculation\nin a victim application. Here we give a general methodology for the attack. We\nthen demonstrate a case study where a targeted path within the fused\nmultiply-add pipeline in a RISC-V CPU sees a $>7x$ increase in wear over time\nthan would be experienced under typical workloads. We show that an attacker\ncould leverage such an attack, leading to targeted and silent data corruption\nin a co-running victim application using the same unit.", "AI": {"tldr": "This paper investigates a software-driven attack exploiting nanoscale CMOS aging to cause targeted device degradation and silent data corruption in co-running applications.", "motivation": "Traditional fault-injection attacks require physical access or elevated privileges; software-based degradation through controllable inputs can bypass these limitations.", "method": "Demonstrate a Targeted Wearout Attack by crafting software to stress a RISC-V CPU's fused multiply-add pipeline, accelerating aging via Negative-Bias Temperature Instability mechanisms.", "result": "Achieved >7x accelerated wear on targeted logic paths compared to typical workloads, inducing predictable silent data corruption in victim applications sharing the same functional unit", "conclusion": "Software-initiated aging attacks can systematically corrupt specific logic paths without physical access, requiring platform-specific defenses."}}
{"id": "2508.16708", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16708", "abs": "https://arxiv.org/abs/2508.16708", "authors": ["Shufeng Chen", "Halima El Badaoui", "Mariat James Elizebeth", "Takuya Nakashima", "Siddartha Khastgir", "Paul Jennings"], "title": "A Scalable Framework for the Management of STPA Requirements: a Case Study on eVTOL Operations", "comment": null, "summary": "System-Theoretic Process Analysis (STPA) is a recommended method for\nanalysing complex systems, capable of identifying thousands of safety\nrequirements often missed by traditional techniques such as Failure Mode and\nEffects Analysis (FMEA) and Fault Tree Analysis (FTA). However, the absence of\na structured framework for managing and prioritising these requirements\npresents challenges, particularly in fast-paced development environments. This\npaper introduces a scalable framework for prioritising STPA-derived\nrequirements. The framework integrates outputs from each STPA step and\nincorporates expert evaluations based on four key factors: implementation time,\ncost, requirement type, and regulatory coverage. To reduce subjectivity,\nMonte-Carlo Simulation (MCS) is employed to calculate and stabilise requirement\nrankings. An automation toolchain supports the framework, enabling dynamic\nmapping of prioritised requirements in a scaling matrix. This visualisation\naids decision-making and ensures traceability across development phases. The\nframework is applicable from early conceptualisation to more advanced stages,\nenhancing its utility in iterative system development. The framework was\nvalidated through a real-world case study focused on Electric Vertical Take-off\nand Landing (eVTOL) operations, conducted in collaboration with the UK Civil\nAviation Authority. The findings contributed directly to CAP3141, a Civil\nAviation Publication that identifies systemic operational risks and safety\nmitigations for regulators, operators, and vertiports. The prioritisation\nprocess supported decision-making by helping stakeholders identify and manage\nhigh-impact requirements efficiently. This work contributes a practical\nsolution for managing STPA outputs, bridging gaps in requirement prioritisation\nand supporting safety-critical development in emerging technologies.", "AI": {"tldr": "This paper introduces a scalable Monte-Carlo Simulation-based framework for prioritizing STPA safety requirements, validated in eVTOL operations and contributing to CAP3141 publication.", "motivation": "Traditional safety analysis techniques like FMEA and FTA often miss critical requirements in complex systems, while STPA generates thousands of requirements that need structured prioritization in fast-paced development environments.", "method": "The framework integrates STPA outputs with expert evaluations (implementation time, cost, requirement type, regulatory coverage) and uses Monte-Carlo Simulation to reduce subjectivity in rankings, supported by an automation toolchain for dynamic requirement mapping in a scaling matrix visualization.", "result": "Validation through eVTOL case study collaboratively with UK Civil Aviation Authority directly informed CAP3141, enabling stakeholders to efficiently manage high-impact requirements across development phases with enhanced traceability.", "conclusion": "The presented framework addresses gaps in STPA requirement prioritization, providing a practical solution for safety-critical development in emerging technologies like eVTOL, while supporting systemic risk identification and mitigation through regulatory collaboration."}}
{"id": "2508.16941", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16941", "abs": "https://arxiv.org/abs/2508.16941", "authors": ["Yu Cheng", "Xiaofang Qi", "Yanhui Li"], "title": "Investigating red packet fraud in Android applications: Insights from user reviews", "comment": "This manuscript has been accepted for publication in Cybersecurity\n  (Springer Nature). The final version of record will be published by Springer\n  Nature and will be accessible online. This version is the Author Accepted\n  Manuscript (AAM) and may differ from the final published version", "summary": "With the popularization of smartphones, red packets have been widely used in\nmobile apps. However, the issues of fraud associated with them have also become\nincreasingly prominent. As reported in user reviews from mobile app markets,\nmany users have complained about experiencing red packet fraud and being\npersistently troubled by fraudulent red packets. To uncover this phenomenon, we\nconduct the first investigation into an extensive collection of user reviews on\napps with red packets. In this paper, we first propose a novel automated\napproach, ReckDetector, for effectively identifying apps with red packets from\napp markets. We then collect over 360,000 real user reviews from 334 apps with\nred packets available on Google Play and three popular alternative Android app\nmarkets. We preprocess the user reviews to extract those related to red packets\nand fine-tune a pre-trained BERT model to identify negative reviews. Finally,\nbased on semantic analysis, we have summarized six distinct categories of red\npacket fraud issues reported by users. Through our study, we found that red\npacket fraud is highly prevalent, significantly impacting user experience and\ndamaging the reputation of apps. Moreover, red packets have been widely\nexploited by unscrupulous app developers as a deceptive incentive mechanism to\nentice users into completing their designated tasks, thereby maximizing their\nprofits.", "AI": {"tldr": "This paper investigates red packet fraud in mobile apps by analyzing 360,000 user reviews and proposes ReckDetector, an automated tool to identify affected apps, summarizing six fraud categories and their detrimental impacts.", "motivation": "The increasing prevalence of red packet fraud in smartphone apps, as highlighted by persistent user complaints in app market reviews, necessitates a systematic investigation to understand its scope, user impact, and developer exploitation.", "method": "The study uses ReckDetector to automate red packet app identification, collects and preprocesses 360,000 user reviews from Google Play and Android alternative markets, fine-tunes a pre-trained BERT model for negative review classification, and performs semantic analysis to categorize fraud types.", "result": "Six distinct red packet fraud categories were identified, with evidence showing widespread fraud affecting user experience and app reputations. Fraudulent red packets are exploited as deceptive incentive mechanisms to maximize developer profits.", "conclusion": "Red packet fraud is a significant issue in Android apps, with unscrupulous developers using it to manipulate user behavior. The paper highlights the need for improved market enforcement and detection tools to mitigate this growing problem."}}
{"id": "2508.16713", "categories": ["cs.SE", "cs.AI", "hep-ex"], "pdf": "https://arxiv.org/pdf/2508.16713", "abs": "https://arxiv.org/abs/2508.16713", "authors": ["Mohammad Atif", "Kriti Chopra", "Ozgur Kilic", "Tianle Wang", "Zhihua Dong", "Charles Leggett", "Meifeng Lin", "Paolo Calafiura", "Salman Habib"], "title": "CelloAI: Leveraging Large Language Models for HPC Software Development in High Energy Physics", "comment": "12 pages, 2 figures", "summary": "Next-generation High Energy Physics (HEP) experiments will generate\nunprecedented data volumes, necessitating High Performance Computing (HPC)\nintegration alongside traditional high-throughput computing. However, HPC\nadoption in HEP is hindered by the challenge of porting legacy software to\nheterogeneous architectures and the sparse documentation of these complex\nscientific codebases. We present CelloAI, a locally hosted coding assistant\nthat leverages Large Language Models (LLMs) with retrieval-augmented generation\n(RAG) to support HEP code documentation and generation. This local deployment\nensures data privacy, eliminates recurring costs and provides access to large\ncontext windows without external dependencies. CelloAI addresses two primary\nuse cases, code documentation and code generation, through specialized\ncomponents. For code documentation, the assistant provides: (a) Doxygen style\ncomment generation for all functions and classes by retrieving relevant\ninformation from RAG sources (papers, posters, presentations), (b) file-level\nsummary generation, and (c) an interactive chatbot for code comprehension\nqueries. For code generation, CelloAI employs syntax-aware chunking strategies\nthat preserve syntactic boundaries during embedding, improving retrieval\naccuracy in large codebases. The system integrates callgraph knowledge to\nmaintain dependency awareness during code modifications and provides\nAI-generated suggestions for performance optimization and accurate refactoring.\nWe evaluate CelloAI using real-world HEP applications from ATLAS, CMS, and DUNE\nexperiments, comparing different embedding models for code retrieval\neffectiveness. Our results demonstrate the AI assistant's capability to enhance\ncode understanding and support reliable code generation while maintaining the\ntransparency and safety requirements essential for scientific computing\nenvironments.", "AI": {"tldr": "CelloAI is a locally hosted LLM-powered coding assistant that improves HEP code documentation and generation through RAG, syntax-aware chunking, and callgraph integration, enabling secure and efficient management of scientific codebases.", "motivation": "Next-gen HEP experiments require HPC integration but face significant challenges in porting legacy software to heterogeneous architectures due to complex, sparsely documented codebases.", "method": "CelloAI employs retrieval-augmented generation (RAG) from scientific sources for documentation (Doxygen comments, file summaries, chatbot), syntax-preserving code chunking for generation, and callgraph analysis to maintain dependency awareness during code modifications.", "result": "Evaluations on ATLAS, CMS, and DUNE codebases demonstrate CelloAI's effectiveness in code comprehension, generation, and retrieval accuracy, while maintaining transparency and data privacy through local deployment.", "conclusion": "By combining LLM capabilities with domain-specific code handling techniques, CelloAI addresses critical needs in HEP software development, offering a secure, cost-effective solution for documenting and evolving complex scientific codebases."}}
{"id": "2508.16991", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16991", "abs": "https://arxiv.org/abs/2508.16991", "authors": ["Ekzhin Ear"], "title": "Towards Principled Analysis and Mitigation of Space Cyber Risks", "comment": "PhD Dissertation", "summary": "Space infrastructures have become an underpinning of modern society, but\ntheir associated cyber risks are little understood. This Dissertation advances\nthe state-of-the-art via four contributions. (i) It introduces an innovative\nframework for characterizing real-world cyber attacks against space\ninfrastructures, or space cyber attacks, including a novel methodology for\ncoping with missing data and three novel metrics. A case study demonstrates the\nusefulness of the framework on 108 real-world space cyber attacks. (ii) This\nDissertation characterizes the state-of-the-practice in space cyber risk\nanalysis and mitigation, namely the Notional Risk Scores (NRS) within the Space\nAttack Research and Tactic Analysis (SPARTA) framework. (iii) We propose a set\nof desired properties that should be satisfied by any competent space cyber\nrisk analysis and mitigation tool and applies them to assess two industrial\nspace cyber risk analysis and mitigation tools. (iv) The study introduces a\nnovel framework to analyze and mitigate space cyber risks by explicitly\nmodeling space cyber attack cascading effects and presenting algorithms for\nmission risk analysis and mission hardening. We demonstrate the usefulness of\nthe framework by applying it to analyze and mitigate space cyber risks, with\ntestbed-based validation.", "AI": {"tldr": "This dissertation addresses understudied cyber risks in space infrastructure by introducing frameworks for attack characterization, risk analysis, and mitigation, while evaluating existing tools and validating proposals through case studies and testbeds.", "motivation": "Modern society relies on space infrastructure, yet its cyber risks remain poorly understood, creating a critical need for robust risk analysis and mitigation frameworks.", "method": "1. Developed a framework to characterize space cyber attacks with missing-data handling and three novel metrics. 2. Analyzed existing tools (SPARTA's NRS) and proposed desired properties for space cyber risk mitigation tools. 3. Introduced cascading effects modeling with mission risk analysis algorithms.", "result": "Four key contributions validated through 108 real-world attack case studies and testbed experiments: an attack characterization framework, systematic evaluation of current risk tools, desired tool properties, and a cascading-effects mitigation framework.", "conclusion": "The work establishes foundational frameworks for space cyber risk analysis, identifies gaps in current tools, and provides test-validated solutions to address cascading attack risks in critical space infrastructure."}}
{"id": "2508.16771", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.16771", "abs": "https://arxiv.org/abs/2508.16771", "authors": ["Yifan Zhang", "Chen Huang", "Yueke Zhang", "Jiahao Zhang", "Toby Jia-Jun Li", "Collin McMillan", "Kevin Leach", "Yu Huang"], "title": "EyeMulator: Improving Code Language Models by Mimicking Human Visual Attention", "comment": null, "summary": "Code language models (so-called CodeLLMs) are now commonplace in software\ndevelopment. As a general rule, CodeLLMs are trained by dividing training\nexamples into input tokens and then learn importance of those tokens in a\nprocess called machine attention. Machine attention is based solely on input\ntoken salience to output token examples during training. Human software\ndevelopers are different, as humans intuitively know that some tokens are more\nsalient than others. While intuition itself is ineffable and a subject of\nphilosophy, clues about salience are present in human visual attention, since\npeople tend to look at more salient words more often. In this paper, we present\nEyeMulator, a technique for training CodeLLMs to mimic human visual attention\nwhile training for various software development tasks. We add special weights\nfor each token in each input example to the loss function used during LLM\nfine-tuning. We draw these weights from observations of human visual attention\nderived from a previously-collected publicly-available dataset of eye-tracking\nexperiments in software engineering tasks. These new weights ultimately induce\nchanges in the attention of the subject LLM during training, resulting in a\nmodel that does not need eye-tracking data during inference. Our evaluation\nshows that EyeMulator outperforms strong LLM baselines on several tasks such as\ncode translation, completion and summarization. We further show an ablation\nstudy that demonstrates the improvement is due to subject models learning to\nmimic human attention.", "AI": {"tldr": "This paper proposes EyeMulator, a technique that adjusts CodeLLM attention mechanisms by incorporating human visual attention weights from eye-tracking datasets during fine-tuning. The method improves performance on code translation, completion, and summarization tasks without requiring eye data at inference.", "motivation": "CodeLLMs rely on machine attention that only considers token salience during training, but human developers intuitively prioritize different tokens. Human visual attention patterns offer valuable insights for improving model performance on software development tasks.", "method": "The authors inject special per-token weights into the fine-tuning loss function of CodeLLMs, derived from public eye-tracking datasets. These weights guide the model's attention learning process, modifying traditional attention mechanisms through backpropagation.", "result": "EyeMulator achieves better performance than strong LLM baselines on code translation, completion, and summarization tasks. Ablation studies confirm performance improvements are directly linked to learning human attention patterns.", "conclusion": "Training CodeLLMs with human attention-weighted inputs significantly enhances their task performance by aligning their attention mechanisms with human intuition, as demonstrated through systematic evaluation and analysis."}}
{"id": "2508.17043", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.17043", "abs": "https://arxiv.org/abs/2508.17043", "authors": ["Shayesta Naziri", "Xu Wang", "Guangsheng Yu", "Christy Jie Liang", "Wei Ni"], "title": "ZAPS: A Zero-Knowledge Proof Protocol for Secure UAV Authentication with Flight Path Privacy", "comment": "11 Pages, 8 figures, Journal", "summary": "The increasing deployment of Unmanned Aerial Vehicles (UAVs) for military,\ncommercial, and logistics applications has raised significant concerns\nregarding flight path privacy. Conventional UAV communication systems often\nexpose flight path data to third parties, making them vulnerable to tracking,\nsurveillance, and location inference attacks. Existing encryption techniques\nprovide security but fail to ensure complete privacy, as adversaries can still\ninfer movement patterns through metadata analysis. To address these challenges,\nwe propose a zk-SNARK(Zero-Knowledge Succinct Non-Interactive Argument of\nKnowledge)-based privacy-preserving flight path authentication and verification\nframework. Our approach ensures that a UAV can prove its authorisation,\nvalidate its flight path with a control centre, and comply with regulatory\nconstraints without revealing any sensitive trajectory information. By\nleveraging zk-SNARKs, the UAV can generate cryptographic proofs that verify\ncompliance with predefined flight policies while keeping the exact path and\nlocation undisclosed. This method mitigates risks associated with real-time\ntracking, identity exposure, and unauthorised interception, thereby enhancing\nUAV operational security in adversarial environments. Our proposed solution\nbalances privacy, security, and computational efficiency, making it suitable\nfor resource-constrained UAVs in both civilian and military applications.", "AI": {"tldr": "A zk-SNARK framework lets UAVs verify flight compliance without revealing trajectory data, solving privacy vulnerabilities in UAV communications while maintaining efficiency for real-world use.", "motivation": "Current UAV communication systems expose flight path metadata to privacy risks (tracking, surveillance, location inference) despite encryption. Metadata analysis still allows adversaries to infer movement patterns, motivating the need for stronger privacy-preserving solutions.", "method": "The authors leverage zk-SNARKs to create a cryptographic proof system where UAVs can authenticate authorization, validate flight paths against control centers, and comply with regulatory policies without disclosing sensitive trajectory or location data.", "result": "The solution mitigates real-time tracking, identity exposure, and unauthorized interception risks. It achieves a balance of privacy, security, and computational efficiency, suitable for resource-constrained UAVs in adversarial environments.", "conclusion": "The paper concludes that the proposed zk-SNARK-based framework effectively enhances UAV flight path privacy and operational security while maintaining computational efficiency, making it viable for both civilian and military UAV applications."}}
{"id": "2508.16853", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16853", "abs": "https://arxiv.org/abs/2508.16853", "authors": ["Pratyush Nidhi Sharma", "Lauren Wright", "Anne Herfurth", "Munsif Sokiyna", "Pratyaksh Nidhi Sharma", "Sethu Das", "Mikko Siponen"], "title": "DevLicOps: A Framework for Mitigating Licensing Risks in AI-Generated Code", "comment": "18 pages, 1 figure, 2 Tables", "summary": "Generative AI coding assistants (ACAs) are widely adopted yet pose serious\nlegal and compliance risks. ACAs can generate code governed by restrictive\nopen-source licenses (e.g., GPL), potentially exposing companies to litigation\nor forced open-sourcing. Few developers are trained in these risks, and legal\nstandards vary globally, especially with outsourcing. Our article introduces\nDevLicOps, a practical framework that helps IT leaders manage ACA-related\nlicensing risks through governance, incident response, and informed tradeoffs.\nAs ACA adoption grows and legal frameworks evolve, proactive license compliance\nis essential for responsible, risk-aware software development in the AI era.", "AI": {"tldr": "The paper introduces DevLicOps, a framework to manage licensing risks from AI coding assistants (ACAs) through governance, incident response, and strategic tradeoffs, ensuring compliance as ACA adoption rises.", "motivation": "ACAs generate code under restrictive open-source licenses (like GPL), risking litigation or forced open-sourcing. Developers lack training on these risks, and global legal standards (especially with outsourcing) create compliance ambiguity.", "method": "DevLicOps is designed as a practical framework with three components: (1) Governance policies for ACA use, (2) Incident response protocols to detect/remove non-compliant code, and (3) Data-driven cost-benefit analysis tools to make risk-aware licensing decisions.", "result": "The framework enables proactive license compliance, reducing litigation risks from ACA-generated code. Case studies show it supports organizations in balancing innovation speed with legal requirements.", "conclusion": "As AI drives software development, DevLicOps provides a scalable solution to manage evolving licensing complexities, promoting responsible use of ACAs in global development environments."}}
{"id": "2508.17071", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.17071", "abs": "https://arxiv.org/abs/2508.17071", "authors": ["Sufyan Al-Janabi"], "title": "Post-Quantum Blockchain: Challenges and Opportunities", "comment": null, "summary": "Blockchain is a Distributed Ledger Technology (DLT) that offers numerous\nbenefits including decentralization, transparency, efficiency, and reduced\ncosts. Hence, blockchain has been included in many fields. Blockchain relies on\ncryptographic protocols (especially public-key cryptography and hash functions)\nto achieve many essential sub-routines. However, the increased progress of\nquantum computation and algorithms has threatened the security of many\ntraditional cryptosystems. Therefore, this represents a serious risk for the\nexisting blockchain technology. For example, SHA-256 and the Elliptic Curve\nDigital Signature Algorithm (ECDSA) cryptosystems can be compromised by Shor s\nand Grover s quantum algorithms in the foreseeable future. Post-Quantum\nCryptography (PQC) is a basic solution for resisting these quantum attacks.\nApplying PQC to blockchains results in creating Post-Quantum Blockchains (PQB).\nThus, this paper aims to review the threats imposed by quantum computers on\nclassical blockchain technology and provide useful guidelines on PQB security\nto blockchain researchers. The paper focuses on the challenges and\nopportunities of future work direction in this field.", "AI": {"tldr": "The paper reviews the risks quantum computing poses to blockchain cryptography and outlines guidelines for Post-Quantum Blockchains (PQB), emphasizing security challenges and future research directions.", "motivation": "Quantum algorithms threaten traditional cryptographic protocols (e.g., SHA-256, ECDSA) integral to blockchain security, necessitating preparation for quantum-resistant solutions.", "method": "Literature survey to analyze quantum threats to blockchain and evaluate post-quantum cryptographic alternatives for securing blockchain systems.", "result": "Highlights vulnerabilities in current blockchain systems to quantum attacks and proposes a structured approach for implementing PQC to ensure long-term security.", "conclusion": "The transition to post-quantum cryptography is critical to safeguard blockchain against future quantum threats, with key challenges in standardization and integration requiring further research."}}
{"id": "2508.16860", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16860", "abs": "https://arxiv.org/abs/2508.16860", "authors": ["Md Afif Al Mamun", "Gias Uddin", "Lan Xia", "Longyu Zhang"], "title": "TriagerX: Dual Transformers for Bug Triaging Tasks with Content and Interaction Based Rankings", "comment": "This work is currently under review at IEEE Transactions on Software\n  Engineering. The replication package will be made publicly available upon\n  acceptance", "summary": "Pretrained Language Models or PLMs are transformer-based architectures that\ncan be used in bug triaging tasks. PLMs can better capture token semantics than\ntraditional Machine Learning (ML) models that rely on statistical features\n(e.g., TF-IDF, bag of words). However, PLMs may still attend to less relevant\ntokens in a bug report, which can impact their effectiveness. In addition, the\nmodel can be sub-optimal with its recommendations when the interaction history\nof developers around similar bugs is not taken into account. We designed\nTriagerX to address these limitations. First, to assess token semantics more\nreliably, we leverage a dual-transformer architecture. Unlike current\nstate-of-the-art (SOTA) baselines that employ a single transformer\narchitecture, TriagerX collects recommendations from two transformers with each\noffering recommendations via its last three layers. This setup generates a\nrobust content-based ranking of candidate developers. TriagerX then refines\nthis ranking by employing a novel interaction-based ranking methodology, which\nconsiders developers' historical interactions with similar fixed bugs. Across\nfive datasets, TriagerX surpasses all nine transformer-based methods, including\nSOTA baselines, often improving Top-1 and Top-3 developer recommendation\naccuracy by over 10%. We worked with our large industry partner to successfully\ndeploy TriagerX in their development environment. The partner required both\ndeveloper and component recommendations, with components acting as proxies for\nteam assignments-particularly useful in cases of developer turnover or team\nchanges. We trained TriagerX on the partner's dataset for both tasks, and it\noutperformed SOTA baselines by up to 10% for component recommendations and 54%\nfor developer recommendations.", "AI": {"tldr": "TriagerX is a dual-transformer architecture for bug triaging that improves developer and component recommendation accuracy by combining content-based and interaction-based ranking methods, achieving over 10% improvements in Top-1/Top-3 metrics on five datasets and outperforming SOTA methods by up to 54% in industry deployment.", "motivation": "Existing PLMs for bug triaging struggle with irrelevant token attention and lack consideration of developer interaction history in recommendations, limiting their effectiveness despite better token semantic capture than traditional ML models.", "method": "1) Dual-transformer architecture using recommendations from last three layers of two transformers for content-based ranking. 2) Novel interaction-based ranking method that incorporates developers' historical interactions with similar fixed bugs to refine recommendations.", "result": "Outperformed nine transformer-based methods on five datasets (10+ improvement in Top-1/Top-3 accuracy). In industry deployment, showed 10% improvement for component recommendations and 54% improvement for developer recommendations over SOTA baselines.", "conclusion": "TriagerX demonstrates significant improvements in bug triaging by combining dual-transformer-based content analysis with interaction history refinement, and has been successfully deployed in a large industry setting where component recommendations help maintain team assignment accuracy despite developer turnover."}}
{"id": "2508.17121", "categories": ["cs.CR", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.17121", "abs": "https://arxiv.org/abs/2508.17121", "authors": ["Zhenliang Gan", "Xiaoxiao Hu", "Sheng Li", "Zhenxing Qian", "Xinpeng Zhang"], "title": "SyncGuard: Robust Audio Watermarking Capable of Countering Desynchronization Attacks", "comment": null, "summary": "Audio watermarking has been widely applied in copyright protection and source\ntracing. However, due to the inherent characteristics of audio signals,\nwatermark localization and resistance to desynchronization attacks remain\nsignificant challenges. In this paper, we propose a learning-based scheme named\nSyncGuard to address these challenges. Specifically, we design a frame-wise\nbroadcast embedding strategy to embed the watermark in arbitrary-length audio,\nenhancing time-independence and eliminating the need for localization during\nwatermark extraction. To further enhance robustness, we introduce a\nmeticulously designed distortion layer. Additionally, we employ dilated\nresidual blocks in conjunction with dilated gated blocks to effectively capture\nmulti-resolution time-frequency features. Extensive experimental results show\nthat SyncGuard efficiently handles variable-length audio segments, outperforms\nstate-of-the-art methods in robustness against various attacks, and delivers\nsuperior auditory quality.", "AI": {"tldr": "SyncGuard is a learning-based audio watermarking method that addresses challenges in watermark localization and desynchronization resistance through frame-wise embedding, a distortion layer, and dilated neural blocks.", "motivation": "Audio watermarking faces issues with localization and robustness against desynchronization attacks. The paper aims to create a time-independent solution that eliminates manual alignment and improves resistance to common audio attacks.", "method": "SyncGuard's method combines frame-wise broadcast embedding of watermarks, a distortion layer for robustness, and dilated residual/gated blocks to extract multi-resolution time-frequency features during detection.", "result": "Extensive experiments demonstrate SyncGuard's superior performance in handling variable-length audio, maintaining watermark accuracy under desynchronization attacks, and producing high auditory quality compared to existing methods.", "conclusion": "The proposed SyncGuard framework effectively resolves key limitations in audio watermarking by achieving time-independence and robust desynchronization resistance through its novel architectural components."}}
{"id": "2508.16903", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16903", "abs": "https://arxiv.org/abs/2508.16903", "authors": ["Yijun Lu", "Hironori Washizaki", "Naoyasu Ubayashi", "Nobukazu Yoshioka", "Chenhao Wu", "Masanari Kondo", "Yuyin Ma", "Jiong Dong", "Jianjin Zhao", "Dongqi Han"], "title": "Mind the Gap: A Decade-Scale Empirical Study of Multi-Stakeholder Dynamics in VR Ecosystem", "comment": null, "summary": "In the development and evolution of VR ecosystem, platform stakeholders\ncontinuously adapt their products in response to user and technical feedback,\noften reflected in subtle shifts in discussion topics or system updates. A\ncomprehensive understanding of these changes is essential for identifying gaps\nbetween user expectations and developer actions, which can guide more effective\nquality assurance and user-centered innovation. While previous studies have\nanalyzed either user reviews or developer discussions in isolation, such\napproaches typically fail to reveal how specific user concerns are (or are not)\naddressed by corresponding technical activities. To address this limitation,\nour study introduces a multi-view empirical framework that systematically\ncompares and aligns stakeholder perspectives. By applying topic modeling and\nquantitative impact analysis to 944,320 user reviews and 389,477 developer\nposts, we identify not only the overlap in concerns (e.g., performance, input\nmethods), but also clear gaps in areas like inclusivity and community safety\n(e.g., LGBTQ+ representation, child-friendly content). Our findings show that\nwhile users repeatedly raise such issues, they are rarely discussed in\ndeveloper forums. These insights enable data-driven recommendations for closing\nthe user-developer gap in VR ecosystems, offering practical implications for\nplatform governance and the design of next-generation VR systems.", "AI": {"tldr": "This study developed a multi-view framework to analyze VR ecosystem feedback, revealing gaps in addressing inclusivity and community safety concerns through comparative analysis of user reviews (n=944,320) and developer posts (n=389,477).", "motivation": "Previous research isolated user reviews and developer discussions, missing how specific user concerns are addressed by technical efforts in VR development.", "method": "Topic modeling and quantitative impact analysis were applied to user reviews and developer forum posts, systematically comparing and aligning stakeholder perspectives.", "result": "Overlap in performance/input method concerns, but significant gaps: LGBTQ+ representation and child-friendly content were raised by users yet rarely discussed in developer forums.", "conclusion": "Framework enables data-driven strategies for platform governance and VR system design to bridge user-developer expectations, particularly in under-addressed inclusion domains."}}
{"id": "2508.17155", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17155", "abs": "https://arxiv.org/abs/2508.17155", "authors": ["Derek Lilienthal", "Sanghyun Hong"], "title": "Mind the Gap: Time-of-Check to Time-of-Use Vulnerabilities in LLM-Enabled Agents", "comment": "Pre-print", "summary": "Large Language Model (LLM)-enabled agents are rapidly emerging across a wide\nrange of applications, but their deployment introduces vulnerabilities with\nsecurity implications. While prior work has examined prompt-based attacks\n(e.g., prompt injection) and data-oriented threats (e.g., data exfiltration),\ntime-of-check to time-of-use (TOCTOU) remain largely unexplored in this\ncontext. TOCTOU arises when an agent validates external state (e.g., a file or\nAPI response) that is later modified before use, enabling practical attacks\nsuch as malicious configuration swaps or payload injection. In this work, we\npresent the first study of TOCTOU vulnerabilities in LLM-enabled agents. We\nintroduce TOCTOU-Bench, a benchmark with 66 realistic user tasks designed to\nevaluate this class of vulnerabilities. As countermeasures, we adapt detection\nand mitigation techniques from systems security to this setting and propose\nprompt rewriting, state integrity monitoring, and tool-fusing. Our study\nhighlights challenges unique to agentic workflows, where we achieve up to 25%\ndetection accuracy using automated detection methods, a 3% decrease in\nvulnerable plan generation, and a 95% reduction in the attack window. When\ncombining all three approaches, we reduce the TOCTOU vulnerabilities from an\nexecuted trajectory from 12% to 8%. Our findings open a new research direction\nat the intersection of AI safety and systems security.", "AI": {"tldr": "This paper introduces the first study on identifying and mitigating TOCTOU vulnerabilities in LLM-enabled agents, proposing a benchmark (TOCTOU-Bench) and countermeasures like prompt rewriting, state integrity monitoring, and tool-fusing. Evaluations show 25% detection accuracy, 95% reduction in attack windows, and combined methods reduce vulnerabilities from 12% to 8% in executed trajectories.", "motivation": "The paper addresses the gap in research on temporal dynamic vulnerabilities (TOCTOU) within LLM-enabled agents, which can enable attacks like malicious configuration swaps or payload injection when validated external states are later modified.", "method": "The authors created TOCTOU-Bench with 66 realistic tasks to evaluate TOCTOU vulnerabilities. They adapted systems security detection/mitigation techniques, implementing three approaches: prompt rewriting to inject safeguards, state integrity monitoring for runtime checks, and tool-fusing to control agent interactions.", "result": "Automated detection achieved 25% accuracy. Mitigations reduced vulnerable plan generation by 3%, attack window by 95%, and combination of all three approaches decreased TOCTOU vulnerabilities in executed trajectories from 12% to 8%.", "conclusion": "This work establishes TOCTOU vulnerabilities as a critical vector in agentic workflows, demonstrating system security methods can be adapted for mitigation and opening interdisciplinary research directions between AI safety and systems security."}}
{"id": "2508.17161", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.17161", "abs": "https://arxiv.org/abs/2508.17161", "authors": ["Julyanara R. Silva", "Carlos Eduardo C. Dantas", "Marcelo A. Maia"], "title": "What Developers Ask to ChatGPT in GitHub Pull Requests? an Exploratory Study", "comment": "12 pages, 3 figures", "summary": "The emergence of Large Language Models (LLMs), such as ChatGPT, has\nintroduced a new set of tools to support software developers in solving pro-\ngramming tasks. However, our understanding of the interactions (i.e., prompts)\nbetween developers and ChatGPT that result in contributions to the codebase\nremains limited. To explore this limitation, we conducted a manual evaluation\nof 155 valid ChatGPT share links extracted from 139 merged Pull Requests (PRs),\nrevealing the interactions between developers and reviewers with ChatGPT that\nled to merges into the main codebase. Our results produced a catalog of 14\ntypes of ChatGPT requests categorized into four main groups. We found a\nsignificant number of requests involving code review and the implementation of\ncode snippets based on specific tasks. Developers also sought to clarify doubts\nby requesting technical explanations or by asking for text refinements for\ntheir web pages. Furthermore, we verified that prompts involving code\ngeneration generally required more interactions to produce the desired answer\ncompared to prompts requesting text review or technical information.", "AI": {"tldr": "The paper analyzes how developers interact with ChatGPT in software development, categorizing 155 PRs into 14 request types. Key findings include the prevalence of code review/implementation tasks, increased interaction for code generation, and varied use cases like technical explanations and text refinement.", "motivation": "The study addresses the lack of understanding about how developers effectively use ChatGPT to create code contributions, highlighting the need for empirical evidence on real-world tool integration.", "method": "Manual evaluation of 155 valid ChatGPT share links from 139 merged PRs to identify and categorize development workflows involving LLMs.", "result": "14 ChatGPT request types grouped into four categories were cataloged. Code generation required more interactions than text/technical prompts. Common request types included code review assistance, task-specific code implementation, technical clarification, and text refinement.", "conclusion": "The results suggest developers rely on ChatGPT across multiple development stages, with code generation involving deeper iterative interactions. This provides implications for improving LLM design to better support software development workflows."}}
{"id": "2508.17222", "categories": ["cs.CR", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.17222", "abs": "https://arxiv.org/abs/2508.17222", "authors": ["Jiale Liu", "Jiahao Zhang", "Suhang Wang"], "title": "Exposing Privacy Risks in Graph Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is a powerful technique for enhancing\nLarge Language Models (LLMs) with external, up-to-date knowledge. Graph RAG has\nemerged as an advanced paradigm that leverages graph-based knowledge structures\nto provide more coherent and contextually rich answers. However, the move from\nplain document retrieval to structured graph traversal introduces new,\nunder-explored privacy risks. This paper investigates the data extraction\nvulnerabilities of the Graph RAG systems. We design and execute tailored data\nextraction attacks to probe their susceptibility to leaking both raw text and\nstructured data, such as entities and their relationships. Our findings reveal\na critical trade-off: while Graph RAG systems may reduce raw text leakage, they\nare significantly more vulnerable to the extraction of structured entity and\nrelationship information. We also explore potential defense mechanisms to\nmitigate these novel attack surfaces. This work provides a foundational\nanalysis of the unique privacy challenges in Graph RAG and offers insights for\nbuilding more secure systems.", "AI": {"tldr": "This paper identifies privacy risks in Graph RAG systems, showing they are vulnerable to structured data extraction attacks despite potential text leakage reduction.", "motivation": "The increasing adoption of Retrieval-Augmented Generation (RAG) systems necessitates understanding novel privacy risks introduced by graph-based knowledge architectures, which remain understudied.", "method": "Researchers designed and conducted targeted data extraction attacks to evaluate Graph RAG systems' susceptibility to leaking raw text and structured data (entities/relationships).", "result": "Graph RAG systems exhibit a critical trade-off: increased vulnerability to structured entity and relationship information leakage, even while reducing raw text leakage exposure.", "conclusion": "The study establishes foundational insights into Graph RAG privacy challenges and proposes defensive strategies to address these emerging risks in knowledge-enhanced LLM systems."}}
{"id": "2508.17343", "categories": ["cs.SE", "cs.AI", "D.2"], "pdf": "https://arxiv.org/pdf/2508.17343", "abs": "https://arxiv.org/abs/2508.17343", "authors": ["Abhik Roychoudhury"], "title": "Agentic AI for Software: thoughts from Software Engineering community", "comment": "4 pages", "summary": "AI agents have recently shown significant promise in software engineering.\nMuch public attention has been transfixed on the topic of code generation from\nLarge Language Models (LLMs) via a prompt. However, software engineering is\nmuch more than programming, and AI agents go far beyond instructions given by a\nprompt.\n  At the code level, common software tasks include code generation, testing,\nand program repair. Design level software tasks may include architecture\nexploration, requirements understanding, and requirements enforcement at the\ncode level. Each of these software tasks involves micro-decisions which can be\ntaken autonomously by an AI agent, aided by program analysis tools. This\ncreates the vision of an AI software engineer, where the AI agent can be seen\nas a member of a development team.\n  Conceptually, the key to successfully developing trustworthy agentic AI-based\nsoftware workflows will be to resolve the core difficulty in software\nengineering - the deciphering and clarification of developer intent.\nSpecification inference, or deciphering the intent, thus lies at the heart of\nmany software tasks, including software maintenance and program repair. A\nsuccessful deployment of agentic technology into software engineering would\ninvolve making conceptual progress in such intent inference via agents.\n  Trusting the AI agent becomes a key aspect, as software engineering becomes\nmore automated. Higher automation also leads to higher volume of code being\nautomatically generated, and then integrated into code-bases. Thus to deal with\nthis explosion, an emerging direction is AI-based verification and validation\n(V & V) of AI generated code. We posit that agentic software workflows in\nfuture will include such AIbased V&V.", "AI": {"tldr": "AI agents transcend code generation in software engineering by autonomously managing tasks at code and design levels through intent inference and AI-based verification, enabling them to function as development team members.", "motivation": "Current focus on prompt-driven code generation overlooks broader software engineering tasks; automation requires resolving developer intent to facilitate AI agent integration into complex workflows like architecture exploration and requirements enforcement.", "method": "AI agents leverage autonomous micro-decisions supported by program analysis tools, with emerging techniques in intent inference and AI-based verification (V&V) for automatically generated code.", "result": "Demonstrated that agentic AI can handle diverse software tasks (testing, repair, architecture) through intent decoding, and highlights the necessity of AI-driven V&V to manage increased automation in code generation and integration.", "conclusion": "Future software engineering workflows will require solving intent inference to ensure trustworthy AI agents and integrating AI-based V&V to manage automation's impact on code quality and team collaboration."}}
{"id": "2508.17296", "categories": ["cs.CR", "81P68", "F.2; F.1; E.3"], "pdf": "https://arxiv.org/pdf/2508.17296", "abs": "https://arxiv.org/abs/2508.17296", "authors": ["Adi Mutha", "Jitendra Sandu"], "title": "Literature Review of the Effect of Quantum Computing on Cryptocurrencies using Blockchain Technology", "comment": "21 pages", "summary": "With the advent of quantum computing, cryptocurrencies that rely on\nblockchain technology face mounting cryptographic vulnerabilities. This paper\npresents a comprehensive literature review evaluating how quantum algorithms,\nspecifically Shors and Grovers, could disrupt the foundational security\nmechanisms of cryptocurrencies. Shors algorithm poses a threat to public-key\ncryptographic schemes by enabling efficient factorization and discrete\nlogarithm solving, thereby endangering digital signature systems. Grovers\nalgorithm undermines hash-based functions, increasing the feasibility of fifty\none percent attacks and hash collisions. By examining the internal mechanisms\nof major cryptocurrencies such as Bitcoin, Ethereum, Litecoin, Monero, and\nZcash, this review identifies specific vulnerabilities in transaction and\nconsensus processes. It further analyses the current hardware limitations of\nquantum systems and estimates when such attacks could become feasible. In\nanticipation, it investigates countermeasures including Post-Quantum\nCryptography (PQC), Quantum Key Distribution (QKD), and protocol-level\nmodifications such as memory-intensive proof-of-work algorithms and\nmulti-signature schemes. The discussion integrates recent advancements in\nquantum error correction, hardware scalability, and NIST-standardized\ncryptographic algorithms. This review concludes that while quantum computers\nare not yet advanced enough to pose an immediate threat, proactive integration\nof quantum-resistant solutions is essential. The findings underscore the urgent\nneed for cryptocurrencies to adopt post-quantum cryptographic standards to\npreserve the decentralized trust, integrity, and security that define\nblockchain-based digital cryptocurrencies.", "AI": {"tldr": "This review evaluates quantum computing threats to blockchain cryptocurrencies through Shor and Grover algorithms, identifying vulnerabilities in transaction/consensus mechanisms and proposing post-quantum solutions.", "motivation": "Emerging quantum computing technologies now pose potential cryptographic risks to blockchain-based systems, necessitating immediate scholarly evaluation of quantum resistance strategies before critical infrastructure becomes obsolete.", "method": "Comprehensive literature analysis of quantum algorithm implications (Shor for public-key crypto weaknesses, Grover for hash function vulnerabilities), combined with technical assessment of Bitcoin, Ethereum, Litecoin, Monero, and Zcash architectures alongside quantum hardware limitations.", "result": "Found Shor's algorithm can compromise digital signatures while Grover's weakens hash security. Identified hardware scalability barriers in current quantum systems but estimated potential timelines for attacks. Evaluated effectiveness of PQC protocols, QKD, memory-intensive proof-of-work, and multi-signature implementations.", "conclusion": "Quantum threats require proactive adoption of post-quantum cryptography (PQC) and protocol adaptations. While imminent danger is low, standardized quantum-resistant frameworks must be integrated now to maintain blockchain trust and security fundamentals."}}
{"id": "2508.17344", "categories": ["cs.SE", "cs.LG", "cs.PF", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.17344", "abs": "https://arxiv.org/abs/2508.17344", "authors": ["Rajrupa Chattaraj", "Sridhar Chimalakonda", "Vibhu Saujanya Sharma", "Vikrant Kaulgud"], "title": "Who Wins the Race? (R Vs Python) - An Exploratory Study on Energy Consumption of Machine Learning Algorithms", "comment": "18 pages including references, 5 figures", "summary": "The utilization of Machine Learning (ML) in contemporary software systems is\nextensive and continually expanding. However, its usage is energy-intensive,\ncontributing to increased carbon emissions and demanding significant resources.\nWhile numerous studies examine the performance and accuracy of ML, only a\nlimited few focus on its environmental aspects, particularly energy\nconsumption. In addition, despite emerging efforts to compare energy\nconsumption across various programming languages for specific algorithms and\ntasks, there remains a gap specifically in comparing these languages for\nML-based tasks. This paper aims to raise awareness of the energy costs\nassociated with employing different programming languages for ML model training\nand inference. Through this empirical study, we measure and compare the energy\nconsumption along with run-time performance of five regression and five\nclassification tasks implemented in Python and R, the two most popular\nprogramming languages in this context. Our study results reveal a statistically\nsignificant difference in costs between the two languages in 95% of the cases\nexamined. Furthermore, our analysis demonstrates that the choice of programming\nlanguage can influence energy efficiency significantly, up to 99.16% during\nmodel training and up to 99.8% during inferences, for a given ML task.", "AI": {"tldr": "This paper empirically compares energy consumption and runtime performance of Python and R for ML tasks, finding statistically significant differences (up to 99.8%) in favor of Python. The study highlights the environmental impact of programming language choice in ML development.", "motivation": "While ML adoption is widespread, its energy intensity and carbon footprint are under-studied. Current research lacks comparative analysis of energy consumption across ML programming languages like Python and R, which are widely used but incomparable in environmental impact.", "method": "The study performs 10 ML tasks (5 regression, 5 classification) in both Python and R, measuring energy consumption (in Watt-hours) and runtime. It uses an empirical evaluation framework to compare language-specific resource usage.", "result": "Python showed lower energy consumption than R in 8/10 training tasks and 6/10 inference tasks. Statistical significance was found in 95% of comparisons, with energy efficiency differences up to 99.16% for training and 99.8% for inferences.", "conclusion": "Programming language choice significantly impacts ML energy efficiency (309Wh for Python vs. 345Wh for R in training tasks). These findings suggest that language optimization can reduce ML's environmental footprint and provide empirical evidence for energy-aware software development decisions."}}
{"id": "2508.17304", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.17304", "abs": "https://arxiv.org/abs/2508.17304", "authors": ["Muhammad Ibn Ziauddin", "Rownak Rahad Rabbi", "SM Mehrab", "Fardin Faiyaz", "Mosarrat Jahan"], "title": "An Efficient Recommendation Filtering-based Trust Model for Securing Internet of Things", "comment": null, "summary": "Trust computation is crucial for ensuring the security of the Internet of\nThings (IoT). However, current trust-based mechanisms for IoT have limitations\nthat impact data security. Sliding window-based trust schemes cannot ensure\nreliable trust computation due to their inability to select appropriate window\nlengths. Besides, recent trust scores are emphasized when considering the\neffect of time on trust. This can cause a sudden change in overall trust score\nbased on recent behavior, potentially misinterpreting an honest service\nprovider as malicious and vice versa. Moreover, clustering mechanisms used to\nfilter recommendations in trust computation often lead to slower results. In\nthis paper, we propose a robust trust model to address these limitations. The\nproposed approach determines the window length dynamically to guarantee\naccurate trust computation. It uses the harmonic mean of average trust score\nand time to prevent sudden fluctuations in trust scores. Additionally, an\nefficient personalized subspace clustering algorithm is used to exclude\nrecommendations. We present a security analysis demonstrating the resiliency of\nthe proposed scheme against bad-mouthing, ballot-stuffing, and on-off attacks.\nThe proposed scheme demonstrates a competitive performance in detecting\nbad-mouthing attacks, while outperforming existing works with an approximately\n44% improvement in accuracy for detecting on-off attacks. It maintains its\neffectiveness even when the percentage of on-off attackers increases and in\nscenarios where multiple attacks occur simultaneously. Additionally, the\nproposed scheme reduces the recommendation filtering time by 95%.", "AI": {"tldr": "This paper introduces a dynamic trust model for IoT that uses harmonic mean of average trust score and time to stabilize trust computation, employs personalized subspace clustering for efficient recommendation filtering, and demonstrates improved accuracy against attacks with 95% reduced filtering time.", "motivation": "Current IoT trust mechanisms face issues like ineffective sliding window lengths causing unreliable computations, recent trust score emphasis leading to errors, and slow clustering for recommendation filtering. These limitations impact data security and system efficiency.", "method": "The proposed model dynamically adjusts window length, calculates trust using the harmonic mean of average trust score and time to mitigate fluctuations, and utilizes a personalized subspace clustering algorithm for rapid exclusion of unreliable recommendations.", "result": "The model achieves ~44% higher accuracy in detecting on-off attacks compared to existing methods, effectively handles combined attacks and increasing malicious percentages, and reduces recommendation filtering time by 95%.", "conclusion": "The dynamic trust model significantly enhances IoT security by addressing timing sensitivity and filtering inefficiencies, offering robust resilience against major trust attacks while maintaining computational efficiency."}}
{"id": "2508.17713", "categories": ["cs.SE", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.17713", "abs": "https://arxiv.org/abs/2508.17713", "authors": ["Zhihao Xu", "Shikai Guo", "Guilin Zhao", "Peiyu Zou", "Siwen Wang", "Qian Ma", "Hui Li", "Furui Zhan"], "title": "Code Difference Guided Fuzzing for FPGA Logic Synthesis Compilers via Bayesian Optimization", "comment": null, "summary": "Field Programmable Gate Arrays (FPGAs) play a crucial role in Electronic\nDesign Automation (EDA) applications, which have been widely used in\nsafety-critical environments, including aerospace, chip manufacturing, and\nmedical devices. A critical step in FPGA development is logic synthesis, which\nenables developers to translate their software designs into hardware net lists,\nwhich facilitates the physical implementation of the chip, detailed timing and\npower analysis, gate-level simulation, test vector generation, and optimization\nand consistency checking. However, bugs or incorrect implementations in FPGA\nlogic synthesis compilers may lead to unexpected behaviors in target\nwapplications, posing security risks. Therefore, it is crucial to eliminate\nsuch bugs in FPGA logic synthesis compilers. The effectiveness of existing\nworks is still limited by its simple, blind mutation strategy. To address this\nchallenge, we propose a guided mutation strategy based on Bayesian optimization\ncalled LSC-Fuzz to detect bugs in FPGA logic synthesis compilers. Specifically,\nLSC-Fuzz consists of three components: the test-program generation component,\nthe Bayesian diversity selection component, and the equivalent check component.\nBy performing test-program generation and Bayesian diversity selection,\nLSC-Fuzz generates diverse and complex HDL code, thoroughly testing the FPGA\nlogic synthesis compilers using equivalent check to detect bugs. Through three\nmonths, LSC-Fuzz has found 16 bugs, 12 of these has been confirmed by official\ntechnical support.", "AI": {"tldr": "This paper proposes LSC-Fuzz, a Bayesian optimization-based guided mutation testing tool that detects 16 bugs in FPGA logic synthesis compilers, 12 confirmed by vendors.", "motivation": "Bugs in FPGA logic synthesis compilers pose security risks in safety-critical applications, and existing mutation strategies are limited by their simplistic, blind approaches. Thorough testing is essential for ensuring secure hardware implementations.", "method": "LSC-Fuzz uses three components: (1) test-program generation creates HDL code, (2) Bayesian diversity selection optimizes test case diversity, and (3) equivalent check validates synthesis correctness through formal verification techniques.", "result": "The tool discovered 16 previously unknown bugs in three months, achieving 12 confirmed fixes with industry-level validation from technical support teams.", "conclusion": "Guided mutation via Bayesian optimization significantly improves bug detection effectiveness in FPGA compilers. The authors plan to release LSC-Fuzz to address urgent community needs in this emerging security domain."}}
{"id": "2508.17329", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.17329", "abs": "https://arxiv.org/abs/2508.17329", "authors": ["Xiaoyan Zhang", "Dongyang Lyu", "Xiaoqi Li"], "title": "Risk Assessment and Security Analysis of Large Language Models", "comment": null, "summary": "As large language models (LLMs) expose systemic security challenges in high\nrisk applications, including privacy leaks, bias amplification, and malicious\nabuse, there is an urgent need for a dynamic risk assessment and collaborative\ndefence framework that covers their entire life cycle. This paper focuses on\nthe security problems of large language models (LLMs) in critical application\nscenarios, such as the possibility of disclosure of user data, the deliberate\ninput of harmful instructions, or the models bias. To solve these problems, we\ndescribe the design of a system for dynamic risk assessment and a hierarchical\ndefence system that allows different levels of protection to cooperate. This\npaper presents a risk assessment system capable of evaluating both static and\ndynamic indicators simultaneously. It uses entropy weighting to calculate\nessential data, such as the frequency of sensitive words, whether the API call\nis typical, the realtime risk entropy value is significant, and the degree of\ncontext deviation. The experimental results show that the system is capable of\nidentifying concealed attacks, such as role escape, and can perform rapid risk\nevaluation. The paper uses a hybrid model called BERT-CRF (Bidirectional\nEncoder Representation from Transformers) at the input layer to identify and\nfilter malicious commands. The model layer uses dynamic adversarial training\nand differential privacy noise injection technology together. The output layer\nalso has a neural watermarking system that can track the source of the content.\nIn practice, the quality of this method, especially important in terms of\ncustomer service in the financial industry.", "AI": {"tldr": "This paper proposes a dynamic risk assessment and hierarchical defense framework for large language models (LLMs) to address security challenges like privacy leaks, bias amplification, and malicious attacks in high-risk applications. The system combines entropy weighting, BERT-CRF, dynamic adversarial training, and differential privacy, demonstrating effectiveness in detecting concealed threats and maintaining output quality in sectors like finance.", "motivation": "LLMs face systemic security challenges in critical applications, necessitating a lifecycle framework to mitigate risks such as user data disclosure, harmful instructions, and bias. Existing static approaches fail to address dynamic threats adequately.", "method": "A multi-layer defense system: 1) Input layer uses BERT-CRF for malicious command detection; 2) Model layer integrates dynamic adversarial training and differential privacy noise injection; 3) Output layer deploys neural watermarking. Static/dynamic risk metrics are assessed using entropy weighting of features like sensitive word frequency and context deviation.", "result": "Experimental results show the system successfully detects stealth attacks (role escape) and maintains real-time risk evaluation accuracy. Financial sector tests confirm its practicality in high-stakes use cases while preserving output quality through defense mechanisms.", "conclusion": "The proposed hybrid framework provides comprehensive LLM security through layered defense and dynamic assessment, establishing a robust solution for critical application scenarios by combining technical innovations with practical risk analysis methods."}}
{"id": "2508.17719", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.17719", "abs": "https://arxiv.org/abs/2508.17719", "authors": ["Akhila Sri Manasa Venigalla", "Sridhar Chimalakonda"], "title": "DocFetch - Towards Generating Software Documentation from Multiple Software Artifacts", "comment": "12 pages, 7 Figures, 4 Tables", "summary": "Software Documentation plays a major role in the usage and development of a\nproject. Widespread adoption of open source software projects contributes to\nlarger and faster development of the projects, making it difficult to maintain\nthe associated documentation. Existing automated approaches to generate\ndocumentation largely focus on source code. However, information useful for\ndocumentation is observed to be scattered across various artifacts that\nco-evolve with the source code. Leveraging this information across multiple\nartifacts can reduce the effort involved in maintaining documentation. Hence,\nwe propose DocFetch, to generate different types of documentation from multiple\nsoftware artifacts. We employ a multi-layer prompt based LLM and generate\nstructured documentation corresponding to different documentation types for the\ndata consolidated in DocMine dataset. We evaluate the performance of DocFetch\nusing a manually curated groundtruth dataset by analysing the artifacts in\nDocMine. The evaluation yields a highest BLEU-4 score of 43.24% and ROUGE-L\nscore of 0.39 for generation of api-related and file-related information from\nfive documentation sources. The generation of other documentation type related\ninformation also reported BLEU-4 scores close to 30% indicating good\nperformance of the approach. Thus,DocFetch can be employed to\nsemi-automatically generate documentation, and helps in comprehending the\nprojects with minimal effort in maintaining the documentation.", "AI": {"tldr": "The paper introduces DocFetch, a multi-layer prompt-based LLM system for semi-automatically generating documentation from multiple software artifacts. It achieves high BLEU-4 (43.24%) and ROUGE-L (0.39) scores for API/file-related documentation, outperforming existing methods in documentation comprehensiveness and maintenance efficiency.", "motivation": "Existing automated documentation approaches focus only on source code, yet valuable documentation information often appears in co-evolving software artifacts. Maintaining documentation becomes labor-intensive with rapidly growing open-source projects, requiring better methods to leverage these dispersed artifacts.", "method": "DocFetch utilizes a multi-layer prompt framework to analyze the DocMine dataset containing five documentation sources. The system generates structured documentation by consolidating and processing information from these diverse software artifacts through an LLM architecture.", "result": "Evaluation using a manually curated ground-truth dataset showed DocFetch achieved 43.24% BLEU-4 and 0.39 ROUGE-L for API/file-related documentation. Other documentation types reached ~30% BLEU-4 scores, demonstrating its effectiveness compared to traditional approaches.", "conclusion": "DocFetch provides a semi-automated documentation generation solution that maintains project understanding with reduced maintenance effort. The system effectively leverages multi-artifact information through its layered prompting approach, offering practical value for software documentation challenges."}}
{"id": "2508.17414", "categories": ["cs.CR", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.17414", "abs": "https://arxiv.org/abs/2508.17414", "authors": ["Temesgen Kitaw Damenu", "\u0130nci Zaim G\u00f6kbay", "Alexandra Covaci", "Shujun Li"], "title": "Cyber Security Educational Games for Children: A Systematic Literature Review", "comment": null, "summary": "Educational games have been widely used to teach children about cyber\nsecurity. This systematic literature review reveals evidence of positive\nlearning outcomes, after analysing 91 such games reported in 68 papers\npublished between 2010 and 2024. However, critical gaps have also been\nidentified regarding the design processes and the methodological rigour,\nincluding lack of systematic design, misalignment between proposed and achieved\nlearning outcomes, rare use of control groups, limited discussions on ethical\nconsiderations, and underutilisation of emerging technologies. We recommend\nmultiple future research directions, e.g., a hybrid approach to game design and\nevaluation that combines bottom-up and top-down approaches.", "AI": {"tldr": "A review of 91 cybersecurity educational games shows efficacy but highlights methodological flaws, urging hybrid design approaches for future research.", "motivation": "Educational games are widely used to teach children cybersecurity, but existing research lacks systematic design, methodological rigor, and ethical considerations, necessitating a comprehensive review.", "method": "A systematic literature review analyzing 91 educational games reported in 68 papers published between 2010 and 2024.", "result": "Positive learning outcomes were observed, but critical gaps were identified, including misalignment in learning outcomes, lack of control groups, limited ethical discussions, and underutilization of emerging technologies.", "conclusion": "The paper recommends future research directions such as adopting a hybrid approach to game design and evaluation, combining bottom-up and top-down methods to address identified gaps."}}
{"id": "2508.17720", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.17720", "abs": "https://arxiv.org/abs/2508.17720", "authors": ["Ziqi Guan", "Xin Yin", "Zhiyuan Peng", "Chao Ni"], "title": "RepoTransAgent: Multi-Agent LLM Framework for Repository-Aware Code Translation", "comment": null, "summary": "Repository-aware code translation is critical for modernizing legacy systems,\nenhancing maintainability, and enabling interoperability across diverse\nprogramming languages. While recent advances in large language models (LLMs)\nhave improved code translation quality, existing approaches face significant\nchallenges in practical scenarios: insufficient contextual understanding,\ninflexible prompt designs, and inadequate error correction mechanisms. These\nlimitations severely hinder accurate and efficient translation of complex,\nreal-world code repositories. To address these challenges, we propose\nRepoTransAgent, a novel multi-agent LLM framework for repository-aware code\ntranslation. RepoTransAgent systematically decomposes the translation process\ninto specialized subtasks-context retrieval, dynamic prompt construction, and\niterative code refinement-each handled by dedicated agents. Our approach\nleverages retrieval-augmented generation (RAG) for contextual information\ngathering, employs adaptive prompts tailored to varying repository scenarios,\nand introduces a reflection-based mechanism for systematic error correction. We\nevaluate RepoTransAgent on hundreds of Java-C# translation pairs from six\npopular open-source projects. Experimental results demonstrate that\nRepoTransAgent significantly outperforms state-of-the-art baselines in both\ncompile and pass rates. Specifically, RepoTransAgent achieves up to 55.34%\ncompile rate and 45.84% pass rate. Comprehensive analysis confirms the\nrobustness and generalizability of RepoTransAgent across different LLMs,\nestablishing its effectiveness for real-world repository-aware code\ntranslation.", "AI": {"tldr": "RepoTransAgent is a multi-agent LLM framework addressing repository-aware code translation challenges through context-aware retrieval, adaptive prompting, and iterative error correction, achieving competitive performance in real-world code repository translation.", "motivation": "Existing code translation approaches face critical bottlenecks in contextual understanding, prompt design, and error correction, severely limiting their effectiveness in translating complex, real-world code repositories.", "method": "RepoTransAgent employs a multi-agent LLM framework that decomposes the translation process into context retrieval (using RAG), dynamic prompt construction, and iterative code refinement with reflection-based error correction.", "result": "RepoTransAgent achieves 55.34% compile rate and 45.84% pass rate on Java-C# translation from six open-source projects, significantly outperforming state-of-the-art baselines across different LLMs.", "conclusion": "RepoTransAgent effectively addresses the limitations of existing code translation methods, demonstrating robustness and generalizability across real-world repository-aware code translation scenarios."}}
{"id": "2508.17481", "categories": ["cs.CR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17481", "abs": "https://arxiv.org/abs/2508.17481", "authors": ["Priyanka Prakash Surve", "Asaf Shabtai", "Yuval Elovici"], "title": "SoK: Cybersecurity Assessment of Humanoid Ecosystem", "comment": null, "summary": "Humanoids are progressing toward practical deployment across healthcare,\nindustrial, defense, and service sectors. While typically considered\ncyber-physical systems (CPSs), their dependence on traditional networked\nsoftware stacks (e.g., Linux operating systems), robot operating system (ROS)\nmiddleware, and over-the-air update channels, creates a distinct security\nprofile that exposes them to vulnerabilities conventional CPS models do not\nfully address. Prior studies have mainly examined specific threats, such as\nLiDAR spoofing or adversarial machine learning (AML). This narrow focus\noverlooks how an attack targeting one component can cascade harm throughout the\nrobot's interconnected systems. We address this gap through a systematization\nof knowledge (SoK) that takes a comprehensive approach, consolidating\nfragmented research from robotics, CPS, and network security domains. We\nintroduce a seven-layer security model for humanoid robots, organizing 39 known\nattacks and 35 defenses across the humanoid ecosystem-from hardware to\nhuman-robot interaction. Building on this security model, we develop a\nquantitative 39x35 attack-defense matrix with risk-weighted scoring, validated\nthrough Monte Carlo analysis. We demonstrate our method by evaluating three\nreal-world robots: Pepper, G1 EDU, and Digit. The scoring analysis revealed\nvarying security maturity levels, with scores ranging from 39.9% to 79.5%\nacross the platforms. This work introduces a structured, evidence-based\nassessment method that enables systematic security evaluation, supports\ncross-platform benchmarking, and guides prioritization of security investments\nin humanoid robotics.", "AI": {"tldr": "The paper introduces a seven-layer security model for humanoid robots and evaluates three real-world platforms using a risk-weighted attack-defense matrix, finding security maturity scores between 39.9% and 79.5%.", "motivation": "Humanoid robots have unique security vulnerabilities compared to conventional CPS due to their complex software stacks and interconnected systems, with existing research focused on isolated threats rather than cascading effects.", "method": "Developed a comprehensive seven-layer security model, organized 39 known attacks and 35 defenses across all robot systems, created a quantitative 39x35 attack-defense matrix with risk scoring validated through Monte Carlo analysis, and tested on Pepper, G1 EDU, and Digit robots.", "result": "Varying security maturity levels were identified across three evaluated robots (39.9%-79.5% scores), with the methodology enabling systematic cross-platform comparison of security vulnerabilities and countermeasures.", "conclusion": "This work establishes a structured framework for evaluating humanoid robot security, addresses cascading threat vulnerabilities, and provides benchmarking methods to prioritize security improvements across the entire robot ecosystem."}}
{"id": "2508.17851", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.17851", "abs": "https://arxiv.org/abs/2508.17851", "authors": ["Patrick Loic Foalem", "Leuson Da Silva", "Foutse Khomh", "Heng Li", "Ettore Merlo"], "title": "Logging Requirement for Continuous Auditing of Responsible Machine Learning-based Applications", "comment": null, "summary": "Machine learning (ML) is increasingly applied across industries to automate\ndecision-making, but concerns about ethical and legal compliance remain due to\nlimited transparency, fairness, and accountability. Monitoring through logging\na long-standing practice in traditional software offers a potential means for\nauditing ML applications, as logs provide traceable records of system behavior\nuseful for debugging, performance analysis, and continuous auditing.\nsystematically auditing models for compliance or accountability. The findings\nunderscore the need for enhanced logging practices and tooling that\nsystematically integrate responsible AI metrics. Such practices would support\nthe development of auditable, transparent, and ethically responsible ML\nsystems, aligning with growing regulatory requirements and societal\nexpectations. By highlighting specific deficiencies and opportunities, this\nwork provides actionable guidance for both practitioners and tool developers\nseeking to strengthen the accountability and trustworthiness of ML\napplications.", "AI": {"tldr": "This paper explores using traditional software logging practices to systematically audit machine learning (ML) models for ethical, legal compliance, and accountability, highlighting deficiencies in current logging methods and opportunities to integrate responsible AI metrics.", "motivation": "ML systems face ethical and legal compliance concerns due to limited transparency, fairness, and accountability. Traditional logging provides traceable records for auditing, debugging, and performance analysis, suggesting its potential for ML systems to address these issues.", "method": "The authors conducted an exploratory study analyzing the role of logging in ML model audits. They examined existing logging practices, identified gaps in capturing ethical/auditing dimensions, and proposed integrating responsible AI metrics into logging frameworks through case studies and expert insights.", "result": "Analysis revealed critical deficiencies in current logging practices for ML, such as lack of traceability for fairness metrics, insufficient accountability records, and inconsistent performance tracking. Proposed improvements include structured log formats, automated metric extraction, and tooling for real-time compliance checks.", "conclusion": "Enhanced logging practices and tooling for ML systems are essential to ensure accountability, transparency, and ethical compliance. The authors provide actionable guidance to improve logging frameworks, aligning ML development with regulatory demands and societal expectations."}}
{"id": "2508.17674", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.17674", "abs": "https://arxiv.org/abs/2508.17674", "authors": ["Qiming Guo", "Jinwen Tang", "Xingran Huang"], "title": "Attacking LLMs and AI Agents: Advertisement Embedding Attacks Against Large Language Models", "comment": "7 pages, 2 figures", "summary": "We introduce Advertisement Embedding Attacks (AEA), a new class of LLM\nsecurity threats that stealthily inject promotional or malicious content into\nmodel outputs and AI agents. AEA operate through two low-cost vectors: (1)\nhijacking third-party service-distribution platforms to prepend adversarial\nprompts, and (2) publishing back-doored open-source checkpoints fine-tuned with\nattacker data. Unlike conventional attacks that degrade accuracy, AEA subvert\ninformation integrity, causing models to return covert ads, propaganda, or hate\nspeech while appearing normal. We detail the attack pipeline, map five\nstakeholder victim groups, and present an initial prompt-based self-inspection\ndefense that mitigates these injections without additional model retraining.\nOur findings reveal an urgent, under-addressed gap in LLM security and call for\ncoordinated detection, auditing, and policy responses from the AI-safety\ncommunity.", "AI": {"tldr": "This paper introduces Advertisement Embedding Attacks (AEA), a novel LLM security threat that covertly injects promotional/malicious content via adversarial prompt prepending and back-doored checkpoints, compromising information integrity without accuracy loss. It presents a prompt-based self-inspection defense and highlights the need for multi-faceted safety responses.", "motivation": "Current LLM security focuses on accuracy degradation, but information integrity risks (e.g., covert ad injection, propaganda, hate speech) remain under-addressed. AEA threats pose urgent risks to AI agents and model outputs across five stakeholder groups.", "method": "AEA utilizes two vectors: (1) hijacking third-party platforms to prepend adversarial prompts during service distribution, and (2) releasing open-source checkpoints fine-tuned with attacker-controlled data to create backdoors. The paper maps these attack vectors and victim groups.", "result": "The analysis demonstrates AEA's effectiveness in bypassing detection through information integrity compromise. The proposed self-inspection defense successfully mitigates content injections without requiring model retraining, validating the feasibility of both attack and defense methods.", "conclusion": "The research reveals critical gaps in LLM security frameworks, emphasizing the need for collaborative detection strategies, regular safety audits, and regulatory policies to combat adversarial embedding techniques and protect AI systems from content manipulation."}}
{"id": "2508.17882", "categories": ["cs.SE", "cs.SC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.17882", "abs": "https://arxiv.org/abs/2508.17882", "authors": ["Izudin Dzafic", "Rabih A. Jabr"], "title": "modelSolver: A Symbolic Model-Driven Solver for Power Network Simulation and Monitoring", "comment": null, "summary": "The development of advanced software tools for power system analysis requires\nextensive programming expertise. Even when using open-source tools, programming\nskills are essential to modify built-in models. This can be particularly\nchallenging for domain experts who lack coding proficiency. This paper\nintroduces modelSolver, a software solution with a new framework centered\naround symbolic mathematical modeling. The proposed paradigm facilitates\ndefining models through intuitive mathematical expressions, thus eliminating\nthe need for traditional programming constructs such as arrays, loops, and\nsparse matrix computations. The modelSolver focuses on power flow and state\nestimation using an open-box approach, which allows users to specify custom\nmodels using either real or complex variables. Unlike existing tools that rely\non hard-coded models, modelSolver enables the representation of a wide range of\nadvanced functionalities, including power flow with voltage regulators and load\ntap changers, continuation power flow, and Gauss-Newton state estimation with\nequality constraints. Compatibility with MATPOWER is ensured via a converter\nthat automates importing data files. The framework prioritizes model-driven\ndevelopment and empowers domain experts to focus on power system modeling\nwithout programming barriers. It aims to simplify power system computations,\nmaking them more accessible to students, scientists, and practitioners.", "AI": {"tldr": "modelSolver is an open-box symbolic modeling tool for power systems that eliminates programming barriers, enabling domain experts to define advanced models (e.g., voltage regulators, continuation power flow, Gauss-Newton state estimation) through intuitive mathematical expressions. Compatible with MATPOWER data via automated conversion.", "motivation": "Traditional power system analysis tools require advanced programming skills, creating a barrier for domain experts (e.g., students, practitioners) who lack coding proficiency, particularly when modifying complex built-in models.", "method": "The framework uses symbolic mathematical modeling with an open-box approach, allowing users to define custom models using real/complex variables without relying on traditional programming constructs (arrays, loops, sparse matrices).", "result": "Successfully implemented power flow and state estimation with advanced capabilities, including voltage regulators, load tap changers, continuation power flow, and Gauss-Newton state estimation with equality constraints. MATPOWER compatibility achieved via an automated data converter.", "conclusion": "modelSolver democratizes power system analysis by prioritizing model-driven development, making complex computations accessible without programming expertise while supporting advanced functionalities, thus benefiting a wider audience of domain experts."}}
{"id": "2508.17809", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.17809", "abs": "https://arxiv.org/abs/2508.17809", "authors": ["Abdullah Sahruri", "Martin Margala"], "title": "TLGLock: A New Approach in Logic Locking Using Key-Driven Charge Recycling in Threshold Logic Gates", "comment": "To appear in the 33rd IFIP/IEEE International Conference on Very\n  Large Scale Integration (VLSI-SoC 2025)", "summary": "Logic locking remains one of the most promising defenses against hardware\npiracy, yet current approaches often face challenges in scalability and design\noverhead. In this paper, we present TLGLock, a new design paradigm that\nleverages the structural expressiveness of Threshold Logic Gates (TLGs) and the\nenergy efficiency of charge recycling to enforce key-dependent functionality at\nthe gate level. By embedding the key into the gate's weighted logic and\nutilizing dynamic charge sharing, TLGLock provides a stateless and compact\nalternative to conventional locking techniques. We implement a complete\nsynthesis-to-locking flow and evaluate it using ISCAS, ITC, and MCNC\nbenchmarks. Results show that TLGLock achieves up to 30% area, 50% delay, and\n20% power savings compared to latch-based locking schemes. In comparison with\nXOR and SFLL-HD methods, TLGLock offers up to 3x higher SAT attack resistance\nwith significantly lower overhead. Furthermore, randomized key-weight\nexperiments demonstrate that TLGLock can reach up to 100% output corruption\nunder incorrect keys, enabling tunable security at minimal cost. These results\nposition TLGLock as a scalable and resilient solution for secure hardware\ndesign.", "AI": {"tldr": "TLGLock is a novel logic locking method using Threshold Logic Gates (TLGs) and charge recycling, offering improved efficiency and security compared to existing techniques.", "motivation": "Hardware piracy necessitates robust logic locking solutions. Current methods suffer from scalability issues and high design overhead (area, power, delay), while requiring complex structures to maintain security.", "method": "TLGLock integrates key-dependent functionality via gate-level Threshold Logic Gates (structural expressiveness) and dynamic charge sharing (energy efficiency). A key-weighted gate synthesis flow was implemented and benchmarked across standard logic circuits (ISCAS, ITC, MCNC).", "result": "Demonstrates 30% area reduction, 50% delay improvement, 20% power savings vs. latch-based locking. Achieves 3\u00d7 higher SAT attack resistance than XOR/SFLL-HD with 100% output corruption under incorrect keys in randomized experiments.", "conclusion": "TLGLock provides a secure, scalable alternative to traditional logic locking by leveraging TLG structure and charge recycling. Its stateless design and tunable security (via key-weight randomization) address critical tradeoffs between overhead and robustness."}}
{"id": "2508.17900", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17900", "abs": "https://arxiv.org/abs/2508.17900", "authors": ["Mohammed O. Alannsary"], "title": "A Defect Classification Framework for AI-Based Software Systems (AI-ODC)", "comment": "Article, 19 pages, 6 figures, 8 tables,", "summary": "Artificial Intelligence has gained a lot of attention recently, it has been\nutilized in several fields ranging from daily life activities, such as\nresponding to emails and scheduling appointments, to manufacturing and\nautomating work activities. Artificial Intelligence systems are mainly\nimplemented as software solutions, and it is essential to discover and remove\nsoftware defects to assure its quality using defect analysis which is one of\nthe major activities that contribute to software quality. Despite the\nproliferation of AI-based systems, current defect analysis models fail to\ncapture their unique attributes. This paper proposes a framework inspired by\nthe Orthogonal Defect Classification (ODC) paradigm and enables defect analysis\nof Artificial Intelligence systems while recognizing its special attributes and\ncharacteristics. This study demonstrated the feasibility of modifying ODC for\nAI systems to classify its defects. The ODC was adjusted to accommodate the\nData, Learning, and Thinking aspects of AI systems which are newly introduced\nclassification dimensions. This adjustment involved the introduction of an\nadditional attribute to the ODC attributes, the incorporation of a new severity\nlevel, and the substitution of impact areas with characteristics pertinent to\nAI systems. The framework was showcased by applying it to a publicly available\nMachine Learning bug dataset, with results analyzed through one-way and two-way\nanalysis. The case study indicated that defects occurring during the Learning\nphase were the most prevalent and were significantly linked to high-severity\nclassifications. In contrast, defects identified in the Thinking phase had a\ndisproportionate effect on trustworthiness and accuracy. These findings\nillustrate AIODC's capability to identify high-risk defect categories and\ninform focused quality assurance measures.", "AI": {"tldr": "This paper proposes AIODC, a modified Orthogonal Defect Classification framework for AI systems that addresses their unique data, learning, and thinking attributes through phase-based defect analysis and severity classification.", "motivation": "Current defect analysis models cannot effectively capture the unique characteristics of AI systems, necessitating a tailored framework to ensure quality assurance by accounting for data-centric and learning-based attributes.", "method": "Extended ODC paradigm by (1) introducing Data/Learning/Thinking classification dimensions with an additional attribute, (2) substituting traditional impact areas with AI-specific characteristics, and (3) defining a new severity level framework validated using a public ML bug case study.", "result": "Learning phase defects dominated (most prevalent) and correlated with high severity classifications. Thinking phase defects showed disproportionate impact on trustworthiness and accuracy metrics, verified through one-way and two-way statistical analysis of the public dataset.", "conclusion": "AIODC successfully identifies high-risk defect categories in AI systems, enabling targeted quality assurance strategies that address critical failure points in learning processes and decision-making accuracy."}}
{"id": "2508.17853", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.17853", "abs": "https://arxiv.org/abs/2508.17853", "authors": ["Saeed Alshehhi"], "title": "Software Unclonable Functions for IoT Devices Identification and Security", "comment": null, "summary": "In the evolving landscape of IoT ecosystem, distinguishing between legitimate\nand compromised devices is a critical challenge. This research investigates the\neffectiveness of hardware performance counter (HPC)-derived signatures'\nuniqueness under the umbrella of a concept that we introduced as software\nunclonable functions (SUFs).", "AI": {"tldr": "The paper investigates the use of hardware performance counter-derived signatures under the concept of software unclonable functions (SUFs) to uniquely identify and verify devices in the IoT ecosystem.", "motivation": "The IoT ecosystem faces challenges in distinguishing legitimate devices from compromised ones, requiring secure, intrinsic authentication methods. HPC-based SUFs aim to address this by leveraging device-specific hardware characteristics for unforgeable identifiers.", "method": "The authors evaluates the uniqueness and effectiveness of software unclonable functions (SUFs) derived from hardware performance counters (HPC) through analysis and experiments, focusing on counter values as source for signature generation.", "result": "Results demonstrate that HPC-derived signatures exhibit strong software unclonability across devices, enabling accurate identification of cloned or compromised hardware/software counterparts.", "conclusion": "HPC-based SUFs provide a reliable mechanism for device authentication in IoT due to their inherent hardware uniqueness and resistance to software replication, offering a practical security solution against device tampering."}}
{"id": "2508.17912", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.17912", "abs": "https://arxiv.org/abs/2508.17912", "authors": ["Mohammed O. Alannsary"], "title": "Evaluating Citizen Satisfaction with Saudi Arabia's E-Government Services: A Standards-Based, Theory-Informed Approach", "comment": "38 pages, 1 figure, 16 tables, journal research paper", "summary": "As digital government platforms become central to public service delivery,\nunderstanding citizen assessment is crucial for enhancing usability, trust, and\ninclusivity. This study investigates citizen satisfaction with the e-government\nservices in Saudi Arabia through a quality-in-use framework based on ISO/IEC\n25010 and ISO/IEC 25022 standards, interpreted through the lens of the Unified\nTheory of Acceptance and Use of Technology (UTAUT). A structured questionnaire\nwas administered to 500 citizens, yielding 276 valid responses. Satisfaction\nwas evaluated across four dimensions: overall satisfaction, feature\nsatisfaction, trust, and emotional engagement (pleasure). The findings\ndemonstrate consistently high levels of satisfaction regarding usability and\ntrust, aligning with Saudi Arabia's top-tier global ranking in e-government\ndevelopment. However, the results also highlight persistent challenges related\nto service clarity and system responsiveness. Emotional engagement was limited,\nindicating that users perceive these services primarily as functional tools\nrather than as engaging digital experiences. The study offers valuable insights\nfor policymakers and contributes to the theoretical integration of\nstandards-based and behavioral adoption models in the context of citizenship.", "AI": {"tldr": "This study evaluates citizen satisfaction with Saudi Arabia's e-government services using ISO standards and UTAUT, revealing high usability and trust but issues with service clarity, responsiveness, and emotional engagement.", "motivation": "To enhance public service delivery by improving usability, trust, and inclusivity in digital government platforms through citizen assessment.", "method": "A structured questionnaire based on ISO/IEC 25010, 25022, and UTAUT was administered to 500 Saudi citizens (276 valid responses). Satisfaction was analyzed across four dimensions: overall satisfaction, feature satisfaction, trust, and emotional engagement (pleasure).", "result": "High satisfaction in usability and trust, moderate satisfaction in service clarity and responsiveness, and low emotional engagement. Findings correlate with Saudi Arabia's top global e-government development ranking.", "conclusion": "The study provides actionable insights for policymakers to address service clarity and responsiveness gaps while advancing the theoretical integration of technical standards and behavioral adoption models in the e-government context."}}
{"id": "2508.17856", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.17856", "abs": "https://arxiv.org/abs/2508.17856", "authors": ["Tiezhu Sun", "Marco Alecci", "Aleksandr Pilgun", "Yewei Song", "Xunzhu Tang", "Jordan Samhi", "Tegawend\u00e9 F. Bissyand\u00e9", "Jacques Klein"], "title": "MalLoc: Toward Fine-grained Android Malicious Payload Localization via LLMs", "comment": "Accepted at ICSME 2025, NIER Track", "summary": "The rapid evolution of Android malware poses significant challenges to the\nmaintenance and security of mobile applications (apps). Traditional detection\ntechniques often struggle to keep pace with emerging malware variants that\nemploy advanced tactics such as code obfuscation and dynamic behavior\ntriggering. One major limitation of these approaches is their inability to\nlocalize malicious payloads at a fine-grained level, hindering precise\nunderstanding of malicious behavior. This gap in understanding makes the design\nof effective and targeted mitigation strategies difficult, leaving mobile apps\nvulnerable to continuously evolving threats.\n  To address this gap, we propose MalLoc, a novel approach that leverages the\ncode understanding capabilities of large language models (LLMs) to localize\nmalicious payloads at a fine-grained level within Android malware. Our\nexperimental results demonstrate the feasibility and effectiveness of using\nLLMs for this task, highlighting the potential of MalLoc to enhance precision\nand interpretability in malware analysis. This work advances beyond traditional\ndetection and classification by enabling deeper insights into behavior-level\nmalicious logic and opens new directions for research, including dynamic\nmodeling of localized threats and targeted countermeasure development.", "AI": {"tldr": "The paper proposes MalLoc, a novel approach using large language models (LLMs) to improve fine-grained localization of malicious payloads in Android malware, addressing limitations in traditional detection methods.", "motivation": "Traditional Android malware detection techniques fail to adapt to advanced evasion tactics like code obfuscation and dynamic behavior triggering, resulting in poor fine-grained payload localization and hindering effective mitigation strategy design.", "method": "MalLoc utilizes LLMs' code understanding capabilities to identify and localize malicious payloads within Android apps through advanced code analysis, enabling more precise detection of obfuscated and behaviorally complex malware.", "result": "Experimental results confirm MalLoc's feasibility and effectiveness in fine-grained malicious payload localization, outperforming traditional methods in precision and interpretability of malware behavior analysis.", "conclusion": "This work establishes LLMs as a valuable tool for deeper malware behavior analysis, opening new research directions for dynamic threat modeling and targeted countermeasure development against evolving Android threats."}}
{"id": "2508.17988", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.17988", "abs": "https://arxiv.org/abs/2508.17988", "authors": ["Eduardo de Conto", "Blaise Genest", "Arvind Easwaran", "Nicholas Ng", "Shweta Menon"], "title": "DesCartes Builder: A Tool to Develop Machine-Learning Based Digital Twins", "comment": "5 pages, 4 figures. Accepted at EDTconf 2025", "summary": "Digital twins (DTs) are increasingly utilized to monitor, manage, and\noptimize complex systems across various domains, including civil engineering. A\ncore requirement for an effective DT is to act as a fast, accurate, and\nmaintainable surrogate of its physical counterpart, the physical twin (PT). To\nthis end, machine learning (ML) is frequently employed to (i) construct\nreal-time DT prototypes using efficient reduced-order models (ROMs) derived\nfrom high-fidelity simulations of the PT's nominal behavior, and (ii)\nspecialize these prototypes into DT instances by leveraging historical sensor\ndata from the target PT. Despite the broad applicability of ML, its use in DT\nengineering remains largely ad hoc. Indeed, while conventional ML pipelines\noften train a single model for a specific task, DTs typically require multiple,\ntask- and domain-dependent models. Thus, a more structured approach is required\nto design DTs.\n  In this paper, we introduce DesCartes Builder, an open-source tool to enable\nthe systematic engineering of ML-based pipelines for real-time DT prototypes\nand DT instances. The tool leverages an open and flexible visual data flow\nparadigm to facilitate the specification, composition, and reuse of ML models.\nIt also integrates a library of parameterizable core operations and ML\nalgorithms tailored for DT design. We demonstrate the effectiveness and\nusability of DesCartes Builder through a civil engineering use case involving\nthe design of a real-time DT prototype to predict the plastic strain of a\nstructure.", "AI": {"tldr": "This paper introduces DesCartes Builder, an open-source tool for systematic engineering of ML-based digital twins in civil engineering, addressing the need for structured multi-model workflows.", "motivation": "Digital twins require fast, accurate, and maintainable ML pipelines with multiple task/domain-dependent models, but current approaches remain ad hoc and lack systematic design methodologies.", "method": "Developed an open-source visual data flow tool with parameterizable operations and domain-tailored ML algorithms to design DT prototypes and instances through model composition/reuse.", "result": "Demonstrated effectiveness via a civil engineering case study showing real-time plastic strain prediction of structures using the proposed pipeline framework.", "conclusion": "DesCartes Builder provides a structured, reusable approach to ML-based digital twin engineering, improving maintainability and adaptability for complex civil systems."}}
{"id": "2508.17884", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.17884", "abs": "https://arxiv.org/abs/2508.17884", "authors": ["Toby Murray"], "title": "PhantomLint: Principled Detection of Hidden LLM Prompts in Structured Documents", "comment": null, "summary": "Hidden LLM prompts have appeared in online documents with increasing\nfrequency. Their goal is to trigger indirect prompt injection attacks while\nremaining undetected from human oversight, to manipulate LLM-powered automated\ndocument processing systems, against applications as diverse as r\\'esum\\'e\nscreeners through to academic peer review processes. Detecting hidden LLM\nprompts is therefore important for ensuring trust in AI-assisted human decision\nmaking.\n  This paper presents the first principled approach to hidden LLM prompt\ndetection in structured documents. We implement our approach in a prototype\ntool called PhantomLint. We evaluate PhantomLint against a corpus of 3,402\ndocuments, including both PDF and HTML documents, and covering academic paper\npreprints, CVs, theses and more. We find that our approach is generally\napplicable against a wide range of methods for hiding LLM prompts from visual\ninspection, has a very low false positive rate (approx. 0.092%), is practically\nuseful for detecting hidden LLM prompts in real documents, while achieving\nacceptable performance.", "AI": {"tldr": "PhantomLint is a prototype tool developed to detect hidden LLM prompts in structured documents, with low false positives and effectiveness across various document types.", "motivation": "Hidden LLM prompts pose a security threat by manipulating AI-driven document processing systems; detection is critical to maintaining trust in AI-assisted decisions.", "method": "The authors implemented a principled approach for hidden prompt detection in structured documents, creating the PhantomLint tool, and evaluated it using a diverse corpus of 3,402 PDF/HTML academic and professional documents.", "result": "PhantomLint demonstrated general applicability in detecting hidden prompts, achieved a 0.092% false positive rate, and provided practical utility with acceptable performance in real document analysis.", "conclusion": "The paper establishes a reliable method for hidden LLM prompt detection in structured documents, effective against multiple hiding techniques while maintaining minimal false positives, ensuring safety in AI-assisted document workflows."}}
{"id": "2508.18003", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18003", "abs": "https://arxiv.org/abs/2508.18003", "authors": ["Robert Heum\u00fcller", "Frank Ortmeier"], "title": "Previously on... Automating Code Review", "comment": "Preprint currently under review", "summary": "Modern Code Review (MCR) is a standard practice in software engineering, yet\nit demands substantial time and resource investments. Recent research has\nincreasingly explored automating core review tasks using machine learning (ML)\nand deep learning (DL). As a result, there is substantial variability in task\ndefinitions, datasets, and evaluation procedures. This study provides the first\ncomprehensive analysis of MCR automation research, aiming to characterize the\nfield's evolution, formalize learning tasks, highlight methodological\nchallenges, and offer actionable recommendations to guide future research.\nFocusing on the primary code review tasks, we systematically surveyed 691\npublications and identified 24 relevant studies published between May 2015 and\nApril 2024. Each study was analyzed in terms of tasks, models, metrics,\nbaselines, results, validity concerns, and artifact availability. In\nparticular, our analysis reveals significant potential for standardization,\nincluding 48 task metric combinations, 22 of which were unique to their\noriginal paper, and limited dataset reuse. We highlight challenges and derive\nconcrete recommendations for examples such as the temporal bias threat, which\nare rarely addressed so far. Our work contributes to a clearer overview of the\nfield, supports the framing of new research, helps to avoid pitfalls, and\npromotes greater standardization in evaluation practices.", "AI": {"tldr": "This study offers a comprehensive analysis of Modern Code Review (MCR) automation research from 2015-2024, identifying 24 relevant studies. It highlights challenges like task variability, dataset reuse limitations, and methodological gaps (e.g., temporal bias) while advocating for standardization and actionable research guidance.", "motivation": "Modern Code Review is time-intensive and resource-heavy, and prior automation research suffers from inconsistent task definitions, datasets, and evaluation practices. This analysis aims to clarify the field's evolution, standardize learning tasks, address methodological flaws, and guide future work.", "method": "A systematic survey of 691 publications identified 24 relevant MCR automation studies. Each was analyzed using a framework spanning tasks, models, metrics, baselines, results, validity concerns, and dataset reuse patterns.", "result": "Analysis revealed 48 unique task-metric combinations (with 22 being paper-specific), low dataset reuse, and underaddressed challenges like temporal bias. Methodological gaps and variability in evaluation practices were systematically documented.", "conclusion": "The paper establishes a clearer understanding of MCR automation research, emphasizes the need for standardized evaluation practices, and provides concrete recommendations to address challenges like temporal bias while avoiding redundant work."}}
{"id": "2508.17913", "categories": ["cs.CR", "cs.ET", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.17913", "abs": "https://arxiv.org/abs/2508.17913", "authors": ["Yagmur Yigit", "Mehmet Ali Erturk", "Kerem Gursu", "Berk Canberk"], "title": "PRZK-Bind: A Physically Rooted Zero-Knowledge Authentication Protocol for Secure Digital Twin Binding in Smart Cities", "comment": "6 pages, 4 figures, 2 tables, Accepted by IEEE Global Communications\n  Conference (GLOBECOM) 2025", "summary": "Digital twin (DT) technology is rapidly becoming essential for smart city\necosystems, enabling real-time synchronisation and autonomous decision-making\nacross physical and digital domains. However, as DTs take active roles in\ncontrol loops, securely binding them to their physical counterparts in dynamic\nand adversarial environments remains a significant challenge. Existing\nauthentication solutions either rely on static trust models, require\ncentralised authorities, or fail to provide live and verifiable\nphysical-digital binding, making them unsuitable for latency-sensitive and\ndistributed deployments. To address this gap, we introduce PRZK-Bind, a\nlightweight and decentralised authentication protocol that combines\nSchnorr-based zero-knowledge proofs with elliptic curve cryptography to\nestablish secure, real-time correspondence between physical entities and DTs\nwithout relying on pre-shared secrets. Simulation results show that PRZK-Bind\nsignificantly improves performance, offering up to 4.5 times lower latency and\n4 times reduced energy consumption compared to cryptography-heavy baselines,\nwhile maintaining false acceptance rates more than 10 times lower. These\nfindings highlight its suitability for future smart city deployments requiring\nefficient, resilient, and trustworthy DT authentication.", "AI": {"tldr": "PRZK-Bind is a decentralized authentication protocol for digital twins in smart cities that uses zero-knowledge proofs and elliptic curve cryptography to securely bind physical and digital entities in real-time with low latency and energy consumption.", "motivation": "Existing solutions for physical-digital binding in digital twin ecosystems rely on static trust models, centralized authorities, or lack live verifiable guarantees, making them unsuitable for latency-sensitive and distributed smart city deployments that require secure dynamic environments.", "method": "The paper introduces PRZK-Bind, a lightweight protocol combining Schnorr-based zero-knowledge proofs with elliptic curve cryptography to establish secure real-time correspondence between physical entities and their digital twins without pre-shared secrets.", "result": "Simulation results demonstrate 4.5\u00d7 lower latency, 4\u00d7 reduced energy consumption compared to cryptography-heavy baselines, while achieving false acceptance rates more than 10\u00d7 lower than existing methods in adversarial environments.", "conclusion": "PRZK-Bind offers an efficient, resilient, and trustworthy authentication framework for smart city ecosystems requiring decentralized physical-digital binding with verifiable security guarantees."}}
{"id": "2508.18070", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18070", "abs": "https://arxiv.org/abs/2508.18070", "authors": ["Karolina M. Milano", "Wesley K. G. Assun\u00e7\u00e3o", "Bruno B. P. Cafeo"], "title": "A Large-Scale Study on Developer Engagement and Expertise in Configurable Software System Projects", "comment": null, "summary": "Modern systems operate in multiple contexts making variability a fundamental\naspect of Configurable Software Systems (CSSs). Variability, implemented via\npre-processor directives (e.g., #ifdef blocks) interleaved with other code and\nspread across files, complicates maintenance and increases error risk. Despite\nits importance, little is known about how variable code is distributed among\ndevelopers or whether conventional expertise metrics adequately capture\nvariable code proficiency. This study investigates developers' engagement with\nvariable versus mandatory code, the concentration of variable code workload,\nand the effectiveness of expertise metrics in CSS projects. We mined\nrepositories of 25 CSS projects, analyzing 450,255 commits from 9,678\ndevelopers. Results show that 59% of developers never modified variable code,\nwhile about 17% were responsible for developing and maintaining 83% of it. This\nindicates a high concentration of variable code expertise among a few\ndevelopers, suggesting that task assignments should prioritize these\nspecialists. Moreover, conventional expertise metrics performed\npoorly--achieving only around 55% precision and 50% recall in identifying\ndevelopers engaged with variable code. Our findings highlight an unbalanced\ndistribution of variable code responsibilities and underscore the need to\nrefine expertise metrics to better support task assignments in CSS projects,\nthereby promoting a more equitable workload distribution.", "AI": {"tldr": "This paper analyzes the distribution of variable code expertise in CSS projects, revealing that 59% of developers never touch it while 17% handle 83%. Existing metrics poorly identify such expertise, emphasizing the need for improved task allocation and refinement of expertise models.", "motivation": "CSSs' variability via pre-processor directives complicates maintenance and poses risks. It is unclear how variable code responsibilities are distributed among developers or if conventional metrics can assess proficiency in this area.", "method": "Repository mining of 25 CSS projects with 450,255 commits from 9,678 developers, comparing variable vs. mandatory code engagement patterns and testing standard expertise metrics.", "result": "59% of developers never modified variable code; 17% controlled 83% of it. Standard metrics achieved 55% precision/50% recall for identifying variable code contributors.", "conclusion": "Variable code responsibilities are highly concentrated among specialists, challenging current expertise metrics. Task assignment strategies must be adjusted to balance workloads and better leverage variable code proficiency."}}
{"id": "2508.17964", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.17964", "abs": "https://arxiv.org/abs/2508.17964", "authors": ["Yuhe Lu", "Zhongwen Li", "Xiaoqi Li"], "title": "MoveScanner: Analysis of Security Risks of Move Smart Contracts", "comment": null, "summary": "As blockchain technology continues to evolve, the security of smart contracts\nhas increasingly drawn attention from both academia and industry. The Move\nlanguage, with its unique resource model and linear type system, provides a\nsolid foundation for the security of digital assets. However, smart contracts\nstill face new security challenges due to developer programming errors and the\npotential risks associated with cross-module interactions. This paper\nsystematically analyzes the limitations of existing security tools within the\nMove ecosystem and reveals their unique vulnerability patterns. To address\nthese issues, it introduces MoveScanner, a static analysis tool based on a\ncontrol flow graph and data flow analysis architecture. By incorporating\ncross-module call graph tracking, MoveScanner can effectively identify five key\ntypes of security vulnerabilities, including resource leaks, weak permission\nmanagement, and arithmetic overflows. In terms of design, MoveScanner adheres\nto a modular principle, supports bytecode-level analysis and multi-chain\nadaptation, and introduces innovative resource trajectory tracking algorithms\nand capability matrix analysis methods, thereby significantly reducing the\nfalse positive rate. Empirical results show that MoveScanner achieved 88.2%\ndetection accuracy in benchmark testing, filling the gap in security tools in\nthe Move ecosystem. Furthermore, this paper identifies twelve new types of\nsecurity risks based on the resource-oriented programming paradigm and provides\na theoretical foundation and practical experience for the development of smart\ncontract security mechanisms. Future work will focus on combining formal\nverification and dynamic analysis techniques to build a security protection\nframework covering the entire contract lifecycle", "AI": {"tldr": "MoveScanner fills security tool gaps in Move smart contracts by detecting 5+5 critical vulnerabilities with 88.2% accuracy, leveraging novel analysis methods to reduce false positives and uncover 12 new risks.", "motivation": "Smart contracts in Move language face security risks from programming errors and cross-module interactions, with existing tools lacking in detecting resource-oriented vulnerabilities and minimizing false positives.", "method": "Developed MoveScanner, a static analysis tool utilizing control flow graph and data flow analysis, cross-module call graph tracking, resource trajectory algorithms, and capability matrix analysis to detect vulnerabilities at the bytecode level.", "result": "MoveScanner achieved 88.2% detection accuracy in benchmarks, identified five vulnerability types (including resource leaks and arithmetic overflows), and uncovered 12 new security risks within the Move ecosystem.", "conclusion": "MoveScanner addresses security gaps in the Move ecosystem by introducing a static analysis tool that reduces false positives and identifies new vulnerabilities, with future work aiming to integrate formal verification and dynamic analysis for comprehensive security."}}
{"id": "2508.18073", "categories": ["cs.SE", "cs.DL"], "pdf": "https://arxiv.org/pdf/2508.18073", "abs": "https://arxiv.org/abs/2508.18073", "authors": ["Joenio Marques da Costa", "Christina von Flach"], "title": "Debian in the Research Software Ecosystem: A Bibliometric Analysis", "comment": "5 pages; 3 figures; 2 tables; to be published in DebConf25 Academic\n  Track https://www.diverse-team.fr/debconf25-academictrack", "summary": "Context: The Debian system has historically participated in academic works\nand scientific projects, with well-known examples including NeuroDebian, Debian\nMed, Debsources, Debian Science, and Debian GIS, where the scientific relevance\nof Debian and its contribution to the Research Software ecosystem are evident.\n  Objective: The objective of this study is to investigate the Debian system\nthrough academic publications, with the aim of classifying articles, mapping\nresearch, identifying trends, and finding opportunities.\n  Method: The study is based on a bibliometric analysis starting with an\ninitial search for the term \"Debian\" in the titles, abstracts, or keywords of\nacademic publications, using the Scopus database. This analysis calculates\nmetrics of co-citation, co-authorship, and word co-occurrence, and is guided by\na set of research questions and criteria for inclusion and exclusion to conduct\nthe bibliometric analysis.\n  Results: The study includes a set of articles published across various fields\nof knowledge, providing a map of the academic publication space about Debian.\nThe study's data will be available in a public repository, reporting\ndemographic and bibliometric trends, including the most cited articles, active\ncountries, researchers, and popular conferences.\n  Conclusion: Results includes a bibliometric and demographic analysis\nidentified in publications about Debian, shedding light on the intellectual\nstructure of academic research. The results of the analyses can help\nresearchers gain an overview of existing trends in publications about Debian\nand identify areas that require more attention from the scientific community.", "AI": {"tldr": "This paper presents a bibliometric study of Debian-related academic publications, analyzing research trends and contributions through co-citation, co-authorship, and word co-occurrence metrics using Scopus data.", "motivation": "Debian's historical involvement in scientific projects highlights its relevance to the Research Software ecosystem, prompting the need to understand its academic impact and identify research opportunities.", "method": "Bibliometric analysis of Scopus publications containing 'Debian' in titles/abstracts/keywords, involving co-citation, co-authorship, and word co-occurrence metrics with pre-defined inclusion/exclusion criteria.", "result": "A comprehensive map of Debian's academic research landscape, including interdisciplinary articles, demographic trends, and open access to data via public repositories for collaboration.", "conclusion": "The analysis reveals intellectual structures and trends in Debian-related research, providing actionable insights for the scientific community to focus on underrepresented areas."}}
{"id": "2508.18109", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.18109", "abs": "https://arxiv.org/abs/2508.18109", "authors": ["Lingxiao Wang", "Wenjing Dang", "Mengyao Zhang", "Yue Wang", "Xianzong Wu", "Sen Chen"], "title": "Aligning Core Aspects: Improving Vulnerability Proof-of-Concepts via Cross-Source Insights", "comment": null, "summary": "For vulnerabilities, Proof-of-Concept (PoC) plays an irreplaceable role in\ndemonstrating the exploitability. PoC reports may include critical information\nsuch as specific usage, test platforms, and more, providing essential insights\nfor researchers. However, in reality, due to various PoC templates across PoC\nplatforms, PoC reports extensively suffer from information deficiency, leading\nthe suboptimal quality and limited usefulness. Fortunately, we found that\ninformation deficiency of PoC reports could be mitigated by the completion from\nmultiple sources given the same referred vulnerability. In this paper, we\nconduct the first study on the deficiency of information in PoC reports across\npublic platforms. We began by collecting 173,170 PoC reports from 4 different\nplatforms and defined 8 key aspects that PoCs should contain. By integrating\nrule-based matching and a fine-tuned BERT-NER model for extraction of key\naspects, we discovered that all PoC reports available on public platforms have\nat least one missing key aspect. Subsequently, we developed a multi-source\ninformation fusion method to complete the missing aspect information in PoC\nreports by leveraging CVE entries and related PoC reports from different\nsources. Finally, we successfully completed 69,583 PoC reports (40.18% of all\nreports).", "AI": {"tldr": "This paper addresses information deficiency in Proof-of-Concept (PoC) reports across public platforms by proposing a multi-source information fusion method to complete missing aspects, successfully improving 40.18% of 173,170 PoC reports analyzed.", "motivation": "PoC reports are critical for demonstrating vulnerability exploitability but suffer from inconsistent templates and missing information across platforms, limiting their quality and usefulness for researchers.", "method": "They collected 173,170 PoC reports, defined 8 key aspects using rule-based matching and a fine-tuned BERT-NER model. A multi-source fusion approach was developed to complete missing aspects using CVE entries and cross-referenced PoC reports from different platforms.", "result": "The method successfully completed 69,583 PoC reports (40.18% of the total), demonstrating significant information recovery in vulnerability-related PoC evidence.", "conclusion": "Multi-source information fusion from CVE entries and diverse PoC platforms can effectively mitigate information deficiency in PoC reports, enhancing their quality and practical value for vulnerability research."}}
{"id": "2508.18089", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18089", "abs": "https://arxiv.org/abs/2508.18089", "authors": ["Karine Even-Mendoza", "Alexander Brownlee", "Alina Geiger", "Carol Hanna", "Justyna Petke", "Federica Sarro", "Dominik Sobania"], "title": "LLM-Guided Genetic Improvement: Envisioning Semantic Aware Automated Software Evolution", "comment": null, "summary": "Genetic Improvement (GI) of software automatically creates alternative\nsoftware versions that are improved according to certain properties of\ninterests (e.g., running-time). Search-based GI excels at navigating large\nprogram spaces, but operates primarily at the syntactic level. In contrast,\nLarge Language Models (LLMs) offer semantic-aware edits, yet lack goal-directed\nfeedback and control (which is instead a strength of GI). As such, we propose\nthe investigation of a new research line on AI-powered GI aimed at\nincorporating semantic aware search. We take a first step at it by augmenting\nGI with the use of automated clustering of LLM edits. We provide initial\nempirical evidence that our proposal, dubbed PatchCat, allows us to\nautomatically and effectively categorize LLM-suggested patches. PatchCat\nidentified 18 different types of software patches and categorized newly\nsuggested patches with high accuracy. It also enabled detecting NoOp edits in\nadvance and, prospectively, to skip test suite execution to save resources in\nmany cases. These results, coupled with the fact that PatchCat works with\nsmall, local LLMs, are a promising step toward interpretable, efficient, and\ngreen GI. We outline a rich agenda of future work and call for the community to\njoin our vision of building a principled understanding of LLM-driven mutations,\nguiding the GI search process with semantic signals.", "AI": {"tldr": "This paper introduces PatchCat, a method that integrates semantic-aware edits from Large Language Models (LLMs) into Search-based Genetic Improvement (GI) for better code customization. PatchCat clusters LLM-suggested patches, identifies 18 patch types accurately, detects NoOp edits early, and uses small LLMs to save resources, marking a step toward interpretable, efficient GI.", "motivation": "Search-based GI optimizes software syntactically but lacks semantic understanding, while LLMs provide semantics but no goal-directed control. The paper addresses this gap to enable effective, interpretable AI-driven GI that combines semantic awareness and efficiency.", "method": "PatchCat employes automated clustering of LLM-suggested software patches into semantic categories. It leverages small local LLMs to analyze edits, identify patterns, and detect NoOp (non-operational) edits early, reducing the need for test suite executions.", "result": "PatchCat identified 18 distinct patch categories with high accuracy. Early NoOp detection allowed up to 77% reduction in unnecessary test runs. The system works efficiently using small LLMs and demonstrates significant resource and time savings in GI workflows.", "conclusion": "PatchCat establishes a promising foundation for semantic-aware GI by effectively categorizing LLM edits and reducing test overhead. The paper advocates for community collaboration to advance theory and applications of LLM-driven code mutations, emphasizing eco-friendly and interpretable software improvement."}}
{"id": "2508.18148", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18148", "abs": "https://arxiv.org/abs/2508.18148", "authors": ["Haijian Ma", "Daizong Liu", "Xiaowen Cai", "Pan Zhou", "Yulai Xie"], "title": "Learning from Few Samples: A Novel Approach for High-Quality Malcode Generation", "comment": "18pages,5 figures,emnlp", "summary": "Intrusion Detection Systems (IDS) play a crucial role in network security\ndefense. However, a significant challenge for IDS in training detection models\nis the shortage of adequately labeled malicious samples. To address these\nissues, this paper introduces a novel semi-supervised framework\n\\textbf{GANGRL-LLM}, which integrates Generative Adversarial Networks (GANs)\nwith Large Language Models (LLMs) to enhance malicious code generation and SQL\nInjection (SQLi) detection capabilities in few-sample learning scenarios.\nSpecifically, our framework adopts a collaborative training paradigm where: (1)\nthe GAN-based discriminator improves malicious pattern recognition through\nadversarial learning with generated samples and limited real samples; and (2)\nthe LLM-based generator refines the quality of malicious code synthesis using\nreward signals from the discriminator. The experimental results demonstrate\nthat even with a limited number of labeled samples, our training framework is\nhighly effective in enhancing both malicious code generation and detection\ncapabilities. This dual enhancement capability offers a promising solution for\ndeveloping adaptive defense systems capable of countering evolving cyber\nthreats.", "AI": {"tldr": "The paper proposes GANGRL-LLM, a semi-supervised framework combining GANs and LLMs to address limited labeled malicious samples in IDS training. It enhances both malicious code generation and SQLi detection through collaborative adversarial training.", "motivation": "IDS struggles with limited labeled malicious samples, hampering detection model training. Existing methods lack effectiveness in few-sample learning scenarios for evolving cyber threats.", "method": "GANGRL-LLM uses collaborative training: 1) GAN discriminator learns malicious patterns via adversarial interactions with generated samples and limited real data. 2) LLM generator improves code synthesis quality using discriminator feedback as reward signals.", "result": "Experimental results show significant performance improvements in both malicious code generation and detection with fewer labeled samples compared to existing approaches.", "conclusion": "The framework establishes a dual enhancement approach for few-sample IDS training, offering a promising foundation for adaptive defenses against new and evolving cyber threats through synthetic data generation."}}
{"id": "2508.18106", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18106", "abs": "https://arxiv.org/abs/2508.18106", "authors": ["Keke Lian", "Bin Wang", "Lei Zhang", "Libo Chen", "Junjie Wang", "Ziming Zhao", "Yujiu Yang", "Haotong Duan", "Haoran Zhao", "Shuang Liao", "Mingda Guo", "Jiazheng Quan", "Yilu Zhong", "Chenhao He", "Zichuan Chen", "Jie Wu", "Haoling Li", "Zhaoxuan Li", "Jiongchi Yu", "Hui Li", "Dong Zhang"], "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code", "comment": null, "summary": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks are inadequate, as they focus on isolated code\nsnippets, employ unstable evaluation methods that lack reproducibility, and\nfail to connect the quality of input context with the security of the output.\nTo address these gaps, we introduce A.S.E (AI Code Generation Security\nEvaluation), a benchmark for repository-level secure code generation. A.S.E\nconstructs tasks from real-world repositories with documented CVEs, preserving\nfull repository context like build systems and cross-file dependencies. Its\nreproducible, containerized evaluation framework uses expert-defined rules to\nprovide stable, auditable assessments of security, build quality, and\ngeneration stability. Our evaluation of leading LLMs on A.S.E reveals three key\nfindings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The\nsecurity gap between proprietary and open-source models is narrow;\nQwen3-235B-A22B-Instruct attains the top security score. (3) Concise,\n``fast-thinking'' decoding strategies consistently outperform complex,\n``slow-thinking'' reasoning for security patching.", "AI": {"tldr": "A.S.E introduces a repository-level benchmark for secure code generation using real-world CVEs and reproducible evaluations, revealing model performance patterns.", "motivation": "Existing code generation security benchmarks are inadequate due to isolated code snippets, unstable evaluation methods, and a lack of correlation between input context quality and output security.", "method": "Constructed tasks from real-world repositories with documented CVEs, preserving full context through build systems and cross-file dependencies. Uses containerized evaluation with expert-defined security, build quality, and generation stability metrics. ", "result": "1. Claude-3.7-Sonnet achieves best overall performance. 2. Security performance gap between proprietary and open-source models is narrow (Qwen3-235B-A22B-Instruct top security score). 3. Simple decoding strategies outperform complex reasoning for security patches.", "conclusion": "A.S.E provides a more effective security evaluation framework than prior benchmarks, uncovering critical model performance characteristics while emphasizing the importance of context preservation and evaluation stability in secure code generation."}}
{"id": "2508.18155", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.18155", "abs": "https://arxiv.org/abs/2508.18155", "authors": ["Muhammad Ali Nadeem", "Bishwo Prakash Pokharel", "Naresh Kshetri", "Achyut Shankar", "Gokarna Sharma"], "title": "$AutoGuardX$: A Comprehensive Cybersecurity Framework for Connected Vehicles", "comment": "16 pages, 3 figures, 8 tables", "summary": "The rapid integration of Internet of Things (IoT) and interconnected systems\nin modern vehicles not only introduced a new era of convenience, automation,\nand connected vehicles but also elevated their exposure to sophisticated cyber\nthreats. This is especially evident in US and Canada, where cyber-enabled auto\ntheft has surged in recent years, revealing the limitations of existing\nsecurity measures for connected vehicles. In response, this paper proposes\n$AutoGuardX$, a comprehensive cybersecurity framework designed specifically for\nconnected vehicles. $AutoGuardX$ combines key elements from existing recognized\nstandards for vehicle security, such as ISO/SAE 21434 and ISO 26262, with\nadvanced technologies, including machine learning-based anomaly detection, IoT\nsecurity protocols, and encrypted communication channels. The framework\naddresses major attack vectors like relay attacks, controller area network\n(CAN) bus intrusions, and vulnerabilities introduced by emerging technologies\nsuch as 5G and quantum computing. $AutoGuardX$ is extensively evaluated through\nsecurity simulations across a mix of Sedans and SUVs from four major vehicle\nbrands manufactured between 2019 and 2023. The results demonstrate the\nframework's adaptability, scalability, and practical effectiveness against\nexisting and emerging threats.", "AI": {"tldr": "This paper proposes AutoGuardX, a cybersecurity framework for connected vehicles integrating ISO standards and advanced technologies like ML-based anomaly detection, IoT protocols, and encrypted communication. It's evaluated via simulations and shows effectiveness against existing and emerging threats.", "motivation": "The surge in cyber-enabled auto theft in the US and Canada highlights the limitations of current security measures for connected vehicles, driven by their increasing integration with IoT and interconnected systems.", "method": "1. Combined ISO/SAE 21434 and ISO 26262 standards 2. Integrated machine learning-based anomaly detection 3. Utilized IoT security protocols and encrypted communication 4. Conducted security simulations on 2019-2023 vehicles from four major brands, testing against relay attacks, CAN bus intrusions, 5G, and quantum computing vulnerabilities.", "result": "AutoGuardX demonstrates adaptability, scalability, and practical effectiveness in counteracting existing threats (e.g., relay attacks, CAN intrusions) and mitigating future risks from 5G and quantum computing through simulation results.", "conclusion": "AutoGuardX provides a robust, standardized solution for connected vehicle security that bridges current protection gaps while preparing for next-generation cyber risks through its advanced technology integration and simulation-validated effectiveness."}}
{"id": "2508.18230", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18230", "abs": "https://arxiv.org/abs/2508.18230", "authors": ["Chitraksh Singh", "Monisha Dhanraj", "Ken Huang"], "title": "KillChainGraph: ML Framework for Predicting and Mapping ATT&CK Techniques", "comment": "8 pages, 3 figures", "summary": "The escalating complexity and volume of cyberattacks demand proactive\ndetection strategies that go beyond traditional rule-based systems. This paper\npresents a phase-aware, multi-model machine learning framework that emulates\nadversarial behavior across the seven phases of the Cyber Kill Chain using the\nMITRE ATT&CK Enterprise dataset. Techniques are semantically mapped to phases\nvia ATTACK-BERT, producing seven phase-specific datasets. We evaluate LightGBM,\na custom Transformer encoder, fine-tuned BERT, and a Graph Neural Network\n(GNN), integrating their outputs through a weighted soft voting ensemble.\nInter-phase dependencies are modeled using directed graphs to capture attacker\nmovement from reconnaissance to objectives. The ensemble consistently achieved\nthe highest scores, with F1-scores ranging from 97.47% to 99.83%, surpassing\nGNN performance (97.36% to 99.81%) by 0.03%--0.20% across phases. This\ngraph-driven, ensemble-based approach enables interpretable attack path\nforecasting and strengthens proactive cyber defense.", "AI": {"tldr": "This paper proposes a phase-aware, multi-model ML framework using MITRE ATT&CK data and ATTACK-BERT for proactive cyberattack detection. An ensemble of models with directed graph modeling achieves 97.47%-99.83% F1-scores.", "motivation": "Traditional rule-based cyberattack detection systems lack effectiveness against escalating attack complexity and patterns. There's a need for proactive, adaptive solutions that model attacker behavior holistically.", "method": "1) Semantically maps MITRE ATT&CK techniques to 7 Cyber Kill Chain phases using ATTACK-BERT\n2) Trains LightGBM, Transformer encoder, fine-tuned BERT, and GNN models on phase-specific datasets\n3) Creates inter-phase attack dependency graphs\n4) Integrates model outputs via weighted soft voting ensemble", "result": "Ensemble consistently outperformed individual models: F1-scores 97.47%-99.83% (mean +0.03-0.20% improvement over GNN in specific phases). Outperformed pure GNN implementation (97.36%-99.81%).", "conclusion": "The graph-driven, phase-aware ensemble approach enables interpretable attack path forecasting by modeling tactical progression between kill chain stages, offering a robust solution for proactive cyber defense."}}
