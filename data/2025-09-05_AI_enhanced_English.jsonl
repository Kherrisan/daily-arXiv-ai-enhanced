{"id": "2509.03541", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03541", "abs": "https://arxiv.org/abs/2509.03541", "authors": ["Chong Wang", "Haoning Wu", "Peng Liang", "Maya Daneva", "Marten van Sinderen"], "title": "Towards the Datasets Used in Requirements Engineering of Mobile Apps: Preliminary Findings from a Systematic Mapping Study", "comment": null, "summary": "[Background] Research on requirements engineering (RE) for mobile apps\nemploys datasets formed by app users, developers or vendors. However, little is\nknown about the sources of these datasets in terms of platforms and the RE\nactivities that were researched with the help of the respective datasets.\n[Aims] The goal of this paper is to investigate the state-of-the-art of the\ndatasets of mobile apps used in existing RE research. [Method] We carried out a\nsystematic mapping study by following the guidelines of Kitchenham et al.\n[Results] Based on 43 selected papers, we found that Google Play and Apple App\nStore provide the datasets for more than 90% of published research in RE for\nmobile apps. We also found that the most investigated RE activities - based on\ndatasets, are requirements elicitation and requirements analysis. [Conclusions]\nOur most important conclusions are: (1) there is a growth in the use of\ndatasets for RE research of mobile apps since 2012, (2) the RE knowledge for\nmobile apps might be skewed due to the overuse of Google Play and Apple App\nStore, (3) there are attempts to supplement reviews of apps from repositories\nwith other data sources, (4) there is a need to expand the alternative sources\nand experiments with complimentary use of multiple sources, if the community\nwants more generalizable results. Plus, it is expected to expand the research\non other RE activities, beyond elicitation and analysis.", "AI": {"tldr": "This paper analyzes mobile app RE research datasets, finding heavy reliance on Google Play/Apple (90%+), a focus on elicitation/analysis, and calls for diversifying data sources to improve generalizability and expanding research to other RE activities.", "motivation": "The study was motivated by the lack of understanding about the origins and usage of mobile app datasets in requirements engineering research, particularly regarding platform sources and RE activity coverage.", "method": "The authors conducted a systematic mapping study following Kitchenham et al.'s guidelines, analyzing 43 selected papers to evaluate datasets and RE activities in mobile app research.", "result": "Results show Google Play and Apple App Store provide data for over 90% of mobile app RE research, with requirements elicitation and analysis being the most studied activities. Alternative data sources are beginning to be explored but remain underutilized.", "conclusion": "The paper concludes that there is a growing use of datasets in mobile app RE research since 2012, but overreliance on Google Play and Apple App Store may skew findings. It highlights a need for alternative data sources and expanded research into RE activities beyond elicitation and analysis."}}
{"id": "2509.03554", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03554", "abs": "https://arxiv.org/abs/2509.03554", "authors": ["Cheng-Yang Tsai", "Tzu-Wei Huang", "Jen-Wei Shih", "I-Hsiang Wang", "Yu-Cheng Lin", "Rung-Bin Lin"], "title": "A Multi-stage Error Diagnosis for APB Transaction", "comment": null, "summary": "Functional verification and debugging are critical bottlenecks in modern\nSystem-on-Chip (SoC) design, with manual detection of Advanced Peripheral Bus\n(APB) transaction errors in large Value Change Dump (VCD) files being\ninefficient and error-prone. Addressing the 2025 ICCAD Contest Problem D, this\nstudy proposes an automated error diagnosis framework using a hierarchical\nRandom Forest-based architecture. The multi-stage error diagnosis employs four\npre-trained binary classifiers to sequentially detect Out-of-Range Access,\nAddress Corruption, and Data Corruption errors, prioritizing high-certainty\naddress-related faults before tackling complex data errors to enhance\nefficiency. Experimental results show an overall accuracy of 91.36%, with\nnear-perfect precision and recall for address errors and robust performance for\ndata errors. Although the final results of the ICCAD 2025 CAD Contest are yet\nto be announced as of the submission date, our team achieved first place in the\nbeta stage, highlighting the method's competitive strength. This research\nvalidates the potential of hierarchical machine learning as a powerful\nautomated tool for hardware debugging in Electronic Design Automation (EDA).", "AI": {"tldr": "This paper proposes a hierarchical Random Forest framework for automated APB error diagnosis, achieving 91.36% accuracy and first-place performance in ICCAD 2025 contest beta testing.", "motivation": "Manual analysis of APB transactions in large VCD files for SoC verification is inefficient and error-prone, necessitating automated diagnostic tools for functional debugging tasks.", "method": "The study implements a hierarchical Random Forest architecture with four pre-trained binary classifiers. It employs a sequential multi-stage approach to prioritize address-related fault detection (Out-of-Range Access, Address Corruption) before addressing data corruption errors.", "result": "The framework achieved 91.36% overall accuracy with near-perfect performance on address errors and robust results on data errors. It secured first place in the ICCAD 2025 contest beta stage despite final results pending.", "conclusion": "This research demonstrates that hierarchical machine learning can serve as an effective automated tool for hardware debugging in EDA, particularly for APB transaction error detection."}}
{"id": "2509.03668", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03668", "abs": "https://arxiv.org/abs/2509.03668", "authors": ["Matt Rau", "Chris Brown", "John Edwards"], "title": "Parse Tree Tracking Through Time for Programming Process Analysis at Scale", "comment": null, "summary": "Background and Context: Programming process data can be utilized to\nunderstand the processes students use to write computer programming\nassignments. Keystroke- and line-level event logs have been used in the past in\nvarious ways, primarily in high-level descriptive statistics (e.g., timings,\ncharacter deletion rate, etc). Analysis of behavior in context (e.g., how much\ntime students spend working on loops) has been cumbersome because of our\ninability to automatically track high-level code representations, such as\nabstract syntax trees, through time and unparseable states.\n  Objective: Our study has two goals. The first is to design the first\nalgorithm that tracks parse tree nodes through time. Second, we utilize this\nalgorithm to perform a partial replication study of prior work that used manual\ntracking of code representations, as well as other novel analyses of student\nprogramming behavior that can now be done at scale.\n  Method: We use two algorithms presented in this paper to track parse tree\nnodes through time and construct tree representations for unparseable code\nstates. We apply these algorithms to a public keystroke data from student\ncoursework in a 2021 CS1 course and conduct analysis on the resulting parse\ntrees.\n  Findings: We discover newly observable statistics at scale, including that\ncode is deleted at similar rates inside and outside of conditionals and loops,\na third of commented out code is eventually restored, and that frequency with\nwhich students jump around in their code may not be indicative of struggle.\n  Implications: The ability to track parse trees through time opens the door to\nunderstanding new dimensions of student programming, such as best practices of\nstructural development of code over time, quantitative measurement of what\nsyntactic constructs students struggle most with, refactoring behavior, and\nattention shifting within the code.", "AI": {"tldr": "This paper introduces the first algorithms to track parse trees over time from programming logs, enabling scalable analysis of student coding patterns using a large 2021 CS1 dataset. Results reveal novel insights like balanced deletion rates in code structures and reinterpretation of code-jumping behaviors.", "motivation": "Previous analyses of student programming behavior were limited to low-level event logs (e.g., keystrokes) due to the inability to automatically track high-level code structures across time and unparseable states. This hinders understanding nuanced coding behaviors like attention to specific constructs (e.g., loops).", "method": "The study introduces two algorithms to track parse tree nodes through time and handle unparseable code states by analyzing public keystroke data from a CS1 course. These algorithms were used to construct and analyze parse trees for student programming behavior.", "result": "The study found (1) equal deletion rates in/around control structures (loops/conditionals) versus surrounding code, (2) 33% of commented-out code is eventually restored, and (3) code navigation patterns do not reliably indicate struggle.", "conclusion": "The ability to track parse trees through time opens the door to understanding new dimensions of student programming, such as structural development best practices, syntactic struggle measurement, refactoring behavior, and attention patterns within code."}}
{"id": "2509.03848", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.03848", "abs": "https://arxiv.org/abs/2509.03848", "authors": ["Rodrigo Oliveira Zacarias", "Rodrigo Pereira dos Santos", "Patricia Lago"], "title": "Towards an Understanding of Developer Experience-Driven Transparency in Software Ecosystems", "comment": "36 pages Submitted to the ACM Transactions on Software Engineering\n  and Methodology. 2025", "summary": "Software ecosystems (SECO) have become a dominant paradigm in the software\nindustry, enabling third-party developers to co-create value through\ncomplementary components and services. While Developer Experience (DX) is\nincreasingly recognized as critical for sustainable SECO, transparency remains\nan underexplored factor shaping how developers perceive and interact with\necosystems. Existing studies acknowledge transparency as essential for trust,\nfairness, and engagement, yet its relationship with DX has not been\nsystematically conceptualized. Hence, this work aims to advance the\nunderstanding of transparency in SECO from a developer-centered perspective. To\nthis end, we propose SECO-TransDX (Transparency in Software Ecosystems from a\nDeveloper Experience Perspective), a conceptual model that introduces the\nnotion of DX-driven transparency. The model identifies 63 interrelated\nconcepts, including conditioning factors, ecosystem procedures, artifacts, and\nrelational dynamics that influence how transparency is perceived and\nconstructed during developer interactions. SECO-TransDX was built upon prior\nresearch and refined through a Delphi study with experts from academia and\nindustry. It offers a structured lens to examine how transparency mediates DX\nacross technical, social, and organizational layers. For researchers, it lays\nthe groundwork for future studies and tool development; for practitioners, it\nsupports the design of trustworthy, developer-centered platforms that improve\ntransparency and foster long-term engagement in SECO.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.03711", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03711", "abs": "https://arxiv.org/abs/2509.03711", "authors": ["Siddharth Muralee", "Sourag Cherupattamoolayil", "James C. Davis", "Antonio Bianchi", "Aravind Machiry"], "title": "Reactive Bottom-Up Testing", "comment": null, "summary": "Modern computing systems remain rife with software vulnerabilities. Engineers\napply many means to detect them, of which dynamic testing is one of the most\ncommon and effective. However, most dynamic testing techniques follow a\ntop-down paradigm, and struggle to reach and exercise functions deep within the\ncall graph. While recent works have proposed Bottom-Up approaches to address\nthese limitations, they face challenges with false positives and generating\nvalid inputs that adhere to the context of the entire program.\n  In this work, we introduce a new paradigm that we call Reactive Bottom-Up\nTesting. Our insight is that function-level testing is necessary but not\nsufficient for the validation of vulnerabilities in functions. What we need is\na systematic approach that not only tests functions in isolation but also\nvalidates their behavior within the broader program context, ensuring that\ndetected vulnerabilities are both reachable and triggerable. We develop a\nthree-stage bottom-up testing scheme: (1) identify likely-vulnerable functions\nand generate type- and context-aware harnesses; (2) fuzz to find crashes and\nextract input constraints via symbolic execution; (3) verify crashes by\ncombining constraints to remove false positives. We implemented an automated\nprototype, which we call Griller. We evaluated Griller in a controlled setting\nusing a benchmark of 48 known vulnerabilities across 5 open-source projects,\nwhere we successfully detected 28 known vulnerabilities. Additionally, we\nevaluated Griller on several real-world applications such as Pacman, and it\ndiscovered 6 previously unknown vulnerabilities. Our findings suggest that\nReactive Bottom-Up Testing can significantly enhance the detection of\nvulnerabilities in complex systems, paving the way for more robust security\npractices.", "AI": {"tldr": "This paper introduces Reactive Bottom-Up Testing, a three-stage method combining fuzzing and symbolic execution to validate software vulnerabilities in context. The Griller implementation successfully detects both known and new vulnerabilities, outperforming existing bottom-up approaches.", "motivation": "Top-down dynamic testing struggles with deep call graph functions, while existing bottom-up approaches face false positives and context-adherent input generation. A systematic method is needed to validate vulnerabilities in isolated and broader program contexts.", "method": "The authors propose a three-stage Reactive Bottom-Up Testing scheme: (1) Identify likely-vulnerable functions and generate type/context-aware harnesses; (2) Fuzz testing combined with symbolic execution to extract input constraints; (3) Constraint combination to verify crashes and eliminate false positives. Implemented in Griller.", "result": "Griller detected 28/48 known vulnerabilities in 5 open-source projects and discovered 6 new vulnerabilities in real-world applications like Pacman. Results demonstrate improved reachability and triggerability analysis compared to prior approaches.", "conclusion": "Reactive Bottom-Up Testing enhances vulnerability detection by integrating function-level testing with context-aware validation, reducing false positives and enabling discovery of deeper vulnerabilities. This approach advances security practices for complex systems."}}
{"id": "2509.03875", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03875", "abs": "https://arxiv.org/abs/2509.03875", "authors": ["Ziyou Jiang", "Mingyang Li", "Guowei Yang", "Lin Shi", "Qing Wang"], "title": "VulRTex: A Reasoning-Guided Approach to Identify Vulnerabilities from Rich-Text Issue Report", "comment": "25 pages, 7 figures, submitting to TOSEM journal", "summary": "Software vulnerabilities exist in open-source software (OSS), and the\ndevelopers who discover these vulnerabilities may submit issue reports (IRs) to\ndescribe their details. Security practitioners need to spend a lot of time\nmanually identifying vulnerability-related IRs from the community, and the time\ngap may be exploited by attackers to harm the system. Previously, researchers\nhave proposed automatic approaches to facilitate identifying these\nvulnerability-related IRs, but these works focus on textual descriptions but\nlack the comprehensive analysis of IR's rich-text information. In this paper,\nwe propose VulRTex, a reasoning-guided approach to identify\nvulnerability-related IRs with their rich-text information. In particular,\nVulRTex first utilizes the reasoning ability of the Large Language Model (LLM)\nto prepare the Vulnerability Reasoning Database with historical IRs. Then, it\nretrieves the relevant cases from the prepared reasoning database to generate\nreasoning guidance, which guides LLM to identify vulnerabilities by reasoning\nanalysis on target IRs' rich-text information. To evaluate the performance of\nVulRTex, we conduct experiments on 973,572 IRs, and the results show that\nVulRTex achieves the highest performance in identifying the\nvulnerability-related IRs and predicting CWE-IDs when the dataset is\nimbalanced, outperforming the best baseline with +11.0% F1, +20.2% AUPRC, and\n+10.5% Macro-F1, and 2x lower time cost than baseline reasoning approaches.\nFurthermore, VulRTex has been applied to identify 30 emerging vulnerabilities\nacross 10 representative OSS projects in 2024's GitHub IRs, and 11 of them are\nsuccessfully assigned CVE-IDs, which illustrates VulRTex's practicality.", "AI": {"tldr": "VulRTex: An LLM-powered approach using reasoning and rich-text analysis to accurately identify software vulnerabilities in open-source projects, outperforming prior methods and enabling faster threat detection.", "motivation": "Manual identification of vulnerability-related issue reports is time-consuming and risky, while prior automated methods overlook rich-text contextual information critical for accurate detection.", "method": "VulRTex leverages large language models (LLMs) with a Vulnerability Reasoning Database to analyze rich-text information, combining historical case retrieval and reasoning analysis for vulnerability identification.", "result": "VulRTex outperforms baselines by +11.0% F1, +20.2% AUPRC, and +10.5% Macro-F1 with 2\u00d7 lower time cost on 973,572 IRs, and detected 11 emerging vulnerabilities with assigned CVE-IDs in 2024 GitHub projects.", "conclusion": "VulRTex demonstrates effectiveness in identifying vulnerability-related IRs through reasoning-guided methods, showing practical applicability and superior performance over existing approaches."}}
