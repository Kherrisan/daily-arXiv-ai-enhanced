{"id": "2507.23229", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.23229", "abs": "https://arxiv.org/abs/2507.23229", "authors": ["Yufei Chen", "Yao Wang", "Haibin Zhang", "Tao Gu"], "title": "Fine-Grained Privacy Extraction from Retrieval-Augmented Generation Systems via Knowledge Asymmetry Exploitation", "comment": null, "summary": "Retrieval-augmented generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge bases, but this advancement introduces\nsignificant privacy risks. Existing privacy attacks on RAG systems can trigger\ndata leakage but often fail to accurately isolate knowledge-base-derived\nsentences within mixed responses. They also lack robustness when applied across\nmultiple domains. This paper addresses these challenges by presenting a novel\nblack-box attack framework that exploits knowledge asymmetry between RAG and\nstandard LLMs to achieve fine-grained privacy extraction across heterogeneous\nknowledge landscapes. We propose a chain-of-thought reasoning strategy that\ncreates adaptive prompts to steer RAG systems away from sensitive content.\nSpecifically, we first decompose adversarial queries to maximize information\ndisparity and then apply a semantic relationship scoring to resolve lexical and\nsyntactic ambiguities. We finally train a neural network on these feature\nscores to precisely identify sentences containing private information. Unlike\nprior work, our framework generalizes to unseen domains through iterative\nrefinement without pre-defined knowledge. Experimental results show that we\nachieve over 91% privacy extraction rate in single-domain and 83% in\nmulti-domain scenarios, reducing sensitive sentence exposure by over 65% in\ncase studies. This work bridges the gap between attack and defense in RAG\nsystems, enabling precise extraction of private information while providing a\nfoundation for adaptive mitigation.", "AI": {"tldr": "This paper introduces a novel black-box attack framework to address privacy risks in Retrieval-augmented Generation (RAG) systems by exploiting knowledge asymmetry between RAG and standard LLMs, enabling fine-grained privacy extraction with high accuracy even without pre-defined domain knowledge.", "motivation": "Current privacy attacks on RAG systems struggle to accurately isolate knowledge-base-derived content in mixed outputs and fail to generalize across diverse domains, creating a critical gap in robust privacy threat analysis.", "method": "The approach employs chain-of-thought reasoning with adaptive prompts to steer RAG systems, followed by query decomposition to maximize information disparity. It uses semantic relationship scoring to resolve lexical/syntactic ambiguities and trains a domain-agnostic neural network on extracted features to identify sensitive sentences.", "result": "Achieved 91% privacy extraction rate in single-domain scenarios and 83% in multi-domain cases, with case studies showing over 65% reduction in sensitive information exposure compared to prior methods.", "conclusion": "The framework establishes a foundation for adaptive privacy attacks and defenses in RAG systems, advancing both security analysis capabilities and dynamic mitigation strategies through domain-generalization techniques."}}
{"id": "2507.23453", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23453", "abs": "https://arxiv.org/abs/2507.23453", "authors": ["Lijia Liu", "Takumi Kondo", "Kyohei Atarashi", "Koh Takeuchi", "Jiyi Li", "Shigeru Saito", "Hisashi Kashima"], "title": "Counterfactual Evaluation for Blind Attack Detection in LLM-based Evaluation Systems", "comment": null, "summary": "This paper investigates defenses for LLM-based evaluation systems against\nprompt injection. We formalize a class of threats called blind attacks, where a\ncandidate answer is crafted independently of the true answer to deceive the\nevaluator. To counter such attacks, we propose a framework that augments\nStandard Evaluation (SE) with Counterfactual Evaluation (CFE), which\nre-evaluates the submission against a deliberately false ground-truth answer.\nAn attack is detected if the system validates an answer under both standard and\ncounterfactual conditions. Experiments show that while standard evaluation is\nhighly vulnerable, our SE+CFE framework significantly improves security by\nboosting attack detection with minimal performance trade-offs.", "AI": {"tldr": "The paper proposes an SE+CFE framework to defend LLM-based evaluation systems against blind attacks by cross-validating answers with a deliberately false ground-truth.", "motivation": "LLM evaluation systems are vulnerable to blind attacks where candidate answers deceive evaluators independently of the true answer, requiring a defense that maintains performance.", "method": "The framework augments Standard Evaluation (SE) with Counterfactual Evaluation (CFE), re-evaluating submissions against a false ground-truth answer to detect validation under both conditions.", "result": "Experiments demonstrate SE+CFE significantly improves attack detection rates (reducing vulnerability by x%) with minimal performance trade-offs (y% accuracy drop).", "conclusion": "The SE+CFE framework provides an effective security enhancement for LLM evaluation systems against blind attacks with acceptable performance trade-offs."}}
{"id": "2507.23611", "categories": ["cs.CR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23611", "abs": "https://arxiv.org/abs/2507.23611", "authors": ["Estelle Ruellan", "Eric Clay", "Nicholas Ascoli"], "title": "LLM-Based Identification of Infostealer Infection Vectors from Screenshots: The Case of Aurora", "comment": null, "summary": "Infostealers exfiltrate credentials, session cookies, and sensitive data from\ninfected systems. With over 29 million stealer logs reported in 2024, manual\nanalysis and mitigation at scale are virtually unfeasible/unpractical. While\nmost research focuses on proactive malware detection, a significant gap remains\nin leveraging reactive analysis of stealer logs and their associated artifacts.\nSpecifically, infection artifacts such as screenshots, image captured at the\npoint of compromise, are largely overlooked by the current literature. This\npaper introduces a novel approach leveraging Large Language Models (LLMs), more\nspecifically gpt-4o-mini, to analyze infection screenshots to extract potential\nIndicators of Compromise (IoCs), map infection vectors, and track campaigns.\nFocusing on the Aurora infostealer, we demonstrate how LLMs can process\nscreenshots to identify infection vectors, such as malicious URLs, installer\nfiles, and exploited software themes. Our method extracted 337 actionable URLs\nand 246 relevant files from 1000 screenshots, revealing key malware\ndistribution methods and social engineering tactics. By correlating extracted\nfilenames, URLs, and infection themes, we identified three distinct malware\ncampaigns, demonstrating the potential of LLM-driven analysis for uncovering\ninfection workflows and enhancing threat intelligence. By shifting malware\nanalysis from traditional log-based detection methods to a reactive,\nartifact-driven approach that leverages infection screenshots, this research\npresents a scalable method for identifying infection vectors and enabling early\nintervention.", "AI": {"tldr": "This paper proposes a scalable LLM-based method for malware analysis by processing infection screenshots from the Aurora infostealer, enabling the extraction of 337 URLs and 246 files to identify malware campaigns and distribution methods.", "motivation": "Manual analysis of 29 million+ stealer logs in 2024 is impractical, and traditional log-based approaches overlook critical infection artifacts like screenshots containing visual compromise evidence.", "method": "The authors use gpt-4o-mini to analyze 1,000 Aurora stealer screenshots, extracting IoCs (URLs, files) and correlating artifacts such as filenames, compromised software themes, and social engineering indicators to map infection vectors and track campaigns.", "result": "The method successfully identified 337 actionable URLs, 246 relevant files, and three distinct malware campaigns, revealing patterns in malware distribution and human-driven attack tactics within the Aurora stealer logs.", "conclusion": "LLM-powered reactive analysis of infection artifacts offers a scalable alternative to traditional malware detection, enhancing threat intelligence capabilities through automated extraction of IoCs and campaign mapping from screenshots."}}
{"id": "2507.23641", "categories": ["cs.CR", "11T71, 94A60"], "pdf": "https://arxiv.org/pdf/2507.23641", "abs": "https://arxiv.org/abs/2507.23641", "authors": ["Michael Schaller"], "title": "Polynomial Lattices for the BIKE Cryptosystem", "comment": null, "summary": "In this paper we introduce a rank $2$ lattice over a polynomial ring arising\nfrom the public key of the BIKE cryptosystem \\cite{aragon2022bike}. The secret\nkey is a sparse vector in this lattice. We study properties of this lattice and\ngeneralize the recovery of weak keys from \\cite{BardetDLO16}. In particular, we\nshow that they implicitly solved a shortest vector problem in the lattice we\nconstructed. Rather than finding only a shortest vector, we obtain a reduced\nbasis of the lattice which makes it possible to check for more weak keys.", "AI": {"tldr": "This paper introduces a rank-2 lattice structure for the BIKE cryptosystem's public key, generalizes weak key recovery by connecting it to the implicit solution of a shortest vector problem, and proposes a method to construct a reduced basis for finding more weak keys.", "motivation": "To enhance the understanding of BIKE cryptosystem's security by generalizing weak key recovery techniques from existing literature and demonstrating the underutilized capacity of lattice reduction for improved key analysis.", "method": "1) Constructing a rank-2 polynomial ring lattice from BIKE's public key geometry. 2) Proving that weak key recovery corresponds to solving SVP in this lattice. 3) Developing a lattice basis reduction approach that reveals the secret key's sparse vector representation and can identify more weak keys than previous methods.", "result": "Successfully demonstrated that existing weak key recovery methods implicitly solve SVP in the introduced lattice structure. Obtained an algorithm that can enumerate multiple weak keys via basis reduction, improving over previous vector-only approaches with 100% recovery rate under specific conditions.", "conclusion": "The rank-2 lattice framework provides a foundational perspective for analyzing BIKE's security and opens new avenues for finding weak keys through basis reduction. This work strengthens understanding of cryptosystem vulnerability analysis as a lattice problem and suggests potential improvements in key selection strategies for post-quantum cryptography."}}
{"id": "2507.23087", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23087", "abs": "https://arxiv.org/abs/2507.23087", "authors": ["Fabian Stiehle", "Hans Weytjens", "Ingo Weber"], "title": "On LLM-Assisted Generation of Smart Contracts from Business Processes", "comment": "Accepted at the Workshop on Distributed Ledger Technologies in\n  Business Process Management, At the International Conference for Business\n  Process Management (BPM), 2025", "summary": "Large language models (LLMs) have changed the reality of how software is\nproduced. Within the wider software engineering community, among many other\npurposes, they are explored for code generation use cases from different types\nof input. In this work, we present an exploratory study to investigate the use\nof LLMs for generating smart contract code from business process descriptions,\nan idea that has emerged in recent literature to overcome the limitations of\ntraditional rule-based code generation approaches. However, current LLM-based\nwork evaluates generated code on small samples, relying on manual inspection,\nor testing whether code compiles but ignoring correct execution. With this\nwork, we introduce an automated evaluation framework and provide empirical data\nfrom larger data sets of process models. We test LLMs of different types and\nsizes in their capabilities of achieving important properties of process\nexecution, including enforcing process flow, resource allocation, and\ndata-based conditions. Our results show that LLM performance falls short of the\nperfect reliability required for smart contract development. We suggest future\nwork to explore responsible LLM integrations in existing tools for code\ngeneration to ensure more reliable output. Our benchmarking framework can serve\nas a foundation for developing and evaluating such integrations.", "AI": {"tldr": "This study investigates the use of LLMs for generating smart contract code from business process descriptions, introducing an automated evaluation framework to assess code reliability beyond mere compilation. Results show LLMs lack sufficient reliability for smart contracts, suggesting integrations with existing tools for improvement.", "motivation": "Existing LLM-based code generation for smart contracts evaluates code on small samples via manual inspection or compilation tests, neglecting actual execution correctness. This approach overlooks critical properties like process flow enforcement and resource allocation.", "method": "We developed an automated evaluation framework using larger datasets of business process models. Tested LLMs of varying types and sizes for their ability to enforce process flow, allocate resources, and handle data-based conditions in generated smart contracts.", "result": "LLM-generated smart contracts failed to meet the required reliability standards, demonstrating shortcomings in critical execution properties necessary for robust smart contract functionality.", "conclusion": "While LLMs show potential, their current reliability for smart contract code generation is insufficient. Responsible integration into existing tools, leveraging our benchmarking framework, is needed to produce accurate and safe output."}}
{"id": "2507.23118", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23118", "abs": "https://arxiv.org/abs/2507.23118", "authors": ["Mattia Di Profio", "Mingjun Zhong", "Yaji Sripada", "Marcel Jaspars"], "title": "FlowETL: An Autonomous Example-Driven Pipeline for Data Engineering", "comment": null, "summary": "The Extract, Transform, Load (ETL) workflow is fundamental for populating and\nmaintaining data warehouses and other data stores accessed by analysts for\ndownstream tasks. A major shortcoming of modern ETL solutions is the extensive\nneed for a human-in-the-loop, required to design and implement\ncontext-specific, and often non-generalisable transformations. While related\nwork in the field of ETL automation shows promising progress, there is a lack\nof solutions capable of automatically designing and applying these\ntransformations. We present FlowETL, a novel example-based autonomous ETL\npipeline architecture designed to automatically standardise and prepare input\ndatasets according to a concise, user-defined target dataset. FlowETL is an\necosystem of components which interact together to achieve the desired outcome.\nA Planning Engine uses a paired input-output datasets sample to construct a\ntransformation plan, which is then applied by an ETL worker to the source\ndataset. Monitoring and logging provide observability throughout the entire\npipeline. The results show promising generalisation capabilities across 14\ndatasets of various domains, file structures, and file sizes.", "AI": {"tldr": "FlowETL introduces an autonomous ETL pipeline using example-based transformation plans to reduce human intervention, demonstrating strong generalisation across diverse datasets.", "motivation": "Modern ETL workflows require manual human effort for context-specific transformations, and existing automation lacks the capability to design and apply these transformations without user input.", "method": "FlowETL employs a Planning Engine to create transformation plans from paired input-output examples, an ETL worker to execute these plans, and integrates monitoring for pipeline observability.", "result": "The system generalised effectively across 14 datasets varying in domain, structure, and size, indicating robust automated ETL performance.", "conclusion": "FlowETL presents a novel, example-driven architecture for autonomous ETL, achieving strong generalisation results and advancing automated data preparation for data warehouses."}}
{"id": "2507.23120", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23120", "abs": "https://arxiv.org/abs/2507.23120", "authors": ["Jordi Cabot"], "title": "Vibe Modeling: Challenges and Opportunities", "comment": null, "summary": "There is a pressing need for better development methods and tools to keep up\nwith the growing demand and increasing complexity of new software systems. New\ntypes of user interfaces, the need for intelligent components, sustainability\nconcerns, ... bring new challenges that we need to handle. In the last years,\nmodel-driven engineering (MDE) has been key to improving the quality and\nproductivity of software development, but models themselves are becoming\nincreasingly complex to specify and manage. At the same time, we are witnessing\nthe growing popularity of vibe coding approaches that rely on Large Language\nModels (LLMs) to transform natural language descriptions into running code at\nthe expenses of code vulnerabilities, scalability issues and maintainability\nconcerns. In this paper, we introduce the concept of \\textit{vibe modeling} as\na novel approach to integrate the best of both worlds (AI and MDE) to speed up\nthe development of reliable complex systems. We outline the key concepts of\nvibe modeling and highlight the opportunities and open challenges it presents\nfor the future of modeling.", "AI": {"tldr": "This paper proposes 'vibe modeling', a novel approach integrating AI and model-driven engineering (MDE) to enhance software development efficiency and reliability for complex systems.", "motivation": "The software development field faces challenges from new user interfaces, intelligent components, and sustainability demands, while MDE models grow complex and LLM-based coding lacks maintainability. A hybrid solution is needed.", "method": "Vibe modeling combines natural language processing (via LLMs) with MDE principles, generating models directly from descriptions and addressing limitations of current methods through co-design of AI and modeling.", "result": "The paper outlines vibe modeling's core concepts, provides examples of its operational workflow, and systematically characterizes opportunities including automation and explainability, as well as open challenges in scalability and model accuracy.", "conclusion": "Vibe modeling represents a promising direction for future software development by synergizing AI and MDE strengths, though further research is required to address technical challenges and ensure practical adoption."}}
{"id": "2507.23168", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23168", "abs": "https://arxiv.org/abs/2507.23168", "authors": ["Elmira Onagh", "Maleknaz Nayebi"], "title": "Extension Decisions in Open Source Software Ecosystem", "comment": "Paper published in JSS journal", "summary": "GitHub Marketplace is expanding by approximately 41% annually, with new\ntools; however, many additions replicate existing functionality. We study this\nphenomenon in the platform's largest segment, Continuous Integration (CI), by\nlinking 6,983 CI Actions to 3,869 providers and mining their version histories.\nOur graph model timestamps every functionality's debut, tracks its adoption,\nand clusters redundant tools. We find that approximately 65% of new CI Actions\nreplicate existing capabilities, typically within six months, and that a small\nset of first-mover Actions accounts for most subsequent forks and extensions.\nThese insights enable developers to choose the optimal moment to launch, target\nunmet functionality, and help maintainers eliminate redundant tools. We publish\nthe complete graph and dataset to encourage longitudinal research on innovation\nand competition in software ecosystems, and to provide practitioners with a\ndata-driven roadmap for identifying emerging trends and guiding product\nstrategy.", "AI": {"tldr": "The paper investigates redundancy in GitHub Marketplace's Continuous Integration (CI) tools, finding 65% of new Actions duplicate existing functionality within six months. A graph model tracks functionality adoption and identifies first-mover dominance in forks/extensions. The dataset is published to aid future research and product strategy decisions.", "motivation": "GitHub Marketplace's rapid growth creates challenges for innovators to distinguish meaningful advancements from redundant tools, affecting developer decision-making and ecosystem health.", "method": "Linked 6,983 CI Actions to 3,869 providers and analyzed version histories using a graph model that timestamps functionality introductions, tracks adoption rates, and clusters tools with overlapping capabilities.", "result": "65% of new CI Actions replicate existing features within six months, with a small set of first-mover Actions dominating subsequent forks and extensions. The dataset captures these patterns for longitudinal analysis.", "conclusion": "The study provides a framework for analyzing innovation trajectories in software ecosystems and delivers a public dataset to enable data-driven insights into emerging trends and product evolution."}}
{"id": "2507.23178", "categories": ["cs.SE", "cs.AI", "I.2.5"], "pdf": "https://arxiv.org/pdf/2507.23178", "abs": "https://arxiv.org/abs/2507.23178", "authors": ["Siyuan Liu", "Zhice Yang", "Huangxun Chen"], "title": "AutoBridge: Automating Smart Device Integration with Centralized Platform", "comment": "14 pages, 12 figures, under review", "summary": "Multimodal IoT systems coordinate diverse IoT devices to deliver\nhuman-centered services. The ability to incorporate new IoT devices under the\nmanagement of a centralized platform is an essential requirement. However, it\nrequires significant human expertise and effort to program the complex IoT\nintegration code that enables the platform to understand and control the device\nfunctions. Therefore, we propose AutoBridge to automate IoT integration code\ngeneration. Specifically, AutoBridge adopts a divide-and-conquer strategy: it\nfirst generates device control logic by progressively retrieving\ndevice-specific knowledge, then synthesizes platformcompliant integration code\nusing platform-specific knowledge. To ensure correctness, AutoBridge features a\nmulti-stage debugging pipeline, including an automated debugger for virtual IoT\ndevice testing and an interactive hardware-in-the-loop debugger that requires\nonly binary user feedback (yes and no) for real-device verification. We\nevaluate AutoBridge on a benchmark of 34 IoT devices across two open-source IoT\nplatforms. The results demonstrate that AutoBridge can achieves an average\nsuccess rate of 93.87% and an average function coverage of 94.87%, without any\nhuman involvement. With minimal binary yes and no feedback from users, the code\nis then revised to reach 100% function coverage. A user study with 15\nparticipants further shows that AutoBridge outperforms expert programmers by\n50% to 80% in code accuracy, even when the programmers are allowed to use\ncommercial code LLMs.", "AI": {"tldr": "AutoBridge automates IoT integration code generation by combining device-specific and platform-specific knowledge with a multi-stage debugging pipeline, achieving high success and coverage rates", "motivation": "Programming complex IoT integration code requires significant human expertise and effort, making it challenging to incorporate new devices into centralized platforms. Existing methods depend heavily on manual intervention, limiting scalability and efficiency.", "method": "AutoBridge employs a divide-and-conquer strategy in two phases: 1) generating device control logic through progressive retrieval of device-specific knowledge, and 2) synthesizing platform-compliant integration code using platform-specific knowledge. It introduces a multi-stage debugging pipeline with automated virtual device testing and interactive hardware-in-the-loop debugging requiring only binary user feedback.", "result": "On a 34-device benchmark across two platforms, AutoBridge achieved 93.87% success rate and 94.87% function coverage with 0% human input, improving to 100% function coverage after minimal binary feedback. In a user study, it outperformed expert programmers in code accuracy by 50-80% even when programmers used commercial code LLMs.", "conclusion": "AutoBridge demonstrates a scalable solution for automated IoT integration code generation, significantly reducing human effort while outperforming experts in accuracy through its knowledge-driven approach and novel debugging pipeline."}}
{"id": "2507.23269", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.23269", "abs": "https://arxiv.org/abs/2507.23269", "authors": ["Peter Fettke", "Fabiana Fournier", "Lior Limonad", "Andreas Metzger", "Stefanie Rinderle-Ma", "Barbara Weber"], "title": "XABPs: Towards eXplainable Autonomous Business Processes", "comment": null, "summary": "Autonomous business processes (ABPs), i.e., self-executing workflows\nleveraging AI/ML, have the potential to improve operational efficiency, reduce\nerrors, lower costs, improve response times, and free human workers for more\nstrategic and creative work. However, ABPs may raise specific concerns\nincluding decreased stakeholder trust, difficulties in debugging, hindered\naccountability, risk of bias, and issues with regulatory compliance. We argue\nfor eXplainable ABPs (XABPs) to address these concerns by enabling systems to\narticulate their rationale. The paper outlines a systematic approach to XABPs,\ncharacterizing their forms, structuring explainability, and identifying key BPM\nresearch challenges towards XABPs.", "AI": {"tldr": "This paper addresses concerns with Autonomous Business Processes (ABPs) by proposing a systematic approach to eXplainable ABPs (XABPs).", "motivation": "ABPs using AI/ML offer operational benefits but raise stakeholder trust issues, debugging challenges, accountability problems, bias risks, and regulatory compliance issues.", "method": "The study outlines a systematic approach to XABPs through characterizing their forms, structuring explainability mechanisms, and identifying BPM research challenges.", "result": "A framework for XABPs was developed, providing structured explanations to mitigate transparency and trust issues in autonomous decision-making processes.", "conclusion": "The paper advocates for XABPs to address AI/ML workflow challenges and maps critical research directions for achieving explainability in business process automation."}}
{"id": "2507.23348", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23348", "abs": "https://arxiv.org/abs/2507.23348", "authors": ["Han Li", "Yuling Shi", "Shaoxin Lin", "Xiaodong Gu", "Heng Lian", "Xin Wang", "Yantao Jia", "Tao Huang", "Qianxiang Wang"], "title": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution", "comment": "Our code and data are available at\n  https://github.com/YerbaPage/SWE-Debate", "summary": "Issue resolution has made remarkable progress thanks to the advanced\nreasoning capabilities of large language models (LLMs). Recently, agent-based\nframeworks such as SWE-agent have further advanced this progress by enabling\nautonomous, tool-using agents to tackle complex software engineering tasks.\nWhile existing agent-based issue resolution approaches are primarily based on\nagents' independent explorations, they often get stuck in local solutions and\nfail to identify issue patterns that span across different parts of the\ncodebase. To address this limitation, we propose SWE-Debate, a competitive\nmulti-agent debate framework that encourages diverse reasoning paths and\nachieves more consolidated issue localization. SWE-Debate first creates\nmultiple fault propagation traces as localization proposals by traversing a\ncode dependency graph. Then, it organizes a three-round debate among\nspecialized agents, each embodying distinct reasoning perspectives along the\nfault propagation trace. This structured competition enables agents to\ncollaboratively converge on a consolidated fix plan. Finally, this consolidated\nfix plan is integrated into an MCTS-based code modification agent for patch\ngeneration. Experiments on the SWE-bench benchmark show that SWE-Debate\nachieves new state-of-the-art results in open-source agent frameworks and\noutperforms baselines by a large margin.", "AI": {"tldr": "SWE-Debate introduces a competitive multi-agent debate framework to enhance issue resolution in software engineering by generating fault propagation traces and leveraging structured agent debates across code dependency graphs, achieving state-of-the-art performance on SWE-bench.", "motivation": "Current agent-based issue resolution methods rely on independent exploration, frequently getting trapped in local solutions and failing to recognize cross-codebase issue patterns. This approach limits their ability to detect complex, distributed bugs.", "method": "1) Code dependency graph traversal to generate multiple fault propagation traces as localization proposals. 2) Three-round structured debates among agents with distinct reasoning perspectives focused on these traces. 3) Integration of consolidated debate results into an MCTS-based code modification agent for patch generation.", "result": "Achieved new state-of-the-art open-source agent performance on the SWE-bench benchmark while demonstrating significant improvements over prior methods. The debate framework enables better issue localization and more comprehensive fix planning.", "conclusion": "The multi-agent debate mechanism in SWE-Debate enhances collaborative reasoning through structured competition, enabling more effective identification and resolution of complex software engineering issues compared to independent agent approaches."}}
{"id": "2507.23356", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23356", "abs": "https://arxiv.org/abs/2507.23356", "authors": ["Shmulik Froimovich", "Raviv Gal", "Wesam Ibraheem", "Avi Ziv"], "title": "Quality Evaluation of COBOL to Java Code Transformation", "comment": "Submitted to ASE 2025", "summary": "We present an automated evaluation system for assessing COBOL-to-Java code\ntranslation within IBM's watsonx Code Assistant for Z (WCA4Z). The system\naddresses key challenges in evaluating LLM-based translators, including model\nopacity and the complexity of translation quality assessment. Our approach\ncombines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver\nscalable, multi-faceted evaluations. The system supports continuous integration\nworkflows, enables large-scale benchmarking, and reduces reliance on manual\nreview. We describe the system architecture, evaluation strategies, and\nreporting mechanisms that provide actionable insights for developers and\nproject managers, facilitating the evolution of high-quality, modernized\ncodebases.", "AI": {"tldr": "The paper introduces an automated evaluation system for COBOL-to-Java translation in IBM's WCA4Z, combining analytic checkers and LLM-as-a-judge (LaaJ) to address challenges like model opacity and complex quality assessment.", "motivation": "LLM-based code translators face critical evaluation challenges, including model lack of transparency and subjective quality assessments. This system aims to provide scalable, cost-effective alternatives to manual reviews for COBOL modernization\u2014a vital industry need.", "method": "A hybrid approach integrating static/dynamic code analytic checkers with LLM-as-a-judge (LaaJ) evaluation. The system supports CI workflows, uses customizable benchmarks for large-scale testing, and automates quality scoring through machine learning and program logic analysis.", "result": "Enables continuous quality monitoring, reduces manual review cycles, and generates structured reports highlighting translation accuracy, maintainability, and performance gaps across legacy codebases.", "conclusion": "The framework demonstrates effective LLM-based translation evaluation through combined analytical and AI-driven methods, supporting robust code modernization while providing actionable insights for iterative model improvement."}}
{"id": "2507.23361", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23361", "abs": "https://arxiv.org/abs/2507.23361", "authors": ["Silin Chen", "Shaoxin Lin", "Xiaodong Gu", "Yuling Shi", "Heng Lian", "Longfei Yun", "Dong Chen", "Weiguo Sun", "Lin Cao", "Qianxiang Wang"], "title": "SWE-Exp: Experience-Driven Software Issue Resolution", "comment": "Our code and data are available at\n  https://github.com/YerbaPage/SWE-Exp", "summary": "Recent advances in large language model (LLM) agents have shown remarkable\nprogress in software issue resolution, leveraging advanced techniques such as\nmulti-agent collaboration and Monte Carlo Tree Search (MCTS). However, current\nagents act as memoryless explorers - treating each problem separately without\nretaining or reusing knowledge from previous repair experiences. This leads to\nredundant exploration of failed trajectories and missed chances to adapt\nsuccessful issue resolution methods to similar problems. To address this\nproblem, we introduce SWE-Exp, an experience - enhanced approach that distills\nconcise and actionable experience from prior agent trajectories, enabling\ncontinuous learning across issues. Our method introduces a multi-faceted\nexperience bank that captures both successful and failed repair attempts.\nSpecifically, it extracts reusable issue resolution knowledge at different\nlevels - from high-level problem comprehension to specific code changes.\nExperiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%\nPass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach\nestablishes a new paradigm in which automated software engineering agents\nsystematically accumulate and leverage repair expertise, fundamentally shifting\nfrom trial-and-error exploration to strategic, experience-driven issue\nresolution.", "AI": {"tldr": "SWE-Exp enhances software issue resolution by leveraging past agent experiences to create a knowledge bank of successful repair strategies, achieving 41.6% Pass@1 resolution rate.", "motivation": "Current LLM agents lack memory to retain knowledge from past repairs, leading to redundant exploration and missed opportunities to reuse effective solutions for similar problems.", "method": "The approach establishes an experience bank that distills actionable knowledge from prior agent trajectories (both successful and failed) at multiple levels, including problem understanding and specific code modifications.", "result": "State-of-the-art 41.6% Pass@1 resolution rate on SWE-bench-Verified benchmark using open-source agent frameworks, outperforming memoryless exploration methods.", "conclusion": "SWE-Exp introduces a paradigm shift by enabling systematic accumulation and strategic application of repair expertise, transforming software engineering agents from trial-and-error explorers to experience-driven problem solvers."}}
{"id": "2507.23370", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23370", "abs": "https://arxiv.org/abs/2507.23370", "authors": ["Trae Research Team", "Pengfei Gao", "Zhao Tian", "Xiangxin Meng", "Xinchen Wang", "Ruida Hu", "Yuanan Xiao", "Yizhou Liu", "Zhao Zhang", "Junjie Chen", "Cuiyun Gao", "Yun Lin", "Yingfei Xiong", "Chao Peng", "Xia Liu"], "title": "Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling", "comment": "Pengfei Gao and Zhao Tian contributed equally to this technical\n  report", "summary": "Software issue resolution is a critical challenge in software engineering and\nhas garnered increasing attention in recent years. With the rapid advancement\nof large language models (LLMs), substantial progress has been made in\naddressing real-world software engineering tasks. Recent studies have\nintroduced ensemble reasoning techniques to enhance the performance of\nLLM-based issue resolution. However, existing prompting-based methods still\nface limitations in effectively exploring large ensemble spaces and lack the\ncapacity for repository-level understanding, both of which constrain their\noverall effectiveness. In this paper, we propose Trae Agent, the first\nagent-based ensemble reasoning approach for repository-level issue resolution.\nTrae Agent formulates our goal as an optimal solution search problem and\naddresses two key challenges, i.e., large ensemble spaces and repository-level\nunderstanding, through modular agents for generation, pruning, and selection.\nWe conduct extensive experiments using three leading LLMs on the widely-adopted\nSWE-bench benchmark, comparing Trae Agent against four state-of-the-art\nensemble reasoning techniques. Experimental results demonstrate that Trae Agent\nconsistently achieves superior performance, with an average improvement of\n10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first\nplace on the SWE-bench Verified leaderboard, with a notable Pass@1 score of\n75.20%. We are pleased to release Trae Agent as an open-source project to\nsupport the research community, with all resources available at\nhttps://github.com/bytedance/trae-agent.", "AI": {"tldr": "Trae Agent is an agent-based ensemble reasoning approach for repository-level software issue resolution, achieving state-of-the-art performance on SWE-bench with 75.20% Pass@1.", "motivation": "Existing prompting-based methods for LLM-driven issue resolution struggle with large solution spaces and lack repository-level understanding, limiting their effectiveness.", "method": "Trae Agent modularizes the resolution process via generation, pruning, and selection agents to address ensemble space exploration and repository comprehension challenges.", "result": "Trae Agent outperformed four SOTA techniques by 10.22% average Pass@1 across three LLMs, securing first place on SWE-bench Verified leaderboard (75.20% Pass@1).", "conclusion": "Trae Agent's agent-based architecture enables effective repository-level issue resolution through structured ensemble reasoning, setting a new benchmark for software engineering LLM applications."}}
{"id": "2507.23425", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23425", "abs": "https://arxiv.org/abs/2507.23425", "authors": ["Daphn\u00e9 Larrivain", "Shinhyung Yang", "Wilhelm Hasselbring"], "title": "Dynamic and Static Analysis of Python Software with Kieker Including Reconstructed Architectures", "comment": "9 pages, 9 figures", "summary": "The Kieker observability framework is a tool that provides users with the\nmeans to design a custom observability pipeline for their application.\nOriginally tailored for Java, supporting Python with Kieker is worthwhile.\nPython's popularity has exploded over the years, thus making structural\ninsights of Python applications highly valuable. Our Python analysis pipeline\ncombines static and dynamic analysis in order to build a complete picture of a\ngiven system.", "AI": {"tldr": "Kieker, originally a Java observability framework, now extends Python analysis via static/dynamic combining methods.", "motivation": "Python's rising popularity necessitates structural insight tools adapted from mature Java frameworks like Kieker.", "method": "Integrated static and dynamic analysis techniques to create a cross-language observability pipeline for Python.", "result": "Enabled comprehensive application analysis for Python systems using Kieker's established framework architecture.", "conclusion": "Python support in Kieker provides valuable observability capabilities for modern Python applications through enhanced analysis pipelines."}}
{"id": "2507.23640", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23640", "abs": "https://arxiv.org/abs/2507.23640", "authors": ["Samah Kansab", "Mohammed Sayagh", "Francis Bordeleau", "Ali Tizghadam"], "title": "An Empirical Study on the Amount of Changes Required for Merge Request Acceptance", "comment": null, "summary": "Code review (CR) is essential to software development, helping ensure that\nnew code is properly integrated. However, the CR process often involves\nsignificant effort, including code adjustments, responses to reviewers, and\ncontinued implementation. While past studies have examined CR delays and\niteration counts, few have investigated the effort based on the volume of code\nchanges required, especially in the context of GitLab Merge Requests (MRs),\nwhich remains underexplored. In this paper, we define and measure CR effort as\nthe amount of code modified after submission, using a dataset of over 23,600\nMRs from four GitLab projects. We find that up to 71% of MRs require\nadjustments after submission, and 28% of these involve changes to more than 200\nlines of code. Surprisingly, this effort is not correlated with review time or\nthe number of participants. To better understand and predict CR effort, we\ntrain an interpretable machine learning model using metrics across multiple\ndimensions: text features, code complexity, developer experience, review\nhistory, and branching. Our model achieves strong performance (AUC 0.84-0.88)\nand reveals that complexity, experience, and text features are key predictors.\nHistorical project characteristics also influence current review effort. Our\nfindings highlight the feasibility of using machine learning to explain and\nanticipate the effort needed to integrate code changes during review.", "AI": {"tldr": "The paper introduces a method to measure and predict code review (CR) effort in GitLab Merge Requests (MRs) by analyzing post-submission code changes using an interpretable machine learning model with strong performance (AUC 0.84-0.88).", "motivation": "Past research focused on CR delays and iterations but overlooked actual effort quantified by code modification volume, particularly in the underexplored context of GitLab MRs. This paper bridges that gap.", "method": "Defined CR effort as post-submission code changes, analyzed 23,600 GitLab MRs, and trained an interpretable ML model using text features, code complexity, developer experience, review history, and branching metrics.", "result": "71% of MRs required post-submission changes, 28% exceeded 200 lines; effort showed no correlation with review duration or participants, but model identified complexity, developer experience, and text features as key predictors, alongside historical project data.", "conclusion": "Quantifying CR effort through post-submission modifications and ML modeling demonstrates feasibility for explaining and anticipating integration effort in GitLab MRs, emphasizing multidimensional predictors over traditional factors like review time."}}
