{"id": "2508.20119", "categories": ["cs.SE", "cs.LG", "68T42", "I.2.6; I.2.2; D.2.2"], "pdf": "https://arxiv.org/pdf/2508.20119", "abs": "https://arxiv.org/abs/2508.20119", "authors": ["Daniel M. Yellin"], "title": "Evaluating LLMs on microservice-based applications: how complex is your specification?", "comment": "20 pages + 7 pages appendices. 7 Figures. 8 Tables", "summary": "In this paper we evaluate how far LLMs have advanced in generating code for\nreal-world problems. Specifically, we explore code synthesis for\nmicroservice-based applications, a widely used architecture pattern. We define\na standard template for specifying these applications, and we propose a metric\nfor judging the difficulty level of a specification. The higher the score, the\nmore difficult it is to generate code for the specification. We develop a\nframework to automate the process of testing LLM-synthesized code for a\nmicroservice using unit tests. Our experimental results show that strong LLMs\n(like GPT-3o-mini) do fairly well on medium difficulty specifications but do\nvery poorly on those of higher difficulty levels. This is due to more intricate\nbusiness logic, a greater use of external services, database integration and\ninclusion of non-functional capabilities such as authentication. We analyzed\nthe errors in LLM-synthesized code and report on the key challenges LLMs face\nin generating code for these specifications thereby suggesting future research\ndirections to improve code synthesis for real-world problems.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.20124", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20124", "abs": "https://arxiv.org/abs/2508.20124", "authors": ["Yunlong Feng", "Yang Xu", "Xiao Xu", "Binyuan Hui", "Junyang Lin"], "title": "Towards Better Correctness and Efficiency in Code Generation", "comment": null, "summary": "While code large language models have demonstrated remarkable progress in\ncode generation, the generated code often exhibits poor runtime efficiency,\nlimiting its practical application in performance-sensitive scenarios. To\naddress this limitation, we propose an efficiency-oriented reinforcement\nlearning framework guided by a novel performance reward. Based on this\nframework, we take a deeper dive into the code efficiency problem, identifying\nthen proposing methods to overcome key bottlenecks: (1) Dynamic exploration\novercomes the static data constraints of offline fine-tuning, enabling the\ndiscovery of more efficient code implementations. (2) The error-insensitive\nreinforcement learning method and high-contrast efficiency signals are crucial\nfor mitigating systematic errors and achieving effective optimization. (3)\nOnline exploration is most effective when starting from a high-correctness\nbaseline, as this allows for efficiency improvements without sacrificing\naccuracy. With these discoveries, we finally propose a two-stage tuning method,\nwhich achieves high and balanced performance across correctness and efficiency.\nThe results of experiments show the effectiveness of the method, which improves\ncode correctness by 10.18\\% and runtime efficiency by 7.75\\% on a 7B model,\nachieving performance comparable to much larger model.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.20340", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.20340", "abs": "https://arxiv.org/abs/2508.20340", "authors": ["Maolin Sun", "Yibiao Yang", "Yuming Zhou"], "title": "Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators", "comment": null, "summary": "Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems\nand programming languages research, providing the foundation for tasks like\nsymbolic execution and automated verification. Because these solvers sit on the\ncritical path, their correctness is essential, and high-quality test formulas\nare key to uncovering bugs. However, while prior testing techniques performed\nwell on earlier solver versions, they struggle to keep pace with rapidly\nevolving features. Recent approaches based on Large Language Models (LLMs) show\npromise in exploring advanced solver capabilities, but two obstacles remain:\nnearly half of the generated formulas are syntactically invalid, and iterative\ninteractions with the LLMs introduce substantial computational overhead. In\nthis study, we present Chimera, a novel LLM-assisted fuzzing framework that\naddresses both issues by shifting from direct formula generation to the\nsynthesis of reusable term (i.e., logical expression) generators. Particularly,\nChimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for\nSMT theories, including solver-specific extensions, from documentation, and (2)\nsynthesize composable Boolean term generators that adhere to these grammars.\nDuring fuzzing, Chimera populates structural skeletons derived from existing\nformulas with the terms iteratively produced by the LLM-synthesized generators.\nThis design ensures syntactic validity while promoting semantic diversity.\nNotably, Chimera requires only one-time LLM interaction investment,\ndramatically reducing runtime cost. We evaluated Chimera on two leading SMT\nsolvers: Z3 and cvc5. Our experiments show that Chimera has identified 43\nconfirmed bugs, 40 of which have already been fixed by developers.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.20370", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20370", "abs": "https://arxiv.org/abs/2508.20370", "authors": ["Lingzhe Zhang", "Tong Jia", "Kangjin Wang", "Weijie Hong", "Chiming Duan", "Minghua He", "Ying Li"], "title": "Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought", "comment": null, "summary": "As contemporary microservice systems become increasingly popular and\ncomplex-often comprising hundreds or even thousands of fine-grained,\ninterdependent subsystems-they are facing more frequent failures. Ensuring\nsystem reliability thus demands accurate root cause localization. While traces\nand metrics have proven to be effective data sources for this task, existing\nmethods either heavily rely on pre-defined schemas, which struggle to adapt to\nevolving operational contexts, or lack interpretability in their reasoning\nprocess, thereby leaving Site Reliability Engineers (SREs) confused. In this\npaper, we conduct a comprehensive study on how SREs localize the root cause of\nfailures, drawing insights from multiple professional SREs across different\norganizations. Our investigation reveals that human root cause analysis\nexhibits three key characteristics: recursiveness, multi-dimensional expansion,\nand cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,\nan adaptive root cause localization method for microservice systems that\nleverages a multi-agent recursion-of-thought framework. RCLAgent employs a\nnovel recursion-of-thought strategy to guide the LLM's reasoning process,\neffectively integrating data from multiple agents and tool-assisted analysis to\naccurately pinpoint the root cause. Experimental evaluations on various public\ndatasets demonstrate that RCLAgent achieves superior performance by localizing\nthe root cause using only a single request-outperforming state-of-the-art\nmethods that depend on aggregating multiple requests. These results underscore\nthe effectiveness of RCLAgent in enhancing the efficiency and precision of root\ncause localization in complex microservice environments.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.20186", "categories": ["cs.CR", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.20186", "abs": "https://arxiv.org/abs/2508.20186", "authors": ["Lukasz Olejnik"], "title": "AI Propaganda factories with language models", "comment": null, "summary": "AI-powered influence operations can now be executed end-to-end on commodity\nhardware. We show that small language models produce coherent, persona-driven\npolitical messaging and can be evaluated automatically without human raters.\nTwo behavioural findings emerge. First, persona-over-model: persona design\nexplains behaviour more than model identity. Second, engagement as a stressor:\nwhen replies must counter-arguments, ideological adherence strengthens and the\nprevalence of extreme content increases. We demonstrate that fully automated\ninfluence-content production is within reach of both large and small actors.\nConsequently, defence should shift from restricting model access towards\nconversation-centric detection and disruption of campaigns and coordination\ninfrastructure. Paradoxically, the very consistency that enables these\noperations also provides a detection signature.", "AI": {"tldr": "AI-powered influence operations can be executed with small language models on regular hardware, revealing two behavioral patterns: persona design impacts output more than model identity, and counter-argument requirements increase ideological adherence and extreme content. Defenses should shift toward conversation-centric detection.", "motivation": "Current AI influence operations on commodity hardware can produce compelling political messaging with small models, but previous defenses focused on model access rather than campaign mechanisms. This shifts defense strategy to address automated influence content at scale.", "method": "The paper evaluates small language models' ability to generate persona-driven political messaging using automated metrics, testing how different personas affect output and analyzing behavioral shifts when models must produce counter-arguments.", "result": "1. Persona design explains model behavior better than the base model's architecture. 2. Engagement requirements (counter-argument generation) lead to stronger ideological alignment and increased extreme content production. 3. Both state and non-state actors can achieve automated influence operations.", "conclusion": "Defenses must evolve to detect and disrupt conversation-level patterns in automated campaigns. The consistency of AI-generated content creates detectable signatures that can counteract its influence capabilities."}}
{"id": "2508.20563", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20563", "abs": "https://arxiv.org/abs/2508.20563", "authors": ["Zheying Zhang", "Tomas Herda", "Victoria Pichler", "Pekka Abrahamsson", "Geir K. Hanssen", "Joshua Kerievsky", "Alex Polyakov", "Mohit Chandna", "Marius Irgens", "Kai-Kristian Kemell", "Ayman Asad Khan", "Crystal Kwok", "Evan Leybourn", "Munish Malik", "Dorota Mleczko", "Morteza Moalagh", "Christopher Morales", "Yuliia Pieskova", "Daniel Plan\u00f6tscher", "Mika Saari", "Anastasiia Tkalich", "Karl Josef Gstettner", "Xiaofeng Wang"], "title": "AI and Agile Software Development: A Research Roadmap from the XP2025 Workshop", "comment": null, "summary": "This paper synthesizes the key findings from a full-day XP2025 workshop on\n\"AI and Agile: From Frustration to Success\", held in Brugg-Windisch,\nSwitzerland. The workshop brought together over 30 interdisciplinary academic\nresearchers and industry practitioners to tackle the concrete challenges and\nemerging opportunities at the intersection of Generative Artificial\nIntelligence (GenAI) and agile software development. Through structured,\ninteractive breakout sessions, participants identified shared pain points like\ntool fragmentation, governance, data quality, and critical skills gaps in AI\nliteracy and prompt engineering. These issues were further analyzed, revealing\nunderlying causes and cross-cutting concerns. The workshop concluded by\ncollaboratively co-creating a multi-thematic research roadmap, articulating\nboth short-term, implementable actions and visionary, long-term research\ndirections. This cohesive agenda aims to guide future investigation and drive\nthe responsible, human-centered integration of GenAI into agile practices.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.20212", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20212", "abs": "https://arxiv.org/abs/2508.20212", "authors": ["Minghao Hu", "Junzhe Wang", "Weisen Zhao", "Qiang Zeng", "Lannan Luo"], "title": "FlowMalTrans: Unsupervised Binary Code Translation for Malware Detection Using Flow-Adapter Architecture", "comment": "This paper is accepted to EMNLP 2025 Findings", "summary": "Applying deep learning to malware detection has drawn great attention due to\nits notable performance. With the increasing prevalence of cyberattacks\ntargeting IoT devices, there is a parallel rise in the development of malware\nacross various Instruction Set Architectures (ISAs). It is thus important to\nextend malware detection capacity to multiple ISAs. However, training a deep\nlearning-based malware detection model usually requires a large number of\nlabeled malware samples. The process of collecting and labeling sufficient\nmalware samples to build datasets for each ISA is labor-intensive and\ntime-consuming. To reduce the burden of data collection, we propose to leverage\nthe ideas of Neural Machine Translation (NMT) and Normalizing Flows (NFs) for\nmalware detection. Specifically, when dealing with malware in a certain ISA, we\ntranslate it to an ISA with sufficient malware samples (like X86-64). This\nallows us to apply a model trained on one ISA to analyze malware from another\nISA. Our approach reduces the data collection effort by enabling malware\ndetection across multiple ISAs using a model trained on a single ISA.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.20737", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20737", "abs": "https://arxiv.org/abs/2508.20737", "authors": ["Wei Ma", "Yixiao Yang", "Qiang Hu", "Shi Ying", "Zhi Jin", "Bo Du", "Zhenchang Xing", "Tianlin Li", "Junjie Shi", "Yang Liu", "Linxiao Jiang"], "title": "Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol", "comment": null, "summary": "Applications of Large Language Models~(LLMs) have evolved from simple text\ngenerators into complex software systems that integrate retrieval augmentation,\ntool invocation, and multi-turn interactions. Their inherent non-determinism,\ndynamism, and context dependence pose fundamental challenges for quality\nassurance. This paper decomposes LLM applications into a three-layer\narchitecture: \\textbf{\\textit{System Shell Layer}}, \\textbf{\\textit{Prompt\nOrchestration Layer}}, and \\textbf{\\textit{LLM Inference Core}}. We then assess\nthe applicability of traditional software testing methods in each layer:\ndirectly applicable at the shell layer, requiring semantic reinterpretation at\nthe orchestration layer, and necessitating paradigm shifts at the inference\ncore. A comparative analysis of Testing AI methods from the software\nengineering community and safety analysis techniques from the AI community\nreveals structural disconnects in testing unit abstraction, evaluation metrics,\nand lifecycle management. We identify four fundamental differences that\nunderlie 6 core challenges. To address these, we propose four types of\ncollaborative strategies (\\emph{Retain}, \\emph{Translate}, \\emph{Integrate},\nand \\emph{Runtime}) and explore a closed-loop, trustworthy quality assurance\nframework that combines pre-deployment validation with runtime monitoring.\nBased on these strategies, we offer practical guidance and a protocol proposal\nto support the standardization and tooling of LLM application testing. We\npropose a protocol \\textbf{\\textit{Agent Interaction Communication Language}}\n(AICL) that is used to communicate between AI agents. AICL has the\ntest-oriented features and is easily integrated in the current agent framework.", "AI": {"tldr": "The paper introduces a three-layer architecture for LLM applications (System Shell, Prompt Orchestration, LLM Inference) and proposes the Agent Interaction Communication Language (AICL) to enable testing across layers, combining traditional methods with new AI safety approaches.", "motivation": "LLM applications are no longer just text generators but complex systems integrating multiple capabilities, making their non-determinism, dynamism, and context dependence problematic for quality assurance. Effective testing strategies must adapt to these new challenges.", "method": "The authors decompose LLM applications into three layers (System Shell, Prompt Orchestration, LLM Inference) and evaluate how traditional software testing methods can be applied in each. They compare existing testing approaches from software engineering and AI safety communities, identifying key differences, and propose collaborative strategies and a protocol for standardized testing.", "result": "The study identifies 6 core challenges due to 4 fundamental differences in traditional and AI testing approaches. It proposes four collaborative strategies and a closed-loop QA framework with pre-deployment validation and runtime monitoring. They also introduce AICL, a test-centric communication protocol for AI agents that integrates well into current architectures.", "conclusion": "Quality assurance for LLM applications requires a layered approach with tailored testing methods for each layer and significant collaboration between software and AI communities. The proposed strategies, framework, and AICL protocol offer a foundation for standardization, but further research on practical implementation remains important."}}
{"id": "2508.20228", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20228", "abs": "https://arxiv.org/abs/2508.20228", "authors": ["Xia Han", "Qi Li", "Jianbing Ni", "Mohammad Zulkernine"], "title": "Robustness Assessment and Enhancement of Text Watermarking for Google's SynthID", "comment": "submitted to TrustCom2025", "summary": "Recent advances in LLM watermarking methods such as SynthID-Text by Google\nDeepMind offer promising solutions for tracing the provenance of AI-generated\ntext. However, our robustness assessment reveals that SynthID-Text is\nvulnerable to meaning-preserving attacks, such as paraphrasing, copy-paste\nmodifications, and back-translation, which can significantly degrade watermark\ndetectability. To address these limitations, we propose SynGuard, a hybrid\nframework that combines the semantic alignment strength of Semantic Information\nRetrieval (SIR) with the probabilistic watermarking mechanism of SynthID-Text.\nOur approach jointly embeds watermarks at both lexical and semantic levels,\nenabling robust provenance tracking while preserving the original meaning.\nExperimental results across multiple attack scenarios show that SynGuard\nimproves watermark recovery by an average of 11.1\\% in F1 score compared to\nSynthID-Text. These findings demonstrate the effectiveness of semantic-aware\nwatermarking in resisting real-world tampering. All code, datasets, and\nevaluation scripts are publicly available at:\nhttps://github.com/githshine/SynGuard.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.20744", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20744", "abs": "https://arxiv.org/abs/2508.20744", "authors": ["Shabnam Hassani", "Mehrdad Sabetzadeh", "Daniel Amyot"], "title": "From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of LLM-Generated Behavioural Specifications from Food-Safety Regulations", "comment": null, "summary": "Context: Laws and regulations increasingly affect software design and quality\nassurance, but legal texts are written in technology-neutral language. This\ncreates challenges for engineers who must develop compliance artifacts such as\nrequirements and acceptance criteria. Manual creation is labor-intensive,\nerror-prone, and requires domain expertise. Advances in Generative AI (GenAI),\nespecially Large Language Models (LLMs), offer a way to automate deriving such\nartifacts.\n  Objective: We present the first systematic human-subject study of LLMs'\nability to derive behavioral specifications from legal texts using a\nquasi-experimental design. These specifications translate legal requirements\ninto a developer-friendly form.\n  Methods: Ten participants evaluated specifications generated from food-safety\nregulations by Claude and Llama. Using Gherkin, a structured BDD language, 60\nspecifications were produced. Each participant assessed 12 across five\ncriteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each\nspecification was reviewed by two participants, yielding 120 assessments.\n  Results: For Relevance, 75% of ratings were highest and 20% second-highest.\nClarity reached 90% highest. Completeness: 75% highest, 19% second.\nSingularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No\nlowest ratings occurred. Mann-Whitney U tests showed no significant differences\nacross participants or models. Llama slightly outperformed Claude in Clarity,\nCompleteness, and Time Savings, while Claude was stronger in Singularity.\nFeedback noted hallucinations and omissions but confirmed the utility of the\nspecifications.\n  Conclusion: LLMs can generate high-quality Gherkin specifications from legal\ntexts, reducing manual effort and providing structured artifacts useful for\nimplementation, assurance, and test generation.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.20282", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20282", "abs": "https://arxiv.org/abs/2508.20282", "authors": ["Hyejun Jeong", "Mohammadreze Teymoorianfard", "Abhinav Kumar", "Amir Houmansadr", "Eugene Badasarian"], "title": "Network-Level Prompt and Trait Leakage in Local Research Agents", "comment": "under review", "summary": "We show that Web and Research Agents (WRAs) -- language model-based systems\nthat investigate complex topics on the Internet -- are vulnerable to inference\nattacks by passive network adversaries such as ISPs. These agents could be\ndeployed \\emph{locally} by organizations and individuals for privacy, legal, or\nfinancial purposes. Unlike sporadic web browsing by humans, WRAs visit\n$70{-}140$ domains with distinguishable timing correlations, enabling unique\nfingerprinting attacks.\n  Specifically, we demonstrate a novel prompt and user trait leakage attack\nagainst WRAs that only leverages their network-level metadata (i.e., visited IP\naddresses and their timings). We start by building a new dataset of WRA traces\nbased on user search queries and queries generated by synthetic personas. We\ndefine a behavioral metric (called OBELS) to comprehensively assess similarity\nbetween original and inferred prompts, showing that our attack recovers over\n73\\% of the functional and domain knowledge of user prompts. Extending to a\nmulti-session setting, we recover up to 19 of 32 latent traits with high\naccuracy. Our attack remains effective under partial observability and noisy\nconditions. Finally, we discuss mitigation strategies that constrain domain\ndiversity or obfuscate traces, showing negligible utility impact while reducing\nattack effectiveness by an average of 29\\%.", "AI": {"tldr": "This paper reveals that Web and Research Agents (WRAs), despite being deployed locally for privacy, are vulnerable to passive adversary inference attacks through network metadata. By creating a dataset of WRA traces and a behavioral metric (OBELS), the authors demonstrate 73% prompt knowledge recovery and 19/32 user trait inferences. They also propose mitigations that reduce attack effectiveness by 29% with negligible utility loss.", "motivation": "WRAs, designed for local use to protect privacy and comply with legal/financial constraints, may still inadvertently expose sensitive information via network patterns distinguishable from human web browsing. This exposure risks privacy, especially as these agents scale in complexity and adoption.", "method": "1. Constructed a dataset of WRA traces from user queries and synthetic personas. 2. Developed the OBELS metric to assess prompt similarity. 3. Designed an inference attack requiring minimal network-level metadata (IP address timings). 4. Evaluated performance in multi-session scenarios and noisy conditions. 5. Tested mitigation strategies focused on modifying access patterns while preserving utility.", "result": "1. 73% functional/domain knowledge recovery from single-session traces. 2. 19 of 32 latent user traits inferred with high accuracy under multi-session settings. 3. Attack persistence in partial observability and noisy environments. 4. Mitigations (domain diversity reduction, trace obfuscation) reduced attack efficacy by 29% without significant utility degradation.", "conclusion": "Local deployment does not inherently secure WRAs against inference attacks. Unique network traffic patterns enable trait and prompt recovery even when agents aren't accessed externally. Mitigation approaches can significantly reduce risks at low utility cost, guiding future WRA design to address these operational privacy limitations."}}
{"id": "2508.20774", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20774", "abs": "https://arxiv.org/abs/2508.20774", "authors": ["Markus Funke", "Patricia Lago"], "title": "Towards an Architectural Perspective for Sustainability: Bundle the Needs from Industry", "comment": null, "summary": "Sustainability is increasingly recognized as an emerging quality property in\nsoftware-intensive systems, yet architects lack structured guidance to address\nit effectively throughout the software design phase. Architectural\nperspectives-an architectural knowledge artifact composed of concerns,\nactivities, tactics, pitfalls, and checklists-offer a promising approach to\ntackle such emerging quality properties across architectural views and are also\nindependent of architecture frameworks and industry contexts. In this paper, we\npresent a sustainability perspective vision, i.e., a revised notion of\narchitectural perspective meant to be filled with its own elements to target\nsustainability concerns. We formulate our sustainability perspective vision\nthrough evidence from applying snowballing to seminal literature and from\nconducting a focus group with experts in the field. Our findings confirm the\nrelevance of the different perspective elements in practice and highlight\nimplications for shaping a sustainability perspective that meets industrial\nneeds.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.20307", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20307", "abs": "https://arxiv.org/abs/2508.20307", "authors": ["Michael R Smith", "Joe Ingram"], "title": "Surveying the Operational Cybersecurity and Supply Chain Threat Landscape when Developing and Deploying AI Systems", "comment": "11 pages, 5 figures", "summary": "The rise of AI has transformed the software and hardware landscape, enabling\npowerful capabilities through specialized infrastructures, large-scale data\nstorage, and advanced hardware. However, these innovations introduce unique\nattack surfaces and objectives which traditional cybersecurity assessments\noften overlook. Cyber attackers are shifting their objectives from conventional\ngoals like privilege escalation and network pivoting to manipulating AI outputs\nto achieve desired system effects, such as slowing system performance, flooding\noutputs with false positives, or degrading model accuracy. This paper serves to\nraise awareness of the novel cyber threats that are introduced when\nincorporating AI into a software system. We explore the operational\ncybersecurity and supply chain risks across the AI lifecycle, emphasizing the\nneed for tailored security frameworks to address evolving threats in the\nAI-driven landscape. We highlight previous exploitations and provide insights\nfrom working in this area. By understanding these risks, organizations can\nbetter protect AI systems and ensure their reliability and resilience.", "AI": {"tldr": "The paper discusses new cybersecurity threats introduced by AI and emphasizes the need for specialized security frameworks to address them.", "motivation": "The integration of AI into software and hardware systems has changed the cybersecurity landscape, creating new attack surfaces that traditional assessments do not cover. Cyber attackers now target AI outputs to affect system performance and reliability, rather than sticking to conventional objectives.", "method": "The paper analyzes operational cybersecurity and supply chain risks across the entire AI lifecycle, discusses past exploitations, and offers insights from hands-on work in the field.", "result": "The analysis reveals how attackers can manipulate AI systems to degrade performance, increase false positives, and reduce model accuracy, while identifying the shortcomings of existing cybersecurity practices in relation to AI risks.", "conclusion": "Organizations must develop tailored security frameworks to counter AI-specific threats and enhance the resilience and trustworthiness of AI-driven systems."}}
{"id": "2508.20902", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20902", "abs": "https://arxiv.org/abs/2508.20902", "authors": ["Baharin A. Jodat", "Khouloud Gaaloul", "Mehrdad Sabetzadeh", "Shiva Nejati"], "title": "Automated Test Oracles for Flaky Cyber-Physical System Simulators: Approach and Evaluation", "comment": null, "summary": "Simulation-based testing of cyber-physical systems (CPS) is costly due to the\ntime-consuming execution of CPS simulators. In addition, CPS simulators may be\nflaky, leading to inconsistent test outcomes and requiring repeated test\nre-execution for reliable test verdicts. Automated test oracles that do not\nrequire system execution are therefore crucial for reducing testing costs.\nIdeally, such test oracles should be interpretable to facilitate human\nunderstanding of test verdicts, and they must be robust against the potential\nflakiness of CPS simulators. In this article, we propose assertion-based test\noracles for CPS as sets of logical and arithmetic predicates defined over the\ninputs of the system under test. Given a test input, our assertion-based test\noracle determines, without requiring test execution, whether the test passes,\nfails, or if the oracle is inconclusive in predicting a verdict. We describe\ntwo methods for generating assertion-based test oracles: one using genetic\nprogramming~(GP) that employs well-known spectrum-based fault localization\n(SBFL) ranking formulas, namely Ochiai, Tarantula, and Naish, as fitness\nfunctions; and the other using decision trees (DT) and decision rules (DR). We\nevaluate our assertion-based test oracles through case studies in the domains\nof aerospace, networking and autonomous driving. We show that test oracles\ngenerated using GP with Ochiai are significantly more accurate than those\nobtained using GP with Tarantula and Naish or using DT or DR. Moreover, this\naccuracy advantage remains even when accounting for the flakiness of the system\nunder test. We further show that the assertion-based test oracles generated by\nGP with Ochiai are robust against flakiness with only 4% average variation in\ntheir accuracy results across four different network and autonomous driving\nsystems with flaky behaviours.", "AI": {"tldr": "Introduces assertion-based test oracles using genetic programming with Ochiai to reduce CPS testing costs and handle flaky simulators.", "motivation": "CPS simulation testing is resource-intensive and prone to flakiness, creating unreliable results and high computational costs. This demands robust, execution-free test oracles to minimize retesting.", "method": "Proposed assertion-based test oracles as logical predicates over system inputs, implemented via two approaches: genetic programming (GP) with SBFL formulas (Ochiai, Tarantula, Naish) as fitness functions, and decision trees/rules (DT/DR) for comparison.", "result": "GP with Ochiai achieved significantly higher accuracy than Tarantula/Naish-based GP and DT/DR methods. Robustness against flakiness was demonstrated with only 4% average accuracy variation across four systems (networking, autonomous driving).", "conclusion": "Assertion-based oracles using GP with Ochiai formula provide accurate, stable test outcomes for CPS, outperforming other approaches and reducing reliance on error-prone simulator executions."}}
{"id": "2508.20412", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20412", "abs": "https://arxiv.org/abs/2508.20412", "authors": ["Zhiqiang Wang", "Junyang Zhang", "Guanquan Shi", "HaoRan Cheng", "Yunhao Yao", "Kaiwen Guo", "Haohua Du", "Xiang-Yang Li"], "title": "MindGuard: Tracking, Detecting, and Attributing MCP Tool Poisoning Attack via Decision Dependence Graph", "comment": null, "summary": "The Model Context Protocol (MCP) is increasingly adopted to standardize the\ninteraction between LLM agents and external tools. However, this trend\nintroduces a new threat: Tool Poisoning Attacks (TPA), where tool metadata is\npoisoned to induce the agent to perform unauthorized operations. Existing\ndefenses that primarily focus on behavior-level analysis are fundamentally\nineffective against TPA, as poisoned tools need not be executed, leaving no\nbehavioral trace to monitor.\n  Thus, we propose MindGuard, a decision-level guardrail for LLM agents,\nproviding provenance tracking of call decisions, policy-agnostic detection, and\npoisoning source attribution against TPA. While fully explaining LLM decision\nremains challenging, our empirical findings uncover a strong correlation\nbetween LLM attention mechanisms and tool invocation decisions. Therefore, we\nchoose attention as an empirical signal for decision tracking and formalize\nthis as the Decision Dependence Graph (DDG), which models the LLM's reasoning\nprocess as a weighted, directed graph where vertices represent logical concepts\nand edges quantify the attention-based dependencies. We further design robust\nDDG construction and graph-based anomaly analysis mechanisms that efficiently\ndetect and attribute TPA attacks. Extensive experiments on real-world datasets\ndemonstrate that MindGuard achieves 94\\%-99\\% average precision in detecting\npoisoned invocations, 95\\%-100\\% attribution accuracy, with processing times\nunder one second and no additional token cost. Moreover, DDG can be viewed as\nan adaptation of the classical Program Dependence Graph (PDG), providing a\nsolid foundation for applying traditional security policies at the decision\nlevel.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.20911", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20911", "abs": "https://arxiv.org/abs/2508.20911", "authors": ["Zuocheng Feng", "Kaiwen Zhang", "Miaomiao Wang", "Yiming Cheng", "Yuandao Cai", "Xiaofeng Li", "Guanjun Liu"], "title": "Deep Learning Based Concurrency Bug Detection and Localization", "comment": null, "summary": "Concurrency bugs, caused by improper synchronization of shared resources in\nmulti-threaded or distributed systems, are notoriously hard to detect and thus\ncompromise software reliability and security. The existing deep learning\nmethods face three main limitations. First, there is an absence of large and\ndedicated datasets of diverse concurrency bugs for them. Second, they lack\nsufficient representation of concurrency semantics. Third, binary\nclassification results fail to provide finer-grained debug information such as\nprecise bug lines. To address these problems, we propose a novel method for\neffective concurrency bug detection as well as localization. We construct a\ndedicated concurrency bug dataset to facilitate model training and evaluation.\nWe then integrate a pre-trained model with a heterogeneous graph neural network\n(GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) that\nconcisely and effectively characterizes concurrency semantics. To further\nfacilitate debugging, we employ SubgraphX, a GNN-based interpretability method,\nwhich explores the graphs to precisely localize concurrency bugs, mapping them\nto specific lines of source code. On average, our method demonstrates an\nimprovement of 10\\% in accuracy and precision and 26\\% in recall compared to\nstate-of-the-art methods across diverse evaluation settings.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.20414", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20414", "abs": "https://arxiv.org/abs/2508.20414", "authors": ["Mengyu Sun", "Ziyuan Yang", "Yongqiang Huang", "Hui Yu", "Yingyu Chen", "Shuren Qi", "Andrew Beng Jin Teoh", "Yi Zhang"], "title": "Federated Learning for Large Models in Medical Imaging: A Comprehensive Review", "comment": null, "summary": "Artificial intelligence (AI) has demonstrated considerable potential in the\nrealm of medical imaging. However, the development of high-performance AI\nmodels typically necessitates training on large-scale, centralized datasets.\nThis approach is confronted with significant challenges due to strict patient\nprivacy regulations and legal restrictions on data sharing and utilization.\nThese limitations hinder the development of large-scale models in medical\ndomains and impede continuous updates and training with new data. Federated\nLearning (FL), a privacy-preserving distributed training framework, offers a\nnew solution by enabling collaborative model development across fragmented\nmedical datasets. In this survey, we review FL's contributions at two stages of\nthe full-stack medical analysis pipeline. First, in upstream tasks such as CT\nor MRI reconstruction, FL enables joint training of robust reconstruction\nnetworks on diverse, multi-institutional datasets, alleviating data scarcity\nwhile preserving confidentiality. Second, in downstream clinical tasks like\ntumor diagnosis and segmentation, FL supports continuous model updating by\nallowing local fine-tuning on new data without centralizing sensitive images.\nWe comprehensively analyze FL implementations across the medical imaging\npipeline, from physics-informed reconstruction networks to diagnostic AI\nsystems, highlighting innovations that improve communication efficiency, align\nheterogeneous data, and ensure secure parameter aggregation. Meanwhile, this\npaper provides an outlook on future research directions, aiming to serve as a\nvaluable reference for the field's development.", "AI": {"tldr": "This survey explores federated learning (FL) in medical imaging, focusing on its roles in CT/MRI reconstruction (upstream) and tumor diagnosis/segmentation (downstream).", "motivation": "High-performance AI in medical imaging requires large-scale centralized data, which is challenging due to patient privacy laws and data-sharing restrictions.", "method": "The study reviews FL implementations in upstream and downstream tasks, examining how FL enables collaborative training and local updates on decentralized datasets.", "result": "FL helps alleviate data scarcity and maintain confidentiality by allowing multi-institutional training and local fine-tuning without centralized data storage.", "conclusion": "The paper discusses innovations in FL for communication efficiency, data alignment, and secure aggregation, and highlights future research directions for the medical imaging field."}}
{"id": "2508.20977", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20977", "abs": "https://arxiv.org/abs/2508.20977", "authors": ["Shiwen Shan", "Yintong Huo", "Yuxin Su", "Zhining Wang", "Dan Li", "Zibin Zheng"], "title": "ConfLogger: Enhance Systems' Configuration Diagnosability through Configuration Logging", "comment": "13 pages, 6 figures, accpeted by ICSE '26 (The 48th IEEE/ACM\n  International Conference on Software Engineering)", "summary": "Modern configurable systems offer customization via intricate configuration\nspaces, yet such flexibility introduces pervasive configuration-related issues\nsuch as misconfigurations and latent softwarebugs. Existing diagnosability\nsupports focus on post-failure analysis of software behavior to identify\nconfiguration issues, but none of these approaches look into whether the\nsoftware clue sufficient failure information for diagnosis. To fill in the\nblank, we propose the idea of configuration logging to enhance existing logging\npractices at the source code level. We develop ConfLogger, the first tool that\nunifies configuration-aware static taint analysis with LLM-based log generation\nto enhance software configuration diagnosability. Specifically, our method 1)\nidentifies configuration-sensitive code segments by tracing\nconfiguration-related data flow in the whole project, and 2) generates\ndiagnostic log statements by analyzing configuration code contexts. Evaluation\nresults on eight popular software systems demonstrate the effectiveness of\nConfLogger to enhance configuration diagnosability. Specifically,\nConfLogger-enhanced logs successfully aid a log-based misconfiguration\ndiagnosis tool to achieve 100% accuracy on error localization in 30 silent\nmisconfiguration scenarios, with 80% directly resolvable through explicit\nconfiguration information exposed. In addition, ConfLogger achieves 74%\ncoverage of existing logging points, outperforming baseline LLM-based loggers\nby 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,\nand 26.2% higher in F1 compared to the state-of-the-art baseline in terms of\nvariable logging while also augmenting diagnostic value. A controlled user\nstudy on 22 cases further validated its utility, speeding up diagnostic time by\n1.25x and improving troubleshooting accuracy by 251.4%.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.20424", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20424", "abs": "https://arxiv.org/abs/2508.20424", "authors": ["Desen Sun", "Shuncheng Jie", "Sihang Liu"], "title": "Breaking Diffusion with Cache: Exploiting Approximate Caches in Diffusion Models", "comment": null, "summary": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.21050", "categories": ["cs.SE", "cs.CY", "K.2; K.6.3; K.4; K.7"], "pdf": "https://arxiv.org/pdf/2508.21050", "abs": "https://arxiv.org/abs/2508.21050", "authors": ["Thomas J. Misa"], "title": "Dynamics of Gender Bias in Software Engineering", "comment": "26 pages, 3 figures", "summary": "The field of software engineering is embedded in both engineering and\ncomputer science, and may embody gender biases endemic to both. This paper\nsurveys software engineering's origins and its long-running attention to\nengineering professionalism, profiling five leaders; it then examines the\nfield's recent attention to gender issues and gender bias. It next\nquantitatively analyzes women's participation as research authors in the\nfield's leading International Conference of Software Engineering (1976-2010),\nfinding a dozen years with statistically significant gender exclusion. Policy\ndimensions of research on gender bias in computing are suggested.", "AI": {"tldr": "The study explores gender biases in software engineering, focusing on the field's historical roots in engineering and computer science, and quantitatively examines women's participation in the International Conference of Software Engineering from 1976 to 2010.", "motivation": "The paper addresses the concern that software engineering, rooted in both engineering and computer science, may inherit and perpetuate gender biases common to these fields.", "method": "The analysis involves surveys on the history and focus areas of software engineering, particularly profiling five field leaders and analyzing data on women's involvement as research authors at the top conference in the field between 1976 and 2010.", "result": "The study identifies a dozen years with statistically significant gender exclusion in women's participation in the International Conference of Software Engineering research author listings.", "conclusion": "The paper suggests policy dimensions for addressing gender bias in computing research, highlighting the need for systemic recognition and action on this issue within the software engineering field and beyond."}}
{"id": "2508.20444", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20444", "abs": "https://arxiv.org/abs/2508.20444", "authors": ["Md Raz", "Meet Udeshi", "P. V. Sai Charan", "Prashanth Krishnamurthy", "Farshad Khorrami", "Ramesh Karri"], "title": "Ransomware 3.0: Self-Composing and LLM-Orchestrated", "comment": null, "summary": "Using automated reasoning, code synthesis, and contextual decision-making, we\nintroduce a new threat that exploits large language models (LLMs) to\nautonomously plan, adapt, and execute the ransomware attack lifecycle.\nRansomware 3.0 represents the first threat model and research prototype of\nLLM-orchestrated ransomware. Unlike conventional malware, the prototype only\nrequires natural language prompts embedded in the binary; malicious code is\nsynthesized dynamically by the LLM at runtime, yielding polymorphic variants\nthat adapt to the execution environment. The system performs reconnaissance,\npayload generation, and personalized extortion, in a closed-loop attack\ncampaign without human involvement. We evaluate this threat across personal,\nenterprise, and embedded environments using a phase-centric methodology that\nmeasures quantitative fidelity and qualitative coherence in each attack phase.\nWe show that open source LLMs can generate functional ransomware components and\nsustain closed-loop execution across diverse environments. Finally, we present\nbehavioral signals and multi-level telemetry of Ransomware 3.0 through a case\nstudy to motivate future development of better defenses and policy enforcements\nto address novel AI-enabled ransomware attacks.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.20504", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.20504", "abs": "https://arxiv.org/abs/2508.20504", "authors": ["Guan-Yan Yang", "Jui-Ning Chen", "Farn Wang", "Kuo-Hui Yeh"], "title": "Enhancing Resilience for IoE: A Perspective of Networking-Level Safeguard", "comment": "To be published in IEEE Network Magazine, 2026", "summary": "The Internet of Energy (IoE) integrates IoT-driven digital communication with\npower grids to enable efficient and sustainable energy systems. Still, its\ninterconnectivity exposes critical infrastructure to sophisticated cyber\nthreats, including adversarial attacks designed to bypass traditional\nsafeguards. Unlike general IoT risks, IoE threats have heightened public safety\nconsequences, demanding resilient solutions. From the networking-level\nsafeguard perspective, we propose a Graph Structure Learning (GSL)-based\nsafeguards framework that jointly optimizes graph topology and node\nrepresentations to resist adversarial network model manipulation inherently.\nThrough a conceptual overview, architectural discussion, and case study on a\nsecurity dataset, we demonstrate GSL's superior robustness over representative\nmethods, offering practitioners a viable path to secure IoE networks against\nevolving attacks. This work highlights the potential of GSL to enhance the\nresilience and reliability of future IoE networks for practitioners managing\ncritical infrastructure. Lastly, we identify key open challenges and propose\nfuture research directions in this novel research area.", "AI": {"tldr": "The paper introduces a Graph Structure Learning (GSL)-based safeguards framework for securing the Internet of Energy (IoE) against adversarial attacks, showcasing its robustness and reliability over traditional methods.", "motivation": "The Internet of Energy (IoE) faces unique cyber threats compared to general IoT systems, with the potential to impact public safety. There is a need for resilient solutions that safeguard critical infrastructure from these evolving threats.", "method": "The authors propose a framework based on Graph Structure Learning (GSL) that jointly optimizes graph topology and node representations to resist adversarial network model manipulation.", "result": "The paper demonstrates the superior robustness of the GSL approach through a conceptual overview, architectural discussion, and case study using a security dataset.", "conclusion": "The GSL-based safeguards framework shows promise in enhancing the resilience and reliability of IoE networks, with the paper highlighting potential applications for practitioners managing critical energy infrastructure and identifying future research directions."}}
{"id": "2508.20962", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20962", "abs": "https://arxiv.org/abs/2508.20962", "authors": ["Weijie Liu", "Hongbo Chen", "Shuo Huai", "Zhen Xu", "Wenhao Wang", "Zhi Li", "Zheli Liu"], "title": "Characterizing Trust Boundary Vulnerabilities in TEE Containers", "comment": null, "summary": "Trusted Execution Environments (TEEs) have emerged as a cornerstone of\nconfidential computing, garnering significant attention from both academia and\nindustry. To enable the secure development, execution, and deployment, of\napplications on TEE platforms, TEE containers have been introduced as\nmiddleware solutions. These containers aim to shield applications from\npotentially malicious operating systems and orchestration interfaces while\nmaintaining usability and reliability. In this paper, we analyze the isolation\nstrategies employed by existing TEE containers to protect secure applications.\nTo address the challenges in analyzing these interfaces, we designed an\nautomated analyzer to precisely identify and evaluate their isolation\nboundaries. We observed that some TEE containers fail to achieve their intended\ngoals due to critical design and implementation flaws, such as information\nleakage, rollback attacks, denial-of-service, and Iago attacks, which pose\nsignificant security risks. Drawing from our findings, we share key lessons to\nguide the development of more secure container solutions and discuss emerging\ntrends in TEE containerization design.", "AI": {"tldr": "The paper analyzes TEE containers' isolation strategies and safety flaws and shares lessons for secure container documentation.", "motivation": "Trusted Execution Environments (TEEs) are vital for confidential computing, but the security of applications on TEE platforms depends on the effectiveness of TEE containers as middleware solutions. There is a need to ensure that these containers properly isolate applications from potentially malicious operating systems and orchestration interfaces.", "method": "The study involves an analysis of isolation strategies used by current TEE containers and the design of an automated analyzer to precisely identify and evaluate their isolation boundaries for security risks.", "result": "The study found that some TEE containers have critical design and implementation flaws, which lead to information leakage, rollback attacks, denial-of-service, and Iago attacks, each posing major security risks.", "conclusion": "The paper concludes by sharing key lessons to improve the development of TL container solutions and discussing emerging trends in TEE containerization design."}}
{"id": "2508.20517", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20517", "abs": "https://arxiv.org/abs/2508.20517", "authors": ["Dan Lin", "Shunfeng Lu", "Ziyan Liu", "Jiajing Wu", "Junyuan Fang", "Kaixin Lin", "Bowen Song", "Zibin Zheng"], "title": "BridgeShield: Enhancing Security for Cross-chain Bridge Applications via Heterogeneous Graph Mining", "comment": null, "summary": "Cross-chain bridges play a vital role in enabling blockchain\ninteroperability. However, due to the inherent design flaws and the enormous\nvalue they hold, they have become prime targets for hacker attacks. Existing\ndetection methods show progress yet remain limited, as they mainly address\nsingle-chain behaviors and fail to capture cross-chain semantics. To address\nthis gap, we leverage heterogeneous graph attention networks, which are\nwell-suited for modeling multi-typed entities and relations, to capture the\ncomplex execution semantics of cross-chain behaviors. We propose BridgeShield,\na detection framework that jointly models the source chain, off-chain\ncoordination, and destination chain within a unified heterogeneous graph\nrepresentation. BridgeShield incorporates intra-meta-path attention to learn\nfine-grained dependencies within cross-chain paths and inter-meta-path\nattention to highlight discriminative cross-chain patterns, thereby enabling\nprecise identification of attack behaviors. Extensive experiments on 51\nreal-world cross-chain attack events demonstrate that BridgeShield achieves an\naverage F1-score of 92.58%, representing a 24.39% improvement over\nstate-of-the-art baselines. These results validate the effectiveness of\nBridgeShield as a practical solution for securing cross-chain bridges and\nenhancing the resilience of multi-chain ecosystems.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.20591", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20591", "abs": "https://arxiv.org/abs/2508.20591", "authors": ["Jose E. Puente", "Carlos Puente"], "title": "Bitcoin as an Interplanetary Monetary Standard with Proof-of-Transit Timestamping", "comment": null, "summary": "We explore the feasibility of deploying Bitcoin as the shared monetary\nstandard between Earth and Mars, accounting for physical constraints of\ninterplanetary communication. We introduce a novel primitive, Proof-of-Transit\nTimestamping (PoTT), to provide cryptographic, tamper-evident audit trails for\nBitcoin data across high-latency, intermittently-connected links. Leveraging\nDelay/Disruption-Tolerant Networking (DTN) and optical low-Earth-orbit (LEO)\nmesh constellations, we propose an architecture for header-first replication,\nlong-horizon Lightning channels with planetary watchtowers, and secure\nsettlement through federated sidechains or blind-merge-mined (BMM) commit\nchains. We formalize PoTT, analyze its security model, and show how it\nmeasurably improves reliability and accountability without altering Bitcoin\nconsensus or its monetary base. Near-term deployments favor strong federations\nfor local settlement; longer-term, blind-merge-mined commit chains (if adopted)\nprovide an alternative. The Earth L1 monetary base remains unchanged, while\nMars can operate a pegged commit chain or strong federation with 1:1 pegged\nassets for local block production. For transparency, if both time-beacon\nregimes are simultaneously compromised, PoTT-M2 (and PoTT generally) reduces to\nadministrative assertions rather than cryptographic time-anchoring.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.20643", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20643", "abs": "https://arxiv.org/abs/2508.20643", "authors": ["Stefano Fumero", "Kai Huang", "Matteo Boffa", "Danilo Giordano", "Marco Mellia", "Zied Ben Houidi", "Dario Rossi"], "title": "CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics", "comment": "Code:\n  https://github.com/SmartData-Polito/LLM_Agent_Cybersecurity_Forensic", "summary": "Large Language Model (LLM) agents are powerful tools for automating complex\ntasks. In cybersecurity, researchers have primarily explored their use in\nred-team operations such as vulnerability discovery and penetration tests.\nDefensive uses for incident response and forensics have received comparatively\nless attention and remain at an early stage. This work presents a systematic\nstudy of LLM-agent design for the forensic investigation of realistic web\napplication attacks. We propose CyberSleuth, an autonomous agent that processes\npacket-level traces and application logs to identify the targeted service, the\nexploited vulnerability (CVE), and attack success. We evaluate the consequences\nof core design decisions - spanning tool integration and agent architecture -\nand provide interpretable guidance for practitioners. We benchmark four agent\narchitectures and six LLM backends on 20 incident scenarios of increasing\ncomplexity, identifying CyberSleuth as the best-performing design. In a\nseparate set of 10 incidents from 2025, CyberSleuth correctly identifies the\nexact CVE in 80% of cases. At last, we conduct a human study with 22 experts,\nwhich rated the reports of CyberSleuth as complete, useful, and coherent. They\nalso expressed a slight preference for DeepSeek R1, a good news for open source\nLLM. To foster progress in defensive LLM research, we release both our\nbenchmark and the CyberSleuth platform as a foundation for fair, reproducible\nevaluation of forensic agents.", "AI": {"tldr": "The paper introduces an autonomous cyber forensic agent and benchmark", "motivation": "Current LLM agents in cybersecurity focus on red-team applications, with limited research on defensive uses like forensics. There's a need for systematic evaluation and design guidance for LLM agents in this area.", "method": "The authors propose CyberSleuth, an autonomous agent using packet-level traces and application logs to identify targeted services, exploited vulnerabilities (CVEs), and attack success in web application attacks. They evaluate different agent architectures and LLM backends through a benchmark.", "result": "CyberSleuth outperforms other agent designs in their benchmark. It correctly identifies the exact CVE in 80% of 2025 incidents. Human experts rate the agent's reports as complete, useful, and coherent, with a slight preference for the open-source DeepSeek R1 model.", "conclusion": "The paper establishes a foundation for defensive LLM research by introducing CyberSleuth and a benchmark. The human study validates the quality of the agent's output, highlighting the potential of open-source models for cyber forensics."}}
{"id": "2508.20816", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20816", "abs": "https://arxiv.org/abs/2508.20816", "authors": ["Isaac David", "Arthur Gervais"], "title": "Multi-Agent Penetration Testing AI for the Web", "comment": null, "summary": "AI-powered development platforms are making software creation accessible to a\nbroader audience, but this democratization has triggered a scalability crisis\nin security auditing. With studies showing that up to 40% of AI-generated code\ncontains vulnerabilities, the pace of development now vastly outstrips the\ncapacity for thorough security assessment.\n  We present MAPTA, a multi-agent system for autonomous web application\nsecurity assessment that combines large language model orchestration with\ntool-grounded execution and end-to-end exploit validation. On the 104-challenge\nXBOW benchmark, MAPTA achieves 76.9% overall success with perfect performance\non SSRF and misconfiguration vulnerabilities, 83% success on broken\nauthorization, and strong results on injection attacks including server-side\ntemplate injection (85%) and SQL injection (83%). Cross-site scripting (57%)\nand blind SQL injection (0%) remain challenging. Our comprehensive cost\nanalysis across all challenges totals $21.38 with a median cost of $0.073 for\nsuccessful attempts versus $0.357 for failures. Success correlates strongly\nwith resource efficiency, enabling practical early-stopping thresholds at\napproximately 40 tool calls or $0.30 per challenge.\n  MAPTA's real-world findings are impactful given both the popularity of the\nrespective scanned GitHub repositories (8K-70K stars) and MAPTA's low average\noperating cost of $3.67 per open-source assessment: MAPTA discovered critical\nvulnerabilities including RCEs, command injections, secret exposure, and\narbitrary file write vulnerabilities. Findings are responsibly disclosed, 10\nfindings are under CVE review.", "AI": {"tldr": "The paper introduces MAPTA, a multi-agent system for     autonomous web application security assessments that can     identify critical vulnerabilities in popular repositories using     LLMs, tools, and exploit validation. Despite high success rates,     some challenges remain.", "motivation": "     The democratization of software development by AI platforms has led to a     surge in application development which outpaces security auditing     capabilities. Research shows that a significant percentage     (up to 40%) of AI-generated code contains vulnerabilities,     necessitating the development of a more efficient and effective     approach to ensure security without hindering development speed.", "method": "     MAPTA is a multi-agent system for autonomous evaluation of web     application security. It integrates large language model     orchestration with tool-grounded execution and end-to-end exploit validation.", "result": "     On the XBOW 104-challenge benchmark, MAPTA achieved 76.9% success     rate overall, with perfect performance on SSRF and misconfiguration     vulnerabilities. It had an 83% success rate on broken authorization,     85% on server-side template injection, and 83% on SQL     injection. Cross-site scripting success was 57%, with 0%     on blind SQL injection. Real-world assessments of     popular GitHub repositories (8K-70K stars) led to discovery     of critical vulnerabilities, with 10 responsibly disclosed     for CVE review. A cost analysis revealed that the     median cost for successful attempts was $0.073 compared to     $0.357 for failures.", "conclusion": "     The paper's findings highlight the effectiveness of     MAPTA in identifying critical vulnerabilities in web applications.     While there is room for improvement with some     challenges, the correlation between success and low resource     usage suggests that early-stopping thresholds (around 40     tool calls or $0.30 per challenge) can be practical.     The impact of MAPTA in real-world is measured by its     discoveries in popular repositories and its low     assessment cost at $3.67 per open-source scan, although     its cost efficiency on blind SQL injection remains a challenge."}}
{"id": "2508.20848", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20848", "abs": "https://arxiv.org/abs/2508.20848", "authors": ["Junjie Chu", "Mingjie Li", "Ziqing Yang", "Ye Leng", "Chenhao Lin", "Chao Shen", "Michael Backes", "Yun Shen", "Yang Zhang"], "title": "JADES: A Universal Framework for Jailbreak Assessment via Decompositional Scoring", "comment": "17 pages, 5 figures. For the code and data supporting this work, see\n  https://trustairlab.github.io/jades.github.io/", "summary": "Accurately determining whether a jailbreak attempt has succeeded is a\nfundamental yet unresolved challenge. Existing evaluation methods rely on\nmisaligned proxy indicators or naive holistic judgments. They frequently\nmisinterpret model responses, leading to inconsistent and subjective\nassessments that misalign with human perception. To address this gap, we\nintroduce JADES (Jailbreak Assessment via Decompositional Scoring), a universal\njailbreak evaluation framework. Its key mechanism is to automatically decompose\nan input harmful question into a set of weighted sub-questions, score each\nsub-answer, and weight-aggregate the sub-scores into a final decision. JADES\nalso incorporates an optional fact-checking module to strengthen the detection\nof hallucinations in jailbreak responses. We validate JADES on JailbreakQR, a\nnewly introduced benchmark proposed in this work, consisting of 400 pairs of\njailbreak prompts and responses, each meticulously annotated by humans. In a\nbinary setting (success/failure), JADES achieves 98.5% agreement with human\nevaluators, outperforming strong baselines by over 9%. Re-evaluating five\npopular attacks on four LLMs reveals substantial overestimation (e.g., LAA's\nattack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results show\nthat JADES could deliver accurate, consistent, and interpretable evaluations,\nproviding a reliable basis for measuring future jailbreak attacks.", "AI": {"tldr": "This paper presents JADES, a new jailbreak evaluation framework that automatically decomposes harmful questions and scores sub-answers to assess success, outperforming existing methods with 98.5% agreement with human annotations and exposing overestimated success rates in prior attacks.", "motivation": "Accurate evaluation of jailbreak success is crucial for understanding and improving the alignment of large language models, yet existing methods are inconsistent and subjective as they rely on proxy indicators or holistic judgments without proper decomposition.", "method": "JADES employs a two-step process: (1) decomposition of jailbreak prompts into sub-questions with attribute-based weights and (2) automated scoring of sub-answers using a customizable rubric, including an optional fact-checking module to detect hallucinations in responses.", "result": "JADES achieved 98.5% agreement with human evaluators on the JailbreakQR benchmark (400 annotated prompts/responses) in a binary success/failure setting, showing performance over 9% better than strong baselines. Re-evaluation of existing attacks showed significant success rate reductions (e.g., from 93% to 69% for LAA on GPT-3.5-Turbo).", "conclusion": "JADES offers a robust, accurate framework for jailbreak evaluation through decomposition and interpretable scoring, demonstrating the need for more precise metrics in LLM safety assessments and providing a reliable foundation for future research."}}
{"id": "2508.20863", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20863", "abs": "https://arxiv.org/abs/2508.20863", "authors": ["Matteo Gioele Collu", "Umberto Salviati", "Roberto Confalonieri", "Mauro Conti", "Giovanni Apruzzese"], "title": "Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review", "comment": null, "summary": "Large Language Models (LLMs) are increasingly being integrated into the\nscientific peer-review process, raising new questions about their reliability\nand resilience to manipulation. In this work, we investigate the potential for\nhidden prompt injection attacks, where authors embed adversarial text within a\npaper's PDF to influence the LLM-generated review. We begin by formalising\nthree distinct threat models that envision attackers with different motivations\n-- not all of which implying malicious intent. For each threat model, we design\nadversarial prompts that remain invisible to human readers yet can steer an\nLLM's output toward the author's desired outcome. Using a user study with\ndomain scholars, we derive four representative reviewing prompts used to elicit\npeer reviews from LLMs. We then evaluate the robustness of our adversarial\nprompts across (i) different reviewing prompts, (ii) different commercial\nLLM-based systems, and (iii) different peer-reviewed papers. Our results show\nthat adversarial prompts can reliably mislead the LLM, sometimes in ways that\nadversely affect a \"honest-but-lazy\" reviewer. Finally, we propose and\nempirically assess methods to reduce detectability of adversarial prompts under\nautomated content checks.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.20866", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20866", "abs": "https://arxiv.org/abs/2508.20866", "authors": ["Amine Lbath", "Massih-Reza Amini", "Aurelien Delaitre", "Vadim Okun"], "title": "AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning", "comment": null, "summary": "The increasing complexity of software systems and the sophistication of\ncyber-attacks have underscored the critical need for effective automated\nvulnerability detection and repair systems. Traditional methods, such as static\nprogram analysis, face significant challenges related to scalability,\nadaptability, and high false-positive and false-negative rates. AI-driven\napproaches, particularly those using machine learning and deep learning models,\nshow promise but are heavily reliant on the quality and quantity of training\ndata. This paper introduces a novel framework designed to automatically\nintroduce realistic, category-specific vulnerabilities into secure C/C++\ncodebases to generate datasets. The proposed approach coordinates multiple AI\nagents that simulate expert reasoning, along with function agents and\ntraditional code analysis tools. It leverages Retrieval-Augmented Generation\nfor contextual grounding and employs Low-Rank approximation of weights for\nefficient model fine-tuning. Our experimental study on 116 code samples from\nthree different benchmarks suggests that our approach outperforms other\ntechniques with regard to dataset accuracy, achieving between 89\\% and 95\\%\nsuccess rates in injecting vulnerabilities at function level.", "AI": {"tldr": "This paper presents a new framework that automatically introduces realistic, category-specific vulnerabilities into secure C/C++ codebases to enhance AI-driven vulnerability detection and repair systems. It uses multiple AI agents, traditional tools, and advanced techniques like Retrieval-Augmented Generation and Low-Rank approximation for efficient model training. The experiments show high success rates in vulnerability injection.", "motivation": "The increasing complexity of software and sophistication of cyber-attacks highlight the need for more effective datasets to train AI-driven vulnerability detection and repair systems.", "method": "The approach uses a framework that coordinates multiple AI agents simulating expert reasoning, function agents, and traditional analysis tools. It incorporates Retrieval-Augmented Generation for context and Low-Rank weight approximation for efficient model fine-tuning.", "result": "Success rates between 89% and 95% in injecting vulnerabilities at the function level across 116 code samples from three benchmarks.", "conclusion": "The proposed framework effectively generates realistic dataset by automatically introducing vulnerabilities, outperforming other techniques in terms of dataset accuracy using fewer computational resources due to the Low-Rank method."}}
{"id": "2508.20890", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20890", "abs": "https://arxiv.org/abs/2508.20890", "authors": ["Mengxiao Wang", "Yuxuan Zhang", "Guofei Gu"], "title": "PromptSleuth: Detecting Prompt Injection via Semantic Intent Invariance", "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated into real-world\napplications, from virtual assistants to autonomous agents. However, their\nflexibility also introduces new attack vectors-particularly Prompt Injection\n(PI), where adversaries manipulate model behavior through crafted inputs. As\nattackers continuously evolve with paraphrased, obfuscated, and even multi-task\ninjection strategies, existing benchmarks are no longer sufficient to capture\nthe full spectrum of emerging threats.\n  To address this gap, we construct a new benchmark that systematically extends\nprior efforts. Our benchmark subsumes the two widely-used existing ones while\nintroducing new manipulation techniques and multi-task scenarios, thereby\nproviding a more comprehensive evaluation setting. We find that existing\ndefenses, though effective on their original benchmarks, show clear weaknesses\nunder our benchmark, underscoring the need for more robust solutions. Our key\ninsight is that while attack forms may vary, the adversary's intent-injecting\nan unauthorized task-remains invariant. Building on this observation, we\npropose PromptSleuth, a semantic-oriented defense framework that detects prompt\ninjection by reasoning over task-level intent rather than surface features.\nEvaluated across state-of-the-art benchmarks, PromptSleuth consistently\noutperforms existing defense while maintaining comparable runtime and cost\nefficiency. These results demonstrate that intent-based semantic reasoning\noffers a robust, efficient, and generalizable strategy for defending LLMs\nagainst evolving prompt injection threats.", "AI": {"tldr": "This paper proposes PromptSleuth, a semantic intent-based defense framework for mitigating prompt injection (PI) attacks in LLMs, supported by a new comprehensive benchmark demonstrating limitations of existing defenses.", "motivation": "Prompt injection attacks are continually evolving with paraphrased/obfuscated inputs and multi-task strategies, undermining existing benchmarks and defenses. Current evaluation methods lack coverage of these sophisticated attack patterns.", "method": "1. Developed an extended benchmark incorporating prior frameworks plus new manipulation techniques and multi-task injection scenarios\n2. Proposed PromptSleuth: a defense framework using task-level semantic intent analysis rather than superficial pattern recognition\n3. Evaluated against state-of-the-art LLM benchmarks with emphasis on multi-task injection scenarios", "result": "PromptSleuth outperformed 8 existing defense methods across multiple benchmarks with comparable runtime efficiency. The new benchmark revealed significant vulnerabilities in current defenses under multi-task injection scenarios.", "conclusion": "Semantic intent analysis (PromptSleuth) provides a robust defense strategy against evolving prompt injection threats by addressing the invariant aspect of adversarial intent. The new benchmark highlights critical gaps in defense capabilities that intent-based approaches can resolve."}}
{"id": "2508.20963", "categories": ["cs.CR", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.20963", "abs": "https://arxiv.org/abs/2508.20963", "authors": ["Brandon Beltz", "Jim Doty", "Yvonne Fonken", "Nikolos Gurney", "Brett Israelsen", "Nathan Lau", "Stacy Marsella", "Rachelle Thomas", "Stoney Trent", "Peggy Wu", "Ya-Ting Yang", "Quanyan Zhu"], "title": "Guarding Against Malicious Biased Threats (GAMBiT) Experiments: Revealing Cognitive Bias in Human-Subjects Red-Team Cyber Range Operations", "comment": null, "summary": "We present three large-scale human-subjects red-team cyber range datasets\nfrom the Guarding Against Malicious Biased Threats (GAMBiT) project. Across\nExperiments 1-3 (July 2024-March 2025), 19-20 skilled attackers per experiment\nconducted two 8-hour days of self-paced operations in a simulated enterprise\nnetwork (SimSpace Cyber Force Platform) while we captured multi-modal data:\nself-reports (background, demographics, psychometrics), operational notes,\nterminal histories, keylogs, network packet captures (PCAP), and NIDS alerts\n(Suricata). Each participant began from a standardized Kali Linux VM and\npursued realistic objectives (e.g., target discovery and data exfiltration)\nunder controlled constraints. Derivative curated logs and labels are included.\nThe combined release supports research on attacker behavior modeling,\nbias-aware analytics, and method benchmarking. Data are available via IEEE\nDataport entries for Experiments 1-3.", "AI": {"tldr": "The paper introduces three large-scale datasets from the GAMBiT project, capturing diverse multi-modal data from simulated cyber attacks by skilled participants over two years. These datasets aim to support research in attack analysis and security strategies.", "motivation": "The motivation is to provide researchers with rich, multi-modal data from real human attackers in a controlled environment. This data can help improve understanding of attacker behavior and develop better bias-aware analytic tools for cyber defense.", "method": "The method involved running three experiments (July 2024-March 2025) with 19-20 skilled attackers each. Attackers used a standardized Kali Linux VM to conduct two 8-hour days of operations in a simulated enterprise network, where they attempted tasks like target discovery and data exfiltration. The researchers collected various types of data including self-reports, operational notes, terminal histories, keylogs, network packet captures (PCAP), and NIDS alerts (Suricata).", "result": "The result of the study is the creation of the three large-scale datasets, each containing detailed multi-modal data from the participants' activities in the simulated enterprise network. These datasets include a variety of data types such as self-reports, notes, terminal histories, keylogs, PCAPs, and NIDS alerts, allowing for comprehensive behavioral and technical analysis.", "conclusion": "The conclusion is that the combined release of these datasets from the GAMBiT project provides valuable resources for researchers to advance studies in attacker behavior modeling, bias-aware analytics, and benchmarking of methods used in cybersecurity. The data is made available via IEEE Dataport for experiments 1 to 3."}}
