{"id": "2509.10572", "categories": ["cs.SE", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.10572", "abs": "https://arxiv.org/abs/2509.10572", "authors": ["Ashlesha Akella", "Akshar Kaul", "Krishnasuri Narayanam", "Sameep Mehta"], "title": "Quality Assessment of Tabular Data using Large Language Models and Code Generation", "comment": "EMNLP industry track submitted", "summary": "Reliable data quality is crucial for downstream analysis of tabular datasets,\nyet rule-based validation often struggles with inefficiency, human\nintervention, and high computational costs. We present a three-stage framework\nthat combines statistical inliner detection with LLM-driven rule and code\ngeneration. After filtering data samples through traditional clustering, we\niteratively prompt LLMs to produce semantically valid quality rules and\nsynthesize their executable validators through code-generating LLMs. To\ngenerate reliable quality rules, we aid LLMs with retrieval-augmented\ngeneration (RAG) by leveraging external knowledge sources and domain-specific\nfew-shot examples. Robust guardrails ensure the accuracy and consistency of\nboth rules and code snippets. Extensive evaluations on benchmark datasets\nconfirm the effectiveness of our approach.", "AI": {"tldr": "The paper proposes a three-stage framework combining statistical inliner detection and LLM-driven rule/code generation to improve tabular data quality validation, addressing inefficiencies of traditional rule-based methods.", "motivation": "Rule-based validation for tabular data is inefficient, requires human intervention, and is computationally expensive, motivating the need for automated, scalable solutions.", "method": "The framework first filters data via traditional clustering, then iteratively leverages LLMs (augmented with RAG and domain-specific examples) to generate semantically valid rules and executable code, enforced by guardrails for accuracy and consistency.", "result": "Extensive evaluations on benchmark datasets demonstrate the approach's effectiveness in generating reliable data quality rules and validations.", "conclusion": "The proposed method successfully addresses limitations of existing approaches by integrating statistical techniques with LLMs, achieving efficient and accurate data quality validation."}}
{"id": "2509.10649", "categories": ["cs.SE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.10649", "abs": "https://arxiv.org/abs/2509.10649", "authors": ["Johan Cederbladh", "Loek Cleophas", "Eduard Kamburjan", "Lucas Lima", "Rakshit Mittal", "Hans Vangheluwe"], "title": "Reasonable Experiments in Model-Based Systems Engineering", "comment": null, "summary": "With the current trend in Model-Based Systems Engineering towards Digital\nEngineering and early Validation & Verification, experiments are increasingly\nused to estimate system parameters and explore design decisions. Managing such\nexperimental configuration metadata and results is of utmost importance in\naccelerating overall design effort. In particular, we observe it is important\nto 'intelligent-ly' reuse experiment-related data to save time and effort by\nnot performing potentially superfluous, time-consuming, and resource-intensive\nexperiments. In this work, we present a framework for managing experiments on\ndigital and/or physical assets with a focus on case-based reasoning with domain\nknowledge to reuse experimental data efficiently by deciding whether an\nalready-performed experiment (or associated answer) can be reused to answer a\nnew (potentially different) question from the engineer/user without having to\nset up and perform a new experiment. We provide the general architecture for\nsuch an experiment manager and validate our approach using an industrial\nvehicular energy system-design case study.", "AI": {"tldr": "This paper introduces an experiment-management framework that leverages case-based reasoning and domain knowledge to intelligently reuse experimental data, reducing redundant experiments and accelerating system design in digital engineering contexts.", "motivation": "The paper addresses the inefficiency and high resource demand of conducting repeated experiments in digital engineering and early validation & verification. Efficient reuse of experimental data is critical to accelerate system design efforts and avoid unnecessary resource expenditure.", "method": "A framework is developed for managing experiments (both digital and physical) using case-based reasoning with domain knowledge. It includes an experiment manager architecture that determines if existing experiments can answer new queries without conducting them afresh. The approach is validated using an industrial vehicular energy system-design case study.", "result": "Experimental validation through an industrial vehicular energy system-design case study demonstrates the feasibility and effectiveness of the proposed framework in reusing experimental data and reducing the need for redundant experiments.", "conclusion": "The framework presented in this paper aims to reduce redundant experiments in Model-Based Systems Engineering by efficiently reusing experimental data through case-based reasoning and domain knowledge, thereby saving time and resources in the design process."}}
{"id": "2509.10819", "categories": ["cs.SE", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.10819", "abs": "https://arxiv.org/abs/2509.10819", "authors": ["Christoph Hochrainer", "Valentin W\u00fcstholz", "Maria Christakis"], "title": "Arguzz: Testing zkVMs for Soundness and Completeness Bugs", "comment": null, "summary": "Zero-knowledge virtual machines (zkVMs) are increasingly deployed in\ndecentralized applications and blockchain rollups since they enable verifiable\noff-chain computation. These VMs execute general-purpose programs, frequently\nwritten in Rust, and produce succinct cryptographic proofs. However, zkVMs are\ncomplex, and bugs in their constraint systems or execution logic can cause\ncritical soundness (accepting invalid executions) or completeness (rejecting\nvalid ones) issues.\n  We present Arguzz, the first automated tool for testing zkVMs for soundness\nand completeness bugs. To detect such bugs, Arguzz combines a novel variant of\nmetamorphic testing with fault injection. In particular, it generates\nsemantically equivalent program pairs, merges them into a single Rust program\nwith a known output, and runs it inside a zkVM. By injecting faults into the\nVM, Arguzz mimics malicious or buggy provers to uncover overly weak\nconstraints.\n  We used Arguzz to test six real-world zkVMs (RISC Zero, Nexus, Jolt, SP1,\nOpenVM, and Pico) and found eleven bugs in three of them. One RISC Zero bug\nresulted in a $50,000 bounty, despite prior audits, demonstrating the critical\nneed for systematic testing of zkVMs.", "AI": {"tldr": "Arguzz is the first automated tool for testing zero-knowledge virtual machines (zkVMs) to detect soundness and completeness bugs. It combines metamorphic testing with fault injection to generate equivalent program pairs, execute them in zkVMs, and identify vulnerabilities, uncovering 11 bugs in three real-world zkVMs (e.g., RISC Zero).", "motivation": "ZKVMs underpin blockchain scalability but are error-prone due to complex constraint systems, risking invalid execution acceptance (soundness flaws) or rejection of valid ones (completeness flaws). Current manual audits fail to catch all issues, necessitating systematic testing tools.", "method": "Arguzz creates semantically equivalent Rust program pairs, merges them into a single test case with a known output, and executes it in a zkVM. Faults are injected to simulate malicious provers, revealing overly weak constraints by observing incorrect proof validation.", "result": "Arguzz tested six major zkVMs and found 11 critical bugs in three (RISC Zero, Jolt, SP1), including a $50,000 bounty-winning vulnerability in RISC Zero. This demonstrates the high error rate in existing zkVMs and the effectiveness of Arguzz\u2019s multifaceted approach.", "conclusion": "Automated testing tools like Arguzz are critical for securing zkVMs, as manual audits and informal testing inadequately address their inherent complexity. The results highlight systemic issues in zkVM design and the need for robust, injection-based evaluation methods to ensure soundness and completeness."}}
{"id": "2509.10920", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.10920", "abs": "https://arxiv.org/abs/2509.10920", "authors": ["Guan-Yan Yang", "Farn Wang", "You-Zong Gu", "Ya-Wen Teng", "Kuo-Hui Yeh", "Ping-Hsueh Ho", "Wei-Ling Wen"], "title": "TPSQLi: Test Prioritization for SQL Injection Vulnerability Detection in Web Applications", "comment": "20 pages; 8 figures", "summary": "The rapid proliferation of network applications has led to a significant\nincrease in network attacks. According to the OWASP Top 10 Projects report\nreleased in 2021, injection attacks rank among the top three vulnerabilities in\nsoftware projects. This growing threat landscape has increased the complexity\nand workload of software testing, necessitating advanced tools to support agile\ndevelopment cycles. This paper introduces a novel test prioritization method\nfor SQL injection vulnerabilities to enhance testing efficiency. By leveraging\nprevious test outcomes, our method adjusts defense strength vectors for\nsubsequent tests, optimizing the testing workflow and tailoring defense\nmechanisms to specific software needs. This approach aims to improve the\neffectiveness and efficiency of vulnerability detection and mitigation through\na flexible framework that incorporates dynamic adjustments and considers the\ntemporal aspects of vulnerability exposure.", "AI": {"tldr": "This paper introduces a dynamic SQL injection testing framework that prioritizes tests based on historical data and temporal vulnerability patterns, enhancing testing efficiency and defense adaptability in agile environments.", "motivation": "The proliferation of network applications and the prevalence of injection attacks (ranked top 3 in OWASP Top 10 2021) have created a pressing need for more efficient vulnerability testing in agile development cycles.", "method": "The proposed method employs a test prioritization framework that adjusts defense strength vectors based on previous test outcomes, incorporating dynamic adjustments and temporal aspects of vulnerability exposure to optimize testing workflows.", "result": "The approach aims to improve vulnerability detection efficiency and mitigation effectiveness through a flexible framework that optimizes testing workflows with context-aware prioritization and adaptive defense mechanisms.", "conclusion": "The paper introduces a novel test prioritization method for SQL injection vulnerabilities that enhances testing efficiency and effectiveness through dynamic adjustments and temporal considerations, contributing to more adaptive and tailored defense mechanisms in agile development cycles."}}
{"id": "2509.10482", "categories": ["cs.CR", "cs.AI", "D.4.6; I.2.7; K.6.5"], "pdf": "https://arxiv.org/pdf/2509.10482", "abs": "https://arxiv.org/abs/2509.10482", "authors": ["Matthew Grofsky"], "title": "AegisShield: Democratizing Cyber Threat Modeling with Generative AI", "comment": "Master's thesis", "summary": "The increasing sophistication of technology systems makes traditional threat\nmodeling hard to scale, especially for small organizations with limited\nresources. This paper develops and evaluates AegisShield, a generative AI\nenhanced threat modeling tool that implements STRIDE and MITRE ATT&CK to\nautomate threat generation and provide systematic assessments. By integrating\nreal time threat intelligence from the National Vulnerability Database and\nAlienVault Open Threat Exchange, AegisShield produces streamlined and\naccessible threat descriptions. Our assessment of 243 threats from 15 case\nstudies and over 8000 AI generated threats shows that AegisShield reduces\ncomplexity (p less than 0.001), yields outputs semantically aligned with expert\ndeveloped threats (p less than 0.05), and achieves an 85.4 percent success rate\nin mapping threats to MITRE ATT&CK techniques (p less than 0.001). Automating\nand standardizing threat modeling helps under resourced organizations address\nrisk earlier and supports wider adoption of secure by design practices.", "AI": {"tldr": "AegisShield, an AI-driven threat modeling tool, streamlines security assessments for small organizations by automating STRIDE/MITRE ATT&CK analysis with real-time threat data, improving efficiency and accuracy.", "motivation": "Traditional threat modeling is inefficient and resource-intensive for small organizations, necessitating scalable, automated solutions to improve risk management.", "method": "Developed AegisShield, a generative AI tool combining STRIDE and MITRE ATT&CK frameworks with real-time threat intelligence from NVD and AlienVault to automate threat generation and assessment.", "result": "AegisShield reduced complexity (p < 0.001), produced semantically aligned threats (p < 0.05), and achieved 85.4% MITRE ATT&CK mapping accuracy (p < 0.001) across 15 case studies and 8000+ AI-generated threats.", "conclusion": "AegisShield automates threat modeling, enabling resource-limited organizations to efficiently manage risks and promoting secure-by-design practices through integration with MITRE ATT&CK and real-time threat intelligence."}}
{"id": "2509.10946", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10946", "abs": "https://arxiv.org/abs/2509.10946", "authors": ["Roberto Morabito", "Guanghan Wu"], "title": "When the Code Autopilot Breaks: Why LLMs Falter in Embedded Machine Learning", "comment": "This paper has been accepted for publication in Computer (IEEE). Upon\n  publication, the copyright will be transferred to IEEE", "summary": "Large Language Models (LLMs) are increasingly used to automate software\ngeneration in embedded machine learning workflows, yet their outputs often fail\nsilently or behave unpredictably. This article presents an empirical\ninvestigation of failure modes in LLM-powered ML pipelines, based on an\nautopilot framework that orchestrates data preprocessing, model conversion, and\non-device inference code generation. We show how prompt format, model behavior,\nand structural assumptions influence both success rates and failure\ncharacteristics, often in ways that standard validation pipelines fail to\ndetect. Our analysis reveals a diverse set of error-prone behaviors, including\nformat-induced misinterpretations and runtime-disruptive code that compiles but\nbreaks downstream. We derive a taxonomy of failure categories and analyze\nerrors across multiple LLMs, highlighting common root causes and systemic\nfragilities. Though grounded in specific devices, our study reveals broader\nchallenges in LLM-based code generation. We conclude by discussing directions\nfor improving reliability and traceability in LLM-powered embedded ML systems.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.10488", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.10488", "abs": "https://arxiv.org/abs/2509.10488", "authors": ["Trueye Tafese"], "title": "Turning CVEs into Educational Labs:Insights and Challenges", "comment": "20 pages, 5 figures, git hub files to run on your enivronment, step\n  by step tutorial, survey results", "summary": "This research focuses on transforming CVEs to hands-on educational lab for\ncybersecurity training. The study shows the practical application of CVEs by\ndeveloping containerized lab environments- Docker to simulate real-world\nvulnerabilities like SQL Injection, arbitrary code execution, and improper SSL\ncertificate validation. These labs has structured tutorials, pre- and\npost-surveys to evaluate learning outcomes, and remediation steps.Key\nchallenges included interpreting limited CVE data, resolving technical\ncomplexities in lab design, and ensuring accessibility for diverse learners.\nDespite these difficulties, the findings highlight the use of educational\nbenefits of vulnerability analysis, bridging theoretical concepts with hands-on\nexperience. The results indicate that students improved comprehension of\ncybersecurity principles, threat mitigation techniques, and secure coding\npractices. This innovative approach provides a scalable and reproducible model\nfor integrating CVEs into cybersecurity education, fostering a deeper\nunderstanding of real-world security challenges in a controlled and safe\nenvironment.", "AI": {"tldr": "This paper proposes a framework for converting CVEs into Docker-based educational labs to enhance cybersecurity training through hands-on vulnerability analysis.", "motivation": "Bridging the gap between theoretical cybersecurity knowledge and practical skills by simulating real-world vulnerabilities in a controlled environment.", "method": "Developed containerized labs using Docker to replicate vulnerabilities like SQL Injection and SSL certificate issues, combined with structured tutorials, pre/post surveys, and remediation steps.", "result": "Students demonstrated improved comprehension of cybersecurity principles, threat mitigation, and secure coding practices through lab-based learning.", "conclusion": "The approach provides a scalable, reproducible model for integrating CVEs into education, enhancing real-world security understanding safely."}}
{"id": "2509.11000", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.11000", "abs": "https://arxiv.org/abs/2509.11000", "authors": ["Omid Gheibi", "Christian K\u00e4stner", "Pooyan Jamshidi"], "title": "Hardness, Structural Knowledge, and Opportunity: An Analytical Framework for Modular Performance Modeling", "comment": null, "summary": "Performance-influence models are beneficial for understanding how\nconfigurations affect system performance, but their creation is challenging due\nto the exponential growth of configuration spaces. While gray-box approaches\nleverage selective \"structural knowledge\" (like the module execution graph of\nthe system) to improve modeling, the relationship between this knowledge, a\nsystem's characteristics (we call them \"structural aspects\"), and potential\nmodel improvements is not well understood. This paper addresses this gap by\nformally investigating how variations in structural aspects (e.g., the number\nof modules and options per module) and the level of structural knowledge impact\nthe creation of \"opportunities\" for improved \"modular performance modeling\". We\nintroduce and quantify the concept of modeling \"hardness\", defined as the\ninherent difficulty of performance modeling. Through controlled experiments\nwith synthetic system models, we establish an \"analytical matrix\" to measure\nthese concepts. Our findings show that modeling hardness is primarily driven by\nthe number of modules and configuration options per module. More importantly,\nwe demonstrate that both higher levels of structural knowledge and increased\nmodeling hardness significantly enhance the opportunity for improvement. The\nimpact of these factors varies by performance metric; for ranking accuracy\n(e.g., in debugging task), structural knowledge is more dominant, while for\nprediction accuracy (e.g., in resource management task), hardness plays a\nstronger role. These results provide actionable insights for system designers,\nguiding them to strategically allocate time and select appropriate modeling\napproaches based on a system's characteristics and a given task's objectives.", "AI": {"tldr": "This paper systematically analyzes how structural aspects (modules, options) and available structural knowledge affect performance modeling. It shows modeling hardness increases with system complexity, with structural knowledge and hardness jointly enhancing opportunities. The impact varies between ranking vs. prediction tasks.", "motivation": "The gap lies in understanding how structural knowledge and system-specific aspects (e.g., module counts) influence model improvements. This work investigates how these factors interact with modeling hardness and opportunities for enhanced performance modeling.", "method": "The authors introduced a concept of 'modeling hardness' and used controlled experiments with synthetic system models to create an 'analytical matrix' measuring how structural aspects (e.g., modules and options per module) and structural knowledge influence modeling opportunities. Experiments quantified the impact on performance metrics (ranking accuracy vs. prediction accuracy).", "result": "Modeling hardness is driven by module counts and configuration options per module. Higher structural knowledge and modeling hardness increase improvement opportunities. Structural knowledge dominates for ranking accuracy (debugging tasks), while hardness dominates for prediction accuracy (resource management tasks).", "conclusion": "The paper concludes that understanding structural aspects and structural knowledge is crucial for improving modular performance modeling. It provides actionable insights for system designers to strategically allocate resources and choose modeling approaches based on system characteristics and task objectives."}}
{"id": "2509.10492", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.10492", "abs": "https://arxiv.org/abs/2509.10492", "authors": ["Philip Laryea Doku"], "title": "Investigation Of The Distinguishability Of Giraud-Verneuil Atomic Blocks", "comment": "Master Thesis", "summary": "In this work, we investigate the security of Elliptic Curve Cryptosystem\n(ECC) implementations against Side-Channel Analysis (SCA). ECC is well known\nfor its efficiency and strong security, yet vulnerable to SCA which exploits\nphysical information leaked during scalar multiplication (kP). Countermeasures\nsuch as regularity and atomicity exist; this thesis focuses on atomicity. In\nthis work, we study the Giraud and Verneuil atomic pattern for kP, implementing\nit using the right-to-left kP algorithm on the NIST EC P-256 curve. We use the\nFLECC library with constant-time operations and execute on the Texas\nInstruments LAUNCHXLF28379D MCU. We measure Electromagnetic (EM) emissions\nduring kP using a Lecroy WavePro 604HD Oscilloscope, a Langer ICS 105\nIntegrated Circuit Scanner, and a Langer MFA-R 0.2-75 Near Field Probe. We\ninvestigate whether the Giraud and Verneuil atomic blocks are distinguishable\nin EM traces. Our findings show that, when additional clock cycle processes are\npresent, the atomic blocks can be visually distinguished; after removing these\nprocesses, they become more synchronised and harder to distinguish, reducing\nthe risk of a successful SCA attack. These results show that, although the\natomic pattern is correctly implemented with dummy operations, resistance to\nSCA can still be affected by additional processes inserted at hardware or\nsoftware level.This means atomicity alone may not fully protect ECC from SCA.\nMore research is needed to investigate the causes of the additional clock cycle\nprocesses and how intermediate operations are addressed in memory registers.\nThis will help to understand the processes that lead to the insertion of these\nadditional clock cycles. This thesis is the first to experimentally implement\nand investigate Giraud and Verneuil's atomic pattern on hardware, and it offers\nuseful results to improve countermeasures against SCA.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.11065", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.11065", "abs": "https://arxiv.org/abs/2509.11065", "authors": ["Yuan Si", "Daming Li", "Hanyuan Shi", "Jialu Zhang"], "title": "ViScratch: Using Large Language Models and Gameplay Videos for Automated Feedback in Scratch", "comment": null, "summary": "Block-based programming environments such as Scratch are increasingly popular\nin programming education, in particular for young learners. While the use of\nblocks helps prevent syntax errors, semantic bugs remain common and difficult\nto debug. Existing tools for Scratch debugging rely heavily on predefined rules\nor user manual inputs, and crucially, they ignore the platform's inherently\nvisual nature.\n  We introduce ViScratch, the first multimodal feedback generation system for\nScratch that leverages both the project's block code and its generated gameplay\nvideo to diagnose and repair bugs. ViScratch uses a two-stage pipeline: a\nvision-language model first aligns visual symptoms with code structure to\nidentify a single critical issue, then proposes minimal, abstract syntax tree\nlevel repairs that are verified via execution in the Scratch virtual machine.\n  We evaluate ViScratch on a set of real-world Scratch projects against\nstate-of-the-art LLM-based tools and human testers. Results show that gameplay\nvideo is a crucial debugging signal: ViScratch substantially outperforms prior\ntools in both bug identification and repair quality, even without access to\nproject descriptions or goals. This work demonstrates that video can serve as a\nfirst-class specification in visual programming environments, opening new\ndirections for LLM-based debugging beyond symbolic code alone.", "AI": {"tldr": "ViScratch is a first-of-its-kind multimodal Scratch debugger that combines gameplay video and block code analysis in a two-stage pipeline, outperforming existing tools by leveraging video as a primary debugging signal.", "motivation": "Block-based programming environments like Scratch suffer from semantic bugs despite syntax protection. Existing tools rely on predefined rules or manual input and ignore the platform\u2019s visual nature, creating a need for visual-aware debugging solutions.", "method": "ViScratch uses a two-stage pipeline: a vision-language model aligns visual symptoms from gameplay video with code structure to identify critical issues, then proposes minimal abstract syntax tree (AST) repairs verified via execution in the Scratch virtual machine.", "result": "ViScratch outperforms prior tools in bug identification and repair quality when evaluated on real-world projects, even without access to project descriptions or goals, showing gameplay video is a crucial debugging signal.", "conclusion": "This work demonstrates that video can serve as a first-class specification in visual programming environments, opening new directions for LLM-based debugging beyond symbolic code alone."}}
{"id": "2509.10540", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10540", "abs": "https://arxiv.org/abs/2509.10540", "authors": ["Pavan Reddy", "Aditya Sanjay Gujral"], "title": "EchoLeak: The First Real-World Zero-Click Prompt Injection Exploit in a Production LLM System", "comment": "8 pages content, 1 page references, 2 figures, Published at AAAI Fall\n  Symposium Series 2025", "summary": "Large language model (LLM) assistants are increasingly integrated into\nenterprise workflows, raising new security concerns as they bridge internal and\nexternal data sources. This paper presents an in-depth case study of EchoLeak\n(CVE-2025-32711), a zero-click prompt injection vulnerability in Microsoft 365\nCopilot that enabled remote, unauthenticated data exfiltration via a single\ncrafted email. By chaining multiple bypasses-evading Microsofts XPIA (Cross\nPrompt Injection Attempt) classifier, circumventing link redaction with\nreference-style Markdown, exploiting auto-fetched images, and abusing a\nMicrosoft Teams proxy allowed by the content security policy-EchoLeak achieved\nfull privilege escalation across LLM trust boundaries without user interaction.\nWe analyze why existing defenses failed, and outline a set of engineering\nmitigations including prompt partitioning, enhanced input/output filtering,\nprovenance-based access control, and strict content security policies. Beyond\nthe specific exploit, we derive generalizable lessons for building secure AI\ncopilots, emphasizing the principle of least privilege, defense-in-depth\narchitectures, and continuous adversarial testing. Our findings establish\nprompt injection as a practical, high-severity vulnerability class in\nproduction AI systems and provide a blueprint for defending against future\nAI-native threats.", "AI": {"tldr": "EchoLeak (CVE-2025-32711), a zero-click prompt injection vulnerability in Microsoft 365 Copilot, enabling unauthenticated data exfiltration via a crafted email by chaining multiple bypasses. The study proposes mitigations and security principles for AI systems.", "motivation": "Enterprise adoption of LLMs introduces novel security risks as they interface internal and external data. This paper addresses the lack of understanding about practical threat models and defensive strategies for AI-native vulnerabilities.", "method": "Chained exploit techniques including: (1)\u200b\u2988 bypassing Microsoft's XPIA (Cross Prompt Injection Attempt) classifier, (2)\u200b\u2988 circumventing link redaction using reference-style Markdown, (3)\u200b\u2988 exploiting auto-fetched images, and (4)\u200b\u2988 leveraging a Microsoft Teams proxy under content security policies to achieve cross-LLM privilege escalation.", "result": "Demonstrated full data exfiltration without user interaction. Proposed mitigations include: (1)\u200b\u2988 Prompt partitioning, (2)\u200b\u2988 Enhanced input/output filtering, (3)\u200b\u2988 Provenance-based access control, and (4)\u200b\u2988 Strict content security policies. Established prompt injection as a critical vulnerability class.", "conclusion": "Highlights the need for least privilege principles, defense-in-depth architectures, and adversarial testing in AI copilots. Provides a blueprint for securing AI systems against future attacks by operationalizing security patterns like input hardening, provenance tracking, and policy enforcement."}}
{"id": "2509.11132", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11132", "abs": "https://arxiv.org/abs/2509.11132", "authors": ["Xiaoyu Zhang", "Weipeng Jiang", "Juan Zhai", "Shiqing Ma", "Qingshuang Bao", "Chenhao Lin", "Chao Shen", "Tianlin Li", "Yang Liu"], "title": "Rethinking Technology Stack Selection with AI Coding Proficiency", "comment": "23 pages", "summary": "Large language models (LLMs) are now an integral part of software development\nworkflows and are reshaping the whole process. Traditional technology stack\nselection has not caught up. Most of the existing selection methods focus\nsolely on the inherent attributes of the technology, overlooking whether the\nLLM can effectively leverage the chosen technology. For example, when\ngenerating code snippets using popular libraries like Selenium (one of the most\nwidely used test automation tools with over 33k GitHub stars), existing LLMs\nfrequently generate low-quality code snippets (e.g., using deprecated APIs and\nmethods, or containing syntax errors). As such, teams using LLM assistants risk\nchoosing technologies that cannot be used effectively by LLMs, yielding high\ndebugging effort and mounting technical debt. We foresee a practical question\nin the LLM era, is a technology ready for AI-assisted development? In this\npaper, we first propose the concept, AI coding proficiency, the degree to which\nLLMs can utilize a given technology to generate high-quality code snippets. We\nconduct the first comprehensive empirical study examining AI proficiency across\n170 third-party libraries and 61 task scenarios, evaluating six widely used\nLLMs. Our findings reveal that libraries with similar functionalities can\nexhibit up to 84% differences in the quality score of LLM-generated code, while\ndifferent models also exhibit quality gaps among their generation results using\nthe same library. These gaps translate into real engineering costs and can\nsteer developer choices toward a narrow set of libraries with high AI coding\nproficiency, threatening technological diversity in the ecosystem. We call on\nthe community to integrate AI proficiency assessments into technology selection\nframeworks and develop mitigation strategies, preserving competitive balance in\nAI-driven development.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.10543", "categories": ["cs.CR", "cs.AI", "cs.LG", "68M12, 68T07", "C.2.0; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.10543", "abs": "https://arxiv.org/abs/2509.10543", "authors": ["Landon Bragg", "Nathan Dorsey", "Josh Prior", "John Ajit", "Ben Kim", "Nate Willis", "Pablo Rivas"], "title": "Robust DDoS-Attack Classification with 3D CNNs Against Adversarial Methods", "comment": "The 27th International Conference on Artificial Intelligence\n  (ICAI'25)", "summary": "Distributed Denial-of-Service (DDoS) attacks remain a serious threat to\nonline infrastructure, often bypassing detection by altering traffic in subtle\nways. We present a method using hive-plot sequences of network data and a 3D\nconvolutional neural network (3D CNN) to classify DDoS traffic with high\naccuracy. Our system relies on three main ideas: (1) using spatio-temporal\nhive-plot encodings to set a pattern-recognition baseline, (2) applying\nadversarial training with FGSM and PGD alongside spatial noise and image\nshifts, and (3) analyzing frame-wise predictions to find early signals. On a\nbenchmark dataset, our method lifts adversarial accuracy from 50-55% to over\n93% while maintaining clean-sample performance. Frames 3-4 offer strong\npredictive signals, showing early-stage classification is possible.", "AI": {"tldr": "This paper introduces a 3D CNN with hive-plot encodings and adversarial training for accurate, early-stage DDoS detection, achieving robustness against evasion tactics.", "motivation": "DDoS attacks evade detection by camouflaging traffic. Traditional methods lack robustness against adversarial manipulations, necessitating a more effective classification technique that can identify subtle attack patterns early and resist adversarial examples.", "method": "The method combines spatio-temporal hive-plot encodings with a 3D CNN trained via adversarial techniques (FGSM, PGD) and data augmentation. This architecture focuses on extracting temporal patterns from network traffic and improving robustness through early signal analysis.", "result": "The model achieves 93% adversarial accuracy on benchmark datasets, exceeding prior methods (50-55%), while maintaining strong clean-sample performance. It identifies frames 3-4 as critical for early classification.", "conclusion": "The paper effectively addresses the challenge of detecting subtle DDoS attacks by introducing a hybrid approach of hive-plot sequences and adversarially trained 3D CNNs. It demonstrates robust performance against adversarial examples and enables early-stage classification, offering a practical advancement for network security systems."}}
{"id": "2509.11238", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11238", "abs": "https://arxiv.org/abs/2509.11238", "authors": ["Dongming Jin", "Zhi Jin", "Yiran Zhang", "Zheng Fang", "Linyu Li", "Yuanpeng He", "Xiaohong Chen", "Weisong Sun"], "title": "UserTrace: User-Level Requirements Generation and Traceability Recovery from Software Project Repositories", "comment": "21page, 5 figures", "summary": "Software maintainability critically depends on high-quality requirements\ndescriptions and explicit traceability between requirements and code. Although\nautomated code summarization (ACS) and requirements traceability (RT)\ntechniques have been widely studied, existing ACS methods mainly generate\nimplementation-level (i.e., developer-oriented) requirements (IRs) for\nfine-grained units (e.g., methods), while RT techniques often overlook the\nimpact of project evolution. As a result, user-level (i.e., end user-oriented)\nrequirements (URs) and live trace links remain underexplored, despite their\nimportance for supporting user understanding and for validating whether\nAI-generated software aligns with user intent. To address this gap, we propose\nUserTrace, a multi-agent system that automatically generates URs and recovers\nlive trace links (from URs to IRs to code) from software repositories.\nUserTrace coordinates four specialized agents (i.e., Code Reviewer, Searcher,\nWriter, and Verifier) through a three-phase process: structuring repository\ndependencies, deriving IRs for code units, and synthesizing URs with\ndomain-specific context. Our comparative evaluation shows that UserTrace\nproduces URs with higher completeness, correctness, and helpfulness than an\nestablished baseline, and achieves superior precision in trace link recovery\ncompared to five state-of-the-art RT approaches. A user study further\ndemonstrates that UserTrace helps end users validate whether the AI-generated\nrepositories align with their intent.", "AI": {"tldr": "UserTrace is a multi-agent system that automates user-level requirement generation and trace link recovery, improving software maintainability and enabling user validation of AI-generated software.", "motivation": "Existing automated code summarization (ACS) and requirements traceability (RT) techniques primarily focus on developer-oriented requirements and lack consideration for project evolution, leaving user-level requirements (URs) and live trace links underexplored despite their critical role in software maintainability and user intent validation.", "method": "UserTrace employs a multi-agent system with four specialized agents (Code Reviewer, Searcher, Writer, and Verifier) operating through three phases: structuring repository dependencies, deriving implementation-level requirements (IRs), and synthesizing user-level requirements (URs) with domain-specific context.", "result": "UserTrace outperforms an established baseline in UR completeness, correctness, and helpfulness, achieves higher precision in trace link recovery than five state-of-the-art RT approaches, and demonstrates practical utility through a user study showing its effectiveness in validating AI-generated software against user intent.", "conclusion": "UserTrace effectively addresses the gap in generating user-level requirements and live trace links, enhancing software maintainability and enabling end users to validate AI-generated software alignment with their intent."}}
{"id": "2509.10545", "categories": ["cs.CR", "cs.IR", "E.1, H.3.3, E.3", "E.1; H.3.3; E.3"], "pdf": "https://arxiv.org/pdf/2509.10545", "abs": "https://arxiv.org/abs/2509.10545", "authors": ["Ruwanga Konara", "Kasun De Zoysa", "Asanka Sayakkara"], "title": "Decentralized Identity Management on Ripple: A Conceptual Framework for High-Speed, Low-Cost Identity Transactions in Attestation-Based Attribute-Based Identity", "comment": null, "summary": "Recent years have seen many industrial implementations and much scholastic\nresearch, i.e., prototypes and theoretical frameworks, in Decentralized\nIdentity Management Systems (DIDMS). It is safe to say that Attestation-Based\nAttribute-Based Decentralized IDM (ABABDIDM) has not received anywhere near the\nsame level of attention in the literature as general Attribute-Based DIDMs\n(ABDIDM), i.e, decentralized Attribute-Based Access Control (ABAC). The use of\ndecentralization, i.e., DIDM, is to improve upon the security and\nprivacy-related issues of centralized Identity Management Systems (IDM) and\nAttribute-Based IDMs (ABIDM). And blockchain is the framework used for\ndecentralization in all these schemes. Many DIDMs - even ABDIDMs - have been\ndefined on popular blockchains such as Hyperledger, Ethereum, and Bitcoin.\nHowever, despite the characteristics of Ripple that makes it appealing for an\nABIDM, there is a lack of research to develop an Identity Management System\n(IDMS) on Ripple in literature. We have attempted to conceptualize an ABABDIDM\non Ripple.", "AI": {"tldr": "This paper identifies the research gap in ABABDIDM on Ripple and proposes a conceptual framework to utilize Ripple's blockchain for decentralized identity management.", "motivation": "The motivation stems from the lack of attention given to ABABDIDM compared to other decentralized identity management systems and the underutilized potential of Ripple's blockchain features for such implementations.", "method": "The paper conceptualizes an Attestation-Based Attribute-Based Decentralized Identity Management System (ABABDIDM) using the Ripple blockchain platform.", "result": "A conceptual framework for ABABDIDM on Ripple is proposed, highlighting how Ripple's characteristics can be leveraged for improved decentralized identity management.", "conclusion": "The authors conclude that there is a significant gap in research regarding ABABDIDM on Ripple, despite its advantages. They propose a conceptual framework for ABABDIDM on Ripple to address these gaps."}}
{"id": "2509.11252", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11252", "abs": "https://arxiv.org/abs/2509.11252", "authors": ["Chengze li", "Yitong Zhang", "Jia Li", "Liyi Cai", "Ge Li"], "title": "Beyond Autoregression: An Empirical Study of Diffusion Large Language Models for Code Generation", "comment": null, "summary": "LLMs have become the mainstream approaches to code generation. Existing LLMs\nmainly employ autoregressive generation, i.e. generating code token-by-token\nfrom left to right. However, the underlying autoregressive generation has two\nlimitations in code generation. First, autoregressive LLMs only generate a\ntoken at each step, showing low efficiency in practice. Second, programming is\na non-sequential process involving back-and-forth editing, while autoregressive\nLLMs only employ the left-to-right generation order. These two intrinsic\nlimitations hinder the further development of LLMs in code generation.\nRecently, diffusion LLMs have emerged as a promising alternative. Diffusion\nLLMs address the above limitations with two advances, including multi-token\nprediction (i.e. generating multiple tokens at each step) and flexible\ngeneration order (i.e. flexibly determining which positions to generate\ntokens). However, there is no systematic study exploring diffusion LLMs in code\ngeneration. To bridge the knowledge gap, we present the first empirical study\nof diffusion LLMs for code generation. Our study involves 9 representative\ndiffusion LLMs and conduct experiments on 4 widely used benchmarks. Based on\nthe results, we summarize the following findings. (1) Existing diffusion LLMs\nare competitive with autoregressive LLMs with similar sizes. (2) Diffusion LLMs\nhave a stronger length extrapolation ability than autoregressive LLMs and\nperform better in long code understanding. (3) We explore factors impacting the\neffectiveness and efficiency of diffusion LLMs, and provide practical guidance.\n(4) We discuss several promising further directions to improve diffusion LLMs\non code generation. We open-source all source code, data, and results to\nfacilitate the following research. The code is publicly available at\nhttps://github.com/zhangyitonggg/dllm4code.", "AI": {"tldr": "This paper investigates diffusion LLMs for code generation, addressing limitations of autoregressive models (low efficiency, fixed left-to-right order). It compares 9 diffusion LLMs with autoregressive counterparts through empirical studies on 4 benchmarks, revealing their competitiveness, strong extrapolation, and practical optimizations.", "motivation": "Autoregressive LLMs for code generation struggle with efficiency (token-by-token generation) and the inherently non-sequential nature of programming. Diffusion LLMs propose multi-token prediction and flexible generation order but lack systematic study in code generation.", "method": "The authors conduct the first empirical study on diffusion LLMs for code generation, evaluating 9 models across 4 benchmarks. They analyze effectiveness, efficiency, and length extrapolation, exploring factors affecting performance and proposing practical guidance.", "result": "Key findings: (1)\ndiffusion LLMs match autoregressive models with similar sizes; (2)\nstronger length extrapolation and long code comprehension; (3)\npractical optimizations based on effectiveness/efficiency factors; (4)\ndirections for model improvement. Open-source resources are provided.", "conclusion": "Diffusion LLMs demonstrate promising potential for code generation, overcoming autoregressive limitations. The study establishes foundational benchmarks and insights for future research, with open-source tools to accelerate development in this area."}}
{"id": "2509.10550", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10550", "abs": "https://arxiv.org/abs/2509.10550", "authors": ["Shivam Akhauri"], "title": "Auditable Early Stopping for Agentic Routing: Ledger-Verified Run-Wise Certificates under Local DP", "comment": null, "summary": "In production tool-use agents (e.g., retrieval $\\to$ summarization $\\to$\ncalculator), routers must know when to stop exploring while preserving local DP\nand leaving an auditable trail. We present run-wise early-stopping certificates\nfor perturb-and-MAP (PaM) best-first search on context-indexed prefix DAGs\nwhose children partition the leaves. We couple realized path scores and pruning\nkeys to a single exponential race realized lazily via offset propagation. With\nexact leaf counts $N(v)$, lazy reuse at winners and independent residuals yield\nan Exact mode with a sound halting rule based on Key$(v) = M_tau(v) - \\log\nt(v)$, where $t(v)$ is the minimum arrival time among leaves under $v$. With\nonly upper bounds $N_{ub} \\ge N$, a Surrogate mode uses a parent-anchored\nsurrogate race without winner reuse; because $-\\log \\hat t \\ge -\\log t$, the\nfrontier invariant holds and stopping remains sound. We add a compiler from\nshared-node DAGs to prefix DAGs, local finiteness checks, a SuffixCountDP\nroutine for exact counts with safe downgrades, a validator-side tightening term\n$\\kappa = \\log(N/N_{ub})$, and an auditable ledger/validator that replays runs\ndeterministically. We also give an absolute LogSumExp tail bound, an acyclicity\ncertificate, and a fallback PRF-per-leaf scheme (NoCert) whose work matches a\nrealized-score best-first baseline up to a small per-node overhead. Finally, we\nintegrate a price/latency/$(\\epsilon, \\delta)$-aware multi-LLM controller and\nDP-trained LoRA adapters chosen at runtime; these choices do not affect the\ntwo-mode frontier invariants. We report Mac/commodity-hardware reproducible\nresults, a small real tool-use pipeline, and validator-checked audit trails,\nwith code and ledgers provided.", "AI": {"tldr": "The paper introduces run-wise early-stopping certificates for differentially private best-first search in tool-use pipelines to maintain local differential privacy and generate auditable trails, with methods for exact and surrogate modes and a compiler for DAGs, validated through experiments and reproducible results.", "motivation": "The motivation behind the paper is to address the challenge of early stopping in differentially private tool-use pipelines where agents sequentially carry out actions like retrieval, summarization, and calculation. Routers need reliable stopping mechanisms to preserve local DP while ensuring audit trails are maintained for transparent tracking of the decision-making process.", "method": "The method proposed in the paper includes a technique for early stopping in probabilistic best-first search, introducing certificates for exact and surrogate modes to enhance privacy guarantees. The approach involves coupling realized path scores with pruning keys using an exponential race realized through offset propagation. It presents a compiler from shared-node DAGs to prefix DAGs, routines for maintaining count guarantees, and a deterministic ledger system for audit trails.", "result": "The results demonstrate the effectiveness of the two-stop modes (Exact and Surrogate) in maintaining differentially private (DP) guarantees, along with the practical utilities of the compiler, validator, and control mechanisms. The paper provides a small real tool-use pipeline and auditable trails, offering evidence of DP preservation through logging and logging of all behaviors with results that are reproducible and verifiable using commodity hardware.", "conclusion": "The conclusion highlights that the introduced method successfully enables private and early stopping best-first search in tool-use systems through two modes of operation and a validation system. It showcases the robustness of the DP guarantees, the comprehensibility of audit trails, and the effectiveness of the compiler and control mechanisms in real-world scenarios."}}
{"id": "2509.11258", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11258", "abs": "https://arxiv.org/abs/2509.11258", "authors": ["Regan Meloche", "Durga Sivakumar", "Amal A. Anda", "Sofana Alfuhaid", "Daniel Amyot", "Luigi Logrippo", "John Mylopoulos"], "title": "A Web-Based Environment for the Specification and Generation of Smart Legal Contracts", "comment": "12 pages, 5 figures, 2 tables, conference", "summary": "Monitoring the compliance of contract performance against legal obligations\nis important in order to detect violations, ideally, as soon as they occur.\nSuch monitoring can nowadays be achieved through the use of smart contracts,\nwhich provide protection against tampering as well as some level of automation\nin handling violations. However, there exists a large gap between natural\nlanguage contracts and smart contract implementations. This paper introduces a\nWeb-based environment that partly fills that gap by supporting the\nuser-assisted refinement of Symboleo specifications corresponding to legal\ncontract templates, followed by the automated generation of monitoring smart\ncontracts deployable on the Hyperledger Fabric platform. This environment,\nillustrated using a sample contract from the transactive energy domain, shows\nmuch potential in accelerating the development of smart contracts in a legal\ncompliance context.", "AI": {"tldr": "This paper presents a Web-based tool for converting legal contracts into smart contracts via Symboleo refinement, enabling automated compliance monitoring on Hyperledger Fabric.", "motivation": "Monitoring contract compliance against legal obligations is critical for detecting violations promptly. However, a significant gap exists between natural language contracts and their smart contract implementations, hindering automation and compliance enforcement.", "method": "A Web-based environment is introduced to support user-assisted refinement of Symboleo specifications derived from legal contract templates, followed by automated generation of monitoring smart contracts deployable on Hyperledger Fabric.", "result": "The environment is illustrated using a transactive energy domain sample contract, showing its effectiveness in streamlining smart contract development while maintaining alignment with legal compliance requirements.", "conclusion": "The proposed environment addresses the gap between natural language contracts and smart contracts, demonstrating significant potential for accelerating smart contract development in legal compliance contexts, particularly through its application in the transactive energy domain."}}
{"id": "2509.10551", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.10551", "abs": "https://arxiv.org/abs/2509.10551", "authors": ["Amal Raj", "Vivek Balachandran"], "title": "A Hybrid Encryption Framework Combining Classical, Post-Quantum, and QKD Methods", "comment": "This version corrects an error in a table entry compared to the\n  accepted Springer version", "summary": "This paper introduces a hybrid encryption framework combining classical\ncryptography (EdDSA, ECDH), post-quantum cryptography (ML-DSA-6x5, ML-KEM-768),\nand Quantum Key Distribution (QKD) via Guardian to counter quantum computing\nthreats. Our prototype implements this integration, using a key derivation\nfunction to generate secure symmetric and HMAC keys, and evaluates its\nperformance across execution time and network metrics. The approach improves\ndata protection by merging classical efficiency with PQC's quantum resilience\nand QKD's key security, offering a practical transition path for cryptographic\nsystems. This research lays the foundation for future adoption of PQC in\nsecuring digital communication.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.11312", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11312", "abs": "https://arxiv.org/abs/2509.11312", "authors": ["Wenchao Gu", "Yupan Chen", "Yanlin Wang", "Hongyu Zhang", "Cuiyun Gao", "Michael R. Lyu"], "title": "Weakly Supervised Vulnerability Localization via Multiple Instance Learning", "comment": null, "summary": "Software vulnerability detection has emerged as a significant concern in the\nfield of software security recently, capturing the attention of numerous\nresearchers and developers. Most previous approaches focus on coarse-grained\nvulnerability detection, such as at the function or file level. However, the\ndevelopers would still encounter the challenge of manually inspecting a large\nvolume of code inside the vulnerable function to identify the specific\nvulnerable statements for modification, indicating the importance of\nvulnerability localization. Training the model for vulnerability localization\nusually requires ground-truth labels at the statement-level, and labeling\nvulnerable statements demands expert knowledge, which incurs high costs. Hence,\nthe demand for an approach that eliminates the need for additional labeling at\nthe statement-level is on the rise. To tackle this problem, we propose a novel\napproach called WAVES for WeAkly supervised Vulnerability Localization via\nmultiplE inStance learning, which does not need the additional statement-level\nlabels during the training. WAVES has the capability to determine whether a\nfunction is vulnerable (i.e., vulnerability detection) and pinpoint the\nvulnerable statements (i.e., vulnerability localization). Specifically,\ninspired by the concept of multiple instance learning, WAVES converts the\nground-truth label at the function-level into pseudo labels for individual\nstatements, eliminating the need for additional statement-level labeling. These\npseudo labels are utilized to train the classifiers for the function-level\nrepresentation vectors. Extensive experimentation on three popular benchmark\ndatasets demonstrates that, in comparison to previous baselines, our approach\nachieves comparable performance in vulnerability detection and state-of-the-art\nperformance in statement-level vulnerability localization.", "AI": {"tldr": "WAVES enables effective vulnerability detection and localization using only function-level labels, reducing reliance on expensive statement-level annotations while achieving state-of-the-art results.", "motivation": "Existing methods require costly expert-labeled statement-level data for vulnerability localization. The need for a label-efficient approach to address this bottleneck drives the proposed weakly supervised solution.", "method": "WAVES employs multiple instance learning to convert function-level labels into pseudo-statement-level labels, training classifiers using these pseudo labels to detect vulnerabilities and localize statements without manual annotation.", "result": "Extensive experiments on three benchmarks demonstrate comparable vulnerability detection accuracy and superior statement-level localization performance compared to prior baselines.", "conclusion": "The paper introduces WAVES, a weakly supervised approach for vulnerability localization that eliminates the need for statement-level labels. It achieves comparable vulnerability detection performance and state-of-the-art localization results on benchmark datasets."}}
{"id": "2509.10561", "categories": ["cs.CR", "cs.AI", "68Q25, 68T50, 68P27", "F.2.2; I.2.7; K.4.1"], "pdf": "https://arxiv.org/pdf/2509.10561", "abs": "https://arxiv.org/abs/2509.10561", "authors": ["Madhava Gaikwad"], "title": "AVEC: Bootstrapping Privacy for Local LLMs", "comment": "12 pages", "summary": "This position paper presents AVEC (Adaptive Verifiable Edge Control), a\nframework for bootstrapping privacy for local language models by enforcing\nprivacy at the edge with explicit verifiability for delegated queries. AVEC\nintroduces an adaptive budgeting algorithm that allocates per-query\ndifferential privacy parameters based on sensitivity, local confidence, and\nhistorical usage, and uses verifiable transformation with on-device integrity\nchecks. We formalize guarantees using R\\'enyi differential privacy with\nodometer-based accounting, and establish utility ceilings, delegation-leakage\nbounds, and impossibility results for deterministic gating and hash-only\ncertification. Our evaluation is simulation-based by design to study mechanism\nbehavior and accounting; we do not claim deployment readiness or task-level\nutility with live LLMs. The contribution is a conceptual architecture and\ntheoretical foundation that chart a pathway for empirical follow-up on\nprivately bootstrapping local LLMs.", "AI": {"tldr": "AVEC proposes a privacy framework for local LLMs using adaptive differential privacy budgets, verifiable edge checks, and theoretical guarantees, validated through simulations but requiring future empirical deployment testing.", "motivation": "The paper addresses the challenge of enforcing privacy for local language models at the edge, particularly when handling delegated queries, through explicit verifiability and adaptive privacy mechanisms to prevent over- or under-protecting sensitive data.", "method": "AVEC introduces an adaptive budgeting algorithm allocating differential privacy parameters based on query sensitivity, local confidence, and historical usage, combined with verifiable transformations and on-device integrity checks. Theoretical guarantees are formalized using R\\'enyi differential privacy, odometer-based accounting, and bounds on delegation-leakage and deterministic gating limitations.", "result": "Simulation-based evaluations validate the behavior of AVEC's mechanisms and accounting systems, though deployment readiness or task-level utility with live LLMs is not claimed. Theoretical contributions include utility ceilings, leakage bounds, and impossibility results.", "conclusion": "The paper establishes AVEC as a conceptual framework and theoretical foundation for future empirical research on bootstrapping privacy in local LLMs, highlighting pathways for follow-up studies despite not claiming deployment readiness."}}
{"id": "2509.11446", "categories": ["cs.SE", "D.2.0; D.2.1; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.11446", "abs": "https://arxiv.org/abs/2509.11446", "authors": ["Mohammad Amin Zadenoori", "Jacek D\u0105browski", "Waad Alhoshan", "Liping Zhao", "Alessio Ferrari"], "title": "Large Language Models (LLMs) for Requirements Engineering (RE): A Systematic Literature Review", "comment": null, "summary": "Large Language Models (LLMs) are finding applications in numerous domains,\nand Requirements Engineering (RE) is increasingly benefiting from their\ncapabilities to assist with complex, language-intensive tasks. This paper\npresents a systematic literature review of 74 primary studies published between\n2023 and 2024, examining how LLMs are being applied in RE. The study\ncategorizes the literature according to several dimensions, including\npublication trends, RE activities, prompting strategies, and evaluation\nmethods. Our findings indicate notable patterns, among which we observe\nsubstantial differences compared to previous works leveraging standard Natural\nLanguage Processing (NLP) techniques. Most of the studies focus on using LLMs\nfor requirements elicitation and validation, rather than defect detection and\nclassification, which were dominant in the past. Researchers have also\nbroadened their focus and addressed novel tasks, e.g., test generation,\nexploring the integration of RE with other software engineering (SE)\ndisciplines. Although requirements specifications remain the primary focus,\nother artifacts are increasingly considered, including issues from issue\ntracking systems, regulations, and technical manuals. The studies mostly rely\non GPT-based models, and often use Zero-shot or Few-shot prompting. They are\nusually evaluated in controlled environments, with limited use in industry\nsettings and limited integration in complex workflows. Our study outlines\nimportant future directions, such as leveraging the potential to expand the\ninfluence of RE in SE, exploring less-studied tasks, improving prompting\nmethods, and testing in real-world environments. Our contribution also helps\nresearchers and practitioners use LLMs more effectively in RE, by providing a\nlist of identified tools leveraging LLMs for RE, as well as datasets.", "AI": {"tldr": "This paper systematically reviews 74 studies (2023-2024) on applying Large Language Models (LLMs) to Requirements Engineering (RE), finding recent trends in usage patterns, prompting strategies, and task focus differences compared to earlier NLP-based work.", "motivation": "The growing adoption of LLMs in RE warrants understanding new application patterns and gaps compared to traditional NLP approaches to guide researchers and practitioners.", "method": "Systematic literature review of 74 primary studies from 2023-2024, categorizing research by RE activities, prompting strategies, evaluation methods, and technical artifacts.", "result": "LLMs are increasingly used for requirements elicitation/validation over defect detection; novel tasks like test generation are emerging; GPT-based models with zero-/few-shot prompting dominate; industry application remains limited.", "conclusion": "The review highlights expanding RE-LLM integration opportunities while identifying gaps in industrial deployment, complex workflow integration, and under-studied tasks, providing a curated list of tools and datasets for future work."}}
{"id": "2509.10563", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.10563", "abs": "https://arxiv.org/abs/2509.10563", "authors": ["Mohammed Yacoubi", "Omar Moussaoui", "C. Drocourt"], "title": "Enhancing IoMT Security with Explainable Machine Learning: A Case Study on the CICIOMT2024 Dataset", "comment": null, "summary": "Explainable Artificial Intelligence (XAI) enhances the transparency and\ninterpretability of AI models, addressing their inherent opacity. In\ncybersecurity, particularly within the Internet of Medical Things (IoMT), the\nblack-box nature of AI-driven threat detection poses a significant challenge.\nCybersecurity professionals must not only detect attacks but also understand\nthe reasoning behind AI decisions to ensure trust and accountability. The rapid\nincrease in cyberattacks targeting connected medical devices threatens patient\nsafety and data privacy, necessitating advanced AI-driven solutions. This study\ncompares two ensemble learning techniques, bagging and boosting, for\ncyber-attack classification in IoMT environments. We selected Random Forest for\nbagging and CatBoost for boosting. Random Forest helps reduce variance, while\nCatBoost improves bias by combining weak classifiers into a strong ensemble\nmodel, making them effective for detecting sophisticated attacks. However,\ntheir complexity often reduces transparency, making it difficult for\ncybersecurity professionals to interpret and trust their decisions. To address\nthis issue, we apply XAI models to generate local and global explanations,\nproviding insights into AI decision-making. Using techniques like SHAP (Shapley\nAdditive Explanations) and LIME (Local Interpretable Model-agnostic\nExplanations), we highlight feature importance to help stakeholders understand\nthe key factors driving cyber threat detection.", "AI": {"tldr": "This study evaluates bagging (Random Forests) and boosting (CatBoost)", "motivation": "The opacity of AI models in cybersecurity, especially for IoMT devices, hinders trust and accountability despite their effectiveness in detecting threats.", "method": "Compared ensemble learners (Random Forest for bagging, CatBoost for boosting), then applied SHAP and LIME XAI techniques to generate local/global explanations for model decisions.", "result": "Ensemble methods showed strong performance for cyber-attack classification, but their complexity reduced transparency; XAI techniques provided feature importance insights to address this limitation.", "conclusion": "XAI integration with ensemble learning offers a balanced approach for effective and interpretable cyber threat detection in medical IoT environments, though further optimization is needed."}}
{"id": "2509.11523", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11523", "abs": "https://arxiv.org/abs/2509.11523", "authors": ["Ziliang Wang", "Ge Li", "Jia Li", "Hao Zhu", "Zhi Jin"], "title": "VulAgent: Hypothesis-Validation based Multi-Agent Vulnerability Detection", "comment": null, "summary": "The application of language models to project-level vulnerability detection\nremains challenging, owing to the dual requirement of accurately localizing\nsecurity-sensitive code and correctly correlating and reasoning over complex\nprogram context. We present VulAgent, a multi-agent vulnerability detection\nframework based on hypothesis validation. Our design is inspired by how human\nauditors review code: when noticing a sensitive operation, they form a\nhypothesis about a possible vulnerability, consider potential trigger paths,\nand then verify the hypothesis against the surrounding context. VulAgent\nimplements a semantics-sensitive, multi-view detection pipeline: specialized\nagents, each aligned to a specific analysis perspective (e.g., memory,\nauthorization), collaboratively surface and precisely localize sensitive code\nsites with higher coverage. Building on this, VulAgent adopts a\nhypothesis-validation paradigm: for each vulnerability report, it builds\nhypothesis conditions and a trigger path, steering the LLM to target the\nrelevant program context and defensive checks during verification, which\nreduces false positives. On average across the two datasets, VulAgent improves\noverall accuracy by 6.6%, increases the correct identification rate of\nvulnerable--fixed code pairs by up to 450% (246% on average), and reduces the\nfalse positive rate by about 36% compared with state-of-the-art LLM-based\nbaselines.", "AI": {"tldr": "VulAgent, a multi-agent hypothesis-validation framework, enhances LLM-based vulnerability detection by structuring audits around context-aware hypotheses, significantly improving accuracy (6.6%+) and fixing false positives (36%\u2193).", "motivation": "Language models struggle with project-level vulnerability detection due to the dual challenge of localizing security-sensitive code and reasoning over complex program context. Human auditors\u2019 hypothesis-driven workflow inspired this solution.", "method": "VulAgent is a multi-agent framework that aligns specialized agents with analysis perspectives (e.g., memory, authorization) to collaboratively detect vulnerabilities. It uses a hypothesis-validation paradigm where hypotheses are formed and verified against relevant program context via LLM interactions, reducing false positives.", "result": "VulAgent achieves a 6.6% higher overall accuracy, up to 450% improvement in identifying vulnerable-fixed code pairs (246% average), and a 36% reduction in false positives compared to state-of-the-art LLM-based baselines across two datasets.", "conclusion": "VulAgent effectively addresses the challenges of project-level vulnerability detection by employing a hypothesis-validation approach, leading to significant improvements in accuracy and a reduction in false positives, as demonstrated by the empirical results on two datasets."}}
{"id": "2509.10568", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.10568", "abs": "https://arxiv.org/abs/2509.10568", "authors": ["Muhammad M. Roomi", "Suhail S. M. Hussain", "Daisuke Mashima"], "title": "SG-ML: Smart Grid Cyber Range Modelling Language", "comment": "28 pages, 38 figures, 3 tables", "summary": "This work provides a detailed specification of the Smart Grid Modelling\nLanguage (SG-ML), which is designed for the automated generation of smart grid\ncyber ranges. SG-ML is defined as a set of XML schemas that describe a smart\ngrid's configuration in both machine-readable and human-friendly ways, thereby\nbridging the gap between system modelling and automated deployment. Unlike\nprior ad-hoc approaches to cyber range design, SG-ML provides a unified\nmethodology that integrates both power system and cyber network\nrepresentations. The SG-ML model can be customized by users to meet specific\nrequirements, such as emulating physical or cyber topologies and configuring\nnetwork devices. An SG-ML Processor then parses this configured model to\ninstantiate the cyber range environment. The modelling language leverages\nestablished standards like the IEC 61850 Substation Configuration Language\n(SCL) and IEC 61131 PLCopen XML to define power system topology, cyber network\ntopology, and device configurations. This approach allows for the reuse of\nexisting assets, reducing the effort needed to create the SG-ML model. To\naddress gaps not covered by these standards such as attack injection\nparameters, scenario-specific metadata, and additional network constraints,\nSG-ML introduces proprietary schemas that complement standard models. Overall,\nSG-ML enables reproducible, scalable, and automated generation of realistic\nsmart grid cyber ranges for research, training, and security assessment.", "AI": {"tldr": "SG-ML is an XML-based modeling language for automated smart grid cyber range generation, integrating power/cyber system standards with customizable extensions to enable reproducible, scalable deployments.", "motivation": "Existing ad-hoc cyber range approaches lack unified integration of power system and cyber network modeling, requiring a standardized solution for automated deployment while reusing existing infrastructure standards like IEC 61850 and PLCopen XML.", "method": "Defined via XML schemas combining IEC 61850 SCL for power system topology, PLCopen XML for industrial automation, and proprietary schemas addressing security requirements (attack parameters, network constraints). An SG-ML Processor compiles these into executable environments.", "result": "Demonstrated that SG-ML enables environment creation through standard asset reuse (IEC examples) and custom extensions for security scenarios, achieving reproducible cyber range generation with reduced manual effort.", "conclusion": "SG-ML fills critical gaps in smart grid security assessment by providing a domain-specific language that harmonizes physical/cyber modeling standards with security requirements, enabling efficient, scalable, and standardized cyber range generation for research/training."}}
{"id": "2509.11566", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11566", "abs": "https://arxiv.org/abs/2509.11566", "authors": ["Hua Guo", "Yunhong Ji", "Xuan Zhou"], "title": "Sedeve-Kit, a Specification-Driven Development Framework for Building Distributed Systems", "comment": null, "summary": "Developing distributed systems presents significant challenges, primarily due\nto the complexity introduced by non-deterministic concurrency and faults. To\naddress these, we propose a specification-driven development framework. Our\nmethod encompasses three key stages. The first stage defines system\nspecifications and invariants using TLA${^+}$. It allows us to perform model\nchecking on the algorithm's correctness and generate test cases for subsequent\ndevelopment phases. In the second stage, based on the established\nspecifications, we write code to ensure consistency and accuracy in the\nimplementation. Finally, after completing the coding process, we rigorously\ntest the system using the test cases generated in the initial stage. This\nprocess ensures system quality by maintaining a strong connection between the\nabstract design and the concrete implementation through continuous\nverification.", "AI": {"tldr": "This paper proposes a three-stage specification-driven framework for distributed systems, using TLA+ to bridge design and implementation through continuous verification, addressing challenges in concurrency and faults.", "motivation": "The paper addresses the challenges of non-deterministic concurrency and faults in distributed systems, necessitating a robust development approach to ensure correctness and reliability.", "method": "The method involves three stages: (1) defining system specifications and invariants using TLA+ for model checking and test case generation, (2) writing code to ensure consistency with specifications, and (3) rigorous system testing using generated test cases.", "result": "The framework enables systematic verification of distributed systems through specification alignment, model checking, and iterative testing, enhancing system quality and consistency.", "conclusion": "The paper concludes that the proposed specification-driven development framework maintains a strong connection between abstract design and concrete implementation through continuous verification, ensuring system quality in distributed systems development."}}
{"id": "2509.10569", "categories": ["cs.CR", "cs.AI", "cs.MM", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.10569", "abs": "https://arxiv.org/abs/2509.10569", "authors": ["Leyi Pan", "Sheng Guan", "Zheyu Fu", "Luyang Si", "Zian Wang", "Xuming Hu", "Irwin King", "Philip S. Yu", "Aiwei Liu", "Lijie Wen"], "title": "MarkDiffusion: An Open-Source Toolkit for Generative Watermarking of Latent Diffusion Models", "comment": "23 pages, 13 figures, 5 tables", "summary": "We introduce MarkDiffusion, an open-source Python toolkit for generative\nwatermarking of latent diffusion models. It comprises three key components: a\nunified implementation framework for streamlined watermarking algorithm\nintegrations and user-friendly interfaces; a mechanism visualization suite that\nintuitively showcases added and extracted watermark patterns to aid public\nunderstanding; and a comprehensive evaluation module offering standard\nimplementations of 24 tools across three essential aspects - detectability,\nrobustness, and output quality - plus 8 automated evaluation pipelines. Through\nMarkDiffusion, we seek to assist researchers, enhance public awareness and\nengagement in generative watermarking, and promote consensus while advancing\nresearch and applications.", "AI": {"tldr": "MarkDiffusion is an open-source toolkit for watermarking latent diffusion models with unified implementation, visualization, and standardized evaluation across 24 tools and 8 automated pipelines.", "motivation": "Existing watermarking methods lack standardized evaluation frameworks and visualization tools, hindering research progress and public understanding of watermarking effectiveness in latent diffusion models.", "method": "The toolkit features: 1) a unified implementation framework for watermarking algorithms and user interfaces 2) visualization tools for watermark patterns and 3) an evaluation module with 24 tools across detectability, robustness, and quality, plus 8 automated pipelines.", "result": "MarkDiffusion provides researchers with 1) a flexible platform for watermarking algorithm development, 2) intuitive visualization mechanisms for public engagement, and 3) standardized evaluation pipelines across 24 tools to benchmark watermarking performance.", "conclusion": "MarkDiffusion advances generative watermarking research and applications by providing a unified, open-source toolkit that fosters collaboration and evaluates watermarking effectiveness through standardized metrics."}}
{"id": "2509.11626", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11626", "abs": "https://arxiv.org/abs/2509.11626", "authors": ["Prerna Agarwal", "Himanshu Gupta", "Soujanya Soni", "Rohith Vallam", "Renuka Sindhgatta", "Sameep Mehta"], "title": "Automated Creation and Enrichment Framework for Improved Invocation of Enterprise APIs as Tools", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) has lead to the\ndevelopment of agents capable of complex reasoning and interaction with\nexternal tools. In enterprise contexts, the effective use of such tools that\nare often enabled by application programming interfaces (APIs), is hindered by\npoor documentation, complex input or output schema, and large number of\noperations. These challenges make tool selection difficult and reduce the\naccuracy of payload formation by up to 25%. We propose ACE, an automated tool\ncreation and enrichment framework that transforms enterprise APIs into\nLLM-compatible tools. ACE, (i) generates enriched tool specifications with\nparameter descriptions and examples to improve selection and invocation\naccuracy, and (ii) incorporates a dynamic shortlisting mechanism that filters\nrelevant tools at runtime, reducing prompt complexity while maintaining\nscalability. We validate our framework on both proprietary and open-source APIs\nand demonstrate its integration with agentic frameworks. To the best of our\nknowledge, ACE is the first end-to-end framework that automates the creation,\nenrichment, and dynamic selection of enterprise API tools for LLM agents.", "AI": {"tldr": "ACE is a novel framework that transforms enterprise APIs into LLM-compatible tools through automated documentation enrichment and dynamic selection, significantly improving agent performance in complex enterprise workflows.", "motivation": "Poor API documentation, complex schemas, and extensive operations hinder accurate tool selection and payload formation in enterprise LLM applications, reducing accuracy by up to 25%.", "method": "ACE employs two core mechanisms: (1) generating enriched tool specifications with parameter descriptions and examples, and (2) a dynamic runtime shortlisting mechanism to filter relevant tools, reducing complexity while maintaining scalability.", "result": "Validation on proprietary and open-source APIs demonstrated ACE's integration with agentic frameworks, confirming its effectiveness as the first end-to-end solution for this problem.", "conclusion": "The ACE framework effectively automates the creation, enrichment, and dynamic selection of enterprise API tools, enhancing LLM agents' efficiency and accuracy in enterprise environments."}}
{"id": "2509.10573", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.10573", "abs": "https://arxiv.org/abs/2509.10573", "authors": ["Christophe Parisel"], "title": "Directionality of the Voynich Script", "comment": null, "summary": "While the Voynich Manuscript was almost certainly written left-to-right\n(LTR), the question whether the underlying script or cipher reads LTR or\nright-to-left (RTL) has received little quantitative attention. We introduce a\nstatistical method that leverages n-gram perplexity asymmetry to determine\ndirectional bias in character sequences.", "AI": {"tldr": "Authors present a statistical n-gram perplexity method to analyze the Voynich Manuscript's script direction, offering a quantitative approach to a historically unresolved problem.", "motivation": "The Voynich Manuscript's script/cipher direction remains debated, and prior studies lack quantitative methods to address this question systematically.", "method": "The authors developed a statistical approach that calculates n-gram perplexity asymmetry in character sequences to detect directional reading bias (LTR vs RTL) in the manuscript.", "result": "The method demonstrates the ability to identify directional bias in character sequences, potentially resolving the ambiguity of whether the underlying system reads LTR or RTL.", "conclusion": "This paper concludes that the proposed statistical method provides a viable approach to determining the directional bias of the Voynich Manuscript's script or cipher, offering a new analytical tool for such historical cryptographic problems."}}
{"id": "2509.11686", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11686", "abs": "https://arxiv.org/abs/2509.11686", "authors": ["Jian Wang", "Xiaofei Xie", "Qiang Hu", "Shangqing Liu", "Yi Li"], "title": "Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based Information for Code Large Language Models", "comment": "EMNLP2025-findings", "summary": "Code Large Language Models (Code LLMs) have opened a new era in programming\nwith their impressive capabilities. However, recent research has revealed\ncritical limitations in their ability to reason about runtime behavior and\nunderstand the actual functionality of programs, which poses significant\nchallenges for their post-training and practical deployment. Specifically, Code\nLLMs encounter two principal issues: (1) a lack of proficiency in reasoning\nabout program execution behavior, as they struggle to interpret what programs\nactually do during runtime, and (2) the inconsistent and fragmented\nrepresentation of semantic information, such as execution traces, across\nexisting methods, which hinders their ability to generalize and reason\neffectively. These challenges underscore the necessity for more systematic\napproaches to enhance the reasoning capabilities of Code LLMs. To address these\nissues, we introduce a generic framework to support integrating semantic\ninformation~(e.g., execution trace) to code task-relevant prompts, and conduct\na comprehensive study to explore the role of semantic information in enhancing\nthe reasoning ability of Code LLMs accordingly. Specifically, we focus on\ninvestigating the usefulness of trace-based semantic information in boosting\nsupervised fine-tuning~(SFT) and post-phase inference of Code LLMs. The\nexperimental results surprisingly disagree with previous works and demonstrate\nthat semantic information has limited usefulness for SFT and test time scaling\nof Code LLM.", "AI": {"tldr": "This study found that integrating semantic execution traces into Code LLMs provides minimal improvement in reasoning capabilities during training and testing, suggesting new approaches are needed.", "motivation": "Current Code LLMs struggle with runtime behavior reasoning and fragmented semantic representations, creating challenges for practical deployment and limiting generalization.", "method": "A generic framework was developed to integrate trace-based semantic information into code task-relevant prompts, combined with a comprehensive analysis of semantic information's role in strengthening reasoning in Code LLMs.", "result": "Experiments revealed that trace-based semantic information provided limited benefits for supervised fine-tuning and test-time scaling of Code LLMs, contradicting prior assumptions about its utility.", "conclusion": "The study highlights the need for improved frameworks to enhance Code LLMs' reasoning capabilities, as current semantic information integration shows limited effectiveness in supervised fine-tuning and test-time scaling."}}
{"id": "2509.10577", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10577", "abs": "https://arxiv.org/abs/2509.10577", "authors": ["Danilo Francati", "Yevin Nikhel Goonatilake", "Shubham Pawar", "Daniele Venturi", "Giuseppe Ateniese"], "title": "The Coding Limits of Robust Watermarking for Generative Models", "comment": null, "summary": "We prove a sharp threshold for the robustness of cryptographic watermarking\nfor generative models. This is achieved by introducing a coding abstraction,\nwhich we call messageless secret-key codes, that formalizes sufficient and\nnecessary requirements of robust watermarking: soundness, tamper detection, and\npseudorandomness. Thus, we establish that robustness has a precise limit: For\nbinary outputs no scheme can survive if more than half of the encoded bits are\nmodified, and for an alphabet of size q the corresponding threshold is\n$(1-1/q)$ of the symbols.\n  Complementing this impossibility, we give explicit constructions that meet\nthe bound up to a constant slack. For every ${\\delta} > 0$, assuming\npseudorandom functions and access to a public counter, we build linear-time\ncodes that tolerate up to $(1/2)(1-{\\delta})$ errors in the binary case and\n$(1-1/q)(1-{\\delta})$ errors in the $q$-ary case. Together with the lower\nbound, these yield the maximum robustness achievable under standard\ncryptographic assumptions.\n  We then test experimentally whether this limit appears in practice by looking\nat the recent watermarking for images of Gunn, Zhao, and Song (ICLR 2025). We\nshow that a simple crop and resize operation reliably flipped about half of the\nlatent signs and consistently prevented belief-propagation decoding from\nrecovering the codeword, erasing the watermark while leaving the image visually\nintact.\n  These results provide a complete characterization of robust watermarking,\nidentifying the threshold at which robustness fails, constructions that achieve\nit, and an experimental confirmation that the threshold is already reached in\npractice.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.11691", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11691", "abs": "https://arxiv.org/abs/2509.11691", "authors": ["Lukas Rauh", "Mel-Rick S\u00fcner", "Daniel Schel", "Thomas Bauernhansl"], "title": "AI Asset Management for Manufacturing (AIM4M): Development of a Process Model for Operationalization", "comment": "10 pages, 4 figures, submitted for revision review at International\n  Conference on Industry of the Future and Smart Manufacturing (ISM) 2025", "summary": "The benefits of adopting artificial intelligence (AI) in manufacturing are\nundeniable. However, operationalizing AI beyond the prototype, especially when\ninvolved with cyber-physical production systems (CPPS), remains a significant\nchallenge due to the technical system complexity, a lack of implementation\nstandards and fragmented organizational processes. To this end, this paper\nproposes a new process model for the lifecycle management of AI assets designed\nto address challenges in manufacturing and facilitate effective\noperationalization throughout the entire AI lifecycle. The process model, as a\ntheoretical contribution, builds on machine learning operations (MLOps)\nprinciples and refines three aspects to address the domain-specific\nrequirements from the CPPS context. As a result, the proposed process model\naims to support organizations in practice to systematically develop, deploy and\nmanage AI assets across their full lifecycle while aligning with CPPS-specific\nconstraints and regulatory demands.", "AI": {"tldr": "Proposes an AI lifecycle process model for manufacturing integrating MLOps and CPPS requirements", "motivation": "Despite AI benefits in manufacturing, operationalizing AI beyond prototypes remains challenging due to technical complexity, lack of standards, and fragmented organizational processes in cyber-physical production systems (CPPS).", "method": "Developed a process model based on MLOps principles refined for CPPS domain-specific requirements, focusing on three key aspects for lifecycle management of AI assets.", "result": "A theoretical framework enabling systematic development, deployment, and management of AI assets aligned with CPPS constraints and regulatory demands.", "conclusion": "The process model addresses operationalization barriers in manufacturing by providing structured AI lifecycle management tailored to CPPS environments through MLOps adaptation."}}
{"id": "2509.10581", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.10581", "abs": "https://arxiv.org/abs/2509.10581", "authors": ["Prokash Barman", "Ratul Chowdhury", "Banani Saha"], "title": "Multi-channel secure communication framework for wireless IoT (MCSC-WoT): enhancing security in Internet of Things", "comment": null, "summary": "In modern smart systems, the convergence of the Internet of Things (IoT) and\nWireless of Things (WoT) have been revolutionized by offering a broad level of\nwireless connectivity and communication among various devices. Hitherto, this\ngreater interconnectivity poses important security problems, including the\nquestion of how to securely interconnect different networks, preserve secure\ncommunication channels, and maintain data integrity. However, the traditional\ncryptographic method and frequency hopping technique, although they provide\nsome protection, are not sufficient to defend against Man-In-The-Middle,\njamming, and replay attacks. In addition, synchronization issues in\nmulti-channel communication systems result in increased latency and energy\nconsumption, which make them unsuitable for resource-constrained IoT and WoT\ndevices. This work presents the Multi-Channel Secure Communication (MCSC)\nframework, which integrates advanced cryptographic protocols with dynamic\nchannel-hopping strategies to enhance security with reduced synchronization\noverhead. The MCSC framework maximizes the critical performance metrics, such\nas packet delivery ratio, latency, throughput, and energy efficiency, and\nfulfills the specific requirements of the IoT and WoT networks. A comprehensive\ncomparison of MCSC with well-established methods, including Frequency Hop\nSpread Spectrum, single channel Advanced Encryption Standard, and various\nElliptic Curve Cryptography-based schemes, indicates that MCSC has lower error\nrates and is more resilient to a wider range of cyber attacks. The efficiency\nof the proposed solution to secure IoT and WoT networks without compromising\nthe operational performance is validated under various interference conditions.", "AI": {"tldr": "This paper proposes MCSC, a framework combining dynamic channel-hopping and advanced cryptography to secure IoT/WoT networks while optimizing performance metrics.", "motivation": "Existing security solutions for IoT/WoT face limitations against evolving attacks (MITM, jamming) and suffer from synchronization challenges that increase latency and energy consumption in resource-constrained devices.", "method": "MCSC integrates advanced cryptographic protocols with dynamic channel-hopping strategies to reduce synchronization overhead and enhance security metrics like packet delivery ratio and energy efficiency.", "result": "MCSC demonstrates lower error rates, stronger resilience to diverse cyber threats, and outperforms Frequency Hop Spread Spectrum, AES, and ECC schemes in critical performance metrics through comprehensive comparisons.", "conclusion": "The proposed MCSC framework effectively secures IoT and WoT networks without compromising operational performance, validated under various interference conditions."}}
{"id": "2509.11708", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11708", "abs": "https://arxiv.org/abs/2509.11708", "authors": ["Zhantong Xue", "Pingchuan Ma", "Zhaoyu Wang", "Shuai Wang"], "title": "From Evaluation to Enhancement: Large Language Models for Zero-Knowledge Proof Code Generation", "comment": null, "summary": "Zero-knowledge proofs (ZKPs) are increasingly deployed in domains such as\nprivacy-preserving authentication, blockchain scalability, and secure finance.\nHowever, authoring ZK programs remains challenging: unlike mainstream\nprogramming, ZK development requires reasoning about finite field arithmetic,\nconstraint systems, and gadgets, making it knowledge-intensive and error-prone.\nWhile large language models (LLMs) have demonstrated strong code generation\ncapabilities in general-purpose languages, their effectiveness for ZK\nprogramming, where correctness hinges on both language mastery and gadget-level\nreasoning, remains unexplored. To address this gap, we propose\n\\textsc{ZK-Eval}, a domain-specific evaluation pipeline that probes LLM\ncapabilities at three levels: language knowledge, gadget competence, and\nend-to-end program generation. Our evaluation of four state-of-the-art LLMs\nreveals that models excel at surface-level syntax but struggle with gadget\nusage and semantic correctness, often yielding incorrect programs. Based on\nthese insights, we introduce \\textsc{ZK-Coder}, an agentic framework that\naugments LLMs with constraint sketching, guided retrieval, and interactive\nrepair. Experiments on Circom and Noir show substantial gains, with success\nrates improving from 17.35\\% to 83.38\\% and from 32.21\\% to 90.05\\%,\nrespectively. With \\textsc{ZK-Eval} and \\textsc{ZK-Coder}, we establish a\nfoundation for systematically measuring and augmenting LLMs in ZK code\ngeneration to lower barriers for practitioners and advance trustworthy\ncomputation.", "AI": {"tldr": "This paper evaluates LLMs for zero-knowledge programming, identifies gaps in gadget reasoning, and introduces a framework (ZK-Coder) to boost code correctness by 50\u201360 percentage points.", "motivation": "While LLMs excel in general programming, their effectiveness in ZK programming\u2014requiring finite field arithmetic and gadget reasoning\u2014remains unexplored, with significant practical implications for privacy and security domains.", "method": "The authors propose ZK-Eval, a three-level evaluation pipeline (language knowledge, gadget competence, end-to-end generation), and ZK-Coder, an agentic framework with constraint sketching, guided retrieval, and interactive repair.", "result": "On Circom and Noir, ZK-Coder improves success rates from 17.35% to 83.38% and 32.21% to 90.05%, respectively, demonstrating substantial gains over baseline LLMs.", "conclusion": "The study establishes ZK-Eval and ZK-Coder as a framework for evaluating and enhancing LLMs in ZK code generation, aiming to reduce entry barriers for practitioners and advance trustworthy computation."}}
{"id": "2509.10655", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.10655", "abs": "https://arxiv.org/abs/2509.10655", "authors": ["Charankumar Akiri", "Harrison Simpson", "Kshitiz Aryal", "Aarav Khanna", "Maanak Gupta"], "title": "Safety and Security Analysis of Large Language Models: Risk Profile and Harm Potential", "comment": null, "summary": "While the widespread deployment of Large Language Models (LLMs) holds great\npotential for society, their vulnerabilities to adversarial manipulation and\nexploitation can pose serious safety, security, and ethical risks. As new\nthreats continue to emerge, it becomes critically necessary to assess the\nlandscape of LLMs' safety and security against evolving adversarial prompt\ntechniques. To understand the behavior of LLMs, this research provides an\nempirical analysis and risk profile of nine prominent LLMs, Claude Opus 4,\nDeepSeek V3 (both open-source and online), Gemini 2.5 Flash, GPT-4o, Grok 3,\nLlama 4 Scout, Mistral 7B, and Qwen 3 1.7B, against 24 different security and\nsafety categories. These LLMs are evaluated on their ability to produce harmful\nresponses for adversarially crafted prompts (dataset has been made public) for\na broad range of safety and security topics, such as promotion of violent\ncriminal behavior, promotion of non-violent criminal activity, societal harms\nrelated to safety, illegal sexual content, dangerous code generation, and\ncybersecurity threats beyond code. Our study introduces the Risk Severity Index\n(RSI), an agile and scalable evaluation score, to quantify and compare the\nsecurity posture and creating a risk profile of LLMs. As the LLM development\nlandscape progresses, the RSI is intended to be a valuable metric for comparing\nthe risks of LLMs across evolving threats. This research finds widespread\nvulnerabilities in the safety filters of the LLMs tested and highlights the\nurgent need for stronger alignment, responsible deployment practices, and model\ngovernance, particularly for open-access and rapidly iterated models.", "AI": {"tldr": "The paper assesses the safety and security of various LLMs against adversarial prompts, introduces a Risk Severity Index, and identifies vulnerabilities calling for improved governance.", "motivation": "The motivation revolves around understanding the safety and security risks of widely used LLMs as adversarial techniques evolve, emphasizing the potential societal threats from these vulnerabilities.", "method": "An empirical analysis is conducted by evaluating nine prominent LLMs across 24 security and safety categories, using adversarial prompts to test their generation of harmful outputs.", "result": "Observed vulnerabilities in safety filters and a preliminary evaluation of the Risk Severity Index (RSI) as a scalable metric to quantify security postures and risk levels of LLMs.", "conclusion": "There is an urgent need for enhanced alignment, responsible deployment practices, and model governance in LLM development to mitigate the identified risks."}}
{"id": "2509.11738", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11738", "abs": "https://arxiv.org/abs/2509.11738", "authors": ["Maria K\u00fc\u00fcsvek", "Hina Anwar"], "title": "Toward Greener Background Processes -- Measuring Energy Cost of Autosave Feature", "comment": "Author version. Accepted for publication in the proceedings of the\n  International Conference on Product-Focused Software Process Improvement\n  (PROFES 2025)", "summary": "Background processes in desktop applications are often overlooked in energy\nconsumption studies, yet they represent continuous, automated workloads with\nsignificant cumulative impact. This paper introduces a reusable process for\nevaluating the energy behavior of such features at the level of operational\ndesign. The process works in three phases: 1) decomposing background\nfunctionality into core operations, 2) operational isolation, and 3) controlled\nmeasurements enabling comparative profiling. We instantiate the process in a\ncase study of autosave implementations across three open-source Python-based\ntext editors. Using 900 empirical software-based energy measurements, we\nidentify key design factors affecting energy use, including save frequency,\nbuffering strategy, and auxiliary logic such as change detection. We give four\nactionable recommendations for greener implementations of autosave features in\nPython to support sustainable software practices.", "AI": {"tldr": "This paper proposes a reusable process to evaluate energy consumption of background processes in desktop apps, identifies design factors in autosave features, and offers four recommendations for sustainable Python-based software development.", "motivation": "Background processes in desktop applications are often overlooked in energy studies despite their significant cumulative impact. The paper aims to address this gap by evaluating energy behavior at the operational design level.", "method": "The method involves a three-phase process: decomposing background functionality into core operations, operational isolation, and controlled measurements for comparative profiling. This was applied to autosave features in three open-source Python text editors with 900 empirical energy measurements.", "result": "Key design factors affecting energy use include save frequency, buffering strategy, and auxiliary logic like change detection. Four actionable recommendations for greener autosave implementations in Python were provided.", "conclusion": "The paper concludes that by identifying key design factors and providing actionable recommendations, developers can create more energy-efficient autosave features in Python, promoting sustainable software practices."}}
{"id": "2509.10682", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10682", "abs": "https://arxiv.org/abs/2509.10682", "authors": ["Vitor Hugo Galhardo Moia", "Igor Jochem Sanz", "Gabriel Antonio Fontes Rebello", "Rodrigo Duarte de Meneses", "Briland Hitaj", "Ulf Lindqvist"], "title": "LLM in the Middle: A Systematic Review of Threats and Mitigations to Real-World LLM-based Systems", "comment": "37 pages, 8 figures, 13 tables", "summary": "The success and wide adoption of generative AI (GenAI), particularly large\nlanguage models (LLMs), has attracted the attention of cybercriminals seeking\nto abuse models, steal sensitive data, or disrupt services. Moreover, providing\nsecurity to LLM-based systems is a great challenge, as both traditional threats\nto software applications and threats targeting LLMs and their integration must\nbe mitigated. In this survey, we shed light on security and privacy concerns of\nsuch LLM-based systems by performing a systematic review and comprehensive\ncategorization of threats and defensive strategies considering the entire\nsoftware and LLM life cycles. We analyze real-world scenarios with distinct\ncharacteristics of LLM usage, spanning from development to operation. In\naddition, threats are classified according to their severity level and to which\nscenarios they pertain, facilitating the identification of the most relevant\nthreats. Recommended defense strategies are systematically categorized and\nmapped to the corresponding life cycle phase and possible attack strategies\nthey attenuate. This work paves the way for consumers and vendors to understand\nand efficiently mitigate risks during integration of LLMs in their respective\nsolutions or organizations. It also enables the research community to benefit\nfrom the discussion of open challenges and edge cases that may hinder the\nsecure and privacy-preserving adoption of LLM-based systems.", "AI": {"tldr": "This survey systematically categorizes security threats and defenses for LLM-based systems across their life cycles, aiding stakeholders in risk mitigation and highlighting open challenges for secure AI adoption.", "motivation": "The growing adoption of LLMs has drawn cybercriminals seeking to exploit them, necessitating effective security measures against both traditional and novel threats targeting LLMs and their integration.", "method": "Systematic review and categorization of threats and defenses across the software and LLM life cycles, analyzing real-world scenarios, classifying threats by severity and context, and mapping defense strategies to life cycle phases and attack strategies.", "result": "A structured analysis of threats, their classification by severity and scenario, and systematically categorized defense strategies mapped to life cycle phases and corresponding attack attenuation.", "conclusion": "This survey provides a comprehensive categorization of security and privacy threats in LLM-based systems, offering defense strategies mapped to life cycles and attack scenarios. It aims to assist stakeholders in mitigating risks and addressing open challenges for secure LLM adoption."}}
{"id": "2509.11748", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.11748", "abs": "https://arxiv.org/abs/2509.11748", "authors": ["Marius Mignard", "Steven Costiou", "Nicolas Anquetil", "Anne Etien"], "title": "Analysing Python Machine Learning Notebooks with Moose", "comment": null, "summary": "Machine Learning (ML) code, particularly within notebooks, often exhibits\nlower quality compared to traditional software. Bad practices arise at three\ndistinct levels: general Python coding conventions, the organizational\nstructure of the notebook itself, and ML-specific aspects such as\nreproducibility and correct API usage. However, existing analysis tools\ntypically focus on only one of these levels and struggle to capture ML-specific\nsemantics, limiting their ability to detect issues. This paper introduces\nVespucci Linter, a static analysis tool with multi-level capabilities, built on\nMoose and designed to address this challenge. Leveraging a metamodeling\napproach that unifies the notebook's structural elements with Python code\nentities, our linter enables a more contextualized analysis to identify issues\nacross all three levels. We implemented 22 linting rules derived from the\nliterature and applied our tool to a corpus of 5,000 notebooks from the Kaggle\nplatform. The results reveal violations at all levels, validating the relevance\nof our multi-level approach and demonstrating Vespucci Linter's potential to\nimprove the quality and reliability of ML development in notebook environments.", "AI": {"tldr": "Vespucci Linter is a multi-level static analysis tool for improving ML code quality in notebooks by addressing general Python, notebook structure, and ML-specific issues.", "motivation": "Machine Learning code in notebooks often has lower quality due to bad practices at three levels: Python coding conventions, notebook organization, and ML-specific aspects like reproducibility and API usage. Existing tools only focus on one level and miss ML semantics.", "method": "Vespucci Linter uses a metamodeling approach to unify notebook structure with Python code entities, enabling multi-level static analysis. Implemented 22 rules based on the literature.", "result": "Applied to 5,000 Kaggle notebooks, Vespucci found violations at all three levels, showing its effectiveness and highlighting widespread issues in ML notebooks.", "conclusion": "Vespucci Linter's multi-level approach successfully addresses multiple ML code quality aspects in notebooks, offering potential to enhance quality and reliability in data science workflows."}}
