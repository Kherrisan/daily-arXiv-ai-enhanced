{"id": "2508.21097", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21097", "abs": "https://arxiv.org/abs/2508.21097", "authors": ["Nazanin Siavash", "Armin Moin"], "title": "Model-Driven Quantum Code Generation Using Large Language Models and Retrieval-Augmented Generation", "comment": "This paper is accepted to the New Ideas and Emerging Results (NIER)\n  track of the ACM/IEEE 28th International Conference on Model Driven\n  Engineering Languages and Systems (MODELS)", "summary": "This paper introduces a novel research direction for model-to-text/code\ntransformations by leveraging Large Language Models (LLMs) that can be enhanced\nwith Retrieval-Augmented Generation (RAG) pipelines. The focus is on quantum\nand hybrid quantum-classical software systems, where model-driven approaches\ncan help reduce the costs and mitigate the risks associated with the\nheterogeneous platform landscape and lack of developers' skills. We validate\none of the proposed ideas regarding generating code out of UML model instances\nof software systems. This Python code uses a well-established library, called\nQiskit, to execute on gate-based or circuit-based quantum computers. The RAG\npipeline that we deploy incorporates sample Qiskit code from public GitHub\nrepositories. Experimental results show that well-engineered prompts can\nimprove CodeBLEU scores by up to a factor of four, yielding more accurate and\nconsistent quantum code. However, the proposed research direction can go beyond\nthis through further investigation in the future by conducting experiments to\naddress our other research questions and ideas proposed here, such as deploying\nsoftware system model instances as the source of information in the RAG\npipelines, or deploying LLMs for code-to-code transformations, for instance,\nfor transpilation use cases.", "AI": {"tldr": "LLMs with RAG pipelines improve quantum code generation from models, boosting accuracy fourfold, but future work is needed to expand applications like code transpilation and model-driven RAG integration.", "motivation": "Developing quantum/hybrid quantum-classical software systems faces challenges due to platform heterogeneity and developer skill gaps. Model-driven approaches aim to mitigate risks and reduce costs by enabling systematic code generation from models.", "method": "The method employs Retrieval-Augmented Generation (RAG) pipelines to enhance Large Language Models (LLMs). It validates code generation from UML software system models using Qiskit, a quantum computing library, augmented with GitHub-sourced code. Optimized prompts were tested to improve output quality.", "result": "Experiments demonstrated a 4\u00d7 improvement in CodeBLEU scores when using well-designed prompts, resulting in more accurate and consistent quantum code. However, limitations in current implementations were noted, motivating further investigation.", "conclusion": "The paper outlines a promising research direction leveraging LLMs and RAG pipelines for model-to-code transformations, particularly in quantum systems. While current results show improved CodeBLEU scores, the study emphasizes the need for further experiments to explore additional research questions like model-based RAG sources and code-to-code transpilation."}}
{"id": "2508.21107", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21107", "abs": "https://arxiv.org/abs/2508.21107", "authors": ["Dongjun Lee", "Changho Hwang", "Kimin Lee"], "title": "Learning to Generate Unit Test via Adversarial Reinforcement Learning", "comment": "Code is available at: https://github.com/dgjun32/UTRL", "summary": "Unit testing is a core practice in programming, enabling systematic\nevaluation of programs produced by human developers or large language models\n(LLMs). Given the challenges in writing comprehensive unit tests, LLMs have\nbeen employed to automate test generation, yet methods for training LLMs to\nproduce high-quality tests remain underexplored. In this work, we propose UTRL,\na novel reinforcement learning framework that trains an LLM to generate\nhigh-quality unit tests given a programming instruction. Our key idea is to\niteratively train two LLMs, the unit test generator and the code generator, in\nan adversarial manner via reinforcement learning. The unit test generator is\ntrained to maximize a discrimination reward, which reflects its ability to\nproduce tests that expose faults in the code generator's solutions, and the\ncode generator is trained to maximize a code reward, which reflects its ability\nto produce solutions that pass the unit tests generated by the test generator.\nIn our experiments, we demonstrate that unit tests generated by Qwen3-4B\ntrained via UTRL show higher quality compared to unit tests generated by the\nsame model trained via supervised fine-tuning on human-written ground-truth\nunit tests, yielding code evaluations that more closely align with those\ninduced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL\noutperforms frontier models such as GPT-4.1 in generating high-quality unit\ntests, highlighting the effectiveness of UTRL in training LLMs for this task.", "AI": {"tldr": "Adversarial RL framework (UTRL) trains LLMs to generate better unit tests, outperforming existing methods including GPT-4.1", "motivation": "Current methods for training LLMs in unit test generation lack effectiveness, as supervised approaches on human-written tests fail to achieve optimal performance, creating a need for better training paradigms.", "method": "A reinforcement learning framework (UTRL) trains two adversarial LLMs: (1) a unit test generator maximizing fault-exposure rewards, and (2) a code generator maximizing pass-rate rewards. Training involves iterative adversarial optimization.", "result": "Qwen3-4B trained with UTRL outperforms supervised baselines (by improving test quality) and frontier models like GPT-4.1 in test generation, producing tests that yield code evaluations closer to ground-truth test outcomes.", "conclusion": "UTRL effectively trains LLMs to generate high-quality unit tests through adversarial reinforcement learning, surpassing existing methods including frontier models like GPT-4.1."}}
{"id": "2508.21156", "categories": ["cs.SE", "D.2.7; I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.21156", "abs": "https://arxiv.org/abs/2508.21156", "authors": ["Kiana Kiashemshaki", "Arsham Khosravani", "Alireza Hosseinpour", "Arshia Akhavan"], "title": "Automated Bug Triaging using Instruction-Tuned Large Language Models", "comment": "11 pages, 7 figures", "summary": "Bug triaging, the task of assigning new issues to developers, is often slow\nand inconsistent in large projects. We present a lightweight framework that\ninstruction-tuned large language model (LLM) with LoRA adapters and uses\ncandidate-constrained decoding to ensure valid assignments. Tested on\nEclipseJDT and Mozilla datasets, the model achieves strong shortlist quality\n(Hit at 10 up to 0.753) despite modest exact Top-1 accuracy. On recent\nsnapshots, accuracy rises sharply, showing the framework's potential for\nreal-world, human-in-the-loop triaging. Our results suggest that\ninstruction-tuned LLMs offer a practical alternative to costly feature\nengineering and graph-based methods.", "AI": {"tldr": "This paper introduces a lightweight LLM-based framework for bug triaging using LoRA and constrained decoding, achieving strong shortlist performance and demonstrating practical advantages over traditional methods.", "motivation": "Large projects often face slow and inconsistent bug triaging, requiring a lightweight and effective solution to improve assignment accuracy.", "method": "A framework that combines instruction-tuned large language models (LLMs) with LoRA adapters and employs candidate-constrained decoding to ensure valid bug assignments.", "result": "The model achieved a Hit at 10 score of up to 0.753 on EclipseJDT and Mozilla datasets, with improved accuracy on recent snapshots, showing effectiveness in shortlist quality and real-world potential.", "conclusion": "The framework using instruction-tuned LLMs with LoRA adapters provides a practical alternative to complex feature engineering and graph-based methods for bug triaging, demonstrating strong performance in real-world scenarios."}}
{"id": "2508.21433", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21433", "abs": "https://arxiv.org/abs/2508.21433", "authors": ["Tobias Lindenbauer", "Igor Slinko", "Ludwig Felder", "Egor Bogomolov", "Yaroslav Zharov"], "title": "The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management", "comment": null, "summary": "Large Language Model (LLM)-based agents solve complex tasks through iterative\nreasoning, exploration, and tool-use, a process that can result in long,\nexpensive context histories. While state-of-the-art Software Engineering ( SE)\nagents like OpenHands or Cursor use LLM-based summarization to tackle this\nissue, it is unclear whether the increased complexity offers tangible\nperformance benefits compared to simply omitting older observations. We present\na systematic comparison of these strategies within SWE-agent on SWE-bench\nVerified across five diverse model configurations. We find that a simple\nobservation-masking strategy halves cost relative to a raw agent while\nmatching, and sometimes slightly exceeding, the solve rate of LLM\nsummarization. For example, with Qwen3-Coder 480B, masking improves solve rate\nfrom 53.8% (raw agent) to 54.8%, while remaining competitive with summarization\nat a lower cost. These results suggest that, at least within SWE-agent on\nSWE-bench Verified, the most effective and efficient context management can be\nthe simplest. We release code and data for reproducibility", "AI": {"tldr": "The paper compares observation-masking and LLM-summarization for context management in SWE-agents, finding that masking is cost-effective and competitive with summarization on SWE-bench Verified.", "motivation": "State-of-the-art SE agents use LLM-summarization for context management, but its performance benefits over simpler methods (e.g., masking older observations) and cost trade-offs remain unclear.", "method": "Systematic comparison of masking vs. summarization across five diverse model configurations in SWE-agent on SWE-bench Verified benchmark.", "result": "Masking reduced costs by half while matching/tying summary solve rates (e.g., Qwen3-Coder 480B: 53.8% \u219254.8%), achieving robust performance with minimal complexity.", "conclusion": "Observation-masking emerges as the most effective and efficient strategy for SWE-agents; simpler approaches can outperform complex ones in cost and task completion when properly optimized. Code and data are released for reproducibility."}}
{"id": "2508.21219", "categories": ["cs.CR", "cs.ET", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.21219", "abs": "https://arxiv.org/abs/2508.21219", "authors": ["A H M Nazmus Sakib", "Mahsin Bin Akram", "Joseph Spracklen", "Sahan Kalutarage", "Raveen Wijewickrama", "Igor Bilogrevic", "Murtuza Jadliwala"], "title": "The WASM Cloak: Evaluating Browser Fingerprinting Defenses Under WebAssembly based Obfuscation", "comment": null, "summary": "Browser fingerprinting defenses have historically focused on detecting\nJavaScript(JS)-based tracking techniques. However, the widespread adoption of\nWebAssembly (WASM) introduces a potential blind spot, as adversaries can\nconvert JS to WASM's low-level binary format to obfuscate malicious logic. This\npaper presents the first systematic evaluation of how such WASM-based\nobfuscation impacts the robustness of modern fingerprinting defenses. We\ndevelop an automated pipeline that translates real-world JS fingerprinting\nscripts into functional WASM-obfuscated variants and test them against two\nclasses of defenses: state-of-the-art detectors in research literature and\ncommercial, in-browser tools. Our findings reveal a notable divergence:\ndetectors proposed in the research literature that rely on feature-based\nanalysis of source code show moderate vulnerability, stemming from outdated\ndatasets or a lack of WASM compatibility. In contrast, defenses such as browser\nextensions and native browser features remained completely effective, as their\nAPI-level interception is agnostic to the script's underlying implementation.\nThese results highlight a gap between academic and practical defense strategies\nand offer insights into strengthening detection approaches against WASM-based\nobfuscation, while also revealing opportunities for more evasive techniques in\nfuture attacks.", "AI": {"tldr": "Study evaluates WASM obfuscation impact on browser fingerprinting defenses by automating JS-to-WASM script translation and testing across academic/research and commercial defense types. Finds academic methods struggle with WASM compatibility while commercial API-level tools remain robust, revealing a research-practice gap.", "motivation": "WebAssembly's adoption creates a blind spot in browser fingerprinting defenses, which historically focus on JavaScript. Adversaries could exploit this by obfuscating tracking logic in WASM binary format. This work addresses the unknown impact of WASM obfuscation on defense robustness.", "method": "Developed an automated pipeline to convert real-world JavaScript fingerprinting scripts into functional WASM-obfuscated variants. Tested these against two defense classes: state-of-the-art research detectors and commercial in-browser tools using API-level interception.", "result": "Academic research detectors relying on feature-based source-code analysis showed moderate vulnerability to WASM obfuscation, attributed to outdated datasets and WASM compatibility issues. Commercial defenses like browser extensions and native features remained 100% effective due to their API-level interception mechanism, which is implementation-agnostic.", "conclusion": "This study highlights a significant gap between academic and practical browser fingerprinting defenses when faced with WebAssembly (WASM)-based obfuscation. While research literature detectors struggle due to WASM incompatibility, API-level defenses like browser extensions remain robust. The findings emphasize the need to strengthen academic approaches against emerging obfuscation techniques and expose potential paths for more evasive future attacks."}}
{"id": "2508.21454", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21454", "abs": "https://arxiv.org/abs/2508.21454", "authors": ["Baijun Cheng", "Kailong Wang", "Ling Shi", "Haoyu Wang", "Yao Guo", "Ding Li", "Xiangqun Chen"], "title": "Enhancing Semantic Understanding in Pointer Analysis using Large Language Models", "comment": "Accepted by LMPL 2025", "summary": "Pointer analysis has been studied for over four decades. However, existing\nframeworks continue to suffer from the propagation of incorrect facts. A major\nlimitation stems from their insufficient semantic understanding of code,\nresulting in overly conservative treatment of user-defined functions. Recent\nadvances in large language models (LLMs) present new opportunities to bridge\nthis gap. In this paper, we propose LMPA (LLM-enhanced Pointer Analysis), a\nvision that integrates LLMs into pointer analysis to enhance both precision and\nscalability. LMPA identifies user-defined functions that resemble system APIs\nand models them accordingly, thereby mitigating erroneous cross-calling-context\npropagation. Furthermore, it enhances summary-based analysis by inferring\ninitial points-to sets and introducing a novel summary strategy augmented with\nnatural language. Finally, we discuss the key challenges involved in realizing\nthis vision.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.21302", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21302", "abs": "https://arxiv.org/abs/2508.21302", "authors": ["Jie Zhu", "Chihao Shen", "Ziyang Li", "Jiahao Yu", "Yizheng Chen", "Kexin Pei"], "title": "Locus: Agentic Predicate Synthesis for Directed Fuzzing", "comment": null, "summary": "Directed fuzzing aims to find program inputs that lead to specified target\nprogram states. It has broad applications, such as debugging system crashes,\nconfirming reported bugs, and generating exploits for potential\nvulnerabilities. This task is inherently challenging because target states are\noften deeply nested in the program, while the search space manifested by\nnumerous possible program inputs is prohibitively large. Existing approaches\nrely on branch distances or manually-specified constraints to guide the search;\nhowever, the branches alone are often insufficient to precisely characterize\nprogress toward reaching the target states, while the manually specified\nconstraints are often tailored for specific bug types and thus difficult to\ngeneralize to diverse target states and programs.\n  We present Locus, a novel framework to improve the efficiency of directed\nfuzzing. Our key insight is to synthesize predicates to capture fuzzing\nprogress as semantically meaningful intermediate states, serving as milestones\ntowards reaching the target states. When used to instrument the program under\nfuzzing, they can reject executions unlikely to reach the target states, while\nproviding additional coverage guidance. To automate this task and generalize to\ndiverse programs, Locus features an agentic framework with program analysis\ntools to synthesize and iteratively refine the candidate predicates, while\nensuring the predicates strictly relax the target states to prevent false\nrejections via symbolic execution. Our evaluation shows that Locus\nsubstantially improves the efficiency of eight state-of-the-art fuzzers in\ndiscovering real-world vulnerabilities, achieving an average speedup of 41.6x.\nSo far, Locus has found eight previously unpatched bugs, with one already\nacknowledged with a draft patch.", "AI": {"tldr": "Locus improves directed fuzzing by auto-generating semantic predicates as milestones. This enables fuzzers to 1) reject unproductive paths, 2) guide coverage more effectively, and 3) discover bugs 41.6x faster on average. Found 8 new real-world bugs including one patched.", "motivation": "Traditional directed fuzzing methods rely on imprecise branch distance metrics or manually-crafted constraints that lack generality. These approaches often fail to effectively guide fuzzers toward deeply nested target states due to insufficient semantic understanding of program progress.", "method": "Locus synthesizes predicates as intermediate states through an agentic framework with program analysis tools. These predicates are automatically generated, iteratively refined, and validated via symbolic execution to ensure they safely relax target states without false rejections. The framework rejects unproductive executions and provides coverage guidance during fuzzing.", "result": "Locus achieves 41.6x speedup on average across 8 state-of-the-art fuzzers. It discovers 8 new unpatched real-world vulnerabilities, including one with a draft patch implemented. The framework demonstrates effectiveness both for specific target states and broad classes of program behaviors.", "conclusion": "The Locus framework outperforms existing directed fuzzing techniques by automating predicate synthesis to create semantically meaningful milestones, enabling more efficient targeting of program states through iterative refinement and symbolic execution verification."}}
{"id": "2508.21553", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21553", "abs": "https://arxiv.org/abs/2508.21553", "authors": ["J\u00f8rn Eirik Betten", "Quentin Mazouni", "Dennis Gross", "Pedro Lind", "Helge Spieker"], "title": "Reusable Test Suites for Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) agents show great promise in solving sequential\ndecision-making tasks. However, validating the reliability and performance of\nthe agent policies' behavior for deployment remains challenging. Most\nreinforcement learning policy testing methods produce test suites tailored to\nthe agent policy being tested, and their relevance to other policies is\nunclear. This work presents Multi-Policy Test Case Selection (MPTCS), a novel\nautomated test suite selection method for RL environments, designed to extract\ntest cases generated by any policy testing framework based on their\nsolvability, diversity, and general difficulty. MPTCS uses a set of policies to\nselect a diverse collection of reusable policy-agnostic test cases that reveal\ntypical flaws in the agents' behavior. The set of policies selects test cases\nfrom a candidate pool, which can be generated by any policy testing method,\nbased on a difficulty score. We assess the effectiveness of the difficulty\nscore and how the method's effectiveness and cost depend on the number of\npolicies in the set. Additionally, a method for promoting diversity in the test\nsuite, a discretized general test case descriptor surface inspired by\nquality-diversity algorithms, is examined to determine how it covers the state\nspace and which policies it triggers to produce faulty behaviors.", "AI": {"tldr": "MPTCS selects reusable policy-agnostic test cases for RL agents by leveraging diversity and difficulty, improving testing reliability across different policies.", "motivation": "Current RL policy testing methods produce policy-specific test suites, limiting their generalization. The work addresses the need for reusable, policy-agnostic test cases to validate agent reliability for deployment.", "method": "MPTCS employs a set of policies to select test cases from a candidate pool based on solvability, diversity, and difficulty scores, utilizing a discretized descriptor surface for promoting diversity inspired by quality-diversity algorithms.", "result": "Results demonstrate the effectiveness of MPTCS in selecting diverse test cases, the impact of policy set size on effectiveness, and the role of diversity mechanisms in covering the state space to trigger policy faults. The difficulty score's utility and method scalability are evaluated.", "conclusion": "The study introduces MPTCS, an automated method for selecting diverse, reusable policy-agnostic test cases, enhancing the identification of agent flaws and improving testing reliability across various reinforcement learning policies."}}
{"id": "2508.21323", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.21323", "abs": "https://arxiv.org/abs/2508.21323", "authors": ["Kunal Mukherjee", "Murat Kantarcioglu"], "title": "LLM-driven Provenance Forensics for Threat Investigation and Detection", "comment": null, "summary": "We introduce PROVSEEK, an LLM-powered agentic framework for automated\nprovenance-driven forensic analysis and threat intelligence extraction.\nPROVSEEK employs specialized toolchains to dynamically retrieve relevant\ncontext by generating precise, context-aware queries that fuse a vectorized\nthreat report knowledge base with data from system provenance databases. The\nframework resolves provenance queries, orchestrates multiple role-specific\nagents to mitigate hallucinations, and synthesizes structured, ground-truth\nverifiable forensic summaries. By combining agent orchestration with\nRetrieval-Augmented Generation (RAG) and chain-of-thought (CoT) reasoning,\nPROVSEEK enables adaptive multi-step analysis that iteratively refines\nhypotheses, verifies supporting evidence, and produces scalable, interpretable\nforensic explanations of attack behaviors. By combining provenance data with\nagentic reasoning, PROVSEEK establishes a new paradigm for grounded agentic\nforecics to investigate APTs. We conduct a comprehensive evaluation on publicly\navailable DARPA datasets, demonstrating that PROVSEEK outperforms\nretrieval-based methods for intelligence extraction task, achieving a 34%\nimprovement in contextual precision/recall; and for threat detection task,\nPROVSEEK achieves 22%/29% higher precision/recall compared to both a baseline\nagentic AI approach and State-Of-The-Art (SOTA) Provenance-based Intrusion\nDetection System (PIDS).", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.21634", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21634", "abs": "https://arxiv.org/abs/2508.21634", "authors": ["Domenico Cotroneo", "Cristina Improta", "Pietro Liguori"], "title": "Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects, Vulnerabilities, and Complexity", "comment": "Accepted to the 36th IEEE International Symposium on Software\n  Reliability Engineering (ISSRE, 2025)", "summary": "As AI code assistants become increasingly integrated into software\ndevelopment workflows, understanding how their code compares to human-written\nprograms is critical for ensuring reliability, maintainability, and security.\nIn this paper, we present a large-scale comparison of code authored by human\ndevelopers and three state-of-the-art LLMs, i.e., ChatGPT, DeepSeek-Coder, and\nQwen-Coder, on multiple dimensions of software quality: code defects, security\nvulnerabilities, and structural complexity. Our evaluation spans over 500k code\nsamples in two widely used languages, Python and Java, classifying defects via\nOrthogonal Defect Classification and security vulnerabilities using the Common\nWeakness Enumeration. We find that AI-generated code is generally simpler and\nmore repetitive, yet more prone to unused constructs and hardcoded debugging,\nwhile human-written code exhibits greater structural complexity and a higher\nconcentration of maintainability issues. Notably, AI-generated code also\ncontains more high-risk security vulnerabilities. These findings highlight the\ndistinct defect profiles of AI- and human-authored code and underscore the need\nfor specialized quality assurance practices in AI-assisted programming.", "AI": {"tldr": "AI-generated code has unique quality tradeoffs: simpler but more vulnerable compared to human code, necessitating tailored QA strategies.", "motivation": "Understanding how AI code assistants compare to human-written code in software quality is critical for ensuring developer workflows prioritize reliability, maintainability, and security.", "method": "The paper evaluates 500k code samples in Python and Java from humans and three LLMs (ChatGPT, DeepSeek-Coder, Qwen-Coder), analyzing code defects through Orthogonal Defect Classification and security vulnerabilities via Common Weakness Enumeration.", "result": "AI-generated code is simpler and more repetitive but contains more unused constructs, hardcoded debugging, and high-risk vulnerabilities, while human code shows greater structural complexity and maintainability challenges.", "conclusion": "The study highlights the distinct defect profiles between AI-generated code and human code, emphasizing the need for specialized quality assurance practices in AI-assisted programming."}}
{"id": "2508.21386", "categories": ["cs.CR", "cs.CY", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21386", "abs": "https://arxiv.org/abs/2508.21386", "authors": ["Jukka Ruohonen", "Jesper L\u00f8ffler Nielsen", "Jakub Sk\u00f3rczynski"], "title": "Risks and Compliance with the EU's Core Cyber Security Legislation", "comment": "Submitted to IST (VSI:RegCompliance in SE)", "summary": "The European Union (EU) has long favored a risk-based approach to regulation.\nSuch an approach is also used in recent cyber security legislation enacted in\nthe EU. Risks are also inherently related to compliance with the new\nlegislation. Objective: The paper investigates how risks are framed in the EU's\nfive core cyber security legislative acts, whether the framings indicate\nconvergence or divergence between the acts and their risk concepts, and what\nqualifying words and terms are used when describing the legal notions of risks.\nMethod : The paper's methodology is based on qualitative legal interpretation\nand taxonomy-building. Results: The five acts have an encompassing coverage of\ndifferent cyber security risks, including but not limited to risks related to\ntechnical, organizational, and human security as well as those not originating\nfrom man-made actions. Both technical aspects and assets are used to frame the\nlegal risk notions in many of the legislative acts. A threat-centric viewpoint\nis also present in one of the acts. Notable gaps are related to acceptable\nrisks, non-probabilistic risks, and residual risks. Conclusion: The EU's new\ncyber security legislation has significantly extended the risk-based approach\nto regulations. At the same time, complexity and compliance burden have\nincreased. With this point in mind, the paper concludes with a few practical\ntakeaways about means to deal with compliance and research it.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.21811", "categories": ["cs.SE", "68", "D.2.9"], "pdf": "https://arxiv.org/pdf/2508.21811", "abs": "https://arxiv.org/abs/2508.21811", "authors": ["Ashley Hourigan", "Ridewaan Hanslo"], "title": "The Integration of Agile Methodologies in DevOps Practices within the Information Technology Industry", "comment": "10 pages, 2 figures, conference", "summary": "The demand for rapid software delivery in the Information Technology (IT)\nindustry has significantly intensified, emphasising the need for faster\nsoftware products and service releases with enhanced features to meet customer\nexpectations. Agile methodologies are replacing traditional approaches such as\nWaterfall, where flexibility, iterative development and adaptation to change\nare favoured over rigid planning and execution. DevOps, a subsequent evolution\nfrom Agile, emphasises collaborative efforts in development and operations\nteams, focusing on continuous integration and deployment to deliver resilient\nand high-quality software products and services. This study aims to critically\nassess both Agile and DevOps practices in the IT industry to identify the\nfeasibility and applicability of Agile methods in DevOps practices. Eleven\nsemi-structured interviews were conducted with Agile and DevOps practitioners\nin varying capacities across several sectors within the IT industry. Through\nthematic analysis, 51 unique codes were extracted and synthesised into 19\nthemes that reported on each phase of the DevOps lifecycle, specifically\nregarding the integration and implementation of Agile methods into DevOps\npractices. Based on the findings, a new understanding detailing the\ninterrelationship of Agile methods in DevOps practices was discussed that met\nthe research objectives.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.21393", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21393", "abs": "https://arxiv.org/abs/2508.21393", "authors": ["Guofu Liao", "Taotao Wang", "Shengli Zhang", "Jiqun Zhang", "Shi Long", "Dacheng Tao"], "title": "zkLoRA: Fine-Tuning Large Language Models with Verifiable Security via Zero-Knowledge Proofs", "comment": null, "summary": "Fine-tuning large language models (LLMs) is crucial for adapting them to\nspecific tasks, yet it remains computationally demanding and raises concerns\nabout correctness and privacy, particularly in untrusted environments. Although\nparameter-efficient methods like Low-Rank Adaptation (LoRA) significantly\nreduce resource requirements, ensuring the security and verifiability of\nfine-tuning under zero-knowledge constraints remains an unresolved challenge.\nTo address this, we introduce zkLoRA, the first framework to integrate LoRA\nfine-tuning with zero-knowledge proofs (ZKPs), achieving provable security and\ncorrectness. zkLoRA employs advanced cryptographic techniques -- such as lookup\narguments, sumcheck protocols, and polynomial commitments -- to verify both\narithmetic and non-arithmetic operations in Transformer-based architectures.\nThe framework provides end-to-end verifiability for forward propagation,\nbackward propagation, and parameter updates during LoRA fine-tuning, while\nsafeguarding the privacy of model parameters and training data. Leveraging\nGPU-based implementations, zkLoRA demonstrates practicality and efficiency\nthrough experimental validation on open-source LLMs like LLaMA, scaling up to\n13 billion parameters. By combining parameter-efficient fine-tuning with ZKPs,\nzkLoRA bridges a critical gap, enabling secure and trustworthy deployment of\nLLMs in sensitive or untrusted environments.", "AI": {"tldr": "zkLoRA enables provably secure and private large language model fine-tuning by verifying LoRA updates via zero-knowledge cryptography, enabling trustworthy deployment in sensitive environments.", "motivation": "Fine-tuning LLMs is computationally expensive and insecure in untrusted environments. Existing parameter-efficient methods like LoRA lack security and verifiability under zero-knowledge constraints.", "method": "The framework combines Low-Rank Adaptation (LoRA) with zero-knowledge proofs (ZKPs) through cryptographic techniques like lookup arguments, sumcheck protocols, and polynomial commitments to verify all operations during fine-tuning while ensuring privacy.", "result": "zkLoRA demonstrates practical efficiency on models up to 13 billion parameters (e.g., LLaMA) via GPU implementations, providing end-to-end verifiability for forward/backward propagation and parameter updates while protecting sensitive data.", "conclusion": "zkLoRA bridges the gap between parameter-efficient fine-tuning and security in untrusted environments by integrating LoRA with zero-knowledge proofs, enabling verifiable and private large language model adaptation."}}
{"id": "2508.21417", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21417", "abs": "https://arxiv.org/abs/2508.21417", "authors": ["Shuhan Liu", "Xing Hu", "Xin Xia", "David Lo", "Xiaohu Yang"], "title": "An Empirical Study of Vulnerable Package Dependencies in LLM Repositories", "comment": null, "summary": "Large language models (LLMs) have developed rapidly in recent years,\nrevolutionizing various fields. Despite their widespread success, LLMs heavily\nrely on external code dependencies from package management systems, creating a\ncomplex and interconnected LLM dependency supply chain. Vulnerabilities in\ndependencies can expose LLMs to security risks. While existing research\npredominantly focuses on model-level security threats, vulnerabilities within\nthe LLM dependency supply chain have been overlooked. To fill this gap, we\nconducted an empirical analysis of 52 open-source LLMs, examining their\nthird-party dependencies and associated vulnerabilities. We then explored\nactivities within the LLM repositories to understand how maintainers manage\nthird-party vulnerabilities in practice. Finally, we compared third-party\ndependency vulnerabilities in the LLM ecosystem to those in the Python\necosystem. Our results show that half of the vulnerabilities in the LLM\necosystem remain undisclosed for more than 56.2 months, significantly longer\nthan those in the Python ecosystem. Additionally, 75.8% of LLMs include\nvulnerable dependencies in their configuration files. This study advances the\nunderstanding of LLM supply chain risks, provides insights for practitioners,\nand highlights potential directions for improving the security of the LLM\nsupply chain.", "AI": {"tldr": "This study reveals severe security risks in LLM dependency chains, showing 75.8% of LLMs have vulnerable dependencies and critical vulnerabilities persist for years.", "motivation": "Existing research focuses on model-level security threats, neglecting vulnerabilities in the LLM dependency supply chain despite its risks.", "method": "Empirical analysis of 52 open-source LLMs, examining third-party dependencies, vulnerability management practices, and ecosystem comparisons.", "result": "Half of LLM vulnerabilities remain undisclosed for over 56.2 months; 75.8% of LLMs include vulnerable dependencies in configuration files.", "conclusion": "The study highlights the critical security risks in the LLM dependency supply chain, provides practitioner insights, and suggests directions for improving LLM security."}}
