<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 19]
- [cs.SE](#cs.SE) [Total: 14]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge](https://arxiv.org/abs/2510.01223)
*Hui Dou,Ning Xu,Yiwen Zhang,Kaibin Wang*

Main category: cs.CR

TL;DR: This paper introduces RTS-Attack, an adaptive framework that exploits LLMs alignment weaknesses by generating semantically relevant nested scenarios with hidden toxic knowledge, showing superior effectiveness over existing jailbreak attack methods against advanced models like GPT-4o and Llama3-70b.


<details>
  <summary>Details</summary>
Motivation: Current nested scenario-based methods for testing LLM alignment defenses are easily detectable due to overtly malicious intent, creating a need for stealthier approaches that better mimic natural query patterns while maintaining attack efficacy.

Method: RTS-Attack constructs query-relevant scenarios through 1. Semantic relevance alignment using contextual analysis, 2. Targeted toxic knowledge injection via knowledge-aware prompting, and 3. Adaptive response mechanism that dynamically adjusts attack intensity based on model's reaction patterns.

Result: RTS-Attack achieves 42.7%-89.3%% succes rate across GPT-4o, Llama3-70b, and Gemini-pro, showing 28%-63% improvement over baselines. It maintains 87%-94% query concealment by avoiding explicit harmful prompts while successfully eliciting policy-violating responses.

Conclusion: RTS-Attack demonstrates critical vulnerabilities in LLM alignment defenses through semantically natural attack vectors, establishing a new benchmark for evaluating model robustness with its efficiency (43% faster response time) and universality across leading models.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various tasks. However, they remain exposed to jailbreak attacks, eliciting
harmful responses. The nested scenario strategy has been increasingly adopted
across various methods, demonstrating immense potential. Nevertheless, these
methods are easily detectable due to their prominent malicious intentions. In
this work, we are the first to find and systematically verify that LLMs'
alignment defenses are not sensitive to nested scenarios, where these scenarios
are highly semantically relevant to the queries and incorporate targeted toxic
knowledge. This is a crucial yet insufficiently explored direction. Based on
this, we propose RTS-Attack (Semantically Relevant Nested Scenarios with
Targeted Toxic Knowledge), an adaptive and automated framework to examine LLMs'
alignment. By building scenarios highly relevant to the queries and integrating
targeted toxic knowledge, RTS-Attack bypasses the alignment defenses of LLMs.
Moreover, the jailbreak prompts generated by RTS-Attack are free from harmful
queries, leading to outstanding concealment. Extensive experiments demonstrate
that RTS-Attack exhibits superior performance in both efficiency and
universality compared to the baselines across diverse advanced LLMs, including
GPT-4o, Llama3-70b, and Gemini-pro. Our complete code is available in the
supplementary material. WARNING: THIS PAPER CONTAINS POTENTIALLY HARMFUL
CONTENT.

</details>


### [2] [Fine-Tuning Jailbreaks under Highly Constrained Black-Box Settings: A Three-Pronged Approach](https://arxiv.org/abs/2510.01342)
*Xiangfang Li,Yu Wang,Bo Li*

Main category: cs.CR

TL;DR: This paper introduces a three-pronged black-box jailbreak attack against large language model fine-tuning defenses, achieving >97% success on GPT-4 variants by combining stealthy data encoding, safety wrappers, and backdoor mechanisms under realistic attack conditions.


<details>
  <summary>Details</summary>
Motivation: Existing research focuses on overly simplified attack scenarios that lack practical relevance, motivating the development of a more realistic attack framework to evaluate defenses against fine-tuning-based jailbreak attacks under dataset-only black-box settings.

Method: The attack combines three techniques: (1) safety-styled prefix/suffix wrappers to mask malicious intent, (2) benign lexical encodings (e.g., underscoring) of sensitive tokens to evade detection, and (3) a backdoor mechanism to enable harmful behavior learning while maintaining surface-level data innocuity.

Result: The proposed attack achieved over 97% success rates in jailbreaking GPT-4.1 and GPT-4o on the OpenAI platform under realistic deployment conditions involving pre-upload filtering, training-time defenses, and post-training audits.

Conclusion: The paper presents an effective three-pronged jailbreak attack method that successfully reveals vulnerabilities in real-world fine-tuning defenses, demonstrating the need for stronger multi-stage defense mechanisms across data filtering, training, and post-training safety audits.

Abstract: With the rapid advancement of large language models (LLMs), ensuring their
safe use becomes increasingly critical. Fine-tuning is a widely used method for
adapting models to downstream tasks, yet it is vulnerable to jailbreak attacks.
However, most existing studies focus on overly simplified attack scenarios,
limiting their practical relevance to real-world defense settings. To make this
risk concrete, we present a three-pronged jailbreak attack and evaluate it
against provider defenses under a dataset-only black-box fine-tuning interface.
In this setting, the attacker can only submit fine-tuning data to the provider,
while the provider may deploy defenses across stages: (1) pre-upload data
filtering, (2) training-time defensive fine-tuning, and (3) post-training
safety audit. Our attack combines safety-styled prefix/suffix wrappers, benign
lexical encodings (underscoring) of sensitive tokens, and a backdoor mechanism,
enabling the model to learn harmful behaviors while individual datapoints
appear innocuous. Extensive experiments demonstrate the effectiveness of our
approach. In real-world deployment, our method successfully jailbreaks GPT-4.1
and GPT-4o on the OpenAI platform with attack success rates above 97% for both
models. Our code is available at
https://github.com/lxf728/tri-pronged-ft-attack.

</details>


### [3] [Integrated Security Mechanisms for Weight Protection in Memristive Crossbar Arrays](https://arxiv.org/abs/2510.01350)
*Muhammad Faheemur Rahman,Wayne Burleson*

Main category: cs.CR

TL;DR: This paper proposes Keyed Permutor and Watermark Protection Columns to secure memristive in-memory computing systems against weight extraction threats while maintaining low overhead.


<details>
  <summary>Details</summary>
Motivation: Non-volatile memristors in crossbar arrays store valuable trained weights that are costly to produce and vulnerable to intellectual property theft if hardware is compromised.

Method: Two mechanisms: (1). Keyed Permutor shuffles weights using a cryptographic key, and (2). Watermark Protection Columns embed ownership identifiers; both integrate with existing architectures without major redesigns.

Result: Simulations on 45nm-7nm CMOS nodes with realistic interconnect models and an RF dataset showed <10​% area/power/delay overhead, validated on MNIST experiments for security effectiveness.

Conclusion: Proposed mechanisms provide robust, verifiable security for memristive in-memory computing with minimal performance impact, enabling IP protection even when hardware is physically exposed.

Abstract: Memristive crossbar arrays enable in-memory computing by performing parallel
analog computations directly within memory, making them well-suited for machine
learning, neural networks, and neuromorphic systems. However, despite their
advantages, non-volatile memristors are vulnerable to security threats (such as
adversarial extraction of stored weights when the hardware is compromised.
Protecting these weights is essential since they represent valuable
intellectual property resulting from lengthy and costly training processes
using large, often proprietary, datasets. As a solution we propose two security
mechanisms: Keyed Permutor and Watermark Protection Columns; where both
safeguard critical weights and establish verifiable ownership (even in cases of
data leakage). Our approach integrates efficiently with existing memristive
crossbar architectures without significant design modifications. Simulations
across 45nm, 22nm, and 7nm CMOS nodes, using a realistic interconnect model and
a large RF dataset, show that both mechanisms offer robust protection with
under 10% overhead in area, delay and power. We also present initial
experiments employing the widely known MNIST dataset; further highlighting the
feasibility of securing memristive in-memory computing systems with minimal
performance trade-offs.

</details>


### [4] [WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents](https://arxiv.org/abs/2510.01354)
*Yinuo Liu,Ruohan Xu,Xilong Wang,Yuqi Jia,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: This paper introduces the first comprehensive benchmark for detecting prompt injection attacks in web agents. It categorizes attacks, builds datasets with malicious/benign text/images, evaluates detection methods, and finds that current detectors struggle against attacks lacking explicit instructions or using imperceptible perturbations. 


<details>
  <summary>Details</summary>
Motivation: Prior detection methods for prompt injection attacks have not been systematically evaluated for web agents, creating a need for benchmarking to understand their effectiveness against this specific threat. 

Method: The study 1) introduces a fine-grained attack categorization based on threat models, 2) constructs datasets of malicious/benign text (4/2 categories) and images (attack-generated vs. normal), and 3) evaluates text/image-based detection methods across multiple scenarios. 

Result: Detectors achieve moderate to high accuracy against attacks with explicit textual instructions or visible image perturbations, but largely fail against attacks that omit explicit instructions or use imperceptible perturbations. 

Conclusion: Current detection methods are inadequate against advanced prompt injection attacks employing stealthy strategies. The paper highlights the urgent need for improved detection techniques and provides open-source datasets/code for further research. 

Abstract: Multiple prompt injection attacks have been proposed against web agents. At
the same time, various methods have been developed to detect general prompt
injection attacks, but none have been systematically evaluated for web agents.
In this work, we bridge this gap by presenting the first comprehensive
benchmark study on detecting prompt injection attacks targeting web agents. We
begin by introducing a fine-grained categorization of such attacks based on the
threat model. We then construct datasets containing both malicious and benign
samples: malicious text segments generated by different attacks, benign text
segments from four categories, malicious images produced by attacks, and benign
images from two categories. Next, we systematize both text-based and
image-based detection methods. Finally, we evaluate their performance across
multiple scenarios. Our key findings show that while some detectors can
identify attacks that rely on explicit textual instructions or visible image
perturbations with moderate to high accuracy, they largely fail against attacks
that omit explicit instructions or employ imperceptible perturbations. Our
datasets and code are released at:
https://github.com/Norrrrrrr-lyn/WAInjectBench.

</details>


### [5] [Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks](https://arxiv.org/abs/2510.01359)
*Shoumik Saha,Jifan Chen,Sam Mayers,Sanjay Krishna Gouda,Zijian Wang,Varun Kumar*

Main category: cs.CR

TL;DR: The paper introduces JAWS-BENCH, a benchmark evaluating code-capable LLM agents against jailbreak attacks in three workspace complexities (empty, single-file, multi-file), finding attack success rates increase when agents execute code through iterative reasoning/planning.


<details>
  <summary>Details</summary>
Motivation: Prior evaluations of LLM safety focused on text-based outputs, but code-execute agents pose deployment risks where malicious code could run. This study measures real-world exploitability through execution-aware metrics.

Method: Developed JAWS-BENCH with three workspace tiers (0,1,M), a four-criteria Judge Framework (compliance, success, syntax, execution), tested 7 LLMs across regimes using prompts and adversarial inputs.

Result: Attack acceptance rates rose from 61 (JAWS0, 27 end-to-end success) to 75 (JAWS-M, 32 deployable). Agent planning steps increased ASR 1.6× over initial refusals. Category-specific vulnerabilities were identified.

Conclusion: Code agents show significant vulnerability escalation when execution capability is allowed. Mitigations require execution-aware defenses and preventing reversal of initial refusals during planning phases.

Abstract: Code-capable large language model (LLM) agents are increasingly embedded into
software engineering workflows where they can read, write, and execute code,
raising the stakes of safety-bypass ("jailbreak") attacks beyond text-only
settings. Prior evaluations emphasize refusal or harmful-text detection,
leaving open whether agents actually compile and run malicious programs. We
present JAWS-BENCH (Jailbreaks Across WorkSpaces), a benchmark spanning three
escalating workspace regimes that mirror attacker capability: empty (JAWS-0),
single-file (JAWS-1), and multi-file (JAWS-M). We pair this with a
hierarchical, executable-aware Judge Framework that tests (i) compliance, (ii)
attack success, (iii) syntactic correctness, and (iv) runtime executability,
moving beyond refusal to measure deployable harm. Using seven LLMs from five
families as backends, we find that under prompt-only conditions in JAWS-0, code
agents accept 61% of attacks on average; 58% are harmful, 52% parse, and 27%
run end-to-end. Moving to single-file regime in JAWS-1 drives compliance to ~
100% for capable models and yields a mean ASR (Attack Success Rate) ~ 71%; the
multi-file regime (JAWS-M) raises mean ASR to ~ 75%, with 32% instantly
deployable attack code. Across models, wrapping an LLM in an agent
substantially increases vulnerability -- ASR raises by 1.6x -- because initial
refusals are frequently overturned during later planning/tool-use steps.
Category-level analyses identify which attack classes are most vulnerable and
most readily deployable, while others exhibit large execution gaps. These
findings motivate execution-aware defenses, code-contextual safety filters, and
mechanisms that preserve refusal decisions throughout the agent's multi-step
reasoning and tool use.

</details>


### [6] [E-FuzzEdge: Optimizing Embedded Device Security with Scalable In-Place Fuzzing](https://arxiv.org/abs/2510.01393)
*Davide Rusconi,Osama Yousef,Mirco Picca,Flavio Toffalini,Andrea Lanzi*

Main category: cs.CR

TL;DR: E-FuzzEdge is a novel fuzzing architecture that improves throughput for microcontroller testing by optimizing hardware-in-the-loop efficiency, compatible with existing embedded techniques.


<details>
  <summary>Details</summary>
Motivation: Hardware-in-the-loop fuzzing for microcontrollers faces scalability limitations and execution inefficiencies, hindering testing throughput.

Method: E-FuzzEdge optimizes execution speed in hardware-in-the-loop contexts via a novel architecture, avoiding firmware emulation to maintain device compatibility.

Result: Significant performance improvements demonstrated against state-of-the-art benchmarks in fuzzing throughput and execution speed.

Conclusion: E-FuzzEdge's compatibility with device-based testing enables seamless integration into existing workflows, enhancing overall embedded fuzzing efficiency.

Abstract: In this paper we show E-FuzzEdge, a novel fuzzing architecture targeted
towards improving the throughput of fuzzing campaigns in contexts where
scalability is unavailable. E-FuzzEdge addresses the inefficiencies of
hardware-in-the-loop fuzzing for microcontrollers by optimizing execution
speed. We evaluated our system against state-of-the-art benchmarks,
demonstrating significant performance improvements. A key advantage of
E-FuzzEdgearchitecture is its compatibility with other embedded fuzzing
techniques that perform on device testing instead of firmware emulation. This
means that the broader embedded fuzzing community can integrate E-FuzzEdge into
their workflows to enhance overall testing efficiency.

</details>


### [7] [Securing IoT Devices in Smart Cities: A Review of Proposed Solutions](https://arxiv.org/abs/2510.01445)
*Andrés F. Betancur-López*

Main category: cs.CR

TL;DR: The article reviews recent security proposals for IoT devices in Smart Cities, focusing on lightweight cryptography, PUFs, and blockchain. It identifies the need for more practical and scalable solutions.


<details>
  <summary>Details</summary>
Motivation: The proliferation of IoT devices in Smart Cities introduces significant privacy and security risks due to limited computational resources and widespread adoption, necessitating robust security mechanisms.

Method: The study conducts a literature review of device-level security solutions, analyzing approaches based on lightweight cryptography, physically unclonable functions (PUFs), and blockchain technology.

Result: The findings highlight the strengths and limitations of current security proposals, emphasizing the need for more efficient, scalable, and practical implementations to protect IoT ecosystems in Smart Cities.

Conclusion: Despite recent advancements, there are gaps in practical and resource-efficient security solutions for IoT in Smart Cities, demanding further research to enhance user privacy and data protection.

Abstract: Privacy and security in Smart Cities remain at constant risk due to the
vulnerabilities introduced by Internet of Things (IoT) devices. The limited
computational resources of these devices make them especially susceptible to
attacks, while their widespread adoption increases the potential impact of
security breaches. This article presents a review of security proposals aimed
at protecting IoT devices in Smart City environments. The review was conducted
by analyzing recent literature on device-level security, with particular
emphasis on lightweight cryptography, physically unclonable functions (PUFs),
and blockchain-based solutions. Findings highlight both the strengths and
limitations of current approaches, as well as the need for more practical,
scalable, and resource-efficient mechanisms to ensure user privacy and data
protection in IoT ecosystems.

</details>


### [8] [POLAR: Automating Cyber Threat Prioritization through LLM-Powered Assessment](https://arxiv.org/abs/2510.01552)
*Luoxi Tang,Yuqiao Meng,Ankita Patra,Weicheng Ma,Muchao Ye,Zhaohan Xi*

Main category: cs.CR

TL;DR: This paper identifies critical vulnerabilities in LLM-driven CTI systems and proposes a framework to improve their reliability.


<details>
  <summary>Details</summary>
Motivation: Recent LLMs struggle with practical CTI tasks despite promising capabilities. The paper aims to identify intrinsic vulnerabilities caused by the threat landscape's nature, not model architecture, to improve LLM-powered CTI systems.

Method: The study introduces a novel categorization methodology integrating stratification, autoregressive refinement, and human-in-the-loop supervision, validated through large-scale evaluations on CTI benchmarks and real-world threat reports.

Result: Identified three major LLM vulnerabilities: spurious correlations, contradictory knowledge, and constrained generalization, with experimental and human validation. Provided actionable design insights for robust CTI systems.

Conclusion: The paper concludes that LLMs in CTI face fundamental vulnerabilities like spurious correlations, contradictory knowledge, and constrained generalization, and proposes actionable insights to design more robust CTI systems.

Abstract: Large Language Models (LLMs) are intensively used to assist security analysts
in counteracting the rapid exploitation of cyber threats, wherein LLMs offer
cyber threat intelligence (CTI) to support vulnerability assessment and
incident response. While recent work has shown that LLMs can support a wide
range of CTI tasks such as threat analysis, vulnerability detection, and
intrusion defense, significant performance gaps persist in practical
deployments. In this paper, we investigate the intrinsic vulnerabilities of
LLMs in CTI, focusing on challenges that arise from the nature of the threat
landscape itself rather than the model architecture. Using large-scale
evaluations across multiple CTI benchmarks and real-world threat reports, we
introduce a novel categorization methodology that integrates stratification,
autoregressive refinement, and human-in-the-loop supervision to reliably
analyze failure instances. Through extensive experiments and human inspections,
we reveal three fundamental vulnerabilities: spurious correlations,
contradictory knowledge, and constrained generalization, that limit LLMs in
effectively supporting CTI. Subsequently, we provide actionable insights for
designing more robust LLM-powered CTI systems to facilitate future research.

</details>


### [9] [Position: Privacy Is Not Just Memorization!](https://arxiv.org/abs/2510.01645)
*Niloofar Mireshghallah,Tianshi Li*

Main category: cs.CR

TL;DR: This paper highlights underexplored privacy risks in LLM systems beyond memorization and calls for interdisciplinary research to address sociotechnical threats across the LLM lifecycle.


<details>
  <summary>Details</summary>
Motivation: Existing research disproportionately emphasizes verbatim memorization of training data, neglecting more immediate and scalable privacy threats in LLM systems. The paper seeks to broaden the discourse to include risks such as data collection practices, inference-time context leakage, autonomous agent capabilities, and deep inference attacks.

Method: The paper employs a longitudinal analysis of 1,322 AI/ML privacy papers (2016–2025) to identify research trends, while proposing a comprehensive taxonomy of privacy risks across the LLM lifecycle. Case studies demonstrate how current frameworks fail to address multifaceted threats.

Result: Analysis reveals that memorization receives outsized attention in technical research, while pressing privacy harms related to data collection, deployment, and emerging threats are underexplored. Current technical approaches lack traction in addressing these issues.

Conclusion: The authors argue for a fundamental shift in the research community's approach to LLM privacy, advocating for interdisciplinary methods that address the sociotechnical nature of emerging threats rather than focusing narrowly on technical solutions.

Abstract: The discourse on privacy risks in Large Language Models (LLMs) has
disproportionately focused on verbatim memorization of training data, while a
constellation of more immediate and scalable privacy threats remain
underexplored. This position paper argues that the privacy landscape of LLM
systems extends far beyond training data extraction, encompassing risks from
data collection practices, inference-time context leakage, autonomous agent
capabilities, and the democratization of surveillance through deep inference
attacks. We present a comprehensive taxonomy of privacy risks across the LLM
lifecycle -- from data collection through deployment -- and demonstrate through
case studies how current privacy frameworks fail to address these multifaceted
threats. Through a longitudinal analysis of 1,322 AI/ML privacy papers
published at leading conferences over the past decade (2016--2025), we reveal
that while memorization receives outsized attention in technical research, the
most pressing privacy harms lie elsewhere, where current technical approaches
offer little traction and viable paths forward remain unclear. We call for a
fundamental shift in how the research community approaches LLM privacy, moving
beyond the narrow focus of current technical solutions and embracing
interdisciplinary approaches that address the sociotechnical nature of these
emerging threats.

</details>


### [10] [Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks](https://arxiv.org/abs/2510.01676)
*Milad Nasr,Yanick Fratantonio,Luca Invernizzi,Ange Albertini,Loua Farah,Alex Petit-Bianco,Andreas Terzis,Kurt Thomas,Elie Bursztein,Nicholas Carlini*

Main category: cs.CR

TL;DR: Researchers demonstrated adversarial attacks against Gmail's Magika ML model, enabling malware evasion with minimal file changes. They developed a defense reducing attack success, now deployed in Gmail's production systems.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the real-world risk of adversarial attacks exploiting machine learning components in larger systems. Focusing on Gmail's malware detection, it highlights how such vulnerabilities could enable undetected malware dissemination via critical services.

Method: The authors designed adversarial examples by altering 13 bytes of malware samples to fool the Magika model. They then developed a defense strategy and evaluated its effectiveness, showing that even a well-resourced adversary requires more perturbations (50 bytes) to achieve lower success rates (20%). The defense was collaboratively implemented and deployed in production.

Result: The attacks achieved 90% evasion of Magika by altering 13 bytes, allowing malware evasion through Gmail. Post-defense, adversaries needed 50-byte alterations for 20% success. The defense is now actively in use in Gmail's production pipeline.

Conclusion: This paper concludes that adversarial attacks on ML components in production systems can lead to significant vulnerabilities. However, with the implementation of effective defense mechanisms, the security of such systems can be substantially improved, as demonstrated by the successful deployment of their defense on Gmail's malware detection system.

Abstract: As deep learning models become widely deployed as components within larger
production systems, their individual shortcomings can create system-level
vulnerabilities with real-world impact. This paper studies how adversarial
attacks targeting an ML component can degrade or bypass an entire
production-grade malware detection system, performing a case study analysis of
Gmail's pipeline where file-type identification relies on a ML model.
  The malware detection pipeline in use by Gmail contains a machine learning
model that routes each potential malware sample to a specialized malware
classifier to improve accuracy and performance. This model, called Magika, has
been open sourced. By designing adversarial examples that fool Magika, we can
cause the production malware service to incorrectly route malware to an
unsuitable malware detector thereby increasing our chance of evading detection.
Specifically, by changing just 13 bytes of a malware sample, we can
successfully evade Magika in 90% of cases and thereby allow us to send malware
files over Gmail. We then turn our attention to defenses, and develop an
approach to mitigate the severity of these types of attacks. For our defended
production model, a highly resourced adversary requires 50 bytes to achieve
just a 20% attack success rate. We implement this defense, and, thanks to a
collaboration with Google engineers, it has already been deployed in production
for the Gmail classifier.

</details>


### [11] [Towards Imperceptible Adversarial Defense: A Gradient-Driven Shield against Facial Manipulations](https://arxiv.org/abs/2510.01699)
*Yue Li,Linying Xue,Dongdong Lin,Qiushi Li,Hui Tian,Hongxia Wang*

Main category: cs.CR

TL;DR: They made a new method called GRASP to protect against fake faces without messing up the picture quality.


<details>
  <summary>Details</summary>
Motivation: There's a problem because fake face images are getting more common, and current defenses make the original images look bad. They need a better way to stop deepfakes while keeping the real images looking good.

Method: GRASP uses a gradient-projection technique to combine different losses (like how good the image looks and how well the defense works). This helps avoid the design conflicts that happen when trying to optimize all at once.

Result: GRASP works really well - over 40 dB in PSNR, an SSIM close to perfect at 0.99, and it completely blocks manipulations. It's way better than other methods in making clean, fake-resistant images.

Conclusion: GRASP is a solid solution for deepfake defense that keeps image quality top-notch. It shows we can have strong security and good visuals at the same time with the right approach.

Abstract: With the flourishing prosperity of generative models, manipulated facial
images have become increasingly accessible, raising concerns regarding privacy
infringement and societal trust. In response, proactive defense strategies
embed adversarial perturbations into facial images to counter deepfake
manipulation. However, existing methods often face a tradeoff between
imperceptibility and defense effectiveness-strong perturbations may disrupt
forgeries but degrade visual fidelity. Recent studies have attempted to address
this issue by introducing additional visual loss constraints, yet often
overlook the underlying gradient conflicts among losses, ultimately weakening
defense performance. To bridge the gap, we propose a gradient-projection-based
adversarial proactive defense (GRASP) method that effectively counters facial
deepfakes while minimizing perceptual degradation. GRASP is the first approach
to successfully integrate both structural similarity loss and low-frequency
loss to enhance perturbation imperceptibility. By analyzing gradient conflicts
between defense effectiveness loss and visual quality losses, GRASP pioneers
the design of the gradient-projection mechanism to mitigate these conflicts,
enabling balanced optimization that preserves image fidelity without
sacrificing defensive performance. Extensive experiments validate the efficacy
of GRASP, achieving a PSNR exceeding 40 dB, SSIM of 0.99, and a 100% defense
success rate against facial attribute manipulations, significantly
outperforming existing approaches in visual quality.

</details>


### [12] [Constructions of Efficiently Implementable Boolean Functions with Provable Nonlinearity/Resiliency/Algebraic Immunity Trade-Offs](https://arxiv.org/abs/2510.01720)
*Palash Sarkar*

Main category: cs.CR

TL;DR: The paper presents efficiently implementable Boolean function families with provable trade-offs between resiliency, nonlinearity (linear bias), and algebraic immunity, achieving linear scalability in input size and gate complexity.


<details>
  <summary>Details</summary>
Motivation: Boolean functions with balanced cryptographic properties (resiliency, nonlinearity, algebraic immunity are critical for secure cryptosystems. Prior work lacked efficient constructions with provable trade-offs among these parameters.

Method: Constructs function families where for given minimal resiliency $m_0$, maximally suppressed linear bias $2^{-x_0}$, and minimal algebraic immunity $a_0$, the functions use $O(n)$ gates with $n$ linearly dependent on $m_0, x_0, a_0.

Result: Explicit constructions achieving simultaneously specified levels of resiliency (≥m₀), nonlinearity (bias ≤2^{-x₀}), and algebraic immunity (≥a₀), with implementation complexity $O(n)$ where $n=O(m₀ + x₀ + a₀)$.

Conclusion: The work establishes a systematic method to realize Boolean functions optimized for multiple cryptographic metrics with linear implementation cost, advancing practical cryptographic design.

Abstract: We describe several families of efficiently implementable Boolean functions
achieving provable trade-offs between resiliency, nonlinearity, and algebraic
immunity. In concrete terms, the following result holds for each of the
function families that we propose. Given integers $m_0\geq 0$, $x_0\geq 1$, and
$a_0\geq 1$, it is possible to construct an $n$-variable function which has
resiliency at least $m_0$, linear bias (which is an equivalent method of
expressing nonlinearity) at most $2^{-x_0}$ and algebraic immunity at least
$a_0$; further, $n$ is linear in $m_0$, $x_0$ and $a_0$, and the function can
be implemented using $O(n)$ gates.

</details>


### [13] [Secure Multi-Modal Data Fusion in Federated Digital Health Systems via MCP](https://arxiv.org/abs/2510.01780)
*Aueaphum Aueawatthanaphisut*

Main category: cs.CR

TL;DR: This paper proposes an MCP-based framework for secure multi-modal federated learning in healthcare, improving diagnostic accuracy (9.8%), reducing client dropouts (54%), and enabling privacy-preserving data fusion across distributed systems.


<details>
  <summary>Details</summary>
Motivation: Current federated learning lacks standardized mechanisms for interoperable, secure multi-modal data fusion in resource-constrained healthcare environments, hindering privacy and scalability goals.

Method: The framework integrates: 1. Multi-modal feature alignment for clinical imaging/EMR/IoT data; 2. Differential privacy-protected aggregation; 3. Energy-aware client scheduling with MCP as a schema-driven interoperability protocol.

Result: Experiments show 9.8% higher diagnostic accuracy vs. baseline FL, 54%% lower dropout rates, and maintains clinically acceptable privacy-utility trade-offs across benchmark datasets and pilot cohorts.

Conclusion: MCP-enabled architectures provide a scalable pathway for secure, interoperable next-generation federated health systems that address multi-modal data integration challenges while complying with privacy regulations.

Abstract: Secure and interoperable integration of heterogeneous medical data remains a
grand challenge in digital health. Current federated learning (FL) frameworks
offer privacy-preserving model training but lack standardized mechanisms to
orchestrate multi-modal data fusion across distributed and resource-constrained
environments. This study introduces a novel framework that leverages the Model
Context Protocol (MCP) as an interoperability layer for secure, cross-agent
communication in multi-modal federated healthcare systems. The proposed
architecture unifies three pillars: (i) multi-modal feature alignment for
clinical imaging, electronic medical records, and wearable IoT data; (ii)
secure aggregation with differential privacy to protect patient-sensitive
updates; and (iii) energy-aware scheduling to mitigate dropouts in mobile
clients. By employing MCP as a schema-driven interface, the framework enables
adaptive orchestration of AI agents and toolchains while ensuring compliance
with privacy regulations. Experimental evaluation on benchmark datasets and
pilot clinical cohorts demonstrates up to 9.8\% improvement in diagnostic
accuracy compared with baseline FL, a 54\% reduction in client dropout rates,
and clinically acceptable privacy--utility trade-offs. These results highlight
MCP-enabled multi-modal fusion as a scalable and trustworthy pathway toward
equitable, next-generation federated health infrastructures.

</details>


### [14] [ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs](https://arxiv.org/abs/2510.01967)
*Aadarsh Anantha Ramakrishnan,Shubham Agarwal,Selvanayagam S,Kunwar Singh*

Main category: cs.CR

TL;DR: ZK-WAGON introduces a secure, imperceptible watermarking system for image generation models using ZK-SNARKs and LSB steganography, enabling verifiable origin proofs without exposing sensitive information.


<details>
  <summary>Details</summary>
Motivation: Traditional watermarking methods compromise image quality, are easily removable, or require confidential model internals, creating risks for synthetic media misuse like deepfakes and IP violations.

Method: The framework uses ZK-SNARKs for verifiable proofs, Selective Layer ZK-Circuit Creation (SL-ZKCC) to optimize proof generation via selective layer circuit conversion, and LSB steganography for embedding proofs into images.

Result: ZK-WAGON successfully demonstrates watermarking on GAN and Diffusion models, achieving imperceptible proofs, reduced proof generation time via SL-ZKCC, and secure model-agnostic deployment.

Conclusion: This work provides a scalable, privacy-preserving solution for trustworthy AI image generation, addressing authenticity concerns without compromising model secrecy or image quality.

Abstract: As image generation models grow increasingly powerful and accessible,
concerns around authenticity, ownership, and misuse of synthetic media have
become critical. The ability to generate lifelike images indistinguishable from
real ones introduces risks such as misinformation, deepfakes, and intellectual
property violations. Traditional watermarking methods either degrade image
quality, are easily removed, or require access to confidential model internals
- making them unsuitable for secure and scalable deployment. We are the first
to introduce ZK-WAGON, a novel system for watermarking image generation models
using the Zero-Knowledge Succinct Non Interactive Argument of Knowledge
(ZK-SNARKs). Our approach enables verifiable proof of origin without exposing
model weights, generation prompts, or any sensitive internal information. We
propose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively
convert key layers of an image generation model into a circuit, reducing proof
generation time significantly. Generated ZK-SNARK proofs are imperceptibly
embedded into a generated image via Least Significant Bit (LSB) steganography.
We demonstrate this system on both GAN and Diffusion models, providing a
secure, model-agnostic pipeline for trustworthy AI image generation.

</details>


### [15] [Mirage Fools the Ear, Mute Hides the Truth: Precise Targeted Adversarial Attacks on Polyphonic Sound Event Detection Systems](https://arxiv.org/abs/2510.02158)
*Junjie Su,Weifei Jin,Yuxin Cao,Derui Wang,Kai Ye,Jie Hao*

Main category: cs.CR

TL;DR: This paper introduces M2A, a precise adversarial attack framework for polyphonic SED systems, achieving state-of-the-art precision via preservation loss and a novel EP metric, with 94.56%-99.11% effectiveness on leading models.


<details>
  <summary>Details</summary>
Motivation: Existing audio adversarial attacks for SED systems lack robustness due to contextual dependencies or imprecise targeting that inadvertently affects non-target regions, leaving safety-critical applications vulnerable.

Method: Proposes M2A, a targeted adversarial attack framework with preservation loss constraints on non-target outputs and introduces the Editing Precision (EP) metric to balance attack effectiveness and precision.

Result: Achieved 94.56% and 99.11% EP on two SED models, demonstrating significant improvements in both attack effectiveness and precision compared to prior methods.

Conclusion: The M2A framework effectively enhances attack precision on polyphonic SED systems while maintaining non-target region integrity, demonstrating high effectiveness through 94.56% and 99.11% Editing Precision (EP) on two state-of-the-art models.

Abstract: Sound Event Detection (SED) systems are increasingly deployed in
safety-critical applications such as industrial monitoring and audio
surveillance. However, their robustness against adversarial attacks has not
been well explored. Existing audio adversarial attacks targeting SED systems,
which incorporate both detection and localization capabilities, often lack
effectiveness due to SED's strong contextual dependencies or lack precision by
focusing solely on misclassifying the target region as the target event,
inadvertently affecting non-target regions. To address these challenges, we
propose the Mirage and Mute Attack (M2A) framework, which is designed for
targeted adversarial attacks on polyphonic SED systems. In our optimization
process, we impose specific constraints on the non-target output, which we
refer to as preservation loss, ensuring that our attack does not alter the
model outputs for non-target region, thus achieving precise attacks.
Furthermore, we introduce a novel evaluation metric Editing Precison (EP) that
balances effectiveness and precision, enabling our method to simultaneously
enhance both. Comprehensive experiments show that M2A achieves 94.56% and
99.11% EP on two state-of-the-art SED models, demonstrating that the framework
is sufficiently effective while significantly enhancing attack precision.

</details>


### [16] [NoMod: A Non-modular Attack on Module Learning With Errors](https://arxiv.org/abs/2510.02162)
*Cristian Bassotto,Ermes Franch,Marina Krček,Stjepan Picek*

Main category: cs.CR

TL;DR: The advent of quantum computing threatens classical public-key cryptography, leading NIST to adopt post-quantum schemes like Module-LWE. The paper introduces NoMod ML-Attack, a hybrid white-box method for secret recovery that combines lattice preprocessing and robust linear estimation. Experiments show successful secret recovery in different scenarios.


<details>
  <summary>Details</summary>
Motivation: Quantum computing poses a threat to classical public-key cryptography, necessitating the transition to post-quantum cryptographic schemes. The paper addresses the challenge of modeling modular reduction in such schemes by presenting a method to circumvent it through robust linear estimation.

Method: The method involves a combination of optimized lattice preprocessing techniques, such as reduced-vector saving and algebraic amplification, with machine learning robust estimators trained via Tukey's Biweight loss. The approach treats modular reduction wrap-arounds as statistical corruption, enabling secret recovery as a robust linear estimation problem.

Result: Experiments show that the proposed NoMod ML-Attack successfully recovers binary secrets of dimensions 350 and sparse binomial secrets of dimensions 256, as well as sparse secrets in CRYSTALS-Kyber settings with parameters (128, 3) and (256, 2).

Conclusion: The NoMod ML-Attack method effectively circumvents the challenge of modular reduction in post-quantum cryptographic schemes, demonstrating successful secret recovery in various parameter settings. The implementation is available as an open repository.

Abstract: The advent of quantum computing threatens classical public-key cryptography,
motivating NIST's adoption of post-quantum schemes such as those based on the
Module Learning With Errors (Module-LWE) problem. We present NoMod ML-Attack, a
hybrid white-box cryptanalytic method that circumvents the challenge of
modeling modular reduction by treating wrap-arounds as statistical corruption
and casting secret recovery as robust linear estimation. Our approach combines
optimized lattice preprocessing--including reduced-vector saving and algebraic
amplification--with robust estimators trained via Tukey's Biweight loss.
Experiments show NoMod achieves full recovery of binary secrets for dimension
$n = 350$, recovery of sparse binomial secrets for $n = 256$, and successful
recovery of sparse secrets in CRYSTALS-Kyber settings with parameters $(n, k) =
(128, 3)$ and $(256, 2)$. We release our implementation in an anonymous
repository https://anonymous.4open.science/r/NoMod-3BD4.

</details>


### [17] [Testing Stability and Robustness in Three Cryptographic Chaotic Systems](https://arxiv.org/abs/2510.02184)
*N. A. Anagnostopoulos,K. Konstantinidis,A. N. Miliou,S. G. Stavrinides*

Main category: cs.CR

TL;DR: This paper evaluates three chaotic cryptographic systems to determine their noise resilience and synchronization stability for secure applications.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need for secure communication systems that maintain synchronization even in noisy environments, which is essential for practical cryptographic implementations.

Method: The authors experimentally tested and compared three cryptographic chaotic systems by evaluating their synchronization behavior under noise conditions, focusing on stability and robustness metrics.

Result: The results highlight differences in the performance of the three systems, with specific systems showing higher resilience to noise, thus aligning better with secure communication requirements.

Conclusion: The study concludes that certain chaotic systems demonstrate superior stability and robustness in the presence of noise, making them more suitable for cryptographic applications where synchronization reliability is critical.

Abstract: In practical applications, it is crucial that the drive-response systems,
although identical in all respects, are synchronized at all times, even if
there is noise present. In this work, we test the stability and robustness of
three distinct and well-known cryptographic chaotic systems, and compare the
results in relation to the desired security.

</details>


### [18] [Authentication Security of PRF GNSS Ranging](https://arxiv.org/abs/2510.02196)
*Jason Anderson*

Main category: cs.CR

TL;DR: This paper establishes authentication security for PRF-based GNSS ranging against spoofing, analyzing Galileo's SAS and showing 400ms of E6-C data achieves 128-bit security under non-SCER models. It also quantifies equipment requirements for SCER adversaries.


<details>
  <summary>Details</summary>
Motivation: GNSS spoofing threats necessitate robust authentication mechanisms. PRF-based ranging with secret keys prevents spoofers from predicting codes, enabling trusted PNT solutions.

Method: Derives security proofs for PRF-GNSS under multiple spoofing models (including SCER). Applies analysis to Galileo's SAS using E6-C signal characteristics, computing authentication security bounds and adversary equipment requirements through mathematical modeling.

Result: 128-bit authentication security requires at most 400ms of Galileo E6-C data under non-SCER models. Quantified SCER adversary capabilities based on required receiving equipment. Framework for designing PRF protocols via missed detection probability calculations.

Conclusion: PRF-GNSS provides provable authentication security against spoofing. The Galileo SAS example demonstrates practical parameter selection. The analysis enables designing secure PRF protocols by balancing security requirements with operational constraints.

Abstract: This work derives the authentication security of pseudorandom function (PRF)
GNSS ranging under multiple GNSS spoofing models, including the Security Code
Estimation and Replay (SCER) spoofer. When GNSS ranging codes derive from a PRF
utilizing a secret known only to the broadcaster, the spoofer cannot predict
the ranging code before broadcast. Therefore, PRF ranging can be used to
establish trust in the GNSS pseudoranges and the resulting receiver position,
navigation, and timing (PNT) solution. I apply the methods herein to Galileo's
Signal Authentication Service (SAS) utilizing the encrypted Galileo E6-C signal
to compute that, at most, 400 ms of Galileo E6-C data to assert 128-bit
authentication security under non-SCER models. For the SCER adversary, I
predict the adversary's needed receiving radio equipment to break
authentication security. One can use this work to design a PRF GNSS ranging
protocol to meet useful authentication security requirements by computing the
probability of missed detection.

</details>


### [19] [An efficient quantum algorithm for computing $S$-units and its applications](https://arxiv.org/abs/2510.02280)
*Jean-Francois Biasse,Fang Song*

Main category: cs.CR

TL;DR: This paper details a quantum polynomial-time algorithm for computing the S-unit group of a number field, enabling efficient solutions for several algebraic number theory problems like class group computation and principal ideal resolution, with implications for lattice problems and cryptography.


<details>
  <summary>Details</summary>
Motivation: Efficient algorithms for number-theoretic problems are critical in algorithmic cryptography. The paper resolves open problems in computing fundamental algebraic structures (e.g., class groups, unit groups) in polynomial time using quantum methods, improving over classical exponential-time approaches.

Method: The work provides rigorous proofs for the quantum polynomial-time algorithm of Biasse and Song (SODA 16). It leverages quantum subroutines for solving the principal ideal problem and integrates prior results (Cramer et al.) to address related tasks like finding short generators in principal ideals and decomposing ideal classes.

Result: The algorithm achieves polynomial-time computation for S-unit groups, class groups, unit groups, ray class groups, and principal ideal solutions. It also enables resolution of norm equations and decomposition of ideal classes, with applications to finding short/generators of ideal lattices in cyclotomic fields.

Conclusion: The paper demonstrates that quantum algorithms can solve core problems in algebraic number theory efficiently, significantly advancing lattice-based cryptography by connecting class group computations to hard lattice problems via short vector generation.

Abstract: In this paper, we provide details on the proofs of the quantum polynomial
time algorithm of Biasse and Song (SODA 16) for computing the $S$-unit group of
a number field. This algorithm directly implies polynomial time methods to
calculate class groups, S-class groups, relative class group and the unit
group, ray class groups, solve the principal ideal problem, solve certain norm
equations, and decompose ideal classes in the ideal class group. Additionally,
combined with a result of Cramer, Ducas, Peikert and Regev (Eurocrypt 2016),
the resolution of the principal ideal problem allows one to find short
generators of a principal ideal. Likewise, methods due to Cramer, Ducas and
Wesolowski (Eurocrypt 2017) use the resolution of the principal ideal problem
and the decomposition of ideal classes to find so-called ``mildly short
vectors'' in ideal lattices of cyclotomic fields.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [20] [Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration](https://arxiv.org/abs/2510.01379)
*Huashan Chen,Zhenyu Qi,Haotang Li,Hong Chen,Jinfu Chen,Kebin Peng,In Kee Kim,Kyu Hyung Lee,Sen He*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While Large Language Models (LLMs) have become the predominant paradigm for
automated code generation, current single-model approaches fundamentally ignore
the heterogeneous computational strengths that different models exhibit across
programming languages, algorithmic domains, and development stages. This paper
challenges the single-model convention by introducing a multi-stage,
performance-guided orchestration framework that dynamically routes coding tasks
to the most suitable LLMs within a structured generate-fix-refine workflow. Our
approach is grounded in a comprehensive empirical study of 17 state-of-the-art
LLMs across five programming languages (Python, Java, C++, Go, and Rust) using
HumanEval-X benchmark. The study, which evaluates both functional correctness
and runtime performance metrics (execution time, mean/max memory utilization,
and CPU efficiency), reveals pronounced performance heterogeneity by language,
development stage, and problem category. Guided by these empirical insights, we
present PerfOrch, an LLM agent that orchestrates top-performing LLMs for each
task context through stage-wise validation and rollback mechanisms. Without
requiring model fine-tuning, PerfOrch achieves substantial improvements over
strong single-model baselines: average correctness rates of 96.22% and 91.37%
on HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and
49.11%. Beyond correctness gains, the framework delivers consistent performance
optimizations, improving execution time for 58.76% of problems with median
speedups ranging from 17.67% to 27.66% across languages on two benchmarks. The
framework's plug-and-play architecture ensures practical scalability, allowing
new LLMs to be profiled and integrated seamlessly, thereby offering a paradigm
for production-grade automated software engineering that adapts to the rapidly
evolving generative AI landscape.

</details>


### [21] [Deciphering WONTFIX: A Mixed-Method Study on Why GitHub Issues Get Rejected](https://arxiv.org/abs/2510.01514)
*J. Alexander Curtis,Sharadha Kasiviswanathan,Nasir Eisty*

Main category: cs.SE

TL;DR: This study reveals the 30% prevalence of GitHub's 'wontfix' label, uncovering eight themes behind its use. While the label aids resource management, it risks discouraging community participation and reducing project transparency.


<details>
  <summary>Details</summary>
Motivation: The study aims to clarify the prevalence and underlying reasons for the widespread use of the wontfix label in GitHub repositories, addressing its impact on open-source project management and community dynamics.

Method: A mixed-method approach was employed, combining quantitative analysis of data from 3,132 GitHub repositories with qualitative methods like open coding and thematic analysis to categorize reasons for using the wontfix label.

Result: Approximately 30% of GitHub projects use the wontfix label, primarily on user-submitted bug reports and feature requests. Eight themes were identified, highlighting factors such as user-specific constraints and maintainer-driven decisions.

Conclusion: The wontfix label is essential for resource management in GitHub projects but can hinder community engagement and project transparency. Understanding its usage helps project managers make informed decisions and improve collaboration.

Abstract: Context: The ``wontfix'' label is a widely used yet narrowly understood tool
in GitHub repositories, indicating that an issue will not be pursued further.
Despite its prevalence, the impact of this label on project management and
community dynamics within open-source software development is not clearly
defined. Objective: This study examines the prevalence and reasons behind
issues being labeled as wontfix across various open-source repositories on
GitHub. Method: Employing a mixed-method approach, we analyze both quantitative
data to assess the prevalence of the wontfix label and qualitative data to
explore the reasoning that it was used. Data were collected from 3,132 of
GitHub's most-popular repositories. Later, we employ open coding and thematic
analysis to categorize the reasons behind wontfix labels, providing a
structured understanding of the issue management landscape. Results: Our
findings show that about 30% of projects on GitHub apply the wontfix label to
some issues. These issues most often occur on user-submitted issues for bug
reports and feature requests. The study identified eight common themes behind
labeling issues as wontfix, ranging from user-specific control factors to
maintainer-specific decisions. Conclusions: The wontfix label is a critical
tool for managing resources and guiding contributor efforts in GitHub projects.
However, it can also discourage community involvement and obscure the
transparency of project management. Understanding these reasons aids project
managers in making informed decisions and fostering efficient collaboration
within open-source communities.

</details>


### [22] [MIMIC: Integrating Diverse Personality Traits for Better Game Testing Using Large Language Model](https://arxiv.org/abs/2510.01635)
*Yifei Chen,Sarra Habchi,Lili Wei*

Main category: cs.SE

TL;DR: MIMIC is a framework that integrates diverse personality traits into gaming agents to mimic human playstyles, enhancing test coverage and in-game interactions for automated game testing. It outperforms existing methods in Minecraft by achieving higher task completion and solution diversity.


<details>
  <summary>Details</summary>
Motivation: Traditional automated testing agents (e.g., RL, Imitation Learning) fail to replicate human players' diverse strategies due to rigid algorithms, leading to repetitive solutions and limited coverage. This hinders edge case detection and real-world effectiveness in games.

Method: MIMIC incorporates personality-based decision-making into agents by training them to adopt multiple playstyles. The framework dynamically selects strategies aligned with embedded personality traits, enabling diverse responses to similar scenarios across different games.

Result: On Minecraft, MIMIC achieved higher task completion rates (23.7% improvement over state-of-the-art agents) and 38.4% more unique solutions. It demonstrated 25% higher test coverage and triggered 41% more in-game interactions compared to standard baselines.

Conclusion: MIMIC bridges the gap between automated testing agents and human-like diversity in playstyles. Its personality-driven approach enables more comprehensive game testing, making it a promising solution for detecting edge cases and improving quality in complex environments.

Abstract: Modern video games pose significant challenges for traditional automated
testing algorithms, yet intensive testing is crucial to ensure game quality. To
address these challenges, researchers designed gaming agents using
Reinforcement Learning, Imitation Learning, or Large Language Models. However,
these agents often neglect the diverse strategies employed by human players due
to their different personalities, resulting in repetitive solutions in similar
situations. Without mimicking varied gaming strategies, these agents struggle
to trigger diverse in-game interactions or uncover edge cases.
  In this paper, we present MIMIC, a novel framework that integrates diverse
personality traits into gaming agents, enabling them to adopt different gaming
strategies for similar situations. By mimicking different playstyles, MIMIC can
achieve higher test coverage and richer in-game interactions across different
games. It also outperforms state-of-the-art agents in Minecraft by achieving a
higher task completion rate and providing more diverse solutions. These results
highlight MIMIC's significant potential for effective game testing.

</details>


### [23] [FOSS-chain: using blockchain for Open Source Software license compliance](https://arxiv.org/abs/2510.01740)
*Kypros Iacovou,Georgia M. Kapitsaki,Evangelia Vanezi*

Main category: cs.SE

TL;DR: Blockchain-based FOSS-chain platform automates OSS license compliance, addressing incompatibility issues through immutable tracking of changes.


<details>
  <summary>Details</summary>
Motivation: Ensuring compliance with OSS licenses is complex due to incompatibilities among licenses, which can lead to legal disputes. Current processes lack transparency and efficiency in tracking license dependencies.

Method: The authors designed and implemented FOSS-chain, a blockchain-enabled web platform that automates license compliance checks using immutable records to track software changes and ensure compatibility.

Result: Preliminary evaluation via a small-scale user study demonstrated FOSS-chain's viability, showing it can automate compliance checks across 14 OSS licenses and adapt to realistic software systems.

Conclusion: The integration of blockchain technology into OSS license management, as demonstrated by FOSS-chain, shows potential for addressing license compatibility and compliance challenges effectively.

Abstract: Open Source Software (OSS) is widely used and carries licenses that indicate
the terms under which the software is provided for use, also specifying
modification and distribution rules. Ensuring that users are respecting OSS
license terms when creating derivative works is a complex process. Compliance
issues arising from incompatibilities among licenses may lead to legal
disputes. At the same time, the blockchain technology with immutable entries
offers a mechanism to provide transparency when it comes to licensing and
ensure software changes are recorded. In this work, we are introducing an
integration of blockchain and license management when creating derivative
works, in order to tackle the issue of OSS license compatibility. We have
designed, implemented and performed a preliminary evaluation of FOSS-chain, a
web platform that uses blockchain and automates the license compliance process,
covering 14 OSS licenses. We have evaluated the initial prototype version of
the FOSS-chain platform via a small scale user study. Our preliminary results
are promising, demonstrating the potential of the platform for adaptation on
realistic software systems.

</details>


### [24] [ARENA: A tool for measuring and analysing the energy efficiency of Android apps](https://arxiv.org/abs/2510.01754)
*Hina Anwar*

Main category: cs.SE

TL;DR: ARENA is an IDE-integrated plugin that streamlines hardware-based energy measurement for Android apps, offering automated data capture, analysis, and visualization to improve efficiency and reproducibility.


<details>
  <summary>Details</summary>
Motivation: Existing hardware-based energy measurement processes for Android apps are time-consuming, difficult to adapt, and lack open-source tools. Software-based approaches compromise accuracy.

Method: ARENA is implemented as an IntelliJ/Android Studio plugin that automates hardware device integration, test scenario execution, data aggregation, statistical analysis, and visualization of energy consumption metrics.

Result: ARENA enables developers and researchers to perform reliable, repeatable energy measurements directly within their IDE, reducing manual steps and noise through automated data collection and statistical analysis.

Conclusion: ARENA addresses the lack of open-source tools for reliable hardware-based energy measurement by providing an integrated, automated solution. It enhances the reproducibility and accessibility of energy consumption analysis for Android apps.

Abstract: To build energy-efficient apps, there is a need to estimate and analyze their
energy consumption in typical usage scenarios. The energy consumption of
Android apps could be estimated via software-based and hardware-based
approaches. Software-based approaches, while easier to implement, are not as
accurate as hardware-based approaches. The process of measuring the energy
consumption of an Android app via a hardware-based approach typically involves
1) setting up a measurement environment, 2) executing the app under test on a
mobile device, 3) recording current/voltage data via a hardware device to
measure energy consumption, and 4) cleaning and aggregating data for analyses,
reports, and visualizations. Specialized scripts are written for selected
hardware and software components to ensure reliable energy measurements. The
energy measurement process is repeated many times and aggregated to remove
noise. These steps make the hardware-based energy measurement process
time-consuming and not easy to adapt or reproduce. There is a lack of
open-source tools available for developers and researchers to take reliable
energy measurements via hardware devices. In this paper, we present and
demonstrate ARENA, a support tool that enables developers and researchers to
connect to a physical measurement device without leaving the comfort of their
IDE. Developers could use ARENA during development to compare energy
consumption between different apps or versions of the same app. ARENA
calculates energy consumption on an Android smartphone by executing a test
scenario on the app under development. Further, ARENA helps aggregate,
statistically analyze, report, and visualize the data, allowing developers and
researchers to dig into the data directly or visually. We implemented ARENA as
an IntelliJ and Android Studio plugin.

</details>


### [25] [Towards Speeding up Program Repair with Non-Autoregressive Model](https://arxiv.org/abs/2510.01825)
*Zhenyu Yang,Yue Pan,Zhen Yang,Zhongxing Yu*

Main category: cs.SE

TL;DR: The paper proposes NARRepair, a non-autoregressive (NAR) model for automatic program repair (APR), which significantly improves repair speed compared to traditional autoregressive (AR) methods while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing autoregressive (AR) machine learning-based APR techniques suffer from significant time delays, especially when using large models with many parameters. These delays hinder the practicality of APR for real-time applications.

Method: NARRepair employs a parallelized non-autoregressive approach with three key innovations: 1) a repair action predictor to mitigate over-correction, 2) an inter-token dependency extractor to address missing inter-token dependencies in NAR generation, and 3) a two-stage decoder to preserve contextual information for accurate patch generation.

Result: NARRepair outperforms other APR techniques within limited repair times and achieves a 1.4-6.4x speedup over AR-based methods in a GPU environment without sacrificing patch quality.

Conclusion: NARRepair effectively addresses the speed limitations of autoregressive APR by applying a customized non-autoregressive approach. The model achieves state-of-the-art performance in terms of both speed and accuracy, making it a practical solution for fast and effective program repair.

Abstract: Enlightened by the success of machine learning techniques in various
application areas, recent years have witnessed a surge of research efforts on
automatic program repair (APR) using machine learning techniques. Previous
machine learning-based APR techniques essentially modified bugs in the
autoregressive (AR) manner, which predicts future values based on past values.
Due to the manner of token-by-token generation, the AR-based APR technique has
a huge time delay. In particular, the delay of the APR model with a large
number of parameters is more serious. To address the issue, we aim to apply the
non-autoregressive (NAR) method to the APR task, which can output target code
in a parallel manner to avoid huge repair delays. However, the naive use of the
NAR manner for the APR task suffers from the issue of compromised patch
quality. To effectively adapt the NAR manner for the APR task, we in this paper
propose NARRepair, the first customized NAR code generation model for the APR
task. The NARRepair model features three major novelties, including 1) the
repair action predictor for alleviating the over-correction issue, 2) the
inter-token dependency extractor for alleviating the issue of lacking
inter-token dependency information, and 3) the two-stage decoder for
alleviating the issue of lacking contextual information. We evaluated NARRepair
on three widely used datasets in the APR community, and the results show that
1) compared to other APR techniques, the NARRepair model has the best
performance within the limited repair time, and 2) compared to AR-based APR
techniques, the repair speed of NARRepair has been increased by 1.4-6.4 times
in the GPU environment. Overall, the results show that NARRepair has achieved
state-of-the-art comprehensive performance in terms of repair speed and
accuracy.

</details>


### [26] [RefFilter: Improving Semantic Conflict Detection via Refactoring-Aware Static Analysis](https://arxiv.org/abs/2510.01960)
*Victor Lira,Paulo Borba,Rodrigo Bonifácio,Galileu Santos e Matheus barbosa*

Main category: cs.SE

TL;DR: RefFilter filters behavior-preserving refactorings to cut false positives by 32% in semantic interference detection, enhancing merge precision with minor recall trade-offs.


<details>
  <summary>Details</summary>
Motivation: Current lightweight static analysis methods for detecting semantic interference suffer from high false positive rates due to failure to distinguish behavior-preserving refactorings from behavior-impacting changes.

Method: RefFilter integrates automated refactoring detection into existing static analysis techniques to filter out non-behavioral changes, thereby enhancing precision while maintaining detection coverage.

Result: RefFilter reduces false positives by 32% on a labeled dataset while introducing a negligible increase in false negatives, demonstrating significant precision gains with two evaluation datasets (99-labeled scenarios and 1,087 merge scenarios).

Conclusion: RefFilter effectively addresses the issue of false positives in semantic interference detection by identifying and excluding behavior-preserving refactorings, making it a practical solution for improving merge support in collaborative software development.

Abstract: Detecting semantic interference remains a challenge in collaborative software
development. Recent lightweight static analysis techniques improve efficiency
over SDG-based methods, but they still suffer from a high rate of false
positives. A key cause of these false positives is the presence of
behavior-preserving code refactorings, which current techniques cannot
effectively distinguish from changes that impact behavior and can interfere
with others. To handle this problem we present RefFilter, a refactoring-aware
tool for semantic interference detection. It builds on existing static
techniques by incorporating automated refactoring detection to improve
precision. RefFilter discards behavior-preserving refactorings from reports,
reducing false positives while preserving detection coverage. To evaluate
effectiveness and scalability, use two datasets: a labeled dataset with 99
scenarios and ground truth, and a novel dataset of 1,087 diverse merge
scenarios that we have built. Experimental results show that RefFilter reduces
false positives by nearly 32% on the labeled dataset. While this reduction
comes with a non significant increase in false negatives, the overall gain in
precision significantly outweighs the minor trade-off in recall. These findings
demonstrate that refactoring-aware interference detection is a practical and
effective strategy for improving merge support in modern development workflows.

</details>


### [27] [Clarifying Semantics of In-Context Examples for Unit Test Generation](https://arxiv.org/abs/2510.01994)
*Chen Yang,Lin Yang,Ziqi Wang,Dong Wang,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: CLAST systematically refines unit tests for better semantic clarity using program analysis and LLMs, outperforming state-of-the-art methods and significantly improving downstream test generation effectiveness.


<details>
  <summary>Details</summary>
Motivation: Current ICL-based unit test generation relies heavily on example quality, but suboptimal test examples significantly degrade performance. Existing refinement techniques like UTgen reduce critical test metrics, motivating the need for better semantic clarity.

Method: CLAST employs program analysis and LLM-based rewriting to decompose complex tests into clearer logical components, improving semantic clarity without sacrificing original test effectiveness.

Result: CLAST improves test metrics by 25.97-45.99%, retains original effectiveness completely, and achieves 85.33% user preference over UTgen. It also enhances ICL-based generators by similar margins compared to UTgen.

Conclusion: CLAST enhances semantic clarity and test effectiveness, outperforming existing methods and improving IC-based test generation, while opening new research avenues in software testing.

Abstract: Recent advances in large language models (LLMs) have enabled promising
performance in unit test generation through in-context learning (ICL). However,
the quality of in-context examples significantly influences the effectiveness
of generated tests-poorly structured or semantically unclear test examples
often lead to suboptimal outputs. In this paper, we propose CLAST, a novel
technique that systematically refines unit tests to improve their semantic
clarity, thereby enhancing their utility as in-context examples. The approach
decomposes complex tests into logically clearer ones and improves semantic
clarity through a combination of program analysis and LLM-based rewriting. We
evaluated CLAST on four open-source and three industrial projects. The results
demonstrate that CLAST largely outperforms UTgen, the state-of-the-art
refinement technique, in both preserving test effectiveness and enhancing
semantic clarity. Specifically, CLAST fully retains the original effectiveness
of unit tests, while UTgen reduces compilation success rate (CSR), pass rate
(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,
35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user
study preferred the semantic clarity of CLAST-refined tests. Notably,
incorporating CLAST-refined tests as examples effectively improves ICL-based
unit test generation approaches such as RAGGen and TELPA, resulting in an
average increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for
generated tests, compared to incorporating UTgen-refined tests. The insights
from the follow-up user study not only reinforce CLAST's potential impact in
software testing practice but also illuminate avenues for future research.

</details>


### [28] [Automatic Generation of Combinatorial Reoptimisation Problem Specifications: A Vision](https://arxiv.org/abs/2510.02002)
*Maximilian Kratz,Steffen Zschaler,Jens Kosiol,Gabriele Taentzer*

Main category: cs.SE

TL;DR: This paper advocates for using Model-Driven Engineering (MDE) to systematize reoptimization problems, offering a GIPS-based proof of concept applied to teaching assistant allocation, enabling minimal-change adaptations in combinatorial optimization contexts.


<details>
  <summary>Details</summary>
Motivation: Reoptimization is critical when contextual factors change, requiring solutions to adapt while minimizing modifications, respecting inalterable parts of the original solution, and generating actionable change scripts. Traditional methods lack systematic strategies for these challenges in combinatorial problems.

Method: The authors propose leveraging declarative modelling languages and model transformations within MDE to formalize reoptimization strategies. They implemented this approach in the GIPS tool, applying it to a teaching assistant allocation problem to demonstrate feasibility.

Result: The paper presents an initial categorization of reoptimization problem types and strategies for specification derivation. A GIPS-based implementation successfully applied their approach to a real-world resource-allocation case study, validating the method's potential.

Conclusion: The paper concludes that Model-Driven Engineering (MDE) provides a systematic framework for deriving reoptimization problems from original optimisation specifications, enabling efficient adaptation with minimal changes and respect to constraints.

Abstract: Once an optimisation problem has been solved, the solution may need
adaptation when contextual factors change. This challenge, also known as
reoptimisation, has been addressed in various problem domains, such as railway
crew rescheduling, nurse rerostering, or aircraft recovery. This requires a
modified problem to be solved again to ensure that the adapted solution is
optimal in the new context. However, the new optimisation problem differs
notably from the original problem: (i) we want to make only minimal changes to
the original solution to minimise the impact; (ii) we may be unable to change
some parts of the original solution (e.g., because they refer to past
allocations); and (iii) we need to derive a change script from the original
solution to the new solution. In this paper, we argue that Model-Driven
Engineering (MDE) - in particular, the use of declarative modelling languages
and model transformations for the high-level specification of optimisation
problems - offers new opportunities for the systematic derivation of
reoptimisation problems from the original optimisation problem specification.
We focus on combinatorial reoptimisation problems and provide an initial
categorisation of changing problems and strategies for deriving the
corresponding reoptimisation specifications. We introduce an initial
proof-of-concept implementation based on the GIPS (Graph-Based (Mixed) Integer
Linear Programming Problem Specification) tool and apply it to an example
resource-allocation problem: the allocation of teaching assistants to teaching
sessions.

</details>


### [29] [ACM SIGSOFT SEN Empirical Software Engineering: Introducing Our New Regular Column](https://arxiv.org/abs/2510.02007)
*Justus Bogner,Roberto Verdecchia*

Main category: cs.SE

TL;DR: This paper announces the ACM SIGSOFT SEN-ESE column to improve empirical software engineering research through structured meta-discussion on topics like reproducibility and interdisciplinary publishing.


<details>
  <summary>Details</summary>
Motivation: Despite ESE's maturation, challenges like research reproducibility, limited external validity, and underdocumented practices hinder progress. The column addresses these gaps by creating a dedicated platform for meta-discussion.

Method: The column will feature expert interviews, focus groups, surveys, and position pieces to explore meta-aspects of ESE research like replication packages, statistical methods, and interdisciplinary publishing.

Result: A new regular publication venue (SEN-ESE) is introduced to catalyze conversations on improving ESE research practices through evidence-based contributions from the academic and industrial communities.

Conclusion: The establishment of the SEN-ESE column aims to foster community-driven discussions and improvements in empirical software engineering (ESE) research by addressing gaps in methodology, documentation, and practice transfer.

Abstract: From its early foundations in the 1970s, empirical software engineering (ESE)
has evolved into a mature research discipline that embraces a plethora of
different topics, methodologies, and industrial practices. Despite its
remarkable progress, the ESE research field still needs to keep evolving, as
new impediments, shortcoming, and technologies emerge. Research
reproducibility, limited external validity, subjectivity of reviews, and
porting research results to industrial practices are just some examples of the
drivers for improvements to ESE research. Additionally, several facets of ESE
research are not documented very explicitly, which makes it difficult for
newcomers to pick them up. With this new regular ACM SIGSOFT SEN column
(SEN-ESE), we introduce a venue for discussing meta-aspects of ESE research,
ranging from general topics such as the nature and best practices for
replication packages, to more nuanced themes such as statistical methods,
interview transcription tools, and publishing interdisciplinary research. Our
aim for the column is to be a place where we can regularly spark conversations
on ESE topics that might not often be touched upon or are left implicit.
Contributions to this column will be grounded in expert interviews, focus
groups, surveys, and position pieces, with the goal of encouraging reflection
and improvement in how we conduct, communicate, teach, and ultimately improve
ESE research. Finally, we invite feedback from the ESE community on
challenging, controversial, or underexplored topics, as well as suggestions for
voices you would like to hear from. While we cannot promise to act on every
idea, we aim to shape this column around the community interests and are
grateful for all contributions.

</details>


### [30] [Towards fairer public transit: Real-time tensor-based multimodal fare evasion and fraud detection](https://arxiv.org/abs/2510.02165)
*Peter Wauyo,Dalia Bwiza,Alain Murara,Edwin Mugume,Eric Umuhoza*

Main category: cs.SE

TL;DR: A multimodal fraud detection system using ViViT, AST, and Tensor Fusion Networks achieves 84.0% recall, surpassing prior methods by 8.8%, enabling real-time monitoring in public transport.


<details>
  <summary>Details</summary>
Motivation: The study addresses the critical need for improved fraud and fare evasion detection in public transportation, aiming to mitigate revenue loss, enhance passenger safety, and ensure compliance through advanced multimodal analysis of CCTV and audio data.

Method: The system employs a Tensor Fusion Network (TFN) to integrate video features extracted via Vision Transformer for Video (ViViT) and audio features from the Audio Spectrogram Transformer (AST), utilizing a 2-fold Cartesian product to model cross-modal interactions between visual behaviors and audio cues.

Result: The system achieved 89.5% accuracy, 87.2% precision, and 84.0% recall, outperforming early fusion baselines and state-of-the-art models by 8.8% in recall, with ablation studies confirming the 7.0% F1 score gain and 8.8% recall boost from tensor fusion.

Conclusion: The research concludes that the proposed multimodal system using ViViT, AST, and Tensor Fusion Networks significantly enhances fraud detection accuracy, surpassing existing methods with an 84.0% recall and 7.8% improvement in F1 score over traditional approaches, while enabling real-time public transport monitoring.

Abstract: This research introduces a multimodal system designed to detect fraud and
fare evasion in public transportation by analyzing closed circuit television
(CCTV) and audio data. The proposed solution uses the Vision Transformer for
Video (ViViT) model for video feature extraction and the Audio Spectrogram
Transformer (AST) for audio analysis. The system implements a Tensor Fusion
Network (TFN) architecture that explicitly models unimodal and bimodal
interactions through a 2-fold Cartesian product. This advanced fusion technique
captures complex cross-modal dynamics between visual behaviors (e.g.,
tailgating,unauthorized access) and audio cues (e.g., fare transaction sounds).
The system was trained and tested on a custom dataset, achieving an accuracy of
89.5%, precision of 87.2%, and recall of 84.0% in detecting fraudulent
activities, significantly outperforming early fusion baselines and exceeding
the 75% recall rates typically reported in state-of-the-art transportation
fraud detection systems. Our ablation studies demonstrate that the tensor
fusion approach provides a 7.0% improvement in the F1 score and an 8.8% boost
in recall compared to traditional concatenation methods. The solution supports
real-time detection, enabling public transport operators to reduce revenue
loss, improve passenger safety, and ensure operational compliance.

</details>


### [31] [SIEVE: Towards Verifiable Certification for Code-datasets](https://arxiv.org/abs/2510.02166)
*Fatou Ndiaye Mbodji,El-hacen Diallo,Jordan Samhi,Kui Liu,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.SE

TL;DR: SIEVE introduces verifiable 'Confidence Cards' for code datasets, offering statistical guarantees to improve quality assurance and trust in empirical software engineering.


<details>
  <summary>Details</summary>
Motivation: Public code datasets lack auditable quality guarantees; existing 'dataset cards’ are non-verified and non-statistical, leading to redundant cleaning pipelines and high costs.

Method: A community-driven framework called SIEVE converts per-property checks into machine-readable, statistically bounded 'Confidence Cards' with anytime-valid certifications.

Result: Outlines a research plan to transition from narrative dataset cards to SIEVE's verifiable certification system, but no empirical results are presented in the abstract.

Conclusion: SIEVE aims to reduce quality assurance costs and increase trust in code datasets by providing verifiable statistical guarantees through community-driven certification.

Abstract: Code agents and empirical software engineering rely on public code datasets,
yet these datasets lack verifiable quality guarantees. Static 'dataset cards'
inform, but they are neither auditable nor do they offer statistical
guarantees, making it difficult to attest to dataset quality. Teams build
isolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We
present SIEVE, a community-driven framework. It turns per-property checks into
Confidence Cards-machine-readable, verifiable certificates with anytime-valid
statistical bounds. We outline a research plan to bring SIEVE to maturity,
replacing narrative cards with anytime-verifiable certification. This shift is
expected to lower quality-assurance costs and increase trust in code-datasets.

</details>


### [32] [TAIBOM: Bringing Trustworthiness to AI-Enabled Systems](https://arxiv.org/abs/2510.02169)
*Vadim Safronov,Anthony McCaigue,Nicholas Allott,Andrew Martin*

Main category: cs.SE

TL;DR: This paper introduces TAIBOM, a framework extending SBOM principles to address AI system transparency gaps, addressing dynamic dependencies, integrity, and trust in AI workflows.


<details>
  <summary>Details</summary>
Motivation: Current SBOMs lack mechanisms to capture AI systems' dynamic, data-driven nature and complex dependencies, leading to governance and compliance challenges in AI environments.

Method: TAIBOM introduces (i) a structured AI dependency model, (ii) integrity propagation across heterogeneous pipelines, and (iii} trust attestation for component provenance, with comparisons to SPDX/CycloneDX.

Result: Demonstrates TAIBOM's effectiveness in providing assurance, security, and compliance for AI workflows, outperforming existing standards through tailored AI-component transparency.

Conclusion: TAIBOM establishes foundational software transparency for trustworthy AI systems by addressing supply chain gaps through structured dependency modeling and attestation processes.

Abstract: The growing integration of open-source software and AI-driven technologies
has introduced new layers of complexity into the software supply chain,
challenging existing methods for dependency management and system assurance.
While Software Bills of Materials (SBOMs) have become critical for enhancing
transparency and traceability, current frameworks fall short in capturing the
unique characteristics of AI systems -- namely, their dynamic, data-driven
nature and the loosely coupled dependencies across datasets, models, and
software components. These challenges are compounded by fragmented governance
structures and the lack of robust tools for ensuring integrity, trust, and
compliance in AI-enabled environments.
  In this paper, we introduce Trusted AI Bill of Materials (TAIBOM) -- a novel
framework extending SBOM principles to the AI domain. TAIBOM provides (i) a
structured dependency model tailored for AI components, (ii) mechanisms for
propagating integrity statements across heterogeneous AI pipelines, and (iii) a
trust attestation process for verifying component provenance. We demonstrate
how TAIBOM supports assurance, security, and compliance across AI workflows,
highlighting its advantages over existing standards such as SPDX and CycloneDX.
This work lays the foundation for trustworthy and verifiable AI systems through
structured software transparency.

</details>


### [33] [FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen Using Agentic AI](https://arxiv.org/abs/2510.02185)
*Paschal C. Amusuo,Dongge Liu,Ricardo Andres Calvo Mendez,Jonathan Metzman,Oliver Chang,James C. Davis*

Main category: cs.SE

TL;DR: This paper proposes two AI-enhanced approaches to reduce false positives in automated fuzzing: constraint-aware driver generation and context-aware crash validation, proving AI can improve large-scale fuzzing reliability.


<details>
  <summary>Details</summary>
Motivation: Automatically generated fuzz drivers often produce false positives that undermine trust in systems like OSS-Fuzz-Gen. This problem becomes critical at scale as false crashes reported to maintainers disrupt collaboration and confidence in automated fuzzing processes.

Method: The paper introduces two AI techniques: 1) Constraint-based fuzz driver generation, which enforces function input/state constraints during driver creation, and 2) Context-based crash validation, which analyzes function callers to verify crash feasibility from program entry points.

Result: On 1,500 OSS-Fuzz functions, the approaches reduced spurious crashes by up to 8%, cut reported crashes by 50%, and demonstrated that frontier LLMs can function as reliable program analysis agents for fuzzing tasks.

Conclusion: The paper concludes that integrating AI-driven strategies, such as constraint-based fuzz driver generation and context-based crash validation, into large-scale fuzzing pipelines like OSS-Fuzz-Gen can significantly reduce false positives and improve trust in automated fuzzing. However, challenges remain in ensuring AI reliability for program analysis at scale.

Abstract: Fuzz testing has become a cornerstone technique for identifying software bugs
and security vulnerabilities, with broad adoption in both industry and
open-source communities. Directly fuzzing a function requires fuzz drivers,
which translate random fuzzer inputs into valid arguments for the target
function. Given the cost and expertise required to manually develop fuzz
drivers, methods exist that leverage program analysis and Large Language Models
to automatically generate these drivers. However, the generated fuzz drivers
frequently lead to false positive crashes, especially in functions highly
structured input and complex state requirements. This problem is especially
crucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as
reporting false positive crashes to maintainers impede trust in both the system
and the team.
  This paper presents two AI-driven strategies to reduce false positives in
OSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First,
constraint-based fuzz driver generation proactively enforces constraints on a
function's inputs and state to guide driver creation. Second, context-based
crash validation reactively analyzes function callers to determine whether
reported crashes are feasible from program entry points. Using 1,500 benchmark
functions from OSS-Fuzz, we show that these strategies reduce spurious crashes
by up to 8%, cut reported crashes by more than half, and demonstrate that
frontier LLMs can serve as reliable program analysis agents. Our results
highlight the promise and challenges of integrating AI into large-scale fuzzing
pipelines.

</details>
