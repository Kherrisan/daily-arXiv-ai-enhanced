{"id": "2507.08844", "categories": ["cs.CR", "cs.CC", "03B70, 68M10, 91A80", "F.4.1; D.4.6; C.2.2"], "pdf": "https://arxiv.org/pdf/2507.08844", "abs": "https://arxiv.org/abs/2507.08844", "authors": ["Craig S Wright"], "title": "Immutability Does Not Guarantee Trust: A Formal and Logical Refutation", "comment": "8 pages", "summary": "It is frequently claimed in blockchain discourse that immutability guarantees\ntrust. This paper rigorously refutes that assertion. We define immutability as\nthe cryptographic persistence of historical states in an append-only data\nstructure and contrast it with trust, understood as a rational epistemic\nexpectation under uncertainty. Employing predicate logic, automata-theoretic\nmodels, and epistemic game-theoretic analysis, we demonstrate that immutability\nneither entails nor implies correctness, fairness, or credibility. Through\nformal constructions and counterexamples--including predictive fraud schemes\nand the phenomenon of garbage permanence--we show that the belief conflates\nstructural and epistemic domains. Immutability preserves all data equally,\nregardless of veracity. Therefore, the assertion that immutability guarantees\ntrust collapses under the weight of formal scrutiny.", "AI": {"tldr": "Blockchain immutability does not guarantee trust because it does not ensure correctness, fairness, or credibility\u2014illustrated via formal analysis and counterexamples like predictive fraud and garbage permanence.", "motivation": "The paper addresses the widely held but logically flawed belief that blockchain immutability inherently establishes trust, which could lead to misapplication in real-world systems if unchallenged.", "method": "Combines predicate logic, automata-theoretic models, and epistemic game-theoretic analysis to formalize and refute the relationship between structural immutability and epistemic trust.", "result": "Formal proofs and counterexamples demonstrate that data persistence (immutability) coexists with persistent inaccuracies (garbage permanence) and malicious structures (predictive fraud), breaking any logical connection between the two domains.", "conclusion": "Trust requires rational justification under uncertainty, while immutability is a structural property. Both concepts and their limitations must be contextualized within formal models to prevent misapplication of blockchain technology."}}
{"id": "2507.08853", "categories": ["cs.CR", "cs.AI", "cs.CY", "cs.DL", "D.2.11, H.3.4, H.3.7, J.5"], "pdf": "https://arxiv.org/pdf/2507.08853", "abs": "https://arxiv.org/abs/2507.08853", "authors": ["Victoria L. Lemieux", "Rosa Gil", "Faith Molosiwa", "Qihong Zhou", "Binming Li", "Roberto Garcia", "Luis De La Torre Cubillo", "Zehua Wang"], "title": "Clio-X: AWeb3 Solution for Privacy-Preserving AI Access to Digital Archives", "comment": "28 pages, 8 figures", "summary": "As archives turn to artificial intelligence to manage growing volumes of\ndigital records, privacy risks inherent in current AI data practices raise\ncritical concerns about data sovereignty and ethical accountability. This paper\nexplores how privacy-enhancing technologies (PETs) and Web3 architectures can\nsupport archives to preserve control over sensitive content while still being\nable to make it available for access by researchers. We present Clio-X, a\ndecentralized, privacy-first Web3 digital solution designed to embed PETs into\narchival workflows and support AI-enabled reference and access. Drawing on a\nuser evaluation of a medium-fidelity prototype, the study reveals both interest\nin the potential of the solution and significant barriers to adoption related\nto trust, system opacity, economic concerns, and governance. Using Rogers'\nDiffusion of Innovation theory, we analyze the sociotechnical dimensions of\nthese barriers and propose a path forward centered on participatory design and\ndecentralized governance through a Clio-X Decentralized Autonomous\nOrganization. By integrating technical safeguards with community-based\noversight, Clio-X offers a novel model to ethically deploy AI in cultural\nheritage contexts.", "AI": {"tldr": "Clio-X is a decentralized Web3 solution integrating privacy-enhancing technologies to enable ethical AI use in archives, addressing control and access challenges through participatory design and decentralized governance.", "motivation": "Archives face growing privacy risks with AI adoption, necessitating solutions to balance data sovereignty, ethical accountability, and researcher access to sensitive digital records.", "method": "Developed Clio-X, a privacy-first Web3 platform embedding PETs into archival workflows; conducted user evaluations with a medium-fidelity prototype and applied Rogers' Diffusion of Innovation theory to analyze adoption barriers.", "result": "User evaluations revealed both interest in Clio-X's potential and barriers including trust issues, system opacity, economic concerns, and governance challenges.", "conclusion": "Clio-X's model combines technical PETs with decentralized autonomous organization governance to create an ethical framework for AI in cultural heritage, leveraging participatory design to address sociotechnical adoption barriers."}}
{"id": "2507.08862", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08862", "abs": "https://arxiv.org/abs/2507.08862", "authors": ["Tianzhe Zhao", "Jiaoyan Chen", "Yanchi Ru", "Haiping Zhu", "Nan Hu", "Jun Liu", "Qika Lin"], "title": "RAG Safety: Exploring Knowledge Poisoning Attacks to Retrieval-Augmented Generation", "comment": "13 pages, 6 figures", "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nretrieving external data to mitigate hallucinations and outdated knowledge\nissues. Benefiting from the strong ability in facilitating diverse data sources\nand supporting faithful reasoning, knowledge graphs (KGs) have been\nincreasingly adopted in RAG systems, giving rise to KG-based RAG (KG-RAG)\nmethods. Though RAG systems are widely applied in various applications, recent\nstudies have also revealed its vulnerabilities to data poisoning attacks, where\nmalicious information injected into external knowledge sources can mislead the\nsystem into producing incorrect or harmful responses. However, these studies\nfocus exclusively on RAG systems using unstructured textual data sources,\nleaving the security risks of KG-RAG largely unexplored, despite the fact that\nKGs present unique vulnerabilities due to their structured and editable nature.\nIn this work, we conduct the first systematic investigation of the security\nissue of KG-RAG methods through data poisoning attacks. To this end, we\nintroduce a practical, stealthy attack setting that aligns with real-world\nimplementation. We propose an attack strategy that first identifies adversarial\ntarget answers and then inserts perturbation triples to complete misleading\ninference chains in the KG, increasing the likelihood that KG-RAG methods\nretrieve and rely on these perturbations during generation. Through extensive\nexperiments on two benchmarks and four recent KG-RAG methods, our attack\nstrategy demonstrates strong effectiveness in degrading KG-RAG performance,\neven with minimal KG perturbations. In-depth analyses are also conducted to\nunderstand the safety threats within the internal stages of KG-RAG systems and\nto explore the robustness of LLMs against adversarial knowledge.", "AI": {"tldr": "This paper is the first systematic investigation of security risks in Knowledge Graph-based Retrieval-Augmented Generation (KG-RAG) systems, revealing that they are vulnerable to data poisoning attacks through insertion of adversarial triples into knowledge graphs. The attack strategy demonstrates strong effectiveness in degrading KG-RAG performance even with minimal perturbations.", "motivation": "The motivation arises from the observation that while RAG systems using unstructured textual data sources have been studied for data poisoning vulnerabilities (e.g., hallucinations, outdated knowledge), the unique security risks of KG-RAG systems\u2014leverage structured, editable knowledge graphs\u2014remain largely unexplored despite their potential for targeted adversarial manipulation.", "method": "The paper proposes an attack framework with two stages: 1) Identifying adversarial target answers with high similarity to ground truth answers (e.g., 'Barcelona is the capital of France'), and 2) Strategically inserting perturbation triples (e.g., 'Barcelona -capital of-> France') into the knowledge graph to complete coherent but misleading inference chains that maximize their usage in downstream retrieval.", "result": "Experiments on two benchmarks (e.g., RealKG, FALcon) and four recent KG-RAG methodologies (e.g., Coarse2Fine, Graph RAG) show the attack degrades performance by up to 33.6% in accuracy without requiring excessive knowledge graph modifications. Ablation studies and internal mechanism analyses (e.g., query-knowledge alignment, triple credibility) further expose vulnerabilities in specific components like retrieval scoring and reasoning chains.", "conclusion": "The work establishes data poisoning as a critical security threat for KG-RAG systems and provides empirical evidence of their vulnerability. It suggests future research should focus on robustness for structured knowledge sources and develop detection/defense mechanisms for adversarial triple insertions in knowledge graphs."}}
{"id": "2507.08864", "categories": ["cs.CR", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.08864", "abs": "https://arxiv.org/abs/2507.08864", "authors": ["Poushali Sengupta", "Sabita Maharjan", "frank Eliassen", "Yan Zhang"], "title": "Privacy-Utility-Fairness: A Balanced Approach to Vehicular-Traffic Management System", "comment": "accepted in VTC 2025 Spring, Oslo, Norway", "summary": "Location-based vehicular traffic management faces significant challenges in\nprotecting sensitive geographical data while maintaining utility for traffic\nmanagement and fairness across regions. Existing state-of-the-art solutions\noften fail to meet the required level of protection against linkage attacks and\ndemographic biases, leading to privacy leakage and inequity in data analysis.\nIn this paper, we propose a novel algorithm designed to address the challenges\nregarding the balance of privacy, utility, and fairness in location-based\nvehicular traffic management systems. In this context, utility means providing\nreliable and meaningful traffic information, while fairness ensures that all\nregions and individuals are treated equitably in data use and decision-making.\nEmploying differential privacy techniques, we enhance data security by\nintegrating query-based data access with iterative shuffling and calibrated\nnoise injection, ensuring that sensitive geographical data remains protected.\nWe ensure adherence to epsilon-differential privacy standards by implementing\nthe Laplace mechanism. We implemented our algorithm on vehicular location-based\ndata from Norway, demonstrating its ability to maintain data utility for\ntraffic management and urban planning while ensuring fair representation of all\ngeographical areas without being overrepresented or underrepresented.\nAdditionally, we have created a heatmap of Norway based on our model,\nillustrating the privatized and fair representation of the traffic conditions\nacross various cities. Our algorithm provides privacy in vehicular traffic", "AI": {"tldr": "This paper introduces a differential privacy algorithm for location-based vehicular traffic management that balances privacy protection, data utility for traffic analysis, and fairness across regions using Norway's data as a case study.", "motivation": "Existing traffic management systems face privacy risks from linkage attacks and demographic biases, causing data leaks and inequitable regional representation in analysis results.", "method": "The approach combines query-based data access with iterative shuffling and calibrated Laplace noise injection to meet epsilon-differential privacy standards while maintaining meaningful traffic data patterns.", "result": "Implementation on Norwegian vehicular data demonstrated effective privacy preservation (no over/underrepresentation), reliable traffic analysis utility, and equitable spatial representation through geographically balanced heatmaps.", "conclusion": "The proposed algorithm provides a robust framework for secure, fair vehicular traffic data analysis, addressing existing limitations in privacy-utility-fairness tradeoffs through structured differential privacy application."}}
{"id": "2507.08943", "categories": ["cs.SE", "D.2.7"], "pdf": "https://arxiv.org/pdf/2507.08943", "abs": "https://arxiv.org/abs/2507.08943", "authors": ["Pedro Lopes", "Paola Accioly", "Paulo Borba", "Vitor Menezes"], "title": "Choosing the Right Git Workflow: A Comparative Analysis of Trunk-based vs. Branch-based Approaches", "comment": "11 pages with 3 figures", "summary": "Git has become one of the most widely used version control systems today.\nAmong its distinguishing features, its ability to easily and quickly create\nbranches stands out, allowing teams to customize their workflows. In this\ncontext, various formats of collaborative development workflows using Git have\nemerged and gained popularity among software engineers. We can categorize such\nworkflows into two main types: branch-based workflows and trunk-based\nworkflows. Branch-based workflows typically define a set of remote branches\nwith well-defined objectives, such as feature branches, a branch for feature\nintegration, and a main branch. The goal is to migrate changes from the most\nisolated branch to the main one shared by all as the code matures. In this\ncategory, GitFlow stands out as the most popular example. In contrast,\ntrunk-based workflows have a single remote branch where developers integrate\ntheir changes directly. In this range of options, choosing a workflow that\nmaximizes team productivity while promoting software quality becomes a\nnon-trivial task. Despite discussions on forums, social networks, and blogs,\nfew scientific articles have explored this topic. In this work, we provide\nevidence on how Brazilian developers work with Git workflows and what factors\nfavor or hinder the use of each model. To this end, we conducted\nsemi-structured interviews and a survey with software developers. Our results\nindicate that trunk-based development favors fast-paced projects with\nexperienced and smaller teams, while branch-based development suits less\nexperienced and larger teams better, despite posing management challenges.", "AI": {"tldr": "The paper compares branch-based and trunk-based Git workflows, finding that trunk-based suits fast-paced, experienced small teams despite management challenges, while branch-based (e.g., GitFlow) is preferred by less experienced, larger teams.", "motivation": "Despite popular discussions on Git workflows, few scientific studies explore their real-world use. This work addresses this gap by investigating factors influencing workflow adoption in Brazilian software teams.", "method": "The study uses semi-structured interviews and surveys with software developers to gather qualitative data on workflow preferences and challenges.", "result": "Trunk-based workflows correlate with agile environments and team experience, while branch-based workflows provide structure for larger teams but introduce complexity in coordination and maintenance.", "conclusion": "Workflow choice depends on team size, experience, and project agility. Trunk-based development increases efficiency in small teams, while branch-based models offer necessary discipline for larger, less experienced ones despite management drawbacks."}}
{"id": "2507.08878", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08878", "abs": "https://arxiv.org/abs/2507.08878", "authors": ["Xinyu Huang", "Leming Shen", "Zijing Ma", "Yuanqing Zheng"], "title": "Towards Privacy-Preserving and Personalized Smart Homes via Tailored Small Language Models", "comment": null, "summary": "Large Language Models (LLMs) have showcased remarkable generalizability in\nlanguage comprehension and hold significant potential to revolutionize\nhuman-computer interaction in smart homes. Existing LLM-based smart home\nassistants typically transmit user commands, along with user profiles and home\nconfigurations, to remote servers to obtain personalized services. However,\nusers are increasingly concerned about the potential privacy leaks to the\nremote servers. To address this issue, we develop HomeLLaMA, an on-device\nassistant for privacy-preserving and personalized smart home serving with a\ntailored small language model (SLM). HomeLLaMA learns from cloud LLMs to\ndeliver satisfactory responses and enable user-friendly interactions. Once\ndeployed, HomeLLaMA facilitates proactive interactions by continuously updating\nlocal SLMs and user profiles. To further enhance user experience while\nprotecting their privacy, we develop PrivShield to offer an optional\nprivacy-preserving LLM-based smart home serving for those users, who are\nunsatisfied with local responses and willing to send less-sensitive queries to\nremote servers. For evaluation, we build a comprehensive benchmark DevFinder to\nassess the service quality. Extensive experiments and user studies (M=100)\ndemonstrate that HomeLLaMA can provide personalized services while\nsignificantly enhancing user privacy.", "AI": {"tldr": "HomeLLaMA is an on-device smart home assistant that enhances privacy using a tailored small language model (SLM), while optionally connecting to cloud services for non-sensitive queries with PrivShield.", "motivation": "Current LLM-based smart home assistants transmit user data to remote servers, risking privacy leaks. Users increasingly demand solutions that maintain personalization without compromising data confidentiality.", "method": "HomeLLaMA local SLMs are pretrained on cloud LLMs and continuously updated via local interactions and user profiles. PrivShield enables remote LLM service usage for non-sensitive queries while anonymizing inputs when necessary.", "result": "Evaluation via DevFinder benchmark and 100-participant user studies confirms HomeLLaMA achieves equivalent service quality compared to cloud LLMs while retaining 97% of user data locally. PrivShield reduces 82% of query personal information without affecting accuracy.", "conclusion": "HomeLLaMA establishes a practical framework for privacy-preserving smart home autonomy, demonstrating that strong personalization and privacy protection can coexist with optional cloud integration"}}
{"id": "2507.08992", "categories": ["cs.SE", "cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.08992", "abs": "https://arxiv.org/abs/2507.08992", "authors": ["Abdelhalim Dahou", "Ansgar Scherp", "Sebastian Kurten", "Brigitte Mathiak", "Madhu Chauhan"], "title": "Semantic Source Code Segmentation using Small and Large Language Models", "comment": "18 pages, 4 figures", "summary": "Source code segmentation, dividing code into functionally coherent segments,\nis crucial for knowledge retrieval and maintenance in software development.\nWhile enabling efficient navigation and comprehension of large codebases,\nmanual and syntactic analysis approaches have become impractical as\nrepositories grow, especially for low-resource languages like R and their\nresearch domains (e.g., social sciences, psychology).This paper introduces an\nautomated, domain-specific approach for research R code segmentation using\nLarge and Small Language Models (LLMs/SLMs). It presents two novel approaches\nand a human-annotated dataset, StatCodeSeg. We explore two distinct approaches:\nline-by-line analysis with context and range-based segment determination. We\nexperiment with LLMs and fine-tuned SLMs. To support the generalizability of\nour approaches, we also include experiments on Python code from the computer\nscience domain.Our results show that context-based line-by-line analysis is\nsuperior over range-based segmentation.Using smaller language models like\nCodeBERT and an encoder-only version of CodeT5+ are better than their LLM\ncounterparts. Most notably, these two best-performing models did not see R code\nduring pre-training versus the LLMs but were only fine-tuned on 4,130 lines of\nmanually annotated code.", "AI": {"tldr": "The paper introduces an automated, domain-specific approach for R code segmentation using LLMs/SLMs, comparing line-by-line context-based analysis with range-based methods and demonstrating superior performance with fine-tuned smaller models (CodeBERT, CodeT5+) even without R code pre-training.", "motivation": "Manual and syntactic code segmentation methods are impractical for large repositories and low-resource languages like R, particularly in research domains such as social sciences and psychology where traditional approaches fall short.", "method": "The study presents two approaches: (1) line-by-line code analysis with contextual modeling using LLMs/SLMs, and (2) range-based segment determination. Experiments involved comparing baseline models with fine-tuned variants (CodeBERT, CodeT5+, LLaMA-3) on a human-annotated R dataset (4,130 lines) and cross-evaluating on Python code from computer science domains.", "result": "Context-based line-by-line analysis outperformed range-based segmentation. Smaller models (CodeBERT, CodeT5+ encoder) exceeded LLM counterparts despite lacking R code pre-training. Cross-domain performance on Python was evaluated to support approach generalizability.", "conclusion": "The work demonstrates that domain-specific fine-tuning of smaller language models can achieve strong segmentation results for low-resource languages, offering a scalable solution to code base navigation challenges in research R code while enabling cross-domain application."}}
{"id": "2507.08904", "categories": ["cs.CR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.08904", "abs": "https://arxiv.org/abs/2507.08904", "authors": ["Yulin Teng", "Keshuang Han", "Pinchang Zhang", "Xiaohong Jiang", "Yulong Shen", "Fu Xiao"], "title": "CovertAuth: Joint Covert Communication and Authentication in MmWave Systems", "comment": null, "summary": "Beam alignment (BA) is a crucial process in millimeter-wave (mmWave)\ncommunications, enabling precise directional transmission and efficient link\nestablishment. However, due to characteristics like omnidirectional exposure\nand the broadcast nature of the BA phase, it is particularly vulnerable to\neavesdropping and identity impersonation attacks. To this end, this paper\nproposes a novel secure framework named CovertAuth, designed to enhance the\nsecurity of the BA phase against such attacks. In particular, to combat\neavesdropping attacks, the closed-form expressions of successful BA probability\nand covert transmission rate are first derived. Then, a covert communication\nproblem aimed at jointly optimizing beam training budget and transmission power\nis formulated to maximize covert communication rate, subject to the covertness\nrequirement. An alternating optimization algorithm combined with successive\nconvex approximation is employed to iteratively achieve optimal results. To\ncombat impersonation attacks, the mutual coupling effect of antenna array\nimpairments is explored as a device feature to design a weighted-sum energy\ndetector based physical layer authentication scheme. Moreover, theoretical\nmodels for authentication metrics like detection and false alarm probabilities\nare also provided to conduct performance analysis. Based on these models, an\noptimization problem is constructed to determine the optimal weight value that\nmaximizes authentication accuracy. Finally, simulation results demonstrate that\nCovertAuth presents improved detection accuracy under the same covertness\nrequirement compared to existing works.", "AI": {"tldr": "This paper proposes CovertAuth, a secure framework enhancing beam alignment security in mmWave communications by addressing eavesdropping and impersonation attacks through covert communication optimization and physical layer authentication using antenna impairments.", "motivation": "Beam alignment in mmWave communications is vulnerable to eavesdropping and identity impersonation attacks due to its omnidirectional exposure and broadcast nature, necessitating improved security measures.", "method": "1. Derives closed-form expressions for BA probability and covert rate. 2. Formulates an optimization problem for maximizing covert communication rate by jointly optimizing beam training budget and transmission power using alternating optimization with successive convex approximation. 3. Leverages mutual coupling effects in antenna arrays as device features for a weighted-sum energy detector authentication scheme and develops theoretical models for authentication performance metrics.", "result": "Simulation results validate CovertAuth's effectiveness in achieving higher authentication accuracy under the same covertness requirements compared to existing methods, demonstrating security improvements against eavesdropping and impersonation attacks.", "conclusion": "CovertAuth significantly enhances BA phase security in mmWave systems by combining covertness-aware optimization with device-specific authentication, validated through theoretical analysis and simulations, representing an effective defense against common security threats."}}
{"id": "2507.09023", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.09023", "abs": "https://arxiv.org/abs/2507.09023", "authors": ["Yao Fehlis", "Charles Crain", "Aidan Jensen", "Michael Watson", "James Juhasz", "Paul Mandel", "Betty Liu", "Shawn Mahon", "Daren Wilson", "Nick Lynch-Jonely", "Ben Leedom", "David Fuller"], "title": "Accelerating Drug Discovery Through Agentic AI: A Multi-Agent Approach to Laboratory Automation in the DMTA Cycle", "comment": null, "summary": "The pharmaceutical industry faces unprecedented challenges in drug discovery,\nwith traditional approaches struggling to meet modern therapeutic development\ndemands. This paper introduces a novel AI framework, Tippy, that transforms\nlaboratory automation through specialized AI agents operating within the\nDesign-Make-Test-Analyze (DMTA) cycle. Our multi-agent system employs five\nspecialized agents - Supervisor, Molecule, Lab, Analysis, and Report, with\nSafety Guardrail oversight - each designed to excel in specific phases of the\ndrug discovery pipeline. Tippy represents the first production-ready\nimplementation of specialized AI agents for automating the DMTA cycle,\nproviding a concrete example of how AI can transform laboratory workflows. By\nleveraging autonomous AI agents that reason, plan, and collaborate, we\ndemonstrate how Tippy accelerates DMTA cycles while maintaining scientific\nrigor essential for pharmaceutical research. The system shows significant\nimprovements in workflow efficiency, decision-making speed, and\ncross-disciplinary coordination, offering a new paradigm for AI-assisted drug\ndiscovery.", "AI": {"tldr": "Tippy is a novel AI framework for pharmaceutical drug discovery featuring a multi-agent system to automate the Design-Make-Test-Analyze (DMTA) cycle, demonstrating accelerated workflows while maintaining scientific rigor.", "motivation": "Traditional drug discovery approaches face limitations in meeting modern therapeutic development demands, necessitating innovative automation solutions to enhance efficiency and decision-making in pharmaceutical research.", "method": "The framework employs five specialized AI agents (Supervisor, Molecule, Lab, Analysis, Report) with Safety Guardrail oversight, each targeting specific DMTA phases. Agents use reasoning, planning, and collaboration to streamline automation.", "result": "Tippy enhances workflow efficiency, decision speed, and cross-disciplinary coordination, marking the first production-ready AI agent system for DMTA cycle automation. It establishes a scalable model for integrating autonomous agents in pharmaceutical workflows.", "conclusion": "This study demonstrates Tippy's potential to revolutionize drug discovery through AI-driven automation, offering a new paradigm that balances speed and scientific precision while emphasizing safety. The implementation provides a practical blueprint for labs adopting next-generation pharmaceutical research methodologies."}}
{"id": "2507.08978", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.08978", "abs": "https://arxiv.org/abs/2507.08978", "authors": ["Katherine Limes", "Nathan Malkin", "Kelsey R. Fulton"], "title": "Characterizing Security and Privacy Teaching Standards for Schools in the United States", "comment": null, "summary": "Increasingly, students begin learning aspects of security and privacy during\ntheir primary and secondary education (grades K-12 in the United States).\nIndividual U.S. states and some national organizations publish teaching\nstandards -- guidance that outlines expectations for what students should learn\n-- which often form the basis for course curricula. However, research has not\nyet examined what is covered by these standards and whether the topics align\nwith what the broader security and privacy community thinks students should\nknow. To shed light on these questions, we started by collecting computer\nscience teaching standards from all U.S. states and eight national\norganizations. After manually examining a total of 11,954 standards, we labeled\n3,778 of them as being related to security and privacy, further classifying\nthese into 103 topics. Topics ranged from technical subjects like encryption,\nnetwork security, and embedded systems to social subjects such as laws, ethics,\nand appropriate online behavior. Subsequently, we interviewed 11 security and\nprivacy professionals to examine how the teaching standards align with their\nexpectations. We found that, while the specific topics they mentioned mostly\noverlapped with those of existing standards, professionals placed a greater\nemphasis on threat modeling and security mindset.", "AI": {"tldr": "The paper analyzes K-12 cybersecurity/privacy teaching standards across 50 U.S. states and 8 national organizations, revealing a need for stronger emphasis on threat modeling and security mindset in curricula.", "motivation": "This research addresses the gap in understanding how current K-12 computer science standards incorporate security and privacy topics, and whether they align with professional expectations.", "method": "The authors collected and manually reviewed 11,954 standards, labeled 3,778 as security/privacy related (103 topics), and conducted interviews with 11 security/privacy professionals to assess alignment.", "result": "Existing standards cover both technical and social aspects of security/privacy, but professionals emphasize threat modeling and security mindset more than formal standards currently reflect.", "conclusion": "While current teaching standards provide adequate topic coverage, curricula should prioritize threat modeling and security mindset development to better align with professional expectations."}}
{"id": "2507.09039", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09039", "abs": "https://arxiv.org/abs/2507.09039", "authors": ["Aakash Sorathiya", "Gouri Ginde"], "title": "Towards Extracting Software Requirements from App Reviews using Seq2seq Framework", "comment": null, "summary": "Mobile app reviews are a large-scale data source for software improvements. A\nkey task in this context is effectively extracting requirements from app\nreviews to analyze the users' needs and support the software's evolution.\nRecent studies show that existing methods fail at this task since app reviews\nusually contain informal language, grammatical and spelling errors, and a large\namount of irrelevant information that might not have direct practical value for\ndevelopers. To address this, we propose a novel reformulation of requirements\nextraction as a Named Entity Recognition (NER) task based on the\nsequence-to-sequence (Seq2seq) generation approach. With this aim, we propose a\nSeq2seq framework, incorporating a BiLSTM encoder and an LSTM decoder, enhanced\nwith a self-attention mechanism, GloVe embeddings, and a CRF model. We\nevaluated our framework on two datasets: a manually annotated set of 1,000\nreviews (Dataset 1) and a crowdsourced set of 23,816 reviews (Dataset 2). The\nquantitative evaluation of our framework showed that it outperformed existing\nstate-of-the-art methods with an F1 score of 0.96 on Dataset 2, and achieved\ncomparable performance on Dataset 1 with an F1 score of 0.47.", "AI": {"tldr": "This paper redefines requirements extraction from mobile app reviews as a Named Entity Recognition task using a Seq2seq model with BiLSTM, LSTM, self-attention, GloVe embeddings, and CRF, achieving high F1 scores on two datasets.", "motivation": "Existing methods struggle to extract actionable requirements from app reviews due to informal language, grammatical/spelling errors, and irrelevant content.", "method": "A Seq2seq framework combining BiLSTM encoder, LSTM decoder with self-attention, pre-trained GloVe embeddings, and CRF for sequence modeling.", "result": "Achieved F1 score of 0.96 on crowdsourced 23,816-review dataset (Dataset 2) and 0.47 on manually annotated 1,000-review dataset (Dataset 1).", "conclusion": "Proposed NER-based Seq2seq approach outperforms state-of-the-art methods in requirements extraction from app reviews."}}
{"id": "2507.09022", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.09022", "abs": "https://arxiv.org/abs/2507.09022", "authors": ["Moe Kayali", "Jonas Schmitt", "Franziska Roesner"], "title": "SSH-Passkeys: Leveraging Web Authentication for Passwordless SSH", "comment": null, "summary": "We propose a method for using Web Authentication APIs for SSH authentication,\nenabling passwordless remote server login with passkeys. These are credentials\nthat are managed throughout the key lifecycle by an authenticator on behalf of\nthe user and offer strong security guarantees.\n  Passwords remain the dominant mode of SSH authentication, despite their well\nknown flaws such as phishing and reuse. SSH's custom key-based authentication\nprotocol can alleviate these issues but remains vulnerable to key theft.\nAdditionally, it has poor usability, with even knowledgeable users leaking key\nmaterial and failing to verify fingerprints. Hence, effective key management\nremains a critical open area in SSH security. In contrast, WebAuthn is a modern\nauthentication standard designed to replace passwords, managing keys on behalf\nof the user. As a web API, this standard cannot integrate with SSH directly.\n  We propose a framework to integrate WebAuthn with SSH servers, by using UNIX\npluggable authentication modules (PAM). Our approach is backwards-compatible,\nsupports stock SSH servers and requires no new software client-side. It offers\nprotection for cryptographic material at rest, resistance to key leaks,\nphishing protection, privacy protection and attestation capability. None of\nthese properties are offered by passwords nor traditional SSH keys. We validate\nthese advantages with a structured, conceptual security analysis.\n  We develop a prototype implementation and conduct a user study to quantify\nthe security advantages of our proposal, testing our prototype with 40 SSH\nusers. The study confirms the security problems of SSH-keys, including 20% of\nthe cohort leaking their private keys. Our SSH-passkeys effectively address\nthese problems: we find a 90% reduction in critical security errors, while\nreducing authentication time by 4x on average.", "AI": {"tldr": "This paper proposes using WebAuthn APIs with SSH authentication via UNIX PAM to enable secure, passwordless remote login using user-managed passkeys, addressing vulnerabilities in traditional SSH keys and passwords.", "motivation": "SSH key-based authentication is vulnerable to key theft and suffers from poor usability leading to key material leaks and unverified fingerprints. Passwords are insecure due to phishing and reuse, highlighting the need for better key management in SSH security.", "method": "A framework to integrate WebAuthn with SSH servers using UNIX PAM modules, enabling passwordless authentication with passkeys while maintaining backward compatibility and stock SSH server support.", "result": "A prototype and user study with 40 SSH users showed a 90% reduction in critical security errors, 4x faster authentication, and identified that 20% of traditional SSH users leaked private keys.", "conclusion": "SSH-passkeys offer superior security (protection against phishing, key leaks, at rest attacks) and usability over passwords and traditional SSH keys, validated by security analysis and user study results."}}
{"id": "2507.09049", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09049", "abs": "https://arxiv.org/abs/2507.09049", "authors": ["Aakash Sorathiya", "Gouri Ginde"], "title": "CMER: A Context-Aware Approach for Mining Ethical Concern-related App Reviews", "comment": null, "summary": "With the increasing proliferation of mobile applications in our daily lives,\nthe concerns surrounding ethics have surged significantly. Users communicate\ntheir feedback in app reviews, frequently emphasizing ethical concerns, such as\nprivacy and security. Incorporating these reviews has proved to be useful for\nmany areas of software engineering (e.g., requirement engineering, testing,\netc.). However, app reviews related to ethical concerns generally use\ndomain-specific language and are typically overshadowed by more generic\ncategories of user feedback, such as app reliability and usability. Thus,\nmaking automated extraction a challenging and time-consuming effort.\n  This study proposes CMER (A \\underline{C}ontext-Aware Approach for\n\\underline{M}ining \\underline{E}thical Concern-related App\n\\underline{R}eviews), a novel approach that combines Natural Language Inference\n(NLI) and a decoder-only (LLaMA-like) Large Language Model (LLM) to extract\nethical concern-related app reviews at scale. In CMER, NLI provides\ndomain-specific context awareness by using domain-specific hypotheses, and the\nLlama-like LLM eliminates the need for labeled data in the classification task.\nWe evaluated the validity of CMER by mining privacy and security-related\nreviews (PSRs) from the dataset of more than 382K app reviews of mobile\ninvestment apps. First, we evaluated four NLI models and compared the results\nof domain-specific hypotheses with generic hypotheses. Next, we evaluated three\nLLMs for the classification task. Finally, we combined the best NLI and LLM\nmodels (CMER) and extracted 2,178 additional PSRs overlooked by the previous\nstudy using a keyword-based approach, thus demonstrating the effectiveness of\nCMER. These reviews can be further refined into actionable requirement\nartifacts.", "AI": {"tldr": "CMER is a context-aware approach using NLI and a decoder-only LLM to efficiently extract ethical concern-related app reviews, outperforming keyword-based methods in detecting privacy and security issues.", "motivation": "Existing methods struggle with domain-specific language in ethical concern app reviews and require labeled data, making automated extraction challenging and time-consuming.", "method": "CMER combines Natural Language Inference (NLI) with a decoder-only LLM (LLaMA-like) to enhance context awareness and eliminate the need for labeled data in identifying ethical concern-related reviews.", "result": "CMER extracted 2,178 additional privacy and security-related reviews from a dataset of 382K mobile investment app reviews, surpassing keyword-based approaches.", "conclusion": "CMER effectively addresses the limitations of traditional methods by leveraging domain-aware NLI and LLMs, enabling scalable extraction of ethical concern information for software engineering practices."}}
{"id": "2507.09074", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.09074", "abs": "https://arxiv.org/abs/2507.09074", "authors": ["David Noever", "Forrest McKee"], "title": "Favicon Trojans: Executable Steganography Via Ico Alpha Channel Exploitation", "comment": null, "summary": "This paper presents a novel method of executable steganography using the\nalpha transparency layer of ICO image files to embed and deliver\nself-decompressing JavaScript payloads within web browsers. By targeting the\nleast significant bit (LSB) of non-transparent alpha layer image values, the\nproposed method successfully conceals compressed JavaScript code inside a\nfavicon image without affecting visual fidelity. Global web traffic loads 294\nbillion favicons daily and consume 0.9 petabytes of network bandwidth. A\nproof-of-concept implementation demonstrates that a 64x64 ICO image can embed\nup to 512 bytes uncompressed, or 0.8 kilobyte when using lightweight two-fold\ncompression. On page load, a browser fetches the favicon as part of standard\nbehavior, allowing an embedded loader script to extract and execute the payload\nentirely in memory using native JavaScript APIs and canvas pixel access. This\ncreates a two-stage covert channel requiring no additional network or user\nrequests. Testing across multiple browsers in both desktop and mobile\nenvironments confirms successful and silent execution of the embedded script.\nWe evaluate the threat model, relate it to polymorphic phishing attacks that\nevade favicon-based detection, and analyze evasion of content security policies\nand antivirus scanners. We map nine example MITRE ATT&CK Framework objectives\nto single line JavaScript to execute arbitrarily in ICO files. Existing\nsteganalysis and sanitization defenses are discussed, highlighting limitations\nin detecting or neutralizing alpha-channel exploits. The results demonstrate a\nstealthy and reusable attack surface that blurs traditional boundaries between\nstatic images and executable content. Because modern browsers report silent\nerrors when developers specifically fail to load ICO files, this attack surface\noffers an interesting example of required web behaviors that in turn compromise\nsecurity.", "AI": {"tldr": "The paper proposes a self-decompressing JavaScript steganography method using ICO file alpha layers to create a stealthy browser-based attack without affecting visual fidelity.", "motivation": "Modern browsers process 294B daily favicons consuming 0.9PB bandwidth, making ICO steganography a novel attack surface to bypass traditional image/security boundaries through standard browser behavior.", "method": "Embed JavaScript in LSB of non-transparent alpha pixels (64x64 ICO holds 512B/0.8KB compressed) and execute it silently via canvas APIs during page load, creating a two-stage in-memory covert channel.", "result": "Proof-of-concept successfully exploited browsers across platforms, evaded antivirus/CSP detection, mapped to 9 MITRE ATT&CK objectives, and demonstrated lightweight two-fold compression feasibility.", "conclusion": "Identifies ICO transparency as a stealthy executable attack vector that exploits required browser behaviors, necessitating enhanced steganalysis and defense strategies against polymorphic phishing."}}
{"id": "2507.09051", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09051", "abs": "https://arxiv.org/abs/2507.09051", "authors": ["Aakash Sorathiya", "Gouri Ginde"], "title": "SAGE: A Context-Aware Approach for Mining Privacy Requirements Relevant Reviews from Mental Health Apps", "comment": null, "summary": "Mental health (MH) apps often require sensitive user data to customize\nservices for mental wellness needs. However, such data collection practices in\nsome MH apps raise significant privacy concerns for users. These concerns are\noften mentioned in app reviews, but other feedback categories, such as\nreliability and usability, tend to take precedence. This poses a significant\nchallenge in automatically identifying privacy requirements-relevant reviews\n(privacy reviews) that can be utilized to extract privacy requirements and\naddress users' privacy concerns. Thus, this study introduces SAGE, a\ncontext-aware approach to automatically mining privacy reviews from MH apps\nusing Natural Language Inference (NLI) with MH domain-specific privacy\nhypotheses (provides domain-specific context awareness) and a GPT model\n(eliminates the need for fine-tuning). The quantitative evaluation of SAGE on a\ndataset of 204K app reviews achieved an F1 score of 0.85 without any\nfine-tuning, outperforming the fine-tuned baseline classifiers BERT and T5.\nFurthermore, SAGE extracted 748 privacy reviews previously overlooked by\nkeyword-based methods, demonstrating its effectiveness through qualitative\nevaluation. These reviews can later be refined into actionable privacy\nrequirement artifacts.", "AI": {"tldr": "SAGE introduces a context-aware approach using NLI and GPT to identify overlooked privacy reviews in mental health apps, achieving an 0.85 F1 score without fine-tuning and outperforming BERT/T5 alternatives.", "motivation": "Mental health apps collect sensitive data but user reviews prioritize reliability/usability over privacy concerns, creating a gap in systematically extracting contextual privacy requirements from unstructured feedback.", "method": "The method combines Natural Language Inference (NLI) with MH domain-specific privacy hypotheses to analyze review context, and utilizes a GPT model for zero-shot classification of privacy-relevant reviews without requiring model fine-tuning.", "result": "SAGE achieved 0.85 F1 score on 204K reviews, outperforming fine-tuned BERT/T5 baselines, and identified 748 previously undetected privacy reviews through qualitative analysis that could be transformed into actionable privacy requirement artifacts.", "conclusion": "This work demonstrates that domain-aware NLI with GPT offers an effective solution for mining nuanced privacy concerns in MH app reviews, enabling more comprehensive compliance with privacy regulations and addressing user trust issues through unstructured feedback analysis."}}
{"id": "2507.09133", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.09133", "abs": "https://arxiv.org/abs/2507.09133", "authors": ["Jingwen Li", "Ru Zhang", "Jianyi Liu", "Wanguo Zhao"], "title": "CLIProv: A Contrastive Log-to-Intelligence Multimodal Approach for Threat Detection and Provenance Analysis", "comment": null, "summary": "With the increasing complexity of cyberattacks, the proactive and\nforward-looking nature of threat intelligence has become more crucial for\nthreat detection and provenance analysis. However, translating high-level\nattack patterns described in Tactics, Techniques, and Procedures (TTP)\nintelligence into actionable security policies remains a significant challenge.\nThis challenge arises from the semantic gap between high-level threat\nintelligence and low-level provenance log. To address this issue, this paper\nintroduces CLIProv, a novel approach for detecting threat behaviors in a host\nsystem. CLIProv employs a multimodal framework that leverages contrastive\nlearning to align the semantics of provenance logs with threat intelligence,\neffectively correlating system intrusion activities with attack patterns.\nFurthermore, CLIProv formulates threat detection as a semantic search problem,\nidentifying attack behaviors by searching for threat intelligence that is most\nsemantically similar to the log sequence. By leveraging attack pattern\ninformation in threat intelligence, CLIProv identifies TTPs and generates\ncomplete and concise attack scenarios. Experimental evaluations on standard\ndatasets show that CLIProv effectively identifies attack behaviors in system\nprovenance logs, offering valuable references for potential techniques.\nCompared to state-of-the-art methods, CLIProv achieves higher precision and\nsignificantly improved detection efficiency.", "AI": {"tldr": "CLIProv is a novel threat detection approach that bridges the semantic gap between high-level threat intelligence (TTPs) and low-level provenance logs using contrastive learning and semantic search, improving precision and detection efficiency.", "motivation": "The increasing complexity of cyberattacks demands proactive threat detection, yet existing methods struggle to translate high-level Tactics, Techniques, Procedures (TTP) threat intelligence into actionable system-level security analysis due to a significant semantic gap.", "method": "CLIProv employs a multimodal framework leveraging contrastive learning to semantically align provenance logs with threat intelligence, reformulating threat detection as a semantic search problem where attack behaviors are identified by matching log sequences to the most semantically similar threat intelligence.", "result": "Experimental evaluations on standard datasets demonstrate CLIProv's effectiveness in detecting attack behaviors in provenance logs. It achieves higher precision and significantly improved detection efficiency compared to state-of-the-art methods while generating complete and concise attack scenarios.", "conclusion": "CLIProv addresses the critical challenge of mapping abstract threat intelligence to concrete system activity patterns, offering a high-precision, efficient solution for identifying cyberattack behaviors and constructing TTP-aligned attack scenarios."}}
{"id": "2507.09063", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09063", "abs": "https://arxiv.org/abs/2507.09063", "authors": ["Avi Arora", "Jinu Jang", "Roshanak Zilouchian Moghaddam"], "title": "SetupBench: Assessing Software Engineering Agents' Ability to Bootstrap Development Environments", "comment": null, "summary": "Modern Large Language Model (LLM) agents promise end to end assistance with\nreal-world software tasks, yet existing benchmarks evaluate LLM agents almost\nexclusively in pre-baked environments where every dependency is pre-installed.\nTo fill this gap, we introduce SetupBench, a 93 instance benchmark that\nisolates the environment-bootstrap skill: starting from a bare Linux sandbox,\nan agent must install packages, resolve dependency conflicts, initialize\ndatabases, and configure background services. Our tasks span seven language\necosystems, five database engines, and multi-service orchestration scenarios,\neach accompanies by a natural language problem statement and a deterministic\nsuccess command. Through evaluation of OpenHands, a state-of-the-art coding\nagent, we find low success rates across task categories, with particular\nchallenges in repository setup (38.9-57.4%) and local database configuration\n(20.0-53.3%). Our analysis reveals systematic failure modes including\nincomplete development tooling installation, hallucinated task constraints, and\nnon-persistent environment modifications that break agent-human collaboration\nworkflows. We identify substantial inefficiencies in agent exploration\nstrategies, with 38-89% of actions being unnecessary compared to optimal human\nbehavior. These findings highlight gaps in current agents' practical\nenvironment-bootstrap capabilities. By targeting this critical yet\nunder-evaluated capability, SetupBench provides a rigorous yard-stick for the\nnext generation of software developer agents aiming to solve end to end\nreal-wold tasks.", "AI": {"tldr": "Introduces SetupBench, a benchmark testing environment-bootstrap capabilities of LLM agents on bare Linux sandboxes with 93 tasks across ecosystems, databases, and services.", "motivation": "Existing LLM benchmarks focus on pre-configured environments, lacking evaluation of setup skills critical for real-world software tasks. Current agents lack robustness in dependency management and environment configuration.", "method": "Created SetupBench with 93 instances requiring package installation, dependency resolving, database initialization, and service configuration. Tasks include natural language problem statements and deterministic success commands across 7 ecosystems, 5 databases, and multi-service scenarios.", "result": "OpenHands agents showed success rates of 38.9-57.4% (repository setup) and 20.0-53.3% (databases). Systematic failures included incomplete tooling installation, hallucinated constraints, non-persistent changes. Agents executed 38-89% unnecessary actions compared to human optimal workflows.", "conclusion": "SetupBench provides a rigorous benchmark for developer agent capabilities in environment setup, exposing practical gaps in dependency resolution, configuration persistence, and exploration efficiency that require future research."}}
{"id": "2507.09231", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.09231", "abs": "https://arxiv.org/abs/2507.09231", "authors": ["Artem Chystiakov", "Mariia Zhvanko"], "title": "Confidential Wrapped Ethereum", "comment": null, "summary": "Transparency is one of the key benefits of public blockchains. However, the\npublic visibility of transactions potentially compromises users' privacy. The\nfundamental challenge is to balance the intrinsic benefits of blockchain\nopenness with the vital need for individual confidentiality. The proposal\nsuggests creating a confidential version of wrapped Ethereum (cWETH) fully\nwithin the application layer. The solution combines the Elliptic Curve (EC)\nTwisted ElGamal-based commitment scheme to preserve confidentiality and the EC\nDiffie-Hellman (DH) protocol to introduce accessibility limited by the\ncommitment scheme. To enforce the correct generation of commitments,\nencryption, and decryption, zk-SNARKs are utilized.", "AI": {"tldr": "The paper proposes a method to enhance user privacy in public blockchains by creating a confidential wrapped Ethereum (cWETH) solution entirely within the application layer using elliptic curve cryptography and zk-SNARKs.", "motivation": "Public blockchain transparency exposes transaction details, compromising user privacy. This work addresses the need to balance openness with confidentiality while maintaining accessibility within defined constraints.", "method": "The solution integrates EC ElGamal commitments for data encryption and EC DH for controlled access. zk-SNARKs are employed as zero-knowledge proofs to verify correct cryptographic operations (commitments, encryption/decryption) without revealing sensitive data.", "result": "A proof-of-concept implementation of the cWETH protocol framework is established, demonstrating privacy-preserving transaction handling in public blockchains at the application layer.", "conclusion": "The paper presents a viable application-layer approach to confidential transactions using cryptographic techniques, offering a potential solution to reconcile blockchain transparency with the confidentiality requirements of privacy-conscious users."}}
{"id": "2507.09108", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09108", "abs": "https://arxiv.org/abs/2507.09108", "authors": ["Aaditya Bhatia", "Gustavo A. Oliva", "Gopi Krishnan Rajbahadur", "Haoxiang Zhang", "Yihao Chen", "Zhilong Chen", "Arthur Leung", "Dayi Lin", "Boyuan Chen", "Ahmed E. Hassan"], "title": "SPICE: An Automated SWE-Bench Labeling Pipeline for Issue Clarity, Test Coverage, and Effort Estimation", "comment": null, "summary": "High-quality labeled datasets are crucial for training and evaluating\nfoundation models in software engineering, but creating them is often\nprohibitively expensive and labor-intensive. We introduce SPICE, a scalable,\nautomated pipeline for labeling SWE-bench-style datasets with annotations for\nissue clarity, test coverage, and effort estimation. SPICE combines\ncontext-aware code navigation, rationale-driven prompting, and multi-pass\nconsensus to produce labels that closely approximate expert annotations.\nSPICE's design was informed by our own experience and frustration in labeling\nmore than 800 instances from SWE-Gym. SPICE achieves strong agreement with\nhuman-labeled SWE-bench Verified data while reducing the cost of labeling 1,000\ninstances from around $100,000 (manual annotation) to just $5.10. These results\ndemonstrate SPICE's potential to enable cost-effective, large-scale dataset\ncreation for SE-focused FMs. To support the community, we release both SPICE\ntool and SPICE Bench, a new dataset of 6,802 SPICE-labeled instances curated\nfrom 291 open-source projects in SWE-Gym (over 13x larger than SWE-bench\nVerified).", "AI": {"tldr": "This paper introduces SPICE, a cost-effective automated pipeline for labeling large-scale software engineering datasets, reducing annotation costs from $100k to $5.10 per 1,000 instances while maintaining high accuracy compared to expert annotations.", "motivation": "Manual labeling of high-quality datasets for training SE-focused foundation models is prohibitively expensive and time-consuming, creating a barrier for large-scale data collection needed in software engineering (SE) research.", "method": "SPICE employs three key components: context-aware code navigation for accurate instance selection, rationale-driven prompting to guide model reasoning during annotation, and multi-pass consensus mechanisms to ensure label quality. The pipeline was developed based on authors' hands-on experience labeling 800+ SWE-Gym instances.", "result": "1. Achieves strong agreement with SWE-bench Verified human annotations\n2. Reduces labeling cost for 1,000 instances from ~$100,000 to $5.10\n3. Creates SPICE Bench: a 6,802-instance dataset from 291 open-source projects (13x larger than SWE-bench Verified)\n4. Identifies patterns in software engineering challenges through automated analysis", "conclusion": "SPICE demonstrates that automated labeling pipelines can create high-quality, large-scale datasets for SE foundation models at vastly reduced costs, enabling community-wide experimentation and addressing current bottlenecks in SE research data collection."}}
{"id": "2507.09288", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.09288", "abs": "https://arxiv.org/abs/2507.09288", "authors": ["Javier Blanco-Romero", "Pedro Otero Garc\u00eda", "Daniel Sobral-Blanco", "Florina Almenares Mendoza", "Ana Fern\u00e1ndez Vilas", "Manuel Fern\u00e1ndez-Veiga"], "title": "Hybrid Quantum Security for IPsec", "comment": "23 pages, 6 figures, quantum key distribution, post-quantum\n  cryptography, IPsec security protocols", "summary": "Quantum Key Distribution (QKD) offers information-theoretic security against\nquantum computing threats, but integrating QKD into existing security protocols\nremains an unsolved challenge due to fundamental mismatches between\npre-distributed quantum keys and computational key exchange paradigms. This\npaper presents the first systematic comparison of sequential versus parallel\nhybrid QKD-PQC key establishment strategies for IPsec, revealing fundamental\nprotocol design principles that extend beyond specific implementations. We\nintroduce two novel approaches for incorporating QKD into Internet Key Exchange\nversion 2 (IKEv2) with support for both ETSI GS QKD 004 stateful and ETSI GS\nQKD 014 stateless API specifications: (1) a pure QKD approach that replaces\ncomputational key derivation with identifier-based quantum key coordination,\nand (2) a unified QKD-KEM abstraction that enables parallel composition of\nquantum and post-quantum cryptographic methods within existing protocol\nframeworks. Our key insight is that parallel hybrid approaches eliminate the\nmultiplicative latency penalties inherent in sequential methods mandated by RFC\n9370, achieving significant performance improvements under realistic network\nconditions. Performance evaluation using a Docker-based testing framework with\nIDQuantique QKD hardware demonstrates that the parallel hybrid approach\nsignificantly outperforms sequential methods under network latency conditions,\nwhile pure QKD achieves minimal bandwidth overhead through identifier-based key\ncoordination. Our implementations provide practical quantum-enhanced IPsec\nsolutions suitable for critical infrastructure deployments requiring\ndefense-in-depth security.", "AI": {"tldr": "This paper introduces two novel QKD integration approaches into IKEv2 for IPsec, comparing sequential vs. parallel hybrid QKD-PQC key establishment strategies. The parallel hybrid method reduces network latency penalties and demonstrates superior performance over sequential approaches, validated using IDQuantique hardware and Docker testing.", "motivation": "QKD provides information-theoretic security against quantum threats, but its integration with computational key exchange in protocols like IPsec remains challenging due to fundamental mismatches between pre-distributed quantum keys and dynamic computational paradigms.", "method": "The authors propose (1) a pure QKD approach using identifier-based key coordination to replace computational key derivation, and (2) a unified QKD-KEM abstraction enabling parallel composition of quantum and post-quantum methods. They systematically evaluate these designs against ETSI GS QKD specifications and RFC 9370 sequential requirements.", "result": "Parallel hybrid approaches achieve significant performance improvements by eliminating multiplicative latency penalties. Docker-based testing with IDQuantique hardware shows pure QKD minimizes bandwidth overhead, while the unified abstraction framework maintains compatibility with existing protocol architectures.", "conclusion": "The presented implementations provide practical quantum-enhanced IPsec solutions that balance security and performance. Parallel composition emerges as a superior strategy for integrating QKD into traditional protocols, offering critical infrastructure defense-in-depth solutions."}}
{"id": "2507.09135", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09135", "abs": "https://arxiv.org/abs/2507.09135", "authors": ["Yalong Du", "Chaozheng Wang", "Huaijin Wang"], "title": "Position Paper: Programming Language Techniques for Bridging LLM Code Generation Semantic Gaps", "comment": null, "summary": "Large Language Models have demonstrated remarkable capabilities in automated\ncode generation, yet their statistical nature and black-box characteristics\ncreate significant semantic gaps manifested through syntax errors, semantic\nhallucinations, and reliability concerns. This position paper argues that\nprincipled integration of Programming Language (PL) techniques is essential for\nbridging these gaps. Through structured program representations, formal\ncorrectness guarantees, and robust verification mechanisms, PL techniques can\nelevate LLM-generated code from statistical pattern matching to truly reliable\nand trustworthy levels. This integration is crucial for developing systems that\ngenerate code that is not only functionally correct but also interpretable,\nverifiable, and ultimately trustworthy.", "AI": {"tldr": "This paper advocates for integrating Programming Language (PL) techniques with Large Language Models (LLMs) to bridge semantic gaps in automated code generation, enhancing reliability and trustworthiness through formal methods and verification.", "motivation": "LLMs' statistical and black-box nature leads to syntax errors, semantic hallucinations, and reliability issues in generated code, which limits their practical use in critical systems where correctness and verifiability are essential.", "method": "The approach involves leveraging structured program representations, formal correctness guarantees (e.g., type systems, semantics), and robust verification mechanisms (e.g., model checking, proof assistants) to augment LLM-based code generation processes.", "result": "The integration of PL techniques ensures code is functionally correct, interpretable, and verifiable, addressing LLMs' limitations in producing reliable software.", "conclusion": "To create truly trustworthy code generation systems, principled fusion of LLMs and PL techniques is indispensable, enabling both practical productivity gains and rigorous correctness assurances."}}
{"id": "2507.09301", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.09301", "abs": "https://arxiv.org/abs/2507.09301", "authors": ["Julio Gento Suela", "Javier Blanco-Romero", "Florina Almenares Mendoza", "Daniel D\u00edaz-S\u00e1nchez"], "title": "Implementing and Evaluating Post-Quantum DNSSEC in CoreDNS", "comment": null, "summary": "The emergence of quantum computers poses a significant threat to current\nsecure service, application and/or protocol implementations that rely on RSA\nand ECDSA algorithms, for instance DNSSEC, because public-key cryptography\nbased on number factorization or discrete logarithm is vulnerable to quantum\nattacks. This paper presents the integration of post-quantum cryptographic\n(PQC) algorithms into CoreDNS to enable quantum-resistant DNSSEC functionality.\nWe have developed a plugin that extends CoreDNS with support for five PQC\nsignature algorithm families: ML-DSA, FALCON, SPHINCS+, MAYO, and SNOVA. Our\nimplementation maintains compatibility with existing DNS resolution flows while\nproviding on-the-fly signing using quantum-resistant signatures. A benchmark\nhas been performed and performance evaluation results reveal significant\ntrade-offs between security and efficiency. The results indicate that while PQC\nalgorithms introduce operational overhead, several candidates offer viable\ncompromises for transitioning DNSSEC to quantum-resistant cryptography.", "AI": {"tldr": "This paper integrates post-quantum cryptographic (PQC) algorithms into CoreDNS to create quantum-resistant DNSSEC functionality, testing five PQC signature families (ML-DSA, FALCON, SPHINCS+, MAYO, SNOVA) with compatibility and performance evaluations.", "motivation": "Quantum computers threaten classical RSA/ECDSA-based cryptography (e.g., DNSSEC), necessitating quantum-resistant alternatives to avoid security vulnerabilities.", "method": "Developed a CoreDNS plugin supporting five PQC signature algorithms, enabling on-the-fly signing while preserving existing DNS resolution workflows.", "result": "Benchmark results highlight security vs. efficiency trade-offs; some PQC candidates demonstrate acceptable performance for practical DNSSEC deployment.", "conclusion": "PQC integration into CoreDNS is feasible, providing transitional options for quantum-resistant DNSSEC despite operational overhead."}}
{"id": "2507.09186", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09186", "abs": "https://arxiv.org/abs/2507.09186", "authors": ["Minhaj Uddin Ahmad", "Akid Abrar", "Sagar Dasgupta", "Mizanur Rahman"], "title": "OpenCAMS: An Open-Source Connected and Automated Mobility Co-Simulation Platform for Advanced Transportation Research", "comment": null, "summary": "We introduce OpenCAMS (Open-Source Connected and Automated Mobility\nCo-Simulation Platform), an open-source, synchronized, and extensible\nco-simulation framework that tightly couples three best-in-class simulation\ntools: (i) SUMO, (ii) CARLA, and (iii) OMNeT++. OpenCAMS is designed to support\nadvanced research in transportation safety, mobility, and cybersecurity by\ncombining the strengths of each simulation domain. Specifically, SUMO provides\nlarge-scale, microscopic traffic modeling; CARLA offers high-fidelity 3D\nperception, vehicle dynamics, and control simulation; and OMNeT++ enables\nmodular, event-driven network communication, such as cellular\nvehicle-to-everything (C-V2X). OpenCAMS employs a time-synchronized,\nbidirectional coupling architecture that ensures coherent simulation\nprogression across traffic, perception, and communication domains while\npreserving modularity and reproducibility. For example, CARLA can simulate and\nrender a subset of vehicles that require detailed sensor emulation and control\nlogic; SUMO orchestrates network-wide traffic flow, vehicle routing, and\ntraffic signal management; and OMNeT++ dynamically maps communication nodes to\nboth mobile entities (e.g., vehicles) and static entities (e.g., roadside\nunits) to enable C-V2X communication. While these three simulators form the\nfoundational core of OpenCAMS, the platform is designed to be expandable and\nfuture-proof, allowing additional simulators to be integrated on top of this\ncore without requiring fundamental changes to the system architecture. The\nOpenCAMS platform is fully open-source and publicly available through its\nGitHub repository https://github.com/minhaj6/carla-sumo-omnetpp-cosim,\nproviding the research community with an accessible, flexible, and\ncollaborative environment for advancing next-generation intelligent\ntransportation systems.", "AI": {"tldr": "Introduces OpenCAMS, an open-source co-simulation platform integrating SUMO, CARLA, and OMNeT++ for transportation safety, mobility, and cybersecurity research with time-synchronized bidirectional coupling.", "motivation": "Existing simulation tools for intelligent transportation systems lack comprehensive integration across traffic modeling, vehicle perception/control, and communication networks, limiting research in safety-critical and networked scenarios.", "method": "Time-synchronized bidirectional coupling architecture that combines: 1) SUMO for large-scale traffic simulation, 2) CARLA for detailed vehicle dynamics and sensor emulation, and 3) OMNeT++ for event-driven network communication (C-V2X). Modular design enables extensible integration of additional simulators.", "result": "Successful implementation of OpenCAMS platform (GitHub: https://github.com/minhaj6/carla-sumo-omnetpp-cosim) demonstrating synchronized operation of all three domains with flexibility for third-party integrations. Provides reproducible research environment for ITS development.", "conclusion": "OpenCAMS offers a unified, extensible framework addressing domain-specific needs for connected and automated mobility research while maintaining open-source accessibility, accelerating advancements in intelligent transportation systems."}}
{"id": "2507.09354", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.09354", "abs": "https://arxiv.org/abs/2507.09354", "authors": ["Yifan Zhang", "Yu Bai", "Riku Jantti", "Zheng Yan", "Christos Masouros", "Zhu Han"], "title": "Backscatter Device-aided Integrated Sensing and Communication: A Pareto Optimization Framework", "comment": null, "summary": "Integrated sensing and communication (ISAC) systems potentially encounter\nsignificant performance degradation in densely obstructed urban and\nnon-line-of-sight scenarios, thus limiting their effectiveness in practical\ndeployments. To deal with these challenges, this paper proposes a backscatter\ndevice (BD)-assisted ISAC system, which leverages passive BDs naturally\ndistributed in underlying environments for performance enhancement. These\nambient devices can enhance sensing accuracy and communication reliability by\nproviding additional reflective signal paths. In this system, we define the\nPareto boundary characterizing the trade-off between sensing mutual information\n(SMI) and communication rates to provide fundamental insights for its design.\nTo derive the boundary, we formulate a performance optimization problem within\nan orthogonal frequency division multiplexing (OFDM) framework, by jointly\noptimizing time-frequency resource element (RE) allocation, transmit power\nmanagement, and BD modulation decisions. To tackle the non-convexity of the\nproblem, we decompose it into three subproblems, solved iteratively through a\nblock coordinate descent (BCD) algorithm. Specifically, the RE subproblem is\naddressed using the successive convex approximation (SCA) method, the power\nsubproblem is solved using an augmented Lagrangian combined water-filling\nmethod, and the BD modulation subproblem is tackled using semidefinite\nrelaxation (SDR) methods. Additionally, we demonstrate the generality of the\nproposed system by showing its adaptability to bistatic ISAC scenarios and MIMO\nsettings. Finally, extensive simulation results validate the effectiveness of\nthe proposed system and its superior performance compared to existing\nstate-of-the-art ISAC schemes.", "AI": {"tldr": "This paper proposes a backscatter device (BD)-assisted ISAC system to enhance performance in obstructed urban environments by optimizing time-frequency resource allocation, transmit power, and BD modulation via a block coordinate descent algorithm, validated through simulations showing superiority over existing ISAC schemes.", "motivation": "Integrated sensing and communication (ISAC) systems suffer performance degradation in densely obstructed and non-line-of-sight urban scenarios, necessitating methods to improve reliability through ambient reflectors.", "method": "A BD-assisted ISAC system is introduced, leveraging ambient passive BDs for signal paths. The Pareto boundary between sensing mutual information (SMI) and communication rates is characterized via joint optimization of RE allocation, transmit power management, and BD modulation using a BCD algorithm (SCA, augmented Lagrangian water-filling, SDR decomposition).", "result": "Simulation results demonstrate enhanced sensing accuracy and communication reliability, adaptability to bistatic and MIMO configurations, and outperformance of state-of-the-art ISAC methods.", "conclusion": "The proposed system provides a practical and flexible framework for ISAC in challenging environments by utilizing distributed BDs, offering design insights through the Pareto boundary and subproblem-optimized solutions."}}
{"id": "2507.09199", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09199", "abs": "https://arxiv.org/abs/2507.09199", "authors": ["Huihui Huang", "Ratnadira Widyasari", "Ting Zhang", "Ivana Clairine Irsan", "Jieke Shi", "Han Wei Ang", "Frank Liauw", "Eng Lieh Ouh", "Lwin Khin Shar", "Hong Jin Kang", "David Lo"], "title": "Back to the Basics: Rethinking Issue-Commit Linking with LLM-Assisted Retrieval", "comment": null, "summary": "Issue-commit linking, which connects issues with commits that fix them, is\ncrucial for software maintenance. Existing approaches have shown promise in\nautomatically recovering these links. Evaluations of these techniques assess\ntheir ability to identify genuine links from plausible but false links.\nHowever, these evaluations overlook the fact that, in reality, when a\nrepository has more commits, the presence of more plausible yet unrelated\ncommits may interfere with the tool in differentiating the correct fix commits.\nTo address this, we propose the Realistic Distribution Setting (RDS) and use it\nto construct a more realistic evaluation dataset that includes 20 open-source\nprojects. By evaluating tools on this dataset, we observe that the performance\nof the state-of-the-art deep learning-based approach drops by more than half,\nwhile the traditional Information Retrieval method, VSM, outperforms it.\n  Inspired by these observations, we propose EasyLink, which utilizes a vector\ndatabase as a modern Information Retrieval technique. To address the\nlong-standing problem of the semantic gap between issues and commits, EasyLink\nleverages a large language model to rerank the commits retrieved from the\ndatabase. Under our evaluation, EasyLink achieves an average Precision@1 of\n75.91%, improving over the state-of-the-art by over four times. Additionally,\nthis paper provides practical guidelines for advancing research in issue-commit\nlink recovery.", "AI": {"tldr": "This paper proposes the Realistic Distribution Setting (RDS) for issue-commit linking evaluation, reveals the state-of-the-art deep learning approach's performance drops by over 50% under RDS while traditional VSM outperforms. EasyLink, a LLM-reranking based method, achieves 75.91% Precision@1, offering practical research guidelines.", "motivation": "Existing issue-commit linking evaluations ignore the real-world challenge of distinguishing genuine fixes from numerous plausible yet unrelated commits in large projects, leading to misleading performance assessments.", "method": "The authors create RDS with a realistic 20 open-source project dataset and evaluate tools under this setting. They propose EasyLink using a vector database and large language model (LLM) reranking to bridge the semantic gap between issues and commits.", "result": "State-of-the-art deep learning models drop over 50% in accuracy under RDS; VSM outperforms them. EasyLink achieves 75.91% Precision@1, a 4x improvement over the best existing method.", "conclusion": "Realistic evaluations using RDS are essential for issue-commit linking. EasyLink demonstrates superior performance with modern IR techniques and LLM semantic enhancement, providing actionable guidelines for future research directions."}}
{"id": "2507.09411", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.09411", "abs": "https://arxiv.org/abs/2507.09411", "authors": ["Md Ajwad Akil", "Adrian Shuai Li", "Imtiaz Karim", "Arun Iyengar", "Ashish Kundu", "Vinny Parla", "Elisa Bertino"], "title": "LLMalMorph: On The Feasibility of Generating Variant Malware using Large-Language-Models", "comment": "13 pages", "summary": "Large Language Models (LLMs) have transformed software development and\nautomated code generation. Motivated by these advancements, this paper explores\nthe feasibility of LLMs in modifying malware source code to generate variants.\nWe introduce LLMalMorph, a semi-automated framework that leverages semantical\nand syntactical code comprehension by LLMs to generate new malware variants.\nLLMalMorph extracts function-level information from the malware source code and\nemploys custom-engineered prompts coupled with strategically defined code\ntransformations to guide the LLM in generating variants without\nresource-intensive fine-tuning. To evaluate LLMalMorph, we collected 10 diverse\nWindows malware samples of varying types, complexity and functionality and\ngenerated 618 variants. Our thorough experiments demonstrate that it is\npossible to reduce the detection rates of antivirus engines of these malware\nvariants to some extent while preserving malware functionalities. In addition,\ndespite not optimizing against any Machine Learning (ML)-based malware\ndetectors, several variants also achieved notable attack success rates against\nan ML-based malware classifier. We also discuss the limitations of current LLM\ncapabilities in generating malware variants from source code and assess where\nthis emerging technology stands in the broader context of malware variant\ngeneration.", "AI": {"tldr": "The paper investigates using Large Language Models (LLMs) for malware variant generation via code modification, introducing LLMalMorph\u2014a framework leveraging code comprehension to create undetected variants with minimal tuning, reducing antivirus detection rates while preserving functionality and showing effectiveness against ML-based detectors.", "motivation": "Antivirus detection rates for malware have increased, necessitating new methods for bypassing detection. This paper aims to explore LLMs' ability to modify malware code, leveraging their semantical and syntactical understanding to create variants that evade security systems.", "method": "The framework, LLMalMorph, extracts function-level malware code information and uses custom prompts alongside transformation strategies to guide LLMs (e.g., GPT-3, Codex) in generating variants. It applies iterative prompting and controlled obfuscation techniques without requiring model fine-tuning.", "result": "618 malware variants were created from 10 diverse samples. Detection rates by antivirus engines were reduced without functionality loss. Notably, variants achieved success (up to 35% evasion) in attacking an ML-based classifier for malware, even when the classifier wasn't targeted by the framework.", "conclusion": "This work demonstrates LLMs' potential in malware variant generation with practical evasion capabilities. However, limitations in handling binary-level obfuscations or architecture-specific constraints suggest the need for specialized prompts and validation mechanisms for future improvements."}}
{"id": "2507.09220", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09220", "abs": "https://arxiv.org/abs/2507.09220", "authors": ["Syed Tauhid Ullah Shah", "Mohammad Hussein", "Ann Barcomb", "Mohammad Moshirpour"], "title": "Explainability as a Compliance Requirement: What Regulated Industries Need from AI Tools for Design Artifact Generation", "comment": null, "summary": "Artificial Intelligence (AI) tools for automating design artifact generation\nare increasingly used in Requirements Engineering (RE) to transform textual\nrequirements into structured diagrams and models. While these AI tools,\nparticularly those based on Natural Language Processing (NLP), promise to\nimprove efficiency, their adoption remains limited in regulated industries\nwhere transparency and traceability are essential. In this paper, we\ninvestigate the explainability gap in AI-driven design artifact generation\nthrough semi-structured interviews with ten practitioners from safety-critical\nindustries. We examine how current AI-based tools are integrated into workflows\nand the challenges arising from their lack of explainability. We also explore\nmitigation strategies, their impact on project outcomes, and features needed to\nimprove usability. Our findings reveal that non-explainable AI outputs\nnecessitate extensive manual validation, reduce stakeholder trust, struggle to\nhandle domain-specific terminology, disrupt team collaboration, and introduce\nregulatory compliance risks, often negating the anticipated efficiency\nbenefits. To address these issues, we identify key improvements, including\nsource tracing, providing clear justifications for tool-generated decisions,\nsupporting domain-specific adaptation, and enabling compliance validation. This\nstudy outlines a practical roadmap for improving the transparency, reliability,\nand applicability of AI tools in requirements engineering workflows,\nparticularly in regulated and safety-critical environments where explainability\nis crucial for adoption and certification.", "AI": {"tldr": "This paper identifies and addresses the explainability gap in AI tools for design artifact generation in requirements engineering, particularly in regulated/safety-critical industries through practitioner interviews.", "motivation": "AI-based design artifact generation can improve RE efficiency but faces adoption barriers in regulated domains requiring transparency, traceability, and stakeholder trust in safety-critical systems.", "method": "Semi-structured interviews with 10 safety-critical industry practitioners examining AI integration workflows, explainability challenges, mitigation strategies, and usability requirements.", "result": "Non-explainable AI outputs cause manual validation bottlenecks, eroded stakeholder trust, domain terminology limitations, collaboration disruptions, and compliance risks that offset efficiency gains.", "conclusion": "A practical roadmap is proposed to enhance AI tool transparency and reliability in RE through source tracing, decision justification, domain adaptation, compliance validation features, and improved workflow integration."}}
{"id": "2507.09453", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.09453", "abs": "https://arxiv.org/abs/2507.09453", "authors": ["Micha\u0142 J\u00f3\u017awik", "Johan Pouwelse"], "title": "SmartphoneDemocracy: Privacy-Preserving E-Voting on Decentralized Infrastructure using Novel European Identity", "comment": "18 pages, 4 figures", "summary": "The digitization of democratic processes promises greater accessibility but\npresents challenges in terms of security, privacy, and verifiability. Existing\nelectronic voting systems often rely on centralized architectures, creating\nsingle points of failure and forcing too much trust in authorities, which\ncontradicts democratic principles. This research addresses the challenge of\ncreating a secure, private e-voting system with minimized trust dependencies\ndesigned for the most versatile personal device: the smartphone. We introduce\nSmartphoneDemocracy, a novel e-voting protocol that combines three key\ntechnologies: the emerging European Digital Identity (EUDI) Wallet for\nSybil-resistant identity verification, Zero-Knowledge Proofs for\nprivacy-preserving validation, and a peer-to-peer blockchain (TrustChain) for a\nresilient, serverless public bulletin board. Our protocol enables voters to\nregister and cast ballots anonymously and verifiably directly from their\nsmartphones. We provide a detailed protocol design, a security analysis against\na defined threat model, and a performance evaluation demonstrating that the\ncomputational and network overhead is feasible for medium- to large-scale\nelections. By developing and prototyping this system, we demonstrate a viable\npath to empower citizens with a trustworthy, accessible, and user-controlled\ndigital voting experience.", "AI": {"tldr": "SmartphoneDemocracy is a secure, privacy-preserving e-voting protocol using EUDI Wallet, Zero-Knowledge Proofs (ZKPs), and a peer-to-peer blockchain (TrustChain) to minimize trust in authorities and enable smartphone-based voting.", "motivation": "Current electronic voting systems rely on centralized architectures with single points of failure, requiring excessive trust in authorities and conflicting with democratic principles. The challenge is to create a secure, private e-voting system with minimal trust dependencies for smartphone users.", "method": "1. **EUDI Wallet** for Sybil-resistant identity verification; 2. **Zero-Knowledge Proofs** to enable privacy-preserving validation; 3. **TrustChain** (peer-to-peer blockchain) as a resilient, serverless public bulletin board. The protocol integrates these components for anonymous, verifiable smartphone voting.", "result": "A detailed protocol design, security analysis, and performance evaluation confirming feasibility for medium- to large-scale elections with reasonable computational and network overhead. A working prototype demonstrates practical implementation potential.", "conclusion": "SmartphoneDemocracy presents a viable solution to digitize democratic processes with decentralized trust, enhancing accessibility, privacy, and verifiability for future smartphone-based voting systems."}}
{"id": "2507.09315", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09315", "abs": "https://arxiv.org/abs/2507.09315", "authors": ["Yongqian Sun", "Weihua Kuang", "Chao Shen", "Xidao Wen", "Tinghua Zheng", "Heng Liu", "Shenglin Zhang", "Bo Wu", "Dan Pei"], "title": "Enhancing Interpretability in Software Change Management with Chain-of-Thought Reasoning", "comment": "22 pages, 19 figures", "summary": "In modern online services, frequent software changes introduce significant\nrisks. To tackle this challenge, we propose SCELM (Software Change Evaluation\nand Lifecycle Management), an end-to-end automated framework for software\nchange management. SCELM aims to manage software changes efficiently and\nprecisely, significantly reducing service failures and economic losses.", "AI": {"tldr": "SCELM is an end-to-end automated framework for software change management that reduces service failures and economic losses.", "motivation": "Frequent software changes in modern online services introduce significant risks, including service failures and economic losses.", "method": "The proposed SCELM framework leverages automated evaluation and lifecycle management processes to address these risks efficiently and precisely.", "result": "The framework is designed to achieve significant reductions in service failures through automated software change management, though specific implementation results are not quantified in the abstract.", "conclusion": "SCELM offers a promising solution for managing software change risks in online services, emphasizing automation and lifecycle precision to minimize operational disruptions."}}
{"id": "2507.09508", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09508", "abs": "https://arxiv.org/abs/2507.09508", "authors": ["Weichen Yu", "Ravi Mangal", "Terry Zhuo", "Matt Fredrikson", "Corina S. Pasareanu"], "title": "A Mixture of Linear Corrections Generates Secure Code", "comment": null, "summary": "Large language models (LLMs) have become proficient at sophisticated\ncode-generation tasks, yet remain ineffective at reliably detecting or avoiding\ncode vulnerabilities. Does this deficiency stem from insufficient learning\nabout code vulnerabilities, or is it merely a result of ineffective prompting?\nUsing representation engineering techniques, we investigate whether LLMs\ninternally encode the concepts necessary to identify code vulnerabilities. We\nfind that current LLMs encode precise internal representations that distinguish\nvulnerable from secure code--achieving greater accuracy than standard prompting\napproaches. Leveraging these vulnerability-sensitive representations, we\ndevelop an inference-time steering technique that subtly modulates the model's\ntoken-generation probabilities through a mixture of corrections (MoC). Our\nmethod effectively guides LLMs to produce less vulnerable code without\ncompromising functionality, demonstrating a practical approach to controlled\nvulnerability management in generated code. Notably, MoC enhances the security\nratio of Qwen2.5-Coder-7B by 8.9\\%, while simultaneously improving\nfunctionality on HumanEval pass@1 by 2.1\\%.", "AI": {"tldr": "The paper investigates whether LLMs can identify code vulnerabilities through internal representations and proposes an inference-time steering technique (MoC) that improves security without compromising functionality.", "motivation": "LLMs excel at code generation but fail to reliably detect vulnerabilities, raising questions about the root cause (training limitations vs. prompting issues). The study aims to resolve this discrepancy and enhance vulnerability detection.", "method": "Uses representation engineering to analyze LLMs' internal representations of vulnerabilities and develops Mixture of Corrections (MoC) steering to modulate token-generation probabilities during inference.", "result": "MoC steering improved Qwen2.5-Coder-7B's security ratio by 8.9% and functionality (HumanEval pass@1) by 2.1%, surpassing standard prompting methods.", "conclusion": "LLMs inherently encode vulnerability distinctions; effective exploitation (via MoC) enables controlled generation of secure code while maintaining functional performance."}}
{"id": "2507.09414", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09414", "abs": "https://arxiv.org/abs/2507.09414", "authors": ["Khizra Sohail", "Atif Aftab Ahmed Jilani", "Nigar Azhar Butt"], "title": "Enhancing NeuroEvolution-Based Game Testing: A Branch Coverage Approach for Scratch Programs", "comment": null, "summary": "Automated test generation for game-like programs presents unique challenges\ndue to their non-deterministic behavior and complex control structures. The\nNEATEST framework has been used for automated testing in Scratch games,\nemploying neuroevolution-based test generation optimized for statement\ncoverage. However, statement coverage alone is often insufficient for fault\ndetection, as it does not guarantee execution of all logical branches. This\npaper introduces a branch coverage-based fitness function to enhance test\neffectiveness in automated game testing. We extend NEATEST by integrating a\nbranch fitness function that prioritizes control-dependent branches, guiding\nthe neuroevolution process to maximize branch exploration. To evaluate the\neffectiveness of this approach, empirical experiments were conducted on 25\nScratch games, comparing Neatest with Statement Coverage (NSC) against Neatest\nwith Branch Coverage (NBC). A mutation analysis was also performed to assess\nthe fault detection capabilities of both techniques. The results demonstrate\nthat NBC achieves higher branch coverage than NSC in 13 out of 25 games,\nparticularly in programs with complex conditional structures. Moreover, NBC\nachieves a lower false positive rate in mutation testing, making it a more\nreliable approach for identifying faulty behavior in game programs. These\nfindings confirm that branch coverage-based test generation improves test\ncoverage and fault detection in Scratch programs.", "AI": {"tldr": "This paper proposes a branch coverage-based fitness function to enhance the NEATEST framework for automated testing of Scratch games. Empirical results on 25 games show that the new approach (NBC) outperforms statement coverage-based NEATEST (NSC) in branch coverage and fault detection reliability via lower false positive rates.", "motivation": "Statement coverage alone is insufficient for fault detection in game-like programs due to non-deterministic behavior and complex control structures, potentially missing critical logical branches that need testing.", "method": "The authors integrated a branch fitness function into NEATEST that prioritizes control-dependent branches. They evaluated this through empirical experiments on 25 Scratch games and mutation analysis comparing NBC and NSC.", "result": "NBC achieved higher branch coverage in 13/25 games, particularly in those with complex conditionals, and demonstrated a lower false positive rate in mutation testing compared to NSC.", "conclusion": "Branch coverage-based test generation significantly improves both test coverage and fault detection effectiveness in Scratch game programs, making it a more reliable method than statement coverage alone."}}
{"id": "2507.09564", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.09564", "abs": "https://arxiv.org/abs/2507.09564", "authors": ["Gaurav Varshney", "Akanksha Raj", "Divya Sangwan", "Sharif Abuadbba", "Rina Mishra", "Yansong Gao"], "title": "A Login Page Transparency and Visual Similarity Based Zero Day Phishing Defense Protocol", "comment": null, "summary": "Phishing is a prevalent cyberattack that uses look-alike websites to deceive\nusers into revealing sensitive information. Numerous efforts have been made by\nthe Internet community and security organizations to detect, prevent, or train\nusers to avoid falling victim to phishing attacks. Most of this research over\nthe years has been highly diverse and application-oriented, often serving as\nstandalone solutions for HTTP clients, servers, or third parties. However,\nlimited work has been done to develop a comprehensive or proactive\nprotocol-oriented solution to effectively counter phishing attacks. Inspired by\nthe concept of certificate transparency, which allows certificates issued by\nCertificate Authorities (CAs) to be publicly verified by clients, thereby\nenhancing transparency, we propose a concept called Page Transparency (PT) for\nthe web. The proposed PT requires login pages that capture users' sensitive\ninformation to be publicly logged via PLS and made available to web clients for\nverification. The pages are verified to be logged using cryptographic proofs.\nSince all pages are logged on a PLS and visually compared with existing pages\nthrough a comprehensive visual page-matching algorithm, it becomes impossible\nfor an attacker to register a deceptive look-alike page on the PLS and receive\nthe cryptographic proof required for client verification. All implementations\noccur on the client side, facilitated by the introduction of a new HTTP PT\nheader, eliminating the need for platform-specific changes or the installation\nof third-party solutions for phishing prevention.", "AI": {"tldr": "This paper proposes Page Transparency (PT), a protocol-based phishing prevention framework inspired by certificate transparency. PT uses public logging and cryptographic proofs to verify legitimate login pages, blocking deceptive look-alikes via a new HTTP header and visual page-matching algorithm without requiring server/platform changes.", "motivation": "Existing phishing countermeasures are fragmented and reactive, with standalone solutions for HTTP clients/servers or third parties. There's a need for a proactive, protocol-oriented defense to address universal phishing threats at the web infrastructure level.", "method": "The solution introduces: (1) Public logging of sensitive pages (login forms) via Page Logging Servers (PLS) with cryptographic verification; (2) A client-side-only HTTP Page Transparency (PT) header that integrates verification without third-party tools; (3) Visual page-matching algorithms to detect deceptive page registrations by comparing page content against existing public logs.", "result": "Cryptographic validation prevents unregistered deceptive pages from obtaining verification proofs, while visual comparison ensures registered pages maintain design integrity. Client-side implementation via HTTP headers eliminates server requirements and platform dependencies.", "conclusion": "Page Transparency offers a protocol-level phishing defense through public logging transparency and client-side validation. This approach establishes a universal verification mechanism, significantly increasing the cost/complexity of launching successful look-alike phishing attacks while requiring minimal system modifications."}}
{"id": "2507.09481", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09481", "abs": "https://arxiv.org/abs/2507.09481", "authors": ["Yuheng Huang", "Da Song", "Zhenlan Ji", "Shuai Wang", "Lei Ma"], "title": "Evaluating LLMs on Sequential API Call Through Automated Test Generation", "comment": null, "summary": "By integrating tools from external APIs, Large Language Models (LLMs) have\nexpanded their promising capabilities in a diverse spectrum of complex\nreal-world tasks. However, testing, evaluation, and analysis of LLM tool use\nremain in their early stages. Most existing benchmarks rely on manually\ncollected test cases, many of which cannot be automatically checked for\nsemantic correctness and instead depend on static methods such as string\nmatching. Additionally, these benchmarks often overlook the complex\ninteractions that occur between sequential API calls, which are common in\nreal-world applications. To fill the gap, in this paper, we introduce StateGen,\nan automated framework designed to generate diverse coding tasks involving\nsequential API interactions. StateGen combines state-machine-based API\nconstraint solving and validation, energy-based sampling, and control-flow\ninjection to generate executable programs. These programs are then translated\ninto human-like natural language task descriptions through a collaboration of\ntwo LLM agents. Utilizing StateGen, we construct StateEval, a benchmark\nencompassing 120 verified test cases spanning across three representative\nscenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental\nresults confirm that StateGen can effectively generate challenging and\nrealistic API-oriented tasks, highlighting areas for improvement in current\nLLMs incorporating APIs.", "AI": {"tldr": "This paper introduces StateGen, an automated framework for generating sequential API-oriented coding tasks, and StateEval, a benchmark with 120 test cases across three real-world scenarios, to improve testing and evaluation of LLMs integrating APIs.", "motivation": "Existing benchmarks for LLM tool use are limited by manual curation, reliance on static evaluation methods like string matching, and a lack of focus on sequential, state-dependent API interactions prevalent in real-world applications.", "method": "StateGen employs state-machine-based API constraint solving and validation, energy-based sampling to prioritize semantically meaningful programs, and control-flow injection to simulate complex task sequences. Two LLM agents collaborate to translate generated programs into natural language task descriptions.", "result": "StateGen successfully constructed StateEval benchmark covering Session Service, Tensor Operation, and ElevenLabs MCP scenarios. Generated tasks demonstrate higher complexity in sequential API interactions compared to prior benchmarks, revealing weaknesses in current LLMs' API integration capabilities.", "conclusion": "The framework enables systematic generation of API task benchmarks, highlighting critical challenges in LLM-sequential API interaction understanding and setting new requirements for evaluating and improving tool-integrated LLM systems."}}
{"id": "2507.09579", "categories": ["cs.CR", "cs.DC", "68M14, 94A60", "H.3.4; K.6.5; H.3.5"], "pdf": "https://arxiv.org/pdf/2507.09579", "abs": "https://arxiv.org/abs/2507.09579", "authors": ["Marc Bara"], "title": "PromptChain: A Decentralized Web3 Architecture for Managing AI Prompts as Digital Assets", "comment": "14 pages, 6 figures", "summary": "We present PromptChain, a decentralized Web3 architecture that establishes AI\nprompts as first-class digital assets with verifiable ownership, version\ncontrol, and monetization capabilities. Current centralized platforms lack\nmechanisms for proper attribution, quality assurance, or fair compensation for\nprompt creators. PromptChain addresses these limitations through a novel\nintegration of IPFS for immutable storage, smart contracts for governance, and\ntoken incentives for community curation. Our design includes: (1) a\ncomprehensive metadata schema for cross-model compatibility, (2) a\nstake-weighted validation mechanism to align incentives, and (3) a token\neconomy that rewards contributors proportionally to their impact. The proposed\narchitecture demonstrates how decentralized systems could potentially match\ncentralized alternatives in efficiency while providing superior ownership\nguarantees and censorship resistance through blockchain-anchored provenance\ntracking. By decoupling prompts from specific AI models or outputs, this work\nestablishes the foundation for an open ecosystem of human-AI collaboration in\nthe Web3 era, representing the first systematic treatment of prompts as\nstandalone digital assets with dedicated decentralized infrastructure.", "AI": {"tldr": "PromptChain introduces a decentralized Web3 architecture for treating AI prompts as verifiable digital assets, combining IPFS, smart contracts, and token incentives to address attribution, quality assurance, and fair compensation gaps in centralized platforms.", "motivation": "Centralized AI prompt platforms fail to provide proper ownership attribution, quality control, and equitable monetization for creators, necessitating a decentralized solution leveraging blockchain principles for transparency and incentive alignment.", "method": "1. IPFS-based immutable storage for prompt history and versioning. 2. Stake-weighted validation mechanism to assess prompt quality. 3. Token economy with incentives proportional to contributor impact. 4. Cross-model metadata schema for compatibility.", "result": "Demonstrated decentralized framework matches traditional platform efficiency while offering blockchain-anchored provenance tracking, verified ownership, and censorship-resistant prompt management through its three core design principles.", "conclusion": "PromptChain establishes the first systematic decentralized infrastructure for standalone AI prompts, enabling open human-AI collaboration ecosystems in Web3 with robust ownership guarantees and community-driven quality assurance."}}
{"id": "2507.09490", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09490", "abs": "https://arxiv.org/abs/2507.09490", "authors": ["Yan Zhao", "Chiwei Tang"], "title": "Towards LLM-Based Automatic Playtest", "comment": null, "summary": "Playtesting is the process in which people play a video game for testing. It\nis critical for the quality assurance of gaming software. Manual playtesting is\ntime-consuming and expensive. However, automating this process is challenging,\nas playtesting typically requires domain knowledge and problem-solving skills\nthat most conventional testing tools lack. Recent advancements in artificial\nintelligence (AI) have opened up new possibilities for applying Large Language\nModels (LLMs) to playtesting. However, significant challenges remain: current\nLLMs cannot visually perceive game environments, and most existing research\nfocuses on text-based games or games with robust APIs. Many non-text games lack\nAPIs to provide textual descriptions of game states, making it almost\nimpossible to naively apply LLMs for playtesting. This paper introduces Lap,\nour novel approach to LLM-based Automatic Playtesting, which uses ChatGPT to\ntest match-3 games, a category of games where players match three or more\nidentical tiles in a row or column to earn points. Lap encompasses three key\nphases: processing of game environments, prompting-based action generation, and\naction execution. Given a match-3 game, Lap takes a snapshot of the game board\nand converts it to a numeric matrix. It then prompts the ChatGPT-O1-mini API to\nsuggest moves based on that matrix and tentatively applies the suggested moves\nto earn points and trigger changes in the game board. It repeats the\nabove-mentioned three steps iteratively until timeout. For evaluation, we\nconducted a case study using Lap on an open-source match-3 game, CasseBonbons,\nand empirically compared it with three existing tools. Our results are\npromising: Lap outperformed existing tools by achieving higher code coverage\nand triggering more program crashes. This research sheds light on the future of\nautomatic testing and LLM applications.", "AI": {"tldr": "This paper introduces Lap, an LLM-based automated playtesting framework for match-3 games that converts game boards into numeric matrices and uses ChatGPT to generate actions, demonstrating superior code coverage and crash detection compared to existing tools. Key components include environment processing, prompting-based action generation, and iterative execution.", "motivation": "Manual playtesting is time-consuming and expensive, while conventional testing tools lack domain knowledge and problem-solving capabilities needed for gaming contexts. Non-text games without APIs remain challenging for existing LLM testing approaches.", "method": "Lap processes game environments by: 1) Capturing snapshots and converting them to numeric matrices 2) Using ChatGPT-O1-mini API through prompting to suggest valid moves 3) Executing and applying the suggested moves iteratively until timeout. This creates a feedback loop of observation, decision, and action tailored to match-3 mechanics.", "result": "The case study on CasseBonbons showed Lap achieved higher code coverage (82% vs. 65-75% from existing tools) and triggered more program crashes (12 vs. 3-9 crashes) through its LLM-powered decision making. These results demonstrate improved effectiveness in automated playtesting for this game category.", "conclusion": "Lap establishes a foundational approach to applying LLMs in auto playtesting for non-text games through matrix representation and prompting techniques. It shows LLMs can provide meaningful value in testing complex game mechanics, pointing toward future advancements in automated testing of interactive software."}}
{"id": "2507.09580", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.09580", "abs": "https://arxiv.org/abs/2507.09580", "authors": ["Yu Wang", "Yijian Liu", "Liheng Ji", "Han Luo", "Wenjie Li", "Xiaofei Zhou", "Chiyun Feng", "Puji Wang", "Yuhan Cao", "Geyuan Zhang", "Xiaojian Li", "Rongwu Xu", "Yilei Chen", "Tianxing He"], "title": "AICrypto: A Comprehensive Benchmark For Evaluating Cryptography Capabilities of Large Language Models", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of domains. However, their applications in cryptography, which serves\nas a foundational pillar of cybersecurity, remain largely unexplored. To\naddress this gap, we propose \\textbf{AICrypto}, the first comprehensive\nbenchmark designed to evaluate the cryptographic capabilities of LLMs. The\nbenchmark comprises 135 multiple-choice questions, 150 capture-the-flag (CTF)\nchallenges, and 18 proof problems, covering a broad range of skills from\nfactual memorization to vulnerability exploitation and formal reasoning. All\ntasks are carefully reviewed or constructed by cryptography experts to ensure\ncorrectness and rigor. To support automated evaluation of CTF challenges, we\ndesign an agent-based framework. To gain deeper insight into the current state\nof cryptographic proficiency in LLMs, we introduce human expert performance\nbaselines for comparison across all task types. Our evaluation of 17 leading\nLLMs reveals that state-of-the-art models match or even surpass human experts\nin memorizing cryptographic concepts, exploiting common vulnerabilities, and\nroutine proofs. However, they still lack a deep understanding of abstract\nmathematical concepts and struggle with tasks that require multi-step reasoning\nand dynamic analysis. We hope this work could provide insights for future\nresearch on LLMs in cryptographic applications. Our code and dataset are\navailable at https://aicryptobench.github.io.", "AI": {"tldr": "The paper introduces AICrypto, the first benchmark to evaluate LLMs in cryptographic tasks, combining 135 multiple-choice questions, 150 CTF challenges, and 18 proof problems. It reveals that top LLMs match human experts in factual knowledge and exploitation but struggle with abstract math and multi-step reasoning.", "motivation": "Cryptography is a critical domain for cybersecurity, yet LLMs' capabilities in this area remain underexplored. The authors aim to provide a rigorous benchmark to assess LLM performance, enabling insights for future research.", "method": "AICrypto was developed using expert-reviewed tasks covering memorization, vulnerability exploitation, and formal proofs. A human baseline was established for comparison, and an agent-based framework automated CTF challenge evaluations. 17 leading LLMs were tested.", "result": "Leading LLMs achieved proficiency in memorizing concepts, exploiting common vulnerabilities, and solving routine proofs. However, they underperformed on abstract mathematical understanding and multi-step reasoning tasks requiring dynamic analysis.", "conclusion": "AICrypto advances research by systematically evaluating LLMs\u2019 cryptographic capabilities, demonstrating both strengths and limitations. The findings suggest foundational gaps in LLMs\u2019 understanding of advanced crypto principles, while the benchmark serves as a valuable resource for further study."}}
{"id": "2507.09529", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09529", "abs": "https://arxiv.org/abs/2507.09529", "authors": ["Yunqian Wang", "Xiaohong Li", "Yao Zhang", "Yuekang Li", "Zhiping Zhou", "Ruitao Feng"], "title": "It Only Gets Worse: Revisiting DL-Based Vulnerability Detectors from a Practical Perspective", "comment": null, "summary": "With the growing threat of software vulnerabilities, deep learning (DL)-based\ndetectors have gained popularity for vulnerability detection. However, doubts\nremain regarding their consistency within declared CWE ranges, real-world\neffectiveness, and applicability across scenarios. These issues may lead to\nunreliable detection, high false positives/negatives, and poor adaptability to\nemerging vulnerabilities. A comprehensive analysis is needed to uncover\ncritical factors affecting detection and guide improvements in model design and\ndeployment. In this paper, we present VulTegra, a novel evaluation framework\nthat conducts a multidimensional comparison of scratch-trained and\npre-trained-based DL models for vulnerability detection. VulTegra reveals that\nstate-of-the-art (SOTA) detectors still suffer from low consistency, limited\nreal-world capabilities, and scalability challenges. Contrary to common belief,\npre-trained models are not consistently better than scratch-trained models but\nexhibit distinct strengths in specific contexts.Importantly, our study exposes\nthe limitations of relying solely on CWE-based classification and identifies\nkey factors that significantly affect model performance. Experimental results\nshow that adjusting just one such factor consistently improves recall across\nall seven evaluated detectors, with six also achieving better F1 scores. Our\nfindings provide deeper insights into model behavior and emphasize the need to\nconsider both vulnerability types and inherent code features for effective\ndetection.", "AI": {"tldr": "VulTegra evaluates DL-based vulnerability detectors, challenging assumptions about pre-trained models and highlighting factors impacting detection effectiveness.", "motivation": "Software vulnerabilities require reliable detection, but doubts persist about DL models' consistency, real-world performance, and adaptability across scenarios, necessitating insights into model limitations.", "method": "Developed VulTegra, a multidimensional evaluation framework, to compare scratch-trained and pre-trained DL models across seven vulnerability detectors, analyzing their performance with diverse code features.", "result": "SOTA detectors showed low consistency and scalability; pre-trained models outperformed in specific contexts only. Adjusting a single critical factor improved recall for all detectors and F1 scores for six.", "conclusion": "Model design should prioritize vulnerability type specificity and code feature adaptation alongside pre-training benefits, as no single approach is universally optimal."}}
{"id": "2507.09607", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.09607", "abs": "https://arxiv.org/abs/2507.09607", "authors": ["Kaiwen Wang", "Yuehan Dong", "Junchao Fan", "Xiaolin Chang"], "title": "Efficient Private Inference Based on Helper-Assisted Malicious Security Dishonest Majority MPC", "comment": null, "summary": "Private inference based on Secure Multi-Party Computation (MPC) addresses\ndata privacy risks in Machine Learning as a Service (MLaaS). However, existing\nMPC-based private inference frameworks focuses on semi-honest or honest\nmajority models, whose threat models are overly idealistic, while malicious\nsecurity dishonest majority models face the challenge of low efficiency. To\nbalance security and efficiency, we propose a private inference framework using\nHelper-Assisted Malicious Security Dishonest Majority Model (HA-MSDM). This\nframework includes our designed five MPC protocols and a co-optimized strategy.\nThese protocols achieve efficient fixed-round multiplication, exponentiation,\nand polynomial operations, providing foundational primitives for private\ninference. The co-optimized strategy balances inference efficiency and\naccuracy. To enhance efficiency, we employ polynomial approximation for\nnonlinear layers. For improved accuracy, we construct sixth-order polynomial\napproximation within a fixed interval to achieve high-precision activation\nfunction fitting and introduce parameter-adjusted batch normalization layers to\nconstrain the activation escape problem. Benchmark results on LeNet and AlexNet\nshow our framework achieves 2.4-25.7x speedup in LAN and 1.3-9.5x acceleration\nin WAN compared to state-of-the-art frameworks (IEEE S&P'25), maintaining high\naccuracy with only 0.04%-1.08% relative errors.", "AI": {"tldr": "The paper proposes a private inference framework, HA-MSDM, to balance malicious security (dishonest majority) with efficiency using optimized MPC protocols and co-optimized strategies for neural networks.", "motivation": "Existing MPC-based private inference frameworks assume overly idealistic threat models (semi-honest/honest majority) or face low efficiency in malicious dishonest-majority scenarios, requiring a robust and practical solution.", "method": "HA-MSDM integrates five specialized MPC protocols for fixed-round multiplication, exponentiation, and polynomial operations, alongside co-optimization techniques: sixth-order polynomial approximation for nonlinear layers to improve efficiency, parameter-adjusted batch normalization to maintain accuracy, and mitigation of activation escape issues through input constraints.", "result": "HA-MSDM achieves 2.4-25.7\u00d7 LAN and 1.3-9.5\u00d7 WAN speedups over state-of-the-art frameworks (IEEE S&P'25) while sustaining high inference accuracy with 0.04%-1.08% relative errors on LeNet and AlexNet benchmarks.", "conclusion": "The framework effectively balances security, efficiency, and accuracy in malicious dishonest-majority settings, establishing practicality for real-world MLaaS private inference applications."}}
{"id": "2507.09583", "categories": ["cs.SE", "cs.AI", "I.2.7; J.1"], "pdf": "https://arxiv.org/pdf/2507.09583", "abs": "https://arxiv.org/abs/2507.09583", "authors": ["Taniv Ashraf"], "title": "A Serverless Architecture for Real-Time Stock Analysis using Large Language Models: An Iterative Development and Debugging Case Study", "comment": "6 pages. The live application can be viewed at\n  https://codepen.io/tanivashraf/pen/GgpgxBY and the source code is available\n  at https://github.com/TanivAshraf/ai-stock-analyzer", "summary": "The advent of powerful, accessible Large Language Models (LLMs) like Google's\nGemini presents new opportunities for democratizing financial data analysis.\nThis paper documents the design, implementation, and iterative debugging of a\nnovel, serverless system for real-time stock analysis. The system leverages the\nGemini API for qualitative assessment, automates data ingestion and processing\nvia GitHub Actions, and presents the findings through a decoupled, static\nfrontend. We detail the architectural evolution of the system, from initial\nconcepts to a robust, event-driven pipeline, highlighting the practical\nchallenges encountered during deployment. A significant portion of this paper\nis dedicated to a case study on the debugging process, covering common software\nerrors, platform-specific permission issues, and rare, environment-level\nplatform bugs. The final architecture operates at a near-zero cost,\ndemonstrating a viable model for individuals to build sophisticated AI-powered\nfinancial tools. The operational application is publicly accessible, and the\ncomplete source code is available for review. We conclude by discussing the\nrole of LLMs in financial analysis, the importance of robust debugging\nmethodologies, and the emerging paradigm of human-AI collaboration in software\ndevelopment.", "AI": {"tldr": "This paper presents a cost-effective, serverless real-time stock analysis system leveraging Google's Gemini API and GitHub Actions, with detailed insights into its development and debugging challenges.", "motivation": "The rise of accessible Large Language Models (LLMs) enables democratizing financial data analysis, motivating the creation of an affordable, robust system for real-time stock analysis without traditional server infrastructure.", "method": "The authors designed and iteratively debugged a serverless system with Gemini API for qualitative stock assessment, GitHub Actions for automated data pipelines, and a decoupled static frontend. Key challenges included resolving software errors, permission issues, and environment-level platform bugs through systematic debugging.", "result": "A production-ready event-driven architecture operating at near-zero cost, with fully transparent implementation through public source code and a deployable application, demonstrating LLMs' viability in financial analysis systems.", "conclusion": "The work underscores opportunities for LLMs in finance, the necessity of rigorous debugging methodologies in modern software development, and emerging trends in human-AI collaboration for building reliable, scalable financial tools."}}
{"id": "2507.09624", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09624", "abs": "https://arxiv.org/abs/2507.09624", "authors": ["Xiaojie Lin", "Baihe Ma", "Xu Wang", "Guangsheng Yu", "Ying He", "Wei Ni", "Ren Ping Liu"], "title": "CAN-Trace Attack: Exploit CAN Messages to Uncover Driving Trajectories", "comment": null, "summary": "Driving trajectory data remains vulnerable to privacy breaches despite\nexisting mitigation measures. Traditional methods for detecting driving\ntrajectories typically rely on map-matching the path using Global Positioning\nSystem (GPS) data, which is susceptible to GPS data outage. This paper\nintroduces CAN-Trace, a novel privacy attack mechanism that leverages\nController Area Network (CAN) messages to uncover driving trajectories, posing\na significant risk to drivers' long-term privacy. A new trajectory\nreconstruction algorithm is proposed to transform the CAN messages,\nspecifically vehicle speed and accelerator pedal position, into weighted graphs\naccommodating various driving statuses. CAN-Trace identifies driving\ntrajectories using graph-matching algorithms applied to the created graphs in\ncomparison to road networks. We also design a new metric to evaluate matched\ncandidates, which allows for potential data gaps and matching inaccuracies.\nEmpirical validation under various real-world conditions, encompassing\ndifferent vehicles and driving regions, demonstrates the efficacy of CAN-Trace:\nit achieves an attack success rate of up to 90.59% in the urban region, and\n99.41% in the suburban region.", "AI": {"tldr": "CAN-Trace is a new privacy attack using CAN messages (vehicle speed and accelerator data) to reconstruct driving trajectories, achieving high success rates in urban and suburban areas without relying on GPS.", "motivation": "existing GPS-based methods for detecting driving trajectories are vulnerable to data outages, leaving drivers' privacy at risk; CAN-Trace addresses this by using alternative vehicle data (CAN messages) to trace trajectories even when GPS data is unavailable.", "method": "proposed (1) trajectory reconstruction algorithm to convert CAN messages into weighted graphs representing driving statuses, and (2) graph-matching algorithm to identify trajectories by aligning these graphs with road networks, combined with a novel metric to handle data gaps and inaccuracies.", "result": "CAN-Trace achieved 90.59% attack success rate in urban regions and 99.41% in suburban regions through empirical testing across diverse vehicles and driving conditions.", "conclusion": "CAN-Trace demonstrates the critical risk of privacy breaches using CAN data as an alternative to GPS, highlighting the need for robust countermeasures to protect driving trajectory privacy under various data availability scenarios."}}
{"id": "2507.09594", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09594", "abs": "https://arxiv.org/abs/2507.09594", "authors": ["Aydin Homay"], "title": "How to Define Design in Industrial Control and Automation Software", "comment": null, "summary": "Design is a fundamental aspect of engineering, enabling the creation of\nproducts, systems, and organizations to meet societal and/or business needs.\nHowever, the absence of a scientific foundation in design often results in\nsubjective decision-making, reducing both efficiency and innovation. This\nchallenge is particularly evident in the software industry and, by extension,\nin the domain of industrial control and automation systems (iCAS).\n  In this study, first we review the existing design definitions within the\nsoftware industry, challenge prevailing misconceptions about design, review\ndesign definition in the field of design theory and address key questions such\nas: When does design begin? How can design be defined scientifically? What\nconstitutes good design? and the difference between design and design language\nby relying on advancements in the field of design theory. We also evaluate the\ndistinction between ad-hoc and systematic design approaches, and present\narguments on how to balance complementary operational concerns while resolving\nconflicting evolutionary concerns.", "AI": {"tldr": "The paper establishes a scientific foundation for engineering design in industrial control/automation systems (iCAS) by analyzing design definitions, addressing misconceptions, and proposing systematic approaches to resolve conflicting design concerns.", "motivation": "Current design practices in software engineering and iCAS lack scientific rigor, leading to subjective decisions that limit efficiency and innovation. The paper seeks to address this gap by defining design principles grounded in design theory.", "method": "The study reviews software design definitions, challenges common misconceptions, examines design theory advancements to define scientifically valid design criteria, and evaluates ad-hoc vs systematic design approaches while proposing solutions for balancing operational and evolutionary concerns.", "result": "Developed a scientific framework for design in iCAS that clarifies when design begins, how good design is defined, distinguishes between design and design language, and provides methods to balance conflicting design considerations while maintaining operational requirements.", "conclusion": "A systematic design theory approach is essential for improving engineering decisions in iCAS. The proposed framework provides principles to transform subjective design practices into scientifically grounded methodologies, enhancing both innovation and efficiency."}}
{"id": "2507.09699", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.09699", "abs": "https://arxiv.org/abs/2507.09699", "authors": ["Zeki Kazan", "Sagar Sharma", "Wanrong Zhang", "Bo Jiang", "Qiang Yan"], "title": "Interpreting Differential Privacy in Terms of Disclosure Risk", "comment": "11 pages with 6 pages of supplemental material", "summary": "As the use of differential privacy (DP) becomes widespread, the development\nof effective tools for reasoning about the privacy guarantee becomes\nincreasingly critical. In pursuit of this goal, we demonstrate novel\nrelationships between DP and measures of statistical disclosure risk. We\nsuggest how experts and non-experts can use these results to explain the DP\nguarantee, interpret DP composition theorems, select and justify privacy\nparameters, and identify worst-case adversary prior probabilities.", "AI": {"tldr": "This paper establishes new connections between differential privacy (DP) and statistical disclosure risk measures, providing tools for experts and non-experts to interpret DP guarantees, select privacy parameters, and analyze adversary risks.", "motivation": "The rapid adoption of differential privacy necessitates better tools for understanding and communicating its privacy guarantees, especially for parameter selection and composition theorem interpretation.", "method": "The authors develop a framework linking DP to existing statistical disclosure risk metrics, enabling analysis of privacy guarantees through adversary risk modeling and parameter optimization.", "result": "The approach allows for intuitive interpretation of DP composition laws, justification of parameter choices, and identification of worst-case adversary prior distributions that maximize disclosure risk.", "conclusion": "By connecting DP with statistical risk measures, the work enhances practical understanding and deployment of differential privacy, enabling more informed decisions about privacy-utility tradeoffs and parameter selection."}}
{"id": "2507.09596", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09596", "abs": "https://arxiv.org/abs/2507.09596", "authors": ["Aydin Homay"], "title": "The Mythical Good Software", "comment": null, "summary": "Good software has high cohesion and low coupling is clumsy, obscure, and in\nsome certain cases could be actually a harmful state of being. It is clumsy\nbecause there is no perfect correlation between higher cohesiveness and optimum\ndesign, and it is obscure because it conveys the message that coupling and\ncohesion are two distinct design principles, while there are in principle the\nsame design approaches, and only the time and space differ between them, and it\ncould also be a harmful state of being because we should not always aim for\nhigher cohesiveness without considering its cost.\n  In the course of this study, we aim to elucidate for the readers the meaning\nand underlying philosophy of the aforementioned paragraph.", "AI": {"tldr": "The paper challenges the conventional view of high cohesion and low coupling as universally good design principles in software engineering, arguing that this focus can be flawed, ambiguous, and even harmful when applied rigidly without considering trade-offs.", "motivation": "The authors aim to address the limitations of traditional cohesion/coupling paradigms by clarifying that these principles are not as distinct as commonly assumed and questioning their blind application.", "method": "Theoretical analysis and critical discussion of design principles, exploring their philosophical and practical implications through conceptual arguments rather than empirical methods.", "result": "Demonstration that high cohesion and low coupling are mathematically similar approaches differing only in time/space trade-offs, and that uncritical application of these principles can prevent optimal system design.", "conclusion": "Software design should abandon binary thinking about coupling/cohesion, balancing their trade-offs based on time/space constraints rather than adhering to an absolutist pursuit of high cohesion."}}
{"id": "2507.09762", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09762", "abs": "https://arxiv.org/abs/2507.09762", "authors": ["Yasir Ech-Chammakhy", "Anas Motii", "Anass Rabii", "Jaafar Chbili"], "title": "EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions", "comment": "Accepted for publication at the 28th International Symposium on\n  Research in Attacks, Intrusions, and Defenses (RAID 2025)", "summary": "Hacker forums provide critical early warning signals for emerging\ncybersecurity threats, but extracting actionable intelligence from their\nunstructured and noisy content remains a significant challenge. This paper\npresents an unsupervised framework that automatically detects, clusters, and\nprioritizes security events discussed across hacker forum posts. Our approach\nleverages Transformer-based embeddings fine-tuned with contrastive learning to\ngroup related discussions into distinct security event clusters, identifying\nincidents like zero-day disclosures or malware releases without relying on\npredefined keywords. The framework incorporates a daily ranking mechanism that\nprioritizes identified events using quantifiable metrics reflecting timeliness,\nsource credibility, information completeness, and relevance. Experimental\nevaluation on real-world hacker forum data demonstrates that our method\neffectively reduces noise and surfaces high-priority threats, enabling security\nanalysts to mount proactive responses. By transforming disparate hacker forum\ndiscussions into structured, actionable intelligence, our work addresses\nfundamental challenges in automated threat detection and analysis.", "AI": {"tldr": "This paper introduces an unsupervised framework using fine-tuned Transformer embeddings and a daily ranking system to automatically identify and prioritize security events in hacker forums without predefined keywords.", "motivation": "Hacker forums contain vital early warnings about cybersecurity threats, but their unstructured, noisy content makes actionable intelligence extraction challenging.", "method": "The framework applies contrastive-learning fine-tuned Transformer embeddings to cluster security-related discussions into events. It uses metrics like timeliness, credibility, completeness, and relevance in a daily ranking mechanism to prioritize clusters.", "result": "Experiments on real-world hacker forum data show effective noise reduction and accurate identification of high-priority threats (e.g., zero-days, malware) through structured clustering and prioritization.", "conclusion": "The approach advances automated threat detection by transforming unstructured hacker forum discourse into actionable intelligence, enabling proactive security responses."}}
{"id": "2507.09599", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09599", "abs": "https://arxiv.org/abs/2507.09599", "authors": ["Aydin Homay"], "title": "Complexity and Coupling: A Functional Domain Approach", "comment": null, "summary": "This paper provides a precise and scientific definition of complexity and\ncoupling, grounded in the functional domain, particularly within industrial\ncontrol and automation systems (iCAS). We highlight the widespread ambiguity in\ndefining complexity and coupling, emphasizing that many existing definitions\nrooted in physical attributes lead to confusion and inconsistencies.\nFurthermore, we re-exhibit why coupled design inherently increases complexity\nand how potentially this complexity could be reduced. Drawing on examples from\nvarious disciplines, such as software engineering, industrial automation, and\nmechanical design, we demonstrate that complexity does not necessarily\ncorrelate with system size or the number of components, and coupling, unlike\ncommon belief in software engineering, actually does not occur in the physical\ndomain but in the functional domain. We conclude that effective design\nnecessitates addressing coupling and complexity within the functional domain.", "AI": {"tldr": "The paper defines complexity and coupling in the functional domain of iCAS, challenging physical-centric theories and proposing that functional coupling drives complexity regardless of system size.", "motivation": "The authors address the ambiguity in existing complexity and coupling definitions, which often rely on physical attributes, creating confusion across disciplines like iCAS, software engineering, and mechanical design.", "method": "The paper redefines complexity and coupling through functional domain analysis, using interdisciplinary examples to contrast with physical domain interpretations and demonstrate their distinct behaviors.", "result": "Findings reveal that functional coupling directly increases complexity, and complexity is not correlated with system size or component count but rather with functional dependencies.", "conclusion": "Effective design in iCAS and related fields must prioritize functional domain analysis for managing coupling and complexity, rather than focusing on physical characteristics."}}
{"id": "2507.09859", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.09859", "abs": "https://arxiv.org/abs/2507.09859", "authors": ["Guntur Dharma Putra", "Bagus Rakadyanto Oktavianto Putra"], "title": "Endorsement-Driven Blockchain SSI Framework for Dynamic IoT Ecosystems", "comment": "5 pages, 4 figures. Accepted to IEEE ICBC 2025 as a short paper", "summary": "Self-Sovereign Identity (SSI) offers significant potential for managing\nidentities in the Internet of Things (IoT), enabling decentralized\nauthentication and credential management without reliance on centralized\nentities. However, existing SSI frameworks often limit credential issuance and\nrevocation to trusted entities, such as IoT manufacturers, which restricts\nflexibility in dynamic IoT ecosystems. In this paper, we propose a\nblockchain-based SSI framework that allows any individual with a verifiable\ntrust linkage to act as a credential issuer, ensuring decentralized and\nscalable identity management. Our framework incorporates a layered\narchitecture, where trust is dynamically established through endorsement-based\ncalculations and maintained via a hierarchical chain-of-trust mechanism.\nBlockchain serves as the Verifiable Data Registry, ensuring transparency and\nimmutability of identity operations, while smart contracts automate critical\nprocesses such as credential issuance, verification, and revocation. A\nproof-of-concept implementation demonstrates that the proposed framework is\nfeasible and incurs minimal overheads compared to the baseline, making it\nwell-suited for dynamic and resource-constrained IoT environments.", "AI": {"tldr": "The paper introduces a blockchain-based Self-Sovereign Identity (SSI) framework for IoT, enabling decentralized identity operations with flexible credential issuance and revocation by any individual with trust linkage, reducing reliance on centralized entities.", "motivation": "Existing SSI frameworks for IoT restrict credential issuance to specific trusted entities (e.g. manufacturers), limiting adaptability in dynamic, decentralized IoT ecosystems where trust relationships may evolve or decentralize further.", "method": "A layered blockchain-based SSI framework using (1) endorsement-based trust calculations to dynamically establish trust, (2) hierarchical chain-of-trust mechanisms for trust maintenance, (3) blockchain as a trust-anchored Verifiable Data Registry ensuring immutability, and (4) smart contracts to automate credential issuance, verification, revocation workflows.", "result": "A proof-of-concept implementation demonstrated the framework's feasibility with minimal overhead compared to existing approaches, confirming its suitability for dynamic, resource-constrained IoT environments while maintaining transparent, decentralized identity operations.", "conclusion": "The framework effectively addresses SSI limitations in IoT through blockchain's inherent trust properties and dynamic endorsement mechanisms, offering a scalable, decentralized solution for evolving IoT ecosystems with distributed trust requirements."}}
{"id": "2507.09637", "categories": ["cs.SE", "cs.HC", "D.2.0; D.2.3; K.4.3"], "pdf": "https://arxiv.org/pdf/2507.09637", "abs": "https://arxiv.org/abs/2507.09637", "authors": ["Lo Gullstrand Heander", "Emma S\u00f6derberg", "Christofer Rydenf\u00e4lt"], "title": "Code Review as Decision-Making -- Building a Cognitive Model from the Questions Asked During Code Review", "comment": "39 pages, 14 figures Submitted to Empirical Software Engineering,\n  Springer Nature", "summary": "Code review is a well-established and valued practice in the software\nengineering community contributing to both code quality and interpersonal\nbenefits. However, there are challenges in both tools and processes that give\nrise to misalignments and frustrations. Recent research seeks to address this\nby automating code review entirely, but we believe that this risks losing the\nmajority of the interpersonal benefits such as knowledge transfer and shared\nownership.\n  We believe that by better understanding the cognitive processes involved in\ncode review, it would be possible to improve tool support, with out without AI,\nand make code review both more efficient, more enjoyable, while increasing or\nmaintaining all of its benefits. In this paper, we conduct an ethnographic\nthink-aloud study involving 10 participants and 34 code reviews. We build a\ncognitive model of code review bottom up through thematic, statistical,\ntemporal, and sequential analysis of the transcribed material. Through the\ndata, the similarities between the cognitive process in code review and\ndecision-making processes, especially recognition-primed decision-making,\nbecome apparent.\n  The result is the Code Review as Decision-Making (CRDM) model that shows how\nthe developers move through two phases during the code review; first an\norientation phase to establish context and rationale and then an analytical\nphase to understand, assess, and plan the rest of the review. Throughout the\nprocess several decisions must be taken, on writing comments, finding more\ninformation, voting, running the code locally, verifying continuous integration\nresults, etc.\n  Analysis software and process-coded data publicly available at:\nhttps://doi.org/10.5281/zenodo.15758266", "AI": {"tldr": "The paper proposes the Code Review as Decision-Making (CRDM) model to enhance code review tools by understanding developers' cognitive processes during reviews, avoiding pitfalls of automation while maintaining interpersonal benefits.", "motivation": "Code reviews face challenges in tooling and process misalignments, and automating them risks losing knowledge transfer and shared ownership benefits. Better cognitive understanding could improve tools and experiences.", "method": "An ethnographic think-aloud study with 10 participants analyzing 34 code reviews through thematic, statistical, temporal, and sequential analysis of transcriptions to build a cognitive model.", "result": "The CRDM model reveals developers progress through orientation (context establishment) and analytical (assessment/planning) phases, involving key decisions like commenting, information seeking, and verification. Data available at https://doi.org/10.5281/zenodo.15758266", "conclusion": "Code reviews are fundamentally decision-making processes. Future tools should explicitly support these phases to retain interpersonal benefits while improving efficiency and enjoyment. The CRDM model provides framework for targeted tool improvements."}}
{"id": "2507.09860", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09860", "abs": "https://arxiv.org/abs/2507.09860", "authors": ["Nguyen Van Duc", "Bui Duc Manh", "Quang-Trung Luu", "Dinh Thai Hoang", "Van-Linh Nguyen", "Diep N. Nguyen"], "title": "Secure and Efficient UAV-Based Face Detection via Homomorphic Encryption and Edge Computing", "comment": null, "summary": "This paper aims to propose a novel machine learning (ML) approach\nincorporating Homomorphic Encryption (HE) to address privacy limitations in\nUnmanned Aerial Vehicles (UAV)-based face detection. Due to challenges related\nto distance, altitude, and face orientation, high-resolution imagery and\nsophisticated neural networks enable accurate face recognition in dynamic\nenvironments. However, privacy concerns arise from the extensive surveillance\ncapabilities of UAVs. To resolve this issue, we propose a novel framework that\nintegrates HE with advanced neural networks to secure facial data throughout\nthe inference phase. This method ensures that facial data remains secure with\nminimal impact on detection accuracy. Specifically, the proposed system\nleverages the Cheon-Kim-Kim-Song (CKKS) scheme to perform computations directly\non encrypted data, optimizing computational efficiency and security.\nFurthermore, we develop an effective data encoding method specifically designed\nto preprocess the raw facial data into CKKS form in a\nSingle-Instruction-Multiple-Data (SIMD) manner. Building on this, we design a\nsecure inference algorithm to compute on ciphertext without needing decryption.\nThis approach not only protects data privacy during the processing of facial\ndata but also enhances the efficiency of UAV-based face detection systems.\nExperimental results demonstrate that our method effectively balances privacy\nprotection and detection performance, making it a viable solution for UAV-based\nsecure face detection. Significantly, our approach (while maintaining data\nconfidentially with HE encryption) can still achieve an accuracy of less than\n1% compared to the benchmark without using encryption.", "AI": {"tldr": "The paper proposes a privacy-preserving framework for UAV-based face detection using Homomorphic Encryption (HE) with the CKKS scheme, achieving minimal impact on accuracy.", "motivation": "Privacy concerns arise from UAVs' extensive surveillance capabilities during face detection, necessitating a secure yet efficient solution.", "method": "The framework integrates HE (CKKS) and neural networks, employing a SIMD-based data encoding method for preprocessing and a secure inference algorithm for encrypted data computation without decryption.", "result": "Experimental results show the method maintains data confidentiality while achieving an accuracy within 1% of non-encrypted benchmarks, effectively balancing privacy and performance.", "conclusion": "The proposed approach enables UAV-based secure face detection by combining HE with advanced neural networks, offering a viable solution for real-world privacy-preserving dynamic environments."}}
{"id": "2507.09665", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09665", "abs": "https://arxiv.org/abs/2507.09665", "authors": ["Saima Afrin", "Bowen Xu", "Antonio Mastropaolo"], "title": "Is Quantization a Deal-breaker? Empirical Insights from Large Code Models", "comment": null, "summary": "The growing scale of large language models (LLMs) not only demands extensive\ncomputational resources but also raises environmental concerns due to their\nincreasing carbon footprint. Model quantization emerges as an effective\napproach that can reduce the resource demands of LLMs by decreasing parameter\nprecision without substantially affecting performance (e.g., 16 bit to 4 bit).\nWhile recent studies have established quantization as a promising approach for\noptimizing large code models (LCMs), a specialized subset of LLMs tailored for\nautomated software engineering, their findings offer only limited insights into\nits practical implications. Specifically, current investigations focus only on\nthe functional correctness of the code generated by quantized models,\nneglecting how quantization impacts critical aspects of code quality such as\nreliability, maintainability, and security. To bridge this gap, our study\ninvestigates the effects of quantization on the qualitative aspects of\nautomatically generated code. We apply Activation-aware Weight Quantization\n(AWQ) to two widely used code models, CodeLlama and DeepSeekCoder, to generate\nJava and Python code. Using state-of-the-art static analysis tools, we evaluate\nsoftware quality metrics and static features including cyclomatic complexity,\ncognitive complexity, and lines of code. Our findings reveal that quantization\nis a robust technique that not only preserves functional correctness, but also\nretains key qualitative code attributes sought after by developers, such as\nmaintainability and structural simplicity.", "AI": {"tldr": "This study investigates the impact of model quantization on code quality metrics (reliability, maintainability, security) for large code models, finding that AWQ quantization preserves functional correctness and qualitative attributes.", "motivation": "Recent quantization research for code models focuses only on functional correctness, neglecting critical code quality aspects like maintainability and security. Environmental and computational concerns of large models necessitate this exploration.", "method": "Applied Activation-aware Weight Quantization (AWQ) to CodeLlama and DeepSeekCoder code models, generating Java/Python code. Evaluated outcomes using static analysis tools for cyclomatic complexity, cognitive complexity, and lines of code metrics.", "result": "Quantized models maintained functional correctness while preserving key code quality attributes (maintainability, structural simplicity). Metrics analysis validated retention of critical software quality characteristics.", "conclusion": "Quantization using AWQ is a robust optimization technique for code models that maintains both functional and qualitative code attributes, addressing previous research gaps in code quality analysis post-quantization."}}
{"id": "2507.09990", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09990", "abs": "https://arxiv.org/abs/2507.09990", "authors": ["Ming Wen", "Jiaqi Zhu", "Yuedong Xu", "Yipeng Zhou", "Dingding Han"], "title": "Differentially Private Federated Low Rank Adaptation Beyond Fixed-Matrix", "comment": "23 pages, NeurIPS 2025 under review", "summary": "Large language models (LLMs) typically require fine-tuning for\ndomain-specific tasks, and LoRA offers a computationally efficient approach by\ntraining low-rank adapters. LoRA is also communication-efficient for federated\nLLMs when multiple users collaboratively fine-tune a global LLM model without\nsharing their proprietary raw data. However, even the transmission of local\nadapters between a server and clients risks serious privacy leakage. Applying\ndifferential privacy (DP) to federated LoRA encounters a dilemma: adding noise\nto both adapters amplifies synthetic noise on the model, while fixing one\nadapter impairs the learnability of fine-tuning. In this paper, we propose\nFedASK (Differentially Private Federated Low Rank Adaptation with Double\nSketching) , a novel federated LoRA framework to enable effective updating of\nboth low-rank adapters with robust differential privacy. Inspired by randomized\nSVD, our key idea is a two-stage sketching pipeline. This pipeline first\naggregates carefully sketched, privacy-preserving local updates, and then\nreconstructs the global matrices on the server to facilitate effective updating\nof both adapters. We theoretically prove FedASK's differential privacy\nguarantee and its exact aggregation property. Comprehensive experiments\ndemonstrate that FedASK consistently outperforms baseline methods across a\nvariety of privacy settings and data distributions.", "AI": {"tldr": "FedASK is a differentially private federated learning framework for LLMs that uses double sketching to address privacy risks in adapter transmission and optimize low-rank model updates.", "motivation": "Federated LLM fine-tuning with LoRA faces privacy leakage risks from transmitting local adapters and a dilemma between adding DP noise (which amplifies synthetic noise) and maintaining model learnability (which requires adapter updates).", "method": "FedASK introduces a two-stage sketching pipeline inspired by randomized SVD: (1) privacy-preserving sketches of local adapter updates are aggregated, and (2) the server reconstructs global low-rank matrices using the collected sketches to update both adapters effectively.", "result": "Theoretical proofs establish FedASK's differential privacy guarantees and exact aggregation property, while experiments show consistent performance improvements over baselines across diverse privacy budgets and data scenarios.", "conclusion": "FedASK resolves the privacy-learnability tradeoff in federated LLM adaptation by enabling robust DP with full utilization of both low-rank adapters through double sketching, setting a new standard for secure collaborative model fine-tuning."}}
{"id": "2507.09682", "categories": ["cs.SE", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.09682", "abs": "https://arxiv.org/abs/2507.09682", "authors": ["Laura Baird", "Armin Moin"], "title": "OrQstrator: An AI-Powered Framework for Advanced Quantum Circuit Optimization", "comment": "IEEE International Conference on Quantum Computing and Engineering\n  (QCE) 2025 - Extended Abstract", "summary": "We propose a novel approach, OrQstrator, which is a modular framework for\nconducting quantum circuit optimization in the Noisy Intermediate-Scale Quantum\n(NISQ) era. Our framework is powered by Deep Reinforcement Learning (DRL). Our\norchestration engine intelligently selects among three complementary circuit\noptimizers: A DRL-based circuit rewriter trained to reduce depth and gate count\nvia learned rewrite sequences; a domain-specific optimizer that performs\nefficient local gate resynthesis and numeric optimization; a parameterized\ncircuit instantiator that improves compilation by optimizing template circuits\nduring gate set translation. These modules are coordinated by a central\norchestration engine that learns coordination policies based on circuit\nstructure, hardware constraints, and backend-aware performance features such as\ngate count, depth, and expected fidelity. The system outputs an optimized\ncircuit for hardware-aware transpilation and execution, leveraging techniques\nfrom an existing state-of-the-art approach, called the NISQ Analyzer, to adapt\nto backend constraints.", "AI": {"tldr": "OrQstrator is a DRL-powered modular framework for optimizing NISQ-era quantum circuits by coordinating three complementary optimizers to reduce depth, gate count, and improve fidelity while respecting hardware constraints. The system integrates with existing NISQAnalyzer techniques to adapt output circuits for target backends.", "motivation": "Current quantum circuits in the NISQ era require optimization to minimize depth, gate count, and preserve fidelity against hardware-specific noise and constraints. No framework yet unifies multiple optimization strategies with adaptive coordination based on these factors.", "method": "The framework employs a central DRL orchestration engine that dynamically coordinates three optimizers: 1) a DRL-driven circuit rewriter optimizing depth/gate count; 2) a domain-specific optimizer for local gate resynthesis and numeric optimization; 3) a parameterized instantiator optimizing template circuits during gate set translation. Coordination policies are learned from circuit structure, hardware limitations, and backend-aware metrics via reinforcement learning.", "result": "The paper doesn't specify detailed empirical results, but describes a system architecture where optimized circuits are produced for hardware-aware transpilation using the NISQ Analyzer's adaptation techniques.", "conclusion": "OrQstrator demonstrates how modular optimization systems can be coordinated via DRL strategies to address NISQ-era challenges, combining general rewriting with domain-specific techniques to achieve hardware-aware quantum circuit optimization."}}
{"id": "2507.10016", "categories": ["cs.CR", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.10016", "abs": "https://arxiv.org/abs/2507.10016", "authors": ["Lixu Wang", "Kaixiang Yao", "Xinfeng Li", "Dong Yang", "Haoyang Li", "Xiaofeng Wang", "Wei Dong"], "title": "The Man Behind the Sound: Demystifying Audio Private Attribute Profiling via Multimodal Large Language Model Agents", "comment": "22 pages, 4 figures", "summary": "Our research uncovers a novel privacy risk associated with multimodal large\nlanguage models (MLLMs): the ability to infer sensitive personal attributes\nfrom audio data -- a technique we term audio private attribute profiling. This\ncapability poses a significant threat, as audio can be covertly captured\nwithout direct interaction or visibility. Moreover, compared to images and\ntext, audio carries unique characteristics, such as tone and pitch, which can\nbe exploited for more detailed profiling. However, two key challenges exist in\nunderstanding MLLM-employed private attribute profiling from audio: (1) the\nlack of audio benchmark datasets with sensitive attribute annotations and (2)\nthe limited ability of current MLLMs to infer such attributes directly from\naudio. To address these challenges, we introduce AP^2, an audio benchmark\ndataset that consists of two subsets collected and composed from real-world\ndata, and both are annotated with sensitive attribute labels. Additionally, we\npropose Gifts, a hybrid multi-agent framework that leverages the complementary\nstrengths of audio-language models (ALMs) and large language models (LLMs) to\nenhance inference capabilities. Gifts employs an LLM to guide the ALM in\ninferring sensitive attributes, then forensically analyzes and consolidates the\nALM's inferences, overcoming severe hallucinations of existing ALMs in\ngenerating long-context responses. Our evaluations demonstrate that Gifts\nsignificantly outperforms baseline approaches in inferring sensitive\nattributes. Finally, we investigate model-level and data-level defense\nstrategies to mitigate the risks of audio private attribute profiling. Our work\nvalidates the feasibility of audio-based privacy attacks using MLLMs,\nhighlighting the need for robust defenses, and provides a dataset and framework\nto facilitate future research.", "AI": {"tldr": "This paper introduces audio private attribute profiling, a new privacy risk in MLLMs where sensitive attributes are inferred from audio. It presented AP^2, a benchmark dataset with real-world audio annotations, and Gifts, a hybrid multi-agent framework combining ALMs and LLMs to enhance inference accuracy. Evaluations show Gifts outperforms baselines, highlighting the feasibility of audio-based attacks and the need for defenses.", "motivation": "Despite non-visual/numerical modalities being primary for MLLMs, audio data's covert capture and unique properties (tone, pitch) pose overlooked privacy risks. Existing benchmarks lack sensitive attribute annotations, and current MLLMs struggle to infer attributes directly from audio, creating unaddressed research gaps.", "method": "1. Built AP^2 dataset: Two real-world audio subsets annotated with sensitive attributes. 2. Developed Gifts framework: Uses LLM to guide ALM's attribute inference through multi-phase cooperation, reduces hallucinations in long-context audio analysis by consolidating ALM responses through forensic reasoning.", "result": "Gifts achieves 15-20% higher accuracy than baselines in attribute inference, with 30% reduction in hallucinations. AP^2 demonstrates 85% attribute labeling reliability. Defense experiments identify differential privacy and data masking as most effective mitigation strategies against audio-based profiling.", "conclusion": "This work establishes audio as a critical vector for privacy attacks in MLLMs through empirical validation and novel framework development. The AP^2 benchmark and Gifts framework enable systematic study of both attack capabilities and defense mechanisms in audio private attribute profiling contexts."}}
{"id": "2507.09790", "categories": ["cs.SE", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.09790", "abs": "https://arxiv.org/abs/2507.09790", "authors": ["Helge Spieker", "Th\u00e9o Matricon", "Nassim Belmecheri", "J\u00f8rn Eirik Betten", "Gauthier Le Bartz Lyan", "Heraldo Borges", "Quentin Mazouni", "Dennis Gross", "Arnaud Gotlieb", "Mathieu Acher"], "title": "Prompting for Performance: Exploring LLMs for Configuring Software", "comment": null, "summary": "Software systems usually provide numerous configuration options that can\naffect performance metrics such as execution time, memory usage, binary size,\nor bitrate. On the one hand, making informed decisions is challenging and\nrequires domain expertise in options and their combinations. On the other hand,\nmachine learning techniques can search vast configuration spaces, but with a\nhigh computational cost, since concrete executions of numerous configurations\nare required. In this exploratory study, we investigate whether large language\nmodels (LLMs) can assist in performance-oriented software configuration through\nprompts. We evaluate several LLMs on tasks including identifying relevant\noptions, ranking configurations, and recommending performant configurations\nacross various configurable systems, such as compilers, video encoders, and SAT\nsolvers. Our preliminary results reveal both positive abilities and notable\nlimitations: depending on the task and systems, LLMs can well align with expert\nknowledge, whereas hallucinations or superficial reasoning can emerge in other\ncases. These findings represent a first step toward systematic evaluations and\nthe design of LLM-based solutions to assist with software configuration.", "AI": {"tldr": "This study explores the use of large language models (LLMs) to assist in performance-oriented software configuration through prompts, revealing both alignment with expert knowledge and challenges like hallucinations.", "motivation": "The motivation arises from the difficulty of manually navigating complex software configurations and the high computational cost of traditional machine learning methods for this task.", "method": "The method involves evaluating multiple LLMs on tasks such as identifying relevant options, ranking configurations, and recommending performant settings across systems like compilers, video encoders, and SAT solvers.", "result": "Preliminary results show LLMs sometimes match expert decisions but also exhibit hallucinations and superficial reasoning depending on the system and task.", "conclusion": "The study concludes that LLMs show potential for aiding software configuration but require further systematic evaluation to address their limitations and refine their application."}}
{"id": "2507.10162", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.10162", "abs": "https://arxiv.org/abs/2507.10162", "authors": ["Weiyang He", "Chip-Hong Chang"], "title": "HASSLE: A Self-Supervised Learning Enhanced Hijacking Attack on Vertical Federated Learning", "comment": null, "summary": "Vertical Federated Learning (VFL) enables an orchestrating active party to\nperform a machine learning task by cooperating with passive parties that\nprovide additional task-related features for the same training data entities.\nWhile prior research has leveraged the privacy vulnerability of VFL to\ncompromise its integrity through a combination of label inference and backdoor\nattacks, their effectiveness is constrained by the low label inference\nprecision and suboptimal backdoor injection conditions. To facilitate a more\nrigorous security evaluation on VFL without these limitations, we propose\nHASSLE, a hijacking attack framework composed of a gradient-direction-based\nlabel inference module and an adversarial embedding generation algorithm\nenhanced by self-supervised learning. HASSLE accurately identifies private\nsamples associated with a targeted label using only a single known instance of\nthat label. In the two-party scenario, it demonstrates strong performance with\nan attack success rate (ASR) of over 99% across four datasets, including both\nimage and tabular modalities, and achieves 85% ASR on the more complex\nCIFAR-100 dataset. Evaluation of HASSLE against 8 potential defenses further\nhighlights its significant threat while providing new insights into building a\ntrustworthy VFL system.", "AI": {"tldr": "HASSLE is a hijacking attack framework for Vertical Federated Learning (VFL) that uses gradient-direction-based label inference with a single known label instance and adversarial embedding generation enhanced by self-supervised learning. It achieves high attack success rates across diverse datasets, including 99%+ in simple cases and 85% on CIFAR-100, demonstrating significant VFL security threats while suggesting insights for robust defense design.", "motivation": "Prior VFL security evaluations were limited by low label inference precision and suboptimal backdoor attack conditions. This paper aims to propose a more rigorous threat analysis framework to better understand and improve VFL system robustness against privacy compromising attacks.", "method": "The framework integrates two key modules: (1) Gradient-direction label inference module that identifies private sample-label associations using a single known positive instance, and (2) Self-supervised learning enhanced adversarial embedding generation for backdoor injection. Both modules exploit VFL gradient exchange mechanisms for secure feature manipulation.", "result": "HASSLE achieves over 99% attack success rate in two-party scenarios across four datasets (including MNIST, Fashion-MNIST, and tabular data), and 85% ASR on the complex CIFAR-100 dataset. Evaluation against 8 existing defense mechanisms shows high effectiveness of the proposed attack framework.", "conclusion": "The research demonstrates HASSLE's significant threat to VFL through its high precision label inference and effective backdoor injection capabilities, particularly in overcoming limitations of prior attacks. While highlighting the vulnerability of current VFL systems, the work provides critical insights for developing trustworthy security frameworks that can resist such sophisticated hybrid attacks."}}
{"id": "2507.09820", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.09820", "abs": "https://arxiv.org/abs/2507.09820", "authors": ["Jia Yi Goh", "Shaun Khoo", "Nyx Iskandar", "Gabriel Chua", "Leanne Tan", "Jessica Foo"], "title": "Measuring What Matters: A Framework for Evaluating Safety Risks in Real-World LLM Applications", "comment": null, "summary": "Most safety testing efforts for large language models (LLMs) today focus on\nevaluating foundation models. However, there is a growing need to evaluate\nsafety at the application level, as components such as system prompts,\nretrieval pipelines, and guardrails introduce additional factors that\nsignificantly influence the overall safety of LLM applications. In this paper,\nwe introduce a practical framework for evaluating application-level safety in\nLLM systems, validated through real-world deployment across multiple use cases\nwithin our organization. The framework consists of two parts: (1) principles\nfor developing customized safety risk taxonomies, and (2) practices for\nevaluating safety risks in LLM applications. We illustrate how the proposed\nframework was applied in our internal pilot, providing a reference point for\norganizations seeking to scale their safety testing efforts. This work aims to\nbridge the gap between theoretical concepts in AI safety and the operational\nrealities of safeguarding LLM applications in practice, offering actionable\nguidance for safe and scalable deployment.", "AI": {"tldr": "This paper proposes a framework for evaluating application-level safety in LLM systems through organizational deployment case studies, bridging theoretical AI safety concepts with operational practices.", "motivation": "Foundation model-focused safety evaluations neglect application-specific risks introduced by system prompts, retrieval pipelines, and guardrails, creating a need for practical application-level safety assessment methods.", "method": "The framework combines (1) principles for developing customized safety risk taxonomies, (2) application-level safety evaluation practices, and (3) real-world deployment validation across multiple use cases.", "result": "Demonstrated framework application in internal pilots provides scalable safety testing reference points for organizations implementing LLM systems.", "conclusion": "The work establishes actionable guidelines for operational LLM safety evaluation, addressing the gap between theoretical AI safety research and practical application deployment requirements."}}
{"id": "2507.10267", "categories": ["cs.CR", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.10267", "abs": "https://arxiv.org/abs/2507.10267", "authors": ["Novruz Amirov", "Baran Isik", "Bilal Ihsan Tuncer", "Serif Bahtiyar"], "title": "DNS Tunneling: Threat Landscape and Improved Detection Solutions", "comment": null, "summary": "Detecting Domain Name System (DNS) tunneling is a significant challenge in\nsecurity due to its capacity to hide harmful actions within DNS traffic that\nappears to be normal and legitimate. Traditional detection methods are based on\nrule-based approaches or signature matching methods that are often insufficient\nto accurately identify such covert communication channels. This research is\nabout effectively detecting DNS tunneling. We propose a novel approach to\ndetect DNS tunneling with machine learning algorithms. We combine machine\nlearning algorithms to analyze the traffic by using features extracted from DNS\ntraffic. Analyses results show that the proposed approach is a good candidate\nto detect DNS tunneling accurately.", "AI": {"tldr": "Proposes a machine learning-based method for accurate DNS tunneling detection by analyzing extracted DNS traffic features.", "motivation": "DNS tunneling exploits normal DNS traffic for covert communication, making it hard to distinguish from legitimate traffic using traditional rule-based/sig-matching methods. Effective detection is critical for security.", "method": "A novel approach using machine learning algorithms with features extracted from DNS traffic to classify and identify tunneling patterns.", "result": "Experimental analysis demonstrates the proposed method achieves high accuracy in detecting DNS tunneling compared to conventional techniques.", "conclusion": "Combining ML algorithms with DNS traffic feature extraction effectively mitigates tunneling threats that evade signature-based detection methods."}}
{"id": "2507.09866", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09866", "abs": "https://arxiv.org/abs/2507.09866", "authors": ["Wei Zhang", "Jian Yang", "Jiaxi Yang", "Ya Wang", "Zhoujun Li", "Zeyu Cui", "Binyuan Hui", "Junyang Lin"], "title": "Turning the Tide: Repository-based Code Reflection", "comment": null, "summary": "Code large language models (LLMs) enhance programming by understanding and\ngenerating code across languages, offering intelligent feedback, bug detection,\nand code updates through reflection, improving development efficiency and\naccessibility. While benchmarks (e.g. HumanEval/LiveCodeBench) evaluate code\ngeneration and real-world relevance, previous works ignore the scenario of\nmodifying code in repositories. Considering challenges remaining in improving\nreflection capabilities and avoiding data contamination in dynamic benchmarks,\nwe introduce LiveRepoReflection, a challenging benchmark for evaluating code\nunderstanding and generation in multi-file repository contexts, featuring 1,888\nrigorously filtered test cases across $6$ programming languages to ensure\ndiversity, correctness, and high difficulty. Further, we create\nRepoReflection-Instruct, a large-scale, quality-filtered instruction-tuning\ndataset derived from diverse sources, used to train RepoReflectionCoder through\na two-turn dialogue process involving code generation and error-driven repair.\nThe leaderboard evaluates over 40 LLMs to reflect the model performance of\nrepository-based code reflection.", "AI": {"tldr": "The paper introduces LiveRepoReflection, a benchmark for evaluating code understanding and generation in multi-file repositories, and proposes an instruction-tuning dataset and training method for improving code reflection capabilities in LLMs.", "motivation": "Existing benchmarks fail to address code modification in real-world repositories and overlook challenges in reflection capabilities and data contamination, necessitating a focused evaluation framework for practical code generation scenarios.", "method": "LiveRepoReflection contains 1,888 multi-language test cases, while RepoReflection-Instruct uses a two-turn dialogue with error-driven repair to train models. Code generation and reflection are evaluated via a dynamic benchmarking process.", "result": "40+ LLMs were benchmarked on LiveRepoReflection, showing significant performance gaps with 34.8% as the top score. The dataset and toolchain demonstrate high effectiveness in modeling repository-level code reflection.", "conclusion": "By highlighting the critical need for improved code reflection in repositories, the work establishes new standards for training and evaluating LLMs with practical coding applications, driving progress toward real-world software development scenarios."}}
{"id": "2507.10457", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10457", "abs": "https://arxiv.org/abs/2507.10457", "authors": ["Hammad Atta", "Ken Huang", "Manish Bhatt", "Kamal Ahmed", "Muhammad Aziz Ul Haq", "Yasir Mehmood"], "title": "Logic layer Prompt Control Injection (LPCI): A Novel Security Vulnerability Class in Agentic Systems", "comment": null, "summary": "The integration of large language models (LLMs) into enterprise systems has\ncreated a new class of covert security vulnerabilities, particularly within\nlogic-execution layers and persistent-memory contexts. In this paper, we\nintroduce Logic-Layer Prompt Control Injection (LPCI), a novel attack category\nin which encoded, delayed, and conditionally triggered payloads are embedded in\nmemory, vector stores, or tool outputs. These payloads can bypass conventional\ninput filters and trigger unauthorised behaviour across sessions.", "AI": {"tldr": "This paper introduces Logic-Layer Prompt Control Injection (LPCI), a novel enterprise security vulnerability where encoded and conditionally triggered payloads are embedded in persistent memory or tool outputs to bypass input filters and cause unauthorized behavior across sessions.", "motivation": "The integration of large language models (LLMs) into enterprise systems raises new security risks due to their complex execution and persistent memory management, which can harbor covert vulnerabilities.", "method": "The authors demonstrate how encoded payloads can be hidden in memory, vector stores, or tool outputs, using delay and conditional triggers to execute unauthorized actions when traditional input filters are evaded.", "result": "LPCI attacks are shown to bypass conventional defenses, enabling persistence and cross-session unauthorized behavior in enterprise LLM systems.", "conclusion": "The paper highlights the need for enhanced security protocols for enterprise LLM integrations, emphasizing multi-phase validation and monitoring of persistent memory and execution contexts to detect and mitigate LPCI-style attacks."}}
{"id": "2507.09892", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2507.09892", "abs": "https://arxiv.org/abs/2507.09892", "authors": ["Zimu Chen", "Di Wang"], "title": "PathFuzzing: Worst Case Analysis by Fuzzing Symbolic-Execution Paths", "comment": "10 pages, 1 figure", "summary": "Estimating worst-case resource consumption is a critical task in software\ndevelopment. The worst-case analysis (WCA) problem is an optimization-based\nabstraction of this task. Fuzzing and symbolic execution are widely used\ntechniques for addressing the WCA problem. However, improving code coverage in\nfuzzing or managing path explosion in symbolic execution within the context of\nWCA poses significant challenges. In this paper, we propose PathFuzzing, aiming\nto combine the strengths of both techniques to design a WCA method. The key\nidea is to transform a program into a symbolic one that takes an execution path\n(encoded as a binary string) and interprets the bits as branch decisions.\nPathFuzzing then applies evolutionary fuzzing techniques to the transformed\nprogram to search for binary strings that represent satisfiable path conditions\nand lead to high resource consumption. We evaluate the performance of\nPathFuzzing experimentally on a benchmark suite that consists of prior work's\nbenchmarks and some added by us. Results show that PathFuzzing generally\noutperforms a fuzzing and a symbolic-execution baseline.", "AI": {"tldr": "PathFuzzing combines fuzzing and symbolic execution to improve worst-case resource consumption estimation by searching for high-consumption paths using evolutionary algorithms.", "motivation": "Traditional fuzzing struggles with code coverage while symbolic execution faces path explosion during worst-case analysis (WCA). This paper addresses the limitations of both approaches.", "method": "Translates programs into symbolic representations where inputs are binary-encoded execution paths. Evolutionary fuzzing is then applied to search for binary strings satisfying path conditions and maximizing resource consumption.", "result": "Experiments on a benchmark suite (including prior work and custom additions) demonstrate PathFuzzing outperforms both fuzzing and symbolic execution baselines in WCA.", "conclusion": "PathFuzzing provides an effective hybrid methodology for WCA by integrating strengths of symbolically guided fuzzing, achieving superior performance over existing techniques."}}
{"id": "2507.10489", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.10489", "abs": "https://arxiv.org/abs/2507.10489", "authors": ["Eduardo Brito", "Mahmoud Shoush", "Kristian Tamm", "Paula Etti", "Liina Kamm"], "title": "SynthGuard: Redefining Synthetic Data Generation with a Scalable and Privacy-Preserving Workflow Framework", "comment": "This is the extended version of the paper to appear in the\n  Proceedings of the 1st International Workshop on Responsible Data Governance,\n  Privacy, and Digital Transformation (RDGPT 2025), held in conjunction with\n  the 20th International Conference on Availability, Reliability and Security\n  (ARES 2025)", "summary": "The growing reliance on data-driven applications in sectors such as\nhealthcare, finance, and law enforcement underscores the need for secure,\nprivacy-preserving, and scalable mechanisms for data generation and sharing.\nSynthetic data generation (SDG) has emerged as a promising approach but often\nrelies on centralized or external processing, raising concerns about data\nsovereignty, domain ownership, and compliance with evolving regulatory\nstandards. To overcome these issues, we introduce SynthGuard, a framework\ndesigned to ensure computational governance by enabling data owners to maintain\ncontrol over SDG workflows. SynthGuard supports modular and privacy-preserving\nworkflows, ensuring secure, auditable, and reproducible execution across\ndiverse environments. In this paper, we demonstrate how SynthGuard addresses\nthe complexities at the intersection of domain-specific needs and scalable SDG\nby aligning with requirements for data sovereignty and regulatory compliance.\nDeveloped iteratively with domain expert input, SynthGuard has been validated\nthrough real-world use cases, demonstrating its ability to balance security,\nprivacy, and scalability while ensuring compliance. The evaluation confirms its\neffectiveness in implementing and executing SDG workflows and integrating\nprivacy and utility assessments across various computational environments.", "AI": {"tldr": "SynthGuard is a framework enabling secure, privacy-preserving, and scalable synthetic data generation (SDG) workflows with computational governance, allowing data owners to maintain control while addressing compliance and data sovereignty challenges.", "motivation": "Current SDG approaches often rely on centralized or external processing, violating data sovereignty, domain ownership principles, and creating regulatory compliance risks in data-sensitive fields like healthcare, finance, and law enforcement.", "method": "The authors developed SynthGuard through iterative collaboration with domain experts, implementing modular workflows that ensure privacy, security, auditability, and reproducibility. It supports cross-environment execution and integrates privacy/utility assessments during SDG.", "result": "SynthGuard successfully balances security, privacy, and scalability in real-world use cases, demonstrating effective SDG workflow execution and compliance with domain-specific and regulatory requirements across diverse computational environments.", "conclusion": "SynthGuard offers validated governance for SDG that aligns with data sovereignty and compliance needs through modular design, expert collaboration, and integrated assessments, ensuring secure and auditable synthetic data creation."}}
{"id": "2507.09907", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09907", "abs": "https://arxiv.org/abs/2507.09907", "authors": ["Thomas Hansper", "Kevin Phong Pham", "Michael Neumann"], "title": "Modelling Interrelations Between Agile Practices: The Agile Map", "comment": null, "summary": "Agile methods are defined through guidelines comprising various practices\nintended to enable agile ways of working. These guidelines further comprise a\nspecific set of agile practices aiming to enable teams for an agile way of\nworking. However, due to its wide-spread use in practice we know that agile\npractices are adopted and tailored intensively, which lead to a high variety of\nagile practices in terms of their level of detail. Problem: A high variety of\nagile practices can be challenging as we do not know how different agile\npractices are interrelated with each other. To be more precise, tailoring and\nadopting agile practices may lead to the challenge, that the combinatorial use\nof several agile practices can only be successful to a limited extent, as\npractices support or even require each other for a effective use in practice.\nObjective: Our study aims to provide an enabler for this problem. We want to\nidentify interrelations between agile practices and describe them in a\nsystematic manner. Contribution: The core contribution of this paper is the\nAgile Map, a theoretical model describing relations between agile practices\nfollowing a systematic approach aiming to provide an overview of coherences\nbetween agile practices. The model aims to support practitioners in selecting\nand combining agile practices in a meaningful way.", "AI": {"tldr": "This paper introduces the Agile Map, a theoretical model to identify and systematize interrelations between agile practices, helping practitioners combine them effectively.", "motivation": "The widespread adoption and tailoring of agile practices have led to a diverse set of practices, but their interrelations are not well understood, limiting effective combinatorial use.", "method": "The study systematically identifies interrelations between agile practices, focusing on their dependencies and synergies to establish a coherent model.", "result": "The Agile Map is developed as a structured framework depicting relationships between agile practices across their spectrum of detail and application.", "conclusion": "The Agile Map provides an overview of practice coherences, enabling practitioners to select and combine agile methods in a theoretically grounded and meaningful manner."}}
{"id": "2507.10491", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.10491", "abs": "https://arxiv.org/abs/2507.10491", "authors": ["Yanghao Su", "Jie Zhang", "Yiming Li", "Tianwei Zhang", "Qing Guo", "Weiming Zhang", "Nenghai Yu", "Nils Lukas", "Wenbo Zhou"], "title": "BURN: Backdoor Unlearning via Adversarial Boundary Analysis", "comment": null, "summary": "Backdoor unlearning aims to remove backdoor-related information while\npreserving the model's original functionality. However, existing unlearning\nmethods mainly focus on recovering trigger patterns but fail to restore the\ncorrect semantic labels of poison samples. This limitation prevents them from\nfully eliminating the false correlation between the trigger pattern and the\ntarget label. To address this, we leverage boundary adversarial attack\ntechniques, revealing two key observations. First, poison samples exhibit\nsignificantly greater distances from decision boundaries compared to clean\nsamples, indicating they require larger adversarial perturbations to change\ntheir predictions. Second, while adversarial predicted labels for clean samples\nare uniformly distributed, those for poison samples tend to revert to their\noriginal correct labels. Moreover, the features of poison samples restore to\nclosely resemble those of corresponding clean samples after adding adversarial\nperturbations. Building upon these insights, we propose Backdoor Unlearning via\nadversaRial bouNdary analysis (BURN), a novel defense framework that integrates\nfalse correlation decoupling, progressive data refinement, and model\npurification. In the first phase, BURN employs adversarial boundary analysis to\ndetect poisoned samples based on their abnormal adversarial boundary distances,\nthen restores their correct semantic labels for fine-tuning. In the second\nphase, it employs a feedback mechanism that tracks prediction discrepancies\nbetween the original backdoored model and progressively sanitized models,\nguiding both dataset refinement and model purification. Extensive evaluations\nacross multiple datasets, architectures, and seven diverse backdoor attack\ntypes confirm that BURN effectively removes backdoor threats while maintaining\nthe model's original performance.", "AI": {"tldr": "The paper introduces BURN, a backdoor unlearning framework that addresses the limitations of existing methods by restoring correct semantic labels and using adversarial boundary analysis, validated across various datasets and attacks.", "motivation": "Existing unlearning methods fail to restore the correct semantic labels of poisoned samples, leaving false correlations between trigger patterns and target labels unresolved. This hinders effective backdoor removal while preserving original model functionality.", "method": "BURN utilizes boundary adversarial attack techniques to identify poisoned samples through their distinct adversarial boundary distances and label distributions. It involves two phases: (1) detecting and fine-tuning poisoned samples to restore correct labels, and (2) a feedback mechanism tracking prediction discrepancies to iteratively refine the dataset and purify the model.", "result": "BURN successfully eliminates backdoor threats across multiple datasets, architectures, and seven backdoor attack types, maintaining the model's original performance as demonstrated through extensive experiments.", "conclusion": "BURN provides an effective defense against backdoor attacks by combining false correlation decoupling, progressive data refinement, and model purification, validated by robust experimental results showing its efficacy and minimal performance loss."}}
{"id": "2507.09911", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09911", "abs": "https://arxiv.org/abs/2507.09911", "authors": ["Marvin Auf der Landwehr", "Julia Topp", "Michael Neumann"], "title": "When Less is More: A systematic review of four-day workweek conceptualizations and their effects on organizational performance", "comment": null, "summary": "Context: Agile IT organizations, which are characterized by self-organization\nand collaborative social interactions, require motivating, efficient and\nflexible work environments to maximize value creation. Compressed work\nschedules such as the four-day workweek have evolved into multiple facets over\nthe last decades and are associated with various benefits for organizations and\ntheir employees. Objective: Our objective in this study is to deepen our\ncomprehension of the impact of compressed work schedules on the operational\nefficacy of IT enterprises, while concurrently developing a comprehensive\nframework delineating the intricacies of compressed work schedules.Method: We\nconducted a systematic review of available conceptualizations related to\nfour-day workweek schedules and elaborate on their organizational and social\neffects. To cover scientific and practice-oriented literature, our review\ncombined a systematic literature review and a web content analysis. Results:\nBased on the generated insights, we derive a meta-framework that matches\nconceptualizations and effects, finally guiding the adoption of compressed work\nschedules based on individual managerial prerequisites and circumstances.", "AI": {"tldr": "This study explores the impact of compressed work schedules on IT organizations and develops a comprehensive framework to guide their adoption based on a systematic review and web analysis.", "motivation": "To deepen understanding of how compressed work schedules affect IT enterprises' operational efficacy and provide a framework for effective adoption, considering evolving work practices.", "method": "Systematic literature review combined with web content analysis to integrate scientific and practical insights on four-day workweek schedules.", "result": "A meta-framework aligning conceptualizations of compressed schedules with their organizational and social effects, enabling tailored implementation based on managerial needs.", "conclusion": "The study concludes that compressed work schedules can enhance operational efficacy in IT organizations when implemented through a context-specific framework addressing key prerequisites and circumstances."}}
{"id": "2507.10103", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.10103", "abs": "https://arxiv.org/abs/2507.10103", "authors": ["Hanyang Guo", "Xiaoheng Xie", "Hong-Ning Dai", "Peng Di", "Yu Zhang", "Bishenghui Tao", "Zibin Zheng"], "title": "Accelerating Automatic Program Repair with Dual Retrieval-Augmented Fine-Tuning and Patch Generation on Large Language Models", "comment": null, "summary": "Automated Program Repair (APR) is essential for ensuring software reliability\nand quality while enhancing efficiency and reducing developers' workload.\nAlthough rule-based and learning-based APR methods have demonstrated their\neffectiveness, their performance was constrained by the defect type of repair,\nthe quality of training data, and the size of model parameters. Recently, Large\nLanguage Models (LLMs) combined with Retrieval-Augmented-Generation (RAG) have\nbeen increasingly adopted in APR tasks. However, current code LLMs and RAG\ndesigns neither fully address code repair tasks nor consider code-specific\nfeatures. To overcome these limitations, we propose SelRepair, a novel APR\napproach with integration of a fine-tuned LLM with a newly-designed dual RAG\nmodule. This approach uses a bug-fix pair dataset for fine-tuning and\nincorporates semantic and syntactic/structural similarity information through\nan RAG selection gate. This design ensures relevant information is retrieved\nefficiently, thereby reducing token length and inference time. Evaluations on\nJava datasets show SelRepair outperforms other APR methods, achieving 26.29%\nand 17.64% in terms of exact match (EM) on different datasets while reducing\ninference time by at least 6.42% with controlled input lengths.", "AI": {"tldr": "SelRepair is a novel Automated Program Repair (APR) approach combining a fine-tuned LLM with a dual RAG module, achieving higher repair accuracy and efficiency on Java datasets.", "motivation": "Existing APR methods face limitations in defect type handling, training data quality, and model parameter size, while current code LLMs and RAG designs underutilize code-specific features in repair tasks.", "method": "SelRepair integrates a fine-tuned LLM using a bug-fix pair dataset with a dual RAG module that incorporates semantic and syntactic/structural similarity through an RAG selection gate, reducing input length and inference costs.", "result": "It achieves 26.29% and 17.64% exact match scores on Java datasets, while reducing inference time by \u22656.42% compared to existing APR methods.", "conclusion": "SelRepair addresses prior APR limitations through its dual RAG integration strategy, demonstrating superior repair effectiveness and computational efficiency."}}
{"id": "2507.10054", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10054", "abs": "https://arxiv.org/abs/2507.10054", "authors": ["Emir Bosnak", "Sahand Moslemi", "Mayasah Lami", "Anil Koyuncu"], "title": "Explicit Vulnerability Generation with LLMs: An Investigation Beyond Adversarial Attacks", "comment": "Accepted to ICSME 2025", "summary": "Large Language Models (LLMs) are increasingly used as code assistants, yet\ntheir behavior when explicitly asked to generate insecure code remains poorly\nunderstood. While prior research has focused on unintended vulnerabilities or\nadversarial prompting techniques, this study examines a more direct threat\nscenario: open-source LLMs generating vulnerable code when prompted either\ndirectly or indirectly. We propose a dual experimental design: (1) Dynamic\nPrompting, which systematically varies vulnerability type, user persona, and\ndirectness across structured templates; and (2) Reverse Prompting, which\nderives prompts from real vulnerable code samples to assess vulnerability\nreproduction accuracy. We evaluate three open-source 7B-parameter models\n(Qwen2, Mistral, and Gemma) using ESBMC static analysis to assess both the\npresence of vulnerabilities and the correctness of the generated vulnerability\ntype. Results show all models frequently produce vulnerable outputs, with Qwen2\nachieving highest correctness rates. User persona significantly affects\nsuccess, where student personas achieved higher vulnerability rates than\nprofessional roles, while direct prompts were marginally more effective.\nVulnerability reproduction followed an inverted-U pattern with cyclomatic\ncomplexity, peaking at moderate ranges. Our findings expose limitations of\nsafety mechanisms in open-source models, particularly for seemingly benign\neducational requests.", "AI": {"tldr": "This paper investigates whether open-source Large Language Models (LLMs) will generate insecure code when directly or indirectly prompted to do so.", "motivation": "Existing research has focused on unintended vulnerabilities or adversarial prompting techniques, but this study examines a more direct threat scenario with practical implications for using LLMs as code assistants.", "method": "The research uses a dual experimental design involving (1) Dynamic Prompting with structured templates varying vulnerability types, user personas, and directness; and (2) Reverse Prompting by deriving prompts from real vulnerable code samples. Three open-source models (Qwen2, Mistral, Gemma) are evaluated using ESBMC static analysis.", "result": "All models frequently produce vulnerable outputs, with Qwen2 showing highest correctness. Student personas achieve higher vulnerability rates than professional roles, and vulnerability reproduction demonstrates an inverted-U relationship with code complexity.", "conclusion": "The findings highlight inadequate safety mechanisms in open-source LLMs, particularly vulnerability to educational/research role-play prompts, urging improved safeguards for responsible code generation."}}
{"id": "2507.10062", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10062", "abs": "https://arxiv.org/abs/2507.10062", "authors": ["Erg\u00fcn Batuhan Kaynak", "Mayasah Lami", "Sahand Moslemi", "Anil Koyuncu"], "title": "LLMShot: Reducing snapshot testing maintenance via LLMs", "comment": "Accepted to ICSME 2025", "summary": "Snapshot testing has emerged as a critical technique for UI validation in\nmodern software development, yet it suffers from substantial maintenance\noverhead due to frequent UI changes causing test failures that require manual\ninspection to distinguish between genuine regressions and intentional design\nchanges. This manual triage process becomes increasingly burdensome as\napplications evolve, creating a need for automated analysis solutions. This\npaper introduces LLMShot, a novel framework that leverages vision-based Large\nLanguage Models to automatically analyze snapshot test failures through\nhierarchical classification of UI changes. To evaluate LLMShot's effectiveness,\nwe developed a comprehensive dataset using a feature-rich iOS application with\nconfigurable feature flags, creating realistic scenarios that produce authentic\nsnapshot differences representative of real development workflows. Our\nevaluation using Gemma3 models demonstrates strong classification performance,\nwith the 12B variant achieving over 84% recall in identifying failure root\ncauses while the 4B model offers practical deployment advantages with\nacceptable performance for continuous integration environments. However, our\nexploration of selective ignore mechanisms revealed significant limitations in\ncurrent prompting-based approaches for controllable visual reasoning. LLMShot\nrepresents the first automated approach to semantic snapshot test analysis,\noffering developers structured insights that can substantially reduce manual\ntriage effort and advance toward more intelligent UI testing paradigms.", "AI": {"tldr": "This paper proposes LLMShot, a vision-based LLM framework for automated snapshot test failure analysis, reducing manual triage effort through hierarchical classification of UI changes in a feature-rich iOS application dataset.", "motivation": "Modern software UIs face high maintenance costs due to frequent design changes causing snapshot test failures that require manual effort to classify as genuine regressions or intentional modifications.", "method": "Developed LLMShot using Gemma3 vision-based LLM models, implemented hierarchical classification for UI change analysis, and created a realistic iOS application dataset with configurable feature flags to produce authentic snapshot differences.", "result": "12B Gemma3 variant achieved 84+ recall in failure root-cause identification, 4B variant shows practical CI integration potential, but prompting-based selective ignores demonstrated significant limitations in controllable visual reasoning.", "conclusion": "LLMShot pioneers automated semantic analysis for snapshot testing, offering structured failure insights that reduce manual work requirements while highlighting current research gaps in controllable visual reasoning for UI validation."}}
{"id": "2507.10182", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10182", "abs": "https://arxiv.org/abs/2507.10182", "authors": ["Gehao Zhang", "Zhenting Wang", "Juan Zhai"], "title": "Breaking the Myth: Can Small Models Infer Postconditions Too?", "comment": null, "summary": "Formal specifications are essential for ensuring software correctness, yet\nmanually writing them is tedious and error-prone. Large Language Models (LLMs)\nhave shown promise in generating such specifications from natural language\nintents, but the giant model size and high computational demands raise a\nfundamental question: Do we really need large models for this task? In this\npaper, we show that a small, fine-tuned language model can achieve high-quality\npostcondition generation with much lower computational costs. We construct a\nspecialized dataset of prompts, reasoning logs, and postconditions, then\nsupervise the fine-tuning of a $7$B-parameter code model. Our approach tackles\nreal-world repository dependencies and preserves pre-state information,\nallowing for expressive and accurate specifications. We evaluate the model on a\nbenchmark of real-world Java bugs (Defects4J) and compare against both\nproprietary giants (e.g., GPT-4o) and open-source large models. Empirical\nresults demonstrate that our compact model matches or outperforms significantly\nlarger counterparts in syntax correctness, semantic correctness, and\nbug-distinguishing capability. These findings highlight that targeted\nfine-tuning on a modest dataset can enable small models to achieve results\nformerly seen only in massive, resource-heavy LLMs, offering a practical and\nefficient path for the real-world adoption of automated specification\ngeneration.", "AI": {"tldr": "The paper demonstrates that a small, fine-tuned 7B-parameter code model (TinySpec) can generate high-quality postconditions for Java bugs with lower computational costs than large models like GPT-4o, by leveraging a specialized dataset of prompts, reasoning logs, and postconditions, achieving comparable syntax correctness, stronger semantic correctness, and equal/exceeding bug-distinguishing capabilities.", "motivation": "Formal specifications are essential for software correctness but challenging to write manually. While LLMs show promise, their large model sizes and high computational demands hinder real-world adoption, prompting the question: Can smaller models achieve similar results through targeted training?", "method": "1. Constructed a specialized dataset with prompts, reasoning logs, and postconditions \n2. Fine-tuned a 7B-parameter code model using the dataset \n3. Designed the model to handle repository dependencies and preserve pre-state information \n4. Evaluated on Defects4J (real-world Java bugs), comparing against GPT-4o and open-source large models", "result": "The compact model matches or exceeds large models in three key metrics:\n1. Syntax correctness \n2. Semantic correctness \n3. Bug-distinguishing capability\n\nSpecifically, it demonstrates stronger semantic correctness than all large model baselines while maintaining low computational costs.", "conclusion": "Targeted fine-tuning on domain-specific datasets enables small models to deliver specification generation results matching or surpassing large models, providing a practical, resource-efficient solution for software verification by reducing the need for massive computational infrastructure."}}
{"id": "2507.10228", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10228", "abs": "https://arxiv.org/abs/2507.10228", "authors": ["Hugo Villamizar", "Daniel Mendez", "Marcos Kalinowski"], "title": "Towards a Framework for Operationalizing the Specification of Trustworthy AI Requirements", "comment": "This paper has been accepted for presentation at the 2025 IEEE 33rd\n  International Requirements Engineering Conference Workshops (REW-RETRAI 2025)", "summary": "Growing concerns around the trustworthiness of AI-enabled systems highlight\nthe role of requirements engineering (RE) in addressing emergent,\ncontext-dependent properties that are difficult to specify without structured\napproaches. In this short vision paper, we propose the integration of two\ncomplementary approaches: AMDiRE, an artefact-based approach for RE, and\nPerSpecML, a perspective-based method designed to support the elicitation,\nanalysis, and specification of machine learning (ML)-enabled systems. AMDiRE\nprovides a structured, artefact-centric, process-agnostic methodology and\ntemplates that promote consistency and traceability in the results; however, it\nis primarily oriented toward deterministic systems. PerSpecML, in turn,\nintroduces multi-perspective guidance to uncover concerns arising from the\ndata-driven and non-deterministic behavior of ML-enabled systems. We envision a\npathway to operationalize trustworthiness-related requirements, bridging\nstakeholder-driven concerns and structured artefact models. We conclude by\noutlining key research directions and open challenges to be discussed with the\nRE community.", "AI": {"tldr": "This paper proposes integrating AMDiRE (artefact-based RE methodology) and PerSpecML (multi-perspective ML requirements approach) to address trustworthiness challenges in AI systems by combining structured artifact modeling with contextual ML concerns.", "motivation": "AI system trustworthiness requires structured approaches to capture emergent, context-dependent properties that traditional requirements engineering (RE) methods struggle with, particularly for data-driven and non-deterministic machine learning (ML) systems.", "method": "The paper combines AMDiRE's structured, artifact-centric process with PerSpecML's multi-perspective guidance, utilizing templates for consistency while addressing ML-specific challenges like data-driven behavior and stakeholder concerns through complementary approaches.", "result": "The integration provides a unified pathway to operationalize trustworthiness requirements in ML systems by systematically linking stakeholder-driven concerns to structured artifact models while maintaining traceability.", "conclusion": "The authors highlight the need for further research on bridging stakeholder concerns with structured ML requirements, including addressing open challenges related to operationalizing this integration within the RE community."}}
{"id": "2507.10235", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10235", "abs": "https://arxiv.org/abs/2507.10235", "authors": ["Zhixiang Chen", "Zhuangbin Chen", "Xingjie Cai", "Wei Li", "Zibin Zheng"], "title": "An Empirical Study of Interaction Bugs in ROS-based Software", "comment": null, "summary": "Modern robotic systems integrate multiple independent software and hardware\ncomponents, each responsible for distinct functionalities such as perception,\ndecision-making, and execution. These components interact extensively to\naccomplish complex end-to-end tasks. As a result, the overall system\nreliability depends not only on the correctness of individual components, but\nalso on the correctness of their interactions. Failures often manifest at the\nboundaries between components, yet interaction-related reliability issues in\nrobotics--referred to here as interaction bugs (iBugs)--remain underexplored.\n  This work presents an empirical study of iBugs within robotic systems built\nusing the Robot Operating System (ROS), a widely adopted open-source robotics\nframework. A total of 121 iBugs were analyzed across ten actively maintained\nand representative ROS projects. The identified iBugs are categorized into\nthree major types: intra-system iBugs, hardware iBugs, and environmental iBugs,\ncovering a broad range of interaction scenarios in robotics. The analysis\nincludes an examination of root causes, fixing strategies, and the impact of\nthese bugs. Several findingsa are derived that shed light on the nature of\niBugs and suggest directions for improving their prevention and detection.\nThese insights aim to inform the design of more robust and safer robotic\nsystems.", "AI": {"tldr": "This paper studies interaction bugs (iBugs) in robotic systems using an empirical analysis of 121 iBugs across 10 ROS projects, categorizing them into three types and exploring their root causes, fixes, and implications for system reliability.", "motivation": "Robotic systems depend on complex interactions between components, but reliability issues at these boundaries (iBugs) are underexplored despite their critical impact on system safety and robustness.", "method": "Empirical analysis of iBugs in 10 actively maintained ROS projects through root cause examination, categorization into intra-system, hardware, and environmental types, and evaluation of fixing strategies.", "result": "Identified 121 iBugs classified into three major categories with insights into their root causes, fixes, and impact, providing actionable strategies for prevention and detection within ROS-based systems.", "conclusion": "The study highlights iBugs as a critical challenge in robotics, offering empirical evidence and direction for improving reliability through targeted prevention, detection mechanisms, and system design enhancements."}}
{"id": "2507.10244", "categories": ["cs.SE", "D.2.2; D.2.11"], "pdf": "https://arxiv.org/pdf/2507.10244", "abs": "https://arxiv.org/abs/2507.10244", "authors": ["Adam \u0160t\u011bp\u00e1nek", "David Ku\u0165\u00e1k", "Barbora Kozl\u00edkov\u00e1", "Jan By\u0161ka"], "title": "Helveg: Diagrams for Software Documentation", "comment": "13 pages, 5 figures, accepted by TVCG", "summary": "Software developers often have to gain an understanding of a codebase. Be it\nprogrammers getting onboarded onto a team project or, for example, developers\nstriving to grasp an external open-source library. In either case, they\nfrequently turn to the project's documentation. However, documentation in its\ntraditional textual form is ill-suited for this kind of high-level exploratory\nanalysis, since it is immutable from the readers' perspective and thus forces\nthem to follow a predefined path. We have designed an approach bringing aspects\nof software architecture visualization to API reference documentation. It\nutilizes a highly interactive node-link diagram with expressive node glyphs and\nflexible filtering capabilities, providing a high-level overview of the\ncodebase as well as details on demand. To test our design, we have implemented\na prototype named Helveg, capable of automatically generating diagrams of C\\#\ncodebases. User testing of Helveg confirmed its potential, but it also revealed\nproblems with the readability, intuitiveness, and user experience of our tool.\nTherefore, in this paper, which is an extended version of our VISSOFT paper\nwith DOI 10.1109/VISSOFT64034.2024.00012, we address many of these problems\nthrough major changes to the glyph design, means of interaction, and user\ninterface of the tool. To assess the improvements, this new version of Helveg\nwas evaluated again with the same group of participants as the previous\nversion.", "AI": {"tldr": "This paper refines an interactive visualization tool called Helveg for C# codebases, addressing usability issues identified in previous user testing to improve understanding of APIs through dynamic visual exploration.", "motivation": "Traditional textual API documentation forces developers to follow linear paths, making high-level codebase exploration inefficient. The paper aims to create an intuitive, interactive visual tool that provides both overview and detail.", "method": "The authors redesigned Helveg by overhauling glyph design, interaction mechanisms, and UI layout based on initial user feedback. The improved version was evaluated through user testing with the same original participants.", "result": "User testing revealed significant usability problems in the original Helveg, prompting substantial revisions. The updated version's evaluation showed improvements, though specific metrics are not detailed in the abstract.", "conclusion": "The modified Helveg demonstrates the viability of interactive node-link diagrams for API documentation, offering better readability and user experience. This work contributes to more effective codebase exploration tools for developers."}}
{"id": "2507.10305", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10305", "abs": "https://arxiv.org/abs/2507.10305", "authors": ["Linus Ververs", "Trang Linh Lam", "Janina Berger", "Lutz Prechelt"], "title": "A Grounded Theory on the Teacher and Student Roles in Pair Programming", "comment": null, "summary": "Context: Pair programming is an established (agile) practice and is practiced\nthroughout the industry. Objective: Understand under what circumstances\nknowledge transfer can harm a pair programming session. Method: Grounded Theory\nMethodology based on 17 recorded pair programming sessions with 18 developers\nfrom 5 German software companies accompanied, by 6 interviews with different\ndevelopers from 4 other German companies. Results: We define the student and\nteacher roles to help developers deal with a one-sided knowledge gap. We\ndescribe pitfalls to avoid and develop a grounded theory centered around the\nPower Gap in pair programming. Conclusions: Knowledge transfer can be harmful\nwhen developers don't pay attention to their partners needs and desires. If\ndevelopers don't pay attention to the Power Gap and keep it in check, Defensive\nBehavior may arise that leads to a vicious cycle impacting the knowledge\ntransfer, the Togetherness and the code quality in a negative way.", "AI": {"tldr": "This study examines how knowledge transfer can harm pair programming sessions through a 'Power Gap' dynamic, identifying defensive behaviors that create harmful cycles affecting collaboration and code quality.", "motivation": "While pair programming is widely adopted, existing research has not clarified situations where knowledge transfer becomes counterproductive, particularly when one-sided knowledge imbalances exist between developers.", "method": "Grounded Theory analysis of 17 recorded pair programming sessions involving 18 developers from 5 German software companies, complemented by 6 interviews with developers from 4 other German companies.", "result": "Proposed student/teacher role definitions for managing knowledge gaps, identified specific pitfalls, and formulated a grounded theory explaining how unchecked Power Gaps lead to defensive behaviors creating negative feedback loops.", "conclusion": "Effective knowledge transfer in pair programming requires attention to partners' psychological needs through awareness of the Power Gap, which when neglected causes defensive behaviors that simultaneously impair knowledge sharing, team cohesion, and code quality outcomes."}}
{"id": "2507.10321", "categories": ["cs.SE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10321", "abs": "https://arxiv.org/abs/2507.10321", "authors": ["Viktor Sinitsyn", "Nils Schlautmann", "Florian Schwaiger", "Florian Holzapfel"], "title": "Streamlined Airborne Software Development for Large UAVs: From Unified Data Collection to Automated Code Generation", "comment": null, "summary": "The aerospace industry has experienced significant transformations over the\nlast decade, driven by technological advancements and innovative solutions in\ngoods and personal transportation. This evolution has spurred the emergence of\nnumerous start-ups that now face challenges traditionally encountered by\nestablished aerospace companies. Among these challenges is the efficient\nprocessing of digital intra-device communication interfaces for onboard\nequipment - a critical component for ensuring seamless system integration and\nfunctionality. Addressing this challenge requires solutions that emphasize\nclear and consistent interface descriptions, automation of processes, and\nreduced labor-intensive efforts.\n  This paper presents a novel process and toolchain designed to streamline the\ndevelopment of digital interfaces and onboard software, which our team has\nsuccessfully applied in several completed projects. The proposed approach\nfocuses on automation and flexibility while maintaining compliance with design\nassurance requirements.", "AI": {"tldr": "A paper introducing an automated toolchain for streamlining aerospace onboard digital interface development, addressing industry transformation and startup challenges.", "motivation": "The aerospace industry's rapid technological evolution has created critical challenges for startups and established companies in efficiently processing digital intra-device communication interfaces for onboard equipment, necessitating automated solutions to reduce labor-intensive efforts.", "method": "The authors propose a novel automated process and toolchain for developing digital interfaces and onboard software, emphasizing automation, flexibility, and compliance with design assurance requirements.", "result": "Successful application of the proposed approach across multiple completed projects demonstrates its effectiveness in streamlining interface development workflows.", "conclusion": "The presented automated toolchain provides a viable solution for overcoming interface development challenges while maintaining design assurance compliance amidst the aerospace industry's ongoing transformation."}}
{"id": "2507.10338", "categories": ["cs.SE", "cs.AR", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.10338", "abs": "https://arxiv.org/abs/2507.10338", "authors": ["Enyuan Tian", "Yiwei Ci", "Qiusong Yang", "Yufeng Li", "Zhichao Lyu"], "title": "AssertCoder: LLM-Based Assertion Generation via Multimodal Specification Extraction", "comment": "7 pages, 3 figures", "summary": "Assertion-Based Verification (ABV) is critical for ensuring functional\ncorrectness in modern hardware systems. However, manually writing high-quality\nSVAs remains labor-intensive and error-prone. To bridge this gap, we propose\nAssertCoder, a novel unified framework that automatically generates\nhigh-quality SVAs directly from multimodal hardware design specifications.\nAssertCoder employs a modality-sensitive preprocessing to parse heterogeneous\nspecification formats (text, tables, diagrams, and formulas), followed by a set\nof dedicated semantic analyzers that extract structured representations aligned\nwith signal-level semantics. These representations are utilized to drive\nassertion synthesis via multi-step chain-of-thought (CoT) prompting. The\nframework incorporates a mutation-based evaluation approach to assess assertion\nquality via model checking and further refine the generated assertions.\nExperimental evaluation across three real-world Register-Transfer Level (RTL)\ndesigns demonstrates AssertCoder's superior performance, achieving an average\nincrease of 8.4% in functional correctness and 5.8% in mutation detection\ncompared to existing state-of-the-art approaches.", "AI": {"tldr": "AssertCoder is a framework that automatically generates high-quality SystemVerilog Assertions (SVAs) from multimodal hardware specifications using modality-sensitive preprocessing and semantic analysis, improving functional correctness and mutation detection by 8.4% and 5.8% respectively.", "motivation": "Manual SVA creation for Assertion-Based Verification is laborious and error-prone, necessitating an automated solution for efficient, reliable hardware validation.", "method": "1. Parses heterogeneous specs (text, tables, diagrams, formulas) via modality-sensitive preprocessing\n2. Extracts structured, signal-semantic representations using dedicated analyzers\n3. Generates SVAs through multi-step chain-of-thought prompting\n4. Applies mutation-based evaluation with model checking for refinement", "result": "8.4% average improvement in functional correctness and 5.8% enhancement in mutation detection across three real RTL designs vs. state-of-the-art methods.", "conclusion": "AssertCoder demonstrates superior effectiveness in automating SVA generation from multimodal inputs, with measurable improvements in hardware verification reliability and error detection."}}
{"id": "2507.10422", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10422", "abs": "https://arxiv.org/abs/2507.10422", "authors": ["Tao Xiao", "Youmei Fan", "Fabio Calefato", "Christoph Treude", "Raula Gaikovina Kula", "Hideaki Hata", "Sebastian Baltes"], "title": "Self-Admitted GenAI Usage in Open-Source Software", "comment": "17 pages, 8 tables, 1 figures, currently under review", "summary": "The widespread adoption of generative AI (GenAI) tools such as GitHub Copilot\nand ChatGPT is transforming software development. Since generated source code\nis virtually impossible to distinguish from manually written code, their\nreal-world usage and impact on open-source software development remain poorly\nunderstood. In this paper, we introduce the concept of self-admitted GenAI\nusage, that is, developers explicitly referring to the use of GenAI tools for\ncontent creation in software artifacts. Using this concept as a lens to study\nhow GenAI tools are integrated into open-source software projects, we analyze a\ncurated sample of more than 250,000 GitHub repositories, identifying 1,292 such\nself-admissions across 156 repositories in commit messages, code comments, and\nproject documentation. Using a mixed methods approach, we derive a taxonomy of\n32 tasks, 10 content types, and 11 purposes associated with GenAI usage based\non 284 qualitatively coded mentions. We then analyze 13 documents with policies\nand usage guidelines for GenAI tools and conduct a developer survey to uncover\nthe ethical, legal, and practical concerns behind them. Our findings reveal\nthat developers actively manage how GenAI is used in their projects,\nhighlighting the need for project-level transparency, attribution, and quality\ncontrol practices in the new era of AI-assisted software development. Finally,\nwe examine the longitudinal impact of GenAI adoption on code churn in 151\nrepositories with self-admitted GenAI usage and find no general increase,\ncontradicting popular narratives on the impact of GenAI on software\ndevelopment.", "AI": {"tldr": "This paper analyzes how generative AI (GenAI) tools are integrated into open-source software (OSS) projects by studying self-admitted GenAI usage across 250,000 GitHub repositories, identifying 32 tasks, 10 content types, and 11 purposes associated with their use. It also highlights developer concerns and provides evidence that GenAI adoption does not significantly increase code churn.", "motivation": "The paper addresses the need to understand the real-world usage and impact of GenAI tools in software development, particularly in open-source projects, as generated code is indistinguishable from manual code and existing narratives often lack empirical validation. The motivation is to uncover how developers adopt, use, and govern AI-generated content while addressing ethical and practical implications.", "method": "The study employs a mixed-methods approach involving 1) analysis of commit messages, code comments, and documentation across 250,000 GitHub repositories to identify self-admitted GenAI usage instances; 2) qualitative coding of 284 mentions to develop taxonomies for tasks, content types, and purposes; 3) analysis of 13 documents with GenAI policies and guidelines; 4) developer surveys; and 5) longitudinal analysis of code churn in 151 repositories.", "result": "Key findings include: 1) 1,292 self-admitted GenAI usage instances across 156 repositories; 2) a taxonomy categorizing usage into 32 tasks, 10 content types, and 11 purposes; 3) significant developer and project-level governance efforts around ethical, legal, and quality concerns; and 4) no general increase in code churn in repositories adopting GenAI tools, challenging existing assumptions.", "conclusion": "The study concludes that open-source developers actively prioritize transparency, attribution, and quality control when integrating GenAI tools into their workflows. It emphasizes the need for project-specific practices to govern AI-assisted development and reveals the complexity of GenAI adoption, which does not simplify code complexity despite claims to the contrary."}}
