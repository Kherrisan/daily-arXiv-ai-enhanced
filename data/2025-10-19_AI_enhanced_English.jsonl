{"id": "2510.13857", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13857", "abs": "https://arxiv.org/abs/2510.13857", "authors": ["Qiang Xu", "Xiangyu Wen", "Changran Xu", "Zeju Li", "Jianyuan Zhong"], "title": "From Craft to Constitution: A Governance-First Paradigm for Principled Agent Engineering", "comment": null, "summary": "The advent of powerful Large Language Models (LLMs) has ushered in an ``Age\nof the Agent,'' enabling autonomous systems to tackle complex goals. However,\nthe transition from prototype to production is hindered by a pervasive ``crisis\nof craft,'' resulting in agents that are brittle, unpredictable, and ultimately\nuntrustworthy in mission-critical applications. This paper argues this crisis\nstems from a fundamental paradigm mismatch -- attempting to command inherently\nprobabilistic processors with the deterministic mental models of traditional\nsoftware engineering. To solve this crisis, we introduce a governance-first\nparadigm for principled agent engineering, embodied in a formal architecture we\ncall ArbiterOS.", "AI": {"tldr": "This paper proposes ArbiterOS, a governance-first architecture to address brittleness in AI agents by aligning engineering practices with LLMs' probabilistic nature.", "motivation": "The work is motivated by the 'crisis of craft' in autonomous agent development, where deterministic software engineering practices yield brittle, unpredictable agents unsuitable for mission-critical applications.", "method": "The paper introduces ArbiterOS, a formal architecture grounded in a governance-first paradigm that reorients agent design around probabilistic reasoning and risk management.", "result": "The governance-first approach provides a principled framework that enables more trustworthy autonomous systems by addressing the paradigm mismatch between probabilistic LLMs and traditional design methods.", "conclusion": "The paper concludes that a governance-first paradigm, implemented through ArbiterOS, can resolve the challenges posed by traditional deterministic approaches in agent engineering for probabilistic LLMs."}}
{"id": "2510.13859", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13859", "abs": "https://arxiv.org/abs/2510.13859", "authors": ["Ruchit Rawal", "Jeffrey Yang Fan Chiang", "Chihao Shen", "Jeffery Siyuan Tian", "Aastha Mahajan", "Tom Goldstein", "Yizheng Chen"], "title": "Benchmarking Correctness and Security in Multi-Turn Code Generation", "comment": null, "summary": "AI coding assistants powered by large language models (LLMs) have transformed\nsoftware development, significantly boosting productivity. While existing\nbenchmarks evaluate the correctness and security of LLM-generated code, they\nare typically limited to single-turn tasks that do not reflect the iterative\nnature of real-world development. We introduce MT-Sec, the first benchmark to\nsystematically evaluate both correctness and security in multi-turn coding\nscenarios. We construct this using a synthetic data pipeline that transforms\nexisting single-turn tasks into semantically aligned multi-turn interaction\nsequences, allowing reuse of original test suites while modeling the complexity\nof real-world coding processes. We evaluate 32 open- and closed-source models,\nand three agent-scaffolding on MT-Sec and observe a consistent 20-27% drop in\n\"correct and secure\" outputs from single-turn to multi-turn settings -- even\namong state-of-the-art models. Beyond full-program generation, we also evaluate\nmodels on multi-turn code-diff generation -- an unexplored yet practically\nrelevant setting -- and find that models perform worse here, with increased\nrates of functionally incorrect and insecure outputs. Finally, we find that\nwhile agent scaffoldings boost single-turn code generation performance, they\nare not quite as effective in multi-turn evaluations. Together, these findings\nhighlight the need for benchmarks that jointly evaluate correctness and\nsecurity in multi-turn, real-world coding workflows.", "AI": {"tldr": "MT-Sec is a new benchmark for evaluating AI coding assistants in multi-turn scenarios, revealing significant drops in correctness and security compared to single-turn tasks.", "motivation": "Existing benchmarks focus on single-turn code generation, ignoring the iterative nature of real-world development. Multi-turn evaluations are needed to assess correctness and security more realistically.", "method": "Constructed synthetic multi-turn tasks from single-turn datasets using a pipeline that preserves semantic alignment. Evaluated 32 models and agent scaffolds across correctness, security, and code-diff generation.", "result": "20-27\\% drop in 'correct and secure' performance in multi-turn settings. Code-diff generation showed higher error rates. Agent scaffolding improved single-turn but not multi-turn results.", "conclusion": "Current models underperform in multi-turn workflows; benchmarks must account for iterative development to ensure real-world security and correctness in AI-assisted coding."}}
{"id": "2510.13914", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.13914", "abs": "https://arxiv.org/abs/2510.13914", "authors": ["Janghan Yoon", "Jaegwan Cho", "Junhyeok Kim", "Jiwan Chung", "Jaehyun Jeon", "Youngjae Yu"], "title": "A11YN: aligning LLMs for accessible web UI code generation", "comment": null, "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nin generating functional and aesthetic web interfaces directly from\ninstructions. However, these models often replicate accessibility flaws from\ntheir training data, resulting in interfaces that exclude users with diverse\nneeds and contexts. To address this gap, we introduce A11yn, the first method\nthat aligns code-generating LLMs to reliably produce accessibility-compliant\nweb UIs. A11yn optimizes a novel reward function that penalizes violations of\nthe Web Content Accessibility Guidelines (WCAG), with penalties scaled to the\nseverity of each violation as identified by an accessibility testing engine. To\nsupport training, we construct UIReq-6.8K, a dataset of 6,800 diverse\ninstructions for web UI generation. For evaluation, we introduce RealUIReq-300,\na benchmark of 300 real-world web UI requests grounded and manually curated\nfrom public web pages, spanning a broad range of use cases. Empirical results\nshow that A11yn significantly outperforms strong baselines, lowering the\nInaccessibility Rate by 60% over the base model while preserving semantic\nfidelity and visual quality of generated UIs. These findings demonstrate that\naccessibility can be systematically optimized within LLMs, showing the\nfeasibility of aligning code generation for accessibility.", "AI": {"tldr": "A11yn is a novel method that optimizes code-generating LLMs to produce web interfaces compliant with accessibility guidelines (WCAG), significantly reducing inaccessibility rates by 60%. It uses a severity-weighted reward function, a new training dataset (UIReq-6.8K), and a benchmark (RealUIReq-300).", "motivation": "LLMs often replicate accessibility flaws from their training data, creating UIs that exclude users with diverse needs. Current models lack systematic alignment with accessibility standards like WCAG.", "method": "A11yn introduces WCAG-compliant web UI optimization by penalizing guideline violations through a severity-scaled reward function trained on UIReq-6.8K (diverse web UI instructions). It evaluates performance using RealUIReq-300 (real-world UI requests).", "result": "A11yn reduced inaccessibility rates by 60%, outperforming baselines, while maintaining semantic fidelity and visual quality. Both datasets (UIReq-6.8K and RealUIReq-300 show strong efficacy in training and evaluation.", "conclusion": "Accessibility can be systematically optimized into code-generating LLMs through structured alignment with WCAG guidelines, demonstrating feasibility of responsible AI development for inclusive UI design."}}
{"id": "2510.13992", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13992", "abs": "https://arxiv.org/abs/2510.13992", "authors": ["Quoc Hung Le", "Thanh Le-Cong", "Bach Le", "Bowen Xu"], "title": "Signature in Code Backdoor Detection, how far are we?", "comment": "20 pages, 3 figures", "summary": "As Large Language Models (LLMs) become increasingly integrated into software\ndevelopment workflows, they also become prime targets for adversarial attacks.\nAmong these, backdoor attacks are a significant threat, allowing attackers to\nmanipulate model outputs through hidden triggers embedded in training data.\nDetecting such backdoors remains a challenge, and one promising approach is the\nuse of Spectral Signature defense methods that identify poisoned data by\nanalyzing feature representations through eigenvectors. While some prior works\nhave explored Spectral Signatures for backdoor detection in neural networks,\nrecent studies suggest that these methods may not be optimally effective for\ncode models. In this paper, we revisit the applicability of Spectral\nSignature-based defenses in the context of backdoor attacks on code models. We\nsystematically evaluate their effectiveness under various attack scenarios and\ndefense configurations, analyzing their strengths and limitations. We found\nthat the widely used setting of Spectral Signature in code backdoor detection\nis often suboptimal. Hence, we explored the impact of different settings of the\nkey factors. We discovered a new proxy metric that can more accurately estimate\nthe actual performance of Spectral Signature without model retraining after the\ndefense.", "AI": {"tldr": "This paper re-evaluates Spectral Signature defenses for code backdoors, identifying suboptimal settings and introducing a new proxy metric to assess defense effectiveness without retraining.", "motivation": "Existing Spectral Signature methods for backdoor detection in code models lack optimal effectiveness, and there is a need to re-evaluate their applicability and identify factors impacting their performance in adversarial code model scenarios.", "method": "The study systematically evaluates Spectral Signature-based defenses under various attack scenarios and defense configurations, analyzes their strengths and limitations, and examines the impact of different settings for key factors. A novel proxy metric is introduced for performance estimation.", "result": "Findings reveal suboptimal performance of standard Spectral Signature settings in code-backdoor detection. The new proxy metric accurately predicts defense efficacy without retraining, and critical factors influencing Spectral Signature effectiveness were identified.", "conclusion": "The paper concludes that the commonly used Spectral Signature settings for backdoor detection in code models are suboptimal. It proposes a new proxy metric to estimate performance without requiring model retraining after defense."}}
{"id": "2510.13822", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.13822", "abs": "https://arxiv.org/abs/2510.13822", "authors": ["Bartosz Burgiel"], "title": "Noisy Networks, Nosy Neighbors: Inferring Privacy Invasive Information from Encrypted Wireless Traffic", "comment": "80 pages, 49 figures, bachelor thesis at the data privacy and\n  security chair of the leipzig university", "summary": "This thesis explores the extent to which passive observation of wireless\ntraffic in a smart home environment can be used to infer privacy-invasive\ninformation about its inhabitants. Using a setup that mimics the capabilities\nof a nosy neighbor in an adjacent flat, we analyze raw 802.11 packets and\nBluetooth Low Energy advertisemets. From this data, we identify devices, infer\ntheir activity states and approximate their location using RSSI-based\ntrilateration. Despite the encrypted nature of the data, we demonstrate that it\nis possible to detect active periods of multimedia devices, infer common\nactivities such as sleeping, working and consuming media, and even approximate\nthe layout of the neighbor's apartment. Our results show that privacy risks in\nsmart homes extend beyond traditional data breaches: a nosy neighbor behind the\nwall can gain privacy-invasive insights into the lives of their neighbors\npurely from encrypted network traffic.", "AI": {"tldr": "This study demonstrates that encrypted smart home wireless traffic can be passively analyzed to infer sensitive user activities and environmental layouts, highlighting new privacy risks beyond traditional data breaches.", "motivation": "The proliferation of smart home devices creates privacy vulnerabilities through encrypted network traffic, which conventional security measures often overlook. This research addresses the risk of passive eavesdropping by neighbors or attackers using basic wireless observation tools.", "method": "The researchers created a simulated neighboring environment to capture 802.11 packets and Bluetooth Low Energy advertisements. They used device fingerprinting, state inference algorithms, and RSSI trilateration to analyze encrypted traffic patterns without decrypting payloads.", "result": "They successfully detected device usage patterns (e.g., streaming, sleeping, work hours), inferred human behaviors, and reconstructed approximate room layouts using only signal strength analysis from encrypted traffic. The system achieved location accuracy within 1-2 meters in testing scenarios.", "conclusion": "Smart home privacy risks extend beyond data breaches to include passive traffic analysis. Even with encryption, temporal patterns and signal metadata can leak sensitive information about occupants' routines and environments, necessitating new countermeasures for location- and state-privacy protection."}}
{"id": "2510.14036", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14036", "abs": "https://arxiv.org/abs/2510.14036", "authors": ["Qiushi Wu", "Yue Xiao", "Dhilung Kirat", "Kevin Eykholt", "Jiyong Jang", "Douglas Lee Schales"], "title": "One Bug, Hundreds Behind: LLMs for Large-Scale Bug Discovery", "comment": null, "summary": "Fixing bugs in large programs is a challenging task that demands substantial\ntime and effort. Once a bug is found, it is reported to the project\nmaintainers, who work with the reporter to fix it and eventually close the\nissue. However, across the program, there are often similar code segments,\nwhich may also contain the bug, but were missed during discovery. Finding and\nfixing each recurring bug instance individually is labor intensive. Even more\nconcerning, bug reports can inadvertently widen the attack surface as they\nprovide attackers with an exploitable pattern that may be unresolved in other\nparts of the program.\n  In this paper, we explore these Recurring Pattern Bugs (RPBs) that appear\nrepeatedly across various code segments of a program or even in different\nprograms, stemming from a same root cause, but are unresolved. Our\ninvestigation reveals that RPBs are widespread and can significantly compromise\nthe security of software programs. This paper introduces BugStone, a program\nanalysis system empowered by LLVM and a Large Language Model (LLM). The key\nobservation is that many RPBs have one patched instance, which can be leveraged\nto identify a consistent error pattern, such as a specific API misuse. By\nexamining the entire program for this pattern, it is possible to identify\nsimilar sections of code that may be vulnerable. Starting with 135 unique RPBs,\nBugStone identified more than 22K new potential issues in the Linux kernel.\nManual analysis of 400 of these findings confirmed that 246 were valid. We also\ncreated a dataset from over 1.9K security bugs reported by 23 recent top-tier\nconference works. We manually annotate the dataset, identify 80 recurring\npatterns and 850 corresponding fixes. Even with a cost-efficient model choice,\nBugStone achieved 92.2% precision and 79.1% pairwise accuracy on the dataset.", "AI": {"tldr": "BugStone combines LLVM and an LLM to detect recurring security bugs in large software, achieving 92.2% precision with high validation success in the Linux kernel. The system automates identification of systemic API misuse patterns, reducing manual effort and mitigating attack surfaces.", "motivation": "The motivation stems from the labor-intensive nature of manually fixing recurring bugs in large programs, which risks missing vulnerabilities and expanding attack surfaces. RPBs, caused by a common root cause but unresolved in similar code segments, pose a critical security threat. Automated detection is essential to address these systemic issues efficiently and reduce reliance on manual efforts.", "method": "The method involves developing BugStone, a program analysis system leveraging LLVM and a Large Language Model (LLM), which identifies RPBs by analyzing patched instances to detect consistent error patterns. The system scales to large codebases and uses a curated dataset of 1.9K security bugs with 80 recurring patterns to train and validate the approach. It employs pairwise accuracy metrics to evaluate detection effectiveness.", "result": "BugStone identified 22,000 potential issues in the Linux kernel, with 246 confirmed as valid after manual analysis of 400 cases. It achieved 92.2% precision and 79.1% pairwise accuracy using a cost-efficient LLM. The dataset of 1.9K security bugs was annotated with 80 recurring patterns and 850 fixes, enabling robust evaluation of the approach.", "conclusion": "This paper concludes that Recurring Pattern Bugs (RPBs) are a significant security threat, and the proposed BugStone system effectively detects and addresses these bugs with high precision, demonstrating practical impact through large-scale validation in the Linux kernel. The study highlights the potential of combining program analysis with LLMs to enhance software security at scale."}}
{"id": "2510.13824", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.13824", "abs": "https://arxiv.org/abs/2510.13824", "authors": ["Wai Ming Chan", "Remi Chou", "Taejoon Kim"], "title": "Multi-Layer Secret Sharing for Cross-Layer Attack Defense in 5G Networks: a COTS UE Demonstration", "comment": null, "summary": "This demo presents the first implementation of multi-layer secret sharing on\ncommercial-off-the-shelf (COTS) 5G user equipment (UE), operating without\ninfrastructure modifications or pre-shared keys. Our XOR-based approach\ndistributes secret shares across network operators and distributed relays,\nensuring perfect recovery and data confidentiality even if one network operator\nand one relay are simultaneously lost (e.g., under denial of service (DoS) or\nunanticipated attacks).", "AI": {"tldr": "First XOR-based multi-layer secret sharing implementation on COTS 5G devices, ensuring security against DoS attacks without infrastructure changes.", "motivation": "Existing secret sharing schemes require infrastructure changes or pre-shared keys, limiting their applicability to COTS 5G devices.", "method": "An XOR-based approach that distributes secret shares across network operators and distributed relays without requiring infrastructure modifications or pre-shared keys.", "result": "Achieved perfect secret recovery and data confidentiality even when one network operator and one relay are compromised.", "conclusion": "This paper demonstrates a practical and secure method for multi-layer secret sharing on standard 5G devices."}}
{"id": "2510.14115", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14115", "abs": "https://arxiv.org/abs/2510.14115", "authors": ["Philipp Bauerfeind", "Amir Salarpour", "David Fernandez", "Pedram MohajerAnsari", "Johannes Reschke", "Mert D. Pes\u00e9"], "title": "David vs. Goliath: A comparative study of different-sized LLMs for code generation in the domain of automotive scenario generation", "comment": null, "summary": "Scenario simulation is central to testing autonomous driving systems. Scenic,\na domain-specific language (DSL) for CARLA, enables precise and reproducible\nscenarios, but NL-to-Scenic generation with large language models (LLMs)\nsuffers from scarce data, limited reproducibility, and inconsistent metrics. We\nintroduce NL2Scenic, an open dataset and framework with 146 NL/Scenic pairs, a\ndifficulty-stratified 30-case test split, an Example Retriever, and 14\nprompting variants (ZS, FS, CoT, SP, MoT). We evaluate 13 models: four\nproprietary (GPT-4o, GPT-5, Claude-Sonnet-4, Gemini-2.5-pro) and nine\nopen-source code models (Qwen2.5Coder 0.5B-32B; CodeLlama 7B/13B/34B), using\ntext metrics (BLEU, ChrF, EDIT-SIM, CrystalBLEU) and execution metrics\n(compilation and generation), and compare them with an expert study (n=11).\nEDIT-SIM correlates best with human judgments; we also propose EDIT-COMP (F1 of\nEDIT-SIM and compilation) as a robust dataset-level proxy that improves ranking\nfidelity. GPT-4o performs best overall, while Qwen2.5Coder-14B reaches about 88\npercent of its expert score on local hardware. Retrieval-augmented prompting,\nFew-Shot with Example Retriever (FSER), consistently boosts smaller models, and\nscaling shows diminishing returns beyond mid-size, with Qwen2.5Coder\noutperforming CodeLlama at comparable scales. NL2Scenic and EDIT-COMP offer a\nstandardized, reproducible basis for evaluating Scenic code generation and\nindicate that mid-size open-source models are practical, cost-effective options\nfor autonomous-driving scenario programming.", "AI": {"tldr": "The paper introduces a new framework and dataset for evaluating natural language processing (NLP) to scenario programming (Scenic) in autonomous driving systems, termed NL2Scenic. It utilizes a new evaluation metric called EDIT-COMP to improve ranking consistency and benchmarks various LLMs, finding that mid-scale open-source models offer a balance of effectiveness and cost-efficiency.", "motivation": "Scenario simulation is a critical but challenging component of autonomous driving system testing, with limitations in data availability, reproducibility, and consistent evaluation of natural language interfaces for scenario generation.", "method": "The researchers developed an open dataset and evaluation framework called NL2Scenic. Key features include 146 paired natural language (NL) and Scenic examples, a test set of 30 cases with difficulty stratification, retrieval techniques via an Example Retriever, and a range of prompting strategies (Zero-shot, Few-shot, Chain-of-Thought, Self-Projection, Multiple Tree-of-Thought variants). Models were tested on large-scale and open-source LLMs including GPT-4o, GPT-5, Claude-Sonnet-4, Gemini-2.5-Pro, Qwen2.5-Coder (0.5B-32B), and CodeLlama (7B/13B/34B). They introduced the EDIT-SIM metric and further developed EDIT-COMP, a combined metric using F1 score to evaluate both output quality and executable correctness in generated Scenic code.", "result": "GPT-4o outperformed all models in the benchmarks. The open-source Qwen2.5-Coder 14B model demonstrated 79.2 % of GPT-4o\u2019s performance while being deployable on local hardware. Retrieval-augmented prompting enhanced the performance of small models effectively and reduced the gap with large models. EDIT-COMP correlated more strongly with human evaluations than existing text metrics, indicating its value as a proxy.", "conclusion": "NL2Scenic provides a standardized approach for evaluating models in autonomous driving scenario coding. Mid-size open-source models demonstrate practicality and cost-effectiveness, while the newly proposed EDIT-COMP metric enhances non-human evaluation accuracy."}}
{"id": "2510.13825", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13825", "abs": "https://arxiv.org/abs/2510.13825", "authors": ["Eugene Neelou", "Ivan Novikov", "Max Moroz", "Om Narayan", "Tiffany Saade", "Mika Ayenson", "Ilya Kabanov", "Jen Ozmen", "Edward Lee", "Vineeth Sai Narajala", "Emmanuel Guilherme Junior", "Ken Huang", "Huseyin Gulsin", "Jason Ross", "Marat Vyshegorodtsev", "Adelin Travers", "Idan Habler", "Rahul Jadav"], "title": "A2AS: Agentic AI Runtime Security and Self-Defense", "comment": null, "summary": "The A2AS framework is introduced as a security layer for AI agents and\nLLM-powered applications, similar to how HTTPS secures HTTP. A2AS enforces\ncertified behavior, activates model self-defense, and ensures context window\nintegrity. It defines security boundaries, authenticates prompts, applies\nsecurity rules and custom policies, and controls agentic behavior, enabling a\ndefense-in-depth strategy. The A2AS framework avoids latency overhead, external\ndependencies, architectural changes, model retraining, and operational\ncomplexity. The BASIC security model is introduced as the A2AS foundation: (B)\nBehavior certificates enable behavior enforcement, (A) Authenticated prompts\nenable context window integrity, (S) Security boundaries enable untrusted input\nisolation, (I) In-context defenses enable secure model reasoning, (C) Codified\npolicies enable application-specific rules. This first paper in the series\nintroduces the BASIC security model and the A2AS framework, exploring their\npotential toward establishing the A2AS industry standard.", "AI": {"tldr": "A2AS is a lightweight security framework for AI agents/LLMs, using the BASIC model to enforce behavior, ensure context integrity, and apply policies, aiming to set an industry security standard.", "motivation": "The paper addresses the need for securing AI/LLM systems against adversarial prompts and context window vulnerabilities while avoiding latency, architectural changes, or model retraining.", "method": "A2AS employs the BASIC security model (Behavior certificates, Authenticated prompts, Security boundaries, In-context defenses, Codified policies) to enforce certified behavior, authenticate inputs, isolate risks, apply defenses during reasoning, and enforce application-specific rules.", "result": "Introduces the BASIC model and A2AS framework, demonstrating their potential to become an industry standard for AI/LLM security without external dependencies or operational complexity.", "conclusion": "The A2AS framework establishes a robust security model for AI agents and LLMs through the BASIC model, offering a defense-in-depth strategy without operational overheads, positioning itself as a candidate for industry standards."}}
