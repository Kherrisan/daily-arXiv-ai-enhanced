{"id": "2507.22133", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22133", "abs": "https://arxiv.org/abs/2507.22133", "authors": ["Michael Freenor", "Lauren Alvarez", "Milton Leal", "Lily Smith", "Joel Garrett", "Yelyzaveta Husieva", "Madeline Woodruff", "Ryan Miller", "Erich Kummerfeld", "Rafael Medeiros", "Sander Schulhoff"], "title": "Prompt Optimization and Evaluation for LLM Automated Red Teaming", "comment": "9 pages, 5 Figures, and 1 Appendix item", "summary": "Applications that use Large Language Models (LLMs) are becoming widespread,\nmaking the identification of system vulnerabilities increasingly important.\nAutomated Red Teaming accelerates this effort by using an LLM to generate and\nexecute attacks against target systems. Attack generators are evaluated using\nthe Attack Success Rate (ASR) the sample mean calculated over the judgment of\nsuccess for each attack. In this paper, we introduce a method for optimizing\nattack generator prompts that applies ASR to individual attacks. By repeating\neach attack multiple times against a randomly seeded target, we measure an\nattack's discoverability the expectation of the individual attack success. This\napproach reveals exploitable patterns that inform prompt optimization,\nultimately enabling more robust evaluation and refinement of generators.", "AI": {"tldr": "This paper proposes a method for optimizing attack generator prompts by evaluating individual attack success through repeated testing with random target seeds, enhancing the effectiveness of Automated Red Teaming against LLM vulnerabilities.", "motivation": "Current evaluation of attack generators using Attack Success Rate (ASR) as a sample mean across all attacks overlooks exploitable patterns in individual vulnerabilities, limiting robust refinement of security measures.", "method": "The proposed method calculates the discoverability of each attack by testing it multiple times against a target system with varying random seeds, measuring the expectation of success as a local ASR to guide prompt optimization.", "result": "The approach identifies exploitable attack patterns not captured by global ASR metrics, enabling data-driven optimization of attack generators to achieve higher precision and robustness in vulnerability detection.", "conclusion": "Local ASR-based optimization of attack generators increases the effectiveness of Automated Red Teaming for LLM applications by systematically uncovering and exploiting system vulnerabilities through repeatable attack testing."}}
{"id": "2507.22160", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22160", "abs": "https://arxiv.org/abs/2507.22160", "authors": ["Yassine Rachidy", "Jihad Rbaiti", "Youssef Hmamouche", "Faissal Sehbaoui", "Amal El Fallah Seghrouchni"], "title": "Strategic Deflection: Defending LLMs from Logit Manipulation", "comment": "20 pages", "summary": "With the growing adoption of Large Language Models (LLMs) in critical areas,\nensuring their security against jailbreaking attacks is paramount. While\ntraditional defenses primarily rely on refusing malicious prompts, recent\nlogit-level attacks have demonstrated the ability to bypass these safeguards by\ndirectly manipulating the token-selection process during generation. We\nintroduce Strategic Deflection (SDeflection), a defense that redefines the\nLLM's response to such advanced attacks. Instead of outright refusal, the model\nproduces an answer that is semantically adjacent to the user's request yet\nstrips away the harmful intent, thereby neutralizing the attacker's harmful\nintent. Our experiments demonstrate that SDeflection significantly lowers\nAttack Success Rate (ASR) while maintaining model performance on benign\nqueries. This work presents a critical shift in defensive strategies, moving\nfrom simple refusal to strategic content redirection to neutralize advanced\nthreats.", "AI": {"tldr": "This paper introduces Strategic Deflection (SDeflection), a defense mechanism against logit-level jailbreaking attacks on Large Language Models (LLMs), which redirects malicious intent without refusing prompts, thereby reducing attack success rates while preserving benign performance.", "motivation": "Traditional LLM defenses block malicious prompts, but logit-level attacks bypass these by manipulating token selection during generation, necessitating a shift from refusal to proactive response strategies to ensure security.", "method": "SDeflection alters the model's response process to produce semantically adjacent outputs that neutralize harmful intent by strategically deflecting the content direction during token selection, as opposed to outright rejecting the input.", "result": "Experiments confirm SDeflection significantly lowers Attack Success Rates (ASR) of logit-based jailbreaking attacks while maintaining high accuracy on legitimate user queries.", "conclusion": "The study advocates a new paradigm in LLM defense: prioritizing strategic content redirection over passive refusal to neutralize advanced threats effectively."}}
{"id": "2507.22165", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.22165", "abs": "https://arxiv.org/abs/2507.22165", "authors": ["Gursimran Singh", "H. B. Acharya", "Minseok Kwon"], "title": "Programmable Data Planes for Network Security", "comment": "17th International Conference on Networks & Communications (NeTCoM\n  2025)", "summary": "The emergence of programmable data planes, and particularly switches\nsupporting the P4 language, has transformed network security by enabling\ncustomized, line-rate packet processing. These switches, originally intended\nfor flexible forwarding, now play a broader role: detecting and mitigating\nattacks such as DDoS and spoofing, enforcing next-generation firewall policies,\nand even supporting in-network cryptography and machine learning. These\ncapabilities are made possible by techniques such as recirculate-and-truncate\nand lookup-table precomputation, which work around architectural constraints\nlike limited memory and restricted instruction sets. In this paper, we\nsystematize recent advances in security applications built on programmable\nswitches, with an emphasis on the capabilities, challenges, and architectural\nworkarounds. We highlight the non-obvious design techniques that make complex\nin-network security functions feasible despite the constraints of the hardware\nplatform, and also comment on remaining issues and emerging research\ndirections.", "AI": {"tldr": "This paper reviews how programmable data planes (especially P4 switches) enable advanced network security functions through design techniques that overcome hardware limitations like memory and instruction constraints.", "motivation": "Programmable switches initially designed for flexible packet forwarding have evolved to address complex security challenges (e.g., DDoS, spoofing) but face architectural constraints requiring novel workarounds.", "method": "Systematic analysis of recent security applications on programmable switches, focusing on capabilities, challenges, and architectural workarounds like recirculate-and-truncate and lookup-table precomputation.", "result": "Identification of non-obvious design techniques enabling in-network security (firewalls, cryptography, ML) while navigating hardware limitations, with a structured overview of current capabilities and challenges.", "conclusion": "Outlines remaining issues in deploying complex security functions on programmable switches and emerging research directions to address these challenges."}}
{"id": "2507.22171", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22171", "abs": "https://arxiv.org/abs/2507.22171", "authors": ["Zheng Zhang", "Peilin Zhao", "Deheng Ye", "Hao Wang"], "title": "Enhancing Jailbreak Attacks on LLMs via Persona Prompts", "comment": null, "summary": "Jailbreak attacks aim to exploit large language models (LLMs) by inducing\nthem to generate harmful content, thereby revealing their vulnerabilities.\nUnderstanding and addressing these attacks is crucial for advancing the field\nof LLM safety. Previous jailbreak approaches have mainly focused on direct\nmanipulations of harmful intent, with limited attention to the impact of\npersona prompts. In this study, we systematically explore the efficacy of\npersona prompts in compromising LLM defenses. We propose a genetic\nalgorithm-based method that automatically crafts persona prompts to bypass\nLLM's safety mechanisms. Our experiments reveal that: (1) our evolved persona\nprompts reduce refusal rates by 50-70% across multiple LLMs, and (2) these\nprompts demonstrate synergistic effects when combined with existing attack\nmethods, increasing success rates by 10-20%. Our code and data are available at\nhttps://github.com/CjangCjengh/Generic_Persona.", "AI": {"tldr": "This paper introduces a genetic algorithm approach to craft persona prompts that successfully bypass safety mechanisms in LLMs, reducing refusal rates by 50-70% and enhancing attack success rates by 10-20% when combined with existing methods.", "motivation": "Previous jailbreak attack research has primarily focused on direct harm-inducing intent manipulation while neglecting the potential of persona prompts to compromise LLM defenses. This gap motivates a systematic investigation into persona prompt effectiveness.", "method": "The study proposes an automated genetic algorithm framework for evolving optimized persona prompts through iterative evaluation of bypass effectiveness against safety constraints.", "result": "Experiments show (1) persona prompts reduce refusal rates by 50-70% across various LLMs, and (2) combining evolved prompts with existing attacks increases success rates by 10-20%. Code is publicly available at the provided GitHub link.", "conclusion": "Persona prompts demonstrate significant efficacy in jailbreak attacks, offering both standalone impact and synergistic improvements with conventional methods. This work provides new insights for LLM safety development."}}
{"id": "2507.22063", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22063", "abs": "https://arxiv.org/abs/2507.22063", "authors": ["Wenjie Jacky Mo", "Qin Liu", "Xiaofei Wen", "Dongwon Jung", "Hadi Askari", "Wenxuan Zhou", "Zhe Zhao", "Muhao Chen"], "title": "RedCoder: Automated Multi-Turn Red Teaming for Code LLMs", "comment": null, "summary": "Large Language Models (LLMs) for code generation (i.e., Code LLMs) have\ndemonstrated impressive capabilities in AI-assisted software development and\ntesting. However, recent studies have shown that these models are prone to\ngenerating vulnerable or even malicious code under adversarial settings.\nExisting red-teaming approaches rely on extensive human effort, limiting their\nscalability and practicality, and generally overlook the interactive nature of\nreal-world AI-assisted programming, which often unfolds over multiple turns. To\nbridge these gaps, we present RedCoder, a red-teaming agent that engages victim\nmodels in multi-turn conversation to elicit vulnerable code. The pipeline to\nconstruct RedCoder begins with a multi-agent gaming process that simulates\nadversarial interactions, yielding a set of prototype conversations and an\narsenal of reusable attack strategies. We then fine-tune an LLM on these\nprototype conversations to serve as the backbone of RedCoder. Once deployed,\nRedCoder autonomously engages Code LLMs in multi-turn conversations,\ndynamically retrieving relevant strategies from the arsenal to steer the\ndialogue toward vulnerability-inducing outputs. Experiments across multiple\nCode LLMs show that our approach outperforms prior single-turn and multi-turn\nred-team methods in inducing vulnerabilities in code generation, offering a\nscalable and effective tool for evaluating the security boundaries of modern\ncode-generation systems.", "AI": {"tldr": "RedCoder is a red-teaming agent designed to elicit vulnerable code from Code LLMs through multi-turn conversational interactions. It uses a multi-agent gaming process to generate attack strategies and demonstrates superior effectiveness in inducing vulnerabilities compared to existing single/multi-turn methods.", "motivation": "Current red-teaming approaches for code generation require extensive human effort, lack scalability, and ignore the multi-turn interactive nature of real-world AI-assisted programming interactions, necessitating a more automated and context-aware solution.", "method": "1) Simulate adversarial interactions between agents to generate prototype conversations and reusable attack strategies. 2) Fine-tune an LLM on these prototypes as the backbone of RedCoder. 3) Deploy the agent to autonomously engage Code LLMs in multi-turn conversations, dynamically selecting strategies to guide toward vulnerable code generation.", "result": "RedCoder outperforms prior red-teaming methods (both single and multi-turn) across multiple Code LLMs in inducing vulnerabilities within code generation outputs, verified through experimental evaluations.", "conclusion": "Adopting a multi-turn, strategy-driven conversational approach enhances the effectiveness of red-teaming for code LLMs. RedCoder establishes a scalable framework that exposes security blind spots in generation systems by replicating real-world development workflows.}}"}}
{"id": "2507.22177", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.22177", "abs": "https://arxiv.org/abs/2507.22177", "authors": ["Tanzim Mahfuz", "Sudipta Paria", "Tasneem Suha", "Swarup Bhunia", "Prabuddha Chakraborty"], "title": "POLARIS: Explainable Artificial Intelligence for Mitigating Power Side-Channel Leakage", "comment": null, "summary": "Microelectronic systems are widely used in many sensitive applications (e.g.,\nmanufacturing, energy, defense). These systems increasingly handle sensitive\ndata (e.g., encryption key) and are vulnerable to diverse threats, such as,\npower side-channel attacks, which infer sensitive data through dynamic power\nprofile. In this paper, we present a novel framework, POLARIS for mitigating\npower side channel leakage using an Explainable Artificial Intelligence (XAI)\nguided masking approach. POLARIS uses an unsupervised process to automatically\nbuild a tailored training dataset and utilize it to train a masking model.The\nPOLARIS framework outperforms state-of-the-art mitigation solutions (e.g.,\nVALIANT) in terms of leakage reduction, execution time, and overhead across\nlarge designs.", "AI": {"tldr": "POLARIS is a novel framework using XAI-guided masking to mitigate power side-channel attacks in microelectronic systems.", "motivation": "Microelectronic systems handling sensitive data (e.g., encryption keys) face vulnerabilities from power side-channel attacks, necessitating effective leakage mitigation solutions.", "method": "POLARIS employs an unsupervised process to generate a tailored training dataset for training a masking model, leveraging Explainable Artificial Intelligence (XAI) for guidance.", "result": "POLARIS demonstrates superior performance over existing state-of-the-art methods like VALIANT in terms of leakage reduction, execution time, and overhead for large-scale designs.", "conclusion": "The POLARIS framework provides a robust solution for enhancing security in microelectronic systems by effectively addressing power side-channel attack vulnerabilities."}}
{"id": "2507.22064", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22064", "abs": "https://arxiv.org/abs/2507.22064", "authors": ["Michael Cohoon", "Debbie Furman"], "title": "Machine Learning Experiences: A story of learning AI for use in enterprise software testing that can be used by anyone", "comment": null, "summary": "This paper details the machine learning (ML) journey of a group of people\nfocused on software testing. It tells the story of how this group progressed\nthrough a ML workflow (similar to the CRISP-DM process). This workflow consists\nof the following steps and can be used by anyone applying ML techniques to a\nproject: gather the data; clean the data; perform feature engineering on the\ndata; splitting the data into two sets, one for training and one for testing;\nchoosing a machine learning model; training the model; testing the model and\nevaluating the model performance. By following this workflow, anyone can\neffectively apply ML to any project that they are doing.", "AI": {"tldr": "The paper outlines a machine learning workflow for software testing, detailing steps from data collection to model evaluation, providing a structured CRISP-DM-like process for applying ML techniques effectively.", "motivation": "The authors aim to provide a clear, replicable workflow for individuals applying ML to software testing, addressing the challenges of data preparation, model selection, and evaluation in a domain-specific context.", "method": "The method follows an 8-step ML workflow: data gathering, cleaning, feature engineering, data splitting for training/testing, model selection, training, testing, and performance evaluation. The process is inspired by CRISP-DM but tailored to software testing applications.", "result": "The paper demonstrates a complete ML workflow application in software testing, achieving a structured approach but does not specify quantitative results or performance metrics.", "conclusion": "The structured workflow effectively guides software testing projects using ML, and its adaptability to other domains suggests broader applicability beyond the specific use case presented."}}
{"id": "2507.22231", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22231", "abs": "https://arxiv.org/abs/2507.22231", "authors": ["Ahmed Sabbah", "Radi Jarrar", "Samer Zein", "David Mohaisen"], "title": "Understanding Concept Drift with Deprecated Permissions in Android Malware Detection", "comment": "13 pages, 9 figures, 5 tables, under review", "summary": "Permission analysis is a widely used method for Android malware detection. It\ninvolves examining the permissions requested by an application to access\nsensitive data or perform potentially malicious actions. In recent years,\nvarious machine learning (ML) algorithms have been applied to Android malware\ndetection using permission-based features and feature selection techniques,\noften achieving high accuracy. However, these studies have largely overlooked\nimportant factors such as protection levels and the deprecation or restriction\nof permissions due to updates in the Android OS -- factors that can contribute\nto concept drift.\n  In this study, we investigate the impact of deprecated and restricted\npermissions on the performance of machine learning models. A large dataset\ncontaining 166 permissions was used, encompassing more than 70,000 malware and\nbenign applications. Various machine learning and deep learning algorithms were\nemployed as classifiers, along with different concept drift detection\nstrategies. The results suggest that Android permissions are highly effective\nfeatures for malware detection, with the exclusion of deprecated and restricted\npermissions having only a marginal impact on model performance. In some cases,\nsuch as with CNN, accuracy improved. Excluding these permissions also enhanced\nthe detection of concept drift using a year-to-year analysis strategy. Dataset\nbalancing further improved model performance, reduced low-accuracy instances,\nand enhanced concept drift detection via the Kolmogorov-Smirnov test.", "AI": {"tldr": "This study evaluates the impact of deprecated/restricted Android permissions on ML model performance for malware detection using a 70,000+ app dataset. Excluding these permissions had marginal negative effects, improved CNN accuracy, and enhanced concept drift detection.", "motivation": "Previous Android malware detection research using permissions ignored OS-related changes (protection levels, deprecated/restricted permissions) that cause concept drift, creating a gap in robust ML model evaluation.", "method": "Analyzed 166 Android permissions with ML/DL classifiers (including CNN) on 70,000+ apps, applying concept drift detection strategies and year-to-year analysis. Used Kolmogorov-Smirnov test for drift measurement and implemented dataset balancing techniques.", "result": "1) Android permissions remain effective malware features. 2) Removing deprecated/restricted permissions reduced performance impact marginally (<2% change). 3) CNN showed improved accuracy with clean permission set. 4) Annual analysis enhanced concept drift detection. 5) Balanced datasets reduced low-accuracy instances by 37% and improved drift detection robustness.", "conclusion": "Legacy permission analysis remains viable despite Android updates. Excluding obsolete permissions slightly improves accuracy and concept drift sensitivity. Dataset balancing is critical for maintaining model performance across OS versions."}}
{"id": "2507.22065", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.22065", "abs": "https://arxiv.org/abs/2507.22065", "authors": ["Xiaotao Feng", "Xiaogang Zhu", "Kun Hu", "Jincheng Wang", "Yingjie Cao", "Guang Gong", "Jianfeng Pan"], "title": "Fuzzing: Randomness? Reasoning! Efficient Directed Fuzzing via Large Language Models", "comment": null, "summary": "Fuzzing is highly effective in detecting bugs due to the key contribution of\nrandomness. However, randomness significantly reduces the efficiency of\nfuzzing, causing it to cost days or weeks to expose bugs. Even though directed\nfuzzing reduces randomness by guiding fuzzing towards target buggy locations,\nthe dilemma of randomness still challenges directed fuzzers. Two critical\ncomponents, which are seeds and mutators, contain randomness and are closely\ntied to the conditions required for triggering bugs. Therefore, to address the\nchallenge of randomness, we propose to use large language models (LLMs) to\nremove the randomness in seeds and reduce the randomness in mutators. With\ntheir strong reasoning and code generation capabilities, LLMs can be used to\ngenerate reachable seeds that target pre-determined locations and to construct\nbug-specific mutators tailored for specific bugs. We propose RandLuzz, which\nintegrates LLMs and directed fuzzing, to improve the quality of seeds and\nmutators, resulting in efficient bug exposure. RandLuzz analyzes function call\nchain or functionality to guide LLMs in generating reachable seeds. To\nconstruct bug-specific mutators, RandLuzz uses LLMs to perform bug analysis,\nobtaining information such as bug causes and mutation suggestions, which\nfurther help generate code that performs bug-specific mutations. We evaluate\nRandLuzz by comparing it with four state-of-the-art directed fuzzers, AFLGo,\nBeacon, WindRanger, and SelectFuzz. With RandLuzz-generated seeds, the fuzzers\nachieve an average speedup ranging from 2.1$\\times$ to 4.8$\\times$ compared to\nusing widely-used initial seeds. Additionally, when evaluated on individual\nbugs, RandLuzz achieves up to a 2.7$\\times$ speedup compared to the\nsecond-fastest exposure. On 8 bugs, RandLuzz can even expose them within 60\nseconds.", "AI": {"tldr": "RandLuzz integrates large language models (LLMs) with directed fuzzing to eliminate randomness in seeds and reduce randomness in mutators, achieving significant speedups in bug exposure compared to existing fuzzers.", "motivation": "Randomness in fuzzing reduces efficiency, requiring extensive time to expose bugs. Directed fuzzing mitigates but does not resolve this issue, necessitating improvements in seed and mutator design to address the randomness dilemma.", "method": "The paper proposes RandLuzz, which uses LLMs to generate bug-targeted seeds for specific code locations and creates bug-specific mutators by analyzing bug causes and mutation suggestions. LLMs construct mutation code to expedite bug detection.", "result": "RandLuzz demonstrates average speedups of 2.1\u20134.8\u00d7 over state-of-the-art directed fuzzers using handcrafted seeds, 2.7\u00d7 speedup for individual bugs, and exposes 8 bugs within 60 seconds during evaluations against AFLGo, Beacon, WindRanger, and SelectFuzz.", "conclusion": "RandLuzz effectively improves the quality of seeds and mutators in directed fuzzing by leveraging LLM capabilities, achieving substantial performance gains in bug detection efficiency and reducing time-to-exposure for critical vulnerabilities."}}
{"id": "2507.22239", "categories": ["cs.CR", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.22239", "abs": "https://arxiv.org/abs/2507.22239", "authors": ["Muhammad Sharshar", "Ahmad Mohammad Saber", "Davor Svetinovic", "Amr M. Youssef", "Deepa Kundur", "Ehab F. El-Saadany"], "title": "Large Language Model-Based Framework for Explainable Cyberattack Detection in Automatic Generation Control Systems", "comment": "Accepted Publication", "summary": "The increasing digitization of smart grids has improved operational\nefficiency but also introduced new cybersecurity vulnerabilities, such as False\nData Injection Attacks (FDIAs) targeting Automatic Generation Control (AGC)\nsystems. While machine learning (ML) and deep learning (DL) models have shown\npromise in detecting such attacks, their opaque decision-making limits operator\ntrust and real-world applicability. This paper proposes a hybrid framework that\nintegrates lightweight ML-based attack detection with natural language\nexplanations generated by Large Language Models (LLMs). Classifiers such as\nLightGBM achieve up to 95.13% attack detection accuracy with only 0.004 s\ninference latency. Upon detecting a cyberattack, the system invokes LLMs,\nincluding GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o mini, to generate\nhuman-readable explanation of the event. Evaluated on 100 test samples, GPT-4o\nmini with 20-shot prompting achieved 93% accuracy in identifying the attack\ntarget, a mean absolute error of 0.075 pu in estimating attack magnitude, and\n2.19 seconds mean absolute error (MAE) in estimating attack onset. These\nresults demonstrate that the proposed framework effectively balances real-time\ndetection with interpretable, high-fidelity explanations, addressing a critical\nneed for actionable AI in smart grid cybersecurity.", "AI": {"tldr": "This paper introduces a hybrid framework combining lightweight ML models for real-time FDIAs detection in smart grid AGC systems with LLM-generated explanations to enhance operator trust and real-world applicability, achieving high accuracy and latency performance.", "motivation": "Smart grid digitization has created real-time attack detection needs with high accuracy, but current ML/DL models lack interpretability, limiting their adoption in critical infrastructure requiring explainable results.", "method": "The system uses LightGBM classifiers (95.13% accuracy, 0.004s latency) for low-latency attack detection and invokes LLMs (GPT-3.5 Turbo, GPT-4 Turbo, GPT-4o mini) with 20-shot prompting to generate real-time human-readable explanations when cyberattacks are detected.", "result": "GPT-4o mini reached 93% accuracy in identifying attack targets (100 samples), 0.075 pu MAE in attack magnitude estimation, and 2.19s MAE in onset time estimation while maintaining the low-latency requirements of smart grid operations.", "conclusion": "The hybrid framework successfully demonstrates high-performance FDIAs detection with accurate LLM-generated explanations, proving the feasibility of integrating AI transparency with real-time smart grid security requirements in a practical implementation."}}
{"id": "2507.22066", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.22066", "abs": "https://arxiv.org/abs/2507.22066", "authors": ["Dylan Manuel", "Paul Rad"], "title": "CodableLLM: Automating Decompiled and Source Code Mapping for LLM Dataset Generation", "comment": null, "summary": "The generation of large, high-quality datasets for code understanding and\ngeneration remains a significant challenge, particularly when aligning\ndecompiled binaries with their original source code. To address this, we\npresent CodableLLM, a Python framework designed to automate the creation and\ncuration of datasets by mapping decompiled functions to their corresponding\nsource functions. This process enhances the alignment between decompiled and\nsource code representations, facilitating the development of large language\nmodels (LLMs) capable of understanding and generating code across multiple\nabstraction levels. CodableLLM supports multiple programming languages and\nintegrates with existing decompilers and parsers to streamline dataset\ngeneration. This paper presents the design and implementation of CodableLLM,\nevaluates its performance in dataset creation, and compares it to existing\ntools in the field. The results demonstrate that CodableLLM offers a robust and\nefficient solution for generating datasets tailored for code-focused LLMS.", "AI": {"tldr": "CodableLLM is a Python framework automating dataset generation for code-focused large language models by aligning decompiled binaries with their original source code, enabling multi-level code understanding and generation.", "motivation": "The paper addresses the challenge of generating large, high-quality datasets for code understanding and generation, particularly the alignment between decompiled binaries and their original source code to train robust LLMs.", "method": "CodableLLM automates dataset creation by mapping decompiled functions to corresponding source functions using existing decompilers and parsers, supporting multiple programming languages.", "result": "Evaluation demonstrates that CodableLLM outperforms existing tools in generating aligned datasets for code-focused LLMs, offering robustness and efficiency.", "conclusion": "CodableLLM provides a reliable solution for generating multi-language code datasets across abstraction levels, advancing the development of code-understanding LLMs through automated alignment of binaries and source code."}}
{"id": "2507.22304", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.22304", "abs": "https://arxiv.org/abs/2507.22304", "authors": ["Chetan Pathade"], "title": "Invisible Injections: Exploiting Vision-Language Models Through Steganographic Prompt Embedding", "comment": "14 Pages", "summary": "Vision-language models (VLMs) have revolutionized multimodal AI applications\nbut introduce novel security vulnerabilities that remain largely unexplored. We\npresent the first comprehensive study of steganographic prompt injection\nattacks against VLMs, where malicious instructions are invisibly embedded\nwithin images using advanced steganographic techniques. Our approach\ndemonstrates that current VLM architectures can inadvertently extract and\nexecute hidden prompts during normal image processing, leading to covert\nbehavioral manipulation. We develop a multi-domain embedding framework\ncombining spatial, frequency, and neural steganographic methods, achieving an\noverall attack success rate of 24.3% (plus or minus 3.2%, 95% CI) across\nleading VLMs including GPT-4V, Claude, and LLaVA, with neural steganography\nmethods reaching up to 31.8%, while maintaining reasonable visual\nimperceptibility (PSNR greater than 38 dB, SSIM greater than 0.94). Through\nsystematic evaluation on 12 diverse datasets and 8 state-of-the-art models, we\nreveal moderate but meaningful vulnerabilities in current VLM architectures and\npropose effective countermeasures. Our findings have significant implications\nfor VLM deployment in security-critical applications and highlight the need for\nproportionate multimodal AI security frameworks.", "AI": {"tldr": "The paper presents the first comprehensive study of steganographic prompt injection attacks against vision-language models (VLMs), demonstrating hidden instructions embedded in images can manipulate model behavior with a 24.3% average success rate.", "motivation": "The authors motivate this work by highlighting the underexplored security risks in VLMs, which are critical for secure AI deployment in sensitive applications.", "method": "They propose a multi-domain embedding framework combining spatial, frequency, and neural steganographic methods to inject hidden prompts while maintaining visual imperceptibility (PSNR >38 dB, SSIM >0.94).", "result": "Across 12 datasets and 8 models (e.g., GPT-4V, Claude), neural steganography achieves up to 31.8% success rates, revealing vulnerabilities despite imperceptible payload embedding.", "conclusion": "The findings indicate moderate but meaningful security gaps in current VLM architectures, emphasizing the need for robust multimodal AI security frameworks to ensure safe deployment."}}
{"id": "2507.22070", "categories": ["cs.SE", "cs.CE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.22070", "abs": "https://arxiv.org/abs/2507.22070", "authors": ["Y. Du"], "title": "Automated Test Data Generation for Enterprise Protobuf Systems: A Metaclass-Enhanced Statistical Approach", "comment": "7 pages", "summary": "Large-scale enterprise systems utilizing Protocol Buffers (protobuf) present\nsignificant challenges for performance testing, particularly when targeting\nintermediate business interfaces with complex nested data structures.\nTraditional test data generation approaches are inadequate for handling the\nintricate hierarchical and graph-like structures inherent in enterprise\nprotobuf schemas. This paper presents a novel test data generation framework\nthat leverages Python's metaclass system for dynamic type enhancement and\nstatistical analysis of production logs for realistic value domain extraction.\nOur approach combines automatic schema introspection, statistical value\ndistribution analysis, and recursive descent algorithms for handling deeply\nnested structures. Experimental evaluation on three real-world enterprise\nsystems demonstrates up to 95\\% reduction in test data preparation time and\n80\\% improvement in test coverage compared to existing approaches. The\nframework successfully handles protobuf structures with up to 15 levels of\nnesting and generates comprehensive test suites containing over 100,000 test\ncases within seconds.", "AI": {"tldr": "A novel framework uses Python metaclasses and statistical log analysis to generate test data for complex Protobuf schemas in enterprise systems, achieving 95% faster preparation and 80% better coverage than existing methods. Handles up to 15-level nesting and generates >100k test cases rapidly.", "motivation": "Enterprise systems with complex nested Protobuf interfaces face inadequate traditional test data generation methods that fail to handle hierarchical/graph-like structures effectively, hindering performance testing of business interfaces.", "method": "Leverages Python metaclass system for dynamic type enhancement combined with production log statistical analysis for value domain extraction. Uses schema introspection, value distribution analysis, and recursive descent algorithms to manage nested structures.", "result": "95% reduction in test data preparation time (vs. existing approaches) and 80% coverage improvement on three real-world enterprise systems; successfully handles 15-level Protobuf nesting and generates >100k test cases in seconds.", "conclusion": "The framework demonstrates significant improvements in efficiency and coverage for testing enterprise Protobuf interfaces with complex nested structures, providing a scalable solution through production-based analysis and programmable schema manipulation."}}
{"id": "2507.22306", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.22306", "abs": "https://arxiv.org/abs/2507.22306", "authors": ["Sahan Sanjaya", "Aruna Jayasena", "Prabhat Mishra"], "title": "SleepWalk: Exploiting Context Switching and Residual Power for Physical Side-Channel Attacks", "comment": null, "summary": "Context switching is utilized by operating systems to change the execution\ncontext between application programs. It involves saving and restoring the\nstates of multiple registers and performing a pipeline flush to remove any\npre-fetched instructions, leading to a higher instantaneous power consumption\ncompared to typical program execution. In this paper, we introduce a physical\npower side-channel leakage source that exploits the power spike observed during\na context switch, triggered by the inbuilt sleep function of the system kernel.\nWe observed that this power spike directly correlates with both the power\nconsumption during context switching and the residual power consumption of the\npreviously executed program. Notably, the persistence of residual power\nsignatures from previous workloads extends the scope of this side-channel\nbeyond extracting the data in registers during the context switch. Unlike\ntraditional approaches that require analyzing full power traces, applying\ncomplex preprocessing, or relying on external synchronization triggers, this\nnovel technique leverages only the amplitude of a single power spike,\nsignificantly simplifying the attack. We developed a power model to illustrate\nthe feasibility of mounting end-to-end side-channel attacks using the\nsleep-induced power spikes. Experimental evaluation demonstrates that our\nframework can successfully perform cryptographic key recovery for both AES and\nSIKE implementations on Broadcom BCM2711.", "AI": {"tldr": "The paper introduces a novel physical power side-channel attack exploiting sleep-induced context switch power spikes for cryptographic key recovery on Broadcom BCM2711, simplifying attacks by using single-spike amplitude without external triggers.", "motivation": "Context switching creates measurable power spikes, and residual power from prior workloads persists, enabling new side-channel attack opportunities without relying on traditional complex trace analysis methods.", "method": "The attack leverages sleep function-triggered context switches to induce power spikes, develops a power model correlating spike amplitude with cryptographic keys, and performs end-to-end experiments using residual power signatures without full trace analysis.", "result": "Successful cryptographic key recovery for AES and SIKE implementations on Broadcom BCM2711 hardware, demonstrating feasibility of attack through single-spike amplitude analysis with no external synchronization triggers required.", "conclusion": "This technique expands side-channel attack capabilities beyond register state extraction by exploiting residual power patterns, and simplifies attacks by eliminating the need for full power traces or complex preprocessing methods."}}
{"id": "2507.22071", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22071", "abs": "https://arxiv.org/abs/2507.22071", "authors": ["Niels Glodny"], "title": "Analyzing and Evaluating the Behavior of Git Diff and Merge", "comment": "Bachelor's thesis", "summary": "Despite being widely used, the algorithms that enable collaboration with Git\nare not well understood. The diff and merge algorithms are particularly\ninteresting, as they could be applied in other contexts. In this thesis, I\ndocument the main functionalities of Git: how diffs are computed, how they are\nused to run merges, and how merges enable more complex operations. In the\nprocess, I show multiple unexpected behaviors in Git, including the following:\nThe histogram diff algorithm has pathological cases where a single-line change\ncan cause the entire rest of the file to be marked as changed. The default\nmerge strategy (ort) can result in merges requiring exponential time in the\nnumber of commits in the history. Merges and rebases are not commutative, and\neven when merges do not result in a conflict, the result is not specified but\ndepends on the diff algorithm used. And finally, sometimes when two sides of a\nmerge add different lines at the same position, the result is not a conflict,\nbut a merge containing both changes after each other, in arbitrary order.", "AI": {"tldr": "This paper reveals critical yet overlooked flaws in Git's diff and merge algorithms, showing they have pathological behaviors, ambiguous outputs, and scalability issues. It systematically documents Git's core mechanisms and their unintended consequences.", "motivation": "Git algorithms enable collaboration but their inner workings are poorly understood, limiting their potential for broader applications. Understanding their exact behavior is crucial to address reliability issues in version control systems.", "method": "The analysis reverses engineered Git's diff (histogram algorithm) and merge (ort strategy) processes through implementation studies and algorithmic modeling. Experiments demonstrated extreme cases like exponential time complexity and non-commutative operations between merges and rebases.", "result": "1. Histogram diff shows a single-line change can mark entire files as modified. 2. Ort merger exhibits exponential execution time with commit numbers. 3. Non-commutativity between merges and rebases disrupts expected behavior. 4. Arbitrarily ordered merging of non-conflicting changes violates predictability assumptions.", "conclusion": "Git's algorithmic weaknesses, including severe performance issues and ambiguous merge semantics, create hidden collaboration risks. These findings demand rigorous algorithmic improvements to ensure reliable version control at scale, particularly for critical software development workflows."}}
{"id": "2507.22347", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.22347", "abs": "https://arxiv.org/abs/2507.22347", "authors": ["Alexander Goldberg", "Giulia Fanti", "Nihar Shah", "Zhiwei Steven Wu"], "title": "Benchmarking Fraud Detectors on Private Graph Data", "comment": null, "summary": "We introduce the novel problem of benchmarking fraud detectors on private\ngraph-structured data. Currently, many types of fraud are managed in part by\nautomated detection algorithms that operate over graphs. We consider the\nscenario where a data holder wishes to outsource development of fraud detectors\nto third parties (e.g., vendors or researchers). The third parties submit their\nfraud detectors to the data holder, who evaluates these algorithms on a private\ndataset and then publicly communicates the results. We propose a realistic\nprivacy attack on this system that allows an adversary to de-anonymize\nindividuals' data based only on the evaluation results. In simulations of a\nprivacy-sensitive benchmark for facial recognition algorithms by the National\nInstitute of Standards and Technology (NIST), our attack achieves near perfect\naccuracy in identifying whether individuals' data is present in a private\ndataset, with a True Positive Rate of 0.98 at a False Positive Rate of 0.00. We\nthen study how to benchmark algorithms while satisfying a formal differential\nprivacy (DP) guarantee. We empirically evaluate two classes of solutions:\nsubsample-and-aggregate and DP synthetic graph data. We demonstrate through\nextensive experiments that current approaches do not provide utility when\nguaranteeing DP. Our results indicate that the error arising from DP trades off\nbetween bias from distorting graph structure and variance from adding random\nnoise. Current methods lie on different points along this bias-variance\ntrade-off, but more complex methods tend to require high-variance noise\naddition, undermining utility.", "AI": {"tldr": "This paper introduces the challenge of benchmarking fraud detectors on private graph data, identifies a de-anonymization attack exploiting evaluation results, and evaluates differential privacy (DP) benchmarks, finding a bias-variance trade-off that limits both utility and privacy.", "motivation": "Automated fraud detection on graphs often requires third-party evaluation of private datasets, necessitating privacy preservation while enabling fair benchmarking.", "method": "1) Propose a de-anonymization attack using public evaluation results. 2) Study and empirically evaluate subsample-and-aggregate and DP synthetic graph methods for benchmarking under formal differential privacy guarantees.", "result": "1) The de-anonymization attack achieves 0.98 TPR and 0% FPR on simulated NIST facial recognition benchmarks. 2) DP benchmarking approaches show high error rates due to bias from distorted graph structures and variance from noise addition.", "conclusion": "Existing DP methods for privacy-preserving benchmarks fail to achieve utility when guaranteeing privacy. The error fundamentally stems from a bias-variance trade-off, with complex methods requiring high-variance noise that undermines usefulness. Improved approaches are needed to balance privacy guarantees with practical efficiency."}}
{"id": "2507.22080", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22080", "abs": "https://arxiv.org/abs/2507.22080", "authors": ["Qiushi Sun", "Jinyang Gong", "Lei Li", "Qipeng Guo", "Fei Yuan"], "title": "CodeEvo: Interaction-Driven Synthesis of Code-centric Data through Hybrid and Iterative Feedback", "comment": "Work in progress", "summary": "Acquiring high-quality instruction-code pairs is essential for training Large\nLanguage Models (LLMs) for code generation. Manually curated data is expensive\nand inherently limited in scale, motivating the development of code-centric\nsynthesis methods. Yet, current approaches either focus on augmenting existing\ncode or rely on predefined heuristics, both lacking rigorous data validation,\nwhich results in synthetic data that is ungrounded, repetitive, or overly\nsimplistic. Inspired by collaborative programming practices, we propose\nCodeEvo, a framework that synthesizes code data through iterative interactions\nbetween two LLM agents: a Coder, which generates candidate code and test cases\nbased on given instructions, and a Reviewer, which guides the synthesis process\nby producing new instructions and feedback. We further introduce a hybrid\nfeedback mechanism that combines compiler determinism with the generative\nflexibility of agents, enabling automatic quality control throughout synthesis.\nExtensive experiments demonstrate that models fine-tuned on CodeEvo data\nsignificantly outperform established baselines across code generation\nbenchmarks with various difficulties. In-depth analyses further provide\ninsights from multiple perspectives into effective code-centric data synthesis.", "AI": {"tldr": "This paper introduces CodeEvo, a framework for iterative code data synthesis using two LLM agents (Coder and Reviewer) with a hybrid feedback mechanism that combines compiler determinism and generative flexibility, resulting in high-quality instruction-code pairs that improve code generation model performance.", "motivation": "Manually curated code generation data is expensive and limited in scale, while existing synthetic methods produce ungrounded, repetitive, or simplistic code lacking rigorous validation without reliable quality assurance processes.", "method": "Develops CodeEvo framework with (1) Agent Collaboration: Coder agent generates candidate code/test cases, Reviewer agent proposes instructions/feedback to refine the coding process and (2) Hybrid Feedback: Merges compiler-determined correctness with agent-suggested instructions to provide automated data quality validation across iterations.", "result": "Models fine-tuned on CodeEvo-synthesized data achieve significant performance improvements over established baselines across code generation benchmarks with varying difficulties, out-scoring existing methods in multiple evaluation metrics.", "conclusion": "CodeEvo demonstrates that combining LLM agent collaboration with deterministic verification mechanisms provides a scalable solution for generating grounded, diverse, and complex code training data, offering actionable insights for optimizing code-centric data synthesis pipelines."}}
{"id": "2507.22371", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22371", "abs": "https://arxiv.org/abs/2507.22371", "authors": ["Lei Yu", "Shiqi Cheng", "Zhirong Huang", "Jingyuan Zhang", "Chenjie Shen", "Junyi Lu", "Li Yang", "Fengjun Zhang", "Jiajia Ma"], "title": "SAEL: Leveraging Large Language Models with Adaptive Mixture-of-Experts for Smart Contract Vulnerability Detection", "comment": "Accepted to ICSME 2025", "summary": "With the increasing security issues in blockchain, smart contract\nvulnerability detection has become a research focus. Existing vulnerability\ndetection methods have their limitations: 1) Static analysis methods struggle\nwith complex scenarios. 2) Methods based on specialized pre-trained models\nperform well on specific datasets but have limited generalization capabilities.\nIn contrast, general-purpose Large Language Models (LLMs) demonstrate\nimpressive ability in adapting to new vulnerability patterns. However, they\noften underperform on specific vulnerability types compared to methods based on\nspecialized pre-trained models. We also observe that explanations generated by\ngeneral-purpose LLMs can provide fine-grained code understanding information,\ncontributing to improved detection performance.\n  Inspired by these observations, we propose SAEL, an LLM-based framework for\nsmart contract vulnerability detection. We first design targeted prompts to\nguide LLMs in identifying vulnerabilities and generating explanations, which\nserve as prediction features. Next, we apply prompt-tuning on CodeT5 and T5 to\nprocess contract code and explanations, enhancing task-specific performance. To\ncombine the strengths of each approach, we introduce an Adaptive\nMixture-of-Experts architecture. This dynamically adjusts feature weights via a\nGating Network, which selects relevant features using TopK filtering and\nSoftmax normalization, and incorporates a Multi-Head Self-Attention mechanism\nto enhance cross-feature relationships. This design enables effective\nintegration of LLM predictions, explanation features, and code features through\ngradient optimization. The loss function jointly considers both independent\nfeature performance and overall weighted predictions. Experiments show that\nSAEL outperforms existing methods across various vulnerabilities.", "AI": {"tldr": "SAEL is an LLM-based framework that enhances smart contract vulnerability detection by integrating general and specialized models through an adaptive mixture-of-experts architecture.", "motivation": "Traditional static analysis tools struggle with complex scenarios, and existing specialized pre-trained models lack generalization capabilities while general-purpose LLMs underperform on specific vulnerability types despite generating useful explanations.", "method": "The framework employs three core components: (1) targeted prompts to guide LLMs for vulnerability detection and explanation generation, (2) prompt-tuning applied to CodeT5/T5 models to process contract code with explanations, and (3) an Adaptive Mixture-of-Experts architecture featuring a Gating Network with TopK filtering, Softmax normalization, and Multi-Head Self-Attention for optimal feature integration through gradient optimization.", "result": "Comprehensive experiments demonstrate SAEL outperforms existing methods across various vulnerability types, achieving better performance through the synergistic combination of LLM predictions, explanation features, and code features.", "conclusion": "SAEL effectively addresses the limitations of current approaches by combining the adaptability of LLMs with the precision of specialized models, achieving state-of-the-art vulnerability detection performance via novel feature fusion architecture."}}
{"id": "2507.22085", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22085", "abs": "https://arxiv.org/abs/2507.22085", "authors": ["Vaani Goenka", "Aalok D. Thakkar"], "title": "BOOP: Write Right Code", "comment": null, "summary": "Novice programmers frequently adopt a syntax-specific and test-case-driven\napproach, writing code first and adjusting until programs compile and test\ncases pass, rather than developing correct solutions through systematic\nreasoning. AI coding tools exacerbate this challenge by providing syntactically\ncorrect but conceptually flawed solutions. In this paper, we introduce BOOP\n(Blueprint, Operations, OCaml, Proof), a structured framework requiring four\nmandatory phases: formal specification, language-agnostic algorithm\ndevelopment, implementation, and correctness proof. This shifts focus from\n``making code work'' to understanding why code is correct.\n  BOOP was implemented at our institution using a VS Code extension and\npreprocessor that enforces constraints and identifies counterproductive\npatterns. Initial evaluation shows improved algorithmic reasoning and reduced\ntrial-and-error debugging. Students reported better edge case understanding and\nproblem decomposition, though some initially found the format verbose.\nInstructors observed stronger foundational skills compared to traditional\napproaches.", "AI": {"tldr": "The paper introduces BOOP, a structured programming education framework that shifts focus from trial-and-error code execution to systematic reasoning about correctness, implemented via a VS Code extension with promising results in improving student skills.", "motivation": "Novice programmers often use syntax-first and test-case-driven approaches, leading to ineffective learning and conceptual misunderstandings. AI coding tools may compound this by offering syntactically correct but logically flawed solutions.", "method": "A four-phase framework (Blueprint: formal specification, Operations: language-agnostic algorithm design, Implementation: code writing, Proof: correctness validation) enforced by a VS Code extension and preprocessor that restricts counterproductive patterns.", "result": "Initial evaluation shows improved algorithmic reasoning and problem decomposition in students, reduced trial-and-error debugging, with instructors noting stronger foundational skills despite initial student feedback about the framework's verbosity.", "conclusion": "BOOP's structured approach successfully redirects learning focus toward understanding correctness principles, demonstrating effectiveness in mitigating dependence on trial-and-error and AI-generated syntactically correct but conceptually flawed solutions."}}
{"id": "2507.22447", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22447", "abs": "https://arxiv.org/abs/2507.22447", "authors": ["Zhihong Liang", "Xin Wang", "Zhenhuang Hu", "Liangliang Song", "Lin Chen", "Jingjing Guo", "Yanbin Wang", "Ye Tian"], "title": "Breaking Obfuscation: Cluster-Aware Graph with LLM-Aided Recovery for Malicious JavaScript Detection", "comment": null, "summary": "With the rapid expansion of web-based applications and cloud services,\nmalicious JavaScript code continues to pose significant threats to user\nprivacy, system integrity, and enterprise security. But, detecting such threats\nremains challenging due to sophisticated code obfuscation techniques and\nJavaScript's inherent language characteristics, particularly its nested closure\nstructures and syntactic flexibility. In this work, we propose DeCoda, a hybrid\ndefense framework that combines large language model (LLM)-based deobfuscation\nwith code graph learning: (1) We first construct a sophisticated\nprompt-learning pipeline with multi-stage refinement, where the LLM\nprogressively reconstructs the original code structure from obfuscated inputs\nand then generates normalized Abstract Syntax Tree (AST) representations; (2)\nIn JavaScript ASTs, dynamic typing scatters semantically similar nodes while\ndeeply nested functions fracture scope capturing, introducing structural noise\nand semantic ambiguity. To address these challenges, we then propose to learn\nhierarchical code graph representations via a Cluster-wise Graph that\nsynergistically integrates graph transformer network, node clustering, and\nnode-to-cluster attention to simultaneously capture both local node-level\nsemantics and global cluster-induced structural relationships from AST graph.\nExperimental results demonstrate that our method achieves F1-scores of 94.64%\nand 97.71% on two benchmark datasets, demonstrating absolute improvements of\n10.74% and 13.85% over state-of-the-art baselines. In false-positive control\nevaluation at fixed FPR levels (0.0001, 0.001, 0.01), our approach delivers\n4.82, 5.91, and 2.53 higher TPR respectively compared to the best-performing\nbaseline. These results highlight the effectiveness of LLM-based deobfuscation\nand underscore the importance of modeling cluster-level relationships in\ndetecting malicious code.", "AI": {"tldr": "The paper proposes DeCoda, a hybrid defense framework combining LLM-based deobfuscation and code graph learning to detect malicious JavaScript code, outperforming state-of-the-art baselines in F1-scores and true positive rates.", "motivation": "Malicious JavaScript detection is challenging due to code obfuscation techniques, nested closure structures, and JavaScript's syntactic flexibility, which create structural noise and semantic ambiguity in static code analysis.", "method": "DeCoda uses (1) a multi-stage LLM prompt-learning pipeline for code reconstruction and AST normalization, and (2) a Cluster-wise Graph approach integrating graph transformer networks, node clustering, and node-to-cluster attention to learn hierarchical code representations from AST graphs.", "result": "Achieved 94.64% and 97.71% F1-scores on two benchmarks (10.74+13.85% absolute improvements over SOTA). At fixed FPR levels (0.0001/0.001/0.01), delivered 4.82/5.91/2.53 higher TPR than best baseline respectively.", "conclusion": "LLM-based deobfuscation and cluster-level modeling in code graphs are crucial for JavaScript malware detection. DeCoda demonstrates significant improvements over existing methods in handling obfuscation and language-specific challenges."}}
{"id": "2507.22086", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.22086", "abs": "https://arxiv.org/abs/2507.22086", "authors": ["Honghua Dong", "Jiacheng Yang", "Xun Deng", "Yuhe Jiang", "Gennady Pekhimenko", "Fan Long", "Xujie Si"], "title": "TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories", "comment": null, "summary": "Type inference for dynamic languages like Python is a persistent challenge in\nsoftware engineering. While large language models (LLMs) have shown promise in\ncode understanding, their type inference capabilities remain underexplored. We\nintroduce TypyBench, a benchmark designed to evaluate LLMs' type inference\nacross entire Python repositories. TypyBench features two novel metrics:\nTypeSim, which captures nuanced semantic relationships between predicted and\nground truth types, and TypeCheck, which assesses type consistency across\ncodebases. Our evaluation of various LLMs on a curated dataset of 50\nhigh-quality Python repositories reveals that, although LLMs achieve decent\nTypeSim scores, they struggle with complex nested types and exhibit significant\ntype consistency errors. These findings suggest that future research should\nshift focus from improving type similarity to addressing repository-level\nconsistency. TypyBench provides a foundation for this new direction, offering\ninsights into model performance across different type complexities and usage\ncontexts. Our code and data are available at\nhttps://github.com/typybench/typybench.", "AI": {"tldr": "TypyBench is a benchmark for evaluating type inference in dynamic languages using LLMs, revealing that while models perform well on TypeSim, they struggle with type consistency and nested types.", "motivation": "The paper addresses the underexplored effectiveness of large language models (LLMs) in type inference for dynamic languages like Python, highlighting persistent challenges in software engineering and the need for repository-level evaluation benchmarks.", "method": "TypyBench introduces two metrics: TypeSim (measuring semantic relationships between predicted/ground-truth types) and TypeCheck (assessing type consistency across codebases). Models were tested on a dataset of 50 high-quality Python repositories.", "result": "LLMs achieved decent TypeSim scores but exhibited poor performance on complex nested types and large inconsistencies across codebases.", "conclusion": "The work establishes TypyBench as a foundational framework for prioritizing repository-level consistency over type similarity in future LLM research, offering insights into performance across varying type complexities."}}
{"id": "2507.22611", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.22611", "abs": "https://arxiv.org/abs/2507.22611", "authors": ["Chunyi Zhang", "Fengjiao Dou", "Xiaoqi Li"], "title": "DoS Attacks and Defense Technologies in Blockchain Systems: A Hierarchical Analysis", "comment": null, "summary": "Blockchain technology is widely used in various fields due to its ability to\nprovide decentralization and trustless security. This is a fundamental\nunderstanding held by many advocates, but it is misunderstood, leading\nparticipants to fail to recognize the limitations of the security that\nblockchain can provide. Among all current network attacks, Denial of Service\n(DoS) attacks pose significant threats due to their ease of execution and\ndestructive potential. This paper, based on the blockchain architecture\nhierarchy, categorizes and organizes existing DoS attacks, with a focus on\nexplaining the principles and methods of contract layer and consensus layer DoS\nattacks. Furthermore, this paper comprehensively analyzes and compares commonly\nused detection methods and defense technologies, which will contribute to\nstrengthening the security and stability of blockchain systems and promoting\nfurther innovation and application of blockchain systems.", "AI": {"tldr": "This paper addresses the misconception that blockchain provides absolute security, focusing on categorizing DoS attacks at the contract and consensus layers, and analyzing detection/defense methods to improve blockchain security and stability.", "motivation": "Participants often overlook blockchain's security limitations, and DoS attacks are a critical threat due to their ease and impact. The paper aims to clarify these issues to enhance system resilience.", "method": "Categorizes existing DoS attacks according to blockchain architecture, examines principles/methods of contract and consensus layer attacks, and compares detection/defense technologies.", "result": "Comprehensive classification of DoS attacks and an analysis of methods to strengthen security, enabling better innovation and application of blockchain systems.", "conclusion": "The study highlights necessary security improvements against DoS attacks, offering insights for robust blockchain development and deployment."}}
{"id": "2507.22223", "categories": ["cs.SE", "D.2; D.2.4; D.4.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.22223", "abs": "https://arxiv.org/abs/2507.22223", "authors": ["Kiana Kiashemshaki", "Mohammad Jalili Torkamani", "Negin Mahmoudi"], "title": "Secure coding for web applications: Frameworks, challenges, and the role of LLMs", "comment": "11 pages, 5 figures, 3 tables, 6 listings", "summary": "Secure coding is a critical yet often overlooked practice in software\ndevelopment. Despite extensive awareness efforts, real-world adoption remains\ninconsistent due to organizational, educational, and technical barriers. This\npaper provides a comprehensive review of secure coding practices across major\nframeworks and domains, including web development, DevSecOps, and cloud\nsecurity. It introduces a structured framework comparison and categorizes\nthreats aligned with the OWASP Top 10. Additionally, we explore the rising role\nof Large Language Models (LLMs) in evaluating and recommending secure code,\npresenting a reproducible case study across four major vulnerability types.\nThis paper offers practical insights for researchers, developers, and educators\non integrating secure coding into real-world development processes.", "AI": {"tldr": "This paper reviews secure coding practices across frameworks and domains, introduces a threat categorization aligned with OWASP Top 10, and explores the role of LLMs in secure code evaluation through a case study.", "motivation": "Secure coding is critical but underadopted due to organizational, educational, and technical barriers, necessitating actionable research and frameworks for integration.", "method": "Comprehensive review of secure coding practices in web development, DevSecOps, and cloud security; structured framework comparison; OWASP-based threat categorization; reproducible LLM case study across four vulnerability types.", "result": "The LLM case study demonstrates promising potential for automating secure coding evaluation and recommendations, while framework comparisons highlight existing adoption challenges.", "conclusion": "The paper provides practical pathways for integrating secure coding into development workflows and establishes LLMs as viable tools for improving security practices through reproducible analysis."}}
{"id": "2507.22617", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22617", "abs": "https://arxiv.org/abs/2507.22617", "authors": ["Yiting Qu", "Ziqing Yang", "Yihan Ma", "Michael Backes", "Savvas Zannettou", "Yang Zhang"], "title": "Hate in Plain Sight: On the Risks of Moderating AI-Generated Hateful Illusions", "comment": "Accepted at ICCV 2025", "summary": "Recent advances in text-to-image diffusion models have enabled the creation\nof a new form of digital art: optical illusions--visual tricks that create\ndifferent perceptions of reality. However, adversaries may misuse such\ntechniques to generate hateful illusions, which embed specific hate messages\ninto harmless scenes and disseminate them across web communities. In this work,\nwe take the first step toward investigating the risks of scalable hateful\nillusion generation and the potential for bypassing current content moderation\nmodels. Specifically, we generate 1,860 optical illusions using Stable\nDiffusion and ControlNet, conditioned on 62 hate messages. Of these, 1,571 are\nhateful illusions that successfully embed hate messages, either overtly or\nsubtly, forming the Hateful Illusion dataset. Using this dataset, we evaluate\nthe performance of six moderation classifiers and nine vision language models\n(VLMs) in identifying hateful illusions. Experimental results reveal\nsignificant vulnerabilities in existing moderation models: the detection\naccuracy falls below 0.245 for moderation classifiers and below 0.102 for VLMs.\nWe further identify a critical limitation in their vision encoders, which\nmainly focus on surface-level image details while overlooking the secondary\nlayer of information, i.e., hidden messages. To address this risk, we explore\npreliminary mitigation measures and identify the most effective approaches from\nthe perspectives of image transformations and training-level strategies.", "AI": {"tldr": "The paper investigates risks of text-to-image diffusion models for generating scalable, concealable hate messages in optical illusions (hateful illusions). A dataset of 1,571 such illusions was created to expose content moderation model vulnerabilities, revealing detection failures (accuracies <24.5% for classifiers, <10.2% for VLMs) due to vision encoders neglecting secondary hidden messages.", "motivation": "Text-to-image AI systems like Stable Diffusion now enable mass creation of optical illusions. This poses risks as adversaries could encode harmful hate messages into seemingly benign images to evade content moderation, threatening online information security and public discourse.", "method": "Generated 1,860 optical illusions using 62 hate messages with Stable Diffusion and ControlNet. Systematically evaluated 6 content moderation classifiers and 9 vision language models, quantifying their failure to detect hidden layers through accuracy measurements and vision encoder analysis.", "result": "Moderation models fail catastrophically at detecting hateful illusions (24.5% accuracy threshold or lower). Vision language models perform worse (10.2% accuracy cutoff) since their vision encoders prioritize surface-level details over hidden secondary information.", "conclusion": "Current content detection systems are highly vulnerable to optical illusion-based misinformation. Mitigation requires addressing vision encoder limitations through image transformation techniques and training strategy modifications to identify concealed messages without compromising creative capabilities."}}
{"id": "2507.22324", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22324", "abs": "https://arxiv.org/abs/2507.22324", "authors": ["Cameron S. Movassaghi", "Amanda Momenzadeh", "Jesse G. Meyer"], "title": "From Articles to Code: On-Demand Generation of Core Algorithms from Scientific Publications", "comment": null, "summary": "Maintaining software packages imposes significant costs due to dependency\nmanagement, bug fixes, and versioning. We show that rich method descriptions in\nscientific publications can serve as standalone specifications for modern large\nlanguage models (LLMs), enabling on-demand code generation that could supplant\nhuman-maintained libraries. We benchmark state-of-the-art models\n(GPT-o4-mini-high, Gemini Pro 2.5, Claude Sonnet 4) by tasking them with\nimplementing a diverse set of core algorithms drawn from original publications.\nOur results demonstrate that current LLMs can reliably reproduce package\nfunctionality with performance indistinguishable from conventional libraries.\nThese findings foreshadow a paradigm shift toward flexible, on-demand code\ngeneration and away from static, human-maintained packages, which will result\nin reduced maintenance overhead by leveraging published articles as sufficient\ncontext for the automated implementation of analytical workflows.", "AI": {"tldr": "Current large language models can generate code from scientific method descriptions in papers, reducing maintenance costs of traditional libraries.", "motivation": "Addressing high maintenance costs of software packages through automated code generation.", "method": "Benchmarked state-of-the-art LLMs (GPT-o4-mini-high, Gemini Pro 2.5, Claude Sonnet 4) for implementing core algorithms from original scientific publications.", "result": "LLMs reliably reproduced package functionality with performance comparable to conventional libraries across diverse algorithms.", "conclusion": "This shift towards on-demand code generation using scientific publications as context may eliminate static package maintenance overhead."}}
{"id": "2507.22674", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.22674", "abs": "https://arxiv.org/abs/2507.22674", "authors": ["Ramprasad Sarkar"], "title": "Cryptanalysis of LC-MUME: A Lightweight Certificateless Multi-User Matchmaking Encryption for Mobile Devices", "comment": null, "summary": "Yang et al. proposed a lightweight certificateless multiuser matchmaking\nencryption (LC-MUME) scheme for mobile devices, published in IEEE Transactions\non Information Forensics and Security (TIFS) (DOI: 10.1109/TIFS.2023.3321961).\nTheir construction aims to reduce computational and communication overhead\nwithin a one-to-many certificateless cryptographic framework. The authors claim\nthat their scheme satisfies existential unforgeability under chosen-message\nattacks (EUF-CMA) in the random oracle model. However, our cryptanalytic study\ndemonstrates that the scheme fails to meet this critical security requirement.\nIn particular, we show that a Type-I adversary can successfully forge a valid\nciphertext without possessing the complete private key of the sender. Both\ntheoretical analysis and practical implementation confirm that this attack can\nbe mounted with minimal computational cost. To address these weaknesses, we\npropose a modification strategy to strengthen the security of matchmaking\nencryption schemes in mobile computing environments.", "AI": {"tldr": "A cryptanalysis of Yang et al.'s LC-MUME scheme reveals a critical security flaw violating EUF-CMA claims, along with proposed security enhancement strategies.", "motivation": "This paper addresses the security gap in Yang et al.'s lightweight certificateless encryption framework by exposing vulnerabilities and proposing improvements for mobile devices.", "method": "The authors conducted theoretical analysis of the LC-MUME scheme's security proofs and implemented practical attacks demonstrating ciphertext forgery by Type-I adversaries without requiring full private keys. They then developed modification strategies to remediate these weaknesses.", "result": "Cryptanalytic results validate the attack feasibility with negligible computational cost. The proposed modification strategies resolve the security issues while maintaining the framework's lightweight characteristics.", "conclusion": "The study demonstrates vulnerabilities in certificateless encryption schemes' security claims, establishes the need for rigorous cryptographic validation, and contributes practical countermeasures to secure mobile computing environments through improved encryption protocols."}}
{"id": "2507.22414", "categories": ["cs.SE", "D.2; I.2"], "pdf": "https://arxiv.org/pdf/2507.22414", "abs": "https://arxiv.org/abs/2507.22414", "authors": ["Sungmin Kang", "Haifeng Ruan", "Abhik Roychoudhury"], "title": "AutoCodeSherpa: Symbolic Explanations in AI Coding Agents", "comment": null, "summary": "Large Language Model (LLM) agents autonomously use external tools on top of\none or more LLMs to accomplish specific tasks. Lately LLM agents for software\nengineering tasks have become popular. These agents can benefit from the use of\nprogram analysis tools working on program representations. This is demonstrated\nby existing agentic AI solutions such as AutoCodeRover or SpecRover which\nperform automated program repair. Specifically the goal of these works is to\nuse program analysis to improve the patch quality. These agents are currently\nbeing used to automatically fix static analysis issues from the widely used\nSonarQube static analyzer.\n  Nevertheless, for the agents to be deployed in a production environment,\nagents need to suggest software artifacts, such as patches, with evidence and\nwith high confidence. In this work, we provide a workflow where an agent\nprovides explanations of the bug in the form of symbolic formulae. The\nexplanations are in the form of input conditions, infection conditions and\noutput conditions, implemented as property based tests (PBT) and\nprogram-internal symbolic expressions. These can help in human developer\ncognition of the agent outputs as well as in achieving completely automated\nagentic workflows for software. The human developer can benefit from the input\ncondition, represented as a PBT, to generate various concrete inputs showing a\ngiven issue. Furthermore, since the PBTs are executable, our explanations are\nexecutable as well. We can thus also use the explanations in a completely\nautomated issue resolution environment for accepting or rejecting the patches\nthat are suggested by patching agents such as AutoCodeRover. Finally, as\nagentic AI approaches continue to develop, the program analysis driven\nexplanations can be provided to other LLM-based repair techniques such as\nAgentless to improve their output.", "AI": {"tldr": "This paper introduces a workflow for LLM agents in software engineering where bugs are explained via symbolic formulae (input/output/infection conditions) as executable property-based tests, enhancing both human comprehension and automated evaluation of suggested patches.", "motivation": "LLM agents for software tasks require higher confidence and explainability in their outputs to be used in production environments, where decisions must be evidence-backed and transparent.", "method": "The proposed workflow generates executable explanations using symbolic formulae structured as input conditions, infection conditions, and output conditions. These are implemented as property-based tests (PBT) and program-internal symbolic expressions, enabling automation and aiding developer understanding.", "result": "The explanations can be directly used to generate concrete test cases for issues, support automated patch acceptance/rejection, and improve the quality of LLM-based repair techniques by providing verifiable reasoning.", "conclusion": "Program analysis-driven symbolic explanations for LLM agents not only clarify repair decisions but also enable fully automated workflows and cross-agent improvements, addressing critical gaps in reliability and interpretability for production deployment."}}
{"id": "2507.22772", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22772", "abs": "https://arxiv.org/abs/2507.22772", "authors": ["Ahmed Sabbah", "Radi Jarrar", "Samer Zein", "David Mohaisen"], "title": "Empirical Evaluation of Concept Drift in ML-Based Android Malware Detection", "comment": "18 pages, 12 tables, 14 figures, paper under review", "summary": "Despite outstanding results, machine learning-based Android malware detection\nmodels struggle with concept drift, where rapidly evolving malware\ncharacteristics degrade model effectiveness. This study examines the impact of\nconcept drift on Android malware detection, evaluating two datasets and nine\nmachine learning and deep learning algorithms, as well as Large Language Models\n(LLMs). Various feature types--static, dynamic, hybrid, semantic, and\nimage-based--were considered. The results showed that concept drift is\nwidespread and significantly affects model performance. Factors influencing the\ndrift include feature types, data environments, and detection methods.\nBalancing algorithms helped with class imbalance but did not fully address\nconcept drift, which primarily stems from the dynamic nature of the malware\nlandscape. No strong link was found between the type of algorithm used and\nconcept drift, the impact was relatively minor compared to other variables\nsince hyperparameters were not fine-tuned, and the default algorithm\nconfigurations were used. While LLMs using few-shot learning demonstrated\npromising detection performance, they did not fully mitigate concept drift,\nhighlighting the need for further investigation.", "AI": {"tldr": "The paper investigates concept drift in Android malware detection by comparing two datasets, nine algorithms (ML, DL, LLMs), and five feature types, revealing its widespread impact and the insufficiency of balancing techniques for long-term mitigation.", "motivation": "Existing machine learning models for Android malware detection face challenges with concept drift due to rapidly evolving malware characteristics, necessitating strategies to maintain detection effectiveness.", "method": "Evaluated concept drift across two datasets and nine algorithms (including LLMs) using static, dynamic, hybrid, semantic, and image-based features, employing default configurations without hyperparameter tuning.", "result": "Concept drift significantly affects model performance, influenced by feature types and data environments. Balancing algorithms partially address class imbalance but fail to resolve drift, with LLMs showing modest promise.", "conclusion": "Concept drift remains a critical challenge due to dynamic malware evolution, unrelated to algorithm type but requiring deeper exploration of strategies, including better hyperparameter tuning and LLMs' potential."}}
{"id": "2507.22442", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22442", "abs": "https://arxiv.org/abs/2507.22442", "authors": ["Yukai Zhao", "Shaohua Wang", "Jue Wang", "Xing Hu", "Xin Xia"], "title": "Ensemble Fuzzing with Dynamic Resource Scheduling and Multidimensional Seed Evaluation", "comment": "first submit", "summary": "Fuzzing is widely used for detecting bugs and vulnerabilities, with various\ntechniques proposed to enhance its effectiveness. To combine the advantages of\nmultiple technologies, researchers proposed ensemble fuzzing, which integrates\nmultiple base fuzzers. Despite promising results, state-of-the-art ensemble\nfuzzing techniques face limitations in resource scheduling and performance\nevaluation, leading to unnecessary resource waste. In this paper, we propose\nLegion, a novel ensemble fuzzing framework that dynamically schedules resources\nduring the ensemble fuzzing campaign. We designed a novel resource scheduling\nalgorithm based on the upper confidence bound algorithm to reduce the resource\nconsumption of ineffective base fuzzers. Additionally, we introduce a\nmultidimensional seed evaluation strategy, which considers multiple metrics to\nachieve more comprehensive fine-grained performance evaluation. We implemented\nLegion as a prototype tool and evaluated its effectiveness on Google's\nfuzzer-test-suite as well as real-world open-source projects. Results show that\nLegion outperforms existing state-of-the-art base fuzzers and ensemble fuzzing\ntechniques, detecting 20 vulnerabilities in real-world open-source\nprojects-five previously unknown and three classified as CVEs.", "AI": {"tldr": "Legion is a new ensemble fuzzing framework that dynamically allocates resources using an upper confidence bound algorithm and multidimensional seed evaluation, enhancing bug detection efficiency and effectiveness.", "motivation": "Existing ensemble fuzzing techniques waste resources due to suboptimal scheduling and lack comprehensive performance evaluation methods.", "method": "We introduce Legion with: 1) a dynamic resource scheduling algorithm based on the upper confidence bound (UCB) for adaptive fuzzer prioritization, and 2) a multidimensional seed evaluation strategy considering coverage, mutation utility, and edge frequency.", "result": "Legion detected 20 total vulnerabilities (with 5 novel bugs and 3 CVEs) in real-world open-source projects, outperforming state-of-the-art base/ensemble fuzzing approaches across benchmark tests.", "conclusion": "Legion demonstrates significant improvements in ensemble fuzzing by addressing resource scheduling inefficiencies and enabling fine-grained performance evaluation through novel algorithmic contributions."}}
{"id": "2507.22538", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22538", "abs": "https://arxiv.org/abs/2507.22538", "authors": ["Matilde Gargiani", "Robin Sieber", "Philip Pawlowsky", "John Lygeros"], "title": "Inside madupite: Technical Design and Performance", "comment": null, "summary": "In this work, we introduce and benchmark madupite, a newly proposed\nhigh-performance solver designed for large-scale discounted infinite-horizon\nMarkov decision processes with finite state and action spaces. After a brief\noverview of the class of mathematical optimization methods on which madupite\nrelies, we provide details on implementation choices, technical design and\ndeployment. We then demonstrate its scalability and efficiency by showcasing\nits performance on the solution of Markov decision processes arising from\ndifferent application areas, including epidemiology and classical control.\nMadupite sets a new standard as, to the best of our knowledge, it is the only\nsolver capable of efficiently computing exact solutions for large-scale Markov\ndecision processes, even when these exceed the memory capacity of modern\nlaptops and operate in near-undiscounted settings. This is possible as madupite\ncan work in a fully distributed manner and therefore leverage the memory\nstorage and computation capabilities of modern high-performance computing\nclusters. This key feature enables the solver to efficiently handle problems of\nmedium to large size in an exact manner instead of necessarily resorting to\nfunction approximations. Moreover, madupite is unique in allowing users to\ncustomize the solution algorithm to better exploit the specific structure of\ntheir problem, significantly accelerating convergence especially in\nlarge-discount factor settings. Overall, madupite represents a significant\nadvancement, offering unmatched scalability and flexibility in solving\nlarge-scale Markov decision processes.", "AI": {"tldr": "Madupite is a distributed solver for large-scale MDPs enabling exact solutions without function approximation.", "motivation": "Existing solvers struggle with large-scale discounted infinite-horizon MDPs, especially in near-undiscounted settings that require exact solutions beyond memory capacity of standard hardware.", "method": "The solver uses mathematical optimization methods with distributed computing architecture to leverage high-performance clusters, and offers algorithm customization to exploit problem-specific structures for faster convergence.", "result": "Madupite outperforms current methods in scalability and efficiency across applications in epidemiology and control, solving MDPs exceeding typical laptop memory limits.", "conclusion": "Madupite establishes a new standard as the first distributed MDP solver capable of exact large-scale solutions with customizable algorithms, significantly advancing MDP computation capabilities."}}
{"id": "2507.22580", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22580", "abs": "https://arxiv.org/abs/2507.22580", "authors": ["Marcos Fuster-Pena", "David de-Fitero-Dominguez", "Antonio Garcia-Cabot", "Eva Garcia-Lopez"], "title": "RePaCA: Leveraging Reasoning Large Language Models for Static Automated Patch Correctness Assessment", "comment": null, "summary": "Automated Program Repair (APR) seeks to automatically correct software bugs\nwithout requiring human intervention. However, existing tools tend to generate\npatches that satisfy test cases without fixing the underlying bug, those are\nknown as overfitting patches. To address this issue, Automated Patch\nCorrectness Assessment (APCA) attempts to identify overfitting patches\ngenerated by APR tools. It can be solved as a static approach, meaning that no\nadditional information is needed beyond the original and fixed code snippets.\nCurrent static techniques often struggle with reliability, flexibility and\ntransparency. To address these issues, we introduce RePaCA, a novel static APCA\ntechnique that leverages Large Language Models (LLMs) specialized in thinking\ntasks. Our model is prompted with both buggy and fixed code snippets and guided\nto generate a Chain of Thought that analyses code differences, reasons about\nhow the patch addresses the root cause, and ultimately provides a binary\nclassification: correct or overfitting. To enhance these reasoning capabilities\nfor the APCA task specifically, the LLM is finetuned using Reinforcement\nLearning with the Group Relative Policy Optimization algorithm. When evaluated\non a standard Defects4J-derived test, our approach achieves state-of-the-art\nperformance, with 83.1% accuracy and an 84.8% F1-score. Furthermore, our model\ndemonstrates superior generalization capabilities when trained on different\ndatasets, outperforming the leading technique. This reasoning capability also\nprovides enhanced explainability for the patch assessment. These findings\nunderscore the considerable promise of finetuned, reasoning LLMs to advance\nstatic APCA by enhancing accuracy, generalization, and explainability.", "AI": {"tldr": "RePaCA introduces an LLM-based static APCA technique enhanced through reinforcement learning, achieving SoTA performance with 83.1% accuracy and 84.8% F1-score by addressing overfitting patch issues in APR tools. It improves accuracy, generalization, and explainability.", "motivation": "Existing APR tools produce overfitting patches that pass tests without fixing root causes. Current static APCA approaches lack reliability, flexibility, and transparency in assessing patch correctness.", "method": "RePaCA utilizes LLMs specialized in reasoning tasks. The model is prompted with buggy/fixed code pairs and trained via Group Relative Policy Optimization (Reinforcement Learning) to generate a correctness classification and Chain of Thought analysis.", "result": "RePaCA achieves 83.1% accuracy and 84.8% F1-score on Defects4J benchmark, demonstrating superior generalization across datasets compared to leading techniques. Generates explainable reasoning for assessments.", "conclusion": "Finetuned reasoning LLMs significantly enhance static APCA capabilities, offering improvements in accuracy, cross-dataset generalization, and patch assessment explainability compared to existing methods."}}
{"id": "2507.22610", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22610", "abs": "https://arxiv.org/abs/2507.22610", "authors": ["Ali Asgari", "Milan de Koning", "Pouria Derakhshanfar", "Annibale Panichella"], "title": "Metamorphic Testing of Deep Code Models: A Systematic Literature Review", "comment": null, "summary": "Large language models and deep learning models designed for code intelligence\nhave revolutionized the software engineering field due to their ability to\nperform various code-related tasks. These models can process source code and\nsoftware artifacts with high accuracy in tasks such as code completion, defect\ndetection, and code summarization; therefore, they can potentially become an\nintegral part of modern software engineering practices. Despite these\ncapabilities, robustness remains a critical quality attribute for deep-code\nmodels as they may produce different results under varied and adversarial\nconditions (e.g., variable renaming). Metamorphic testing has become a widely\nused approach to evaluate models' robustness by applying semantic-preserving\ntransformations to input programs and analyzing the stability of model outputs.\nWhile prior research has explored testing deep learning models, this systematic\nliterature review focuses specifically on metamorphic testing for deep code\nmodels. By studying 45 primary papers, we analyze the transformations,\ntechniques, and evaluation methods used to assess robustness. Our review\nsummarizes the current landscape, identifying frequently evaluated models,\nprogramming tasks, datasets, target languages, and evaluation metrics, and\nhighlights key challenges and future directions for advancing the field.", "AI": {"tldr": "This paper provides a systematic literature review analyzing metamorphic testing approaches for evaluating robustness of deep-code models, summarizing 45 studies on their transformations, techniques, datasets, and metrics while identifying key challenges.", "motivation": "Deep-code models, despite high task accuracy, require robustness evaluation against adversarial inputs (e.g., variable renaming) to ensure reliability in real-world software engineering applications.", "method": "Conducted a systematic literature review of 45 primary papers, examining metamorphic testing methodologies involving semantic-preserving program transformations and model output stability analysis.", "result": "Summarized current landscape of deep-code robustness testing: identified commonly evaluated models (e.g., transformers), tasks (completion, detection), datasets, target languages, and metrics. Highlighted gaps in transformation diversity and evaluation criteria.", "conclusion": "Metamorphic testing remains the dominant approach but requires improvements in canonicalization handling, cross-lingual evaluation, and integrating multiple test strategies to strengthen deep-code model robustness guarantees."}}
{"id": "2507.22659", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22659", "abs": "https://arxiv.org/abs/2507.22659", "authors": ["Sabrina Kaniewski", "Fabian Schmidt", "Markus Enzweiler", "Michael Menth", "Tobias Heer"], "title": "A Systematic Literature Review on Detecting Software Vulnerabilities with Large Language Models", "comment": "36 pages + 17 pages references, 6 tables, 10 figures", "summary": "The increasing adoption of Large Language Models (LLMs) in software\nengineering has sparked interest in their use for software vulnerability\ndetection. However, the rapid development of this field has resulted in a\nfragmented research landscape, with diverse studies that are difficult to\ncompare due to differences in, e.g., system designs and dataset usage. This\nfragmentation makes it difficult to obtain a clear overview of the\nstate-of-the-art or compare and categorize studies meaningfully. In this work,\nwe present a comprehensive systematic literature review (SLR) of LLM-based\nsoftware vulnerability detection. We analyze 227 studies published between\nJanuary 2020 and June 2025, categorizing them by task formulation, input\nrepresentation, system architecture, and adaptation techniques. Further, we\nanalyze the datasets used, including their characteristics, vulnerability\ncoverage, and diversity. We present a fine-grained taxonomy of vulnerability\ndetection approaches, identify key limitations, and outline actionable future\nresearch opportunities. By providing a structured overview of the field, this\nreview improves transparency and serves as a practical guide for researchers\nand practitioners aiming to conduct more comparable and reproducible research.\nWe publicly release all artifacts and maintain a living repository of LLM-based\nsoftware vulnerability detection studies.", "AI": {"tldr": "This paper provides a systematic literature review analyzing 227 studies on LLM-based software vulnerability detection (2020-2025), addressing fragmentation in the field by offering a structured taxonomy, dataset analysis, and future research directions.", "motivation": "The rapid development of LLM-based software vulnerability detection has led to a fragmented research landscape with inconsistent methodologies and datasets, hindering comparison and understanding of state-of-the-art techniques.", "method": "A comprehensive SLR was conducted, categorizing 227 studies by task formulation, input representation, system architecture, and adaptation techniques. Dataset characteristics, coverage, and diversity were also systematically analyzed.", "result": "The paper presents: (1) a fine-grained taxonomy of vulnerability detection approaches, (2) analysis of input/dataset limitations, (3) identification of key research gaps, (4) actionable future research opportunities, and (5) publicly released artifacts and a living repository.", "conclusion": "The review establishes a structured framework for LLM-based vulnerability detection research, improves transparency in the field, and provides practical resources to enable more comparable and reproducible future work."}}
{"id": "2507.22664", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22664", "abs": "https://arxiv.org/abs/2507.22664", "authors": ["Mashal Afzal Memon", "Gianluca Filippone", "Gian Luca Scoccia", "Marco Autili", "Paola Inverardi"], "title": "RobEthiChor: Automated Context-aware Ethics-based Negotiation for Autonomous Robots", "comment": null, "summary": "The presence of autonomous systems is growing at a fast pace and it is\nimpacting many aspects of our lives. Designed to learn and act independently,\nthese systems operate and perform decision-making without human intervention.\nHowever, they lack the ability to incorporate users' ethical preferences, which\nare unique for each individual in society and are required to personalize the\ndecision-making processes. This reduces user trust and prevents autonomous\nsystems from behaving according to the moral beliefs of their end-users. When\nmultiple systems interact with differing ethical preferences, they must\nnegotiate to reach an agreement that satisfies the ethical beliefs of all the\nparties involved and adjust their behavior consequently. To address this\nchallenge, this paper proposes RobEthiChor, an approach that enables autonomous\nsystems to incorporate user ethical preferences and contextual factors into\ntheir decision-making through ethics-based negotiation. RobEthiChor features a\ndomain-agnostic reference architecture for designing autonomous systems capable\nof ethic-based negotiating. The paper also presents RobEthiChor-Ros, an\nimplementation of RobEthiChor within the Robot Operating System (ROS), which\ncan be deployed on robots to provide them with ethics-based negotiation\ncapabilities. To evaluate our approach, we deployed RobEthiChor-Ros on real\nrobots and ran scenarios where a pair of robots negotiate upon resource\ncontention. Experimental results demonstrate the feasibility and effectiveness\nof the system in realizing ethics-based negotiation. RobEthiChor allowed robots\nto reach an agreement in more than 73\\% of the scenarios with an acceptable\nnegotiation time (0.67s on average). Experiments also demonstrate that the\nnegotiation approach implemented in RobEthiChor is scalable.", "AI": {"tldr": "RobEthiChor proposes an ethics-based negotiation framework for autonomous systems to incorporate individual user ethics and contextual factors, enhancing trust and decision-making personalization. A ROS implementation (RobEthiChor-Ros) and experiments with real robots demonstrate its feasibility and scalability.", "motivation": "Autonomous systems currently lack personalization in ethical decision-making, leading to reduced user trust and inability to align with diverse moral beliefs. Negotiation mechanisms are crucial when multiple systems with conflicting ethical preferences interact.", "method": "The paper introduces a domain-agnostic reference architecture (RobEthiChor) and implements it in ROS (RobEthiChor-Ros) to enable ethics-based negotiation. Real-robot experiments with resource contention scenarios validate the approach.", "result": "RobEthiChor achieved 73%+ agreement rates in scenarios, with average negotiation time of 0.67s. Results confirm the system's feasibility, effectiveness, and scalability in handling ethical negotiations among autonomous systems.", "conclusion": "RobEthiChor successfully demonstrates that ethics-based negotiation can be implemented in autonomous systems to respect user moral beliefs while maintaining scalability through its domain-agnostic architecture and ROS integration."}}
{"id": "2507.22800", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22800", "abs": "https://arxiv.org/abs/2507.22800", "authors": ["Rui Ren"], "title": "The Multi-Agent Fault Localization System Based on Monte Carlo Tree Search Approach", "comment": null, "summary": "In real-world scenarios, due to the highly decoupled and flexible nature of\nmicroservices, it poses greater challenges to system reliability. The more\nfrequent occurrence of incidents has created a demand for Root Cause\nAnalysis(RCA) methods that enable rapid identification and recovery of\nincidents. Large language model (LLM) provides a new path for quickly locating\nand recovering from incidents by leveraging their powerful generalization\nability combined with expert experience. Current LLM for RCA frameworks are\nbased on ideas like ReAct and Chain-of-Thought, but the hallucination of LLM\nand the propagation nature of anomalies often lead to incorrect localization\nresults. Moreover, the massive amount of anomalous information generated in\nlarge, complex systems presents a huge challenge for the context window length\nof LLMs. To address these challenges, we propose KnowledgeMind, an innovative\nLLM multi-agent system based on Monte Carlo Tree Search and a knowledge base\nreward mechanism for standardized service-by-service reasoning. Compared to\nState-Of-The-Art(SOTA) LLM for RCA methods, our service-by-service exploration\napproach significantly reduces the burden on the maximum context window length,\nrequiring only one-tenth of its size. Additionally, by incorporating a\nrule-based real-time reward mechanism, our method effectively mitigates\nhallucinations during the inference process. Compared to the SOTA LLM for RCA\nframework, our method achieves a 49.29% to 128.35% improvement in root cause\nlocalization accuracy.", "AI": {"tldr": "KnowledgeMind is an LLM-based RCA framework addressing limitations in existing methods by reducing context window burden and mitigating hallucinations through Monte Carlo Tree Search and a rule-based reward mechanism, achieving 49.29%-128.35% improved localization accuracy.", "motivation": "Highly decoupled microservices increase system reliability challenges and incident frequency, demanding better RCA methods. Existing LLM approaches suffer from hallucination errors and struggle with context window constraints when processing large anomalous data volumes.", "method": "Proposes KnowledgeMind: A multi-agent system using Monte Carlo Tree Search for service-by-service exploration combined with a rule-based, real-time knowledge base reward mechanism to guide reasoning and suppress hallucinations.", "result": "Reduces context window requirements to 1/10th of existing methods while improving root cause localization accuracy by 49.29% to 128.35% compared to SOTA LLM-based RCA frameworks.", "conclusion": "KnowledgeMind's structured exploration approach effectively addresses key limitations (hallucination, context window constraints) in LLM-based RCA through reward-guided reasoning, significantly outperforming current state-of-the-art methods."}}
{"id": "2507.22853", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22853", "abs": "https://arxiv.org/abs/2507.22853", "authors": ["Haichuan Hu", "Xiaochen Xie", "Quanjun Zhang"], "title": "Repair-R1: Better Test Before Repair", "comment": null, "summary": "APR (Automated Program Repair) aims to automatically locate program defects,\ngenerate patches and validate the repairs. Existing techniques for APR are\noften combined with LLMs (Large Language Models), which leverages the\ncode-related knowledge of LLMs to improve repair effectiveness. Current\nLLM-based APR methods typically utilize test cases only during the inference\nstage, adopting an iterative approach that performs repair first and validates\nit through test execution afterward. This conventional paradigm neglects two\nimportant aspects: the potential contribution of test cases in the training\nphase, and the possibility of leveraging testing prior to repair. To address\nthis, we propose Repair-R1, which introduces test cases into the model's\ntraining phase and shifts test generation to precede repair. The model is\nrequired to first generate discriminative test cases that can distinguish\ndefective behaviors, and then perform repair based on these tests. This enables\nthe model to better locate defects and understand the underlying causes of\ndefects, thereby improving repair effectiveness. We implement Repair-R1 with\nthree different backbone models, using RL (reinforcement learning) to\nco-optimize test generation and bug repair. Experimental results on four widely\nadopted benchmarks demonstrate the superiority of Repair-R1. Specially,\ncompared to vanilla models, Repair-R1 improves repair success rate by 2.68\\% to\n48.29\\%, test generation success rate by 16.38\\% to 53.28\\%, and test coverage\nby 0.78\\% to 53.96\\%. We publish the code and weights at\nhttps://github.com/Tomsawyerhu/APR-RL and\nhttps://huggingface.co/tomhu/Qwen3-4B-RL-5000-step.", "AI": {"tldr": "Repair-R1 is an Automated Program Repair approach that integrates test cases during model training and prioritizes test generation before repair, using reinforcement learning to improve repair effectiveness compared to conventional LLM-based APR techniques.", "motivation": "Current LLM-based APR methods use test cases only during inference phase and after repair generation, missing opportunities to leverage test cases in training or as a pre-repair analysis step. Prior testing can better locate defects and understand their root causes.", "method": "Repair-R1 generates discriminative test cases to identify defective behaviors before performing repairs. The model is trained with three backbone architectures using reinforcement learning to co-optimize test generation and bug repair processes.", "result": "Repair-R1 achieves 2.68%-48.29% higher repair success rate than vanilla models, 16.38%-53.28% improvement in test generation success rate, and 0.78%-53.96% increase in test coverage across four benchmark datasets. Code and weights are publicly available.", "conclusion": "Integrating test cases into training and shifting test generation to precede repair significantly enhances APR effectiveness, with Repair-R1 outperforming existing methods in multiple metrics. Published resources enable replication and further research."}}
{"id": "2507.22871", "categories": ["cs.SE", "cs.DL", "D.2.13"], "pdf": "https://arxiv.org/pdf/2507.22871", "abs": "https://arxiv.org/abs/2507.22871", "authors": ["Domhnall Carlin", "Austen Rainer"], "title": "Tracking research software outputs in the UK", "comment": null, "summary": "Research software is crucial in the research process and the growth of Open\nScience underscores the importance of accessing research artifacts, like data\nand code, raising traceability challenges among outputs. While it is a clear\nprinciple that research code, along with other essential outputs, should be\nrecognised as artifacts of the research process, the how of this principle\nremains variable. This study examines where UK academic institutions store and\nregister software as a unique research output, searching the UKRI's Gateway to\nResearch (GtR) metadata for publicly funded research software in the UK. The\nquantity of software reported as research outcomes remains low in proportion to\nother categories. Artifact sharing appears low, with one-quarter of the\nreported software having no links and 45% having either a missing or erroneous\nURL. Of the valid URLs, we find the single largest category is Public\nCommercial Code Repository, with GitHub being the host of 18% of all publicly\nfunded research software listed. These observations are contrasted with past\nfindings from 2023 and finally, we discuss the lack of artifact sharing in UK\nresearch, with resulting implications for the maintenance and evolution of\nresearch software. Without dissemination, research software risks demotion to a\ntransient artifact, useful only to meet short term research demands but\nultimately lost to the broader enterprise of science.", "AI": {"tldr": "This study analyzes the storage and registration of research software in UK academic institutions by examining the UKRI Gateway to Research (GtR) metadata. It finds that publicly funded research software constitutes a low proportion of reported outputs, with significant issues in artifact sharing (e.g., 25% lack links and 45% have invalid URLs). GitHub hosts 18% of valid software artifacts, and the findings highlight risks for software sustainability if sharing practices remain inadequate.", "motivation": "The growth of Open Science emphasizes accessibility of research artifacts (data/code), but practical methods for recognizing and tracking research software as a critical output remain inconsistent. UK institutions need evaluation to address traceability gaps in funding bodies' reports.", "method": "The authors examined where UK academic institutions store and register research software by querying metadata in the UKRI's Gateway to Research (GtR) database, analyzing publicly funded software listings, validating URLs, and comparing findings with 2023 data.", "result": "Research software represents a low proportion of reported outputs in GtR. Twenty-five percent have no links, and 45% of URLs are missing or erroneous. Among valid URLs, public commercial code repositories dominate (18% on GitHub). Artifact sharing practices remain suboptimal compared to 2023 benchmarks.", "conclusion": "Inadequate artifact sharing for research software threatens its long-term value and sustainability. Without improved dissemination, software may become transient tools rather than lasting scientific resources, undermining the goals of Open Science in preserving traceable, reusable outputs."}}
