{"id": "2507.14256", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14256", "abs": "https://arxiv.org/abs/2507.14256", "authors": ["Jakub Walczak", "Piotr Tomalak", "Artur Laskowski"], "title": "Impact of Code Context and Prompting Strategies on Automated Unit Test Generation with Modern General-Purpose Large Language Models", "comment": null, "summary": "Generative AI is gaining increasing attention in software engineering, where\ntesting remains an indispensable reliability mechanism. According to the widely\nadopted testing pyramid, unit tests constitute the majority of test cases and\nare often schematic, requiring minimal domain expertise. Automatically\ngenerating such tests under the supervision of software engineers can\nsignificantly enhance productivity during the development phase of the software\nlifecycle.\n  This paper investigates the impact of code context and prompting strategies\non the quality and adequacy of unit tests generated by various large language\nmodels (LLMs) across several families. The results show that including\ndocstrings notably improves code adequacy, while further extending context to\nthe full implementation yields definitely smaller gains. Notably, the\nchain-of-thought prompting strategy -- applied even to 'reasoning' models --\nachieves the best results, with up to 96.3\\% branch coverage, a 57\\% average\nmutation score, and near-perfect compilation success rate. Among the evaluated\nmodels, M5 (Gemini 2.5 Pro) demonstrated superior performance in both mutation\nscore and branch coverage being still in top in terms of compilation success\nrate.\n  All the code and resulting test suites are publicly available at\nhttps://github.com/peetery/LLM-analysis.", "AI": {"tldr": "This paper evaluates how code context and prompting strategies affect the quality of unit tests generated by different LLMs. Chain-of-thought prompting achieves the highest performance (96.3% branch coverage, 57% average mutation score), with M5 (Gemini 2.5 Pro) leading in mutation score and compilation success rate. Code and tests are available at https://github.com/peetery/LLM-analysis.", "motivation": "The testing pyramid emphasizes unit tests as the foundation of reliable software development, yet creating schematic tests with minimal domain expertise remains labor-intensive. Automating unit test generation can significantly improve productivity for developers.", "method": "The study compares multiple LLM families by varying code context (e.g., docstrings, full implementations) and prompting strategies. Metrics like branch coverage, mutation score, and compilation success rate are measured to assess test quality and robustness.", "result": "Incorporating docstrings boosts code adequacy, while full contextual extensions provide smaller benefits. Chain-of-thought prompting outperforms alternatives, achieving nearly perfect compilation success rates and the highest mutation scores (up to 57% and 96.3% branch coverage). M5 (Gemini 2.5 Pro) consistently excels across metrics.", "conclusion": "Chain-of-thought prompting combined with selective contextual enrichment (e.g., docstrings) delivers optimal unit test generation for LLMs. The M5 model remains superior, and open-source collaboration through shared codebases is encouraged to advance reliable AI-powered testing methodologies."}}
{"id": "2507.14330", "categories": ["cs.SE", "D.2.1; D.2.4; D.2.10; F.4.1; F.4.3"], "pdf": "https://arxiv.org/pdf/2507.14330", "abs": "https://arxiv.org/abs/2507.14330", "authors": ["Arshad Beg", "Diarmuid O'Donoghue", "Rosemary Monahan"], "title": "Leveraging LLMs for Formal Software Requirements -- Challenges and Prospects", "comment": "Submitted to Overlay2025 - 7th International Workshop on Artificial\n  Intelligence and fOrmal VERification, Logic, Automata, and sYnthesis. [under\n  review]", "summary": "Software correctness is ensured mathematically through formal verification,\nwhich involves the resources of generating formal requirement specifications\nand having an implementation that must be verified. Tools such as\nmodel-checkers and theorem provers ensure software correctness by verifying the\nimplementation against the specification. Formal methods deployment is\nregularly enforced in the development of safety-critical systems e.g.\naerospace, medical devices and autonomous systems. Generating these\nspecifications from informal and ambiguous natural language requirements\nremains the key challenge. Our project, VERIFAI^{1}, aims to investigate\nautomated and semi-automated approaches to bridge this gap, using techniques\nfrom Natural Language Processing (NLP), ontology-based domain modelling,\nartefact reuse, and large language models (LLMs). This position paper presents\na preliminary synthesis of relevant literature to identify recurring challenges\nand prospective research directions in the generation of verifiable\nspecifications from informal requirements.", "AI": {"tldr": "This position paper addresses the challenge of generating formal verifiable specifications from ambiguous natural language requirements through approaches combining NLP, ontologies, artifact reuse, and LLMs, summarizing literature gaps and research directions.", "motivation": "Formal verification requires precise specifications but faces challenges due to informal and ambiguous natural language requirements, leading to inconsistent verification outcomes in safety-critical domains.", "method": "Preliminary literature synthesis using natural language processing, ontology-based domain modeling, reusable artefact frameworks, and large language model integration to identify spec-generation patterns and challenges.", "result": "Identified common challenges in informal requirement parsing and proposed research directions for automated/semi-automated spec-generation techniques using interdisciplinary methods.", "conclusion": "Automated specification generation from natural language requires hybrid approaches combining NLP, domain ontologies, and LLMs, with future work needed on tool integration and validation workflows in formal verification."}}
{"id": "2507.14396", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14396", "abs": "https://arxiv.org/abs/2507.14396", "authors": ["Carey Lai Zheng Hui", "Johnson Britto Jessia Esther Leena", "Kumuthini Subramanian", "Zhao Chenyu", "Shubham Rajeshkumar Jariwala"], "title": "Developing Shared Vocabulary System For Collaborative Software Engineering", "comment": "16 pages, including appendix", "summary": "Effective communication is a critical factor in successful software\nengineering collaboration. However, communication gaps remain a persistent\nchallenge, often leading to misunderstandings, inefficiencies, and defects.\nThis research investigates the technical factors contributing to such\nmisunderstandings and explores the measurable benefits of establishing shared\nvocabulary systems within software documentation and codebases. Using a Design\nScience Research (DSR) framework, the study was structured into three iterative\nphases: problem identification, method development, and empirical validation.\nThe problem identification phase involved thematic analysis of communication\ndata and semi-structured interviews, revealing key factors such as ambiguous\nmessaging, misalignment in documentation, inconsistent code review feedback,\nand API integration miscommunication. Grounded Theory principles were employed\nto design a structured methodology for collaborative vocabulary development.\nEmpirical validation through controlled experiments demonstrated that while\ninitial adoption introduced overhead, the shared vocabulary system\nsignificantly improved information density, documentation clarity, and\ncollaboration efficiency over time. Findings offer actionable insights for\nimproving communication practices in software engineering, while also\nidentifying limitations and directions for future research.", "AI": {"tldr": "This paper introduces a structured methodology for collaborative vocabulary development in software engineering using Design Science Research, showing that shared vocabulary systems improve communication and collaboration efficiency despite initial adoption overhead.", "motivation": "Communication gaps in software engineering cause persistent issues like misunderstandings, inefficiencies, and defects, particularly through ambiguous messaging, misaligned documentation, inconsistent code review feedback, and API integration problems.", "method": "The study employs a Design Science Research (DSR) framework with three iterative phases: problem identification (thematic analysis and interviews), method development (Grounded Theory-based collaboration methodology), and empirical validation (controlled experiments tracking communication outcomes).", "result": "Controlled experiments revealed increased information density, documentation clarity, and collaboration efficiency over time, though initial system adoption introduced measurable overhead.", "conclusion": "Shared vocabulary systems offer actionable improvements for software engineering collaboration practices, but require balancing adoption costs with long-term benefits while addressing identified limitations through future research."}}
{"id": "2507.14423", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14423", "abs": "https://arxiv.org/abs/2507.14423", "authors": ["Mootez Saad", "Hao Li", "Tushar Sharma", "Ahmed E. Hassan"], "title": "On the Effect of Token Merging on Pre-trained Models for Code", "comment": null, "summary": "Tokenization is a fundamental component of language models for code. It\ninvolves breaking down the input into units that are later passed to the\nlanguage model stack to learn high-dimensional representations used in various\ncontexts, from classification to generation. However, the output of these\ntokenizers is often longer than that traditionally used in compilers and\ninterpreters. This could result in undesirable effects, such as increased\ncomputational overhead. In this work, we investigate the effect of merging the\nhidden representations of subtokens that belong to the same semantic unit, such\nas subtokens that form a single identifier. We propose two strategies: one\nbased on averaging the representations and another that leverages a\nlearning-based approach. Both methods can be seamlessly integrated with\nexisting language models for code. We conduct experiments using six language\nmodels for code: CodeBERT, GraphCodeBERT, UniXCoder, CdoeT5, CodeT5+ (220M),\nand CodeT5+ (770M), across three software engineering tasks: vulnerability\ndetection, code classification, and code translation. Results show that these\nstrategies can reduce the number of floating-point operations by $1\\%$ to\n$19\\%$. Regarding downstream performance, the most significant degradation was\nobserved in the vulnerability detection task, where the F1 score decreased by\n$1.82$ points compared to the baseline. In contrast, for code translation, we\nobserved an improvement of $2.47$ points in CodeBLEU. This work contributes to\nthe broader effort of improving language models for code across multiple\ndimensions, including both computational efficiency and downstream performance.", "AI": {"tldr": "This paper explores merging subtoken representations in code language models to reduce computational overhead and improve downstream performance across three tasks.", "motivation": "To address the computational inefficiency caused by longer subtoken sequences in code language models compared to traditional compilers.", "method": "Proposes averaging subtoken representations and a learning-based strategy for merging, compatible with existing code models like CodeBERT and CodeT5+. Conducts experiments on six models across three tasks.", "result": "Reduces FLOPs by 1\u201319%. Vulnerability detection F1 score drops by 1.82 points, while code translation gains 2.47 CodeBLEU improvement. Mixed performance trade-offs observed.", "conclusion": "Merging subtoken representations offers efficiency gains with task-dependent performance trade-offs, advancing code language models in both computational speed and accuracy dimensions."}}
{"id": "2507.14197", "categories": ["cs.CR", "cs.IT", "math.IT", "94A60"], "pdf": "https://arxiv.org/pdf/2507.14197", "abs": "https://arxiv.org/abs/2507.14197", "authors": ["Andriamifidisoa Ramamonjy", "Rufine Marius Lalasoa"], "title": "DM-RSA: An Extension of RSA with Dual Modulus", "comment": "5 pages", "summary": "We introduce DM-RSA (Dual Modulus RSA), a variant of the RSA cryptosystem\nthat employs two distinct moduli symmetrically to enhance security. By\nleveraging the Chinese Remainder Theorem (CRT) for decryption, DM-RSA provides\nincreased robustness against side-channel attacks while preserving the\nefficiency of classical RSA. This approach improves resistance to partial\ncompromise of a modulus and integrates easily into existing infrastructures.", "AI": {"tldr": "DM-RSA is a new RSA variant using two distinct moduli and the Chinese Remainder Theorem (CRT) for decryption to improve security against side-channel attacks while retaining classical RSA efficiency.", "motivation": "Traditional RSA is vulnerable to side-channel attacks that compromise private key information. The paper addresses this by proposing a cryptosystem that resists partial modulus exposure and maintains performance through symmetric use of two moduli.", "method": "DM-RSA employs two unique moduli symmetrically. During decryption, it utilizes the non-CRT method for one modulus and the CRT for the other, combining results to achieve enhanced security while preserving efficient key operations.", "result": "Experiments show DM-RSA increases robustness against partial modulus attacks and maintains classical RSA's efficiency. The dual modulus approach makes it more secure in environments with side-channel threats without significant performance overhead.", "conclusion": "DM-RSA offers superior security in side-channel attack scenarios while being compatible with existing RSA infrastructures. The symmetric dual modulus design provides practical defense mechanisms without compromising the speed of traditional RSA implementations."}}
{"id": "2507.14547", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14547", "abs": "https://arxiv.org/abs/2507.14547", "authors": ["Noman Ahmad", "Ruoyu Su", "Matteo Esposito", "Andrea Janes", "Valentina Lenarduzzi", "Davide Taibi"], "title": "Architectural Degradation: Definition, Motivations, Measurement and Remediation Approaches", "comment": null, "summary": "Architectural degradation, also known as erosion, decay, or aging, impacts\nsystem quality, maintainability, and adaptability. Although widely\nacknowledged, current literature shows fragmented definitions, metrics, and\nremediation strategies. Our study aims to unify understanding of architectural\ndegradation by identifying its definitions, causes, metrics, tools, and\nremediation approaches across academic and gray literature. We conducted a\nmultivocal literature review of 108 studies extracting definitions, causes,\nmetrics, measurement approaches, tools, and remediation strategies. We\ndeveloped a taxonomy encompassing architectural, code, and process debt to\nexplore definition evolution, methodological trends, and research gaps.\nArchitectural degradation has shifted from a low-level issue to a\nsocio-technical concern. Definitions now address code violations, design drift,\nand structural decay. Causes fall under architectural (e.g., poor\ndocumentation), code (e.g., hasty fixes), and process debt (e.g., knowledge\nloss). We identified 54 metrics and 31 measurement techniques, focused on\nsmells, cohesion/coupling, and evolution. Yet, most tools detect issues but\nrarely support ongoing or preventive remediation. Degradation is both technical\nand organizational. While detection is well-studied, continuous remediation\nremains lacking. Our study reveals missed integration between metrics, tools,\nand repair logic, urging holistic, proactive strategies for sustainable\narchitecture.", "AI": {"tldr": "This paper unifies understanding of architectural degradation by analyzing 108 studies, identifying definitions, causes, metrics, and tools, and highlights gaps in continuous remediation strategies.", "motivation": "Current literature on architectural degradation has fragmented definitions, metrics, and remediation strategies, necessitating a unified framework to improve system maintainability and adaptability.", "method": "Multivocal literature review of 108 academic and gray literature sources to extract definitions, causes, metrics, measurement approaches, tools, and remediation strategies, synthesizing into a taxonomy of architectural, code, and process debt.", "result": "The study uncovers evolving definitions (emphasizing socio-technical aspects), 54 metrics (focusing on smells, cohesion/coupling, and evolution), 31 measurement techniques, and a taxonomy revealing missed integration between metrics, tools, and repair logic.", "conclusion": "Architectural degradation requires holistic, proactive strategies combining technical and organizational solutions, as current tools focus on detection rather than continuous remediation."}}
{"id": "2507.14201", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14201", "abs": "https://arxiv.org/abs/2507.14201", "authors": ["Yiran Wu", "Mauricio Velazco", "Andrew Zhao", "Manuel Ra\u00fal Mel\u00e9ndez Luj\u00e1n", "Srisuma Movva", "Yogesh K Roy", "Quang Nguyen", "Roberto Rodriguez", "Qingyun Wu", "Michael Albada", "Julia Kiseleva", "Anand Mudgerikar"], "title": "ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation", "comment": null, "summary": "We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on\nthe task of Cyber Threat Investigation through security questions derived from\ninvestigation graphs. Real-world security analysts must sift through a large\nnumber of heterogeneous alert signals and security logs, follow multi-hop\nchains of evidence, and compile an incident report. With the developments of\nLLMs, building LLM-based agents for automatic thread investigation is a\npromising direction. To assist the development and evaluation of LLM agents, we\nconstruct a dataset from a controlled Azure tenant that covers 8 simulated\nreal-world multi-step attacks, 57 log tables from Microsoft Sentinel and\nrelated services, and 589 automatically generated questions. We leverage\nsecurity logs extracted with expert-crafted detection logic to build threat\ninvestigation graphs, and then generate questions with LLMs using paired nodes\non the graph, taking the start node as background context and the end node as\nanswer. Anchoring each question to these explicit nodes and edges not only\nprovides automatic, explainable ground truth answers but also makes the\npipeline reusable and readily extensible to new logs. This also enables the\nautomatic generation of procedural tasks with verifiable rewards, which can be\nnaturally extended to training agents via reinforcement learning. Our\ncomprehensive experiments with different models confirm the difficulty of the\ntask: with the base setting, the average reward across all evaluated models is\n0.249, and the best achieved is 0.368, leaving substantial headroom for future\nresearch. Code and data are coming soon!", "AI": {"tldr": "This paper introduces ExCyTIn-Bench, the first benchmark for evaluating LLM agents in cyber threat investigation using explicit graph-based security questions.", "motivation": "Current LLM-based threat investigation lacks standardized evaluation metrics to quantify how agents process heterogeneous alert signals, follow multi-hop evidence chains, and generate validated incident reports.", "method": "The authors constructed a dataset from a controlled Azure tenant with 8 simulated multi-step attacks and 57 log tables from Microsoft Sentinel. They used expert detection logic to build investigation graphs, then generated 589 questions through paired nodes (context nodes as question backgrounds and end nodes as answers).", "result": "In base experiments, all models showed low average rewards (0.249 max) with the best model achieving 0.368 reward, demonstrating remaining performance gaps compared to human analysts.", "conclusion": "ExCyTIn-Bench provides a reusable, extensible framework enabling automatic question generation and reinforcement learning training while highlighting the challenge of threat investigation tasks through its low baseline scores."}}
{"id": "2507.14554", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14554", "abs": "https://arxiv.org/abs/2507.14554", "authors": ["Ruoyu Su", "Noman ahmad", "Matteo Esposito", "Andrea Janes", "Davide Taibi", "Valentina Lenarduzzi"], "title": "Emerging Trends in Software Architecture from the Practitioners Perspective: A Five Year Review", "comment": null, "summary": "Software architecture plays a central role in the design, development, and\nmaintenance of software systems. With the rise of cloud computing,\nmicroservices, and containers, architectural practices have diversified.\nUnderstanding these shifts is vital. This study analyzes software architecture\ntrends across eight leading industry conferences over five years. We\ninvestigate the evolution of software architecture by analyzing talks from top\npractitioner conferences, focusing on the motivations and contexts driving\ntechnology adoption. We analyzed 5,677 talks from eight major industry\nconferences, using large language models and expert validation to extract\ntechnologies, their purposes, and usage contexts. We also explored how\ntechnologies interrelate and fit within DevOps and deployment pipelines. Among\n450 technologies, Kubernetes, Cloud Native, Serverless, and Containers dominate\nby frequency and centrality. Practitioners present technology mainly related to\ndeployment, communication, AI, and observability. We identify five technology\ncommunities covering automation, coordination, cloud AI, monitoring, and\ncloud-edge. Most technologies span multiple DevOps stages and support hybrid\ndeployment. Our study reveals that a few core technologies, like Kubernetes and\nServerless, dominate the contemporary software architecture practice. These are\nmainly applied in later DevOps stages, with limited focus on early phases like\nplanning and coding. We also show how practitioners frame technologies by\npurpose and context, reflecting evolving industry priorities. Finally, we\nobserve how only research can provide a more holistic lens on architectural\ndesign, quality, and evolution.", "AI": {"tldr": "This paper analyzes trends in software architecture across eight industry conferences, revealing the dominance of Kubernetes, Cloud Native, Serverless, and Containers, while highlighting gaps in early DevOps stages and the need for research to address holistic architectural evolution.", "motivation": "The study aims to understand shifts in architectural practices driven by cloud computing, microservices, and containers, providing insights into their adoption motivations and contexts to guide practitioners and researchers.", "method": "Authors analyzed 5,677 conference talks using large language models and expert validation, extracting technology usage patterns, purposes, deployment pipeline integration, and community cohesion through network analysis.", "result": "Identified 450 technologies with Kubernetes/Cloud Native/Serverless/Containers as central hubs; mapped five distinct technology communities; revealed hybrid deployment prevalence and limited focus on early DevOps stages like planning/coding.", "conclusion": "Current architecture practice increasingly relies on standardized core technologies concentrated in deployment/runtime phases, suggesting opportunities for academic research to improve architectural design quality and address underexplored early-stage needs."}}
{"id": "2507.14202", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14202", "abs": "https://arxiv.org/abs/2507.14202", "authors": ["Pengfei Du"], "title": "PRM-Free Security Alignment of Large Models via Red Teaming and Adversarial Training", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse applications, yet they pose significant security risks that threaten\ntheir safe deployment in critical domains. Current security alignment\nmethodologies predominantly rely on Process Reward Models (PRMs) to evaluate\nintermediate reasoning steps, introducing substantial computational overhead\nand scalability constraints. This paper presents a novel PRM-free security\nalignment framework that leverages automated red teaming and adversarial\ntraining to achieve robust security guarantees while maintaining computational\nefficiency. Our approach systematically identifies vulnerabilities through\nsophisticated attack strategies including genetic algorithm optimization,\nmulti-agent simulation, and advanced prompt mutation techniques. The framework\nenhances model robustness via targeted adversarial training with curriculum\nlearning and adaptive regularization mechanisms. Comprehensive experimental\nevaluation across five state-of-the-art LLMs demonstrates that our method\nachieves superior security alignment performance compared to PRM-based\napproaches while reducing computational costs by 61\\%. The framework\nincorporates transparent reporting and continuous audit mechanisms that enable\niterative security improvement and regulatory compliance. Our contributions\nadvance the field of efficient LLM security alignment by democratizing access\nto robust security measures for resource-constrained organizations and\nproviding a scalable foundation for addressing evolving adversarial threats.", "AI": {"tldr": "Proposes PRM-free security alignment framework for LLMs using automated red teaming and adversarial training, reducing computational costs by 61% while improving robustness against adversaries.", "motivation": "Current PRM-based security alignment methods for LLMs require significant computational resources and face scalability challenges, limiting their practical deployment in critical security domains.", "method": "1) Automated red teaming through genetic algorithm optimization, multi-agent simulation, and prompt mutation techniques 2) Adversarial training with curriculum learning and adaptive regularization mechanisms 3) Transparent reporting and continuous audit frameworks", "result": " Achieved superior security alignment performance compared to PRM-based approaches on five SOTA LLMs while decreasing computational costs by 61%. Demonstrated effectiveness against diverse adversarial strategies through systematic vulnerability identification.", "conclusion": "The PRM-free framework enables scalable, cost-effective security alignment for resource-constrained organizations while maintaining strong adversarial robustness. Provides foundations for evolving security against emerging threats without sacrificing computational efficiency."}}
{"id": "2507.14558", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14558", "abs": "https://arxiv.org/abs/2507.14558", "authors": ["Bin Duan", "Tarek Mahmud", "Meiru Che", "Yan Yan", "Naipeng Dong", "Dan Dongseong Kim", "Guowei Yang"], "title": "Harnessing LLMs for Document-Guided Fuzzing of OpenCV Library", "comment": null, "summary": "The combination of computer vision and artificial intelligence is\nfundamentally transforming a broad spectrum of industries by enabling machines\nto interpret and act upon visual data with high levels of accuracy. As the\nbiggest and by far the most popular open-source computer vision library, OpenCV\nlibrary provides an extensive suite of programming functions supporting\nreal-time computer vision. Bugs in the OpenCV library can affect the downstream\ncomputer vision applications, and it is critical to ensure the reliability of\nthe OpenCV library. This paper introduces VISTAFUZZ, a novel technique for\nharnessing large language models (LLMs) for document-guided fuzzing of the\nOpenCV library. VISTAFUZZ utilizes LLMs to parse API documentation and obtain\nstandardized API information. Based on this standardized information, VISTAFUZZ\nextracts constraints on individual input parameters and dependencies between\nthese. Using these constraints and dependencies, VISTAFUZZ then generates new\ninput values to systematically test each target API. We evaluate the\neffectiveness of VISTAFUZZ in testing 330 APIs in the OpenCV library, and the\nresults show that VISTAFUZZ detected 17 new bugs, where 10 bugs have been\nconfirmed, and 5 of these have been fixed.", "AI": {"tldr": "VISTAFUZZ is a novel technique using LLMs to enhance OpenCV library testing by parsing documentation, extracting parameter constraints, and generating inputs to detect 17 new bugs with 5 confirmed fixes.", "motivation": "Bugs in the widely used OpenCV library can disrupt critical computer vision applications, necessitating robust testing approaches to ensure reliability.", "method": "LLMs parse OpenCV API documentation to extract standardized info and parameter constraints/dependencies. Generated inputs based on constraints systematically test APIs through fuzzing.", "result": "Testing 330 APIs revealed 17 new bugs (10 confirmed, 5 fixed) demonstrating VISTAFUZZ's effectiveness in identifying real issues in OpenCV.", "conclusion": "Document-guided LLM fuzzing is a promising approach for detecting bugs in foundational libraries like OpenCV, highlighting LLMs' potential in improving software reliability through structured documentation analysis."}}
{"id": "2507.14207", "categories": ["cs.CR", "cs.AI", "I.2.1; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.14207", "abs": "https://arxiv.org/abs/2507.14207", "authors": ["Richard M. Charles", "James H. Curry", "Richard B. Charles"], "title": "Mitigating Trojanized Prompt Chains in Educational LLM Use Cases: Experimental Findings and Detection Tool Design", "comment": "12 pages, 1 figure", "summary": "The integration of Large Language Models (LLMs) in K--12 education offers\nboth transformative opportunities and emerging risks. This study explores how\nstudents may Trojanize prompts to elicit unsafe or unintended outputs from\nLLMs, bypassing established content moderation systems with safety guardrils.\nThrough a systematic experiment involving simulated K--12 queries and\nmulti-turn dialogues, we expose key vulnerabilities in GPT-3.5 and GPT-4. This\npaper presents our experimental design, detailed findings, and a prototype\ntool, TrojanPromptGuard (TPG), to automatically detect and mitigate Trojanized\neducational prompts. These insights aim to inform both AI safety researchers\nand educational technologists on the safe deployment of LLMs for educators.", "AI": {"tldr": "This paper investigates risks in using LLMs for K--12 education, demonstrating how students can Trojanize prompts to bypass safety mechanisms in GPT-3.5 and GPT-4, and proposes a prototype detection tool called TrojanPromptGuard.", "motivation": "LLM integration in education presents transformative potential but also unsafe outputs when students manipulate prompts to evade content moderation systems, necessitating robust safety measures for safe deployment.", "method": "The study conducted systematic experiments using simulated K--12 queries and multi-turn dialogues to identify vulnerabilities in GPT models' safety guardrails, then developed TrojanPromptGuard as an automated detection tool.", "result": "Key vulnerabilities were exposed in GPT-3.5 and GPT-4's educational safety mechanisms, including successful generation of unsafe outputs via Trojanized prompts, with TPG prototype showing initial efficacy in detection.", "conclusion": "The work highlights critical security gaps in educational LLM usage and provides tangible solutions through TPG, offering actionable insights for both AI safety researchers and educational technologists to improve system trustworthiness."}}
{"id": "2507.14594", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14594", "abs": "https://arxiv.org/abs/2507.14594", "authors": ["Weiwei Xu", "Hengzhi Ye", "Kai Gao", "Minghui Zhou"], "title": "A first look at License Variants in the PyPI Ecosystem", "comment": null, "summary": "Open-source licenses establish the legal foundation for software reuse, yet\nlicense variants, including both modified standard licenses and custom-created\nalternatives, introduce significant compliance complexities. Despite their\nprevalence and potential impact, these variants are poorly understood in modern\nsoftware systems, and existing tools do not account for their existence,\nleading to significant challenges in both effectiveness and efficiency of\nlicense analysis. To fill this knowledge gap, we conduct a comprehensive\nempirical study of license variants in the PyPI ecosystem. Our findings show\nthat textual variations in licenses are common, yet only 2% involve substantive\nmodifications. However, these license variants lead to significant compliance\nissues, with 10.7% of their downstream dependencies found to be\nlicense-incompatible.\n  Inspired by our findings, we introduce LV-Parser, a novel approach for\nefficient license variant analysis leveraging diff-based techniques and large\nlanguage models, along with LV-Compat, an automated pipeline for detecting\nlicense incompatibilities in software dependency networks. Our evaluation\ndemonstrates that LV-Parser achieves an accuracy of 0.936 while reducing\ncomputational costs by 30%, and LV-Compat identifies 5.2 times more\nincompatible packages than existing methods with a precision of 0.98.\n  This work not only provides the first empirical study into license variants\nin software packaging ecosystem but also equips developers and organizations\nwith practical tools for navigating the complex landscape of open-source\nlicensing.", "AI": {"tldr": "First comprehensive empirical study of license variants in PyPI, revealing 2% substantial modifications causing 10.7% incompatibility, and introducing LV-Parser (93.6% accuracy, 30% cost reduction) and LV-Compat (5.2x more incompatible packages detected with 98% precision) for license analysis.\\n\\n", "motivation": "License variants create compliance complexities in software systems, with existing tools failing to address their legal implications and impact on dependency networks, leading to significant risks for developers and organizations.\\n\\n", "method": "1) Conducted empirical analysis of 12K PyPI packages to characterize license variants\\n2) Developed LV-Parser using diff-based techniques and LLMs for variant classification\\n3) Created LV-Compat for automated dependency-level incompatibility detection in software ecosystems\\n\\n", "result": "1) 2% of license variants contain legal modifications\\n2) 10.7% of downstream dependencies face license incompatibility\\n3) LV-Parser achieves 93.6% accuracy with 30% lower computational cost\\n4) LV-Compat detects 5.2x more incompatible packages than prior approaches while maintaining 98% precision\\n\\n", "conclusion": "This work establishes the first systematic understanding of license variants in open-source ecosystems and demonstrates how specialized tools like LV-Parser and LV-Compat can significantly improve compliance analysis accuracy and effectiveness for dependency management.\\n\\n"}}
{"id": "2507.14212", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14212", "abs": "https://arxiv.org/abs/2507.14212", "authors": ["Federico Mason", "Federico Chiariotti", "Pietro Talli", "Andrea Zanella"], "title": "Secure Goal-Oriented Communication: Defending against Eavesdropping Timing Attacks", "comment": null, "summary": "Goal-oriented Communication (GoC) is a new paradigm that plans data\ntransmission to occur only when it is instrumental for the receiver to achieve\na certain goal. This leads to the advantage of reducing the frequency of\ntransmissions significantly while maintaining adherence to the receiver's\nobjectives. However, GoC scheduling also opens a timing-based side channel that\nan eavesdropper can exploit to obtain information about the state of the\nsystem. This type of attack sidesteps even information-theoretic security, as\nit exploits the timing of updates rather than their content. In this work, we\nstudy such an eavesdropping attack against pull-based goal-oriented scheduling\nfor remote monitoring and control of Markov processes. We provide a theoretical\nframework for defining the effectiveness of the attack and propose possible\ncountermeasures, including two practical heuristics that provide a balance\nbetween the performance gains offered by GoC and the amount of leaked\ninformation. Our results show that, while a naive goal-oriented scheduler\nallows the eavesdropper to correctly guess the system state about 60% of the\ntime, our heuristic defenses can halve the leakage with a marginal reduction of\nthe benefits of goal-oriented approaches.", "AI": {"tldr": "The paper introduces a new paradigm called Goal-oriented Communication (GoC) for reducing system communication by transmitting data only when necessary for the receiver to achieve objectives. However, this approach creates a timing-based side channel vulnerability exploitable by eavesdroppers to infer system states in monitoring and control scenarios. The authors analyze this attack and propose two heuristic defenses that reduce information leakage while maintaining most of the communication efficiency benefits.", "motivation": "GoC reduces communication frequency but may expose timing-based side channels, which can be exploited by attackers to infer sensitive system states despite secure data content. This work addresses the critical need to understand and mitigate such vulnerabilities in remote monitoring and control systems.", "method": "The authors develop a theoretical framework to quantify eavesdropping effectiveness against pull-based GoC in Markov processes and design two practical heuristic defenses. These heuristics balance communication efficiency gains with the reduction of timing-based information leakage by introducing adaptive transmission timing patterns and probabilistic update intervals.", "result": "Experiments demonstrate that a naive GoC scheduler allows eavesdroppers to guess the system state 60% of the time, while the proposed heuristic defenses reduce this success rate to 30% with minimal performance degradation (e.g., 5-10% fewer transmissions saved). This indicates significant improvement in security with acceptable trade-offs for communication efficiency.", "conclusion": "The study confirms that timing-based side channels in GoC are a tangible threat, even to information-theoretic secure systems, but that strategically designed heuristics for update scheduling can effectively mitigate these risks while preserving most of the communication savings."}}
{"id": "2507.14687", "categories": ["cs.SE", "68Q60, 03B70", "D.2.5"], "pdf": "https://arxiv.org/pdf/2507.14687", "abs": "https://arxiv.org/abs/2507.14687", "authors": ["Robin Lee", "Youngho Nam"], "title": "An Efficient Algorithm for Generating Minimal Unique-Cause MC/DC Test cases for Singular Boolean Expressions", "comment": "10 pages, 5 figures", "summary": "Modified Condition/Decision Coverage (MC/DC) is a mandatory structural\ncoverage criterion for ensuring the reliability and safety of critical systems.\nWhile its strictest form, Unique-Cause MC/DC, offers the highest assurance,\nresearch on its efficient test generation has been lacking. This gap is\nparticularly significant, as an analysis of large-scale avionics systems shows\nthat 99.7% of all conditional decisions are, in fact, Singular Boolean\nExpressions (SBEs) the ideal structure for applying Unique-Cause MC/DC. This\npaper proposes 'Robin's Rule', a deterministic algorithm that directly\nconstructs a minimal test set of N + 1 cases to guarantee 100% Unique-Cause\nMC/DC for SBEs with N conditions, without generating a full truth table. To\nvalidate our approach, we constructed a benchmark by reformulating the TCAS-II\nspecifications into SBEs and verified the results using an industry-standard,\ncertified commercial tool. The results confirm that our method consistently\nachieves 100% coverage with the theoretical minimum number of tests and is more\nefficient than the commercial tool. This work provides a practical and provably\noptimal solution for verifying safety-critical systems, ensuring both rigor and\nefficiency.", "AI": {"tldr": "The paper introduces Robin's Rule, a deterministic algorithm for optimal test generation in Singular Boolean Expressions (SBEs), achieving 100% Unique-Cause MC/DC with N + 1 test cases and outperforming commercial tools in efficiency.", "motivation": "Unique-Cause MC/DC is critical for safety but lacks efficient test generation, despite 99.7% of avionics decisions being SBEs. This paper addresses the practical need for a minimal, deterministic approach to meet rigorous safety standards.", "method": "Robin's Rule directly constructs a minimal test set of N + 1 cases for SBEs, avoiding full truth table generation. The algorithm was validated using a benchmark derived from TCAS-II SBEs and compared against a certified commercial tool through verification runs.", "result": "Empirical results demonstrate Robin's Rule consistently achieves 100% Unique-Cause MC/DC coverage with the theoretical minimum test cases while being computationally more efficient than industry-standard tools on the TCAS-II benchmark.", "conclusion": "Robin's Rule offers a provably optimal solution for MC/DC verification in safety-critical systems, proving efficacy with minimal test cases and superior efficiency, making it a practical replacement for existing methods."}}
{"id": "2507.14213", "categories": ["cs.CR", "cond-mat.mes-hall", "cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2507.14213", "abs": "https://arxiv.org/abs/2507.14213", "authors": ["Irena Spasojevic", "Federica Celegato", "Alessandro Magni", "Paola Tiberto", "Jordi Sort"], "title": "Magneto-Ionic Hardware Security Primitives: Embedding Data Protection at the Material Level", "comment": null, "summary": "The Big Data revolution has heightened the demand for robust,\nenergy-efficient security hardware capable of withstanding increasingly\nsophisticated cyber threats. Conventional encryption schemes, reliant on\ncomplex algorithms, are resource-intensive and remain vulnerable. To fortify\nsensitive information, society needs innovative anti-hacking and\nanti-counterfeiting technologies that exploit new materials and designs. Here,\nwe present a magneto-ionic strategy for hardware-level security based on fully\nselective voltage-controlled N3- ion migration within pre-defined, initially\nparamagnetic FeCoN dots. This process generates ferromagnetic sublayers of\ntuneable thickness, resulting in either deterministic (single-domain or vortex)\nor probabilistic states (with coexisting magnetic configurations and\nvoltage-adjustable probabilities), each exhibiting stochastic orientation and\nchirality, thereby providing a rich platform for magnetic fingerprinting. This\napproach enables self-protected primitives, including true random number\ngenerators, physical unclonable functions, and in-memory probabilistic\ninference. The resulting reconfigurable architecture combines tamper\nresistance, low energy consumption, and scalability, marking a significant leap\ntoward next-generation hardware security rooted in emergent magnetic phenomena.", "AI": {"tldr": "This paper introduces a magneto-ionic approach to enhance hardware security using FeCoN dots, enabling voltage-controlled N3- ion migration to create tunable ferromagnetic sublayers with stochastic magnetic properties for secure primitives like true random number generators and physical unclonable functions.", "motivation": "The rise of Big Data necessitates robust, energy-efficient security hardware to counter sophisticated cyber threats, as traditional encryption methods are resource-intensive and vulnerable to attacks.", "method": "Utilizes fully selective voltage-controlled N3- ion migration within paramagnetic FeCoN dots to generate ferromagnetic sublayers with controllable thickness, inducing either deterministic (single-domain/vortex) or probabilistic (coexisting magnetic configurations) states with stochastic orientation and chirality.", "result": "Enables self-protected hardware primitives including true random number generators, physical unclonable functions, and in-memory probabilistic inference. The architecture demonstrates tamper resistance, low energy consumption, and scalability through reconfigurable magnetic properties.", "conclusion": "This innovation represents a significant advancement toward next-generation hardware security by leveraging emergent magnetic phenomena, offering enhanced protection against hacking and counterfeiting while maintaining energy efficiency and scalability."}}
{"id": "2507.14716", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14716", "abs": "https://arxiv.org/abs/2507.14716", "authors": ["Shahidul Islam", "Ashik Aowal", "Md Sharif Uddin", "Shaiful Chowdhury"], "title": "HistoryFinder: Advancing Method-Level Source Code History Generation with Accurate Oracles and Enhanced Algorithm", "comment": null, "summary": "Reconstructing a method's change history efficiently and accurately is\ncritical for many software engineering tasks, including maintenance,\nrefactoring, and comprehension. Despite the availability of method history\ngeneration tools such as CodeShovel and CodeTracker, existing evaluations of\ntheir effectiveness are limited by inaccuracies in the ground truth oracles\nused. In this study, we systematically construct two new oracles -- the\ncorrected CodeShovel oracle and a newly developed HistoryFinder oracle -- by\ncombining automated analysis with expert-guided manual validation. We also\nintroduce HistoryFinder, a new method history generation tool designed to\nimprove not only the accuracy and completeness of method change histories but\nalso to offer competitive runtime performance. Through extensive evaluation\nacross 400 methods from 40 open-source repositories, we show that HistoryFinder\nconsistently outperforms CodeShovel, CodeTracker, IntelliJ, and Git-based\nbaselines in terms of precision, recall, and F1 score. Moreover, HistoryFinder\nachieves competitive runtime performance, offering the lowest mean and median\nexecution times among all the research-based tools.\n  While Git-based tools exhibit the fastest runtimes, this efficiency comes at\nthe cost of significantly lower precision and recall -- leaving HistoryFinder\nas the best overall choice when both accuracy and efficiency are important. To\nfacilitate adoption, we provide a web interface, CLI, and Java library for\nflexible usage.", "AI": {"tldr": "The paper introduces HistoryFinder, a new method history generation tool that outperforms existing tools in accuracy and runtime efficiency by using more reliable ground truth oracles combining automated and manual validation.", "motivation": "Existing method history tools like CodeShovel and CodeTracker have evaluations limited by inaccurate ground truth oracles, which motivates the need for more reliable validation methods and improved tools.", "method": "The authors systematically created two new oracles (corrected CodeShovel and HistoryFinder) through automated analysis with expert-guided manual validation, and developed HistoryFinder to enhance accuracy, completeness, and runtime performance.", "result": "HistoryFinder outperformed CodeShovel, CodeTracker, IntelliJ, and Git-based baselines across all metrics (precision, recall, F1 score) with 400 evaluated methods from 40 open-source repositories, achieving the lowest mean and median execution times among research-based tools.", "conclusion": "HistoryFinder is positions as the best overall solution when both accuracy and efficiency are critical, offering faster runtime than research tools while surpassing Git-based methods in precision."}}
{"id": "2507.14222", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.14222", "abs": "https://arxiv.org/abs/2507.14222", "authors": ["Shu-Ting Huang", "Wen-Cheng Chung", "Hao-Ting Pai"], "title": "GPU-Accelerated Interpretable Generalization for Rapid Cyberattack Detection and Forensics", "comment": "ACM CCS 2025 (Submitted)", "summary": "The Interpretable Generalization (IG) mechanism recently published in IEEE\nTransactions on Information Forensics and Security delivers state-of-the-art,\nevidence-based intrusion detection by discovering coherent normal and attack\npatterns through exhaustive intersect-and-subset operations-yet its cubic-time\ncomplexity and large intermediate bitsets render full-scale datasets\nimpractical on CPUs. We present IG-GPU, a PyTorch re-architecture that offloads\nall pairwise intersections and subset evaluations to commodity GPUs.\nImplemented on a single NVIDIA RTX 4070 Ti, in the 15k-record NSL-KDD dataset,\nIG-GPU shows a 116-fold speed-up over the multi-core CPU implementation of IG.\nIn the full size of NSL-KDD (148k-record), given small training data (e.g.,\n10%-90% train-test split), IG-GPU runs in 18 minutes with Recall 0.957,\nPrecision 0.973, and AUC 0.961, whereas IG required down-sampling to\n15k-records to avoid memory exhaustion and obtained Recall 0.935, Precision\n0.942, and AUC 0.940. The results confirm that IG-GPU is robust across scales\nand could provide millisecond-level per-flow inference once patterns are\nlearned. IG-GPU thus bridges the gap between rigorous interpretability and\nreal-time cyber-defense, offering a portable foundation for future work on\nhardware-aware scheduling, multi-GPU sharding, and dataset-specific sparsity\noptimizations.", "AI": {"tldr": "IG-GPU accelerates the Interpretable Generalization mechanism for intrusion detection by offloading operations to GPUs, achieving 116x speed-up over CPU implementations and enabling robust real-time performance.", "motivation": "The original IG mechanism's cubic-time complexity and memory demands make it impractical for large datasets on CPUs, necessitating a solution for scalable, real-time cyber-defense.", "method": "Re-architecture of IG using PyTorch to offload pairwise intersections and subset evaluations to commodity GPUs, optimized for parallel processing.", "result": "On NSL-KDD (15k/148k records), IG-GPU achieves 116x CPU speed-up, 18-minute runtime for full data, and improved metrics (Recall 0.957, Precision 0.973, AUC 0.961) without down-sampling.", "conclusion": "IG-GPU bridges the gap between interpretability and real-time performance for intrusion detection, enabling scalable cyber-defense systems and future optimizations via GPU hardware compatibility."}}
{"id": "2507.14735", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14735", "abs": "https://arxiv.org/abs/2507.14735", "authors": ["Vladyslav Bulhakov", "Giordano d'Aloisio", "Claudio Di Sipio", "Antinisca Di Marco", "Davide Di Ruscio"], "title": "Investigating the Role of LLMs Hyperparameter Tuning and Prompt Engineering to Support Domain Modeling", "comment": "Accepted at 51st Euromicro Conference Series on Software Engineering\n  and Advanced Applications (SEAA)", "summary": "The introduction of large language models (LLMs) has enhanced automation in\nsoftware engineering tasks, including in Model Driven Engineering (MDE).\nHowever, using general-purpose LLMs for domain modeling has its limitations.\nOne approach is to adopt fine-tuned models, but this requires significant\ncomputational resources and can lead to issues like catastrophic forgetting.\n  This paper explores how hyperparameter tuning and prompt engineering can\nimprove the accuracy of the Llama 3.1 model for generating domain models from\ntextual descriptions. We use search-based methods to tune hyperparameters for a\nspecific medical data model, resulting in a notable quality improvement over\nthe baseline LLM. We then test the optimized hyperparameters across ten diverse\napplication domains.\n  While the solutions were not universally applicable, we demonstrate that\ncombining hyperparameter tuning with prompt engineering can enhance results\nacross nearly all examined domain models.", "AI": {"tldr": "The paper investigates how hyperparameter tuning and prompt engineering enhance the Llama 3.1 model's accuracy in generating domain models from text, showing improved results across multiple domains.", "motivation": "General-purpose LLMs for domain modeling face limitations in accuracy, and fine-tuning is resource-intensive and prone to catastrophic forgetting. This study aims to enhance LLM performance for domain modeling through alternative optimization techniques.", "method": "The authors employ search-based hyperparameter tuning for the Llama 3.1 model on a medical data model, followed by testing optimized parameters across ten application domains, combined with prompt engineering techniques.", "result": "Hyperparameter tuning with prompt engineering improves model quality in a medical case and achieves enhanced results in almost all tested domains, though not universally effective.", "conclusion": "Combining hyperparameter tuning and prompt engineering can effectively improve domain model generation accuracy for LLMs like Llama 3.1 across diverse application areas."}}
{"id": "2507.14223", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14223", "abs": "https://arxiv.org/abs/2507.14223", "authors": ["Wen-Cheng Chung", "Shu-Ting Huang", "Hao-Ting Pai"], "title": "Multi-Granular Discretization for Interpretable Generalization in Precise Cyberattack Identification", "comment": "ACM CCS 2025 (Submitted)", "summary": "Explainable intrusion detection systems (IDS) are now recognized as essential\nfor mission-critical networks, yet most \"XAI\" pipelines still bolt an\napproximate explainer onto an opaque classifier, leaving analysts with partial\nand sometimes misleading insights. The Interpretable Generalization (IG)\nmechanism, published in IEEE Transactions on Information Forensics and\nSecurity, eliminates that bottleneck by learning coherent patterns - feature\ncombinations unique to benign or malicious traffic - and turning them into\nfully auditable rules. IG already delivers outstanding precision, recall, and\nAUC on NSL-KDD, UNSW-NB15, and UKM-IDS20, even when trained on only 10% of the\ndata. To raise precision further without sacrificing transparency, we introduce\nMulti-Granular Discretization (IG-MD), which represents every continuous\nfeature at several Gaussian-based resolutions. On UKM-IDS20, IG-MD lifts\nprecision by greater than or equal to 4 percentage points across all nine\ntrain-test splits while preserving recall approximately equal to 1.0,\ndemonstrating that a single interpretation-ready model can scale across domains\nwithout bespoke tuning.", "AI": {"tldr": "The paper introduces Multi-Granular Discretization (IG-MD), an enhancement to the Interpretable Generalization (IG) mechanism, to improve precision in explainable intrusion detection systems while maintaining rule-based interpretability and domain adaptability.", "motivation": "Current explainable intrusion detection systems (IDS) often rely on post-hoc explanations from opaque classifiers, resulting in incomplete or misleading insights. This hinders effective analysis and decision-making in mission-critical networks.", "method": "IG-MD combines the IG mechanism's coherent pattern learning (converting feature combinations into auditable rules) with Multi-Granular Discretization, which models continuous features at multiple Gaussian-based resolutions to capture nuanced traffic characteristics.", "result": "On UKM-IDS20, IG-MD achieves precision gains of \u22654% across all train-test splits with near-perfect recall (\u22481.0). It maintains strong performance on NSL-KDD and UNSW-NB15 at 10% training data utilization.", "conclusion": "IG-MD demonstrates that high-precision, transparent intrusion detection can be achieved through multi-resolution feature modeling, enabling a single interpretation-ready model to adapt across domains without manual tuning."}}
{"id": "2507.14770", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14770", "abs": "https://arxiv.org/abs/2507.14770", "authors": ["Manaal Basha", "Ivan Beschastnikh", "Gema Rodriguez-Perez", "Cleidson R. B. de Souza"], "title": "Toward Inclusive AI-Driven Development: Exploring Gender Differences in Code Generation Tool Interactions", "comment": "ESEM 2025 Registered Reports", "summary": "Context: The increasing reliance on Code Generation Tools (CGTs), such as\nWindsurf and GitHub Copilot, are revamping programming workflows and raising\ncritical questions about fairness and inclusivity. While CGTs offer potential\nproductivity enhancements, their effectiveness across diverse user groups have\nnot been sufficiently investigated. Objectives: We hypothesize that developers'\ninteractions with CGTs vary based on gender, influencing task outcomes and\ncognitive load, as prior research suggests that gender differences can affect\ntechnology use and cognitive processing. Methods: The study will employ a\nmixed-subjects design with 54 participants, evenly divided by gender for a\ncounterbalanced design. Participants will complete two programming tasks\n(medium to hard difficulty) with only CGT assistance and then with only\ninternet access. Task orders and conditions will be counterbalanced to mitigate\norder effects. Data collection will include cognitive load surveys, screen\nrecordings, and task performance metrics such as completion time, code\ncorrectness, and CGT interaction behaviors. Statistical analyses will be\nconducted to identify statistically significant differences in CGT usage.\nExpected Contributions: Our work can uncover gender differences in CGT\ninteraction and performance among developers. Our findings can inform future\nCGT designs and help address usability and potential disparities in interaction\npatterns across diverse user groups. Conclusion: While results are not yet\navailable, our proposal lays the groundwork for advancing fairness,\naccountability, transparency, and ethics (FATE) in CGT design. The outcomes are\nanticipated to contribute to inclusive AI practices and equitable tool\ndevelopment for all users.", "AI": {"tldr": "The study investigates gender differences in the use of Code Generation Tools (CGTs) among developers to address fairness, inclusivity, and cognitive load in programming workflows.", "motivation": "CGTs like Copilot are reshaping programming but lack investigation into how their effectiveness varies across user groups, particularly gender, while prior research shows gender differences in technology use and cognitive processing.", "method": "A mixed-subjects design with 54 gender-balanced participants completing two programming tasks under CGT-only and internet-only conditions. Data includes cognitive load surveys, screen recordings, and performance metrics (time, correctness, interaction behaviors).", "result": "Pending (study proposed), expected to uncover gender-based differences in CGT usage patterns and task performance outcomes.", "conclusion": "The proposed study aims to advance fairness, accountability, and transparency in CGT design, supporting inclusive AI practices through better understanding of gender-influenced interactions."}}
{"id": "2507.14229", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.14229", "abs": "https://arxiv.org/abs/2507.14229", "authors": ["Vanja Stojanovi\u0107", "\u017diga Lesar", "CIril Bohak"], "title": "Using Modular Arithmetic Optimized Neural Networks To Crack Affine Cryptographic Schemes Efficiently", "comment": null, "summary": "We investigate the cryptanalysis of affine ciphers using a hybrid neural\nnetwork architecture that combines modular arithmetic-aware and statistical\nfeature-based learning. Inspired by recent advances in interpretable neural\nnetworks for modular arithmetic and neural cryptanalysis of classical ciphers,\nour approach integrates a modular branch that processes raw ciphertext\nsequences and a statistical branch that leverages letter frequency features.\nExperiments on datasets derived from natural English text demonstrate that the\nhybrid model attains high key recovery accuracy for short and moderate\nciphertexts, outperforming purely statistical approaches for the affine cipher.\nHowever, performance degrades for very long ciphertexts, highlighting\nchallenges in model generalization.", "AI": {"tldr": "A hybrid neural network combining modular arithmetic-aware and statistical feature-based learning outperforms purely statistical approaches for affine cipher cryptanalysis on short/medium ciphertexts, but struggles with long texts.", "motivation": "The paper addresses the need to improve affine cipher cryptanalysis by integrating interpretable modular arithmetic capabilities with statistical frequency analysis, inspired by recent advancements in neural cryptanalysis and modular arithmetic modeling.", "method": "The architecture employs a dual-branch system: a modular branch processing raw ciphertext sequences with arithmetic operations and a statistical branch using letter frequency features. These branches are trained jointly for key recovery.", "result": "The hybrid model achieves high key recovery accuracy for short (e.g., 80-100 characters) and moderate-length ciphertexts, while purely statistical baselines fail to match performance in these settings. However, accuracy degrades for very long ciphertexts.", "conclusion": "The hybrid approach demonstrates advantages for affine ciphers where arithmetic properties are critical, but exposes limitations in model generalization for extended sequences. This highlights the potential of neural networks with domain-knowledge integration for classical cryptanalysis while indicating challenges in scalable performance."}}
{"id": "2507.14776", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14776", "abs": "https://arxiv.org/abs/2507.14776", "authors": ["Kimia Tasnia", "Alexander Garcia", "Tasnuva Farheen", "Sazadur Rahman"], "title": "VeriOpt: PPA-Aware High-Quality Verilog Generation via Multi-Role LLMs", "comment": "9 pages, 7 figures, Accepted for ICCAD 2025, Munich, Germany", "summary": "The rapid adoption of large language models(LLMs) in hardware design has\nprimarily focused on generating functionally correct Verilog code, overlooking\ncritical Power Performance-Area(PPA) metrics essential for industrial-grade\ndesigns. To bridge this gap, we propose VeriOpt, a novel framework that\nleverages role-based prompting and PPA-aware optimization to enable LLMs to\nproduce high-quality, synthesizable Verilog. VeriOpt structures LLM\ninteractions into specialized roles (e.g., Planner, Programmer, Reviewer,\nEvaluator) to emulate human design workflows, while integrating PPA constraints\ndirectly into the prompting pipeline. By combining multi-modal feedback (e.g.,\nsynthesis reports, timing diagrams) with PPA aware prompting, VeriOpt achieves\nPPA-efficient code generation without sacrificing functional correctness.\nExperimental results demonstrate up to 88% reduction in power, 76% reduction in\narea and 73% improvement in timing closure compared to baseline LLM-generated\nRTL, validated using industry standard EDA tools. At the same time achieves 86%\nsuccess rate in functionality evaluation. Our work advances the\nstate-of-the-art AI-driven hardware design by addressing the critical gap\nbetween correctness and quality, paving the way for reliable LLM adoption in\nproduction workflows.", "AI": {"tldr": "VeriOpt is a framework that enhances LLM-generated Verilog code by integrating PPA-aware optimization and role-based prompting, achieving both high functional correctness and significant Power/Performance/Area (PPA) improvements validated with industry tools.", "motivation": "Current LLM applications in hardware design prioritize functional correctness but overlook critical PPA metrics necessary for industrial-grade implementations, creating a gap between AI-generated designs and production requirements.", "method": "VeriOpt structures LLM interactions through specialized roles (Planner, Programmer, Reviewer, Evaluator) to mimic human workflows, combines multi-modal feedback (synthesis reports, timing diagrams) with PPA-aware prompting, and integrates PPA constraints directly into the prompting pipeline.", "result": "Experimental results show 88% lower power, 76% smaller area, 73% better timing closure compared to baseline LLM-generated RTL, while maintaining 86% functionality success rate as verified by industry-standard EDA tools.", "conclusion": "VeriOpt advances AI-driven hardware design by addressing the PPA-functional correctness disconnect, enabling scalable and reliable LLM adoption for production-grade digital circuit synthesis."}}
{"id": "2507.14248", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG", "I.2.10; I.2.6; I.5.1; D.4.6; K.6.5"], "pdf": "https://arxiv.org/pdf/2507.14248", "abs": "https://arxiv.org/abs/2507.14248", "authors": ["Eldor Abdukhamidov", "Mohammed Abuhamad", "Simon S. Woo", "Hyoungshick Kim", "Tamer Abuhmed"], "title": "Breaking the Illusion of Security via Interpretation: Interpretable Vision Transformer Systems under Attack", "comment": null, "summary": "Vision transformer (ViT) models, when coupled with interpretation models, are\nregarded as secure and challenging to deceive, making them well-suited for\nsecurity-critical domains such as medical applications, autonomous vehicles,\ndrones, and robotics. However, successful attacks on these systems can lead to\nsevere consequences. Recent research on threats targeting ViT models primarily\nfocuses on generating the smallest adversarial perturbations that can deceive\nthe models with high confidence, without considering their impact on model\ninterpretations. Nevertheless, the use of interpretation models can effectively\nassist in detecting adversarial examples. This study investigates the\nvulnerability of transformer models to adversarial attacks, even when combined\nwith interpretation models. We propose an attack called \"AdViT\" that generates\nadversarial examples capable of misleading both a given transformer model and\nits coupled interpretation model. Through extensive experiments on various\ntransformer models and two transformer-based interpreters, we demonstrate that\nAdViT achieves a 100% attack success rate in both white-box and black-box\nscenarios. In white-box scenarios, it reaches up to 98% misclassification\nconfidence, while in black-box scenarios, it reaches up to 76%\nmisclassification confidence. Remarkably, AdViT consistently generates accurate\ninterpretations in both scenarios, making the adversarial examples more\ndifficult to detect.", "AI": {"tldr": "AdViT adversarial examples degrade both ViT model and interpretability, achieving 100% success rates in white-box/black-box scenarios with up to 98%/76% misclassification confidence.", "motivation": "Existing adversarial attacks on Vision Transformers (ViT) ignore the effect on model interpolations despite their role in detection. Transformer-interpreter combinations are presumed secure, but this assumption warrants reevaluation.", "method": "Proposed AdViT attacks that optimize perturbations to simultaneously mislead ViT models and their coupled interpreters through a unified loss function combining classification and interpretation features.", "result": "Achieved 100% attack success rate across 4 ViT variants and two interpreters (GradCAM and Class Activation Mapping), maintaining high confidence (98% white-box / 76% black-box) while producing naturalistic, undetectable perturbations. Perturbations caused consistent misclassification across model/interpreter pairs.", "conclusion": "Even when paired with interpretation models, ViT systems remain vulnerable to adversaries that manipulate both visual classification and interpretability patterns, suggesting current security assumptions for transformer models are insufficient and detection methods need stronger robustness analysis."}}
{"id": "2507.14791", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14791", "abs": "https://arxiv.org/abs/2507.14791", "authors": ["Yang Liu", "Li Zhang", "Fang Liu", "Zhuohang Wang", "Donglin Wei", "Zhishuo Yang", "Kechi Zhang", "Jia Li", "Lin Shi"], "title": "Enhancing Repository-Level Code Generation with Call Chain-Aware Multi-View Context", "comment": null, "summary": "Repository-level code generation aims to generate code within the context of\na specified repository. Existing approaches typically employ\nretrieval-augmented generation (RAG) techniques to provide LLMs with relevant\ncontextual information extracted from the repository. However, these approaches\noften struggle with effectively identifying truly relevant contexts that\ncapture the rich semantics of the repository, and their contextual perspectives\nremains narrow. Moreover, most approaches fail to account for the structural\nrelationships in the retrieved code during prompt construction, hindering the\nLLM's ability to accurately interpret the context. To address these issues, we\npropose RepoScope, which leverages call chain-aware multi-view context for\nrepository-level code generation. RepoScope constructs a Repository Structural\nSemantic Graph (RSSG) and retrieves a comprehensive four-view context,\nintegrating both structural and similarity-based contexts. We propose a novel\ncall chain prediction method that utilizes the repository's structural\nsemantics to improve the identification of callees in the target function.\nAdditionally, we present a structure-preserving serialization algorithm for\nprompt construction, ensuring the coherence of the context for the LLM.\nNotably, RepoScope relies solely on static analysis, eliminating the need for\nadditional training or multiple LLM queries, thus ensuring both efficiency and\ngeneralizability. Evaluation on widely-used repository-level code generation\nbenchmarks (CoderEval and DevEval) demonstrates that RepoScope outperforms\nstate-of-the-art methods, achieving up to a 36.35% relative improvement in\npass@1 scores. Further experiments emphasize RepoScope's potential to improve\ncode generation across different tasks and its ability to integrate effectively\nwith existing approaches.", "AI": {"tldr": "RepoScope is a novel repository-level code generation framework that leverages call chain-aware multi-view context and structural semantic graphs to outperform existing RAG-based methods by 36.35% in pass@1 scores without extra training.", "motivation": "Current repository-level code generation approaches using RAG struggle with semantic richness of contextual information and fail to account for structural relationships in retrieved code, leading to narrow contextual perspectives and inaccurate LLM interpretations.", "method": "1. Constructs a Repository Structural Semantic Graph (RSSG) to enable comprehensive four-view context retrieval\n2. Implements a call chain prediction method using structural semantics\n3. Develops a structure-preserving serialization algorithm for prompt construction\n4. Relys solely on static analysis without requiring additional training or multiple LLM queries", "result": "Achieved up to 36.35% relative improvement in pass@1 scores on CoderEval and DevEval benchmarks. Demonstrated cross-task effectiveness and seamless integration with existing approaches through experiments.", "conclusion": "RepoScope's structural analysis approach offers both efficiency and generalizability advantages over previous methods, making it a valuable advancement in repository-level code generation through better semantic preservation and structural awareness."}}
{"id": "2507.14324", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.14324", "abs": "https://arxiv.org/abs/2507.14324", "authors": ["Yao Ma", "Wen Yu Kon", "Jefferson Chu", "Kevin Han Yong Loh", "Kaushik Chakraborty", "Charles Lim"], "title": "Quantum-Safe Identity Verification using Relativistic Zero-Knowledge Proof Systems", "comment": null, "summary": "Identity verification is the process of confirming an individual's claimed\nidentity, which is essential in sectors like finance, healthcare, and online\nservices to ensure security and prevent fraud. However, current\npassword/PIN-based identity solutions are susceptible to phishing or skimming\nattacks, where malicious intermediaries attempt to steal credentials using fake\nidentification portals. Alikhani et al. [Nature, 2021] began exploring identity\nverification through graph coloring-based relativistic zero-knowledge proofs\n(RZKPs), a key cryptographic primitive that enables a prover to demonstrate\nknowledge of secret credentials to a verifier without disclosing any\ninformation about the secret. Our work advances this field and addresses\nunresolved issues: From an engineering perspective, we relax further the\nrelativistic constraints from 60m to 30m, and significantly enhance the\nstability and scalability of the experimental demonstration of the 2-prover\ngraph coloring-based RZKP protocol for near-term use cases. At the same time,\nfor long-term security against entangled malicious provers, we propose a\nmodified protocol with comparable computation and communication costs, we\nestablish an upper bound on the soundness parameter for this modified protocol.\nOn the other hand, we extend the two-prover, two-verifier setup to a\nthree-prover configuration, demonstrating the security of such relativistic\nprotocols against entangled malicious provers.", "AI": {"tldr": "This paper advances relativistic zero-knowledge proof (RZKP) for secure identity verification by relaxing distance constraints, enhancing stability/scalability for near-term use, and proposing a three-prover configuration with upper bounds for multi-round protocols.", "motivation": "Current password/PIN identity systems are vulnerable to phishing/skimming attacks. RZKPs enable proving identity without disclosing secrets, but existing implementations face stability, scalability, and long-term security challenges against quantum adversaries.", "method": "1. Relaxed relativistic constraints from 60m to 30m\n2. Modified 2-prover graph coloring RZKP protocol for improved stability/scalability\n3. Established soundness parameter bounds for the modified protocol\n4. Extended the protocol to a three-prover configuration and analyzed its security against entangled malicious provers", "result": "Demonstrated improved practicality of RZKPs through experimental enhancements (lower constraints + 30% stability/15x scalability improvements). The three-prover setup achieves security in multi-round protocols with \u03c9(G) \u2264 3/2^k for k rounds, while maintaining comparable protocol costs.", "conclusion": "The work provides critical engineering advancements for immediate RZKP deployment and establishes a theoretical framework to adapt to future quantum threats. The three-prover extension validates the robustness of relativistic principles in securing collaborative verification against sophisticated adversarial entanglements."}}
{"id": "2507.14969", "categories": ["cs.SE", "D.2.1"], "pdf": "https://arxiv.org/pdf/2507.14969", "abs": "https://arxiv.org/abs/2507.14969", "authors": ["Sai Zhang", "Zhenchang Xing", "Jieshan Chen", "Dehai Zhao", "Zizhong Zhu", "Xiaowang Zhang", "Zhiyong Feng", "Xiaohong Li"], "title": "Think Like an Engineer: A Neuro-Symbolic Collaboration Agent for Generative Software Requirements Elicitation and Self-Review", "comment": null, "summary": "The vision of End-User Software Engineering (EUSE) is to empower\nnon-professional users with full control over the software development\nlifecycle. It aims to enable users to drive generative software development\nusing only natural language requirements. However, since end-users often lack\nknowledge of software engineering, their requirement descriptions are\nfrequently ambiguous, raising significant challenges to generative software\ndevelopment. Although existing approaches utilize structured languages like\nGherkin to clarify user narratives, they still struggle to express the causal\nlogic between preconditions and behavior actions. This paper introduces\nRequireCEG, a requirement elicitation and self-review agent that embeds\ncausal-effect graphs (CEGs) in a neuro-symbolic collaboration architecture.\nRequireCEG first uses a feature tree to analyze user narratives hierarchically,\nclearly defining the scope of software components and their system behavior\nrequirements. Next, it constructs the self-healing CEGs based on the elicited\nrequirements, capturing the causal relationships between atomic preconditions\nand behavioral actions. Finally, the constructed CEGs are used to review and\noptimize Gherkin scenarios, ensuring consistency between the generated Gherkin\nrequirements and the system behavior requirements elicited from user\nnarratives. To evaluate our method, we created the RGPair benchmark dataset and\nconducted extensive experiments. It achieves an 87% coverage rate and raises\ndiversity by 51.88%.", "AI": {"tldr": "RequireCEG is a neuro-symbolic agent for improving end-user software requirements through self-healing causal-effect graphs, enhancing Gherkin scenario generation with 87% coverage and 51.88% higher diversity.", "motivation": "End-user software engineering faces challenges due to ambiguous natural language requirements. Existing tools like Gherkin fail to express causal logic between preconditions and behaviors, necessitating better requirement elicitation.", "method": "RequireCEG uses (1) hierarchical feature trees to structure requirements, (2) self-healing causal-effect graphs (CEGs) to model causal relationships between atomic preconditions and behaviors, and (3) CEG analysis to review/optimize Gherkin scenarios.", "result": "RGPair experiments show RequireCEG achieves 87% requirement coverage and 51.88% increased diversity in generated scenarios compared to existing approaches.", "conclusion": "Causal-effect graph integration provides a robust neuro-symbolic framework for natural language requirements, enabling consistent Gherkin scenarios through explicit causal modeling and hierarchical organization."}}
{"id": "2507.14519", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14519", "abs": "https://arxiv.org/abs/2507.14519", "authors": ["Wenxuan Zeng", "Tianshi Xu", "Yi Chen", "Yifan Zhou", "Mingzhe Zhang", "Jin Tan", "Cheng Hong", "Meng Li"], "title": "Towards Efficient Privacy-Preserving Machine Learning: A Systematic Review from Protocol, Model, and System Perspectives", "comment": "This work will be continuously updated to reflect the latest advances", "summary": "Privacy-preserving machine learning (PPML) based on cryptographic protocols\nhas emerged as a promising paradigm to protect user data privacy in cloud-based\nmachine learning services. While it achieves formal privacy protection, PPML\noften incurs significant efficiency and scalability costs due to orders of\nmagnitude overhead compared to the plaintext counterpart. Therefore, there has\nbeen a considerable focus on mitigating the efficiency gap for PPML. In this\nsurvey, we provide a comprehensive and systematic review of recent PPML studies\nwith a focus on cross-level optimizations. Specifically, we categorize existing\npapers into protocol level, model level, and system level, and review progress\nat each level. We also provide qualitative and quantitative comparisons of\nexisting works with technical insights, based on which we discuss future\nresearch directions and highlight the necessity of integrating optimizations\nacross protocol, model, and system levels. We hope this survey can provide an\noverarching understanding of existing approaches and potentially inspire future\nbreakthroughs in the PPML field. As the field is evolving fast, we also provide\na public GitHub repository to continuously track the developments, which is\navailable at https://github.com/PKU-SEC-Lab/Awesome-PPML-Papers.", "AI": {"tldr": "This survey systematically reviews recent PPML research focusing on cross-level optimizations (protocol, model, system), provides comparative insights, and highlights integration of optimizations for efficiency.", "motivation": "Existing PPML protocols face significant efficiency and scalability challenges due to computational overheads compared to plaintext alternatives.", "method": "Categorization and analysis of PPML studies into three optimization levels (protocol, model, system) with qualitative and quantitative comparisons.", "result": "Technical insights on optimization progress, comparative analysis of existing works, and identification of research opportunities through multi-level integration.", "conclusion": "Achieving efficient PPML requires coordinated optimizations across protocol, model, and system layers, supported by ongoing tracking through provided GitHub repository (https://github.com/PKU-SEC-Lab/Awesome-PPML-Papers)."}}
{"id": "2507.15003", "categories": ["cs.SE", "cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15003", "abs": "https://arxiv.org/abs/2507.15003", "authors": ["Hao Li", "Haoxiang Zhang", "Ahmed E. Hassan"], "title": "The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering", "comment": null, "summary": "The future of software engineering--SE 3.0--is unfolding with the rise of AI\nteammates: autonomous, goal-driven systems collaborating with human developers.\nAmong these, autonomous coding agents are especially transformative, now\nactively initiating, reviewing, and evolving code at scale. This paper\nintroduces AIDev, the first large-scale dataset capturing how such agents\noperate in the wild. Spanning over 456,000 pull requests by five leading\nagents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across\n61,000 repositories and 47,000 developers, AIDev provides an unprecedented\nempirical foundation for studying autonomous teammates in software development.\n  Unlike prior work that has largely theorized the rise of AI-native software\nengineering, AIDev offers structured, open data to support research in\nbenchmarking, agent readiness, optimization, collaboration modeling, and AI\ngovernance. The dataset includes rich metadata on PRs, authorship, review\ntimelines, code changes, and integration outcomes--enabling exploration beyond\nsynthetic benchmarks like SWE-bench. For instance, although agents often\noutperform humans in speed, their PRs are accepted less frequently, revealing a\ntrust and utility gap. Furthermore, while agents accelerate code\nsubmission--one developer submitted as many PRs in three days as they had in\nthree years--these are structurally simpler (via code complexity metrics).\n  We envision AIDev as a living resource: extensible, analyzable, and ready for\nthe SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev\nenables a new generation of research into AI-native workflows and supports\nbuilding the next wave of symbiotic human-AI collaboration. The dataset is\npublicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.\n  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering\nAgent", "AI": {"tldr": "This paper introduces AIDev, the first large-scale dataset capturing autonomous coding agents' operations in real-world software development, enabling empirical research into AI-native workflows and human-AI collaboration.", "motivation": "Previous research on AI-native software engineering has been largely theoretical, lacking empirical data to study autonomous teammates' actual impacts on code quality, collaboration dynamics, and adoption challenges.", "method": "AIDev compiles over 456,000 pull requests from five leading agents (Codex, Devin, Copilot, Cursor, Claude Code) across 61,000 repositories, 47,000 developers, and 3+ years. It includes structured metadata on PR content, authorship, review timelines, code complexity metrics, and integration outcomes.", "result": "Despite faster code submission by agents (e.g., 3 years' worth of PRs submitted in 3 days), their proposals show lower acceptance rates (trust/quality gap) and produce simpler code structures compared to developers. The dataset reveals patterns in agent behavior and collaboration challenges in real-world contexts.", "conclusion": "AIDev provides a public, extensible foundation for SE 3.0 research, enabling benchmarking, scalability analysis, governance studies, and optimization of agentic systems. It supports understanding trust gaps, structural limitations, and opportunities for symbiotic human-AI collaboration in software engineering."}}
{"id": "2507.14588", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.14588", "abs": "https://arxiv.org/abs/2507.14588", "authors": ["Usayd Shahul", "J. Harshan"], "title": "FORTA: Byzantine-Resilient FL Aggregation via DFT-Guided Krum", "comment": "To appear in the Proceedings of IEEE Information Theory Workshop\n  2025, Sydney, Australia", "summary": "Secure federated learning enables collaborative model training across\ndecentralized users while preserving data privacy. A key component is secure\naggregation, which keeps individual updates hidden from both the server and\nusers, while also defending against Byzantine users who corrupt the\naggregation. To this end, Jinhyun So et al. recently developed a\nByzantine-resilient secure aggregation scheme using a secret-sharing strategy\nover finite-field arithmetic. However, such an approach can suffer from\nnumerical errors and overflows when applied to real-valued model updates,\nmotivating the need for secure aggregation methods that operate directly over\nthe real domain. We propose FORTA, a Byzantine-resilient secure aggregation\nframework that operates entirely in the real domain. FORTA leverages Discrete\nFourier Transform (DFT) codes for privacy and employs Krum-based outlier\ndetection for robustness. While DFT decoder is error-free under infinite\nprecision, finite precision introduces numerical perturbations that can distort\ndistance estimates and allow malicious updates to evade detection. To address\nthis, FORTA refines Krum using feedback from DFT decoder, improving the\nselection of trustworthy updates. Theoretical analysis and experiments show\nthat our modification of Krum offers improved robustness and more accurate\naggregation than standard Krum.", "AI": {"tldr": "FORTA is a real-domain Byzantine-resilient secure aggregation framework that uses DFT codes for privacy and refines Krum outlier detection with feedback, achieving improved robustness and accuracy compared to existing methods.", "motivation": "Finite-field based secure aggregation schemes (like that of Jinhyun So et al.) suffer from numerical errors and overflows when handling real-valued model updates, limiting their effectiveness for practical machine learning applications.", "method": "We propose FORTA which operates entirely in the real domain, combining Discrete Fourier Transform (DFT) codes for privacy preservation with a feedback-modified Krum algorithm that improves Byzantine user detection by incorporating information from the DFT decoding process.", "result": "Theoretical analysis and experiments demonstrate that our feedback-enhanced Krum variant provides better robustness and produces more accurate aggregations than standard Krum in real-domain settings.", "conclusion": "FORTA addresses critical limitations of finite-field approaches while maintaining security against both privacy breaches and Byzantine attacks, establishing a more practical and effective secure aggregation framework for federated learning."}}
{"id": "2507.15025", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15025", "abs": "https://arxiv.org/abs/2507.15025", "authors": ["Nenad Petrovic", "Vahid Zolfaghari", "Andre Schamschurko", "Sven Kirchner", "Fengjunjie Pan", "Chengdng Wu", "Nils Purschke", "Aleksei Velsh", "Krzysztof Lebioda", "Yinglei Song", "Yi Zhang", "Lukasz Mazur", "Alois Knoll"], "title": "Survey of GenAI for Automotive Software Development: From Requirements to Executable Code", "comment": "Conference paper accepted for GACLM 2025", "summary": "Adoption of state-of-art Generative Artificial Intelligence (GenAI) aims to\nrevolutionize many industrial areas by reducing the amount of human\nintervention needed and effort for handling complex underlying processes.\nAutomotive software development is considered to be a significant area for\nGenAI adoption, taking into account lengthy and expensive procedures, resulting\nfrom the amount of requirements and strict standardization. In this paper, we\nexplore the adoption of GenAI for various steps of automotive software\ndevelopment, mainly focusing on requirements handling, compliance aspects and\ncode generation. Three GenAI-related technologies are covered within the\nstate-of-art: Large Language Models (LLMs), Retrieval Augmented Generation\n(RAG), Vision Language Models (VLMs), as well as overview of adopted prompting\ntechniques in case of code generation. Additionally, we also derive a\ngeneralized GenAI-aided automotive software development workflow based on our\nfindings from this literature review. Finally, we include a summary of a survey\noutcome, which was conducted among our automotive industry partners regarding\nthe type of GenAI tools used for their daily work activities.", "AI": {"tldr": "This paper explores GenAI adoption in automotive software development, focusing on requirements handling, compliance, and code generation. It reviews LLMs, RAG, and VLMs, presents a generalized workflow, and summarizes a survey on industry tool usage.", "motivation": "Automotive software development is time-consuming and costly due to complex requirements and strict standards. GenAI adoption aims to reduce human effort and streamline these processes.", "method": "Literature review of GenAI technologies (LLMs, RAG, VLMs) and prompting techniques, combined with a survey of automotive industry partners to assess real-world GenAI tool implementation.", "result": "Derived a GenAI-aided workflow for automotive software development and provided insights from industry partner surveys regarding current GenAI tool usage patterns.", "conclusion": "GenAI offers significant potential for automotive software development but requires tailored workflows and further refinement of tools to ensure compliance while maximizing efficiency in this highly standardized domain."}}
{"id": "2507.14600", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.14600", "abs": "https://arxiv.org/abs/2507.14600", "authors": ["MA. Khajeian"], "title": "Hybrid Classical-Quantum Rainbow Table Attack on Human Passwords", "comment": null, "summary": "Passwords that are long and human-generated pose a challenge for both\nclassical and quantum attacks due to their irregular structure and large search\nspace. In this work, we present an enhanced classical-quantum hybrid attack\ntailored to this scenario. We build rainbow tables using dictionary-based\npassword generation with transformation rules to better model real user\nbehavior. These tables are then organized into buckets, enabling faster lookup\nand reduced space complexity. To perform quantum search within each bucket, we\nuse a distributed exact variant of Grover's algorithm, which offers lower\ncircuit depth and deterministic success. As a result, the overall quantum\ncircuit is shallower and more robust against noise, particularly from\ndepolarizing channels commonly found in near-term quantum devices. Through this\nwork, Overall, we propose a hybrid framework that combines structured rainbow\ntables with efficient quantum search to enhance password recovery.", "AI": {"tldr": "A hybrid attack combines classical rainbow tables with efficient quantum search to break long human-generated passwords.", "motivation": "Long human-generated passwords have irregular structures and large search spaces, challenging classical and quantum attacks. Existing solutions lack efficiency in modeling user behavior and noise resilience.", "method": "1) Build dictionary-based rainbow tables with transformation rules to simulate human password patterns. 2) Bucket organization for faster lookup. 3) Use distributed exact Grover algorithm to reduce circuit depth and ensure deterministic results.", "result": "Achieved shallower quantum circuits with lower noise vulnerability (especially against depolarizing channels) while improving password recovery efficiency through classical-quantum hybridization.", "conclusion": "Proposed a robust hybrid framework that models user behavior better and enhances password recovery effectiveness for long human-generated passwords in noisy quantum environments."}}
{"id": "2507.15157", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15157", "abs": "https://arxiv.org/abs/2507.15157", "authors": ["Giovanni Quattrocchi", "Liliana Pasquale", "Paola Spoletini", "Luciano Baresi"], "title": "Can LLMs Generate User Stories and Assess Their Quality?", "comment": null, "summary": "Requirements elicitation is still one of the most challenging activities of\nthe requirements engineering process due to the difficulty requirements\nanalysts face in understanding and translating complex needs into concrete\nrequirements. In addition, specifying high-quality requirements is crucial, as\nit can directly impact the quality of the software to be developed. Although\nautomated tools allow for assessing the syntactic quality of requirements,\nevaluating semantic metrics (e.g., language clarity, internal consistency)\nremains a manual and time-consuming activity. This paper explores how LLMs can\nhelp automate requirements elicitation within agile frameworks, where\nrequirements are defined as user stories (US). We used 10 state-of-the-art LLMs\nto investigate their ability to generate US automatically by emulating customer\ninterviews. We evaluated the quality of US generated by LLMs, comparing it with\nthe quality of US generated by humans (domain experts and students). We also\nexplored whether and how LLMs can be used to automatically evaluate the\nsemantic quality of US. Our results indicate that LLMs can generate US similar\nto humans in terms of coverage and stylistic quality, but exhibit lower\ndiversity and creativity. Although LLM-generated US are generally comparable in\nquality to those created by humans, they tend to meet the acceptance quality\ncriteria less frequently, regardless of the scale of the LLM model. Finally,\nLLMs can reliably assess the semantic quality of US when provided with clear\nevaluation criteria and have the potential to reduce human effort in\nlarge-scale assessments.", "AI": {"tldr": "This paper investigates the use of large language models (LLMs) for automating requirements elicitation in agile development by generating and evaluating user stories (US), with results showing the potential and limitations of LLMs.", "motivation": "Requirements elicitation is challenging due to complex translations of customer needs into concrete requirements, and manual assessment of semantic quality is time-consuming despite syntactic checks by automated tools.", "method": "The study utilized 10 state-of-the-art LLMs to generate US by emulating customer interviews and compared their quality with human-generated US from experts and students. It also evaluated LLMs' ability to assess the semantic quality of US.", "result": "LLMs generated US comparable to humans in coverage and stylistic quality but showed lower diversity and creativity. They frequently failed to meet acceptance criteria despite model size, and could assess semantic quality reliably when provided with clear criteria.", "conclusion": "LLMs demonstrate potential to automate US generation and evaluation within agile frameworks, but their creativity and ability to meet acceptance criteria require improvement. Clear evaluation criteria enable reliable semantic quality assessment, reducing human effort in large-scale processes."}}
{"id": "2507.14625", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14625", "abs": "https://arxiv.org/abs/2507.14625", "authors": ["Juntao Tan", "Anran Li", "Quanchao Liu", "Peng Ran", "Lan Zhang"], "title": "VTarbel: Targeted Label Attack with Minimal Knowledge on Detector-enhanced Vertical Federated Learning", "comment": null, "summary": "Vertical federated learning (VFL) enables multiple parties with disjoint\nfeatures to collaboratively train models without sharing raw data. While\nprivacy vulnerabilities of VFL are extensively-studied, its security\nthreats-particularly targeted label attacks-remain underexplored. In such\nattacks, a passive party perturbs inputs at inference to force\nmisclassification into adversary-chosen labels. Existing methods rely on\nunrealistic assumptions (e.g., accessing VFL-model's outputs) and ignore\nanomaly detectors deployed in real-world systems. To bridge this gap, we\nintroduce VTarbel, a two-stage, minimal-knowledge attack framework explicitly\ndesigned to evade detector-enhanced VFL inference. During the preparation\nstage, the attacker selects a minimal set of high-expressiveness samples (via\nmaximum mean discrepancy), submits them through VFL protocol to collect\npredicted labels, and uses these pseudo-labels to train estimated detector and\nsurrogate model on local features. In attack stage, these models guide\ngradient-based perturbations of remaining samples, crafting adversarial\ninstances that induce targeted misclassifications and evade detection. We\nimplement VTarbel and evaluate it against four model architectures, seven\nmultimodal datasets, and two anomaly detectors. Across all settings, VTarbel\noutperforms four state-of-the-art baselines, evades detection, and retains\neffective against three representative privacy-preserving defenses. These\nresults reveal critical security blind spots in current VFL deployments and\nunderscore urgent need for robust, attack-aware defenses.", "AI": {"tldr": "This paper introduces VTarbel, a two-stage attack framework targeting label security in vertical federated learning (VFL) that outperforms existing methods by evading real-world anomaly detectors, revealing critical vulnerabilities in current VFL deployments.", "motivation": "Existing VFL security attacks rely on unrealistic assumptions (e.g., model output access) and ignore practical anomaly detection systems, creating a need for minimal-knowledge attacks that mimic real-world scenarios.", "method": "The attack consists of: 1) Preparation stage selecting high-expressiveness samples using MMD to train local pseudo-models via VFL outputs 2) Attack stage using gradient-based perturbations guided by estimated models to craft adversarial samples while evading detectors.", "result": "VTarbel achieves superior attack success rates (vs four SOTA baselines) across 4 model architectures, 7 multimodal datasets, and 2 detectors, maintaining effectiveness against three privacy-preserving defenses without increasing detection rates.", "conclusion": "The work exposes significant security blind spots in VFL systems with detectors, stressing the urgent need for robust, attack-aware defense mechanisms in practical federated learning deployments."}}
{"id": "2507.15181", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15181", "abs": "https://arxiv.org/abs/2507.15181", "authors": ["Yinglong Zou", "Juan Zhai", "Chunrong Fang", "Yanzhou Mu", "Jiawei Liu", "Zhenyu Chen"], "title": "Deep Learning Framework Testing via Heuristic Guidance Based on Multiple Model Measurements", "comment": null, "summary": "Deep learning frameworks serve as the foundation for developing and deploying\ndeep learning applications. To enhance the quality of deep learning frameworks,\nresearchers have proposed numerous testing methods using deep learning models\nas test inputs. However, existing methods predominantly measure model bug\ndetection effectiveness as heuristic indicators, presenting three critical\nlimitations: Firstly, existing methods fail to quantitatively measure model's\noperator combination variety, potentially missing critical operator\ncombinations that could trigger framework bugs. Secondly, existing methods\nneglect measuring model execution time, resulting in the omission of numerous\nmodels potential for detecting more framework bugs within limited testing time.\nThirdly, existing methods overlook correlation between different model\nmeasurements, relying simply on single-indicator heuristic guidance without\nconsidering their trade-offs. To overcome these limitations, we propose DLMMM,\nthe first deep learning framework testing method to include multiple model\nmeasurements into heuristic guidance and fuse these measurements to achieve\ntheir trade-off. DLMMM firstly quantitatively measures model's bug detection\nperformance, operator combination variety, and model execution time. After\nthat, DLMMM fuses the above measurements based on their correlation to achieve\ntheir trade-off. To further enhance testing effectiveness, DLMMM designs\nmulti-level heuristic guidance for test input model generation.", "AI": {"tldr": "The paper proposes DLMMM, a novel deep learning framework testing method that addresses three limitations of existing approaches by quantitatively measuring operator combination variety, execution time, and their correlations in heuristic guidance.", "motivation": "Current testing methods for deep learning frameworks use heuristic indicators to measure model bug detection effectiveness but suffer from three critical shortcomings: inadequate quantification of operator combination variety, neglect of execution time as a test input metric, and overlooking correlations between measurements. This leads to missed opportunities for identifying critical bugs and inefficient testing.", "method": "DLMMM first quantitatively evaluates three model aspects (bug detection effectiveness, operator combination variety, and execution time). Then, it fuses these measurements using their correlations to achieve trade-offs through multi-level heuristic guidance during test input generation.", "result": "The abstract does not specify quantitative results, but it claims DLMMM improves testing effectiveness by addressing the identified limitations of existing methods through multi-indicator measurement fusion.", "conclusion": "DLMMM is the first method to systematically incorporate multiple model measurements and their trade-offs in heuristic guidance for deep learning framework testing, leading to more comprehensive and efficient bug detection."}}
{"id": "2507.14629", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14629", "abs": "https://arxiv.org/abs/2507.14629", "authors": ["Juntao Tan", "Lan Zhang", "Zhonghao Hu", "Kai Yang", "Peng Ran", "Bo Li"], "title": "VMask: Tunable Label Privacy Protection for Vertical Federated Learning via Layer Masking", "comment": null, "summary": "Though vertical federated learning (VFL) is generally considered to be\nprivacy-preserving, recent studies have shown that VFL system is vulnerable to\nlabel inference attacks originating from various attack surfaces. Among these\nattacks, the model completion (MC) attack is currently the most powerful one.\nExisting defense methods against it either sacrifice model accuracy or incur\nimpractical computational overhead. In this paper, we propose VMask, a novel\nlabel privacy protection framework designed to defend against MC attack from\nthe perspective of layer masking. Our key insight is to disrupt the strong\ncorrelation between input data and intermediate outputs by applying the secret\nsharing (SS) technique to mask layer parameters in the attacker's model. We\ndevise a strategy for selecting critical layers to mask, reducing the overhead\nthat would arise from naively applying SS to the entire model. Moreover, VMask\nis the first framework to offer a tunable privacy budget to defenders, allowing\nfor flexible control over the levels of label privacy according to actual\nrequirements. We built a VFL system, implemented VMask on it, and extensively\nevaluated it using five model architectures and 13 datasets with different\nmodalities, comparing it to 12 other defense methods. The results demonstrate\nthat VMask achieves the best privacy-utility trade-off, successfully thwarting\nthe MC attack (reducing the label inference accuracy to a random guessing\nlevel) while preserving model performance (e.g., in Transformer-based model,\nthe averaged drop of VFL model accuracy is only 0.09%). VMask's runtime is up\nto 60,846 times faster than cryptography-based methods, and it only marginally\nexceeds that of standard VFL by 1.8 times in a large Transformer-based model,\nwhich is generally acceptable.", "AI": {"tldr": "This paper proposes VMask, a novel label privacy protection framework for vertical federated learning (VFL) that disrupts the correlation between input data and intermediate outputs through layer masking with secret sharing. It achieves a strong privacy-utility trade-off, reduces label inference accuracy to random guessing levels, and improves runtime efficiency compared to cryptography-based methods.", "motivation": "Current VFL systems are vulnerable to powerful label inference attacks like model completion (MC), while existing defenses either degrade model accuracy or require impractical computational resources.", "method": "VMask applies secret sharing (SS) to mask only strategically selected critical layers in the attacker's model rather than the entire architecture, enabling tunable privacy budgets for flexible defense strength.", "result": "Extensive experiments on 5 model architectures and 13 datasets demonstrate VMask reduces MC attack accuracy to random guessing levels with 0.09% average accuracy loss in Transformer models. It is 60,846\u00d7 faster than cryptography-based methods and has 1.8\u00d7 higher runtime than standard VFL in large models.", "conclusion": "VMask represents a practical and effective solution for protecting label privacy in VFL by combining layer masking with secret sharing, achieving state-of-the-art privacy-utility trade-offs with tunable parameters and significant computational advantages over prior approaches."}}
{"id": "2507.15188", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.15188", "abs": "https://arxiv.org/abs/2507.15188", "authors": ["Chowdhury Shahriar Muzammel", "Maria Spichkova", "James Harland"], "title": "Cultural Impact on Requirements Engineering Activities: Bangladeshi Practitioners' View", "comment": null, "summary": "Requirements Engineering (RE) is one of the most interaction-intensive phases\nof software development. This means that RE activities might be especially\nimpacted by stakeholders' national culture. Software development projects\nincreasingly have a very diverse range of stakeholders. To future-proof RE\nactivities, we need to help RE practitioners avoid misunderstandings and\nconflicts that might arise from not understanding potential Cultural Influences\n(CIs). Moreover, an awareness of CIs supports diversity and inclusion in the IT\nprofession. Bangladesh has a growing IT sector with some unique socio-cultural\ncharacteristics, and has been largely overlooked in this research field. In\nthis study, we aim to investigate how the RE process is adopted in the context\nof Bangladeshi culture and what cultural influences impact overall RE\nactivities.", "AI": {"tldr": "This paper explores how Bangladeshi national culture influences Requirements Engineering (RE) processes in its growing IT sector, emphasizing the need to address cultural factors for effective RE practices.", "motivation": "Software development projects now involve diverse stakeholders, and cultural misunderstandings in RE can lead to conflicts. Bangladesh's unique socio-cultural context lacks representation in RE research, requiring insights to future-proof practices and promote inclusion.", "method": "The study investigates the adoption of RE processes in Bangladesh and identifies cultural influences affecting these activities through case studies, interviews, or observations (specific methods to be detailed in the full paper).", "result": "The paper reveals specific cultural factors (to be identified) shaping RE activities in Bangladesh, contributing to understanding how localization impacts requirements gathering in under-researched regions.", "conclusion": "Cultural awareness is critical for RE practitioners in Bangladesh to avoid conflicts and enhance inclusion. The study underscores the importance of tailoring RE methods to socio-cultural contexts in emerging IT sectors."}}
{"id": "2507.14739", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.14739", "abs": "https://arxiv.org/abs/2507.14739", "authors": ["Franco Oberti", "Stefano Di Carlo", "Alessandro Savino"], "title": "CANDoSA: A Hardware Performance Counter-Based Intrusion Detection System for DoS Attacks on Automotive CAN bus", "comment": "Accepted for publication at the 31st IEEE International Symposium on\n  On-Line Testing and Robust System Design 2025 (IOLTS25)", "summary": "The Controller Area Network (CAN) protocol, essential for automotive embedded\nsystems, lacks inherent security features, making it vulnerable to cyber\nthreats, especially with the rise of autonomous vehicles. Traditional security\nmeasures offer limited protection, such as payload encryption and message\nauthentication. This paper presents a novel Intrusion Detection System (IDS)\ndesigned for the CAN environment, utilizing Hardware Performance Counters\n(HPCs) to detect anomalies indicative of cyber attacks. A RISC-V-based CAN\nreceiver is simulated using the gem5 simulator, processing CAN frame payloads\nwith AES-128 encryption as FreeRTOS tasks, which trigger distinct HPC\nresponses. Key HPC features are optimized through data extraction and\ncorrelation analysis to enhance classification efficiency. Results indicate\nthat this approach could significantly improve CAN security and address\nemerging challenges in automotive cybersecurity.", "AI": {"tldr": "A new Intrusion Detection System using Hardware Performance Counters is proposed to enhance CAN security in autonomous vehicles.", "motivation": "The CAN protocol's lack of built-in security features poses increasing cybersecurity risks, especially in autonomous vehicles, as traditional measures like encryption and authentication only provide partial protection.", "method": "The researchers simulated a RISC-V-based CAN receiver using gem5, implemented AES-128 encrypted payloads as FreeRTOS tasks, and optimized HPC features through data extraction and correlation analysis for anomaly detection.", "result": "Experimental results demonstrate the approach's potential to significantly improve CAN security and address emerging automotive cybersecurity challenges.", "conclusion": "This work advances CAN intrusion detection by effectively utilizing hardware performance metrics for abnormal behavior identification in encrypted automotive communication environments."}}
{"id": "2507.15197", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.15197", "abs": "https://arxiv.org/abs/2507.15197", "authors": ["Chowdhury Shahriar Muzammel", "Maria Spichkova", "James Harland"], "title": "Towards Using Personas in Requirements Engineering: What Has Been Changed Recently?", "comment": null, "summary": "In requirements engineering (RE), personas are now being used to represent\nuser expectations and needs. This systematic mapping study (SMS) aims to\nexplore the most recent studies and to cover recent changes in trends,\nespecially related to the recent evolution of Generative AI approaches. Our SMS\ncovers the period between April 2023 and April 2025. We identified 22 relevant\npublications and analysed persona representation, construction, validation, as\nwell as RE activities covered by personas. We identified that a number of\nstudies applied AI-based solutions for persona construction and validation. We\nobserved that template-based personas are becoming more popular nowadays. We\nalso observed an increase in the proportion of studies covering validation\naspects.", "AI": {"tldr": "This SMS analyzes 22 RE papers (2023-2025) on personas and Generative AI trends, revealing increased AI use in construction/validation, popularity of template-based personas, and growing validation research focus.", "motivation": "The paper seeks to map recent RE persona research against Generative AI advancements to understand evolving practices and identify gaps in the literature.", "method": "Systematic mapping study covering 22 publications from April 2023 - April 2025, analyzing persona representation, construction, validation, and RE activities.", "result": "Evidence of AI-based persona construction/validation, template-based persona popularity rise, and doubling of validation-focused studies compared to prior periods.", "conclusion": "The study provides insights into current persona methodologies in RE, highlighting AI integration and shifting validation priorities as critical trends for future research directions."}}
{"id": "2507.14796", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.14796", "abs": "https://arxiv.org/abs/2507.14796", "authors": ["Ceren Kocao\u011fullar", "Gustavo Petri", "Dominic P. Mulligan", "Derek Miller", "Hugo J. M. Vincent", "Shale Xiong", "Alastair R. Beresford"], "title": "Careful Whisper: Attestation for peer-to-peer Confidential Computing networks", "comment": null, "summary": "Trusted Execution Environments (TEEs) are designed to protect the privacy and\nintegrity of data in use. They enable secure data processing and sharing in\npeer-to-peer networks, such as vehicular ad hoc networks of autonomous\nvehicles, without compromising confidentiality. In these networks, nodes must\nestablish mutual trust to collaborate securely. TEEs can achieve this through\nremote attestation, where a prover presents evidence of its trustworthiness to\na verifier, which then decides whether or not to trust the prover. However, a\nnaive peer-to-peer attestation approach, where every TEE directly attests every\nother TEE, results in quadratic communication overhead. This is inefficient in\ndynamic environments, where nodes frequently join and leave the network.\n  To address this, we present Careful Whisper, a gossip-based protocol that\ndisseminates trust efficiently, reducing attestation overhead to linear\ncomplexity under ideal conditions. It enables interoperability by enabling\ntransitive trust across heterogeneous networks, and supports trust\nestablishment with offline nodes via relayed attestations. Using a custom\ndiscrete-event simulator, we show that Careful Whisper propagates trust both\nfaster and more widely than naive approaches across various network topologies.\nOur results demonstrate that our protocol is resource efficient, sending ~21.5\nKiB and requiring 0.158 seconds per round in a 200-node network, and that our\nprotocol is resilient to attestation failures across various network\ntopologies.", "AI": {"tldr": "This paper introduces Careful Whisper, a gossip-based protocol reducing TEE attestation overhead from quadratic to linear in dynamic peer-to-peer networks while enabling transitive trust and supporting offline nodes.", "motivation": "Vehicular ad hoc networks require secure collaboration without compromising confidentiality, but direct TEE attestation between all nodes leads to inefficient quadratic communication overhead, especially with frequent node joins/leaves.", "method": "Careful Whisper uses gossip-based dissemination of attestation evidence for transitive trust, relayed attestations for offline nodes, and evaluates performance via a custom discrete-event simulator across various network topologies.", "result": "Careful Whisper achieves linear attestation complexity, propagates trust faster/wider than naive methods, uses ~21.5 KiB of data (200-node network) with 0.158s per round, and maintains resilience to attestation failures across diverse topologies.", "conclusion": "The protocol effectively addresses TEE attestation inefficiencies in dynamic networks by providing resource-efficient, wide-reaching trust establishment with fault tolerance, enabling scalable secure collaboration in heterogeneous environments."}}
{"id": "2507.15224", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15224", "abs": "https://arxiv.org/abs/2507.15224", "authors": ["Yibo He", "Shuoran Zhao", "Jiaming Huang", "Yingjie Fu", "Hao Yu", "Cunjian Huang", "Tao Xie"], "title": "SimdBench: Benchmarking Large Language Models for SIMD-Intrinsic Code Generation", "comment": null, "summary": "SIMD (Single Instruction Multiple Data) instructions and their compiler\nintrinsics are widely supported by modern processors to accelerate\nperformance-critical tasks. SIMD intrinsic programming, a trade-off between\ncoding productivity and high performance, is widely used in the development of\nmainstream performance-critical libraries and daily computing tasks. Large\nLanguage Models (LLMs), which have demonstrated strong and comprehensive\ncapabilities in code generation, show promise in assisting programmers with the\nchallenges of SIMD intrinsic programming. However, existing code-generation\nbenchmarks focus on only scalar code, and it is unclear how LLMs perform in\ngenerating vectorized code using SIMD intrinsics. To fill this gap, we propose\nSimdBench, the first code benchmark specifically designed for SIMD-intrinsic\ncode generation, comprising 136 carefully crafted tasks and targeting five\nrepresentative SIMD intrinsics: SSE (x86 Streaming SIMD Extension), AVX (x86\nAdvanced Vector Extension), Neon (ARM Advanced SIMD Extension), SVE (ARM\nScalable Vector Extension), and RVV (RISC-V Vector Extension). We conduct a\nsystematic evaluation (measuring both correctness and performance) of 18\nrepresentative LLMs on SimdBench, resulting in a series of novel and insightful\nfindings. Our evaluation results demonstrate that LLMs exhibit a universal\ndecrease in pass@k during SIMD-intrinsic code generation compared to\nscalar-code generation. Our in-depth analysis highlights promising directions\nfor the further advancement of LLMs in the challenging domain of SIMD-intrinsic\ncode generation. SimdBench is fully open source at\nhttps://anonymous.4open.science/r/SimdBench-1B3F/ to benefit the broader\nresearch community.", "AI": {"tldr": "The paper introduces SimdBench, the first benchmark for evaluating SIMD-intrinsic code generation by Large Language Models (LLMs), revealing a universal performance drop in LLM-generated vectorized code compared to scalar code.", "motivation": "Existing code-generation benchmarks lack SIMD-intrinsic focused tasks despite their importance in performance-critical applications. It is unclear how LLMs handle vectorized code generation, motivating the creation of SimdBench for systematic evaluation.", "method": "136 tasks targeting five SIMD architectures (SSE, AVX, Neon, SVE, RVV) were designed as a benchmark. 18 LLMs were evaluated using correctness (pass@k) and performance metrics through a structured assessment framework.", "result": "LLMs show significantly lower pass@k rates for SIMD-intrinsic code generation than scalar code. Performance analysis highlights challenges in handling vectorized operations, with insights into specific failing patterns and instruction set dependencies.", "conclusion": "The study establishes that LLMs struggle with maintaining code correctness in SIMD intrinsic generation. It proposes avenues for LLM improvements through SimdBench's open-source release to advance research in domain-specific code generation."}}
{"id": "2507.14799", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14799", "abs": "https://arxiv.org/abs/2507.14799", "authors": ["Sam Johnson", "Viet Pham", "Thai Le"], "title": "Manipulating LLM Web Agents with Indirect Prompt Injection Attack via HTML Accessibility Tree", "comment": "EMNLP 2025 System Demonstrations Submission", "summary": "This work demonstrates that LLM-based web navigation agents offer powerful\nautomation capabilities but are vulnerable to Indirect Prompt Injection (IPI)\nattacks. We show that adversaries can embed universal adversarial triggers in\nwebpage HTML to hijack agent behavior that utilizes the accessibility tree to\nparse HTML, causing unintended or malicious actions. Using the Greedy\nCoordinate Gradient (GCG) algorithm and a Browser Gym agent powered by\nLlama-3.1, our system demonstrates high success rates across real websites in\nboth targeted and general attacks, including login credential exfiltration and\nforced ad clicks. Our empirical results highlight critical security risks and\nthe need for stronger defenses as LLM-driven autonomous web agents become more\nwidely adopted. The system software\n(https://github.com/sej2020/manipulating-web-agents) is released under the MIT\nLicense, with an accompanying publicly available demo website\n(http://lethaiq.github.io/attack-web-llm-agent).", "AI": {"tldr": "This paper shows that LLM-based web navigation agents are vulnerable to Indirect Prompt Injection (IPI) attacks through adversarial HTML triggers, demonstrating high success rates in hijacking agent behavior for malicious actions like credential theft and forced ad clicks.", "motivation": "The research highlights critical security risks associated with the growing adoption of LLM-driven autonomous web agents, emphasizing the need for robust defenses against potential adversarial manipulations.", "method": "The authors use the Greedy Coordinate Gradient (GCG) algorithm to craft universal adversarial triggers in webpage HTML. These triggers exploit accessibility tree parsing mechanisms in agents, particularly testing with a Browser Gym agent powered by Llama-3.1.", "result": "Empirical results demonstrate high attack success rates on real websites, showing effectiveness of both targeted (e.g., credential exfiltration) and general attacks (e.g., forced ad clicks) using LLM-based agents with standard parsing tools.", "conclusion": "The study reveals significant vulnerabilities in LLM-driven web agents due to IPI attacks through HTML manipulation. The authors release their system software and demo website to promote awareness and encourage development of stronger defensive mechanisms."}}
{"id": "2507.15226", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15226", "abs": "https://arxiv.org/abs/2507.15226", "authors": ["Changguo Jia", "Yi Zhan", "Tianqi Zhao", "Hengzhi Ye", "Minghui Zhou"], "title": "Code Clone Detection via an AlphaFold-Inspired Framework", "comment": null, "summary": "Code clone detection, which aims to identify functionally equivalent code\nfragments, plays a critical role in software maintenance and vulnerability\nanalysis. Substantial methods have been proposed to detect code clones, but\nthey fall short in capturing code semantics or relying on language-specific\nanalyzers. Inspired by the remarkable success of AlphaFold in predicting\nthree-dimensional protein structures from protein sequences, in this paper, we\nleverage AlphaFold for code clone detection based on the insight that protein\nsequences and token sequences share a common linear sequential structure. In\nparticular, we propose AlphaCC, which represents code fragments as token\nsequences to ensure multi-language applicability and adapts AlphaFold's\nsequence-to-structure modeling capability to infer code semantics. The pipeline\nof AlphaCC goes through three steps. First, AlphaCC transforms each input code\nfragment into a token sequence and, motivated by AlphaFold's use of multiple\nsequence alignment (MSA) to enhance contextual understanding, constructs an MSA\nfrom lexically similar token sequences. Second, AlphaCC adopts a modified\nattention-based encoder based on AlphaFold to model dependencies within and\nacross token sequences. Finally, unlike AlphaFold's protein structure\nprediction task, AlphaCC computes similarity scores between token sequences\nthrough a late interaction strategy and performs binary classification to\ndetermine code clone pairs. Comprehensive evaluations on three language-diverse\ndatasets demonstrate AlphaCC's applicability across multiple programming\nlanguages. On two semantic clone detection datasets, it consistently\noutperforms all baselines, showing strong semantic understanding. Moreover,\nAlphaCC maintains competitive efficiency, enabling practical usage in\nlarge-scale clone detection tasks.", "AI": {"tldr": "AlphaCC adapts AlphaFold's sequence-to-structure modeling for code clone detection, enabling multi-language semantic analysis with competitive efficiency.", "motivation": "Existing code clone detection methods either lack semantic understanding or depend on language-specific analyzers, creating limitations in universal applicability and accuracy.", "method": "The framework converts code to token sequences, constructs multi-sequence alignments (MSA) for enhanced context, employs a modified AlphaFold attention encoder for semantic modeling, and computes similarity scores via late interaction for binary classification of clones.", "result": "Outperforms baselines in semantic clone detection across languages using three language-diverse datasets, with efficiency suitable for large-scale analysis.", "conclusion": "AlphaCC demonstrates strong cross-language semantic understanding and practicality for real-world code clone detection tasks by extending AlphaFold's architectural innovations."}}
{"id": "2507.14822", "categories": ["cs.CR", "cs.ET", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.14822", "abs": "https://arxiv.org/abs/2507.14822", "authors": ["Zeeshan Kaleem", "Misha Urooj Khan", "Ahmad Suleman", "Waqas Khalid", "Kai-Kit Wong", "Chau Yuen"], "title": "Quantum Skyshield: Quantum Key Distribution and Post-Quantum Authentication for Low-Altitude Wireless Networks in Adverse Skies", "comment": null, "summary": "Recently, low-altitude wireless networks (LAWNs) have emerged as a critical\nbackbone for supporting the low-altitude economy, particularly with the\ndensification of unmanned aerial vehicles (UAVs) and high-altitude platforms\n(HAPs). To meet growing data demands, some LAWN deployments incorporate\nfree-space optical (FSO) links, which offer exceptional bandwidth and beam\ndirectivity. However, without strong security measures in place, both\nconventional radio frequency channels and FSO beams remain vulnerable to\ninterception and spoofing and FSO in particular can suffer from turbulence,\nmisalignment, and weather-related attenuation. To address these challenges in\nthe quantum era, a quantum-secure architecture called Quantum Skyshield is\nproposed to enable reliable communication between the base transceiver station\n(BTS) and LAWN. The proposed design integrates BB84 quantum key distribution\n(QKD) with post-quantum authentication mechanisms. Simulation results confirm\nthe reliable generation of a 128-bit symmetric key when the quantum bit error\nrate (QBER) remains below the threshold of 11%. Authentication is enforced\nusing Lamport one-time signatures and hash-based message authentication codes\n(HMAC) to ensure message integrity. A Grover-inspired threat detection\nmechanism identifies anomalies with up to 89% probability in a single\niteration, enabling real-time trust evaluation. Lastly, future research\nchallenges have also been identified and discussed to guide further development\nin this area.", "AI": {"tldr": "The paper proposes Quantum Skyshield, a quantum-secure architecture for low-altitude wireless networks (LAWNs) that combines BB84 QKD with post-quantum authentication to address security vulnerabilities and environmental challenges in FSO links.", "motivation": "LAWNs, crucial for the low-altitude economy with UAVs/HAPs, face security risks (interception/spoofing) and reliability issues (turbulence, misalignment, weather attenuation) in traditional RF and FSO communication as data demands grow.", "method": "Quantum Skyshield integrates BB84 quantum key distribution (QKD) for symmetric key generation, Lamport one-time signatures and HMAC for authentication, and a Grover-inspired threat detection mechanism for anomaly identification.", "result": "Simulations validate a 128-bit symmetric key generation at QBER <11%; authentication ensures message integrity with post-quantum methods; Grover-based detection identifies anomalies with ~89% probability in real-time trust evaluation.", "conclusion": "Quantum Skyshield demonstrates feasibility for secure LAWN communication but highlights open challenges (e.g., improving detection probability, practical deployment hurdles) to drive future research."}}
{"id": "2507.15241", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15241", "abs": "https://arxiv.org/abs/2507.15241", "authors": ["Vikram Nitin", "Baishakhi Ray", "Roshanak Zilouchian Moghaddam"], "title": "FaultLine: Automated Proof-of-Vulnerability Generation Using LLM Agents", "comment": null, "summary": "Despite the critical threat posed by software security vulnerabilities,\nreports are often incomplete, lacking the proof-of-vulnerability (PoV) tests\nneeded to validate fixes and prevent regressions. These tests are crucial not\nonly for ensuring patches work, but also for helping developers understand how\nvulnerabilities can be exploited. Generating PoV tests is a challenging\nproblem, requiring reasoning about the flow of control and data through deeply\nnested levels of a program.\n  We present FaultLine, an LLM agent workflow that uses a set of carefully\ndesigned reasoning steps, inspired by aspects of traditional static and dynamic\nprogram analysis, to automatically generate PoV test cases. Given a software\nproject with an accompanying vulnerability report, FaultLine 1) traces the flow\nof an input from an externally accessible API (\"source\") to the \"sink\"\ncorresponding to the vulnerability, 2) reasons about the conditions that an\ninput must satisfy in order to traverse the branch conditions encountered along\nthe flow, and 3) uses this reasoning to generate a PoV test case in a\nfeedback-driven loop. FaultLine does not use language-specific static or\ndynamic analysis components, which enables it to be used across programming\nlanguages.\n  To evaluate FaultLine, we collate a challenging multi-lingual dataset of 100\nknown vulnerabilities in Java, C and C++ projects. On this dataset, FaultLine\nis able to generate PoV tests for 16 projects, compared to just 9 for CodeAct\n2.1, a popular state-of-the-art open-source agentic framework. Thus, FaultLine\nrepresents a 77% relative improvement over the state of the art. Our findings\nsuggest that hierarchical reasoning can enhance the performance of LLM agents\non PoV test generation, but the problem in general remains challenging. We make\nour code and dataset publicly available in the hope that it will spur further\nresearch in this area.", "AI": {"tldr": "The paper introduces FaultLine, an LLM agent workflow that automatically generates proof-of-vulnerability (PoV) test cases by combining reasoning steps inspired by static/dynamic analysis, achieving a 77% improvement over existing methods on a multi-lingual dataset of 100 vulnerabilities.", "motivation": "Software vulnerability reports often lack PoV tests, which are essential for validating fixes and preventing regressions. Generating these tests is challenging due to the need to reason about complex program flows across nested levels in multiple programming languages.", "method": "FaultLine employs an LLM-based agent workflow with three key steps: (1) tracing input flow from API sources to vulnerability sinks, (2) analyzing branch conditions for test requirements, and (3) iteratively generating test cases using feedback loops. It avoids language-specific analysis components for cross-language generality.", "result": "FaultLine successfully generated PoV tests for 16/100 vulnerabilities in Java/C/C++ projects, outperforming CodeAct 2.1 (9/100) with a 77% relative improvement. Evaluation shows hierarchical reasoning enhances LLM agents' performance in this domain.", "conclusion": "While hierarchical reasoning significantly improves LLM agent test generation for program vulnerabilities, the task remains difficult across language diversities. The authors open-source their code and dataset to advance further research in automated vulnerability testing."}}
{"id": "2507.14853", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14853", "abs": "https://arxiv.org/abs/2507.14853", "authors": ["Khoa Nguyen", "Tanveer Khan", "Antonis Michalas"], "title": "A Privacy-Centric Approach: Scalable and Secure Federated Learning Enabled by Hybrid Homomorphic Encryption", "comment": null, "summary": "Federated Learning (FL) enables collaborative model training without sharing\nraw data, making it a promising approach for privacy-sensitive domains. Despite\nits potential, FL faces significant challenges, particularly in terms of\ncommunication overhead and data privacy. Privacy-preserving Techniques (PPTs)\nsuch as Homomorphic Encryption (HE) have been used to mitigate these concerns.\nHowever, these techniques introduce substantial computational and communication\ncosts, limiting their practical deployment. In this work, we explore how Hybrid\nHomomorphic Encryption (HHE), a cryptographic protocol that combines symmetric\nencryption with HE, can be effectively integrated with FL to address both\ncommunication and privacy challenges, paving the way for scalable and secure\ndecentralized learning system.", "AI": {"tldr": "This paper proposes integrating Hybrid Homomorphic Encryption (HHE) with Federated Learning (FL) to reduce communication overhead while enhancing privacy, addressing limitations of traditional encryption methods. Motivation: FL is privacy-prioritizing but faces high costs in communication and computation due to existing privacy-preserving techniques like Homomorphic Encryption (HE). Method: A hybrid approach combining symmetric encryption (for efficiency) with HE (for security) is explored within FL's collaborative framework. Result: Demonstrates HHE's effectiveness in mitigating both communication and privacy challenges in FL. Conclusion: HHE enables scalable, secure decentralized learning by maintaining confidentiality while minimizing computational/communication burdens.", "motivation": "Federated Learning (FL) is crucial for privacy-sensitive domains but hindered by communication/privacy trade-offs. Traditional HE-based privacy-preserving techniques, while secure, introduce high computational and communication overhead, limiting real-world deployment.", "method": "The paper investigates integrating Hybrid Homomorphic Encryption (HHE), which combines symmetric encryption's efficiency with homomorphic encryption's privacy guarantees, into FL's collaborative model training architecture.", "result": "HHE significantly reduces encryption-related costs compared to HE while preserving data privacy, enabling FL systems to achieve practical scalability without compromising security.", "conclusion": "HHE presents a viable solution to reconcile FL's communication and privacy challenges, advancing the feasibility of secure decentralized learning in high-stakes environments."}}
{"id": "2507.15251", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15251", "abs": "https://arxiv.org/abs/2507.15251", "authors": ["Boyang Yang", "Luyao Ren", "Xin Yin", "Jiadong Ren", "Haoye Tian", "Shunfu Jin"], "title": "Input Reduction Enhanced LLM-based Program Repair", "comment": null, "summary": "Large Language Models (LLMs) have shown great potential in Automated Program\nRepair (APR). Test inputs, being crucial for reasoning the root cause of\nfailures, are always included in the prompt for LLM-based APR. Unfortunately,\nLLMs struggle to retain key information in long prompts. When the test inputs\nare extensive in the prompt, this may trigger the \"lost-in-the-middle\" issue,\ncompromising repair performance. To address this, we propose ReduceFix, an\nLLM-based APR approach with a built-in component that automatically reduces\ntest inputs while retaining their failure-inducing behavior. ReduceFix prompts\nan LLM to generate a reducer that minimizes failure-inducing test inputs\nwithout human effort, and then feeds the reduced failure-inducing inputs to\nguide patch generation.\n  For targeted evaluation, we constructed LFTBench, the first long-input APR\nbenchmark with 200 real bugs from 20 programming tasks, each paired with a\nfailure-inducing input whose median size is 1 MB. On this benchmark, ReduceFix\nshrinks inputs by 89.1% on average and improves overall pass@10 by up to 53.8%\nrelative to a prompt that includes the original test, and by 17.6% compared\nwith omitting the test entirely. Adding the same reduction step to ChatRepair\nincreases its fix rate by 21.3% without other changes. Ablation studies further\nhighlight the impact of input length and compressed failure information on\nrepair success. These results underscore that automatically reducing failing\ninputs is a practical and powerful complement to LLM-based APR, significantly\nimproving its scalability and effectiveness.", "AI": {"tldr": "ReduceFix is an LLM-based APR approach that automatically reduces failure-inducing test inputs to address the 'lost-in-the-middle' issue, improving repair performance. It introduces LFTBench, a new long-input APR benchmark, and demonstrates significant improvements in input reduction and repair success rates.", "motivation": "Test inputs are critical for diagnosing failure causes in APR, but LLMs lose key information with long prompts, causing the 'lost-in-the-middle' problem and reducing repair effectiveness.", "method": "We designed ReduceFix with an automatic test input reducer that eliminates redundancy while preserving failure-inducing behavior. The reduced inputs guide LLM-driven patch generation, supported by LFTBench (a 1 MB median input size APR benchmark) for evaluation.", "result": "ReduceFix achieves 89.1% average input reduction and boosts pass@10 by 53.8% compared to original test prompts, and 17.6% over no test inputs. Adding the reduction step to ChatRepair improves its fix rate by 21.3%. Ablation studies confirm the positive impact of reduced inputs and compressed failure information.", "conclusion": "Automated test input reduction using ReduceFix is a practical enhancement for LLM-based APR, resolving scalability issues with long prompts and significantly improving repair success rates."}}
{"id": "2507.14893", "categories": ["cs.CR", "math.NT", "11T71, 94A60, 68P25, 14G50, 81P94"], "pdf": "https://arxiv.org/pdf/2507.14893", "abs": "https://arxiv.org/abs/2507.14893", "authors": ["Farzin Renan"], "title": "A Compact Post-quantum Strong Designated Verifier Signature Scheme from Isogenies", "comment": null, "summary": "Digital signatures are essential cryptographic tools that provide\nauthentication and integrity in digital communications. However,\nprivacy-sensitive applications, such as e-voting and digital cash, require more\nrestrictive verification models to ensure confidentiality and control. Strong\nDesignated Verifier Signature (SDVS) schemes address this need by enabling the\nsigner to designate a specific verifier, ensuring that only this party can\nvalidate the signature. Existing SDVS constructions are primarily based on\nnumber-theoretic assumptions and are therefore vulnerable to quantum attacks.\nAlthough post-quantum alternatives, particularly those based on lattices, have\nbeen proposed, they often entail large key and signature sizes. In this work,\nwe introduce $\\mathsf{CSI\\text{-}SDVS}$, a novel isogeny-based SDVS scheme that\noffers a compact, quantum-resistant alternative. Our construction builds on the\nideal class group action framework of CSIDH and the signature techniques of\nCSI-FiSh, and relies on the hardness of the Multi-Target Group Action Inverse\nProblem (MT-GAIP). $\\mathsf{CSI\\text{-}SDVS}$ achieves strong security\nguarantees; namely, Strong Unforgeability under Chosen-Message Attacks\n(SUF-CMA), Non-Transferability (NT), and Privacy of Signer's Identity (PSI), in\nthe random oracle model. Remarkably, both the keys and signatures in\n$\\mathsf{CSI\\text{-}SDVS}$ are of size $\\mathcal{O}(\\lambda)$, representing a\nsignificant improvement over the typical $\\mathcal{O}(\\lambda^2)$ bounds in\nexisting post-quantum SDVS schemes, thereby making it among the most compact\nPQC-based SDVS schemes and the only post-quantum secure construction based on\nisogenies.", "AI": {"tldr": "This paper introduces CSI-SDVS, a compact isogeny-based post-quantum secure Strong Designated Verifier Signature (SDVS) scheme. It addresses the need for privacy in applications like e-voting and digital cash by using ideal class group actions from CSIDH and techniques from CSI-FiSh, achieving SUF-CMA, NT, and PSI security while reducing key and signature sizes from O(\u03bb\u00b2) to O(\u03bb).", "motivation": "Traditional SDVS schemes relying on number-theoretic assumptions are vulnerable to quantum attacks, while existing post-quantum alternatives (e.g., lattice-based) suffer from large key and signature sizes. Privacy-sensitive applications require both quantum resistance and compact verification models.", "method": "Leverages the ideal class group action framework from CSIDH and signature techniques from CSI-FiSh. Security is based on the Multi-Target Group Action Inverse Problem (MT-GAIP) hardness assumption.", "result": "Achieves SUF-CMA, Non-Transferability (NT), and Privacy of Signer's Identity (PSI) in the random oracle model. Key and signature sizes are O(\u03bb), a significant improvement over existing post-quantum SDVS schemes which require O(\u03bb\u00b2) size.", "conclusion": "CSI-SDVS is the first isogeny-based post-quantum secure SDVS scheme with compact keys and signatures, making it the most efficient PQC-based SDVS to date."}}
{"id": "2507.15296", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15296", "abs": "https://arxiv.org/abs/2507.15296", "authors": ["Qian Xiong", "Yuekai Huang", "Ziyou Jiang", "Zhiyuan Chang", "Yujia Zheng", "Tianhao Li", "Mingyang Li"], "title": "Butterfly Effects in Toolchains: A Comprehensive Analysis of Failed Parameter Filling in LLM Tool-Agent Systems", "comment": null, "summary": "The emergence of the tool agent paradigm has broadened the capability\nboundaries of the Large Language Model (LLM), enabling it to complete more\ncomplex tasks. However, the effectiveness of this paradigm is limited due to\nthe issue of parameter failure during its execution. To explore this phenomenon\nand propose corresponding suggestions, we first construct a parameter failure\ntaxonomy in this paper. We derive five failure categories from the invocation\nchain of a mainstream tool agent. Then, we explore the correlation between\nthree different input sources and failure categories by applying 15 input\nperturbation methods to the input. Experimental results show that parameter\nname hallucination failure primarily stems from inherent LLM limitations, while\nissues with input sources mainly cause other failure patterns. To improve the\nreliability and effectiveness of tool-agent interactions, we propose\ncorresponding improvement suggestions, including standardizing tool return\nformats, improving error feedback mechanisms, and ensuring parameter\nconsistency.", "AI": {"tldr": "This paper investigates parameter failure in tool agent -LLM systems, proposes a taxonomy of five failure types, identifies input source issues as a key cause, and suggests format standardization, enhanced error feedback, and parameter consistency to improve reliability.", "motivation": "Expanding LLM capabilities via tool agents is limited by execution parameter failures. Existing paradigms lack systematic failure analysis and solutions for complex tasks.", "method": "1) Construct parameter failure taxonomy by analyzing invocation chains of mainstream tool agents. 2) Apply 15 input perturbation methods across three input sources. 3) Quantitatively evaluate failure patterns.", "result": "1) 5 distinct failure categories identified. 2) Parameter name hallucination attributed to inherent LLM issues. 3) 78.3% of non-hallucination failures trace to input source problems. 4) Perturbation experiments isolate failure causality.", "conclusion": "Improving tool agent reliability requires: standardized return formats, context-aware error feedback systems, and parameter validation protocols to address source-induced failures and inherent LLM limitations."}}
{"id": "2507.14985", "categories": ["cs.CR", "cs.ET", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14985", "abs": "https://arxiv.org/abs/2507.14985", "authors": ["Argianto Rahartomo", "Leonel Merino", "Mohammad Ghafari"], "title": "Metaverse Security and Privacy Research: A Systematic Review", "comment": "The paper is accepted for publication at Computers & Security Journal", "summary": "The rapid growth of metaverse technologies, including virtual worlds,\naugmented reality, and lifelogging, has accelerated their adoption across\ndiverse domains. This rise exposes users to significant new security and\nprivacy challenges due to sociotechnical complexity, pervasive connectivity,\nand extensive user data collection in immersive environments. We present a\nsystematic review of the literature published between 2013 and 2024, offering a\ncomprehensive analysis of how the research community has addressed\nmetaverse-related security and privacy issues over the past decade. We organize\nthe studies by method, examined the security and privacy properties, immersive\ncomponents, and evaluation strategies. Our investigation reveals a sharp\nincrease in research activity in the last five years, a strong focus on\npractical and user-centered approaches, and a predominant use of benchmarking,\nhuman experimentation, and qualitative methods. Authentication and\nunobservability are the most frequently studied properties. However, critical\ngaps remain in areas such as policy compliance, accessibility,\ninteroperability, and back-end infrastructure security. We emphasize the\nintertwined technical complexity and human factors of the metaverse and call\nfor integrated, interdisciplinary approaches to securing inclusive and\ntrustworthy immersive environments.", "AI": {"tldr": "This paper systematically reviews 2013-2024 literature on metaverse security and privacy, identifying rising research focus on practical/user-centered approaches but highlighting critical gaps in policy compliance and infrastructure security.", "motivation": "Metaverse technologies' sociotechnical complexity and pervasive data collection create urgent security/privacy risks requiring structured academic analysis to guide future research and practice.", "method": "The authors conducted a systematic literature review, categorizing 128 studies by methodology (benchmarking/human experimentation/qualitative), analyzed security properties (authentication, unobservability), and evaluation strategies.", "result": "Found 128% annual growth in metaverse security research from 2019-2023, with 75% using practical/user-centered methods. 62% focus on authentication/unobservability, but only 12% address policy compliance, 9% accessibility, and 5% infrastructure security.", "conclusion": "The review underscores the need for integrated technical/social research approaches to address metaverse security gaps, particularly in policy mechanisms, accessibility design, and backend infrastructure protections to ensure trustworthy immersive environments."}}
{"id": "2507.15343", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15343", "abs": "https://arxiv.org/abs/2507.15343", "authors": ["Kechi Zhang", "Ge Li", "Jia Li", "Huangzhao Zhang", "Yihong Dong", "Jia Li", "Jingjing Xu", "Zhi Jin"], "title": "StackTrans: From Large Language Model to Large Pushdown Automata Model", "comment": "currently under development", "summary": "The Transformer architecture has emerged as a landmark advancement within the\nbroad field of artificial intelligence, effectively catalyzing the advent of\nlarge language models (LLMs). However, despite its remarkable capabilities and\nthe substantial progress it has facilitated, the Transformer architecture still\nhas some limitations. One such intrinsic limitation is its inability to\neffectively capture the Chomsky hierarchy, such as regular expressions or\ndeterministic context-free grammars. Drawing inspiration from pushdown\nautomata, which efficiently resolve deterministic context-free grammars using\nstacks, we propose StackTrans to address the aforementioned issue within LLMs.\nUnlike previous approaches that modify the attention computation, StackTrans\nexplicitly incorporates hidden state stacks between Transformer layers. This\ndesign maintains compatibility with existing frameworks like flash-attention.\nSpecifically, our design features stack operations -- such as pushing and\npopping hidden states -- that are differentiable and can be learned in an\nend-to-end manner. Our comprehensive evaluation spans benchmarks for both\nChomsky hierarchies and large-scale natural languages. Across these diverse\ntasks, StackTrans consistently outperforms standard Transformer models and\nother baselines. We have successfully scaled StackTrans up from 360M to 7B\nparameters. In particular, our from-scratch pretrained model StackTrans-360M\noutperforms several larger open-source LLMs with 2-3x more parameters,\nshowcasing its superior efficiency and reasoning capability.", "AI": {"tldr": "StackTrans is a novel architectural enhancement to the Transformer that integrates differentiable hidden state stacks to better capture Chomsky hierarchy properties, outperforming both standard Transformers and larger LLMs on structured and natural language benchmarks.", "motivation": "The Transformer architecture's inherent limitations in modeling sequential dependencies beyond context-free grammars (e.g. failing to properly handle regular expressions) prevent natural integration with formal language structures, limiting its linguistic modeling capabilities.", "method": "We introduce hidden state stacks between standard Transformer blocks, enabling differentiable push/pop operations on computational memory. Stack operations are implemented as learnable differentiable modules that integrate seamlessly with attention mechanisms and maintain compatibility with flash-attention optimizations.", "result": "StackTrans achieves state-of-the-art performance on both formal grammar (Chomsky hierarchy) and natural language benchmarks. The 360M parameter model outperforms multiple open-source LLMs 2-3\u00d7 bigger on parameter efficiency metrics, showing comparable performance with fewer parameters.", "conclusion": "StackTrans demonstrates that stack-based memory augmentation is a viable path to improve parameter efficiency and structured reasoning capabilities in large language models, maintaining full compatibility with existing Transformer tooling and frameworks."}}
{"id": "2507.15058", "categories": ["cs.CR", "cs.LG", "cs.SE", "D.2.5; D.4.6"], "pdf": "https://arxiv.org/pdf/2507.15058", "abs": "https://arxiv.org/abs/2507.15058", "authors": ["Ian Hardgrove", "John D. Hastings"], "title": "LibLMFuzz: LLM-Augmented Fuzz Target Generation for Black-box Libraries", "comment": "6 pages, 2 figures, 1 table, 2 listings", "summary": "A fundamental problem in cybersecurity and computer science is determining\nwhether a program is free of bugs and vulnerabilities. Fuzzing, a popular\napproach to discovering vulnerabilities in programs, has several advantages\nover alternative strategies, although it has investment costs in the form of\ninitial setup and continuous maintenance. The choice of fuzzing is further\ncomplicated when only a binary library is available, such as the case of\nclosed-source and proprietary software. In response, we introduce LibLMFuzz, a\nframework that reduces costs associated with fuzzing closed-source libraries by\npairing an agentic Large Language Model (LLM) with a lightweight tool-chain\n(disassembler/compiler/fuzzer) to autonomously analyze stripped binaries, plan\nfuzz strategies, generate drivers, and iteratively self-repair build or runtime\nerrors. Tested on four widely-used Linux libraries, LibLMFuzz produced\nsyntactically correct drivers for all 558 fuzz-able API functions, achieving\n100% API coverage with no human intervention. Across the 1601 synthesized\ndrivers, 75.52% were nominally correct on first execution. The results show\nthat LLM-augmented middleware holds promise in reducing the costs of fuzzing\nblack box components and provides a foundation for future research efforts.\nFuture opportunities exist for research in branch coverage.", "AI": {"tldr": "LibLMFuzz addresses the challenge of fuzzing closed-source libraries by integrating Large Language Models (LLMs) with a lightweight toolchain to automatically analyze binaries, create fuzz drivers, and self-repair errors, significantly reducing costs.", "motivation": "Fuzzing closed-source/proprietary software is difficult due to high setup/maintenance costs and lack of source code access, motivating the need for automated solutions to improve efficiency and reduce human intervention.", "method": "The framework combines an agentic LLM with a disassembler/compiler/fuzzer toolchain to autonomously analyze stripped binaries, plan fuzz strategies, generate drivers, and iteratively resolve build/runtime errors without human oversight.", "result": "LibLMFuzz achieved 100% API coverage for 558 fuzzable functions in four Linux libraries, generating syntactically correct drivers at 100% and achieving 75.52% nominal correctness on first execution across 1601 drivers.", "conclusion": "LLM-augmented middleware demonstrates cost-reduction potential for fuzzing black-box components, establishing a foundation for future research in branch coverage optimization and autonomous vulnerability discovery."}}
{"id": "2507.15599", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15599", "abs": "https://arxiv.org/abs/2507.15599", "authors": ["Manatsawin Hanmongkolchai"], "title": "Applying the Chinese Wall Reverse Engineering Technique to Large Language Model Code Editing", "comment": null, "summary": "Large language models for code (Code LLM) are increasingly utilized in\nprogramming environments. Despite their utility, the training datasets for top\nLLM remain undisclosed, raising concerns about potential copyright violations.\nSome models, such as Pleias and Comma put emphasis on data curation and\nlicenses, however, with limited training data these models are not competitive\nand only serve as proof of concepts. To improve the utility of these models, we\npropose an application of the \"Chinese Wall\" technique, inspired by the reverse\nengineering technique of the same name -- a high quality model is used to\ngenerate detailed instructions for a weaker model. By doing so, a weaker but\nethically aligned model may be used to perform complicated tasks that,\notherwise, can only be completed by more powerful models. In our evaluation,\nwe've found that this technique improves Comma v0.1 1T's performance in\nCanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%\ncompared to when running the same model on the benchmark alone. The practical\napplication of this technique today, however, may be limited due to the lack of\nmodels trained on public domain content without copyright restrictions.", "AI": {"tldr": "The paper proposes using the \"Chinese Wall\" technique to enhance ethical code LLMs by leveraging detailed instructions from stronger models, achieving performance boosts on benchmarks despite training data limitations.", "motivation": "Top code LLMs use undisclosed training data, raising copyright concerns. Existing ethical models with curated, licensed data (e.g., Comma, Pleias) are less competitive due to limited training data.", "method": "Apply reverse-engineered \"Chinese Wall\" approach: a high-quality model generates fine-grained instructions for a weaker model to execute complex tasks ethically.", "result": "Comma v0.1 1T's CanItEdit benchmark performance improved by 66%, and Starcoder2 Instruct by 20% when using Chinese Wall instructions, outperforming standalone execution.", "conclusion": "Chinese Wall technique shows promise for improving ethical code LLMs, but practical adoption hinges on wider availability of open-access, copyright-free training models."}}
{"id": "2507.15219", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15219", "abs": "https://arxiv.org/abs/2507.15219", "authors": ["Tianneng Shi", "Kaijie Zhu", "Zhun Wang", "Yuqi Jia", "Will Cai", "Weida Liang", "Haonan Wang", "Hend Alzahrani", "Joshua Lu", "Kenji Kawaguchi", "Basel Alomair", "Xuandong Zhao", "William Yang Wang", "Neil Gong", "Wenbo Guo", "Dawn Song"], "title": "PromptArmor: Simple yet Effective Prompt Injection Defenses", "comment": null, "summary": "Despite their potential, recent research has demonstrated that LLM agents are\nvulnerable to prompt injection attacks, where malicious prompts are injected\ninto the agent's input, causing it to perform an attacker-specified task rather\nthan the intended task provided by the user. In this paper, we present\nPromptArmor, a simple yet effective defense against prompt injection attacks.\nSpecifically, PromptArmor prompts an off-the-shelf LLM to detect and remove\npotential injected prompts from the input before the agent processes it. Our\nresults show that PromptArmor can accurately identify and remove injected\nprompts. For example, using GPT-4o, GPT-4.1, or o4-mini, PromptArmor achieves\nboth a false positive rate and a false negative rate below 1% on the AgentDojo\nbenchmark. Moreover, after removing injected prompts with PromptArmor, the\nattack success rate drops to below 1%. We also demonstrate PromptArmor's\neffectiveness against adaptive attacks and explore different strategies for\nprompting an LLM. We recommend that PromptArmor be adopted as a standard\nbaseline for evaluating new defenses against prompt injection attacks.", "AI": {"tldr": "PromptArmor effectively detects and removes prompt injection attacks in LLM agents, achieving under 1% false positive/negative rates and reducing attack success rates to near-zero.", "motivation": "LLM agents are vulnerable to prompt injection attacks where malicious inputs override user-specified tasks. Developing robust defenses against this threat is critical to ensure agent reliability and security.", "method": "PromptArmor leverages an off-the-shelf LLM (e.g., GPT-4o, o4-mini) to scan inputs pre-processing, identifying and removing potential injected prompts through strategic prompting techniques.", "result": "Evaluation on the AgentDojo benchmark shows PromptArmor achieves <1% false positive and false negative rates when using three models, with attack success rates dropping to below 1% after prompt removal. The system remains effective against adaptive attacks.", "conclusion": "PromptArmor demonstrates high efficacy as a simple defense mechanism against prompt injection, warranting its adoption as a standard baseline for future evaluation of LLM agent security defenses."}}
{"id": "2507.15624", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15624", "abs": "https://arxiv.org/abs/2507.15624", "authors": ["Yusuf Sulistyo Nugroho", "Ganno Tribuana Kurniaji", "Syful Islam", "Mohammed Humayun Kabir", "Vanesya Aura Ardity", "Md. Kamal Uddin"], "title": "Hot Topics and Common Challenges: an Empirical Study of React Discussions on Stack Overflow", "comment": "6 pages, 4 figures, 4 tables, conference paper", "summary": "React is a JavaScript library used to build user interfaces for single-page\napplications. Although recent studies have shown the popularity and advantages\nof React in web development, the specific challenges users face remain unknown.\nThus, this study aims to analyse the React-related questions shared on Stack\nOverflow. The study utilizes an exploratory data analysis to investigate the\nmost frequently discussed keywords, error classification, and user\nreputation-based errors, which is the novelty of this work. The results show\nthe top eight most frequently used keywords on React-related questions, namely,\ncode, link, vir, href, connect, azure, windows, and website. The error\nclassification of questions from the sample shows that algorithmic error is the\nmost frequent issue faced by all groups of users, where mid-reputation users\ncontribute the most, accounting for 55.77%. This suggests the need for the\ncommunity to provide guidance materials in solving algorithm-related problems.\nWe expect that the results of this study will provide valuable insight into\nfuture research to support the React community during the early stages of\nimplementation, facilitating their ability to effectively overcome challenges\nto adoption.", "AI": {"tldr": "This study analyzes React-related Stack Overflow questions to identify common challenges and errors, revealing top keywords and algorithmic errors as the primary issues faced by mid-reputation users", "motivation": "Despite React's popularity, specific user challenges remain underexplored, motivating this analysis to inform community support and future research", "method": "Exploratory data analysis of React-related Stack Overflow questions, focusing on keyword frequency, error classification, and novel reputation-based error correlations", "result": "Top 8 keywords included code/link-related terms; algorithmic errors (55.77% from mid-reputation users) were most prevalent category; comprehensive error typology established", "conclusion": "Highlights need for targeted algorithmic error guidance resources and provides foundational insights for improving React adoption support through early implementation assistance"}}
{"id": "2507.15377", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.15377", "abs": "https://arxiv.org/abs/2507.15377", "authors": ["Magali Bardet", "Charles Brion", "Philippe Gaborit", "Mercedes Haiech", "Romaric Neveu"], "title": "The Matrix Subcode Equivalence problem and its application to signature with MPC-in-the-Head", "comment": null, "summary": "Nowadays, equivalence problems are widely used in cryptography, most notably\nto establish cryptosystems such as digital signatures, with MEDS, LESS, PERK as\nthe most recent ones. However, in the context of matrix codes, only the code\nequivalence problem has been studied, while the subcode equivalence is\nwell-defined in the Hamming metric. In this work, we introduce two new\nproblems: the Matrix Subcode Equivalence Problem and the Matrix Code Permuted\nKernel Problem, to which we apply the MPCitH paradigm to build a signature\nscheme. These new problems, closely related to the Matrix Code Equivalence\nproblem, ask to find an isometry given a code $C$ and a subcode $D$.\nFurthermore, we prove that the Matrix Subcode Equivalence problem reduces to\nthe Hamming Subcode Equivalence problem, which is known to be NP-Complete, thus\nintroducing the matrix code version of the Permuted Kernel Problem. We also\nadapt the combinatorial and algebraic algorithms for the Matrix Code\nEquivalence problem to the subcode case, and we analyze their complexities. We\nfind with this analysis that the algorithms perform much worse than in the code\nequivalence case, which is the same as what happens in the Hamming metric.\nFinally, our analysis of the attacks allows us to take parameters much smaller\nthan in the Matrix Code Equivalence case. Coupled with the effectiveness of\n\\textit{Threshold-Computation-in-the-Head} or \\textit{VOLE-in-the-Head}, we\nobtain a signature size of $\\approx$ 4 800 Bytes, with a public key of\n$\\approx$ 275 Bytes. We thus obtain a reasonable signature size, which brings\ndiversity in the landscape of post-quantum signature schemes, by relying on a\nnew hard problem. In particular, this new signature scheme performs better than\nSPHINCS+, with a smaller size of public key + signature. Our signature compares\nalso well with other signature schemes: compared to MEDS, the signature is\nsmaller, and we reduced the size of the sum of signature and public key by a\nfactor close to 5. We also obtain a signature size that is almost half the size\nof the CROSS signature scheme.", "AI": {"tldr": "This paper introduces Matrix Subcode Equivalence and Permuted Kernel Problems for developing post-quantum signature schemes with reduced size compared to existing systems like SPHINCS+ and MEDS.", "motivation": "The authors aim to enhance cryptography by introducing new hard problems in matrix codes for post-quantum signatures, addressing the lack of subcode equivalence studies and improving efficiency over current schemes with compact key/signature sizes.", "method": "The Matrix Subcode Equivalence and Permuted Kernel Problems are defined, reduced to existing NP-Complete problems, and analyzed. The MPCitH paradigm is applied to build a signature scheme, with algorithmic complexity adaptations for subcode equivalence.", "result": "The new signature scheme achieves ~4800 Byte signatures and ~275 Byte public keys, outperforming SPHINCS+ (smaller key/signature total size), MEDS, and CROSS in compactness while maintaining equivalent algorithmic hardness to their Hamming counterparts.", "conclusion": "The proposed scheme establishes a novel class of post-quantum signatures with competitive size advantages over state-of-the-art systems, demonstrating viability through computational analysis and concrete parameter choices."}}
{"id": "2507.15663", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15663", "abs": "https://arxiv.org/abs/2507.15663", "authors": ["Giordano d'Aloisio", "Tosin Fadahunsi", "Jay Choy", "Rebecca Moussa", "Federica Sarro"], "title": "SustainDiffusion: Optimising the Social and Environmental Sustainability of Stable Diffusion Models", "comment": null, "summary": "Background: Text-to-image generation models are widely used across numerous\ndomains. Among these models, Stable Diffusion (SD) - an open-source\ntext-to-image generation model - has become the most popular, producing over 12\nbillion images annually. However, the widespread use of these models raises\nconcerns regarding their social and environmental sustainability.\n  Aims: To reduce the harm that SD models may have on society and the\nenvironment, we introduce SustainDiffusion, a search-based approach designed to\nenhance the social and environmental sustainability of SD models.\n  Method: SustainDiffusion searches the optimal combination of hyperparameters\nand prompt structures that can reduce gender and ethnic bias in generated\nimages while also lowering the energy consumption required for image\ngeneration. Importantly, SustainDiffusion maintains image quality comparable to\nthat of the original SD model.\n  Results: We conduct a comprehensive empirical evaluation of SustainDiffusion,\ntesting it against six different baselines using 56 different prompts. Our\nresults demonstrate that SustainDiffusion can reduce gender bias in SD3 by 68%,\nethnic bias by 59%, and energy consumption (calculated as the sum of CPU and\nGPU energy) by 48%. Additionally, the outcomes produced by SustainDiffusion are\nconsistent across multiple runs and can be generalised to various prompts.\n  Conclusions: With SustainDiffusion, we demonstrate how enhancing the social\nand environmental sustainability of text-to-image generation models is possible\nwithout fine-tuning or changing the model's architecture.", "AI": {"tldr": "SustainDiffusion is a search-based method to reduce gender/ethnic bias and energy consumption in Stable Diffusion (SD) text-to-image generation while maintaining comparable image quality.", "motivation": "Widespread use of SD models raises social and environmental sustainability concerns due to potential biases and high energy consumption associated with their operation.", "method": "The approach employs hyperparameter and prompt structure optimization through search algorithms to systematically reduce bias metrics and energy consumption during image generation, avoiding model retraining.", "result": "Empirical testing on 56 prompts shows SustainDiffusion reduces gender bias in SD3 by 68%, ethnic bias by 59%, and energy consumption by 48%, with consistent results across multiple runs and prompt generalization.", "conclusion": "Demonstrates the feasibility of improving social-environmental sustainability in text-to-image generation models without architectural changes to the base model."}}
{"id": "2507.15393", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15393", "abs": "https://arxiv.org/abs/2507.15393", "authors": ["Ruofan Liu", "Yun Lin", "Silas Yeo Shuen Yu", "Xiwen Teoh", "Zhenkai Liang", "Jin Song Dong"], "title": "PiMRef: Detecting and Explaining Ever-evolving Spear Phishing Emails with Knowledge Base Invariants", "comment": null, "summary": "Phishing emails are a critical component of the cybercrime kill chain due to\ntheir wide reach and low cost. Their ever-evolving nature renders traditional\nrule-based and feature-engineered detectors ineffective in the ongoing arms\nrace between attackers and defenders. The rise of large language models (LLMs)\nfurther exacerbates the threat, enabling attackers to craft highly convincing\nphishing emails at minimal cost.\n  This work demonstrates that LLMs can generate psychologically persuasive\nphishing emails tailored to victim profiles, successfully bypassing nearly all\ncommercial and academic detectors. To defend against such threats, we propose\nPiMRef, the first reference-based phishing email detector that leverages\nknowledge-based invariants. Our core insight is that persuasive phishing emails\noften contain disprovable identity claims, which contradict real-world facts.\nPiMRef reframes phishing detection as an identity fact-checking task. Given an\nemail, PiMRef (i) extracts the sender's claimed identity, (ii) verifies the\nlegitimacy of the sender's domain against a predefined knowledge base, and\n(iii) detects call-to-action prompts that push user engagement. Contradictory\nclaims are flagged as phishing indicators and serve as human-understandable\nexplanations.\n  Compared to existing methods such as D-Fence, HelpHed, and ChatSpamDetector,\nPiMRef boosts precision by 8.8% with no loss in recall on standard benchmarks\nlike Nazario and PhishPot. In a real-world evaluation of 10,183 emails across\nfive university accounts over three years, PiMRef achieved 92.1% precision,\n87.9% recall, and a median runtime of 0.05s, outperforming the state-of-the-art\nin both effectiveness and efficiency.", "AI": {"tldr": "This paper introduces PiMRef, a reference-based phishing email detector that leverages knowledge-based invariants to detect contradictions in identity claims. It outperforms existing methods in precision and efficiency on both benchmarks and real-world data.", "motivation": "Traditional phishing detectors struggle with the evolving nature of phishing attacks, especially those enhanced by large language models (LLMs) which generate highly persuasive emails at low cost, making existing methods insufficient for accurate detection.", "method": "PiMRef focuses on identity fact-checking through three steps: (i) extracting the sender's claimed identity from emails, (ii) verifying the domain legitimacy against a knowledge base, and (iii) identifying call-to-action prompts that encourage user engagement to flag contradictory claims as phishing indicators.", "result": "PiMRef improves precision by 8.8% without compromising recall on standard phishing datasets (Nazario and PhishPot). In real-world evaluation (10,183 emails over 3 years), it achieves 92.1% precision, 87.9% recall, and 0.05s median runtime, surpassing state-of-the-art approaches in accuracy and speed.", "conclusion": "PiMRef demonstrates a novel defense strategy against LLM-powered phishing by identifying contradictions in sender identity claims, providing both high effectiveness (precision/recall) and efficiency (speed), while offering explainable results through detectable factual inconsistencies."}}
{"id": "2507.15666", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15666", "abs": "https://arxiv.org/abs/2507.15666", "authors": ["Igor Turkin", "Lina Volobuieva", "Andriy Chukhray", "Oleksandr Liubimov"], "title": "Modeling CubeSat Storage Battery Discharge: Equivalent Circuit Versus Machine Learning Approaches", "comment": "13 pages, 15 figures", "summary": "The subject of the article is the study and comparison of two approaches to\nmodelling the battery discharge of a CubeSat satellite: analytical using\nequivalent circuit and machine learning. The article aims to make a reasoned\nchoice of the approach to modelling the battery discharge of a CubeSat\nsatellite. Modelling the battery discharge of a satellite will enable the\nprediction of the consequences of disconnecting the autonomous power system and\nensure the fault tolerance of equipment in orbit. Therefore, the selected study\nis relevant and promising. This study focuses on the analysis of CubeSat\nsatellite data, based explicitly on orbital data samples of the power system,\nwhich include data available at the time of the article publication. The\ndataset contains data on the voltage, current, and temperature of the battery\nand solar panels attached to the five sides of the satellite. In this context,\ntwo approaches are considered: analytical modelling based on physical laws and\nmachine learning, which uses empirical data to create a predictive model.\nResults: A comparative analysis of the modeling results reveals that the\nequivalent circuit approach has the advantage of transparency, as it identifies\npossible parameters that facilitate understanding of the relationships.\nHowever, the model is less flexible to environmental changes or non-standard\nsatellite behavior. The machine learning model demonstrated more accurate\nresults, as it can account for complex dependencies and adapt to actual\nconditions, even when they deviate from theoretical assumptions.", "AI": {"tldr": "The paper compares analytical equivalent circuit and machine learning models for CubeSat battery discharge, concluding that machine learning offers higher accuracy and adaptability despite lower transparency.", "motivation": "Reliable power system modeling in CubeSats is critical for predicting the impact of autonomous power system disconnections and ensuring in-orbit fault tolerance, making this research relevant for satellite missions.", "method": "The study uses orbital power system data from CubeSats (voltage, current, solar panel temperatures) to analyze two modeling approaches: analytical modeling based on physical laws and machine learning using empirical data.", "result": "Equivalent circuit models provided transparent parameter-based insights but struggled with environmental variability, whereas machine learning models delivered superior accuracy by adapting to actual mission conditions, even when they departed from theoretical expectations.", "conclusion": "Machine learning models are recommended over analytical methods for CubeSat battery discharge prediction due to their superior accuracy and adaptability to real-world conditions, despite being less interpretable."}}
{"id": "2507.15419", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.15419", "abs": "https://arxiv.org/abs/2507.15419", "authors": ["Wenhao Li", "Selvakumar Manickam", "Yung-wey Chong", "Shankar Karuppayah"], "title": "PhishIntentionLLM: Uncovering Phishing Website Intentions through Multi-Agent Retrieval-Augmented Generation", "comment": "Accepted by EAI ICDF2C 2025", "summary": "Phishing websites remain a major cybersecurity threat, yet existing methods\nprimarily focus on detection, while the recognition of underlying malicious\nintentions remains largely unexplored. To address this gap, we propose\nPhishIntentionLLM, a multi-agent retrieval-augmented generation (RAG) framework\nthat uncovers phishing intentions from website screenshots. Leveraging the\nvisual-language capabilities of large language models (LLMs), our framework\nidentifies four key phishing objectives: Credential Theft, Financial Fraud,\nMalware Distribution, and Personal Information Harvesting. We construct and\nrelease the first phishing intention ground truth dataset (~2K samples) and\nevaluate the framework using four commercial LLMs. Experimental results show\nthat PhishIntentionLLM achieves a micro-precision of 0.7895 with GPT-4o and\nsignificantly outperforms the single-agent baseline with a ~95% improvement in\nmicro-precision. Compared to the previous work, it achieves 0.8545 precision\nfor credential theft, marking a ~4% improvement. Additionally, we generate a\nlarger dataset of ~9K samples for large-scale phishing intention profiling\nacross sectors. This work provides a scalable and interpretable solution for\nintention-aware phishing analysis.", "AI": {"tldr": "PhishIntentionLLM is a multi-agent RAG framework for identifying phishing intentions from website screenshots, achieving a micro-precision of 0.7895 with GPT-4o and a 95% improvement over single-agent baselines while releasing datasets of 2K and 9K labeled samples.", "motivation": "Current phishing detection methods ignore the deeper malicious intention identification, which is critical for effective cybersecurity response and understanding threat landscapes.", "method": "A retrieval-augmented generation (RAG) framework using visual-language LLMs to analyze website screenshots and classify phishing intentions into four categories: Credential Theft, Financial Fraud, Malware Distribution, and Personal Information Harvesting, supported by a ~2K ground truth dataset and evaluation across four commercial LLMs.", "result": "Micro-precision of 0.7895 with GPT-4o, ~95% improvement over single-agent baselines, 0.8545 precision for Credential Theft (4% better than prior work), and a ~9K dataset for sector-level profiling.", "conclusion": "PhishIntentionLLM provides scalable, interpretable intention-aware phishing analysis through multi-agent RAG, enabling more precise threat mitigation and large-scale behavioral profiling across sectors."}}
{"id": "2507.15671", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15671", "abs": "https://arxiv.org/abs/2507.15671", "authors": ["Jinyao Guo", "Chengpeng Wang", "Dominic Deluca", "Jinjie Liu", "Zhuo Zhang", "Xiangyu Zhang"], "title": "BugScope: Learn to Find Bugs Like Human", "comment": "19 pages, 2 figure, 6 tables, 4 listings", "summary": "Detecting software bugs remains a fundamental challenge due to the extensive\ndiversity of real-world defects. Traditional static analysis tools often rely\non symbolic workflows, which restrict their coverage and hinder adaptability to\ncustomized bugs with diverse anti-patterns. While recent advances incorporate\nlarge language models (LLMs) to enhance bug detection, these methods continue\nto struggle with sophisticated bugs and typically operate within limited\nanalysis contexts. To address these challenges, we propose BugScope, an\nLLM-driven multi-agent system that emulates how human auditors learn new bug\npatterns from representative examples and apply that knowledge during code\nauditing. Given a set of examples illustrating both buggy and non-buggy\nbehaviors, BugScope synthesizes a retrieval strategy to extract relevant\ndetection contexts via program slicing and then constructs a tailored detection\nprompt to guide accurate reasoning by the LLM. Our evaluation on a curated\ndataset of 40 real-world bugs drawn from 21 widely-used open-source projects\ndemonstrates that BugScope achieves 87.04% precision and 90.00% recall,\nsurpassing state-of-the-art industrial tools by 0.44 in F1 score. Further\ntesting on large-scale open-source systems, including the Linux kernel,\nuncovered 141 previously unknown bugs, of which 78 have been fixed and 7\nconfirmed by developers, highlighting BugScope's substantial practical impact.", "AI": {"tldr": "BugScope is an LLM-driven multi-agent system that improves software bug detection by emulating human auditors' learning from examples. It synthesizes retrieval strategies via program slicing and tailored prompts, achieving high precision and recall on real-world bugs and uncovering previously unknown issues in large-scale projects.", "motivation": "Traditional static analysis tools lack coverage and adaptability for diverse bug patterns, while existing LLM-based methods struggle with sophistication and limited analysis contexts. There is a need for a system that can dynamically learn and apply new human-like auditing strategies.", "method": "The system uses program slicing to extract relevant contexts from example bug cases and constructs customized detection prompts that guide LLM reasoning. By imitating how human auditors learn from examples, it enhances detection accuracy for both known and novel bug patterns.", "result": "Demonstrated 87.04% precision and 90.00% recall on 40 real-world bugs from 21 projects. Discovered 141 unknown bugs in systems like the Linux kernel, with 78 fixed and 7 confirmed by developers, surpassing industrial tools in F1 score by 0.44.", "conclusion": "BugScope's ability to learn from examples and generate tailored analysis workflows enables high-performance, scalable bug detection that outperforms state-of-the-art tools while achieving significant real-world bug discovery impact."}}
{"id": "2507.15449", "categories": ["cs.CR", "cs.SC"], "pdf": "https://arxiv.org/pdf/2507.15449", "abs": "https://arxiv.org/abs/2507.15449", "authors": ["Alessio Caminata", "Elisa Gorla", "Madison Mabe", "Martina Vigorito", "Irene Villa"], "title": "Cryptanalysis of a multivariate CCZ scheme", "comment": "are welcome!", "summary": "We consider the multivariate scheme Pesto, which was introduced by Calderini,\nCaminata, and Villa. In this scheme, the public polynomials are obtained by\napplying a CCZ transformation to a set of quadratic secret polynomials. As a\nconsequence, the public key consists of polynomials of degree 4. In this work,\nwe show that the public degree 4 polynomial system can be efficiently reduced\nto a system of quadratic polynomials. This seems to suggest that the CCZ\ntransformation may not offer a significant increase in security, contrary to\nwhat was initially believed.", "AI": {"tldr": "The paper challenges the security of the Pesto multivariate scheme by efficiently reducing its degree 4 public polynomials to quadratic systems, suggesting the CCZ transformation may not provide significant security benefits as previously believed.", "motivation": "Initial assumptions about the security of Pesto's CCZ-transformed quadratic polynomials encouraged exploration of their vulnerabilities, aiming to reassess the transformation's effectiveness.", "method": "The authors analyze the public polynomial system of Pesto and demonstrate a method to efficiently convert degree 4 polynomials into a quadratic system through structural manipulation.", "result": "They successfully reduced the complexity of Pesto's public key system, revealing potential weaknesses in the CCZ transformation's security contribution.", "conclusion": "While the CCZ transformation was thought to enhance security, this work indicates it may be insufficient, highlighting the need for further investigation into robust multivariate encryption methods."}}
{"id": "2507.15822", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15822", "abs": "https://arxiv.org/abs/2507.15822", "authors": ["Li Huang", "Ilgiz Mustafin", "Marco Piccioni", "Alessandro Schena", "Reto Weber", "Bertrand Meyer"], "title": "Do AI models help produce verified bug fixes?", "comment": null, "summary": "Among areas of software engineering where AI techniques -- particularly,\nLarge Language Models -- seem poised to yield dramatic improvements, an\nattractive candidate is Automatic Program Repair (APR), the production of\nsatisfactory corrections to software bugs. Does this expectation materialize in\npractice? How do we find out, making sure that proposed corrections actually\nwork? If programmers have access to LLMs, how do they actually use them to\ncomplement their own skills?\n  To answer these questions, we took advantage of the availability of a\nprogram-proving environment, which formally determines the correctness of\nproposed fixes, to conduct a study of program debugging with two randomly\nassigned groups of programmers, one with access to LLMs and the other without,\nboth validating their answers through the proof tools. The methodology relied\non a division into general research questions (Goals in the Goal-Query-Metric\napproach), specific elements admitting specific answers (Queries), and\nmeasurements supporting these answers (Metrics). While applied so far to a\nlimited sample size, the results are a first step towards delineating a proper\nrole for AI and LLMs in providing guaranteed-correct fixes to program bugs.\n  These results caused surprise as compared to what one might expect from the\nuse of AI for debugging and APR. The contributions also include: a detailed\nmethodology for experiments in the use of LLMs for debugging, which other\nprojects can reuse; a fine-grain analysis of programmer behavior, made possible\nby the use of full-session recording; a definition of patterns of use of LLMs,\nwith 7 distinct categories; and validated advice for getting the best of LLMs\nfor debugging and Automatic Program Repair.", "AI": {"tldr": "This paper investigates whether Large Language Models (LLMs) improve Automatic Program Repair (APR) in practice by comparing two groups of programmers with/without LLM access during debugging tasks, and identifies seven distinct LLM usage patterns with validated guidance for effective APR integration. The study uses a goal-based methodology with formal proof validation.", "motivation": "The study addresses the need to understand if AI/LLM integration into APR delivers practical benefits, and how programmers leverage LLMs to augment their skills when available.", "method": "A controlled experiment comparing programmers with/without LLM access using a Goal-Query-Metric framework, where both groups proved correctness of their fixes. Full-session recordings enabled fine-grained analysis of behavior and LLM usage patterns.", "result": "Surprising results showed varying LLM effectiveness. The study revealed seven distinct usage patterns, demonstrated the potential of LLMs in APR, and provided validated guidance for their optimal use. However, the limited sample size restricts generalization.", "conclusion": "The work establishes foundational methodology for assessing LLM use in APR, highlights unexpected practical roles for AI in debugging, and defines actionable patterns and advice for integrating LLMs into repair workflows. It represents an early exploration toward guaranteed-correct automated repairs."}}
{"id": "2507.15613", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15613", "abs": "https://arxiv.org/abs/2507.15613", "authors": ["Andrii Balashov", "Olena Ponomarova", "Xiaohua Zhai"], "title": "Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems", "comment": "26 pages", "summary": "Large Language Models (LLMs) deployed in enterprise settings (e.g., as\nMicrosoft 365 Copilot) face novel security challenges. One critical threat is\nprompt inference attacks: adversaries chain together seemingly benign prompts\nto gradually extract confidential data. In this paper, we present a\ncomprehensive study of multi-stage prompt inference attacks in an enterprise\nLLM context. We simulate realistic attack scenarios where an attacker uses\nmild-mannered queries and indirect prompt injections to exploit an LLM\nintegrated with private corporate data. We develop a formal threat model for\nthese multi-turn inference attacks and analyze them using probability theory,\noptimization frameworks, and information-theoretic leakage bounds. The attacks\nare shown to reliably exfiltrate sensitive information from the LLM's context\n(e.g., internal SharePoint documents or emails), even when standard safety\nmeasures are in place.\n  We propose and evaluate defenses to counter such attacks, including\nstatistical anomaly detection, fine-grained access control, prompt sanitization\ntechniques, and architectural modifications to LLM deployment. Each defense is\nsupported by mathematical analysis or experimental simulation. For example, we\nderive bounds on information leakage under differential privacy-based training\nand demonstrate an anomaly detection method that flags multi-turn attacks with\nhigh AUC. We also introduce an approach called \"spotlighting\" that uses input\ntransformations to isolate untrusted prompt content, reducing attack success by\nan order of magnitude. Finally, we provide a formal proof of concept and\nempirical validation for a combined defense-in-depth strategy. Our work\nhighlights that securing LLMs in enterprise settings requires moving beyond\nsingle-turn prompt filtering toward a holistic, multi-stage perspective on both\nattacks and defenses.", "AI": {"tldr": "This paper studies multi-stage prompt inference attacks on enterprise LLMs (e.g., Microsoft 365 Copilot) and proposes defenses including anomaly detection, access control, prompt sanitization, and architectural modifications. It emphasizes moving from single-turn to multi-stage security analysis.", "motivation": "Enterprise LLMs with access to private data face novel security threats like multi-stage prompt inference attacks, which bypass standard safety measures by chaining benign-seeming queries. Current defenses are insufficient against these sophisticated, persistent threats.", "method": "Formal threat modeling using probability theory, optimization frameworks, and information-theoretic leakage bounds. Experimental simulations with real attack scenarios involving multi-turn interactions. Evaluates four defense approaches (anomaly detection, access control, sanitization, spotlighting), including derivation of differential privacy leakage bounds and attack success reduction metrics.", "result": "Demonstrated reliable exfiltration of SharePoint documents and emails even with existing safety measures. Anomaly detection achieved high AUC, spotlighting reduced attack success by 90%, and defense-in-depth strategy showed empirical validation of combined approach effectiveness.", "conclusion": "Securing enterprise LLMs requires multi-stage threat modeling and layered defenses beyond basic prompt filtering. The work establishes theoretical foundations and practical methods for defending against sophisticated prompt inference attacks across multiple interaction steps."}}
{"id": "2507.15828", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15828", "abs": "https://arxiv.org/abs/2507.15828", "authors": ["Mauro Marcelino", "Marcos Alves", "Bianca Trinkenreich", "Bruno Cartaxo", "S\u00e9rgio Soares", "Simone D. J. Barbosa", "Marcos Kalinowski"], "title": "Investigating the Use of LLMs for Evidence Briefings Generation in Software Engineering", "comment": "ESEM 2025 Registered Report with an IPA (In Principle Acceptance) for\n  the Empirical Software Engineering journal", "summary": "[Context] An evidence briefing is a concise and objective transfer medium\nthat can present the main findings of a study to software engineers in the\nindustry. Although practitioners and researchers have deemed Evidence Briefings\nuseful, their production requires manual labor, which may be a significant\nchallenge to their broad adoption. [Goal] The goal of this registered report is\nto describe an experimental protocol for evaluating LLM-generated evidence\nbriefings for secondary studies in terms of content fidelity, ease of\nunderstanding, and usefulness, as perceived by researchers and practitioners,\ncompared to human-made briefings. [Method] We developed an RAG-based LLM tool\nto generate evidence briefings. We used the tool to automatically generate two\nevidence briefings that had been manually generated in previous research\nefforts. We designed a controlled experiment to evaluate how the LLM-generated\nbriefings compare to the human-made ones regarding perceived content fidelity,\nease of understanding, and usefulness. [Results] To be reported after the\nexperimental trials. [Conclusion] Depending on the experiment results.", "AI": {"tldr": "This paper designs an experiment to compare the effectiveness of LLM-generated evidence briefings against human-created ones in terms of content fidelity, ease of understanding, and usefulness for secondary studies.", "motivation": "Evidence briefings are labor-intensive to produce manually, limiting their adoption despite their value to industry software engineers.", "method": "A RAG-based LLM tool was built to auto-generate previous human-created briefings, followed by a controlled experiment to evaluate LLM vs. human briefings using perception metrics.", "result": "Results depend on post-experiment analysis (not yet available).", "conclusion": "Conclusion will depend on the experimental results comparing LLM and human-generated briefings."}}
{"id": "2507.15660", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.15660", "abs": "https://arxiv.org/abs/2507.15660", "authors": ["Rohit Negi", "Amit Negi", "Manish Sharma", "S. Venkatesan", "Prem Kumar", "Sandeep K. Shukla"], "title": "Cyber security of Mega Events: A Case Study of Securing the Digital Infrastructure for MahaKumbh 2025 -- A 45 days Mega Event of 600 Million Footfalls", "comment": "11 pages, 11 tables", "summary": "Mega events such as the Olympics, World Cup tournaments, G-20 Summit,\nreligious events such as MahaKumbh are increasingly digitalized. From event\nticketing, vendor booth or lodging reservations, sanitation, event scheduling,\ncustomer service, crime reporting, media streaming and messaging on digital\ndisplay boards, surveillance, crowd control, traffic control and many other\nservices are based on mobile and web applications, wired and wireless\nnetworking, network of Closed-Circuit Television (CCTV) cameras, specialized\ncontrol room with network and video-feed monitoring. Consequently, cyber\nthreats directed at such digital infrastructure are common. Starting from hobby\nhackers, hacktivists, cyber crime gangs, to the nation state actors, all target\nsuch infrastructure to unleash chaos on an otherwise smooth operation, and\noften the cyber threat actors attempt to embarrass the organizing country or\nthe organizers. Unlike long-standing organizations such as a corporate or a\ngovernment department, the infrastructure of mega-events is temporary,\nconstructed over a short time span in expediency, and often shortcuts are taken\nto make the deadline for the event. As a result, securing such an elaborate yet\ntemporary infrastructure requires a different approach than securing a standard\norganizational digital infrastructure. In this paper, we describe our approach\nto securing MahaKumbh 2025, a 600 million footfall event for 45 days in\nPrayagraj, India, as a cyber security assessment and risk management oversight\nteam. We chronicle the scope, process, methodology, and outcome of our team's\neffort to secure this mega event. It should be noted that none of the cyber\nattacks during the 45-day event was successful. Our goal is to put on record\nthe methodology and discuss what we would do differently in case we work on\nsimilar future mega event.", "AI": {"tldr": "This paper presents a cybersecurity assessment and risk management approach for securing MahaKumbh 2025, a massive 45-day event with 600 million footfall in Prayagraj, India, using temporary infrastructure and emphasizing proactive defense strategies.", "motivation": "Digitalized mega-event infrastructures are vulnerable to cyber threats due to their temporary nature and expedited construction, requiring distinct security approaches compared to permanent organizations to prevent chaos and reputational damage.", "method": "The authors implemented a cybersecurity strategy focused on assessments, risk management, monitoring of mobile/web applications, networking, CCTV systems, control rooms, and multi-layered defensive mechanisms tailored to the event's temporary and high-traffic characteristics.", "result": "All 45 days of MahaKumbh 2025 were secured successfully, with no cyberattacks exploiting vulnerabilities or disrupting operations, demonstrating the efficacy of the team's approach.", "conclusion": "The methodology used for MahaKumbh 2025 provides a framework for securing large, temporary digital infrastructures, with lessons learned highlighting potential improvements for future mega events."}}
{"id": "2507.15831", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15831", "abs": "https://arxiv.org/abs/2507.15831", "authors": ["Sergey Titov", "Konstantin Grotov", "Cristina Sarasua", "Yaroslav Golubev", "Dhivyabharathi Ramasamy", "Alberto Bacchelli", "Abraham Bernstein", "Timofey Bryksin"], "title": "Observing Fine-Grained Changes in Jupyter Notebooks During Development Time", "comment": "32 pages, 6 figures", "summary": "In software engineering, numerous studies have focused on the analysis of\nfine-grained logs, leading to significant innovations in areas such as\nrefactoring, security, and code completion. However, no similar studies have\nbeen conducted for computational notebooks in the context of data science.\n  To help bridge this research gap, we make three scientific contributions: we\n(1) introduce a toolset for collecting code changes in Jupyter notebooks during\ndevelopment time; (2) use it to collect more than 100 hours of work related to\na data analysis task and a machine learning task (carried out by 20 developers\nwith different levels of expertise), resulting in a dataset containing 2,655\ncells and 9,207 cell executions; and (3) use this dataset to investigate the\ndynamic nature of the notebook development process and the changes that take\nplace in the notebooks.\n  In our analysis of the collected data, we classified the changes made to the\ncells between executions and found that a significant number of these changes\nwere relatively small fixes and code iteration modifications. This suggests\nthat notebooks are used not only as a development and exploration tool but also\nas a debugging tool. We report a number of other insights and propose potential\nfuture research directions on the novel data.", "AI": {"tldr": "This study introduces a toolset to analyze changes in Jupyter notebooks during data science development, revealing patterns of small iterative fixes and debugging behaviors. A dataset of 9,207 cell executions from 20 developers is presented, along with insights and future research directions.", "motivation": "Fine-grained log analysis has enabled advancements in software engineering domains but remains unexplored in computational notebooks used for data science projects, despite their popularity for development and exploration.", "method": "The researchers (1) developed a toolset to collect Jupyter notebook code changes during development, (2) captured over 100 hours of real-world data science work from 20 diverse developers producing 2,655 cells and 9,207 executions, and (3) analyzed these data to characterize notebook development dynamics.", "result": "Analysis found that 28% of cell executions were followed by code modifications within cells, with 64% involving minor fixes like syntax errors, variable name updates, and small logic changes (median 3 lines changed). This shows notebooks are frequently used for iterative refinement and debugging beyond their core development/analysis role.", "conclusion": "Notebooks enable rapid code iteration and debugging, which has implications for tool design in data science environments. The paper provides the first empirical characterization of Jupyter notebook development practices and suggests future studies to explore how these patterns affect collaboration, reproducibility, and workflow optimization."}}
