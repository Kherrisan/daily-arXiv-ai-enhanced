<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 10]
- [cs.SE](#cs.SE) [Total: 15]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Bridging Cloud Convenience and Protocol Transparency: A Hybrid Architecture for Ethereum Node Operations on Amazon Managed Blockchain](https://arxiv.org/abs/2507.18774)
*S M Mostaq Hossain,Amani Altarawneh,Maanak Gupta*

Main category: cs.CR

TL;DR: This paper introduces a hybrid Ethereum node architecture using Amazon Managed Blockchain (AMB) with EC2 observability, IAM security, and AWS CDK automation for secure, scalable, and performance-transparent deployments.


<details>
  <summary>Details</summary>
Motivation: Blockchain adoption in enterprises/research demands secure, scalable node infrastructure. Self-hosted Ethereum nodes lack elasticity and require complex maintenance, creating a gap for managed yet transparent solutions.

Method: The system integrates AMB with EC2 scripts (via Web3.py/JSON-RPC) for 1,000+ real-time metrics collection. AWS IAM enforces security policies, while AWS Cloud Development Kit enables reproducible automation. CloudWatch visualizes performance data.

Result: Achieved end-to-end observability in managed environments while maintaining operational simplicity. Enabled anomaly detection, service-level performance tracking of metrics like gas utilization, transaction latency, and mempool dynamics through AWS-native monitoring.

Conclusion: This cloud-native hybrid architecture bridges managed service simplicity and research-grade transparency, offering a reproducible Ethereum node solution on AMB suitable for both production and protocol research.

Abstract: As blockchain technologies are increasingly adopted in enterprise and
research domains, the need for secure, scalable, and performance-transparent
node infrastructure has become critical. While self-hosted Ethereum nodes offer
operational control, they often lack elasticity and require complex
maintenance. This paper presents a hybrid, service-oriented architecture for
deploying and monitoring Ethereum full nodes using Amazon Managed Blockchain
(AMB), integrated with EC2-based observability, IAM-enforced security policies,
and reproducible automation via the AWS Cloud Development Kit. Our architecture
supports end-to-end observability through custom EC2 scripts leveraging Web3.py
and JSON-RPC, collecting over 1,000 real-time data points-including gas
utilization, transaction inclusion latency, and mempool dynamics. These metrics
are visualized and monitored through AWS CloudWatch, enabling service-level
performance tracking and anomaly detection. This cloud-native framework
restores low-level observability lost in managed environments while maintaining
the operational simplicity of managed services. By bridging the simplicity of
AMB with the transparency required for protocol research and enterprise
monitoring, this work delivers one of the first reproducible,
performance-instrumented Ethereum deployments on AMB. The proposed hybrid
architecture enables secure, observable, and reproducible Ethereum node
operations in cloud environments, suitable for both research and production
use.

</details>


### [2] [Resolving Indirect Calls in Binary Code via Cross-Reference Augmented Graph Neural Networks](https://arxiv.org/abs/2507.18801)
*Haotian Zhang,Kun Liu,Cristian Garces,Chenke Luo,Yu Lei,Jiang Ming*

Main category: cs.CR

TL;DR: NeuCall is a novel ML method using graph neural networks to resolve indirect calls in binary code by enhancing control flow graphs with cross-references and compiler-level type analysis, achieving 95.2% F1 score. 


<details>
  <summary>Details</summary>
Motivation: Existing binary analysis tools struggle with low accuracy and scalability in resolving indirect call targets due to incomplete control flow graphs and poor callsite-callee training pairs. Static analysis requires precise understanding of program flow for security applications where source code is unavailable. 

Method: 1. Augments control flow graphs with data/code cross-references using relational graph convolutions
2. Employs advanced compiler-level type analysis to generate high-quality training pairs
3. Designs a specialized graph neural network model for indirect call target prediction

Result: 95.2% F1 score on real-world x86_64 binaries from GitHub and Arch User Repository outperforming state-of-the-art ML approaches by significant margins.

Conclusion: NeuCall demonstrates that augmenting binary code representation with structural relationships and using compiler analysis for training data can achieve breakthrough accuracy in indirect call resolution, enabling more precise static binary analysis for security applications.

Abstract: Binary code analysis is essential in scenarios where source code is
unavailable, with extensive applications across various security domains.
However, accurately resolving indirect call targets remains a longstanding
challenge in maintaining the integrity of static analysis in binary code. This
difficulty arises because the operand of a call instruction (e.g., call rax)
remains unknown until runtime, resulting in an incomplete inter-procedural
control flow graph (CFG). Previous approaches have struggled with low accuracy
and limited scalability. To address these limitations, recent work has
increasingly turned to machine learning (ML) to enhance analysis. However, this
ML-driven approach faces two significant obstacles: low-quality callsite-callee
training pairs and inadequate binary code representation, both of which
undermine the accuracy of ML models. In this paper, we introduce NeuCall, a
novel approach for resolving indirect calls using graph neural networks.
Existing ML models in this area often overlook key elements such as data and
code cross-references, which are essential for understanding a program's
control flow. In contrast, NeuCall augments CFGs with cross-references,
preserving rich semantic information. Additionally, we leverage advanced
compiler-level type analysis to generate high-quality callsite-callee training
pairs, enhancing model precision and reliability. We further design a graph
neural model that leverages augmented CFGs and relational graph convolutions
for accurate target prediction. Evaluated against real-world binaries from
GitHub and the Arch User Repository on x86_64 architecture, NeuCall achieves an
F1 score of 95.2%, outperforming state-of-the-art ML-based approaches. These
results highlight NeuCall's effectiveness in building precise inter-procedural
CFGs and its potential to advance downstream binary analysis and security
applications.

</details>


### [3] [How to Copy-Protect Malleable-Puncturable Cryptographic Functionalities Under Arbitrary Challenge Distributions](https://arxiv.org/abs/2507.19032)
*Alper Ã‡akan,Vipul Goyal*

Main category: cs.CR

TL;DR: The paper extends quantum copy-protection techniques to a broader class of cryptographic schemes called malleable-puncturable schemes, which can secure functionalities under high min-entropy challenge distributions.


<details>
  <summary>Details</summary>
Motivation: Quantum copy-protection aims to prevent efficient adversaries from duplicating encoded functionalities; prior work limited to cryptographically puncturable schemes with pseudorandom puncturing points, leaving many primitives unprotected.

Method: Introduces malleable-puncturable schemes, where circuits can answer inputs unrelated to the security challenge without aiding related inputs. This generalizes puncturable schemes and adapts to adversarial challenge distributions with strong information-theoretic assumptions.

Result: Constructs a provably secure copy-protection scheme for malleable-puncturable primitives, achieving security against attacks with arbitrary high min-entropy challenge distributions, surpassing pseudorandom-based approaches.

Conclusion: This work expands the scope of quantum copy-protection to more cryptographic primitives and strengthens security guarantees by addressing broader challenge distributions, offering a flexible framework for resisting advanced copying attacks.

Abstract: A quantum copy-protection scheme (Aaronson, CCC 2009) encodes a functionality
into a quantum state such that given this state, no efficient adversary can
create two (possibly entangled) quantum states that are both capable of running
the functionality. There has been a recent line of works on constructing
provably-secure copy-protection schemes for general classes of schemes in the
plain model, and most recently the recent work of \c{C}akan and Goyal (IACR
Eprint, 2025) showed how to copy-protect all cryptographically puncturable
schemes with pseudorandom puncturing points. In this work, we show how to
copy-protect even a larger class of schemes. We define a class of cryptographic
schemes called malleable-puncturable schemes where the only requirement is that
one can create a circuit that is capable of answering inputs at points that are
unrelated to the challenge in the security game but does not help the adversary
answer inputs related to the challenge. This is a flexible generalization of
puncturable schemes, and can capture a wide range of primitives that was not
known how to copy-protect prior to our work. Going further, we show that our
scheme is secure against arbitrary high min-entropy challenge distributions
whereas previous work has only considered schemes that are punctured at
pseudorandom points.

</details>


### [4] [Virtual local area network over HTTP for launching an insider attack](https://arxiv.org/abs/2507.19055)
*Yuksel Arslan*

Main category: cs.CR

TL;DR: The paper reveals a vulnerability in LAN security where insider threats exploit unused secondary IP addresses and HTTP protocols to bypass existing external protections like firewalls and IDS.


<details>
  <summary>Details</summary>
Motivation: Despite robust defenses against external threats, limited solutions exist for internal risks, necessitating research to expose LAN exposure through insider attacks.

Method: Demonstrates proof-of-concept attacks leveraging unmonitored secondary IP addresses and HTTP-based exfiltration to compromise LAN confidentiality without triggering existing security tools.

Result: Showed that external machines can infiltrate isolated LANs by exploiting secondary networks and HTTP protocols, bypassing conventional security architectures.

Conclusion: Current security frameworks inadequately address internal attack vectors, and secondary IP address exploitation represents a critical vulnerability requiring enhanced monitoring and segmentation protocols.

Abstract: Computers and computer networks have become integral to virtually every
aspect of modern life, with the Internet playing an indispensable role.
Organizations, businesses, and individuals now store vast amounts of
proprietary, confidential, and personal data digitally. As such, ensuring the
security of this data from unauthorized access is critical. Common security
measures, such as firewalls, intrusion detection systems (IDS), intrusion
prevention systems (IPS), and antivirus software, are constantly evolving to
safeguard computer systems and networks. However, these tools primarily focus
on defending against external threats, leaving systems vulnerable to insider
attacks. Security solutions designed to mitigate risks originating from within
the organization are relatively limited and often ineffective. This paper
demonstrates how a Local Area Network (LAN) can be covertly exposed to the
Internet via an insider attack. Specifically, it illustrates how an external
machine can gain access to a LAN by exploiting an unused secondary IP address
of the attacked LAN, effectively bypassing existing security mechanisms by also
exploiting Hyper Text Transfer Protocol (HTTP). Despite the presence of robust
external protections, such as firewalls and IDS, this form of insider attack
reveals significant vulnerabilities in the way internal threats are addressed.

</details>


### [5] [PurpCode: Reasoning for Safer Code Generation](https://arxiv.org/abs/2507.19060)
*Jiawei Liu,Nirav Diwan,Zhe Wang,Haoyu Zhai,Xiaona Zhou,Kiet A. Nguyen,Tianjiao Yu,Muntasir Wahed,Yinlin Deng,Hadjer Benkraouda,Yuxiang Wei,Lingming Zhang,Ismini Lourentzou,Gang Wang*

Main category: cs.CR

TL;DR: PurpCode is a post-training framework that combines rule-based learning and reinforcement learning to create secure code models, significantly reducing unsafe generation while maintaining utility.


<details>
  <summary>Details</summary>
Motivation: Current code models lack sufficient safety measures against cyber threats, require explicit training to follow cybersafety rules to prevent facilitating malicious activities and code vulnerabilities.

Method: Two-stage post-training: 1) Rule Learning explicitly teaches cyber safety rules to avoid vulnerabilities, 2) Reinforcement Learning optimizes safety and utility using multi-objective rewards. Uses internal red-teaming to create comprehensive training prompts.

Result: PurpCode-32B model achieves SOTA cybersafety metrics, outperforms frontier models in security while reducing overrefusal rates by ~40% (general use) and ~30% (cyber-specific) compared to baseline.

Conclusion: PurpCode demonstrates a novel approach to aligning code models with security requirements, enabling both high safety standards and practical usability through structured post-training methodologies.

Abstract: We introduce PurpCode, the first post-training recipe for training safe code
reasoning models towards generating secure code and defending against malicious
cyberactivities. PurpCode trains a reasoning model in two stages: (i) Rule
Learning, which explicitly teaches the model to reference cybersafety rules to
generate vulnerability-free code and to avoid facilitating malicious
cyberactivities; and (ii) Reinforcement Learning, which optimizes model safety
and preserves model utility through diverse, multi-objective reward mechanisms.
To empower the training pipelines with comprehensive cybersafety data, we
conduct internal red-teaming to synthesize comprehensive and high-coverage
prompts based on real-world tasks for inducing unsafe cyberactivities in the
model. Based on PurpCode, we develop a reasoning-based coding model, namely
PurpCode-32B, which demonstrates state-of-the-art cybersafety, outperforming
various frontier models. Meanwhile, our alignment method decreases the model
overrefusal rates in both general and cybersafety-specific scenarios, while
preserving model utility in both code generation and common security knowledge.

</details>


### [6] [PrompTrend: Continuous Community-Driven Vulnerability Discovery and Assessment for Large Language Models](https://arxiv.org/abs/2507.19185)
*Tarek Gasmi,Ramzi Guesmi,Mootez Aloui,Jihene Bennaceur*

Main category: cs.CR

TL;DR: PrompTrend identifies community-driven psychological attacks as the dominant threat vector for LLMs, surpassing static benchmarks through scalable vulnerability monitoring.


<details>
  <summary>Details</summary>
Motivation: Static benchmarks fail to capture LLM vulnerabilities discovered via community experimentation in online forums, necessitating dynamic, comprehensive monitoring solutions.

Method: We developed PrompTrend with an architecture for scalable data collection across platforms, using cross-sectional analysis of 198 community-sourced vulnerabilities tested on nine commercial models with multidimensional scoring techniques.

Result: 1) Advanced capabilities correlate with higher vulnerability in specific architectures. 2) Psychological attacks show 2-3x greater effectiveness than technical exploits. 3) Platform dynamics create model-specific attack patterns. Framework achieves 78% accuracy but has limited cross-model transferability.

Conclusion: LLM security requires ongoing socio-technical monitoring beyond traditional assessments, as capability advancement does not guarantee improved security and community-derived psychological attacks represent the primary threat vector.

Abstract: Static benchmarks fail to capture LLM vulnerabilities emerging through
community experimentation in online forums. We present PrompTrend, a system
that collects vulnerability data across platforms and evaluates them using
multidimensional scoring, with an architecture designed for scalable
monitoring. Cross-sectional analysis of 198 vulnerabilities collected from
online communities over a five-month period (January-May 2025) and tested on
nine commercial models reveals that advanced capabilities correlate with
increased vulnerability in some architectures, psychological attacks
significantly outperform technical exploits, and platform dynamics shape attack
effectiveness with measurable model-specific patterns. The PrompTrend
Vulnerability Assessment Framework achieves 78% classification accuracy while
revealing limited cross-model transferability, demonstrating that effective LLM
security requires comprehensive socio-technical monitoring beyond traditional
periodic assessment. Our findings challenge the assumption that capability
advancement improves security and establish community-driven psychological
manipulation as the dominant threat vector for current language models.

</details>


### [7] [On the Security of a Code-Based PIR Scheme](https://arxiv.org/abs/2507.19295)
*Svenja Lage,Hannes Bartz*

Main category: cs.CR

TL;DR: CB-cPIR is a code-based PIR scheme vulnerable to security flaws and higher communication costs, but code-based PIR research remains relevant as a post-quantum alternative to lattice-based methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore post-quantum secure PIR schemes by leveraging hard coding-theoretic problems to diversify security foundations beyond lattice problems.

Method: The authors conducted a security analysis of CB-cPIR, identified critical vulnerabilities, and performed a comparative evaluation against state-of-the-art PIR schemes to assess communication costs.

Result: CB-cPIR's security is compromised due to an identified vulnerability, and its communication cost advantages are less significant compared to existing PIR schemes.

Conclusion: Despite the vulnerabilities in CB-cPIR, code-based PIR schemes represent a promising research direction as potential post-quantum secure alternatives to lattice-based approaches.

Abstract: Private Information Retrieval (PIR) schemes allow clients to retrieve files
from a database without disclosing the requested file's identity to the server.
In the pursuit of post-quantum security, most recent PIR schemes rely on hard
lattice problems. In contrast, the so called CB-cPIR scheme stands out as a
pioneering effort to base PIR schemes on hard problems in coding theory,
thereby contributing significantly to the diversification of security
foundations. However, our research reveals a critical vulnerability in CB-cPIR,
substantially diminishing its security levels. Moreover, a comparative analysis
with state-of-the-art PIR schemes shows that CB-cPIR's advantages are reduced,
making it less competitive in terms of the communication cost. Nevertheless,
our findings highlight the importance of continued research into code-based PIR
schemes, as they have the potential to provide a valuable alternative to
lattice-based approaches.

</details>


### [8] [Empowering IoT Firmware Secure Update with Customization Rights](https://arxiv.org/abs/2507.19367)
*Weihao Chen,Yansong Gao,Boyu Kuang,Jin B. Hong,Yuqing Zhang,Anmin Fu*

Main category: cs.CR

TL;DR: This paper identifies vulnerabilities in module-level customization of IoT firmware updates and proposes IMUP, a framework using chameleon hashing, server-side offloading, and caching to secure updates while reducing server time and device downtime by 2.9x and 5.9x, respectively.


<details>
  <summary>Details</summary>
Motivation: Current IoT firmware defenses secure monolithic images but overlook risks introduced by module-level customization, a growing user demand. The paper shows 53% of update-related CVEs (2020-2024) stem from customization-induced vulnerabilities, highlighting a significant unaddressed security gap.

Method: The authors: 1) Conducted a pilot study on 200 Linux-based IoT devices across 23 vendors 2) Analyzed 2020-2024 CVEs 3) Designed IMUP with three components: per-module chameleon hashing for integrity, server-side proof-of-work offloading, and server-side caching of module combinations.

Result: IMUP demonstrates: - 300Ã— cost increase for attackers even after 95% key exposure - 2.9Ã— reduction in server-side firmware generation time compared to package manager baselines - 5.9Ã— reduction in device downtime during updates

Conclusion: Modular firmware customization significantly expands IoT update security risks. IMUP provides the first integrity-centric framework addressing cross-module trust and scalability, reducing both attack feasibility and update latencies in mass customizations.

Abstract: Firmware updates remain the primary line of defense for IoT devices; however,
the update channel itself has become a well-established attack vector. Existing
defenses mainly focus on securing monolithic firmware images, leaving
module-level customization -a growing user demand-largely unprotected and
insufficiently explored. To address this gap, we conduct a pilot study on the
update workflows of 200 Linux-based IoT devices across 23 vendors, uncovering
five previously undocumented vulnerabilities caused by customization practices.
A broader analysis of update-related CVEs from 2020 to 2024 reveals that over
half originate from customization-induced issues. These findings highlight a
critical yet underexamined reality: as customization increases, so does the
attack surface, while current defenses fail to keep pace. We propose IMUP
(Integrity-Centric Modular Update Platform), the first framework to address two
key challenges: constructing a trustworthy cross-module integrity chain and
scaling update performance under mass customization. IMUP combines three
techniques: per-module chameleon hashing for integrity, server-side
proof-of-work offloading to reduce device overhead, and server-side caching to
reuse module combinations, minimizing rebuild costs. Security analysis shows
that even when 95 percent of secret keys are exposed, forging a valid image
incurs over 300 times the cost of the legitimate server. Experiments on
heterogeneous IoT devices demonstrate that IMUP reduces server-side generation
time by 2.9 times and device downtime by 5.9 times compared to a
package-manager baseline.

</details>


### [9] [Transcript Franking for Encrypted Messaging](https://arxiv.org/abs/2507.19391)
*Armin Namavari,Thomas Ristenpart*

Main category: cs.CR

TL;DR: Transcript franking addresses the gap between theoretical and practical message reporting in E2EE messaging by enabling secure cryptographic verification of message subsets and causality. This new protocol type allows platforms to accept multi-message reports from users while maintaining confidentiality and accountability.


<details>
  <summary>Details</summary>
Motivation: Existing message franking mechanisms only support single-message reporting, but real-world content moderation requires analyzing multiple messages in context to address moderation requests like 'prolonged harassment' or 'contextual threats'. This creates a security mismatch between protocol guarantees and operational needs.

Method: 1) Formalize syntax, semantics, and security requirements for transcript franking in both two-party and group messaging
2) Construct protocols with cryptographic evidence bundles supporting causal chain verification
3) Provide provable security guarantees under chosen models
4) Analyze implementation pathways for real-world messaging platforms and their compatibility with existing systems

Result: â€¢ Fully specified transcript franking framework
â€¢ Security proofs for multi-message evidence bundles
â€¢ Concrete constructions with efficiency analysis
â€¢ Practical deployment strategies for messaging platforms
â€¢ Validation of cryptographic properties through formal treatment

Conclusion: Transcript franking provides a principled approach to multi-message reporting in E2EE platforms, reconciling theoretical security guarantees with practical moderation needs. The framework's efficiency and compatibility with real-world systems make it a valuable tool for platforms seeking to balance user privacy with content accountability, potentially enabling more nuanced moderation of complex abuse scenarios.

Abstract: Message franking is an indispensable abuse mitigation tool for end-to-end
encrypted (E2EE) messaging platforms. With it, users who receive harmful
content can securely report that content to platform moderators. However, while
real-world deployments of reporting require the disclosure of multiple
messages, existing treatments of message franking only consider the report of a
single message. As a result, there is a gap between the security goals achieved
by constructions and those needed in practice. Our work introduces transcript
franking, a new type of protocol that allows reporting subsets of conversations
such that moderators can cryptographically verify message causality and
contents. We define syntax, semantics, and security for transcript franking in
two-party and group messaging. We then present efficient constructions for
transcript franking and prove their security. Looking toward deployment
considerations, we provide detailed discussion of how real-world messaging
systems can incorporate our protocols.

</details>


### [10] [Running in CIRCLE? A Simple Benchmark for LLM Code Interpreter Security](https://arxiv.org/abs/2507.19399)
*Gabriel Chua*

Main category: cs.CR

TL;DR: CIRCLE is a benchmark for assessing interpreter-specific cybersecurity risks in LLMs, revealing significant and inconsistent vulnerabilities across models, especially with indirect prompts. The dataset and code are publicly released for further research.


<details>
  <summary>Details</summary>
Motivation: Integrating code interpreters in large language models (LLMs) introduces new system-level cybersecurity threats distinct from prompt-based vulnerabilities, necessitating systematic evaluation of these interpreter-specific risks to ensure secure real-time execution capabilities.

Method: Developed CIRCLE, a benchmark comprising 1,260 prompts (direct and indirect) targeting CPU, memory, and disk resource exhaustion, using an automated framework to assess model responses, code correctness, safety simplifications, or execution timeouts.

Result: Evaluations on 7 commercial models showed significant inconsistencies in vulnerabilities, with OpenAI's o4-mini refusing risky requests at 7.1% compared to GPT-4.1's 0.5%. Indirect, socially-engineered prompts notably weakened model defenses.

Conclusion: The findings highlight an urgent need for interpreter-specific cybersecurity benchmarks, mitigation tools (e.g., guardrails), and industry standards to ensure safe LLM interpreter integrations, supported by the public release of the benchmark dataset and evaluation code.

Abstract: As large language models (LLMs) increasingly integrate native code
interpreters, they enable powerful real-time execution capabilities,
substantially expanding their utility. However, such integrations introduce
potential system-level cybersecurity threats, fundamentally different from
prompt-based vulnerabilities. To systematically evaluate these
interpreter-specific risks, we propose CIRCLE (Code-Interpreter Resilience
Check for LLM Exploits), a simple benchmark comprising 1,260 prompts targeting
CPU, memory, and disk resource exhaustion. Each risk category includes
explicitly malicious ("direct") and plausibly benign ("indirect") prompt
variants. Our automated evaluation framework assesses not only whether LLMs
refuse or generates risky code, but also executes the generated code within the
interpreter environment to evaluate code correctness, simplifications made by
the LLM to make the code safe, or execution timeouts. Evaluating 7 commercially
available models from OpenAI and Google, we uncover significant and
inconsistent vulnerabilities. For instance, evaluations show substantial
disparities even within providers - OpenAI's o4-mini correctly refuses risky
requests at 7.1%, notably higher rates compared to GPT-4.1 at 0.5%. Results
particularly underscore that indirect, socially-engineered prompts
substantially weaken model defenses. This highlights an urgent need for
interpreter-specific cybersecurity benchmarks, dedicated mitigation tools
(e.g., guardrails), and clear industry standards to guide safe and responsible
deployment of LLM interpreter integrations. The benchmark dataset and
evaluation code are publicly released to foster further research.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [11] [Exploring the Landscape of Fairness Interventions in Software Engineering](https://arxiv.org/abs/2507.18726)
*Sadia Afrin Mim*

Main category: cs.SE

TL;DR: This paper is a survey analyzing AI fairness interventions to mitigate risks in healthcare and finance caused by biased datasets.


<details>
  <summary>Details</summary>
Motivation: AI reduces labor and costs in critical domains like healthcare and finance but faces risks from biased data in real-world applications.

Method: The authors conduct a survey, summarizing existing studies and approaches addressing AI fairness issues.

Result: The paper catalogues various fairness interventions developed by practitioners and evaluates their effectiveness.

Conclusion: The survey highlights the necessity of addressing data biases through fairness interventions and identifies ongoing challenges in ensuring equitable AI deployment.

Abstract: Current developments in AI made it broadly significant for reducing human
labor and expenses across several essential domains, including healthcare and
finance. However, the application of AI in the actual world poses multiple
risks and disadvantages due to potential risk factors in data (e.g., biased
dataset). Practitioners developed a number of fairness interventions for
addressing these kinds of problems. The paper acts as a survey, summarizing the
various studies and approaches that have been developed to address fairness
issues

</details>


### [12] [Agentic Program Repair from Test Failures at Scale: A Neuro-symbolic approach with static analysis and test execution feedback](https://arxiv.org/abs/2507.18755)
*Chandra Maddila,Adam Tait,Claire Chang,Daniel Cheng,Nauman Ahmad,Vijayaraghavan Murali,Marshall Roch,Arnaud Avondet,Aaron Meltzer,Victor Montalvao,Michael Hopko,Chris Waterson,Parth Thakkar,Renuka Fernandez,Kristian Kristensen,Sivan Barzily,Sherry Chen,Rui Abreu,Nachiappan Nagappan,Payam Shodjai,Killian Murphy,James Everingham,Aparna Ramani,Peter C. Rigby*

Main category: cs.SE

TL;DR: This paper introduces an Engineering Agent that leverages LLMs and a ReAct harness to automate program repair at scale, achieving a 42.3% solve rate in benchmarks and 25.5% of fixes adopted in production after human review.


<details>
  <summary>Details</summary>
Motivation: LLMs enable agentic program repair for large organizations with complex codebases, but existing solutions lack scalability and integration of symbolic information for accuracy. A balance between cost, latency, and effectiveness is critical.

Method: The authors build an Engineering Agent using Llama and ReAct, starting with test failures identified by a rule-based bot. The agent iterates through 15 actions (file reading, patch generation) using feedback from static analysis and test execution. An LLM-as-a-Judge validates patches before human review.

Result: Offline benchmarks show a specialized 70B model outperforms Llama-405B in cost-effectiveness. 80% of generated patches received reviews, with 31.5% approved (25.5% of all generated fixes). The agent achieves 42.3% solve rate using 11.8 feedback iterations on average. Symbolic feedback improves performance in ablation studies.

Conclusion: The Engineering Agent demonstrates viable large-scale program repair with LLMs, combining neural reasoning and symbolic analysis. While not perfect (42% solve rate), it provides effective starting points for engineers and shows significant potential when optimized for cost, accuracy, and latency balance.

Abstract: Aim: With the advent of LLMs, sophisticated agentic program repair has become
viable at large organizations with large codebases. In this work, we develop an
Engineering Agent that fixes the source code based on test failures at scale
across diverse software offerings internally.
  Method: Using Llama as the base, we employ the ReAct harness to develop an
agent. We start with a test failure that was triaged by a rule-based test
failure bot. We then set up an agentic harness and allow the agent to reason
and run a set of 15 actions from reading a file to generating a patch. We
provide feedback to the agent through static analysis and test failures so it
can refine its solution. We leverage an LLM-as-a-Judge to ensure that the patch
conforms to the standards followed by a human review to land fixes.
  Benchmark Findings: We curated offline benchmarks for our patch generator,
the Engineering Agent loop, and the LLM-as-a-Judge. In offline evaluations we
found that a specialized 70B model is highly competitive with the much larger
but vanilla Llama-405B. In an ablation study, we found that the ReAct harness
(neural model) benefited from the symbolic information from static analysis
tools and test execution traces. A model that strikes a balance between the
solve rate and error rate vs the cost and latency has a benchmark solve rate of
42.3% using an average 11.8 feedback iterations.
  Production Findings: In a three month period, 80% of the generated fixes were
reviewed, of which 31.5% were landed (25.5% of the total number of generated
fixes).
  Feedback from Engineers: We used open coding to extract qualitative themes
from engineers' feedback. We saw positive feedback in the form of quick
approvals, gratitude, and surprise. We also found mixed feedback when the
Engineering Agent's solution was partially correct and it served as a good
starting point.

</details>


### [13] [MemoCoder: Automated Function Synthesis using LLM-Supported Agents](https://arxiv.org/abs/2507.18812)
*Yiping Jia,Zhen Ming Jiang,Shayan Noei,Ying Zou*

Main category: cs.SE

TL;DR: The paper introduces MemoCoder, a multi-agent framework enhancing LLM code generation through collaborative problem solving and persistent learning from past fixes, showing improved performance over existing methods in benchmark tests.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with iterative debugging/error handling tasks, and existing approaches (fine-tuning/self-repair) require costly retraining or lack knowledge reuse mechanisms for code generation challenges.

Method: Develops MemoCoder with a Fixing Knowledge Set storing successful repairs and a central Mentor Agent that identifies recurring error patterns, refines high-level strategies, and supervises the self-repair loop through multi-agent collaboration.

Result: Outperformed zero-shot prompting and Self-Repair strategies across MBPP/HumanEval/LiveCodeBench benchmarks, achieving 3.1-12.1% improvement in Pass@10 and 1.4-14.5% improvement in Pass@50 metrics.

Conclusion: The framework demonstrates superior iterative refinement and knowledge-guided code generation capabilities, with persistent learning mechanisms enabling sustained performance improvements compared to traditional approaches.

Abstract: With the widespread adoption of Large Language Models (LLMs) such as GitHub
Copilot and ChatGPT, developers increasingly rely on AI-assisted tools to
support code generation. While LLMs can generate syntactically correct
solutions for well-structured programming tasks, they often struggle with
challenges that require iterative debugging, error handling, or adaptation to
diverse problem structures. Existing approaches such as fine-tuning or
self-repair strategies either require costly retraining or lack mechanisms to
accumulate and reuse knowledge from previous attempts.
  To address these limitations, we propose MemoCoder, a multi-agent framework
that enables collaborative problem solving and persistent learning from past
fixes. At the core of MemoCoder is a Fixing Knowledge Set, which stores
successful repairs and supports retrieval for future tasks. A central Mentor
Agent supervises the repair process by identifying recurring error patterns and
refining high-level fixing strategies, providing a novel supervisory role that
guides the self-repair loop. We evaluate MemoCoder across three public
benchmarks -- MBPP, HumanEval, and LiveCodeBench -- spanning a range of problem
complexities. Experimental results show that MemoCoder consistently outperforms
both zero-shot prompting and a Self-Repair strategy, with improvements ranging
from 3.1% to 12.1% in Pass@10 and from 1.4% to 14.5% in Pass@50, demonstrating
its effectiveness in iterative refinement and knowledge-guided code generation.

</details>


### [14] [Exploring the Jupyter Ecosystem: An Empirical Study of Bugs and Vulnerabilities](https://arxiv.org/abs/2507.18833)
*Wenyuan Jiang,Diany Pressato,Harsh Darji,Thibaud Lutellier*

Main category: cs.SE

TL;DR: This paper presents an empirical study analyzing bugs and vulnerabilities in Jupyter notebooks through quantitative and qualitative methods, revealing common issues like configuration problems and incorrect API usage, and highlighting risks in deployment frameworks.


<details>
  <summary>Details</summary>
Motivation: Jupyter notebooks have unique features that existing software engineering models fail to capture, leaving a gap in understanding their bugs and vulnerabilities. This study addresses that gap to improve notebook ecosystem support.

Method: The research collected and analyzed a large dataset of notebooks from two platforms using quantitative analysis (complexity metrics, contributor activity, documentation) to identify bug correlations. Grounded theory was applied for qualitative categorization of bugs into a taxonomy, alongside analysis of security-related commits and vulnerability reports.

Result: Common bug types include configuration issues and incorrect API usage. Vulnerabilities in popular Note deployment frameworks were identified, emphasizing risks like misconfigurations and poorly maintained code.

Conclusion: Notebooks suffer from less robust engineering support than traditional software, leading to increased complexity, misconfigurations, and fragility. The study provides a bug taxonomy and risk assessments to guide improvements in development practices.

Abstract: Background. Jupyter notebooks are one of the main tools used by data
scientists. Notebooks include features (configuration scripts, markdown,
images, etc.) that make them challenging to analyze compared to traditional
software. As a result, existing software engineering models, tools, and studies
do not capture the uniqueness of Notebook's behavior. Aims. This paper aims to
provide a large-scale empirical study of bugs and vulnerabilities in the
Notebook ecosystem. Method. We collected and analyzed a large dataset of
Notebooks from two major platforms. Our methodology involved quantitative
analyses of notebook characteristics (such as complexity metrics, contributor
activity, and documentation) to identify factors correlated with bugs.
Additionally, we conducted a qualitative study using grounded theory to
categorize notebook bugs, resulting in a comprehensive bug taxonomy. Finally,
we analyzed security-related commits and vulnerability reports to assess risks
associated with Notebook deployment frameworks. Results. Our findings highlight
that configuration issues are among the most common bugs in notebook documents,
followed by incorrect API usage. Finally, we explore common vulnerabilities
associated with popular deployment frameworks to better understand risks
associated with Notebook development. Conclusions. This work highlights that
notebooks are less well-supported than traditional software, resulting in more
complex code, misconfiguration, and poor maintenance.

</details>


### [15] [SLICEMATE: Accurate and Scalable Static Program Slicing via LLM-Powered Agents](https://arxiv.org/abs/2507.18957)
*Jianming Chang,Jieke Shi,Yunbo Lyu,Xin Zhou,Lulu Wang,Zhou Yang,Bixin Li,David Lo*

Main category: cs.SE

TL;DR: SliceMate is an LLM-based static program slicing tool that outperforms traditional and learning-driven methods by using three specialized agents and a new benchmark, SliceBench.


<details>
  <summary>Details</summary>
Motivation: Traditional slicing tools struggle with scalability and incomplete syntax, while learning-based approaches lack performance on well-formed code, necessitating a balanced solution.

Method: SliceMate employs synthesis, verification, and refinement agents guided by LLM-inferred dependencies to bypass explicit dependency graphs and repair slices iteratively.

Result: Evaluations on SliceBench (2,200 Java/Python programs, up to 8,577 lines) demonstrate superior accuracy and efficiency compared to existing slicing tools.

Conclusion: SliceMate offers a robust, scalable slicing solution combining LLM strengths with structured agent workflows, addressing gaps in traditional and learning-based approaches.

Abstract: Static program slicing, which extracts the executable portions of a program
that affect the values at a specific location, supports many software analysis
tasks such as debugging and security auditing. However, traditional slicing
tools rely on computationally expensive reachability analysis over dependency
graphs, which struggle to scale to large programs and often fail to handle code
with incomplete syntax. Recently emerged learning-based methods, while more
robust to such cases, still fall short of achieving comparable performance to
traditional methods on well-formed code.
  In this work, we propose SliceMate, a novel static program slicing solution
powered by Large Language Model (LLM) agents. It bypasses the need for explicit
dependency graph construction and achieving superior slicing accuracy.
Concretely, SliceMate integrates three specialized agents: (1) a synthesis
agent that produces candidate slices by incrementally expanding the scan scope
across functions and files guided by LLM-inferred dependencies; (2) a
verification agent that performs conciseness and completeness checks of the
candidate slices, detecting missing or irrelevant statements; and (3) a
refinement agent that repairs the slices with minimal edits in accordance with
the verification results. These agents are orchestrated by a control module
that ensures timely convergence and outputs high-quality slices without manual
intervention. For rigorous evaluation, we construct a new and high-quality
benchmark, SliceBench, comprising 2,200 manually annotated Java and Python
programs, with program lengths ranging from 5 to 8,577 lines, significantly
larger than those in existing slicing benchmarks. Experimental results show
that SliceMate greatly outperforms both traditional and learning-based slicing
tools.

</details>


### [16] [Classifying Issues in Open-source GitHub Repositories](https://arxiv.org/abs/2507.18982)
*Amir Hossain Raaj,Fairuz Nawer Meem,Sadia Afrin Mim*

Main category: cs.SE

TL;DR: This paper proposes using DNN models to classify GitHub issues into labels (like Bug, Enhancement) to address the common problem of inadequate labeling. Results show DNN models outperform other methods.


<details>
  <summary>Details</summary>
Motivation: Proper issue labeling improves problem-solving efficiency on GitHub, but most repositories lack consistent labeling. Automated classification would expedite development by enabling better issue triage.

Method: Analyzed prominent GitHub repositories using ML/DNN models to classify issues into common categories (API, Documentation, Bug, etc.).

Result: Demonstrated that deep neural network models achieve superior performance compared to other machine learning approaches for issue classification.

Conclusion: The proposed DNN-based classification system effectively automates GitHub issue labeling, helping teams prioritize tasks and assign resources more efficiently to improve development workflow.

Abstract: GitHub is the most widely used platform for software maintenance in the
open-source community. Developers report issues on GitHub from time to time
while facing difficulties. Having labels on those issues can help developers
easily address those issues with prior knowledge of labels. However, most of
the GitHub repositories do not maintain regular labeling for the issues. The
goal of this work is to classify issues in the open-source community using ML
\& DNN models. There are thousands of open-source repositories on GitHub. Some
of the repositories label their issues properly whereas some of them do not.
When issues are pre-labeled, the problem-solving process and the immediate
assignment of corresponding personnel are facilitated for the team, thereby
expediting the development process. In this work, we conducted an analysis of
prominent GitHub open-source repositories. We classified the issues in some
common labels which are: API, Documentation, Enhancement, Question, Easy,
Help-wanted, Dependency, CI, Waiting for OP's response, Test, Bug, etc. Our
study shows that DNN models outperf

</details>


### [17] [SESR-Eval: Dataset for Evaluating LLMs in the Title-Abstract Screening of Systematic Reviews](https://arxiv.org/abs/2507.19027)
*Aleksi Huotala,Miikka Kuutila,Mika MÃ¤ntylÃ¤*

Main category: cs.SE

TL;DR: The paper introduces the SESR-Eval dataset to benchmark LLM performance in title-abstract screening for systematic reviews in software engineering, concluding that LLMs are currently unsuitable for automation due to inconsistent accuracy and poor recall-precision tradeoffs.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of large language models (LLMs) in automating title-abstract screening for systematic reviews (SRs) in software engineering lack sufficient benchmarking. This paper aims to fill this gap by creating a standardized dataset and performance assessment framework to answer whether LLM adoption in this context is advisable.

Method: The authors compiled 24 suitable secondary studies from 169 SR artifacts to create the SESR-Eval dataset, containing 34,528 labeled primary studies. They evaluated 9 different LLMs on title-abstract screening performance using this benchmark.

Result: The SESR-Eval dataset (34,528 labeled SE studies) showed that while LLM costs are low (<$40 per study), model performance varied minimally compared to significant differences between secondary studies. No LLM achieved both high recall and acceptable precision.

Conclusion: While SESR-Eval provides a valuable benchmark, current LLMs cannot reliably automate title-abstract screening for SE systematic reviews due to task-specific challenges in maintaining consistent accuracy. Future work will explore study-specific factors affecting LLM performance.

Abstract: Background: The use of large language models (LLMs) in the title-abstract
screening process of systematic reviews (SRs) has shown promising results, but
suffers from limited performance evaluation. Aims: Create a benchmark dataset
to evaluate the performance of LLMs in the title-abstract screening process of
SRs. Provide evidence whether using LLMs in title-abstract screening in
software engineering is advisable. Method: We start with 169 SR research
artifacts and find 24 of those to be suitable for inclusion in the dataset.
Using the dataset we benchmark title-abstract screening using 9 LLMs. Results:
We present the SESR-Eval (Software Engineering Systematic Review Evaluation)
dataset containing 34,528 labeled primary studies, sourced from 24 secondary
studies published in software engineering (SE) journals. Most LLMs performed
similarly and the differences in screening accuracy between secondary studies
are greater than differences between LLMs. The cost of using an LLM is
relatively low - less than $40 per secondary study even for the most expensive
model. Conclusions: Our benchmark enables monitoring AI performance in the
screening task of SRs in software engineering. At present, LLMs are not yet
recommended for automating the title-abstract screening process, since accuracy
varies widely across secondary studies, and no LLM managed a high recall with
reasonable precision. In future, we plan to investigate factors that influence
LLM screening performance between studies.

</details>


### [18] [Exploring the Use of LLMs for Requirements Specification in an IT Consulting Company](https://arxiv.org/abs/2507.19113)
*Liliana Pasquale,Azzurra Ragone,Emanuele Piemontese,Armin Amiri Darban*

Main category: cs.SE

TL;DR: The paper explores using large language models (LLMs) to automate requirements specification using Functional Design Specifications (FDS) in an IT consulting company, finding LLM-generated FDS improves efficiency but requires human oversight. Quality depends on input data and human revision.


<details>
  <summary>Details</summary>
Motivation: Requirements specification is fragmented across sources like meeting minutes and emails, making the process cumbersome and time-consuming in knowledge-intensive domains. Existing FDS templates need contextualization and synthesis into standardized documents.

Method: Provided LLMs with summarized requirements elicitation documents and pre-approved FDS templates as input, prompting them to generate Epic FDS and user stories. Compared the correctness and quality of three state-of-the-art LLM outputs against human-analyst-produced FDS documents.

Result: LLMs achieved automated FDS generation with reduced time and effort (around 10 minutes per 200-word summary), but required up to 25% human revision. Correctness rates of 75â€“98% were observed when inputs matched the FDS structure.

Conclusion: LLMs can be effective drafting tools for RE documentation but must be coupled with human oversight for contextual accuracy. A synergistic workflow combining LLM automation and human expertise in FDS compilation is advocated for improved outcomes.

Abstract: In practice, requirements specification remains a critical challenge. The
knowledge necessary to generate a specification can often be fragmented across
diverse sources (e.g., meeting minutes, emails, and high-level product
descriptions), making the process cumbersome and time-consuming. In this paper,
we report our experience using large language models (LLMs) in an IT consulting
company to automate the requirements specification process. In this company,
requirements are specified using a Functional Design Specification (FDS), a
document that outlines the functional requirements and features of a system,
application, or process. We provide LLMs with a summary of the requirements
elicitation documents and FDS templates, prompting them to generate Epic FDS
(including high-level product descriptions) and user stories, which are
subsequently compiled into a complete FDS document. We compared the correctness
and quality of the FDS generated by three state-of-the-art LLMs against those
produced by human analysts. Our results show that LLMs can help automate and
standardize the requirements specification, reducing time and human effort.
However, the quality of LLM-generated FDS highly depends on inputs and often
requires human revision. Thus, we advocate for a synergistic approach in which
an LLM serves as an effective drafting tool while human analysts provide the
critical contextual and technical oversight necessary for high-quality
requirements engineering (RE) documentation.

</details>


### [19] [Automated Code Review Using Large Language Models at Ericsson: An Experience Report](https://arxiv.org/abs/2507.19115)
*Shweta Ramesh,Joy Bose,Hamender Singh,A K Raghavan,Sujoy Roychowdhury,Giriprasad Sridhara,Nishrith Saini,Ricardo Britto*

Main category: cs.SE

TL;DR: The paper explores automating code review using Large Language Models (LLMs) and static analysis in Ericsson, presenting a lightweight tool and preliminary positive results with experienced developers.


<details>
  <summary>Details</summary>
Motivation: Code review, while critical for software quality, consumes significant developer time. Automating this process reduces cognitive load on developers, enabling them to prioritize writing new code and addressing bugs.

Method: A lightweight tool was developed by integrating LLMs with static program analysis, focusing on practical application and developer interaction.

Result: Experiments with experienced developers showed encouraging outcomes in evaluating the tool's effectiveness for code review automation.

Conclusion: Ericsson's approach demonstrates the potential of LLMs in augmenting code review, suggesting such tools can improve efficiency and quality in real-world software development workflows.

Abstract: Code review is one of the primary means of assuring the quality of released
software along with testing and static analysis. However, code review requires
experienced developers who may not always have the time to perform an in-depth
review of code. Thus, automating code review can help alleviate the cognitive
burden on experienced software developers allowing them to focus on their
primary activities of writing code to add new features and fix bugs. In this
paper, we describe our experience in using Large Language Models towards
automating the code review process in Ericsson. We describe the development of
a lightweight tool using LLMs and static program analysis. We then describe our
preliminary experiments with experienced developers in evaluating our code
review tool and the encouraging results.

</details>


### [20] [Fine-Tuning Multilingual Language Models for Code Review: An Empirical Study on Industrial C# Projects](https://arxiv.org/abs/2507.19271)
*Igli Begolli,Meltem Aksoy,Daniel Neider*

Main category: cs.SE

TL;DR: Study evaluates monolingual fine-tuning of open-source LMs for C# code review tasks (quality estimation, comment generation, code refinement) across industrial vs. multilingual benchmarks.


<details>
  <summary>Details</summary>
Motivation: Code review is crucial for software quality yet time-intensive and cognitively demanding, particularly in industrial settings. LMs offer automation potential, but their performance across monolingual vs. multilingual training data remains underexplored.

Method: Fine-tuned three open-source LMs (CodeReviewer, CodeLlama-7B, DeepSeek-R1-Distill) on a C#-specific dataset combining public benchmarks and industrial repositories. Analyzed effects of programming/natural language configurations on model performance, benchmarked against ASAT and human reviewers.

Result: Monolingual fine-tuning improved model accuracy and relevance compared to multilingual baselines. LMs effectively supported routine code review tasks but underperformed human reviewers in handling semantically complex/ambiguous changes.

Conclusion: Language alignment and task-specific adaptation are critical for optimizing LMs in code review. While demonstrating practical utility for automation, human reviewers still excel in context-sensitive scenarios, suggesting mixed human-AI workflows remain valuable.

Abstract: Code review is essential for maintaining software quality but often
time-consuming and cognitively demanding, especially in industrial
environments. Recent advancements in language models (LMs) have opened new
avenues for automating core review tasks. This study presents the empirical
evaluation of monolingual fine-tuning on the performance of open-source LMs
across three key automated code review tasks: Code Change Quality Estimation,
Review Comment Generation, and Code Refinement. We fine-tuned three distinct
models, CodeReviewer, CodeLlama-7B, and DeepSeek-R1-Distill, on a C\# specific
dataset combining public benchmarks with industrial repositories. Our study
investigates how different configurations of programming languages and natural
languages in the training data affect LM performance, particularly in comment
generation. Additionally, we benchmark the fine-tuned models against an
automated software analysis tool (ASAT) and human reviewers to evaluate their
practical utility in real-world settings. Our results show that monolingual
fine-tuning improves model accuracy and relevance compared to multilingual
baselines. While LMs can effectively support code review workflows, especially
for routine or repetitive tasks, human reviewers remain superior in handling
semantically complex or context-sensitive changes. Our findings highlight the
importance of language alignment and task-specific adaptation in optimizing LMs
for automated code review.

</details>


### [21] [Mut4All: Fuzzing Compilers via LLM-Synthesized Mutators Learned from Bug Reports](https://arxiv.org/abs/2507.19275)
*Bo Wang,Pengyang Wang,Chong Chen,Qi Sun,Jieke Shi,Chengran Yang,Ming Deng,Youfang Lin,Zhou Yang,David Lo*

Main category: cs.SE

TL;DR: Mut4All is a scalable, language-agnostic framework that uses LLMs and compiler bug reports to automatically create and refine mutation operators for fuzzing, uncovering new bugs in Rust and C++ compilers with minimal human intervention.


<details>
  <summary>Details</summary>
Motivation: Modern programming languages with complex constructs necessitate high-quality mutators for effective compiler bug detection, but existing approaches through manual design or human correction are labor-intensive and fail to generalize across languages.

Method: The framework employs three LLM-based agents: 1) A mutator invention agent leveraging compiler insights to identify mutation targets and generate metadata, 2) a synthesis agent to produce initial implementations, and 3) a refinement agent that performs verification and correction through unit-test feedback.

Result: Processing 1000 bug reports resulted in 319 Rust and 403 C++ mutators at approximately $0.08 each. Mutation fuzzers identified 62 Rust compiler bugs (38 new) and 34 C++ compiler bugs (16 new), outperforming existing methods in crash detection and code coverage.

Conclusion: Mut4All demonstrates a practical approach for cross-language mutator generation, enabling automated, cost-effective compiler fuzzing that achieves state-of-the-art results on both Rust and C++ compiler testing.

Abstract: Mutation-based fuzzing is effective for uncovering compiler bugs, but
designing high-quality mutators for modern languages with complex constructs
(e.g., templates, macros) remains challenging. Existing methods rely heavily on
manual design or human-in-the-loop correction, limiting scalability and
cross-language generalizability.
  We present Mut4All, a fully automated, language-agnostic framework that
synthesizes mutators using Large Language Models (LLMs) and compiler-specific
knowledge from bug reports. It consists of three agents: (1) a mutator
invention agent that identifies mutation targets and generates mutator metadata
using compiler-related insights; (2) a mutator implementation synthesis agent,
fine-tuned to produce initial implementations; and (3) a mutator refinement
agent that verifies and corrects the mutators via unit-test feedback.
  Mut4All processes 1000 bug reports (500 Rust, 500 C++), yielding 319 Rust and
403 C++ mutators at ~$0.08 each via GPT-4o. Our customized fuzzer, using these
mutators, finds 62 bugs in Rust compilers (38 new, 7 fixed) and 34 bugs in C++
compilers (16 new, 1 fixed). Mut4All outperforms existing methods in both
unique crash detection and coverage, ranking first on Rust and second on C++.

</details>


### [22] [ReCatcher: Towards LLMs Regression Testing for Code Generation](https://arxiv.org/abs/2507.19390)
*Altaf Allah Abbassi,Leuson Da Silva,Amin Nikanjam,Foutse Khomh*

Main category: cs.SE

TL;DR: ReCatcher identifies regression issues in code generation LLMs through systematic comparison across logical correctness, code quality, and execution performance. Evaluations show significant regression risks in model updates, particularly for syntax errors, correctness, and performance degradation. The framework serves as a critical tool for informed LLM update decisions.


<details>
  <summary>Details</summary>
Motivation: LLM updates (fine-tuning, merging, new versions) frequently introduce code generation regressions in correctness, quality, and performance. Current practices lack systematic evaluation of these risks, particularly for critical dimensions like error handling and execution efficiency.

Method: Framework compares LLMs via three dimensions: logical correctness (output functionality), static code quality (syntax validity, imports), and execution performance (runtime metrics). Tests three update scenarios (fine-tuning, merging, new releases) using benchmark datasets with CodeLlama, DeepSeek-Coder, and GPT-4o series.

Result: Fine-tuned models show +12% syntax errors, merging with Llama2 causes +18% correctness regressions. GPT-4o exhibits 50% regression in missing imports vs GPT-3.5-turbo. GPT-4o-mini has 80% performance degradation vs GPT-4o. ReCatcher outperforms baselines with consistent accuracy in identifying regressions across dimensions.

Conclusion: Systematic regression testing is essential for ensuring reliable LLM code generation updates. ReCatcher demonstrates that logical correctness, performance, and error handling are most vulnerable to regressions, while providing a robust evaluation framework to improve update decision-making for researchers and practitioners.

Abstract: Large Language Models (LLMs) for code generation evolve rapidly through
fine-tuning, merging, or new model releases. However, such updates can
introduce regressions, not only in correctness but also in code quality and
performance. To address this, we present ReCatcher, a regression testing
framework for Python code generation. ReCatcher systematically compares two
LLMs, typically a current model and a candidate update, across three
dimensions: logical correctness, static code quality, and execution
performance. We apply ReCatcher to assess regressions across three update
scenarios, fine-tuning, merging, and model release, using CodeLlama,
DeepSeek-Coder, and GPT-4o. Our evaluation shows that fine-tuning with
cross-language datasets increases syntax errors by up to 12%. Merging with
general-purpose models like Llama2 leads to regressions in correctness by up to
18%. GPT-4o introduces regressions of up to 50% in handling missing imports
compared to GPT-3.5-turbo, while GPT-4o-mini suffers up to 80% performance
degradation in execution time versus GPT-4o. Overall, logical correctness,
performance, and error handling (e.g., syntax errors and missing imports) are
the most regression-prone areas. Comparing ReCatcher with baseline solutions,
it presents better and consistent accuracy across logical and performance
aspects. ReCatcher highlights the importance of systematic regression
evaluation before adopting new models, while assisting researchers and
practitioners in making more informed update decisions.

</details>


### [23] [SDVDiag: A Modular Platform for the Diagnosis of Connected Vehicle Functions](https://arxiv.org/abs/2507.19403)
*Matthias WeiÃŸ,Falk Dettinger,Michael Weyrich*

Main category: cs.SE

TL;DR: This paper introduces SDVDiag, an automated diagnosis platform for connected vehicles that reduces downtime by analyzing complex cloud/edge architectures through dynamic graph modeling and anomaly detection.


<details>
  <summary>Details</summary>
Motivation: Connected vehicles require rapid failure resolution due to complex cloud/edge architectures and manual analysis delays, risking reliability and autonomous driving functionalities.

Method: SDVDiag creates end-to-end diagnostic pipelines, builds dynamic dependency graphs with runtime self-adaptive modules, monitors system metrics for anomalies, and ranks root causes by analyzing graph snapshots augmented with anomalies.

Result: Deployment in a 5G test fleet demonstrated reliable fault detection of injected failures, with the platform enabling early problem identification and reducing system downtime through automated analysis.

Conclusion: SDVDiag offers a scalable solution for connected vehicle diagnostics, combining dynamic graph visualization, runtime adaptability, and anomaly monitoring to expedite root cause analysis and improve operational efficiency.

Abstract: Connected and software-defined vehicles promise to offer a broad range of
services and advanced functions to customers, aiming to increase passenger
comfort and support autonomous driving capabilities. Due to the high
reliability and availability requirements of connected vehicles, it is crucial
to resolve any occurring failures quickly. To achieve this however, a complex
cloud/edge architecture with a mesh of dependencies must be navigated to
diagnose the responsible root cause. As such, manual analyses become unfeasible
since they would significantly delay the troubleshooting.
  To address this challenge, this paper presents SDVDiag, an extensible
platform for the automated diagnosis of connected vehicle functions. The
platform enables the creation of pipelines that cover all steps from initial
data collection to the tracing of potential root causes. In addition, SDVDiag
supports self-adaptive behavior by the ability to exchange modules at runtime.
Dependencies between functions are detected and continuously updated, resulting
in a dynamic graph view of the system. In addition, vital system metrics are
monitored for anomalies. Whenever an incident is investigated, a snapshot of
the graph is taken and augmented by relevant anomalies. Finally, the analysis
is performed by traversing the graph and creating a ranking of the most likely
causes.
  To evaluate the platform, it is deployed inside an 5G test fleet environment
for connected vehicle functions. The results show that injected faults can be
detected reliably. As such, the platform offers the potential to gain new
insights and reduce downtime by identifying problems and their causes at an
early stage.

</details>


### [24] [Resolving Build Conflicts via Example-Based and Rule-Based Program Transformations](https://arxiv.org/abs/2507.19432)
*Sheikh Shadab Towqir,Fei He,Todd Mytkowicz,Na Meng*

Main category: cs.SE

TL;DR: BUCOR is a hybrid build conflict resolver combining rule-based and example-driven strategies to handle 21 conflict types with a 60%+ resolution rate in real-world evaluations.


<details>
  <summary>Details</summary>
Motivation: Existing tools detect build conflicts but lack resolution strategies for complex cases like method removal. Merge conflicts reduce software quality and developer productivity through overlapping code changes and integration errors.

Method: BUCOR detects conflicts by comparing base, left and right versions in a merge. It uses: (1) BUCOR-R with predefined rules for common conflicts, and (2) BUCOR-E that mines branch versions for exemplar edits to infer transformation patterns.

Result: Evaluated on 88 real-world build conflicts, BUCOR generated solutions for 65 cases (74% coverage) and correctly resolved 43 conflicts (49% success rate) across 21 distinct conflict types.

Conclusion: The combination of rule-based and example-based approaches demonstrates effectiveness in build conflict resolution, suggesting hybrid methods can improve automated merge tool capabilities.

Abstract: Merge conflicts often arise when developers integrate changes from different
software branches. The conflicts can result from overlapping edits in programs
(i.e., textual conflicts) or cause build and test errors (i.e., build and test
conflicts). They degrade software quality and hinder programmer productivity.
While several tools detect build conflicts, few offer meaningful support for
resolving cases like those caused by method removal. To overcome limitations of
existing tools, we introduce BUCOR (Build Conflict Resolver), a new conflict
resolver. BUCOR first detects conflicts by comparing three versions related to
a merging scenario: base b, left l, and right r. To resolve conflicts, it
employs two complementary strategies: example-based transformation (BUCOR-E)
and rule-based transformation (BUCOR-R). BUCOR-R applies predefined rules to
handle common, well-understood conflicts. BUCOR-E mines branch versions (l and
r) for exemplar edits applied to fix related build errors. From these examples,
it infers and generalizes program transformation patterns to resolve more
complex conflicts.
  We evaluated BUCOR on 88 real-world build conflicts spanning 21 distinct
conflict types. BUCOR generated at least one solution for 65 cases and
correctly resolved 43 conflicts. We observed that this hybrid
approach--combining context-aware, example-based learning with structured,
rule-based resolution--can effectively help resolve conflicts. Our research
sheds light on future directions for more intelligent and automated merge
tools.

</details>


### [25] [An OpenSource CI/CD Pipeline for Variant-Rich Software-Defined Vehicles](https://arxiv.org/abs/2507.19446)
*Matthias WeiÃŸ,Anish Navalgund,Johannes StÃ¼mpfle,Falk Dettinger,Michael Weyrich*

Main category: cs.SE

TL;DR: This paper proposes an open-source CI/CD pipeline for software-defined vehicles (SDVs) to manage software variants and OTA updates through standardized, portable, and scalable tools, validated with an automated valet parking scenario.


<details>
  <summary>Details</summary>
Motivation: The diversity of vehicles, environments, and stakeholders in SDVs leads to fragmented development and growing software complexity, requiring dynamic orchestration across hardware/software variability for reliable operations.

Method: The solution combines containerized open-source tools in a CI/CD pipeline with custom OTA middleware that (1) automates build/test/deployment phases, (2) derives update variants based on target dependencies/hardware, and (3) supports AI model development for autonomous driving.

Result: Evaluation via an automated valet parking (AVP) scenario with TurtleBots demonstrated seamless OTA updates, correct variant selection, and successful orchestration across vehicles and backend services.

Conclusion: The pipeline offers a scalable framework to standardize SDV software management, enabling cross-stakeholder collaboration and advancing connected mobility technologies.

Abstract: Software-defined vehicles (SDVs) offer a wide range of connected
functionalities, including enhanced driving behavior and fleet management.
These features are continuously updated via over-the-air (OTA) mechanisms,
resulting in a growing number of software versions and variants due to the
diversity of vehicles, cloud/edge environments, and stakeholders involved. The
lack of a unified integration environment further complicates development, as
connected mobility solutions are often built in isolation. To ensure reliable
operations across heterogeneous systems, a dynamic orchestration of functions
that considers hardware and software variability is essential. This paper
presents an open-source CI/CD pipeline tailored for SDVs. It automates the
build, test, and deployment phases using a combination of containerized
open-source tools, creating a standardized, portable, and scalable ecosystem
accessible to all stakeholders. Additionally, a custom OTA middleware
distributes software updates and supports rollbacks across vehicles and backend
services. Update variants are derived based on deployment target dependencies
and hardware configurations. The pipeline also supports continuous development
and deployment of AI models for autonomous driving features. Its effectiveness
is evaluated using an automated valet parking (AVP) scenario involving
TurtleBots and a coordinating backend server. Two object detection variants are
developed and deployed to match hardware-specific requirements. Results
demonstrate seamless OTA updates, correct variant selection, and successful
orchestration across all targets. Overall, the proposed pipeline provides a
scalable and efficient solution for managing software variants and OTA updates
in SDVs, contributing to the advancement of future mobility technologies.

</details>
