<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 24]
- [cs.SE](#cs.SE) [Total: 6]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [A Robust Pipeline for Differentially Private Federated Learning on Imbalanced Clinical Data using SMOTETomek and FedProx](https://arxiv.org/abs/2508.10017)
*Rodrigo Tertulino*

Main category: cs.CR

TL;DR: This paper proposes a federated learning framework for cardiovascular risk prediction that addresses privacy-utility trade-offs and imbalanced medical data through SMOTETomek and optimzed FedProx, achieving strong privacy (epsilon=9.0) with high model recall (77%).


<details>
  <summary>Details</summary>
Motivation: Federated Learning (FL) enables privacy-preserving healthcare collaboration, but faces challenges from severe class imbalance and the privacy-utility trade-off when combining with Differential Privacy (DP). This study aims to develop practical solutions for creating accurate clinical models under these constraints.

Method: The framework combines 1) client-level hybrid SMOTETomek oversampling for imbalanced data handling and 2) tuned FedProx algorithm for non-IID data optimization in FL settings.

Result: 1) Standard FL methods achieved zero recall on imbalanced clinical data
2) SMOTETomek integration created clinically useful models
3) Optimized FedProx outperformed standard FedAvg
4) Non-linear privacy-utility trade-off identified
5) Achieved 77%+ recall at epsilon=9.0 (strong privacy guarantee)

Conclusion: Provides a methodological blueprint for developing effective, privacy-preserving diagnostic tools for real-world healthcare scenarios, demonstrating that 77%+ clinical utility can be maintained while achieving epsilon=9.0 level privacy protection in non-IID medical data contexts.

Abstract: Federated Learning (FL) presents a groundbreaking approach for collaborative
health research, allowing model training on decentralized data while
safeguarding patient privacy. FL offers formal security guarantees when
combined with Differential Privacy (DP). The integration of these technologies,
however, introduces a significant trade-off between privacy and clinical
utility, a challenge further complicated by the severe class imbalance often
present in medical datasets. The research presented herein addresses these
interconnected issues through a systematic, multi-stage analysis. An FL
framework was implemented for cardiovascular risk prediction, where initial
experiments showed that standard methods struggled with imbalanced data,
resulting in a recall of zero. To overcome such a limitation, we first
integrated the hybrid Synthetic Minority Over-sampling Technique with Tomek
Links (SMOTETomek) at the client level, successfully developing a clinically
useful model. Subsequently, the framework was optimized for non-IID data using
a tuned FedProx algorithm. Our final results reveal a clear, non-linear
trade-off between the privacy budget (epsilon) and model recall, with the
optimized FedProx consistently out-performing standard FedAvg. An optimal
operational region was identified on the privacy-utility frontier, where strong
privacy guarantees (with epsilon 9.0) can be achieved while maintaining high
clinical utility (recall greater than 77%). Ultimately, our study provides a
practical methodological blueprint for creating effective, secure, and accurate
diagnostic tools that can be applied to real-world, heterogeneous healthcare
data.

</details>


### [2] [A Comparative Performance Evaluation of Kyber, sntrup761, and FrodoKEM for Post-Quantum Cryptography](https://arxiv.org/abs/2508.10023)
*Samet Ãœnsal*

Main category: cs.CR

TL;DR: This paper compares leading post-quantum cryptographic algorithms (Kyber, sntrup761, FrodoKEM) in security, performance, and real-world applicability, while discussing transition challenges and future directions for quantum-resistant cryptography.


<details>
  <summary>Details</summary>
Motivation: The advent of quantum computing threatens classical cryptographic algorithms, necessitating the development and analysis of quantum-resistant alternatives to ensure long-term information security across industries.

Method: The study conducts a comprehensive review and comparative analysis of prominent lattice-based PQC algorithms, evaluating their technical properties, performance metrics, and practical implementations through literature examination.

Result: Kyber shows balanced security-performance tradeoffs, sntrup761 demonstrates efficiency advantages in specific parameters, and FrodoKEM exhibits strong security but higher cryptographic overhead, with transition challenges identified in interoperation and industry adoption.

Conclusion: The paper establishes a framework for understanding PQC maturity while emphasizing the urgency of addressing technical and implementation challenges to prepare for the quantum computing era transition.

Abstract: Post-quantum cryptography (PQC) aims to develop cryptographic algorithms that
are secure against attacks from quantum computers. This paper compares the
leading postquantum cryptographic algorithms, such as Kyber, sntrup761, and
FrodoKEM, in terms of their security, performance, and real-world
applicability. The review highlights the strengths and weaknesses of each
algorithm and provides insights into future research directions. We also
discuss the challenges of transitioning from classical to post-quantum systems
and the potential impacts on various industries. This paper serves as a
foundation for understanding the current state of post-quantum cryptography and
its future prospects in the quantum computing era.

</details>


### [3] [Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs](https://arxiv.org/abs/2508.10031)
*Jinhwa Kim,Ian G. Harris*

Main category: cs.CR

TL;DR: The study introduces a Context Filtering model to defend against jailbreak attacks on Large Language Models (LLMs), aiming to reduce attack success rates while preserving LLM performance. It achieves up to 88% reduction in attacks with state-of-the-art Safety and Helpfulness Product results and is a plug-and-play solution applicable to various LLMs.


<details>
  <summary>Details</summary>
Motivation: LLMs face increasing safety and ethical risks due to jailbreak attacks that exploit adversarial context to generate harmful responses. Existing defenses often compromise LLM helpfulness, negatively impacting benign users.

Method: Context Filtering model: an input pre-processing method to filter untrustworthy context, identify primary prompts containing user intent, and uncover concealed malicious intent without requiring LLM fine-tuning. It is compatible with both white-box and black-box models.

Result: The model reduces jailbreak attack success rates by 88% while maintaining LLM performance, achieving state-of-the-art Safety and Helpfulness Product results. Comparative analysis shows superior effectiveness against six different attacks.

Conclusion: The Context Filtering model provides a universal, non-intrusive defense for LLMs against jailbreak attacks, preserving helpfulness without model adjustments. It is publicly available for research.

Abstract: While Large Language Models (LLMs) have shown significant advancements in
performance, various jailbreak attacks have posed growing safety and ethical
risks. Malicious users often exploit adversarial context to deceive LLMs,
prompting them to generate responses to harmful queries. In this study, we
propose a new defense mechanism called Context Filtering model, an input
pre-processing method designed to filter out untrustworthy and unreliable
context while identifying the primary prompts containing the real user intent
to uncover concealed malicious intent. Given that enhancing the safety of LLMs
often compromises their helpfulness, potentially affecting the experience of
benign users, our method aims to improve the safety of the LLMs while
preserving their original performance. We evaluate the effectiveness of our
model in defending against jailbreak attacks through comparative analysis,
comparing our approach with state-of-the-art defense mechanisms against six
different attacks and assessing the helpfulness of LLMs under these defenses.
Our model demonstrates its ability to reduce the Attack Success Rates of
jailbreak attacks by up to 88% while maintaining the original LLMs'
performance, achieving state-of-the-art Safety and Helpfulness Product results.
Notably, our model is a plug-and-play method that can be applied to all LLMs,
including both white-box and black-box models, to enhance their safety without
requiring any fine-tuning of the models themselves. We will make our model
publicly available for research purposes.

</details>


### [4] [Cognitive Cybersecurity for Artificial Intelligence: Guardrail Engineering with CCS-7](https://arxiv.org/abs/2508.10033)
*Yuksel Aydin*

Main category: cs.CR

TL;DR: This paper introduces CCS-7, a taxonomy of seven cognitive vulnerabilities in language models, demonstrating through human experiments and model evaluations that cognitive safety requires architecture-specific interventions due to varying efficacy and backfire effects.


<details>
  <summary>Details</summary>
Motivation: Traditional behavioral alignment methods fail to address human-like cognitive vulnerabilities (e.g., emotional framing) in language models, necessitating new frameworks for robust cognitive safety.

Method: 1. Proposed CCS-7 taxonomy based on human cognitive security research. 2. Conducted a randomized controlled trial with 151 human participants to benchmark the 'Think First, Verify Always' (TFVA) intervention. 3. Evaluated TFVA-style guardrails across 12,180 experiments on seven model architectures.

Result: Architecture-dependent vulnerability patterns emerged: identity confusion was nearly mitigated, but source interference risk increased by 135% in some models. Humans showed consistent 7.9% improvement with TFVA training.

Conclusion: Cognitive safety must be approached as an architecture-specific engineering challenge; interventions effective in one model may backfire in another, requiring pre-deployment architecture-aware testing.

Abstract: Language models exhibit human-like cognitive vulnerabilities, such as
emotional framing, that escape traditional behavioral alignment. We present
CCS-7 (Cognitive Cybersecurity Suite), a taxonomy of seven vulnerabilities
grounded in human cognitive security research. To establish a human benchmark,
we ran a randomized controlled trial with 151 participants: a "Think First,
Verify Always" (TFVA) lesson improved cognitive security by +7.9% overall. We
then evaluated TFVA-style guardrails across 12,180 experiments on seven diverse
language model architectures. Results reveal architecture-dependent risk
patterns: some vulnerabilities (e.g., identity confusion) are almost fully
mitigated, while others (e.g., source interference) exhibit escalating
backfire, with error rates increasing by up to 135% in certain models. Humans,
in contrast, show consistent moderate improvement. These findings reframe
cognitive safety as a model-specific engineering problem: interventions
effective in one architecture may fail, or actively harm, another, underscoring
the need for architecture-aware cognitive safety testing before deployment.

</details>


### [5] [Neural Network-Based Detection and Multi-Class Classification of FDI Attacks in Smart Grid Home Energy Systems](https://arxiv.org/abs/2508.10035)
*Varsha Sen,Biswash Basnet*

Main category: cs.CR

TL;DR: This paper introduces a machine learning-based framework using a lightweight ANN for real-time detection and a Bidirectional LSTM for classification of False Data Injection Attacks (FDIAs) in smart grids, particularly targeting Home Area Networks (HANs) to enhance grid resilience against data manipulation.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the vulnerability of smart grids, especially HANs, to FDIAs which can manipulate demand patterns and disrupt grid operations due to insufficient security measures, necessitating scalable and effective detection solutions.

Method: The authors propose two models: a lightweight Artificial Neural Network (ANN) for real-time FDIAs detection leveraging energy consumption, cost, and time context features, and a Bidirectional Long Short-Term Memory (LSTM) network for classifying attack types (normal, trapezoidal, sigmoid) by learning sequential dependencies. They validated the framework using a synthetic time-series dataset mimicking household energy behavior.

Result: Experimental evaluations confirmed the effectiveness of both models, achieving accurate detection and classification of FDIAs. The proposed solution demonstrates scalability and real-time usability for enhancing smart grid security at the edge level.

Conclusion: The paper concludes that the framework provides a robust, data-driven approach to defend smart grids against FDIAs by deploying ML techniques at residential endpoints, contributing toward improved cybersecurity and operational resilience in HANs.

Abstract: False Data Injection Attacks (FDIAs) pose a significant threat to smart grid
infrastructures, particularly Home Area Networks (HANs), where real-time
monitoring and control are highly adopted. Owing to the comparatively less
stringent security controls and widespread availability of HANs, attackers view
them as an attractive entry point to manipulate aggregated demand patterns,
which can ultimately propagate and disrupt broader grid operations. These
attacks undermine the integrity of smart meter data, enabling malicious actors
to manipulate consumption values without activating conventional alarms,
thereby creating serious vulnerabilities across both residential and
utility-scale infrastructures. This paper presents a machine learning-based
framework for both the detection and classification of FDIAs using residential
energy data. A real-time detection is provided by the lightweight Artificial
Neural Network (ANN), which works by using the most vital features of energy
consumption, cost, and time context. For the classification of different attack
types, a Bidirectional LSTM is trained to recognize normal, trapezoidal, and
sigmoid attack shapes through learning sequential dependencies in the data. A
synthetic time-series dataset was generated to emulate realistic household
behaviour. Experimental results demonstrate that the proposed models are
effective in identifying and classifying FDIAs, offering a scalable solution
for enhancing grid resilience at the edge. This work contributes toward
building intelligent, data-driven defence mechanisms that strengthen smart grid
cybersecurity from residential endpoints.

</details>


### [6] [Certifiably robust malware detectors by design](https://arxiv.org/abs/2508.10038)
*Pierre-Francois Gimenez,Sarath Sivaprasad,Mario Fritz*

Main category: cs.CR

TL;DR: This paper proposes a certifiably robust malware detection framework (ERDALT) designed to resist adversarial examples without compromising detection performance, addressing vulnerabilities in static ML-based malware analysis.


<details>
  <summary>Details</summary>
Motivation: Despite high accuracy, static malware detectors using ML are vulnerable to adversarial attacks where minor code modifications can evade detection without altering behavior. Unlike computer vision, malware adversarial examples require precise functional transformations.

Method: The authors introduce a novel model architecture for robust malware detection by design and demonstrate that robust detectors can be decomposed into a specific structure. Their ERDALT framework leverages this architectural principle to create certifiably robust malware classifiers.

Result: ERDALT achieves robust detection against adversarial examples while maintaining strong detection performance, validated through comparisons with standard machine learning-based malware detection approaches.

Conclusion: The proposed structured approach enables scalable malware detection systems that inherently resist adversarial attacks, even when using fragile features, bridging a critical security gap in static analysis.

Abstract: Malware analysis involves analyzing suspicious software to detect malicious
payloads. Static malware analysis, which does not require software execution,
relies increasingly on machine learning techniques to achieve scalability.
Although such techniques obtain very high detection accuracy, they can be
easily evaded with adversarial examples where a few modifications of the sample
can dupe the detector without modifying the behavior of the software. Unlike
other domains, such as computer vision, creating an adversarial example of
malware without altering its functionality requires specific transformations.
We propose a new model architecture for certifiably robust malware detection by
design. In addition, we show that every robust detector can be decomposed into
a specific structure, which can be applied to learn empirically robust malware
detectors, even on fragile features. Our framework ERDALT is based on this
structure. We compare and validate these approaches with machine-learning-based
malware detection methods, allowing for robust detection with limited reduction
of detection performance.

</details>


### [7] [Multi-task Adversarial Attacks against Black-box Model with Few-shot Queries](https://arxiv.org/abs/2508.10039)
*Wenqiang Wang,Yan Xiao,Hao Lin,Yangshijie Zhang,Xiaochun Cao*

Main category: cs.CR

TL;DR: This paper introduces CEMA, a black-box adversarial text attack method with high query efficiency and multi-task versatility by leveraging a plug-and-play substitute model and ensemble attack strategies across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Current multi-task adversarial text attacks require excessive model access/internal features and are limited to single-task scenarios, reducing their effectiveness in practical black-box settings with restricted queries.

Method: CEMA uses a deep-level substitute model trained independently for text classification to bypass target model mimicry, combined with adversarial candidate ensembles and selection based on substitute model attack effectiveness.

Result: Achieves high attack success rates with 100 queries on multi-task models (2-6 tasks) across classification, translation, summarization, and image generation, effectively targeting commercial APIs (Baidu, Google Translate), ChatGPT 4o, and Stable Diffusion V2.

Conclusion: CEMA demonstrates that multi-task adversarial attacks can be simplified to classification problems through substitute model transferability, enabling practical black-box attacks with limited queries across diverse tasks and models.

Abstract: Current multi-task adversarial text attacks rely on abundant access to shared
internal features and numerous queries, often limited to a single task type. As
a result, these attacks are less effective against practical scenarios
involving black-box feedback APIs, limited queries, or multiple task types. To
bridge this gap, we propose \textbf{C}luster and \textbf{E}nsemble
\textbf{M}ulti-task Text Adversarial \textbf{A}ttack (\textbf{CEMA}), an
effective black-box attack that exploits the transferability of adversarial
texts across different tasks. CEMA simplifies complex multi-task scenarios by
using a \textit{deep-level substitute model} trained in a
\textit{plug-and-play} manner for text classification, enabling attacks without
mimicking the victim model. This approach requires only a few queries for
training, converting multi-task attacks into classification attacks and
allowing attacks across various tasks.
  CEMA generates multiple adversarial candidates using different text
classification methods and selects the one that most effectively attacks
substitute models.
  In experiments involving multi-task models with two, three, or six
tasks--spanning classification, translation, summarization, and text-to-image
generation--CEMA demonstrates significant attack success with as few as 100
queries. Furthermore, CEMA can target commercial APIs (e.g., Baidu and Google
Translate), large language models (e.g., ChatGPT 4o), and image-generation
models (e.g., Stable Diffusion V2), showcasing its versatility and
effectiveness in real-world applications.

</details>


### [8] [Quantum Prime Factorization: A Novel Approach Based on Fermat Method](https://arxiv.org/abs/2508.10041)
*Julien Mellaerts*

Main category: cs.CR

TL;DR: This paper presents a novel quantum algorithm for factoring composite odd numbers, enhancing the Fermat method fourfold and using Quantum Annealers to achieve the largest factorization on a quantum device to date.


<details>
  <summary>Details</summary>
Motivation: Factoring large composite numbers is crucial for cryptography and demonstrating quantum computing capabilities. The classical Fermat method's limitations in complexity necessitate improvements.

Method: Improved Fermat method by reducing computational complexity by fourfold, then reformulated it as an optimization problem for Quantum Annealers.

Result: Successfully factorized 8,689,739 using a quantum device, the largest number achieved with a quantum approach, and reduced the computational complexity.

Conclusion: The combination of classical method enhancement and quantum optimization establishes a new benchmark for factoring with potential applications in quantum algorithms.

Abstract: In this paper, we introduce a novel quantum algorithm for the factorization
of composite odd numbers. This work makes two significant contributions. First,
we present a new improvement to the classical Fermat method, fourfold reducing
the computational complexity of factoring. Second, we reformulate Fermat
factorization method as an optimization problem suitable for Quantum Annealers
which allowed us to factorize 8,689,739, the biggest number ever factorized
using a quantum device to our knowledge.

</details>


### [9] [FIDELIS: Blockchain-Enabled Protection Against Poisoning Attacks in Federated Learning](https://arxiv.org/abs/2508.10042)
*Jane Carney,Kushal Upreti,Gaby G. Dagher,Tim Andersen*

Main category: cs.CR

TL;DR: The paper introduces a blockchain-powered poison detection framework for federated learning, decentralizing the global server's role and leveraging client-agreed judge models to enhance robustness against data poisoning attacks while maintaining scalability.


<details>
  <summary>Details</summary>
Motivation: Federated learning is vulnerable to data poisoning attacks due to reliance on decentralized, untrusted client data. Existing detection methods lack standardization or trustworthy verification mechanisms.

Method: Sys utilizes blockchain to distribute the global server's responsibilities across clients. Each client generates a judge model to identify poisoned updates, with consensus mechanisms ensuring agreement on a unified judge model for validation.

Result: Implementation results demonstrate Sys effectively resists data poisoning attacks and exhibits scalability in judging model creation, validating its practicality for real-world applications.

Conclusion: Sys represents a secure, decentralized solution for federated learning robustness, achieving consensus for poison detection without compromising privacy or scalability, as evidenced by successful implementation.

Abstract: Federated learning enhances traditional deep learning by enabling the joint
training of a model with the use of IoT device's private data. It ensures
privacy for clients, but is susceptible to data poisoning attacks during
training that degrade model performance and integrity. Current poisoning
detection methods in federated learning lack a standardized detection method or
take significant liberties with trust. In this paper, we present \Sys, a novel
blockchain-enabled poison detection framework in federated learning. The
framework decentralizes the role of the global server across participating
clients. We introduce a judge model used to detect data poisoning in model
updates. The judge model is produced by each client and verified to reach
consensus on a single judge model. We implement our solution to show \Sys is
robust against data poisoning attacks and the creation of our judge model is
scalable.

</details>


### [10] [Securing Agentic AI: Threat Modeling and Risk Analysis for Network Monitoring Agentic AI System](https://arxiv.org/abs/2508.10043)
*Pallavi Zambare,Venkata Nikhil Thanikella,Ying Liu*

Main category: cs.CR

TL;DR: This paper analyzes security vulnerabilities in autonomous agentic AI systems combined with Large Language Models (LLMs) through the MAESTRO threat modeling framework. It identifies two critical threatsâ€”resource denial-of-service and memory poisoningâ€”and proposes a multilayered defense strategy to ensure system reliability.


<details>
  <summary>Details</summary>
Motivation: The integration of LLMs with autonomous agents in network monitoring and decision-making systems introduces significant security risks, necessitating proactive threat modeling to prevent adversarial exploitation and ensure robust system design.

Method: The authors applied the MAESTRO seven-layer threat modeling architecture to assess vulnerabilities. They built a prototype agent system using Python, LangChain, and WebSocket telemetry, incorporating modules for inference, memory, parameter tuning, and anomaly detection. Two threat scenarios were validated experimentally.

Result: Practical confirmation of two vulnerabilities: (1) traffic replay denial-of-service leading to delayed telemetry updates, and (2) memory poisoning via log file tampering causing increased computational loads. These demonstrated measurable system performance degradation.

Conclusion: MAESTRO's multilayered defense-in-depth approachâ€”which includes memory isolation, planner validation, and real-time anomaly responseâ€”provides viable solutions for operational threat mapping, risk scoring, and resilient system design. The study underscores memory integrity and cross-layer communication protection as critical for agentic AI reliability.

Abstract: When combining Large Language Models (LLMs) with autonomous agents, used in
network monitoring and decision-making systems, this will create serious
security issues. In this research, the MAESTRO framework consisting of the
seven layers threat modeling architecture in the system was used to expose,
evaluate, and eliminate vulnerabilities of agentic AI. The prototype agent
system was constructed and implemented, using Python, LangChain, and telemetry
in WebSockets, and deployed with inference, memory, parameter tuning, and
anomaly detection modules. Two practical threat cases were confirmed as
follows: (i) resource denial of service by traffic replay denial-of-service,
and (ii) memory poisoning by tampering with the historical log file maintained
by the agent. These situations resulted in measurable levels of performance
degradation, i.e. telemetry updates were delayed, and computational loads were
increased, as a result of poor system adaptations. It was suggested to use a
multilayered defense-in-depth approach with memory isolation, validation of
planners and anomaly response systems in real-time. These findings verify that
MAESTRO is viable in operational threat mapping, prospective risk scoring, and
the basis of the resilient system design. The authors bring attention to the
importance of the enforcement of memory integrity, paying attention to the
adaptation logic monitoring, and cross-layer communication protection that
guarantee the agentic AI reliability in adversarial settings.

</details>


### [11] [Generative AI for Cybersecurity of Energy Management Systems: Methods, Challenges, and Future Directions](https://arxiv.org/abs/2508.10044)
*Aydin Zaboli,Junho Hong*

Main category: cs.CR

TL;DR: The paper introduces a comprehensive security framework for energy management systems (EMSs) that addresses dynamic cybersecurity vulnerabilities using a multi-point attack/model methodology, generative AI-based anomaly detection, and the SoM-GI framework for visual and multimodal analysis. It validates the frameworkâ€™s effectiveness on an IEEE 14-Bus system.


<details>
  <summary>Details</summary>
Motivation: Modern EMSs face interconnected cybersecurity threats and system errors across the data processing pipeline, necessitating adaptive frameworks to handle evolving attack vectors and their real-time impacts on operational systems like HMI displays and databases.

Method: 1. Proposes a multi-point attack/error model to identify vulnerabilities in EMS data processing (post-SE stealth attacks, database manipulation, HMI display corruption). 2. Introduces genAI-based anomaly detection systems (ADSs) for the first time in power systems. 3. Develops the SoM-GI framework, which integrates visual markers with rules to improve spatial reasoning and detect visual anomalies in HMI displays.

Result: Validation on the IEEE 14-Bus system demonstrates the frameworkâ€™s ability to detect anomalies across diverse scenarios and identify inconsistencies in real-time operations, including failures in numerical detection methods. The integrated approach combines numerical and visual pattern recognition with linguistic rules.

Conclusion: The hybrid framework effectively mitigates cyber threats and system errors in EMSs by leveraging genAI capabilities, multimodal analysis, and visual indicators, offering a novel solution for dynamic and complex attack environments in power systems.

Abstract: This paper elaborates on an extensive security framework specifically
designed for energy management systems (EMSs), which effectively tackles the
dynamic environment of cybersecurity vulnerabilities and/or system problems
(SPs), accomplished through the incorporation of novel methodologies. A
comprehensive multi-point attack/error model is initially proposed to
systematically identify vulnerabilities throughout the entire EMS data
processing pipeline, including post state estimation (SE) stealth attacks, EMS
database manipulation, and human-machine interface (HMI) display corruption
according to the real-time database (RTDB) storage. This framework acknowledges
the interconnected nature of modern attack vectors, which utilize various
phases of supervisory control and data acquisition (SCADA) data flow. Then,
generative AI (GenAI)-based anomaly detection systems (ADSs) for EMSs are
proposed for the first time in the power system domain to handle the scenarios.
Further, a set-of-mark generative intelligence (SoM-GI) framework, which
leverages multimodal analysis by integrating visual markers with rules
considering the GenAI capabilities, is suggested to overcome inherent spatial
reasoning limitations. The SoM-GI methodology employs systematic visual
indicators to enable accurate interpretation of segmented HMI displays and
detect visual anomalies that numerical methods fail to identify. Validation on
the IEEE 14-Bus system shows the framework's effectiveness across scenarios,
while visual analysis identifies inconsistencies. This integrated approach
combines numerical analysis with visual pattern recognition and linguistic
rules to protect against cyber threats and system errors.

</details>


### [12] [NetMoniAI: An Agentic AI Framework for Network Security & Monitoring](https://arxiv.org/abs/2508.10052)
*Pallavi Zambare,Venkata Nikhil Thanikella,Nikhil Padmanabh Kottur,Sree Akhil Akula,Ying Liu*

Main category: cs.CR

TL;DR: NetMoniAI combines decentralized and centralized AI for efficient network monitoring and security.


<details>
  <summary>Details</summary>
Motivation: The paper addresses scalability and accuracy challenges in network monitoring systems by introducing a two-tier architecture that minimizes redundancy while maintaining situational awareness across diverse threat scenarios.

Method: The framework employs autonomous micro-agents for local traffic analysis and anomaly detection at each network node, coupled with a central controller that aggregates node-specific insights to identify coordinated attacks and systemic issues. Evaluation used local testbeds and NS-3 simulations.

Result: NetMoniAI demonstrates improved scalability, reduced redundancy, faster response times, and preserved accuracy under resource constraints across test environments and simulated scenarios.

Conclusion: The agentic-AI framework offers a reproducible, open-source solution for network monitoring that balances decentralized automation with centralized coordination, enabling researchers to extend its capabilities to varied environments.

Abstract: In this paper, we present NetMoniAI, an agentic AI framework for automatic
network monitoring and security that integrates decentralized analysis with
lightweight centralized coordination. The framework consists of two layers:
autonomous micro-agents at each node perform local traffic analysis and anomaly
detection. A central controller then aggregates insights across nodes to detect
coordinated attacks and maintain system-wide situational awareness. We
evaluated NetMoniAI on a local micro-testbed and through NS-3 simulations.
Results confirm that the two-tier agentic-AI design scales under resource
constraints, reduces redundancy, and improves response time without
compromising accuracy. To facilitate broader adoption and reproducibility, the
complete framework is available as open source. This enables researchers and
practitioners to replicate, validate, and extend it across diverse network
environments and threat scenarios. Github link:
https://github.com/pzambare3/NetMoniAI

</details>


### [13] [Invisible Watermarks, Visible Gains: Steering Machine Unlearning with Bi-Level Watermarking Design](https://arxiv.org/abs/2508.10065)
*Yuhao Sun,Yihua Zhang,Gaowen Liu,Hongtao Xie,Sijia Liu*

Main category: cs.CR

TL;DR: This paper introduces Water4MU, a novel machine unlearning approach using digital watermarking to modify data content. It enhances unlearning effectiveness via bi-level optimization (BLO) and outperforms existing methods in 'challenging forgets' scenarios for image classification and generation tasks.


<details>
  <summary>Details</summary>
Motivation: Current machine unlearning (MU) methods focus on in-training weight adjustments with limited data-level approach exploration. The paper addresses the need for data-level modifications to improve unlearning efficiency and comply with data removal regulations like the right to be forgotten.

Method: The framework uses a bi-level optimization (BLO) process: (1) Upper level optimizes a watermarking network to minimize unlearning difficulty for targeted data, and (2) Lower level trains the main model independently of the watermarking. Watermarking embeds controllable data patterns to facilitate selective influence removal.

Result: Water4MU achieves effective unlearning performance across image classification and generation tasks, demonstrating superior results in challenging scenarios (e.g., 'challenging forgets') compared to state-of-the-art in-training MU methods.

Conclusion: Water4MU provides a data-centric path for machine unlearning by leveraging watermarking to strategically alter data content. It achieves precise sensitive data removal while preserving model utility, offering a promising direction for future unlearning research and applications.

Abstract: With the increasing demand for the right to be forgotten, machine unlearning
(MU) has emerged as a vital tool for enhancing trust and regulatory compliance
by enabling the removal of sensitive data influences from machine learning (ML)
models. However, most MU algorithms primarily rely on in-training methods to
adjust model weights, with limited exploration of the benefits that data-level
adjustments could bring to the unlearning process. To address this gap, we
propose a novel approach that leverages digital watermarking to facilitate MU
by strategically modifying data content. By integrating watermarking, we
establish a controlled unlearning mechanism that enables precise removal of
specified data while maintaining model utility for unrelated tasks. We first
examine the impact of watermarked data on MU, finding that MU effectively
generalizes to watermarked data. Building on this, we introduce an
unlearning-friendly watermarking framework, termed Water4MU, to enhance
unlearning effectiveness. The core of Water4MU is a bi-level optimization (BLO)
framework: at the upper level, the watermarking network is optimized to
minimize unlearning difficulty, while at the lower level, the model itself is
trained independently of watermarking. Experimental results demonstrate that
Water4MU is effective in MU across both image classification and image
generation tasks. Notably, it outperforms existing methods in challenging MU
scenarios, known as "challenging forgets".

</details>


### [14] [An Architecture for Distributed Digital Identities in the Physical World](https://arxiv.org/abs/2508.10185)
*RenÃ© Mayrhofer,Michael Roland,Tobias HÃ¶ller,Philipp Hofer,Mario Lins*

Main category: cs.CR

TL;DR: This paper proposes a decentralized digital identity architecture for physical world transactions, introducing a Personal Identity Agent (PIA) to reduce privacy and availability risks from centralized providers, with a verified protocol and feasible implementation for low-latency applications.


<details>
  <summary>Details</summary>
Motivation: Centralized digital identity systems have single points of failure and control, making them susceptible to global attacks across technical, organizational, and legal domains, which compromises both availability and privacy.

Method: The architecture integrates sensors, identity authorities, attribute verifiers, and PIAs. It employs a decentralized protocol, formal verification under a realistic threat model involving strong global adversaries, and a proof-of-concept implementation to test feasibility.

Result: A working implementation demonstrates the architecture and protocol's practicality, confirming they can handle real-world applications with acceptable latency (a few seconds) while maintaining security.

Conclusion: The paper concludes that the decentralized architecture with PIAs effectively addresses privacy and availability challenges in physical service interactions, offering a sustainable alternative to centralized identity management systems.

Abstract: Digital identities are increasingly important for mediating not only digital
but also physical service transactions. Managing such identities through
centralized providers can cause both availability and privacy concerns: single
points of failure and control are ideal targets for global attacks on
technical, organizational, or legal fronts. We design, analyze, and build a
distributed digital identity architecture for physical world transactions in
common scenarios like unlocking doors, public transport, or crossing country
borders. This architecture combines (biometric and other) sensors, (established
and upcoming) identity authorities, attribute verifiers, and a new core
component we call the \emph{Personal Identity Agent (PIA)} that represents
individuals with their identity attributes in the digital domain. All
transactions are conducted in a completely decentralized manner, and the
components for which we currently assume central coordination are optional and
only used for assisting with service discovery and latency reduction. We
present a first protocol between these parties and formally verify that it
achieves relevant security properties based on a realistic threat model
including strong global adversaries. A proof-of-concept implementation
demonstrates practical feasibility of both architecture and initial protocol
for applications that can tolerate end-to-end latencies in the range of a few
seconds.

</details>


### [15] [Detecting Untargeted Attacks and Mitigating Unreliable Updates in Federated Learning for Underground Mining Operations](https://arxiv.org/abs/2508.10212)
*Md Sazedur Rahman,Mohamed Elmahallawy,Sanjay Madria,Samuel Frimpong*

Main category: cs.CR

TL;DR: MineDetect: A robust federated learning framework addressing security and data quality challenges in underground mining operations by detecting attacked models and mitigating adversarial influences from low-quality data.


<details>
  <summary>Details</summary>
Motivation: Federated Learning (FL) is needed in underground mining to preserve privacy of local sensor data while enabling collaborative hazard detection. Existing FL approaches face risks from model attacks (sign-flipping/additive noise) and data degradation caused by harsh mining conditions.

Method: MineDetect implements two innovations: (1) A history-aware gradient update mechanism tracking local and global averages for detecting malicious model attacks, and (2) A method for identifying and eliminating adversarial influences from unreliable models caused by poor data quality.

Result: Comprehensive simulations demonstrate superior robustness and accuracy compared to existing methods across challenging non-IID data scenarios, while maintaining computational efficiency.

Conclusion: MineDetect advances FL security and reliability for underground mining applications, offering practical solutions to both adversarial attacks and data quality issues critical for operational safety.

Abstract: Underground mining operations rely on distributed sensor networks to collect
critical data daily, including mine temperature, toxic gas concentrations, and
miner movements for hazard detection and operational decision-making. However,
transmitting raw sensor data to a central server for training deep learning
models introduces significant privacy risks, potentially exposing sensitive
mine-specific information. Federated Learning (FL) offers a transformative
solution by enabling collaborative model training while ensuring that raw data
remains localized at each mine. Despite its advantages, FL in underground
mining faces key challenges: (i) An attacker may compromise a mine's local
model by employing techniques such as sign-flipping attacks or additive noise,
leading to erroneous predictions; (ii) Low-quality (yet potentially valuable)
data, caused by poor lighting conditions or sensor inaccuracies in mines may
degrade the FL training process. In response, this paper proposes MineDetect, a
defense FL framework that detects and isolates the attacked models while
mitigating the impact of mines with low-quality data. MineDetect introduces two
key innovations: (i) Detecting attacked models (maliciously manipulated) by
developing a history-aware mechanism that leverages local and global averages
of gradient updates; (ii) Identifying and eliminating adversarial influences
from unreliable models (generated by clients with poor data quality) on the FL
training process. Comprehensive simulations across diverse datasets demonstrate
that MineDetect outperforms existing methods in both robustness and accuracy,
even in challenging non-IID data scenarios. Its ability to counter adversarial
influences while maintaining lower computational efficiency makes it a vital
advancement for improving safety and operational effectiveness in underground
mining.

</details>


### [16] [BERTector: Intrusion Detection Based on Joint-Dataset Learning](https://arxiv.org/abs/2508.10327)
*Haoyang Hu,Xun Huang,Chenyu Wu,Shiwen Liu,Zhichao Lian,Shuangquan Zhang*

Main category: cs.CR

TL;DR: This paper proposes BERTector, a BERT-based intrusion detection system with a joint-dataset training paradigm, NSS-Tokenizer for traffic-aware semantic tokenization, and low-rank adaptation for efficient training, achieving state-of-the-art performance in accuracy, generalization, and adversarial robustness.


<details>
  <summary>Details</summary>
Motivation: Modern IDS face challenges in generalization and robustness due to diverse network traffic patterns and varying attack types, necessitating a more adaptable and efficient detection framework.

Method: The framework integrates three components: (1) NSS-Tokenizer for traffic-aware semantic tokenization of network data, (2) supervised fine-tuning using hybrid datasets combining different traffic types, and (3) low-rank adaptation (LoRA) to optimize training efficiency while maintaining model performance.

Result: Extensive experiments demonstrate:
1. State-of-the-art intrusion detection accuracy across standard datasets
2. Strong cross-dataset generalization capabilities (13.8% higher accuracy than prior methods on unseen data)
3. Excellent robustness to adversarial perturbations (96.7% accuracy under attacks compared to 22.3% for baseline models)

Conclusion: BERTector establishes a unified, efficient solution for intrusion detection in complex network environments by combining transformer architectures with specialized network traffic features and adaptive training techniques, significantly improving practical deployment viability through its joint-dataset approach and resource-efficient design.

Abstract: Intrusion detection systems (IDS) are facing challenges in generalization and
robustness due to the heterogeneity of network traffic and the diversity of
attack patterns. To address this issue, we propose a new joint-dataset training
paradigm for IDS and propose a scalable BERTector framework based on BERT.
BERTector integrates three key components: NSS-Tokenizer for traffic-aware
semantic tokenization, supervised fine-tuning with a hybrid dataset, and
low-rank adaptation (LoRA) for efficient training. Extensive experiments show
that BERTector achieves state-of-the-art detection accuracy, strong
cross-dataset generalization capabilities, and excellent robustness to
adversarial perturbations. This work establishes a unified and efficient
solution for modern IDS in complex and dynamic network environments.

</details>


### [17] [Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based Side-Channel Attacks on Fully Associative Randomized Caches](https://arxiv.org/abs/2508.10431)
*Chris Cao,Gururaj Saileshwar*

Main category: cs.CR

TL;DR: This paper refutes claims that occupancy-based attacks can recover AES keys from the MIRAGE cache, blaming the prior results on flawed modeling with a constant RNG seed. Correcting this flaw renders the attack ineffective.


<details>
  <summary>Details</summary>
Motivation: To evaluate and correct the flawed claims of vulnerabilities in MIRAGE by examining the simulation methodology of prior occupancy-based attacks.

Method: The authors re-analyzed the MIRAGE simulation used in the prior work, identifying the constant RNG seed for global evictions as a critical flaw. They redesigned experiments with randomized seeds per AES encryption run.

Result: Attacker runtime correlation with AES T-table accesses vanished when eviction seeds were randomized, conclusively invalidating the prior attack's success.

Conclusion: The apparent vulnerability in MIRAGE was an artifact of incorrect simulation methodology. Properly modeled with random eviction sequences, this attack approach is infeasible, confirming MIRAGE's security against these techniques.

Abstract: Recent work presented at USENIX Security 2025 claims that occupancy-based
attacks can recover AES keys from the MIRAGE randomized cache. In this paper,
we examine these claims and find that they arise from fundamental modeling
flaws. Most critically, the authors' simulation of MIRAGE uses a constant seed
to initialize the random number generator used for global evictions in MIRAGE,
causing every AES encryption they trace to evict the same deterministic
sequence of cache lines. This artificially creates a highly repeatable timing
pattern that is not representative of a realistic implementation of MIRAGE,
where eviction sequences vary randomly between encryptions. When we instead
randomize the eviction seed for each run, reflecting realistic operation, the
correlation between AES T-table accesses and attacker runtimes disappears, and
the attack fails. These findings show that the reported leakage is an artifact
of incorrect modeling, and not an actual vulnerability in MIRAGE.

</details>


### [18] [AlDBaran: Towards Blazingly Fast State Commitments for Blockchains](https://arxiv.org/abs/2508.10493)
*Bernhard Kauer,Aleksandr Petrosyan,Benjamin Livshits*

Main category: cs.CR

TL;DR: AlDBaran is an authenticated database for high-throughput blockchain systems, enabling 50 Gbps network throughput, 48M updates/s on standard hardware, and 8M/5M updates/s on portable devices. It outperforms existing solutions with disk I/O removal and Merkle tree optimizations.


<details>
  <summary>Details</summary>
Motivation: Blockchain systems require higher transaction throughput (multi-million TPS) than current authenticated database approaches can efficiently handle, while maintaining integrity and supporting lightweight clients/rollups.

Method: The paper introduces AlDBaran, an authenticated data structure that eliminates disk I/O from the critical path, implements prefetching strategies, and refines Merkle tree updates through parallelization and batched proof generation techniques.

Result: AlDBaran achieves 48 million updates per second on standard machine configurations, 8 million updates/s on in-memory portable hardware, and 5 million updates/s with sub-second snapshots. It supports historical state proofs modularly.

Conclusion: AlDBaran provides state-of-the-art throughput and efficiency for authenticated blockchain databases, making it suitable for resource-constrained environments while enabling new applications through historical state proof capabilities.

Abstract: The fundamental basis for maintaining integrity within contemporary
blockchain systems is provided by authenticated databases. Our analysis
indicates that a significant portion of the approaches applied in this domain
fail to sufficiently meet the stringent requirements of systems processing
transactions at rates of multi-million TPS. AlDBaran signifies a substantial
advancement in authenticated databases. By eliminating disk I/O operations from
the critical path, implementing prefetching strategies, and refining the update
mechanism of the Merkle tree, we have engineered an authenticated data
structure capable of handling state updates efficiently at a network throughput
of 50 Gbps. This throughput capacity significantly surpasses any empirically
documented blockchain throughput, guaranteeing the ability of even the most
high-throughput blockchains to generate state commitments effectively.
  AlDBaran provides support for historical state proofs, which facilitates a
wide array of novel applications. For instance, the deployment of AlDBaran
could enable blockchains that do not currently support state commitments to
offer functionalities for light clients and/or implement rollups.
  When benchmarked against alternative authenticated data structure projects,
AlDBaran exhibits superior performance and simplicity. In particular, AlDBaran
achieves speeds of approximately 48 million updates per second using an
identical machine configuration. This characteristic renders AlDBaran an
attractive solution for resource-limited environments, as its historical data
capabilities can be modularly isolated (and deactivated), which further
enhances performance. On consumer-level portable hardware, it achieves
approximately 8 million updates/s in an in-memory setting and 5 million
updates/s with snapshots at sub-second intervals, illustrating compelling and
cost-effective scalability.

</details>


### [19] [Codes on any Cayley Graph have an Interactive Oracle Proof of Proximity](https://arxiv.org/abs/2508.10510)
*Hugo Delavenne,Louise Lallemand*

Main category: cs.CR

TL;DR: Generalizes the flowering IOPP protocol from specific (2,n)-regular Tanner codes to Cayley graph-indexed codes, preserves soundness with slight complexity improvements, enabling codes with constant rate and distance due to Cayley graphs' expansion properties.


<details>
  <summary>Details</summary>
Motivation: The paper aims to extend the efficiency and practicality of IOPPs for code-based SNARKs by generalizing the protocol to broader code families while maintaining strong soundness parameters.

Method: The authors adapt the flowering IOPP framework using Cayley graph expansion properties, which allow improved code parameters (constant rate and distance) while preserving the soundness guarantees from [DMR25].

Result: Achieves similar soundness to [DMR25] with only a mild complexity increase, enabling application to codes with constant rate and minimum distance, unlike the previous $o(1)$ distance limitation.

Conclusion: This generalization demonstrates that Cayley graph-indexed codes can maintain the security and efficiency benefits of existing IOPP protocols, paving the way for more practical code-based SNARK implementations.

Abstract: Interactive Oracle Proofs of Proximity (IOPP) are at the heart of code-based
SNARKs, a family of zeroknowledge protocols. The first and most famous one is
the FRI protocol [BBHR18a], that efficiently tests proximity to Reed-Solomon
codes. This paper generalizes the flowering IOPP introduced in [DMR25] for some
specific (2, n)-regular Tanner codes to a much broader variety of codes: any
code with symbols indexed on the edges of a Cayley graph. The flowering
protocol of [DMR25] had a soundness parameter much lower than the FRI protocol
[BCI + 23], and complexity parameters that could compete with the FRI
[BBHR18a]. The lower soundness and the absence of restriction on the base field
may lead to other practical speedups, however the codes considered in [DMR25]
have an o(1) minimum distance. The generalization proposed in this paper
preserves the soundness parameter with a slight decrease of the complexity
parameters, while allowing being applied on codes with constant rate and
constant minimum distance thanks to the good expansion properties of some
families of Cayley graphs.

</details>


### [20] [A Transformer-Based Approach for DDoS Attack Detection in IoT Networks](https://arxiv.org/abs/2508.10636)
*Sandipan Dey,Payal Santosh Kate,Vatsala Upadhyay,Abhishek Vaish*

Main category: cs.CR

TL;DR: The paper proposes using Transformer models for detecting DDoS attacks in IoT networks, demonstrating superior performance over traditional methods via self-attention mechanisms and real-world dataset validation.


<details>
  <summary>Details</summary>
Motivation: Traditional DDoS detection methods struggle with IoT's dynamic network behavior, protocol diversity (e.g., MQTT, CoAP), high traffic volume, and device variability, lacking scalability and adaptability required for effective security.

Method: A Transformer-based model employs a self-attention mechanism to extract features from network traffic data, designed to adapt to IoT environments' unique challenges through scalable sequence processing.

Result: Experiments on real-world datasets showed the Transformer model achieved higher accuracy, precision, recall, and F1-score compared to conventional machine learning techniques, emphasizing its robustness in variable IoT scenarios.

Conclusion: Transformer models effectively address DDoS attack detection in IoT networks, offering a scalable and protocol-agnostic solution with potential for real-world deployment due to their adaptability and predictive performance.

Abstract: DDoS attacks have become a major threat to the security of IoT devices and
can cause severe damage to the network infrastructure. IoT devices suffer from
the inherent problem of resource constraints and are therefore susceptible to
such resource-exhausting attacks. Traditional methods for detecting DDoS
attacks are not efficient enough to cope with the dynamic nature of IoT
networks, as well as the scalability of the attacks, diversity of protocols,
high volume of traffic, and variability in device behavior, and variability of
protocols like MQTT, CoAP, making it hard to implement security across all the
protocols. In this paper, we propose a novel approach, i.e., the use of
Transformer models, which have shown remarkable performance in natural language
processing tasks, for detecting DDoS attacks on IoT devices. The proposed model
extracts features from network traffic data and processes them using a
self-attention mechanism. Experiments conducted on a real-world dataset
demonstrate that the proposed approach outperforms traditional machine learning
techniques, which can be validated by comparing both approaches' accuracy,
precision, recall, and F1-score. The results of this study show that the
Transformer models can be an effective solution for detecting DDoS attacks on
IoT devices and have the potential to be deployed in real-world IoT
environments.

</details>


### [21] [MirGuard: Towards a Robust Provenance-based Intrusion Detection System Against Graph Manipulation Attacks](https://arxiv.org/abs/2508.10639)
*Anyuan Sang,Lu Zhou,Li Yang,Junbo Jia,Huipeng Yang,Pengbin Feng,Jianfeng Ma*

Main category: cs.CR

TL;DR: This paper introduces MirGuard, a robust anomaly detection framework combining logic-aware graph augmentation with contrastive learning to defend against semantic-preserving attacks on provenance-based intrusion detection systems.


<details>
  <summary>Details</summary>
Motivation: Learning-based provenance intrusion detection systems (PIDSes) face practical limitations due to vulnerability to graph manipulation attacks that exploit arbitrary structural perturbations while maintaining semantic validity.

Method: MirGuard employs Logic-Aware Noise Injection (LNI) to generate semantically valid multi-view graph representations through causal semantics preservation, followed by Logic-Preserving Contrastive Learning to enhance adversarial robustness.

Result: Extensive evaluations on provenance datasets demonstrate superior performance over existing detectors against diverse graph manipulation attacks, while maintaining detection accuracy and computational efficiency.

Conclusion: MirGuard represents the first targeted solution addressing graph manipulation attacks in PIDSes, offering a robust and effective framework for modern cybersecurity challenges through logic-preserving representation learning.

Abstract: Learning-based Provenance-based Intrusion Detection Systems (PIDSes) have
become essential tools for anomaly detection in host systems due to their
ability to capture rich contextual and structural information, as well as their
potential to detect unknown attacks. However, recent studies have shown that
these systems are vulnerable to graph manipulation attacks, where attackers
manipulate the graph structure to evade detection. While some previous
approaches have discussed this type of attack, none have fully addressed it
with a robust detection solution, limiting the practical applicability of
PIDSes.
  To address this challenge, we propose MirGuard, a robust anomaly detection
framework that combines logic-aware multi-view augmentation with contrastive
representation learning. Rather than applying arbitrary structural
perturbations, MirGuard introduces Logic-Aware Noise Injection (LNI) to
generate semantically valid graph views, ensuring that all augmentations
preserve the underlying causal semantics of the provenance data. These views
are then used in a Logic-Preserving Contrastive Learning framework, which
encourages the model to learn representations that are invariant to benign
transformations but sensitive to adversarial inconsistencies. Comprehensive
evaluations on multiple provenance datasets demonstrate that MirGuard
significantly outperforms state-of-the-art detectors in robustness against
various graph manipulation attacks without sacrificing detection performance
and efficiency. Our work represents the first targeted study to enhance PIDS
against such adversarial threats, providing a robust and effective solution to
modern cybersecurity challenges.

</details>


### [22] [A Novel Study on Intelligent Methods and Explainable AI for Dynamic Malware Analysis](https://arxiv.org/abs/2508.10652)
*Richa Dasila,Vatsala Upadhyay,Samo Bobek,Abhishek Vaish*

Main category: cs.CR

TL;DR: This research integrates XAI techniques with deep learning models (MLP, CNN, RNN, CNN-LSTM) to improve interpretability and trustworthiness in dynamic malware detection.


<details>
  <summary>Details</summary>
Motivation: The 'black box' nature of deep learning models limits trust and understanding in critical security applications, necessitating explainability for effective threat mitigation.

Method: The study evaluates four deep learning architectures using XAI methods for dynamic malware analysis, focusing on Metamorphic Malware detection through enhanced model transparency.

Result: The comprehensive XAI framework successfully analyzes model decision-making processes, though specific performance metrics are not detailed in the abstract.

Conclusion: By demonstrating how XAI can demystify deep learning in cybersecurity, this work provides a novel methodology to balance machine learning efficacy with humaninterpretable explanations for malware classification.

Abstract: Deep learning models are one of the security strategies, trained on extensive
datasets, and play a critical role in detecting and responding to these threats
by recognizing complex patterns in malicious code. However, the opaque nature
of these models-often described as "black boxes"-makes their decision-making
processes difficult to understand, even for their creators. This research
addresses these challenges by integrating Explainable AI (XAI) techniques to
enhance the interpretability and trustworthiness of malware detection models.
In this research, the use of Multi-Layer Perceptrons (MLP) for dynamic malware
analysis has been considered, a less explored area, and its efficacy in
detecting Metamorphic Malware, and further the effectiveness and transparency
of MLPs, CNNs, RNNs, and CNN-LSTM models in malware classification, evaluating
these models through the lens of Explainable AI (XAI). This comprehensive
approach aims to demystify the internal workings of deep learning models,
promoting a better understanding and trust in their predictive capabilities in
cybersecurity contexts. Such in-depth analysis and implementation haven't been
done to the best of our knowledge.

</details>


### [23] [Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat Intelligence](https://arxiv.org/abs/2508.10677)
*Amine Tellache,Abdelaziz Amara Korba,Amdjed Mokhtari,Horea Moldovan,Yacine Ghamri-Doudane*

Main category: cs.CR

TL;DR: This paper proposes a RAG-based framework using LLMs to enhance and automate incident response by integrating real-time Cyber Threat Intelligence, improving accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Security teams face alert fatigue, high false-positive rates, and challenges in manually analyzing fragmented CTI documents, leading to inefficiencies in threat mitigation.

Method: A hybrid retrieval mechanism combining NLP-based similarity searches in a CTI vector database with standardized queries to external CTI platforms, followed by LLM-powered response generation of context-aware mitigation strategies.

Result: Empirical validation on real-world and simulated alerts showed improved accuracy, contextualization, reduced response latency, and alleviated analyst workload compared to traditional methods.

Conclusion: LLM-driven CTI fusion demonstrates potential to advance autonomous security operations and enable intelligent, adaptive cybersecurity frameworks.

Abstract: Effective incident response (IR) is critical for mitigating cyber threats,
yet security teams are overwhelmed by alert fatigue, high false-positive rates,
and the vast volume of unstructured Cyber Threat Intelligence (CTI) documents.
While CTI holds immense potential for enriching security operations, its
extensive and fragmented nature makes manual analysis time-consuming and
resource-intensive. To bridge this gap, we introduce a novel
Retrieval-Augmented Generation (RAG)-based framework that leverages Large
Language Models (LLMs) to automate and enhance IR by integrating dynamically
retrieved CTI. Our approach introduces a hybrid retrieval mechanism that
combines NLP-based similarity searches within a CTI vector database with
standardized queries to external CTI platforms, facilitating context-aware
enrichment of security alerts. The augmented intelligence is then leveraged by
an LLM-powered response generation module, which formulates precise,
actionable, and contextually relevant incident mitigation strategies. We
propose a dual evaluation paradigm, wherein automated assessment using an
auxiliary LLM is systematically cross-validated by cybersecurity experts.
Empirical validation on real-world and simulated alerts demonstrates that our
approach enhances the accuracy, contextualization, and efficiency of IR,
alleviating analyst workload and reducing response latency. This work
underscores the potential of LLM-driven CTI fusion in advancing autonomous
security operations and establishing a foundation for intelligent, adaptive
cybersecurity frameworks.

</details>


### [24] [Searching for Privacy Risks in LLM Agents via Simulation](https://arxiv.org/abs/2508.10880)
*Yanzhe Zhang,Diyi Yang*

Main category: cs.CR

TL;DR: The paper proposes a search-based framework simulating privacy-critical interactions between LLM agents to iteratively improve attacker and defender instructions, revealing escalation paths in attack strategies and defense mechanisms.


<details>
  <summary>Details</summary>
Motivation: Malicious LLM agents can proactively exploit multi-turn conversations for sensitive data extraction, creating dynamic privacy threats that are difficult to detect through traditional manual methods.

Method: A simulation framework with three roles (data subject, sender, recipient) using parallel LLM-optimized search with multiple threads and cross-thread propagation to explore attacker-defender interaction trajectories and refine instructions systematically.

Result: Attack strategies evolve from direct requests to sophisticated impersonation tactics, while defenses progress from rule-based systems to state machines. The findings demonstrate cross-scenario and model transferability of both attacks and defenses.

Conclusion: The proposed framework provides practical utility for developing privacy-aware agents by automatically uncovering both emerging vulnerabilities and adaptive defense patterns in dynamic agent interactions.

Abstract: The widespread deployment of LLM-based agents is likely to introduce a
critical privacy threat: malicious agents that proactively engage others in
multi-turn interactions to extract sensitive information. These dynamic
dialogues enable adaptive attack strategies that can cause severe privacy
violations, yet their evolving nature makes it difficult to anticipate and
discover sophisticated vulnerabilities manually. To tackle this problem, we
present a search-based framework that alternates between improving attacker and
defender instructions by simulating privacy-critical agent interactions. Each
simulation involves three roles: data subject, data sender, and data recipient.
While the data subject's behavior is fixed, the attacker (data recipient)
attempts to extract sensitive information from the defender (data sender)
through persistent and interactive exchanges. To explore this interaction space
efficiently, our search algorithm employs LLMs as optimizers, using parallel
search with multiple threads and cross-thread propagation to analyze simulation
trajectories and iteratively propose new instructions. Through this process, we
find that attack strategies escalate from simple direct requests to
sophisticated multi-turn tactics such as impersonation and consent forgery,
while defenses advance from rule-based constraints to identity-verification
state machines. The discovered attacks and defenses transfer across diverse
scenarios and backbone models, demonstrating strong practical utility for
building privacy-aware agents.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [25] [FormalGrad: Integrating Formal Methods with Gradient-Based LLM Refinement](https://arxiv.org/abs/2508.10059)
*Yueke Zhang,Yifan Zhang,Kevin Leach,Yu Huang*

Main category: cs.SE

TL;DR: FormalGrad integrates formal methods into iterative LLM-based code generation to produce correct, robust, and efficient solutions by treating code as a differentiable variable and using textual pseudo-gradients for refinement.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) generate code lacking guarantees of correctness, robustness, and efficiency, which is problematic in high-constraint domains requiring reliable solutions.

Method: FormalGrad converts structured feedback and formal constraints into a textual pseudo-gradient, allowing code to be treated as a differentiable variable and iteratively refined by the LLM within a generation loop.

Result: The framework achieves up to a 27% absolute improvement on HumanEval and a 41% relative improvement on LiveCodeBench V6, outperforming strong baselines.

Conclusion: FormalGrad enables reliable AI-assisted software development by generating formally justified and robust code, making it suitable for high-stakes applications.

Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities
in code generation, they often produce solutions that lack guarantees of
correctness, robustness, and efficiency. The limitation is acute in domains
requiring strict constraints. FormalGrad introduces a principled framework that
integrates formal methods directly into an iterative LLM-based generation loop.
It uniquely treats code as a differentiable variable, converting structured
feedback and formal constraints into a textual pseudo-gradient. This gradient
guides the model to iteratively refine solutions, ensuring they are not only
functional but also robust and formally justified. We evaluate FormalGrad on
the HumanEval, HumanEval+, and LiveCodeBench benchmarks. Our implementation
outperforms strong baselines, achieving an absolute improvement of up to 27% on
HumanEval and a 41% relative improvement on the challenging LiveCodeBench V6.
FormalGrad generates formally justified code that is robust and efficient,
paving the way for reliable AI-assisted software development in high-stakes
applications.

</details>


### [26] [SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion](https://arxiv.org/abs/2508.10068)
*Xiaohan Chen,Zhongying Pan,Quan Feng,Yu Tian,Shuqun Yang,Mengru Wang,Lina Gong,Yuxia Geng,Piji Li,Xiang Chen*

Main category: cs.SE

TL;DR: Saracoder is a hierarchical feature-optimized retrieval framework that improves repository-level code completion by refining candidates based on semantic relationships, diversity, and cross-file symbol disambiguation, outperforming existing baselines on major benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing RAG methods for code completion rely on superficial text similarity, leading to semantic misguidance, redundant/homogeneous results, and failure to resolve external symbol ambiguity in repositories.

Method: 1) Hierarchical Feature Optimization module: refines candidates through semantic distillation, duplicate pruning, and graph-based structural similarity metric with topological edit weighting. 2) External-Aware Identifier Disambiguator: resolves cross-file symbol ambiguity via dependency analysis.

Result: Significantly outperforms existing baselines on CrossCodeEval and RepoEval-Updated benchmarks across multiple programming languages, demonstrating improved accuracy and robustness in code completion.

Conclusion: Systematic refinement of retrieval results across semantic, structural, and contextual dimensions establishes a new paradigm for repository-level code completion systems through hierarchical feature optimization.

Abstract: Retrieval-augmented generation (RAG) for repository-level code completion
commonly relies on superficial text similarity, leading to results plagued by
semantic misguidance, redundancy, and homogeneity, while also failing to
resolve external symbol ambiguity. To address these challenges, we introduce
Saracoder, a Hierarchical Feature-Optimized retrieval framework. Its core
Hierarchical Feature Optimization module systematically refines candidates by
distilling deep semantic relationships, pruning exact duplicates, assessing
structural similarity with a novel graph-based metric that weighs edits by
their topological importance, and reranking results to maximize both relevance
and diversity. Furthermore, an External-Aware Identifier Disambiguator module
accurately resolves cross-file symbol ambiguity via dependency analysis.
Extensive experiments on the challenging CrossCodeEval and RepoEval-Updated
benchmarks demonstrate that Saracoder significantly outperforms existing
baselines across multiple programming languages and models. Our work proves
that systematically refining retrieval results across multiple dimensions
provides a new paradigm for building more accurate and robust repository-level
code completion systems.

</details>


### [27] [Next Edit Prediction: Learning to Predict Code Edits from Context and Interaction History](https://arxiv.org/abs/2508.10074)
*Ruofan Lu,Yintong Huo,Meng Zhang,Yichen Li,Michael R. Lyu*

Main category: cs.SE

TL;DR: This paper proposes Next Edit Prediction, a new task to anticipate developers' subsequent code edits by jointly predicting location and content from interaction history, addressing limitations of existing code completion and chat-based editing approaches.


<details>
  <summary>Details</summary>
Motivation: Current code editing paradigms have inherent limitations: low-latency completion is restricted to the current cursor position, while chat-based editing requires developers to describe intentions in natural language, creating workflow disruptions.

Method: The authors (1) curate a high-quality supervised fine-tuning dataset and evaluation benchmark for next edit prediction, (2) perform supervised fine-tuning on multiple models, and (3) conduct comprehensive evaluations comparing fine-tuned models with baselines.

Result: The work yields novel findings from model evaluations, demonstrating the effectiveness of this new paradigm in anticipating sequential code edits beyond existing capabilities.

Conclusion: Next Edit Prediction establishes a proactive collaboration paradigm for coding assistants, enabling AI to predict and suggest sequential edits rather than just reacting to explicit instructions or confined code context.

Abstract: The rapid advancement of large language models (LLMs) has led to the
widespread adoption of AI-powered coding assistants integrated into a
development environment. On one hand, low-latency code completion offers
completion suggestions but is fundamentally constrained to the cursor's current
position. On the other hand, chat-based editing can perform complex
modifications, yet forces developers to stop their work, describe the intent in
natural language, which causes a context-switch away from the code. This
creates a suboptimal user experience, as neither paradigm proactively predicts
the developer's next edit in a sequence of related edits. To bridge this gap
and provide the seamless code edit suggestion, we introduce the task of Next
Edit Prediction, a novel task designed to infer developer intent from recent
interaction history to predict both the location and content of the subsequent
edit. Specifically, we curate a high-quality supervised fine-tuning dataset and
an evaluation benchmark for the Next Edit Prediction task. Then, we conduct
supervised fine-tuning on a series of models and performed a comprehensive
evaluation of both the fine-tuned models and other baseline models, yielding
several novel findings. This work lays the foundation for a new interaction
paradigm that proactively collaborate with developers by anticipating their
following action, rather than merely reacting to explicit instructions.

</details>


### [28] [On the synchronization between Hugging Face pre-trained language models and their upstream GitHub repository](https://arxiv.org/abs/2508.10157)
*Ajibode Adekunle,Abdul Ali Bangash,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: The paper analyzes synchronization challenges between GitHub and Hugging Face repositories for PTLMs, revealing 8 distinct patterns and their implications for model fragmentation.


<details>
  <summary>Details</summary>
Motivation: Fragmentation between code repositories (GitHub) and model distribution platforms (Hugging Face) risks exposing end users to inconsistent, outdated, or incomplete PTLMs due to misaligned release practices.

Method: Mixed-method study of 325 PTLM families (904 Hugging Face variants), analyzing commit activities across three dimensions: lag, synchronization type, and intensity to identify cross-platform coordination patterns.

Result: Identified 8 synchronization patterns, including Disperse and Sparse types, showing frequent isolated changes and platform abandonment when PTLM developers modify code/scripts on GitHub or metadata/model files on Hugging Face without cross-referencing.

Conclusion: Recognizing these synchronization patterns is essential for improving traceability and oversight in PTLM release workflows to prevent user exposure to fragmented models.

Abstract: Pretrained language models (PTLMs) have advanced natural language processing
(NLP), enabling progress in tasks like text generation and translation. Like
software package management, PTLMs are trained using code and environment
scripts in upstream repositories (e.g., GitHub, GH) and distributed as variants
via downstream platforms like Hugging Face (HF). Coordinating development
between GH and HF poses challenges such as misaligned release timelines,
inconsistent versioning, and limited reuse of PTLM variants. We conducted a
mixed-method study of 325 PTLM families (904 HF variants) to examine how commit
activities are coordinated. Our analysis reveals that GH contributors typically
make changes related to specifying the version of the model, improving code
quality, performance optimization, and dependency management within the
training scripts, while HF contributors make changes related to improving model
descriptions, data set handling, and setup required for model inference.
Furthermore, to understand the synchronization aspects of commit activities
between GH and HF, we examined three dimensions of these activities -- lag
(delay), type of synchronization, and intensity -- which together yielded eight
distinct synchronization patterns. The prevalence of partially synchronized
patterns, such as Disperse synchronization and Sparse synchronization, reveals
structural disconnects in current cross-platform release practices. These
patterns often result in isolated changes -- where improvements or fixes made
on one platform are never replicated on the other -- and in some cases,
indicate an abandonment of one repository in favor of the other. Such
fragmentation risks exposing end users to incomplete, outdated, or behaviorally
inconsistent models. Hence, recognizing these synchronization patterns is
critical for improving oversight and traceability in PTLM release workflows.

</details>


### [29] [Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart Contract Compilation Error Resolution](https://arxiv.org/abs/2508.10517)
*Likai Ye,Mengliang Li,Dehai Zhao,Jiamou Sun,Xiaoxue Ren*

Main category: cs.SE

TL;DR: The paper addresses the challenges of Solidity version evolution, which causes compilation errors in most contracts. It evaluates large language models (LLMs) for error resolution but highlights their limitations, particularly for semantic errors and prompt engineering. The study introduces SMCFIXER, a framework integrating expert knowledge retrieval and LLM-based repair, achieving a 24.24% improvement and 96.97% accuracy over baseline GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Frequent Solidity version updates lead to compilation errors, code migration complexities, and maintenance issues, with 81.68% of contracts encountering errors and 86.92% resulting in compilation failures. This necessitates effective strategies for error resolution during version migrations.

Method: The research involves an empirical analysis of Solidity version issues, evaluating both open-source (LLaMA3, DeepSeek) and closed-source (GPT-4o, GPT-3.5-turbo) LLMs for error repair. Key insights led to the development of SMCFIXER, which includes three phases: context-aware code slicing, expert knowledge retrieval from documentation, and iterative patch generation.

Result: LLMs demonstrated error repair capabilities but struggled with semantic errors and required robust prompt engineering. SMCFIXER achieved a statistically significant 24.24% improvement over GPT-4o and 96.97% accuracy on real-world datasets during Solidity version migrations.

Conclusion: The study underscores the need for domain-specific adaptation in LLM-based smart contract repair systems. SMCFIXERâ€™s framework, combining expert knowledge and iterative patch generation, effectively mitigates version evolution challenges, emphasizing the importance of tailored solutions for complex semantic errors in Solidity.

Abstract: Solidity, the dominant smart contract language for Ethereum, has rapidly
evolved with frequent version updates to enhance security, functionality, and
developer experience. However, these continual changes introduce significant
challenges, particularly in compilation errors, code migration, and
maintenance. Therefore, we conduct an empirical study to investigate the
challenges in the Solidity version evolution and reveal that 81.68% of examined
contracts encounter errors when compiled across different versions, with 86.92%
of compilation errors.
  To mitigate these challenges, we conducted a systematic evaluation of large
language models (LLMs) for resolving Solidity compilation errors during version
migrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek)
and closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these
models exhibit error repair capabilities, their effectiveness diminishes
significantly for semantic-level issues and shows strong dependency on prompt
engineering strategies. This underscores the critical need for domain-specific
adaptation in developing reliable LLM-based repair systems for smart contracts.
  Building upon these insights, we introduce SMCFIXER, a novel framework that
systematically integrates expert knowledge retrieval with LLM-based repair
mechanisms for Solidity compilation error resolution. The architecture
comprises three core phases: (1) context-aware code slicing that extracts
relevant error information; (2) expert knowledge retrieval from official
documentation; and (3) iterative patch generation for Solidity migration.
Experimental validation across Solidity version migrations demonstrates our
approach's statistically significant 24.24% improvement over baseline GPT-4o on
real-world datasets, achieving near-perfect 96.97% accuracy.

</details>


### [30] [EVOSCAT: Exploring Software Change Dynamics in Large-Scale Historical Datasets](https://arxiv.org/abs/2508.10852)
*Souhaila Serbout,Diana Carolina MuÃ±oz Hurtado,Hassan Atwi,Edoardo Riggio,Cesare Pautasso*

Main category: cs.SE

TL;DR: EvoScat is an interactive density scatterplot tool for scalable analysis of large software evolution datasets, enabling researchers to explore artifact aging, metric trends, and comparative histories across thousands of open-source artifacts.


<details>
  <summary>Details</summary>
Motivation: Chronically evolving software projects generate massive revision datasets, requiring scalable visualization tools to explore/compare artifact histories, track aging rates, and analyze metric improvements/worsenings over multi-year periods.

Method: The tool uses interactive density scatterplots with configurable history scaling/alignment, artifact sorting, and dynamic color mapping to visualize millions of events from tens of thousands of software artifacts in open-source repositories.

Result: EvoScat successfully analyzes datasets including OpenAPI descriptions, GitHub workflow definitions, and popular open-source projects, demonstrating scalability through flexible configuration options for change pace comparison, clone detection, and freshness assessment.

Conclusion: EvoScat provides an effective visualization approach for large-scale software evolution analysis by addressing temporal scalability challenges, offering researchers customized analysis capabilities through interactive dense dataset exploration.

Abstract: Long lived software projects encompass a large number of artifacts, which
undergo many revisions throughout their history. Empirical software engineering
researchers studying software evolution gather and collect datasets with
millions of events, representing changes introduced to specific artifacts. In
this paper, we propose EvoScat, a tool that attempts addressing temporal
scalability through the usage of interactive density scatterplot to provide a
global overview of large historical datasets mined from open source
repositories in a single visualization. EvoScat intents to provide researchers
with a mean to produce scalable visualizations that can help them explore and
characterize evolution datasets, as well as comparing the histories of
individual artifacts, both in terms of 1) observing how rapidly different
artifacts age over multiple-year-long time spans 2) how often metrics
associated with each artifacts tend towards an improvement or worsening. The
paper shows how the tool can be tailored to specific analysis needs (pace of
change comparison, clone detection, freshness assessment) thanks to its support
for flexible configuration of history scaling and alignment along the time
axis, artifacts sorting and interactive color mapping, enabling the analysis of
millions of events obtained by mining the histories of tens of thousands of
software artifacts. We include in this paper a gallery showcasing datasets
gathering specific artifacts (OpenAPI descriptions, GitHub workflow
definitions) across multiple repositories, as well as diving into the history
of specific popular open source projects.

</details>
