<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 15]
- [cs.SE](#cs.SE) [Total: 19]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Learning to Attack: Uncovering Privacy Risks in Sequential Data Releases](https://arxiv.org/abs/2510.24807)
*Ziyao Cui,Minxing Zhang,Jian Pei*

Main category: cs.CR

TL;DR: The paper proposes a novel attack model exploiting sequential data publishing in AI applications to compromise privacy, using a Hidden Markov Model with a reinforcement learning mechanism for trajectory data analysis, revealing existing privacy-preserving methods are insufficient as they treat each data release independently.


<details>
  <summary>Details</summary>
Motivation: Modern AI applications often deal with sensitive data, and the sequential release of data introduces privacy risks as individual releases satisfying privacy guarantees can collectively leak information when analyzed temporally. The paper addresses the need for new privacy-preserving frameworks considering temporal dependencies in such scenarios.

Method: The paper introduces a new attack framework that uses a Hidden Markov Model combined with a reinforcement learning-based bi-directional inference mechanism to model sequential dependencies in trajectory data, allowing adversaries to infer private information by using both earlier and later observations.

Result: Experiments on Geolife, Porto Taxi, and SynMob datasets demonstrate the proposed model consistently outperforms baseline approaches that treat data releases independently, showing the effectiveness of exploiting temporal correlations for privacy breach.

Conclusion: The research uncovers a fundamental privacy risk in sequential data publishing, highlighting the necessity for privacy-preserving frameworks that explicitly model temporal dependencies, such as time-aware differential privacy or strategic sequential data obfuscation methods.

Abstract: Privacy concerns have become increasingly critical in modern AI and data
science applications, where sensitive information is collected, analyzed, and
shared across diverse domains such as healthcare, finance, and mobility. While
prior research has focused on protecting privacy in a single data release, many
real-world systems operate under sequential or continuous data publishing,
where the same or related data are released over time. Such sequential
disclosures introduce new vulnerabilities, as temporal correlations across
releases may enable adversaries to infer sensitive information that remains
hidden in any individual release. In this paper, we investigate whether an
attacker can compromise privacy in sequential data releases by exploiting
dependencies between consecutive publications, even when each individual
release satisfies standard privacy guarantees. To this end, we propose a novel
attack model that captures these sequential dependencies by integrating a
Hidden Markov Model with a reinforcement learning-based bi-directional
inference mechanism. This enables the attacker to leverage both earlier and
later observations in the sequence to infer private information. We instantiate
our framework in the context of trajectory data, demonstrating how an adversary
can recover sensitive locations from sequential mobility datasets. Extensive
experiments on Geolife, Porto Taxi, and SynMob datasets show that our model
consistently outperforms baseline approaches that treat each release
independently. The results reveal a fundamental privacy risk inherent to
sequential data publishing, where individually protected releases can
collectively leak sensitive information when analyzed temporally. These
findings underscore the need for new privacy-preserving frameworks that
explicitly model temporal dependencies, such as time-aware differential privacy
or sequential data obfuscation strategies.

</details>


### [2] [S3C2 Summit 2025-03: Industry Secure Supply Chain Summit](https://arxiv.org/abs/2510.24920)
*Elizabeth Lin,Jonah Ghebremichael,William Enck,Yasemin Acar,Michel Cukier,Alexandros Kapravelos,Christian Kastner,Laurie Williams*

Main category: cs.CR

TL;DR: The paper summarizes a 2025 Secure Software Supply Chain Summit hosted by the NSF-backed Secure Software Supply Chain Center (S3C2), detailing discussions and challenges around software supply chain security across six key topics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the rising threat of software supply chain attacks by fostering collaboration and knowledge-sharing among practitioners in diverse industries and government agencies.

Method: The method involved organizing a summit with four researchers and 18 practitioners from 17 organizations, discussing six topics while using open-ended questions for each to facilitate conversation and identify challenges.

Result: The result is a report summarizing the summit's discussions, highlighting open questions and challenges for each of the six topics, which serve as a foundation for future research and collaboration.

Conclusion: The conclusion emphasizes the value of cross-industry collaboration in improving software supply chain security and the need to address the identified open challenges to strengthen resilience against cyberthreats.

Abstract: Software supply chains, while providing immense economic and software
development value, are only as strong as their weakest link. Over the past
several years, there has been an exponential increase in cyberattacks
specifically targeting vulnerable links in critical software supply chains.
These attacks disrupt the day-to-day functioning and threaten the security of
nearly everyone on the internet, from billion-dollar companies and government
agencies to hobbyist open-source developers. The ever-evolving threat of
software supply chain attacks has garnered interest from both the software
industry and US government in improving software supply chain security. On
Thursday, March 6th, 2025, four researchers from the NSF-backed Secure Software
Supply Chain Center (S3C2) conducted a Secure Software Supply Chain Summit with
a diverse set of 18 practitioners from 17 organizations. The goals of the
Summit were: (1) to enable sharing between participants from different
industries regarding practical experiences and challenges with software supply
chain security; (2) to help form new collaborations; and (3) to learn about the
challenges facing participants to inform our future research directions. The
summit consisted of discussions of six topics relevant to the government
agencies represented, including software bill of materials (SBOMs); compliance;
malicious commits; build infrastructure; culture; and large language models
(LLMs) and security. For each topic of discussion, we presented a list of
questions to participants to spark conversation. In this report, we provide a
summary of the summit. The open questions and challenges that remained after
each topic are listed at the end of each topic's section, and the initial
discussion questions for each topic are provided in the appendix.

</details>


### [3] [Hammering the Diagnosis: Rowhammer-Induced Stealthy Trojan Attacks on ViT-Based Medical Imaging](https://arxiv.org/abs/2510.24976)
*Banafsheh Saber Latibari,Najmeh Nazari,Hossein Sayadi,Houman Homayoun,Abhijit Mahalanobis*

Main category: cs.CR

TL;DR: This paper introduces Med-Hammer, a novel hardware-based attack combining Rowhammer and neural Trojans to target medical Vision Transformers, demonstrating high attack success rates on major models while remaining stealthy.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the critical vulnerability of medical Vision Transformers (ViTs) to hardware-level attacks, highlighting the potential for these models to be compromised through malicious bit flips leading to misclassification or suppression of critical diagnoses in healthcare applications.

Method: The method involves Rowhammer-induced bit flips in hardware to trigger implanted neural Trojans within ViT models, enabling targeted misclassifications or suppression of critical medical findings. The attack is evaluated using benchmark medical imaging datasets (ISIC, Brain Tumor, MedMNIST) to measure success rates and stealth, while also analyzing how architectural properties like model sparsity and attention weight distribution influence attack effectiveness.

Result: The experiments demonstrate high attack success rates of about 82.51% for MobileViT and 92.56% for SwinTransformer across multiple medical imaging datasets, with the attacks remaining difficult to detect. The study also reveals insights into how different architectural features of the models affect the performance of the Med-Hammer attack.

Conclusion: The research highlights the underexplored intersection between hardware-level faults and deep learning security in healthcare AI, emphasizing the urgent need for robust defense mechanisms at both the model and hardware levels to prevent such vulnerabilities from being exploited.

Abstract: Vision Transformers (ViTs) have emerged as powerful architectures in medical
image analysis, excelling in tasks such as disease detection, segmentation, and
classification. However, their reliance on large, attention-driven models makes
them vulnerable to hardware-level attacks. In this paper, we propose a novel
threat model referred to as Med-Hammer that combines the Rowhammer hardware
fault injection with neural Trojan attacks to compromise the integrity of
ViT-based medical imaging systems. Specifically, we demonstrate how malicious
bit flips induced via Rowhammer can trigger implanted neural Trojans, leading
to targeted misclassification or suppression of critical diagnoses (e.g.,
tumors or lesions) in medical scans. Through extensive experiments on benchmark
medical imaging datasets such as ISIC, Brain Tumor, and MedMNIST, we show that
such attacks can remain stealthy while achieving high attack success rates
about 82.51% and 92.56% in MobileViT and SwinTransformer, respectively. We
further investigate how architectural properties, such as model sparsity,
attention weight distribution, and the number of features of the layer, impact
attack effectiveness. Our findings highlight a critical and underexplored
intersection between hardware-level faults and deep learning security in
healthcare applications, underscoring the urgent need for robust defenses
spanning both model architectures and underlying hardware platforms.

</details>


### [4] [FaRAccel: FPGA-Accelerated Defense Architecture for Efficient Bit-Flip Attack Resilience in Transformer Models](https://arxiv.org/abs/2510.24985)
*Najmeh Nazari,Banafsheh Saber Latibari,Elahe Hosseini,Fatemeh Movafagh,Chongzhou Fang,Hosein Mohammadi Makrani,Kevin Immanuel Gubbi,Abhijit Mahalanobis,Setareh Rafatirad,Hossein Sayadi,Houman Homayoun*

Main category: cs.CR

TL;DR: FaRAccel is a hardware accelerator for the Forget and Rewire (FaR) methodology, improving its resilience against Bit-Flip Attacks (BFAs) on Transformer-based models while reducing performance and memory overheads.


<details>
  <summary>Details</summary>
Motivation: Existing FaR methodology, while effective against BFAs, introduces significant performance and memory costs due to runtime pathway modifications and lack of hardware optimization.

Method: FaRAccel is implemented on FPGA and features reconfigurable logic for dynamic activation rerouting, along with lightweight storage of rewiring configurations to optimize FaR operations.

Result: FaRAccel demonstrates notable reductions in FaR inference latency and enhanced energy efficiency while preserving the model's robustness against BFAs.

Conclusion: FaRAccel presents the first hardware-accelerated defense against BFAs in Transformers, addressing both algorithmic resilience and practical deployment efficiency.

Abstract: Forget and Rewire (FaR) methodology has demonstrated strong resilience
against Bit-Flip Attacks (BFAs) on Transformer-based models by obfuscating
critical parameters through dynamic rewiring of linear layers. However, the
application of FaR introduces non-negligible performance and memory overheads,
primarily due to the runtime modification of activation pathways and the lack
of hardware-level optimization. To overcome these limitations, we propose
FaRAccel, a novel hardware accelerator architecture implemented on FPGA,
specifically designed to offload and optimize FaR operations. FaRAccel
integrates reconfigurable logic for dynamic activation rerouting, and
lightweight storage of rewiring configurations, enabling low-latency inference
with minimal energy overhead. We evaluate FaRAccel across a suite of
Transformer models and demonstrate substantial reductions in FaR inference
latency and improvement in energy efficiency, while maintaining the robustness
gains of the original FaR methodology. To the best of our knowledge, this is
the first hardware-accelerated defense against BFAs in Transformers,
effectively bridging the gap between algorithmic resilience and efficient
deployment on real-world AI platforms.

</details>


### [5] [SLIP-SEC: Formalizing Secure Protocols for Model IP Protection](https://arxiv.org/abs/2510.24999)
*Racchit Jain,Satya Lokam,Yehonathan Refael,Adam Hakim,Lev Greenberg,Jay Tenenbaum*

Main category: cs.CR

TL;DR: This paper introduces SLIP, a hybrid inference protocol for securing large language models by splitting computation between trusted and untrusted devices, providing information-theoretic security against honest-but-curious adversaries and robustness against malicious ones.


<details>
  <summary>Details</summary>
Motivation: Large language models are valuable intellectual property, and deploying them on insecure devices risks model theft, necessitating secure inference protocols with proven security.

Method: The authors present a formal framework and security foundations for SLIP, using additive decompositions of weight matrices combined with masking and probabilistic verification techniques.

Result: The protocols achieve information-theoretic security against honest-but-curious adversaries and robustness against malicious adversaries with negligible soundness error.

Conclusion: The paper provides the theoretical foundations for SLIP, complementing practical validation in a companion paper, and offers a complete approach to securing LLM intellectual property through hybrid inference.

Abstract: Large Language Models (LLMs) represent valuable intellectual property (IP),
reflecting significant investments in training data, compute, and expertise.
Deploying these models on partially trusted or insecure devices introduces
substantial risk of model theft, making it essential to design inference
protocols with provable security guarantees.
  We present the formal framework and security foundations of SLIP, a hybrid
inference protocol that splits model computation between a trusted and an
untrusted resource. We define and analyze the key notions of model
decomposition and hybrid inference protocols, and introduce formal properties
including safety, correctness, efficiency, and t-soundness. We construct secure
inference protocols based on additive decompositions of weight matrices,
combined with masking and probabilistic verification techniques. We prove that
these protocols achieve information-theoretic security against
honest-but-curious adversaries, and provide robustness against malicious
adversaries with negligible soundness error.
  This paper focuses on the theoretical underpinnings of SLIP: precise
definitions, formal protocols, and proofs of security. Empirical validation and
decomposition heuristics appear in the companion SLIP paper. Together, the two
works provide a complete account of securing LLM IP via hybrid inference,
bridging both practice and theory.

</details>


### [6] [Secure Retrieval-Augmented Generation against Poisoning Attacks](https://arxiv.org/abs/2510.25025)
*Zirui Cheng,Jikai Sun,Anjun Gao,Yueyang Quan,Zhuqing Liu,Xiaohua Hu,Minghong Fang*

Main category: cs.CR

TL;DR: This paper introduces RAGuard, a defense framework for RAG systems that combats data poisoning by expanding retrieval scope and using perplexity/similarity filters. It outperforms existing methods in detecting poisoned texts, even against advanced adaptive attacks.


<details>
  <summary>Details</summary>
Motivation: Existing RAG defenses are ineffective against sophisticated data poisoning attacks, necessitating a robust, non-parametric solution to secure knowledge sources in retrieval-augmented systems.

Method: RAGuard employs two-phase filtering: (1) Expanding retrieval scope to prioritize clean texts, reducing poisoned content exposure; (2) Chunk-wise perplexity filtering to detect anomalies and text similarity filtering to identify highly similar suspicious texts.

Result: Experiments on large-scale datasets show RAGuard achieves high detection accuracy for poisoned texts, including strong adaptive attacks, with minimal impact on retrieval efficiency and model performance.

Conclusion: RAGuard effectively enhances the security of Retrieval-Augmented Generation (RAG) systems by mitigating data poisoning attacks through a non-parametric detection framework, demonstrating robustness against advanced threats.

Abstract: Large language models (LLMs) have transformed natural language processing
(NLP), enabling applications from content generation to decision support.
Retrieval-Augmented Generation (RAG) improves LLMs by incorporating external
knowledge but also introduces security risks, particularly from data poisoning,
where the attacker injects poisoned texts into the knowledge database to
manipulate system outputs. While various defenses have been proposed, they
often struggle against advanced attacks. To address this, we introduce RAGuard,
a detection framework designed to identify poisoned texts. RAGuard first
expands the retrieval scope to increase the proportion of clean texts, reducing
the likelihood of retrieving poisoned content. It then applies chunk-wise
perplexity filtering to detect abnormal variations and text similarity
filtering to flag highly similar texts. This non-parametric approach enhances
RAG security, and experiments on large-scale datasets demonstrate its
effectiveness in detecting and mitigating poisoning attacks, including strong
adaptive attacks.

</details>


### [7] [AgentCyTE: Leveraging Agentic AI to Generate Cybersecurity Training & Experimentation Scenarios](https://arxiv.org/abs/2510.25189)
*Ana M. Rodriguez,Jaime Acosta,Anantaa Kotal,Aritran Piplai*

Main category: cs.CR

TL;DR: AgentCyTE combines LLMs with schema-constrained network emulation to generate valid and realistic threat scenarios automatically.


<details>
  <summary>Details</summary>
Motivation: Creating realistic and adaptive networked threat scenarios is a core challenge in cybersecurity research and training, requiring substantial manual effort. Unconstrained LLM generation often results in invalid scenarios.

Method: AgentCyTE integrates LLM-based reasoning with deterministic, schema-constrained network emulation in an agentic feedback loop to observe, validate, and refine scenario outcomes.

Result: The hybrid approach ensures structural validity and enables scalable, data-driven experimentation for threat modeling and adaptive training.

Conclusion: AgentCyTE provides a reliable and automatic framework for generating realistic threat environments, reducing manual effort.

Abstract: Designing realistic and adaptive networked threat scenarios remains a core
challenge in cybersecurity research and training, still requiring substantial
manual effort. While large language models (LLMs) show promise for automated
synthesis, unconstrained generation often yields configurations that fail
validation or execution. We present AgentCyTE, a framework integrating
LLM-based reasoning with deterministic, schema-constrained network emulation to
generate and refine executable threat environments. Through an agentic feedback
loop, AgentCyTE observes scenario outcomes, validates correctness, and
iteratively enhances realism and consistency. This hybrid approach preserves
LLM flexibility while enforcing structural validity, enabling scalable,
data-driven experimentation and reliable scenario generation for threat
modeling and adaptive cybersecurity training. Our framework can be accessed at:
https://github.com/AnantaaKotal/AgentCyTE

</details>


### [8] [Is Protective DNS Blocking the Wild West?](https://arxiv.org/abs/2510.25352)
*David Plonka,Branden Palacio,Debbie Perouli*

Main category: cs.CR

TL;DR: The study examines how protective DNS services may work with current blocklists in a large network.


<details>
  <summary>Details</summary>
Motivation: To understand the feasibility and challenges of using protective DNS in a research and education network with many institutions.

Method: The researchers used an extensive set of real DNS queries collected over a week and tested them against freely available blocklists.

Result: Blocklists showed inconsistencies in names, goals, transparency, and origins, making it hard to compare and use them.

Conclusion: The lack of organization in blocklists poses operational challenges and risks for large-scale protective DNS implementations.

Abstract: We perform a passive measurement study investigating how a Protective DNS
service might perform in a Research & Education Network serving hundreds of
member institutions. Utilizing freely-available DNS blocklists consisting of
domain names deemed to be threats, we test hundreds of millions of users' real
DNS queries, observed over a week's time, to find which answers would be
blocked because they involve domain names that are potential threats. We find
the blocklists disorderly regarding their names, goals, transparency, and
provenance making them quite difficult to compare. Consequently, these
Protective DNS underpinnings lack organized oversight, presenting challenges
and risks in operation at scale.

</details>


### [9] [From ECU to VSOC: UDS Security Monitoring Strategies](https://arxiv.org/abs/2510.25375)
*Ali Recai Yekta,Nicolas Loza,Jens Gramm,Michael Peter Schneider,Stefan Katzenbeisser*

Main category: cs.CR

TL;DR: The paper proposes security monitoring strategies for the UDS protocol to detect cyberattacks in modern vehicles, evaluating the effectiveness of these methods against a UDS attack taxonomy and identifying gaps in current AUTOSAR standards.


<details>
  <summary>Details</summary>
Motivation: Modern vehicles are highly complex and interconnected, making them vulnerable to cyberattacks. The UDS protocol, being a key component in vehicle diagnostics, presents a critical area where security monitoring and attack detection are needed to ensure safety and integrity.

Method: The authors specify security event logging requirements, collect contextual data, and develop detection strategies. These strategies are then applied to a comprehensive taxonomy of UDS attack techniques to validate their coverage and effectiveness.

Result: The developed security monitoring strategies cover a wide range of UDS attack vectors. Additionally, the evaluation highlights the insufficient coverage of current AUTOSAR standardized security events for detecting certain UDS attacks.

Conclusion: The paper enhances the understanding of vehicle security monitoring and provides a framework for improving cybersecurity measures in automotive communication protocols, particularly the UDS protocol on which current standards are insufficient.

Abstract: Increasing complexity and connectivity of modern vehicles have heightened
their vulnerability to cyberattacks. This paper addresses security challenges
associated with the Unified Diagnostic Services (UDS) protocol, a critical
communication framework for vehicle diagnostics in the automotive industry. We
present security monitoring strategies for the UDS protocol that leverage
in-vehicle logging and remote analysis through a Vehicle Security Operations
Center (VSOC). Our approach involves specifying security event logging
requirements, contextual data collection, and the development of detection
strategies aimed at identifying UDS attack scenarios. By applying these
strategies to a comprehensive taxonomy of UDS attack techniques, we demonstrate
that our detection methods cover a wide range of potential attack vectors.
Furthermore, we assess the adequacy of current AUTOSAR standardized security
events in supporting UDS attack detection, identifying gaps in the current
standard. This work enhances the understanding of vehicle security monitoring
and provides an example for developing robust cybersecurity measures in
automotive communication protocols.

</details>


### [10] [An In-Depth Analysis of Cyber Attacks in Secured Platforms](https://arxiv.org/abs/2510.25470)
*Parick Ozoh,John K Omoniyi,Bukola Ibitoye*

Main category: cs.CR

TL;DR: This paper systematically reviews machine learning techniques for Android malware detection, highlighting challenges in handling false reviews and data requirements while evaluating methods using an Android Applications dataset.


<details>
  <summary>Details</summary>
Motivation: The surge in encryption-type ransomware threats on Android devices necessitates robust detection methods. Existing approaches face limitations in distinguishing malicious threats and handling synthetically generated false reviews that compromise detection accuracy.

Method: Conducted a comparative study of existing ML techniques through literature survey and empirical evaluation using an Android Applications dataset, measuring performance via standard accuracy metrics.

Result: Found that most detection methods require massive training data, creating practical challenges for deployed anti-malware systems. Presented comprehensive performance comparisons across different ML approaches.

Conclusion: While ML techniques show promise for malware detection, current methods face scalability and data dependency challenges. The study emphasizes the need for more efficient models that maintain accuracy despite synthetic review attacks and limited data availability.

Abstract: There is an increase in global malware threats. To address this, an
encryption-type ransomware has been introduced on the Android operating system.
The challenges associated with malicious threats in phone use have become a
pressing issue in mobile communication, disrupting user experiences and posing
significant privacy threats. This study surveys commonly used machine learning
techniques for detecting malicious threats in phones and examines their
performance. The majority of past research focuses on customer feedback and
reviews, with concerns that people might create false reviews to promote or
devalue products and services for personal gain. Hence, the development of
techniques for detecting malicious threats using machine learning has been a
key focus. This paper presents a comprehensive comparative study of current
research on the issue of malicious threats and methods for tackling these
challenges. Nevertheless, a huge amount of information is required by these
methods, presenting a challenge for developing robust, specialized automated
anti-malware systems. This research describes the Android Applications dataset,
and the accuracy of the techniques is measured using the accuracy levels of the
metrics employed in this study.

</details>


### [11] [NetEcho: From Real-World Streaming Side-Channels to Full LLM Conversation Recovery](https://arxiv.org/abs/2510.25472)
*Zheng Zhang,Guanlong Wu,Sen Deng,Shuai Wang,Yinqian Zhang*

Main category: cs.CR

TL;DR: The paper analyzes network side-channel defenses in LLM applications and proposes NetEcho, a framework to exploit vulnerabilities, showing that despite existing measures, significant information remains recoverable.


<details>
  <summary>Details</summary>
Motivation: Recent research shows encrypted traffic patterns can infer sensitive info. Current defenses like padding are insufficient. Need better understanding and solutions to enhance network traffic security.

Method: 1. Conduct systematic analysis of side-channel defenses in LLM apps (OpenAI, DeepSeek). 2. Create framework to exploit vulnerabilities (NetEcho). 3. Evaluate effectiveness on leading models (DeepSeek-v3, GPT-4o). 4. Analyze results.

Result: Even with active/passive mitigation techniques, residual network traffic information is recoverable. NetEcho can recovers avg ~70% conversation information (prompts and responses). Demonstrates limitations in current defense mechanisms.

Conclusion: Existing defense mechanisms for network side-channels are insufficient. Need to develop new approaches for network traffic security. Implications for LLM security research and future directions proposed.

Abstract: In the rapidly expanding landscape of Large Language Model (LLM)
applications, real-time output streaming has become the dominant interaction
paradigm. While this enhances user experience, recent research reveals that it
exposes a non-trivial attack surface through network side-channels. Adversaries
can exploit patterns in encrypted traffic to infer sensitive information and
reconstruct private conversations. In response, LLM providers and third-party
services are deploying defenses such as traffic padding and obfuscation to
mitigate these vulnerabilities.
  This paper starts by presenting a systematic analysis of contemporary
side-channel defenses in mainstream LLM applications, with a focus on services
from vendors like OpenAI and DeepSeek. We identify and examine seven
representative deployment scenarios, each incorporating active/passive
mitigation techniques. Despite these enhanced security measures, our
investigation uncovers significant residual information that remains vulnerable
to leakage within the network traffic.
  Building on this discovery, we introduce NetEcho, a novel, LLM-based
framework that comprehensively unleashes the network side-channel risks of
today's LLM applications. NetEcho is designed to recover entire conversations
-- including both user prompts and LLM responses -- directly from encrypted
network traffic. It features a deliberate design that ensures high-fidelity
text recovery, transferability across different deployment scenarios, and
moderate operational cost. In our evaluations on medical and legal applications
built upon leading models like DeepSeek-v3 and GPT-4o, NetEcho can recover avg
$\sim$70\% information of each conversation, demonstrating a critical
limitation in current defense mechanisms. We conclude by discussing the
implications of our findings and proposing future directions for augmenting
network traffic security.

</details>


### [12] [A Study on Privacy-Preserving Scholarship Evaluation Based on Decentralized Identity and Zero-Knowledge Proofs](https://arxiv.org/abs/2510.25477)
*Yi Chen,Bin Chen,Peichang Zhang,Da Che*

Main category: cs.CR

TL;DR: A scholarship evaluation system using DID and ZKP to protect privacy and ensure transparency. It uses off-chain ZKP aggregation and smart contract verification. Efficiency and privacy are demonstrated.


<details>
  <summary>Details</summary>
Motivation: Centralized scholarship evaluation processes expose student data to privacy risks. The need to automate scholarship evaluation while preserving data privacy.

Method: Design and implement a scholarship evaluation system leveraging decentralize identity (DID) and Zero-Knowledge Proof (ZKP) technologies. The system processes off-chain ZKP aggregations, and employs smart contracts to verify compliance with evaluation criteria without revealing raw data.

Result: The system proved to be efficient in automated scholarship evaluations and ensured high data privacy and integrity according to experimental results.

Conclusion: The proposed system can automate scholarship evaluation efficiently while preserving student privacy and data integrity, offering a new solution for the current problem.

Abstract: Traditional centralized scholarship evaluation processes typically require
students to submit detailed academic records and qualification information,
which exposes them to risks of data leakage and misuse, making it difficult to
simultaneously ensure privacy protection and transparent auditability. To
address these challenges, this paper proposes a scholarship evaluation system
based on Decentralized Identity (DID) and Zero-Knowledge Proofs (ZKP). The
system aggregates multidimensional ZKPs off-chain, and smart contracts verify
compliance with evaluation criteria without revealing raw scores or
computational details. Experimental results demonstrate that the proposed
solution not only automates the evaluation efficiently but also maximally
preserves student privacy and data integrity, offering a practical and
trustworthy technical paradigm for higher education scholarship programs.

</details>


### [13] [ZK-SenseLM: Verifiable Large-Model Wireless Sensing with Selective Abstention and Zero-Knowledge Attestation](https://arxiv.org/abs/2510.25677)
*Hasan Akgul,Mari Eplik,Javier Rojas,Aina Binti Abdullah,Pieter van der Merwe*

Main category: cs.CR

TL;DR: ZK-SenseLM combines wireless sensing with zero-knowledge proofs to provide secure, auditable inference across diverse tasks. It uses masked spectral pretraining, cross-modal alignment, and a novel proving pipeline with selective-abstention to improve accuracy, robustness, and verification without sacrificing performance, demonstrating strong practical security features.


<details>
  <summary>Details</summary>
Motivation: Existing wireless sensing systems lack verifiable security guarantees and robustness to distribution shifts. ZK-SenseLM addresses these gaps by introducing end-to-end zero-knowledge proofs for inference integrity, ensuring auditable decisions while maintaining performance in tasks like activity recognition and intrusion detection.

Method: The framework employs (1) a masked spectral pretraining encoder with phase-consistency regularization, (2) cross-modal alignment of RF features to policy tokens, (3) a calibrated selective-abstention head for uncertain decisions, and (4) a four-stage proving pipeline (C1-C4) combining feature commitment and PLONK-style proofs. Additional features include micro-batch amortization and gateway-based offloading for low-power devices.

Result: The system achieves improved macro-F1 scores and calibration across activity, presence, respiratory proxy, and RF fingerprinting tasks. It demonstrates favorable coverage-risk trade-offs under perturbations and effectively rejects tampering and replays using compact proofs (verification latency unmentioned) with fast verification. The selective-abstention mechanism prevents unsafe actions during distribution shifts.

Conclusion: ZK-SenseLM is a secure and auditable wireless sensing framework that successfully integrates masked spectral pretraining, cross-modal alignment, and end-to-end zero-knowledge proofs. It addresses security challenges in wireless sensing systems while maintaining verifiability through methods like selective-abstention heads and calibrated risk-coverage integration. The system's design improves inference accuracy, robustness to perturbations, and tamper detection, with practical deployment features like micro-batch proving and federated learning compatibility.

Abstract: ZK-SenseLM is a secure and auditable wireless sensing framework that pairs a
large-model encoder for Wi-Fi channel state information (and optionally mmWave
radar or RFID) with a policy-grounded decision layer and end-to-end
zero-knowledge proofs of inference. The encoder uses masked spectral
pretraining with phase-consistency regularization, plus a light cross-modal
alignment that ties RF features to compact, human-interpretable policy tokens.
To reduce unsafe actions under distribution shift, we add a calibrated
selective-abstention head; the chosen risk-coverage operating point is
registered and bound into the proof. We implement a four-stage proving
pipeline: (C1) feature sanity and commitment, (C2) threshold and version
binding, (C3) time-window binding, and (C4) PLONK-style proofs that the
quantized network, given the committed window, produced the logged action and
confidence. Micro-batched proving amortizes cost across adjacent windows, and a
gateway option offloads proofs from low-power devices. The system integrates
with differentially private federated learning and on-device personalization
without weakening verifiability: model hashes and the registered threshold are
part of each public statement. Across activity, presence or intrusion,
respiratory proxy, and RF fingerprinting tasks, ZK-SenseLM improves macro-F1
and calibration, yields favorable coverage-risk curves under perturbations, and
rejects tamper and replay with compact proofs and fast verification.

</details>


### [14] [Model Inversion Attacks Meet Cryptographic Fuzzy Extractors](https://arxiv.org/abs/2510.25687)
*Mallika Prabhakar,Louise Xu,Prateek Saxena*

Main category: cs.CR

TL;DR: This paper addresses model inversion attacks in ML-based face authentication by formalizing defense properties and introducing L2FE-Hash, a new secure fuzzy extractor that neutralizes existing and new attacks without re-training.


<details>
  <summary>Details</summary>
Motivation: Model inversion attacks threaten privacy in ML applications like face authentication. Existing defenses lack systematic evaluation, and current fuzzy extractors are insecure, allowing high success rate attacks such as PIPE.

Method: The authors formalize the desired properties of a robust defense against model inversion attacks. They analyze existing fuzzy extractors for insecurity using a new PIPE attack and propose L2FE-Hash. The new method supports standard Euclidean distance comparators and is evaluated for security and accuracy under a full breach threat model.

Result: L2FE-Hash nullifies prior and new inversion attacks with over 89% success rates. It achieves usable accuracy in face authentication and provides attack-agnostic security without needing re-training of the ML model.

Conclusion: The paper introduces a provably secure and accurate fuzzy extractor for ML applications, offering strong defense against model inversion attacks through novel design and validation, even in the threat of full data breach.

Abstract: Model inversion attacks pose an open challenge to privacy-sensitive
applications that use machine learning (ML) models. For example, face
authentication systems use modern ML models to compute embedding vectors from
face images of the enrolled users and store them. If leaked, inversion attacks
can accurately reconstruct user faces from the leaked vectors. There is no
systematic characterization of properties needed in an ideal defense against
model inversion, even for the canonical example application of a face
authentication system susceptible to data breaches, despite a decade of
best-effort solutions.
  In this paper, we formalize the desired properties of a provably strong
defense against model inversion and connect it, for the first time, to the
cryptographic concept of fuzzy extractors. We further show that existing fuzzy
extractors are insecure for use in ML-based face authentication. We do so
through a new model inversion attack called PIPE, which achieves a success rate
of over 89% in most cases against prior schemes. We then propose L2FE-Hash, the
first candidate fuzzy extractor which supports standard Euclidean distance
comparators as needed in many ML-based applications, including face
authentication. We formally characterize its computational security guarantees,
even in the extreme threat model of full breach of stored secrets, and
empirically show its usable accuracy in face authentication for practical face
distributions. It offers attack-agnostic security without requiring any
re-training of the ML model it protects. Empirically, it nullifies both prior
state-of-the-art inversion attacks as well as our new PIPE attack.

</details>


### [15] [Exact zCDP Characterizations for Fundamental Differentially Private Mechanisms](https://arxiv.org/abs/2510.25746)
*Charlie Harrison,Pasin Manurangsi*

Main category: cs.CR

TL;DR: This work closes gaps in zCDP bounds for key DP mechanisms by proving tight characterizations, including resolving a conjecture for the Laplace mechanism and deriving results for other widely used algorithms.


<details>
  <summary>Details</summary>
Motivation: Existing zCDP conversion bounds for worst-case mechanisms do not reflect stronger privacy guarantees of common algorithms, motivating the derivation of tighter, mechanism-specific characterizations.

Method: The authors derive and prove tight zCDP bounds via mathematical analysis, addressing mechanisms like Laplace, discrete Laplace, k-Randomized Response (k ≤ 6), RAPPOR, and the worst-case bounded range mechanism.

Result: The paper confirms a conjecture for the ε-DP Laplace mechanism's zCDP bound (ε + e^{-ε} - 1) and provides tight bounds for discrete Laplace, k-Randomized Response (k ≤ 6), RAPPOR, and the worst-case bounded range mechanism.

Conclusion: The paper establishes tight zCDP characterizations for several fundamental DP mechanisms, improving understanding and application of privacy guarantees through optimal bounds.

Abstract: Zero-concentrated differential privacy (zCDP) is a variant of differential
privacy (DP) that is widely used partly thanks to its nice composition
property. While a tight conversion from $\epsilon$-DP to zCDP exists for the
worst-case mechanism, many common algorithms satisfy stronger guarantees. In
this work, we derive tight zCDP characterizations for several fundamental
mechanisms. We prove that the tight zCDP bound for the $\epsilon$-DP Laplace
mechanism is exactly $\epsilon + e^{-\epsilon} - 1$, confirming a recent
conjecture by Wang (2022). We further provide tight bounds for the discrete
Laplace mechanism, $k$-Randomized Response (for $k \leq 6$), and RAPPOR.
Lastly, we also provide a tight zCDP bound for the worst case bounded range
mechanism.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [16] [Beyond Function-Level Search: Repository-Aware Dual-Encoder Code Retrieval with Adversarial Verification](https://arxiv.org/abs/2510.24749)
*Aofan Liu,Shiyuan Song,Haoxuan Li,Cehao Yang,Yiyan Qi*

Main category: cs.SE

TL;DR: The paper introduces RepoAlign-Bench, a new benchmark for repository-level code retrieval under change requests, and proposes ReflectCode, an adversarial reflection augmented dual-tower model that enhances contextual relevance in code retrieval with significant accuracy improvements.\n


<details>
  <summary>Details</summary>
Motivation: The existing function-level code retrieval systems lack the ability to interpret cross-component change intents, which is crucial for modern, complex codebases. The paper is motivated by the need to improve contextual relevance in retrieving code for specific change requests.\n

Method: The authors propose RepoAlign-Bench, the first benchmark explicitly for evaluating repository-level code retrieval scenarios involving change requests. Additionally, they present ReflectCode, a dual-tower architecture that uses adversarial reflection augmentation, which includes a code_encoder and a doc_encoder. ReflectCode dynamically integrates syntactic patterns, function dependencies, and semantic intents through large language models to enhance code retrieval.\n

Result: RepoAlign-Bench provides 52k annotated instances for evaluating repository-level retrieval. ReflectCode outperforms state-of-the-art baselines by 12.2% in Top-5 Accuracy and 7.1% in Recall, validating its effectiveness in context-aware code retrieval.\n

Conclusion: The paper concludes that the proposed approaches open a new research direction for enhancing code retrieval by focusing on repository-level change intent understanding and context-aware retrieval mechanisms.\n

Abstract: The escalating complexity of modern codebases has intensified the need for
retrieval systems capable of interpreting cross-component change intents, a
capability fundamentally absent in conventional function-level search
paradigms. While recent studies have improved the alignment between natural
language queries and code snippets, retrieving contextually relevant code for
specific change requests remains largely underexplored. To address this gap, we
introduce RepoAlign-Bench, the first benchmark specifically designed to
evaluate repository-level code retrieval under change request driven scenarios,
encompassing 52k annotated instances. This benchmark shifts the retrieval
paradigm from function-centric matching to holistic repository-level reasoning.
Furthermore, we propose ReflectCode, an adversarial reflection augmented
dual-tower architecture featuring disentangled code_encoder and doc_encoder
components. ReflectCode dynamically integrates syntactic patterns, function
dependencies, and semantic expansion intents through large language model
guided reflection. Comprehensive experiments demonstrate that ReflectCode
achieves 12.2% improvement in Top-5 Accuracy and 7.1% in Recall over
state-of-the-art baselines, establishing a new direction for context-aware code
retrieval.

</details>


### [17] [Compiler.next: A Search-Based Compiler to Power the AI-Native Future of Software Engineering](https://arxiv.org/abs/2510.24799)
*Filipe R. Cogo,Gustavo A. Oliva,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: generate a too long; didn't read summary


<details>
  <summary>Details</summary>
Motivation: describe the motivation in this paper

Method: method of this paper

Result: result of this paper

Conclusion: conclusion of this paper

Abstract: The rapid advancement of AI-assisted software engineering has brought
transformative potential to the field of software engineering, but existing
tools and paradigms remain limited by cognitive overload, inefficient tool
integration, and the narrow capabilities of AI copilots. In response, we
propose Compiler.next, a novel search-based compiler designed to enable the
seamless evolution of AI-native software systems as part of the emerging
Software Engineering 3.0 era. Unlike traditional static compilers,
Compiler.next takes human-written intents and automatically generates working
software by searching for an optimal solution. This process involves dynamic
optimization of cognitive architectures and their constituents (e.g., prompts,
foundation model configurations, and system parameters) while finding the
optimal trade-off between several objectives, such as accuracy, cost, and
latency. This paper outlines the architecture of Compiler.next and positions it
as a cornerstone in democratizing software development by lowering the
technical barrier for non-experts, enabling scalable, adaptable, and reliable
AI-powered software. We present a roadmap to address the core challenges in
intent compilation, including developing quality programming constructs,
effective search heuristics, reproducibility, and interoperability between
compilers. Our vision lays the groundwork for fully automated, search-driven
software development, fostering faster innovation and more efficient AI-driven
systems.

</details>


### [18] [A Roadmap for Tamed Interactions with Large Language Models](https://arxiv.org/abs/2510.24819)
*Vincenzo Scotti,Jan Keim,Tobias Hey,Andreas Metzger,Anne Koziolek,Raffaela Mirandola*

Main category: cs.SE

TL;DR: The paper proposes a domain-specific scripting language for LLMs to address their reliability issues by enabling controlled and structured interactions, and integrating formal SE practices.


<details>
  <summary>Details</summary>
Motivation: Despite the impressive applications of Large Language Models (LLMs), their unreliability, tendency to produce faulty or hallucinated content, and lack of tools for ensuring trustworthiness in workflows and pipelines hinder their adoption.

Method: The authors present a vision for an LLM Scripting Language (LSL), a Domain Specific Language (DSL) that allows scripting interactions with LLMs, focusing on controlling outputs and integrating with Software Engineering (SE) techniques like verification and validation.

Result: They outline a vision for LSL that could make interactions with LLMs programmable, reliable, and explainable, while decoupling the scripting from implementation and training details.

Conclusion: LSL has the potential to bridge the gap between AI and SE, improving the robustness and trustworthiness of LLM-based applications, thus enabling them to be used more effectively in software systems.

Abstract: We are witnessing a bloom of AI-powered software driven by Large Language
Models (LLMs). Although the applications of these LLMs are impressive and
seemingly countless, their unreliability hinders adoption. In fact, the
tendency of LLMs to produce faulty or hallucinated content makes them
unsuitable for automating workflows and pipelines. In this regard, Software
Engineering (SE) provides valuable support, offering a wide range of formal
tools to specify, verify, and validate software behaviour. Such SE tools can be
applied to define constraints over LLM outputs and, consequently, offer
stronger guarantees on the generated content. In this paper, we argue that the
development of a Domain Specific Language (DSL) for scripting interactions with
LLMs using an LLM Scripting Language (LSL) may be key to improve AI-based
applications. Currently, LLMs and LLM-based software still lack reliability,
robustness, and trustworthiness, and the tools or frameworks to cope with these
issues suffer from fragmentation. In this paper, we present our vision of LSL.
With LSL, we aim to address the limitations above by exploring ways to control
LLM outputs, enforce structure in interactions, and integrate these aspects
with verification, validation, and explainability. Our goal is to make LLM
interaction programmable and decoupled from training or implementation.

</details>


### [19] [VeriStruct: AI-assisted Automated Verification of Data-Structure Modules in Verus](https://arxiv.org/abs/2510.25015)
*Chuyue Sun,Yican Sun,Daneshvar Amrollahi,Ethan Zhang,Shuvendu Lahiri,Shan Lu,David Dill,Clark Barrett*

Main category: cs.SE

TL;DR: VeriStruct is a framework that extends AI-assisted verification to complex data structures, using a planner and error correction to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Current AI-assisted verification is limited to single functions and struggles with data structure modules due to annotation syntax and semantic misunderstandings.

Method: VeriStruct uses a planner module for systematic generation of abstractions and a repair stage to correct annotation errors via syntax-guided prompts.

Result: VeriStruct successfully verified 99.2% of functions in 11 Rust modules, succeeding in 10 out of 11 modules.

Conclusion: VeriStruct represents a significant progress in automatic AI-assisted formal verification for complex data structures.

Abstract: We introduce VeriStruct, a novel framework that extends AI-assisted automated
verification from single functions to more complex data structure modules in
Verus. VeriStruct employs a planner module to orchestrate the systematic
generation of abstractions, type invariants, specifications, and proof code. To
address the challenge that LLMs often misunderstand Verus' annotation syntax
and verification-specific semantics, VeriStruct embeds syntax guidance within
prompts and includes a repair stage to automatically correct annotation errors.
In an evaluation on eleven Rust data structure modules, VeriStruct succeeds on
ten of the eleven, successfully verifying 128 out of 129 functions (99.2%) in
total. These results represent an important step toward the goal of automatic
AI-assisted formal verification.

</details>


### [20] [Towards Human-AI Synergy in Requirements Engineering: A Framework and Preliminary Study](https://arxiv.org/abs/2510.25016)
*Mateen Ahmed Abbasi,Petri Ihantola,Tommi Mikkonen,Niko Mäkitalo*

Main category: cs.SE

TL;DR: This paper introduces HARE-SM, a framework integrating AI and human oversight to enhance requirements engineering while addressing ethical concerns.


<details>
  <summary>Details</summary>
Motivation: Traditional RE is labor-intensive and error-prone, and while AI offers solutions, challenges like bias and explainability need addressing.

Method: The study develops the HARE-SM framework, using a multi-phase methodology involving dataset preparation, AI model fine-tuning, and collaborative workflow design.

Result: The paper presents the HARE-SM conceptual framework and an early prototype implementation.

Conclusion: HARE-SM provides a research agenda and design direction for ethically integrating AI in RE, promoting collaboration between humans and AI.

Abstract: The future of Requirements Engineering (RE) is increasingly driven by
artificial intelligence (AI), reshaping how we elicit, analyze, and validate
requirements. Traditional RE is based on labor-intensive manual processes prone
to errors and complexity. AI-powered approaches, specifically large language
models (LLMs), natural language processing (NLP), and generative AI, offer
transformative solutions and reduce inefficiencies. However, the use of AI in
RE also brings challenges like algorithmic bias, lack of explainability, and
ethical concerns related to automation. To address these issues, this study
introduces the Human-AI RE Synergy Model (HARE-SM), a conceptual framework that
integrates AI-driven analysis with human oversight to improve requirements
elicitation, analysis, and validation. The model emphasizes ethical AI use
through transparency, explainability, and bias mitigation. We outline a
multi-phase research methodology focused on preparing RE datasets, fine-tuning
AI models, and designing collaborative human-AI workflows. This preliminary
study presents the conceptual framework and early-stage prototype
implementation, establishing a research agenda and practical design direction
for applying intelligent data science techniques to semi-structured and
unstructured RE data in collaborative environments.

</details>


### [21] [Automating Benchmark Design](https://arxiv.org/abs/2510.25039)
*Amanda Dsouza,Harit Vishwakarma,Zhengyang Qi,Justin Bauer,Derek Pham,Thomas Walshe,Armin Parchami,Frederic Sala,Paroma Varma*

Main category: cs.SE

TL;DR: BeTaL is a dynamic benchmark framework that uses LLMs to automate creating challenging, realistic benchmarks with controllable difficulty levels, outperforming existing methods by 2-4x.


<details>
  <summary>Details</summary>
Motivation: Static benchmarks lose effectiveness over time, while manual dynamic benchmarks are costly. Evaluators need a scalable way to generate benchmarks that match model capabilities as they evolve.

Method: The framework uses LLMs to explore parameterized benchmark templates, combining environment design principles with automated reasoning to achieve target properties like difficulty and realism.

Result: BeTaL created benchmarks with average 5.3-13.2\% deviation from target difficulty levels, compared to 11.9-29.4\% for baselines. Generated two new benchmarks and enhanced $\tau\$-bench.

Conclusion: BeTaL provides efficient dynamic benchmark generation over existing solutions, enabling rigorous and timely evaluation of evolving LLMs through its LLM-mediated parameter space exploration.

Abstract: The rapid progress and widespread deployment of LLMs and LLM-powered agents
has outpaced our ability to evaluate them. Hand-crafted, static benchmarks are
the primary tool for assessing model capabilities, but these quickly become
saturated. In contrast, dynamic benchmarks evolve alongside the models they
evaluate, but are expensive to create and continuously update. To address these
challenges, we develop BeTaL (Benchmark Tuning with an LLM-in-the-loop), a
framework that leverages environment design principles to automate the process
of dynamic benchmark design. BeTaL works by parameterizing key design choices
in base benchmark templates and uses LLMs to reason through the resulting
parameter space to obtain target properties (such as difficulty and realism) in
a cost-efficient manner. We validate this approach on its ability to create
benchmarks with desired difficulty levels. Using BeTaL, we create two new
benchmarks and extend a popular agentic benchmark $\tau$-bench. Extensive
evaluation on these three tasks and multiple target difficulty levels shows
that BeTaL produces benchmarks much closer to the desired difficulty, with
average deviations ranging from 5.3% to 13.2% -- a 2-4x improvement over the
baselines.

</details>


### [22] [Same Same But Different: Preventing Refactoring Attacks on Software Plagiarism Detection](https://arxiv.org/abs/2510.25057)
*Robin Maisch,Larissa Schmid,Timur Sağlam,Nils Niehues*

Main category: cs.SE

TL;DR: A new framework using code property graphs to detect refactored plagiarism in student programming submissions.


<details>
  <summary>Details</summary>
Motivation: Current plagiarism detection systems are vulnerable to refactoring-based obfuscation techniques as they can't effectively track structural modifications that maintain program behavior.

Method: The method employs code property graphs and graph transformations to identify suspicious similarities despite structural changes.

Result: The framework achieves a significant enhancement in detecting plagiarized code on real-world student submissions obfuscated with algorithmic and AI-based methods.

Conclusion: The proposed framework provides an effective and extensible solution against refactoring-based code obfuscation to improve plagiarism detection in educational settings.

Abstract: Plagiarism detection in programming education faces growing challenges due to
increasingly sophisticated obfuscation techniques, particularly automated
refactoring-based attacks. While code plagiarism detection systems used in
education practice are resilient against basic obfuscation, they struggle
against structural modifications that preserve program behavior, especially
caused by refactoring-based obfuscation. This paper presents a novel and
extensible framework that enhances state-of-the-art detectors by leveraging
code property graphs and graph transformations to counteract refactoring-based
obfuscation. Our comprehensive evaluation of real-world student submissions,
obfuscated using both algorithmic and AI-based obfuscation attacks,
demonstrates a significant improvement in detecting plagiarized code.

</details>


### [23] [Adaptive Proof Refinement with LLM-Guided Strategy Selection](https://arxiv.org/abs/2510.25103)
*Minghai Lu,Zhe Zhou,Danning Xie,Songlin Jia,Benjamin Delaware,Tianyi Zhang*

Main category: cs.SE

TL;DR: The paper presents Adapt, a proof refinement framework that uses an LLM-guided decision-maker to dynamically select refinement strategies, resulting in significant improvements in proving theorems compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Formal verification via theorem proving requires significant manual effort and expertise, and current LLM-based methods for proof generation often produce incorrect proofs and use fixed refinement strategies that do not adapt to specific proof issues.

Method: Adapt is a novel framework that employs an LLM-guided decision-maker to dynamically choose the most suitable refinement strategy based on the proof assistant's state and the context of an incorrect proof.

Result: Adapt outperforms existing methods on two benchmarks by proving 16.63% and 18.58% more theorems, respectively. It also generalizes well across five different LLMs and demonstrates the effectiveness of its components through ablation studies.

Conclusion: The paper concludes that Adapt provides a significant improvement in proof refinement by dynamically selecting strategies, and it offers insights into the design of decision-makers for such frameworks.

Abstract: Formal verification via theorem proving enables the expressive specification
and rigorous proof of software correctness, but it is difficult to scale due to
the significant manual effort and expertise required. While Large Language
Models (LLMs) show potential in proof generation, they frequently produce
incorrect proofs on the first attempt and require additional strategies for
iterative refinement. However, existing approaches employ fixed refinement
strategies and cannot dynamically choose an effective strategy based on the
particular issues in a generated proof, which limits their performance. To
overcome this limitation, we introduce Adapt, a novel proof refinement
framework that leverages an LLM-guided decision-maker to dynamically select a
suitable refinement strategy according to the state of the proof assistant and
available context of an incorrect proof. We evaluate Adapt on two benchmarks
against four existing methods and find that it significantly outperforms the
best baseline on both by proving 16.63% and 18.58% more theorems, respectively.
Furthermore, we demonstrate Adapt's generalizability by evaluating it across
five different LLMs. We also conduct ablation studies to measure the
contribution of each component and compare the trade-offs of alternative
decision-maker designs.

</details>


### [24] [Automated Program Repair Based on REST API Specifications Using Large Language Models](https://arxiv.org/abs/2510.25148)
*Katsuki Yamagishi,Norihiro Yoshida,Erina Makihara,Katsuro Inoue*

Main category: cs.SE

TL;DR: dcFix automatically detects and repairs REST API misuses in client programs using LLMs, improving accuracy over baselines.


<details>
  <summary>Details</summary>
Motivation: Developers struggle with diagnosing REST API specification violations due to vague error messages during testing, leading to time-consuming trial and error fixes.

Method: Integrates non-conforming code with API specs in LLM prompts to generate corrections, leveraging Large Language Models for automatic repair.

Result: Evaluation shows dcFix enhances detection and repair effectiveness, outperforming baseline methods with non-compliant code integrated prompts.

Conclusion: dcFix offers an effective solution for automatic REST API misuse detection and repair, utilizing better prompt engineering to improve LLM output.

Abstract: Many cloud services provide REST API accessible to client applications.
However, developers often identify specification violations only during
testing, as error messages typically lack the detail necessary for effective
diagnosis. Consequently, debugging requires trial and error. This study
proposes dcFix, a method for detecting and automatically repairing REST API
misuses in client programs. In particular, dcFix identifies non-conforming code
fragments, integrates them with the relevant API specifications into prompts,
and leverages a Large Language Model (LLM) to produce the corrected code. Our
evaluation demonstrates that dcFix accurately detects misuse and outperforms
the baseline approach, in which prompts to the LLM omit any indication of code
fragments non conforming to REST API specifications.

</details>


### [25] [Optimizing Knowledge Utilization for Multi-Intent Comment Generation with Large Language Models](https://arxiv.org/abs/2510.25195)
*Shuochuan Li,Zan Wang,Xiaoning Du,Zhuo Wu,Jiuqiao Yu,Junjie Chen*

Main category: cs.SE

TL;DR: The paper proposes KUMIC, a framework for multi-intent comment generation using in-context learning and CoT, which improves LLM performance in generating intent-specific comments with better code-comment consistency.


<details>
  <summary>Details</summary>
Motivation: Multi-intent comment generation is crucial as generic summaries fail to address diverse developer needs. However, existing LLM-based methods struggle to link different aspects of code and comments effectively when demonstrations are limited.

Method: The authors introduce KUMIC, which leverages in-context learning with a Chain-of-Thought (CoT) approach. It features a retrieval mechanism for code-comment aligned examples and a knowledge chain that connects code to intent-specific statements for precise comment generation.

Result: KUMIC outperforms state-of-the-art methods across four metrics (BLEU, METEOR, ROUGE-L, SBERT) with improvements of 14.49%, 22.41%, 20.72%, and 12.94%, respectively, showing significant gains in code-comment consistency.

Conclusion: The paper concludes that KUMIC effectively addresses the limitations of LLM-based multi-intent comment generation through a structured CoT approach and retrieval-based knowledge chaining, resulting in superior performance compared to existing methods.

Abstract: Code comment generation aims to produce a generic overview of a code snippet,
helping developers understand and maintain code. However, generic summaries
alone are insufficient to meet the diverse needs of practitioners; for example,
developers expect the implementation insights to be presented in an untangled
manner, while users seek clear usage instructions. This highlights the
necessity of multi-intent comment generation. With the widespread adoption of
Large Language Models (LLMs) for code-related tasks, these models have been
leveraged to tackle the challenge of multi-intent comment generation. Despite
their successes, state-of-the-art LLM-based approaches often struggle to
construct correct relationships among intents, code, and comments within a
smaller number of demonstration examples. To mitigate this issue, we propose a
framework named KUMIC for multi-intent comment generation. Built upon
in-context learning, KUMIC leverages Chain-of-Thought (CoT) to optimize
knowledge utilization for LLMs to generate intent-specific comments.
Specifically, KUMIC first designs a retrieval mechanism to obtain similar
demonstration examples, which exhibit high code-comment consistency. Then,
KUMIC leverages CoT to guide LLMs to focus on statements facilitating the
derivation of code comments aligned with specific intents. In this context,
KUMIC constructs a mapping knowledge chain, linking code to intent-specific
statements to comments, which enables LLMs to follow similar reasoning steps
when generating the desired comments. We conduct extensive experiments to
evaluate KUMIC, and the results demonstrate that KUMIC outperforms
state-of-the-art baselines by 14.49\%, 22.41\%, 20.72\%, and 12.94\% in terms
of BLEU, METEOR, ROUGE-L, and SBERT, respectively.

</details>


### [26] [TECS/Rust-OE: Optimizing Exclusive Control in Rust-based Component Systems for Embedded Devices](https://arxiv.org/abs/2510.25242)
*Nao Yoshimura,Hiroshi Oyama,Takuya Azumi*

Main category: cs.SE

TL;DR: TECS/Rust has limitations because of excessive exclusive control mechanisms, which make performance degradation. In order to solve this, TECS/Rust-OE is proposed, which uses real-time OS exclusive control mechanisms and memory-safe CBD framework utilizing call flows. And the evaluations show that it has good performance.


<details>
  <summary>Details</summary>
Motivation: The diversification of functionalities and the development of the IoT are making embedded systems larger and more complex in structure. Ensuring system reliability, especially in terms of security, necessitates selecting an appropriate programming language.

Method: Proposing TECS/Rust-OE, a memory-safe CBD framework utilizing call flows, which applies the exclusive control of real-time OS to optimize performance. Rust code is automatically generated based on component descriptions.

Result: The evaluations demonstrate reduced overhead due to optimized exclusion control and high reusability of the generated code.

Conclusion: The TECS/Rust-OE was designed to alleviate the problem of performance degradation which is caused by the excessive exclusive controls of TECS/Rust. Through using call flows and Rust code automatic generation, it has been confirmed that it has good performance and high reusability.

Abstract: The diversification of functionalities and the development of the IoT are
making embedded systems larger and more complex in structure. Ensuring system
reliability, especially in terms of security, necessitates selecting an
appropriate programming language. As part of existing research, TECS/Rust has
been proposed as a framework that combines Rust and component-based development
(CBD) to enable scalable system design and enhanced reliability. This framework
represents system structures using static mutable variables, but excessive
exclusive controls applied to ensure thread safety have led to performance
degradation. This paper proposes TECS/Rust-OE, a memory-safe CBD framework
utilizing call flows to address these limitations. The proposed Rust code
leverages real-time OS exclusive control mechanisms, optimizing performance
without compromising reusability. Rust code is automatically generated based on
component descriptions. Evaluations demonstrate reduced overhead due to
optimized exclusion control and high reusability of the generated code.

</details>


### [27] [TECS/Rust: Memory-safe Component Framework for Embedded Systems](https://arxiv.org/abs/2510.25270)
*Nao Yoshimura,Hiroshi Oyama,Takuya Azumi*

Main category: cs.SE

TL;DR: The paper introduces TECS/Rust, a memory-safe framework for component-based development in embedded systems, leveraging Rust's compiler features to automate code generation and ensure safety without significant performance overhead.


<details>
  <summary>Details</summary>
Motivation: Embedded systems are becoming more complex, requiring component-based development (CBD) for better manageability and reuse. However, traditional C-based CBD is prone to memory vulnerabilities, which motivates the need for a safer alternative.

Method: The authors propose TECS/Rust, a Rust-based CBD framework for embedded systems. It uses Rust's compile-time memory safety features (lifetime and borrowing) to eliminate memory-related issues. The framework automates Rust code generation for components and is compatible with real-time operating systems.

Result: The framework generates a significant portion of the actual code, and the execution time difference between the generated and manually coded versions is minimal, showing negligible overhead.

Conclusion: TECS/Rust effectively provides memory safety in component-based embedded systems while maintaining flexibility and performance, offering a promising alternative to traditional C-based CBD methods.

Abstract: As embedded systems grow in complexity and scale due to increased functional
diversity, component-based development (CBD) emerges as a solution to
streamline their architecture and enhance functionality reuse. CBD typically
utilizes the C programming language for its direct hardware access and
low-level operations, despite its susceptibility to memory-related issues. To
address these concerns, this paper proposes TECS/Rust, a Rust-based framework
specifically designed for TECS, which is a component framework for embedded
systems. It leverages Rust's compile-time memory-safe features, such as
lifetime and borrowing, to mitigate memory vulnerabilities common with C. The
proposed framework not only ensures memory safety but also maintains the
flexibility of CBD, automates Rust code generation for CBD components, and
supports efficient integration with real-time operating systems. An evaluation
of the amount of generated code indicates that the code generated by this paper
framework accounts for a large percentage of the actual code. Compared to code
developed without the proposed framework, the difference in execution time is
minimal, indicating that the overhead introduced by the proposed framework is
negligible.

</details>


### [28] [Understanding the Characteristics of LLM-Generated Property-Based Tests in Exploring Edge Cases](https://arxiv.org/abs/2510.25297)
*Hidetake Tanaka,Haruto Tanaka,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: Combining PBT and EBT enhances LLM code testing by 12.5% detection, showing complementary strengths in edge/boundary case identification


<details>
  <summary>Details</summary>
Motivation: LLM-generated code quality requires better testing methods. Traditional Example-based Testing (EBT) frequently misses edge cases, necessitating investigation into alternative approaches like Property-based Testing (PBT).

Method: Analyzed 16 HumanEval problems with failing solutions using Claude-4-sonnet to generate PBT and EBT test codes. Compared individual and combined bug detection rates (68.75% vs. 81.25%) while characterizing method-specific strengths.

Result: Combined PBT+EBT achieved 81.25% bug detection (vs. 68.75% individually). PBT excelled at performance issues/edge cases; EBT identified specific boundary conditions. Demonstrated complementary testing characteristics.

Conclusion: A hybrid approach combining Property-based Testing (PBT) and Example-based Testing (EBT) significantly improves the reliability of LLM-generated code by leveraging complementary strengths in detecting edge cases and boundary conditions.

Abstract: As Large Language Models (LLMs) increasingly generate code in software
development, ensuring the quality of LLM-generated code has become important.
Traditional testing approaches using Example-based Testing (EBT) often miss
edge cases -- defects that occur at boundary values, special input patterns, or
extreme conditions. This research investigates the characteristics of
LLM-generated Property-based Testing (PBT) compared to EBT for exploring edge
cases. We analyze 16 HumanEval problems where standard solutions failed on
extended test cases, generating both PBT and EBT test codes using
Claude-4-sonnet. Our experimental results reveal that while each method
individually achieved a 68.75\% bug detection rate, combining both approaches
improved detection to 81.25\%. The analysis demonstrates complementary
characteristics: PBT effectively detects performance issues and edge cases
through extensive input space exploration, while EBT effectively detects
specific boundary conditions and special patterns. These findings suggest that
a hybrid approach leveraging both testing methods can improve the reliability
of LLM-generated code, providing guidance for test generation strategies in
LLM-based code generation.

</details>


### [29] [Dissect-and-Restore: AI-based Code Verification with Transient Refactoring](https://arxiv.org/abs/2510.25406)
*Changjie Wang,Mariano Scazzariello,Anoud Alshnaka,Roberto Guanciale,Dejan Kostić,Marco Chiesa*

Main category: cs.SE

TL;DR: Prometheus is an AI-assisted system that enhances program verification by decomposing and recomposing complex logic, improving verification success from 68% to 86% in a dataset with significant gains in complex tasks.


<details>
  <summary>Details</summary>
Motivation: Formal verification struggles with complexity and cost due to specialized expertise required. AI systems offer potential but integrating them remains challenging. The paper addresses this gap by leveraging AI with modular refactoring.

Method: Prometheus uses a decomposition-recomposition workflow, breaking code into verifiable components, verifying them, and reconstructing proofs. It guides AI through structured lemma decomposition and allows lightweight natural language guidance when needed.

Result: Prometheus verifies 86% of tasks in the curated dataset, outperforming the 68% baseline. Significant improvements are noted with complex specifications (30% to 69%) and when integrating proof outlines (25% to 87%).

Conclusion: Modular restructuring improves AI's effectiveness in formal verification. Prometheus shows that decomposition into sub-lemmas combined with natural language guidance can efficiently handle complex proofs.

Abstract: Formal verification is increasingly recognized as a critical foundation for
building reliable software systems. However, the need for specialized expertise
to write precise specifications, navigate complex proof obligations, and learn
annotations often makes verification an order of magnitude more expensive than
implementation. While modern AI systems can recognize patterns in mathematical
proofs and interpret natural language, effectively integrating them into the
formal verification process remains an open challenge. We present Prometheus, a
novel AI-assisted system that facilitates automated code verification with
current AI capabilities in conjunction with modular software engineering
principles (e.g., modular refactoring). Our approach begins by decomposing
complex program logic, such as nested loops, into smaller, verifiable
components. Once verified, these components are recomposed to construct a proof
of the original program. This decomposition-recomposition workflow is
non-trivial. Prometheus addresses this by guiding the proof search through
structured decomposition of complex lemmas into smaller, verifiable sub-lemmas.
When automated tools are insufficient, users can provide lightweight natural
language guidance to steer the proof process effectively. Our evaluation
demonstrates that transiently applying modular restructuring to the code
substantially improves the AI's effectiveness in verifying individual
components. This approach successfully verifies 86% of tasks in our curated
dataset, compared to 68% for the baseline. Gains are more pronounced with
increasing specification complexity, improving from 30% to 69%, and when
integrating proof outlines for complex programs, from 25% to 87%.

</details>


### [30] [What Challenges Do Developers Face in AI Agent Systems? An Empirical Study on Stack Overflow](https://arxiv.org/abs/2510.25423)
*Ali Asgari,Annibale Panichella,Pouria Derakhshanfar,Mitchell Olsthoorn*

Main category: cs.SE

TL;DR: The paper identifies challenges in developing AI agents by analyzing Stack Overflow discussions, revealing seven...


<details>
  <summary>Details</summary>
Motivation: To understand common issues faced by developers in building AI agents and provide guidance on addressing agent reliability...

Method: Constructed a taxonomy of challenges via tag analysis, used LDA-MALLET for topic modeling, validated manually.

Result: Found 77 distinct technical challenges across seven categories; quantified popularity/difficulty; mapped tools and languages; tracked evolutions from 2021-2025.

Conclusion: Offers practical guidance for agent development based on empirical data from developer discussions.

Abstract: AI agents have rapidly gained popularity across research and industry as
systems that extend large language models with additional capabilities to plan,
use tools, remember, and act toward specific goals. Yet despite their promise,
developers face persistent and often underexplored challenges when building,
deploying, and maintaining these emerging systems. To identify these
challenges, we study developer discussions on Stack Overflow, the world's
largest developer-focused Q and A platform with about 60 million questions and
answers and 30 million users. We construct a taxonomy of developer challenges
through tag expansion and filtering, apply LDA-MALLET for topic modeling, and
manually validate and label the resulting themes. Our analysis reveals seven
major areas of recurring issues encompassing 77 distinct technical challenges
related to runtime integration, dependency management, orchestration
complexity, and evaluation reliability. We further quantify topic popularity
and difficulty to identify which issues are most common and hardest to resolve,
map the tools and programming languages used in agent development, and track
their evolution from 2021 to 2025 in relation to major AI model and framework
releases. Finally, we present the implications of our results, offering
concrete guidance for practitioners, researchers, and educators on agent
reliability and developer support.

</details>


### [31] [Reflections on the Reproducibility of Commercial LLM Performance in Empirical Software Engineering Studies](https://arxiv.org/abs/2510.25506)
*Florian Angermeir,Maximilian Amougou,Mark Kreitz,Andreas Bauer,Matthias Linhuber,Davide Fucci,Fabiola Moyón C.,Daniel Mendez,Tony Gorschek*

Main category: cs.SE

TL;DR: Analysis of 86 LLM papers found critical reproducibility issues across ICSE/ASE 2024: Only 5 of 18 replicable studies achieved full reproducibility, prompting calls for stronger artefact standards and experimental design rigor.


<details>
  <summary>Details</summary>
Motivation: The surge in LLM research highlights reproducibility challenges. Existing studies lack consistent reproducibility frameworks, necessitating assessment of current reproducibility levels and impediments.

Method: The authors examined 86 articles from ICSE 2024 and ASE 2024. They tested 18 studies utilizing OpenAI LLMs and available research artefacts for reproducibility.

Result: From 18 attempts: 5 were reproduction-worthy but all results non-reproducible, 2 partially reproducible, 3 irreproducible. This highlights systemic reproducibility failures.

Conclusion: The findings suggest a pressing need for stricter evaluations of research artefacts and more robust study designs to enhance the reproducibility of future LLM-centred publications.

Abstract: Large Language Models have gained remarkable interest in industry and
academia. The increasing interest in LLMs in academia is also reflected in the
number of publications on this topic over the last years. For instance, alone
78 of the around 425 publications at ICSE 2024 performed experiments with LLMs.
Conducting empirical studies with LLMs remains challenging and raises questions
on how to achieve reproducible results, for both other researchers and
practitioners. One important step towards excelling in empirical research on
LLMs and their application is to first understand to what extent current
research results are eventually reproducible and what factors may impede
reproducibility. This investigation is within the scope of our work. We
contribute an analysis of the reproducibility of LLM-centric studies, provide
insights into the factors impeding reproducibility, and discuss suggestions on
how to improve the current state. In particular, we studied the 86 articles
describing LLM-centric studies, published at ICSE 2024 and ASE 2024. Of the 86
articles, 18 provided research artefacts and used OpenAI models. We attempted
to replicate those 18 studies. Of the 18 studies, only five were fit for
reproduction. For none of the five studies, we were able to fully reproduce the
results. Two studies seemed to be partially reproducible, and three studies did
not seem to be reproducible. Our results highlight not only the need for
stricter research artefact evaluations but also for more robust study designs
to ensure the reproducible value of future publications.

</details>


### [32] [Fuzz Smarter, Not Harder: Towards Greener Fuzzing with GreenAFL](https://arxiv.org/abs/2510.25665)
*Ayse Irmak Ercevik,Aidan Dakhama,Melane Navaratnarajah,Yazhuo Cao,Leo Fernandes*

Main category: cs.SE

TL;DR: GreenAFL optimizes software fuzzing by prioritizing energy efficiency through power-aware test case selection and mutation, reducing the environmental impact of continuous testing without compromising code coverage.


<details>
  <summary>Details</summary>
Motivation: Traditional grey-box fuzzing tools prioritize coverage maximization without addressing energy efficiency, leading to significant computational costs and carbon footprints.

Method: GreenAFL introduces energy-aware corpus minimization (reducing initial test cases based on power consumption) and energy-guided heuristics (directing mutations towards high-coverage, low-energy inputs) to standard fuzzing workflows.

Result: Ablation studies show that GreenAFL achieves higher coverage and lower energy consumption compared to vanilla AFL++ when at least one of the proposed modifications (corpus minimization or energy-guided heuristics) is applied.

Conclusion: GreenAFL demonstrates that energy-aware fuzzing can reduce environmental impact while maintaining coverage, achieving optimal results when either energy-aware corpus minimization or energy-guided heuristics are applied.

Abstract: Fuzzing has become a key search-based technique for software testing, but
continuous fuzzing campaigns consume substantial computational resources and
generate significant carbon footprints. Existing grey-box fuzzing approaches
like AFL++ focus primarily on coverage maximisation, without considering the
energy costs of exploring different execution paths. This paper presents
GreenAFL, an energy-aware framework that incorporates power consumption into
the fuzzing heuristics to reduce the environmental impact of automated testing
whilst maintaining coverage. GreenAFL introduces two key modifications to
traditional fuzzing workflows: energy-aware corpus minimisation considering
power consumption when reducing initial corpora, and energy-guided heuristics
that direct mutation towards high-coverage, low-energy inputs. We conduct an
ablation study comparing vanilla AFL++, energy-based corpus minimisation, and
energy-based heuristics to evaluate the individual contributions of each
component. Results show that highest coverage, and lowest energy usage is
achieved whenever at least one of our modifications is used.

</details>


### [33] [A Configuration-First Framework for Reproducible, Low-Code Localization](https://arxiv.org/abs/2510.25692)
*Tim Strnad,Blaž Bertalanič,Carolina Fortuna*

Main category: cs.SE

TL;DR: This paper introduces LOCALIZE, a low-code configuration-driven framework for reproducible and extensible machine learning-based radio localization, addressing gaps in coding effort, reproducibility, and extensibility. It demonstrates reduced authoring effort and scalable orchestration through versioned workflows and pipeline standardization.


<details>
  <summary>Details</summary>
Motivation: Existing tools lack simultaneous support for low-coding-effort workflows, default reproducibility (through versioning/isolation), and extensibility for radio localization tasks. Researchers need standardized pipelines to maintain reliability while enabling innovation.

Method: LOCALIZE implements a configuration-first architecture with declarative experiment specs, workflow orchestration for data-preprocessing-to-report pipelines, and artifact versioning (models/datasets/metrics). It uses preconfigured datasets to reduce boilerplate and provides extension points for custom components without infrastructure rework.

Result: Quantitative results show ~50-70%​ reduction in authoring time vs. Jupyter notebooks without runtime/memory tradeoffs. Scalability tests with BLE datasets demonstrate orchestration overhead remains bounded (5-8% of total runtime) even as data scales 10x. Qualitative validation shows enhanced reproducibility and maintainability features.

Conclusion: LOCALIZE bridges the three key gaps in ML-based localization frameworks, making reproducible experimentation both practical for novices (through low-code interfaces) and extensible for experts (through customizable components) without compromising performance or rigor.

Abstract: Machine learning is increasingly permeating radio-based localization
services. To keep results credible and comparable, everyday workflows should
make rigorous experiment specification and exact repeatability the default,
without blocking advanced experimentation. However, in practice, researchers
face a three-way gap that could be filled by a framework that offers (i) low
coding effort for end-to-end studies, (ii) reproducibility by default including
versioned code, data, and configurations, controlled randomness, isolated runs,
and recorded artifacts, and (iii) built-in extensibility so new models,
metrics, and stages can be added with minimal integration effort. Existing
tools rarely deliver all three for machine learning in general and localization
workflows in particular. In this paper we introduce LOCALIZE, a low-code,
configuration-first framework for radio localization in which experiments are
declared in human-readable configuration, a workflow orchestrator runs
standardized pipelines from data preparation to reporting, and all artifacts,
such as datasets, models, metrics, and reports, are versioned. The
preconfigured, versioned datasets reduce initial setup and boilerplate,
speeding up model development and evaluation. The design, with clear extension
points, allows experts to add components without reworking the infrastructure.
In a qualitative comparison and a head-to-head study against a plain Jupyter
notebook baseline, we show that the framework reduces authoring effort while
maintaining comparable runtime and memory behavior. Furthermore, using a
Bluetooth Low Energy dataset, we show that scaling across training data (1x to
10x) keeps orchestration overheads bounded as data grows. Overall, the
framework makes reproducible machine-learning-based localization
experimentation practical, accessible, and extensible.

</details>


### [34] [Process-Level Trajectory Evaluation for Environment Configuration in Software Engineering Agents](https://arxiv.org/abs/2510.25694)
*Jiayi Kuang,Yinghui Li,Xin Zhang,Yangning Li,Di Yin,Xing Sun,Ying Shen,Philip S. Yu*

Main category: cs.SE

TL;DR: A new benchmark, Enconda-bench, provides process-level evaluation for environment configuration in software engineering LLM agents.


<details>
  <summary>Details</summary>
Motivation: Environment configuration in software engineering with LLM agents requires manual effort and lacks large, high-quality datasets. Existing benchmarks only show end results, missing the breakdown of agent capabilities.

Method: The authors created Enconda-bench, which evaluates agent capabilities during environment setup via task instances generated by injecting README errors, validated in Docker.

Result: Evaluations revealed agents can identify errors but struggle to use feedback to fix them, impacting end-to-end effectiveness.

Conclusion: Enconda-bench offers a new, detailed assessment method for improving future software engineering agents.

Abstract: Large language model-based agents show promise for software engineering, but
environment configuration remains a bottleneck due to heavy manual effort and
scarce large-scale, high-quality datasets. Existing benchmarks assess only
end-to-end build/test success, obscuring where and why agents succeed or fail.
We introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench,
which provides process-level trajectory assessment of fine-grained agent
capabilities during environment setup-planning, perception-driven error
diagnosis, feedback-driven repair, and action to execute final environment
configuration. Our task instances are automatically constructed by injecting
realistic README errors and are validated in Docker for scalable, high-quality
evaluation. Enconda-bench combines process-level analysis with end-to-end
executability to enable capability assessments beyond aggregate success rates.
Evaluations across state-of-the-art LLMs and agent frameworks show that while
agents can localize errors, they struggle to translate feedback into effective
corrections, limiting end-to-end performance. To our knowledge, Enconda-bench
is the first framework to provide process-level internal capability assessment
for environment configuration, offering actionable insights for improving
software engineering agents.

</details>
