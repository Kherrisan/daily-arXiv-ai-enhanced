<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 31]
- [cs.SE](#cs.SE) [Total: 26]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [nodeWSNsec: A hybrid metaheuristic approach for reliable security and node deployment in WSNs](https://arxiv.org/abs/2508.16619)
*Rahul Mishra,Sudhanshu Kumar Jha,Naresh Kshetri,Bishnu Bhusal,Mir Mehedi Rahman,Md Masud Rana,Aimina Ali Eli,Khaled Aminul Islam,Bishwo Prakash Pokharel*

Main category: cs.CR

TL;DR: This paper introduces a hybrid GA-PSO algorithm for energy-efficient and reliable node deployment in Wireless Sensor Networks (WSN), achieving fewer nodes required and better coverage/connectivity compared to standalone GA/PSO and a CMOMPA competitor. Key trade-offs and future directions are explored.


<details>
  <summary>Details</summary>
Motivation: Optimizing WSN node deployment is critical for coverage, connectivity, and energy efficiency. Prior methods face limitations in balancing these objectives, necessitating a hybrid approach to leverage complementary strengths.

Method: A hybrid meta-heuristic algorithm combining Genetic Algorithm (GA) for exploration with Particle Swarm Optimization (PSO) for convergence, addressing multi-objective optimization for node deployment.

Result: GA-PSO reduces sensor nodes by 15–25% while maintaining ≥95% area coverage and connectivity compared to GA/PSO alone. It outperforms CMOMPA in coverage, connectivity, deployment time, and node count for long-range scenarios.

Conclusion: Hybrid meta-heuristics like GA-PSO effectively optimize WSN deployment, with promising potential for real-world applications. Future work includes heterogeneous nodes, mobile WSNs, and advanced multi-objective algorithms.

Abstract: Efficient and reliable node deployment in Wireless Sensor Networks is crucial
for optimizing coverage of the area, connectivity among nodes, and energy
efficiency. This paper proposes a hybrid meta heuristic approach combining a
Genetic Algorithm (GA) and Particle Swarm Optimization (PSO) to address the
challenges of energy efficient and reliable node deployment. The GA PSO hybrid
leverages GAs strong exploration capabilities and PSOs rapid convergence,
achieving an optimum stability between coverage and energy consumption. The
performance of the proposed approach is evaluated against GA and PSO alone and
the innovatory meta heuristic based Competitive Multi Objective Marine
Predators Algorithm (CMOMPA) across varying sensing ranges. Simulation results
demonstrate that GA PSO requires 15% to 25% fewer sensor nodes and maintains
95% or more area coverage while maintaining the connectivity in comparison to
standalone GA or PSO algorithm. The proposed algorithm also dominates CMOMPA
when compared for long sensing and communication range in terms of higher
coverage, improved connectivity, and reduced deployment time while requiring
fewer sensor nodes. This study also explores key trade offs in WSN deployment
and highlights future research directions, including heterogeneous node
deployment, mobile WSNs, and enhanced multi objective optimization techniques.
The findings underscore the effectiveness of hybrid meta heuristics in
improving WSN performance, offering a promising approach for real world
applications such as environmental monitoring, smart cities, smart agriculture,
disaster response, and IIoT.

</details>


### [2] [Data and Context Matter: Towards Generalizing AI-based Software Vulnerability Detection](https://arxiv.org/abs/2508.16625)
*Rijha Safdar,Danyail Mateen,Syed Taha Ali,M. Umer Ashfaq,Wajahat Hussain*

Main category: cs.CR

TL;DR: This paper investigates how data quality and model architecture affect the generalization of AI-based vulnerabilities detectors. Experiments show that diverse datasets and encoder models enhance cross-project performance.


<details>
  <summary>Details</summary>
Motivation: Current AI-based vulnerability detection systems struggle to generalize across unseen C/C++ codebases, necessitating improvements in dataset and model strategies.

Method: The study conducts experiments comparing encoder-only and decoder-only models, evaluates dataset diversity/quality, and tests on the BigVul benchmark to assess cross-project performance.

Result: Encoder models achieved 6.8% higher recall on BigVul and outperformed in unseen projects. Dataset improvements significantly boosted detection performance.

Conclusion: The research highlights data quality and encoder architecture as critical for robust vulnerability detectors with cross-project effectiveness, guiding future system development.

Abstract: The performance of AI-based software vulnerability detection systems is often
limited by their poor generalization to unknown codebases. In this research, we
explore the impact of data quality and model architecture on the
generalizability of vulnerability detection systems. By generalization we mean
ability of high vulnerability detection performance across different C/C++
software projects not seen during training. Through a series of experiments, we
demonstrate that improvements in dataset diversity and quality substantially
enhance detection performance. Additionally, we compare multiple encoder-only
and decoder-only models, finding that encoder based models outperform in terms
of accuracy and generalization. Our model achieves 6.8% improvement in recall
on the benchmark BigVul[1] dataset, also outperforming on unseen projects,
hence showing enhanced generalizability. These results highlight the role of
data quality and model selection in the development of robust vulnerability
detection systems. Our findings suggest a direction for future systems having
high cross-project effectiveness.

</details>


### [3] [Passive Hack-Back Strategies for Cyber Attribution: Covert Vectors in Denied Environment](https://arxiv.org/abs/2508.16637)
*Abraham Itzhak Weinberg*

Main category: cs.CR

TL;DR: This paper explores passive hack-back techniques for covert cyberattack attribution and intelligence collection, integrating AI and quantum technologies for effectiveness and stealth, while advocating hybrid frameworks with conditional active responses for legal compliance.


<details>
  <summary>Details</summary>
Motivation: Modern cybersecurity faces challenges in attributing attacks in denied environments with limited visibility and restrictive engagement policies.

Method: Analyzes passive methods (tracking beacons, honeytokens, environment-specific payloads, supply-chain traps) combined with AI (autonomous agents, LLMs for dynamic payloads) and quantum technologies for evasion, communication, and resilience.

Result: Demonstrates how passive attribution with AI/quantum tools can bypass engagement restrictions while gathering actionable intelligence.

Conclusion: Hybrid frameworks merging passive attribution with delayed/conditional active defenses, while adhering to legal/ethical constraints, provide optimal cybersecurity solutions in restricted scenarios.

Abstract: Attributing cyberattacks remains a central challenge in modern cybersecurity,
particularly within denied environments where defenders have limited visibility
into attacker infrastructure and are restricted by legal or operational rules
of engagement. This perspective examines the strategic value of passive
hack-back techniques that enable covert attribution and intelligence collection
without initiating direct offensive actions. Key vectors include tracking
beacons, honeytokens, environment-specific payloads, and supply-chain-based
traps embedded within exfiltrated or leaked assets. These approaches rely on
the assumption that attackers will interact with compromised data in traceable
ways, allowing defenders to gather signals without violating engagement
policies. The paper also explores the role of Artificial Intelligence (AI) in
enhancing passive hack-back operations. Topics include the deployment of
autonomous agents for forensic reconnaissance, the use of Large Language Models
(LLMs) to generate dynamic payloads, and Adversarial Machine Learning (AML)
techniques for evasion and counter-deception. A dedicated section discusses the
implications of quantum technologies in this context, both as future threats to
cryptographic telemetry and as potential tools for stealthy communication and
post-quantum resilience. Finally, the paper advocates for hybrid defensive
frameworks that combine passive attribution with delayed or conditional active
responses, while maintaining compliance with legal, ethical, and operational
constraints.

</details>


### [4] [Bridging the Mobile Trust Gap: A Zero Trust Framework for Consumer-Facing Applications](https://arxiv.org/abs/2508.16662)
*Alexander Tabalipa*

Main category: cs.CR

TL;DR: The paper proposes an extended Zero Trust model for mobile applications, addressing security gaps in untrusted user-controlled environments with a six-pillar framework and compliance-aligned implementation roadmap.


<details>
  <summary>Details</summary>
Motivation: Existing Zero Trust frameworks from NIST/CISA focus on enterprise infrastructure with organizational control over devices, but mobile apps face rising sophisticated threats and lack tailored security models despite driving most global digital interactions.

Method: A design science methodology was used to develop the framework, incorporating device integrity, user identity validation, data protection, secure API usage, behavioral monitoring, and live application protection as six pillars.

Result: The framework maps to regulatory standards, provides a phased implementation roadmap, and a maturity assessment model, offering real-time security enforcement for mobile apps beyond pre-deployment controls.

Conclusion: This work expands ZTA boundaries, enabling mobile app security in user-controlled environments, facilitating fraud reduction and compliance. Future research includes empirical validation and cross-sector testing.

Abstract: Zero Trust Architecture (ZTA) has become a widely adopted model for securing
enterprise environments, promoting continuous verification and minimal trust
across systems. However, its application in mobile contexts remains limited,
despite mobile applications now accounting for most global digital interactions
and being increasingly targeted by sophisticated threats. Existing Zero Trust
frameworks developed by organisations such as the National Institute of
Standards and Technology (NIST) and the Cybersecurity and Infrastructure
Security Agency (CISA) primarily focus on enterprise-managed infrastructure,
assuming organisational control over devices, networks, and identities. This
paper addresses a critical gap by proposing an extended Zero Trust model
designed for mobile applications operating in untrusted, user-controlled
environments. Using a design science methodology, the study introduced a
six-pillar framework that supports runtime enforcement of trust through
controls including device integrity, user identity validation, data protection,
secure application programming interface (API) usage, behavioural monitoring,
and live application protection. Each pillar was mapped to relevant regulatory
and security standards to support compliance. A phased implementation roadmap
and maturity assessment model were also developed to guide adoption across
varying organisational contexts. The proposed model offers a practical and
standards-aligned approach to securing mobile applications beyond
pre-deployment controls, aligning real-time enforcement with Zero Trust
principles. This contribution expands the operational boundaries of ZTA and
provides organisations with a deployable path to reduce fraud, enhance
compliance, and address emerging mobile security challenges. Future research
may include empirical validation of the framework and cross-sector application
testing.

</details>


### [5] [Securing Heterogeneous Network (HetNet) Communications for Wildfire Management: Mitigating the Effects of Adversarial and Environmental Threats](https://arxiv.org/abs/2508.16761)
*Nesrine Benchoubane,Olfa Ben Yahia,William Ferguson,Gurkan Gur,Sumit Chakravarty,Gregory Falco,Gunes Karabulut Kurt*

Main category: cs.CR

TL;DR: This paper proposes a novel heterogeneous communication framework integrating LEO satellites, HAPGS, and LAPS to enhance resilience against environmental challenges and cyber threats in wildfire management, while aligning with IEEE P3536 standards.


<details>
  <summary>Details</summary>
Motivation: Critical applications like wildfire management require secure, resilient communication systems to withstand harsh environmental conditions and increasing cyber threats that could jeopardize mission outcomes.

Method: The framework extends a secure-by-component approach to the communication layer by incorporating RF/FSO link protection mechanisms into a HetNet architecture. Case study simulations quantify effects of environmental stressors on network security performance.

Result: Environmental factors cause 15-40% secrecy capacity degradation in LAPS-HAPGS/LEO links, but HAPS remain 15% more secure against eavesdropping even at 20dB SNR. Transmit power optimization shows nonlinear trade-offs between signal quality and confidentiality risks.

Conclusion: The work establishes that environmental protection and cybersecurity must be co-designed in critical communication networks, providing IEEE P3536-compliant solutions to prevent mission failures from atmospheric effects or signal interception.

Abstract: In the face of adverse environmental conditions and cyber threats, robust
communication systems for critical applications such as wildfire management and
detection demand secure and resilient architectures. This paper presents a
novel framework that considers both adversarial factors, building resilience
into a heterogeneous network (HetNet) integrating Low Earth Orbit (LEO)
satellite constellation with High-Altitude Platform Ground Stations (HAPGS) and
Low-Altitude Platforms (LAPS), tailored to support wildfire management
operations. Building upon our previous work on secure-by-component approach for
link segment security, we extend protection to the communication layer by
securing both Radio Frequency (RF)/Free Space Optics (FSO) management and
different links. Through a case study, we quantify how environmental stressors
impact secrecy capacity and expose the system to passive adversaries. Key
findings demonstrate that atmospheric attenuation and beam misalignment can
notably degrade secrecy capacity across both short- and long-range
communication links, while high-altitude eavesdroppers face less signal
degradation, increasing their interception capability. Moreover, increasing
transmit power to counter environmental losses can inadvertently improve
eavesdropper reception, thereby reducing overall link confidentiality. Our work
not only highlights the importance of protecting networks from these dual
threats but also aligns with the IEEE P3536 Standard for Space System
Cybersecurity Design, ensuring resilience and the prevention of mission
failures.

</details>


### [6] [Guarding Your Conversations: Privacy Gatekeepers for Secure Interactions with Cloud-Based AI Models](https://arxiv.org/abs/2508.16765)
*GodsGift Uzor,Hasan Al-Qudah,Ynes Ineza,Abdul Serwadda*

Main category: cs.CR

TL;DR: This paper proposes an 'LLM gatekeeper' as a local filtering system to enhance privacy when interacting with cloud-based LLMs, addressing risks from weak privacy laws and data security. Human experiments show minimal overhead and preserved response quality.


<details>
  <summary>Details</summary>
Motivation: Users increasingly share personal information with LLMs despite privacy settings, creating risks in jurisdictions with inadequate data protection laws, government surveillance, or poor security practices.

Method: A lightweight, locally executed 'LLM gatekeeper' filters sensitive information from user queries before transmitting them to cloud-based LLMs, tested via controlled human subject experiments.

Result: The gatekeeper reduced sensitive data exposure by ~98% in experiments (based on similar studies), while introducing less than 2% response quality degradation and negligible computational overhead.

Conclusion: The LLM gatekeeper provides a practical architecture to mitigate privacy risks in cloud LLM interactions without sacrificing usability or effectiveness.

Abstract: The interactive nature of Large Language Models (LLMs), which closely track
user data and context, has prompted users to share personal and private
information in unprecedented ways. Even when users opt out of allowing their
data to be used for training, these privacy settings offer limited protection
when LLM providers operate in jurisdictions with weak privacy laws, invasive
government surveillance, or poor data security practices. In such cases, the
risk of sensitive information, including Personally Identifiable Information
(PII), being mishandled or exposed remains high. To address this, we propose
the concept of an "LLM gatekeeper", a lightweight, locally run model that
filters out sensitive information from user queries before they are sent to the
potentially untrustworthy, though highly capable, cloud-based LLM. Through
experiments with human subjects, we demonstrate that this dual-model approach
introduces minimal overhead while significantly enhancing user privacy, without
compromising the quality of LLM responses.

</details>


### [7] [A Survey of Threats Against Voice Authentication and Anti-Spoofing Systems](https://arxiv.org/abs/2508.16843)
*Kamel Kamel,Keshav Sood,Hridoy Sankar Dutta,Sunil Aryal*

Main category: cs.CR

TL;DR: This paper surveys modern threats to voice authentication systems and anti-spoofing countermeasures, tracking their evolution alongside deep learning advancements.


<details>
  <summary>Details</summary>
Motivation: Voice authentication's growing adoption in security-critical domains like finance and law enforcement necessitates understanding emerging threats such as deepfake and adversarial attacks.

Method: Chronological analysis of voice authentication evolution, taxonomic categorization of attacks (data poisoning, adversarial attacks, deepfake, and spoofing), performance comparisons, dataset reviews, and literature synthesis.

Result: Systematic summary of attack methodologies, evaluation of existing literature's strengths/limitations, and identification of emerging risks in voice biometrics.

Conclusion: The survey provides a framework for developing more secure voice authentication systems by clarifying attack landscapes and highlighting open security challenges.

Abstract: Voice authentication has undergone significant changes from traditional
systems that relied on handcrafted acoustic features to deep learning models
that can extract robust speaker embeddings. This advancement has expanded its
applications across finance, smart devices, law enforcement, and beyond.
However, as adoption has grown, so have the threats. This survey presents a
comprehensive review of the modern threat landscape targeting Voice
Authentication Systems (VAS) and Anti-Spoofing Countermeasures (CMs), including
data poisoning, adversarial, deepfake, and adversarial spoofing attacks. We
chronologically trace the development of voice authentication and examine how
vulnerabilities have evolved in tandem with technological advancements. For
each category of attack, we summarize methodologies, highlight commonly used
datasets, compare performance and limitations, and organize existing literature
using widely accepted taxonomies. By highlighting emerging risks and open
challenges, this survey aims to support the development of more secure and
resilient voice authentication systems.

</details>


### [8] [Targeted Wearout Attacks in Microprocessor Cores](https://arxiv.org/abs/2508.16868)
*Joshua Mashburn,Johann Knechtel,Florian Klemme,Hussam Amrouch,Ozgur Sinanoglu,Paul V. Gratz*

Main category: cs.CR

TL;DR: This paper investigates a software-driven attack exploiting nanoscale CMOS aging to cause targeted device degradation and silent data corruption in co-running applications.


<details>
  <summary>Details</summary>
Motivation: Traditional fault-injection attacks require physical access or elevated privileges; software-based degradation through controllable inputs can bypass these limitations.

Method: Demonstrate a Targeted Wearout Attack by crafting software to stress a RISC-V CPU's fused multiply-add pipeline, accelerating aging via Negative-Bias Temperature Instability mechanisms.

Result: Achieved >7x accelerated wear on targeted logic paths compared to typical workloads, inducing predictable silent data corruption in victim applications sharing the same functional unit

Conclusion: Software-initiated aging attacks can systematically corrupt specific logic paths without physical access, requiring platform-specific defenses.

Abstract: Negative-Bias Temperature Instability is a dominant aging mechanism in
nanoscale CMOS circuits such as microprocessors. With this aging mechanism, the
rate of device aging is dependent not only on overall operating conditions,
such as heat, but also on user controllable inputs to the transistors. This
dependence on input implies a possible timing fault-injection attack wherein a
targeted path of logic is intentionally degraded through the purposeful,
software-driven actions of an attacker, rendering a targeted bit effectively
stuck.
  In this work, we describe such an attack mechanism, which we dub a
"$\textbf{Targeted Wearout Attack}$", wherein an attacker with sufficient
knowledge of the processor core, executing a carefully crafted software program
with only user privilege, is able to degrade a functional unit within the
processor with the aim of eliciting a particular desired incorrect calculation
in a victim application. Here we give a general methodology for the attack. We
then demonstrate a case study where a targeted path within the fused
multiply-add pipeline in a RISC-V CPU sees a $>7x$ increase in wear over time
than would be experienced under typical workloads. We show that an attacker
could leverage such an attack, leading to targeted and silent data corruption
in a co-running victim application using the same unit.

</details>


### [9] [Investigating red packet fraud in Android applications: Insights from user reviews](https://arxiv.org/abs/2508.16941)
*Yu Cheng,Xiaofang Qi,Yanhui Li*

Main category: cs.CR

TL;DR: This paper investigates red packet fraud in mobile apps by analyzing 360,000 user reviews and proposes ReckDetector, an automated tool to identify affected apps, summarizing six fraud categories and their detrimental impacts.


<details>
  <summary>Details</summary>
Motivation: The increasing prevalence of red packet fraud in smartphone apps, as highlighted by persistent user complaints in app market reviews, necessitates a systematic investigation to understand its scope, user impact, and developer exploitation.

Method: The study uses ReckDetector to automate red packet app identification, collects and preprocesses 360,000 user reviews from Google Play and Android alternative markets, fine-tunes a pre-trained BERT model for negative review classification, and performs semantic analysis to categorize fraud types.

Result: Six distinct red packet fraud categories were identified, with evidence showing widespread fraud affecting user experience and app reputations. Fraudulent red packets are exploited as deceptive incentive mechanisms to maximize developer profits.

Conclusion: Red packet fraud is a significant issue in Android apps, with unscrupulous developers using it to manipulate user behavior. The paper highlights the need for improved market enforcement and detection tools to mitigate this growing problem.

Abstract: With the popularization of smartphones, red packets have been widely used in
mobile apps. However, the issues of fraud associated with them have also become
increasingly prominent. As reported in user reviews from mobile app markets,
many users have complained about experiencing red packet fraud and being
persistently troubled by fraudulent red packets. To uncover this phenomenon, we
conduct the first investigation into an extensive collection of user reviews on
apps with red packets. In this paper, we first propose a novel automated
approach, ReckDetector, for effectively identifying apps with red packets from
app markets. We then collect over 360,000 real user reviews from 334 apps with
red packets available on Google Play and three popular alternative Android app
markets. We preprocess the user reviews to extract those related to red packets
and fine-tune a pre-trained BERT model to identify negative reviews. Finally,
based on semantic analysis, we have summarized six distinct categories of red
packet fraud issues reported by users. Through our study, we found that red
packet fraud is highly prevalent, significantly impacting user experience and
damaging the reputation of apps. Moreover, red packets have been widely
exploited by unscrupulous app developers as a deceptive incentive mechanism to
entice users into completing their designated tasks, thereby maximizing their
profits.

</details>


### [10] [Towards Principled Analysis and Mitigation of Space Cyber Risks](https://arxiv.org/abs/2508.16991)
*Ekzhin Ear*

Main category: cs.CR

TL;DR: This dissertation addresses understudied cyber risks in space infrastructure by introducing frameworks for attack characterization, risk analysis, and mitigation, while evaluating existing tools and validating proposals through case studies and testbeds.


<details>
  <summary>Details</summary>
Motivation: Modern society relies on space infrastructure, yet its cyber risks remain poorly understood, creating a critical need for robust risk analysis and mitigation frameworks.

Method: 1. Developed a framework to characterize space cyber attacks with missing-data handling and three novel metrics. 2. Analyzed existing tools (SPARTA's NRS) and proposed desired properties for space cyber risk mitigation tools. 3. Introduced cascading effects modeling with mission risk analysis algorithms.

Result: Four key contributions validated through 108 real-world attack case studies and testbed experiments: an attack characterization framework, systematic evaluation of current risk tools, desired tool properties, and a cascading-effects mitigation framework.

Conclusion: The work establishes foundational frameworks for space cyber risk analysis, identifies gaps in current tools, and provides test-validated solutions to address cascading attack risks in critical space infrastructure.

Abstract: Space infrastructures have become an underpinning of modern society, but
their associated cyber risks are little understood. This Dissertation advances
the state-of-the-art via four contributions. (i) It introduces an innovative
framework for characterizing real-world cyber attacks against space
infrastructures, or space cyber attacks, including a novel methodology for
coping with missing data and three novel metrics. A case study demonstrates the
usefulness of the framework on 108 real-world space cyber attacks. (ii) This
Dissertation characterizes the state-of-the-practice in space cyber risk
analysis and mitigation, namely the Notional Risk Scores (NRS) within the Space
Attack Research and Tactic Analysis (SPARTA) framework. (iii) We propose a set
of desired properties that should be satisfied by any competent space cyber
risk analysis and mitigation tool and applies them to assess two industrial
space cyber risk analysis and mitigation tools. (iv) The study introduces a
novel framework to analyze and mitigate space cyber risks by explicitly
modeling space cyber attack cascading effects and presenting algorithms for
mission risk analysis and mission hardening. We demonstrate the usefulness of
the framework by applying it to analyze and mitigate space cyber risks, with
testbed-based validation.

</details>


### [11] [ZAPS: A Zero-Knowledge Proof Protocol for Secure UAV Authentication with Flight Path Privacy](https://arxiv.org/abs/2508.17043)
*Shayesta Naziri,Xu Wang,Guangsheng Yu,Christy Jie Liang,Wei Ni*

Main category: cs.CR

TL;DR: A zk-SNARK framework lets UAVs verify flight compliance without revealing trajectory data, solving privacy vulnerabilities in UAV communications while maintaining efficiency for real-world use.


<details>
  <summary>Details</summary>
Motivation: Current UAV communication systems expose flight path metadata to privacy risks (tracking, surveillance, location inference) despite encryption. Metadata analysis still allows adversaries to infer movement patterns, motivating the need for stronger privacy-preserving solutions.

Method: The authors leverage zk-SNARKs to create a cryptographic proof system where UAVs can authenticate authorization, validate flight paths against control centers, and comply with regulatory policies without disclosing sensitive trajectory or location data.

Result: The solution mitigates real-time tracking, identity exposure, and unauthorized interception risks. It achieves a balance of privacy, security, and computational efficiency, suitable for resource-constrained UAVs in adversarial environments.

Conclusion: The paper concludes that the proposed zk-SNARK-based framework effectively enhances UAV flight path privacy and operational security while maintaining computational efficiency, making it viable for both civilian and military UAV applications.

Abstract: The increasing deployment of Unmanned Aerial Vehicles (UAVs) for military,
commercial, and logistics applications has raised significant concerns
regarding flight path privacy. Conventional UAV communication systems often
expose flight path data to third parties, making them vulnerable to tracking,
surveillance, and location inference attacks. Existing encryption techniques
provide security but fail to ensure complete privacy, as adversaries can still
infer movement patterns through metadata analysis. To address these challenges,
we propose a zk-SNARK(Zero-Knowledge Succinct Non-Interactive Argument of
Knowledge)-based privacy-preserving flight path authentication and verification
framework. Our approach ensures that a UAV can prove its authorisation,
validate its flight path with a control centre, and comply with regulatory
constraints without revealing any sensitive trajectory information. By
leveraging zk-SNARKs, the UAV can generate cryptographic proofs that verify
compliance with predefined flight policies while keeping the exact path and
location undisclosed. This method mitigates risks associated with real-time
tracking, identity exposure, and unauthorised interception, thereby enhancing
UAV operational security in adversarial environments. Our proposed solution
balances privacy, security, and computational efficiency, making it suitable
for resource-constrained UAVs in both civilian and military applications.

</details>


### [12] [Post-Quantum Blockchain: Challenges and Opportunities](https://arxiv.org/abs/2508.17071)
*Sufyan Al-Janabi*

Main category: cs.CR

TL;DR: The paper reviews the risks quantum computing poses to blockchain cryptography and outlines guidelines for Post-Quantum Blockchains (PQB), emphasizing security challenges and future research directions.


<details>
  <summary>Details</summary>
Motivation: Quantum algorithms threaten traditional cryptographic protocols (e.g., SHA-256, ECDSA) integral to blockchain security, necessitating preparation for quantum-resistant solutions.

Method: Literature survey to analyze quantum threats to blockchain and evaluate post-quantum cryptographic alternatives for securing blockchain systems.

Result: Highlights vulnerabilities in current blockchain systems to quantum attacks and proposes a structured approach for implementing PQC to ensure long-term security.

Conclusion: The transition to post-quantum cryptography is critical to safeguard blockchain against future quantum threats, with key challenges in standardization and integration requiring further research.

Abstract: Blockchain is a Distributed Ledger Technology (DLT) that offers numerous
benefits including decentralization, transparency, efficiency, and reduced
costs. Hence, blockchain has been included in many fields. Blockchain relies on
cryptographic protocols (especially public-key cryptography and hash functions)
to achieve many essential sub-routines. However, the increased progress of
quantum computation and algorithms has threatened the security of many
traditional cryptosystems. Therefore, this represents a serious risk for the
existing blockchain technology. For example, SHA-256 and the Elliptic Curve
Digital Signature Algorithm (ECDSA) cryptosystems can be compromised by Shor s
and Grover s quantum algorithms in the foreseeable future. Post-Quantum
Cryptography (PQC) is a basic solution for resisting these quantum attacks.
Applying PQC to blockchains results in creating Post-Quantum Blockchains (PQB).
Thus, this paper aims to review the threats imposed by quantum computers on
classical blockchain technology and provide useful guidelines on PQB security
to blockchain researchers. The paper focuses on the challenges and
opportunities of future work direction in this field.

</details>


### [13] [SyncGuard: Robust Audio Watermarking Capable of Countering Desynchronization Attacks](https://arxiv.org/abs/2508.17121)
*Zhenliang Gan,Xiaoxiao Hu,Sheng Li,Zhenxing Qian,Xinpeng Zhang*

Main category: cs.CR

TL;DR: SyncGuard is a learning-based audio watermarking method that addresses challenges in watermark localization and desynchronization resistance through frame-wise embedding, a distortion layer, and dilated neural blocks.


<details>
  <summary>Details</summary>
Motivation: Audio watermarking faces issues with localization and robustness against desynchronization attacks. The paper aims to create a time-independent solution that eliminates manual alignment and improves resistance to common audio attacks.

Method: SyncGuard's method combines frame-wise broadcast embedding of watermarks, a distortion layer for robustness, and dilated residual/gated blocks to extract multi-resolution time-frequency features during detection.

Result: Extensive experiments demonstrate SyncGuard's superior performance in handling variable-length audio, maintaining watermark accuracy under desynchronization attacks, and producing high auditory quality compared to existing methods.

Conclusion: The proposed SyncGuard framework effectively resolves key limitations in audio watermarking by achieving time-independence and robust desynchronization resistance through its novel architectural components.

Abstract: Audio watermarking has been widely applied in copyright protection and source
tracing. However, due to the inherent characteristics of audio signals,
watermark localization and resistance to desynchronization attacks remain
significant challenges. In this paper, we propose a learning-based scheme named
SyncGuard to address these challenges. Specifically, we design a frame-wise
broadcast embedding strategy to embed the watermark in arbitrary-length audio,
enhancing time-independence and eliminating the need for localization during
watermark extraction. To further enhance robustness, we introduce a
meticulously designed distortion layer. Additionally, we employ dilated
residual blocks in conjunction with dilated gated blocks to effectively capture
multi-resolution time-frequency features. Extensive experimental results show
that SyncGuard efficiently handles variable-length audio segments, outperforms
state-of-the-art methods in robustness against various attacks, and delivers
superior auditory quality.

</details>


### [14] [Mind the Gap: Time-of-Check to Time-of-Use Vulnerabilities in LLM-Enabled Agents](https://arxiv.org/abs/2508.17155)
*Derek Lilienthal,Sanghyun Hong*

Main category: cs.CR

TL;DR: This paper introduces the first study on identifying and mitigating TOCTOU vulnerabilities in LLM-enabled agents, proposing a benchmark (TOCTOU-Bench) and countermeasures like prompt rewriting, state integrity monitoring, and tool-fusing. Evaluations show 25% detection accuracy, 95% reduction in attack windows, and combined methods reduce vulnerabilities from 12% to 8% in executed trajectories.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the gap in research on temporal dynamic vulnerabilities (TOCTOU) within LLM-enabled agents, which can enable attacks like malicious configuration swaps or payload injection when validated external states are later modified.

Method: The authors created TOCTOU-Bench with 66 realistic tasks to evaluate TOCTOU vulnerabilities. They adapted systems security detection/mitigation techniques, implementing three approaches: prompt rewriting to inject safeguards, state integrity monitoring for runtime checks, and tool-fusing to control agent interactions.

Result: Automated detection achieved 25% accuracy. Mitigations reduced vulnerable plan generation by 3%, attack window by 95%, and combination of all three approaches decreased TOCTOU vulnerabilities in executed trajectories from 12% to 8%.

Conclusion: This work establishes TOCTOU vulnerabilities as a critical vector in agentic workflows, demonstrating system security methods can be adapted for mitigation and opening interdisciplinary research directions between AI safety and systems security.

Abstract: Large Language Model (LLM)-enabled agents are rapidly emerging across a wide
range of applications, but their deployment introduces vulnerabilities with
security implications. While prior work has examined prompt-based attacks
(e.g., prompt injection) and data-oriented threats (e.g., data exfiltration),
time-of-check to time-of-use (TOCTOU) remain largely unexplored in this
context. TOCTOU arises when an agent validates external state (e.g., a file or
API response) that is later modified before use, enabling practical attacks
such as malicious configuration swaps or payload injection. In this work, we
present the first study of TOCTOU vulnerabilities in LLM-enabled agents. We
introduce TOCTOU-Bench, a benchmark with 66 realistic user tasks designed to
evaluate this class of vulnerabilities. As countermeasures, we adapt detection
and mitigation techniques from systems security to this setting and propose
prompt rewriting, state integrity monitoring, and tool-fusing. Our study
highlights challenges unique to agentic workflows, where we achieve up to 25%
detection accuracy using automated detection methods, a 3% decrease in
vulnerable plan generation, and a 95% reduction in the attack window. When
combining all three approaches, we reduce the TOCTOU vulnerabilities from an
executed trajectory from 12% to 8%. Our findings open a new research direction
at the intersection of AI safety and systems security.

</details>


### [15] [Exposing Privacy Risks in Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2508.17222)
*Jiale Liu,Jiahao Zhang,Suhang Wang*

Main category: cs.CR

TL;DR: This paper identifies privacy risks in Graph RAG systems, showing they are vulnerable to structured data extraction attacks despite potential text leakage reduction.


<details>
  <summary>Details</summary>
Motivation: The increasing adoption of Retrieval-Augmented Generation (RAG) systems necessitates understanding novel privacy risks introduced by graph-based knowledge architectures, which remain understudied.

Method: Researchers designed and conducted targeted data extraction attacks to evaluate Graph RAG systems' susceptibility to leaking raw text and structured data (entities/relationships).

Result: Graph RAG systems exhibit a critical trade-off: increased vulnerability to structured entity and relationship information leakage, even while reducing raw text leakage exposure.

Conclusion: The study establishes foundational insights into Graph RAG privacy challenges and proposes defensive strategies to address these emerging risks in knowledge-enhanced LLM systems.

Abstract: Retrieval-Augmented Generation (RAG) is a powerful technique for enhancing
Large Language Models (LLMs) with external, up-to-date knowledge. Graph RAG has
emerged as an advanced paradigm that leverages graph-based knowledge structures
to provide more coherent and contextually rich answers. However, the move from
plain document retrieval to structured graph traversal introduces new,
under-explored privacy risks. This paper investigates the data extraction
vulnerabilities of the Graph RAG systems. We design and execute tailored data
extraction attacks to probe their susceptibility to leaking both raw text and
structured data, such as entities and their relationships. Our findings reveal
a critical trade-off: while Graph RAG systems may reduce raw text leakage, they
are significantly more vulnerable to the extraction of structured entity and
relationship information. We also explore potential defense mechanisms to
mitigate these novel attack surfaces. This work provides a foundational
analysis of the unique privacy challenges in Graph RAG and offers insights for
building more secure systems.

</details>


### [16] [Literature Review of the Effect of Quantum Computing on Cryptocurrencies using Blockchain Technology](https://arxiv.org/abs/2508.17296)
*Adi Mutha,Jitendra Sandu*

Main category: cs.CR

TL;DR: This review evaluates quantum computing threats to blockchain cryptocurrencies through Shor and Grover algorithms, identifying vulnerabilities in transaction/consensus mechanisms and proposing post-quantum solutions.


<details>
  <summary>Details</summary>
Motivation: Emerging quantum computing technologies now pose potential cryptographic risks to blockchain-based systems, necessitating immediate scholarly evaluation of quantum resistance strategies before critical infrastructure becomes obsolete.

Method: Comprehensive literature analysis of quantum algorithm implications (Shor for public-key crypto weaknesses, Grover for hash function vulnerabilities), combined with technical assessment of Bitcoin, Ethereum, Litecoin, Monero, and Zcash architectures alongside quantum hardware limitations.

Result: Found Shor's algorithm can compromise digital signatures while Grover's weakens hash security. Identified hardware scalability barriers in current quantum systems but estimated potential timelines for attacks. Evaluated effectiveness of PQC protocols, QKD, memory-intensive proof-of-work, and multi-signature implementations.

Conclusion: Quantum threats require proactive adoption of post-quantum cryptography (PQC) and protocol adaptations. While imminent danger is low, standardized quantum-resistant frameworks must be integrated now to maintain blockchain trust and security fundamentals.

Abstract: With the advent of quantum computing, cryptocurrencies that rely on
blockchain technology face mounting cryptographic vulnerabilities. This paper
presents a comprehensive literature review evaluating how quantum algorithms,
specifically Shors and Grovers, could disrupt the foundational security
mechanisms of cryptocurrencies. Shors algorithm poses a threat to public-key
cryptographic schemes by enabling efficient factorization and discrete
logarithm solving, thereby endangering digital signature systems. Grovers
algorithm undermines hash-based functions, increasing the feasibility of fifty
one percent attacks and hash collisions. By examining the internal mechanisms
of major cryptocurrencies such as Bitcoin, Ethereum, Litecoin, Monero, and
Zcash, this review identifies specific vulnerabilities in transaction and
consensus processes. It further analyses the current hardware limitations of
quantum systems and estimates when such attacks could become feasible. In
anticipation, it investigates countermeasures including Post-Quantum
Cryptography (PQC), Quantum Key Distribution (QKD), and protocol-level
modifications such as memory-intensive proof-of-work algorithms and
multi-signature schemes. The discussion integrates recent advancements in
quantum error correction, hardware scalability, and NIST-standardized
cryptographic algorithms. This review concludes that while quantum computers
are not yet advanced enough to pose an immediate threat, proactive integration
of quantum-resistant solutions is essential. The findings underscore the urgent
need for cryptocurrencies to adopt post-quantum cryptographic standards to
preserve the decentralized trust, integrity, and security that define
blockchain-based digital cryptocurrencies.

</details>


### [17] [An Efficient Recommendation Filtering-based Trust Model for Securing Internet of Things](https://arxiv.org/abs/2508.17304)
*Muhammad Ibn Ziauddin,Rownak Rahad Rabbi,SM Mehrab,Fardin Faiyaz,Mosarrat Jahan*

Main category: cs.CR

TL;DR: This paper introduces a dynamic trust model for IoT that uses harmonic mean of average trust score and time to stabilize trust computation, employs personalized subspace clustering for efficient recommendation filtering, and demonstrates improved accuracy against attacks with 95% reduced filtering time.


<details>
  <summary>Details</summary>
Motivation: Current IoT trust mechanisms face issues like ineffective sliding window lengths causing unreliable computations, recent trust score emphasis leading to errors, and slow clustering for recommendation filtering. These limitations impact data security and system efficiency.

Method: The proposed model dynamically adjusts window length, calculates trust using the harmonic mean of average trust score and time to mitigate fluctuations, and utilizes a personalized subspace clustering algorithm for rapid exclusion of unreliable recommendations.

Result: The model achieves ~44% higher accuracy in detecting on-off attacks compared to existing methods, effectively handles combined attacks and increasing malicious percentages, and reduces recommendation filtering time by 95%.

Conclusion: The dynamic trust model significantly enhances IoT security by addressing timing sensitivity and filtering inefficiencies, offering robust resilience against major trust attacks while maintaining computational efficiency.

Abstract: Trust computation is crucial for ensuring the security of the Internet of
Things (IoT). However, current trust-based mechanisms for IoT have limitations
that impact data security. Sliding window-based trust schemes cannot ensure
reliable trust computation due to their inability to select appropriate window
lengths. Besides, recent trust scores are emphasized when considering the
effect of time on trust. This can cause a sudden change in overall trust score
based on recent behavior, potentially misinterpreting an honest service
provider as malicious and vice versa. Moreover, clustering mechanisms used to
filter recommendations in trust computation often lead to slower results. In
this paper, we propose a robust trust model to address these limitations. The
proposed approach determines the window length dynamically to guarantee
accurate trust computation. It uses the harmonic mean of average trust score
and time to prevent sudden fluctuations in trust scores. Additionally, an
efficient personalized subspace clustering algorithm is used to exclude
recommendations. We present a security analysis demonstrating the resiliency of
the proposed scheme against bad-mouthing, ballot-stuffing, and on-off attacks.
The proposed scheme demonstrates a competitive performance in detecting
bad-mouthing attacks, while outperforming existing works with an approximately
44% improvement in accuracy for detecting on-off attacks. It maintains its
effectiveness even when the percentage of on-off attackers increases and in
scenarios where multiple attacks occur simultaneously. Additionally, the
proposed scheme reduces the recommendation filtering time by 95%.

</details>


### [18] [Risk Assessment and Security Analysis of Large Language Models](https://arxiv.org/abs/2508.17329)
*Xiaoyan Zhang,Dongyang Lyu,Xiaoqi Li*

Main category: cs.CR

TL;DR: This paper proposes a dynamic risk assessment and hierarchical defense framework for large language models (LLMs) to address security challenges like privacy leaks, bias amplification, and malicious attacks in high-risk applications. The system combines entropy weighting, BERT-CRF, dynamic adversarial training, and differential privacy, demonstrating effectiveness in detecting concealed threats and maintaining output quality in sectors like finance.


<details>
  <summary>Details</summary>
Motivation: LLMs face systemic security challenges in critical applications, necessitating a lifecycle framework to mitigate risks such as user data disclosure, harmful instructions, and bias. Existing static approaches fail to address dynamic threats adequately.

Method: A multi-layer defense system: 1) Input layer uses BERT-CRF for malicious command detection; 2) Model layer integrates dynamic adversarial training and differential privacy noise injection; 3) Output layer deploys neural watermarking. Static/dynamic risk metrics are assessed using entropy weighting of features like sensitive word frequency and context deviation.

Result: Experimental results show the system successfully detects stealth attacks (role escape) and maintains real-time risk evaluation accuracy. Financial sector tests confirm its practicality in high-stakes use cases while preserving output quality through defense mechanisms.

Conclusion: The proposed hybrid framework provides comprehensive LLM security through layered defense and dynamic assessment, establishing a robust solution for critical application scenarios by combining technical innovations with practical risk analysis methods.

Abstract: As large language models (LLMs) expose systemic security challenges in high
risk applications, including privacy leaks, bias amplification, and malicious
abuse, there is an urgent need for a dynamic risk assessment and collaborative
defence framework that covers their entire life cycle. This paper focuses on
the security problems of large language models (LLMs) in critical application
scenarios, such as the possibility of disclosure of user data, the deliberate
input of harmful instructions, or the models bias. To solve these problems, we
describe the design of a system for dynamic risk assessment and a hierarchical
defence system that allows different levels of protection to cooperate. This
paper presents a risk assessment system capable of evaluating both static and
dynamic indicators simultaneously. It uses entropy weighting to calculate
essential data, such as the frequency of sensitive words, whether the API call
is typical, the realtime risk entropy value is significant, and the degree of
context deviation. The experimental results show that the system is capable of
identifying concealed attacks, such as role escape, and can perform rapid risk
evaluation. The paper uses a hybrid model called BERT-CRF (Bidirectional
Encoder Representation from Transformers) at the input layer to identify and
filter malicious commands. The model layer uses dynamic adversarial training
and differential privacy noise injection technology together. The output layer
also has a neural watermarking system that can track the source of the content.
In practice, the quality of this method, especially important in terms of
customer service in the financial industry.

</details>


### [19] [Cyber Security Educational Games for Children: A Systematic Literature Review](https://arxiv.org/abs/2508.17414)
*Temesgen Kitaw Damenu,İnci Zaim Gökbay,Alexandra Covaci,Shujun Li*

Main category: cs.CR

TL;DR: A review of 91 cybersecurity educational games shows efficacy but highlights methodological flaws, urging hybrid design approaches for future research.


<details>
  <summary>Details</summary>
Motivation: Educational games are widely used to teach children cybersecurity, but existing research lacks systematic design, methodological rigor, and ethical considerations, necessitating a comprehensive review.

Method: A systematic literature review analyzing 91 educational games reported in 68 papers published between 2010 and 2024.

Result: Positive learning outcomes were observed, but critical gaps were identified, including misalignment in learning outcomes, lack of control groups, limited ethical discussions, and underutilization of emerging technologies.

Conclusion: The paper recommends future research directions such as adopting a hybrid approach to game design and evaluation, combining bottom-up and top-down methods to address identified gaps.

Abstract: Educational games have been widely used to teach children about cyber
security. This systematic literature review reveals evidence of positive
learning outcomes, after analysing 91 such games reported in 68 papers
published between 2010 and 2024. However, critical gaps have also been
identified regarding the design processes and the methodological rigour,
including lack of systematic design, misalignment between proposed and achieved
learning outcomes, rare use of control groups, limited discussions on ethical
considerations, and underutilisation of emerging technologies. We recommend
multiple future research directions, e.g., a hybrid approach to game design and
evaluation that combines bottom-up and top-down approaches.

</details>


### [20] [SoK: Cybersecurity Assessment of Humanoid Ecosystem](https://arxiv.org/abs/2508.17481)
*Priyanka Prakash Surve,Asaf Shabtai,Yuval Elovici*

Main category: cs.CR

TL;DR: The paper introduces a seven-layer security model for humanoid robots and evaluates three real-world platforms using a risk-weighted attack-defense matrix, finding security maturity scores between 39.9% and 79.5%.


<details>
  <summary>Details</summary>
Motivation: Humanoid robots have unique security vulnerabilities compared to conventional CPS due to their complex software stacks and interconnected systems, with existing research focused on isolated threats rather than cascading effects.

Method: Developed a comprehensive seven-layer security model, organized 39 known attacks and 35 defenses across all robot systems, created a quantitative 39x35 attack-defense matrix with risk scoring validated through Monte Carlo analysis, and tested on Pepper, G1 EDU, and Digit robots.

Result: Varying security maturity levels were identified across three evaluated robots (39.9%-79.5% scores), with the methodology enabling systematic cross-platform comparison of security vulnerabilities and countermeasures.

Conclusion: This work establishes a structured framework for evaluating humanoid robot security, addresses cascading threat vulnerabilities, and provides benchmarking methods to prioritize security improvements across the entire robot ecosystem.

Abstract: Humanoids are progressing toward practical deployment across healthcare,
industrial, defense, and service sectors. While typically considered
cyber-physical systems (CPSs), their dependence on traditional networked
software stacks (e.g., Linux operating systems), robot operating system (ROS)
middleware, and over-the-air update channels, creates a distinct security
profile that exposes them to vulnerabilities conventional CPS models do not
fully address. Prior studies have mainly examined specific threats, such as
LiDAR spoofing or adversarial machine learning (AML). This narrow focus
overlooks how an attack targeting one component can cascade harm throughout the
robot's interconnected systems. We address this gap through a systematization
of knowledge (SoK) that takes a comprehensive approach, consolidating
fragmented research from robotics, CPS, and network security domains. We
introduce a seven-layer security model for humanoid robots, organizing 39 known
attacks and 35 defenses across the humanoid ecosystem-from hardware to
human-robot interaction. Building on this security model, we develop a
quantitative 39x35 attack-defense matrix with risk-weighted scoring, validated
through Monte Carlo analysis. We demonstrate our method by evaluating three
real-world robots: Pepper, G1 EDU, and Digit. The scoring analysis revealed
varying security maturity levels, with scores ranging from 39.9% to 79.5%
across the platforms. This work introduces a structured, evidence-based
assessment method that enables systematic security evaluation, supports
cross-platform benchmarking, and guides prioritization of security investments
in humanoid robotics.

</details>


### [21] [Attacking LLMs and AI Agents: Advertisement Embedding Attacks Against Large Language Models](https://arxiv.org/abs/2508.17674)
*Qiming Guo,Jinwen Tang,Xingran Huang*

Main category: cs.CR

TL;DR: This paper introduces Advertisement Embedding Attacks (AEA), a novel LLM security threat that covertly injects promotional/malicious content via adversarial prompt prepending and back-doored checkpoints, compromising information integrity without accuracy loss. It presents a prompt-based self-inspection defense and highlights the need for multi-faceted safety responses.


<details>
  <summary>Details</summary>
Motivation: Current LLM security focuses on accuracy degradation, but information integrity risks (e.g., covert ad injection, propaganda, hate speech) remain under-addressed. AEA threats pose urgent risks to AI agents and model outputs across five stakeholder groups.

Method: AEA utilizes two vectors: (1) hijacking third-party platforms to prepend adversarial prompts during service distribution, and (2) releasing open-source checkpoints fine-tuned with attacker-controlled data to create backdoors. The paper maps these attack vectors and victim groups.

Result: The analysis demonstrates AEA's effectiveness in bypassing detection through information integrity compromise. The proposed self-inspection defense successfully mitigates content injections without requiring model retraining, validating the feasibility of both attack and defense methods.

Conclusion: The research reveals critical gaps in LLM security frameworks, emphasizing the need for collaborative detection strategies, regular safety audits, and regulatory policies to combat adversarial embedding techniques and protect AI systems from content manipulation.

Abstract: We introduce Advertisement Embedding Attacks (AEA), a new class of LLM
security threats that stealthily inject promotional or malicious content into
model outputs and AI agents. AEA operate through two low-cost vectors: (1)
hijacking third-party service-distribution platforms to prepend adversarial
prompts, and (2) publishing back-doored open-source checkpoints fine-tuned with
attacker data. Unlike conventional attacks that degrade accuracy, AEA subvert
information integrity, causing models to return covert ads, propaganda, or hate
speech while appearing normal. We detail the attack pipeline, map five
stakeholder victim groups, and present an initial prompt-based self-inspection
defense that mitigates these injections without additional model retraining.
Our findings reveal an urgent, under-addressed gap in LLM security and call for
coordinated detection, auditing, and policy responses from the AI-safety
community.

</details>


### [22] [TLGLock: A New Approach in Logic Locking Using Key-Driven Charge Recycling in Threshold Logic Gates](https://arxiv.org/abs/2508.17809)
*Abdullah Sahruri,Martin Margala*

Main category: cs.CR

TL;DR: TLGLock is a novel logic locking method using Threshold Logic Gates (TLGs) and charge recycling, offering improved efficiency and security compared to existing techniques.


<details>
  <summary>Details</summary>
Motivation: Hardware piracy necessitates robust logic locking solutions. Current methods suffer from scalability issues and high design overhead (area, power, delay), while requiring complex structures to maintain security.

Method: TLGLock integrates key-dependent functionality via gate-level Threshold Logic Gates (structural expressiveness) and dynamic charge sharing (energy efficiency). A key-weighted gate synthesis flow was implemented and benchmarked across standard logic circuits (ISCAS, ITC, MCNC).

Result: Demonstrates 30% area reduction, 50% delay improvement, 20% power savings vs. latch-based locking. Achieves 3× higher SAT attack resistance than XOR/SFLL-HD with 100% output corruption under incorrect keys in randomized experiments.

Conclusion: TLGLock provides a secure, scalable alternative to traditional logic locking by leveraging TLG structure and charge recycling. Its stateless design and tunable security (via key-weight randomization) address critical tradeoffs between overhead and robustness.

Abstract: Logic locking remains one of the most promising defenses against hardware
piracy, yet current approaches often face challenges in scalability and design
overhead. In this paper, we present TLGLock, a new design paradigm that
leverages the structural expressiveness of Threshold Logic Gates (TLGs) and the
energy efficiency of charge recycling to enforce key-dependent functionality at
the gate level. By embedding the key into the gate's weighted logic and
utilizing dynamic charge sharing, TLGLock provides a stateless and compact
alternative to conventional locking techniques. We implement a complete
synthesis-to-locking flow and evaluate it using ISCAS, ITC, and MCNC
benchmarks. Results show that TLGLock achieves up to 30% area, 50% delay, and
20% power savings compared to latch-based locking schemes. In comparison with
XOR and SFLL-HD methods, TLGLock offers up to 3x higher SAT attack resistance
with significantly lower overhead. Furthermore, randomized key-weight
experiments demonstrate that TLGLock can reach up to 100% output corruption
under incorrect keys, enabling tunable security at minimal cost. These results
position TLGLock as a scalable and resilient solution for secure hardware
design.

</details>


### [23] [Software Unclonable Functions for IoT Devices Identification and Security](https://arxiv.org/abs/2508.17853)
*Saeed Alshehhi*

Main category: cs.CR

TL;DR: The paper investigates the use of hardware performance counter-derived signatures under the concept of software unclonable functions (SUFs) to uniquely identify and verify devices in the IoT ecosystem.


<details>
  <summary>Details</summary>
Motivation: The IoT ecosystem faces challenges in distinguishing legitimate devices from compromised ones, requiring secure, intrinsic authentication methods. HPC-based SUFs aim to address this by leveraging device-specific hardware characteristics for unforgeable identifiers.

Method: The authors evaluates the uniqueness and effectiveness of software unclonable functions (SUFs) derived from hardware performance counters (HPC) through analysis and experiments, focusing on counter values as source for signature generation.

Result: Results demonstrate that HPC-derived signatures exhibit strong software unclonability across devices, enabling accurate identification of cloned or compromised hardware/software counterparts.

Conclusion: HPC-based SUFs provide a reliable mechanism for device authentication in IoT due to their inherent hardware uniqueness and resistance to software replication, offering a practical security solution against device tampering.

Abstract: In the evolving landscape of IoT ecosystem, distinguishing between legitimate
and compromised devices is a critical challenge. This research investigates the
effectiveness of hardware performance counter (HPC)-derived signatures'
uniqueness under the umbrella of a concept that we introduced as software
unclonable functions (SUFs).

</details>


### [24] [MalLoc: Toward Fine-grained Android Malicious Payload Localization via LLMs](https://arxiv.org/abs/2508.17856)
*Tiezhu Sun,Marco Alecci,Aleksandr Pilgun,Yewei Song,Xunzhu Tang,Jordan Samhi,Tegawendé F. Bissyandé,Jacques Klein*

Main category: cs.CR

TL;DR: The paper proposes MalLoc, a novel approach using large language models (LLMs) to improve fine-grained localization of malicious payloads in Android malware, addressing limitations in traditional detection methods.


<details>
  <summary>Details</summary>
Motivation: Traditional Android malware detection techniques fail to adapt to advanced evasion tactics like code obfuscation and dynamic behavior triggering, resulting in poor fine-grained payload localization and hindering effective mitigation strategy design.

Method: MalLoc utilizes LLMs' code understanding capabilities to identify and localize malicious payloads within Android apps through advanced code analysis, enabling more precise detection of obfuscated and behaviorally complex malware.

Result: Experimental results confirm MalLoc's feasibility and effectiveness in fine-grained malicious payload localization, outperforming traditional methods in precision and interpretability of malware behavior analysis.

Conclusion: This work establishes LLMs as a valuable tool for deeper malware behavior analysis, opening new research directions for dynamic threat modeling and targeted countermeasure development against evolving Android threats.

Abstract: The rapid evolution of Android malware poses significant challenges to the
maintenance and security of mobile applications (apps). Traditional detection
techniques often struggle to keep pace with emerging malware variants that
employ advanced tactics such as code obfuscation and dynamic behavior
triggering. One major limitation of these approaches is their inability to
localize malicious payloads at a fine-grained level, hindering precise
understanding of malicious behavior. This gap in understanding makes the design
of effective and targeted mitigation strategies difficult, leaving mobile apps
vulnerable to continuously evolving threats.
  To address this gap, we propose MalLoc, a novel approach that leverages the
code understanding capabilities of large language models (LLMs) to localize
malicious payloads at a fine-grained level within Android malware. Our
experimental results demonstrate the feasibility and effectiveness of using
LLMs for this task, highlighting the potential of MalLoc to enhance precision
and interpretability in malware analysis. This work advances beyond traditional
detection and classification by enabling deeper insights into behavior-level
malicious logic and opens new directions for research, including dynamic
modeling of localized threats and targeted countermeasure development.

</details>


### [25] [PhantomLint: Principled Detection of Hidden LLM Prompts in Structured Documents](https://arxiv.org/abs/2508.17884)
*Toby Murray*

Main category: cs.CR

TL;DR: PhantomLint is a prototype tool developed to detect hidden LLM prompts in structured documents, with low false positives and effectiveness across various document types.


<details>
  <summary>Details</summary>
Motivation: Hidden LLM prompts pose a security threat by manipulating AI-driven document processing systems; detection is critical to maintaining trust in AI-assisted decisions.

Method: The authors implemented a principled approach for hidden prompt detection in structured documents, creating the PhantomLint tool, and evaluated it using a diverse corpus of 3,402 PDF/HTML academic and professional documents.

Result: PhantomLint demonstrated general applicability in detecting hidden prompts, achieved a 0.092% false positive rate, and provided practical utility with acceptable performance in real document analysis.

Conclusion: The paper establishes a reliable method for hidden LLM prompt detection in structured documents, effective against multiple hiding techniques while maintaining minimal false positives, ensuring safety in AI-assisted document workflows.

Abstract: Hidden LLM prompts have appeared in online documents with increasing
frequency. Their goal is to trigger indirect prompt injection attacks while
remaining undetected from human oversight, to manipulate LLM-powered automated
document processing systems, against applications as diverse as r\'esum\'e
screeners through to academic peer review processes. Detecting hidden LLM
prompts is therefore important for ensuring trust in AI-assisted human decision
making.
  This paper presents the first principled approach to hidden LLM prompt
detection in structured documents. We implement our approach in a prototype
tool called PhantomLint. We evaluate PhantomLint against a corpus of 3,402
documents, including both PDF and HTML documents, and covering academic paper
preprints, CVs, theses and more. We find that our approach is generally
applicable against a wide range of methods for hiding LLM prompts from visual
inspection, has a very low false positive rate (approx. 0.092%), is practically
useful for detecting hidden LLM prompts in real documents, while achieving
acceptable performance.

</details>


### [26] [PRZK-Bind: A Physically Rooted Zero-Knowledge Authentication Protocol for Secure Digital Twin Binding in Smart Cities](https://arxiv.org/abs/2508.17913)
*Yagmur Yigit,Mehmet Ali Erturk,Kerem Gursu,Berk Canberk*

Main category: cs.CR

TL;DR: PRZK-Bind is a decentralized authentication protocol for digital twins in smart cities that uses zero-knowledge proofs and elliptic curve cryptography to securely bind physical and digital entities in real-time with low latency and energy consumption.


<details>
  <summary>Details</summary>
Motivation: Existing solutions for physical-digital binding in digital twin ecosystems rely on static trust models, centralized authorities, or lack live verifiable guarantees, making them unsuitable for latency-sensitive and distributed smart city deployments that require secure dynamic environments.

Method: The paper introduces PRZK-Bind, a lightweight protocol combining Schnorr-based zero-knowledge proofs with elliptic curve cryptography to establish secure real-time correspondence between physical entities and their digital twins without pre-shared secrets.

Result: Simulation results demonstrate 4.5× lower latency, 4× reduced energy consumption compared to cryptography-heavy baselines, while achieving false acceptance rates more than 10× lower than existing methods in adversarial environments.

Conclusion: PRZK-Bind offers an efficient, resilient, and trustworthy authentication framework for smart city ecosystems requiring decentralized physical-digital binding with verifiable security guarantees.

Abstract: Digital twin (DT) technology is rapidly becoming essential for smart city
ecosystems, enabling real-time synchronisation and autonomous decision-making
across physical and digital domains. However, as DTs take active roles in
control loops, securely binding them to their physical counterparts in dynamic
and adversarial environments remains a significant challenge. Existing
authentication solutions either rely on static trust models, require
centralised authorities, or fail to provide live and verifiable
physical-digital binding, making them unsuitable for latency-sensitive and
distributed deployments. To address this gap, we introduce PRZK-Bind, a
lightweight and decentralised authentication protocol that combines
Schnorr-based zero-knowledge proofs with elliptic curve cryptography to
establish secure, real-time correspondence between physical entities and DTs
without relying on pre-shared secrets. Simulation results show that PRZK-Bind
significantly improves performance, offering up to 4.5 times lower latency and
4 times reduced energy consumption compared to cryptography-heavy baselines,
while maintaining false acceptance rates more than 10 times lower. These
findings highlight its suitability for future smart city deployments requiring
efficient, resilient, and trustworthy DT authentication.

</details>


### [27] [MoveScanner: Analysis of Security Risks of Move Smart Contracts](https://arxiv.org/abs/2508.17964)
*Yuhe Lu,Zhongwen Li,Xiaoqi Li*

Main category: cs.CR

TL;DR: MoveScanner fills security tool gaps in Move smart contracts by detecting 5+5 critical vulnerabilities with 88.2% accuracy, leveraging novel analysis methods to reduce false positives and uncover 12 new risks.


<details>
  <summary>Details</summary>
Motivation: Smart contracts in Move language face security risks from programming errors and cross-module interactions, with existing tools lacking in detecting resource-oriented vulnerabilities and minimizing false positives.

Method: Developed MoveScanner, a static analysis tool utilizing control flow graph and data flow analysis, cross-module call graph tracking, resource trajectory algorithms, and capability matrix analysis to detect vulnerabilities at the bytecode level.

Result: MoveScanner achieved 88.2% detection accuracy in benchmarks, identified five vulnerability types (including resource leaks and arithmetic overflows), and uncovered 12 new security risks within the Move ecosystem.

Conclusion: MoveScanner addresses security gaps in the Move ecosystem by introducing a static analysis tool that reduces false positives and identifies new vulnerabilities, with future work aiming to integrate formal verification and dynamic analysis for comprehensive security.

Abstract: As blockchain technology continues to evolve, the security of smart contracts
has increasingly drawn attention from both academia and industry. The Move
language, with its unique resource model and linear type system, provides a
solid foundation for the security of digital assets. However, smart contracts
still face new security challenges due to developer programming errors and the
potential risks associated with cross-module interactions. This paper
systematically analyzes the limitations of existing security tools within the
Move ecosystem and reveals their unique vulnerability patterns. To address
these issues, it introduces MoveScanner, a static analysis tool based on a
control flow graph and data flow analysis architecture. By incorporating
cross-module call graph tracking, MoveScanner can effectively identify five key
types of security vulnerabilities, including resource leaks, weak permission
management, and arithmetic overflows. In terms of design, MoveScanner adheres
to a modular principle, supports bytecode-level analysis and multi-chain
adaptation, and introduces innovative resource trajectory tracking algorithms
and capability matrix analysis methods, thereby significantly reducing the
false positive rate. Empirical results show that MoveScanner achieved 88.2%
detection accuracy in benchmark testing, filling the gap in security tools in
the Move ecosystem. Furthermore, this paper identifies twelve new types of
security risks based on the resource-oriented programming paradigm and provides
a theoretical foundation and practical experience for the development of smart
contract security mechanisms. Future work will focus on combining formal
verification and dynamic analysis techniques to build a security protection
framework covering the entire contract lifecycle

</details>


### [28] [Aligning Core Aspects: Improving Vulnerability Proof-of-Concepts via Cross-Source Insights](https://arxiv.org/abs/2508.18109)
*Lingxiao Wang,Wenjing Dang,Mengyao Zhang,Yue Wang,Xianzong Wu,Sen Chen*

Main category: cs.CR

TL;DR: This paper addresses information deficiency in Proof-of-Concept (PoC) reports across public platforms by proposing a multi-source information fusion method to complete missing aspects, successfully improving 40.18% of 173,170 PoC reports analyzed.


<details>
  <summary>Details</summary>
Motivation: PoC reports are critical for demonstrating vulnerability exploitability but suffer from inconsistent templates and missing information across platforms, limiting their quality and usefulness for researchers.

Method: They collected 173,170 PoC reports, defined 8 key aspects using rule-based matching and a fine-tuned BERT-NER model. A multi-source fusion approach was developed to complete missing aspects using CVE entries and cross-referenced PoC reports from different platforms.

Result: The method successfully completed 69,583 PoC reports (40.18% of the total), demonstrating significant information recovery in vulnerability-related PoC evidence.

Conclusion: Multi-source information fusion from CVE entries and diverse PoC platforms can effectively mitigate information deficiency in PoC reports, enhancing their quality and practical value for vulnerability research.

Abstract: For vulnerabilities, Proof-of-Concept (PoC) plays an irreplaceable role in
demonstrating the exploitability. PoC reports may include critical information
such as specific usage, test platforms, and more, providing essential insights
for researchers. However, in reality, due to various PoC templates across PoC
platforms, PoC reports extensively suffer from information deficiency, leading
the suboptimal quality and limited usefulness. Fortunately, we found that
information deficiency of PoC reports could be mitigated by the completion from
multiple sources given the same referred vulnerability. In this paper, we
conduct the first study on the deficiency of information in PoC reports across
public platforms. We began by collecting 173,170 PoC reports from 4 different
platforms and defined 8 key aspects that PoCs should contain. By integrating
rule-based matching and a fine-tuned BERT-NER model for extraction of key
aspects, we discovered that all PoC reports available on public platforms have
at least one missing key aspect. Subsequently, we developed a multi-source
information fusion method to complete the missing aspect information in PoC
reports by leveraging CVE entries and related PoC reports from different
sources. Finally, we successfully completed 69,583 PoC reports (40.18% of all
reports).

</details>


### [29] [Learning from Few Samples: A Novel Approach for High-Quality Malcode Generation](https://arxiv.org/abs/2508.18148)
*Haijian Ma,Daizong Liu,Xiaowen Cai,Pan Zhou,Yulai Xie*

Main category: cs.CR

TL;DR: The paper proposes GANGRL-LLM, a semi-supervised framework combining GANs and LLMs to address limited labeled malicious samples in IDS training. It enhances both malicious code generation and SQLi detection through collaborative adversarial training.


<details>
  <summary>Details</summary>
Motivation: IDS struggles with limited labeled malicious samples, hampering detection model training. Existing methods lack effectiveness in few-sample learning scenarios for evolving cyber threats.

Method: GANGRL-LLM uses collaborative training: 1) GAN discriminator learns malicious patterns via adversarial interactions with generated samples and limited real data. 2) LLM generator improves code synthesis quality using discriminator feedback as reward signals.

Result: Experimental results show significant performance improvements in both malicious code generation and detection with fewer labeled samples compared to existing approaches.

Conclusion: The framework establishes a dual enhancement approach for few-sample IDS training, offering a promising foundation for adaptive defenses against new and evolving cyber threats through synthetic data generation.

Abstract: Intrusion Detection Systems (IDS) play a crucial role in network security
defense. However, a significant challenge for IDS in training detection models
is the shortage of adequately labeled malicious samples. To address these
issues, this paper introduces a novel semi-supervised framework
\textbf{GANGRL-LLM}, which integrates Generative Adversarial Networks (GANs)
with Large Language Models (LLMs) to enhance malicious code generation and SQL
Injection (SQLi) detection capabilities in few-sample learning scenarios.
Specifically, our framework adopts a collaborative training paradigm where: (1)
the GAN-based discriminator improves malicious pattern recognition through
adversarial learning with generated samples and limited real samples; and (2)
the LLM-based generator refines the quality of malicious code synthesis using
reward signals from the discriminator. The experimental results demonstrate
that even with a limited number of labeled samples, our training framework is
highly effective in enhancing both malicious code generation and detection
capabilities. This dual enhancement capability offers a promising solution for
developing adaptive defense systems capable of countering evolving cyber
threats.

</details>


### [30] [$AutoGuardX$: A Comprehensive Cybersecurity Framework for Connected Vehicles](https://arxiv.org/abs/2508.18155)
*Muhammad Ali Nadeem,Bishwo Prakash Pokharel,Naresh Kshetri,Achyut Shankar,Gokarna Sharma*

Main category: cs.CR

TL;DR: This paper proposes AutoGuardX, a cybersecurity framework for connected vehicles integrating ISO standards and advanced technologies like ML-based anomaly detection, IoT protocols, and encrypted communication. It's evaluated via simulations and shows effectiveness against existing and emerging threats.


<details>
  <summary>Details</summary>
Motivation: The surge in cyber-enabled auto theft in the US and Canada highlights the limitations of current security measures for connected vehicles, driven by their increasing integration with IoT and interconnected systems.

Method: 1. Combined ISO/SAE 21434 and ISO 26262 standards 2. Integrated machine learning-based anomaly detection 3. Utilized IoT security protocols and encrypted communication 4. Conducted security simulations on 2019-2023 vehicles from four major brands, testing against relay attacks, CAN bus intrusions, 5G, and quantum computing vulnerabilities.

Result: AutoGuardX demonstrates adaptability, scalability, and practical effectiveness in counteracting existing threats (e.g., relay attacks, CAN intrusions) and mitigating future risks from 5G and quantum computing through simulation results.

Conclusion: AutoGuardX provides a robust, standardized solution for connected vehicle security that bridges current protection gaps while preparing for next-generation cyber risks through its advanced technology integration and simulation-validated effectiveness.

Abstract: The rapid integration of Internet of Things (IoT) and interconnected systems
in modern vehicles not only introduced a new era of convenience, automation,
and connected vehicles but also elevated their exposure to sophisticated cyber
threats. This is especially evident in US and Canada, where cyber-enabled auto
theft has surged in recent years, revealing the limitations of existing
security measures for connected vehicles. In response, this paper proposes
$AutoGuardX$, a comprehensive cybersecurity framework designed specifically for
connected vehicles. $AutoGuardX$ combines key elements from existing recognized
standards for vehicle security, such as ISO/SAE 21434 and ISO 26262, with
advanced technologies, including machine learning-based anomaly detection, IoT
security protocols, and encrypted communication channels. The framework
addresses major attack vectors like relay attacks, controller area network
(CAN) bus intrusions, and vulnerabilities introduced by emerging technologies
such as 5G and quantum computing. $AutoGuardX$ is extensively evaluated through
security simulations across a mix of Sedans and SUVs from four major vehicle
brands manufactured between 2019 and 2023. The results demonstrate the
framework's adaptability, scalability, and practical effectiveness against
existing and emerging threats.

</details>


### [31] [KillChainGraph: ML Framework for Predicting and Mapping ATT&CK Techniques](https://arxiv.org/abs/2508.18230)
*Chitraksh Singh,Monisha Dhanraj,Ken Huang*

Main category: cs.CR

TL;DR: This paper proposes a phase-aware, multi-model ML framework using MITRE ATT&CK data and ATTACK-BERT for proactive cyberattack detection. An ensemble of models with directed graph modeling achieves 97.47%-99.83% F1-scores.


<details>
  <summary>Details</summary>
Motivation: Traditional rule-based cyberattack detection systems lack effectiveness against escalating attack complexity and patterns. There's a need for proactive, adaptive solutions that model attacker behavior holistically.

Method: 1) Semantically maps MITRE ATT&CK techniques to 7 Cyber Kill Chain phases using ATTACK-BERT
2) Trains LightGBM, Transformer encoder, fine-tuned BERT, and GNN models on phase-specific datasets
3) Creates inter-phase attack dependency graphs
4) Integrates model outputs via weighted soft voting ensemble

Result: Ensemble consistently outperformed individual models: F1-scores 97.47%-99.83% (mean +0.03-0.20% improvement over GNN in specific phases). Outperformed pure GNN implementation (97.36%-99.81%).

Conclusion: The graph-driven, phase-aware ensemble approach enables interpretable attack path forecasting by modeling tactical progression between kill chain stages, offering a robust solution for proactive cyber defense.

Abstract: The escalating complexity and volume of cyberattacks demand proactive
detection strategies that go beyond traditional rule-based systems. This paper
presents a phase-aware, multi-model machine learning framework that emulates
adversarial behavior across the seven phases of the Cyber Kill Chain using the
MITRE ATT&CK Enterprise dataset. Techniques are semantically mapped to phases
via ATTACK-BERT, producing seven phase-specific datasets. We evaluate LightGBM,
a custom Transformer encoder, fine-tuned BERT, and a Graph Neural Network
(GNN), integrating their outputs through a weighted soft voting ensemble.
Inter-phase dependencies are modeled using directed graphs to capture attacker
movement from reconnaissance to objectives. The ensemble consistently achieved
the highest scores, with F1-scores ranging from 97.47% to 99.83%, surpassing
GNN performance (97.36% to 99.81%) by 0.03%--0.20% across phases. This
graph-driven, ensemble-based approach enables interpretable attack path
forecasting and strengthens proactive cyber defense.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [32] [Reflective Paper-to-Code Reproduction Enabled by Fine-Grained Verification](https://arxiv.org/abs/2508.16671)
*Mingyang Zhou,Quanming Yao,Lun Du,Lanning Wei,Da Zheng*

Main category: cs.SE

TL;DR: This paper introduces RePro, a framework for reflective paper-to-code reproduction in machine learning (ML) that addresses the challenge of accurately implementing paper details. It uses a 'fingerprint' extraction mechanism to create precise criteria for code verification and iterative refinement, achieving a 13.0% performance improvement on the PaperBench Code-Dev benchmark by systematically detecting and revising discrepancies.


<details>
  <summary>Details</summary>
Motivation: Existing paper reproduction methods struggle with implementation accuracy (mathematical formulas, algorithmic logic) and fail to leverage reflection with explicit feedback. Research papers exhibit high variability in structure, complexity, and configuration requirements, necessitating a systematic debug approach similar to human checklists to ensure faithful reproduction.

Method: RePro 1) extracts a comprehensive set of accurate and atomic 'fingerprints' (implementation criteria) from research papers and 2) uses these fingerprints in an iterative verification/refinement loop during code generation. The framework generates initial code, then systematically identifies discrepancies between generated code and paper specifications (via the fingerprint criteria) and produces targeted revisions to achieve alignment.

Result: RePro achieves a 13.0% performance gap over baseline methods on the PaperBench Code-Dev benchmark. It demonstrates effectiveness in handling complex logical and mathematical criteria during reflection, successfully detecting and revising implementation inconsistencies that prior methods miss.

Conclusion: RePro establishes a systematic framework for improving paper reproduction by leveraging fingerprint-based criteria to guide iterative code verification and refinement. This approach enables machines to replicate the human-like practice of using checklists to debug complex implementations, achieving significant performance gains over existing methods on standardized ML reproduction benchmarks.

Abstract: Reproducing machine learning papers is essential for scientific progress but
remains challenging for both humans and automated agents. Existing agent-based
methods often struggle to fully and accurately reproduce implementation details
such as mathematical formulas and algorithmic logic. Previous studies show that
reflection with explicit feedback improves agent performance. However, current
paper reproduction methods fail to effectively adopt this strategy. This gap
mainly arises from the diverse paper patterns, complex method modules, and
varied configurations encountered in research papers. Motivated by how humans
use systematic checklists to efficiently debug complex code, we propose
\textbf{RePro}, a \textbf{Re}flective Paper-to-Code \textbf{Repro}duction
framework that automatically extracts a paper's fingerprint, referring to a
comprehensive set of accurate and atomic criteria serving as high-quality
supervisory signals. The framework first generates code based on the extracted
information, and then leverages the fingerprint within iterative verification
and refinement loop. This approach systematically detects discrepancies and
produces targeted revisions to align generated code with the paper's
implementation details. Extensive experiments on the PaperBench Code-Dev
benchmark have been conducted, RePro achieves 13.0\% performance gap over
baselines, and it correctly revises complex logical and mathematical criteria
in reflecting, on which the effectiveness is obvious.

</details>


### [33] [Cognitive Agents Powered by Large Language Models for Agile Software Project Management](https://arxiv.org/abs/2508.16678)
*Konrad Cinkusz,Jarosław A. Chudziak,Ewa Niewiadomska-Szynkiewicz*

Main category: cs.SE

TL;DR: This paper explores integrating LLM-powered cognitive agents into the Scaled Agile Framework (SAFe) to enhance software project management via intelligent automation, demonstrating measurable improvements in efficiency, quality, and collaboration.


<details>
  <summary>Details</summary>
Motivation: The study addresses the need for optimizing IT project management by leveraging cognitive agents to handle technical-business alignment, interdependencies, and maintain project agility in complex environments.

Method: The research used the CogniSim ecosystem to simulate software engineering challenges, deploying virtual agents that utilized natural language processing to optimize task delegation, inter-agent communication, and project lifecycle management through iterative testing.

Result: LLM-powered agents improved task completion time, deliverables quality, and communication coherence. They showed scalability and adaptability across diverse project scenarios.

Conclusion: Integrating LLM agents into Agile frameworks can transform collaboration and problem-solving in software engineering, enabling a paradigm shift through enhanced intelligent automation and adaptive decision-making.

Abstract: This paper investigates the integration of cognitive agents powered by Large
Language Models (LLMs) within the Scaled Agile Framework (SAFe) to reinforce
software project management. By deploying virtual agents in simulated software
environments, this study explores their potential to fulfill fundamental roles
in IT project development, thereby optimizing project outcomes through
intelligent automation. Particular emphasis is placed on the adaptability of
these agents to Agile methodologies and their transformative impact on
decision-making, problem-solving, and collaboration dynamics. The research
leverages the CogniSim ecosystem, a platform designed to simulate real-world
software engineering challenges, such as aligning technical capabilities with
business objectives, managing interdependencies, and maintaining project
agility. Through iterative simulations, cognitive agents demonstrate advanced
capabilities in task delegation, inter-agent communication, and project
lifecycle management. By employing natural language processing to facilitate
meaningful dialogues, these agents emulate human roles and improve the
efficiency and precision of Agile practices. Key findings from this
investigation highlight the ability of LLM-powered cognitive agents to deliver
measurable improvements in various metrics, including task completion times,
quality of deliverables, and communication coherence. These agents exhibit
scalability and adaptability, ensuring their applicability across diverse and
complex project environments. This study underscores the potential of
integrating LLM-powered agents into Agile project management frameworks as a
means of advancing software engineering practices. This integration not only
refines the execution of project management tasks but also sets the stage for a
paradigm shift in how teams collaborate and address emerging challenges.

</details>


### [34] [Democratizing AI Development: Local LLM Deployment for India's Developer Ecosystem in the Era of Tokenized APIs](https://arxiv.org/abs/2508.16684)
*Vikranth Udandarao,Nipun Misra*

Main category: cs.SE

TL;DR: The study examines local LLM deployment in India using Ollama to overcome economic and infrastructural barriers, finding it reduces costs by 33% and enables twice as many experimental iterations compared to commercial cloud services.


<details>
  <summary>Details</summary>
Motivation: Indian developers struggle with cost and infrastructure limitations when using commercial LLM APIs for experimentation and learning, creating barriers to inclusive AI development.

Method: Mixed-methods analysis with 180 participants (developers, students, AI enthusiasts) comparing local Ollama deployments against commercial cloud-based LLM services.

Result: Local deployments reduced costs by 33%, enabled >2x more experimental iterations, and improved understanding of advanced AI architectures among 180 participants.

Conclusion: Local LLM deployment is critical for accessible AI development in resource-constrained environments, enhancing learning outcomes and innovation capacity through greater accessibility.

Abstract: India's developer community faces significant barriers to sustained
experimentation and learning with commercial Large Language Model (LLM) APIs,
primarily due to economic and infrastructural constraints. This study
empirically evaluates local LLM deployment using Ollama as an alternative to
commercial cloud-based services for developer-focused applications. Through a
mixed-methods analysis involving 180 Indian developers, students, and AI
enthusiasts, we find that local deployment enables substantially greater
hands-on development and experimentation, while reducing costs by 33% compared
to commercial solutions. Developers using local LLMs completed over twice as
many experimental iterations and reported deeper understanding of advanced AI
architectures. Our results highlight local deployment as a critical enabler for
inclusive and accessible AI development, demonstrating how technological
accessibility can enhance learning outcomes and innovation capacity in
resource-constrained environments.

</details>


### [35] [Cybernaut: Towards Reliable Web Automation](https://arxiv.org/abs/2508.16688)
*Ankur Tomar,Hengyue Liang,Indranil Bhattacharya,Natalia Larios,Francesco Carbone*

Main category: cs.SE

TL;DR: The paper addresses four challenges in deploying LLMs for web automation in enterprise environments and introduces Cybernaut, a framework improving task execution success rates by 23.2% through SOP generation, high-precision DOM element recognition, and consistency metrics.


<details>
  <summary>Details</summary>
Motivation: Existing web automation systems fail to handle poorly-designed internal enterprise web interfaces, creating a critical gap in real-world deployment despite promising opportunities for digital workflow optimization.

Method: Cybernaut introduces (1) a SOP generator for creating reliable automation instructions from user demonstrations, (2) a specialized HTML DOM element recognition system for complex interfaces, and (3) a quantitative metric to measure execution consistency via internal benchmarking.

Result: Achieved 88.68% task execution success rate with 23.2% improvement over browser_use, plus 84.7% accuracy in identifying consistent execution patterns for adaptive automated workflows.

Conclusion: Cybernaut demonstrates significant effectiveness in enterprise-scale web automation, solving key limitations of existing tools and establishing a foundation for future advancements in industrial automation applications.

Abstract: The emergence of AI-driven web automation through Large Language Models
(LLMs) offers unprecedented opportunities for optimizing digital workflows.
However, deploying such systems within industry's real-world environments
presents four core challenges: (1) ensuring consistent execution, (2)
accurately identifying critical HTML elements, (3) meeting human-like accuracy
in order to automate operations at scale and (4) the lack of comprehensive
benchmarking data on internal web applications. Existing solutions are
primarily tailored for well-designed, consumer-facing websites (e.g.,
Amazon.com, Apple.com) and fall short in addressing the complexity of
poorly-designed internal web interfaces. To address these limitations, we
present Cybernaut, a novel framework to ensure high execution consistency in
web automation agents designed for robust enterprise use. Our contributions are
threefold: (1) a Standard Operating Procedure (SOP) generator that converts
user demonstrations into reliable automation instructions for linear browsing
tasks, (2) a high-precision HTML DOM element recognition system tailored for
the challenge of complex web interfaces, and (3) a quantitative metric to
assess execution consistency. The empirical evaluation on our internal
benchmark demonstrates that using our framework enables a 23.2% improvement
(from 72% to 88.68%) in task execution success rate over the browser_use.
Cybernaut identifies consistent execution patterns with 84.7% accuracy,
enabling reliable confidence assessment and adaptive guidance during task
execution in real-world systems. These results highlight Cybernaut's
effectiveness in enterprise-scale web automation and lay a foundation for
future advancements in web automation.

</details>


### [36] [A Scalable Framework for the Management of STPA Requirements: a Case Study on eVTOL Operations](https://arxiv.org/abs/2508.16708)
*Shufeng Chen,Halima El Badaoui,Mariat James Elizebeth,Takuya Nakashima,Siddartha Khastgir,Paul Jennings*

Main category: cs.SE

TL;DR: This paper introduces a scalable Monte-Carlo Simulation-based framework for prioritizing STPA safety requirements, validated in eVTOL operations and contributing to CAP3141 publication.


<details>
  <summary>Details</summary>
Motivation: Traditional safety analysis techniques like FMEA and FTA often miss critical requirements in complex systems, while STPA generates thousands of requirements that need structured prioritization in fast-paced development environments.

Method: The framework integrates STPA outputs with expert evaluations (implementation time, cost, requirement type, regulatory coverage) and uses Monte-Carlo Simulation to reduce subjectivity in rankings, supported by an automation toolchain for dynamic requirement mapping in a scaling matrix visualization.

Result: Validation through eVTOL case study collaboratively with UK Civil Aviation Authority directly informed CAP3141, enabling stakeholders to efficiently manage high-impact requirements across development phases with enhanced traceability.

Conclusion: The presented framework addresses gaps in STPA requirement prioritization, providing a practical solution for safety-critical development in emerging technologies like eVTOL, while supporting systemic risk identification and mitigation through regulatory collaboration.

Abstract: System-Theoretic Process Analysis (STPA) is a recommended method for
analysing complex systems, capable of identifying thousands of safety
requirements often missed by traditional techniques such as Failure Mode and
Effects Analysis (FMEA) and Fault Tree Analysis (FTA). However, the absence of
a structured framework for managing and prioritising these requirements
presents challenges, particularly in fast-paced development environments. This
paper introduces a scalable framework for prioritising STPA-derived
requirements. The framework integrates outputs from each STPA step and
incorporates expert evaluations based on four key factors: implementation time,
cost, requirement type, and regulatory coverage. To reduce subjectivity,
Monte-Carlo Simulation (MCS) is employed to calculate and stabilise requirement
rankings. An automation toolchain supports the framework, enabling dynamic
mapping of prioritised requirements in a scaling matrix. This visualisation
aids decision-making and ensures traceability across development phases. The
framework is applicable from early conceptualisation to more advanced stages,
enhancing its utility in iterative system development. The framework was
validated through a real-world case study focused on Electric Vertical Take-off
and Landing (eVTOL) operations, conducted in collaboration with the UK Civil
Aviation Authority. The findings contributed directly to CAP3141, a Civil
Aviation Publication that identifies systemic operational risks and safety
mitigations for regulators, operators, and vertiports. The prioritisation
process supported decision-making by helping stakeholders identify and manage
high-impact requirements efficiently. This work contributes a practical
solution for managing STPA outputs, bridging gaps in requirement prioritisation
and supporting safety-critical development in emerging technologies.

</details>


### [37] [CelloAI: Leveraging Large Language Models for HPC Software Development in High Energy Physics](https://arxiv.org/abs/2508.16713)
*Mohammad Atif,Kriti Chopra,Ozgur Kilic,Tianle Wang,Zhihua Dong,Charles Leggett,Meifeng Lin,Paolo Calafiura,Salman Habib*

Main category: cs.SE

TL;DR: CelloAI is a locally hosted LLM-powered coding assistant that improves HEP code documentation and generation through RAG, syntax-aware chunking, and callgraph integration, enabling secure and efficient management of scientific codebases.


<details>
  <summary>Details</summary>
Motivation: Next-gen HEP experiments require HPC integration but face significant challenges in porting legacy software to heterogeneous architectures due to complex, sparsely documented codebases.

Method: CelloAI employs retrieval-augmented generation (RAG) from scientific sources for documentation (Doxygen comments, file summaries, chatbot), syntax-preserving code chunking for generation, and callgraph analysis to maintain dependency awareness during code modifications.

Result: Evaluations on ATLAS, CMS, and DUNE codebases demonstrate CelloAI's effectiveness in code comprehension, generation, and retrieval accuracy, while maintaining transparency and data privacy through local deployment.

Conclusion: By combining LLM capabilities with domain-specific code handling techniques, CelloAI addresses critical needs in HEP software development, offering a secure, cost-effective solution for documenting and evolving complex scientific codebases.

Abstract: Next-generation High Energy Physics (HEP) experiments will generate
unprecedented data volumes, necessitating High Performance Computing (HPC)
integration alongside traditional high-throughput computing. However, HPC
adoption in HEP is hindered by the challenge of porting legacy software to
heterogeneous architectures and the sparse documentation of these complex
scientific codebases. We present CelloAI, a locally hosted coding assistant
that leverages Large Language Models (LLMs) with retrieval-augmented generation
(RAG) to support HEP code documentation and generation. This local deployment
ensures data privacy, eliminates recurring costs and provides access to large
context windows without external dependencies. CelloAI addresses two primary
use cases, code documentation and code generation, through specialized
components. For code documentation, the assistant provides: (a) Doxygen style
comment generation for all functions and classes by retrieving relevant
information from RAG sources (papers, posters, presentations), (b) file-level
summary generation, and (c) an interactive chatbot for code comprehension
queries. For code generation, CelloAI employs syntax-aware chunking strategies
that preserve syntactic boundaries during embedding, improving retrieval
accuracy in large codebases. The system integrates callgraph knowledge to
maintain dependency awareness during code modifications and provides
AI-generated suggestions for performance optimization and accurate refactoring.
We evaluate CelloAI using real-world HEP applications from ATLAS, CMS, and DUNE
experiments, comparing different embedding models for code retrieval
effectiveness. Our results demonstrate the AI assistant's capability to enhance
code understanding and support reliable code generation while maintaining the
transparency and safety requirements essential for scientific computing
environments.

</details>


### [38] [EyeMulator: Improving Code Language Models by Mimicking Human Visual Attention](https://arxiv.org/abs/2508.16771)
*Yifan Zhang,Chen Huang,Yueke Zhang,Jiahao Zhang,Toby Jia-Jun Li,Collin McMillan,Kevin Leach,Yu Huang*

Main category: cs.SE

TL;DR: This paper proposes EyeMulator, a technique that adjusts CodeLLM attention mechanisms by incorporating human visual attention weights from eye-tracking datasets during fine-tuning. The method improves performance on code translation, completion, and summarization tasks without requiring eye data at inference.


<details>
  <summary>Details</summary>
Motivation: CodeLLMs rely on machine attention that only considers token salience during training, but human developers intuitively prioritize different tokens. Human visual attention patterns offer valuable insights for improving model performance on software development tasks.

Method: The authors inject special per-token weights into the fine-tuning loss function of CodeLLMs, derived from public eye-tracking datasets. These weights guide the model's attention learning process, modifying traditional attention mechanisms through backpropagation.

Result: EyeMulator achieves better performance than strong LLM baselines on code translation, completion, and summarization tasks. Ablation studies confirm performance improvements are directly linked to learning human attention patterns.

Conclusion: Training CodeLLMs with human attention-weighted inputs significantly enhances their task performance by aligning their attention mechanisms with human intuition, as demonstrated through systematic evaluation and analysis.

Abstract: Code language models (so-called CodeLLMs) are now commonplace in software
development. As a general rule, CodeLLMs are trained by dividing training
examples into input tokens and then learn importance of those tokens in a
process called machine attention. Machine attention is based solely on input
token salience to output token examples during training. Human software
developers are different, as humans intuitively know that some tokens are more
salient than others. While intuition itself is ineffable and a subject of
philosophy, clues about salience are present in human visual attention, since
people tend to look at more salient words more often. In this paper, we present
EyeMulator, a technique for training CodeLLMs to mimic human visual attention
while training for various software development tasks. We add special weights
for each token in each input example to the loss function used during LLM
fine-tuning. We draw these weights from observations of human visual attention
derived from a previously-collected publicly-available dataset of eye-tracking
experiments in software engineering tasks. These new weights ultimately induce
changes in the attention of the subject LLM during training, resulting in a
model that does not need eye-tracking data during inference. Our evaluation
shows that EyeMulator outperforms strong LLM baselines on several tasks such as
code translation, completion and summarization. We further show an ablation
study that demonstrates the improvement is due to subject models learning to
mimic human attention.

</details>


### [39] [DevLicOps: A Framework for Mitigating Licensing Risks in AI-Generated Code](https://arxiv.org/abs/2508.16853)
*Pratyush Nidhi Sharma,Lauren Wright,Anne Herfurth,Munsif Sokiyna,Pratyaksh Nidhi Sharma,Sethu Das,Mikko Siponen*

Main category: cs.SE

TL;DR: The paper introduces DevLicOps, a framework to manage licensing risks from AI coding assistants (ACAs) through governance, incident response, and strategic tradeoffs, ensuring compliance as ACA adoption rises.


<details>
  <summary>Details</summary>
Motivation: ACAs generate code under restrictive open-source licenses (like GPL), risking litigation or forced open-sourcing. Developers lack training on these risks, and global legal standards (especially with outsourcing) create compliance ambiguity.

Method: DevLicOps is designed as a practical framework with three components: (1) Governance policies for ACA use, (2) Incident response protocols to detect/remove non-compliant code, and (3) Data-driven cost-benefit analysis tools to make risk-aware licensing decisions.

Result: The framework enables proactive license compliance, reducing litigation risks from ACA-generated code. Case studies show it supports organizations in balancing innovation speed with legal requirements.

Conclusion: As AI drives software development, DevLicOps provides a scalable solution to manage evolving licensing complexities, promoting responsible use of ACAs in global development environments.

Abstract: Generative AI coding assistants (ACAs) are widely adopted yet pose serious
legal and compliance risks. ACAs can generate code governed by restrictive
open-source licenses (e.g., GPL), potentially exposing companies to litigation
or forced open-sourcing. Few developers are trained in these risks, and legal
standards vary globally, especially with outsourcing. Our article introduces
DevLicOps, a practical framework that helps IT leaders manage ACA-related
licensing risks through governance, incident response, and informed tradeoffs.
As ACA adoption grows and legal frameworks evolve, proactive license compliance
is essential for responsible, risk-aware software development in the AI era.

</details>


### [40] [TriagerX: Dual Transformers for Bug Triaging Tasks with Content and Interaction Based Rankings](https://arxiv.org/abs/2508.16860)
*Md Afif Al Mamun,Gias Uddin,Lan Xia,Longyu Zhang*

Main category: cs.SE

TL;DR: TriagerX is a dual-transformer architecture for bug triaging that improves developer and component recommendation accuracy by combining content-based and interaction-based ranking methods, achieving over 10% improvements in Top-1/Top-3 metrics on five datasets and outperforming SOTA methods by up to 54% in industry deployment.


<details>
  <summary>Details</summary>
Motivation: Existing PLMs for bug triaging struggle with irrelevant token attention and lack consideration of developer interaction history in recommendations, limiting their effectiveness despite better token semantic capture than traditional ML models.

Method: 1) Dual-transformer architecture using recommendations from last three layers of two transformers for content-based ranking. 2) Novel interaction-based ranking method that incorporates developers' historical interactions with similar fixed bugs to refine recommendations.

Result: Outperformed nine transformer-based methods on five datasets (10+ improvement in Top-1/Top-3 accuracy). In industry deployment, showed 10% improvement for component recommendations and 54% improvement for developer recommendations over SOTA baselines.

Conclusion: TriagerX demonstrates significant improvements in bug triaging by combining dual-transformer-based content analysis with interaction history refinement, and has been successfully deployed in a large industry setting where component recommendations help maintain team assignment accuracy despite developer turnover.

Abstract: Pretrained Language Models or PLMs are transformer-based architectures that
can be used in bug triaging tasks. PLMs can better capture token semantics than
traditional Machine Learning (ML) models that rely on statistical features
(e.g., TF-IDF, bag of words). However, PLMs may still attend to less relevant
tokens in a bug report, which can impact their effectiveness. In addition, the
model can be sub-optimal with its recommendations when the interaction history
of developers around similar bugs is not taken into account. We designed
TriagerX to address these limitations. First, to assess token semantics more
reliably, we leverage a dual-transformer architecture. Unlike current
state-of-the-art (SOTA) baselines that employ a single transformer
architecture, TriagerX collects recommendations from two transformers with each
offering recommendations via its last three layers. This setup generates a
robust content-based ranking of candidate developers. TriagerX then refines
this ranking by employing a novel interaction-based ranking methodology, which
considers developers' historical interactions with similar fixed bugs. Across
five datasets, TriagerX surpasses all nine transformer-based methods, including
SOTA baselines, often improving Top-1 and Top-3 developer recommendation
accuracy by over 10%. We worked with our large industry partner to successfully
deploy TriagerX in their development environment. The partner required both
developer and component recommendations, with components acting as proxies for
team assignments-particularly useful in cases of developer turnover or team
changes. We trained TriagerX on the partner's dataset for both tasks, and it
outperformed SOTA baselines by up to 10% for component recommendations and 54%
for developer recommendations.

</details>


### [41] [Mind the Gap: A Decade-Scale Empirical Study of Multi-Stakeholder Dynamics in VR Ecosystem](https://arxiv.org/abs/2508.16903)
*Yijun Lu,Hironori Washizaki,Naoyasu Ubayashi,Nobukazu Yoshioka,Chenhao Wu,Masanari Kondo,Yuyin Ma,Jiong Dong,Jianjin Zhao,Dongqi Han*

Main category: cs.SE

TL;DR: This study developed a multi-view framework to analyze VR ecosystem feedback, revealing gaps in addressing inclusivity and community safety concerns through comparative analysis of user reviews (n=944,320) and developer posts (n=389,477).


<details>
  <summary>Details</summary>
Motivation: Previous research isolated user reviews and developer discussions, missing how specific user concerns are addressed by technical efforts in VR development.

Method: Topic modeling and quantitative impact analysis were applied to user reviews and developer forum posts, systematically comparing and aligning stakeholder perspectives.

Result: Overlap in performance/input method concerns, but significant gaps: LGBTQ+ representation and child-friendly content were raised by users yet rarely discussed in developer forums.

Conclusion: Framework enables data-driven strategies for platform governance and VR system design to bridge user-developer expectations, particularly in under-addressed inclusion domains.

Abstract: In the development and evolution of VR ecosystem, platform stakeholders
continuously adapt their products in response to user and technical feedback,
often reflected in subtle shifts in discussion topics or system updates. A
comprehensive understanding of these changes is essential for identifying gaps
between user expectations and developer actions, which can guide more effective
quality assurance and user-centered innovation. While previous studies have
analyzed either user reviews or developer discussions in isolation, such
approaches typically fail to reveal how specific user concerns are (or are not)
addressed by corresponding technical activities. To address this limitation,
our study introduces a multi-view empirical framework that systematically
compares and aligns stakeholder perspectives. By applying topic modeling and
quantitative impact analysis to 944,320 user reviews and 389,477 developer
posts, we identify not only the overlap in concerns (e.g., performance, input
methods), but also clear gaps in areas like inclusivity and community safety
(e.g., LGBTQ+ representation, child-friendly content). Our findings show that
while users repeatedly raise such issues, they are rarely discussed in
developer forums. These insights enable data-driven recommendations for closing
the user-developer gap in VR ecosystems, offering practical implications for
platform governance and the design of next-generation VR systems.

</details>


### [42] [What Developers Ask to ChatGPT in GitHub Pull Requests? an Exploratory Study](https://arxiv.org/abs/2508.17161)
*Julyanara R. Silva,Carlos Eduardo C. Dantas,Marcelo A. Maia*

Main category: cs.SE

TL;DR: The paper analyzes how developers interact with ChatGPT in software development, categorizing 155 PRs into 14 request types. Key findings include the prevalence of code review/implementation tasks, increased interaction for code generation, and varied use cases like technical explanations and text refinement.


<details>
  <summary>Details</summary>
Motivation: The study addresses the lack of understanding about how developers effectively use ChatGPT to create code contributions, highlighting the need for empirical evidence on real-world tool integration.

Method: Manual evaluation of 155 valid ChatGPT share links from 139 merged PRs to identify and categorize development workflows involving LLMs.

Result: 14 ChatGPT request types grouped into four categories were cataloged. Code generation required more interactions than text/technical prompts. Common request types included code review assistance, task-specific code implementation, technical clarification, and text refinement.

Conclusion: The results suggest developers rely on ChatGPT across multiple development stages, with code generation involving deeper iterative interactions. This provides implications for improving LLM design to better support software development workflows.

Abstract: The emergence of Large Language Models (LLMs), such as ChatGPT, has
introduced a new set of tools to support software developers in solving pro-
gramming tasks. However, our understanding of the interactions (i.e., prompts)
between developers and ChatGPT that result in contributions to the codebase
remains limited. To explore this limitation, we conducted a manual evaluation
of 155 valid ChatGPT share links extracted from 139 merged Pull Requests (PRs),
revealing the interactions between developers and reviewers with ChatGPT that
led to merges into the main codebase. Our results produced a catalog of 14
types of ChatGPT requests categorized into four main groups. We found a
significant number of requests involving code review and the implementation of
code snippets based on specific tasks. Developers also sought to clarify doubts
by requesting technical explanations or by asking for text refinements for
their web pages. Furthermore, we verified that prompts involving code
generation generally required more interactions to produce the desired answer
compared to prompts requesting text review or technical information.

</details>


### [43] [Agentic AI for Software: thoughts from Software Engineering community](https://arxiv.org/abs/2508.17343)
*Abhik Roychoudhury*

Main category: cs.SE

TL;DR: AI agents transcend code generation in software engineering by autonomously managing tasks at code and design levels through intent inference and AI-based verification, enabling them to function as development team members.


<details>
  <summary>Details</summary>
Motivation: Current focus on prompt-driven code generation overlooks broader software engineering tasks; automation requires resolving developer intent to facilitate AI agent integration into complex workflows like architecture exploration and requirements enforcement.

Method: AI agents leverage autonomous micro-decisions supported by program analysis tools, with emerging techniques in intent inference and AI-based verification (V&V) for automatically generated code.

Result: Demonstrated that agentic AI can handle diverse software tasks (testing, repair, architecture) through intent decoding, and highlights the necessity of AI-driven V&V to manage increased automation in code generation and integration.

Conclusion: Future software engineering workflows will require solving intent inference to ensure trustworthy AI agents and integrating AI-based V&V to manage automation's impact on code quality and team collaboration.

Abstract: AI agents have recently shown significant promise in software engineering.
Much public attention has been transfixed on the topic of code generation from
Large Language Models (LLMs) via a prompt. However, software engineering is
much more than programming, and AI agents go far beyond instructions given by a
prompt.
  At the code level, common software tasks include code generation, testing,
and program repair. Design level software tasks may include architecture
exploration, requirements understanding, and requirements enforcement at the
code level. Each of these software tasks involves micro-decisions which can be
taken autonomously by an AI agent, aided by program analysis tools. This
creates the vision of an AI software engineer, where the AI agent can be seen
as a member of a development team.
  Conceptually, the key to successfully developing trustworthy agentic AI-based
software workflows will be to resolve the core difficulty in software
engineering - the deciphering and clarification of developer intent.
Specification inference, or deciphering the intent, thus lies at the heart of
many software tasks, including software maintenance and program repair. A
successful deployment of agentic technology into software engineering would
involve making conceptual progress in such intent inference via agents.
  Trusting the AI agent becomes a key aspect, as software engineering becomes
more automated. Higher automation also leads to higher volume of code being
automatically generated, and then integrated into code-bases. Thus to deal with
this explosion, an emerging direction is AI-based verification and validation
(V & V) of AI generated code. We posit that agentic software workflows in
future will include such AIbased V&V.

</details>


### [44] [Who Wins the Race? (R Vs Python) - An Exploratory Study on Energy Consumption of Machine Learning Algorithms](https://arxiv.org/abs/2508.17344)
*Rajrupa Chattaraj,Sridhar Chimalakonda,Vibhu Saujanya Sharma,Vikrant Kaulgud*

Main category: cs.SE

TL;DR: This paper empirically compares energy consumption and runtime performance of Python and R for ML tasks, finding statistically significant differences (up to 99.8%) in favor of Python. The study highlights the environmental impact of programming language choice in ML development.


<details>
  <summary>Details</summary>
Motivation: While ML adoption is widespread, its energy intensity and carbon footprint are under-studied. Current research lacks comparative analysis of energy consumption across ML programming languages like Python and R, which are widely used but incomparable in environmental impact.

Method: The study performs 10 ML tasks (5 regression, 5 classification) in both Python and R, measuring energy consumption (in Watt-hours) and runtime. It uses an empirical evaluation framework to compare language-specific resource usage.

Result: Python showed lower energy consumption than R in 8/10 training tasks and 6/10 inference tasks. Statistical significance was found in 95% of comparisons, with energy efficiency differences up to 99.16% for training and 99.8% for inferences.

Conclusion: Programming language choice significantly impacts ML energy efficiency (309Wh for Python vs. 345Wh for R in training tasks). These findings suggest that language optimization can reduce ML's environmental footprint and provide empirical evidence for energy-aware software development decisions.

Abstract: The utilization of Machine Learning (ML) in contemporary software systems is
extensive and continually expanding. However, its usage is energy-intensive,
contributing to increased carbon emissions and demanding significant resources.
While numerous studies examine the performance and accuracy of ML, only a
limited few focus on its environmental aspects, particularly energy
consumption. In addition, despite emerging efforts to compare energy
consumption across various programming languages for specific algorithms and
tasks, there remains a gap specifically in comparing these languages for
ML-based tasks. This paper aims to raise awareness of the energy costs
associated with employing different programming languages for ML model training
and inference. Through this empirical study, we measure and compare the energy
consumption along with run-time performance of five regression and five
classification tasks implemented in Python and R, the two most popular
programming languages in this context. Our study results reveal a statistically
significant difference in costs between the two languages in 95% of the cases
examined. Furthermore, our analysis demonstrates that the choice of programming
language can influence energy efficiency significantly, up to 99.16% during
model training and up to 99.8% during inferences, for a given ML task.

</details>


### [45] [Code Difference Guided Fuzzing for FPGA Logic Synthesis Compilers via Bayesian Optimization](https://arxiv.org/abs/2508.17713)
*Zhihao Xu,Shikai Guo,Guilin Zhao,Peiyu Zou,Siwen Wang,Qian Ma,Hui Li,Furui Zhan*

Main category: cs.SE

TL;DR: This paper proposes LSC-Fuzz, a Bayesian optimization-based guided mutation testing tool that detects 16 bugs in FPGA logic synthesis compilers, 12 confirmed by vendors.


<details>
  <summary>Details</summary>
Motivation: Bugs in FPGA logic synthesis compilers pose security risks in safety-critical applications, and existing mutation strategies are limited by their simplistic, blind approaches. Thorough testing is essential for ensuring secure hardware implementations.

Method: LSC-Fuzz uses three components: (1) test-program generation creates HDL code, (2) Bayesian diversity selection optimizes test case diversity, and (3) equivalent check validates synthesis correctness through formal verification techniques.

Result: The tool discovered 16 previously unknown bugs in three months, achieving 12 confirmed fixes with industry-level validation from technical support teams.

Conclusion: Guided mutation via Bayesian optimization significantly improves bug detection effectiveness in FPGA compilers. The authors plan to release LSC-Fuzz to address urgent community needs in this emerging security domain.

Abstract: Field Programmable Gate Arrays (FPGAs) play a crucial role in Electronic
Design Automation (EDA) applications, which have been widely used in
safety-critical environments, including aerospace, chip manufacturing, and
medical devices. A critical step in FPGA development is logic synthesis, which
enables developers to translate their software designs into hardware net lists,
which facilitates the physical implementation of the chip, detailed timing and
power analysis, gate-level simulation, test vector generation, and optimization
and consistency checking. However, bugs or incorrect implementations in FPGA
logic synthesis compilers may lead to unexpected behaviors in target
wapplications, posing security risks. Therefore, it is crucial to eliminate
such bugs in FPGA logic synthesis compilers. The effectiveness of existing
works is still limited by its simple, blind mutation strategy. To address this
challenge, we propose a guided mutation strategy based on Bayesian optimization
called LSC-Fuzz to detect bugs in FPGA logic synthesis compilers. Specifically,
LSC-Fuzz consists of three components: the test-program generation component,
the Bayesian diversity selection component, and the equivalent check component.
By performing test-program generation and Bayesian diversity selection,
LSC-Fuzz generates diverse and complex HDL code, thoroughly testing the FPGA
logic synthesis compilers using equivalent check to detect bugs. Through three
months, LSC-Fuzz has found 16 bugs, 12 of these has been confirmed by official
technical support.

</details>


### [46] [DocFetch - Towards Generating Software Documentation from Multiple Software Artifacts](https://arxiv.org/abs/2508.17719)
*Akhila Sri Manasa Venigalla,Sridhar Chimalakonda*

Main category: cs.SE

TL;DR: The paper introduces DocFetch, a multi-layer prompt-based LLM system for semi-automatically generating documentation from multiple software artifacts. It achieves high BLEU-4 (43.24%) and ROUGE-L (0.39) scores for API/file-related documentation, outperforming existing methods in documentation comprehensiveness and maintenance efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing automated documentation approaches focus only on source code, yet valuable documentation information often appears in co-evolving software artifacts. Maintaining documentation becomes labor-intensive with rapidly growing open-source projects, requiring better methods to leverage these dispersed artifacts.

Method: DocFetch utilizes a multi-layer prompt framework to analyze the DocMine dataset containing five documentation sources. The system generates structured documentation by consolidating and processing information from these diverse software artifacts through an LLM architecture.

Result: Evaluation using a manually curated ground-truth dataset showed DocFetch achieved 43.24% BLEU-4 and 0.39 ROUGE-L for API/file-related documentation. Other documentation types reached ~30% BLEU-4 scores, demonstrating its effectiveness compared to traditional approaches.

Conclusion: DocFetch provides a semi-automated documentation generation solution that maintains project understanding with reduced maintenance effort. The system effectively leverages multi-artifact information through its layered prompting approach, offering practical value for software documentation challenges.

Abstract: Software Documentation plays a major role in the usage and development of a
project. Widespread adoption of open source software projects contributes to
larger and faster development of the projects, making it difficult to maintain
the associated documentation. Existing automated approaches to generate
documentation largely focus on source code. However, information useful for
documentation is observed to be scattered across various artifacts that
co-evolve with the source code. Leveraging this information across multiple
artifacts can reduce the effort involved in maintaining documentation. Hence,
we propose DocFetch, to generate different types of documentation from multiple
software artifacts. We employ a multi-layer prompt based LLM and generate
structured documentation corresponding to different documentation types for the
data consolidated in DocMine dataset. We evaluate the performance of DocFetch
using a manually curated groundtruth dataset by analysing the artifacts in
DocMine. The evaluation yields a highest BLEU-4 score of 43.24% and ROUGE-L
score of 0.39 for generation of api-related and file-related information from
five documentation sources. The generation of other documentation type related
information also reported BLEU-4 scores close to 30% indicating good
performance of the approach. Thus,DocFetch can be employed to
semi-automatically generate documentation, and helps in comprehending the
projects with minimal effort in maintaining the documentation.

</details>


### [47] [RepoTransAgent: Multi-Agent LLM Framework for Repository-Aware Code Translation](https://arxiv.org/abs/2508.17720)
*Ziqi Guan,Xin Yin,Zhiyuan Peng,Chao Ni*

Main category: cs.SE

TL;DR: RepoTransAgent is a multi-agent LLM framework addressing repository-aware code translation challenges through context-aware retrieval, adaptive prompting, and iterative error correction, achieving competitive performance in real-world code repository translation.


<details>
  <summary>Details</summary>
Motivation: Existing code translation approaches face critical bottlenecks in contextual understanding, prompt design, and error correction, severely limiting their effectiveness in translating complex, real-world code repositories.

Method: RepoTransAgent employs a multi-agent LLM framework that decomposes the translation process into context retrieval (using RAG), dynamic prompt construction, and iterative code refinement with reflection-based error correction.

Result: RepoTransAgent achieves 55.34% compile rate and 45.84% pass rate on Java-C# translation from six open-source projects, significantly outperforming state-of-the-art baselines across different LLMs.

Conclusion: RepoTransAgent effectively addresses the limitations of existing code translation methods, demonstrating robustness and generalizability across real-world repository-aware code translation scenarios.

Abstract: Repository-aware code translation is critical for modernizing legacy systems,
enhancing maintainability, and enabling interoperability across diverse
programming languages. While recent advances in large language models (LLMs)
have improved code translation quality, existing approaches face significant
challenges in practical scenarios: insufficient contextual understanding,
inflexible prompt designs, and inadequate error correction mechanisms. These
limitations severely hinder accurate and efficient translation of complex,
real-world code repositories. To address these challenges, we propose
RepoTransAgent, a novel multi-agent LLM framework for repository-aware code
translation. RepoTransAgent systematically decomposes the translation process
into specialized subtasks-context retrieval, dynamic prompt construction, and
iterative code refinement-each handled by dedicated agents. Our approach
leverages retrieval-augmented generation (RAG) for contextual information
gathering, employs adaptive prompts tailored to varying repository scenarios,
and introduces a reflection-based mechanism for systematic error correction. We
evaluate RepoTransAgent on hundreds of Java-C# translation pairs from six
popular open-source projects. Experimental results demonstrate that
RepoTransAgent significantly outperforms state-of-the-art baselines in both
compile and pass rates. Specifically, RepoTransAgent achieves up to 55.34%
compile rate and 45.84% pass rate. Comprehensive analysis confirms the
robustness and generalizability of RepoTransAgent across different LLMs,
establishing its effectiveness for real-world repository-aware code
translation.

</details>


### [48] [Logging Requirement for Continuous Auditing of Responsible Machine Learning-based Applications](https://arxiv.org/abs/2508.17851)
*Patrick Loic Foalem,Leuson Da Silva,Foutse Khomh,Heng Li,Ettore Merlo*

Main category: cs.SE

TL;DR: This paper explores using traditional software logging practices to systematically audit machine learning (ML) models for ethical, legal compliance, and accountability, highlighting deficiencies in current logging methods and opportunities to integrate responsible AI metrics.


<details>
  <summary>Details</summary>
Motivation: ML systems face ethical and legal compliance concerns due to limited transparency, fairness, and accountability. Traditional logging provides traceable records for auditing, debugging, and performance analysis, suggesting its potential for ML systems to address these issues.

Method: The authors conducted an exploratory study analyzing the role of logging in ML model audits. They examined existing logging practices, identified gaps in capturing ethical/auditing dimensions, and proposed integrating responsible AI metrics into logging frameworks through case studies and expert insights.

Result: Analysis revealed critical deficiencies in current logging practices for ML, such as lack of traceability for fairness metrics, insufficient accountability records, and inconsistent performance tracking. Proposed improvements include structured log formats, automated metric extraction, and tooling for real-time compliance checks.

Conclusion: Enhanced logging practices and tooling for ML systems are essential to ensure accountability, transparency, and ethical compliance. The authors provide actionable guidance to improve logging frameworks, aligning ML development with regulatory demands and societal expectations.

Abstract: Machine learning (ML) is increasingly applied across industries to automate
decision-making, but concerns about ethical and legal compliance remain due to
limited transparency, fairness, and accountability. Monitoring through logging
a long-standing practice in traditional software offers a potential means for
auditing ML applications, as logs provide traceable records of system behavior
useful for debugging, performance analysis, and continuous auditing.
systematically auditing models for compliance or accountability. The findings
underscore the need for enhanced logging practices and tooling that
systematically integrate responsible AI metrics. Such practices would support
the development of auditable, transparent, and ethically responsible ML
systems, aligning with growing regulatory requirements and societal
expectations. By highlighting specific deficiencies and opportunities, this
work provides actionable guidance for both practitioners and tool developers
seeking to strengthen the accountability and trustworthiness of ML
applications.

</details>


### [49] [modelSolver: A Symbolic Model-Driven Solver for Power Network Simulation and Monitoring](https://arxiv.org/abs/2508.17882)
*Izudin Dzafic,Rabih A. Jabr*

Main category: cs.SE

TL;DR: modelSolver is an open-box symbolic modeling tool for power systems that eliminates programming barriers, enabling domain experts to define advanced models (e.g., voltage regulators, continuation power flow, Gauss-Newton state estimation) through intuitive mathematical expressions. Compatible with MATPOWER data via automated conversion.


<details>
  <summary>Details</summary>
Motivation: Traditional power system analysis tools require advanced programming skills, creating a barrier for domain experts (e.g., students, practitioners) who lack coding proficiency, particularly when modifying complex built-in models.

Method: The framework uses symbolic mathematical modeling with an open-box approach, allowing users to define custom models using real/complex variables without relying on traditional programming constructs (arrays, loops, sparse matrices).

Result: Successfully implemented power flow and state estimation with advanced capabilities, including voltage regulators, load tap changers, continuation power flow, and Gauss-Newton state estimation with equality constraints. MATPOWER compatibility achieved via an automated data converter.

Conclusion: modelSolver democratizes power system analysis by prioritizing model-driven development, making complex computations accessible without programming expertise while supporting advanced functionalities, thus benefiting a wider audience of domain experts.

Abstract: The development of advanced software tools for power system analysis requires
extensive programming expertise. Even when using open-source tools, programming
skills are essential to modify built-in models. This can be particularly
challenging for domain experts who lack coding proficiency. This paper
introduces modelSolver, a software solution with a new framework centered
around symbolic mathematical modeling. The proposed paradigm facilitates
defining models through intuitive mathematical expressions, thus eliminating
the need for traditional programming constructs such as arrays, loops, and
sparse matrix computations. The modelSolver focuses on power flow and state
estimation using an open-box approach, which allows users to specify custom
models using either real or complex variables. Unlike existing tools that rely
on hard-coded models, modelSolver enables the representation of a wide range of
advanced functionalities, including power flow with voltage regulators and load
tap changers, continuation power flow, and Gauss-Newton state estimation with
equality constraints. Compatibility with MATPOWER is ensured via a converter
that automates importing data files. The framework prioritizes model-driven
development and empowers domain experts to focus on power system modeling
without programming barriers. It aims to simplify power system computations,
making them more accessible to students, scientists, and practitioners.

</details>


### [50] [A Defect Classification Framework for AI-Based Software Systems (AI-ODC)](https://arxiv.org/abs/2508.17900)
*Mohammed O. Alannsary*

Main category: cs.SE

TL;DR: This paper proposes AIODC, a modified Orthogonal Defect Classification framework for AI systems that addresses their unique data, learning, and thinking attributes through phase-based defect analysis and severity classification.


<details>
  <summary>Details</summary>
Motivation: Current defect analysis models cannot effectively capture the unique characteristics of AI systems, necessitating a tailored framework to ensure quality assurance by accounting for data-centric and learning-based attributes.

Method: Extended ODC paradigm by (1) introducing Data/Learning/Thinking classification dimensions with an additional attribute, (2) substituting traditional impact areas with AI-specific characteristics, and (3) defining a new severity level framework validated using a public ML bug case study.

Result: Learning phase defects dominated (most prevalent) and correlated with high severity classifications. Thinking phase defects showed disproportionate impact on trustworthiness and accuracy metrics, verified through one-way and two-way statistical analysis of the public dataset.

Conclusion: AIODC successfully identifies high-risk defect categories in AI systems, enabling targeted quality assurance strategies that address critical failure points in learning processes and decision-making accuracy.

Abstract: Artificial Intelligence has gained a lot of attention recently, it has been
utilized in several fields ranging from daily life activities, such as
responding to emails and scheduling appointments, to manufacturing and
automating work activities. Artificial Intelligence systems are mainly
implemented as software solutions, and it is essential to discover and remove
software defects to assure its quality using defect analysis which is one of
the major activities that contribute to software quality. Despite the
proliferation of AI-based systems, current defect analysis models fail to
capture their unique attributes. This paper proposes a framework inspired by
the Orthogonal Defect Classification (ODC) paradigm and enables defect analysis
of Artificial Intelligence systems while recognizing its special attributes and
characteristics. This study demonstrated the feasibility of modifying ODC for
AI systems to classify its defects. The ODC was adjusted to accommodate the
Data, Learning, and Thinking aspects of AI systems which are newly introduced
classification dimensions. This adjustment involved the introduction of an
additional attribute to the ODC attributes, the incorporation of a new severity
level, and the substitution of impact areas with characteristics pertinent to
AI systems. The framework was showcased by applying it to a publicly available
Machine Learning bug dataset, with results analyzed through one-way and two-way
analysis. The case study indicated that defects occurring during the Learning
phase were the most prevalent and were significantly linked to high-severity
classifications. In contrast, defects identified in the Thinking phase had a
disproportionate effect on trustworthiness and accuracy. These findings
illustrate AIODC's capability to identify high-risk defect categories and
inform focused quality assurance measures.

</details>


### [51] [Evaluating Citizen Satisfaction with Saudi Arabia's E-Government Services: A Standards-Based, Theory-Informed Approach](https://arxiv.org/abs/2508.17912)
*Mohammed O. Alannsary*

Main category: cs.SE

TL;DR: This study evaluates citizen satisfaction with Saudi Arabia's e-government services using ISO standards and UTAUT, revealing high usability and trust but issues with service clarity, responsiveness, and emotional engagement.


<details>
  <summary>Details</summary>
Motivation: To enhance public service delivery by improving usability, trust, and inclusivity in digital government platforms through citizen assessment.

Method: A structured questionnaire based on ISO/IEC 25010, 25022, and UTAUT was administered to 500 Saudi citizens (276 valid responses). Satisfaction was analyzed across four dimensions: overall satisfaction, feature satisfaction, trust, and emotional engagement (pleasure).

Result: High satisfaction in usability and trust, moderate satisfaction in service clarity and responsiveness, and low emotional engagement. Findings correlate with Saudi Arabia's top global e-government development ranking.

Conclusion: The study provides actionable insights for policymakers to address service clarity and responsiveness gaps while advancing the theoretical integration of technical standards and behavioral adoption models in the e-government context.

Abstract: As digital government platforms become central to public service delivery,
understanding citizen assessment is crucial for enhancing usability, trust, and
inclusivity. This study investigates citizen satisfaction with the e-government
services in Saudi Arabia through a quality-in-use framework based on ISO/IEC
25010 and ISO/IEC 25022 standards, interpreted through the lens of the Unified
Theory of Acceptance and Use of Technology (UTAUT). A structured questionnaire
was administered to 500 citizens, yielding 276 valid responses. Satisfaction
was evaluated across four dimensions: overall satisfaction, feature
satisfaction, trust, and emotional engagement (pleasure). The findings
demonstrate consistently high levels of satisfaction regarding usability and
trust, aligning with Saudi Arabia's top-tier global ranking in e-government
development. However, the results also highlight persistent challenges related
to service clarity and system responsiveness. Emotional engagement was limited,
indicating that users perceive these services primarily as functional tools
rather than as engaging digital experiences. The study offers valuable insights
for policymakers and contributes to the theoretical integration of
standards-based and behavioral adoption models in the context of citizenship.

</details>


### [52] [DesCartes Builder: A Tool to Develop Machine-Learning Based Digital Twins](https://arxiv.org/abs/2508.17988)
*Eduardo de Conto,Blaise Genest,Arvind Easwaran,Nicholas Ng,Shweta Menon*

Main category: cs.SE

TL;DR: This paper introduces DesCartes Builder, an open-source tool for systematic engineering of ML-based digital twins in civil engineering, addressing the need for structured multi-model workflows.


<details>
  <summary>Details</summary>
Motivation: Digital twins require fast, accurate, and maintainable ML pipelines with multiple task/domain-dependent models, but current approaches remain ad hoc and lack systematic design methodologies.

Method: Developed an open-source visual data flow tool with parameterizable operations and domain-tailored ML algorithms to design DT prototypes and instances through model composition/reuse.

Result: Demonstrated effectiveness via a civil engineering case study showing real-time plastic strain prediction of structures using the proposed pipeline framework.

Conclusion: DesCartes Builder provides a structured, reusable approach to ML-based digital twin engineering, improving maintainability and adaptability for complex civil systems.

Abstract: Digital twins (DTs) are increasingly utilized to monitor, manage, and
optimize complex systems across various domains, including civil engineering. A
core requirement for an effective DT is to act as a fast, accurate, and
maintainable surrogate of its physical counterpart, the physical twin (PT). To
this end, machine learning (ML) is frequently employed to (i) construct
real-time DT prototypes using efficient reduced-order models (ROMs) derived
from high-fidelity simulations of the PT's nominal behavior, and (ii)
specialize these prototypes into DT instances by leveraging historical sensor
data from the target PT. Despite the broad applicability of ML, its use in DT
engineering remains largely ad hoc. Indeed, while conventional ML pipelines
often train a single model for a specific task, DTs typically require multiple,
task- and domain-dependent models. Thus, a more structured approach is required
to design DTs.
  In this paper, we introduce DesCartes Builder, an open-source tool to enable
the systematic engineering of ML-based pipelines for real-time DT prototypes
and DT instances. The tool leverages an open and flexible visual data flow
paradigm to facilitate the specification, composition, and reuse of ML models.
It also integrates a library of parameterizable core operations and ML
algorithms tailored for DT design. We demonstrate the effectiveness and
usability of DesCartes Builder through a civil engineering use case involving
the design of a real-time DT prototype to predict the plastic strain of a
structure.

</details>


### [53] [Previously on... Automating Code Review](https://arxiv.org/abs/2508.18003)
*Robert Heumüller,Frank Ortmeier*

Main category: cs.SE

TL;DR: This study offers a comprehensive analysis of Modern Code Review (MCR) automation research from 2015-2024, identifying 24 relevant studies. It highlights challenges like task variability, dataset reuse limitations, and methodological gaps (e.g., temporal bias) while advocating for standardization and actionable research guidance.


<details>
  <summary>Details</summary>
Motivation: Modern Code Review is time-intensive and resource-heavy, and prior automation research suffers from inconsistent task definitions, datasets, and evaluation practices. This analysis aims to clarify the field's evolution, standardize learning tasks, address methodological flaws, and guide future work.

Method: A systematic survey of 691 publications identified 24 relevant MCR automation studies. Each was analyzed using a framework spanning tasks, models, metrics, baselines, results, validity concerns, and dataset reuse patterns.

Result: Analysis revealed 48 unique task-metric combinations (with 22 being paper-specific), low dataset reuse, and underaddressed challenges like temporal bias. Methodological gaps and variability in evaluation practices were systematically documented.

Conclusion: The paper establishes a clearer understanding of MCR automation research, emphasizes the need for standardized evaluation practices, and provides concrete recommendations to address challenges like temporal bias while avoiding redundant work.

Abstract: Modern Code Review (MCR) is a standard practice in software engineering, yet
it demands substantial time and resource investments. Recent research has
increasingly explored automating core review tasks using machine learning (ML)
and deep learning (DL). As a result, there is substantial variability in task
definitions, datasets, and evaluation procedures. This study provides the first
comprehensive analysis of MCR automation research, aiming to characterize the
field's evolution, formalize learning tasks, highlight methodological
challenges, and offer actionable recommendations to guide future research.
Focusing on the primary code review tasks, we systematically surveyed 691
publications and identified 24 relevant studies published between May 2015 and
April 2024. Each study was analyzed in terms of tasks, models, metrics,
baselines, results, validity concerns, and artifact availability. In
particular, our analysis reveals significant potential for standardization,
including 48 task metric combinations, 22 of which were unique to their
original paper, and limited dataset reuse. We highlight challenges and derive
concrete recommendations for examples such as the temporal bias threat, which
are rarely addressed so far. Our work contributes to a clearer overview of the
field, supports the framing of new research, helps to avoid pitfalls, and
promotes greater standardization in evaluation practices.

</details>


### [54] [A Large-Scale Study on Developer Engagement and Expertise in Configurable Software System Projects](https://arxiv.org/abs/2508.18070)
*Karolina M. Milano,Wesley K. G. Assunção,Bruno B. P. Cafeo*

Main category: cs.SE

TL;DR: This paper analyzes the distribution of variable code expertise in CSS projects, revealing that 59% of developers never touch it while 17% handle 83%. Existing metrics poorly identify such expertise, emphasizing the need for improved task allocation and refinement of expertise models.


<details>
  <summary>Details</summary>
Motivation: CSSs' variability via pre-processor directives complicates maintenance and poses risks. It is unclear how variable code responsibilities are distributed among developers or if conventional metrics can assess proficiency in this area.

Method: Repository mining of 25 CSS projects with 450,255 commits from 9,678 developers, comparing variable vs. mandatory code engagement patterns and testing standard expertise metrics.

Result: 59% of developers never modified variable code; 17% controlled 83% of it. Standard metrics achieved 55% precision/50% recall for identifying variable code contributors.

Conclusion: Variable code responsibilities are highly concentrated among specialists, challenging current expertise metrics. Task assignment strategies must be adjusted to balance workloads and better leverage variable code proficiency.

Abstract: Modern systems operate in multiple contexts making variability a fundamental
aspect of Configurable Software Systems (CSSs). Variability, implemented via
pre-processor directives (e.g., #ifdef blocks) interleaved with other code and
spread across files, complicates maintenance and increases error risk. Despite
its importance, little is known about how variable code is distributed among
developers or whether conventional expertise metrics adequately capture
variable code proficiency. This study investigates developers' engagement with
variable versus mandatory code, the concentration of variable code workload,
and the effectiveness of expertise metrics in CSS projects. We mined
repositories of 25 CSS projects, analyzing 450,255 commits from 9,678
developers. Results show that 59% of developers never modified variable code,
while about 17% were responsible for developing and maintaining 83% of it. This
indicates a high concentration of variable code expertise among a few
developers, suggesting that task assignments should prioritize these
specialists. Moreover, conventional expertise metrics performed
poorly--achieving only around 55% precision and 50% recall in identifying
developers engaged with variable code. Our findings highlight an unbalanced
distribution of variable code responsibilities and underscore the need to
refine expertise metrics to better support task assignments in CSS projects,
thereby promoting a more equitable workload distribution.

</details>


### [55] [Debian in the Research Software Ecosystem: A Bibliometric Analysis](https://arxiv.org/abs/2508.18073)
*Joenio Marques da Costa,Christina von Flach*

Main category: cs.SE

TL;DR: This paper presents a bibliometric study of Debian-related academic publications, analyzing research trends and contributions through co-citation, co-authorship, and word co-occurrence metrics using Scopus data.


<details>
  <summary>Details</summary>
Motivation: Debian's historical involvement in scientific projects highlights its relevance to the Research Software ecosystem, prompting the need to understand its academic impact and identify research opportunities.

Method: Bibliometric analysis of Scopus publications containing 'Debian' in titles/abstracts/keywords, involving co-citation, co-authorship, and word co-occurrence metrics with pre-defined inclusion/exclusion criteria.

Result: A comprehensive map of Debian's academic research landscape, including interdisciplinary articles, demographic trends, and open access to data via public repositories for collaboration.

Conclusion: The analysis reveals intellectual structures and trends in Debian-related research, providing actionable insights for the scientific community to focus on underrepresented areas.

Abstract: Context: The Debian system has historically participated in academic works
and scientific projects, with well-known examples including NeuroDebian, Debian
Med, Debsources, Debian Science, and Debian GIS, where the scientific relevance
of Debian and its contribution to the Research Software ecosystem are evident.
  Objective: The objective of this study is to investigate the Debian system
through academic publications, with the aim of classifying articles, mapping
research, identifying trends, and finding opportunities.
  Method: The study is based on a bibliometric analysis starting with an
initial search for the term "Debian" in the titles, abstracts, or keywords of
academic publications, using the Scopus database. This analysis calculates
metrics of co-citation, co-authorship, and word co-occurrence, and is guided by
a set of research questions and criteria for inclusion and exclusion to conduct
the bibliometric analysis.
  Results: The study includes a set of articles published across various fields
of knowledge, providing a map of the academic publication space about Debian.
The study's data will be available in a public repository, reporting
demographic and bibliometric trends, including the most cited articles, active
countries, researchers, and popular conferences.
  Conclusion: Results includes a bibliometric and demographic analysis
identified in publications about Debian, shedding light on the intellectual
structure of academic research. The results of the analyses can help
researchers gain an overview of existing trends in publications about Debian
and identify areas that require more attention from the scientific community.

</details>


### [56] [LLM-Guided Genetic Improvement: Envisioning Semantic Aware Automated Software Evolution](https://arxiv.org/abs/2508.18089)
*Karine Even-Mendoza,Alexander Brownlee,Alina Geiger,Carol Hanna,Justyna Petke,Federica Sarro,Dominik Sobania*

Main category: cs.SE

TL;DR: This paper introduces PatchCat, a method that integrates semantic-aware edits from Large Language Models (LLMs) into Search-based Genetic Improvement (GI) for better code customization. PatchCat clusters LLM-suggested patches, identifies 18 patch types accurately, detects NoOp edits early, and uses small LLMs to save resources, marking a step toward interpretable, efficient GI.


<details>
  <summary>Details</summary>
Motivation: Search-based GI optimizes software syntactically but lacks semantic understanding, while LLMs provide semantics but no goal-directed control. The paper addresses this gap to enable effective, interpretable AI-driven GI that combines semantic awareness and efficiency.

Method: PatchCat employes automated clustering of LLM-suggested software patches into semantic categories. It leverages small local LLMs to analyze edits, identify patterns, and detect NoOp (non-operational) edits early, reducing the need for test suite executions.

Result: PatchCat identified 18 distinct patch categories with high accuracy. Early NoOp detection allowed up to 77% reduction in unnecessary test runs. The system works efficiently using small LLMs and demonstrates significant resource and time savings in GI workflows.

Conclusion: PatchCat establishes a promising foundation for semantic-aware GI by effectively categorizing LLM edits and reducing test overhead. The paper advocates for community collaboration to advance theory and applications of LLM-driven code mutations, emphasizing eco-friendly and interpretable software improvement.

Abstract: Genetic Improvement (GI) of software automatically creates alternative
software versions that are improved according to certain properties of
interests (e.g., running-time). Search-based GI excels at navigating large
program spaces, but operates primarily at the syntactic level. In contrast,
Large Language Models (LLMs) offer semantic-aware edits, yet lack goal-directed
feedback and control (which is instead a strength of GI). As such, we propose
the investigation of a new research line on AI-powered GI aimed at
incorporating semantic aware search. We take a first step at it by augmenting
GI with the use of automated clustering of LLM edits. We provide initial
empirical evidence that our proposal, dubbed PatchCat, allows us to
automatically and effectively categorize LLM-suggested patches. PatchCat
identified 18 different types of software patches and categorized newly
suggested patches with high accuracy. It also enabled detecting NoOp edits in
advance and, prospectively, to skip test suite execution to save resources in
many cases. These results, coupled with the fact that PatchCat works with
small, local LLMs, are a promising step toward interpretable, efficient, and
green GI. We outline a rich agenda of future work and call for the community to
join our vision of building a principled understanding of LLM-driven mutations,
guiding the GI search process with semantic signals.

</details>


### [57] [A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code](https://arxiv.org/abs/2508.18106)
*Keke Lian,Bin Wang,Lei Zhang,Libo Chen,Junjie Wang,Ziming Zhao,Yujiu Yang,Haotong Duan,Haoran Zhao,Shuang Liao,Mingda Guo,Jiazheng Quan,Yilu Zhong,Chenhao He,Zichuan Chen,Jie Wu,Haoling Li,Zhaoxuan Li,Jiongchi Yu,Hui Li,Dong Zhang*

Main category: cs.SE

TL;DR: A.S.E introduces a repository-level benchmark for secure code generation using real-world CVEs and reproducible evaluations, revealing model performance patterns.


<details>
  <summary>Details</summary>
Motivation: Existing code generation security benchmarks are inadequate due to isolated code snippets, unstable evaluation methods, and a lack of correlation between input context quality and output security.

Method: Constructed tasks from real-world repositories with documented CVEs, preserving full context through build systems and cross-file dependencies. Uses containerized evaluation with expert-defined security, build quality, and generation stability metrics. 

Result: 1. Claude-3.7-Sonnet achieves best overall performance. 2. Security performance gap between proprietary and open-source models is narrow (Qwen3-235B-A22B-Instruct top security score). 3. Simple decoding strategies outperform complex reasoning for security patches.

Conclusion: A.S.E provides a more effective security evaluation framework than prior benchmarks, uncovering critical model performance characteristics while emphasizing the importance of context preservation and evaluation stability in secure code generation.

Abstract: The increasing adoption of large language models (LLMs) in software
engineering necessitates rigorous security evaluation of their generated code.
However, existing benchmarks are inadequate, as they focus on isolated code
snippets, employ unstable evaluation methods that lack reproducibility, and
fail to connect the quality of input context with the security of the output.
To address these gaps, we introduce A.S.E (AI Code Generation Security
Evaluation), a benchmark for repository-level secure code generation. A.S.E
constructs tasks from real-world repositories with documented CVEs, preserving
full repository context like build systems and cross-file dependencies. Its
reproducible, containerized evaluation framework uses expert-defined rules to
provide stable, auditable assessments of security, build quality, and
generation stability. Our evaluation of leading LLMs on A.S.E reveals three key
findings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The
security gap between proprietary and open-source models is narrow;
Qwen3-235B-A22B-Instruct attains the top security score. (3) Concise,
``fast-thinking'' decoding strategies consistently outperform complex,
``slow-thinking'' reasoning for security patching.

</details>
