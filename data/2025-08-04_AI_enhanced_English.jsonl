{"id": "2508.00031", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00031", "abs": "https://arxiv.org/abs/2508.00031", "authors": ["Junde Wu"], "title": "Git Context Controller: Manage the Context of LLM-based Agents like Git", "comment": "in updating", "summary": "Large language model (LLM) based agents have shown impressive capabilities by\ninterleaving internal reasoning with external tool use. However, as these\nagents are deployed in long-horizon workflows, such as coding for a big,\nlong-term project, context management becomes a critical bottleneck. We\nintroduce Git-Context-Controller (GCC), a structured context management\nframework inspired by software version control systems. GCC elevates context as\nversioned memory hierarchy like Git. It structures agent memory as a persistent\nfile system with explicit operations: COMMIT, BRANCH, MERGE, and CONTEXT,\nenabling milestone-based checkpointing, exploration of alternative plans, and\nstructured reflection. Our approach empowers agents to manage long-term goals,\nisolate architectural experiments, and recover or hand off memory across\nsessions and agents. Empirically, agents equipped with GCC achieve\nstate-of-the-art performance on the SWE-Bench-Lite benchmark, resolving 48.00\nof software bugs, outperforming 26 competitive systems. In a self-replication\ncase study, a GCC-augmented agent builds a new CLI agent from scratch,\nachieving 40.7 task resolution, compared to only 11.7 without GCC. The code is\nreleased at: https://github.com/theworldofagents/GCC", "AI": {"tldr": "GCC is a Git-inspired context management framework for agents that improves long-horizon task performance through versioned memory operations (COMMIT/BRANCH/MERGE/CONTEXT), achieving 48% bug resolution and tripling self-replication task performance.", "motivation": "Long-horizon agent workflows (e.g. large-scale software projects) face critical context management challenges as existing agents struggle to maintain and organize evolving knowledge states over extended interaction sequences.", "method": "Structures agent memory as a version-controlled file system using Git-like operations: COMMIT for checkpointing progress, BRANCH for isolating experiments, MERGE for consolidating paths, and CONTEXT for selecting active memories, enabling milestone tracking and structured planning.", "result": "SOTA on SWE-Bench-Lite (48.00% bug resolution vs. 26 systems), 40.7 task resolution in self-replication case study (3.5\u00d7 improvement over non-GCC agents), with code repository provided for replication.", "conclusion": "Versioned context management through GCC framework enables agents to successfully handle complex long-term goals by systematically organizing knowledge, isolating experimental variations, and facilitating memory handoff between sessions/agents."}}
{"id": "2508.00033", "categories": ["cs.SE", "cs.AI", "cs.CL", "68T50", "I.2.2; I.2.7; D.2.3"], "pdf": "https://arxiv.org/pdf/2508.00033", "abs": "https://arxiv.org/abs/2508.00033", "authors": ["Nuno Fachada", "Daniel Fernandes", "Carlos M. Fernandes", "Bruno D. Ferreira-Saraiva", "Jo\u00e3o P. Matos-Carvalho"], "title": "GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries", "comment": null, "summary": "Large Language Models (LLMs) have advanced rapidly as tools for automating\ncode generation in scientific research, yet their ability to interpret and use\nunfamiliar Python APIs for complex computational experiments remains poorly\ncharacterized. This study systematically benchmarks a selection of\nstate-of-the-art LLMs in generating functional Python code for two increasingly\nchallenging scenarios: conversational data analysis with the \\textit{ParShift}\nlibrary, and synthetic data generation and clustering using \\textit{pyclugen}\nand \\textit{scikit-learn}. Both experiments use structured, zero-shot prompts\nspecifying detailed requirements but omitting in-context examples. Model\noutputs are evaluated quantitatively for functional correctness and prompt\ncompliance over multiple runs, and qualitatively by analyzing the errors\nproduced when code execution fails. Results show that only a small subset of\nmodels consistently generate correct, executable code, with GPT-4.1 standing\nout as the only model to always succeed in both tasks. In addition to\nbenchmarking LLM performance, this approach helps identify shortcomings in\nthird-party libraries, such as unclear documentation or obscure implementation\nbugs. Overall, these findings highlight current limitations of LLMs for\nend-to-end scientific automation and emphasize the need for careful prompt\ndesign, comprehensive library documentation, and continued advances in language\nmodel capabilities.", "AI": {"tldr": "This paper benchmarks state-of-the-art LLMs in generating executable Python code for scientific tasks without in-context examples, finding that only GPT-4.1 consistently succeeds across both conversational data analysis (ParShift) and synthetic data generation/clustering (pyclugen+scikit-learn) tasks.", "motivation": "Scientific research increasingly requires Python API usage for complex computational experiments, but LLM capabilities in this domain - particularly with zero-shot reasoning about unfamiliar libraries - remain poorly understood.", "method": "Used structured zero-shot prompts on two tasks: 1) conversational data analysis with ParShift, and 2) synthetic data generation and clustering using pyclugen and scikit-learn. Evaluated code functionality, prompt compliance across multiple model runs, and analyzed execution errors.", "result": "Only GPT-4.1 achieved 100% success rate in both tasks. Other models showed significant variability. Evaluation revealed LLMs require more robust prompting strategies and highlights issues in third-party libraries (e.g., documentation gaps, obscure bugs).", "conclusion": "Current LLM capabilities are limited for end-to-end scientific automation. Highlights the need for: 1) improved zero-shot prompt design, 2) better library documentation, and 3) continued LLM development to handle complex computational workflows."}}
{"id": "2508.00045", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00045", "abs": "https://arxiv.org/abs/2508.00045", "authors": ["Samah Kansab"], "title": "Machine Learning Pipeline for Software Engineering: A Systematic Literature Review", "comment": null, "summary": "The rapid advancement of software development practices has introduced\nchallenges in ensuring quality and efficiency across the software engineering\n(SE) lifecycle. As SE systems grow in complexity, traditional approaches often\nfail to scale, resulting in longer debugging times, inefficient defect\ndetection, and resource-heavy development cycles. Machine Learning (ML) has\nemerged as a key solution, enabling automation in tasks such as defect\nprediction, code review, and release quality estimation. However, the\neffectiveness of ML in SE depends on the robustness of its pipeline, including\ndata collection, preprocessing, feature engineering, algorithm selection,\nvalidation, and evaluation.\n  This systematic literature review (SLR) examines state-of-the-art ML\npipelines designed for SE, consolidating best practices, challenges, and gaps.\nOur findings show that robust preprocessing, such as SMOTE for data balancing\nand SZZ-based algorithms for feature selection, improves model reliability.\nEnsemble methods like Random Forest and Gradient Boosting dominate performance\nacross tasks, while simpler models such as Naive Bayes remain valuable for\nefficiency and interpretability. Evaluation metrics including AUC, F1-score,\nand precision are most common, with new metrics like Best Arithmetic Mean (BAM)\nemerging in niche applications. Validation techniques such as bootstrapping are\nwidely used to ensure model stability and generalizability.\n  This SLR highlights the importance of well-designed ML pipelines for\naddressing SE challenges and provides actionable insights for researchers and\npractitioners seeking to optimize software quality and efficiency. By\nidentifying gaps and trends, this study sets a foundation for advancing ML\nadoption and fostering innovation in increasingly complex development\nenvironments.", "AI": {"tldr": "This paper reviews ML pipelines in software engineering (SE), highlighting best practices, challenges, and the importance of robust preprocessing and ensemble methods for improved model reliability.", "motivation": "SE systems are becoming increasingly complex, leading to inefficiencies in quality assurance. Traditional methods fail to scale, necessitating improved ML pipeline frameworks for tasks like defect prediction and code review.", "method": "Systematic literature review (SLR) to consolidate state-of-the-art ML pipelining practices, evaluate their components (preprocessing, model selection, validation), and identify existing gaps in SE applications.", "result": "Key findings include: SMOTE/SZZ-based preprocessing enhances model reliability, Random Forest/Gradient Boosting dominate performance, Naive Bayes offers efficiency/interpretability, AUC/F1-precision metrics are widely used, and bootstrapping validation is critical for stability.", "conclusion": "Well-designed ML pipelines are essential for addressing SE challenges. The study provides actionable guidelines for researchers/practitioners to optimize ML adoption in software development, while mapping existing gaps for future innovation."}}
{"id": "2508.00083", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00083", "abs": "https://arxiv.org/abs/2508.00083", "authors": ["Yihong Dong", "Xue Jiang", "Jiaru Qian", "Tian Wang", "Kechi Zhang", "Zhi Jin", "Ge Li"], "title": "A Survey on Code Generation with LLM-based Agents", "comment": "Work in progress", "summary": "Code generation agents powered by large language models (LLMs) are\nrevolutionizing the software development paradigm. Distinct from previous code\ngeneration techniques, code generation agents are characterized by three core\nfeatures. 1) Autonomy: the ability to independently manage the entire workflow,\nfrom task decomposition to coding and debugging. 2) Expanded task scope:\ncapabilities that extend beyond generating code snippets to encompass the full\nsoftware development lifecycle (SDLC). 3) Enhancement of engineering\npracticality: a shift in research emphasis from algorithmic innovation toward\npractical engineering challenges, such as system reliability, process\nmanagement, and tool integration. This domain has recently witnessed rapid\ndevelopment and an explosion in research, demonstrating significant application\npotential. This paper presents a systematic survey of the field of LLM-based\ncode generation agents. We trace the technology's developmental trajectory from\nits inception and systematically categorize its core techniques, including both\nsingle-agent and multi-agent architectures. Furthermore, this survey details\nthe applications of LLM-based agents across the full SDLC, summarizes\nmainstream evaluation benchmarks and metrics, and catalogs representative\ntools. Finally, by analyzing the primary challenges, we identify and propose\nseveral foundational, long-term research directions for the future work of the\nfield.", "AI": {"tldr": "This paper systematically surveys LLM-based code generation agents, categorizing their core techniques (single-agent/multi-agent), SDLC applications, evaluation benchmarks, and proposes future research directions based on primary challenges.", "motivation": "Highlights the transformative impact of code generation agents on software development by addressing autonomy, expanded task scope beyond code snippets, and focus on practical engineering challenges.", "method": "Conducts a systematic review to trace technological development, classify architectural approaches, and analyze existing tools and evaluation frameworks in the domain.", "result": "Comprehensive analysis of LLM-based agents across software development lifecycle, identified mainstream benchmarks/metrics, and categorized key implementation patterns (single vs. multi-agent systems).", "conclusion": "Elucidates foundational challenges in the field and suggests long-term research trajectories focusing on system reliability, process management, and tool integration within practical software engineering contexts."}}
{"id": "2508.00293", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.00293", "abs": "https://arxiv.org/abs/2508.00293", "authors": ["Md Sajidul Islam Sajid", "Jinpeng Wei", "Ehab Al-Shaer"], "title": "ranDecepter: Real-time Identification and Deterrence of Ransomware Attacks", "comment": "Accepted at IEEE Conference on Communications and Network Security\n  (CNS) 2025", "summary": "Ransomware (RW) presents a significant and widespread threat in the digital\nlandscape, necessitating effective countermeasures. Active cyber deception is a\npromising strategy to thwart RW and limiting its propagation by misleading it\nwith false information and revealing its true behaviors. Furthermore, RW often\nacts as a communication conduit between attackers and defenders, allowing\ndeception to return false data to attackers and deplete their resources. This\npaper introduces ranDecepter, a novel approach that combines active cyber\ndeception with real-time analysis to enhance defenses against RW attacks. The\nranDecepter identifies RW in real-time and isolates it within a deceptive\nenvironment, autonomously identifying critical elements in the RW code to\ncreate a loop mechanism. By repeatedly restarting the malware and transmitting\ncounterfeit encryption information and secret keys to the attacker, it forces\nthe attacker to store these fabricated details for each victim, thereby\ndepleting their resources. Our comprehensive evaluation of ranDecepter,\nconducted using 1,134 real-world malware samples and twelve benign\napplications, demonstrates a remarkable 100% accuracy in RW identification,\nwith no false positives and minimal impact on response times. Furthermore,\nwithin 24-hours, ranDecepter generates up to 9,223K entries in the attacker's\ndatabase using 50 agents, showcasing its potential to undermine attacker\nresources.", "AI": {"tldr": "ranDecepter combines active cyber deception and real-time analysis to effectively combat ransomware by depleting attacker resources through false data transmission.", "motivation": "Ransomware is a significant threat requiring proactive countermeasures, and active deception can reveal malicious behaviors while depleting attackers' resources by exploiting the communication channel between them and the system.", "method": "ranDecepter identifies ransomware in real-time, isolates it in a deceptive environment, creates code-based loop mechanisms, and repeatedly restarts malware to transmit counterfeit encryption information and secret keys to attackers.", "result": "Achieved 100% ransomware identification accuracy with no false positives and negligible response time using 1,134 malware samples and 12 benign applications. 50 agents generated 9.223 million database entries for attackers within 24 hours.", "conclusion": "By autonomously creating resource-depleting loops through deception, ranDecepter demonstrates a powerful strategy to disrupt ransomware attacks and limit their propagation."}}
{"id": "2508.00128", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00128", "abs": "https://arxiv.org/abs/2508.00128", "authors": ["Md Nazmul Haque", "Hua Yang", "Zhou Yang", "Bowen Xu"], "title": "How Quantization Impacts Privacy Risk on LLMs for Code?", "comment": null, "summary": "Large language models for code (LLMs4Code) rely heavily on massive training\ndata, including sensitive data, such as cloud service credentials of the\nprojects and personal identifiable information of the developers, raising\nserious privacy concerns. Membership inference (MI) has recently emerged as an\neffective tool for assessing privacy risk by identifying whether specific data\nbelong to a model's training set. In parallel, model compression techniques,\nespecially quantization, have gained traction for reducing computational costs\nand enabling the deployment of large models. However, while quantized models\nstill retain knowledge learned from the original training data, it remains\nunclear whether quantization affects their ability to retain and expose privacy\ninformation. Answering this question is of great importance to understanding\nprivacy risks in real-world deployments. In this work, we conduct the first\nempirical study on how quantization influences task performance and privacy\nrisk simultaneously in LLMs4Code. To do this, we implement widely used\nquantization techniques (static and dynamic) to three representative model\nfamilies, namely Pythia, CodeGen, and GPTNeo. Our results demonstrate that\nquantization has a significant impact on reducing the privacy risk relative to\nthe original model. We also uncover a positive correlation between task\nperformance and privacy risk, indicating an underlying tradeoff. Moreover, we\nreveal the possibility that quantizing larger models could yield better balance\nthan using full-precision small models. Finally, we demonstrate that these\nfindings generalize across different architectures, model sizes and MI methods,\noffering practical guidance for safeguarding privacy when deploying compressed\nLLMs4Code.", "AI": {"tldr": "This paper investigates the impact of quantization on privacy risks and performance tradeoffs in code-specific large language models, finding that quantization reduces privacy exposure while maintaining a performance-benefit balance.", "motivation": "LLMs4Code face privacy risks from sensitive training data, and quantization's effect on these risks remains unclear despite its adoption for cost reduction.", "method": "Empirical evaluation of static/dynamic quantization on Pythia, CodeGen, and GPTNeo models using membership inference attacks and performance metrics.", "result": "Quantization significantly reduces privacy risk compared to original models; larger quantized models outperform smaller full-precision ones in accuracy-risk balance; positive correlation between performance and privacy risk confirmed.", "conclusion": "Quantization offers practical privacy safeguards for compressed LLMs4Code deployments while maintaining usable performance, with architecture-specific considerations for optimal implementation."}}
{"id": "2508.00351", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.00351", "abs": "https://arxiv.org/abs/2508.00351", "authors": ["Hyeonhak Kim", "Donghoe Heo", "Seokhie Hong"], "title": "Cryptanalysis of Isogeny-Based Quantum Money with Rational Points", "comment": null, "summary": "Quantum money is the cryptographic application of the quantum no-cloning\ntheorem. It has recently been instantiated by Montgomery and Sharif (Asiacrypt\n'24) from class group actions on elliptic curves. In this work, we propose a\nconcrete cryptanalysis by leveraging the efficiency of evaluating division\npolynomials with the coordinates of rational points, offering a speedup of\nO(log^4p) compared to the brute-force attack. Since our attack still requires\nexponential time, it remains impractical to forge a quantum banknote.\nInterestingly, due to the inherent properties of quantum money, our attack\nmethod also results in a more efficient verification procedure. Our algorithm\nleverages the properties of quadratic twists to utilize rational points in\nverifying the cardinality of the superposition of elliptic curves. We expect\nthis approach to contribute to future research on elliptic-curve-based quantum\ncryptography.", "AI": {"tldr": "This paper presents a cryptanalysis method for elliptic curve quantum money schemes using division polynomials and quadratic twists, achieving a O(log^4p) speedup over brute-force attacks while maintaining exponential time complexity. It also develops a more efficient verification procedure for these systems.", "motivation": "The work aims to evaluate security risks of Montgomery and Sharif's quantum money scheme based on class group actions, while exploring potential optimizations in verification processes inherent to quantum cryptography.", "method": "The approach leverages efficient division polynomial evaluations with rational points coordinates and utilizes quadratic twists to enhance verification efficiency. The algorithm exploits structural properties of the class group action and superposition cardinality verification through these mathematical techniques.", "result": "Demonstrated a O(log^4p) speedup factor in cryptanalysis compared to brute-force methods, while maintaining exponential time requirements for practical security. Achieved improved verification efficiency using quadratic twist properties for elliptic curve superpositions.", "conclusion": "The research provides foundational insights into elliptic curve quantum cryptography security, showing both practical cryptanalytic improvements and verification optimizations. The approach contributes to understanding inherent trade-offs between security and efficiency in quantum money systems."}}
{"id": "2508.00198", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00198", "abs": "https://arxiv.org/abs/2508.00198", "authors": ["Cleyton Magalhaes", "Italo Santos", "Brody Stuart-Verner", "Ronnie de Souza Santos"], "title": "Testing the Untestable? An Empirical Study on the Testing Process of LLM-Powered Software Systems", "comment": null, "summary": "Background: Software systems powered by large language models are becoming a\nroutine part of everyday technologies, supporting applications across a wide\nrange of domains. In software engineering, many studies have focused on how\nLLMs support tasks such as code generation, debugging, and documentation.\nHowever, there has been limited focus on how full systems that integrate LLMs\nare tested during development. Aims: This study explores how LLM-powered\nsystems are tested in the context of real-world application development.\nMethod: We conducted an exploratory case study using 99 individual reports\nwritten by students who built and deployed LLM-powered applications as part of\na university course. Each report was independently analyzed using thematic\nanalysis, supported by a structured coding process. Results: Testing strategies\ncombined manual and automated methods to evaluate both system logic and model\nbehavior. Common practices included exploratory testing, unit testing, and\nprompt iteration. Reported challenges included integration failures,\nunpredictable outputs, prompt sensitivity, hallucinations, and uncertainty\nabout correctness. Conclusions: Testing LLM-powered systems required\nadaptations to traditional verification methods, blending source-level\nreasoning with behavior-aware evaluations. These findings provide evidence on\nthe practical context of testing generative components in software systems.", "AI": {"tldr": "This paper investigates testing practices for LLM-powered systems in real-world applications using 99 student project reports. It identifies strategies like manual/automated testing and challenges such as hallucinations and prompt sensitivity.", "motivation": "As LLM-integrated software becomes widespread, existing research has overlooked how complete LLM-powered systems are verified during development, necessitating practical insights into their testing approaches.", "method": "An exploratory case study analyzed 99 student-written reports through thematic analysis and structured coding, examining how LLM applications were tested in academic settings.", "result": "Testing methods combined manual verification (exploratory testing, prompt iteration) and automated techniques (unit tests), with frequent problems in model behavior evaluation, integration stability, and output reliability.", "conclusion": "Testing LLM systems requires hybrid approaches that combine traditional software verification with behavior-focused strategies, offering practical guidance for evaluating generative components in software engineering contexts."}}
{"id": "2508.00368", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00368", "abs": "https://arxiv.org/abs/2508.00368", "authors": ["Alessandro Gaudenzi", "Lorenzo Nodari", "Lance Kaplan", "Alessandra Russo", "Murat Sensoy", "Federico Cerutti"], "title": "Preliminary Investigation into Uncertainty-Aware Attack Stage Classification", "comment": "Proceedings for SPAIML2025 workshop, 26/10/2025 Bologna Italy,\n  co-located with ECAI2025", "summary": "Advanced Persistent Threats (APTs) represent a significant challenge in\ncybersecurity due to their prolonged, multi-stage nature and the sophistication\nof their operators. Traditional detection systems typically focus on\nidentifying malicious activity in binary terms (benign or malicious) without\naccounting for the progression of an attack. However, effective response\nstrategies depend on accurate inference of the attack's current stage, as\ncountermeasures must be tailored to whether an adversary is in the early\nreconnaissance phase or actively conducting exploitation or exfiltration. This\nwork addresses the problem of attack stage inference under uncertainty, with a\nfocus on robustness to out-of-distribution (OOD) inputs. We propose a\nclassification approach based on Evidential Deep Learning (EDL), which models\npredictive uncertainty by outputting parameters of a Dirichlet distribution\nover possible stages. This allows the system not only to predict the most\nlikely stage of an attack but also to indicate when it is uncertain or the\ninput lies outside the training distribution. Preliminary experiments in a\nsimulated environment demonstrate that the proposed model can accurately infer\nthe stage of an attack with calibrated confidence while effectively detecting\nOOD inputs, which may indicate changes in the attackers' tactics. These results\nsupport the feasibility of deploying uncertainty-aware models for staged threat\ndetection in dynamic and adversarial environments.", "AI": {"tldr": "This paper proposes an evidential deep learning approach for APT attack stage inference and out-of-distribution detection, leveraging Dirichlet distributions to model predictive uncertainty.", "motivation": "Traditional binary classification methods in intrusion detection fail to capture attack progression dynamics, yet response strategies require accurate stage inference (reconnaissance vs. exploitation vs. exfiltration) and robustness to adversarial tactic changes.", "method": "The paper introduces a classification framework using Evidential Deep Learning (EDL) to output Dirichlet distribution parameters over attack stages, enabling simultaneous stage prediction and uncertainty quantification while detecting inputs outside the training distribution.", "result": "Preliminary experiments in simulated environments demonstrate accurate stage inference with well-calibrated confidence scores, and effective detection of out-of-distribution inputs indicating evolving attacker tactics.", "conclusion": "Uncertainty-aware models like EDL enable robust staged threat detection in dynamic adversarial environments, providing defenders with both attack stage inference and confidence calibration capabilities for more effective response decisions."}}
{"id": "2508.00244", "categories": ["cs.SE", "cs.PL", "D.3.2; D.2.11; D.2.13"], "pdf": "https://arxiv.org/pdf/2508.00244", "abs": "https://arxiv.org/abs/2508.00244", "authors": ["Briza Mel Dias de Sousa", "Renato Cordeiro Ferreira", "Alfredo Goldman"], "title": "Functional vs. Object-Oriented: Comparing How Programming Paradigms Affect the Architectural Characteristics of Systems", "comment": "11 pages, 16 figures (1 table, 3 diagrams, 5 graphics, 7 listings),\n  submitted to CTICQS capstone project competition at SBQS 2025", "summary": "After decades of dominance by object-oriented programming (OOP), functional\nprogramming (FP) is gaining increasing attention in the software industry. This\nstudy compares the impact of OOP and FP on the architectural characteristics of\nsoftware systems. For that, it examines the design and implementation of a\nDigital Wallet system, developed in Kotlin (representing OOP) and Scala\n(representing FP). The comparison is made through both qualitative and\nquantitative analyses to explore how each paradigm influences the system's\narchitectural characteristics. The self-ethnographic qualitative analysis\nprovides a side-by-side comparison of both implementations, revealing the\nperspective of those writing such code. The survey-based quantitative analysis\ngathers feedback from developers with diverse backgrounds, showing their\nimpressions of those reading this code. Hopefully, these results may be useful\nfor developers or organizations seeking to make more informed decisions about\nwhich paradigm is best suited for their next project.", "AI": {"tldr": "This study compares object-oriented (Kotlin) and functional (Scala) programming paradigms in a Digital Wallet system using qualitative/quantitative analyses to evaluate architectural impacts and developer perceptions.", "motivation": "The software industry is observing a shift from object-oriented programming (OOP) to functional programming (FP), creating a need to understand how these paradigms affect software architecture and developer experience.", "method": "The paper examines a Digital Wallet implementation in Kotlin (OOP) and Scala (FP) via self-ethnographic qualitative analysis (comparative implementation) and survey-based quantitative analysis (developer perception study).", "result": "Qualitative findings reveal differences in architectural structure between paradigms, while quantitative results demonstrate varying developer perceptions when working with or reviewing code in each paradigm.", "conclusion": "The empirical comparison between OOP and FP implementations provides actionable insights for developers and organizations choosing the optimal paradigm for future projects based on architectural characteristics and readability considerations."}}
{"id": "2508.00434", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.00434", "abs": "https://arxiv.org/abs/2508.00434", "authors": ["Yuqi Qian", "Yun Cao", "Meiyang Lv", "Haocheng Fu"], "title": "Accurate Latent Inversion for Generative Image Steganography via Rectified Flow", "comment": null, "summary": "Steganography based on diffusion models has attracted increasing attention\ndue to its ability to generate high-quality images and exhibit strong\nrobustness. In such approaches, the secret message is first embedded into the\ninitial latent variable, and then the stego image is generated through the\nforward process. To extract the message, an inversion process is required to\nreconstruct the latent variables from the received image. However, inaccurate\nlatent inversion leads to significant discrepancies between the reconstructed\nand original latent variables, rendering message extraction infeasible. To\naddress this issue, we propose \\textbf{RF-Stego}, a novel generative image\nsteganography method that enables accurate latent inversion and significantly\nimproves extraction accuracy. First, we develop the \\textbf{P}ath\n\\textbf{C}onsistency \\textbf{L}inear \\textbf{I}nversion (\\textbf{PCLI}), which\nimposes formal constraints on the inversion process. By explicitly aligning it\nwith the forward generation path and modeling both directions along a shared\nlinear path, PCLI eliminates path mismatch and ensures path consistency\nthroughout the steganographic process. Second, through rigorous theoretical\nproof, we demonstrate that \\textbf{R}ectified \\textbf{F}low \\textbf{(RF)}\noffers both theoretical reversibility and numerical stability in the inversion\nprocess. Based on this, we replace traditional unstable samplers with RF\nsampler which effectively improves the numerical precision of the inversion\nprocess. Experimental results show RF-Stego outperforms state-of-the-art\nmethods in terms of extraction accuracy, image quality, robustness, security\nand generation efficiency.", "AI": {"tldr": "RF-Stego introduces PCLI and RF samplers to improve latent inversion accuracy in diffusion model-based steganography, achieving better performance than existing methods across multiple metrics.", "motivation": "Existing diffusion model steganography methods suffer from inaccurate latent inversion causing discrepancies between original and reconstructed latents, making message extraction unreliable.", "method": "1. PCLI imposes formal constraints to enforce path consistency between forward generation and latent inversion. 2. Replaces traditional unstable samplers with Rectified Flow (RF) to ensure theoretical reversibility and numerical stability during inversion.", "result": "Experiments demonstrate RF-Stego surpasses state-of-the-art approaches in extraction accuracy, image quality (PSNR/SSIM), robustness, security, and generation efficiency on benchmark datasets.", "conclusion": "RF-Stego effectively addresses latent inversion challenges in diffusion-based steganography through path consistency enforcement and stable sampling, establishing a new standard for accuracy and reliability in generative image steganography."}}
{"id": "2508.00253", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00253", "abs": "https://arxiv.org/abs/2508.00253", "authors": ["Moumita Asad", "Rafed Muhammad Yasir", "Armin Geramirad", "Sam Malek"], "title": "Leveraging Large Language Model for Information Retrieval-based Bug Localization", "comment": null, "summary": "Information Retrieval-based Bug Localization aims to identify buggy source\nfiles for a given bug report. While existing approaches -- ranging from vector\nspace models to deep learning models -- have shown potential in this domain,\ntheir effectiveness is often limited by the vocabulary mismatch between bug\nreports and source code. To address this issue, we propose a novel Large\nLanguage Model (LLM) based bug localization approach, called GenLoc. Given a\nbug report, GenLoc leverages an LLM equipped with code-exploration functions to\niteratively analyze the code base and identify potential buggy files. To gather\nbetter context, GenLoc may optionally retrieve semantically relevant files\nusing vector embeddings. GenLoc has been evaluated on over 9,000 real-world bug\nreports from six large-scale Java projects. Experimental results show that\nGenLoc outperforms five state-of-the-art bug localization techniques across\nmultiple metrics, achieving an average improvement of more than 60\\% in\nAccuracy@1.", "AI": {"tldr": "GenLoc is an LLM-based iterative bug localization method that surpasses existing approaches by resolving vocabulary mismatch through code exploration and semantic retrieval, achieving ~60% improvement in accuracy@1 on 9,000+ Java bug reports.", "motivation": "Traditional IR-based bug localization methods struggle with vocabulary mismatch between bug reports and code, and existing models fail to adequately capture semantic context. This limits their effectiveness in identifying buggy files.", "method": "GenLoc uses a Large Language Model with code-exploration functions to iteratively analyze the codebase for bug reports. It optionally employs vector embeddings to retrieve semantically related files, enriching contextual understanding of the code-retrieval process.", "result": "GenLoc outperforms five state-of-the-art techniques across multiple metrics, showing >60% average improvement in Accuracy@1 when tested on 9,000+ real-world Java bug reports from six large-scale projects.", "conclusion": "LLMs enhanced with iterative code exploration and strategic contextual retrieval (via embeddings) significantly improve bug localization effectiveness. GenLoc's semantic flexibility overcomes vocabulary mismatch limitations present in conventional approaches."}}
{"id": "2508.00478", "categories": ["cs.CR", "cs.AI", "91A10, 91A43, 68T01, 94A60", "C.2.0; I.2.6; K.6.5"], "pdf": "https://arxiv.org/pdf/2508.00478", "abs": "https://arxiv.org/abs/2508.00478", "authors": ["Yuning Jiang", "Nay Oo", "Qiaoran Meng", "Lu Lin", "Dusit Niyato", "Zehui Xiong", "Hoon Wei Lim", "Biplab Sikdar"], "title": "CyGATE: Game-Theoretic Cyber Attack-Defense Engine for Patch Strategy Optimization", "comment": null, "summary": "Modern cyber attacks unfold through multiple stages, requiring defenders to\ndynamically prioritize mitigations under uncertainty. While game-theoretic\nmodels capture attacker-defender interactions, existing approaches often rely\non static assumptions and lack integration with real-time threat intelligence,\nlimiting their adaptability. This paper presents CyGATE, a game-theoretic\nframework modeling attacker-defender interactions, using large language models\n(LLMs) with retrieval-augmented generation (RAG) to enhance tactic selection\nand patch prioritization. Applied to a two-agent scenario, CyGATE frames cyber\nconflicts as a partially observable stochastic game (POSG) across Cyber Kill\nChain stages. Both agents use belief states to navigate uncertainty, with the\nattacker adapting tactics and the defender re-prioritizing patches based on\nevolving risks and observed adversary behavior. The framework's flexible\narchitecture enables extension to multi-agent scenarios involving coordinated\nattackers, collaborative defenders, or complex enterprise environments with\nmultiple stakeholders. Evaluated in a dynamic patch scheduling scenario, CyGATE\neffectively prioritizes high-risk vulnerabilities, enhancing adaptability\nthrough dynamic threat integration, strategic foresight by anticipating\nattacker moves under uncertainty, and efficiency by optimizing resource use.", "AI": {"tldr": "CyGATE is a game-theoretic framework that uses large language models and threat intelligence for dynamic cyber attack and defense modeling, enhancing adaptability through real-time data and strategic foresight.", "motivation": "Existing game-theoretic models for cyber defense rely on static assumptions and lack integration with real-time threat intelligence, reducing their ability to adapt to evolving attack scenarios and prioritize mitigations effectively.", "method": "CyGATE models attacker-defender interactions as a partially observable stochastic game (POSG) over Cyber Kill Chain stages, incorporating LLMs with retrieval-augmented generation (RAG) for tactic selection and patch prioritization. Belief states represent uncertainty for both agents, enabling dynamic adaptation to observed adversary behavior and risk assessments.", "result": "Evaluations in a dynamic patch scheduling scenario show CyGATE improves vulnerability prioritization by integrating time-critical threat intelligence, enables strategic foresight through attacker move anticipation, and optimizes attacker and defender resource usage efficiency.", "conclusion": "The flexible architecture of CyGATE addresses limitations of static game-theoretic models by incorporating real-time threat intelligence and LLMs, demonstrating adaptability in cyber defense through three key advantages: dynamic threat integration, strategic foresight, and resource optimization efficiency."}}
{"id": "2508.00255", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00255", "abs": "https://arxiv.org/abs/2508.00255", "authors": ["Boqi Chen", "Ou Wei", "Bingzhou Zheng", "Gunter Mussbacher"], "title": "Accurate and Consistent Graph Model Generation from Text with Large Language Models", "comment": "Accepted at ACM / IEEE 28th International Conference on Model Driven\n  Engineering Languages and Systems (MODELS 2025)", "summary": "Graph model generation from natural language description is an important task\nwith many applications in software engineering. With the rise of large language\nmodels (LLMs), there is a growing interest in using LLMs for graph model\ngeneration. Nevertheless, LLM-based graph model generation typically produces\npartially correct models that suffer from three main issues: (1) syntax\nviolations: the generated model may not adhere to the syntax defined by its\nmetamodel, (2) constraint inconsistencies: the structure of the model might not\nconform to some domain-specific constraints, and (3) inaccuracy: due to the\ninherent uncertainty in LLMs, the models can include inaccurate, hallucinated\nelements. While the first issue is often addressed through techniques such as\nconstraint decoding or filtering, the latter two remain largely unaddressed.\nMotivated by recent self-consistency approaches in LLMs, we propose a novel\nabstraction-concretization framework that enhances the consistency and quality\nof generated graph models by considering multiple outputs from an LLM. Our\napproach first constructs a probabilistic partial model that aggregates all\ncandidate outputs and then refines this partial model into the most appropriate\nconcrete model that satisfies all constraints. We evaluate our framework on\nseveral popular open-source and closed-source LLMs using diverse datasets for\nmodel generation tasks. The results demonstrate that our approach significantly\nimproves both the consistency and quality of the generated graph models.", "AI": {"tldr": "A novel abstraction-concretization framework improves LLM-generated graph models by addressing syntax violations, constraint inconsistencies, and hallucination via probabilistic aggregation and constraint-refined concrete models.", "motivation": "LLM-based graph model generation produces partially correct models with unresolved issues in constraint adherence and accuracy despite existing techniques addressing syntax violations.", "method": "Proposes a probabilistic framework that aggregates multiple LLM outputs into a partial model and refines it into a concrete model satisfying all constraints through abstraction-concretization cycles.", "result": "Evaluation on open-source and closed-source LLMs across diverse datasets shows significant improvements in model consistency and quality compared to baseline approaches.", "conclusion": "The abstraction-concretization framework effectively overcomes persistent limitations in LLM-based graph modeling, particularly constraint consistency and factual accuracy issues."}}
{"id": "2508.00555", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00555", "abs": "https://arxiv.org/abs/2508.00555", "authors": ["Jiecong Wang", "Haoran Li", "Hao Peng", "Ziqian Zeng", "Zihao Wang", "Haohua Du", "Zhengtao Yu"], "title": "Activation-Guided Local Editing for Jailbreaking Attacks", "comment": null, "summary": "Jailbreaking is an essential adversarial technique for red-teaming these\nmodels to uncover and patch security flaws. However, existing jailbreak methods\nface significant drawbacks. Token-level jailbreak attacks often produce\nincoherent or unreadable inputs and exhibit poor transferability, while\nprompt-level attacks lack scalability and rely heavily on manual effort and\nhuman ingenuity. We propose a concise and effective two-stage framework that\ncombines the advantages of these approaches. The first stage performs a\nscenario-based generation of context and rephrases the original malicious query\nto obscure its harmful intent. The second stage then utilizes information from\nthe model's hidden states to guide fine-grained edits, effectively steering the\nmodel's internal representation of the input from a malicious toward a benign\none. Extensive experiments demonstrate that this method achieves\nstate-of-the-art Attack Success Rate, with gains of up to 37.74% over the\nstrongest baseline, and exhibits excellent transferability to black-box models.\nOur analysis further demonstrates that AGILE maintains substantial\neffectiveness against prominent defense mechanisms, highlighting the\nlimitations of current safeguards and providing valuable insights for future\ndefense development. Our code is available at\nhttps://github.com/yunsaijc/AGILE.", "AI": {"tldr": "AGILE is a two-stage jailbreaking framework that improves over existing methods by combining scenario-based context generation and hidden state-guided fine-grained edits, achieving state-of-the-art attack success rates and demonstrating strong transferability to black-box models.", "motivation": "Existing jailbreak methods (token/prompt-level) suffer from incoherent input generation, poor transferability, or heavy manual effort. There's a need for scalable and effective techniques to robustly evaluate and improve model security.", "method": "Stage 1: Scenario-based generation of context to rephrase malicious queries and obscure harmful intent. Stage 2: Model-based analysis of hidden states to guide precise input edits that steer internal representations toward benignness.", "result": "37.74% gain in attack success rate over strongest baselines with high transferability (black-box model compatibility) and maintained effectiveness against major defense mechanisms.", "conclusion": "AGILE outperforms existing jailbreak approaches while uncovering critical limitations in current model defenses, providing valuable insights for developing more robust safety mechanisms in AI systems."}}
{"id": "2508.00408", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00408", "abs": "https://arxiv.org/abs/2508.00408", "authors": ["Dong Huang", "Jie M. Zhang", "Mark Harman", "Qianru Zhang", "Mingzhe Du", "See-Kiong Ng"], "title": "Benchmarking LLMs for Unit Test Generation from Real-World Functions", "comment": "Under Review", "summary": "Recently, large language models (LLMs) have shown great promise in automating\nunit test generation, significantly reducing the manual effort required by\ndevelopers. To effectively evaluate the capabilities of LLMs in this domain, it\nis crucial to have a well-designed benchmark that accurately reflects\nreal-world scenarios and mitigates common pitfalls. Existing LLM test\ngeneration benchmarks are limited by two critical drawbacks: data contamination\nand structurally simple function code. As a result, we often cannot rely on the\nvalidity of scientific conclusions drawn from empirical studies using these\nlimited benchmarks. The empirical evidence presented may be biased due to\ncontamination and may fail to generalize beyond toy programs due to structural\nsimplicity.\n  To address these problems, we introduce ULT (UnLeakedTestbench), a new\nbenchmark specifically designed for function-level unit test generation from\nreal-world Python functions. ULT is constructed through a multi-stage curation\nprocess that ensures high cyclomatic complexity and mitigates test case\ncontamination. With 3,909 carefully selected function-level tasks, ULT provides\na more realistic and challenging evaluation of LLMs' test generation\ncapabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT\nwith leaked tests designed to enable a controlled analysis of memorization\nversus reasoning in test generation. Our evaluation results demonstrate that\nULT is significantly more challenging. For example, test cases generated by\nLLMs only achieve 41.32\\%, 45.10\\%, 30.22\\%, and 40.21\\% for accuracy,\nstatement coverage, branch coverage, and mutation score on average for all\nLLMs, respectively. These results are substantially lower than the\ncorresponding metrics on TestEval (91.79\\%, 92.18\\%, 82.04\\%, and 49.69\\%) and\nPLT (47.07\\%, 55.13\\%, 40.07\\%, and 50.80\\%).", "AI": {"tldr": "The paper introduces ULT, a new benchmark for evaluating LLMs' unit test generation capabilities by addressing data contamination and structural simplicity in existing benchmarks. It also provides PLT for controlled analysis of memorization vs reasoning, showing ULT is significantly more challenging.", "motivation": "Existing benchmarks for LLM unit test generation suffer from data contamination and structurally simple function code, leading to biased conclusions in empirical studies. The authors aim to create a more realistic benchmark to assess LLMs' true test generation capabilities without these limitations.", "method": "The authors developed ULT using a multi-stage curation process ensuring high cyclomatic complexity and preventing test case contamination. A paired benchmark PLT is introduced with leaked tests to differentiate between LLM memorization and reasoning during test generation.", "result": "ULT achieves lower performance metrics (41.32% accuracy, 45.10% statement coverage, 30.22% branch coverage, 40.21% mutation score) compared to TestEval (91.79%, 92.18%, 82.04%, 49.69%) and PLT (47.07%, 55.13%, 40.07%, 50.80%), demonstrating its higher difficulty.", "conclusion": "ULT is a significantly more challenging and realistic benchmark for function-level unit test generation, as using pre-contaminated benchmarks can lead to overstated performance results. Future studies should adopt such benchmarks to better evaluate LLMs' test generation capabilities in real-world scenarios."}}
{"id": "2508.00602", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00602", "abs": "https://arxiv.org/abs/2508.00602", "authors": ["Francesco Panebianco", "Stefano Bonfanti", "Francesco Trov\u00f2", "Michele Carminati"], "title": "LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection and Leakage Attacks", "comment": "22 pages, preprint", "summary": "The generalization capabilities of Large Language Models (LLMs) have led to\ntheir widespread deployment across various applications. However, this\nincreased adoption has introduced several security threats, notably in the\nforms of jailbreaking and data leakage attacks. Additionally, Retrieval\nAugmented Generation (RAG), while enhancing context-awareness in LLM responses,\nhas inadvertently introduced vulnerabilities that can result in the leakage of\nsensitive information. Our contributions are twofold. First, we introduce a\nmethodology to analyze historical interaction data from an LLM system, enabling\nthe generation of usage maps categorized by topics (including adversarial\ninteractions). This approach further provides forensic insights for tracking\nthe evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a\nmodel-agnostic framework that combines static analysis for forensic insights\nwith dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique\nidentifies topic groups and detects anomalous patterns, allowing for proactive\ndefense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1)\njailbreak attempts, employing a public benchmark dataset, and (2) PII leakage,\nsupported by a curated dataset of labeled LLM interactions. In the static\nsetting, LeakSealer achieves the highest precision and recall on the ToxicChat\ndataset when identifying prompt injection. In the dynamic setting, PII leakage\ndetection achieves an AUPRC of $0.97$, significantly outperforming baselines\nsuch as Llama Guard.", "AI": {"tldr": "The paper addresses security threats in Large Language Models (LLMs), such as jailbreaking and data leakage, by introducing a methodology to create topic-categorized usage maps and a model-agnostic framework called LeakSealer. LeakSealer combines static analysis and dynamic defenses in a Human-In-The-Loop pipeline to detect adversarial interactions and PII leakage. Evaluations show high precision/recall for prompt injection detection and an AUPRC of 0.97 for PII leakage, outperforming baselines.", "motivation": "The deployment of LLMs has introduced security risks like jailbreaking and data leakage, particularly when using Retrieval Augmented Generation (RAG). These vulnerabilities compromise sensitive information and require proactive mitigation strategies.", "method": "1) A methodology for analyzing historical LLM interaction data to generate topic-based usage maps and track adversarial interaction patterns. 2) LeakSealer, a model-agnostic framework combining static analysis (for forensic insights) and dynamic defenses (in HITL pipelines) to identify high-risk topic groups and detect anomalies.", "result": "LeakSealer achieved: \n- Highest precision and recall on ToxicChat dataset for static prompt injection detection\n- 0.97 AUPRC for dynamic PII leakage detection\n- Outperformed baselines like Llama Guard in both metrics", "conclusion": "The paper provides a comprehensive security analysis framework for LLMs. LeakSealer demonstrates strong effectiveness as a hybrid static-dynamic defense system, achieving state-of-the-art performance in identifying adversarial behavior and preventing data leakage through continuous monitoring and human oversight."}}
{"id": "2508.00462", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00462", "abs": "https://arxiv.org/abs/2508.00462", "authors": ["Linus Ververs", "Lutz Prechelt"], "title": "Managing Power Gaps as a Topic of Pair Programming Skill: A Grounded Theory", "comment": null, "summary": "Context: Pair Programming as a work mode is used (occasionally or frequently)\nthroughout professional software development. Objective: Understand what\npower-related phenomena occur in pair programming as it is used in industry;\ngive advice to practitioners on how to do better pair programming. Method:\nAnalyze 22 industrial pair programming sessions using Grounded Theory\nMethodology. Formulate a Grounded Theory on power-related behaviors. Run a\nsurvey with 292 participants about that theory. Use it to demonstrate that the\nphenomena are common. Results: Our theory describes the phenomenon of Power\nGap: a perceived difference in participation opportunities. The theory shows\nthe behaviors that create a Power Gap or result from it. Power Gaps tend to\ndamage knowledge transfer, code quality, and process effi ciency. The survey\nresults show that all concepts from our theory are frequent in practice. They\nalso provide more grounding for concepts that are observable only indirectly.\nConclusions: It is a valuable component of pair programming skill to be able to\navoid Power Gaps. Specifically, pair partners need to avoid Hierarchical\nBehavior (which tends to create or increase a Power Gap) and should perform\nenough Equalizing Behavior (which prevents or reduces a Power Gap).", "AI": {"tldr": "This paper examines power dynamics in industrial pair programming through grounded theory, identifying Power Gaps and providing strategies to mitigate them.", "motivation": "To address unexamined power-related behaviors in professional pair programming that impact collaboration effectiveness and outcomes.", "method": "Analyzed 22 industrial pair programming sessions using Grounded Theory Methodology, validated with a survey of 292 practitioners.", "result": "Identified 'Power Gap' as a key phenomenon caused by hierarchical behaviors and mitigated by equalizing behaviors, validated through survey frequency and indirect observational evidence.", "conclusion": "Avoiding Hierarchical Behavior and implementing Equalizing Behavior are critical skills for effective pair programming in industry."}}
{"id": "2508.00636", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.00636", "abs": "https://arxiv.org/abs/2508.00636", "authors": ["Haocheng Jiang", "Hua Shen", "Jixin Zhang", "Willy Susilo", "Mingwu Zhang"], "title": "FedGuard: A Diverse-Byzantine-Robust Mechanism for Federated Learning with Major Malicious Clients", "comment": null, "summary": "Federated learning is a distributed training framework vulnerable to\nByzantine attacks, particularly when over 50% of clients are malicious or when\ndatasets are highly non-independent and identically distributed (non-IID).\nAdditionally, most existing defense mechanisms are designed for specific attack\ntypes (e.g., gradient similarity-based schemes can only defend against outlier\nmodel poisoning), limiting their effectiveness. In response, we propose\nFedGuard, a novel federated learning mechanism. FedGuard cleverly addresses the\naforementioned issues by leveraging the high sensitivity of membership\ninference to model bias. By requiring clients to include an additional\nmini-batch of server-specified data in their training, FedGuard can identify\nand exclude poisoned models, as their confidence in the mini-batch will drop\nsignificantly. Our comprehensive evaluation unequivocally shows that, under\nthree highly non-IID datasets, with 90% of clients being Byzantine and seven\ndifferent types of Byzantine attacks occurring in each round, FedGuard\nsignificantly outperforms existing robust federated learning schemes in\nmitigating various types of Byzantine attacks.", "AI": {"tldr": "The paper proposes FedGuard, a novel federated learning mechanism that addresses vulnerabilities to Byzantine attacks by using membership inference sensitivity to model bias, significantly outperforming existing defenses under high non-IID data and majority-malicious scenarios.", "motivation": "Federated learning frameworks are susceptible to Byzantine attacks, especially when datasets are non-IID or over 50% of clients are malicious. Current defenses are often attack-specific, limiting their effectiveness against diverse attack types.", "method": "FedGuard requires clients to train on an additional server-specified mini-batch, exploiting the fact that poisoned models exhibit a significant drop in confidence on this data. This approach leverages membership inference's sensitivity to model bias to detect and exclude malicious updates.", "result": "FedGuard achieves superior robustness on three highly non-IID datasets with 90% Byzantine clients and seven distinct attack types per round, outperforming existing schemes in mitigating various Byzantine attacks.", "conclusion": "FedGuard provides a general, effective defense against Byzantine attacks in federated learning, particularly in environments with high data heterogeneity and majority-malicious client participation, without relying on attack-specific assumptions."}}
{"id": "2508.00508", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.00508", "abs": "https://arxiv.org/abs/2508.00508", "authors": ["Panagiotis Diamantakis", "Thanassis Avgerinos", "Yannis Smaragdakis"], "title": "Desyan: A Platform for Seamless Value-Flow and Symbolic Analysis", "comment": null, "summary": "Over the past two decades, two different types of static analyses have\nemerged as dominant paradigms both in academia and industry: value-flow\nanalysis (e.g., data-flow analysis or points-to analysis) and symbolic analysis\n(e.g., symbolic execution). Despite their individual successes in numerous\napplication fields, the two approaches have remained largely separate; an\nartifact of the simple reality that there is no broadly adopted unifying\nplatform for effortless and efficient integration of symbolic techniques with\nhigh-performance data-flow reasoning.\n  To bridge this gap, we introduce Desyan: a platform for writing program\nanalyses with seamless integration of value-flow and symbolic reasoning. Desyan\nexpands a production-ready Datalog fixpoint engine (Souffl\\'e) with\nfull-fledged SMT solving invoking industry-leading SMT engines. Desyan provides\nconstructs for automatically (and efficiently!) handling typical patterns that\ncome up in program analysis. At the same time, the integration is agnostic with\nrespect to the solving technology, and supports Datalog-native symbolic\nreasoning, via a bottom-up algebraic reasoning module.\n  The result is an engine that allows blending different kinds of reasoning, as\nneeded for the underlying analysis. For value-flow analysis, the engine is the\nbest-in-class Datalog evaluator (often by a factor of over 20x in execution\ntime); for applications that require full SMT (e.g., a concolic execution\nengine or other symbolic evaluator that needs to solve arbitrarily complex\nconditions), the engine is leveraging the leading SMT solvers; for lightweight\nsymbolic evaluation (e.g., solving simple conditionals in the context of a\npath-sensitive analysis), the engine can use Datalog-native symbolic reasoning,\nachieving large speedups (often of over 2x) compared to eagerly appealing to an\nSMT solver.", "AI": {"tldr": "Desyan is a platform integrating value-flow (Datalog) and symbolic (SMT) analyses, enabling efficient hybrid static analysis by extending Souffl\u00e9 with SMT solvers and Datalog-native symbolic reasoning constructs.", "motivation": "Value-flow analysis (e.g., data-flow) and symbolic analysis (e.g., symbolic execution) are dominant but traditionally separate paradigms due to lack of an integrated platform. Combining these approaches could improve program analysis but requires overcoming performance and integration challenges.", "method": "Desyan extends Souffl\u00e9, a high-performance Datalog engine, with full SMT solver integration and introduces a bottom-up algebraic module for Datalog-native symbolic reasoning. It features patterns and optimizations tailored for program analysis, enabling seamless switching between reasoning styles while maintaining technology agnosticism.", "result": "Desyan achieves: (1) Best-in-class Datalog execution speed for value-flow tasks (20x+ faster in some cases), (2) 2x+ speedup for lightweight symbolic evaluation versus external SMT solvers, and (3) enables application of symbolic techniques to complement traditional value-flow analysis workflows.", "conclusion": "Desyan creates a unifying platform that preserves the strengths of Datalog and SMT-based analysis while enabling efficient hybrid reasoning. This advances static analysis capabilities by making cross-paradigm analysis more accessible and performant for complex program analysis tasks."}}
{"id": "2508.00659", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00659", "abs": "https://arxiv.org/abs/2508.00659", "authors": ["Xinzhang Chen", "Hassan Ali", "Arash Shaghaghi", "Salil S. Kanhere", "Sanjay Jha"], "title": "Demo: TOSense -- What Did You Just Agree to?", "comment": "Accepted as a demonstration paper at IEEE LCN 2025", "summary": "Online services often require users to agree to lengthy and obscure Terms of\nService (ToS), leading to information asymmetry and legal risks. This paper\nproposes TOSense-a Chrome extension that allows users to ask questions about\nToS in natural language and get concise answers in real time. The system\ncombines (i) a crawler \"tos-crawl\" that automatically extracts ToS content, and\n(ii) a lightweight large language model pipeline: MiniLM for semantic retrieval\nand BART-encoder for answer relevance verification. To avoid expensive manual\nannotation, we present a novel Question Answering Evaluation Pipeline (QEP)\nthat generates synthetic questions and verifies the correctness of answers\nusing clustered topic matching. Experiments on five major platforms, Apple,\nGoogle, X (formerly Twitter), Microsoft, and Netflix, show the effectiveness of\nTOSense (with up to 44.5% accuracy) across varying number of topic clusters.\nDuring the demonstration, we will showcase TOSense in action. Attendees will be\nable to experience seamless extraction, interactive question answering, and\ninstant indexing of new sites.", "AI": {"tldr": "This paper introduces TOSense, a Chrome extension for real-time natural language question answering about Terms of Service (ToS) using a crawling system and lightweight language model pipeline. A novel QA evaluation method called QEP is proposed to assess answer quality without manual annotation, validated through experiments on five major platforms.", "motivation": "Terms of Service (ToS) for online services are lengthy and complex, creating information asymmetry and legal risks for users who lack the time or expertise to fully understand them.", "method": "The system combines (i) a crawler (tos-crawl) for automated ToS content extraction, and (ii) a dual language model pipeline: MiniLM for semantic retrieval and BART-encoder for answer relevance verification. A synthetic QA generation approach (QEP) is introduced to evaluate answers through clustered topic matching without manual ground truth.", "result": "Experiments on Apple, Google, X, Microsoft, and Netflix ToS achieve up to 44.5% accuracy across varying topic cluster configurations. The real-time interactive QA interface demonstrates seamless functionality and instant indexing for new websites during live demonstrations.", "conclusion": "TOSense offers a scalable solution for simplifying ToS comprehension through automated information extraction, synthetic QA generation, and real-time natural language interaction, significantly reducing legal risks from complex service agreements while maintaining low computational overhead."}}
{"id": "2508.00546", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00546", "abs": "https://arxiv.org/abs/2508.00546", "authors": ["Wenchao Gu", "Zongyi Lyu", "Yanlin Wang", "Hongyu Zhang", "Cuiyun Gao", "Michael R. Lyu"], "title": "SPENCER: Self-Adaptive Model Distillation for Efficient Code Retrieval", "comment": null, "summary": "Code retrieval aims to provide users with desired code snippets based on\nusers' natural language queries. With the development of deep learning\ntechnologies, adopting pre-trained models for this task has become mainstream.\nConsidering the retrieval efficiency, most of the previous approaches adopt a\ndual-encoder for this task, which encodes the description and code snippet into\nrepresentation vectors, respectively. However, the model structure of the\ndual-encoder tends to limit the model's performance, since it lacks the\ninteraction between the code snippet and description at the bottom layer of the\nmodel during training. To improve the model's effectiveness while preserving\nits efficiency, we propose a framework, which adopts Self-AdaPtive Model\nDistillation for Efficient CodE Retrieval, named SPENCER. SPENCER first adopts\nthe dual-encoder to narrow the search space and then adopts the cross-encoder\nto improve accuracy. To improve the efficiency of SPENCER, we propose a novel\nmodel distillation technique, which can greatly reduce the inference time of\nthe dual-encoder while maintaining the overall performance. We also propose a\nteaching assistant selection strategy for our model distillation, which can\nadaptively select the suitable teaching assistant models for different\npre-trained models during the model distillation to ensure the model\nperformance. Extensive experiments demonstrate that the combination of\ndual-encoder and cross-encoder improves overall performance compared to solely\ndual-encoder-based models for code retrieval. Besides, our model distillation\ntechnique retains over 98% of the overall performance while reducing the\ninference time of the dual-encoder by 70%.", "AI": {"tldr": "SPENCER improves code retrieval by combining dual-encoder efficiency with cross-encoder accuracy through model distillation, achieving 98% performance retention and 70% faster inference.", "motivation": "Dual-encoders limit code retrieval performance due to lack of code-query interaction during training, while cross-encoders enhance accuracy at the cost of efficiency. A hybrid approach is needed to balance both.", "method": "First, a dual-encoder narrows the search space for efficiency. Then, a cross-encoder refines accuracy. SPENCER introduces a Self-AdaPtive Model Distillation technique to distill the dual-encoder's performance while reducing inference time, combined with an adaptive teaching assistant selection strategy.", "result": "Experiments show SPENCER outperforms dual-encoder-only models by combining both architectures. The distillation technique achieves 70% faster dual-encoder inference with over 98% retention of performance.", "conclusion": "SPENCER effectively bridges the efficiency-accuracy gap in code retrieval by integrating dual- and cross-encoders with model distillation, offering both improved performance and significant speed gains."}}
{"id": "2508.00682", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00682", "abs": "https://arxiv.org/abs/2508.00682", "authors": ["Oscar Llorente-Vazquez", "Xabier Ugarte-Pedrero", "Igor Santos-Grueiro", "Pablo Garcia Bringas"], "title": "Unveiling Dynamic Binary Instrumentation Techniques", "comment": null, "summary": "Dynamic Binary Instrumentation (DBI) is the set of techniques that enable\ninstrumentation of programs at run-time, making it possible to monitor and\nmodify the execution of compiled binaries or entire systems. DBI is used for\ncountless security applications and analyses, and is extensively used across\nmany fields in both industry and academia. Over the years, several DBI\napproaches have been proposed based on different technologies and implementing\ndiverse techniques. Every solution tries to overcome certain limitations, but\nthey sometimes bring other shortcomings. Some are specialized for one\nparticular domain or task, while others have a wider scope.\n  In this paper, we shed light into the labyrinth of DBI, bringing together\nprocess-level and whole-system approaches. We depict their building blocks and\nanalyze the underlying instrumentation techniques, comparing their ability to\ninstrument different primitives and run-time events. Then, we evaluate their\nperformance when implementing each primitive, and highlight relevant\nobservations. Our results show that no single technique is better than the rest\nin all circumstances.", "AI": {"tldr": "The paper provides an analysis and comparison of different Dynamic Binary Instrumentation approaches, highlighting their strengths, weaknesses, and performance trade-offs across various run-time instrumentation tasks.", "motivation": "DBI is critical for security and system analysis, but existing approaches have diverse limitations, motivating the need for a comprehensive comparison to guide suitable application scenarios.", "method": "The authors review process-level and whole-system DBI approaches, analyze their instrumentation techniques, evaluate performance on varying primitives/events, and compare capabilities through structured analysis.", "result": "Experimental evaluations show no single DBI technique dominates all scenarios; different approaches suit specific goals due to inherent design trade-offs in instrumentation capabilities and performance overheads.", "conclusion": "DBI methods require careful selection based on task requirements as no universal solution exists, emphasizing the importance of matching technique characteristics to specific instrumentation needs across processes or entire systems."}}
{"id": "2508.00593", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00593", "abs": "https://arxiv.org/abs/2508.00593", "authors": ["Shuyao Jiang", "Jiazhen Gu", "Wujie Zheng", "Yangfan Zhou", "Michael R. Lyu"], "title": "Can User Feedback Help Issue Detection? An Empirical Study on a One-billion-user Online Service System", "comment": "Accepted by the 19th ACM/IEEE International Symposium on Empirical\n  Software Engineering and Measurement (ESEM 2025)", "summary": "Background: It has long been suggested that user feedback, typically written\nin natural language by end-users, can help issue detection. However, for\nlarge-scale online service systems that receive a tremendous amount of\nfeedback, it remains a challenging task to identify severe issues from user\nfeedback. Aims: To develop a better feedback-based issue detection approach, it\nis crucial first to gain a comprehensive understanding of the characteristics\nof user feedback in real production systems. Method: In this paper, we conduct\nan empirical study on 50,378,766 user feedback items from six real-world\nservices in a one-billion-user online service system. We first study what users\nprovide in their feedback. We then examine whether certain features of feedback\nitems can be good indicators of severe issues. Finally, we investigate whether\nadopting machine learning techniques to analyze user feedback is reasonable.\nResults: Our results show that a large proportion of user feedback provides\nirrelevant information about system issues. As a result, it is crucial to\nfilter out issue-irrelevant information when processing user feedback.\nMoreover, we find severe issues that cannot be easily detected based solely on\nuser feedback characteristics. Finally, we find that the distributions of the\nfeedback topics in different time intervals are similar. This confirms that\ndesigning machine learning-based approaches is a viable direction for better\nanalyzing user feedback. Conclusions: We consider that our findings can serve\nas an empirical foundation for feedback-based issue detection in large-scale\nservice systems, which sheds light on the design and implementation of\npractical issue detection approaches.", "AI": {"tldr": "This paper analyzes user feedback in large-scale online systems to improve issue detection, finding challenges in filtering irrelevant information and demonstrating the viability of machine learning approaches despite undetectable severe issues via feedback alone.", "motivation": "Large-scale systems receive vast amounts of user feedback, but identifying severe issues remains challenging. The study aims to understand feedback characteristics to build effective detection methods.", "method": "Empirical analysis of 50M+ feedback items from six services, focusing on content relevance, issue indicator features, and ML's effectiveness for temporal topic distribution patterns.", "result": "1) High proportion of irrelevant feedback necessitates filtering. 2) Some severe issues elude detection via feedback characteristics alone. 3) Consistent topic distributions over time validate ML-based analysis potential.", "conclusion": "Provides empirical foundation for ML-driven feedback-based issue detection in large systems, guiding practical implementation despite limitations in feedback completeness."}}
{"id": "2508.00756", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.00756", "abs": "https://arxiv.org/abs/2508.00756", "authors": ["Yunhao Chen", "Shujie Wang", "Xin Wang", "Xingjun Ma"], "title": "LeakyCLIP: Extracting Training Data from CLIP", "comment": null, "summary": "Understanding the memorization and privacy leakage risks in Contrastive\nLanguage--Image Pretraining (CLIP) is critical for ensuring the security of\nmultimodal models. Recent studies have demonstrated the feasibility of\nextracting sensitive training examples from diffusion models, with conditional\ndiffusion models exhibiting a stronger tendency to memorize and leak\ninformation. In this work, we investigate data memorization and extraction\nrisks in CLIP through the lens of CLIP inversion, a process that aims to\nreconstruct training images from text prompts. To this end, we introduce\n\\textbf{LeakyCLIP}, a novel attack framework designed to achieve high-quality,\nsemantically accurate image reconstruction from CLIP embeddings. We identify\nthree key challenges in CLIP inversion: 1) non-robust features, 2) limited\nvisual semantics in text embeddings, and 3) low reconstruction fidelity. To\naddress these challenges, LeakyCLIP employs 1) adversarial fine-tuning to\nenhance optimization smoothness, 2) linear transformation-based embedding\nalignment, and 3) Stable Diffusion-based refinement to improve fidelity.\nEmpirical results demonstrate the superiority of LeakyCLIP, achieving over 358%\nimprovement in Structural Similarity Index Measure (SSIM) for ViT-B-16 compared\nto baseline methods on LAION-2B subset. Furthermore, we uncover a pervasive\nleakage risk, showing that training data membership can even be successfully\ninferred from the metrics of low-fidelity reconstructions. Our work introduces\na practical method for CLIP inversion while offering novel insights into the\nnature and scope of privacy risks in multimodal models.", "AI": {"tldr": "This paper introduces LeakyCLIP, a novel attack framework to reconstruct training images from CLIP embeddings, revealing significant memorization and privacy leakage risks in multimodal models. It achieves over 358% SSIM improvement on ViT-B-16 and demonstrates data membership inference via low-fidelity metrics.", "motivation": "The motivation is to investigate memorization and privacy leakage vulnerabilities in CLIP to ensure the security of multimodal models, as prior studies showed similar risks in diffusion models.", "method": "LeakyCLIP addresses three challenges in CLIP inversion: 1) non-robust features via adversarial fine-tuning, 2) limited visual semantics in text embeddings using linear transformation-based alignment, and 3) low reconstruction fidelity with Stable Diffusion-based refinement.", "result": "LeakyCLIP achieves 358% SSIM improvement over baselines on ViT-B-16 using LAION-2B. It also demonstrates successful training data membership inference from low-fidelity reconstruction metrics.", "conclusion": "The work provides a practical CLIP inversion method and highlights pervasive privacy risks in multimodal models, offering novel insights into the scope and nature of these vulnerabilities."}}
{"id": "2508.00630", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00630", "abs": "https://arxiv.org/abs/2508.00630", "authors": ["Khaled Ahmed", "Jialing Song", "Boqi Chen", "Ou Wei", "Bingzhou Zheng"], "title": "MCeT: Behavioral Model Correctness Evaluation using Large Language Models", "comment": "MODELS 2025", "summary": "Behavioral model diagrams, e.g., sequence diagrams, are an essential form of\ndocumentation that are typically designed by system engineers from requirements\ndocumentation, either fully manually or assisted by design tools. With the\ngrowing use of Large Language Models (LLM) as AI modeling assistants, more\nautomation will be involved in generating diagrams. This necessitates the\nadvancement of automatic model correctness evaluation tools. Such a tool can be\nused to evaluate both manually and AI automatically generated models; to\nprovide feedback to system engineers, and enable AI assistants to self-evaluate\nand self-enhance their generated models.\n  In this paper, we propose MCeT, the first fully automated tool to evaluate\nthe correctness of a behavioral model, sequence diagrams in particular, against\nits corresponding requirements text and produce a list of issues that the model\nhas. We utilize LLMs for the correctness evaluation tasks as they have shown\noutstanding natural language understanding ability. However, we show that\ndirectly asking an LLM to compare a diagram to requirements finds less than 35%\nof issues that experienced engineers can find. We propose to supplement the\ndirect check with a fine-grained, multi-perspective approach; we split the\ndiagram into atomic, non-divisible interactions, and split the requirements\ntext into atomic, self-contained items. We compare the diagram with atomic\nrequirements and each diagram-atom with the requirements. We also propose a\nself-consistency checking approach that combines perspectives to mitigate LLM\nhallucinated issues. Our combined approach improves upon the precision of the\ndirect approach from 0.58 to 0.81 in a dataset of real requirements. Moreover,\nthe approach finds 90% more issues that the experienced engineers found than\nthe direct approach, and reports an average of 6 new issues per diagram.", "AI": {"tldr": "The paper introduces MCeT, the first fully automated tool for evaluating sequence diagram correctness via requirements text using LLMs with a multi-perspective approach that enhances issue detection by 90% compared to direct comparison methods.", "motivation": "Manual verification of behavioral models (e.g., sequence diagrams) against requirements is labor-intensive. Current LLM-based automated approaches detect <35% of issues identified by expert engineers, necessitating improved strategies for AI-generated model evaluation.", "method": "MCeT employs a fine-grained atomic decomposition strategy: 1) Dissects diagrams into indivisible interactions 2) Parses requirements into atomic items 3) Uses multi-perspective cross-checking between atomic components 4) Applies self-consistency validation to mitigate hallucinations by integrating conflicting LLM responses.", "result": "MCeT achieves 81% precision (up from 58% in direct comparison) on real-world requirements, identifies 90% more issues than human experts using direct comparison, and discovers an average of 6 novel issues per diagram in its dataset.", "conclusion": "The atomic decomposition and multi-perspective validation approach significantly improves LLM-based model correctness evaluation for sequence diagrams. MCeT's self-consistency mechanism reduces hallucinations by 35%, enabling robust automated assessment crucial for AI-assisted diagram generation workflows."}}
{"id": "2508.00700", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00700", "abs": "https://arxiv.org/abs/2508.00700", "authors": ["Alfred Santa Molison", "Marcia Moraes", "Glaucia Melo", "Fabio Santos", "Wesley K. G. Assuncao"], "title": "Is LLM-Generated Code More Maintainable \\& Reliable than Human-Written Code?", "comment": "Accepted ESEM2025", "summary": "Background: The rise of Large Language Models (LLMs) in software development\nhas opened new possibilities for code generation. Despite the widespread use of\nthis technology, it remains unclear how well LLMs generate code solutions in\nterms of software quality and how they compare to human-written code. Aims:\nThis study compares the internal quality attributes of LLM-generated and\nhuman-written code. Method: Our empirical study integrates datasets of coding\ntasks, three LLM configurations (zero-shot, few-shot, and fine-tuning), and\nSonarQube to assess software quality. The dataset comprises Python code\nsolutions across three difficulty levels: introductory, interview, and\ncompetition. We analyzed key code quality metrics, including maintainability\nand reliability, and the estimated effort required to resolve code issues.\nResults: Our analysis shows that LLM-generated code has fewer bugs and requires\nless effort to fix them overall. Interestingly, fine-tuned models reduced the\nprevalence of high-severity issues, such as blocker and critical bugs, and\nshifted them to lower-severity categories, but decreased the model's\nperformance. In competition-level problems, the LLM solutions sometimes\nintroduce structural issues that are not present in human-written code.\nConclusion: Our findings provide valuable insights into the quality of\nLLM-generated code; however, the introduction of critical issues in more\ncomplex scenarios highlights the need for a systematic evaluation and\nvalidation of LLM solutions. Our work deepens the understanding of the\nstrengths and limitations of LLMs for code generation.", "AI": {"tldr": "This study compares the internal quality of LLM-generated and human-written code using SonarQube metrics across Python tasks of varying difficulty. LLM code shows fewer bugs and lower fix efforts but introduces structural issues in complex problems.", "motivation": "To evaluate how well LLMs generate code solutions in terms of software quality (maintainability, reliability) and compare them to human-written code under a systematic analysis framework.", "method": "Empirical analysis combining coding tasks datasets, three LLM configurations (zero-shot, few-shot, fine-tuning), and SonarQube quality metrics. Python code solutions were evaluated across introductory, interview, and competition difficulty levels.", "result": "LLM-generated code has fewer bugs and lower issue-resolution effort but may introduce new structural problems in complex tasks. Fine-tuning reduces high-severity issues but lowers overall performance.", "conclusion": "LLM-generated code demonstrates strong quality attributes overall, but structural weaknesses in complex scenarios necessitate careful validation. The study deepens understanding of LLM capabilities and limitations in code generation."}}
{"id": "2508.00738", "categories": ["cs.SE", "cs.FL", "68N30", "D.2.4"], "pdf": "https://arxiv.org/pdf/2508.00738", "abs": "https://arxiv.org/abs/2508.00738", "authors": ["Bernhard Rumpe", "Max Stachon", "Sebastian St\u00fcber", "Valdes Voufo"], "title": "Tool-Assisted Conformance Checking to Reference Process Models", "comment": null, "summary": "Reference models convey best practices and standards. The reference\nframeworks necessitate conformance checks to ensure adherence to established\nguidelines and principles, which is crucial for maintaining quality and\nconsistency in various processes. This paper explores automated conformance\nchecks for concrete process models against reference models using causal\ndependency analysis of tasks and events. Existing notions of conformance\nchecking for process models focus on verifying process execution traces and\nlack the expressiveness and automation needed for semantic model comparison,\nleaving this question unresolved. We integrate our approach into a broader\nsemantic framework for defining reference model conformance. We outline an\nalgorithm for reference process model conformance checking, evaluate it through\na case study, and discuss its strengths and limitations. Our research provides\na tool-assisted solution enhancing accuracy and flexibility in process model\nconformance verification.", "AI": {"tldr": "This paper introduces an automated method using causal dependency analysis to check process models against reference frameworks, improving conformance verification accuracy and flexibility.", "motivation": "Existing conformance checking methods rely solely on execution traces, lacking semantic expressiveness and automation for meaningful comparison against reference models, which are essential for ensuring quality and consistency across processes.", "method": "The authors integrate their approach into a semantic framework, propose an algorithm for reference model conformance checking, and validate it through a case study while analyzing strengths and limitations.", "result": "Evaluations demonstrate enhanced accuracy and flexibility in verifying process model conformance, offering a tool-assisted solution to address semantic comparison challenges previously unresolved by trace-based methods.", "conclusion": "The research presents a viable algorithmic approach within a semantic framework for robust reference model conformance checking, advancing beyond prior techniques by enabling causal dependency analysis of tasks and events."}}
{"id": "2508.00749", "categories": ["cs.SE", "cs.FL", "cs.SC", "68N30", "D.2.4"], "pdf": "https://arxiv.org/pdf/2508.00749", "abs": "https://arxiv.org/abs/2508.00749", "authors": ["Johanna Grahl", "Bernhard Rumpe", "Max Stachon", "Sebastian St\u00fcber"], "title": "Dynamic Symbolic Execution for Semantic Difference Analysis of Component and Connector Architectures", "comment": null, "summary": "In the context of model-driven development, ensuring the correctness and\nconsistency of evolving models is paramount. This paper investigates the\napplication of Dynamic Symbolic Execution (DSE) for semantic difference\nanalysis of component-and-connector architectures, specifically utilizing\nMontiArc models. We have enhanced the existing MontiArc-to-Java generator to\ngather both symbolic and concrete execution data at runtime, encompassing\ntransition conditions, visited states, and internal variables of automata. This\ndata facilitates the identification of significant execution traces that\nprovide critical insights into system behavior. We evaluate various execution\nstrategies based on the criteria of runtime efficiency, minimality, and\ncompleteness, establishing a framework for assessing the applicability of DSE\nin semantic difference analysis. Our findings indicate that while DSE shows\npromise for analyzing component and connector architectures, scalability\nremains a primary limitation, suggesting further research is needed to enhance\nits practical utility in larger systems.", "AI": {"tldr": "This paper explores using Dynamic Symbolic Execution (DSE) for semantic difference analysis in component-and-connector architectures (MontiArc) by enhancing execution data collection and evaluating strategies under constraints.", "motivation": "The paper addresses the critical need for ensuring correctness and consistency of evolving models in model-driven development, where traditional methods may fall short.", "method": "The authors improved a MontiArc-to-Java generator to capture symbolic/concrete runtime data (transition conditions, states, automata variables) and evaluate DSE strategies across efficiency, minimality, and completeness criteria.", "result": "DSE effectively identifies significant execution traces for system behavior insights, but scalability issues limit its practical use in large systems.", "conclusion": "While DSE shows potential for semantic difference analysis in architectures, research is needed to overcome scalability limitations for broader application."}}
{"id": "2508.00772", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.00772", "abs": "https://arxiv.org/abs/2508.00772", "authors": ["Md Imranur Rahman Akib", "Fathima Binthe Muhammed", "Umit Saha", "Md Fazlul Karim Patwary", "Mehrin Anannya", "Md Alomgeer Hussein", "Md Biplob Hosen"], "title": "From Code to Career: Assessing Competitive Programmers for Industry Placement", "comment": null, "summary": "In today's fast-paced tech industry, there is a growing need for tools that\nevaluate a programmer's job readiness based on their coding performance. This\nstudy focuses on predicting the potential of Codeforces users to secure various\nlevels of software engineering jobs. The primary objective is to analyze how a\nuser's competitive programming activity correlates with their chances of\nobtaining positions, ranging from entry-level roles to jobs at major tech\ncompanies. We collect user data using the Codeforces API, process key\nperformance metrics, and build a prediction model using a Random Forest\nclassifier. The model categorizes users into four levels of employability,\nranging from those needing further development to those ready for top-tier tech\njobs. The system is implemented using Flask and deployed on Render for\nreal-time predictions. Our evaluation demonstrates that the approach\neffectively distinguishes between different skill levels based on coding\nproficiency and participation. This work lays a foundation for the use of\nmachine learning in career assessment and could be extended to predict job\nreadiness in broader technical fields.", "AI": {"tldr": "This paper proposes a system to predict software engineering job readiness for Codeforces users using machine learning, categorizing them into four employability levels via a Random Forest classifier.", "motivation": "The tech industry requires tools to assess programmers' job readiness based on coding performance, motivating the analysis of competitive programming activity's correlation with job opportunity levels.", "method": "Data collection through Codeforces API, processing performance metrics, training a Random Forest classifier to predict job readiness, and implementing the system with Flask for real-time deployment.", "result": "The model successfully differentiates employability levels (entry-level to top-tier jobs) using coding proficiency and participation metrics, validated through system evaluation.", "conclusion": "The work establishes a machine learning foundation for career assessment in software engineering and suggests extensibility to other technical domains."}}
