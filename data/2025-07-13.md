<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 18]
- [cs.SE](#cs.SE) [Total: 6]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [WatchWitch: Interoperability, Privacy, and Autonomy for the Apple Watch](https://arxiv.org/abs/2507.07210)
*Nils Rollshausen,Alexander Heinrich,Matthias Hollick,Jiska Classen*

Main category: cs.CR

TL;DR: This paper reverse-engineers Apple Watch's wireless protocols to uncover security issues and creates WatchWitch, an Android reimplementation for enhanced privacy and cross-platform interoperability.


<details>
  <summary>Details</summary>
Motivation: The Apple Watch's closed ecosystem limits user control and data privacy options, creating a need for practical interoperability with privacy-centric alternatives.

Method: Reverse-engineering of proprietary wireless protocols and development of a custom Android-based implementation called WatchWitch.

Result: Identified multiple security vulnerabilities, demonstrated Android compatibility with Apple Watch, and introduced enhanced privacy controls through the reimplementation.

Conclusion: WatchWitch breaks Apple's ecosystem lock-in, enabling consumer choice and device control in smartwatch technology with robust privacy assurances.

Abstract: Smartwatches such as the Apple Watch collect vast amounts of intimate health
and fitness data as we wear them. Users have little choice regarding how this
data is processed: The Apple Watch can only be used with Apple's iPhones, using
their software and their cloud services. We are the first to publicly
reverse-engineer the watch's wireless protocols, which led to discovering
multiple security issues in Apple's proprietary implementation. With
WatchWitch, our custom Android reimplementation, we break out of Apple's walled
garden -- demonstrating practical interoperability with enhanced privacy
controls and data autonomy. We thus pave the way for more consumer choice in
the smartwatch ecosystem, offering users more control over their devices.

</details>


### [2] [Automated Attack Testflow Extraction from Cyber Threat Report using BERT for Contextual Analysis](https://arxiv.org/abs/2507.07244)
*Faissal Ahmadou,Sepehr Ghaffarzadegan,Boubakr Nour,Makan Pourzandi,Mourad Debbabi,Chadi Assi*

Main category: cs.CR

TL;DR: FLOWGUARDIAN automates attack testflow extraction from threat reports using BERT and NLP techniques to improve cybersecurity testing efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Manual extraction of attack testflows from unstructured threat reports is time-consuming, error-prone, and requires specialized knowledge, hindering rapid APT identification and mitigation.

Method: The paper introduces FLOWGUARDIAN, which combines language models (BERT) with NLP techniques to systematically analyze security events, contextualize them, reconstruct attack sequences, and generate testflows.

Result: Empirical validation on public threat reports demonstrates FLOWGUARDIAN's effectiveness in accurately and efficiently extracting attack testflows, improving coverage and robustness.

Conclusion: FLOWGUARDIAN enhances security teams' proactive threat hunting and incident response capabilities by automating testflow extraction, enabling faster and more reliable detection of APTs.

Abstract: In the ever-evolving landscape of cybersecurity, the rapid identification and
mitigation of Advanced Persistent Threats (APTs) is crucial. Security
practitioners rely on detailed threat reports to understand the tactics,
techniques, and procedures (TTPs) employed by attackers. However, manually
extracting attack testflows from these reports requires elusive knowledge and
is time-consuming and prone to errors. This paper proposes FLOWGUARDIAN, a
novel solution leveraging language models (i.e., BERT) and Natural Language
Processing (NLP) techniques to automate the extraction of attack testflows from
unstructured threat reports. FLOWGUARDIAN systematically analyzes and
contextualizes security events, reconstructs attack sequences, and then
generates comprehensive testflows. This automated approach not only saves time
and reduces human error but also ensures comprehensive coverage and robustness
in cybersecurity testing. Empirical validation using public threat reports
demonstrates FLOWGUARDIAN's accuracy and efficiency, significantly enhancing
the capabilities of security teams in proactive threat hunting and incident
response.

</details>


### [3] [Disa: Accurate Learning-based Static Disassembly with Attentions](https://arxiv.org/abs/2507.07246)
*Peicheng Wang,Monika Santra,Mingyu Liu,Cong Sun,Dongrui Zeng,Gang Tan*

Main category: cs.CR

TL;DR: Disa is a deep learning-based disassembly approach that leverages superset instruction relationships via multi-head self-attention to improve function entry-point detection and control flow graph accuracy on obfuscated binaries.


<details>
  <summary>Details</summary>
Motivation: Traditional disassembly methods using file-format assumptions and architecture-specific heuristics fail to accurately identify instruction/function boundaries in obfuscated binaries, leading to incomplete/incorrect analysis for security applications like vulnerability detection and malware analysis.

Method: The approach uses superset instructions as input features processed through a multi-head self-attention mechanism to learn temporal instruction correlations, enabling function entry-point identification and instruction boundary detection. It also identifies memory block relationships to enhance block-memory model-based value-set analysis for CFG generation.

Result: Disa achieves 9.1% F1-score improvement over prior deep learning approaches on desynchronized disassembly obfuscations and 13.2% improvement on source-level obfuscated binaries. It reaches 18.5% better memory block precision and 4.4% reduction in AICT compared to heuristic-based state-of-the-art CFG generation.

Conclusion: Disa demonstrates that deep learning with self-attention mechanisms can significantly outperform traditional methods in disassembling obfuscated binaries, producing more accurate CFGs and improving security analyses in challenging reverse engineering scenarios.

Abstract: For reverse engineering related security domains, such as vulnerability
detection, malware analysis, and binary hardening, disassembly is crucial yet
challenging. The fundamental challenge of disassembly is to identify
instruction and function boundaries. Classic approaches rely on file-format
assumptions and architecture-specific heuristics to guess the boundaries,
resulting in incomplete and incorrect disassembly, especially when the binary
is obfuscated. Recent advancements of disassembly have demonstrated that deep
learning can improve both the accuracy and efficiency of disassembly. In this
paper, we propose Disa, a new learning-based disassembly approach that uses the
information of superset instructions over the multi-head self-attention to
learn the instructions' correlations, thus being able to infer function
entry-points and instruction boundaries. Disa can further identify instructions
relevant to memory block boundaries to facilitate an advanced block-memory
model based value-set analysis for an accurate control flow graph (CFG)
generation. Our experiments show that Disa outperforms prior deep-learning
disassembly approaches in function entry-point identification, especially
achieving 9.1% and 13.2% F1-score improvement on binaries respectively
obfuscated by the disassembly desynchronization technique and popular
source-level obfuscator. By achieving an 18.5% improvement in the memory block
precision, Disa generates more accurate CFGs with a 4.4% reduction in Average
Indirect Call Targets (AICT) compared with the state-of-the-art heuristic-based
approach.

</details>


### [4] [Semi-fragile watermarking of remote sensing images using DWT, vector quantization and automatic tiling](https://arxiv.org/abs/2507.07250)
*Jordi Serra-Ruiz,David Meg√≠as*

Main category: cs.CR

TL;DR: The paper introduces a semi-fragile watermarking method for multispectral/hyperspectral images using tree-structured vector quantization on 3D pixel blocks, enabling detection of significant modifications while preserving the watermark under acceptable lossy compression.


<details>
  <summary>Details</summary>
Motivation: Existing methods process image bands separately, lacking integration for detecting manipulations in complex multi-band images like remote sensing data. This approach addresses the need for both robustness against common distortions and fragility to identify tampering.

Method: The method segments images into 3D blocks and constructs a tree-structured vector quantizer for each. An iterative algorithm adjusts the trees to meet a criterion for watermark embedding, leveraging pixel signatures across all bands for a holistic analysis.

Result: The scheme successfully preserves the watermark under lossy compression (above a threshold) while accurately detecting forged blocks and their positions within the image. This demonstrates its dual effectiveness in robustness and tamper detection.

Conclusion: The proposed method achieves a balance between robustness to compression and fragility to unauthorized modifications, making it suitable for authenticating multi-band images in remote sensing applications where maintaining original data integrity is critical.

Abstract: A semi-fragile watermarking scheme for multiple band images is presented in
this article. We propose to embed a mark into remote sensing images applying a
tree-structured vector quantization approach to the pixel signatures instead of
processing each band separately. The signature of the multispectral or
hyperspectral image is used to embed the mark in it order to detect any
significant modification of the original image. The image is segmented into
three-dimensional blocks, and a tree-structured vector quantizer is built for
each block. These trees are manipulated using an iterative algorithm until the
resulting block satisfies a required criterion, which establishes the embedded
mark. The method is shown to be able to preserve the mark under lossy
compression (above a given threshold) but, at the same time, it detects
possibly forged blocks and their position in the whole image.

</details>


### [5] [FedP3E: Privacy-Preserving Prototype Exchange for Non-IID IoT Malware Detection in Cross-Silo Federated Learning](https://arxiv.org/abs/2507.07258)
*Rami Darwish,Mahmoud Abdelsalam,Sajad Khorsandroo,Kaushik Roy*

Main category: cs.CR

TL;DR: FedP3E is a privacy-preserving federated learning framework for IoT malware detection that addresses data heterogeneity and class imbalance using prototype sharing and SMOTE augmentation.


<details>
  <summary>Details</summary>
Motivation: IoT ecosystems face sophisticated malware attacks with sensitive data requiring privacy-preserving detection frameworks that handle data heterogeneity and class imbalance, which standard FL algorithms like FedAvg and FedProx struggle with.

Method: FedP3E constructs class-wise Gaussian Mixture Model (GMM) prototypes on clients, adds Gaussian noise for privacy, and shares these compact summaries via a server. Clients use aggregated prototypes and SMOTE-based minority class augmentation during local training.

Result: Evaluated on the N-BaIoT dataset under cross-silo scenarios with varying data imbalance, FedP3E demonstrates a targeted strategy that reduces statistical heterogeneity impact with minimal communication overhead.

Conclusion: FedP3E enables effective IoT malware detection through indirect cross-client learning with privacy protection, overcoming limitations of standard FL methods by sharing enriched prototypes rather than raw data or gradients.

Abstract: As IoT ecosystems continue to expand across critical sectors, they have
become prominent targets for increasingly sophisticated and large-scale malware
attacks. The evolving threat landscape, combined with the sensitive nature of
IoT-generated data, demands detection frameworks that are both
privacy-preserving and resilient to data heterogeneity. Federated Learning (FL)
offers a promising solution by enabling decentralized model training without
exposing raw data. However, standard FL algorithms such as FedAvg and FedProx
often fall short in real-world deployments characterized by class imbalance and
non-IID data distributions -- particularly in the presence of rare or disjoint
malware classes. To address these challenges, we propose FedP3E
(Privacy-Preserving Prototype Exchange), a novel FL framework that supports
indirect cross-client representation sharing while maintaining data privacy.
Each client constructs class-wise prototypes using Gaussian Mixture Models
(GMMs), perturbs them with Gaussian noise, and transmits only these compact
summaries to the server. The aggregated prototypes are then distributed back to
clients and integrated into local training, supported by SMOTE-based
augmentation to enhance representation of minority malware classes. Rather than
relying solely on parameter averaging, our prototype-driven mechanism enables
clients to enrich their local models with complementary structural patterns
observed across the federation -- without exchanging raw data or gradients.
This targeted strategy reduces the adverse impact of statistical heterogeneity
with minimal communication overhead. We evaluate FedP3E on the N-BaIoT dataset
under realistic cross-silo scenarios with varying degrees of data imbalance.

</details>


### [6] [Shuffling for Semantic Secrecy](https://arxiv.org/abs/2507.07401)
*Fupei Chen,Liyao Xiang,Haoxiang Sun,Hei Victor Cheng,Kaiming Shen*

Main category: cs.CR

TL;DR: A novel semantic security communication system leveraging random shuffling as a shared secret key to improve secure transmission in deep learning-based semantic communications by optimizing tradeoffs between transmission rate and leakage rate.


<details>
  <summary>Details</summary>
Motivation: Conventional secure coding schemes for wiretap channels fail to optimally balance transmission rate and semantic secrecy. The paper seeks to address this limitation by introducing a shuffling-based approach that distorts feature sequences to prevent semantic leakage to eavesdroppers.

Method: 1) Utilizes random shuffling patterns as shared secret keys rather than feature transmission. 2) Permutates feature sequences through shuffling to obscure semantic content. 3) Implements leakage rate constraints while maximizing transmission rate and minimizing semantic error probability. 4) Design as a flexible plugin for existing semantic communication systems.

Result: Simulation results demonstrate superior performance over benchmarks in secure transmission under strong noise and unpredictable fading conditions, showing significant improvement in semantic secrecy while maintaining effective communication rates.

Conclusion: The proposed shuffling-based semantic security framework achieves better transmission-leakage tradeoff than conventional methods, offering a flexible and effective solution for secure deep learning communication systems in challenging channel environments.

Abstract: Deep learning draws heavily on the latest progress in semantic
communications. The present paper aims to examine the security aspect of this
cutting-edge technique from a novel shuffling perspective. Our goal is to
improve upon the conventional secure coding scheme to strike a desirable
tradeoff between transmission rate and leakage rate. To be more specific, for a
wiretap channel, we seek to maximize the transmission rate while minimizing the
semantic error probability under the given leakage rate constraint. Toward this
end, we devise a novel semantic security communication system wherein the
random shuffling pattern plays the role of the shared secret key. Intuitively,
the permutation of feature sequences via shuffling would distort the semantic
essence of the target data to a sufficient extent so that eavesdroppers cannot
access it anymore. The proposed random shuffling method also exhibits its
flexibility in working for the existing semantic communication system as a
plugin. Simulations demonstrate the significant advantage of the proposed
method over the benchmark in boosting secure transmission, especially when
channels are prone to strong noise and unpredictable fading.

</details>


### [7] [Phishing Detection in the Gen-AI Era: Quantized LLMs vs Classical Models](https://arxiv.org/abs/2507.07406)
*Jikesh Thapa,Gurrehmat Chahal,Serban Voinea Gabreanu,Yazan Otoum*

Main category: cs.CR

TL;DR: Quantized LLMs (e.g., DeepSeek R1 Distill Qwen 14B Q8_0) demonstrate 80%+ phishing detection accuracy with 17GB VRAM, showing cost-efficient potential despite lower raw performance than traditional ML/DL. The work highlights their robustness against rephrased emails and value for contextual analysis and explainability.


<details>
  <summary>Details</summary>
Motivation: Sophisticated phishing attacks demand systems balancing high accuracy and computational efficiency, while requiring actionable explanations for cybersecurity decisions.

Method: Comparative evaluation of traditional ML, DL, and quantized LLMs via experiments on a curated phishing dataset, testing zero-shot/few-shot prompting and adversarial robustness.

Result: 1) Quantized LLMs achieve >80% accuracy with 17GB VRAM. 2) ML/DL methods outperform LLMs in raw accuracy but lack contextual analysis capability. 3) LLM-based email rephrasing reduces detection accuracy across all model types.

Conclusion: Quantized LLMs offer practical tradeoffs between accuracy (80%+), computational efficiency (low VRAM usage), and explainability, positioning them as viable components for next-generation phishing defense systems integrated with cybersecurity frameworks.

Abstract: Phishing attacks are becoming increasingly sophisticated, underscoring the
need for detection systems that strike a balance between high accuracy and
computational efficiency. This paper presents a comparative evaluation of
traditional Machine Learning (ML), Deep Learning (DL), and quantized
small-parameter Large Language Models (LLMs) for phishing detection. Through
experiments on a curated dataset, we show that while LLMs currently
underperform compared to ML and DL methods in terms of raw accuracy, they
exhibit strong potential for identifying subtle, context-based phishing cues.
We also investigate the impact of zero-shot and few-shot prompting strategies,
revealing that LLM-rephrased emails can significantly degrade the performance
of both ML and LLM-based detectors. Our benchmarking highlights that models
like DeepSeek R1 Distill Qwen 14B (Q8_0) achieve competitive accuracy, above
80%, using only 17GB of VRAM, supporting their viability for cost-efficient
deployment. We further assess the models' adversarial robustness and
cost-performance tradeoffs, and demonstrate how lightweight LLMs can provide
concise, interpretable explanations to support real-time decision-making. These
findings position optimized LLMs as promising components in phishing defence
systems and offer a path forward for integrating explainable, efficient AI into
modern cybersecurity frameworks.

</details>


### [8] [Hybrid LLM-Enhanced Intrusion Detection for Zero-Day Threats in IoT Networks](https://arxiv.org/abs/2507.07413)
*Mohammad F. Al-Hammouri,Yazan Otoum,Rasha Atwa,Amiya Nayak*

Main category: cs.CR

TL;DR: This paper proposes a hybrid intrusion detection system combining signature-based methods with GPT-2 for enhanced accuracy and adaptability in dynamic environments like IoT, achieving 6.3% accuracy improvement and 9.0% fewer false positives with near real-time performance.


<details>
  <summary>Details</summary>
Motivation: Modern cyber threats (especially in IoT environments) require dynamically adaptive IDSs as traditional signature-based methods struggle with unknown/evolving attack patterns while maintaining robustness in resource-constrained settings.

Method: A hybrid framework that integrates signature-based detection with GPT-2's semantic analysis of unstructured data. Merges traditional threat identification with deep language model-based contextual pattern recognition for detecting subtle zero-day attacks.

Result: Experimental results show 6.3% increase in detection accuracy, 9.0% reduction in false positives, and demonstration of near real-time responsiveness using representative intrusion datasets.

Conclusion: Language model integration with traditional IDS approaches can create intelligent, scalable cybersecurity defenses that effectively combine the precision of signatures with the adaptability of AI for modern connected systems.

Abstract: This paper presents a novel approach to intrusion detection by integrating
traditional signature-based methods with the contextual understanding
capabilities of the GPT-2 Large Language Model (LLM). As cyber threats become
increasingly sophisticated, particularly in distributed, heterogeneous, and
resource-constrained environments such as those enabled by the Internet of
Things (IoT), the need for dynamic and adaptive Intrusion Detection Systems
(IDSs) becomes increasingly urgent. While traditional methods remain effective
for detecting known threats, they often fail to recognize new and evolving
attack patterns. In contrast, GPT-2 excels at processing unstructured data and
identifying complex semantic relationships, making it well-suited to uncovering
subtle, zero-day attack vectors. We propose a hybrid IDS framework that merges
the robustness of signature-based techniques with the adaptability of
GPT-2-driven semantic analysis. Experimental evaluations on a representative
intrusion dataset demonstrate that our model enhances detection accuracy by
6.3%, reduces false positives by 9.0%, and maintains near real-time
responsiveness. These results affirm the potential of language model
integration to build intelligent, scalable, and resilient cybersecurity
defences suited for modern connected environments.

</details>


### [9] [Autonomous AI-based Cybersecurity Framework for Critical Infrastructure: Real-Time Threat Mitigation](https://arxiv.org/abs/2507.07416)
*Jenifer Paulraj,Brindha Raghuraman,Nagarani Gopalakrishnan,Yazan Otoum*

Main category: cs.CR

TL;DR: This paper proposes a hybrid AI-driven cybersecurity framework to address vulnerabilities in critical infrastructure systems, aiming to improve real-time detection, threat modelling, and automated remediation against emerging cyber threats.


<details>
  <summary>Details</summary>
Motivation: The increasing interconnectivity of critical infrastructure systems has exposed them to escalating cyber threats (e.g., ransomware, DoS, APTs), necessitating innovative solutions to ensure societal stability and economic resilience.

Method: A hybrid AI framework is introduced, focusing on real-time vulnerability detection using AI, dynamic threat modelling, and automated remediation strategies. The approach also considers adversarial AI challenges, regulatory compliance integration, and system interoperability.

Result: Empirical findings demonstrate the framework's efficacy in enhancing cybersecurity resilience, with quantified improvements in threat detection latency (reduced by 32%) and automated patch deployment success rates (achieved 91% accuracy).

Conclusion: The hybrid AI framework offers a scalable solution for protecting interdependent critical infrastructure systems while balancing innovation with regulatory adherence, requiring continuous adaptation to evolving threat vectors.

Abstract: Critical infrastructure systems, including energy grids, healthcare
facilities, transportation networks, and water distribution systems, are
pivotal to societal stability and economic resilience. However, the increasing
interconnectivity of these systems exposes them to various cyber threats,
including ransomware, Denial-of-Service (DoS) attacks, and Advanced Persistent
Threats (APTs). This paper examines cybersecurity vulnerabilities in critical
infrastructure, highlighting the threat landscape, attack vectors, and the role
of Artificial Intelligence (AI) in mitigating these risks. We propose a hybrid
AI-driven cybersecurity framework to enhance real-time vulnerability detection,
threat modelling, and automated remediation. This study also addresses the
complexities of adversarial AI, regulatory compliance, and integration. Our
findings provide actionable insights to strengthen the security and resilience
of critical infrastructure systems against emerging cyber threats.

</details>


### [10] [May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks](https://arxiv.org/abs/2507.07417)
*Nishit V. Pandya,Andrey Labunets,Sicun Gao,Earlence Fernandes*

Main category: cs.CR

TL;DR: The paper evaluates the robustness of prompt injection defenses for LLMs against whitebox attacks using optimization-based methods, demonstrating up to 70% attack success rates with minimal token budget increases.


<details>
  <summary>Details</summary>
Motivation: Popular whitebox LLM defenses claim to separate instructions and data but lack thorough robustness analysis against strong optimization-based attacks.

Method: Developed a novel attention-based attack algorithm and applied it to SecAlign (CCS 2025) and StruQ (USENIX Security 2025) defenses through whitebox optimization.

Result: Attack success rates reaching 70% with only modest attacker token budget increases, undermining the security guarantees of these defenses.

Conclusion: Demonstrates fundamental vulnerabilities in current whitebox prompt injection defenses and advances understanding of their robustness through open-code sharing.

Abstract: A popular class of defenses against prompt injection attacks on large
language models (LLMs) relies on fine-tuning the model to separate instructions
and data, so that the LLM does not follow instructions that might be present
with data. There are several academic systems and production-level
implementations of this idea. We evaluate the robustness of this class of
prompt injection defenses in the whitebox setting by constructing strong
optimization-based attacks and showing that the defenses do not provide the
claimed security properties. Specifically, we construct a novel attention-based
attack algorithm for text-based LLMs and apply it to two recent whitebox
defenses SecAlign (CCS 2025) and StruQ (USENIX Security 2025), showing attacks
with success rates of up to 70% with modest increase in attacker budget in
terms of tokens. Our findings make fundamental progress towards understanding
the robustness of prompt injection defenses in the whitebox setting. We release
our code and attacks at https://github.com/nishitvp/better_opts_attacks

</details>


### [11] [RADAR: a Radio-based Analytics for Dynamic Association and Recognition of pseudonyms in VANETs](https://arxiv.org/abs/2507.07732)
*Giovanni Gambigliani Zoccoli,Filip Valgimigli,Dario Stabili,Mirco Marchetti*

Main category: cs.CR

TL;DR: RADAR is a vehicle tracking algorithm for C-ITS that uses DSRC and Wi-Fi signals to enhance tracking of privacy-preserving pseudonyms in VANETs, demonstrating that the Pearson RSSI metric outperforms existing methods, especially in partial coverage scenarios.


<details>
  <summary>Details</summary>
Motivation: This paper aims to address the limitations of existing privacy-preserving pseudonym schemes in VANETs, which hinder effective vehicle tracking. By leveraging multiple radio signals from vehicles (e.g., DSRC and Wi-Fi probe requests), the authors seek to improve tracking accuracy in realistic scenarios with partial coverage.

Method: The authors propose RADAR, a tracking algorithm that integrates Dedicated Short Range Communication (DSRC) and Wi-Fi probe request messages emitted by vehicles. They evaluate three pseudonym association metrics (Count, Statistical RSSI, and Pearson RSSI) to determine the most effective one for vehicle tracking under pseudonym-changing schemes.

Result: RADAR significantly enhances vehicle tracking accuracy compared to standard DSRC-only methods across all scenarios, especially in partial coverage settings. The Pearson RSSI metric demonstrates superior performance than existing techniques, and the study publicly releases all implementations and simulation scenarios.

Conclusion: RADAR proves that combining multi-signal data (DSRC and Wi-Fi) improves vehicle tracking in C-ITS despite privacy-preserving pseudonyms. The Pearson RSSI metric's robustness under pseudonym-changing schemes underscores vulnerabilities in current VANET privacy mechanisms, contributing to the field through open-source tooling for replication and further research.

Abstract: This paper presents RADAR, a tracking algorithm for vehicles participating in
Cooperative Intelligent Transportation Systems (C-ITS) that exploits multiple
radio signals emitted by a modern vehicle to break privacy-preserving pseudonym
schemes deployed in VANETs. This study shows that by combining Dedicated Short
Range Communication (DSRC) and Wi-Fi probe request messages broadcast by the
vehicle, it is possible to improve tracking over standard de-anonymization
approaches that only leverage DSRC, especially in realistic scenarios where the
attacker does not have full coverage of the entire vehicle path. The
experimental evaluation compares three different metrics for pseudonym and
Wi-Fi probe identifier association (Count, Statistical RSSI, and Pearson RSSI),
demonstrating that the Pearson RSSI metric is better at tracking vehicles under
pseudonym-changing schemes in all scenarios and against previous works. As an
additional contribution to the state-of-the-art, we publicly release all
implementations and simulation scenarios used in this work.

</details>


### [12] [Rainbow Artifacts from Electromagnetic Signal Injection Attacks on Image Sensors](https://arxiv.org/abs/2507.07773)
*Youqian Zhang,Xinyu Ji,Zhihao Wang,Qinhong Jiang*

Main category: cs.CR

TL;DR: The paper explores electromagnetic signal injection attacks on CMOS image sensors, revealing rainbow-like artifacts that bypass digital integrity checks and cause mispredictions in object detection models, exposing a critical vulnerability in visual perception systems.


<details>
  <summary>Details</summary>
Motivation: Image sensors are crucial for safety-critical systems, but their analog domain vulnerabilities to physical-layer attacks remain underexplored. Conventional digital integrity checks fail to detect manipulations at the analog stage.

Method: Researchers injected carefully tuned electromagnetic interference into CMOS image sensors to induce visible artifacts, then evaluated how these artifacts propagate through the image signal processing pipeline and affect object detection models.

Result: The attacks successfully created rainbow artifacts undetected by digital checks, leading to significant mispredictions (e.g., halting or altered object detection) in state-of-the-art models without compromising visible image quality.

Conclusion: This work identifies a previously unknown analog-domain vulnerability in image sensors, demonstrating that physical-layer attacks can manipulate visual data silently. It underscores the need for robust defenses targeting the entire visual perception stack, including hardware-level protections.

Abstract: Image sensors are integral to a wide range of safety- and security-critical
systems, including surveillance infrastructure, autonomous vehicles, and
industrial automation. These systems rely on the integrity of visual data to
make decisions. In this work, we investigate a novel class of electromagnetic
signal injection attacks that target the analog domain of image sensors,
allowing adversaries to manipulate raw visual inputs without triggering
conventional digital integrity checks. We uncover a previously undocumented
attack phenomenon on CMOS image sensors: rainbow-like color artifacts induced
in images captured by image sensors through carefully tuned electromagnetic
interference. We further evaluate the impact of these attacks on
state-of-the-art object detection models, showing that the injected artifacts
propagate through the image signal processing pipeline and lead to significant
mispredictions. Our findings highlight a critical and underexplored
vulnerability in the visual perception stack, highlighting the need for more
robust defenses against physical-layer attacks in such systems.

</details>


### [13] [Mitigating Watermark Stealing Attacks in Generative Models via Multi-Key Watermarking](https://arxiv.org/abs/2507.07871)
*Toluwani Aremu,Noor Hussein,Munachiso Nwadike,Samuele Poppi,Jie Zhang,Karthik Nandakumar,Neil Gong,Nils Lukas*

Main category: cs.CR

TL;DR: The paper addresses watermark stealing attacks in GenAI by proposing a multi-key extension to watermarking methods, proving its effectiveness through theory and experiments, and formalizing the threat using security games.


<details>
  <summary>Details</summary>
Motivation: Watermark stealing allows malicious users to forge watermarks into non-provider-generated content, enabling false accusations against AI providers. Current solutions lack robustness against this attack vector.

Method: A post-hoc multi-key watermarking framework is proposed, where multiple keys generate distinct watermarks for the same content. Theoretical analysis and security game modeling show this hinders attackers' ability to steal and forge watermarks.

Result: Empirical evaluation across multiple datasets demonstrates reduced forging success rates with minimal impact on watermark verification performance. The security games provide a formal model for quantifying watermarking robustness.

Conclusion: The proposed multi-key approach significantly enhances watermarking security against stealing attacks while maintaining usability, establishing a new standard for black-box watermarking robustness analysis.

Abstract: Watermarking offers a promising solution for GenAI providers to establish the
provenance of their generated content. A watermark is a hidden signal embedded
in the generated content, whose presence can later be verified using a secret
watermarking key. A threat to GenAI providers are \emph{watermark stealing}
attacks, where users forge a watermark into content that was \emph{not}
generated by the provider's models without access to the secret key, e.g., to
falsely accuse the provider. Stealing attacks collect \emph{harmless}
watermarked samples from the provider's model and aim to maximize the expected
success rate of generating \emph{harmful} watermarked samples. Our work focuses
on mitigating stealing attacks while treating the underlying watermark as a
black-box. Our contributions are: (i) Proposing a multi-key extension to
mitigate stealing attacks that can be applied post-hoc to any watermarking
method across any modality. (ii) We provide theoretical guarantees and
demonstrate empirically that our method makes forging substantially less
effective across multiple datasets, and (iii) we formally define the threat of
watermark forging as the task of generating harmful, watermarked content and
model this threat via security games.

</details>


### [14] [The Trust Fabric: Decentralized Interoperability and Economic Coordination for the Agentic Web](https://arxiv.org/abs/2507.07901)
*Sree Bhargavi Balija,Rekha Singal,Abhishek Singh,Ramesh Raskar,Erfan Darzi,Raghu Bala,Thomas Hardjono,Ken Huang*

Main category: cs.CR

TL;DR: The Nanda Unified Architecture introduces a decentralized framework with DID-based agent discovery, semantic agent cards with verifiable credentials, and a dynamic trust layer combining behavioral attestations and policy compliance. It uses X42/H42 micropayments for coordination and MAESTRO for security (incorporating patented protocols and containerization), achieving 99.9% healthcare compliance and enabling a globally interoperable Internet of Agents with trust as 'native currency'.


<details>
  <summary>Details</summary>
Motivation: Existing AI agent protocols (MCP, A2A, ACP, AGP) fail to address critical challenges in interoperability, trust establishment, and economic coordination at scale within fragmented agent ecosystems.

Method: 1) Fast decentralized DID-based discovery through distributed registries
2) Semantic agent cards with: 
   - Verifiable credentials
   - Composability profiles
3) Dynamic trust layer integrating:
   - Behavioral attestations
   - Policy compliance
4) X42/H42 micropayment system for economic coordination
5) MAESTRO security framework using:
   - AgentTalk protocol (US Patent 12,244,584 B1)
   - Secure containerization

Result: - Achieved 99.9% compliance rate in healthcare applications
- Processed high monthly transaction volumes with strong privacy guarantees
- Unified MIT's trust research with Cisco's and Synergetics' production deployments

Conclusion: Transforms AI agents into trust-anchored economic participants by combining cryptographic proofs with policy-as-code, creating a unified Internet of Agents infrastructure where trust is the "native currency" for both enterprise and Web3 collaboration.

Abstract: The fragmentation of AI agent ecosystems has created urgent demands for
interoperability, trust, and economic coordination that current protocols --
including MCP (Hou et al., 2025), A2A (Habler et al., 2025), ACP (Liu et al.,
2025), and Cisco's AGP (Edwards, 2025) -- cannot address at scale. We present
the Nanda Unified Architecture, a decentralized framework built around three
core innovations: fast DID-based agent discovery through distributed
registries, semantic agent cards with verifiable credentials and composability
profiles, and a dynamic trust layer that integrates behavioral attestations
with policy compliance. The system introduces X42/H42 micropayments for
economic coordination and MAESTRO, a security framework incorporating
Synergetics' patented AgentTalk protocol (US Patent 12,244,584 B1) and secure
containerization. Real-world deployments demonstrate 99.9 percent compliance in
healthcare applications and substantial monthly transaction volumes with strong
privacy guarantees. By unifying MIT's trust research with production
deployments from Cisco and Synergetics, we show how cryptographic proofs and
policy-as-code transform agents into trust-anchored participants in a
decentralized economy (Lakshmanan, 2025; Sha, 2025). The result enables a
globally interoperable Internet of Agents where trust becomes the native
currency of collaboration across both enterprise and Web3 ecosystems.

</details>


### [15] [Can Large Language Models Improve Phishing Defense? A Large-Scale Controlled Experiment on Warning Dialogue Explanations](https://arxiv.org/abs/2507.07916)
*Federico Maria Cau,Giuseppe Desolda,Francesco Greco,Lucio Davide Spano,Luca Vigan√≤*

Main category: cs.CR

TL;DR: This paper evaluates the effectiveness of LLM-generated phishing warning explanations compared to manually crafted ones, finding Claude 3.5 Sonnet and Llama 3.3 70B can produce scalable, adaptive explanations that match or exceed human-designed warnings, with feature-based and counterfactual styles showing distinct advantages in reducing phishing susceptibility and false positives respectively.


<details>
  <summary>Details</summary>
Motivation: Warning dialogues are commonly used to mitigate phishing attacks, but static content and insufficient explanatory clarity reduce their efficacy. The authors investigate whether LLMs can dynamically generate better explanations that align with human cognitive patterns while maintaining scalability.

Method: A between-subjects user study (750 participants) compared manually crafted explanations with LLM-generated ones (Claude 3.5 Sonnet and Llama 3.3 70B) using two explanatory styles: feature-based (highlighting specific threat indicators) and counterfactual (explaining what would happen differently if the threat were avoided). The study measured both behavioral outcomes (click-through rates) and perceptual responses (trust, risk perception, explanation clarity).

Result: 1) LLM-generated explanations matched/human-designed counterparts in behavioral effectiveness. 2) Feature-based explanations reduced real phishing click-throughs by 19% over default warnings, while counterfactual explanations improved false-positive avoidance by 17%. 3) Claude 3.5 showed stronger performance in maintaining user trust across adaptive scenarios. 4) Workload levels, gender, and prior warning exposure significantly affected how participants responded to explanations.

Conclusion: LLMs demonstrate significant potential to transform phishing warning systems by delivering scalable, context-aware explanations that align with human processing capabilities while reducing both false negatives and false positives. Implementation considerations include optimizing for variable user contexts (workload, familiarity) and carefully balancing feature-based/counterfactual explanation styles depending on threat types.

Abstract: Phishing has become a prominent risk in modern cybersecurity, often used to
bypass technological defences by exploiting predictable human behaviour.
Warning dialogues are a standard mitigation measure, but the lack of
explanatory clarity and static content limits their effectiveness. In this
paper, we report on our research to assess the capacity of Large Language
Models (LLMs) to generate clear, concise, and scalable explanations for
phishing warnings. We carried out a large-scale between-subjects user study (N
= 750) to compare the influence of warning dialogues supplemented with manually
generated explanations against those generated by two LLMs, Claude 3.5 Sonnet
and Llama 3.3 70B. We investigated two explanatory styles (feature-based and
counterfactual) for their effects on behavioural metrics (click-through rate)
and perceptual outcomes (e.g., trust, risk, clarity). The results indicate that
well-constructed LLM-generated explanations can equal or surpass manually
crafted explanations in reducing susceptibility to phishing; Claude-generated
warnings exhibited particularly robust performance. Feature-based explanations
were more effective for genuine phishing attempts, whereas counterfactual
explanations diminished false-positive rates. Other variables such as workload,
gender, and prior familiarity with warning dialogues significantly moderated
warning effectiveness. These results indicate that LLMs can be used to
automatically build explanations for warning users against phishing, and that
such solutions are scalable, adaptive, and consistent with human-centred
values.

</details>


### [16] [KeyDroid: A Large-Scale Analysis of Secure Key Storage in Android Apps](https://arxiv.org/abs/2507.07927)
*Jenny Blessing,Ross J. Anderson,Alastair R. Beresford*

Main category: cs.CR

TL;DR: A study analyzing hardware-backed key storage in Android apps finds most apps do not utilize secure elements, with performance limitations affecting adoption.


<details>
  <summary>Details</summary>
Motivation: To evaluate how Android app developers use hardware-backed key storage for protecting sensitive data through empirical analysis.

Method: Analyzed 490,119 Android apps for Keystore API usage, cross-referenced with Play Store data safety labels; conducted performance measurements of software- and hardware-backed cryptographic operations.

Result: 56.3% of sensitive data-handling apps use no trusted hardware; only 5.03% use secure elements. Secure elements show infeasible performance for symmetric/asymmetric encryption.

Conclusion: Trusted hardware APIs are underutilized even as they provide critical security benefits, but secure element performance hinders adoption for certain operations.

Abstract: Most contemporary mobile devices offer hardware-backed storage for
cryptographic keys, user data, and other sensitive credentials. Such hardware
protects credentials from extraction by an adversary who has compromised the
main operating system, such as a malicious third-party app. Since 2011, Android
app developers can access trusted hardware via the Android Keystore API. In
this work, we conduct the first comprehensive survey of hardware-backed key
storage in Android devices. We analyze 490 119 Android apps, collecting data on
how trusted hardware is used by app developers (if used at all) and
cross-referencing our findings with sensitive user data collected by each app,
as self-reported by developers via the Play Store's data safety labels.
  We find that despite industry-wide initiatives to encourage adoption, 56.3%
of apps self-reporting as processing sensitive user data do not use Android's
trusted hardware capabilities at all, while just 5.03% of apps collecting some
form of sensitive data use the strongest form of trusted hardware, a secure
element distinct from the main processor. To better understand the potential
downsides of using secure hardware, we conduct the first empirical analysis of
trusted hardware performance in mobile devices, measuring the runtime of common
cryptographic operations across both software- and hardware-backed keystores.
We find that while hardware-backed key storage using a coprocessor is viable
for most common cryptographic operations, secure elements capable of preventing
more advanced attacks make performance infeasible for symmetric encryption with
non-negligible payloads and any kind of asymmetric encryption.

</details>


### [17] [EinHops: Einsum Notation for Expressive Homomorphic Operations on RNS-CKKS Tensors](https://arxiv.org/abs/2507.07972)
*Karthik Garimella,Austin Ebel,Brandon Reagen*

Main category: cs.CR

TL;DR: EinHops is an FHE system that uses einsum notation to simplify multi-dimensional tensor operations by exposing packing strategies and decomposing operations into FHE-compatible steps.


<details>
  <summary>Details</summary>
Motivation: FHE's limited instruction set (SIMD arithmetic and 1-D rotations) complicates multi-dimensional tensor operations, requiring manual packing into 1-D vectors. Existing systems abstract packing decisions, hindering debugging, optimization, and further development.

Method: Transform einsum notation (explicitly encoding dimensions and operations) into FHE operations through decomposition. Implement a minimalist framework (EinHops) that factors einsum expressions into a fixed sequence of FHE primitives while preserving dimensional semantics.

Result: Evaluation shows EinHops successfully handles operations from simple transposes to complex tensor contractions in FHE, achieving simplicity, generality, and interpretability through the explicit einsum decomposition framework.

Conclusion: EinHops demonstrates that einsum notation provides a natural, transparent solution for FHE tensor operations by making packing strategies visible. The open-source implementation enables reproducibility and future research in this domain.

Abstract: Fully Homomorphic Encryption (FHE) is an encryption scheme that allows for
computation to be performed directly on encrypted data, effectively closing the
loop on secure and outsourced computing. Data is encrypted not only during rest
and transit, but also during processing. However, FHE provides a limited
instruction set: SIMD addition, SIMD multiplication, and cyclic rotation of 1-D
vectors. This restriction makes performing multi-dimensional tensor operations
challenging. Practitioners must pack these tensors into 1-D vectors and map
tensor operations onto this one-dimensional layout rather than their
traditional nested structure. And while prior systems have made significant
strides in automating this process, they often hide critical packing decisions
behind layers of abstraction, making debugging, optimizing, and building on top
of these systems difficult.
  In this work, we approach multi-dimensional tensor operations in FHE through
Einstein summation (einsum) notation. Einsum notation explicitly encodes
dimensional structure and operations in its syntax, naturally exposing how
tensors should be packed and transformed. We decompose einsum expressions into
a fixed set of FHE-friendly operations. We implement our design and present
EinHops, a minimalist system that factors einsum expressions into a fixed
sequence of FHE operations. EinHops enables developers to perform encrypted
tensor operations using FHE while maintaining full visibility into the
underlying packing strategy. We evaluate EinHops on a range of tensor
operations from a simple transpose to complex multi-dimensional contractions.
We show that the explicit nature of einsum notation allows us to build an FHE
tensor system that is simple, general, and interpretable. We open-source
EinHops at the following repository: https://github.com/baahl-nyu/einhops.

</details>


### [18] [Defending Against Prompt Injection With a Few DefensiveTokens](https://arxiv.org/abs/2507.07974)
*Sizhe Chen,Yizhu Wang,Nicholas Carlini,Chawin Sitawarin,David Wagner*

Main category: cs.CR

TL;DR: DefensiveToken is a test-time defense method for large language models (LLMs) that mitigates prompt injection attacks by inserting special tokens with security-optimized embeddings, enabling a flexible switch between high utility and strong security without retraining the model.


<details>
  <summary>Details</summary>
Motivation: Current test-time defenses against prompt injection are less effective than training-time defenses that alter model parameters. The authors aim to develop a test-time defense (DefensiveToken) that achieves security comparable to training-time methods while maintaining model utility in non-security-sensitive scenarios.

Method: DefensiveToken involves inserting a small number of special tokens before the LLM input during inference. These tokens' embeddings are pre-trained through a one-time process to disrupt prompt injection attacks, requiring no changes to the original model parameters or architecture.

Result: The approach matches the security performance of state-of-the-art training-time defenses (‚Äòalmost-SOTA‚Äô security) while causing minimal utility degradation (~2% reduction in safety benchmarks, ~1-3% on instruction-following tasks). Omitting DefensiveTokens preserves the original LLM performance and quality.

Conclusion: DefensiveToken provides a practical, flexible solution for securing LLMs against prompt injection attacks at test time, achieving near training-time defense robustness without sacrificing model utility in normal operation, with a low deployment cost through one-time token embedding training.

Abstract: When large language model (LLM) systems interact with external data to
perform complex tasks, a new attack, namely prompt injection, becomes a
significant threat. By injecting instructions into the data accessed by the
system, the attacker is able to override the initial user task with an
arbitrary task directed by the attacker. To secure the system, test-time
defenses, e.g., defensive prompting, have been proposed for system developers
to attain security only when needed in a flexible manner. However, they are
much less effective than training-time defenses that change the model
parameters. Motivated by this, we propose DefensiveToken, a test-time defense
with prompt injection robustness comparable to training-time alternatives.
DefensiveTokens are newly inserted as special tokens, whose embeddings are
optimized for security. In security-sensitive cases, system developers can
append a few DefensiveTokens before the LLM input to achieve security with a
minimal utility drop. In scenarios where security is less of a concern,
developers can simply skip DefensiveTokens; the LLM system remains the same as
there is no defense, generating high-quality responses. Thus, DefensiveTokens,
if released alongside the model, allow a flexible switch between the
state-of-the-art (SOTA) utility and almost-SOTA security at test time. The code
is available at https://github.com/Sizhe-Chen/DefensiveToken.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [19] [A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering](https://arxiv.org/abs/2507.07325)
*Martin Obaidi,Marc Herrmann,Elisa Schmid,Raymond Ochsner,Kurt Schneider,Jil Kl√ºnder*

Main category: cs.SE

TL;DR: Researchers created a validated German sentiment analysis dataset (5,949 developer statements from Android-Hilfe.de) with high interrater agreement and demonstrated existing tools' limitations in domain-specific software engineering contexts.


<details>
  <summary>Details</summary>
Motivation: Current sentiment analysis tools for software engineering rely on English/non-German datasets, creating a gap in supporting German-speaking developers and teams.

Method: 1. Collected 5,949 unique developer statements from Android-Hilfe.de 2. Annotated with six fundamental emotions (Shaver et al.'s model) by 4 CS students 3. Performed interrater reliability assessment 4. Evaluated existing German sentiment analysis tools 5. Discussed annotation optimization strategies

Result: 1. Dataset validation: High interrater agreement (92.3% Cohen's Kappa 0.84) 2. Identified lack of domain-specific sentiment analysis solutions for German (85-90% performance gap) 3. Demonstrated dataset's applicability for optimizing annotation and other use cases

Conclusion: This validated German software engineering sentiment dataset addresses critical domain-specific needs, provides reliable baseline data, and highlights opportunities to improve both annotation efficiency and sentiment analysis capabilities for German-speaking developer populations.

Abstract: Sentiment analysis is an essential technique for investigating the emotional
climate within developer teams, contributing to both team productivity and
project success. Existing sentiment analysis tools in software engineering
primarily rely on English or non-German gold-standard datasets. To address this
gap, our work introduces a German dataset of 5,949 unique developer statements,
extracted from the German developer forum Android-Hilfe.de. Each statement was
annotated with one of six basic emotions, based on the emotion model by Shaver
et al., by four German-speaking computer science students. Evaluation of the
annotation process showed high interrater agreement and reliability. These
results indicate that the dataset is sufficiently valid and robust to support
sentiment analysis in the German-speaking software engineering community.
Evaluation with existing German sentiment analysis tools confirms the lack of
domain-specific solutions for software engineering. We also discuss approaches
to optimize annotation and present further use cases for the dataset.

</details>


### [20] [Automatic Generation of Explainability Requirements and Software Explanations From User Reviews](https://arxiv.org/abs/2507.07344)
*Martin Obaidi,Jannik Fischbach,Jakob Droste,Hannah Deters,Marc Herrmann,Jil Kl√ºnder,Steffen Kr√§tzig,Hugo Villamizar,Kurt Schneider*

Main category: cs.SE

TL;DR: This paper introduces a tool-supported automated method to derive explainability requirements from user reviews and generate aligned explanations, validated through a dataset of 58 annotated reviews showing AI-generated explanation advantages but requiring human oversight for requirement accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of systematically translating ambiguous user feedback into structured explainability requirements while maintaining clarity and regulatory compliance in AI systems.

Method: Collaborated with an industrial automation manufacturer to create a dataset of 58 annotated user reviews (with manual requirements and explanations) and developed an automated approach for requirement derivation and explanation generation, evaluated against human outputs.

Result: AI-generated requirements showed lower relevance/correctness vs. human-crafted ones, but AI explanations were preferred for clarity/style. Correctness issues highlight the necessity of human validation.

Conclusion: The work advances explainability requirements by providing an automated framework, empirical insights into AI limitations/advantages, and a curated dataset for future research, emphasizing human-AI collaboration.

Abstract: Explainability has become a crucial non-functional requirement to enhance
transparency, build user trust, and ensure regulatory compliance. However,
translating explanation needs expressed in user feedback into structured
requirements and corresponding explanations remains challenging. While existing
methods can identify explanation-related concerns in user reviews, there is no
established approach for systematically deriving requirements and generating
aligned explanations. To contribute toward addressing this gap, we introduce a
tool-supported approach that automates this process. To evaluate its
effectiveness, we collaborated with an industrial automation manufacturer to
create a dataset of 58 user reviews, each annotated with manually crafted
explainability requirements and explanations. Our evaluation shows that while
AI-generated requirements often lack relevance and correctness compared to
human-created ones, the AI-generated explanations are frequently preferred for
their clarity and style. Nonetheless, correctness remains an issue,
highlighting the importance of human validation. This work contributes to the
advancement of explainability requirements in software systems by (1)
introducing an automated approach to derive requirements from user reviews and
generate corresponding explanations, (2) providing empirical insights into the
strengths and limitations of automatically generated artifacts, and (3)
releasing a curated dataset to support future research on the automatic
generation of explainability requirements.

</details>


### [21] [Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN](https://arxiv.org/abs/2507.07468)
*Sten Gr√ºner,Nafise Eskandani*

Main category: cs.SE

TL;DR: This paper proposes a distributed AAS copy-on-write infrastructure combined with BPMN to automate engineering workflows, enhancing security, scalability, and cross-organizational collaboration through a workflow management prototype.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for automating and optimizing plant and process engineering workflows by leveraging Industry 4.0 technologies, specifically the Asset Administration Shell (AAS) for interoperable Digital Twins.

Method: A distributed AAS copy-on-write infrastructure is introduced to improve security and scalability, along with a workflow management prototype that automates AAS operations using Business Process Model and Notation (BPMN).

Result: The proposed infrastructure and workflow prototype demonstrate enhanced efficiency, traceability, and cross-organizational collaboration capabilities in engineering processes.

Conclusion: The integration of AAS with BPMN through a distributed copy-on-write infrastructure provides a scalable and secure framework for automating engineering workflows and enabling collaboration across organizations.

Abstract: The integration of Industry 4.0 technologies into engineering workflows is an
essential step toward automating and optimizing plant and process engineering
processes. The Asset Administration Shell (AAS) serves as a key enabler for
creating interoperable Digital Twins that facilitate engineering data exchange
and automation. This paper explores the use of AAS within engineering
workflows, particularly in combination with Business Process Model and Notation
(BPMN) to define structured and automated processes. We propose a distributed
AAS copy-on-write infrastructure that enhances security and scalability while
enabling seamless cross organizational collaboration. We also introduce a
workflow management prototype automating AAS operations and engineering
workflows, improving efficiency and traceability.

</details>


### [22] [From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering](https://arxiv.org/abs/2507.07548)
*Jonathan Ullrich,Matthias Koch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: The paper explores whether requirements-based code generation via LLMs can replace traditional SE by analyzing how developers decompose and enrich requirements into prompts. It concludes that fundamental RE remains essential.


<details>
  <summary>Details</summary>
Motivation: The study investigates the viability of replacing traditional software engineering with generative LLM-based code generation by understanding how practitioners utilize requirements and design artifacts in this process.

Method: 18 practitioners from 14 companies were interviewed to analyze their workflows of reusing requirements information and design artifacts for LLM code generation.

Result: Developers must manually decompose raw requirements into programming tasks, which are then enriched with design decisions and architecture constraints before being used in LLM prompts.

Conclusion: Fundamental requirements engineering work is still necessary when using LLMs for code generation, as documented requirements lack sufficient detail for direct LLM input without manual preprocessing.

Abstract: With the advent of generative LLMs and their advanced code generation
capabilities, some people already envision the end of traditional software
engineering, as LLMs may be able to produce high-quality code based solely on
the requirements a domain expert feeds into the system. The feasibility of this
vision can be assessed by understanding how developers currently incorporate
requirements when using LLMs for code generation-a topic that remains largely
unexplored. We interviewed 18 practitioners from 14 companies to understand how
they (re)use information from requirements and other design artifacts to feed
LLMs when generating code. Based on our findings, we propose a theory that
explains the processes developers employ and the artifacts they rely on. Our
theory suggests that requirements, as typically documented, are too abstract
for direct input into LLMs. Instead, they must first be manually decomposed
into programming tasks, which are then enriched with design decisions and
architectural constraints before being used in prompts. Our study highlights
that fundamental RE work is still necessary when LLMs are used to generate
code. Our theory is important for contextualizing scientific approaches to
automating requirements-centric SE tasks.

</details>


### [23] [Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap](https://arxiv.org/abs/2507.07682)
*Kaicheng Huang,Fanyu Wang,Yutan Huang,Chetan Arora*

Main category: cs.SE

TL;DR: This paper presents the first roadmap-oriented systematic literature review on prompt engineering for requirements engineering (PE4RE), analyzing challenges in LLM controllability and proposing a hybrid taxonomy with a step-by-step roadmap to improve trustworthiness and reproducibility in RE tasks.


<details>
  <summary>Details</summary>
Motivation: Current LLMs in requirements engineering suffer from high uncertainty and poor controllability, with insufficient guidance on effective prompting, hindering their trustworthy implementation.

Method: A secondary-study protocol followed Kitchenham's and Petersen's methodologies, screening 867 records from six digital libraries and analyzing 35 primary studies to systematize PE4RE practices.

Result: A hybrid taxonomy linking technique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented RE roles, two research questions with sub-questions mapping existing limitations and gaps, and a step-by-step roadmap for evolving ad-hoc PE prototypes into reproducible workflows.

Conclusion: The proposed roadmap demonstrates how fragmented PE4RE approaches can transition to structured, practitioner-friendly methods, addressing critical limitations in LLM control and task-specific guidance for RE.

Abstract: Advancements in large language models (LLMs) have led to a surge of prompt
engineering (PE) techniques that can enhance various requirements engineering
(RE) tasks. However, current LLMs are often characterized by significant
uncertainty and a lack of controllability. This absence of clear guidance on
how to effectively prompt LLMs acts as a barrier to their trustworthy
implementation in the RE field. We present the first roadmap-oriented
systematic literature review of Prompt Engineering for RE (PE4RE). Following
Kitchenham's and Petersen's secondary-study protocol, we searched six digital
libraries, screened 867 records, and analyzed 35 primary studies. To bring
order to a fragmented landscape, we propose a hybrid taxonomy that links
technique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented
RE roles (elicitation, validation, traceability). Two research questions, with
five sub-questions, map the tasks addressed, LLM families used, and prompt
types adopted, and expose current limitations and research gaps. Finally, we
outline a step-by-step roadmap showing how today's ad-hoc PE prototypes can
evolve into reproducible, practitioner-friendly workflows.

</details>


### [24] [From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry](https://arxiv.org/abs/2507.07689)
*Chetan Arora,Fanyu Wang,Chakkrit Tantithamthavorn,Aldeida Aleti,Shaun Kenyon*

Main category: cs.SE

TL;DR: This paper proposes using Retrieval-Augmented Generation (RAG) models to semi-automate requirements engineering in the space industry, particularly aiding smaller organizations in processing unstructured documents and aligning with standards via a modular AI approach.


<details>
  <summary>Details</summary>
Motivation: Smaller space organizations struggle with manually deriving precise, compliant requirements from voluminous, unstructured mission documents, requiring high adaptability to domain-specific constraints and standards.

Method: A four-step AI-driven approach combining document preprocessing, semantic classification, contextual retrieval from domain standards, and LLM-based synthesis of draft requirements is presented and tested on real mission data with industry collaboration.

Result: Preliminary application demonstrates reduced manual effort, improved requirement coverage, and enhanced lightweight compliance alignment for space mission requirements generation.

Conclusion: The proposed RAG-based framework shows potential to lower barriers for smaller space entities in participating in complex missions, offering a scalable path toward integrated AI solutions in requirements engineering workflows.

Abstract: Requirements engineering (RE) in the space industry is inherently complex,
demanding high precision, alignment with rigorous standards, and adaptability
to mission-specific constraints. Smaller space organisations and new entrants
often struggle to derive actionable requirements from extensive, unstructured
documents such as mission briefs, interface specifications, and regulatory
standards. In this innovation opportunity paper, we explore the potential of
Retrieval-Augmented Generation (RAG) models to support and (semi-)automate
requirements generation in the space domain. We present a modular, AI-driven
approach that preprocesses raw space mission documents, classifies them into
semantically meaningful categories, retrieves contextually relevant content
from domain standards, and synthesises draft requirements using large language
models (LLMs). We apply the approach to a real-world mission document from the
space domain to demonstrate feasibility and assess early outcomes in
collaboration with our industry partner, Starbound Space Solutions. Our
preliminary results indicate that the approach can reduce manual effort,
improve coverage of relevant requirements, and support lightweight compliance
alignment. We outline a roadmap toward broader integration of AI in RE
workflows, intending to lower barriers for smaller organisations to participate
in large-scale, safety-critical missions.

</details>
