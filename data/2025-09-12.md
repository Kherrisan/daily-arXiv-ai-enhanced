<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 20]
- [cs.SE](#cs.SE) [Total: 13]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Cross-Service Token: Finding Attacks in 5G Core Networks](https://arxiv.org/abs/2509.08992)
*Anqi Chen,Riccardo Preatoni,Alessandro Brighente,Mauro Conti,Cristina Nita-Rotaru*

Main category: cs.CR

TL;DR: This paper introduces FivGeeFuzz, a fuzzing framework that discovers security flaws in 5G core networks by generating malicious API inputs. It identified 8 critical vulnerabilities in free5GC, highlighting risks like the Cross-Service Token Attack, and demonstrates practical effectiveness in improving 5G security.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to address security risks in 5G's Service-Based Architecture, particularly vulnerabilities in Network Functions (NFs) that could be exploited by insiders or attackers to gain unauthorized access to cloud-based resources through compromised NFs.

Method: The method involves developing a grammar-based fuzzing framework called FivGeeFuzz, which automatically derives grammars from 3GPP API specifications to generate malicious inputs. It combines automated bug detection with manual validation and root-cause analysis to identify vulnerabilities in 5G Service-Based Interfaces.

Result: FivGeeFuzz uncovered 8 novel vulnerabilities in the free5GC open-source 5G core, including a severe Cross-Service Token Attack. These vulnerabilities caused runtime crashes, improper error handling, and unauthorized access, with 7 confirmed fixes and 1 under development by the free5GC team.

Conclusion: The paper concludes that FivGeeFuzz is an effective framework for identifying critical security vulnerabilities in 5G core services, demonstrating its value through the discovery and resolution of 8 previously unknown flaws in the free5GC implementation.

Abstract: 5G marks a major departure from previous cellular architectures, by
transitioning from a monolithic design of the core network to a Service-Based
Architecture (SBA) where services are modularized as Network Functions (NFs)
which communicate with each other via standard-defined HTTP-based APIs called
Service-Based Interfaces (SBIs). These NFs are deployed in private and public
cloud infrastructure, and an access control framework based on OAuth restricts
how they communicate with each other and obtain access to resources. Given the
increased vulnerabilities of clouds to insiders, it is important to study the
security of the 5G Core services for vulnerabilities that allow attackers to
use compromised NFs to obtain unauthorized access to resources.
  We present FivGeeFuzz, a grammar-based fuzzing framework designed to uncover
security flaws in 5G core SBIs. FivGeeFuzz automatically derives grammars from
3GPP API specifications to generate malformed, unexpected, or semantically
inconsistent inputs, and it integrates automated bug detection with manual
validation and root-cause analysis. We evaluate our approach on free5GC, the
only open-source 5G core implementing Release 17-compliant SBIs with an access
control mechanism. Using FivGeeFuzz, we discovered 8 previously unknown
vulnerabilities in free5GC, leading to runtime crashes, improper error
handling, and unauthorized access to resources, including a very severe attack
we call Cross-Service Token Attack. All bugs were confirmed by the free5GC
team, 7 have already been patched, and the remaining one has a patch under
development.

</details>


### [2] [When FinTech Meets Privacy: Securing Financial LLMs with Differential Private Fine-Tuning](https://arxiv.org/abs/2509.08995)
*Sichen Zhu,Hoyeung Leung,Xiaoyi Wang,Jia Wei,Honghui Xu*

Main category: cs.CR

TL;DR: DPFinLLM: A privacy-enhanced, lightweight large language model for on-device financial applications.


<details>
  <summary>Details</summary>
Motivation: The deployment of AI models on edge devices for financial applications presents significant privacy challenges for sensitive financial data. This research aims to overcome these challenges by integrating differential privacy within a LLM that is suitable for on-device use.

Method: DPFinLLM combines a robust differential privacy mechanism with a streamlined LLM architecture, optimized for on-device financial tasks. This model is trained with privacy guarantees, allowing secure inference on sensitive financial data.

Result: dpFinLLM demonstrates performance on par with fully fine-tuned models across various financial sentiment datasets, even when stringent privacy constraints are applied. This indicates its effectiveness in preserving privacy without substantially compromising performance.

Conclusion: DpFinLLM offers a promising solution for the secure deployment of AI in financial applications on edge devices, ensuring both privacy protection and high performance.

Abstract: The integration of Large Language Models (LLMs) into financial technology
(FinTech) has revolutionized the analysis and processing of complex financial
data, driving advancements in real-time decision-making and analytics. With the
growing trend of deploying AI models on edge devices for financial
applications, ensuring the privacy of sensitive financial data has become a
significant challenge. To address this, we propose DPFinLLM, a
privacy-enhanced, lightweight LLM specifically designed for on-device financial
applications. DPFinLLM combines a robust differential privacy mechanism with a
streamlined architecture inspired by state-of-the-art models, enabling secure
and efficient processing of financial data. This proposed DPFinLLM can not only
safeguard user data from privacy breaches but also ensure high performance
across diverse financial tasks. Extensive experiments on multiple financial
sentiment datasets validate the effectiveness of DPFinLLM, demonstrating its
ability to achieve performance comparable to fully fine-tuned models, even
under strict privacy constraints.

</details>


### [3] [Beyond Tag Collision: Cluster-based Memory Management for Tag-based Sanitizers](https://arxiv.org/abs/2509.09089)
*Mengfei Xie,Yan Lin,Hongtao Wu,Jianming Fu,Chenke Luo,Guojun Peng*

Main category: cs.CR

TL;DR: ClusterTag is a cluster-based memory allocator that addresses tag collisions in tag-based sanitizers by partitioning memory into clusters with randomized heap layouts, achieving deterministic memory violation detection with minimal performance overhead.


<details>
  <summary>Details</summary>
Motivation: Tag-based sanitizers suffer from limited tag encoding space, causing temporal and spatial tag collisions that lead to false negatives in detecting memory violations.

Method: ClusterTag divides memory objects into independent clusters with local tag allocation to confine collisions, and employs cluster-grained heap randomization to break entropy limitations by introducing random address intervals between clusters.

Result: ClusterTag integrates with HWASan, maintains <1â€‹% performance overhead, achieves deterministic detection on Juliet dataset (5,652 reported/1,530 missed bugs), and outperforms existing tag strategies in tag collision metrics (min, average, unpredictability).

Conclusion: ClusterTag provides balanced improvements in tag collision prevention across all metrics, eliminates probabilistic false negatives through deterministic allocation, and maintains compatibility with existing tag-based sanitizers with negligible performance impact.

Abstract: Tag-based sanitizers attach a small "key" to each pointer and a matching
"lock" tag to its target memory object, enabling runtime verification of
pointer-object consistency and helping developers to detect potential memory
violations. However, the limited tag encoding space challenges existing studies
in assigning distinct tags to memory objects across temporal and spatial
dimensions, leading to potential tag collisions. In this paper, we present
ClusterTag, a novel cluster-based memory allocator aimed at simultaneously
mitigating tag collisions in both temporal and spatial dimensions. The core
design of ClusterTag effectively balances the significant mismatch between tag
encoding space and memory objects: it divides memory objects into multiple
independent clusters, thereby limiting tag collisions to finite chunks within
each cluster. To mitigate tag collisions across clusters, we design a
cluster-grained heap randomization scheme. This approach introduces random
address intervals between clusters and further breaks the entropy limitation of
the tag space. ClusterTag has been implemented as an independent memory
allocator that seamlessly integrates with tag-based sanitizers such as HWASan,
and maintains comparable performance overhead (within 1%) at various
randomization densities. Security evaluations on the Juliet dataset indicate
that ClusterTag exhibits deterministic results across 500 repeated tests (5,652
reported and 1,530 missed), while the existing three types of tag assignment
strategies all exhibit probabilistic false negatives due to tag collisions.
Quantitative analysis across three tag collision distance metrics-minimum,
average, and unpredictability-demonstrates that ClusterTag achieves balanced
improvements across all three, whereas prior tag assignment schemes (random,
staggered, fixed) show significant trade-offs in at least one metric.

</details>


### [4] [Towards Confidential and Efficient LLM Inference with Dual Privacy Protection](https://arxiv.org/abs/2509.09091)
*Honglan Yu,Yibin Wang,Feifei Dai,Dong Liu,Haihui Fan,Xiaoyan Gu*

Main category: cs.CR

TL;DR: CMIF addresses privacy and efficiency challenges in large language model inference by combining TEEs and GPUs with optimized DP mechanisms.


<details>
  <summary>Details</summary>
Motivation: CPU-based TEEs cause high latency, GPU offloading of nonlinear layers incurs communication overhead, and DP approaches harm model performance.

Method: Deploys embedding layer in client-side TEE and subsequent layers on GPU servers while optimizing the Report-Noisy-Max mechanism for privacy-preserving inference.

Result: Experiments on Llama-series models show reduced TEE overhead and maintained privacy with minimal model performance degradation.

Conclusion: CMIF demonstrates effective balance between inference efficiency, data confidentiality, and model utility for secure LLM deployment.

Abstract: CPU-based trusted execution environments (TEEs) and differential privacy (DP)
have gained wide applications for private inference. Due to high inference
latency in TEEs, researchers use partition-based approaches that offload linear
model components to GPUs. However, dense nonlinear layers of large language
models (LLMs) result in significant communication overhead between TEEs and
GPUs. DP-based approaches apply random noise to protect data privacy, but this
compromises LLM performance and semantic understanding. To overcome the above
drawbacks, this paper proposes CMIF, a Confidential and efficient Model
Inference Framework. CMIF confidentially deploys the embedding layer in the
client-side TEE and subsequent layers on GPU servers. Meanwhile, it optimizes
the Report-Noisy-Max mechanism to protect sensitive inputs with a slight
decrease in model performance. Extensive experiments on Llama-series models
demonstrate that CMIF reduces additional inference overhead in TEEs while
preserving user data privacy.

</details>


### [5] [DP-FedLoRA: Privacy-Enhanced Federated Fine-Tuning for On-Device Large Language Models](https://arxiv.org/abs/2509.09097)
*Honghui Xu,Shiva Shrestha,Wei Chen,Zhiyuan Li,Zhipeng Cai*

Main category: cs.CR

TL;DR: DP-FedLoRA is a communication-efficient, privacy-enhanced federated fine-tuning framework for on-device LLMs using LoRA and differential privacy.


<details>
  <summary>Details</summary>
Motivation: Federated fine-tuning of on-device LLMs processes sensitive user data, creating privacy risks in federated learning environments.

Method: The framework employs LoRA-based adaptation with Gaussian-noise perturbation and gradient clipping on client devices. Theoretical analysis ensures unbiased updates and variance bounds for privacy calibration.

Result: DP-FedLoRA achieves competitive performance on benchmarks while providing strong ($\epsilon, \delta$)-differential privacy guarantees, demonstrating practical effectiveness.

Conclusion: DP-FedLoRA addresses privacy challenges in federated LLM fine-tuning by integrating differential privacy with LoRA adaptation, enabling scalable and privacy-preserving on-device deployment.

Abstract: As on-device large language model (LLM) systems become increasingly
prevalent, federated fine-tuning enables advanced language understanding and
generation directly on edge devices; however, it also involves processing
sensitive, user-specific data, raising significant privacy concerns within the
federated learning framework. To address these challenges, we propose
DP-FedLoRA, a privacy-enhanced federated fine-tuning framework that integrates
LoRA-based adaptation with differential privacy in a communication-efficient
setting. Each client locally clips and perturbs its LoRA matrices using
Gaussian noise to satisfy ($\epsilon$, $\delta$)-differential privacy. We
further provide a theoretical analysis demonstrating the unbiased nature of the
updates and deriving bounds on the variance introduced by noise, offering
practical guidance for privacy-budget calibration. Experimental results across
mainstream benchmarks show that DP-FedLoRA delivers competitive performance
while offering strong privacy guarantees, paving the way for scalable and
privacy-preserving LLM deployment in on-device environments.

</details>


### [6] [AgriSentinel: Privacy-Enhanced Embedded-LLM Crop Disease Alerting System](https://arxiv.org/abs/2509.09103)
*Chanti Raju Mylay,Bobin Deng,Zhipeng Cai,Honghui Xu*

Main category: cs.CR

TL;DR: First privacy-enhanced embedded-LLM crop disease system combining differential privacy, lightweight deep learning, and on-device domain knowledge for actionable alerts and market protection.


<details>
  <summary>Details</summary>
Motivation: Current crop disease alerting systems neglect data privacy, market fairness, and usability for farmers, creating vulnerability to privacy breaches and economic exploitation. Novel solutions must address these interdependent challenges.

Method: The system integrates differential privacy for data protection, a lightweight deep learning model for mobile optimization, and an on-device LLM with curated domain knowledge to deliver actionable disease management strategies.

Result: Experiments demonstrate AgriSentinel achieves strong data privacy guarantees without sacrificing classification accuracy, while its LLM integration outperforms baselines in generating actionable, crop-specific management recommendations.

Conclusion: AgriSentinel offers a robust, farmer-friendly framework for privacy-preserving crop disease detection and management, addressing critical gaps in existing systems to enhance agricultural productivity and decision-making.

Abstract: Crop diseases pose significant threats to global food security, agricultural
productivity, and sustainable farming practices, directly affecting farmers'
livelihoods and economic stability. To address the growing need for effective
crop disease management, AI-based disease alerting systems have emerged as
promising tools by providing early detection and actionable insights for timely
intervention. However, existing systems often overlook critical aspects such as
data privacy, market pricing power, and farmer-friendly usability, leaving
farmers vulnerable to privacy breaches and economic exploitation. To bridge
these gaps, we propose AgriSentinel, the first Privacy-Enhanced Embedded-LLM
Crop Disease Alerting System. AgriSentinel incorporates a differential privacy
mechanism to protect sensitive crop image data while maintaining classification
accuracy. Its lightweight deep learning-based crop disease classification model
is optimized for mobile devices, ensuring accessibility and usability for
farmers. Additionally, the system includes a fine-tuned, on-device large
language model (LLM) that leverages a curated knowledge pool to provide farmers
with specific, actionable suggestions for managing crop diseases, going beyond
simple alerting. Comprehensive experiments validate the effectiveness of
AgriSentinel, demonstrating its ability to safeguard data privacy, maintain
high classification performance, and deliver practical, actionable disease
management strategies. AgriSentinel offers a robust, farmer-friendly solution
for automating crop disease alerting and management, ultimately contributing to
improved agricultural decision-making and enhanced crop productivity.

</details>


### [7] [CryptGNN: Enabling Secure Inference for Graph Neural Networks](https://arxiv.org/abs/2509.09107)
*Pritam Sen,Yao Ma,Cristian Borcea*

Main category: cs.CR

TL;DR: ASecureGNNInference Solution for Cloud-Based MLaaS: CryptGNN uses distributed SMPC to protect both client data and model parameters from the cloud provider and model owner.


<details>
  <summary>Details</summary>
Motivation: Clients often wish to perform GNN inference on sensitive data via third-party cloud models without revealing data or model secrets, leading to privacy and security concerns.

Method: CryptGNN introduces secure message passing and feature transformation layers leveraging distributed SMPC techniques, ensuring no party learns private information.

Result: Theoretical analysis shows security against P-1-out-of-P collusion, and experiments confirm efficiency. CryptGNN supports any SMPC party count with no trusted servers.

Conclusion: CryptGNN effectively addresses privacy and security challenges in cloud-based GNN inference for MLaaS without compromising performance.

Abstract: We present CryptGNN, a secure and effective inference solution for
third-party graph neural network (GNN) models in the cloud, which are accessed
by clients as ML as a service (MLaaS). The main novelty of CryptGNN is its
secure message passing and feature transformation layers using distributed
secure multi-party computation (SMPC) techniques. CryptGNN protects the
client's input data and graph structure from the cloud provider and the
third-party model owner, and it protects the model parameters from the cloud
provider and the clients. CryptGNN works with any number of SMPC parties, does
not require a trusted server, and is provably secure even if P-1 out of P
parties in the cloud collude. Theoretical analysis and empirical experiments
demonstrate the security and efficiency of CryptGNN.

</details>


### [8] [Character-Level Perturbations Disrupt LLM Watermarks](https://arxiv.org/abs/2509.09112)
*Zhaoxi Zhang,Xiaomei Zhang,Yanjun Zhang,He Zhang,Shirui Pan,Bo Liu,Asif Qumer Gill,Leo Yu Zhang*

Main category: cs.CR

TL;DR: This paper analyzes vulnerabilities in LLM watermarking, introduces character-level perturbations and Genetic Algorithm (GA)-based attacks, and reveals inherent adversarial dilemmas in existing defense mechanisms.


<details>
  <summary>Details</summary>
Motivation: Prior watermark removal methods require excessive perturbations or strong adversaries, creating a misguided perception of security. The study aims to uncover more realistic, subtle attack strategies to expose weaknesses in current LLM watermarking systems.

Method: 1) Formalizes system models and threat scenarios with limited detector access
2Analyzes perturbation effectiveness via tokenization disruption
3) Introduces GA-guided removal attacks under query constraints
4Proposes adaptive compound character-level attacks for defensive circumvention

Result: Character-level attacks outperform existing methods under strict threat models; GA-based approach shows robust removal with limited queries; Adaptive attacks successfully defeat various defenses, confirming vulnerabilities in current watermarking schemes.

Conclusion: Current LLM watermarking systems have critical vulnerabilities to low-effort, character-level attacks. The adversarial dilemma between fixed defenses and adaptable attacks necessitates development of fundamentally more robust watermarking mechanisms.

Abstract: Large Language Model (LLM) watermarking embeds detectable signals into
generated text for copyright protection, misuse prevention, and content
detection. While prior studies evaluate robustness using watermark removal
attacks, these methods are often suboptimal, creating the misconception that
effective removal requires large perturbations or powerful adversaries.
  To bridge the gap, we first formalize the system model for LLM watermark, and
characterize two realistic threat models constrained on limited access to the
watermark detector. We then analyze how different types of perturbation vary in
their attack range, i.e., the number of tokens they can affect with a single
edit. We observe that character-level perturbations (e.g., typos, swaps,
deletions, homoglyphs) can influence multiple tokens simultaneously by
disrupting the tokenization process. We demonstrate that character-level
perturbations are significantly more effective for watermark removal under the
most restrictive threat model. We further propose guided removal attacks based
on the Genetic Algorithm (GA) that uses a reference detector for optimization.
Under a practical threat model with limited black-box queries to the watermark
detector, our method demonstrates strong removal performance. Experiments
confirm the superiority of character-level perturbations and the effectiveness
of the GA in removing watermarks under realistic constraints. Additionally, we
argue there is an adversarial dilemma when considering potential defenses: any
fixed defense can be bypassed by a suitable perturbation strategy. Motivated by
this principle, we propose an adaptive compound character-level attack.
Experimental results show that this approach can effectively defeat the
defenses. Our findings highlight significant vulnerabilities in existing LLM
watermark schemes and underline the urgency for the development of new robust
mechanisms.

</details>


### [9] [IoTFuzzSentry: A Protocol Guided Mutation Based Fuzzer for Automatic Vulnerability Testing in Commercial IoT Devices](https://arxiv.org/abs/2509.09158)
*Priyanka Rushikesh Chaudhary,Rajib Ranjan Maiti*

Main category: cs.CR

TL;DR: This paper introduces IoTFuzzSentry, a mutation-based protocol fuzzing tool for identifying non-trivial vulnerabilities in commercial IoT devices. It uncovers four vulnerability categories (credentail leakage, video/image stream exploitation, command injection) and demonstrates real-world exploits on multiple devices.


<details>
  <summary>Details</summary>
Motivation: IoT devices using lightweight servers for user interaction are vulnerable to security flaws in transport/application-layer implementations, enabling threats like unauthorized access and data leakage. Traditional security assessments lack scalability for the rapidly growing IoT ecosystem.

Method: The authors developed IoTFuzzSentry, a mutation-based fuzzing tool integrated with Cotopaxi, to inject crafted packets into IoT communication protocols. The tool analyzes traffic patterns of commercial IoT devices (IP cameras, Smart Plug) to identify vulnerabilities through protocol-level injection of malformed packets.

Result: 1. Discovery of 4 vulnerability categories through evaluation of commercial IoT devices
2. Identification of 3 new exploitable vulnerabilities (2 CVEs issued, 1 pending)
3. Demonstration of successful exploitation across three device types
4. Detection of similar vulnerability patterns in additional 6 IoT devices due to shared protocol implementations

Conclusion: IoTFuzzSentry proves effective in uncovering unconventional security threats in IoT devices. The mutation-based protocol fuzzing approach provides a scalable, low-overhead solution for automated vulnerability detection, enabling vendors to strengthen security in commercial IoT products through automated testing.

Abstract: Protocol fuzzing is a scalable and cost-effective technique for identifying
security vulnerabilities in deployed Internet of Things devices. During their
operational phase, IoT devices often run lightweight servers to handle user
interactions, such as video streaming or image capture in smart cameras.
Implementation flaws in transport or application-layer security mechanisms can
expose IoT devices to a range of threats, including unauthorized access and
data leakage. This paper addresses the challenge of uncovering such
vulnerabilities by leveraging protocol fuzzing techniques that inject crafted
transport and application-layer packets into IoT communications. We present a
mutation-based fuzzing tool, named IoTFuzzSentry, to identify specific
non-trivial vulnerabilities in commercial IoT devices. We further demonstrate
how these vulnerabilities can be exploited in real-world scenarios. We
integrated our fuzzing tool into a well-known testing tool Cotopaxi and
evaluated it with commercial-off-the-shelf IoT devices such as IP cameras and
Smart Plug. Our evaluation revealed vulnerabilities categorized into 4 types
(IoT Access Credential Leakage, Sneak IoT Live Video Stream, Creep IoT Live
Image, IoT Command Injection) and we show their exploits using three IoT
devices. We have responsibly disclosed all these vulnerabilities to the
respective vendors. So far, we have published two CVEs, CVE-2024-41623 and
CVE-2024-42531, and one is awaiting. To extend the applicability, we have
investigated the traffic of six additional IoT devices and our analysis shows
that these devices can have similar vulnerabilities, due to the presence of a
similar set of application protocols. We believe that IoTFuzzSentry has the
potential to discover unconventional security threats and allow IoT vendors to
strengthen the security of their commercialized IoT devices automatically with
negligible overhead.

</details>


### [10] [Enhancing Cyber Threat Hunting -- A Visual Approach with the Forensic Visualization Toolkit](https://arxiv.org/abs/2509.09185)
*Jihane Najar,Marinos Tsantekidis,Aris Sotiropoulos,Vassilis Prevelakis*

Main category: cs.CR

TL;DR: The paper introduces the Forensic Visualization Toolkit (FVT), a digital forensics tool for cybersecurity threat detection, which enhances situational awareness through advanced visualizations and real-world application.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the growing need for proactive threat hunting in cybersecurity, as traditional methods fail to detect advanced threats in dynamic environments.

Method: The authors present the Forensic Visualization Toolkit, designed for digital evidence analysis, interactive visualizations, and integration into EU-funded research projects to refine its capabilities.

Result: FVT enables cybersecurity professionals to identify, analyze, and respond to threats more effectively, with demonstrated success in practical scenarios and ongoing enhancements through collaborative research.

Conclusion: The Forensic Visualization Toolkit is positioned as a transformative tool for cybersecurity, offering intuitive threat analysis and risk management while validating its impact through sustained academic and industry collaboration.

Abstract: In today's dynamic cyber threat landscape, organizations must take proactive
steps to bolster their cybersecurity defenses. Cyber threat hunting is a
proactive and iterative process aimed at identifying and mitigating advanced
threats that may go undetected by traditional security measures. Rather than
waiting for automated security systems to flag potential threats, threat
hunting involves actively searching for signs of malicious activity within an
organization's network. In this paper, we present the Forensic Visualization
Toolkit, a powerful tool designed for digital forensics investigations,
analysis of digital evidence, and advanced visualizations to enhance
cybersecurity situational awareness and risk management and empower security
analysts with an intuitive and interactive tool. Through practical, real-world
scenarios, we demonstrate how FVT significantly amplifies the capabilities of
cybersecurity professionals, enabling them to effectively identify, analyze,
and respond to threats. Furthermore, it is important to highlight that FVT has
been integrated into, utilized, and continually enhanced within various
EU-funded research projects over recent years.

</details>


### [11] [Shell or Nothing: Real-World Benchmarks and Memory-Activated Agents for Automated Penetration Testing](https://arxiv.org/abs/2509.09207)
*Wuyuao Mai,Geng Hong,Qi Liu,Jinsong Chen,Jiarun Dai,Xudong Pan,Yuan Zhang,Min Yang*

Main category: cs.CR

TL;DR: This paper introduces TermiBench, the first real-world agent-oriented pentesting benchmark focusing on full system control, and TermiAgent, a multi-agent framework with memory activation and structured code analysis. Both significantly improve AI-driven pentesting capabilities.


<details>
  <summary>Details</summary>
Motivation: Traditional pentesting is costly/expert-dependent, while existing AI agents use oversimplified CTF environments. Real-world evaluation gaps hinder progress in autonomous security testing.

Method: 1. TermiBench benchmark with 510 hosts/25 services/30 CVEs requiring system control. 2. TermiAgent framework featuring: - Located Memory Activation (mitigates long-context forgetting) - Structured code understanding for exploit development instead of naive retrieval methods.

Result: Existing agents fail to obtain system shells in realistic scenarios. TermiAgent outperforms SOTA agents with: - 32.7% faster execution - 48.3% lower cost - Practical deployment on laptop-scale systems - System shell success rate 24.5pp higher than baselines

Conclusion: TermiBench establishes the first open-source benchmark for real-world autonomous pentesting while TermiAgent demonstrates the viability of AI-driven security analysis through memory retention and structured exploit development techniques.

Abstract: Penetration testing is critical for identifying and mitigating security
vulnerabilities, yet traditional approaches remain expensive, time-consuming,
and dependent on expert human labor. Recent work has explored AI-driven
pentesting agents, but their evaluation relies on oversimplified
capture-the-flag (CTF) settings that embed prior knowledge and reduce
complexity, leading to performance estimates far from real-world practice. We
close this gap by introducing the first real-world, agent-oriented pentesting
benchmark, TermiBench, which shifts the goal from 'flag finding' to achieving
full system control. The benchmark spans 510 hosts across 25 services and 30
CVEs, with realistic environments that require autonomous reconnaissance,
discrimination between benign and exploitable services, and robust exploit
execution. Using this benchmark, we find that existing systems can hardly
obtain system shells under realistic conditions.
  To address these challenges, we propose TermiAgent, a multi-agent penetration
testing framework. TermiAgent mitigates long-context forgetting with a Located
Memory Activation mechanism and builds a reliable exploit arsenal via
structured code understanding rather than naive retrieval. In evaluations, our
work outperforms state-of-the-art agents, exhibiting stronger penetration
testing capability, reducing execution time and financial cost, and
demonstrating practicality even on laptop-scale deployments. Our work delivers
both the first open-source benchmark for real-world autonomous pentesting and a
novel agent framework that establishes a milestone for AI-driven penetration
testing.

</details>


### [12] [A Cyber-Twin Based Honeypot for Gathering Threat Intelligence](https://arxiv.org/abs/2509.09222)
*Muhammad Azmi Umer,Zhan Xuna,Yan Lin Aung,Aditya P. Mathur,Jianying Zhou*

Main category: cs.CR

TL;DR: This paper introduces a cyber-twin-based honeypot for water treatment plants to capture and analyze real cyberattacks, sharing insights to improve infrastructure security.


<details>
  <summary>Details</summary>
Motivation: Critical Infrastructure (CI) systems, including water treatment plants, face significant cyber threats. Traditional protective measures may lack the depth to predict or analyze emerging attacks. This work aims to address gaps in CI cybersecurity by creating a dynamic honeypot that provides real-time threat data and insights to strengthen defenses.

Method: The authors developed an operational honeypot using a cyber twin to replicate a water treatment plant, designed to attract attackers and log their activities for threat intelligence analysis. The honeypot's realistic setup allows for the collection of actionable data on attacks, which is then shared with management for security enhancements.

Result: The honeypot has successfully drawn multiple attacks, including a documented ransomware incident. The recorded attacks provide detailed threat intelligence that can inform plant operators of potential vulnerabilities and attack patterns, directly aiding in the reinforcement of their security protocols.

Conclusion: The paper concludes that the honeypot based on a cyber twin effectively gathers threat intelligence by emulating a water treatment plant, enabling proactive improvements in CI protection systems. Real-world attacks, such as ransomware, validate its practical utility for enhancing infrastructure security.

Abstract: Critical Infrastructure (CI) is prone to cyberattacks. Several techniques
have been developed to protect CI against such attacks. In this work, we
describe a honeypot based on a cyber twin for a water treatment plant. The
honeypot is intended to serve as a realistic replica of a water treatment plant
that attracts potential attackers. The attacks launched on the honeypot are
recorded and analyzed for threat intelligence. The intelligence so obtained is
shared with the management of water treatment plants, who in turn may use it to
improve plant protection systems. The honeypot used here is operational and has
been attacked on several occasions using, for example, a ransomware attack that
is described in detail.

</details>


### [13] [What You Code Is What We Prove: Translating BLE App Logic into Formal Models with LLMs for Vulnerability Detection](https://arxiv.org/abs/2509.09291)
*Biwei Yan,Yue Zhang,Minghui Xu,Runyu Pan,Jinku Li,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: This paper proposes VerifiaBLE, a system using large language models (LLMs) to bridge real-world BLE code with formal verification tools, enabling scalable security analysis of encryption, randomness, and authentication in 1,050 Android apps.


<details>
  <summary>Details</summary>
Motivation: Manual formal verification of BLE security is impractical for large-scale analysis due to high effort, and BLE apps lack critical security features like encryption/authentications. Automated methods are needed to uncover systemic weaknesses.

Method: Reframes BLE security analysis as a semantic translation problem: (1)Leverages LLMs as code-to-process-model translators, (2)pairs with static analysis and symbolic verification (ProVerif) to validate three security properties. Implements the pipeline in VerifiaBLE.

Result: 53.9% of 1,050 Android BLE apps lacked all three security protections; only 10.29% implemented encryption, randomness and authentication together. Demonstrates LLM-based translation enables scalable formal verification.

Conclusion: Using LLMs as translators for formal verification significantly lowers the barrier to security-critical analysis. This approach enables large-scale validation of security properties inBLE applications, revealing critical implementation gaps.

Abstract: The application layer of Bluetooth Low Energy (BLE) is a growing source of
security vulnerabilities, as developers often neglect to implement critical
protections such as encryption, authentication, and freshness. While formal
verification offers a principled way to check these properties, the manual
effort of constructing formal models makes it impractical for large-scale
analysis. This paper introduces a key insight: BLE application security
analysis can be reframed as a semantic translation problem, i.e., from
real-world code to formal models. We leverage large language models (LLMs) not
to directly detect vulnerabilities, but to serve as translators that convert
BLE-specific code into process models verifiable by tools like ProVerif. We
implement this idea in VerifiaBLE, a system that combines static analysis,
prompt-guided LLM translation, and symbolic verification to check three core
security features: encryption, randomness, and authentication. Applied to 1,050
Android BLE apps, VerifiaBLE uncovers systemic weaknesses: only 10.2\% of apps
implement all three protections, while 53.9\% omit them entirely. Our work
demonstrates that using LLMs as structured translators can lower the barrier to
formal methods, unlocking scalable verification across security-critical
domains.

</details>


### [14] [On the Security of SSH Client Signatures](https://arxiv.org/abs/2509.09331)
*Fabian BÃ¤umer,Marcus Brinkmann,Maximilian Radoy,JÃ¶rg Schwenk,Juraj Somorovsky*

Main category: cs.CR

TL;DR: Analyzed 31.6M SSH client keys, found 98 broken, 139 weak, and a critical PuTTY ECDSA flaw (CVE-2024-31497). Shift to EdDSA observed.


<details>
  <summary>Details</summary>
Motivation: Current SSH security analysis focuses on servers via internet scans, but client-side keys and signatures remain unmeasured. This work addresses this gap to assess risks in widely used client tools.

Method: Authors collected SSH client public keys from GitHub, GitLab, and other platforms through two longitudinal scans. They performed security tests on these keys and conducted black-box experiments on 24 popular SSH clients to analyze algorithm implementations.

Result: 31.6 million keys were analyzed, showing a shift from RSA to EdDSA. Weaknesses included 98 broken short keys, 139 with weak randomness, and 149 with shared factors. A critical vulnerability in PuTTYâ€™s ECDSA implementation (CVE-2024-31497) allowed recovery of private keys from signatures.

Conclusion: The study highlights the importance of SSH client-side security, revealing ongoing vulnerabilities despite a trend toward more secure algorithms. Findings emphasize the need for rigorous randomness in cryptographic implementations and proactive monitoring of client-side SSH practices.

Abstract: Administrators and developers use SSH client keys and signatures for
authentication, for example, to access internet backbone servers or to commit
new code on platforms like GitHub. However, unlike servers, SSH clients cannot
be measured through internet scans. We close this gap in two steps. First, we
collect SSH client public keys. Such keys are regularly published by their
owners on open development platforms like GitHub and GitLab. We systematize
previous non-academic work by subjecting these keys to various security tests
in a longitudinal study. Second, in a series of black-box lab experiments, we
analyze the implementations of algorithms for SSH client signatures in 24
popular SSH clients for Linux, Windows, and macOS.
  We extracted 31,622,338 keys from three public sources in two scans. Compared
to previous work, we see a clear tendency to abandon RSA signatures in favor of
EdDSA signatures. Still, in January 2025, we found 98 broken short keys, 139
keys generated from weak randomness, and 149 keys with common or small
factors-the large majority of the retrieved keys exposed no weakness.
  Weak randomness can not only compromise a secret key through its public key,
but also through signatures. It is well-known that a bias in random nonces in
ECDSA can reveal the secret key through public signatures. For the first time,
we show that the use of deterministic nonces in ECDSA can also be dangerous:
The private signing key of a PuTTY client can be recovered from just 58 valid
signatures if ECDSA with NIST curve P-521 is used. PuTTY acknowledged our
finding in CVE-2024-31497, and they subsequently replaced the nonce generation
algorithm.

</details>


### [15] [[Extended] Ethics in Computer Security Research: A Data-Driven Assessment of the Past, the Present, and the Possible Future](https://arxiv.org/abs/2509.09351)
*Harshini Sri Ramulu,Helen Schmitt,Bogdan Rerich,Rachel Gonzalez Rodriguez,Tadayoshi Kohno,Yasemin Acar*

Main category: cs.CR

TL;DR: This study highlights inconsistent ethics reporting and decision-making in computer security research, identifies areas for improvement, and suggests frameworks to enhance ethical standards based on a review of 2024 top-tier papers and interviews with 24 researchers.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of clear ethical guidance for computer security researchers, particularly in scenarios where ethical clarity is ambiguous, despite frequent discussions on ethical questions.

Method: The study involved a comprehensive review of 1154 top-tier security papers from 2024 and semi-structured interviews with 24 security and privacy researchers to examine ethical decision-making practices and reporting standards.

Result: Analysis revealed inconsistent ethics reporting in papers, emphasizing institutional approvals and human protections while neglecting harm-benefit balance. Interviews highlighted a desire for ethical research but noted inconsistencies in values, frameworks, and decision outcomes.

Conclusion: The paper concludes by presenting an overview of the current state of ethics discussion in computer security research and offers suggestions to enhance ethical practices in the field.

Abstract: Ethical questions are discussed regularly in computer security. Still,
researchers in computer security lack clear guidance on how to make, document,
and assess ethical decisions in research when what is morally right or
acceptable is not clear-cut. In this work, we give an overview of the
discussion of ethical implications in current published work in computer
security by reviewing all 1154 top-tier security papers published in 2024,
finding inconsistent levels of ethics reporting with a strong focus of
reporting institutional or ethics board approval, human subjects protection,
and responsible disclosure, and a lack of discussion of balancing harms and
benefits. We further report on the results of a semi-structured interview study
with 24 computer security and privacy researchers (among whom were also:
reviewers, ethics committee members, and/or program chairs) and their ethical
decision-making both as authors and during peer review, finding a strong desire
for ethical research, but a lack of consistency in considered values, ethical
frameworks (if articulated), decision-making, and outcomes. We present an
overview of the current state of the discussion of ethics and current de-facto
standards in computer security research, and contribute suggestions to improve
the state of ethics in computer security research.

</details>


### [16] [ENSI: Efficient Non-Interactive Secure Inference for Large Language Models](https://arxiv.org/abs/2509.09424)
*Zhiyu He,Maojiang Wang,Xinwen Gao,Yuchuan Luo,Lin Liu,Shaojing Fu*

Main category: cs.CR

TL;DR: ENSI is a novel non-interactive secure inference framework for encrypted large language models (LLMs), reducing computational complexity and bootstrapping frequency through co-design of cryptographic protocols and LLM architecture.


<details>
  <summary>Details</summary>
Motivation: Integrating cryptographic protocols with LLMs for secure inference faces usability challenges due to LLMsâ€™ massive scale and complexity, hindering practical deployment.

Method: ENSI co-designs CKKS encryption with BitNet (a lightweight LLM), optimizes encrypted matrix multiplications via encoding strategies, replaces HE softmax with sigmoid attention (no retraining), and embeds Bootstrapping within RMSNorm to reduce costly operations.

Result: 8x faster matrix multiplications and 2.6x faster softmax inference on CPU vs. existing methods, with 198 cryptocurrency options, including more than 50 stablecoins like Tether, USDC, and Binance USD. You can buy with your regional currency and convert it to Bitcoin, Ethereum, and other famous tokens at this exchange.

Conclusion: ENSI demonstrates that co-designing cryptographic frameworks and LLM architecture can dramatically improve secure inference performance, making privacy-preserving LLM deployment more viable in practice.

Abstract: Secure inference enables privacy-preserving machine learning by leveraging
cryptographic protocols that support computations on sensitive user data
without exposing it. However, integrating cryptographic protocols with large
language models (LLMs) presents significant challenges, as the inherent
complexity of these protocols, together with LLMs' massive parameter scale and
sophisticated architectures, severely limits practical usability. In this work,
we propose ENSI, a novel non-interactive secure inference framework for LLMs,
based on the principle of co-designing the cryptographic protocols and LLM
architecture. ENSI employs an optimized encoding strategy that seamlessly
integrates CKKS scheme with a lightweight LLM variant, BitNet, significantly
reducing the computational complexity of encrypted matrix multiplications. In
response to the prohibitive computational demands of softmax under homomorphic
encryption (HE), we pioneer the integration of the sigmoid attention mechanism
with HE as a seamless, retraining-free alternative. Furthermore, by embedding
the Bootstrapping operation within the RMSNorm process, we efficiently refresh
ciphertexts while markedly decreasing the frequency of costly bootstrapping
invocations. Experimental evaluations demonstrate that ENSI achieves
approximately an 8x acceleration in matrix multiplications and a 2.6x speedup
in softmax inference on CPU compared to state-of-the-art method, with the
proportion of bootstrapping is reduced to just 1%.

</details>


### [17] [Prompt Pirates Need a Map: Stealing Seeds helps Stealing Prompts](https://arxiv.org/abs/2509.09488)
*Felix MÃ¤chtle,Ashwath Shetty,Jonas Sander,Nils Loose,SÃ¶ren Pirk,Thomas Eisenbarth*

Main category: cs.CR

TL;DR: This paper analyzes and improves prompt-stealing attacks on diffusion models by exploiting a noise-generation vulnerability (CWE-339). They introduce SeedSnitch and PromptPirate, outperforming existing methods in LPIPS similarity, and suggest countermeasures against such attacks.


<details>
  <summary>Details</summary>
Motivation: Prompts in diffusion models hold significant intellectual and economic value. However, the way initial random noise is generated using seeds in these models, particularly the restriction of seed values to $2^{32}$ on CPUs, makes them vulnerable to prompt theft through seed recovery. This poses a serious security and privacy concern.

Method: The authors investigate prompt-stealing attacks, identifying and exploiting a specific noise-generation vulnerability arising from how seeds are managed in popular image-generation frameworks. They develop SeedSnitch, a tool to recover seeds by brute-forcing due to the limited seed space. Then, they propose PromptPirate, a genetic algorithm-based approach to optimize prompt recovery given the recovered seed. The genetic algorithm enhances the recovery process by leveraging evolutionary techniques for refinement and better results.

Result: The research finds that about 95% of seed values in images shared on CivitAI can be recovered within 140 minutes per seed using SeedSnitch. When combined with PromptPirate, the system achieves an 8-11% improvement over state-of-the-art methods in LPIPS similarity, effectively showing its efficacy in recovering challenging prompts. Additionally, the proposed countermeasures have been confirmed to neutralize seed stealing and related prompt-piracy attempts.

Conclusion: The paper concludes that current diffusion model frameworks are highly susceptible to prompt-theft attacks through optimization and seed recovery approaches. The authors stress the importance of securing the seed generation processes in these models to prevent adversaries from reverse-engineering sensitive prompts. Their findings also highlight the vulnerability (CWE-339), urging the community to adopt the suggested countermeasures and collaborate in ensuring the security of diffusion models.

Abstract: Diffusion models have significantly advanced text-to-image generation,
enabling the creation of highly realistic images conditioned on textual prompts
and seeds. Given the considerable intellectual and economic value embedded in
such prompts, prompt theft poses a critical security and privacy concern. In
this paper, we investigate prompt-stealing attacks targeting diffusion models.
We reveal that numerical optimization-based prompt recovery methods are
fundamentally limited as they do not account for the initial random noise used
during image generation. We identify and exploit a noise-generation
vulnerability (CWE-339), prevalent in major image-generation frameworks,
originating from PyTorch's restriction of seed values to a range of $2^{32}$
when generating the initial random noise on CPUs. Through a large-scale
empirical analysis conducted on images shared via the popular platform CivitAI,
we demonstrate that approximately 95% of these images' seed values can be
effectively brute-forced in 140 minutes per seed using our seed-recovery tool,
SeedSnitch. Leveraging the recovered seed, we propose PromptPirate, a genetic
algorithm-based optimization method explicitly designed for prompt stealing.
PromptPirate surpasses state-of-the-art methods, i.e., PromptStealer, P2HP, and
CLIP-Interrogator, achieving an 8-11% improvement in LPIPS similarity.
Furthermore, we introduce straightforward and effective countermeasures that
render seed stealing, and thus optimization-based prompt stealing, ineffective.
We have disclosed our findings responsibly and initiated coordinated mitigation
efforts with the developers to address this critical vulnerability.

</details>


### [18] [What Does Normal Even Mean? Evaluating Benign Traffic in Intrusion Detection Datasets](https://arxiv.org/abs/2509.09564)
*Meghan Wilkinson,Robert H Thomson*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Supervised machine learning techniques rely on labeled data to achieve high
task performance, but this requires the labels to capture some meaningful
differences in the underlying data structure. For training network intrusion
detection algorithms, most datasets contain a series of attack classes and a
single large benign class which captures all non-attack network traffic. A
review of intrusion detection papers and guides that explicitly state their
data preprocessing steps identified that the majority took the labeled
categories of the dataset at face value when training their algorithms. The
present paper evaluates the structure of benign traffic in several common
intrusion detection datasets (NSL-KDD, UNSW-NB15, and CIC-IDS 2017) and
determines whether there are meaningful sub-categories within this traffic
which may improve overall multi-classification performance using common machine
learning techniques. We present an overview of some unsupervised clustering
techniques (e.g., HDBSCAN, Mean Shift Clustering) and show how they
differentially cluster the benign traffic space.

</details>


### [19] [Bridging the Gap in Phishing Detection: A Comprehensive Phishing Dataset Collector](https://arxiv.org/abs/2509.09592)
*Aditya Kulkarni,Shahil Manishbhai Patel,Shivam Pradip Tirmare,Vivek Balachandran,Tamal Das*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: To combat phishing attacks -- aimed at luring web users to divulge their
sensitive information -- various phishing detection approaches have been
proposed. As attackers focus on devising new tactics to bypass existing
detection solutions, researchers have adapted by integrating machine learning
and deep learning into phishing detection. Phishing dataset collection is vital
to developing effective phishing detection approaches, which highly depend on
the diversity of the gathered datasets. The lack of diversity in the dataset
results in a biased model. Since phishing websites are often short-lived,
collecting them is also a challenge. Consequently, very few phishing webpage
dataset repositories exist to date. No single repository comprehensively
consolidates all phishing elements corresponding to a phishing webpage, namely,
URL, webpage source code, screenshot, and related webpage resources. This paper
introduces a resource collection tool designed to gather various resources
associated with a URL, such as CSS, Javascript, favicons, webpage images, and
screenshots. Our tool leverages PhishTank as the primary source for obtaining
active phishing URLs. Our tool fetches several additional webpage resources
compared to PyWebCopy Python library, which provides webpage content for a
given URL. Additionally, we share a sample dataset generated using our tool
comprising 4,056 legitimate and 5,666 phishing URLs along with their associated
resources. We also remark on the top correlated phishing features with their
associated class label found in our dataset. Our tool offers a comprehensive
resource set that can aid researchers in developing effective phishing
detection approaches.

</details>


### [20] [CryptoGuard: An AI-Based Cryptojacking Detection Dashboard Prototype](https://arxiv.org/abs/2509.09638)
*Amitabh Chakravorty,Jess Kropczynski,Nelly Elsayed*

Main category: cs.CR

TL;DR: CryptoGuard is a design-concept dashboard using user-centered principles to help non-technical crypto users detect and respond to cryptojacking threats, proving usability and technical security must be co-designed for effective digital protection.


<details>
  <summary>Details</summary>
Motivation: The work addresses the rising threat of cryptojacking by proposing an accessible solution that empowers non-expert users to detect and respond to security risks proactively, highlighting the limitations of purely technical approaches.

Method: The authors developed a user-centered design prototype via high-fidelity Figma mockups, prioritizing intuitive visuals and direct transaction/login monitoring features tailored for non-technical audiences, with conceptual AI elements implemented as interactive alerts.

Result: A functional design prototype demonstrating how usability heuristics can be directly integrated into threat detection interfaces, enabling rapid decision-making while showcasing key risk communication strategies for user empowerment.

Conclusion: The paper concludes that effective security tools must integrate robust backend security features with user-centric design principles to enable non-technical users to confidently respond to cryptojacking threats, bridging the gap between detection research and practical usability.

Abstract: With the widespread adoption of cryptocurrencies, cryptojacking has become a
significant security threat to crypto wallet users. This paper presents a
front-end prototype of an AI-powered security dashboard, namely, CryptoGuard.
Developed through a user-centered design process, the prototype was constructed
as a high-fidelity, click-through model from Figma mockups to simulate key user
interactions. It is designed to assist users in monitoring their login and
transaction activity, identifying any suspicious behavior, and enabling them to
take action directly within the wallet interface. The dashboard is designed for
a general audience, prioritizing an intuitive user experience for non-technical
individuals. Although its AI functionality is conceptual, the prototype
demonstrates features like visual alerts and reporting. This work is positioned
explicitly as a design concept, bridging cryptojacking detection research with
human-centered interface design. This paper also demonstrates how usability
heuristics can directly inform a tool's ability to support rapid and confident
decision-making under real-world threats. This paper argues that practical
security tools require not only robust backend functionality but also a
user-centric design that communicates risk and empowers users to take
meaningful action.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [21] [Pattern-Based File and Data Access with Python Glob: A Comprehensive Guide for Computational Research](https://arxiv.org/abs/2509.08843)
*Sidney Shapiro*

Main category: cs.SE

TL;DR: This paper positions Python's glob module as a critical tool for efficient file management in research workflows, using practical examples to demonstrate its role in scalable data science, AI, and reproducibility.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of documentation around pattern-based file access in computational research, aiming to establish glob as a foundational tool for scalable, reproducible workflows in data science, business analytics, and AI.

Method: The authors employ practical Python examples using libraries like pandas and scikit-learn to demonstrate glob's integration into analytical pipelines. They present case studies on data ingestion, organizational analysis, AI dataset construction, and reproducibility.

Result: The paper successfully illustrates glob's utility in large-scale data ingestion, cross-domain data analysis, AI pipeline construction, and reproducible research through concrete code examples and use cases.

Conclusion: The paper concludes that the glob module is an essential tool for Python-based research workflows, advocating for its adoption as a standard for file pattern matching due to its versatility and efficiency in handling data across various domains.

Abstract: Pattern-based file access is a fundamental but often under-documented aspect
of computational research. The Python glob module provides a simple yet
powerful way to search, filter, and ingest files using wildcard patterns,
enabling scalable workflows across disciplines. This paper introduces glob as a
versatile tool for data science, business analytics, and artificial
intelligence applications. We demonstrate use cases including large-scale data
ingestion, organizational data analysis, AI dataset construction, and
reproducible research practices. Through concrete Python examples with widely
used libraries such as pandas,scikit-learn, and matplotlib, we show how glob
facilitates efficient file traversal and integration with analytical pipelines.
By situating glob within the broader context of reproducible research and data
engineering, we highlight its role as a methodological building block. Our goal
is to provide researchers and practitioners with a concise reference that
bridges foundational concepts and applied practice, making glob a default
citation for file pattern matching in Python-based research workflows.

</details>


### [22] [A Systematic Mapping Study on Chatbots in Programming Education](https://arxiv.org/abs/2509.08857)
*Marcelino Garcia,Renato Garcia,Arthur Parizotto,Andre Mendes,Pedro Valle,Ricardo Vilela,Renato Balancieri,Williamson Silva*

Main category: cs.SE

TL;DR: This paper maps 54 studies on programming education chatbots, revealing Python-centric, concept-focused tools with varied pedagogies, while identifying research gaps for future development.


<details>
  <summary>Details</summary>
Motivation: Educational chatbots are increasingly used in programming education, necessitating a systematic review of their development and application to guide future tool creation.

Method: A Systematic Mapping Study (SMS) analyzed 54 selected studies from 3,216 publications, addressing five research subquestions on chatbot types, languages, content, interaction models, and application contexts.

Result: Chatbots predominantly focus on Python instruction, fundamental programming concepts, and employ diverse pedagogical approaches and technological architectures.

Conclusion: This study provides insights to inform the development of new educational tools for programming instruction by identifying trends and gaps in chatbot applications.

Abstract: Educational chatbots have gained prominence as support tools for teaching
programming, particularly in introductory learning contexts. This paper
presents a Systematic Mapping Study (SMS) that investigated how such agents
have been developed and applied in programming education. From an initial set
of 3,216 publications, 54 studies were selected and analyzed based on five
research subquestions, addressing chatbot types, programming languages used,
educational content covered, interaction models, and application contexts. The
results reveal a predominance of chatbots designed for Python instruction,
focusing on fundamental programming concepts, and employing a wide variety of
pedagogical approaches and technological architectures. In addition to
identifying trends and gaps in the literature, this study provides insights to
inform the development of new educational tools for programming instruction.

</details>


### [23] [GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation](https://arxiv.org/abs/2509.08863)
*Qianqian Luo,Liuchang Xu,Qingming Lin,Sensen Wu,Ruichen Mao,Chao Wang,Hailin Feng,Bo Huang,Zhenhong Du*

Main category: cs.SE

TL;DR: This paper introduces GeoJSON Agents, a multi-agent LLM framework for GIS automation. It shows code generation outperforms function calling by 11.43% accuracy while maintaining execution stability, offering a new approach for GeoAI systems.


<details>
  <summary>Details</summary>
Motivation: LLMs face limitations in GIS tasks without domain expertise. This study addresses the need for scalable automation frameworks that enable non-experts to perform spatial data processing and analysis effectively.

Method: The framework transforms natural language tasks into structured GeoJSON commands through task parsing (Planner agent), executes spatial data analysis via specialized Worker agents using either Function Calling or Code Generation techniques, and integrates outputs into standards-compliant GeoJSON files.

Result: Code Generation-based agents achieved 97.14% accuracy on a 70-task benchmark, outperforming Function Calling (85.71%) and general models (48.57%). Function Calling showed more stable execution while Code Generation provided greater flexibility.

Conclusion: GeoJSON Agents offer a novel multi-agent LLM framework that significantly improves GIS automation performance compared to standard models, with code generation showing higher flexibility and function calling providing more stable execution. This approach provides new perspectives for GeoAI system design.

Abstract: LLMs have made substantial progress in task automation and natural language
understanding.However,without expertise in GIS,they continue to encounter
limitations.To address these issues, we propose GeoJSON Agents-a multi-agent
LLM architecture.This framework transforms natural language tasks into
structured GeoJSON operation commands and processes spatial data using two
widely adopted LLM enhancement techniques:Function Calling and Code
Generation.The architecture consists of three components-task parsing,agent
collaboration,and result integration-aimed at enhancing both the performance
and scalability of GIS automation.The Planner agent interprets natural language
tasks into structured GeoJSON commands.Then,specialized Worker agents
collaborate according to assigned roles to perform spatial data processing and
analysis,either by invoking predefined function APIs or by dynamically
generating and executing Python-based spatial analysis code.Finally,the system
integrates the outputs from multiple execution rounds into
reusable,standards-compliant GeoJSON files.To systematically evaluate the
performance of the two approaches,we constructed a benchmark dataset of 70
tasks with varying complexity and conducted experiments using OpenAI's GPT-4o
as the core model.Results indicate that the Function Calling-based GeoJSON
Agent achieved an accuracy of 85.71%,while the Code Generation-based agent
reached 97.14%,both significantly outperforming the best-performing
general-purpose model (48.57%).Further analysis reveals that the Code
Generation provides greater flexibility,whereas the Function Calling approach
offers more stable execution.This study is the first to introduce an LLM
multi-agent framework for GeoJSON data and to compare the strengths and
limitations of two mainstream LLM enhancement methods,offering new perspectives
for improving GeoAI system performance.

</details>


### [24] [TraceRAG: A LLM-Based Framework for Explainable Android Malware Detection and Behavior Analysis](https://arxiv.org/abs/2509.08865)
*Guangyu Zhang,Xixuan Wang,Shiyu Sun,Peiyan Xiao,Kun Sun,Yanhai Xiong*

Main category: cs.SE

TL;DR: Strong performance in malware detection and behavior identification


<details>
  <summary>Details</summary>
Motivation: Traditional analysis techniques struggle to detect deep behaviors or explain decisions, necessitating advanced, explainable methods

Method: Introduce TraceRAG, a RAG framework using LLMs to interpret and report behaviors, enabling linking of code to behavior through query and retrieval of relevant snippets

Result: Experiments show malware detection accuracy of 96% with 83.81% behavior identification accuracy, verified by updated VT and manual validation

Conclusion: TraceRAG effectively combines LLMs with retrieval for explainable malware analysis and demonstrates high utility in expert evaluations

Abstract: Sophisticated evasion tactics in malicious Android applications, combined
with their intricate behavioral semantics, enable attackers to conceal
malicious logic within legitimate functions, underscoring the critical need for
robust and in-depth analysis frameworks. However, traditional analysis
techniques often fail to recover deeply hidden behaviors or provide
human-readable justifications for their decisions. Inspired by advances in
large language models (LLMs), we introduce TraceRAG, a retrieval-augmented
generation (RAG) framework that bridges natural language queries and Java code
to deliver explainable malware detection and analysis. First, TraceRAG
generates summaries of method-level code snippets, which are indexed in a
vector database. At query time, behavior-focused questions retrieve the most
semantically relevant snippets for deeper inspection. Finally, based on the
multi-turn analysis results, TraceRAG produces human-readable reports that
present the identified malicious behaviors and their corresponding code
implementations. Experimental results demonstrate that our method achieves 96\%
malware detection accuracy and 83.81\% behavior identification accuracy based
on updated VirusTotal (VT) scans and manual verification. Furthermore, expert
evaluation confirms the practical utility of the reports generated by TraceRAG.

</details>


### [25] [Benchmarking Energy Efficiency of Large Language Models Using vLLM](https://arxiv.org/abs/2509.08867)
*K. Pronk,Q. Zhao*

Main category: cs.SE

TL;DR: The paper introduces the LLM Efficiency Benchmark, which simulates real-world scenarios to evaluate the energy efficiency of Large Language Models using vLLM, offering practical insights for sustainable AI development.


<details>
  <summary>Details</summary>
Motivation: The increasing use of Large Language Models (LLMs) is significantly impacting the climate due to their high energy consumption, necessitating a more accurate understanding of their energy efficiency for developers.

Method: The authors present the LLM Efficiency Benchmark, a tool designed to simulate real-world production scenarios. They use vLLM, a high-throughput, production-ready LLM serving backend, to evaluate how model size, architecture, and concurrent request volume influence inference energy efficiency.

Result: The results demonstrate that the LLM Efficiency Benchmark can accurately represent practical deployment conditions, providing actionable insights into how different factors affect energy consumption.

Conclusion: The study successfully introduces a benchmark that better reflects real-world energy efficiency of LLMs, supporting the development of more sustainable AI systems by identifying factors that impact their energy usage.

Abstract: The prevalence of Large Language Models (LLMs) is having an growing impact on
the climate due to the substantial energy required for their deployment and
use. To create awareness for developers who are implementing LLMs in their
products, there is a strong need to collect more information about the energy
efficiency of LLMs. While existing research has evaluated the energy efficiency
of various models, these benchmarks often fall short of representing realistic
production scenarios. In this paper, we introduce the LLM Efficiency Benchmark,
designed to simulate real-world usage conditions. Our benchmark utilizes vLLM,
a high-throughput, production-ready LLM serving backend that optimizes model
performance and efficiency. We examine how factors such as model size,
architecture, and concurrent request volume affect inference energy efficiency.
Our findings demonstrate that it is possible to create energy efficiency
benchmarks that better reflect practical deployment conditions, providing
valuable insights for developers aiming to build more sustainable AI systems.

</details>


### [26] [CLARA: A Developer's Companion for Code Comprehension and Analysis](https://arxiv.org/abs/2509.09072)
*Ahmed Adnan,Mushfiqur Rahman,Saad Sakib Noor,Kazi Sakib*

Main category: cs.SE

TL;DR: This paper introduces CLARA, a browser-based code analysis tool that uses advanced inference models to aid developers and researchers. Through rigorous evaluation and user testing, CLARA is shown to be a practical, accurate solution for code comprehension and refactoring tasks.


<details>
  <summary>Details</summary>
Motivation: Existing code analysis tools require prior setup, lack context-awareness, and demand significant manual effort. CLARA aims to overcome these limitations by providing an intuitive, integrated, and automated solution.

Method: The authors developed CLARA, a browser extension leveraging a state-of-the-art inference model, and evaluated its performance qualitatively through existing datasets and a user study involving 10 developers and researchers.

Result: The evaluation confirmed CLARA's accuracy and effectiveness in code comprehension, refactoring, and quality attribute detection. The user study further validated its usability and practical utility.

Conclusion: CLARA is an open-source browser extension that effectively addresses the limitations of existing code analysis tools by offering a context-aware, automated solution. The paper demonstrates its utility, accuracy, and practicality through evaluations and user studies, making it a valuable resource for developers and researchers.

Abstract: Code comprehension and analysis of open-source project codebases is a task
frequently performed by developers and researchers. However, existing tools
that practitioners use for assistance with such tasks often require prior
project setup, lack context-awareness, and involve significant manual effort.
To address this, we present CLARA, a browser extension that utilizes a
state-of-the-art inference model to assist developers and researchers in: (i)
comprehending code files and code fragments, (ii) code refactoring, and (iii)
code quality attribute detection. We qualitatively evaluated CLARA's inference
model using existing datasets and methodology, and performed a comprehensive
user study with 10 developers and academic researchers to assess its usability
and usefulness. The results show that CLARA is useful, accurate, and practical
in code comprehension and analysis tasks. CLARA is an open-source tool
available at https://github.com/SaadNoor555/CLARA_tool_demo. A video showing
the full capabilities of CLARA can be found at
https://youtu.be/VDKVXvIH41Q?si=qBFsmS_Y4m_9x3YH.

</details>


### [27] [Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset](https://arxiv.org/abs/2509.09192)
*Doha Nam,Taehyoun Kim,Duksan Ryu,Jongmoon Baik*

Main category: cs.SE

TL;DR: ReDef improves JIT-SDP benchmark quality. PLMs benefit from diff-style encodings but fail to grasp semantic changes, indicating superficial learning patterns.


<details>
  <summary>Details</summary>
Motivation: Existing JIT-SDP datasets suffer from noisy labels and low precision in identifying bug-inducing commits. This study aims to create a reliable benchmark and evaluate how PLMs reason about code modifications.

Method: 1) Constructed ReDef dataset with 22 C/C++ projects using revert commits and post-hoc validation. 2) Filtered ambiguous instances via GPT-assisted triage. 3) Evaluated PLMs (CodeBERT, CodeT5+, UniXcoder) under five encoding strategies and counterfactual perturbations (block swapping, polarity inversion, spurious markers).

Result: ReDef achieved 3,164 defective and 10,268 clean modifications with high-confidence labels. Diff-style encodings outperformed whole-function formats across all PLMs, but counterfactual tests revealed model reliance on superficial cues (e.g., diff polarity) rather than semantic understanding.

Conclusion: Current pre-trained language models (PLMs) demonstrate limited ability to genuinely comprehend code modifications, as they rely on superficial cues rather than true semantic understanding. ReDef provides a more reliable benchmark for JIT-SDP.

Abstract: Just-in-Time software defect prediction (JIT-SDP) plays a critical role in
prioritizing risky code changes during code review and continuous integration.
However, existing datasets often suffer from noisy labels and low precision in
identifying bug-inducing commits. To address this, we present ReDef
(Revert-based Defect dataset), a high-confidence benchmark of function-level
modifications curated from 22 large-scale C/C++ projects. Defective cases are
anchored by revert commits, while clean cases are validated through post-hoc
history checks. Ambiguous instances are conservatively filtered out via a
GPT-assisted triage process involving multiple votes and audits. This pipeline
yields 3,164 defective and 10,268 clean modifications, offering substantially
more reliable labels than prior existing resources. Beyond dataset
construction, we provide the first systematic evaluation of how pre-trained
language models (PLMs) reason about code modifications -- specifically, which
input encodings most effectively expose change information, and whether models
genuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder
under five encoding strategies, and further probe their sensitivity through
counterfactual perturbations that swap added/deleted blocks, invert diff
polarity, or inject spurious markers. Our results show that compact diff-style
encodings consistently outperform whole-function formats across all PLMs, with
statistical tests confirming large, model-independent effects. However, under
counterfactual tests, performance degrades little or not at all -- revealing
that what appears to be robustness in fact reflects reliance on superficial
cues rather than true semantic understanding. These findings indicate that,
unlike in snapshot-based tasks, current PLMs remain limited in their ability to
genuinely comprehend code modifications.

</details>


### [28] [On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability](https://arxiv.org/abs/2509.09194)
*Ayelet Berzack,Guy Katz*

Main category: cs.SE

TL;DR: This paper addresses LLM unreliability in software development by combining large language models with Scenario-Based Programming. The resulting methodology enables error-resistant development through AI-human collaboration, demonstrated via a verified Connect4 agent implementation.


<details>
  <summary>Details</summary>
Motivation: Despite LLMs' benefits, they often produce erroneous code with confidence. This creates risks in trusting their outputs. The motivation is to develop a structured framework that retains LLM advantages (speed, creativity) while mitigating their inherent unreliability through rigorous verification techniques.

Method: The authors propose a methodology combining LLMs with traditional software engineering practices via the SBP paradigm. This approach lets developers inject their expertise into LLM inputs and validate/inspect LLM-generated outputs through event-driven, scenario-based workflows.

Result: The methodology was validated through a Connect4 case study, demonstrating: (1) creation of a strong agent capable of defeating existing agents, and (2) successful formal verification of the agent's correctness in specific scenarios, proving the hybrid approach's practicality and reliability.

Conclusion: The paper concludes that integrating LLMs with Scenario-Based Programming (SBP) enhances reliability in software development by reducing errors, streamlining processes, and enabling verification of critical program properties through human-AI collaboration.

Abstract: Large Language Models (LLMs) are fast becoming indispensable tools for
software developers, assisting or even partnering with them in crafting complex
programs. The advantages are evident -- LLMs can significantly reduce
development time, generate well-organized and comprehensible code, and
occasionally suggest innovative ideas that developers might not conceive on
their own. However, despite their strengths, LLMs will often introduce
significant errors and present incorrect code with persuasive confidence,
potentially misleading developers into accepting flawed solutions.
  In order to bring LLMs into the software development cycle in a more reliable
manner, we propose a methodology for combining them with ``traditional''
software engineering techniques in a structured way, with the goal of
streamlining the development process, reducing errors, and enabling users to
verify crucial program properties with increased confidence. Specifically, we
focus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,
scenario-based approach for software engineering -- to allow human developers
to pour their expert knowledge into the LLM, as well as to inspect and verify
its outputs.
  To evaluate our methodology, we conducted a significant case study, and used
it to design and implement the Connect4 game. By combining LLMs and SBP we were
able to create a highly-capable agent, which could defeat various strong
existing agents. Further, in some cases, we were able to formally verify the
correctness of our agent. Finally, our experience reveals interesting insights
regarding the ease-of-use of our proposed approach. The full code of our
case-study will be made publicly available with the final version of this
paper.

</details>


### [29] [Altered Histories in Version Control System Repositories: Evidence from the Trenches](https://arxiv.org/abs/2509.09294)
*Solal Rapaport,Laurent Pautet,Samuel Tardieu,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: This paper studies the impact of Git history rewriting on public repositories, analyzing 111 million archives to identify 8.7 million altered histories. It introduces GitHistorian, a tool for detecting problematic history changes, including retroactive license alterations and secret removals.


<details>
  <summary>Details</summary>
Motivation: History rewriting in Git can break workflows and compromise repository integrity, while hidden risks like supply chain attacks and governance issues remain unstudied at scale.

Method: 111M Software Heritage repositories were analyzed using historical archives. Reprospective studies categorized alteration types and locations, with case studies on license changes and secret removals.

Result: 1.22M repositories with altered history found, showing patterns of retroactive license modification and accidental secret deletion. These behaviors indicate poor governance and security practices.

Conclusion: GitHistorian enables automatic detection of suspicious history alterations, highlighting the need for tools and practices to monitor public repository integrity and prevent governance/security risks.

Abstract: Version Control Systems (VCS) like Git allow developers to locally rewrite
recorded history, e.g., to reorder and suppress commits or specific data in
them. These alterations have legitimate use cases, but become problematic when
performed on public branches that have downstream users: they break push/pull
workflows, challenge the integrity and reproducibility of repositories, and
create opportunities for supply chain attackers to sneak into them nefarious
changes. We conduct the first large-scale investigation of Git history
alterations in public code repositories. We analyze 111 M (millions)
repositories archived by Software Heritage, which preserves VCS histories even
across alterations. We find history alterations in 1.22 M repositories, for a
total of 8.7 M rewritten histories. We categorize changes by where they happen
(which repositories, which branches) and what is changed in them (files or
commit metadata). Conducting two targeted case studies we show that altered
histories recurrently change licenses retroactively, or are used to remove
''secrets'' (e.g., private keys) committed by mistake. As these behaviors
correspond to bad practices-in terms of project governance or security
management, respectively-that software recipients might want to avoid, we
introduce GitHistorian, an automated tool, that developers can use to spot and
describe history alterations in public Git repositories.

</details>


### [30] [Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data](https://arxiv.org/abs/2509.09313)
*Moritz Mock,Thomas Forrer,Barbara Russo*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Deep learning solutions for vulnerability detection proposed in academic
research are not always accessible to developers, and their applicability in
industrial settings is rarely addressed. Transferring such technologies from
academia to industry presents challenges related to trustworthiness, legacy
systems, limited digital literacy, and the gap between academic and industrial
expertise. For deep learning in particular, performance and integration into
existing workflows are additional concerns. In this work, we first evaluate the
performance of CodeBERT for detecting vulnerable functions in industrial and
open-source software. We analyse its cross-domain generalisation when
fine-tuned on open-source data and tested on industrial data, and vice versa,
also exploring strategies for handling class imbalance. Based on these results,
we develop AI-DO(Automating vulnerability detection Integration for Developers'
Operations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated
recommender system that uses fine-tuned CodeBERT to detect and localise
vulnerabilities during code review without disrupting workflows. Finally, we
assess the tool's perceived usefulness through a survey with the company's IT
professionals. Our results show that models trained on industrial data detect
vulnerabilities accurately within the same domain but lose performance on
open-source code, while a deep learner fine-tuned on open data, with
appropriate undersampling techniques, improves the detection of
vulnerabilities.

</details>


### [31] [ORCA: Unveiling Obscure Containers In The Wild](https://arxiv.org/abs/2509.09322)
*Jacopo Bufalino,Agathe Blaise,Stefano Secci*

Main category: cs.SE

TL;DR: This paper highlights the security risks in containerized environments due to outdated components and incomplete images caused by filesystem modifications. It introduces ORCA, an open-source obscuration-resilient container analyzer, which improves file coverage for SCA by 40% on average compared to existing tools.


<details>
  <summary>Details</summary>
Motivation: Containerized applications rely on third-party components, but filesystem modifications can obscure these components, making it hard for standard SCA tools to detect vulnerabilities accurately. This work aims to address the challenge of incomplete container images to enhance the reliability of software security.

Method: The researchers studied the limitations of existing SCA tools through an analysis of 600 popular containers, identifying the prevalence of obscure images. They then developed a methodology that is resilient to such obscuration and implemented it in the open-source tool ORCA.

Result: ORCA successfully detects content in obscure containers with a median 40% improvement in file coverage compared to Docker Scout and Syft, demonstrating its effectiveness in handling incomplete or obscured filesystems in container images.

Conclusion: The paper concludes that obscuration in container filesystems is a significant yet underestimated problem for SCA tools. ORCA's methodology and implementation provide a solution to improve SCA accuracy by being resilient to filesystem modifications, encouraging its adoption in the open-source and software security communities.

Abstract: Modern software development increasingly depends on open-source libraries and
third-party components, which are often encapsulated into containerized
environments. While improving the development and deployment of applications,
this approach introduces security risks, particularly when outdated or
vulnerable components are inadvertently included in production environments.
Software Composition Analysis (SCA) is a critical process that helps identify
and manage packages and dependencies inside a container. However, unintentional
modifications to the container filesystem can lead to incomplete container
images, which compromise the reliability of SCA tools. In this paper, we
examine the limitations of both cloud-based and open-source SCA tools when
faced with such obscure images. An analysis of 600 popular containers revealed
that obscure containers exist in well-known registries and trusted images and
that many tools fail to analyze such containers. To mitigate these issues, we
propose an obscuration-resilient methodology for container analysis and
introduce ORCA (Obscuration-Resilient Container Analyzer), its open-source
implementation. We reported our findings to all vendors using their appropriate
channels. Our results demonstrate that ORCA effectively detects the content of
obscure containers and achieves a median 40% improvement in file coverage
compared to Docker Scout and Syft.

</details>


### [32] [LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering](https://arxiv.org/abs/2509.09614)
*Jielin Qiu,Zuxin Liu,Zhiwei Liu,Rithesh Murthy,Jianguo Zhang,Haolin Chen,Shiyu Wang,Ming Zhu,Liangwei Yang,Juntao Tan,Zhepeng Cen,Cheng Qian,Shelby Heinecke,Weiran Yao,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.SE

TL;DR: LoCoBench is a new benchmark for evaluating long-context language models in complex software development tasks, addressing gaps in current benchmarks by testing multi-file reasoning, architectural consistency, and large-scale codebase understanding across 10 languages and 8 task categories with 10Kâ€“1M token contexts.


<details>
  <summary>Details</summary>
Motivation: Existing code evaluation benchmarks focus on short-context or single-function tasks, neglecting the critical need to assess long-context capabilities required for real-world software development, such as cross-file reasoning and codebase-scale understanding.

Method: LoCoBench creates 8,000 scenarios via a 5-phase pipeline across 10 programming languages, with 8 task categories (e.g., architectural understanding, cross-file refactoring, security analysis) and context lengths varying 100x (10Kâ€“1M tokens). It introduces 17 evaluation metrics (8 novel ones) consolidated into a composite LoCoBench Score (LCBS).

Result: Evaluation of leading long-context models reveals significant performance gaps in handling complex real-world scenarios, demonstrating that scalable code understanding remains an unsolved challenge despite advances in model context length capabilities.

Conclusion: LoCoBench establishes a standardized framework to evaluate and advance long-context LLMs for software development, highlighting the need for further research to bridge identified performance gaps while providing freely accessible benchmark (https://github.com/SalesforceAIResearch/LoCoBench).

Abstract: The emergence of long-context language models with context windows extending
to millions of tokens has created new opportunities for sophisticated code
understanding and software development evaluation. We propose LoCoBench, a
comprehensive benchmark specifically designed to evaluate long-context LLMs in
realistic, complex software development scenarios. Unlike existing code
evaluation benchmarks that focus on single-function completion or short-context
tasks, LoCoBench addresses the critical evaluation gap for long-context
capabilities that require understanding entire codebases, reasoning across
multiple files, and maintaining architectural consistency across large-scale
software systems. Our benchmark provides 8,000 evaluation scenarios
systematically generated across 10 programming languages, with context lengths
spanning 10K to 1M tokens, a 100x variation that enables precise assessment of
long-context performance degradation in realistic software development
settings. LoCoBench introduces 8 task categories that capture essential
long-context capabilities: architectural understanding, cross-file refactoring,
multi-session development, bug investigation, feature implementation, code
comprehension, integration testing, and security analysis. Through a 5-phase
pipeline, we create diverse, high-quality scenarios that challenge LLMs to
reason about complex codebases at unprecedented scale. We introduce a
comprehensive evaluation framework with 17 metrics across 4 dimensions,
including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our
evaluation of state-of-the-art long-context models reveals substantial
performance gaps, demonstrating that long-context understanding in complex
software development represents a significant unsolved challenge that demands
more attention. LoCoBench is released at:
https://github.com/SalesforceAIResearch/LoCoBench.

</details>


### [33] [I Know Who Clones Your Code: Interpretable Smart Contract Similarity Detection](https://arxiv.org/abs/2509.09630)
*Zhenguang Liu,Lixun Ma,Zhongzheng Mu,Chengkun Wei,Xiaojun Xu,Yingying Jiao,Kui Ren*

Main category: cs.SE

TL;DR: SmartDetector solves smart contract similarity detection by breaking down code into statement trees and using explainable classification, achieving 95.88% F1-score with significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: The increasing reuse of open-source smart contracts amplifies bug propagation risks, while existing AST-based and deep-learning methods face limitations in semantic comparison and interpretability.

Method: SmartDetector decomposes smart contract ASTs into statement trees for structural analysis, uses a classifier for pairwise comparison, and mathematically derives a cosine-wise diffusion process to optimize hyperparameters.

Result: Experiments on three large datasets show SmartDetector achieves an average F1-score of 95.88%, outperforming state-of-the-art methods by 14.01%.

Conclusion: SmartDetector effectively addresses the research gap in smart contract similarity detection by introducing an explainable, fine-grained method that outperforms existing approaches with a 14.01% improvement in F1-score.

Abstract: Widespread reuse of open-source code in smart contract development boosts
programming efficiency but significantly amplifies bug propagation across
contracts, while dedicated methods for detecting similar smart contract
functions remain very limited. Conventional abstract-syntax-tree (AST) based
methods for smart contract similarity detection face challenges in handling
intricate tree structures, which impedes detailed semantic comparison of code.
Recent deep-learning based approaches tend to overlook code syntax and
detection interpretability, resulting in suboptimal performance.
  To fill this research gap, we introduce SmartDetector, a novel approach for
computing similarity between smart contract functions, explainable at the
fine-grained statement level. Technically, SmartDetector decomposes the AST of
a smart contract function into a series of smaller statement trees, each
reflecting a structural element of the source code. Then, SmartDetector uses a
classifier to compute the similarity score of two functions by comparing each
pair of their statement trees. To address the infinite hyperparameter space of
the classifier, we mathematically derive a cosine-wise diffusion process to
efficiently search optimal hyperparameters. Extensive experiments conducted on
three large real-world datasets demonstrate that SmartDetector outperforms
current state-of-the-art methods by an average improvement of 14.01% in
F1-score, achieving an overall average F1-score of 95.88%.

</details>
