{"id": "2508.10991", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10991", "abs": "https://arxiv.org/abs/2508.10991", "authors": ["Wenpeng Xing", "Zhonghao Qi", "Yupeng Qin", "Yilin Li", "Caini Chang", "Jiahui Yu", "Changting Lin", "Zhenzhen Xie", "Meng Han"], "title": "MCP-Guard: A Defense Framework for Model Context Protocol Integrity in Large Language Model Applications", "comment": null, "summary": "The integration of Large Language Models (LLMs) with external tools via\nprotocols such as the Model Context Protocol (MCP) introduces critical security\nvulnerabilities, including prompt injection, data exfiltration, and other\nthreats. To counter these challenges, we propose MCP-Guard, a robust, layered\ndefense architecture designed for LLM--tool interactions. MCP-Guard employs a\nthree-stage detection pipeline that balances efficiency with accuracy: it\nprogresses from lightweight static scanning for overt threats and a deep neural\ndetector for semantic attacks, to our fine-tuned E5-based model achieves\n(96.01) accuracy in identifying adversarial prompts. Finally, a lightweight LLM\narbitrator synthesizes these signals to deliver the final decision while\nminimizing false positives. To facilitate rigorous training and evaluation, we\nalso introduce MCP-AttackBench, a comprehensive benchmark of over 70,000\nsamples. Sourced from public datasets and augmented by GPT-4, MCP-AttackBench\nsimulates diverse, real-world attack vectors in the MCP format, providing a\nfoundation for future research into securing LLM-tool ecosystems.", "AI": {"tldr": "This paper introduces MCP-Guard, a defense architecture for securing LLM-tool interactions against threats like prompt injection, and proposes MCP-AttackBench, a benchmark of 70,000+ samples for training/evaluating such defenses.", "motivation": "The integration of LLMs with external tools via protocols like Model Context Protocol (MCP) creates significant security risks, including prompt injection and data exfiltration. These threats require systematic protection mechanisms to maintain trust and safety in LLM-based systems.", "method": "A three-stage defense pipeline: (1) lightweight static scanning for obvious threats, (2) a fine-tuned E5-based deep neural detector for semantic attacks, and (3) a lightweight LLM arbitrator for final validation. Combined with MCP-AttackBench - a benchmark containing 70,000+ GPT-4 augmented attack samples in MCP format.", "result": "The E5-based model achieves 96.01% accuracy in detecting adversarial prompts, while the layered approach effectively reduces false positives through multiple detection stages and LLM validation.", "conclusion": "MCP-Guard provides a robust, practical solution for securing LLM-tool ecosystems against both syntactic and semantic attacks, while MCP-AttackBench establishes a foundational benchmark for future security research in this domain."}}
{"id": "2508.11082", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11082", "abs": "https://arxiv.org/abs/2508.11082", "authors": ["Sina Bagheri", "Masoud Kaveh", "Francisco Hernando-Gallego", "Diego Mart\u00edn", "Nuria Serrano"], "title": "A Constant-Time Hardware Architecture for the CSIDH Key-Exchange Protocol", "comment": null, "summary": "The commutative supersingular isogeny Diffie-Hellman (CSIDH) algorithm is a\npromising post-quantum key exchange protocol, notable for its exceptionally\nsmall key sizes, but hindered by computationally intensive key generation.\nFurthermore, practical implementations must operate in constant time to\nmitigate side-channel vulnerabilities, which presents an additional performance\nchallenge. This paper presents, to our knowledge, the first comprehensive\nhardware study of CSIDH, establishing a performance baseline with a unified\narchitecture on both field-programmable gate array (FPGA) and\napplication-specific integrated circuit (ASIC) platforms. The architecture\nfeatures a top-level finite state machine (FSM) that orchestrates a deeply\npipelined arithmetic logic unit (ALU) to accelerate the underlying 512-bit\nfinite field operations. The ALU employs a parallelized schoolbook multiplier,\ncompleting a 512$\\times$512-bit multiplication in 22 clock cycles and enabling\na full Montgomery modular multiplication in 87 cycles. The constant-time\nCSIDH-512 design requires $1.03\\times10^{8}$ clock cycles per key generation.\nWhen implemented on a Xilinx Zynq UltraScale+ FPGA, the architecture achieves a\n200 MHz clock frequency, corresponding to a 515 ms latency. For ASIC\nimplementation in a 180nm process, the design requires $1.065\\times10^{8}$\nclock cycles and achieves a \\textasciitilde 180 MHz frequency, resulting in a\nkey generation latency of 591 ms. By providing the first public hardware\nperformance metrics for CSIDH on both FPGA and ASIC platforms, this work\ndelivers a crucial benchmark for future isogeny-based post-quantum cryptography\n(PQC) accelerators.", "AI": {"tldr": "This paper presents the first comprehensive hardware study of the CSIDH post-quantum key exchange protocol, providing performance baselines on both FPGA and ASIC platforms. The design uses a pipelined ALU and parallelized schoolbook multiplier to accelerate 512-bit field operations, achieving key generation latencies of 515 ms (FPGA) and 591 ms (ASIC).", "motivation": "CSIDH is recognized for its small key sizes in post-quantum cryptography but faces challenges in computational efficiency and constant-time implementation to avoid side-channel attacks. No prior hardware benchmarks exist for CSIDH, necessitating a study to establish performance baselines for future isogeny-based cryptography accelerators.", "method": "The authors developed a unified hardware architecture featuring a finite state machine (FSM) to manage a deeply pipelined arithmetic logic unit (ALU). The ALU leverages a parallelized schoolbook multiplier optimized for 512-bit finite field operations, supporting both FPGA and ASIC platforms. Key design metrics include completing 512\u00d7512-bit multiplications in 22 cycles and Montgomery modular multiplication in 87 cycles.", "result": "On a Xilinx Zynq UltraScale+ FPGA (200 MHz), the design achieves a key generation latency of 515 ms. In an ASIC implementation (180nm process) with similar clocking, the latency is 591 ms. Key generation requires 1.03\u00d710\u2078 and 1.065\u00d710\u2078 clock cycles respectively, offering the first public hardware performance metrics for CSIDH.", "conclusion": "By delivering the first FPGA and ASIC baselines for CSIDH, this work establishes critical benchmarks for optimizing isogeny-based post-quantum cryptography hardware. The results address the computational complexity of CSIDH while emphasizing secure, constant-time implementation requirements for practical adoption."}}
{"id": "2508.11095", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11095", "abs": "https://arxiv.org/abs/2508.11095", "authors": ["Asra Ali", "Jaeho Choi", "Bryant Gipson", "Shruthi Gorantala", "Jeremy Kun", "Wouter Legiest", "Lawrence Lim", "Alexander Viand", "Meron Zerihun Demissie", "Hongren Zheng"], "title": "HEIR: A Universal Compiler for Homomorphic Encryption", "comment": null, "summary": "This work presents Homomorphic Encryption Intermediate Representation (HEIR),\na unified approach to building homomorphic encryption (HE) compilers. HEIR aims\nto support all mainstream techniques in homomorphic encryption, integrate with\nall major software libraries and hardware accelerators, and advance the field\nby providing a platform for research and benchmarking. Built on the MLIR\ncompiler framework, HEIR introduces HE-specific abstraction layers at which\nexisting optimizations and new research ideas may be easily implemented.\nAlthough many HE optimization techniques have been proposed, it remains\ndifficult to combine or compare them effectively. HEIR provides a means to\neffectively explore the space of HE optimizations. HEIR addresses the entire HE\nstack and includes support for various frontends, including Python. The\ncontribution of this work includes: (1) We introduce HEIR as a framework for\nbuilding HE compilers. (2) We validate HEIR's design by porting a large\nfraction of the HE literature to HEIR, and we argue that HEIR can tackle more\ncomplicated and diverse programs than prior literature. (3) We provide evidence\nthat HEIR is emerging as the de facto HE compiler for academic research and\nindustry development.", "AI": {"tldr": "HEIR is a unified framework for building homomorphic encryption compilers using MLIR, enabling integration of diverse HE techniques and optimizations for research and industry applications.", "motivation": "Current HE optimization techniques are fragmented and difficult to combine or compare, necessitating a platform for exploration and benchmarking across the entire HE stack.", "method": "Built on MLIR, HEIR introduces HE-specific abstraction layers that facilitate implementation of existing optimizations and novel research ideas, with support for multiple frontends like Python.", "result": "HEIR's design was validated by porting extensive HE research literature, demonstrating its capacity to handle more complex programs than prior works, and showing adoption as a de facto compiler.", "conclusion": "HEIR establishes itself as the emerging standard for HE compiler development, bridging academic research and industry implementation through its flexible, extensible architecture."}}
{"id": "2508.11325", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11325", "abs": "https://arxiv.org/abs/2508.11325", "authors": ["Georgios Michail Makrakis", "Jeroen Pijpker", "Remco Hassing", "Rob Loves", "Stephen McCombie"], "title": "Salty Seagull: A VSAT Honeynet to Follow the Bread Crumb of Attacks in Ship Networks", "comment": null, "summary": "Cyber threats against the maritime industry have increased notably in recent\nyears, highlighting the need for innovative cybersecurity approaches. Ships, as\ncritical assets, possess highly specialized and interconnected network\ninfrastructures, where their legacy systems and operational constraints further\nexacerbate their vulnerability to cyberattacks. To better understand this\nevolving threat landscape, we propose the use of cyber-deception techniques and\nin particular honeynets, as a means to gather valuable insights into ongoing\nattack campaigns targeting the maritime sector.\n  In this paper we present Salty Seagull, a honeynet conceived to simulate a\nVSAT system for ships. This environment mimics the operations of a functional\nVSAT system onboard and, at the same time, enables a user to interact with it\nthrough a Web dashboard and a CLI environment. Furthermore, based on existing\nvulnerabilities, we purposefully integrate them into our system to increase\nattacker engagement. We exposed our honeynet for 30 days to the Internet to\nassess its capability and measured the received interaction. Results show that\nwhile numerous generic attacks have been attempted, only one curious attacker\nwith knowledge of the nature of the system and its vulnerabilities managed to\naccess it, without however exploring its full potential.", "AI": {"tldr": "The paper introduces Salty Seagull, a honeynet simulating a VSAT system for maritime cybersecurity research. Exposed to the internet for 30 days, it observed many generic attacks but only one attacker exploited its specialized vulnerabilities without fully accessing the system.", "motivation": "The maritime industry faces growing cyber threats due to ships' specialized, interconnected network infrastructures and legacy systems, creating urgent needs for innovative methods to analyze attacker behaviors and identify vulnerabilities.", "method": "The authors designed Salty Seagull as a VSAT simulation honeynet that integrates known maritime system vulnerabilities. They implemented dual interaction channels (Web dashboard and CLI) and monitored network activity for 30 days to collect attack data.", "result": "Despite 30 days of exposure, only one attacker demonstrated awareness of the honeynet's maritime-specific setup. While multiple generic attacks occurred, no incidents reached deeply simulated legacy components suggesting limited advanced attacker engagement.", "conclusion": "The deployment demonstrates honeynets' potential to detect and analyze maritime-specific cyber threats, but also reveals the relative obscurity of ship systems to most attackers despite their strategic importance to global trade."}}
{"id": "2508.11034", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11034", "abs": "https://arxiv.org/abs/2508.11034", "authors": ["Antonio Collante", "Samuel Abedu", "SayedHassan Khatoonabadi", "Ahmad Abdellatif", "Ebube Alor", "Emad Shihab"], "title": "The Impact of Large Language Models (LLMs) on Code Review Process", "comment": null, "summary": "Large language models (LLMs) have recently gained prominence in the field of\nsoftware development, significantly boosting productivity and simplifying\nteamwork. Although prior studies have examined task-specific applications, the\nphase-specific effects of LLM assistance on the efficiency of code review\nprocesses remain underexplored. This research investigates the effect of GPT on\nGitHub pull request (PR) workflows, with a focus on reducing resolution time,\noptimizing phase-specific performance, and assisting developers. We curated a\ndataset of 25,473 PRs from 9,254 GitHub projects and identified GPT-assisted\nPRs using a semi-automated heuristic approach that combines keyword-based\ndetection, regular expression filtering, and manual verification until\nachieving 95% labeling accuracy. We then applied statistical modeling,\nincluding multiple linear regression and Mann-Whitney U test, to evaluate\ndifferences between GPT-assisted and non-assisted PRs, both at the overall\nresolution level and across distinct review phases. Our research has revealed\nthat early adoption of GPT can substantially boost the effectiveness of the PR\nprocess, leading to considerable time savings at various stages. Our findings\nsuggest that GPT-assisted PRs reduced median resolution time by more than 60%\n(9 hours compared to 23 hours for non-assisted PRs). We discovered that\nutilizing GPT can reduce the review time by 33% and the waiting time before\nacceptance by 87%. Analyzing a sample dataset of 300 GPT-assisted PRs, we\ndiscovered that developers predominantly use GPT for code optimization (60%),\nbug fixing (26%), and documentation updates (12%). This research sheds light on\nthe impact of the GPT model on the code review process, offering actionable\ninsights for software teams seeking to enhance workflows and promote seamless\ncollaboration.", "AI": {"tldr": "This paper examines the impact of GPT-assisted tools on GitHub pull request (PR) workflows, revealing significant reductions in resolution time and phase-specific improvements in code review efficiency.", "motivation": "The study aims to address the lack of research on how LLMs like GPT affect specific phases of the code review process, building on the broader context of LLMs enhancing software development productivity.", "method": "The researchers analyzed 25,473 PRs from 9,254 GitHub projects to identify GPT-assisted PRs using keyword detection, regex filtering, and manual verification (95% accuracy). Statistical methods, including multiple linear regression and Mann-Whitney U tests, were applied to assess performance differences across PR phases.", "result": "GPT-assisted PRs reduced median resolution time by 60% (9 vs. 23 hours), with 33% shorter review times and 87% less waiting time before acceptance. Analysis of 300 PRs showed developers primarily used GPT for code optimization (60%), bug fixing (26%), and documentation (12%).", "conclusion": "Early integration of GPT in code reviews enhances efficiency at multiple stages. The findings highlight actionable benefits for software teams, such as time savings and improved error resolution, and emphasize GPT's role in optimizing workflows and collaboration."}}
{"id": "2508.11472", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11472", "abs": "https://arxiv.org/abs/2508.11472", "authors": ["Yang Wang", "Yaxin Zhao", "Xinyu Jiao", "Sihan Xu", "Xiangrui Cai", "Ying Zhang", "Xiaojie Yuan"], "title": "RMSL: Weakly-Supervised Insider Threat Detection with Robust Multi-sphere Learning", "comment": "15 pages", "summary": "Insider threat detection aims to identify malicious user behavior by\nanalyzing logs that record user interactions. Due to the lack of fine-grained\nbehavior-level annotations, detecting specific behavior-level anomalies within\nuser behavior sequences is challenging. Unsupervised methods face high false\npositive rates and miss rates due to the inherent ambiguity between normal and\nanomalous behaviors. In this work, we instead introduce weak labels of behavior\nsequences, which have lower annotation costs, i.e., the training labels\n(anomalous or normal) are at sequence-level instead of behavior-level, to\nenhance the detection capability for behavior-level anomalies by learning\ndiscriminative features. To achieve this, we propose a novel framework called\nRobust Multi-sphere Learning (RMSL). RMSL uses multiple hyper-spheres to\nrepresent the normal patterns of behaviors. Initially, a one-class classifier\nis constructed as a good anomaly-supervision-free starting point. Building on\nthis, using multiple instance learning and adaptive behavior-level\nself-training debiasing based on model prediction confidence, the framework\nfurther refines hyper-spheres and feature representations using weak\nsequence-level labels. This approach enhances the model's ability to\ndistinguish between normal and anomalous behaviors. Extensive experiments\ndemonstrate that RMSL significantly improves the performance of behavior-level\ninsider threat detection.", "AI": {"tldr": "This paper proposes Robust Multi-sphere Learning (RMSL), a weakly supervised framework for behavior-level insider threat detection that uses sequence-level labels to address annotation challenges and improve discriminative feature learning through multiple hyper-spheres and adaptive self-training.", "motivation": "Traditional unsupervised insider threat detection struggles with high false positives due to ambiguity in behavior-level anomalies, while fully supervised approaches require costly fine-grained annotations. Sequence-level annotations provide a scalable intermediate solution.", "method": "RMSL combines (1) one-class classification as initial anomaly-free learning, (2) multiple instance learning to capture sequence patterns, and (3) adaptive behavior-level self-training using confidence scores to refine hyper-sphere representations of normal behaviors within weakly labeled sequences.", "result": "Experiments show RMSL significantly enhances behavior-level detection performance by effectively distinguishing anomalies from normal behaviors using sequence-level supervision, outperforming unsupervised and conventional weakly supervised approaches.", "conclusion": "The RMSL framework demonstrates that weak sequence-level labels can effectively train models to detect behavior-level anomalies, overcoming limitations of annotation costs and ambiguity through innovative hyper-sphere refinement strategies."}}
{"id": "2508.11110", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11110", "abs": "https://arxiv.org/abs/2508.11110", "authors": ["Mukul Singh", "Gust Verbruggen", "Vu Le", "Sumit Gulwani"], "title": "Diffusion is a code repair operator and generator", "comment": "12 pages", "summary": "Code diffusion models generate code by iteratively removing noise from the\nlatent representation of a code snippet. During later steps of the diffusion\nprocess, when the code snippet has almost converged, differences between\ndiscrete representations of these snippets look like last-mile repairs applied\nto broken or incomplete code. We evaluate the extent to which this resemblance\ncan be exploited to leverage pre-trained code diffusion models for the problem\nof last-mile repair by considering two applications with significant potential.\nFirst, we can leverage the diffusion model for last-mile repair by adding noise\nto a broken code snippet and resuming the diffusion process. Second, we can\nleverage the diffusion model to generate arbitrary amount of training data for\nlast-mile repair tasks (that are computationally more efficient) by sampling an\nintermediate program (input) and the final program (output) from the diffusion\nprocess. We perform experiments on 3 domains (Python, Excel and PowerShell) to\nevaluate applications, as well as analyze properties.", "AI": {"tldr": "This paper explores using pre-trained code diffusion models for last-mile repair tasks through two applications: resuming diffusion on noisy broken code and generating synthetic training data. Experiments span Python, Excel, and PowerShell domains.", "motivation": "Code diffusion models iteratively denoise code latents, with later steps resembling small edits typical of last-mile repairs. Leveraging existing models avoids costly retraining while potentially improving repair efficiency.", "method": "1) Noise-based repair: Add noise to broken code then resume diffusion for repairs. 2) Data generation: Sample intermediate (input) and final (output) program states from diffusion process for synthetic training pairs. Evaluated across three code domains.", "result": "Demonstrated applications of diffusion models to last-mile repair through experiments in three domains, establishing their potential for 1) direct repair by resuming diffusion and 2) generating efficient synthetic training data for repair tasks.", "conclusion": "Pre-trained code diffusion models can effectively address last-mile repairs through noise resumption and synthetic data generation, offering computationally efficient solutions that repurpose existing model capabilities without task-specific training."}}
{"id": "2508.11495", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11495", "abs": "https://arxiv.org/abs/2508.11495", "authors": ["Jingnan Xu", "Leixia Wang", "Xiaofeng Meng"], "title": "KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value Estimation", "comment": null, "summary": "To protect privacy for data-collection-based services, local differential\nprivacy (LDP) is widely adopted due to its rigorous theoretical bound on\nprivacy loss. However, mistakes in complex theoretical analysis or subtle\nimplementation errors may undermine its practical guarantee. To address this,\nauditing is crucial to confirm that LDP protocols truly protect user data.\nHowever, existing auditing methods, though, mainly target machine learning and\nfederated learning tasks based on centralized differentially privacy (DP), with\nlimited attention to LDP. Moreover, the few studies on LDP auditing focus\nsolely on simple frequency estimation task for discrete data, leaving\ncorrelated key-value data - which requires both discrete frequency estimation\nfor keys and continuous mean estimation for values - unexplored.\n  To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based\nkey-value estimation mechanisms by estimating their empirical privacy lower\nbounds. Rather than traditional LDP auditing methods that relies on binary\noutput predictions, KV-Auditor estimates this lower bound by analyzing\nunbounded output distributions, supporting continuous data. Specifically, we\nclassify state-of-the-art LDP key-value mechanisms into interactive and\nnon-interactive types. For non-interactive mechanisms, we propose horizontal\nKV-Auditor for small domains with sufficient samples and vertical KV-Auditor\nfor large domains with limited samples. For interactive mechanisms, we design a\nsegmentation strategy to capture incremental privacy leakage across iterations.\nFinally, we perform extensive experiments to validate the effectiveness of our\napproach, offering insights for optimizing LDP-based key-value estimators.", "AI": {"tldr": "The paper proposes KV-Auditor, a framework for auditing local differential privacy (LDP) key-value estimation protocols by deriving empirical privacy lower bounds, addressing gaps in existing LDP auditing methods that focus mainly on discrete frequency estimation.", "motivation": "Existing LDP auditing methods are limited to discrete data frequency estimation, neglecting correlated key-value data requiring both discrete key and continuous value analysis, creating a need for robust auditing frameworks in practical implementations.", "method": "KV-Auditor analyzes unbounded output distributions instead of binary predictions, classifies key-value mechanisms into interactive/non-interactive categories, and introduces horizontal/vertical auditing for non-interactive cases (depending on domain size) and iterative segmentation for interactive ones.", "result": "Extensive experiments validated KV-Auditor's effectiveness in auditing LDP mechanisms, providing actionable insights to optimize key-value estimation protocols that handle both discrete keys and continuous values under LDP constraints.", "conclusion": "KV-Auditor enables comprehensive auditing of LDP key-value mechanisms by addressing domain-specific challenges through tailored empirical privacy bounds, offering a practical solution to verify real-world LDP implementations across diverse data types."}}
{"id": "2508.11126", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11126", "abs": "https://arxiv.org/abs/2508.11126", "authors": ["Huanting Wang", "Jingzhi Gong", "Huawei Zhang", "Zheng Wang"], "title": "AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities", "comment": null, "summary": "AI agentic programming is an emerging paradigm in which large language models\n(LLMs) autonomously plan, execute, and interact with external tools like\ncompilers, debuggers, and version control systems to iteratively perform\ncomplex software development tasks. Unlike conventional code generation tools,\nagentic systems are capable of decomposing high-level goals, coordinating\nmulti-step processes, and adapting their behavior based on intermediate\nfeedback. These capabilities are transforming the software development\npractice. As this emerging field evolves rapidly, there is a need to define its\nscope, consolidate its technical foundations, and identify open research\nchallenges. This survey provides a comprehensive and timely review of AI\nagentic programming. We introduce a taxonomy of agent behaviors and system\narchitectures, and examine core techniques including planning, memory and\ncontext management, tool integration, and execution monitoring. We also analyze\nexisting benchmarks and evaluation methodologies used to assess coding agent\nperformance. Our study identifies several key challenges, including limitations\nin handling long context, a lack of persistent memory across tasks, and\nconcerns around safety, alignment with user intent, and collaboration with\nhuman developers. We discuss emerging opportunities to improve the reliability,\nadaptability, and transparency of agentic systems. By synthesizing recent\nadvances and outlining future directions, this survey aims to provide a\nfoundation for research and development in building the next generation of\nintelligent and trustworthy AI coding agents.", "AI": {"tldr": "This survey reviews AI agentic programming, a paradigm using large language models to autonomously plan, execute, and iteratively refine software development tasks through interaction with external tools, examining techniques, challenges, and future directions.", "motivation": "The rapid emergence of AI agentic programming necessitates defining its scope, consolidating technical foundations, and identifying open research challenges to guide future development and ensure reliable, trustworthy systems.", "method": "The study provides a taxonomy of agent behaviors and architectures, analyzes core techniques (planning, memory/context management, tool integration, execution monitoring), evaluates benchmarks, and assesses current methodologies for coding agent performance.", "result": "Identified key challenges in long-context handling, memory persistence across tasks, and system safety/alignment with human collaboration issues; highlighted opportunities for improving reliability, adaptability, and transparency in agentic systems.", "conclusion": "This survey synthesizes advancements and outlines future research directions to establish foundational frameworks for next-generation AI agentic programming systems that are both capable and trustworthy."}}
{"id": "2508.11548", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11548", "abs": "https://arxiv.org/abs/2508.11548", "authors": ["Zhenhua Xu", "Xubin Yue", "Zhebo Wang", "Qichen Liu", "Xixiang Zhao", "Jingxuan Zhang", "Wenjun Zeng", "Wengpeng Xing", "Dezhang Kong", "Changting Lin", "Meng Han"], "title": "Copyright Protection for Large Language Models: A Survey of Methods, Challenges, and Trends", "comment": null, "summary": "Copyright protection for large language models is of critical importance,\ngiven their substantial development costs, proprietary value, and potential for\nmisuse. Existing surveys have predominantly focused on techniques for tracing\nLLM-generated content-namely, text watermarking-while a systematic exploration\nof methods for protecting the models themselves (i.e., model watermarking and\nmodel fingerprinting) remains absent. Moreover, the relationships and\ndistinctions among text watermarking, model watermarking, and model\nfingerprinting have not been comprehensively clarified. This work presents a\ncomprehensive survey of the current state of LLM copyright protection\ntechnologies, with a focus on model fingerprinting, covering the following\naspects: (1) clarifying the conceptual connection from text watermarking to\nmodel watermarking and fingerprinting, and adopting a unified terminology that\nincorporates model watermarking into the broader fingerprinting framework; (2)\nproviding an overview and comparison of diverse text watermarking techniques,\nhighlighting cases where such methods can function as model fingerprinting; (3)\nsystematically categorizing and comparing existing model fingerprinting\napproaches for LLM copyright protection; (4) presenting, for the first time,\ntechniques for fingerprint transfer and fingerprint removal; (5) summarizing\nevaluation metrics for model fingerprints, including effectiveness,\nharmlessness, robustness, stealthiness, and reliability; and (6) discussing\nopen challenges and future research directions. This survey aims to offer\nresearchers a thorough understanding of both text watermarking and model\nfingerprinting technologies in the era of LLMs, thereby fostering further\nadvances in protecting their intellectual property.", "AI": {"tldr": "This work provides a comprehensive survey of LLM copyright protection, bridging gaps in model fingerprinting research by clarifying terminology, analyzing techniques, and presenting novel approaches like fingerprint transfer/removal.", "motivation": "Need for protecting costly, proprietary LLMs from misuse; existing surveys focus only on text watermarking, neglecting model-level protection methods (watermarking/fingerprinting) despite their critical importance.", "method": "Systematic analysis covering (1) terminology unification of text/model watermarking/fingerprinting, (2) comparative review of text watermarking techniques usable for model fingerprinting, (3) categorization of LLM model fingerprinting approaches, (4) introduction of fingerprint transfer/removal techniques, (5) evaluation metrics framework.", "result": "First clarification of conceptual relationships between text and model fingerprinting; comprehensive comparison of techniques; novel methods for fingerprint manipulation (transfer, removal); structured evaluation criteria for effectiveness/robustness/stealthiness.", "conclusion": "Establishes foundational understanding of LLM copyright protection technologies to guide future research in model fingerprinting and enhance commercial IP protection for LLMs."}}
{"id": "2508.11147", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2508.11147", "abs": "https://arxiv.org/abs/2508.11147", "authors": ["Zhengquan Li", "Zhenhao Li", "Zishuo Ding"], "title": "From Feedback to Failure: Automated Android Performance Issue Reproduction", "comment": "10page, 8 figures", "summary": "Mobile application performance is a vital factor for user experience. Yet,\nperformance issues are notoriously difficult to detect within development\nenvironments, where their manifestations are often less conspicuous and\ndiagnosis proves more challenging. To address this limitation, we propose\nRevPerf, an advanced performance issue reproduction tool that leverages app\nreviews from Google Play to acquire pertinent information. RevPerf employs\nrelevant reviews and prompt engineering to enrich the original review with\nperformance issue details. An execution agent is then employed to generate and\nexecute commands to reproduce the issue. After executing all necessary steps,\nthe system incorporates multifaceted detection methods to identify performance\nissues by monitoring Android logs, GUI changes, and system resource utilization\nduring the reproduction process. Experimental results demonstrate that our\nproposed framework achieves a 70\\% success rate in reproducing performance\nissues on the dataset we constructed and manually validated.", "AI": {"tldr": "RevPerf is a tool that uses Google Play app reviews to reproduce performance issues, achieving a 70% success rate through enhanced review data, command execution, and multifaceted issue detection.", "motivation": "Performance issues in mobile apps are challenging to detect in development environments due to subtle manifestations and diagnostic difficulties, necessitating a reliable reproduction method.", "method": "RevPerf enriches app reviews with performance issue details using prompt engineering, then employs an execution agent to automate command generation and execution. It uses Android logs, GUI observation, and resource monitoring for detection.", "result": "The framework successfully reproduced performance issues in 70% of cases on a manually validated dataset, demonstrating its effectiveness in automating issue reproduction.", "conclusion": "RevPerf offers a practical framework for leveraging user-reported reviews to reproduce and detect performance issues, significantly improving diagnosis success rates in app development."}}
{"id": "2508.11563", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11563", "abs": "https://arxiv.org/abs/2508.11563", "authors": ["Nathaniel Moyer", "Charalampos Papamanthou", "Evgenios Kornaropoulos"], "title": "Pushing the Limits of Frequency Analysis in Leakage Abuse Attacks", "comment": null, "summary": "Searchable encryption (SE) is the most scalable cryptographic primitive for\nsearching on encrypted data. Typical SE constructions often allow\naccess-pattern leakage, revealing which encrypted records are retrieved in the\nserver's responses. All the known generic cryptanalyses assume either that the\nqueries are issued uniformly at random or that the attacker observes the\nsearch-pattern leakage. It remains unclear what can be reconstructed when using\nonly the access-pattern leakage and knowledge of the query distribution. In\nthis work, we focus on the cryptanalytic technique of frequency analysis in the\ncontext of leakage-abuse attacks on schemes that support encrypted range\nqueries. Frequency analysis matches the frequency of retrieval of an encrypted\nrecord with a plaintext value based on its probability of retrieval that\nfollows from the knowledge of the query distribution. We generalize this\nunderexplored cryptanalytic technique and introduce a generic attack framework\ncalled Leakage-Abuse via Matching (LAMA) that works even on high-dimensional\nencrypted data. We identify a parameterization of LAMA that brings frequency\nanalysis to its limit -- that is, we prove that there is no additional\nfrequency matching that an attacker can perform to refine the result.\nFurthermore, we show that our results hold for any class of convex queries, and\nnot just axis-aligned rectangles, which is the assumption in all other attacks\non range schemes. Using these results, we identify query distributions that\nmake frequency analysis challenging for the attacker and, thus, can act as a\nmitigation mechanism. Finally, we implement and benchmark LAMA and reconstruct,\nfor the first time, plaintext data from encrypted range queries spanning up to\nfour dimensions.", "AI": {"tldr": "The paper introduces a new framework, LAMA, for frequency analysis attacks on searchable encryption schemes with access-pattern leakage, demonstrating its effectiveness on high-dimensional data and identifying optimal query distributions for mitigation.", "motivation": "Existing cryptanalyses of encrypted range queries assume either uniformly random queries or search-pattern leakage. This work addresses the unknown impact of access-pattern leakage combined with query distribution knowledge, aiming to understand reconstruction limits and potential mitigations.", "method": "The authors generalize frequency analysis via leakage-abuse attacks using LAMA, a parameterized framework that optimally matches retrieval frequencies with plaintext probabilities. They prove LAMA's limits for any convex query class (not just axis-aligned) and implement it for multi-dimensional queries.", "result": "LAMA successfully reconstructs plaintext data from encrypted range queries in up to 4 dimensions. The analysis identifies query distributions (like uniform) that maximize entropy for encrypted range queries, complicating attacker reconstruction.", "conclusion": "LAMA provides a comprehensive model for frequency analysis attacks on SE schemes, proving theoretical limits and practical capabilities. The work shows how carefully designed query distributions can resist frequency-based reconstruction, offering immediate design guidance for more secure encrypted search systems."}}
{"id": "2508.11179", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11179", "abs": "https://arxiv.org/abs/2508.11179", "authors": ["Pei Liu", "Terry Zhuo", "Jiawei Deng", "Zhenchang Xing", "Qinghua Lu", "Xiaoning Du", "Hongyu Zhan"], "title": "PTMPicker: Facilitating Efficient Pretrained Model Selection for Application Developers", "comment": null, "summary": "The rapid emergence of pretrained models (PTMs) has attracted significant\nattention from both Deep Learning (DL) researchers and downstream application\ndevelopers. However, selecting appropriate PTMs remains challenging because\nexisting methods typically rely on keyword-based searches in which the keywords\nare often derived directly from function descriptions. This often fails to\nfully capture user intent and makes it difficult to identify suitable models\nwhen developers also consider factors such as bias mitigation, hardware\nrequirements, or license compliance. To address the limitations of\nkeyword-based model search, we propose PTMPicker to accurately identify\nsuitable PTMs. We first define a structured template composed of common and\nessential attributes for PTMs and then PTMPicker represents both candidate\nmodels and user-intended features (i.e., model search requests) in this unified\nformat. To determine whether candidate models satisfy user requirements, it\ncomputes embedding similarities for function-related attributes and uses\nwell-crafted prompts to evaluate special constraints such as license compliance\nand hardware requirements. We scraped a total of 543,949 pretrained models from\nHugging Face to prepare valid candidates for selection. PTMPicker then\nrepresented them in the predefined structured format by extracting their\nassociated descriptions. Guided by the extracted metadata, we synthesized a\ntotal of 15,207 model search requests with carefully designed prompts, as no\nsuch search requests are readily available. Experiments on the curated PTM\ndataset and the synthesized model search requests show that PTMPicker can help\nusers effectively identify models,with 85% of the sampled requests successfully\nlocating appropriate PTMs within the top-10 ranked candidates.", "AI": {"tldr": "PTMPicker addresses the limitations of keyword-based PTM selection by using a structured template and embedding similarities alongside constraint evaluations, achieving 85% top-10 accuracy with 15,207 synthesized search requests and 543,949 Hugging Face models.", "motivation": "Traditional keyword-based PTM searches fail to capture user intent beyond function descriptions due to limited consideration of bias mitigation, hardware requirements, and license compliance in selection criteria.", "method": "PTMPicker employs a structured template for PTM representation, embedding similarities for function-related attributes, and custom prompts to evaluate special constraints (e.g., license, hardware). 543,949 models were scraped from Hugging Face and 15,207 search requests synthesized using metadata and prompting techniques.", "result": "Experimental evaluation on the curated PTM dataset and synthetic search requests showed PTMPicker successfully identifies suitable models for 85% of sampled requests within the top-10 ranked candidates.", "conclusion": "PTMPicker's structured approach improves PTM selection effectiveness by bridging the gap between model descriptions and nuanced user requirements, including functional and non-functional constraints, with strong empirical validation."}}
{"id": "2508.11575", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11575", "abs": "https://arxiv.org/abs/2508.11575", "authors": ["Nges Brian Njungle", "Michel A. Kinsy"], "title": "Activate Me!: Designing Efficient Activation Functions for Privacy-Preserving Machine Learning with Fully Homomorphic Encryption", "comment": null, "summary": "The growing adoption of machine learning in sensitive areas such as\nhealthcare and defense introduces significant privacy and security challenges.\nThese domains demand robust data protection, as models depend on large volumes\nof sensitive information for both training and inference. Fully Homomorphic\nEncryption (FHE) presents a compelling solution by enabling computations\ndirectly on encrypted data, maintaining confidentiality across the entire\nmachine learning workflow. However, FHE inherently supports only linear\noperations, making it difficult to implement non-linear activation functions,\nessential components of modern neural networks. This work focuses on designing,\nimplementing, and evaluating activation functions tailored for FHE-based\nmachine learning. We investigate two commonly used functions: the Square\nfunction and Rectified Linear Unit (ReLU), using LeNet-5 and ResNet-20\narchitectures with the CKKS scheme from the OpenFHE library. For ReLU, we\nassess two methods: a conventional low-degree polynomial approximation and a\nnovel scheme-switching technique that securely evaluates ReLU under FHE\nconstraints. Our findings show that the Square function performs well in\nshallow networks like LeNet-5, achieving 99.4% accuracy with 128 seconds per\nimage. In contrast, deeper models like ResNet-20 benefit more from ReLU. The\npolynomial approximation yields 83.8% accuracy with 1,145 seconds per image,\nwhile our scheme-switching method improves accuracy to 89.8%, albeit with a\nlonger inference time of 1,697 seconds. These results underscore a critical\ntrade-off in FHE-based ML: faster activation functions often reduce accuracy,\nwhereas those preserving accuracy demand greater computational resources.", "AI": {"tldr": "This paper designs and evaluates FHE-compatible activation functions for neural networks, demonstrating trade-offs between accuracy and inference time in encrypted ML models.", "motivation": "Machine learning in sensitive domains requires privacy-preserving techniques like FHE, which supports only linear operations but necessitates non-linear activation functions for modern neural networks.", "method": "The authors implemented Square and ReLU functions on LeNet-5 and ResNet-20 using OpenFHE's CKKS scheme, evaluating a traditional polynomial approximation versus a novel scheme-switching approach for ReLU.", "result": "Square achieved 99.4% accuracy at 128s/image for LeNet-5; ReLU using polynomial approximation yielded 83.8% accuracy at 1,145s/image for ResNet-20, while scheme-switching improved accuracy to 89.8% at 1,697s/image.", "conclusion": "The study reveals a critical efficiency-accuracy trade-off in FHE-based ML, with shallow networks favoring fast Square functions and deeper models requiring careful selection of ReLU implementations to optimize performance."}}
{"id": "2508.11222", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.11222", "abs": "https://arxiv.org/abs/2508.11222", "authors": ["Haonan Zhang", "Dongxia Wang", "Yi Liu", "Kexin Chen", "Jiashui Wang", "Xinlei Ying", "Long Liu", "Wenhai Wang"], "title": "ORFuzz: Fuzzing the \"Other Side\" of LLM Safety -- Testing Over-Refusal", "comment": null, "summary": "Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously\nrejecting benign queries due to overly conservative safety measures - a\ncritical functional flaw that undermines their reliability and usability.\nCurrent methods for testing this behavior are demonstrably inadequate,\nsuffering from flawed benchmarks and limited test generation capabilities, as\nhighlighted by our empirical user study. To the best of our knowledge, this\npaper introduces the first evolutionary testing framework, ORFuzz, for the\nsystematic detection and analysis of LLM over-refusals. ORFuzz uniquely\nintegrates three core components: (1) safety category-aware seed selection for\ncomprehensive test coverage, (2) adaptive mutator optimization using reasoning\nLLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge\nmodel validated to accurately reflect user perception of toxicity and refusal.\nOur extensive evaluations demonstrate that ORFuzz generates diverse, validated\nover-refusal instances at a rate (6.98% average) more than double that of\nleading baselines, effectively uncovering vulnerabilities. Furthermore,\nORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly\ntransferable test cases that achieves a superior 63.56% average over-refusal\nrate across 10 diverse LLMs, significantly outperforming existing datasets.\nORFuzz and ORFuzzSet provide a robust automated testing framework and a\nvaluable community resource, paving the way for developing more reliable and\ntrustworthy LLM-based software systems.", "AI": {"tldr": "This paper introduces ORFuzz, the first evolutionary testing framework for detecting LLM over-refusals, which improves testing by 6.98% and creates a benchmark with 1,855 test cases achieving 63.56% over-refusal rate across models.", "motivation": "Current testing methods for LLM over-refusals are inadequate due to flawed benchmarks and limited test generation, evidenced by a user study highlighting issues in validity and conservativeness.", "method": "ORFuzz integrates three components: safety category-aware seed selection for coverage, adaptive mutator optimization using reasoning LLMs for effective test generation, and OR-Judge, a human-aligned model for validating test cases.", "result": "ORFuzz achieved a 6.98% average over-refusal detection rate (double leading baselines) and generates the ORFuzzSet benchmark with 1,855 test cases, achieving 63.56% average over-refusal rate across 10 LLMs.", "conclusion": "ORFuzz and ORFuzzSet provide a robust framework and community resource for improving the reliability and trustworthiness of LLM-based systems through systematic over-refusal detection and analysis."}}
{"id": "2508.11599", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11599", "abs": "https://arxiv.org/abs/2508.11599", "authors": ["Zhihao Li", "Zimo Ji", "Tao Zheng", "Hao Ren", "Xiao Lan"], "title": "CryptoScope: Utilizing Large Language Models for Automated Cryptographic Logic Vulnerability Detection", "comment": null, "summary": "Cryptographic algorithms are fundamental to modern security, yet their\nimplementations frequently harbor subtle logic flaws that are hard to detect.\nWe introduce CryptoScope, a novel framework for automated cryptographic\nvulnerability detection powered by Large Language Models (LLMs). CryptoScope\ncombines Chain-of-Thought (CoT) prompting with Retrieval-Augmented Generation\n(RAG), guided by a curated cryptographic knowledge base containing over 12,000\nentries. We evaluate CryptoScope on LLM-CLVA, a benchmark of 92 cases primarily\nderived from real-world CVE vulnerabilities, complemented by cryptographic\nchallenges from major Capture The Flag (CTF) competitions and synthetic\nexamples across 11 programming languages. CryptoScope consistently improves\nperformance over strong LLM baselines, boosting DeepSeek-V3 by 11.62%,\nGPT-4o-mini by 20.28%, and GLM-4-Flash by 28.69%. Additionally, it identifies 9\npreviously undisclosed flaws in widely used open-source cryptographic projects.", "AI": {"tldr": "CryptoScope is an LLM-based framework that enhances cryptographic vulnerability detection through CoT prompting and RAG, outperforming existing models and discovering 9 new flaws in open-source projects.", "motivation": "Cryptographic algorithm implementations often contain hard-to-detect logic flaws, necessitating automated detection methods to improve security.", "method": "CryptoScope integrates Chain-of-Thought (CoT) prompting with Retrieval-Augmented Generation (RAG), leveraging a cryptographic knowledge base with 12,000+ entries to analyze code across 11 programming languages.", "result": "Achieved 11.62% improvement over DeepSeek-V3, 20.28% over GPT-4o-mini, and 28.69% over GLM-4-Flash on the LLM-CLVA benchmark (92 real-world/synthetic CVE cases). Identified 9 undisclosed vulnerabilities in popular open-source cryptographic projects.", "conclusion": "CryptoScope demonstrates significant performance gains over state-of-the-art LLM baselines in detecting cryptographic vulnerabilities, validating its effectiveness through both benchmark results and real-world security discoveries."}}
{"id": "2508.11257", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11257", "abs": "https://arxiv.org/abs/2508.11257", "authors": ["Marc Pavel", "Nenad Petrovic", "Lukasz Mazur", "Vahid Zolfaghari", "Fengjunjie Pan", "Alois Knoll"], "title": "Hallucination in LLM-Based Code Generation: An Automotive Case Study", "comment": null, "summary": "Large Language Models (LLMs) have shown significant potential in automating\ncode generation tasks offering new opportunities across software engineering\ndomains. However, their practical application remains limited due to\nhallucinations - outputs that appear plausible but are factually incorrect,\nunverifiable or nonsensical. This paper investigates hallucination phenomena in\nthe context of code generation with a specific focus on the automotive domain.\nA case study is presented that evaluates multiple code LLMs for three different\nprompting complexities ranging from a minimal one-liner prompt to a prompt with\nCovesa Vehicle Signal Specifications (VSS) as additional context and finally to\na prompt with an additional code skeleton. The evaluation reveals a high\nfrequency of syntax violations, invalid reference errors and API knowledge\nconflicts in state-of-the-art models GPT-4.1, Codex and GPT-4o. Among the\nevaluated models, only GPT-4.1 and GPT-4o were able to produce a correct\nsolution when given the most context-rich prompt. Simpler prompting strategies\nfailed to yield a working result, even after multiple refinement iterations.\nThese findings highlight the need for effective mitigation techniques to ensure\nthe safe and reliable use of LLM generated code, especially in safety-critical\ndomains such as automotive software systems.", "AI": {"tldr": "This paper investigates hallucinations in code generation for the automotive domain, comparing various LLMs under different prompting strategies and highlighting the need for mitigation techniques.", "motivation": "Large Language Models (LLMs) face limitations in code generation due to hallucinations, particularly in safety-critical domains like automotive software systems.", "method": "The authors conducted a case study evaluating GPT-4.1, Codex, and GPT-4o using three prompting complexities: one-liner prompts, VSS-context prompts, and context-rich prompts with code skeletons.", "result": "GPT-4.1 and GPT-4o produced correct solutions only with the most context-rich prompts. Simpler strategies failed despite refinement, revealing common issues like syntax violations, invalid references, and API conflicts.", "conclusion": "The study underscores the necessity of reliable mitigation techniques to ensure safe use of LLM-generated code in critical domains, as current models struggle with context-poor prompts and real-world complexity."}}
{"id": "2508.11305", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11305", "abs": "https://arxiv.org/abs/2508.11305", "authors": ["Xin Wang", "Zhenhao Li", "Zishuo Ding"], "title": "Defects4Log: Benchmarking LLMs for Logging Code Defect Detection and Reasoning", "comment": null, "summary": "Logging code is written by developers to capture system runtime behavior and\nplays a vital role in debugging, performance analysis, and system monitoring.\nHowever, defects in logging code can undermine the usefulness of logs and lead\nto misinterpretations. Although prior work has identified several logging\ndefect patterns and provided valuable insights into logging practices, these\nstudies often focus on a narrow range of defect patterns derived from limited\nsources (e.g., commit histories) and lack a systematic and comprehensive\nanalysis. Moreover, large language models (LLMs) have demonstrated promising\ngeneralization and reasoning capabilities across a variety of code-related\ntasks, yet their potential for detecting logging code defects remains largely\nunexplored.\n  In this paper, we derive a comprehensive taxonomy of logging code defects,\nwhich encompasses seven logging code defect patterns with 14 detailed\nscenarios. We further construct a benchmark dataset, \\dataset, consisting of\n164 developer-verified real-world logging defects. Then we propose an automated\nframework that leverages various prompting strategies and contextual\ninformation to evaluate LLMs' capability in detecting and reasoning logging\ncode defects. Experimental results reveal that LLMs generally struggle to\naccurately detect and reason logging code defects based on the source code\nonly. However, incorporating proper knowledge (e.g., detailed scenarios of\ndefect patterns) can lead to 10.9\\% improvement in detection accuracy. Overall,\nour findings provide actionable guidance for practitioners to avoid common\ndefect patterns and establish a foundation for improving LLM-based reasoning in\nlogging code defect detection.", "AI": {"tldr": "The paper presents a comprehensive study on logging code defects, introducing a taxonomy of seven defect patterns with 14 scenarios, a benchmark dataset of 164 real-world defects, and an automated LLM-based detection framework. It highlights that LLMs improve defect detection accuracy (10.9%) when domain-specific knowledge is incorporated.", "motivation": "Previous research on logging defects has prioritized limited patterns and source code-based analysis, while LLMs' potential for detecting such defects remains underexplored despite their generalization capabilities in code tasks.", "method": "The work derives a defect taxonomy through systematic analysis, constructs a developer-verified dataset, and evaluates LLM performance using prompting strategies combined with contextual defect pattern knowledge.", "result": "LLMs achieve improved detection accuracy (10.9% increase) when provided with detailed defect pattern scenarios, but perform poorly with source code alone. The taxonomy and dataset are validated through experimental analysis.", "conclusion": "The proposed taxonomy and benchmark enable practitioners to avoid common logging defects while improving LLM-based detection frameworks. Future work should further integrate domain-specific knowledge to enhance LLM performance in this domain."}}
{"id": "2508.11468", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11468", "abs": "https://arxiv.org/abs/2508.11468", "authors": ["Zhihao Gong", "Zeyu Sun", "Dong Huang", "Qingyuan Liang", "Jie M. Zhang", "Dan Hao"], "title": "TRACY: Benchmarking Execution Efficiency of LLM-Based Code Translation", "comment": null, "summary": "Automatic code translation is a fundamental task in modern software\ndevelopment. While the advent of Large Language Models (LLMs) has significantly\nimproved the correctness of code translation, the critical dimension of\nexecution efficiency remains overlooked. To address this gap, we introduce\nTRACY, the first comprehensive benchmark designed to evaluate the execution\nefficiency of LLM-translated code. TRACY is constructed through an LLM-driven\ntwo-stage pipeline: an initial stage generates a suite of stress tests to\namplify performance differences, followed by an efficiency-oriented task\npruning stage that isolates the efficiency-distinguishing tasks. The resulting\nbenchmark comprises 1,011 code translation tasks across C++, Java, and Python,\neach accompanied by an average of 22.1 verified reference translations and 10\ncomputationally demanding tests. Our extensive evaluation of 26 representative\nLLMs reveals that even top-tier LLMs struggle to consistently produce efficient\ncode translations. For instance, Claude-4-think, the leading model for\ncorrectness, ranks eighth overall when time efficiency is taken into account,\nsurpassed by several smaller open-source models. We further pinpoint that\nalgorithmic flaws and improper resource handling are the most detrimental,\ncausing a median time slowdown of 5.6$\\times$ and memory increase of\n12.0$\\times$, respectively. Our work underscores the necessity of jointly\noptimizing for correctness and efficiency in future LLM-based code translation.", "AI": {"tldr": "TRACY is a new benchmark evaluating execution efficiency of LLM-translated code, revealing that top models struggle with efficiency despite good correctness. It identifies algorithmic flaws and resource handling as major issues.", "motivation": "Current LLM-based code translation prioritizes correctness over execution efficiency, leaving a critical gap in performance evaluation and optimization.", "method": "LLM-driven two-stage pipeline: stress test generation to amplify performance differences, followed by efficiency-oriented pruning to isolate distinguishing tasks, creating 1,011 C++/Java/Python tasks with 22.1 verified references and 10 demanding tests each.", "result": "Top-tier LLMs fail to produce consistent efficient code (Claude-4-think ranks 8th in time efficiency). Algorithmic flaws cause 5.6\u00d7 median slowdown, improper resource handling leads to 12.0\u00d7 memory increase.", "conclusion": "Future LLM development must address both correctness and efficiency in code translation, with TRACY providing a foundational benchmark for this dual metric evaluation."}}
{"id": "2508.11571", "categories": ["cs.SE", "cs.DM"], "pdf": "https://arxiv.org/pdf/2508.11571", "abs": "https://arxiv.org/abs/2508.11571", "authors": ["Alexander Bakhtin"], "title": "Temporal Network Analysis of Microservice Architectural Degradation", "comment": null, "summary": "Microservice architecture can be modeled as a network of microservices making\ncalls to each other, commonly known as the service dependency graph. Network\nScience can provide methods to study such networks. In particular, temporal\nnetwork analysis is a branch of Network Science that analyzes networks evolving\nwith time. In microservice systems, temporal networks can arise if we examine\nthe architecture of the system across releases or monitor a deployed system\nusing tracing.\n  In this research summary paper, I discuss the challenges in obtaining\ntemporal networks from microservice systems and analyzing them with the\ntemporal network methods. In particular, the most complete temporal network\nthat we could obtain contains 7 time instances and 42 microservices, which\nlimits the potential analysis that could be applied.", "AI": {"tldr": "This paper explores the application of temporal network analysis to microservice architectures, highlighting challenges in data collection and analysis limitations due to small-scale datasets.", "motivation": "Microservice systems evolve over time, necessitating temporal network methods to capture dynamic dependencies for effective monitoring and development.", "method": "The author models microservice architectures as temporal service dependency graphs using tracing data and release history, then examines analytical challenges in such networks.", "result": "A case study demonstrates the feasibility of the approach but reveals significant limitations due to small network size (7 time instances, 42 microservices).", "conclusion": "Temporal network analysis offers valuable insights into microservice systems, but requires larger-scale datasets to overcome current analytical limitations."}}
