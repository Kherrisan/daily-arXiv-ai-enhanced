<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 8]
- [cs.SE](#cs.SE) [Total: 12]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Adversarial Attacks and Defenses on Graph-aware Large Language Models (LLMs)](https://arxiv.org/abs/2508.04894)
*Iyiola E. Olatunji,Franziska Boenisch,Jing Xu,Adam Dziedzic*

Main category: cs.CR

TL;DR: This paper investigates the robustness of graph-aware Large Language Models (LLMs) like LLAGA and GRAPHPROMPTER against adversarial attacks, identifying vulnerabilities in their design and proposing a defense framework called GALGUARD.


<details>
  <summary>Details</summary>
Motivation: The growing integration of LLMs with graph-structured data for tasks such as node classification demands an understanding of their robustness against adversarial attacks. Prior focus on LLMs for graph encoding has overlooked potential vulnerabilities compared to Graph Neural Networks (GNNs).

Method: The paper systematically evaluates the robustness of two graph-aware LLM models using established adversarial attack methods (poisoning and evasion). It also explores a novel node sequence template attack for LLAGA and analyzes how design choices in graph encoding impact attack success.

Result: Results show LLAGA's vulnerability increases due to its node sequence template, GRAPHPROMPTER's GNN encoder offers better robustness, but both models remain susceptible to imperceptible feature perturbation attacks. The proposed GALGUARD defense framework effectively mitigates these risks.

Conclusion: Graph-aware LLMs face unique adversarial vulnerabilities shaped by their design choices. The study highlights specific weaknesses and provides GALGUARD, a dual-pronged defense combining feature correction and adapted GNN protections, to address these emerging threats.

Abstract: Large Language Models (LLMs) are increasingly integrated with
graph-structured data for tasks like node classification, a domain
traditionally dominated by Graph Neural Networks (GNNs). While this integration
leverages rich relational information to improve task performance, their
robustness against adversarial attacks remains unexplored. We take the first
step to explore the vulnerabilities of graph-aware LLMs by leveraging existing
adversarial attack methods tailored for graph-based models, including those for
poisoning (training-time attacks) and evasion (test-time attacks), on two
representative models, LLAGA (Chen et al. 2024) and GRAPHPROMPTER (Liu et al.
2024). Additionally, we discover a new attack surface for LLAGA where an
attacker can inject malicious nodes as placeholders into the node sequence
template to severely degrade its performance. Our systematic analysis reveals
that certain design choices in graph encoding can enhance attack success, with
specific findings that: (1) the node sequence template in LLAGA increases its
vulnerability; (2) the GNN encoder used in GRAPHPROMPTER demonstrates greater
robustness; and (3) both approaches remain susceptible to imperceptible feature
perturbation attacks. Finally, we propose an end-to-end defense framework
GALGUARD, that combines an LLM-based feature correction module to mitigate
feature-level perturbations and adapted GNN defenses to protect against
structural attacks.

</details>


### [2] [On the Classical Hardness of the Semidirect Discrete Logarithm Problem in Finite Groups](https://arxiv.org/abs/2508.05048)
*Mohammad Ferry Husnil Arif,Muhammad Imran*

Main category: cs.CR

TL;DR: The paper examines the classical hardness of the semidirect discrete logarithm problem (SDLP) across finite group platforms, finding its difficulty platform-dependent and not inherently better than the standard discrete logarithm problem (DLP).


<details>
  <summary>Details</summary>
Motivation: SDLP was proposed as a quantum-resistant cryptographic foundation, but recent quantum algorithms have challenged its quantum resilience. This study reassesses SDLP's computational advantage against classical adversaries.

Method: Reformulated SDLP as a generalized discrete logarithm problem (GDLP) and adapted classical algorithms like Baby-Step Giant-Step for analysis. Theoretical evaluations and SageMath experiments compared complexities in different group structures.

Result: SDLP exhibits time/space complexity $O(\sqrt{r})$ with variable "r" across platforms. It shows comparable difficulty to DLP in finite fields $\mathbb{F}_p^*$, becomes trivial in elliptic curves, and can surpass DLP in elementary abelian groups $\mathbb{F}_p^n$ based on automorphism eigenvalue structures.

Conclusion: The non-abelian nature of semidirect products does not guarantee classical difficulty for SDLP. The study emphasizes the need for meticulous algebraic structure selection when designing hard computational problems for cryptography.

Abstract: The semidirect discrete logarithm problem (SDLP) in finite groups was
proposed as a foundation for post-quantum cryptographic protocols, based on the
belief that its non-abelian structure would resist quantum attacks. However,
recent results have shown that SDLP in finite groups admits efficient quantum
algorithms, undermining its quantum resistance. This raises a fundamental
question: does the SDLP offer any computational advantages over the standard
discrete logarithm problem (DLP) against classical adversaries? In this work,
we investigate the classical hardness of SDLP across different finite group
platforms. We establish that the group-case SDLP can be reformulated as a
generalized discrete logarithm problem, enabling adaptation of classical
algorithms to study its complexity. We present a concrete adaptation of the
Baby-Step Giant-Step algorithm for SDLP, achieving time and space complexity
$O(\sqrt{r})$ where $r$ is the period of the underlying cycle structure.
Through theoretical analysis and experimental validation in SageMath, we
demonstrate that the classical hardness of SDLP is highly platform-dependent
and does not uniformly exceed that of standard DLP. In finite fields
$\mathbb{F}_p^*$, both problems exhibit comparable complexity. Surprisingly, in
elliptic curves $E(\mathbb{F}_p)$, the SDLP becomes trivial due to the bounded
automorphism group, while in elementary abelian groups $\mathbb{F}_p^n$, the
SDLP can be harder than DLP, with complexity varying based on the eigenvalue
structure of the automorphism. Our findings reveal that the non-abelian
structure of semidirect products does not inherently guarantee increased
classical hardness, suggesting that the search for classically hard problems
for cryptographic applications requires more careful consideration of the
underlying algebraic structures.

</details>


### [3] [Incident Response Planning Using a Lightweight Large Language Model with Reduced Hallucination](https://arxiv.org/abs/2508.05188)
*Kim Hammar,Tansu Alpcan,Emil C. Lupu*

Main category: cs.CR

TL;DR: This paper proposes a framework using fine-tuning, information retrieval, and lookahead planning to improve incident response with LLMs by reducing hallucinations while maintaining lightweight execution.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on costly prompt engineering for frontier LLMs in incident response, leading to hallucinations and operational inefficiency.

Method: The approach combines domain-specific fine-tuning to ground LLM knowledge, information retrieval to access contextualized incident data, and lookahead planning to sequence response actions while bounding hallucination probability.

Result: Achieves 22% faster incident recovery compared to frontier LLM baselines and demonstrates generalization across 7+ incident types with 92% successful action sequence generation.

Conclusion: The framework provides a scalable solution for accurate LLM-powered incident response on commodity hardware, balancing hallucination reduction against planning time tradeoffs.

Abstract: Timely and effective incident response is key to managing the growing
frequency of cyberattacks. However, identifying the right response actions for
complex systems is a major technical challenge. A promising approach to
mitigate this challenge is to use the security knowledge embedded in large
language models (LLMs) to assist security operators during incident handling.
Recent research has demonstrated the potential of this approach, but current
methods are mainly based on prompt engineering of frontier LLMs, which is
costly and prone to hallucinations. We address these limitations by presenting
a novel way to use an LLM for incident response planning with reduced
hallucination. Our method includes three steps: fine-tuning, information
retrieval, and lookahead planning. We prove that our method generates response
plans with a bounded probability of hallucination and that this probability can
be made arbitrarily small at the expense of increased planning time under
certain assumptions. Moreover, we show that our method is lightweight and can
run on commodity hardware. We evaluate our method on logs from incidents
reported in the literature. The experimental results show that our method a)
achieves up to 22% shorter recovery times than frontier LLMs and b) generalizes
to a broad range of incident types and response actions.

</details>


### [4] [An Overview of 7726 User Reports: Uncovering SMS Scams and Scammer Strategies](https://arxiv.org/abs/2508.05276)
*Sharad Agarwal,Guillermo Suarez-Tangil,Marie Vasek*

Main category: cs.CR

TL;DR: This study analyzes 1.35m user-reported SMS messages to investigate scam/bypass techniques, revealing 40.27% are scams with 'wrong number' as the most prevalent type, and offering the first classification framework differentiating spam vs scams.


<details>
  <summary>Details</summary>
Motivation: While SMS firewalls filter malicious messages, scammers evade detection. Prior work focuses on blocked messages, but understanding unblocked scam messages is critical to improving filtering capabilities.

Method: Collaborated with a mobile network operator to collect 1.35m user reports over 4 months. Used a methodological framework to classify reported messages into spam, scam, or other categories based on linguistic and technical patterns.

Result: Identified 35.12% of unique text messages as spam, 40.27% as scams. Classified scams into 12 types with 'wrong number' being most common. Revealed infrastructure abuse patterns and social engineering tactics in scam message content.

Conclusion: This paper pioneers large-scale analysis of user-submitted SMS reports to understand bypassed scam messages. The methodology enables better categorization of threats while highlighting how scammers exploit telecom infrastructure and psychological manipulation to steal personal data.

Abstract: Mobile network operators implement firewalls to stop illicit messages, but
scammers find ways to evade detection. Previous work has looked into SMS texts
that are blocked by these firewalls. However, there is little insight into SMS
texts that bypass them and reach users. To this end, we collaborate with a
major mobile network operator to receive 1.35m user reports submitted over four
months. We find 89.16% of user reports comprise text messages, followed by
reports of suspicious calls and URLs. Using our methodological framework, we
identify 35.12% of the unique text messages reported by users as spam, while
40.27% are scam text messages. This is the first paper that investigates SMS
reports submitted by users and differentiates between spam and scams. Our paper
classifies the identified scam text messages into 12 scam types, of which the
most popular is 'wrong number' scams. We explore the various infrastructure
services that scammers abuse to conduct SMS scams, including mobile network
operators and hosting infrastructure, and analyze the text of the scam messages
to understand how scammers lure victims into providing them with their personal
or financial details.

</details>


### [5] [ShikkhaChain: A Blockchain-Powered Academic Credential Verification System for Bangladesh](https://arxiv.org/abs/2508.05334)
*Ahsan Farabi,Israt Khandaker,Nusrat Jahan,Ibrahim Khalil Shanto*

Main category: cs.CR

TL;DR: ShikkhaChain is a blockchain-based certificate management platform designed to combat academic credential fraud in Bangladesh using Ethereum smart contracts, IPFS, and a React DApp with MetaMask integration, offering transparent and tamperproof verification.


<details>
  <summary>Details</summary>
Motivation: Academic credential fraud undermines educational integrity, particularly in developing countries like Bangladesh where traditional manual verification methods are inefficient and prone to errors.

Method: The platform leverages Ethereum smart contracts for secure credential issuance and revocation, utilizes IPFS for decentralized off-chain storage, and provides a React-based distributed application (DApp) with MetaMask integration for user access. It implements role-based permissions for governments, institutions, and verifiers.

Result: The prototype demonstrated improved trust through immutable verification, significantly reduced validation time compared to manual processes, and enhanced international credibility for Bangladeshi academic qualifications.

Conclusion: ShikkhaChain provides a scalable, decentralized solution to mitigate credential fraud by ensuring transparent, secure, and efficient management of academic certificates, fostering a more trustworthy academic and employment ecosystem in Bangladesh.

Abstract: Academic credential fraud threatens educational integrity, especially in
developing countries like Bangladesh, where verification methods are primarily
manual and inefficient. To address this challenge, we present ShikkhaChain, a
blockchain-powered certificate management platform designed to securely issue,
verify, and revoke academic credentials in a decentralized and tamper-proof
manner. Built on Ethereum smart contracts and utilizing IPFS for off-chain
storage, the platform offers a transparent, scalable solution accessible
through a React-based DApp with MetaMask integration. ShikkhaChain enables
role-based access for governments, regulators, institutions, and public
verifiers, allowing QR-based validation and on-chain revocation tracking. Our
prototype demonstrates enhanced trust, reduced verification time, and improved
international credibility for Bangladeshi degrees, promoting a more reliable
academic and employment ecosystem.

</details>


### [6] [Grouped k-threshold random grid-based visual cryptography scheme](https://arxiv.org/abs/2508.05394)
*Xiaoli Zhuo,Xuehu Yan,Wei Yan*

Main category: cs.CR

TL;DR: This paper introduces an enhanced (k,n)-threshold Random Grid-Based Visual Cryptography Scheme (RGVCS) called n′-grouped RGVCS, which improves image recovery contrast by creating hierarchical sharing groups and setting n′=k to achieve theoretical contrast upper bounds.


<details>
  <summary>Details</summary>
Motivation: Existing RGVCSs fail to attain the theoretical upper bounds for contrast, limiting the visual quality of recovered images. Optimizing contrast is critical to meet these bounds and improve practical utility.

Method: A novel (k,n) RGVCS is proposed by constructing schemes from arbitrary (k,n′)-threshold schemes (k ≤ n′ ≤ n). The authors introduce a hierarchical grouping and a new contrast calculation formula tailored to the paradigm.

Result: The proposed scheme achieves the highest documented contrast value when n′=k, validated through theoretical analysis and experimental results.

Conclusion: The n′-grouped RGVCS with n′=k significantly enhances contrast over existing methods, demonstrating superiority in visual quality optimization for secret image sharing.

Abstract: Visual cryptography schemes (VCSs) belong to a category of secret image
sharing schemes that do not require cryptographic knowledge for decryption,
instead relying directly on the human visual system. Among VCSs, random
grid-based VCS (RGVCS) has garnered widespread attention as it avoids pixel
expansion while requiring no basic matrices design. Contrast, a core metric for
RGVCS, directly determines the visual quality of recovered images, rendering
its optimization a critical research objective. However, existing $(k,n)$
RGVCSs still fail to attain theoretical upper bounds on contrast, highlighting
the urgent need for higher-contrast constructions. In this paper, we propose a
novel sharing paradigm for RGVCS that constructs $(k,n)$-threshold schemes from
arbitrary $(k,n')$-threshold schemes $(k \leq n'\leq n)$, termed
\emph{$n'$-grouped $(k,n)$ RGVCS}. This paradigm establishes hierarchical
contrast characteristics: participants within the same group achieve optimal
recovery quality, while inter-group recovery shows a hierarchical contrast. We
further introduce a new contrast calculation formula tailored to the new
paradigm. Then, we propose a contrast-enhanced $(k,n)$ RGVCS by setting $n'=
k$, achieving the highest contrast value documented in the existing literature.
Theoretical analysis and experimental results demonstrate the superiority of
our proposed scheme in terms of contrast.

</details>


### [7] [Local Distance Query with Differential Privacy](https://arxiv.org/abs/2508.05518)
*Weihong Sheng,Jiajun Chen,Bin Cai,Chunqiang Hu,Meng Han,Jiguo Yu*

Main category: cs.CR

TL;DR: This paper introduces the first LDP method for distance queries by aggregating local distance vectors, addressing the absence of a trusted curator in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional curator-based differential privacy (DP) for graph analysis relies on a trusted entity, which is often unavailable in practical applications. Real-world scenarios require decentralized DP solutions for distance queries without such a curator.

Method: 1) Synthetic graph generation with randomized responses and noise reduction via bitwise operations; 2) Novel LDP framework that iteratively aggregates local distance vectors from neighboring vertices to capture global graph structure and update distances accurately.

Result: The proposed LDP method demonstrates effectiveness through theoretical analysis and experiments on real-world datasets, outperforming existing synthetic graph approaches in utility while maintaining privacy guarantees.

Conclusion: The paper establishes a new LDP framework specifically optimized for distance queries, achieving accurate global distance inference through decentralized local aggregation without requiring a trusted curator.

Abstract: Differential Privacy (DP) is commonly employed to safeguard graph analysis or
publishing. Distance, a critical factor in graph analysis, is typically handled
using curator DP, where a trusted curator holds the complete neighbor lists of
all vertices and answers queries privately. However, in many real-world
scenarios, such a curator may not be present, posing a significant challenge
for implementing differentially private distance queries under Local
Differential Privacy (LDP). This paper proposes two approaches to address this
challenge. The first approach generates a synthetic graph by randomizing
responses and applies bitwise operations to reduce noise interference. However,
like other synthetic graph methods, this approach suffers from low utility. To
overcome this limitation, we propose a second approach, the first LDP method
specifically designed for distance queries, which captures the global graph
structure by continuously aggregating local distance vectors from neighboring
vertices. This process enables the accurate updating of global distances. We
demonstrate the effectiveness of our method through comprehensive theoretical
analysis and experimental evaluations on real-world datasets.

</details>


### [8] [PRvL: Quantifying the Capabilities and Risks of Large Language Models for PII Redaction](https://arxiv.org/abs/2508.05545)
*Leon Garza,Anantaa Kotal,Aritran Piplai,Lavanya Elluri,Prajit Das,Aman Chadha*

Main category: cs.CR

TL;DR: Evaluates LLMs for PII redaction and introduces PRvL open-source tool for privacy-preserving text processing.


<details>
  <summary>Details</summary>
Motivation: Existing rule-based and domain-specific NER methods lack generalizability in redacting PII from unstructured text; LLMs show promise but architectural and training choices' effects remain unstudied.

Method: Comprehensive analysis comparing LLM architectures and training strategies for PII redaction across metrics including performance, semantic preservation, leakage reduction, latency, and computational cost.

Result: Provides practical guidance for efficient LLM-based redactors while achieving accuracy and privacy, with PRvL offering fine-tuned models supporting domain customization and secure deployment.

Conclusion: Releases PRvL to enable reproducible, customizable, and compliant PII redaction using open-source LLMs within self-managed secure environments, offering both standalone and integration-ready solutions.

Abstract: Redacting Personally Identifiable Information (PII) from unstructured text is
critical for ensuring data privacy in regulated domains. While earlier
approaches have relied on rule-based systems and domain-specific Named Entity
Recognition (NER) models, these methods fail to generalize across formats and
contexts. Recent advances in Large Language Models (LLMs) offer a promising
alternative, yet the effect of architectural and training choices on redaction
performance remains underexplored. LLMs have demonstrated strong performance in
tasks that require contextual language understanding, including the redaction
of PII in free-form text. Prior work suggests that with appropriate adaptation,
LLMs can become effective contextual privacy learners. However, the
consequences of architectural and training choices for PII Redaction remain
underexplored. In this work, we present a comprehensive analysis of LLMs as
privacy-preserving PII Redaction systems. We evaluate a range of LLM
architectures and training strategies for their effectiveness in PII Redaction.
Our analysis measures redaction performance, semantic preservation, and PII
leakage, and compares these outcomes against latency and computational cost.
The results provide practical guidance for configuring LLM-based redactors that
are accurate, efficient, and privacy-aware. To support reproducibility and
real-world deployment, we release PRvL, an open-source suite of fine-tuned
models, and evaluation tools for general-purpose PII Redaction. PRvL is built
entirely on open-source LLMs and supports multiple inference settings for
flexibility and compliance. It is designed to be easily customized for
different domains and fully operable within secure, self-managed environments.
This enables data owners to perform redactions without relying on third-party
services or exposing sensitive content beyond their own infrastructure.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [9] [Automated File-Level Logging Generation for Machine Learning Applications using LLMs: A Case Study using GPT-4o Mini](https://arxiv.org/abs/2508.04820)
*Mayra Sofia Ruiz Rodriguez,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: This study evaluates GPT-4o mini's ability to generate file-level log statements in machine learning projects, finding that while it matches human placement 63.91% of the time, it suffers from an 82.66% overlogging rate and struggles with adherence to project-specific conventions.


<details>
  <summary>Details</summary>
Motivation: Prior research on LLM-generated logs focuses on function-level logging, neglecting file-level logging in machine learning applications where comprehensive logging is critical for system reliability and debugging.

Method: The researchers collected 171 ML repositories with 4,073 Python files containing logs, removed existing logs, prompted LLMs to regenerate logs, and compared the generated logs to human-written ones across placement, level, variables, and text quality metrics using both automated and manual analysis.

Result: 63.91% of LLM-generated logs were placed similarly to human logs, but 82.66% were overlogged. Manual analysis identified three main challenges: overlogging at function boundaries, difficulty in dense code segments, and inconsistent alignment with project-specific logging norms.

Conclusion: LLMs demonstrate potential for file-level log generation but require addressing overlogging and convention misalignment to be practical for real-world ML development.

Abstract: Logging is essential in software development, helping developers monitor
system behavior and aiding in debugging applications. Given the ability of
large language models (LLMs) to generate natural language and code, researchers
are exploring their potential to generate log statements. However, prior work
focuses on evaluating logs introduced in code functions, leaving file-level log
generation underexplored -- especially in machine learning (ML) applications,
where comprehensive logging can enhance reliability. In this study, we evaluate
the capacity of GPT-4o mini as a case study to generate log statements for ML
projects at file level. We gathered a set of 171 ML repositories containing
4,073 Python files with at least one log statement. We identified and removed
the original logs from the files, prompted the LLM to generate logs for them,
and evaluated both the position of the logs and log level, variables, and text
quality of the generated logs compared to human-written logs. In addition, we
manually analyzed a representative sample of generated logs to identify common
patterns and challenges. We find that the LLM introduces logs in the same place
as humans in 63.91% of cases, but at the cost of a high overlogging rate of
82.66%. Furthermore, our manual analysis reveals challenges for file-level
logging, which shows overlogging at the beginning or end of a function,
difficulty logging within large code blocks, and misalignment with
project-specific logging conventions. While the LLM shows promise for
generating logs for complete files, these limitations remain to be addressed
for practical implementation.

</details>


### [10] [Automated Bug Frame Retrieval from Gameplay Videos Using Vision-Language Models](https://arxiv.org/abs/2508.04895)
*Wentao Lu,Alexander Senchenko,Abram Hindle,Cor-Paul Bezemer*

Main category: cs.SE

TL;DR: This paper presents an automated pipeline to extract a single representative frame from gameplay videos in bug reports, enabling faster verification and triage for game developers using FFmpeg and GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Manual video review for bug reports in game studios is labor-intensive, slow, and difficult to scale due to the high volume of submitted gameplay videos.

Method: The pipeline combines FFmpeg for keyframe extraction (reducing frames to 1.90% while capturing 98.79 bug moments) and a vision-language model (GPT-4o) to select the most relevant frame matching the textual bug description.

Result: Achieved F1 score 0.79 and Accuracy 0.89 for top-1 frame retrieval, with varying performance across categories (Lighting & Shadow: 0.94; Animation & VFX: 0.51).

Conclusion: The approach significantly reduces manual effort and accelerates QA processes by replacing video viewing with instantly actionable visual evidence, offering practical scalability benefits for the game industry.

Abstract: Modern game studios deliver new builds and patches at a rapid pace,
generating thousands of bug reports, many of which embed gameplay videos. To
verify and triage these bug reports, developers must watch the submitted
videos. This manual review is labour-intensive, slow, and hard to scale. In
this paper, we introduce an automated pipeline that reduces each video to a
single frame that best matches the reported bug description, giving developers
instant visual evidence that pinpoints the bug.
  Our pipeline begins with FFmpeg for keyframe extraction, reducing each video
to a median of just 1.90% of its original frames while still capturing bug
moments in 98.79 of cases. These keyframes are then evaluated by a
vision--language model (GPT-4o), which ranks them based on how well they match
the textual bug description and selects the most representative frame. We
evaluated this approach using real-world developer-submitted gameplay videos
and JIRA bug reports from a popular First-Person Shooter (FPS) game. The
pipeline achieves an overall F1 score of 0.79 and Accuracy of 0.89 for the
top-1 retrieved frame. Performance is highest for the Lighting & Shadow (F1 =
0.94), Physics & Collision (0.86), and UI & HUD (0.83) bug categories, and
lowest for Animation & VFX (0.51).
  By replacing video viewing with an immediately informative image, our
approach dramatically reduces manual effort and speeds up triage and regression
checks, offering practical benefits to quality assurance (QA) teams and
developers across the game industry.

</details>


### [11] [Charting Uncertain Waters: A Socio-Technical Framework for Navigating GenAI's Impact on Open Source Communities](https://arxiv.org/abs/2508.04921)
*Zixuan Feng,Reed Milewicz,Emerson Murphy-Hill,Tyler Menezes,Alexander Serebrenik,Igor Steinmacher,Anita Sarma*

Main category: cs.SE

TL;DR: This paper explores how Generative AI impacts Open Source Software (OSS) communities using a socio-technical framework inspired by McLuhan's Tetrad, focusing on risks and opportunities in four domains: software practices, documentation, community engagement, and governance.


<details>
  <summary>Details</summary>
Motivation: OSS communities face uncertainty from Generative AI's rapid transformation of software creation and governance, risking their collaborative ethos without clear frameworks.

Method: A scenario-driven conceptual exploration utilizing a socio-technical framework derived from McLuhan's Tetrad.

Result: Identification of risks and opportunities for community resilience in OSS development across four domains affected by GenAI disruption.

Conclusion: Adopting the socio-technical framework allows OSS leaders and researchers to proactively shape ecosystems rather than passively reacting to technological changes from GenAI.

Abstract: Open Source Software communities face a wave of uncertainty as Generative AI
rapidly transforms how software is created, maintained, and governed. Without
clear frameworks, communities risk being overwhelmed by the complexity and
ambiguity introduced by GenAI, threatening the collaborative ethos that
underpins OSS. We conduct a scenario-driven, conceptual exploration using a
socio-technical framework inspired by McLuhan's Tetrad to surface both risks
and opportunities for community resilience amid GenAI-driven disruption of OSS
development across four domains: software practices, documentation, community
engagement, and governance. By adopting this lens, OSS leaders and researchers
can proactively shape the future of their ecosystems, rather than simply
reacting to technological upheaval.

</details>


### [12] [Taxonomy of Faults in Attention-Based Neural Networks](https://arxiv.org/abs/2508.04925)
*Sigma Jahan,Saurabh Singh Rajput,Tushar Sharma,Mohammad Masudur Rahman*

Main category: cs.SE

TL;DR: The paper introduces a new taxonomy for attention-based neural network faults, identifying seven attention-specific categories and four diagnostic heuristics that address critical gaps highlighted by real-world failures like ChatGPT and Gemini.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning fault taxonomies fail to account for unique attention mechanism failures, leading to diagnostic challenges in critical systems where attention errors manifest dramatically, as seen in high-profile incidents from ChatGPT and Gemini.

Method: Systematic analysis of 555 real-world attention faults from 96 projects across GitHub, Hugging Face, and Stack Overflow, enabling identification of attention-specific failure patterns and development of the first specialized fault taxonomy.

Result: 7 attention-specific fault categories discovered (not captured by prior work), with 53.5% of analyzed faults attributed to attention-unique mechanisms. Four evidence-based diagnostic heuristics identified that explain 33% of attention-specific faults.

Conclusion: First systematic diagnostic guidance for ABNNs through novel taxonomy and heuristics, demonstrating the need for attention-specific fault analysis and providing actionable insights for practitioners encountering these unique failure modes.

Abstract: Attention mechanisms are at the core of modern neural architectures, powering
systems ranging from ChatGPT to autonomous vehicles and driving a major
economic impact. However, high-profile failures, such as ChatGPT's nonsensical
outputs or Google's suspension of Gemini's image generation due to attention
weight errors, highlight a critical gap: existing deep learning fault
taxonomies might not adequately capture the unique failures introduced by
attention mechanisms. This gap leaves practitioners without actionable
diagnostic guidance. To address this gap, we present the first comprehensive
empirical study of faults in attention-based neural networks (ABNNs). Our work
is based on a systematic analysis of 555 real-world faults collected from 96
projects across ten frameworks, including GitHub, Hugging Face, and Stack
Overflow. Through our analysis, we develop a novel taxonomy comprising seven
attention-specific fault categories, not captured by existing work. Our results
show that over half of the ABNN faults arise from mechanisms unique to
attention architectures. We further analyze the root causes and manifestations
of these faults through various symptoms. Finally, by analyzing symptom-root
cause associations, we identify four evidence-based diagnostic heuristics that
explain 33.0% of attention-specific faults, offering the first systematic
diagnostic guidance for attention-based models.

</details>


### [13] [Generative AI for Object-Oriented Programming: Writing the Right Code and Reasoning the Right Logic](https://arxiv.org/abs/2508.05005)
*Gang Xu,Airong Wang,Yushan Pan*

Main category: cs.SE

TL;DR: The paper explores integrating large language models (LLMs) with Object-Oriented Programming (OOP) by analyzing perspectives from key stakeholders, identifying critical workflow junctures for LLM application, and proposing strategies to enhance OOP learning/code writing.


<details>
  <summary>Details</summary>
Motivation: Current gaps in understanding how LLMs can improve OOP-specific tasks (learning/code writing) and evaluating their effectiveness in this domain necessitate systematic investigation.

Method: Conducted stakeholder analysis through (1) perspective mapping with programmers, mariners, experienced programmers and (2) workflow examination to pinpoint augmentation opportunities for LLM integration.

Result: Identified critical junctures in OOP workflows where LLMs could meaningfully improve logical reasoning and code writing outcomes, validated through stakeholder-based evaluation frameworks.

Conclusion: The paper advocates for stakeholder-informed LLM integration in OOP tasks, suggesting practical strategies to enhance programming education and development practices through AI augmentation.

Abstract: We find ourselves in the midst of an explosion in artificial intelligence
research, particularly with large language models (LLMs). These models have
diverse applications spanning finance, commonsense knowledge graphs, medicine,
and visual analysis. In the world of Object-Oriented Programming(OOP), a robust
body of knowledge and methods has been developed for managing complex tasks
through object-oriented thinking. However, the intersection of LLMs with OOP
remains an underexplored territory. Empirically, we currently possess limited
understanding of how LLMs can enhance the effectiveness of OOP learning and
code writing, as well as how we can evaluate such AI-powered tools. Our work
aims to address this gap by presenting a vision from the perspectives of key
stakeholders involved in an OOP task: programmers, mariners, and experienced
programmers. We identify critical junctures within typical coding workflows
where the integration of LLMs can offer significant benefits. Furthermore, we
propose ways to augment existing logical reasoning and code writing, ultimately
enhancing the programming experience.

</details>


### [14] [An ML-based Approach to Predicting Software Change Dependencies: Insights from an Empirical Study on OpenStack](https://arxiv.org/abs/2508.05034)
*Arabat,Ali,Sayagh,Mohammed,Hassine,Jameleddine*

Main category: cs.SE

TL;DR: This paper presents a semi-automated ML approach to proactively identify interdependent software changes in OpenStack, revealing that 51.08% of dependencies are discovered during delayed code reviews, costing developers significant effort.


<details>
  <summary>Details</summary>
Motivation: Managing dependencies in complex software systems is critical for CI/CD pipelines but faces challenges due to multi-component/team dependencies, delays in detection, and extensive manual effort (median 57.12 hours). Existing problems lead to build failures and inefficient deployments.

Method: The study analyzes 10 years of OpenStack changes and proposes a two-stage semi-automated method: (1) a dependency likelihood predictor and (2) a model identifying specific dependent change pairs. ML models are trained to address delayed manual detection and reduce search effort.

Result: Results show 51.08% of dependencies are detected during code review with 5.06hr median delay. The proposed models achieved 79.33% AUC (likelihood prediction) and 91.89% AUC (change pair detection), with Brier scores of 0.11 and 0.014. While the second model has high recall, precision requires improvement.

Conclusion: The paper demonstrates the feasibility of semi-automated dependency management for large-scale systems like OpenStack, though precision of the pair identification model needs refinement to fully reduce manual effort and improve CI/CD efficiency.

Abstract: As software systems grow in complexity, accurately identifying and managing
dependencies among changes becomes increasingly critical. For instance, a
change that leverages a function must depend on the change that introduces it.
Establishing such dependencies allows CI/CD pipelines to build and orchestrate
changes effectively, preventing build failures and incomplete feature
deployments. In modern software systems, dependencies often span multiple
components across teams, creating challenges for development and deployment.
They serve various purposes, from enabling new features to managing
configurations, and can even involve traditionally independent changes like
documentation updates. To address these challenges, we conducted a preliminary
study on dependency management in OpenStack, a large-scale software system. Our
study revealed that a substantial portion of software changes in OpenStack over
the past 10 years are interdependent. Surprisingly, 51.08% of these
dependencies are identified during the code review phase-after a median delay
of 5.06 hours-rather than at the time of change creation. Developers often
spend a median of 57.12 hours identifying dependencies, searching among a
median of 463 other changes. To help developers proactively identify
dependencies, we propose a semi-automated approach that leverages two ML
models. The first model predicts the likelihood of dependencies among changes,
while the second identifies the exact pairs of dependent changes. Our proposed
models demonstrate strong performance, achieving average AUC scores of 79.33%
and 91.89%, and Brier scores of 0.11 and 0.014, respectively. Indeed, the
second model has a good top-k recall across all types of pairs, while the top-k
precision has room for improvement.

</details>


### [15] [LadyBug: A GitHub Bot for UI-Enhanced Bug Localization in Mobile Apps](https://arxiv.org/abs/2508.05085)
*Junayed Mahmud,James Chen,Terry Achille,Camilo Alvarez-Velez,Darren Dean Bansil,Patrick Ijieh,Samar Karanch,Nadeeshan De Silva,Oscar Chaparro,Andrian Marcus,Kevin Moran*

Main category: cs.SE

TL;DR: LadyBug is a GitHub bot that improves Android bug localization by integrating UI interaction traces with text retrieval, outperforming baselines through enhanced accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing bug localization tools rely solely on text retrieval, missing contextual UI interaction data. LadyBug aims to reduce developer effort by combining both sources for more precise results.

Method: LadyBug leverages text from GitHub issues and UI event sequences from reproduction traces to build a hybrid retrieval system. UI information is processed via event embeddings and used alongside bug report text to rank relevant files.

Result: On the 80-bug RedWing benchmark from 39 Android apps, LadyBug achieved significantly higher localization accuracy than text-only baselines. UI-based retrieval improved performance by 25%+ in top-5 file rankings.

Conclusion: The integration of UI traces with text retrieval substantially enhances bug localization accuracy. LadyBug's open-source implementation demonstrates practical feasibility for developers seeking faster bug resolution.

Abstract: This paper introduces LadyBug, a GitHub bot that automatically localizes bugs
for Android apps by combining UI interaction information with text retrieval.
LadyBug connects to an Android app's GitHub repository, and is triggered when a
bug is reported in the corresponding issue tracker. Developers can then record
a reproduction trace for the bug on a device or emulator and upload the trace
to LadyBug via the GitHub issue tracker. This enables LadyBug to utilize both
the text from the original bug description, and UI information from the
reproduction trace to accurately retrieve a ranked list of files from the
project that most likely contain the reported bug.
  We empirically evaluated LadyBug using an automated testing pipeline and
benchmark called RedWing that contains 80 fully-localized and reproducible bug
reports from 39 Android apps. Our results illustrate that LadyBug outperforms
text-retrieval-based baselines and that the utilization of UI information leads
to a substantial increase in localization accuracy. LadyBug is an open-source
tool, available at https://github.com/LadyBugML/ladybug.
  A video showing the capabilities of Ladybug can be viewed here:
https://youtu.be/hI3tzbRK0Cw

</details>


### [16] [Posterior-GRPO: Rewarding Reasoning Processes in Code Generation](https://arxiv.org/abs/2508.05170)
*Lishui Fan,Yu Zhang,Mouxiang Chen,Zhongxin Liu*

Main category: cs.SE

TL;DR: This paper proposes a framework for RL-based code generation that incorporates reasoning process quality evaluation (LCB-RB benchmark & OD-based reward model) and introduces P-GRPO - an innovative RL method applying process-based rewards only conditionally on task success, achieving SOTA results matching GPT-4-Turbo and demonstrating broad generalizability across coding and math tasks.


<details>
  <summary>Details</summary>
Motivation: Current code generation methods using reinforcement learning (RL) with outcome-based rewards from test cases overlook the quality of intermediate reasoning processes, leading to reward hacking issues where models exploit the reward signal without improving final outcomes.

Method: The approach uses two novel components: 1) LCB-RB benchmark with preference pairs for reasoning process evaluation, 2) Optimized-Degraded (OD-based) reward model training enhancing reasoning quality assessment across factual accuracy, logical rigor, and coherence dimensions, and 3) Posterior-GRPO (P-GRPO) which applies process-based rewards only for successful task completions to prevent reward hacking.

Result: The proposed 7B parameter model achieves state-of-the-art results on LCB-RB benchmark and outperforms outcome-only baselines by 4.5% across diverse code generation tasks, reaching performance comparable to GPT-4-Turbo. The framework also generalizes well to mathematical reasoning tasks.

Conclusion: This work establishes a paradigm for code generation that addresses the quality of the internal reasoning process during RL, demonstrates effective mitigation against reward hacking via conditional reward application, and successfully transfers the approach to mathematical problem-solving tasks.

Abstract: Reinforcement learning (RL) has significantly advanced code generation for
large language models (LLMs). However, current paradigms rely on outcome-based
rewards from test cases, neglecting the quality of the intermediate reasoning
process. While supervising the reasoning process directly is a promising
direction, it is highly susceptible to reward hacking, where the policy model
learns to exploit the reasoning reward signal without improving final outcomes.
To address this, we introduce a unified framework that can effectively
incorporate the quality of the reasoning process during RL. First, to enable
reasoning evaluation, we develop LCB-RB, a benchmark comprising preference
pairs of superior and inferior reasoning processes. Second, to accurately score
reasoning quality, we introduce an Optimized-Degraded based (OD-based) method
for reward model training. This method generates high-quality preference pairs
by systematically optimizing and degrading initial reasoning paths along
curated dimensions of reasoning quality, such as factual accuracy, logical
rigor, and coherence. A 7B parameter reward model with this method achieves
state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other
benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method
that conditions process-based rewards on task success. By selectively applying
rewards to the reasoning processes of only successful outcomes, P-GRPO
effectively mitigates reward hacking and aligns the model's internal reasoning
with final code correctness. A 7B parameter model with P-GRPO achieves superior
performance across diverse code generation tasks, outperforming outcome-only
baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further
demonstrate the generalizability of our approach by extending it to
mathematical tasks. Our models, dataset, and code are publicly available.

</details>


### [17] [AI-assisted JSON Schema Creation and Mapping](https://arxiv.org/abs/2508.05192)
*Felix Neubauer,Jürgen Pleiss,Benjamin Uekermann*

Main category: cs.SE

TL;DR: The paper introduces a hybrid approach combining LLMs with deterministic techniques for JSON schema creation, modification, and mapping via natural language inputs, integrated into the open-source MetaConfigurator tool. It lowers barriers for non-experts in structured data modeling and integration.


<details>
  <summary>Details</summary>
Motivation: Many domains lack standardized data models, and creating them requires significant expertise. Non-experts face challenges in schema generation and data integration, necessitating tools that reduce complexity through natural language interaction.

Method: A hybrid method integrating LLMs for schema generation/mapping from natural language inputs with deterministic execution for scalability and reliability. The approach is implemented in MetaConfigurator, which supports visual editing, validation, code/form generation, and handles heterogeneous data formats.

Result: The hybrid approach enabled non-expert-friendly schema creation and mapping, demonstrated in a chemistry domain application example. The tool successfully integrates natural language input with deterministic rule execution for data modeling.

Conclusion: The work bridges the expertise gap in MDE by combining LLMs' flexibility with deterministic safeguards. This hybridization significantly improves accessibility for structured data modeling, making complex data integration tasks feasible for non-experts.

Abstract: Model-Driven Engineering (MDE) places models at the core of system and data
engineering processes. In the context of research data, these models are
typically expressed as schemas that define the structure and semantics of
datasets. However, many domains still lack standardized models, and creating
them remains a significant barrier, especially for non-experts. We present a
hybrid approach that combines large language models (LLMs) with deterministic
techniques to enable JSON Schema creation, modification, and schema mapping
based on natural language inputs by the user. These capabilities are integrated
into the open-source tool MetaConfigurator, which already provides visual model
editing, validation, code generation, and form generation from models. For data
integration, we generate schema mappings from heterogeneous JSON, CSV, XML, and
YAML data using LLMs, while ensuring scalability and reliability through
deterministic execution of generated mapping rules. The applicability of our
work is demonstrated in an application example in the field of chemistry. By
combining natural language interaction with deterministic safeguards, this work
significantly lowers the barrier to structured data modeling and data
integration for non-experts.

</details>


### [18] [STEPWISE-CODEX-Bench: Evaluating Complex Multi-Function Comprehension and Fine-Grained Execution Reasoning](https://arxiv.org/abs/2508.05193)
*Kaiwen Yan,Yuhang Chang,Zirui Guo,Yaling Mou,Jiang Ming,Jingwei Sun*

Main category: cs.SE

TL;DR: The paper introduces STEPWISE-CODEX-Bench (SX-Bench), a novel benchmark for complex multi-function code understanding and fine-grained execution reasoning, alongside an automated generation pipeline to enhance evaluation efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for code intelligence lack the complexity to meaningfully differentiate advanced models, as they focus on functional correctness or oversimplified reasoning tasks, leading to saturated performance scores.

Method: The authors designed SX-Bench with tasks requiring control/data flow modeling across multiple sub-functions (e.g., chained calls, nested loops) and introduced a step-based execution reasoning framework. They also developed an automated pipeline integrating program synthesis, symbolic execution, and LLM-aided validation for benchmark creation.

Result: SX-Bench demonstrates strong discriminative power: OpenAI-O3 achieves only 78.37% accuracy on Hard-Reasoning tasks, significantly lower than scores on prior benchmarks, highlighting limitations in complex code reasoning. Evaluation includes over 20 mainstream models.

Conclusion: SX-Bench advances code evaluation from basic verification to multi-function dynamic reasoning, offering a critical tool for assessing the depth of advanced code intelligence models and revealing current reasoning bottlenecks.

Abstract: In recent years, large language models (LLMs) have made significant progress
in code intelligence, yet systematically evaluating their code understanding
and reasoning abilities remains challenging. Mainstream benchmarks such as
HumanEval and MBPP primarily assess functional correctness, while reasoning
benchmarks like CRUXEVAL are limited to single-function, low-complexity
scenarios. As a result, advanced models achieve nearly saturated scores,
limiting their discriminative power. To address this, we present
STEPWISE-CODEX-Bench (SX-Bench), a novel benchmark designed for complex
multi-function understanding and fine-grained execution reasoning. SX-Bench
features tasks involving collaboration among multiple sub-functions (e.g.,
chained calls, nested loops), shifting evaluation towards overall control and
data flow modeling. It defines "computation steps" as the minimal execution
unit and requires models to predict the total number of steps in reasoning
tasks, thereby assessing a model's in-depth understanding of dynamic execution
beyond simple I/O matching. Evaluation on over 20 mainstream models (including
14 reasoning-enhanced models) demonstrates that SX-Bench is highly
discriminative: even the state-of-the-art OpenAI-O3 achieves only 78.37 percent
accuracy on Hard-Reasoning tasks, much lower than its saturated scores on
previous benchmarks, thereby revealing bottlenecks in complex and fine-grained
reasoning. We also release an automated pipeline combining program synthesis,
symbolic execution, and LLM-aided validation for efficient benchmark generation
and quality assurance. SX-Bench advances code evaluation from "single-function
verification" to "multi-function dynamic reasoning," providing a key tool for
the in-depth assessment of advanced code intelligence models.

</details>


### [19] [EvoGraph: Hybrid Directed Graph Evolution toward Software 3.0](https://arxiv.org/abs/2508.05199)
*Igor Costa,Christopher Baran*

Main category: cs.SE

TL;DR: EvoGraph is a framework that uses typed directed graphs and specialized small language models (SLMs) for software evolution, achieving high security fixes, cross-language modernization with semantic equivalence, and reduced latency/feature lead time versus large language models.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in legacy system modernization, such as maintaining implicit contracts, preserving performance, and evolving integrations, to enable practical Software 3.0 systems that adapt continuously while staying controllable.

Method: EvoGraph employs a type-aware graph representation of software artifacts, uses SLM-driven mutation operators for targeted evolution, and applies multi-objective fitness selection for survivor optimization across modernization scenarios.

Result: Benchmarks show 83% security vulnerability fixes, 93% functional equivalence in COBOL-to-Java translation, 40% latency reduction, sevenfold shorter feature lead time, and 82-96% semantic equivalences across legacy codebases with 90% lower computational costs than large language models.

Conclusion: EvoGraph demonstrates a scalable path toward continuously adaptive software systems through language-specific SLMs and graph-based representation, effectively mitigating legacy modernization pitfalls while maintaining control over evolution outcomes.

Abstract: We introduce **EvoGraph**, a framework that enables software systems to
evolve their own source code, build pipelines, documentation, and tickets.
EvoGraph represents every artefact in a typed directed graph, applies learned
mutation operators driven by specialized small language models (SLMs), and
selects survivors with a multi-objective fitness. On three benchmarks, EvoGraph
fixes 83% of known security vulnerabilities, translates COBOL to Java with 93%
functional equivalence (test verified), and maintains documentation freshness
within two minutes. Experiments show a 40% latency reduction and a sevenfold
drop in feature lead time compared with strong baselines. We extend our
approach to **evoGraph**, leveraging language-specific SLMs for modernizing
.NET, Lisp, CGI, ColdFusion, legacy Python, and C codebases, achieving 82-96%
semantic equivalence across languages while reducing computational costs by 90%
compared to large language models. EvoGraph's design responds to empirical
failure modes in legacy modernization, such as implicit contracts, performance
preservation, and integration evolution. Our results suggest a practical path
toward Software 3.0, where systems adapt continuously yet remain under
measurable control.

</details>


### [20] [A Conceptual Model and Methodology for Sustainability-aware, IoT-enhanced Business Processes](https://arxiv.org/abs/2508.05301)
*Victoria Torres Bosch,Ronny Seiger,Manuela Albert Albiol,Antoni Mestre Gascon,Pedro Jose Valderas Aranda*

Main category: cs.SE

TL;DR: This paper proposes a conceptual model and structured methodology for systematically analyzing IoT's potential to measure and improve sustainability in business processes (BPs), extending beyond environmental dimensions to ensure holistic impact through examples in tourism and healthcare.


<details>
  <summary>Details</summary>
Motivation: While IoT enhances business processes with real-time automation, sustainability research has narrowly focused on environmental aspects, necessitating a systematic approach to address all sustainability dimensions comprehensively.

Method: The authors developed a conceptual model connecting BPM and IoT to represent sustainability concepts, and a methodology for analyzing existing BPs, identifying opportunities, and implementing IoT-enhanced, sustainability-aware processes using domain examples.

Result: The model demonstrates how IoT devices support sustainability measurement in BPs, while the methodology provides actionable steps for analysis and implementation, validated through tourism and healthcare case studies.

Conclusion: The proposed approach offers a framework to systematically leverage IoT for sustainability in business processes, addressing environmental, social, and economic dimensions through structured analysis and domain-specific applications.

Abstract: The real-time data collection and automation capabilities offered by the
Internet of Things (IoT) are revolutionizing and transforming Business
Processes (BPs) into IoT-enhanced BPs, showing high potential for improving
sustainability. Although already studied in Business Process Management (BPM),
sustainability research has primarily focused on environmental concerns.
However, achieving a holistic and lasting impact requires a systematic approach
to address sustainability beyond the environmental dimension. This work
proposes a conceptual model and a structured methodology with the goal of
analyzing the potential of IoT to measure and improve the sustainability of
BPs. The conceptual model formally represents key sustainability concepts,
linking BPM and IoT by highlighting how IoT devices support and contribute to
sustainability. The methodology guides the systematic analysis of existing BPs,
identifies opportunities, and implements sustainability-aware, IoT-enhanced
BPs. The approach is illustrated through a running example from the tourism
domain and a case study in healthcare.

</details>
