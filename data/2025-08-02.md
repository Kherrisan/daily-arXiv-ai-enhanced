<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 4]
- [cs.SE](#cs.SE) [Total: 12]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Fine-Grained Privacy Extraction from Retrieval-Augmented Generation Systems via Knowledge Asymmetry Exploitation](https://arxiv.org/abs/2507.23229)
*Yufei Chen,Yao Wang,Haibin Zhang,Tao Gu*

Main category: cs.CR

TL;DR: This paper introduces a novel black-box attack framework to address privacy risks in Retrieval-augmented Generation (RAG) systems by exploiting knowledge asymmetry between RAG and standard LLMs, enabling fine-grained privacy extraction with high accuracy even without pre-defined domain knowledge.


<details>
  <summary>Details</summary>
Motivation: Current privacy attacks on RAG systems struggle to accurately isolate knowledge-base-derived content in mixed outputs and fail to generalize across diverse domains, creating a critical gap in robust privacy threat analysis.

Method: The approach employs chain-of-thought reasoning with adaptive prompts to steer RAG systems, followed by query decomposition to maximize information disparity. It uses semantic relationship scoring to resolve lexical/syntactic ambiguities and trains a domain-agnostic neural network on extracted features to identify sensitive sentences.

Result: Achieved 91% privacy extraction rate in single-domain scenarios and 83% in multi-domain cases, with case studies showing over 65% reduction in sensitive information exposure compared to prior methods.

Conclusion: The framework establishes a foundation for adaptive privacy attacks and defenses in RAG systems, advancing both security analysis capabilities and dynamic mitigation strategies through domain-generalization techniques.

Abstract: Retrieval-augmented generation (RAG) systems enhance large language models
(LLMs) by integrating external knowledge bases, but this advancement introduces
significant privacy risks. Existing privacy attacks on RAG systems can trigger
data leakage but often fail to accurately isolate knowledge-base-derived
sentences within mixed responses. They also lack robustness when applied across
multiple domains. This paper addresses these challenges by presenting a novel
black-box attack framework that exploits knowledge asymmetry between RAG and
standard LLMs to achieve fine-grained privacy extraction across heterogeneous
knowledge landscapes. We propose a chain-of-thought reasoning strategy that
creates adaptive prompts to steer RAG systems away from sensitive content.
Specifically, we first decompose adversarial queries to maximize information
disparity and then apply a semantic relationship scoring to resolve lexical and
syntactic ambiguities. We finally train a neural network on these feature
scores to precisely identify sentences containing private information. Unlike
prior work, our framework generalizes to unseen domains through iterative
refinement without pre-defined knowledge. Experimental results show that we
achieve over 91% privacy extraction rate in single-domain and 83% in
multi-domain scenarios, reducing sensitive sentence exposure by over 65% in
case studies. This work bridges the gap between attack and defense in RAG
systems, enabling precise extraction of private information while providing a
foundation for adaptive mitigation.

</details>


### [2] [Counterfactual Evaluation for Blind Attack Detection in LLM-based Evaluation Systems](https://arxiv.org/abs/2507.23453)
*Lijia Liu,Takumi Kondo,Kyohei Atarashi,Koh Takeuchi,Jiyi Li,Shigeru Saito,Hisashi Kashima*

Main category: cs.CR

TL;DR: The paper proposes an SE+CFE framework to defend LLM-based evaluation systems against blind attacks by cross-validating answers with a deliberately false ground-truth.


<details>
  <summary>Details</summary>
Motivation: LLM evaluation systems are vulnerable to blind attacks where candidate answers deceive evaluators independently of the true answer, requiring a defense that maintains performance.

Method: The framework augments Standard Evaluation (SE) with Counterfactual Evaluation (CFE), re-evaluating submissions against a false ground-truth answer to detect validation under both conditions.

Result: Experiments demonstrate SE+CFE significantly improves attack detection rates (reducing vulnerability by x%) with minimal performance trade-offs (y% accuracy drop).

Conclusion: The SE+CFE framework provides an effective security enhancement for LLM evaluation systems against blind attacks with acceptable performance trade-offs.

Abstract: This paper investigates defenses for LLM-based evaluation systems against
prompt injection. We formalize a class of threats called blind attacks, where a
candidate answer is crafted independently of the true answer to deceive the
evaluator. To counter such attacks, we propose a framework that augments
Standard Evaluation (SE) with Counterfactual Evaluation (CFE), which
re-evaluates the submission against a deliberately false ground-truth answer.
An attack is detected if the system validates an answer under both standard and
counterfactual conditions. Experiments show that while standard evaluation is
highly vulnerable, our SE+CFE framework significantly improves security by
boosting attack detection with minimal performance trade-offs.

</details>


### [3] [LLM-Based Identification of Infostealer Infection Vectors from Screenshots: The Case of Aurora](https://arxiv.org/abs/2507.23611)
*Estelle Ruellan,Eric Clay,Nicholas Ascoli*

Main category: cs.CR

TL;DR: This paper proposes a scalable LLM-based method for malware analysis by processing infection screenshots from the Aurora infostealer, enabling the extraction of 337 URLs and 246 files to identify malware campaigns and distribution methods.


<details>
  <summary>Details</summary>
Motivation: Manual analysis of 29 million+ stealer logs in 2024 is impractical, and traditional log-based approaches overlook critical infection artifacts like screenshots containing visual compromise evidence.

Method: The authors use gpt-4o-mini to analyze 1,000 Aurora stealer screenshots, extracting IoCs (URLs, files) and correlating artifacts such as filenames, compromised software themes, and social engineering indicators to map infection vectors and track campaigns.

Result: The method successfully identified 337 actionable URLs, 246 relevant files, and three distinct malware campaigns, revealing patterns in malware distribution and human-driven attack tactics within the Aurora stealer logs.

Conclusion: LLM-powered reactive analysis of infection artifacts offers a scalable alternative to traditional malware detection, enhancing threat intelligence capabilities through automated extraction of IoCs and campaign mapping from screenshots.

Abstract: Infostealers exfiltrate credentials, session cookies, and sensitive data from
infected systems. With over 29 million stealer logs reported in 2024, manual
analysis and mitigation at scale are virtually unfeasible/unpractical. While
most research focuses on proactive malware detection, a significant gap remains
in leveraging reactive analysis of stealer logs and their associated artifacts.
Specifically, infection artifacts such as screenshots, image captured at the
point of compromise, are largely overlooked by the current literature. This
paper introduces a novel approach leveraging Large Language Models (LLMs), more
specifically gpt-4o-mini, to analyze infection screenshots to extract potential
Indicators of Compromise (IoCs), map infection vectors, and track campaigns.
Focusing on the Aurora infostealer, we demonstrate how LLMs can process
screenshots to identify infection vectors, such as malicious URLs, installer
files, and exploited software themes. Our method extracted 337 actionable URLs
and 246 relevant files from 1000 screenshots, revealing key malware
distribution methods and social engineering tactics. By correlating extracted
filenames, URLs, and infection themes, we identified three distinct malware
campaigns, demonstrating the potential of LLM-driven analysis for uncovering
infection workflows and enhancing threat intelligence. By shifting malware
analysis from traditional log-based detection methods to a reactive,
artifact-driven approach that leverages infection screenshots, this research
presents a scalable method for identifying infection vectors and enabling early
intervention.

</details>


### [4] [Polynomial Lattices for the BIKE Cryptosystem](https://arxiv.org/abs/2507.23641)
*Michael Schaller*

Main category: cs.CR

TL;DR: This paper introduces a rank-2 lattice structure for the BIKE cryptosystem's public key, generalizes weak key recovery by connecting it to the implicit solution of a shortest vector problem, and proposes a method to construct a reduced basis for finding more weak keys.


<details>
  <summary>Details</summary>
Motivation: To enhance the understanding of BIKE cryptosystem's security by generalizing weak key recovery techniques from existing literature and demonstrating the underutilized capacity of lattice reduction for improved key analysis.

Method: 1) Constructing a rank-2 polynomial ring lattice from BIKE's public key geometry. 2) Proving that weak key recovery corresponds to solving SVP in this lattice. 3) Developing a lattice basis reduction approach that reveals the secret key's sparse vector representation and can identify more weak keys than previous methods.

Result: Successfully demonstrated that existing weak key recovery methods implicitly solve SVP in the introduced lattice structure. Obtained an algorithm that can enumerate multiple weak keys via basis reduction, improving over previous vector-only approaches with 100% recovery rate under specific conditions.

Conclusion: The rank-2 lattice framework provides a foundational perspective for analyzing BIKE's security and opens new avenues for finding weak keys through basis reduction. This work strengthens understanding of cryptosystem vulnerability analysis as a lattice problem and suggests potential improvements in key selection strategies for post-quantum cryptography.

Abstract: In this paper we introduce a rank $2$ lattice over a polynomial ring arising
from the public key of the BIKE cryptosystem \cite{aragon2022bike}. The secret
key is a sparse vector in this lattice. We study properties of this lattice and
generalize the recovery of weak keys from \cite{BardetDLO16}. In particular, we
show that they implicitly solved a shortest vector problem in the lattice we
constructed. Rather than finding only a shortest vector, we obtain a reduced
basis of the lattice which makes it possible to check for more weak keys.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [On LLM-Assisted Generation of Smart Contracts from Business Processes](https://arxiv.org/abs/2507.23087)
*Fabian Stiehle,Hans Weytjens,Ingo Weber*

Main category: cs.SE

TL;DR: This study investigates the use of LLMs for generating smart contract code from business process descriptions, introducing an automated evaluation framework to assess code reliability beyond mere compilation. Results show LLMs lack sufficient reliability for smart contracts, suggesting integrations with existing tools for improvement.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based code generation for smart contracts evaluates code on small samples via manual inspection or compilation tests, neglecting actual execution correctness. This approach overlooks critical properties like process flow enforcement and resource allocation.

Method: We developed an automated evaluation framework using larger datasets of business process models. Tested LLMs of varying types and sizes for their ability to enforce process flow, allocate resources, and handle data-based conditions in generated smart contracts.

Result: LLM-generated smart contracts failed to meet the required reliability standards, demonstrating shortcomings in critical execution properties necessary for robust smart contract functionality.

Conclusion: While LLMs show potential, their current reliability for smart contract code generation is insufficient. Responsible integration into existing tools, leveraging our benchmarking framework, is needed to produce accurate and safe output.

Abstract: Large language models (LLMs) have changed the reality of how software is
produced. Within the wider software engineering community, among many other
purposes, they are explored for code generation use cases from different types
of input. In this work, we present an exploratory study to investigate the use
of LLMs for generating smart contract code from business process descriptions,
an idea that has emerged in recent literature to overcome the limitations of
traditional rule-based code generation approaches. However, current LLM-based
work evaluates generated code on small samples, relying on manual inspection,
or testing whether code compiles but ignoring correct execution. With this
work, we introduce an automated evaluation framework and provide empirical data
from larger data sets of process models. We test LLMs of different types and
sizes in their capabilities of achieving important properties of process
execution, including enforcing process flow, resource allocation, and
data-based conditions. Our results show that LLM performance falls short of the
perfect reliability required for smart contract development. We suggest future
work to explore responsible LLM integrations in existing tools for code
generation to ensure more reliable output. Our benchmarking framework can serve
as a foundation for developing and evaluating such integrations.

</details>


### [6] [FlowETL: An Autonomous Example-Driven Pipeline for Data Engineering](https://arxiv.org/abs/2507.23118)
*Mattia Di Profio,Mingjun Zhong,Yaji Sripada,Marcel Jaspars*

Main category: cs.SE

TL;DR: FlowETL introduces an autonomous ETL pipeline using example-based transformation plans to reduce human intervention, demonstrating strong generalisation across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Modern ETL workflows require manual human effort for context-specific transformations, and existing automation lacks the capability to design and apply these transformations without user input.

Method: FlowETL employs a Planning Engine to create transformation plans from paired input-output examples, an ETL worker to execute these plans, and integrates monitoring for pipeline observability.

Result: The system generalised effectively across 14 datasets varying in domain, structure, and size, indicating robust automated ETL performance.

Conclusion: FlowETL presents a novel, example-driven architecture for autonomous ETL, achieving strong generalisation results and advancing automated data preparation for data warehouses.

Abstract: The Extract, Transform, Load (ETL) workflow is fundamental for populating and
maintaining data warehouses and other data stores accessed by analysts for
downstream tasks. A major shortcoming of modern ETL solutions is the extensive
need for a human-in-the-loop, required to design and implement
context-specific, and often non-generalisable transformations. While related
work in the field of ETL automation shows promising progress, there is a lack
of solutions capable of automatically designing and applying these
transformations. We present FlowETL, a novel example-based autonomous ETL
pipeline architecture designed to automatically standardise and prepare input
datasets according to a concise, user-defined target dataset. FlowETL is an
ecosystem of components which interact together to achieve the desired outcome.
A Planning Engine uses a paired input-output datasets sample to construct a
transformation plan, which is then applied by an ETL worker to the source
dataset. Monitoring and logging provide observability throughout the entire
pipeline. The results show promising generalisation capabilities across 14
datasets of various domains, file structures, and file sizes.

</details>


### [7] [Vibe Modeling: Challenges and Opportunities](https://arxiv.org/abs/2507.23120)
*Jordi Cabot*

Main category: cs.SE

TL;DR: This paper proposes 'vibe modeling', a novel approach integrating AI and model-driven engineering (MDE) to enhance software development efficiency and reliability for complex systems.


<details>
  <summary>Details</summary>
Motivation: The software development field faces challenges from new user interfaces, intelligent components, and sustainability demands, while MDE models grow complex and LLM-based coding lacks maintainability. A hybrid solution is needed.

Method: Vibe modeling combines natural language processing (via LLMs) with MDE principles, generating models directly from descriptions and addressing limitations of current methods through co-design of AI and modeling.

Result: The paper outlines vibe modeling's core concepts, provides examples of its operational workflow, and systematically characterizes opportunities including automation and explainability, as well as open challenges in scalability and model accuracy.

Conclusion: Vibe modeling represents a promising direction for future software development by synergizing AI and MDE strengths, though further research is required to address technical challenges and ensure practical adoption.

Abstract: There is a pressing need for better development methods and tools to keep up
with the growing demand and increasing complexity of new software systems. New
types of user interfaces, the need for intelligent components, sustainability
concerns, ... bring new challenges that we need to handle. In the last years,
model-driven engineering (MDE) has been key to improving the quality and
productivity of software development, but models themselves are becoming
increasingly complex to specify and manage. At the same time, we are witnessing
the growing popularity of vibe coding approaches that rely on Large Language
Models (LLMs) to transform natural language descriptions into running code at
the expenses of code vulnerabilities, scalability issues and maintainability
concerns. In this paper, we introduce the concept of \textit{vibe modeling} as
a novel approach to integrate the best of both worlds (AI and MDE) to speed up
the development of reliable complex systems. We outline the key concepts of
vibe modeling and highlight the opportunities and open challenges it presents
for the future of modeling.

</details>


### [8] [Extension Decisions in Open Source Software Ecosystem](https://arxiv.org/abs/2507.23168)
*Elmira Onagh,Maleknaz Nayebi*

Main category: cs.SE

TL;DR: The paper investigates redundancy in GitHub Marketplace's Continuous Integration (CI) tools, finding 65% of new Actions duplicate existing functionality within six months. A graph model tracks functionality adoption and identifies first-mover dominance in forks/extensions. The dataset is published to aid future research and product strategy decisions.


<details>
  <summary>Details</summary>
Motivation: GitHub Marketplace's rapid growth creates challenges for innovators to distinguish meaningful advancements from redundant tools, affecting developer decision-making and ecosystem health.

Method: Linked 6,983 CI Actions to 3,869 providers and analyzed version histories using a graph model that timestamps functionality introductions, tracks adoption rates, and clusters tools with overlapping capabilities.

Result: 65% of new CI Actions replicate existing features within six months, with a small set of first-mover Actions dominating subsequent forks and extensions. The dataset captures these patterns for longitudinal analysis.

Conclusion: The study provides a framework for analyzing innovation trajectories in software ecosystems and delivers a public dataset to enable data-driven insights into emerging trends and product evolution.

Abstract: GitHub Marketplace is expanding by approximately 41% annually, with new
tools; however, many additions replicate existing functionality. We study this
phenomenon in the platform's largest segment, Continuous Integration (CI), by
linking 6,983 CI Actions to 3,869 providers and mining their version histories.
Our graph model timestamps every functionality's debut, tracks its adoption,
and clusters redundant tools. We find that approximately 65% of new CI Actions
replicate existing capabilities, typically within six months, and that a small
set of first-mover Actions accounts for most subsequent forks and extensions.
These insights enable developers to choose the optimal moment to launch, target
unmet functionality, and help maintainers eliminate redundant tools. We publish
the complete graph and dataset to encourage longitudinal research on innovation
and competition in software ecosystems, and to provide practitioners with a
data-driven roadmap for identifying emerging trends and guiding product
strategy.

</details>


### [9] [AutoBridge: Automating Smart Device Integration with Centralized Platform](https://arxiv.org/abs/2507.23178)
*Siyuan Liu,Zhice Yang,Huangxun Chen*

Main category: cs.SE

TL;DR: AutoBridge automates IoT integration code generation by combining device-specific and platform-specific knowledge with a multi-stage debugging pipeline, achieving high success and coverage rates


<details>
  <summary>Details</summary>
Motivation: Programming complex IoT integration code requires significant human expertise and effort, making it challenging to incorporate new devices into centralized platforms. Existing methods depend heavily on manual intervention, limiting scalability and efficiency.

Method: AutoBridge employs a divide-and-conquer strategy in two phases: 1) generating device control logic through progressive retrieval of device-specific knowledge, and 2) synthesizing platform-compliant integration code using platform-specific knowledge. It introduces a multi-stage debugging pipeline with automated virtual device testing and interactive hardware-in-the-loop debugging requiring only binary user feedback.

Result: On a 34-device benchmark across two platforms, AutoBridge achieved 93.87% success rate and 94.87% function coverage with 0% human input, improving to 100% function coverage after minimal binary feedback. In a user study, it outperformed expert programmers in code accuracy by 50-80% even when programmers used commercial code LLMs.

Conclusion: AutoBridge demonstrates a scalable solution for automated IoT integration code generation, significantly reducing human effort while outperforming experts in accuracy through its knowledge-driven approach and novel debugging pipeline.

Abstract: Multimodal IoT systems coordinate diverse IoT devices to deliver
human-centered services. The ability to incorporate new IoT devices under the
management of a centralized platform is an essential requirement. However, it
requires significant human expertise and effort to program the complex IoT
integration code that enables the platform to understand and control the device
functions. Therefore, we propose AutoBridge to automate IoT integration code
generation. Specifically, AutoBridge adopts a divide-and-conquer strategy: it
first generates device control logic by progressively retrieving
device-specific knowledge, then synthesizes platformcompliant integration code
using platform-specific knowledge. To ensure correctness, AutoBridge features a
multi-stage debugging pipeline, including an automated debugger for virtual IoT
device testing and an interactive hardware-in-the-loop debugger that requires
only binary user feedback (yes and no) for real-device verification. We
evaluate AutoBridge on a benchmark of 34 IoT devices across two open-source IoT
platforms. The results demonstrate that AutoBridge can achieves an average
success rate of 93.87% and an average function coverage of 94.87%, without any
human involvement. With minimal binary yes and no feedback from users, the code
is then revised to reach 100% function coverage. A user study with 15
participants further shows that AutoBridge outperforms expert programmers by
50% to 80% in code accuracy, even when the programmers are allowed to use
commercial code LLMs.

</details>


### [10] [XABPs: Towards eXplainable Autonomous Business Processes](https://arxiv.org/abs/2507.23269)
*Peter Fettke,Fabiana Fournier,Lior Limonad,Andreas Metzger,Stefanie Rinderle-Ma,Barbara Weber*

Main category: cs.SE

TL;DR: This paper addresses concerns with Autonomous Business Processes (ABPs) by proposing a systematic approach to eXplainable ABPs (XABPs).


<details>
  <summary>Details</summary>
Motivation: ABPs using AI/ML offer operational benefits but raise stakeholder trust issues, debugging challenges, accountability problems, bias risks, and regulatory compliance issues.

Method: The study outlines a systematic approach to XABPs through characterizing their forms, structuring explainability mechanisms, and identifying BPM research challenges.

Result: A framework for XABPs was developed, providing structured explanations to mitigate transparency and trust issues in autonomous decision-making processes.

Conclusion: The paper advocates for XABPs to address AI/ML workflow challenges and maps critical research directions for achieving explainability in business process automation.

Abstract: Autonomous business processes (ABPs), i.e., self-executing workflows
leveraging AI/ML, have the potential to improve operational efficiency, reduce
errors, lower costs, improve response times, and free human workers for more
strategic and creative work. However, ABPs may raise specific concerns
including decreased stakeholder trust, difficulties in debugging, hindered
accountability, risk of bias, and issues with regulatory compliance. We argue
for eXplainable ABPs (XABPs) to address these concerns by enabling systems to
articulate their rationale. The paper outlines a systematic approach to XABPs,
characterizing their forms, structuring explainability, and identifying key BPM
research challenges towards XABPs.

</details>


### [11] [SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution](https://arxiv.org/abs/2507.23348)
*Han Li,Yuling Shi,Shaoxin Lin,Xiaodong Gu,Heng Lian,Xin Wang,Yantao Jia,Tao Huang,Qianxiang Wang*

Main category: cs.SE

TL;DR: SWE-Debate introduces a competitive multi-agent debate framework to enhance issue resolution in software engineering by generating fault propagation traces and leveraging structured agent debates across code dependency graphs, achieving state-of-the-art performance on SWE-bench.


<details>
  <summary>Details</summary>
Motivation: Current agent-based issue resolution methods rely on independent exploration, frequently getting trapped in local solutions and failing to recognize cross-codebase issue patterns. This approach limits their ability to detect complex, distributed bugs.

Method: 1) Code dependency graph traversal to generate multiple fault propagation traces as localization proposals. 2) Three-round structured debates among agents with distinct reasoning perspectives focused on these traces. 3) Integration of consolidated debate results into an MCTS-based code modification agent for patch generation.

Result: Achieved new state-of-the-art open-source agent performance on the SWE-bench benchmark while demonstrating significant improvements over prior methods. The debate framework enables better issue localization and more comprehensive fix planning.

Conclusion: The multi-agent debate mechanism in SWE-Debate enhances collaborative reasoning through structured competition, enabling more effective identification and resolution of complex software engineering issues compared to independent agent approaches.

Abstract: Issue resolution has made remarkable progress thanks to the advanced
reasoning capabilities of large language models (LLMs). Recently, agent-based
frameworks such as SWE-agent have further advanced this progress by enabling
autonomous, tool-using agents to tackle complex software engineering tasks.
While existing agent-based issue resolution approaches are primarily based on
agents' independent explorations, they often get stuck in local solutions and
fail to identify issue patterns that span across different parts of the
codebase. To address this limitation, we propose SWE-Debate, a competitive
multi-agent debate framework that encourages diverse reasoning paths and
achieves more consolidated issue localization. SWE-Debate first creates
multiple fault propagation traces as localization proposals by traversing a
code dependency graph. Then, it organizes a three-round debate among
specialized agents, each embodying distinct reasoning perspectives along the
fault propagation trace. This structured competition enables agents to
collaboratively converge on a consolidated fix plan. Finally, this consolidated
fix plan is integrated into an MCTS-based code modification agent for patch
generation. Experiments on the SWE-bench benchmark show that SWE-Debate
achieves new state-of-the-art results in open-source agent frameworks and
outperforms baselines by a large margin.

</details>


### [12] [Quality Evaluation of COBOL to Java Code Transformation](https://arxiv.org/abs/2507.23356)
*Shmulik Froimovich,Raviv Gal,Wesam Ibraheem,Avi Ziv*

Main category: cs.SE

TL;DR: The paper introduces an automated evaluation system for COBOL-to-Java translation in IBM's WCA4Z, combining analytic checkers and LLM-as-a-judge (LaaJ) to address challenges like model opacity and complex quality assessment.


<details>
  <summary>Details</summary>
Motivation: LLM-based code translators face critical evaluation challenges, including model lack of transparency and subjective quality assessments. This system aims to provide scalable, cost-effective alternatives to manual reviews for COBOL modernization—a vital industry need.

Method: A hybrid approach integrating static/dynamic code analytic checkers with LLM-as-a-judge (LaaJ) evaluation. The system supports CI workflows, uses customizable benchmarks for large-scale testing, and automates quality scoring through machine learning and program logic analysis.

Result: Enables continuous quality monitoring, reduces manual review cycles, and generates structured reports highlighting translation accuracy, maintainability, and performance gaps across legacy codebases.

Conclusion: The framework demonstrates effective LLM-based translation evaluation through combined analytical and AI-driven methods, supporting robust code modernization while providing actionable insights for iterative model improvement.

Abstract: We present an automated evaluation system for assessing COBOL-to-Java code
translation within IBM's watsonx Code Assistant for Z (WCA4Z). The system
addresses key challenges in evaluating LLM-based translators, including model
opacity and the complexity of translation quality assessment. Our approach
combines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver
scalable, multi-faceted evaluations. The system supports continuous integration
workflows, enables large-scale benchmarking, and reduces reliance on manual
review. We describe the system architecture, evaluation strategies, and
reporting mechanisms that provide actionable insights for developers and
project managers, facilitating the evolution of high-quality, modernized
codebases.

</details>


### [13] [SWE-Exp: Experience-Driven Software Issue Resolution](https://arxiv.org/abs/2507.23361)
*Silin Chen,Shaoxin Lin,Xiaodong Gu,Yuling Shi,Heng Lian,Longfei Yun,Dong Chen,Weiguo Sun,Lin Cao,Qianxiang Wang*

Main category: cs.SE

TL;DR: SWE-Exp enhances software issue resolution by leveraging past agent experiences to create a knowledge bank of successful repair strategies, achieving 41.6% Pass@1 resolution rate.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents lack memory to retain knowledge from past repairs, leading to redundant exploration and missed opportunities to reuse effective solutions for similar problems.

Method: The approach establishes an experience bank that distills actionable knowledge from prior agent trajectories (both successful and failed) at multiple levels, including problem understanding and specific code modifications.

Result: State-of-the-art 41.6% Pass@1 resolution rate on SWE-bench-Verified benchmark using open-source agent frameworks, outperforming memoryless exploration methods.

Conclusion: SWE-Exp introduces a paradigm shift by enabling systematic accumulation and strategic application of repair expertise, transforming software engineering agents from trial-and-error explorers to experience-driven problem solvers.

Abstract: Recent advances in large language model (LLM) agents have shown remarkable
progress in software issue resolution, leveraging advanced techniques such as
multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current
agents act as memoryless explorers - treating each problem separately without
retaining or reusing knowledge from previous repair experiences. This leads to
redundant exploration of failed trajectories and missed chances to adapt
successful issue resolution methods to similar problems. To address this
problem, we introduce SWE-Exp, an experience - enhanced approach that distills
concise and actionable experience from prior agent trajectories, enabling
continuous learning across issues. Our method introduces a multi-faceted
experience bank that captures both successful and failed repair attempts.
Specifically, it extracts reusable issue resolution knowledge at different
levels - from high-level problem comprehension to specific code changes.
Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%
Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach
establishes a new paradigm in which automated software engineering agents
systematically accumulate and leverage repair expertise, fundamentally shifting
from trial-and-error exploration to strategic, experience-driven issue
resolution.

</details>


### [14] [Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling](https://arxiv.org/abs/2507.23370)
*Trae Research Team,Pengfei Gao,Zhao Tian,Xiangxin Meng,Xinchen Wang,Ruida Hu,Yuanan Xiao,Yizhou Liu,Zhao Zhang,Junjie Chen,Cuiyun Gao,Yun Lin,Yingfei Xiong,Chao Peng,Xia Liu*

Main category: cs.SE

TL;DR: Trae Agent is an agent-based ensemble reasoning approach for repository-level software issue resolution, achieving state-of-the-art performance on SWE-bench with 75.20% Pass@1.


<details>
  <summary>Details</summary>
Motivation: Existing prompting-based methods for LLM-driven issue resolution struggle with large solution spaces and lack repository-level understanding, limiting their effectiveness.

Method: Trae Agent modularizes the resolution process via generation, pruning, and selection agents to address ensemble space exploration and repository comprehension challenges.

Result: Trae Agent outperformed four SOTA techniques by 10.22% average Pass@1 across three LLMs, securing first place on SWE-bench Verified leaderboard (75.20% Pass@1).

Conclusion: Trae Agent's agent-based architecture enables effective repository-level issue resolution through structured ensemble reasoning, setting a new benchmark for software engineering LLM applications.

Abstract: Software issue resolution is a critical challenge in software engineering and
has garnered increasing attention in recent years. With the rapid advancement
of large language models (LLMs), substantial progress has been made in
addressing real-world software engineering tasks. Recent studies have
introduced ensemble reasoning techniques to enhance the performance of
LLM-based issue resolution. However, existing prompting-based methods still
face limitations in effectively exploring large ensemble spaces and lack the
capacity for repository-level understanding, both of which constrain their
overall effectiveness. In this paper, we propose Trae Agent, the first
agent-based ensemble reasoning approach for repository-level issue resolution.
Trae Agent formulates our goal as an optimal solution search problem and
addresses two key challenges, i.e., large ensemble spaces and repository-level
understanding, through modular agents for generation, pruning, and selection.
We conduct extensive experiments using three leading LLMs on the widely-adopted
SWE-bench benchmark, comparing Trae Agent against four state-of-the-art
ensemble reasoning techniques. Experimental results demonstrate that Trae Agent
consistently achieves superior performance, with an average improvement of
10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first
place on the SWE-bench Verified leaderboard, with a notable Pass@1 score of
75.20%. We are pleased to release Trae Agent as an open-source project to
support the research community, with all resources available at
https://github.com/bytedance/trae-agent.

</details>


### [15] [Dynamic and Static Analysis of Python Software with Kieker Including Reconstructed Architectures](https://arxiv.org/abs/2507.23425)
*Daphné Larrivain,Shinhyung Yang,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: Kieker, originally a Java observability framework, now extends Python analysis via static/dynamic combining methods.


<details>
  <summary>Details</summary>
Motivation: Python's rising popularity necessitates structural insight tools adapted from mature Java frameworks like Kieker.

Method: Integrated static and dynamic analysis techniques to create a cross-language observability pipeline for Python.

Result: Enabled comprehensive application analysis for Python systems using Kieker's established framework architecture.

Conclusion: Python support in Kieker provides valuable observability capabilities for modern Python applications through enhanced analysis pipelines.

Abstract: The Kieker observability framework is a tool that provides users with the
means to design a custom observability pipeline for their application.
Originally tailored for Java, supporting Python with Kieker is worthwhile.
Python's popularity has exploded over the years, thus making structural
insights of Python applications highly valuable. Our Python analysis pipeline
combines static and dynamic analysis in order to build a complete picture of a
given system.

</details>


### [16] [An Empirical Study on the Amount of Changes Required for Merge Request Acceptance](https://arxiv.org/abs/2507.23640)
*Samah Kansab,Mohammed Sayagh,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: The paper introduces a method to measure and predict code review (CR) effort in GitLab Merge Requests (MRs) by analyzing post-submission code changes using an interpretable machine learning model with strong performance (AUC 0.84-0.88).


<details>
  <summary>Details</summary>
Motivation: Past research focused on CR delays and iterations but overlooked actual effort quantified by code modification volume, particularly in the underexplored context of GitLab MRs. This paper bridges that gap.

Method: Defined CR effort as post-submission code changes, analyzed 23,600 GitLab MRs, and trained an interpretable ML model using text features, code complexity, developer experience, review history, and branching metrics.

Result: 71% of MRs required post-submission changes, 28% exceeded 200 lines; effort showed no correlation with review duration or participants, but model identified complexity, developer experience, and text features as key predictors, alongside historical project data.

Conclusion: Quantifying CR effort through post-submission modifications and ML modeling demonstrates feasibility for explaining and anticipating integration effort in GitLab MRs, emphasizing multidimensional predictors over traditional factors like review time.

Abstract: Code review (CR) is essential to software development, helping ensure that
new code is properly integrated. However, the CR process often involves
significant effort, including code adjustments, responses to reviewers, and
continued implementation. While past studies have examined CR delays and
iteration counts, few have investigated the effort based on the volume of code
changes required, especially in the context of GitLab Merge Requests (MRs),
which remains underexplored. In this paper, we define and measure CR effort as
the amount of code modified after submission, using a dataset of over 23,600
MRs from four GitLab projects. We find that up to 71% of MRs require
adjustments after submission, and 28% of these involve changes to more than 200
lines of code. Surprisingly, this effort is not correlated with review time or
the number of participants. To better understand and predict CR effort, we
train an interpretable machine learning model using metrics across multiple
dimensions: text features, code complexity, developer experience, review
history, and branching. Our model achieves strong performance (AUC 0.84-0.88)
and reveals that complexity, experience, and text features are key predictors.
Historical project characteristics also influence current review effort. Our
findings highlight the feasibility of using machine learning to explain and
anticipate the effort needed to integrate code changes during review.

</details>
