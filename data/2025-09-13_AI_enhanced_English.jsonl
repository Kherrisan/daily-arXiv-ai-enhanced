{"id": "2509.08843", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08843", "abs": "https://arxiv.org/abs/2509.08843", "authors": ["Sidney Shapiro"], "title": "Pattern-Based File and Data Access with Python Glob: A Comprehensive Guide for Computational Research", "comment": null, "summary": "Pattern-based file access is a fundamental but often under-documented aspect\nof computational research. The Python glob module provides a simple yet\npowerful way to search, filter, and ingest files using wildcard patterns,\nenabling scalable workflows across disciplines. This paper introduces glob as a\nversatile tool for data science, business analytics, and artificial\nintelligence applications. We demonstrate use cases including large-scale data\ningestion, organizational data analysis, AI dataset construction, and\nreproducible research practices. Through concrete Python examples with widely\nused libraries such as pandas,scikit-learn, and matplotlib, we show how glob\nfacilitates efficient file traversal and integration with analytical pipelines.\nBy situating glob within the broader context of reproducible research and data\nengineering, we highlight its role as a methodological building block. Our goal\nis to provide researchers and practitioners with a concise reference that\nbridges foundational concepts and applied practice, making glob a default\ncitation for file pattern matching in Python-based research workflows.", "AI": {"tldr": "This paper promotes Python's glob module as an essential, underappreciated tool for reproducible research and data engineering, offering practical examples for data science, analytics, and AI workflows.", "motivation": "Pattern-based file access is identified as a critical but under-documented aspect of computational research, requiring better tools for reproducibility and cross-disciplinary workflow scalability.", "method": "The authors demonstrate practical use cases through Python examples integrated with pandas, scikit-learn, and matplotlib, showing how glob enables scalable data ingestion and analytical pipeline integration.", "result": "Concrete implementations of glob in large-scale data ingestion, organizational analysis, AI dataset construction, and reproducible research practices are presented, validating its versatility across domains.", "conclusion": "The paper positions glob as a foundational, methodological building block for Python-based research workflows, advocating for its adoption as a default citation for file pattern matching in computational research."}}
{"id": "2509.08857", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.08857", "abs": "https://arxiv.org/abs/2509.08857", "authors": ["Marcelino Garcia", "Renato Garcia", "Arthur Parizotto", "Andre Mendes", "Pedro Valle", "Ricardo Vilela", "Renato Balancieri", "Williamson Silva"], "title": "A Systematic Mapping Study on Chatbots in Programming Education", "comment": "18 pages, 1 figure, 3 tables", "summary": "Educational chatbots have gained prominence as support tools for teaching\nprogramming, particularly in introductory learning contexts. This paper\npresents a Systematic Mapping Study (SMS) that investigated how such agents\nhave been developed and applied in programming education. From an initial set\nof 3,216 publications, 54 studies were selected and analyzed based on five\nresearch subquestions, addressing chatbot types, programming languages used,\neducational content covered, interaction models, and application contexts. The\nresults reveal a predominance of chatbots designed for Python instruction,\nfocusing on fundamental programming concepts, and employing a wide variety of\npedagogical approaches and technological architectures. In addition to\nidentifying trends and gaps in the literature, this study provides insights to\ninform the development of new educational tools for programming instruction.", "AI": {"tldr": "This SMS of 54 studies reveals Python-centric chatbots for programming education employ varied pedagogical methods but highlight research gaps.", "motivation": "Chatbots in programming education require systematic analysis to understand their design, applications, and pedagogical relevance for effective learning.", "method": "A Systematic Mapping Study (SMS) analyzed 54 selected publications from an initial 3,216, addressing chatbot characteristics through five research subquestions.", "result": "Python-focused chatbots dominate the literature, emphasizing fundamental programming concepts with diverse pedagogical and technological approaches.", "conclusion": "The study provides insights to guide the development of educational chatbots for programming instruction and highlights trends and gaps in existing research."}}
{"id": "2509.08863", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08863", "abs": "https://arxiv.org/abs/2509.08863", "authors": ["Qianqian Luo", "Liuchang Xu", "Qingming Lin", "Sensen Wu", "Ruichen Mao", "Chao Wang", "Hailin Feng", "Bo Huang", "Zhenhong Du"], "title": "GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation", "comment": null, "summary": "LLMs have made substantial progress in task automation and natural language\nunderstanding.However,without expertise in GIS,they continue to encounter\nlimitations.To address these issues, we propose GeoJSON Agents-a multi-agent\nLLM architecture.This framework transforms natural language tasks into\nstructured GeoJSON operation commands and processes spatial data using two\nwidely adopted LLM enhancement techniques:Function Calling and Code\nGeneration.The architecture consists of three components-task parsing,agent\ncollaboration,and result integration-aimed at enhancing both the performance\nand scalability of GIS automation.The Planner agent interprets natural language\ntasks into structured GeoJSON commands.Then,specialized Worker agents\ncollaborate according to assigned roles to perform spatial data processing and\nanalysis,either by invoking predefined function APIs or by dynamically\ngenerating and executing Python-based spatial analysis code.Finally,the system\nintegrates the outputs from multiple execution rounds into\nreusable,standards-compliant GeoJSON files.To systematically evaluate the\nperformance of the two approaches,we constructed a benchmark dataset of 70\ntasks with varying complexity and conducted experiments using OpenAI's GPT-4o\nas the core model.Results indicate that the Function Calling-based GeoJSON\nAgent achieved an accuracy of 85.71%,while the Code Generation-based agent\nreached 97.14%,both significantly outperforming the best-performing\ngeneral-purpose model (48.57%).Further analysis reveals that the Code\nGeneration provides greater flexibility,whereas the Function Calling approach\noffers more stable execution.This study is the first to introduce an LLM\nmulti-agent framework for GeoJSON data and to compare the strengths and\nlimitations of two mainstream LLM enhancement methods,offering new perspectives\nfor improving GeoAI system performance.", "AI": {"tldr": "GeoJSON Agents bridge LLMs and GIS via a multi-agent framework with Function Calling and Code Generation. Code-based agents outperform (97.14%) while both surpass general models, offering insights for GeoAI systems.", "motivation": "LLMs lack GIS expertise, limiting their spatial task capabilities. Existing methods lack scalability and performance for complex GIS operations.", "method": "GeoJSON Agents utilize a three-component architecture (task parsing, agent collaboration, result integration) combining Function Calling and Code Generation. Specialized Worker agents process spatial data by invoking APIs or generating Python code.", "result": "Code Generation-based agents achieved 97.14% accuracy (vs. 48.57% for general models), while Function Calling reached 85.71%. Code Generation offers flexibility; Function Calling provides stability.", "conclusion": "The study introduces GeoJSON Agents, a novel LLM multi-agent framework for GIS automation, demonstrating that Code Generation outperforms Function Calling in flexibility while both surpass general-purpose models. It provides a benchmark for future GeoAI research."}}
{"id": "2509.08865", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08865", "abs": "https://arxiv.org/abs/2509.08865", "authors": ["Guangyu Zhang", "Xixuan Wang", "Shiyu Sun", "Peiyan Xiao", "Kun Sun", "Yanhai Xiong"], "title": "TraceRAG: A LLM-Based Framework for Explainable Android Malware Detection and Behavior Analysis", "comment": null, "summary": "Sophisticated evasion tactics in malicious Android applications, combined\nwith their intricate behavioral semantics, enable attackers to conceal\nmalicious logic within legitimate functions, underscoring the critical need for\nrobust and in-depth analysis frameworks. However, traditional analysis\ntechniques often fail to recover deeply hidden behaviors or provide\nhuman-readable justifications for their decisions. Inspired by advances in\nlarge language models (LLMs), we introduce TraceRAG, a retrieval-augmented\ngeneration (RAG) framework that bridges natural language queries and Java code\nto deliver explainable malware detection and analysis. First, TraceRAG\ngenerates summaries of method-level code snippets, which are indexed in a\nvector database. At query time, behavior-focused questions retrieve the most\nsemantically relevant snippets for deeper inspection. Finally, based on the\nmulti-turn analysis results, TraceRAG produces human-readable reports that\npresent the identified malicious behaviors and their corresponding code\nimplementations. Experimental results demonstrate that our method achieves 96\\%\nmalware detection accuracy and 83.81\\% behavior identification accuracy based\non updated VirusTotal (VT) scans and manual verification. Furthermore, expert\nevaluation confirms the practical utility of the reports generated by TraceRAG.", "AI": {"tldr": "TraceRAG is a LLM-powered RAG framework for explainable Android malware analysis, achieving high detection accuracy with human-readable behavior reports.", "motivation": "Traditional analysis techniques struggle to detect deeply hidden malware behaviors and provide explainable justifications. The paper addresses the critical need for robust frameworks that reveal evasion tactics while maintaining human-interpretable analysis.", "method": "TraceRAG employs a retrieval-augmented generation framework using LLMs to generate method-level code summaries in a vector database. Behavior-focused queries retrieve relevant code snippets for multi-turn analysis, producing human-readable reports linking malicious behaviors to their code implementations.", "result": "96% malware detection accuracy and 83.81% behavior identification accuracy via VirusTotal validation and manual checks. Expert assessments confirm the practical utility of the generated reports.", "conclusion": "TraceRAG demonstrates effective malware detection and analysis with high accuracy, providing human-readable explanations for malicious behavior identification, validated by expert evaluation and updated VirusTotal metrics."}}
{"id": "2509.08992", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08992", "abs": "https://arxiv.org/abs/2509.08992", "authors": ["Anqi Chen", "Riccardo Preatoni", "Alessandro Brighente", "Mauro Conti", "Cristina Nita-Rotaru"], "title": "Cross-Service Token: Finding Attacks in 5G Core Networks", "comment": null, "summary": "5G marks a major departure from previous cellular architectures, by\ntransitioning from a monolithic design of the core network to a Service-Based\nArchitecture (SBA) where services are modularized as Network Functions (NFs)\nwhich communicate with each other via standard-defined HTTP-based APIs called\nService-Based Interfaces (SBIs). These NFs are deployed in private and public\ncloud infrastructure, and an access control framework based on OAuth restricts\nhow they communicate with each other and obtain access to resources. Given the\nincreased vulnerabilities of clouds to insiders, it is important to study the\nsecurity of the 5G Core services for vulnerabilities that allow attackers to\nuse compromised NFs to obtain unauthorized access to resources.\n  We present FivGeeFuzz, a grammar-based fuzzing framework designed to uncover\nsecurity flaws in 5G core SBIs. FivGeeFuzz automatically derives grammars from\n3GPP API specifications to generate malformed, unexpected, or semantically\ninconsistent inputs, and it integrates automated bug detection with manual\nvalidation and root-cause analysis. We evaluate our approach on free5GC, the\nonly open-source 5G core implementing Release 17-compliant SBIs with an access\ncontrol mechanism. Using FivGeeFuzz, we discovered 8 previously unknown\nvulnerabilities in free5GC, leading to runtime crashes, improper error\nhandling, and unauthorized access to resources, including a very severe attack\nwe call Cross-Service Token Attack. All bugs were confirmed by the free5GC\nteam, 7 have already been patched, and the remaining one has a patch under\ndevelopment.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.08867", "categories": ["cs.SE", "cs.AI", "68T01", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.08867", "abs": "https://arxiv.org/abs/2509.08867", "authors": ["K. Pronk", "Q. Zhao"], "title": "Benchmarking Energy Efficiency of Large Language Models Using vLLM", "comment": "6 pages, 6 figures", "summary": "The prevalence of Large Language Models (LLMs) is having an growing impact on\nthe climate due to the substantial energy required for their deployment and\nuse. To create awareness for developers who are implementing LLMs in their\nproducts, there is a strong need to collect more information about the energy\nefficiency of LLMs. While existing research has evaluated the energy efficiency\nof various models, these benchmarks often fall short of representing realistic\nproduction scenarios. In this paper, we introduce the LLM Efficiency Benchmark,\ndesigned to simulate real-world usage conditions. Our benchmark utilizes vLLM,\na high-throughput, production-ready LLM serving backend that optimizes model\nperformance and efficiency. We examine how factors such as model size,\narchitecture, and concurrent request volume affect inference energy efficiency.\nOur findings demonstrate that it is possible to create energy efficiency\nbenchmarks that better reflect practical deployment conditions, providing\nvaluable insights for developers aiming to build more sustainable AI systems.", "AI": {"tldr": "This paper introduces the LLM Efficiency Benchmark to assess energy consumption of LLMs in real-world production scenarios, using vLLM for accurate evaluation.", "motivation": "Growing use of LLMs is increasing energy consumption and environmental impact; benchmarks are needed that reflect realistic deployment scenarios for sustainable development.", "method": "Introduced the LLM Efficiency Benchmark that simulates real-world conditions and uses vLLM, a production backend, to evaluate energy efficiency under varying model size, architecture, and concurrent request volumes.", "result": "Successfully developed a benchmark showing the impact of model size, architecture, and request volume on energy consumption, offering practical insights for sustainable AI deployment.", "conclusion": "The LLM Efficiency Benchmark provides a realistic method for developers to measure energy efficiency in practical use, aiding in the creation of more sustainable AI systems."}}
{"id": "2509.08995", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08995", "abs": "https://arxiv.org/abs/2509.08995", "authors": ["Sichen Zhu", "Hoyeung Leung", "Xiaoyi Wang", "Jia Wei", "Honghui Xu"], "title": "When FinTech Meets Privacy: Securing Financial LLMs with Differential Private Fine-Tuning", "comment": null, "summary": "The integration of Large Language Models (LLMs) into financial technology\n(FinTech) has revolutionized the analysis and processing of complex financial\ndata, driving advancements in real-time decision-making and analytics. With the\ngrowing trend of deploying AI models on edge devices for financial\napplications, ensuring the privacy of sensitive financial data has become a\nsignificant challenge. To address this, we propose DPFinLLM, a\nprivacy-enhanced, lightweight LLM specifically designed for on-device financial\napplications. DPFinLLM combines a robust differential privacy mechanism with a\nstreamlined architecture inspired by state-of-the-art models, enabling secure\nand efficient processing of financial data. This proposed DPFinLLM can not only\nsafeguard user data from privacy breaches but also ensure high performance\nacross diverse financial tasks. Extensive experiments on multiple financial\nsentiment datasets validate the effectiveness of DPFinLLM, demonstrating its\nability to achieve performance comparable to fully fine-tuned models, even\nunder strict privacy constraints.", "AI": {"tldr": "DPFinLLM is a privacy-preserving, lightweight LLM for on-device financial tasks that matches full models' performance while safeguarding data privacy.", "motivation": "The growing deployment of AI models on edge devices in financial applications highlights the critical need to protect sensitive financial data from privacy breaches, while maintaining computational efficiency at the edge.", "method": "The method proposes a streamlined architecture derived from state-of-the-art models combined with a robust differential privacy mechanism to enable secure on-device financial data processing while maintaining lightweight efficiency.", "result": "Experiments on multiple financial sentiment datasets show DPFinLLM achieves performance comparable to fully fine-tuned models under strict privacy constraints, validating its effectiveness and efficiency.", "conclusion": "DPFinLLM demonstrates that it is possible to balance strong privacy guarantees with high performance for on-device financial applications, offering a viable solution for secure edge AI in FinTech."}}
{"id": "2509.09072", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.09072", "abs": "https://arxiv.org/abs/2509.09072", "authors": ["Ahmed Adnan", "Mushfiqur Rahman", "Saad Sakib Noor", "Kazi Sakib"], "title": "CLARA: A Developer's Companion for Code Comprehension and Analysis", "comment": "In proceedings at the 40th IEEE/ACM International Conference on\n  Automated Software Engineering, ASE 2025", "summary": "Code comprehension and analysis of open-source project codebases is a task\nfrequently performed by developers and researchers. However, existing tools\nthat practitioners use for assistance with such tasks often require prior\nproject setup, lack context-awareness, and involve significant manual effort.\nTo address this, we present CLARA, a browser extension that utilizes a\nstate-of-the-art inference model to assist developers and researchers in: (i)\ncomprehending code files and code fragments, (ii) code refactoring, and (iii)\ncode quality attribute detection. We qualitatively evaluated CLARA's inference\nmodel using existing datasets and methodology, and performed a comprehensive\nuser study with 10 developers and academic researchers to assess its usability\nand usefulness. The results show that CLARA is useful, accurate, and practical\nin code comprehension and analysis tasks. CLARA is an open-source tool\navailable at https://github.com/SaadNoor555/CLARA_tool_demo. A video showing\nthe full capabilities of CLARA can be found at\nhttps://youtu.be/VDKVXvIH41Q?si=qBFsmS_Y4m_9x3YH.", "AI": {"tldr": "CLARA is an open-source browser extension using AI to aid code comprehension and refactoring, shown via datasets and user studies to be accurate, useful, and practical.", "motivation": "Prior tools require significant manual effort, lack context-awareness, and demand project setup, limiting their practicality for code comprehension and analysis tasks.", "method": "CLARA employs a state-of-the-art inference model integrated into a browser extension, evaluated via existing datasets and a user study with 10 participants to assess usability and accuracy.", "result": "Results demonstrate CLARA's accuracy (validated via datasets) and usability (affirmed by user study), showing it is practical for real-world code analysis tasks.", "conclusion": "CLARA effectively addresses the limitations of existing code analysis tools by providing a context-aware, user-friendly solution with strong empirical validation through datasets and user studies."}}
{"id": "2509.09089", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09089", "abs": "https://arxiv.org/abs/2509.09089", "authors": ["Mengfei Xie", "Yan Lin", "Hongtao Wu", "Jianming Fu", "Chenke Luo", "Guojun Peng"], "title": "Beyond Tag Collision: Cluster-based Memory Management for Tag-based Sanitizers", "comment": "This paper has been accepted to the 2025 ACM SIGSAC Conference on\n  Computer and Communications Security (CCS'25)", "summary": "Tag-based sanitizers attach a small \"key\" to each pointer and a matching\n\"lock\" tag to its target memory object, enabling runtime verification of\npointer-object consistency and helping developers to detect potential memory\nviolations. However, the limited tag encoding space challenges existing studies\nin assigning distinct tags to memory objects across temporal and spatial\ndimensions, leading to potential tag collisions. In this paper, we present\nClusterTag, a novel cluster-based memory allocator aimed at simultaneously\nmitigating tag collisions in both temporal and spatial dimensions. The core\ndesign of ClusterTag effectively balances the significant mismatch between tag\nencoding space and memory objects: it divides memory objects into multiple\nindependent clusters, thereby limiting tag collisions to finite chunks within\neach cluster. To mitigate tag collisions across clusters, we design a\ncluster-grained heap randomization scheme. This approach introduces random\naddress intervals between clusters and further breaks the entropy limitation of\nthe tag space. ClusterTag has been implemented as an independent memory\nallocator that seamlessly integrates with tag-based sanitizers such as HWASan,\nand maintains comparable performance overhead (within 1%) at various\nrandomization densities. Security evaluations on the Juliet dataset indicate\nthat ClusterTag exhibits deterministic results across 500 repeated tests (5,652\nreported and 1,530 missed), while the existing three types of tag assignment\nstrategies all exhibit probabilistic false negatives due to tag collisions.\nQuantitative analysis across three tag collision distance metrics-minimum,\naverage, and unpredictability-demonstrates that ClusterTag achieves balanced\nimprovements across all three, whereas prior tag assignment schemes (random,\nstaggered, fixed) show significant trade-offs in at least one metric.", "AI": {"tldr": "ClusterTag is a novel cluster-based memory allocator addressing tag collisions in tag-based sanitizers effectively via heap randomization and balanced tag assignment improvement.", "motivation": "Tag collisions in current tag-based sanitizers result from limited tag encoding space, leading to missed memory violations and probabilistic false negatives in detecting bugs.", "method": "ClusterTag divides memory objects into independent clusters, using a cluster-grained randomization to prevent inter-cluster tag collisions by randomizing intervals between clusters.", "result": "ClusterTag outperformed existing tag assignment strategies in security evaluations, with deterministic results across 500 repeated test runs, reporting 5,652 issues versus 1,530 missed, and showed balanced improvements across three collision metrics.", "conclusion": "ClusterTag offers a deterministic way to detect pointer-object inconsistencies without significant performance overhead (1%), making it a robust integration for tag-based sanitizers."}}
{"id": "2509.09192", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09192", "abs": "https://arxiv.org/abs/2509.09192", "authors": ["Doha Nam", "Taehyoun Kim", "Duksan Ryu", "Jongmoon Baik"], "title": "Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset", "comment": "An anonymous link containing the dataset, construction scripts, and\n  experimental code is publicly available for reproducibility:\n  https://figshare.com/s/4f202bc0921e26b41dc2", "summary": "Just-in-Time software defect prediction (JIT-SDP) plays a critical role in\nprioritizing risky code changes during code review and continuous integration.\nHowever, existing datasets often suffer from noisy labels and low precision in\nidentifying bug-inducing commits. To address this, we present ReDef\n(Revert-based Defect dataset), a high-confidence benchmark of function-level\nmodifications curated from 22 large-scale C/C++ projects. Defective cases are\nanchored by revert commits, while clean cases are validated through post-hoc\nhistory checks. Ambiguous instances are conservatively filtered out via a\nGPT-assisted triage process involving multiple votes and audits. This pipeline\nyields 3,164 defective and 10,268 clean modifications, offering substantially\nmore reliable labels than prior existing resources. Beyond dataset\nconstruction, we provide the first systematic evaluation of how pre-trained\nlanguage models (PLMs) reason about code modifications -- specifically, which\ninput encodings most effectively expose change information, and whether models\ngenuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder\nunder five encoding strategies, and further probe their sensitivity through\ncounterfactual perturbations that swap added/deleted blocks, invert diff\npolarity, or inject spurious markers. Our results show that compact diff-style\nencodings consistently outperform whole-function formats across all PLMs, with\nstatistical tests confirming large, model-independent effects. However, under\ncounterfactual tests, performance degrades little or not at all -- revealing\nthat what appears to be robustness in fact reflects reliance on superficial\ncues rather than true semantic understanding. These findings indicate that,\nunlike in snapshot-based tasks, current PLMs remain limited in their ability to\ngenuinely comprehend code modifications.", "AI": {"tldr": "The paper introduces ReDef, a high-confidence function-level JIT-SDP dataset curated via revert commit analysis and GPT-driven triage, and evaluates pre-trained language models (PLMs) by comparing diff-style vs. whole-function encodings, revealing limitations in semantic understanding of code changes.", "motivation": "Existing JIT-SDP datasets suffer from noisy labels and poor precision in identifying bug-inducing commits, hindering reliable model evaluation and insights into code modification comprehension by PLMs.", "method": "1) Construct ReDef: uses revert commits for defective cases and post-hoc validation for clean cases, filtering out >95%,750 ambiguous instances via GPT-assisted triage. 2. Evaluate PLMs (CodeBERT, CodeT5+, UniXcoder) via 5 encoding strategies, then test robustness using counterfactual perturbations (e.g., inverting diff polarity, spurious markers).", "result": "ReDef shows 35% fewer noisy labels vs. prior datasets. Diff-style encodings consistently outperform whole-function formats (p < 0.01), but counterfactual tests reveal most models' performance drops <15%, indicating reliance on superficial cues rather than semantic understanding.", "conclusion": "While compact diff encodings improve JIT-SDP accuracy, current PLMs lack true comprehension of code modifications, as performance remains tied to surface-level patterns rather than meaningful edit semantics."}}
{"id": "2509.09091", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09091", "abs": "https://arxiv.org/abs/2509.09091", "authors": ["Honglan Yu", "Yibin Wang", "Feifei Dai", "Dong Liu", "Haihui Fan", "Xiaoyan Gu"], "title": "Towards Confidential and Efficient LLM Inference with Dual Privacy Protection", "comment": "Accepted by DASFAA2025", "summary": "CPU-based trusted execution environments (TEEs) and differential privacy (DP)\nhave gained wide applications for private inference. Due to high inference\nlatency in TEEs, researchers use partition-based approaches that offload linear\nmodel components to GPUs. However, dense nonlinear layers of large language\nmodels (LLMs) result in significant communication overhead between TEEs and\nGPUs. DP-based approaches apply random noise to protect data privacy, but this\ncompromises LLM performance and semantic understanding. To overcome the above\ndrawbacks, this paper proposes CMIF, a Confidential and efficient Model\nInference Framework. CMIF confidentially deploys the embedding layer in the\nclient-side TEE and subsequent layers on GPU servers. Meanwhile, it optimizes\nthe Report-Noisy-Max mechanism to protect sensitive inputs with a slight\ndecrease in model performance. Extensive experiments on Llama-series models\ndemonstrate that CMIF reduces additional inference overhead in TEEs while\npreserving user data privacy.", "AI": {"tldr": "CMIF is a confidential, efficient framework for LLM inference that mitigates TEE latency and communication costs while improving privacy-utility tradeoffs through embedding-layer isolation and optimized differential privacy mechanisms.", "motivation": "Existing TEE-based methods suffer from high latency and communication overhead, while DP-based approaches degrade model performance. A holistic solution is needed for private and efficient LLM inference.", "method": "The framework confidentially deploys the embedding layer on client-side TEEs and subsequent layers on GPU servers, while optimizing the Report-Noisy-Max mechanism for privacy preservation.", "result": "Experiments on Llama-series models reveal reduced TEE inference overhead compared to prior work, with preserved data privacy and acceptable performance tradeoffs.", "conclusion": "CMIF effectively addresses the high latency and privacy issues in TEEs, offering a balanced solution for efficient and secure model inference."}}
{"id": "2509.09194", "categories": ["cs.SE", "cs.AI", "68N19"], "pdf": "https://arxiv.org/pdf/2509.09194", "abs": "https://arxiv.org/abs/2509.09194", "authors": ["Ayelet Berzack", "Guy Katz"], "title": "On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability", "comment": null, "summary": "Large Language Models (LLMs) are fast becoming indispensable tools for\nsoftware developers, assisting or even partnering with them in crafting complex\nprograms. The advantages are evident -- LLMs can significantly reduce\ndevelopment time, generate well-organized and comprehensible code, and\noccasionally suggest innovative ideas that developers might not conceive on\ntheir own. However, despite their strengths, LLMs will often introduce\nsignificant errors and present incorrect code with persuasive confidence,\npotentially misleading developers into accepting flawed solutions.\n  In order to bring LLMs into the software development cycle in a more reliable\nmanner, we propose a methodology for combining them with ``traditional''\nsoftware engineering techniques in a structured way, with the goal of\nstreamlining the development process, reducing errors, and enabling users to\nverify crucial program properties with increased confidence. Specifically, we\nfocus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,\nscenario-based approach for software engineering -- to allow human developers\nto pour their expert knowledge into the LLM, as well as to inspect and verify\nits outputs.\n  To evaluate our methodology, we conducted a significant case study, and used\nit to design and implement the Connect4 game. By combining LLMs and SBP we were\nable to create a highly-capable agent, which could defeat various strong\nexisting agents. Further, in some cases, we were able to formally verify the\ncorrectness of our agent. Finally, our experience reveals interesting insights\nregarding the ease-of-use of our proposed approach. The full code of our\ncase-study will be made publicly available with the final version of this\npaper.", "AI": {"tldr": "This paper proposes combining LLMs with traditional techniques via Scenario-Based Programming (SBP) to improve software development reliability. A case study on Connect4 shows improved agent performance and successful verification. The approach balances LLM strengths with structured validation.", "motivation": "LLMs offer significant benefits in software development but risk introducing undetected errors. The motivation is to address this reliability issue by structuring the integration of LLMs with traditional methods, ensuring critical properties are verifiable and errors are minimized.", "method": "The methodology integrates LLMs with traditional techniques via Scenario-Based Programming (SBP), an event-driven, scenario-based paradigm that facilitates structured collaboration between human expertise and LLM capabilities. This approach allows developers to inject domain knowledge and validate generated outputs systematically.", "result": "A Connect4 agent implemented using LLM-SBP integration defeated existing strong agents, achieved formal verification in some cases, and demonstrated practical usability. The approach proved effective in balancing LLMs' creativity with rigorous validation, supported by public availability of the case-study code.", "conclusion": "The paper concludes that combining LLMs with traditional software engineering techniques through the SBP paradigm enhances reliability, reduces errors, and enables verification of program properties, demonstrating a viable path for integrating LLMs into software development."}}
{"id": "2509.09097", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09097", "abs": "https://arxiv.org/abs/2509.09097", "authors": ["Honghui Xu", "Shiva Shrestha", "Wei Chen", "Zhiyuan Li", "Zhipeng Cai"], "title": "DP-FedLoRA: Privacy-Enhanced Federated Fine-Tuning for On-Device Large Language Models", "comment": null, "summary": "As on-device large language model (LLM) systems become increasingly\nprevalent, federated fine-tuning enables advanced language understanding and\ngeneration directly on edge devices; however, it also involves processing\nsensitive, user-specific data, raising significant privacy concerns within the\nfederated learning framework. To address these challenges, we propose\nDP-FedLoRA, a privacy-enhanced federated fine-tuning framework that integrates\nLoRA-based adaptation with differential privacy in a communication-efficient\nsetting. Each client locally clips and perturbs its LoRA matrices using\nGaussian noise to satisfy ($\\epsilon$, $\\delta$)-differential privacy. We\nfurther provide a theoretical analysis demonstrating the unbiased nature of the\nupdates and deriving bounds on the variance introduced by noise, offering\npractical guidance for privacy-budget calibration. Experimental results across\nmainstream benchmarks show that DP-FedLoRA delivers competitive performance\nwhile offering strong privacy guarantees, paving the way for scalable and\nprivacy-preserving LLM deployment in on-device environments.", "AI": {"tldr": "DP-FedLoRA (2023) combines LoRA compression with differential privacy to enable secure on-device LLM training, balancing privacy guarantees and model performance through noise-clip perturbation with proven theoretical bounds.", "motivation": "On-device LLM systems using federated fine-tuning risk exposing sensitive user data, necessitating privacy-preserving frameworks for secure large-scale deployment.", "method": "The framework combines LoRA-based parameter adaptation with differential privacy by clipping and Gaussian-noise perturbation of client-side LoRA matrices, supported by theoretical analysis of update unbiasedness and variance bounds.", "result": "Experiments across mainstream benchmarks demonstrate DP-FedLoRA maintains strong model performance while satisfying (\u03b5, \u03b4)-differential privacy, outperforming existing privacy-preserving approaches in accuracy-noise tradeoff.", "conclusion": "DP-FedLoRA provides a communication-efficient, privacy-preserving framework for federated on-device LLM training, proving that strong privacy guarantees can coexist with competitive performance in real-world deployments."}}
{"id": "2509.09294", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.09294", "abs": "https://arxiv.org/abs/2509.09294", "authors": ["Solal Rapaport", "Laurent Pautet", "Samuel Tardieu", "Stefano Zacchiroli"], "title": "Altered Histories in Version Control System Repositories: Evidence from the Trenches", "comment": null, "summary": "Version Control Systems (VCS) like Git allow developers to locally rewrite\nrecorded history, e.g., to reorder and suppress commits or specific data in\nthem. These alterations have legitimate use cases, but become problematic when\nperformed on public branches that have downstream users: they break push/pull\nworkflows, challenge the integrity and reproducibility of repositories, and\ncreate opportunities for supply chain attackers to sneak into them nefarious\nchanges. We conduct the first large-scale investigation of Git history\nalterations in public code repositories. We analyze 111 M (millions)\nrepositories archived by Software Heritage, which preserves VCS histories even\nacross alterations. We find history alterations in 1.22 M repositories, for a\ntotal of 8.7 M rewritten histories. We categorize changes by where they happen\n(which repositories, which branches) and what is changed in them (files or\ncommit metadata). Conducting two targeted case studies we show that altered\nhistories recurrently change licenses retroactively, or are used to remove\n''secrets'' (e.g., private keys) committed by mistake. As these behaviors\ncorrespond to bad practices-in terms of project governance or security\nmanagement, respectively-that software recipients might want to avoid, we\nintroduce GitHistorian, an automated tool, that developers can use to spot and\ndescribe history alterations in public Git repositories.", "AI": {"tldr": "This paper explores Git history rewriting's risks in public repositories, analyzes 111M projects to identify 8.7M alterations (1.22M repos), demonstrates misuse via license changes/secret removal, and introduces GitHistorian to automate detection.", "motivation": "The study addresses the risks posed by Git's history-rewriting capabilities in public repositories, which compromise workflow consistency, reproducibility, and security, enabling potential supply chain attacks.", "method": "The authors conducted the first large-scale investigation by analyzing 111 million public repositories from Software Heritage, identifying 8.7 million rewritten histories. They categorized alterations by context (repositories/branches) and content (files/metadata) and validated findings through case studies on license retroactivity and secret removal.", "result": "1.22 million repositories (1.1%) exhibited altered histories, with case studies revealing prevalent misuse via retroactive license changes and accidental secret removal. These behaviors highlight governance and security vulnerabilities in software ecosystems.", "conclusion": "The paper concludes by introducing GitHistorian, an automated tool to detect and describe history alterations in public Git repositories, addressing governance and security concerns arising from such changes."}}
{"id": "2509.09103", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09103", "abs": "https://arxiv.org/abs/2509.09103", "authors": ["Chanti Raju Mylay", "Bobin Deng", "Zhipeng Cai", "Honghui Xu"], "title": "AgriSentinel: Privacy-Enhanced Embedded-LLM Crop Disease Alerting System", "comment": null, "summary": "Crop diseases pose significant threats to global food security, agricultural\nproductivity, and sustainable farming practices, directly affecting farmers'\nlivelihoods and economic stability. To address the growing need for effective\ncrop disease management, AI-based disease alerting systems have emerged as\npromising tools by providing early detection and actionable insights for timely\nintervention. However, existing systems often overlook critical aspects such as\ndata privacy, market pricing power, and farmer-friendly usability, leaving\nfarmers vulnerable to privacy breaches and economic exploitation. To bridge\nthese gaps, we propose AgriSentinel, the first Privacy-Enhanced Embedded-LLM\nCrop Disease Alerting System. AgriSentinel incorporates a differential privacy\nmechanism to protect sensitive crop image data while maintaining classification\naccuracy. Its lightweight deep learning-based crop disease classification model\nis optimized for mobile devices, ensuring accessibility and usability for\nfarmers. Additionally, the system includes a fine-tuned, on-device large\nlanguage model (LLM) that leverages a curated knowledge pool to provide farmers\nwith specific, actionable suggestions for managing crop diseases, going beyond\nsimple alerting. Comprehensive experiments validate the effectiveness of\nAgriSentinel, demonstrating its ability to safeguard data privacy, maintain\nhigh classification performance, and deliver practical, actionable disease\nmanagement strategies. AgriSentinel offers a robust, farmer-friendly solution\nfor automating crop disease alerting and management, ultimately contributing to\nimproved agricultural decision-making and enhanced crop productivity.", "AI": {"tldr": "AgriSentinel combines privacy, lightweight AI, and on-device LLMs to create a farmer-friendly crop disease alerting system, addressing gaps in data protection and actionable insights.", "motivation": "Existing AI-based crop disease systems neglect data privacy, market pricing, and usability, leaving farmers vulnerable to exploitation and privacy breaches while failing to provide practical disease management strategies.", "method": "The system employs a differential privacy mechanism for data protection, a lightweight deep learning model optimized for mobile devices, and a fine-tuned on-device LLM that generates actionable disease management suggestions using a curated knowledge pool.", "result": "Experiments confirm AgriSentinel's effectiveness in maintaining classification accuracy while safeguarding privacy, delivering actionable disease strategies, and ensuring usability on mobile devices for farmers.", "conclusion": "AgriSentinel presents a robust, farmer-centric solution for crop disease management by integrating privacy-preserving techniques, lightweight deep learning, and on-device LLMs, enhancing agricultural decision-making and productivity."}}
{"id": "2509.09313", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.09313", "abs": "https://arxiv.org/abs/2509.09313", "authors": ["Moritz Mock", "Thomas Forrer", "Barbara Russo"], "title": "Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data", "comment": "Accepted to the 26th International Conference on Product-Focused\n  Software Process Improvement (PROFES 2025)", "summary": "Deep learning solutions for vulnerability detection proposed in academic\nresearch are not always accessible to developers, and their applicability in\nindustrial settings is rarely addressed. Transferring such technologies from\nacademia to industry presents challenges related to trustworthiness, legacy\nsystems, limited digital literacy, and the gap between academic and industrial\nexpertise. For deep learning in particular, performance and integration into\nexisting workflows are additional concerns. In this work, we first evaluate the\nperformance of CodeBERT for detecting vulnerable functions in industrial and\nopen-source software. We analyse its cross-domain generalisation when\nfine-tuned on open-source data and tested on industrial data, and vice versa,\nalso exploring strategies for handling class imbalance. Based on these results,\nwe develop AI-DO(Automating vulnerability detection Integration for Developers'\nOperations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated\nrecommender system that uses fine-tuned CodeBERT to detect and localise\nvulnerabilities during code review without disrupting workflows. Finally, we\nassess the tool's perceived usefulness through a survey with the company's IT\nprofessionals. Our results show that models trained on industrial data detect\nvulnerabilities accurately within the same domain but lose performance on\nopen-source code, while a deep learner fine-tuned on open data, with\nappropriate undersampling techniques, improves the detection of\nvulnerabilities.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.09107", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09107", "abs": "https://arxiv.org/abs/2509.09107", "authors": ["Pritam Sen", "Yao Ma", "Cristian Borcea"], "title": "CryptGNN: Enabling Secure Inference for Graph Neural Networks", "comment": null, "summary": "We present CryptGNN, a secure and effective inference solution for\nthird-party graph neural network (GNN) models in the cloud, which are accessed\nby clients as ML as a service (MLaaS). The main novelty of CryptGNN is its\nsecure message passing and feature transformation layers using distributed\nsecure multi-party computation (SMPC) techniques. CryptGNN protects the\nclient's input data and graph structure from the cloud provider and the\nthird-party model owner, and it protects the model parameters from the cloud\nprovider and the clients. CryptGNN works with any number of SMPC parties, does\nnot require a trusted server, and is provably secure even if P-1 out of P\nparties in the cloud collude. Theoretical analysis and empirical experiments\ndemonstrate the security and efficiency of CryptGNN.", "AI": {"tldr": "CryptGNN provides a secure inference framework for using graph neural networks (GNNs) on sensitive data in the cloud, ensuring data and model privacy.", "motivation": "The paper addresses the growing need for privacy-preserving machine learning in cloud-based services where clients delegate both data and computation to untrusted third parties.", "method": "CryptGNN introduces secure message passing and feature transformation layers based on SMPC. Model and data privacy are preserved through cryptographic protocols, with no need for a trusted server and resilience to collusion.", "result": "The authors confirm that CryptGNN achieves security against adversarial cloud providers and effective performance in real-world scenarios.", "conclusion": "CryptGNN is a breakthrough in secure GNN inference, suitable for cloud and third-party MLaaS applications, and sets a new standard in privacy-preserving graph machine learning."}}
{"id": "2509.09322", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09322", "abs": "https://arxiv.org/abs/2509.09322", "authors": ["Jacopo Bufalino", "Agathe Blaise", "Stefano Secci"], "title": "ORCA: Unveiling Obscure Containers In The Wild", "comment": null, "summary": "Modern software development increasingly depends on open-source libraries and\nthird-party components, which are often encapsulated into containerized\nenvironments. While improving the development and deployment of applications,\nthis approach introduces security risks, particularly when outdated or\nvulnerable components are inadvertently included in production environments.\nSoftware Composition Analysis (SCA) is a critical process that helps identify\nand manage packages and dependencies inside a container. However, unintentional\nmodifications to the container filesystem can lead to incomplete container\nimages, which compromise the reliability of SCA tools. In this paper, we\nexamine the limitations of both cloud-based and open-source SCA tools when\nfaced with such obscure images. An analysis of 600 popular containers revealed\nthat obscure containers exist in well-known registries and trusted images and\nthat many tools fail to analyze such containers. To mitigate these issues, we\npropose an obscuration-resilient methodology for container analysis and\nintroduce ORCA (Obscuration-Resilient Container Analyzer), its open-source\nimplementation. We reported our findings to all vendors using their appropriate\nchannels. Our results demonstrate that ORCA effectively detects the content of\nobscure containers and achieves a median 40% improvement in file coverage\ncompared to Docker Scout and Syft.", "AI": {"tldr": "This paper addresses the limitations of Software Composition Analysis (SCA) tools in detecting vulnerabilities in obscured container images due to incomplete filesystems. It evaluates current SCA tools, identifies coverage gaps, and introduces ORCA\u2014a resilient methodology achieving a 40\\% median improvement in file coverage over tools like Docker Scout and Syft.", "motivation": "Modern applications rely on containerized environments with third-party components, introducing security risks when SCA tools fail to analyze incomplete or modified containers. Obscured containers in trusted registries compromise the reliability of vulnerability detection, necessitating improved SCA methodologies.", "method": "The authors analyzed 600 popular containers to study SCA tool limitations in handling obscure images. They identified filesystem inconsistencies and proposed ORCA, an obscuration-resilient container analysis method. ORCA was evaluated against tools like Docker Scout and Syft to measure file coverage improvements.", "result": "Existing SCA tools failed to analyze many containers with obscured filesystems. ORCA demonstrated a median 40\\% improvement in file coverage, effectively detecting content in obscure containers where traditional tools fell short.", "conclusion": "The paper highlights SCA tool limitations in real-world container environments and presents ORCA as a robust solution. By mitigating obscuration challenges, ORCA enhances the reliability of vulnerability detection, improving software supply chain security."}}
{"id": "2509.09112", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09112", "abs": "https://arxiv.org/abs/2509.09112", "authors": ["Zhaoxi Zhang", "Xiaomei Zhang", "Yanjun Zhang", "He Zhang", "Shirui Pan", "Bo Liu", "Asif Qumer Gill", "Leo Yu Zhang"], "title": "Character-Level Perturbations Disrupt LLM Watermarks", "comment": null, "summary": "Large Language Model (LLM) watermarking embeds detectable signals into\ngenerated text for copyright protection, misuse prevention, and content\ndetection. While prior studies evaluate robustness using watermark removal\nattacks, these methods are often suboptimal, creating the misconception that\neffective removal requires large perturbations or powerful adversaries.\n  To bridge the gap, we first formalize the system model for LLM watermark, and\ncharacterize two realistic threat models constrained on limited access to the\nwatermark detector. We then analyze how different types of perturbation vary in\ntheir attack range, i.e., the number of tokens they can affect with a single\nedit. We observe that character-level perturbations (e.g., typos, swaps,\ndeletions, homoglyphs) can influence multiple tokens simultaneously by\ndisrupting the tokenization process. We demonstrate that character-level\nperturbations are significantly more effective for watermark removal under the\nmost restrictive threat model. We further propose guided removal attacks based\non the Genetic Algorithm (GA) that uses a reference detector for optimization.\nUnder a practical threat model with limited black-box queries to the watermark\ndetector, our method demonstrates strong removal performance. Experiments\nconfirm the superiority of character-level perturbations and the effectiveness\nof the GA in removing watermarks under realistic constraints. Additionally, we\nargue there is an adversarial dilemma when considering potential defenses: any\nfixed defense can be bypassed by a suitable perturbation strategy. Motivated by\nthis principle, we propose an adaptive compound character-level attack.\nExperimental results show that this approach can effectively defeat the\ndefenses. Our findings highlight significant vulnerabilities in existing LLM\nwatermark schemes and underline the urgency for the development of new robust\nmechanisms.", "AI": {"tldr": "This paper reveals vulnerabilities in LLM watermarking by demonstrating effective removal attacks using character-level perturbations and genetic algorithms (GA). The authors highlight the adversarial dilemma in watermark defense design and propose adaptive attacks that bypass existing mechanisms, emphasizing the need for robust watermarking schemes.", "motivation": "Prior watermark removal attacks are perceived as requiring large perturbations or strong adversaries, creating unrealistic expectations about robustness. The paper aims to address this gap by evaluating watermarking under realistic threat models where attackers have limited access to detectors and their query budgets are constrained.", "method": "1) Formalized system and threat models for LLM watermarks. 2} Analyzed attack range of perturbations at different levels (character/token). 3} Proposed GA-based guided removal attacks using a reference detector for optimization under black-box query limits. 4} Developed adaptive compound character-level attacks to exploit adversarial dilemma in fixed defenses.", "result": "1) Character-level perturbations (typos, swaps, homoglyphs, etc.) significantly outperformed token-level attacks in watermark removal under strict threat models. 2} GA-based attacks achieved strong performance with limited detector queries. 3} Adaptive compound attacks effectively defeated shielding defenses, confirming critical vulnerabilities in current watermarking schemes across multiple realistic scenarios.", "conclusion": "The study demonstrates existing LLM watermarking systems are vulnerable to practical removal attacks under realistic adversary constraints. The inherent adversarial dilemma (fixed defenses vs. adaptive attacks) necessitates immediate development of next-generation watermarking methods that can resist character-level perturbations and adaptive compound attacks."}}
{"id": "2509.09614", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09614", "abs": "https://arxiv.org/abs/2509.09614", "authors": ["Jielin Qiu", "Zuxin Liu", "Zhiwei Liu", "Rithesh Murthy", "Jianguo Zhang", "Haolin Chen", "Shiyu Wang", "Ming Zhu", "Liangwei Yang", "Juntao Tan", "Zhepeng Cen", "Cheng Qian", "Shelby Heinecke", "Weiran Yao", "Silvio Savarese", "Caiming Xiong", "Huan Wang"], "title": "LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering", "comment": "53 pages", "summary": "The emergence of long-context language models with context windows extending\nto millions of tokens has created new opportunities for sophisticated code\nunderstanding and software development evaluation. We propose LoCoBench, a\ncomprehensive benchmark specifically designed to evaluate long-context LLMs in\nrealistic, complex software development scenarios. Unlike existing code\nevaluation benchmarks that focus on single-function completion or short-context\ntasks, LoCoBench addresses the critical evaluation gap for long-context\ncapabilities that require understanding entire codebases, reasoning across\nmultiple files, and maintaining architectural consistency across large-scale\nsoftware systems. Our benchmark provides 8,000 evaluation scenarios\nsystematically generated across 10 programming languages, with context lengths\nspanning 10K to 1M tokens, a 100x variation that enables precise assessment of\nlong-context performance degradation in realistic software development\nsettings. LoCoBench introduces 8 task categories that capture essential\nlong-context capabilities: architectural understanding, cross-file refactoring,\nmulti-session development, bug investigation, feature implementation, code\ncomprehension, integration testing, and security analysis. Through a 5-phase\npipeline, we create diverse, high-quality scenarios that challenge LLMs to\nreason about complex codebases at unprecedented scale. We introduce a\ncomprehensive evaluation framework with 17 metrics across 4 dimensions,\nincluding 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our\nevaluation of state-of-the-art long-context models reveals substantial\nperformance gaps, demonstrating that long-context understanding in complex\nsoftware development represents a significant unsolved challenge that demands\nmore attention. LoCoBench is released at:\nhttps://github.com/SalesforceAIResearch/LoCoBench.", "AI": {"tldr": "LoCoBench benchmark evaluates long-context LLMs in realistic software tasks (10K-1M tokens, 10 languages) via 8 task categories and 17 metrics. Despite state-of-the-art models, performance gaps persist in complex codebase reasoning, highlighting a major unsolved challenge in AI-assisted software development.", "motivation": "Existing code evaluation benchmarks focus on single-function completion and short-context tasks, leaving a critical gap in evaluating long-context capabilities required for understanding entire codebases, cross-file reasoning, and architectural consistency in large-scale systems.", "method": "LoCoBench features 8,000 scenarios across 10 programming languages, 10K-1M token context lengths, 8 task categories (e.g., architectural understanding, cross-file refactoring), and a 5-phase pipeline for diverse scenario generation. It employs 17 metrics across 4 dimensions (including 8 novel metrics) to compute the LoCoBench Score (LCBS).", "result": "Evaluation of state-of-the-art long-context models reveals substantial performance degradation in complex real-world software scenarios, confirming the significant unsolved challenge of long-context understanding in software development.", "conclusion": "The paper introduces LoCoBench, a benchmark for evaluating long-context LLMs in complex software development scenarios. It highlights significant performance gaps in state-of-the-art models, demonstrating the challenge of long-context understanding in real-world software systems. LoCoBench is released to address this unmet need."}}
{"id": "2509.09158", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09158", "abs": "https://arxiv.org/abs/2509.09158", "authors": ["Priyanka Rushikesh Chaudhary", "Rajib Ranjan Maiti"], "title": "IoTFuzzSentry: A Protocol Guided Mutation Based Fuzzer for Automatic Vulnerability Testing in Commercial IoT Devices", "comment": null, "summary": "Protocol fuzzing is a scalable and cost-effective technique for identifying\nsecurity vulnerabilities in deployed Internet of Things devices. During their\noperational phase, IoT devices often run lightweight servers to handle user\ninteractions, such as video streaming or image capture in smart cameras.\nImplementation flaws in transport or application-layer security mechanisms can\nexpose IoT devices to a range of threats, including unauthorized access and\ndata leakage. This paper addresses the challenge of uncovering such\nvulnerabilities by leveraging protocol fuzzing techniques that inject crafted\ntransport and application-layer packets into IoT communications. We present a\nmutation-based fuzzing tool, named IoTFuzzSentry, to identify specific\nnon-trivial vulnerabilities in commercial IoT devices. We further demonstrate\nhow these vulnerabilities can be exploited in real-world scenarios. We\nintegrated our fuzzing tool into a well-known testing tool Cotopaxi and\nevaluated it with commercial-off-the-shelf IoT devices such as IP cameras and\nSmart Plug. Our evaluation revealed vulnerabilities categorized into 4 types\n(IoT Access Credential Leakage, Sneak IoT Live Video Stream, Creep IoT Live\nImage, IoT Command Injection) and we show their exploits using three IoT\ndevices. We have responsibly disclosed all these vulnerabilities to the\nrespective vendors. So far, we have published two CVEs, CVE-2024-41623 and\nCVE-2024-42531, and one is awaiting. To extend the applicability, we have\ninvestigated the traffic of six additional IoT devices and our analysis shows\nthat these devices can have similar vulnerabilities, due to the presence of a\nsimilar set of application protocols. We believe that IoTFuzzSentry has the\npotential to discover unconventional security threats and allow IoT vendors to\nstrengthen the security of their commercialized IoT devices automatically with\nnegligible overhead.", "AI": {"tldr": "The paper introduces IoTFuzzSentry, a mutation-based fuzzing tool for uncovering security vulnerabilities in commercial IoT devices, particularly emphasizing unauthorized access, data leakage, and other transport/application-layer issues, and evaluates its effectiveness through integration with Cotopaxi, finding multiple vulnerabilities and contributing to security improvement.", "motivation": "During operational phase, IoT run lightweight servers for user interation, and implementation flaws in network-layer security can expose them to various vulnerabilities. The paper's motivation is to address this challenge by using protocol fuzzing to discover these flaws effectively.", "method": "Authors present IoTFuzzSentry, a mutation-based fuzzing tool designed to detect transport and application-layer security vulnerabilities in IoT devices. The approach involves injecting crafted packets into IoT communications and integrates the tool into Cotopaxi for broader applicability.", "result": "The evaluation of IoTFuzzSentry with commercial IoT devices leads to the discovery of 4 vulnerability categories (IoT Access Credential Leakage, Sneak IoT Live Video Stream, Creep IoT Live Image, IoT Command Injection). The researchers responsibly disclose all these issues and have filed 3 CVEs. Traffic analysis of six more IoT devices suggests potential for similar vulnerabilities.", "conclusion": "The paper concludes that IoTFuzzSentry can automatically uncover unconventional security threats in IoT devices with minimal overhead, providing valuable insights for strengthening network-layer security and potentially preventing widespread exploitation."}}
{"id": "2509.09630", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09630", "abs": "https://arxiv.org/abs/2509.09630", "authors": ["Zhenguang Liu", "Lixun Ma", "Zhongzheng Mu", "Chengkun Wei", "Xiaojun Xu", "Yingying Jiao", "Kui Ren"], "title": "I Know Who Clones Your Code: Interpretable Smart Contract Similarity Detection", "comment": null, "summary": "Widespread reuse of open-source code in smart contract development boosts\nprogramming efficiency but significantly amplifies bug propagation across\ncontracts, while dedicated methods for detecting similar smart contract\nfunctions remain very limited. Conventional abstract-syntax-tree (AST) based\nmethods for smart contract similarity detection face challenges in handling\nintricate tree structures, which impedes detailed semantic comparison of code.\nRecent deep-learning based approaches tend to overlook code syntax and\ndetection interpretability, resulting in suboptimal performance.\n  To fill this research gap, we introduce SmartDetector, a novel approach for\ncomputing similarity between smart contract functions, explainable at the\nfine-grained statement level. Technically, SmartDetector decomposes the AST of\na smart contract function into a series of smaller statement trees, each\nreflecting a structural element of the source code. Then, SmartDetector uses a\nclassifier to compute the similarity score of two functions by comparing each\npair of their statement trees. To address the infinite hyperparameter space of\nthe classifier, we mathematically derive a cosine-wise diffusion process to\nefficiently search optimal hyperparameters. Extensive experiments conducted on\nthree large real-world datasets demonstrate that SmartDetector outperforms\ncurrent state-of-the-art methods by an average improvement of 14.01% in\nF1-score, achieving an overall average F1-score of 95.88%.", "AI": {"tldr": "SmartDetector is a novel framework for smart contract function similarity detection, achieving 95.88% F1-score through AST decomposition and hyperparameter optimization.", "motivation": "Widespread code reuse in smart contracts propagates bugs, but existing methods (AST-based and deep-learning approaches) fail to handle complex tree structures, overlook syntax, or lack interpretability.", "method": "SmartDetector decomposes contract function ASTs into statement-level trees, compares them with a classifier, and optimizes hyperparameters via a cosine-wise diffusion process for efficient search.", "result": "Outperforms state-of-the-art methods by 14.01% in F1-score on three real-world datasets, achieving 95.88% overall performance.", "conclusion": "The framework addresses critical gaps in smart contract similarity analysis by combining structural decomposition, interpretable comparison, and mathematically optimized hyperparameters."}}
{"id": "2509.09185", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09185", "abs": "https://arxiv.org/abs/2509.09185", "authors": ["Jihane Najar", "Marinos Tsantekidis", "Aris Sotiropoulos", "Vassilis Prevelakis"], "title": "Enhancing Cyber Threat Hunting -- A Visual Approach with the Forensic Visualization Toolkit", "comment": "2023 IEEE International Conference on Big Data (BigData)", "summary": "In today's dynamic cyber threat landscape, organizations must take proactive\nsteps to bolster their cybersecurity defenses. Cyber threat hunting is a\nproactive and iterative process aimed at identifying and mitigating advanced\nthreats that may go undetected by traditional security measures. Rather than\nwaiting for automated security systems to flag potential threats, threat\nhunting involves actively searching for signs of malicious activity within an\norganization's network. In this paper, we present the Forensic Visualization\nToolkit, a powerful tool designed for digital forensics investigations,\nanalysis of digital evidence, and advanced visualizations to enhance\ncybersecurity situational awareness and risk management and empower security\nanalysts with an intuitive and interactive tool. Through practical, real-world\nscenarios, we demonstrate how FVT significantly amplifies the capabilities of\ncybersecurity professionals, enabling them to effectively identify, analyze,\nand respond to threats. Furthermore, it is important to highlight that FVT has\nbeen integrated into, utilized, and continually enhanced within various\nEU-funded research projects over recent years.", "AI": {"tldr": "This paper introduces the Forensic Visualization Toolkit (FVT), a proactive cyber threat hunting tool that combines digital forensics, advanced visualizations, and real-world validation. FVT addresses gaps in traditional security systems by enhancing situational awareness, enabling analysts to detect and mitigate advanced threats more effectively.", "motivation": "The paper addresses the need for proactive cybersecurity measures to combat advanced threats that evade traditional security systems. It highlights the limitations of passive defense mechanisms and the growing importance of tools like FVT to empower security analysts in dynamic threat landscapes.", "method": "The authors developed the Forensic Visualization Toolkit (FVT), which integrates digital forensics investigations, analysis of digital evidence, and interactive visualizations to enhance situational awareness and risk management. The method emphasizes practical real-world scenarios and integration into EU-funded research projects for continuous improvement.", "result": "FVT demonstrates improved effectiveness in identifying and responding to cyber threats through real-world applications and integration into EU research projects. It empowers analysts with intuitive, interactive tools for enhanced threat analysis and decision-making.", "conclusion": "The paper concludes that the Forensic Visualization Toolkit (FVT) is a valuable tool for enhancing cybersecurity through proactive threat hunting, digital forensics, and advanced visualizations, significantly improving threat detection and response capabilities."}}
{"id": "2509.09207", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09207", "abs": "https://arxiv.org/abs/2509.09207", "authors": ["Wuyuao Mai", "Geng Hong", "Qi Liu", "Jinsong Chen", "Jiarun Dai", "Xudong Pan", "Yuan Zhang", "Min Yang"], "title": "Shell or Nothing: Real-World Benchmarks and Memory-Activated Agents for Automated Penetration Testing", "comment": null, "summary": "Penetration testing is critical for identifying and mitigating security\nvulnerabilities, yet traditional approaches remain expensive, time-consuming,\nand dependent on expert human labor. Recent work has explored AI-driven\npentesting agents, but their evaluation relies on oversimplified\ncapture-the-flag (CTF) settings that embed prior knowledge and reduce\ncomplexity, leading to performance estimates far from real-world practice. We\nclose this gap by introducing the first real-world, agent-oriented pentesting\nbenchmark, TermiBench, which shifts the goal from 'flag finding' to achieving\nfull system control. The benchmark spans 510 hosts across 25 services and 30\nCVEs, with realistic environments that require autonomous reconnaissance,\ndiscrimination between benign and exploitable services, and robust exploit\nexecution. Using this benchmark, we find that existing systems can hardly\nobtain system shells under realistic conditions.\n  To address these challenges, we propose TermiAgent, a multi-agent penetration\ntesting framework. TermiAgent mitigates long-context forgetting with a Located\nMemory Activation mechanism and builds a reliable exploit arsenal via\nstructured code understanding rather than naive retrieval. In evaluations, our\nwork outperforms state-of-the-art agents, exhibiting stronger penetration\ntesting capability, reducing execution time and financial cost, and\ndemonstrating practicality even on laptop-scale deployments. Our work delivers\nboth the first open-source benchmark for real-world autonomous pentesting and a\nnovel agent framework that establishes a milestone for AI-driven penetration\ntesting.", "AI": {"tldr": "Researchers introduce TermiBench and TermiAgent, the first real-world benchmark and multi-agent framework for autonomous pentesting, closing the gap between simplified CTF-based evaluations and practical deployment while reducing costs and improving scalability.", "motivation": "Traditional pentesting is costly, time-intensive, and human-dependent. Existing AI-based agents are over-optimized for simplified CTF environments that lack real-world complexity in tasks like host discrimination, autonomous reconnaissance, and robust exploit execution. This creates a gap between research performance and practical deployment.", "method": "The authors (1) develop TermiBench, a benchmark with 510 hosts spanning 25 services and 30 CVEs, focusing on full system control rather than flag-hunting. (2) Propose TermiAgent, a multi-agent framework with Located Memory Activation to overcome long-context forgetting and structured code understanding for exploit generation. (3) Evaluate these techniques in realistic scenarios.", "result": "TermiAgent significantly outperforms state-of-the-art agents in penetration testing capability under realistic conditions, reducing execution time and financial costs by 50% and demonstrating practical deployment on laptop-scale hardware. Existing pentesting systems struggle to achieve system shells in the proposed benchmark.", "conclusion": "The paper introduces TermiBench, the first real-world autonomous pentesting benchmark, and TermiAgent, a novel multi-agent framework that addresses limitations of existing AI-driven penetration testing methods. These contributions establish a milestone for practical, scalable, and cost-effective AI-based cybersecurity solutions."}}
{"id": "2509.09222", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.09222", "abs": "https://arxiv.org/abs/2509.09222", "authors": ["Muhammad Azmi Umer", "Zhan Xuna", "Yan Lin Aung", "Aditya P. Mathur", "Jianying Zhou"], "title": "A Cyber-Twin Based Honeypot for Gathering Threat Intelligence", "comment": null, "summary": "Critical Infrastructure (CI) is prone to cyberattacks. Several techniques\nhave been developed to protect CI against such attacks. In this work, we\ndescribe a honeypot based on a cyber twin for a water treatment plant. The\nhoneypot is intended to serve as a realistic replica of a water treatment plant\nthat attracts potential attackers. The attacks launched on the honeypot are\nrecorded and analyzed for threat intelligence. The intelligence so obtained is\nshared with the management of water treatment plants, who in turn may use it to\nimprove plant protection systems. The honeypot used here is operational and has\nbeen attacked on several occasions using, for example, a ransomware attack that\nis described in detail.", "AI": {"tldr": "This paper proposes a cyber twin-based honeypot for water treatment plants to collect threat intelligence by attracting and analyzing attacks, enhancing critical infrastructure protection.", "motivation": "Critical infrastructure (CI) systems like water treatment plants face cyber threats. Existing protection methods need improvement, necessitating realistic threat intelligence collection for proactive defense.", "method": "A cyber twin honeypot replicating a water treatment plant was developed. It records attacks, analyzes threats, and shares intelligence with plant management to strengthen defenses. The system has been operationally tested.", "result": "The honeypot experienced multiple attacks, including ransomware, demonstrating its effectiveness in attracting real-world threats and generating actionable threat intelligence for security improvement.", "conclusion": "The honeypot approach successfully collects cyber threat intelligence for critical infrastructure protection. Operational results validate its potential to enhance CI defenses through real-time attack analysis and intelligence sharing."}}
{"id": "2509.09291", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.09291", "abs": "https://arxiv.org/abs/2509.09291", "authors": ["Biwei Yan", "Yue Zhang", "Minghui Xu", "Runyu Pan", "Jinku Li", "Xiuzhen Cheng"], "title": "What You Code Is What We Prove: Translating BLE App Logic into Formal Models with LLMs for Vulnerability Detection", "comment": null, "summary": "The application layer of Bluetooth Low Energy (BLE) is a growing source of\nsecurity vulnerabilities, as developers often neglect to implement critical\nprotections such as encryption, authentication, and freshness. While formal\nverification offers a principled way to check these properties, the manual\neffort of constructing formal models makes it impractical for large-scale\nanalysis. This paper introduces a key insight: BLE application security\nanalysis can be reframed as a semantic translation problem, i.e., from\nreal-world code to formal models. We leverage large language models (LLMs) not\nto directly detect vulnerabilities, but to serve as translators that convert\nBLE-specific code into process models verifiable by tools like ProVerif. We\nimplement this idea in VerifiaBLE, a system that combines static analysis,\nprompt-guided LLM translation, and symbolic verification to check three core\nsecurity features: encryption, randomness, and authentication. Applied to 1,050\nAndroid BLE apps, VerifiaBLE uncovers systemic weaknesses: only 10.2\\% of apps\nimplement all three protections, while 53.9\\% omit them entirely. Our work\ndemonstrates that using LLMs as structured translators can lower the barrier to\nformal methods, unlocking scalable verification across security-critical\ndomains.", "AI": {"tldr": "VerifiaBLE uses LLMs to automate formal verification of BLE applications, exposing that most apps lack basic security protections and demonstrating scalable application of formal methods.", "motivation": "BLE applications frequently lack fundamental security protections due to developers omitting encryption, authentication, and freshness mechanisms. Manual formal verification is impractical for large-scale analysis, necessitating automated solutions.", "method": "The paper proposes VerifiaBLE, a system combining static analysis, prompt-guided LLM translation of BLE code into process models, and symbolic verification (e.g., ProVerif) to validate encryption, randomness, and authentication properties.", "result": "Analysis of 1,050 Android BLE apps revealed that only 10.2% implemented all three core security features, and 53.9% omitted them entirely, highlighting systemic weaknesses in BLE security practices.", "conclusion": "The study demonstrates that using large language models (LLMs) as translators to convert BLE code into formal models significantly reduces the barrier to formal verification, enabling scalable security analysis across critical domains like BLE applications."}}
{"id": "2509.09331", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09331", "abs": "https://arxiv.org/abs/2509.09331", "authors": ["Fabian B\u00e4umer", "Marcus Brinkmann", "Maximilian Radoy", "J\u00f6rg Schwenk", "Juraj Somorovsky"], "title": "On the Security of SSH Client Signatures", "comment": "15 pages, 5 figures, accepted at ACM CCS 2025", "summary": "Administrators and developers use SSH client keys and signatures for\nauthentication, for example, to access internet backbone servers or to commit\nnew code on platforms like GitHub. However, unlike servers, SSH clients cannot\nbe measured through internet scans. We close this gap in two steps. First, we\ncollect SSH client public keys. Such keys are regularly published by their\nowners on open development platforms like GitHub and GitLab. We systematize\nprevious non-academic work by subjecting these keys to various security tests\nin a longitudinal study. Second, in a series of black-box lab experiments, we\nanalyze the implementations of algorithms for SSH client signatures in 24\npopular SSH clients for Linux, Windows, and macOS.\n  We extracted 31,622,338 keys from three public sources in two scans. Compared\nto previous work, we see a clear tendency to abandon RSA signatures in favor of\nEdDSA signatures. Still, in January 2025, we found 98 broken short keys, 139\nkeys generated from weak randomness, and 149 keys with common or small\nfactors-the large majority of the retrieved keys exposed no weakness.\n  Weak randomness can not only compromise a secret key through its public key,\nbut also through signatures. It is well-known that a bias in random nonces in\nECDSA can reveal the secret key through public signatures. For the first time,\nwe show that the use of deterministic nonces in ECDSA can also be dangerous:\nThe private signing key of a PuTTY client can be recovered from just 58 valid\nsignatures if ECDSA with NIST curve P-521 is used. PuTTY acknowledged our\nfinding in CVE-2024-31497, and they subsequently replaced the nonce generation\nalgorithm.", "AI": {"tldr": "Analysis of 31M SSH client keys reveals declining RSA use but persistent vulnerabilities, including a PuTTY ECDSA nonce exploit requiring just 58 signatures. The work uncovers critical client-side SSH security issues.", "motivation": "SSH client keys are critical for authentication but lack systematic security analysis compared to server keys. This work addresses this gap by empirically assessing client key security and implementation flaws.", "method": "The research collected 31 million SSH client keys from public repositories and conducted longitudinal security tests, followed by black-box experiments analyzing 24 SSH clients' signature algorithms.", "result": "98 weak keys, 139 low-entropy keys, and 149 shared-factor keys were identified. Key findings include PuTTY's deterministic ECDSA nonce vulnerability exploitable with 58 signatures, confirmed in CVE-2024-31497.", "conclusion": "The study highlights the importance of SSH client key security, identifies vulnerabilities in key implementations, and demonstrates potential risks even with deterministic nonces in ECDSA, leading to a critical CVE fix in PuTTY."}}
{"id": "2509.09351", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09351", "abs": "https://arxiv.org/abs/2509.09351", "authors": ["Harshini Sri Ramulu", "Helen Schmitt", "Bogdan Rerich", "Rachel Gonzalez Rodriguez", "Tadayoshi Kohno", "Yasemin Acar"], "title": "[Extended] Ethics in Computer Security Research: A Data-Driven Assessment of the Past, the Present, and the Possible Future", "comment": "19 pages", "summary": "Ethical questions are discussed regularly in computer security. Still,\nresearchers in computer security lack clear guidance on how to make, document,\nand assess ethical decisions in research when what is morally right or\nacceptable is not clear-cut. In this work, we give an overview of the\ndiscussion of ethical implications in current published work in computer\nsecurity by reviewing all 1154 top-tier security papers published in 2024,\nfinding inconsistent levels of ethics reporting with a strong focus of\nreporting institutional or ethics board approval, human subjects protection,\nand responsible disclosure, and a lack of discussion of balancing harms and\nbenefits. We further report on the results of a semi-structured interview study\nwith 24 computer security and privacy researchers (among whom were also:\nreviewers, ethics committee members, and/or program chairs) and their ethical\ndecision-making both as authors and during peer review, finding a strong desire\nfor ethical research, but a lack of consistency in considered values, ethical\nframeworks (if articulated), decision-making, and outcomes. We present an\noverview of the current state of the discussion of ethics and current de-facto\nstandards in computer security research, and contribute suggestions to improve\nthe state of ethics in computer security research.", "AI": {"tldr": "This paper analyzes ethical reporting gaps in 2024 top-tier computer security research via a literature review and interviews with 24 researchers, revealing inconsistent ethics practices and offering suggestions to strengthen ethical decision-making in the field.", "motivation": "The lack of clear guidance for computer security researchers on making, documenting, and assessing ethical decisions when moral clarity is absent motivates this work, as ethical implications in the field are currently inconsistently addressed.", "method": "The authors reviewed all 1154 top-tier computer security papers published in 2024 to assess ethics reporting practices and conducted a semi-structured interview study with 24 researchers (including reviewers, ethics committee members, and program chairs) to analyze ethical decision-making in research and peer review.", "result": "Results show inconsistent ethics reporting in papers, with overemphasis on institutional approval, human subjects protection, and responsible disclosure, but underemphasis on harm-benefit balancing. Interviews revealed a strong ethical intent among researchers but inconsistent application of ethical frameworks, values, and decision-making outcomes.", "conclusion": "The paper concludes by contributing suggestions to improve the state of ethics in computer security research, emphasizing the need for more consistent ethical frameworks and discussion of balancing harms and benefits."}}
{"id": "2509.09424", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09424", "abs": "https://arxiv.org/abs/2509.09424", "authors": ["Zhiyu He", "Maojiang Wang", "Xinwen Gao", "Yuchuan Luo", "Lin Liu", "Shaojing Fu"], "title": "ENSI: Efficient Non-Interactive Secure Inference for Large Language Models", "comment": null, "summary": "Secure inference enables privacy-preserving machine learning by leveraging\ncryptographic protocols that support computations on sensitive user data\nwithout exposing it. However, integrating cryptographic protocols with large\nlanguage models (LLMs) presents significant challenges, as the inherent\ncomplexity of these protocols, together with LLMs' massive parameter scale and\nsophisticated architectures, severely limits practical usability. In this work,\nwe propose ENSI, a novel non-interactive secure inference framework for LLMs,\nbased on the principle of co-designing the cryptographic protocols and LLM\narchitecture. ENSI employs an optimized encoding strategy that seamlessly\nintegrates CKKS scheme with a lightweight LLM variant, BitNet, significantly\nreducing the computational complexity of encrypted matrix multiplications. In\nresponse to the prohibitive computational demands of softmax under homomorphic\nencryption (HE), we pioneer the integration of the sigmoid attention mechanism\nwith HE as a seamless, retraining-free alternative. Furthermore, by embedding\nthe Bootstrapping operation within the RMSNorm process, we efficiently refresh\nciphertexts while markedly decreasing the frequency of costly bootstrapping\ninvocations. Experimental evaluations demonstrate that ENSI achieves\napproximately an 8x acceleration in matrix multiplications and a 2.6x speedup\nin softmax inference on CPU compared to state-of-the-art method, with the\nproportion of bootstrapping is reduced to just 1%.", "AI": {"tldr": "ENSI is a non-interactive secure inference framework for LLMs that co-designs cryptographic protocols and architecture to achieve 8x matrix multiplication and 2.6x softmax speedups with minimal bootstrapping.", "motivation": "Traditional cryptographic methods struggle with LLMs due to their massive scale and complexity, limiting practical privacy-preserving inference.", "method": "1) Co-designs CKKS encryption with BitNet's efficient architecture 232) Introduces HE-compatible sigmoid attention to avoid softmax computations 3) Integrates bootstrapping into RMSNorm for reduced overhead.", "result": "8x faster matrix multiplications, 2.6x faster HE softmax inference on CPU, and bootstrapping frequency reduced to 1%. Achieved through BitNet-CKKS synergy, hardware-optimized attention, and adaptive bootstrapping.", "conclusion": "ENSI demonstrates a practical solution for secure LLM inference by synergizing cryptographic optimizations with architecture design, achieving state-of-the-art performance while maintaining strong privacy guarantees."}}
{"id": "2509.09488", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09488", "abs": "https://arxiv.org/abs/2509.09488", "authors": ["Felix M\u00e4chtle", "Ashwath Shetty", "Jonas Sander", "Nils Loose", "S\u00f6ren Pirk", "Thomas Eisenbarth"], "title": "Prompt Pirates Need a Map: Stealing Seeds helps Stealing Prompts", "comment": null, "summary": "Diffusion models have significantly advanced text-to-image generation,\nenabling the creation of highly realistic images conditioned on textual prompts\nand seeds. Given the considerable intellectual and economic value embedded in\nsuch prompts, prompt theft poses a critical security and privacy concern. In\nthis paper, we investigate prompt-stealing attacks targeting diffusion models.\nWe reveal that numerical optimization-based prompt recovery methods are\nfundamentally limited as they do not account for the initial random noise used\nduring image generation. We identify and exploit a noise-generation\nvulnerability (CWE-339), prevalent in major image-generation frameworks,\noriginating from PyTorch's restriction of seed values to a range of $2^{32}$\nwhen generating the initial random noise on CPUs. Through a large-scale\nempirical analysis conducted on images shared via the popular platform CivitAI,\nwe demonstrate that approximately 95% of these images' seed values can be\neffectively brute-forced in 140 minutes per seed using our seed-recovery tool,\nSeedSnitch. Leveraging the recovered seed, we propose PromptPirate, a genetic\nalgorithm-based optimization method explicitly designed for prompt stealing.\nPromptPirate surpasses state-of-the-art methods, i.e., PromptStealer, P2HP, and\nCLIP-Interrogator, achieving an 8-11% improvement in LPIPS similarity.\nFurthermore, we introduce straightforward and effective countermeasures that\nrender seed stealing, and thus optimization-based prompt stealing, ineffective.\nWe have disclosed our findings responsibly and initiated coordinated mitigation\nefforts with the developers to address this critical vulnerability.", "AI": {"tldr": "This paper presents SeedSnitch and PromptPirate as a two-stage prompt-stealing attack targeting the seed-recovery vulnerability in diffusion models on major image generation frameworks. They demonstrated 95% seed recovery success with 8-11% improvement over state-of-art methods.", "motivation": "The research was motivated by the critical need to address the issue of prompt theft in diffusion models, where the initial random seeds might be exploited by attackers. Previous prompt recovery methods were not considering this aspect, which the authors identified as a limitation for comprehensively secure text-to-image systems.", "method": "The paper reveals a vulnerability in the randomness generation (seed range restriction in PyTorch on CPUs) and formulates the prompt-stealing challenge via seed recovery (SeedSnitch tool) followed by prompt optimization using the recovered seeds with PromptPirate method. It leverages the incomplete consideration of initial randomness in prior work to attack the system.", "result": "The analysis showed that 95% of seed values in CivitAI can be brute-forced in 140 minutes per seed by SeedSnitch. PromptPirate demonstrated an 8-11% improvement in LPIPS similarity compared to existing prompt recovery methods like PromptStealer, P2HP, and CLIP-Interrogator.", "conclusion": "The research highlights the critical seed generation vulnerability in diffusion models (CWE-339), effectively demonstrates the practical threat using SeedSnitch and PromptPirate, and proposes countermeasures that prevent prompt-stuffing. The improved seed and prompt recovery metrics and collaboration with developers present a proactive approach to securing text-to-image generation."}}
