{"id": "2507.18726", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18726", "abs": "https://arxiv.org/abs/2507.18726", "authors": ["Sadia Afrin Mim"], "title": "Exploring the Landscape of Fairness Interventions in Software Engineering", "comment": null, "summary": "Current developments in AI made it broadly significant for reducing human\nlabor and expenses across several essential domains, including healthcare and\nfinance. However, the application of AI in the actual world poses multiple\nrisks and disadvantages due to potential risk factors in data (e.g., biased\ndataset). Practitioners developed a number of fairness interventions for\naddressing these kinds of problems. The paper acts as a survey, summarizing the\nvarious studies and approaches that have been developed to address fairness\nissues"}
{"id": "2507.18755", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.18755", "abs": "https://arxiv.org/abs/2507.18755", "authors": ["Chandra Maddila", "Adam Tait", "Claire Chang", "Daniel Cheng", "Nauman Ahmad", "Vijayaraghavan Murali", "Marshall Roch", "Arnaud Avondet", "Aaron Meltzer", "Victor Montalvao", "Michael Hopko", "Chris Waterson", "Parth Thakkar", "Renuka Fernandez", "Kristian Kristensen", "Sivan Barzily", "Sherry Chen", "Rui Abreu", "Nachiappan Nagappan", "Payam Shodjai", "Killian Murphy", "James Everingham", "Aparna Ramani", "Peter C. Rigby"], "title": "Agentic Program Repair from Test Failures at Scale: A Neuro-symbolic approach with static analysis and test execution feedback", "comment": null, "summary": "Aim: With the advent of LLMs, sophisticated agentic program repair has become\nviable at large organizations with large codebases. In this work, we develop an\nEngineering Agent that fixes the source code based on test failures at scale\nacross diverse software offerings internally.\n  Method: Using Llama as the base, we employ the ReAct harness to develop an\nagent. We start with a test failure that was triaged by a rule-based test\nfailure bot. We then set up an agentic harness and allow the agent to reason\nand run a set of 15 actions from reading a file to generating a patch. We\nprovide feedback to the agent through static analysis and test failures so it\ncan refine its solution. We leverage an LLM-as-a-Judge to ensure that the patch\nconforms to the standards followed by a human review to land fixes.\n  Benchmark Findings: We curated offline benchmarks for our patch generator,\nthe Engineering Agent loop, and the LLM-as-a-Judge. In offline evaluations we\nfound that a specialized 70B model is highly competitive with the much larger\nbut vanilla Llama-405B. In an ablation study, we found that the ReAct harness\n(neural model) benefited from the symbolic information from static analysis\ntools and test execution traces. A model that strikes a balance between the\nsolve rate and error rate vs the cost and latency has a benchmark solve rate of\n42.3% using an average 11.8 feedback iterations.\n  Production Findings: In a three month period, 80% of the generated fixes were\nreviewed, of which 31.5% were landed (25.5% of the total number of generated\nfixes).\n  Feedback from Engineers: We used open coding to extract qualitative themes\nfrom engineers' feedback. We saw positive feedback in the form of quick\napprovals, gratitude, and surprise. We also found mixed feedback when the\nEngineering Agent's solution was partially correct and it served as a good\nstarting point."}
{"id": "2507.18812", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18812", "abs": "https://arxiv.org/abs/2507.18812", "authors": ["Yiping Jia", "Zhen Ming Jiang", "Shayan Noei", "Ying Zou"], "title": "MemoCoder: Automated Function Synthesis using LLM-Supported Agents", "comment": null, "summary": "With the widespread adoption of Large Language Models (LLMs) such as GitHub\nCopilot and ChatGPT, developers increasingly rely on AI-assisted tools to\nsupport code generation. While LLMs can generate syntactically correct\nsolutions for well-structured programming tasks, they often struggle with\nchallenges that require iterative debugging, error handling, or adaptation to\ndiverse problem structures. Existing approaches such as fine-tuning or\nself-repair strategies either require costly retraining or lack mechanisms to\naccumulate and reuse knowledge from previous attempts.\n  To address these limitations, we propose MemoCoder, a multi-agent framework\nthat enables collaborative problem solving and persistent learning from past\nfixes. At the core of MemoCoder is a Fixing Knowledge Set, which stores\nsuccessful repairs and supports retrieval for future tasks. A central Mentor\nAgent supervises the repair process by identifying recurring error patterns and\nrefining high-level fixing strategies, providing a novel supervisory role that\nguides the self-repair loop. We evaluate MemoCoder across three public\nbenchmarks -- MBPP, HumanEval, and LiveCodeBench -- spanning a range of problem\ncomplexities. Experimental results show that MemoCoder consistently outperforms\nboth zero-shot prompting and a Self-Repair strategy, with improvements ranging\nfrom 3.1% to 12.1% in Pass@10 and from 1.4% to 14.5% in Pass@50, demonstrating\nits effectiveness in iterative refinement and knowledge-guided code generation."}
{"id": "2507.18833", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18833", "abs": "https://arxiv.org/abs/2507.18833", "authors": ["Wenyuan Jiang", "Diany Pressato", "Harsh Darji", "Thibaud Lutellier"], "title": "Exploring the Jupyter Ecosystem: An Empirical Study of Bugs and Vulnerabilities", "comment": null, "summary": "Background. Jupyter notebooks are one of the main tools used by data\nscientists. Notebooks include features (configuration scripts, markdown,\nimages, etc.) that make them challenging to analyze compared to traditional\nsoftware. As a result, existing software engineering models, tools, and studies\ndo not capture the uniqueness of Notebook's behavior. Aims. This paper aims to\nprovide a large-scale empirical study of bugs and vulnerabilities in the\nNotebook ecosystem. Method. We collected and analyzed a large dataset of\nNotebooks from two major platforms. Our methodology involved quantitative\nanalyses of notebook characteristics (such as complexity metrics, contributor\nactivity, and documentation) to identify factors correlated with bugs.\nAdditionally, we conducted a qualitative study using grounded theory to\ncategorize notebook bugs, resulting in a comprehensive bug taxonomy. Finally,\nwe analyzed security-related commits and vulnerability reports to assess risks\nassociated with Notebook deployment frameworks. Results. Our findings highlight\nthat configuration issues are among the most common bugs in notebook documents,\nfollowed by incorrect API usage. Finally, we explore common vulnerabilities\nassociated with popular deployment frameworks to better understand risks\nassociated with Notebook development. Conclusions. This work highlights that\nnotebooks are less well-supported than traditional software, resulting in more\ncomplex code, misconfiguration, and poor maintenance."}
{"id": "2507.18774", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.18774", "abs": "https://arxiv.org/abs/2507.18774", "authors": ["S M Mostaq Hossain", "Amani Altarawneh", "Maanak Gupta"], "title": "Bridging Cloud Convenience and Protocol Transparency: A Hybrid Architecture for Ethereum Node Operations on Amazon Managed Blockchain", "comment": "11 pages, 5 figures, 6 tables. Conference name is 2025 IEEE\n  International Conference on Service-Oriented System Engineering (SOSE)", "summary": "As blockchain technologies are increasingly adopted in enterprise and\nresearch domains, the need for secure, scalable, and performance-transparent\nnode infrastructure has become critical. While self-hosted Ethereum nodes offer\noperational control, they often lack elasticity and require complex\nmaintenance. This paper presents a hybrid, service-oriented architecture for\ndeploying and monitoring Ethereum full nodes using Amazon Managed Blockchain\n(AMB), integrated with EC2-based observability, IAM-enforced security policies,\nand reproducible automation via the AWS Cloud Development Kit. Our architecture\nsupports end-to-end observability through custom EC2 scripts leveraging Web3.py\nand JSON-RPC, collecting over 1,000 real-time data points-including gas\nutilization, transaction inclusion latency, and mempool dynamics. These metrics\nare visualized and monitored through AWS CloudWatch, enabling service-level\nperformance tracking and anomaly detection. This cloud-native framework\nrestores low-level observability lost in managed environments while maintaining\nthe operational simplicity of managed services. By bridging the simplicity of\nAMB with the transparency required for protocol research and enterprise\nmonitoring, this work delivers one of the first reproducible,\nperformance-instrumented Ethereum deployments on AMB. The proposed hybrid\narchitecture enables secure, observable, and reproducible Ethereum node\noperations in cloud environments, suitable for both research and production\nuse."}
{"id": "2507.18957", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18957", "abs": "https://arxiv.org/abs/2507.18957", "authors": ["Jianming Chang", "Jieke Shi", "Yunbo Lyu", "Xin Zhou", "Lulu Wang", "Zhou Yang", "Bixin Li", "David Lo"], "title": "SLICEMATE: Accurate and Scalable Static Program Slicing via LLM-Powered Agents", "comment": null, "summary": "Static program slicing, which extracts the executable portions of a program\nthat affect the values at a specific location, supports many software analysis\ntasks such as debugging and security auditing. However, traditional slicing\ntools rely on computationally expensive reachability analysis over dependency\ngraphs, which struggle to scale to large programs and often fail to handle code\nwith incomplete syntax. Recently emerged learning-based methods, while more\nrobust to such cases, still fall short of achieving comparable performance to\ntraditional methods on well-formed code.\n  In this work, we propose SliceMate, a novel static program slicing solution\npowered by Large Language Model (LLM) agents. It bypasses the need for explicit\ndependency graph construction and achieving superior slicing accuracy.\nConcretely, SliceMate integrates three specialized agents: (1) a synthesis\nagent that produces candidate slices by incrementally expanding the scan scope\nacross functions and files guided by LLM-inferred dependencies; (2) a\nverification agent that performs conciseness and completeness checks of the\ncandidate slices, detecting missing or irrelevant statements; and (3) a\nrefinement agent that repairs the slices with minimal edits in accordance with\nthe verification results. These agents are orchestrated by a control module\nthat ensures timely convergence and outputs high-quality slices without manual\nintervention. For rigorous evaluation, we construct a new and high-quality\nbenchmark, SliceBench, comprising 2,200 manually annotated Java and Python\nprograms, with program lengths ranging from 5 to 8,577 lines, significantly\nlarger than those in existing slicing benchmarks. Experimental results show\nthat SliceMate greatly outperforms both traditional and learning-based slicing\ntools."}
{"id": "2507.18801", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.18801", "abs": "https://arxiv.org/abs/2507.18801", "authors": ["Haotian Zhang", "Kun Liu", "Cristian Garces", "Chenke Luo", "Yu Lei", "Jiang Ming"], "title": "Resolving Indirect Calls in Binary Code via Cross-Reference Augmented Graph Neural Networks", "comment": null, "summary": "Binary code analysis is essential in scenarios where source code is\nunavailable, with extensive applications across various security domains.\nHowever, accurately resolving indirect call targets remains a longstanding\nchallenge in maintaining the integrity of static analysis in binary code. This\ndifficulty arises because the operand of a call instruction (e.g., call rax)\nremains unknown until runtime, resulting in an incomplete inter-procedural\ncontrol flow graph (CFG). Previous approaches have struggled with low accuracy\nand limited scalability. To address these limitations, recent work has\nincreasingly turned to machine learning (ML) to enhance analysis. However, this\nML-driven approach faces two significant obstacles: low-quality callsite-callee\ntraining pairs and inadequate binary code representation, both of which\nundermine the accuracy of ML models. In this paper, we introduce NeuCall, a\nnovel approach for resolving indirect calls using graph neural networks.\nExisting ML models in this area often overlook key elements such as data and\ncode cross-references, which are essential for understanding a program's\ncontrol flow. In contrast, NeuCall augments CFGs with cross-references,\npreserving rich semantic information. Additionally, we leverage advanced\ncompiler-level type analysis to generate high-quality callsite-callee training\npairs, enhancing model precision and reliability. We further design a graph\nneural model that leverages augmented CFGs and relational graph convolutions\nfor accurate target prediction. Evaluated against real-world binaries from\nGitHub and the Arch User Repository on x86_64 architecture, NeuCall achieves an\nF1 score of 95.2%, outperforming state-of-the-art ML-based approaches. These\nresults highlight NeuCall's effectiveness in building precise inter-procedural\nCFGs and its potential to advance downstream binary analysis and security\napplications."}
{"id": "2507.18982", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18982", "abs": "https://arxiv.org/abs/2507.18982", "authors": ["Amir Hossain Raaj", "Fairuz Nawer Meem", "Sadia Afrin Mim"], "title": "Classifying Issues in Open-source GitHub Repositories", "comment": null, "summary": "GitHub is the most widely used platform for software maintenance in the\nopen-source community. Developers report issues on GitHub from time to time\nwhile facing difficulties. Having labels on those issues can help developers\neasily address those issues with prior knowledge of labels. However, most of\nthe GitHub repositories do not maintain regular labeling for the issues. The\ngoal of this work is to classify issues in the open-source community using ML\n\\& DNN models. There are thousands of open-source repositories on GitHub. Some\nof the repositories label their issues properly whereas some of them do not.\nWhen issues are pre-labeled, the problem-solving process and the immediate\nassignment of corresponding personnel are facilitated for the team, thereby\nexpediting the development process. In this work, we conducted an analysis of\nprominent GitHub open-source repositories. We classified the issues in some\ncommon labels which are: API, Documentation, Enhancement, Question, Easy,\nHelp-wanted, Dependency, CI, Waiting for OP's response, Test, Bug, etc. Our\nstudy shows that DNN models outperf"}
{"id": "2507.19032", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.19032", "abs": "https://arxiv.org/abs/2507.19032", "authors": ["Alper Çakan", "Vipul Goyal"], "title": "How to Copy-Protect Malleable-Puncturable Cryptographic Functionalities Under Arbitrary Challenge Distributions", "comment": null, "summary": "A quantum copy-protection scheme (Aaronson, CCC 2009) encodes a functionality\ninto a quantum state such that given this state, no efficient adversary can\ncreate two (possibly entangled) quantum states that are both capable of running\nthe functionality. There has been a recent line of works on constructing\nprovably-secure copy-protection schemes for general classes of schemes in the\nplain model, and most recently the recent work of \\c{C}akan and Goyal (IACR\nEprint, 2025) showed how to copy-protect all cryptographically puncturable\nschemes with pseudorandom puncturing points. In this work, we show how to\ncopy-protect even a larger class of schemes. We define a class of cryptographic\nschemes called malleable-puncturable schemes where the only requirement is that\none can create a circuit that is capable of answering inputs at points that are\nunrelated to the challenge in the security game but does not help the adversary\nanswer inputs related to the challenge. This is a flexible generalization of\npuncturable schemes, and can capture a wide range of primitives that was not\nknown how to copy-protect prior to our work. Going further, we show that our\nscheme is secure against arbitrary high min-entropy challenge distributions\nwhereas previous work has only considered schemes that are punctured at\npseudorandom points."}
{"id": "2507.19027", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.19027", "abs": "https://arxiv.org/abs/2507.19027", "authors": ["Aleksi Huotala", "Miikka Kuutila", "Mika Mäntylä"], "title": "SESR-Eval: Dataset for Evaluating LLMs in the Title-Abstract Screening of Systematic Reviews", "comment": "12 pages (10 + 2 pages for references)", "summary": "Background: The use of large language models (LLMs) in the title-abstract\nscreening process of systematic reviews (SRs) has shown promising results, but\nsuffers from limited performance evaluation. Aims: Create a benchmark dataset\nto evaluate the performance of LLMs in the title-abstract screening process of\nSRs. Provide evidence whether using LLMs in title-abstract screening in\nsoftware engineering is advisable. Method: We start with 169 SR research\nartifacts and find 24 of those to be suitable for inclusion in the dataset.\nUsing the dataset we benchmark title-abstract screening using 9 LLMs. Results:\nWe present the SESR-Eval (Software Engineering Systematic Review Evaluation)\ndataset containing 34,528 labeled primary studies, sourced from 24 secondary\nstudies published in software engineering (SE) journals. Most LLMs performed\nsimilarly and the differences in screening accuracy between secondary studies\nare greater than differences between LLMs. The cost of using an LLM is\nrelatively low - less than $40 per secondary study even for the most expensive\nmodel. Conclusions: Our benchmark enables monitoring AI performance in the\nscreening task of SRs in software engineering. At present, LLMs are not yet\nrecommended for automating the title-abstract screening process, since accuracy\nvaries widely across secondary studies, and no LLM managed a high recall with\nreasonable precision. In future, we plan to investigate factors that influence\nLLM screening performance between studies."}
{"id": "2507.19055", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.19055", "abs": "https://arxiv.org/abs/2507.19055", "authors": ["Yuksel Arslan"], "title": "Virtual local area network over HTTP for launching an insider attack", "comment": null, "summary": "Computers and computer networks have become integral to virtually every\naspect of modern life, with the Internet playing an indispensable role.\nOrganizations, businesses, and individuals now store vast amounts of\nproprietary, confidential, and personal data digitally. As such, ensuring the\nsecurity of this data from unauthorized access is critical. Common security\nmeasures, such as firewalls, intrusion detection systems (IDS), intrusion\nprevention systems (IPS), and antivirus software, are constantly evolving to\nsafeguard computer systems and networks. However, these tools primarily focus\non defending against external threats, leaving systems vulnerable to insider\nattacks. Security solutions designed to mitigate risks originating from within\nthe organization are relatively limited and often ineffective. This paper\ndemonstrates how a Local Area Network (LAN) can be covertly exposed to the\nInternet via an insider attack. Specifically, it illustrates how an external\nmachine can gain access to a LAN by exploiting an unused secondary IP address\nof the attacked LAN, effectively bypassing existing security mechanisms by also\nexploiting Hyper Text Transfer Protocol (HTTP). Despite the presence of robust\nexternal protections, such as firewalls and IDS, this form of insider attack\nreveals significant vulnerabilities in the way internal threats are addressed."}
{"id": "2507.19113", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.19113", "abs": "https://arxiv.org/abs/2507.19113", "authors": ["Liliana Pasquale", "Azzurra Ragone", "Emanuele Piemontese", "Armin Amiri Darban"], "title": "Exploring the Use of LLMs for Requirements Specification in an IT Consulting Company", "comment": "11 pages, 5 figures. Accepted for presentation at the Industrial\n  Innovation Track of the 33rd IEEE International Requirements Engineering\n  Conference (RE 2025), Valencia, Spain", "summary": "In practice, requirements specification remains a critical challenge. The\nknowledge necessary to generate a specification can often be fragmented across\ndiverse sources (e.g., meeting minutes, emails, and high-level product\ndescriptions), making the process cumbersome and time-consuming. In this paper,\nwe report our experience using large language models (LLMs) in an IT consulting\ncompany to automate the requirements specification process. In this company,\nrequirements are specified using a Functional Design Specification (FDS), a\ndocument that outlines the functional requirements and features of a system,\napplication, or process. We provide LLMs with a summary of the requirements\nelicitation documents and FDS templates, prompting them to generate Epic FDS\n(including high-level product descriptions) and user stories, which are\nsubsequently compiled into a complete FDS document. We compared the correctness\nand quality of the FDS generated by three state-of-the-art LLMs against those\nproduced by human analysts. Our results show that LLMs can help automate and\nstandardize the requirements specification, reducing time and human effort.\nHowever, the quality of LLM-generated FDS highly depends on inputs and often\nrequires human revision. Thus, we advocate for a synergistic approach in which\nan LLM serves as an effective drafting tool while human analysts provide the\ncritical contextual and technical oversight necessary for high-quality\nrequirements engineering (RE) documentation."}
{"id": "2507.19060", "categories": ["cs.CR", "cs.CL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.19060", "abs": "https://arxiv.org/abs/2507.19060", "authors": ["Jiawei Liu", "Nirav Diwan", "Zhe Wang", "Haoyu Zhai", "Xiaona Zhou", "Kiet A. Nguyen", "Tianjiao Yu", "Muntasir Wahed", "Yinlin Deng", "Hadjer Benkraouda", "Yuxiang Wei", "Lingming Zhang", "Ismini Lourentzou", "Gang Wang"], "title": "PurpCode: Reasoning for Safer Code Generation", "comment": null, "summary": "We introduce PurpCode, the first post-training recipe for training safe code\nreasoning models towards generating secure code and defending against malicious\ncyberactivities. PurpCode trains a reasoning model in two stages: (i) Rule\nLearning, which explicitly teaches the model to reference cybersafety rules to\ngenerate vulnerability-free code and to avoid facilitating malicious\ncyberactivities; and (ii) Reinforcement Learning, which optimizes model safety\nand preserves model utility through diverse, multi-objective reward mechanisms.\nTo empower the training pipelines with comprehensive cybersafety data, we\nconduct internal red-teaming to synthesize comprehensive and high-coverage\nprompts based on real-world tasks for inducing unsafe cyberactivities in the\nmodel. Based on PurpCode, we develop a reasoning-based coding model, namely\nPurpCode-32B, which demonstrates state-of-the-art cybersafety, outperforming\nvarious frontier models. Meanwhile, our alignment method decreases the model\noverrefusal rates in both general and cybersafety-specific scenarios, while\npreserving model utility in both code generation and common security knowledge."}
{"id": "2507.19115", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19115", "abs": "https://arxiv.org/abs/2507.19115", "authors": ["Shweta Ramesh", "Joy Bose", "Hamender Singh", "A K Raghavan", "Sujoy Roychowdhury", "Giriprasad Sridhara", "Nishrith Saini", "Ricardo Britto"], "title": "Automated Code Review Using Large Language Models at Ericsson: An Experience Report", "comment": null, "summary": "Code review is one of the primary means of assuring the quality of released\nsoftware along with testing and static analysis. However, code review requires\nexperienced developers who may not always have the time to perform an in-depth\nreview of code. Thus, automating code review can help alleviate the cognitive\nburden on experienced software developers allowing them to focus on their\nprimary activities of writing code to add new features and fix bugs. In this\npaper, we describe our experience in using Large Language Models towards\nautomating the code review process in Ericsson. We describe the development of\na lightweight tool using LLMs and static program analysis. We then describe our\npreliminary experiments with experienced developers in evaluating our code\nreview tool and the encouraging results."}
{"id": "2507.19185", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19185", "abs": "https://arxiv.org/abs/2507.19185", "authors": ["Tarek Gasmi", "Ramzi Guesmi", "Mootez Aloui", "Jihene Bennaceur"], "title": "PrompTrend: Continuous Community-Driven Vulnerability Discovery and Assessment for Large Language Models", "comment": null, "summary": "Static benchmarks fail to capture LLM vulnerabilities emerging through\ncommunity experimentation in online forums. We present PrompTrend, a system\nthat collects vulnerability data across platforms and evaluates them using\nmultidimensional scoring, with an architecture designed for scalable\nmonitoring. Cross-sectional analysis of 198 vulnerabilities collected from\nonline communities over a five-month period (January-May 2025) and tested on\nnine commercial models reveals that advanced capabilities correlate with\nincreased vulnerability in some architectures, psychological attacks\nsignificantly outperform technical exploits, and platform dynamics shape attack\neffectiveness with measurable model-specific patterns. The PrompTrend\nVulnerability Assessment Framework achieves 78% classification accuracy while\nrevealing limited cross-model transferability, demonstrating that effective LLM\nsecurity requires comprehensive socio-technical monitoring beyond traditional\nperiodic assessment. Our findings challenge the assumption that capability\nadvancement improves security and establish community-driven psychological\nmanipulation as the dominant threat vector for current language models."}
{"id": "2507.19271", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.19271", "abs": "https://arxiv.org/abs/2507.19271", "authors": ["Igli Begolli", "Meltem Aksoy", "Daniel Neider"], "title": "Fine-Tuning Multilingual Language Models for Code Review: An Empirical Study on Industrial C# Projects", "comment": null, "summary": "Code review is essential for maintaining software quality but often\ntime-consuming and cognitively demanding, especially in industrial\nenvironments. Recent advancements in language models (LMs) have opened new\navenues for automating core review tasks. This study presents the empirical\nevaluation of monolingual fine-tuning on the performance of open-source LMs\nacross three key automated code review tasks: Code Change Quality Estimation,\nReview Comment Generation, and Code Refinement. We fine-tuned three distinct\nmodels, CodeReviewer, CodeLlama-7B, and DeepSeek-R1-Distill, on a C\\# specific\ndataset combining public benchmarks with industrial repositories. Our study\ninvestigates how different configurations of programming languages and natural\nlanguages in the training data affect LM performance, particularly in comment\ngeneration. Additionally, we benchmark the fine-tuned models against an\nautomated software analysis tool (ASAT) and human reviewers to evaluate their\npractical utility in real-world settings. Our results show that monolingual\nfine-tuning improves model accuracy and relevance compared to multilingual\nbaselines. While LMs can effectively support code review workflows, especially\nfor routine or repetitive tasks, human reviewers remain superior in handling\nsemantically complex or context-sensitive changes. Our findings highlight the\nimportance of language alignment and task-specific adaptation in optimizing LMs\nfor automated code review."}
{"id": "2507.19295", "categories": ["cs.CR", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.19295", "abs": "https://arxiv.org/abs/2507.19295", "authors": ["Svenja Lage", "Hannes Bartz"], "title": "On the Security of a Code-Based PIR Scheme", "comment": null, "summary": "Private Information Retrieval (PIR) schemes allow clients to retrieve files\nfrom a database without disclosing the requested file's identity to the server.\nIn the pursuit of post-quantum security, most recent PIR schemes rely on hard\nlattice problems. In contrast, the so called CB-cPIR scheme stands out as a\npioneering effort to base PIR schemes on hard problems in coding theory,\nthereby contributing significantly to the diversification of security\nfoundations. However, our research reveals a critical vulnerability in CB-cPIR,\nsubstantially diminishing its security levels. Moreover, a comparative analysis\nwith state-of-the-art PIR schemes shows that CB-cPIR's advantages are reduced,\nmaking it less competitive in terms of the communication cost. Nevertheless,\nour findings highlight the importance of continued research into code-based PIR\nschemes, as they have the potential to provide a valuable alternative to\nlattice-based approaches."}
{"id": "2507.19275", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.19275", "abs": "https://arxiv.org/abs/2507.19275", "authors": ["Bo Wang", "Pengyang Wang", "Chong Chen", "Qi Sun", "Jieke Shi", "Chengran Yang", "Ming Deng", "Youfang Lin", "Zhou Yang", "David Lo"], "title": "Mut4All: Fuzzing Compilers via LLM-Synthesized Mutators Learned from Bug Reports", "comment": null, "summary": "Mutation-based fuzzing is effective for uncovering compiler bugs, but\ndesigning high-quality mutators for modern languages with complex constructs\n(e.g., templates, macros) remains challenging. Existing methods rely heavily on\nmanual design or human-in-the-loop correction, limiting scalability and\ncross-language generalizability.\n  We present Mut4All, a fully automated, language-agnostic framework that\nsynthesizes mutators using Large Language Models (LLMs) and compiler-specific\nknowledge from bug reports. It consists of three agents: (1) a mutator\ninvention agent that identifies mutation targets and generates mutator metadata\nusing compiler-related insights; (2) a mutator implementation synthesis agent,\nfine-tuned to produce initial implementations; and (3) a mutator refinement\nagent that verifies and corrects the mutators via unit-test feedback.\n  Mut4All processes 1000 bug reports (500 Rust, 500 C++), yielding 319 Rust and\n403 C++ mutators at ~$0.08 each via GPT-4o. Our customized fuzzer, using these\nmutators, finds 62 bugs in Rust compilers (38 new, 7 fixed) and 34 bugs in C++\ncompilers (16 new, 1 fixed). Mut4All outperforms existing methods in both\nunique crash detection and coverage, ranking first on Rust and second on C++."}
{"id": "2507.19367", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.19367", "abs": "https://arxiv.org/abs/2507.19367", "authors": ["Weihao Chen", "Yansong Gao", "Boyu Kuang", "Jin B. Hong", "Yuqing Zhang", "Anmin Fu"], "title": "Empowering IoT Firmware Secure Update with Customization Rights", "comment": null, "summary": "Firmware updates remain the primary line of defense for IoT devices; however,\nthe update channel itself has become a well-established attack vector. Existing\ndefenses mainly focus on securing monolithic firmware images, leaving\nmodule-level customization -a growing user demand-largely unprotected and\ninsufficiently explored. To address this gap, we conduct a pilot study on the\nupdate workflows of 200 Linux-based IoT devices across 23 vendors, uncovering\nfive previously undocumented vulnerabilities caused by customization practices.\nA broader analysis of update-related CVEs from 2020 to 2024 reveals that over\nhalf originate from customization-induced issues. These findings highlight a\ncritical yet underexamined reality: as customization increases, so does the\nattack surface, while current defenses fail to keep pace. We propose IMUP\n(Integrity-Centric Modular Update Platform), the first framework to address two\nkey challenges: constructing a trustworthy cross-module integrity chain and\nscaling update performance under mass customization. IMUP combines three\ntechniques: per-module chameleon hashing for integrity, server-side\nproof-of-work offloading to reduce device overhead, and server-side caching to\nreuse module combinations, minimizing rebuild costs. Security analysis shows\nthat even when 95 percent of secret keys are exposed, forging a valid image\nincurs over 300 times the cost of the legitimate server. Experiments on\nheterogeneous IoT devices demonstrate that IMUP reduces server-side generation\ntime by 2.9 times and device downtime by 5.9 times compared to a\npackage-manager baseline."}
{"id": "2507.19390", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19390", "abs": "https://arxiv.org/abs/2507.19390", "authors": ["Altaf Allah Abbassi", "Leuson Da Silva", "Amin Nikanjam", "Foutse Khomh"], "title": "ReCatcher: Towards LLMs Regression Testing for Code Generation", "comment": "24 pages, 3 Figures, 2 Tables", "summary": "Large Language Models (LLMs) for code generation evolve rapidly through\nfine-tuning, merging, or new model releases. However, such updates can\nintroduce regressions, not only in correctness but also in code quality and\nperformance. To address this, we present ReCatcher, a regression testing\nframework for Python code generation. ReCatcher systematically compares two\nLLMs, typically a current model and a candidate update, across three\ndimensions: logical correctness, static code quality, and execution\nperformance. We apply ReCatcher to assess regressions across three update\nscenarios, fine-tuning, merging, and model release, using CodeLlama,\nDeepSeek-Coder, and GPT-4o. Our evaluation shows that fine-tuning with\ncross-language datasets increases syntax errors by up to 12%. Merging with\ngeneral-purpose models like Llama2 leads to regressions in correctness by up to\n18%. GPT-4o introduces regressions of up to 50% in handling missing imports\ncompared to GPT-3.5-turbo, while GPT-4o-mini suffers up to 80% performance\ndegradation in execution time versus GPT-4o. Overall, logical correctness,\nperformance, and error handling (e.g., syntax errors and missing imports) are\nthe most regression-prone areas. Comparing ReCatcher with baseline solutions,\nit presents better and consistent accuracy across logical and performance\naspects. ReCatcher highlights the importance of systematic regression\nevaluation before adopting new models, while assisting researchers and\npractitioners in making more informed update decisions."}
{"id": "2507.19391", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.19391", "abs": "https://arxiv.org/abs/2507.19391", "authors": ["Armin Namavari", "Thomas Ristenpart"], "title": "Transcript Franking for Encrypted Messaging", "comment": null, "summary": "Message franking is an indispensable abuse mitigation tool for end-to-end\nencrypted (E2EE) messaging platforms. With it, users who receive harmful\ncontent can securely report that content to platform moderators. However, while\nreal-world deployments of reporting require the disclosure of multiple\nmessages, existing treatments of message franking only consider the report of a\nsingle message. As a result, there is a gap between the security goals achieved\nby constructions and those needed in practice. Our work introduces transcript\nfranking, a new type of protocol that allows reporting subsets of conversations\nsuch that moderators can cryptographically verify message causality and\ncontents. We define syntax, semantics, and security for transcript franking in\ntwo-party and group messaging. We then present efficient constructions for\ntranscript franking and prove their security. Looking toward deployment\nconsiderations, we provide detailed discussion of how real-world messaging\nsystems can incorporate our protocols."}
{"id": "2507.19403", "categories": ["cs.SE", "cs.AI", "cs.DC", "B.8.2; C.2.4"], "pdf": "https://arxiv.org/pdf/2507.19403", "abs": "https://arxiv.org/abs/2507.19403", "authors": ["Matthias Weiß", "Falk Dettinger", "Michael Weyrich"], "title": "SDVDiag: A Modular Platform for the Diagnosis of Connected Vehicle Functions", "comment": "7 pages, 5 figures", "summary": "Connected and software-defined vehicles promise to offer a broad range of\nservices and advanced functions to customers, aiming to increase passenger\ncomfort and support autonomous driving capabilities. Due to the high\nreliability and availability requirements of connected vehicles, it is crucial\nto resolve any occurring failures quickly. To achieve this however, a complex\ncloud/edge architecture with a mesh of dependencies must be navigated to\ndiagnose the responsible root cause. As such, manual analyses become unfeasible\nsince they would significantly delay the troubleshooting.\n  To address this challenge, this paper presents SDVDiag, an extensible\nplatform for the automated diagnosis of connected vehicle functions. The\nplatform enables the creation of pipelines that cover all steps from initial\ndata collection to the tracing of potential root causes. In addition, SDVDiag\nsupports self-adaptive behavior by the ability to exchange modules at runtime.\nDependencies between functions are detected and continuously updated, resulting\nin a dynamic graph view of the system. In addition, vital system metrics are\nmonitored for anomalies. Whenever an incident is investigated, a snapshot of\nthe graph is taken and augmented by relevant anomalies. Finally, the analysis\nis performed by traversing the graph and creating a ranking of the most likely\ncauses.\n  To evaluate the platform, it is deployed inside an 5G test fleet environment\nfor connected vehicle functions. The results show that injected faults can be\ndetected reliably. As such, the platform offers the potential to gain new\ninsights and reduce downtime by identifying problems and their causes at an\nearly stage."}
{"id": "2507.19399", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19399", "abs": "https://arxiv.org/abs/2507.19399", "authors": ["Gabriel Chua"], "title": "Running in CIRCLE? A Simple Benchmark for LLM Code Interpreter Security", "comment": null, "summary": "As large language models (LLMs) increasingly integrate native code\ninterpreters, they enable powerful real-time execution capabilities,\nsubstantially expanding their utility. However, such integrations introduce\npotential system-level cybersecurity threats, fundamentally different from\nprompt-based vulnerabilities. To systematically evaluate these\ninterpreter-specific risks, we propose CIRCLE (Code-Interpreter Resilience\nCheck for LLM Exploits), a simple benchmark comprising 1,260 prompts targeting\nCPU, memory, and disk resource exhaustion. Each risk category includes\nexplicitly malicious (\"direct\") and plausibly benign (\"indirect\") prompt\nvariants. Our automated evaluation framework assesses not only whether LLMs\nrefuse or generates risky code, but also executes the generated code within the\ninterpreter environment to evaluate code correctness, simplifications made by\nthe LLM to make the code safe, or execution timeouts. Evaluating 7 commercially\navailable models from OpenAI and Google, we uncover significant and\ninconsistent vulnerabilities. For instance, evaluations show substantial\ndisparities even within providers - OpenAI's o4-mini correctly refuses risky\nrequests at 7.1%, notably higher rates compared to GPT-4.1 at 0.5%. Results\nparticularly underscore that indirect, socially-engineered prompts\nsubstantially weaken model defenses. This highlights an urgent need for\ninterpreter-specific cybersecurity benchmarks, dedicated mitigation tools\n(e.g., guardrails), and clear industry standards to guide safe and responsible\ndeployment of LLM interpreter integrations. The benchmark dataset and\nevaluation code are publicly released to foster further research."}
{"id": "2507.19432", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.19432", "abs": "https://arxiv.org/abs/2507.19432", "authors": ["Sheikh Shadab Towqir", "Fei He", "Todd Mytkowicz", "Na Meng"], "title": "Resolving Build Conflicts via Example-Based and Rule-Based Program Transformations", "comment": null, "summary": "Merge conflicts often arise when developers integrate changes from different\nsoftware branches. The conflicts can result from overlapping edits in programs\n(i.e., textual conflicts) or cause build and test errors (i.e., build and test\nconflicts). They degrade software quality and hinder programmer productivity.\nWhile several tools detect build conflicts, few offer meaningful support for\nresolving cases like those caused by method removal. To overcome limitations of\nexisting tools, we introduce BUCOR (Build Conflict Resolver), a new conflict\nresolver. BUCOR first detects conflicts by comparing three versions related to\na merging scenario: base b, left l, and right r. To resolve conflicts, it\nemploys two complementary strategies: example-based transformation (BUCOR-E)\nand rule-based transformation (BUCOR-R). BUCOR-R applies predefined rules to\nhandle common, well-understood conflicts. BUCOR-E mines branch versions (l and\nr) for exemplar edits applied to fix related build errors. From these examples,\nit infers and generalizes program transformation patterns to resolve more\ncomplex conflicts.\n  We evaluated BUCOR on 88 real-world build conflicts spanning 21 distinct\nconflict types. BUCOR generated at least one solution for 65 cases and\ncorrectly resolved 43 conflicts. We observed that this hybrid\napproach--combining context-aware, example-based learning with structured,\nrule-based resolution--can effectively help resolve conflicts. Our research\nsheds light on future directions for more intelligent and automated merge\ntools."}
{"id": "2507.19446", "categories": ["cs.SE", "cs.DC", "B.8.2; C.2.4"], "pdf": "https://arxiv.org/pdf/2507.19446", "abs": "https://arxiv.org/abs/2507.19446", "authors": ["Matthias Weiß", "Anish Navalgund", "Johannes Stümpfle", "Falk Dettinger", "Michael Weyrich"], "title": "An OpenSource CI/CD Pipeline for Variant-Rich Software-Defined Vehicles", "comment": "7 pages, 5 figures", "summary": "Software-defined vehicles (SDVs) offer a wide range of connected\nfunctionalities, including enhanced driving behavior and fleet management.\nThese features are continuously updated via over-the-air (OTA) mechanisms,\nresulting in a growing number of software versions and variants due to the\ndiversity of vehicles, cloud/edge environments, and stakeholders involved. The\nlack of a unified integration environment further complicates development, as\nconnected mobility solutions are often built in isolation. To ensure reliable\noperations across heterogeneous systems, a dynamic orchestration of functions\nthat considers hardware and software variability is essential. This paper\npresents an open-source CI/CD pipeline tailored for SDVs. It automates the\nbuild, test, and deployment phases using a combination of containerized\nopen-source tools, creating a standardized, portable, and scalable ecosystem\naccessible to all stakeholders. Additionally, a custom OTA middleware\ndistributes software updates and supports rollbacks across vehicles and backend\nservices. Update variants are derived based on deployment target dependencies\nand hardware configurations. The pipeline also supports continuous development\nand deployment of AI models for autonomous driving features. Its effectiveness\nis evaluated using an automated valet parking (AVP) scenario involving\nTurtleBots and a coordinating backend server. Two object detection variants are\ndeveloped and deployed to match hardware-specific requirements. Results\ndemonstrate seamless OTA updates, correct variant selection, and successful\norchestration across all targets. Overall, the proposed pipeline provides a\nscalable and efficient solution for managing software variants and OTA updates\nin SDVs, contributing to the advancement of future mobility technologies."}
{"id": "2507.19060", "categories": ["cs.CR", "cs.CL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.19060", "abs": "https://arxiv.org/abs/2507.19060", "authors": ["Jiawei Liu", "Nirav Diwan", "Zhe Wang", "Haoyu Zhai", "Xiaona Zhou", "Kiet A. Nguyen", "Tianjiao Yu", "Muntasir Wahed", "Yinlin Deng", "Hadjer Benkraouda", "Yuxiang Wei", "Lingming Zhang", "Ismini Lourentzou", "Gang Wang"], "title": "PurpCode: Reasoning for Safer Code Generation", "comment": null, "summary": "We introduce PurpCode, the first post-training recipe for training safe code\nreasoning models towards generating secure code and defending against malicious\ncyberactivities. PurpCode trains a reasoning model in two stages: (i) Rule\nLearning, which explicitly teaches the model to reference cybersafety rules to\ngenerate vulnerability-free code and to avoid facilitating malicious\ncyberactivities; and (ii) Reinforcement Learning, which optimizes model safety\nand preserves model utility through diverse, multi-objective reward mechanisms.\nTo empower the training pipelines with comprehensive cybersafety data, we\nconduct internal red-teaming to synthesize comprehensive and high-coverage\nprompts based on real-world tasks for inducing unsafe cyberactivities in the\nmodel. Based on PurpCode, we develop a reasoning-based coding model, namely\nPurpCode-32B, which demonstrates state-of-the-art cybersafety, outperforming\nvarious frontier models. Meanwhile, our alignment method decreases the model\noverrefusal rates in both general and cybersafety-specific scenarios, while\npreserving model utility in both code generation and common security knowledge."}
