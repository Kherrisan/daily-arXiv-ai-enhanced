<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 29]
- [cs.SE](#cs.SE) [Total: 18]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Lightweight MobileNetV1+GRU for ECG Biometric Authentication: Federated and Adversarial Evaluation](https://arxiv.org/abs/2509.20382)
*Dilli Hang Rai,Sabin Kafley*

Main category: cs.CR

TL;DR: The paper proposes a lightweight deep learning model for secure ECG authentication on wearables, achieving high accuracy but showing vulnerabilities to adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: The motivation addresses the need for real-time ECG biometric processing on wearable devices, considering challenges like processing time, privacy concerns, and susceptibility to spoofing attacks.

Method: The method involves a lightweight model combining MobileNetV1 and GRU, custom preprocessing with added Gaussian noise to simulate wearable conditions, and uses ECGID, MIT-BIH, CYBHi, and PTB datasets for evaluation.

Result: Results demonstrate strong performance with high accuracy, F1-scores, and low EER across datasets, but the model's accuracy drops severely under FGSM adversarial attacks.

Conclusion: The conclusion underscores the importance of federated learning, adversarial testing, and diverse datasets to enhance security and scalability in ECG biometrics for wearables.

Abstract: ECG biometrics offer a unique, secure authentication method, yet their
deployment on wearable devices faces real-time processing, privacy, and
spoofing vulnerability challenges. This paper proposes a lightweight deep
learning model (MobileNetV1+GRU) for ECG-based authentication, injection of
20dB Gaussian noise & custom preprocessing. We simulate wearable conditions and
edge deployment using the ECGID, MIT-BIH, CYBHi, and PTB datasets, achieving
accuracies of 99.34%, 99.31%, 91.74%, and 98.49%, F1-scores of 0.9869, 0.9923,
0.9125, and 0.9771, Precision of 0.9866, 0.9924, 0.9180 and 0.9845, Recall of
0.9878, 0.9923, 0.9129, and 0.9756, equal error rates (EER) of 0.0009, 0.00013,
0.0091, and 0.0009, and ROC-AUC values of 0.9999, 0.9999, 0.9985, and 0.9998,
while under FGSM adversarial attacks, accuracy drops from 96.82% to as low as
0.80%. This paper highlights federated learning, adversarial testing, and the
need for diverse wearable physiological datasets to ensure secure and scalable
biometrics.

</details>


### [2] [MARS: A Malignity-Aware Backdoor Defense in Federated Learning](https://arxiv.org/abs/2509.20383)
*Wei Wan,Yuxuan Ning,Zhicong Huang,Cheng Hong,Shengshan Hu,Ziqi Zhou,Yechao Zhang,Tianqing Zhu,Wanlei Zhou,Leo Yu Zhang*

Main category: cs.CR

TL;DR: This paper proposes MARS, a malignity-aware defense mechanism for federated learning to counter backdoor attacks, demonstrating superior performance against existing methods.


<details>
  <summary>Details</summary>
Motivation: The multiplicative weights method in FL is insufficient for backdoor attack prevention as it relies on weakly-correlated statistical measures.

Method: MARS employs backdoor energy (BE) and concentrated backdoor energy (CBE) to detect malicious neurons, using a Wasserstein distance-based clustering method to identify backdoor models.

Result: MARS effectively defends against SOTA backdoor attacks and outperforms existing defenses in experiments.

Conclusion: MARS provides a robust solution for detecting and mitigating backdoor attacks in FL by focusing on backdoor energy and novel clustering techniques.

Abstract: Federated Learning (FL) is a distributed paradigm aimed at protecting
participant data privacy by exchanging model parameters to achieve high-quality
model training. However, this distributed nature also makes FL highly
vulnerable to backdoor attacks. Notably, the recently proposed state-of-the-art
(SOTA) attack, 3DFed (SP2023), uses an indicator mechanism to determine whether
the backdoor models have been accepted by the defender and adaptively optimizes
backdoor models, rendering existing defenses ineffective. In this paper, we
first reveal that the failure of existing defenses lies in the employment of
empirical statistical measures that are loosely coupled with backdoor attacks.
Motivated by this, we propose a Malignity-Aware backdooR defenSe (MARS) that
leverages backdoor energy (BE) to indicate the malicious extent of each neuron.
To amplify malignity, we further extract the most prominent BE values from each
model to form a concentrated backdoor energy (CBE). Finally, a novel
Wasserstein distance-based clustering method is introduced to effectively
identify backdoor models. Extensive experiments demonstrate that MARS can
defend against SOTA backdoor attacks and significantly outperforms existing
defenses.

</details>


### [3] [R1-Fuzz: Specializing Language Models for Textual Fuzzing via Reinforcement Learning](https://arxiv.org/abs/2509.20384)
*Jiayi Lin,Liangcai Su,Junzhe Li,Chenxiong Qian*

Main category: cs.CR

TL;DR: R1-Fuzz enables efficient, effective fuzzing of complex systems via RL-specialized LMs, achieving 75% coverage gains and discovering 29 new vulnerabilities using a cost-effective small model.


<details>
  <summary>Details</summary>
Motivation: Traditional fuzzing struggles with complex targets due to syntactic/semantic constraints, while existing LM-based approaches face (1) Inadequate exploration of deep program logic and (2) High computational costs of large models. This limits practical adoption despite LMs' potential for code understanding.

Method: The framework introduces: (1) Coverage-slicing-based question construction to guide LM training, (2) Distance-based reward calculation for RL post-training, and (3) An integrated fuzzing workflow that leverages LMs for semantic reasoning during input generation. This enables cost-effective specialization of language models for syntactic/semantic constraints of complex systems.

Result: Evaluations show R1-Fuzz achieves 75% higher code coverage than state-of-the-art fuzzers. Using a 7B parameter model (R1-Fuzz-7B), it discovers 29 new vulnerabilities in real-world targets, outperforming significantly larger LM-based approaches while maintaining cost-efficiency.

Conclusion: R1-Fuzz addresses challenges in real-world codebase fuzzing by combining reinforcement learning with language models, achieving state-of-the-art coverage and vulnerability discovery while maintaining cost-efficiency. The approach demonstrates that smaller specialized LMs can match or exceed larger models' performance in complex fuzzing tasks.

Abstract: Fuzzing is effective for vulnerability discovery but struggles with complex
targets such as compilers, interpreters, and database engines, which accept
textual input that must satisfy intricate syntactic and semantic constraints.
Although language models (LMs) have attracted interest for this task due to
their vast latent knowledge and reasoning potential, their practical adoption
has been limited. The major challenges stem from insufficient exploration of
deep program logic among real-world codebases, and the high cost of leveraging
larger models. To overcome these challenges, we propose R1-Fuzz, the first
framework that leverages reinforcement learning (RL) to specialize
cost-efficient LMs and integrate them for complex textual fuzzing input
generation. R1-Fuzz introduces two key designs: coverage-slicing-based question
construction and a distance-based reward calculation. Through RL-based
post-training of a model with our constructed dataset, R1-Fuzz designs a
fuzzing workflow that tightly integrates LMs to reason deep program semantics
during fuzzing. Evaluations on diverse real-world targets show that our design
enables a small model, named R1-Fuzz-7B, to rival or even outperform much
larger models in real-world fuzzing. Notably, R1-Fuzz achieves up to 75\%
higher coverage than state-of-the-art fuzzers and discovers 29 previously
unknown vulnerabilities, demonstrating its practicality.

</details>


### [4] [Can You Trust Your Copilot? A Privacy Scorecard for AI Coding Assistants](https://arxiv.org/abs/2509.20388)
*Amir AL-Maamari*

Main category: cs.CR

TL;DR: Paper evaluates AI coding assistants' privacy practices using an expert-scored framework, uncovering risks and promoting evidence-based tool selection to drive better privacy standards.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from growing privacy and trust concerns as developers entrust proprietary code to AI coding assistants, where unclear data handling practices pose security and compliance risks.

Method: The paper introduces a privacy scorecard evaluated by a legal expert and data protection officer, analyzing four document types against 14 weighted criteria across five leading AI assistants.

Result: Results reveal a 20-point gap between top and bottom performers, with common weaknesses including opt-out consent models and failure to proactively filter secrets from user prompts.

Conclusion: This paper concludes that a novel privacy scorecard establishes a new benchmark for transparency in AI-powered coding assistants, advocating for user-centric privacy standards and providing actionable guidance for developers to select tools based on evidence.

Abstract: The rapid integration of AI-powered coding assistants into developer
workflows has raised significant privacy and trust concerns. As developers
entrust proprietary code to services like OpenAI's GPT, Google's Gemini, and
GitHub Copilot, the unclear data handling practices of these tools create
security and compliance risks. This paper addresses this challenge by
introducing and applying a novel, expert-validated privacy scorecard. The
methodology involves a detailed analysis of four document types; from legal
policies to external audits; to score five leading assistants against 14
weighted criteria. A legal expert and a data protection officer refined these
criteria and their weighting. The results reveal a distinct hierarchy of
privacy protections, with a 20-point gap between the highest- and lowest-ranked
tools. The analysis uncovers common industry weaknesses, including the
pervasive use of opt-out consent for model training and a near-universal
failure to filter secrets from user prompts proactively. The resulting
scorecard provides actionable guidance for developers and organizations,
enabling evidence-based tool selection. This work establishes a new benchmark
for transparency and advocates for a shift towards more user-centric privacy
standards in the AI industry.

</details>


### [5] [A Comparative Analysis of Ensemble-Based Machine Learning Approaches with Explainable AI for Multi-Class Intrusion Detection in Drone Networks](https://arxiv.org/abs/2509.20391)
*Md. Alamgir Hossain,Waqas Ishtiaq,Md. Samiul Islam*

Main category: cs.CR

TL;DR: This study addresses cybersecurity threats in drone networks by developing a high-performing intrusion detection framework using ensemble machine learning models, emphasizing interpretability for real-time applications.


<details>
  <summary>Details</summary>
Motivation: Drones are increasingly integrated into multiple sectors, creating vulnerabilities to sophisticated network-based intrusions. Existing intrusion detection solutions lack the necessary robustness and interpretability for such dynamic environments.

Method: The authors conduct a comparative analysis of ensemble-based machine learning models (Random Forest, Extra Trees, AdaBoost, CatBoost, XGBoost) using a labeled dataset with benign traffic and nine intrusion types. They apply comprehensive data preprocessing and evaluate their results using statistical tests (Friedman's test, Wilcoxon signed-rank test with Holm correction) and performance metrics (macro F1-score, ROC AUC, etc.). For explainability,they use SHAP and LIME to interpret global and local feature importance.

Result: Random Forest outperforms other models with a macro F1-score of 0.9998 and ROC AUC of 1.0000. Statistical validation confirms the effectiveness of the ensemble approach. SHAP and LIME successfully identify critical features for intrusion detection.

Conclusion: The proposed framework provides a robust, accurate, and interpretable solution for securing drone networks against diverse cyber threats, making it suitable for real-time applications in safety-critical environments.

Abstract: The growing integration of drones into civilian, commercial, and defense
sectors introduces significant cybersecurity concerns, particularly with the
increased risk of network-based intrusions targeting drone communication
protocols. Detecting and classifying these intrusions is inherently challenging
due to the dynamic nature of drone traffic and the presence of multiple
sophisticated attack vectors such as spoofing, injection, replay, and
man-in-the-middle (MITM) attacks. This research aims to develop a robust and
interpretable intrusion detection framework tailored for drone networks, with a
focus on handling multi-class classification and model explainability. We
present a comparative analysis of ensemble-based machine learning models,
namely Random Forest, Extra Trees, AdaBoost, CatBoost, and XGBoost, trained on
a labeled dataset comprising benign traffic and nine distinct intrusion types.
Comprehensive data preprocessing was performed, including missing value
imputation, scaling, and categorical encoding, followed by model training and
extensive evaluation using metrics such as macro F1-score, ROC AUC, Matthews
Correlation Coefficient, and Log Loss. Random Forest achieved the highest
performance with a macro F1-score of 0.9998 and ROC AUC of 1.0000. To validate
the superiority of the models, statistical tests, including Friedmans test, the
Wilcoxon signed-rank test with Holm correction, and bootstrapped confidence
intervals, were applied. Furthermore, explainable AI methods, SHAP and LIME,
were integrated to interpret both global and local feature importance,
enhancing model transparency and decision trustworthiness. The proposed
approach not only delivers near-perfect accuracy but also ensures
interpretability, making it highly suitable for real-time and safety-critical
drone operations.

</details>


### [6] [Centralized vs. Decentralized Security for Space AI Systems? A New Look](https://arxiv.org/abs/2509.20395)
*Noam Schmitt,Marc Antoine Lacoste*

Main category: cs.CR

TL;DR: The paper compares centralized, distributed, and federated AI architectures for satellite constellation security management, finding centralized systems optimal short-term for fast training but hindered by communication latency, while decentralized approaches offer better long-term scalability and security.


<details>
  <summary>Details</summary>
Motivation: Satellite constellations require balancing security and performance as systems scale. Traditional centralized security management introduces latency, while decentralized approaches remain underexplored.

Method: Analysis of three AI architectures (centralized, distributed, federated), evaluating their trade-offs in training speed, communication latency, scalability, and security for space systems.

Result: Centralized architectures enable rapid training but face latency challenges. Decentralized architectures provide superior long-term scalability and security but require addressing coordination complexities.

Conclusion: Decentralized security management is positioned as the optimal long-term solution for satellite constellations despite short-term technical challenges, emphasizing the need for adaptive architectural choices based on deployment timelines.

Abstract: This paper investigates the trade-off between centralized and decentralized
security management in constellations of satellites to balance security and
performance. We highlight three key AI architectures for automated security
management: (a) centralized, (b) distributed and (c) federated. The centralized
architecture is the best option short term, providing fast training, despite
the hard challenge of the communication latency overhead across space.
Decentralized architectures are better alternatives in the longer term,
providing enhanced scalability and security.

</details>


### [7] [Defending against Stegomalware in Deep Neural Networks with Permutation Symmetry](https://arxiv.org/abs/2509.20399)
*Birk Torpmann-Hagen,Michael A. Riegler,Pål Halvorsen,Dag Johansen*

Main category: cs.CR

TL;DR: This paper introduces an effective defense against stegomalware in neural networks through weight matrix shuffling, neutralizing malware without accuracy loss and exceeding existing methods' performance.


<details>
  <summary>Details</summary>
Motivation: The work highlights the security risk of neural network stegomalware - malware embedded in shared checkpoints with negligible accuracy loss - which is neglected by both deep learning practitioners and security specialists despite its significant threat potential.

Method: The authors propose shuffling the column order of weight-/bias-matrices (or channel-order in convolutional layers) to corrupt embedded stegomalware payloads while preserving network accuracy. They experimentally demonstrate this defense's effectiveness against state-of-the-art stegomalware.

Result: The proposed defense outperforms existing methods by a significant margin in neutralizing stegomalware while maintaining accuracy. The paper also analyzes potential bypasses and suggests additional defense strategies.

Conclusion: The paper concludes that shuffling weight and bias matrix columns in neural networks effectively neutralizes stegal malware without compromising accuracy, addressing a critical yet overlooked security threat in deep learning. Continued research into machine learning security is advocated.

Abstract: Deep neural networks are being utilized in a growing number of applications,
both in production systems and for personal use. Network checkpoints are as a
consequence often shared and distributed on various platforms to ease the
development process. This work considers the threat of neural network
stegomalware, where malware is embedded in neural network checkpoints at a
negligible cost to network accuracy. This constitutes a significant security
concern, but is nevertheless largely neglected by the deep learning
practitioners and security specialists alike. We propose the first effective
countermeasure to these attacks. In particular, we show that state-of-the-art
neural network stegomalware can be efficiently and effectively neutralized
through shuffling the column order of the weight- and bias-matrices, or
equivalently the channel-order of convolutional layers. We show that this
effectively corrupts payloads that have been embedded by state-of-the-art
methods in neural network steganography at no cost to network accuracy,
outperforming competing methods by a significant margin. We then discuss
possible means by which to bypass this defense, additional defense methods, and
advocate for continued research into the security of machine learning systems.

</details>


### [8] [Why Speech Deepfake Detectors Won't Generalize: The Limits of Detection in an Open World](https://arxiv.org/abs/2509.20405)
*Visar Berisha,Prad Kadambi,Isabella Lenz*

Main category: cs.CR

TL;DR: Speech deepfake detectors face 'coverage debt' due to real-world deployment challenges like device shifts and new attack methods, creating blind spots that weaken security. New synthesizers eliminate detectable artifacts while conversational speech domains remain especially insecure, arguing detectors must be part of multi-layered defenses.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations focus on controlled conditions, but real-world deployment encounters diverse devices/codecs/environments, which create exponentially-growing data blind spots. Security risks arise since attackers can specifically exploit these uncovered regions.

Method: Analyzed cross-testing framework results by grouping performance across 1. bona fide domains (e.g., teleconferencing) and 2. spoof release years to identify patterns in detection vulnerabilities.

Result: 1. New synthesizers remove detectable artifacts of older methods | 2. Conversational speech domains consistently exhibit lowest detection accuracy | 3. Worst-case performance reveals critical security gaps

Conclusion: Reliance on detection alone is insufficient for high-stakes applications. Detection should be augmented with layered security approaches combining provenance tracking, personhood verification, and policy safeguards to address coverage debt.

Abstract: Speech deepfake detectors are often evaluated on clean, benchmark-style
conditions, but deployment occurs in an open world of shifting devices,
sampling rates, codecs, environments, and attack families. This creates a
``coverage debt" for AI-based detectors: every new condition multiplies with
existing ones, producing data blind spots that grow faster than data can be
collected. Because attackers can target these uncovered regions, worst-case
performance (not average benchmark scores) determines security. To demonstrate
the impact of the coverage debt problem, we analyze results from a recent
cross-testing framework. Grouping performance by bona fide domain and spoof
release year, two patterns emerge: newer synthesizers erase the legacy
artifacts detectors rely on, and conversational speech domains
(teleconferencing, interviews, social media) are consistently the hardest to
secure. These findings show that detection alone should not be relied upon for
high-stakes decisions. Detectors should be treated as auxiliary signals within
layered defenses that include provenance, personhood credentials, and policy
safeguards.

</details>


### [9] [Adversarial Defense in Cybersecurity: A Systematic Review of GANs for Threat Detection and Mitigation](https://arxiv.org/abs/2509.20411)
*Tharcisse Ndayipfukamiye,Jianguo Ding,Doreen Sebastian Sarwatt,Adamu Gaston Philipo,Huansheng Ning*

Main category: cs.CR

TL;DR: This survey examines GAN-based adversarial defenses in cybersecurity from 2021 to 2025, highlighting their benefits and current challenges. It introduces a taxonomy, reviews techniques like WGAN-GP, and suggests future research directions.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to comprehensively review the advancements and challenges in GAN-based adversarial defenses for cybersecurity, given GANs' dual role as both powerful attack enablers and strong defenses against adversarial threats.

Method: The method used in this paper involves a PRISMA-compliant systematic literature review protocol across five major digital libraries. Data synthesis included quantitative trend analysis and the development of a thematic taxonomy from 185 peer-reviewed studies selected from 829 initial records.

Result: The result of the survey reveals that GANs have enhanced detection accuracy, robustness, and data utility in areas like network intrusion detection and malware analysis. Techniques such as WGAN-GP, CGANs, and hybrid GAN models have shown notable improvements, but challenges in training stability and benchmarking persist.

Conclusion: The paper concludes GAN-based defenses hold significant potential for cybersecurity but require improvements in training stability, benchmarking standards, and deployment strategies to achieve scalable and adaptive solutions against emerging threats, such as LLM-driven attacks.

Abstract: Machine learning-based cybersecurity systems are highly vulnerable to
adversarial attacks, while Generative Adversarial Networks (GANs) act as both
powerful attack enablers and promising defenses. This survey systematically
reviews GAN-based adversarial defenses in cybersecurity (2021--August 31,
2025), consolidating recent progress, identifying gaps, and outlining future
directions. Using a PRISMA-compliant systematic literature review protocol, we
searched five major digital libraries. From 829 initial records, 185
peer-reviewed studies were retained and synthesized through quantitative trend
analysis and thematic taxonomy development. We introduce a four-dimensional
taxonomy spanning defensive function, GAN architecture, cybersecurity domain,
and adversarial threat model. GANs improve detection accuracy, robustness, and
data utility across network intrusion detection, malware analysis, and IoT
security. Notable advances include WGAN-GP for stable training, CGANs for
targeted synthesis, and hybrid GAN models for improved resilience. Yet,
persistent challenges remain such as instability in training, lack of
standardized benchmarks, high computational cost, and limited explainability.
GAN-based defenses demonstrate strong potential but require advances in stable
architectures, benchmarking, transparency, and deployment. We propose a roadmap
emphasizing hybrid models, unified evaluation, real-world integration, and
defenses against emerging threats such as LLM-driven cyberattacks. This survey
establishes the foundation for scalable, trustworthy, and adaptive GAN-powered
defenses.

</details>


### [10] [A Taxonomy of Data Risks in AI and Quantum Computing (QAI) - A Systematic Review](https://arxiv.org/abs/2509.20418)
*Grace Billiris,Asif Gill,Madhushi Bandara*

Main category: cs.CR

TL;DR: This study creates a 22-risk taxonomy for QAI, revealing unique vulnerabilities and gaps, offering a framework for future risk assessment tools.


<details>
  <summary>Details</summary>
Motivation: Quantum AI inherits data risks from both AI and QC, creating under-studied privacy/security vulnerabilities that impact system trustworthiness, necessitating systematic understanding for reliable development.

Method: Systematic review of 67 privacy/security-related studies, resulting in a taxonomy of 22 key data risks organized into five categories (governance, risk assessment, control implementation, user considerations, and continuous monitoring).

Result: Revealed QAI-specific vulnerabilities, identified gaps in holistic risk assessment, and established a structured risk taxonomy, contributing to the foundation of future risk assessment tools.

Conclusion: The study provides a foundational framework for trustworthy AI and QAI research, advancing future risk assessment tool development by systematically identifying QAI-specific data vulnerabilities and assessment gaps.

Abstract: Quantum Artificial Intelligence (QAI), the integration of Artificial
Intelligence (AI) and Quantum Computing (QC), promises transformative advances,
including AI-enabled quantum cryptography and quantum-resistant encryption
protocols. However, QAI inherits data risks from both AI and QC, creating
complex privacy and security vulnerabilities that are not systematically
studied. These risks affect the trustworthiness and reliability of AI and QAI
systems, making their understanding critical. This study systematically reviews
67 privacy- and security-related studies to expand understanding of QAI data
risks. We propose a taxonomy of 22 key data risks, organised into five
categories: governance, risk assessment, control implementation, user
considerations, and continuous monitoring. Our findings reveal vulnerabilities
unique to QAI and identify gaps in holistic risk assessment. This work
contributes to trustworthy AI and QAI research and provides a foundation for
developing future risk assessment tools.

</details>


### [11] [Differential Privacy of Network Parameters from a System Identification Perspective](https://arxiv.org/abs/2509.20460)
*Andrew Campbell,Anna Scaglione,Hang Liu,Victor Elvira,Sean Peisert,Daniel Arnold*

Main category: cs.CR

TL;DR: This paper investigates privacy protection against graph structure identification attacks in cyber-physical system simulations by applying differential privacy (DP) to input signals, linking privacy guarantees to the spectral properties of graph filters and noise covariance.


<details>
  <summary>Details</summary>
Motivation: Traditional privacy-preserving methods focus on parameter estimation, but this work addresses the inverse problem: preventing adversaries from inferring the graph shift operator (GSO) while maintaining utility for legitimate analysis. Such a gap exists as prior work lacks formal privacy bounds for graph structure protection in simulation data sharing scenarios.

Method: The authors model the analyst's observations as time-series outputs of a graph filter driven by DP nodal excitation signals. They mathematically analyze how spectral characteristics of the filter and covariance matrix of the DP noise influence the $(\epsilon,\delta)$-DP guarantees for the GSO, focusing on Gaussian signal scenarios.

Result: They derive formal privacy bounds for the GSO based on the interplay between filter smoothness, noise covariance condition number, and DP parameters. Results show smoother filters and low-condition-number noise covariances yield stronger privacy protection without compromising utility.

Conclusion: The paper demonstrates that input-level DP mechanisms can preserve graph structure privacy in simulations while maintaining analytical utility, providing actionable insights into designing DP systems by optimizing spectral characteristics of graph filters and noise structures.

Abstract: This paper addresses the problem of protecting network information from
privacy system identification (SI) attacks when sharing cyber-physical system
simulations. We model analyst observations of networked states as time-series
outputs of a graph filter driven by differentially private (DP) nodal
excitations, with the analyst aiming to infer the underlying graph shift
operator (GSO). Unlike traditional SI, which estimates system parameters, we
study the inverse problem: what assumptions prevent adversaries from
identifying the GSO while preserving utility for legitimate analysis. We show
that applying DP mechanisms to inputs provides formal privacy guarantees for
the GSO, linking the $(\epsilon,\delta)$-DP bound to the spectral properties of
the graph filter and noise covariance. More precisely, for DP Gaussian signals,
the spectral characteristics of both the filter and noise covariance determine
the privacy bound, with smooth filters and low-condition-number covariance
yielding greater privacy.

</details>


### [12] [Advancing Practical Homomorphic Encryption for Federated Learning: Theoretical Guarantees and Efficiency Optimizations](https://arxiv.org/abs/2509.20476)
*Ren-Yi Huang,Dumindu Samaraweera,Prashant Shekhar,J. Morris Chang*

Main category: cs.CR

TL;DR: This paper bridges theory and practice in selective encryption for Federated Learning, providing a framework to analyze privacy-preserving gradient sharing and identifying key factors that determine defense effectiveness against model inversion attacks.


<details>
  <summary>Details</summary>
Motivation: Existing selective encryption approaches in Federated Learning lack theoretical analysis of their spectral behavior and rely on empirical evaluations, creating a gap in understanding how gradient selection impacts privacy preservation against model inversion attacks like DLG.

Method: The study presents a theoretical framework to analyze selective encryption principles and conducts a comprehensive empirical evaluation to quantify factors influencing defense effectiveness, such as model complexity, encryption ratios, and exposed gradients.

Result: The theoretical framework establishes principles linking gradient selection to privacy, while experiments demonstrate how factors like encryption ratios and model complexity determine defense robustness, validating the effectiveness of selective encryption strategies.

Conclusion: The paper advances understanding of selective encryption mechanisms in Federated Learning, offering theoretical and empirical guidance for designing efficient, privacy-preserving systems by clarifying the relationship between gradient selection and defense robustness against model inversion attacks.

Abstract: Federated Learning (FL) enables collaborative model training while preserving
data privacy by keeping raw data locally stored on client devices, preventing
access from other clients or the central server. However, recent studies reveal
that sharing model gradients creates vulnerability to Model Inversion Attacks,
particularly Deep Leakage from Gradients (DLG), which reconstructs private
training data from shared gradients. While Homomorphic Encryption has been
proposed as a promising defense mechanism to protect gradient privacy, fully
encrypting all model gradients incurs high computational overhead. Selective
encryption approaches aim to balance privacy protection with computational
efficiency by encrypting only specific gradient components. However, the
existing literature largely overlooks a theoretical exploration of the spectral
behavior of encrypted versus unencrypted parameters, relying instead primarily
on empirical evaluations. To address this gap, this paper presents a framework
for theoretical analysis of the underlying principles of selective encryption
as a defense against model inversion attacks. We then provide a comprehensive
empirical study that identifies and quantifies the critical factors, such as
model complexity, encryption ratios, and exposed gradients, that influence
defense effectiveness. Our theoretical framework clarifies the relationship
between gradient selection and privacy preservation, while our experimental
evaluation demonstrates how these factors shape the robustness of defenses
against model inversion attacks. Collectively, these contributions advance the
understanding of selective encryption mechanisms and offer principled guidance
for designing efficient, scalable, privacy-preserving federated learning
systems.

</details>


### [13] [Every Character Counts: From Vulnerability to Defense in Phishing Detection](https://arxiv.org/abs/2509.20589)
*Maria Chiper,Radu Tudor Ionescu*

Main category: cs.CR

TL;DR: The paper explores character-level deep learning models for phishing detection, focusing on robustness and interpretability. It evaluates CharCNN, CharGRU, and CharBiLSTM under standard and adversarial conditions, finding CharGRU as the best performer in constrained setups. Adversarial training improves robustness, and Grad-CAM is adapted for visual interpretation.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the growing threat of phishing attacks and the insufficient explainability and robustness of current automatic detection methods in keeping up with new attack strategies.

Method: The method involves evaluating three character-level neural architectures (CharCNN, CharGRU, CharBiLSTM) on a custom email dataset under three scenarios: standard, adversarial testing, and training with adversarial examples, while testing under limited computational resources for browser extension use.

Result: In a resource-constrained setup, CharGRU outperforms in all scenarios. All models are vulnerable to adversarial attacks but show improved robustness via adversarial training. Model decision influences in emails are visualized using an adapted Grad-CAM approach.

Conclusion: The study concludes that character-level models, particularly CharGRU, offer viable solutions for phishing detection where robustness, interpretability, and low computational demands are essential. Adversarial training and interpretation methods like Grad-CAM are valuable for enhancing performance and understanding decision mechanisms.

Abstract: Phishing attacks targeting both organizations and individuals are becoming an
increasingly significant threat as technology advances. Current automatic
detection methods often lack explainability and robustness in detecting new
phishing attacks. In this work, we investigate the effectiveness of
character-level deep learning models for phishing detection, which can provide
both robustness and interpretability. We evaluate three neural architectures
adapted to operate at the character level, namely CharCNN, CharGRU, and
CharBiLSTM, on a custom-built email dataset, which combines data from multiple
sources. Their performance is analyzed under three scenarios: (i) standard
training and testing, (ii) standard training and testing under adversarial
attacks, and (iii) training and testing with adversarial examples. Aiming to
develop a tool that operates as a browser extension, we test all models under
limited computational resources. In this constrained setup, CharGRU proves to
be the best-performing model across all scenarios. All models show
vulnerability to adversarial attacks, but adversarial training substantially
improves their robustness. In addition, by adapting the Gradient-weighted Class
Activation Mapping (Grad-CAM) technique to character-level inputs, we are able
to visualize which parts of each email influence the decision of each model.
Our open-source code and data is released at
https://github.com/chipermaria/every-character-counts.

</details>


### [14] [Beyond SSO: Mobile Money Authentication for Inclusive e-Government in Sub-Saharan Africa](https://arxiv.org/abs/2509.20592)
*Oluwole Adewusi,Wallace S. Msagusa,Jean Pierre Imanirumva,Okemawo Obadofin,Jema D. Ndibwile*

Main category: cs.CR

TL;DR: This study introduces a hybrid MMA framework for Sub-Saharan Africa that combines USSD-based multi-factor authentication with secure session management via JWT, offering faster authentication, better performance under poor network conditions, and enhanced security against attacks like phishing and SIM swapping.


<details>
  <summary>Details</summary>
Motivation: The rapid adoption of Mobile Money Services (MMS) in Sub-Saharan Africa (SSA) presents an opportunity to boost e-Government accessibility despite low internet penetration. But existing MMS authentication methods are flawed, leaving systems less secure and less scalable.

Method: A hybrid framework is proposed, integrating USSD multi-factor authentication with secure session management through cryptographically bound JSON Web Tokens (JWT). It employs a three-factor authentication model: SIM verification, PIN entry, and session token binding, optimized for resource-constrained environments. The method is compared with OAuth-based SSO through simulations and tested using penetration testing and threat modeling.

Result: The proposed method shows a 45% faster authentication (8 seconds vs. 12 to 15), 15% higher success under poor network (95% vs. 80%), and stronger resilience to phishing and brute-force attacks. The framework demonstrates efficient scalability for large user bases with reduced infrastructure costs.

Conclusion: The hybrid authentication framework provides an innovative solution to the limitations of current MMA systems in SSA. It maintains offline accessibility while mitigating major security threats and can scale securely. This approach supports broader digital inclusion in low-connectivity regions.

Abstract: The rapid adoption of Mobile Money Services (MMS) in Sub-Saharan Africa (SSA)
offers a viable path to improve e-Government service accessibility in the face
of persistent low internet penetration. However, existing Mobile Money
Authentication (MMA) methods face critical limitations, including
susceptibility to SIM swapping, weak session protection, and poor scalability
during peak demand. This study introduces a hybrid MMA framework that combines
Unstructured Supplementary Service Data (USSD)-based multi-factor
authentication with secure session management via cryptographically bound JSON
Web Tokens (JWT). Unlike traditional MMA systems that rely solely on SIM-PIN
verification or smartphone-dependent biometrics, our design implements a
three-factor authentication model; SIM verification, PIN entry, and session
token binding, tailored for resource-constrained environments. Simulations and
comparative analysis against OAuth-based Single Sign-On (SSO) methods reveal a
45% faster authentication time (8 seconds vs. 12 to 15 seconds), 15% higher
success under poor network conditions (95% vs. 80%), and increased resistance
to phishing and brute-force attacks. Penetration testing and threat modeling
further demonstrate a substantial reduction in vulnerability exposure compared
to conventional approaches. The primary contributions of this work are: (1) a
hybrid authentication protocol that ensures offline accessibility and secure
session continuity; (2) a tailored security framework addressing threats like
SIM swapping and social engineering in SSA; and (3) demonstrated scalability
for thousands of users with reduced infrastructure overhead. The proposed
approach advances secure digital inclusion in SSA and other regions with
similar constraints.

</details>


### [15] [A Framework for Rapidly Developing and Deploying Protection Against Large Language Model Attacks](https://arxiv.org/abs/2509.20639)
*Adam Swanda,Amy Chang,Alexander Chen,Fraser Burch,Paul Kassianik,Konstantin Berlin*

Main category: cs.CR

TL;DR: This paper proposes an LLM defense system inspired by malware protection, combining threat intelligence, data aggregation, and automated updates to combat evolving threats through continuous adaptation and non-disruptive deployment.


<details>
  <summary>Details</summary>
Motivation: The motivation addresses the growing security vulnerabilities of autonomous LLM applications. Prior work lacks end-to-end systems for adapting to dynamic threats, creating a need for robust, production-ready solutions akin to traditional malware defense frameworks.

Method: The method introduces a production-grade defense system with three components: (1) a threat intelligence system converting emerging threats into protections, (2) a data platform for aggregation, enrichment, and ML operations, and (3) a release platform enabling non-disruptive updates. These components work synergistically to improve model training and adapt to new threats.

Result: The system achieves layered protection against LLM threats, generates training data for model improvement, and facilitates rapid updates without disrupting workflows. It bridges the gap between theoretical detection models and practical, adaptive defense systems.

Conclusion: The paper concludes that AI protection systems, similar to traditional malware defenses, must focus on observability, multi-layered strategies, and rapid threat response to mitigate risks posed by evolving threats to LLMs. Their proposed system emphasizes continuous adaptation and safe deployment of updates.

Abstract: The widespread adoption of Large Language Models (LLMs) has revolutionized AI
deployment, enabling autonomous and semi-autonomous applications across
industries through intuitive language interfaces and continuous improvements in
model development. However, the attendant increase in autonomy and expansion of
access permissions among AI applications also make these systems compelling
targets for malicious attacks. Their inherent susceptibility to security flaws
necessitates robust defenses, yet no known approaches can prevent zero-day or
novel attacks against LLMs. This places AI protection systems in a category
similar to established malware protection systems: rather than providing
guaranteed immunity, they minimize risk through enhanced observability,
multi-layered defense, and rapid threat response, supported by a threat
intelligence function designed specifically for AI-related threats.
  Prior work on LLM protection has largely evaluated individual detection
models rather than end-to-end systems designed for continuous, rapid adaptation
to a changing threat landscape. We present a production-grade defense system
rooted in established malware detection and threat intelligence practices. Our
platform integrates three components: a threat intelligence system that turns
emerging threats into protections; a data platform that aggregates and enriches
information while providing observability, monitoring, and ML operations; and a
release platform enabling safe, rapid detection updates without disrupting
customer workflows. Together, these components deliver layered protection
against evolving LLM threats while generating training data for continuous
model improvement and deploying updates without interrupting production.

</details>


### [16] [Reliability Analysis of Fully Homomorphic Encryption Systems Under Memory Faults](https://arxiv.org/abs/2509.20686)
*Rian Adam Rajagede,Yan Solihin*

Main category: cs.CR

TL;DR: This paper explores the reliability of Fully Homomorphic Encryption (FHE) systems under memory faults, examining the behavior of individual operations and applications across different FHE schemes, and evaluating traditional and FHE-specific fault mitigation techniques.


<details>
  <summary>Details</summary>
Motivation: Understanding how FHE computation behaves in the presence of memory faults is critical for ensuring the reliability of FHE systems, which have increasingly been deployed in real platforms. The neglect of reliability aspects, particularly in fault response, motivates this study to assess the impact of such faults and the effectiveness of mitigation strategies.

Method: The analysis involves rigorously examining FHE computation within the context of memory faults, evaluating individual operations and application-level behaviors across various FHE schemes. The effectiveness of traditional and FHE-specific fault mitigation techniques is also investigated through systematic testing.

Result: The paper identifies how FHE systems behave when subjected to memory faults, revealing the levels of impact at both operation and application scales for different FHE schemes. The results show the effectiveness of certain fault mitigation techniques, potentially highlighting those that are better suited for protecting FHE computations.

Conclusion: The study concludes that FHE systems must be evaluated not only for their cryptographic properties but also for their resilience to memory faults. It provides insights into the reliability of current FHE implementations and guidance for selecting or developing effective fault mitigation strategies to ensure secure and dependable computation in real-world applications.

Abstract: Fully Homomorphic Encryption (FHE) represents a paradigm shift in
cryptography, enabling computation directly on encrypted data and unlocking
privacy-critical computation. Despite being increasingly deployed in real
platforms, the reliability aspects of FHE systems, especially how they respond
to faults, have been mostly neglected. This paper aims to better understand of
how FHE computation behaves in the presence of memory faults, both in terms of
individual operations as well as at the level of applications, for different
FHE schemes. Finally, we investigate how effective traditional and FHE-specific
fault mitigation techniques are.

</details>


### [17] [Cryptographic Backdoor for Neural Networks: Boon and Bane](https://arxiv.org/abs/2509.20714)
*Anh Tu Ngo,Anupam Chattopadhyay,Subhamoy Maitra*

Main category: cs.CR

TL;DR: Cryptographic backdoors in NNs enable both powerful, invisible attacks and provably secure defenses (watermarking, authentication, IP tracking). The paper establishes theoretical foundations for robust protocols and explores quantum-era applications through post-quantum cryptography.


<details>
  <summary>Details</summary>
Motivation: The work addresses the duality of cryptographic backdoors in ML: improving attack stealth (via undetectable adversarial mechanisms) and enabling formally verified defenses. Previous research often focused unilaterally on attacks or defenses, but this study bridges the gap to provide secure, provably resilient protocols.

Method: The paper combines cryptographic techniques (e.g., Goldwasser et al.'s theoretical foundations) with NN architectures to design backdoor-based attacks and defenses. It provides theoretical proofs for defense robustness against black-box adversaries and validates results empirically on state-of-the-art models.

Result: The defense protocols (watermarking, authentication, IP tracking) are theoretically robust against adversaries with black-box access. Empirical results confirm effectiveness, and the inclusion of post-quantum primitives ensures future readiness. Conversely, backdoor-enabled attacks under standard cryptographic assumptions are unpreventable.

Conclusion: The paper demonstrates that cryptographic backdoors in neural networks (NNs) are effective for both adversarial attacks and defensive protocols. It introduces provably robust watermarking, authentication, and IP-tracking methods while highlighting the inherent vulnerability of backdoor-enabled attacks. The framework also aligns with quantum-era ML applications through post-quantum cryptographic primitives.

Abstract: In this paper we show that cryptographic backdoors in a neural network (NN)
can be highly effective in two directions, namely mounting the attacks as well
as in presenting the defenses as well. On the attack side, a carefully planted
cryptographic backdoor enables powerful and invisible attack on the NN.
Considering the defense, we present applications: first, a provably robust NN
watermarking scheme; second, a protocol for guaranteeing user authentication;
and third, a protocol for tracking unauthorized sharing of the NN intellectual
property (IP). From a broader theoretical perspective, borrowing the ideas from
Goldwasser et. al. [FOCS 2022], our main contribution is to show that all these
instantiated practical protocol implementations are provably robust. The
protocols for watermarking, authentication and IP tracking resist an adversary
with black-box access to the NN, whereas the backdoor-enabled adversarial
attack is impossible to prevent under the standard assumptions. While the
theoretical tools used for our attack is mostly in line with the Goldwasser et.
al. ideas, the proofs related to the defense need further studies. Finally, all
these protocols are implemented on state-of-the-art NN architectures with
empirical results corroborating the theoretical claims. Further, one can
utilize post-quantum primitives for implementing the cryptographic backdoors,
laying out foundations for quantum-era applications in machine learning (ML).

</details>


### [18] [ExpIDS: A Drift-adaptable Network Intrusion Detection System With Improved Explainability](https://arxiv.org/abs/2509.20767)
*Ayush Kumar,Kar Wai Fok,Vrizlynn L. L. Thing*

Main category: cs.CR

TL;DR: This paper proposes ExpIDS, a deep learning-based NIDS with high decision tree explanation fidelity and adaptability to traffic distribution drift, achieving performance comparable to state-of-the-art systems.


<details>
  <summary>Details</summary>
Motivation: Machine learning-based NIDS are underutilized due to opacity and lack of trust in their decision-making processes. Cybersecurity experts require explainable and adaptive models for real-world deployment.

Method: ExpIDS combines deep learning with decision tree explanation mechanisms to ensure predictions align closely with interpretable explanations. It incorporates techniques to adapt to network traffic drift through extensive experimental validation.

Result: ExpIDS demonstrates higher decision tree explanation fidelity than existing models and maintains effective malicious traffic detection performance across varying real-world traffic distributions and common attack schemas.

Conclusion: ExpIDS bridges the gap between high-performance NIDS and human-interpretable explanations, enabling practical deployment of ML-based security systems while maintaining adaptability to evolving network environments.

Abstract: Despite all the advantages associated with Network Intrusion Detection
Systems (NIDSs) that utilize machine learning (ML) models, there is a
significant reluctance among cyber security experts to implement these models
in real-world production settings. This is primarily because of their opaque
nature, meaning it is unclear how and why the models make their decisions. In
this work, we design a deep learning-based NIDS, ExpIDS to have high decision
tree explanation fidelity, i.e., the predictions of decision tree explanation
corresponding to ExpIDS should be as close to ExpIDS's predictions as possible.
ExpIDS can also adapt to changes in network traffic distribution (drift). With
the help of extensive experiments, we verify that ExpIDS achieves higher
decision tree explanation fidelity and a malicious traffic detection
performance comparable to state-of-the-art NIDSs for common attacks with
varying levels of real-world drift.

</details>


### [19] [Fast Revocable Attribute-Based Encryption with Data Integrity for Internet of Things](https://arxiv.org/abs/2509.20796)
*Yongjiao Li,Liang Zhu,Yalin Deng,Qikun Zhang,Zhenlei Wang,Zhu Cao*

Main category: cs.CR

TL;DR: A novel RABE scheme for IoT improves efficiency and security by offloading revocation to clouds and ensuring data integrity, outperforming existing methods computationally by 7–9 times.


<details>
  <summary>Details</summary>
Motivation: Current RABE schemes fail to balance efficiency, security, and scalability in IoT environments, limiting their real-world applicability. Efficient access control and data sharing in IoT and cloud storage necessitate optimized RABE solutions.

Method: A fast RABE scheme with data integrity is proposed, leveraging multiple challenge ciphertexts for adaptive security. Revocation computations are offloaded to the cloud, and data integrity is consistently guaranteed.

Result: Experimental results demonstrate the scheme's adaptive security under defined models and a 7–9× reduction in computational costs compared to existing solutions, achieving superior performance and data integrity.

Conclusion: The proposed RABE scheme enhances practicality for IoT by achieving adaptive security, dynamic scalability, and reduced computational overhead, with significant improvements in performance compared to existing methods.

Abstract: Efficient and secure revocable attribute-based encryption (RABE) is vital for
ensuring flexible and fine-grained access control and data sharing in cloud
storage and outsourced data environments within the Internet of Things (IoT).
However, current RABE schemes often struggle to achieve an optimal balance
between efficiency, security, dynamic scalability, and other important
features, which hampers their practical application. To overcome these
limitations, we propose a fast RABE scheme with data integrity for IoT that
achieves adaptive security with multiple challenge ciphertexts. Our scheme
supports the revocation of authorized users and transfers the computationally
heavy revocation processes to the cloud, thereby easing the computational
burden on IoT devices. Moreover, it consistently guarantees the integrity and
correctness of data. We have demonstrated its adaptive security within the
defined security model with multiple challenge ciphertexts and optimized its
performance. Experimental results indicate that our scheme provides better
performance than existing solutions. Under the same access policy, our scheme
reduces computational consumption by 7 to 9 times compared to previous schemes.

</details>


### [20] [Intelligent Graybox Fuzzing via ATPG-Guided Seed Generation and Submodule Analysis](https://arxiv.org/abs/2509.20808)
*Raghul Saravanan,Sudipta Paria,Aritra Dasgupta,Swarup Bhunia,Sai Manoj P D*

Main category: cs.CR

TL;DR: TLDR: PROFUZZ is a novel framework that combines directed gray-box fuzzing with ATPG for efficient hardware fuzzing, outperforming DirectFuzz in scalability, coverage, and speed.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the limitations of existing directed gray-box fuzzing (DGF) techniques in hardware fuzzing, such as DirectFuzz, which suffers from limited hardware description language support, poor scalability to large circuits, and abstraction mismatch issues. High-quality input seeds that maximize coverage and speed up verification remain a major challenge in hardware fuzzing.

Method: The proposed method, PROFUZZ, integrates directed gray-box fuzzing with Automatic Test Pattern Generation (ATPG). It uses ATPG's structural analysis to generate precise input seeds that target specific regions of the hardware design, thereby improving efficiency and maintaining high fuzzing throughput. This combination allows for better scalability and targeted exploration of the design space compared to existing DGF approaches.

Result: Experimental results demonstrate that PROFUZZ scales 30 times better than DirectFuzz when dealing with multiple target sites. Additionally, it improves coverage by 11.66% and operates 2.76 times faster, showcasing significant performance advantages.

Conclusion: By combining directed gray-box fuzzing with ATPG, PROFUZZ effectively overcomes key limitations of current hardware fuzzing techniques. It achieves superior scalability, coverage, and speed, making it a highly effective solution for directed fuzzing in complex hardware systems.

Abstract: Hardware Fuzzing emerged as one of the crucial techniques for finding
security flaws in modern hardware designs by testing a wide range of input
scenarios. One of the main challenges is creating high-quality input seeds that
maximize coverage and speed up verification. Coverage-Guided Fuzzing (CGF)
methods help explore designs more effectively, but they struggle to focus on
specific parts of the hardware. Existing Directed Gray-box Fuzzing (DGF)
techniques like DirectFuzz try to solve this by generating targeted tests, but
it has major drawbacks, such as supporting only limited hardware description
languages, not scaling well to large circuits, and having issues with
abstraction mismatches. To address these problems, we introduce a novel
framework, PROFUZZ, that follows the DGF approach and combines fuzzing with
Automatic Test Pattern Generation (ATPG) for more efficient fuzzing. By
leveraging ATPG's structural analysis capabilities, PROFUZZ can generate
precise input seeds that target specific design regions more effectively while
maintaining high fuzzing throughput. Our experiments show that PROFUZZ scales
30x better than DirectFuzz when handling multiple target sites, improves
coverage by 11.66%, and runs 2.76x faster, highlighting its scalability and
effectiveness for directed fuzzing in complex hardware systems.

</details>


### [21] [Security-aware Semantic-driven ISAC via Paired Adversarial Residual Networks](https://arxiv.org/abs/2509.20835)
*Yu Liu,Boxiang He,Fanggang Wang*

Main category: cs.CR

TL;DR: The paper introduces security semantic ISAC (SS-ISAC), a framework integrating sensing and communication with security. It uses adversarial residual networks (ARNs)-based encryption/decryption modules to enhance security without hardware changes, validated by simulations.


<details>
  <summary>Details</summary>
Motivation: Existing ISAC systems lack integrated security solutions to prevent eavesdropping while maintaining performance. Current methods either compromise security or require expensive hardware modifications. Adversarial attacks inspire innovate security mechanisms.

Method: Designs a pluggable encryption module (after semantic transmitter with ARN to generate adversarial noise) and decryption module (before receiver with ARN to neutralize noise). Two ARNs are jointly trained via a multi-objective loss function balancing SAC performance, adversarial strength, and privacy risk.

Result: Simulations demonstrate SS-ISAC achieves competitive sensing/communication performance while effectively mitigating eavesdropping threats compared to baseline methods.

Conclusion: SS-ISAC provides a flexible, hardware-friendly security framework for future ISAC systems. The adversarial-driven design adaptively adjusts security levels while maintaining core performance metrics.

Abstract: This paper proposes a novel and flexible security-aware semantic-driven
integrated sensing and communication (ISAC) framework, namely security semantic
ISAC (SS-ISAC). Inspired by the positive impact of the adversarial attack, a
pair of pluggable encryption and decryption modules is designed in the proposed
SS-ISAC framework. The encryption module is installed after the semantic
transmitter, adopting a trainable adversarial residual network (ARN) to create
the adversarial attack. Correspondingly, the decryption module before the
semantic receiver utilizes another trainable ARN to mitigate the adversarial
attack and noise. These two modules can be flexibly assembled considering the
system security demands, without drastically modifying the hardware
infrastructure. To ensure the sensing and communication (SAC) performance while
preventing the eavesdropping threat, the above ARNs are jointly optimized by
minimizing a carefully designed loss function that relates to the adversarial
attack power, SAC performance, as well as the privacy leakage risk. Simulation
results validate the effectiveness of the proposed SS-ISAC framework in terms
of both SAC and eavesdropping prevention performance.

</details>


### [22] [FlowXpert: Context-Aware Flow Embedding for Enhanced Traffic Detection in IoT Network](https://arxiv.org/abs/2509.20861)
*Chao Zha,Haolin Pan,Bing Bai,Jiangxing Wu,Ruyun Zhang*

Main category: cs.CR

TL;DR: The paper addresses limitations of traditional ML-based IoT network traffic detection by proposing (1) a context-aware semantic feature extraction tool eliminating time/length features, and (2) a DBSCAN-contrastive learning framework for fine-grained representations, validated on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Existing feature extraction tools (e.g., CICMeterFlow) suffer from high sparsity due to time/length features, while current methods lack effective embedding mechanisms to capture traffic semantics.

Method: Develops a feature extraction tool with host-context semantic features, and an embedding framework combining unsupervised DBSCAN clustering with contrastive learning to capture traffic semantics.

Result: Superior detection accuracy, robustness, and generalization demonstrated on Mawi dataset against SOTA models, with real-time deployability confirmed.

Conclusion: Proposed method resolves feature sparsity and semantic embedding limitations in IoT traffic detection, offering an effective and deployable solution for dynamic network environments.

Abstract: In the Internet of Things (IoT) environment, continuous interaction among a
large number of devices generates complex and dynamic network traffic, which
poses significant challenges to rule-based detection approaches. Machine
learning (ML)-based traffic detection technology, capable of identifying
anomalous patterns and potential threats within this traffic, serves as a
critical component in ensuring network security. This study first identifies a
significant issue with widely adopted feature extraction tools (e.g.,
CICMeterFlow): the extensive use of time- and length-related features leads to
high sparsity, which adversely affects model convergence. Furthermore, existing
traffic detection methods generally lack an embedding mechanism capable of
efficiently and comprehensively capturing the semantic characteristics of
network traffic. To address these challenges, we propose a novel feature
extraction tool that eliminates traditional time and length features in favor
of context-aware semantic features related to the source host, thus improving
the generalizability of the model. In addition, we design an embedding training
framework that integrates the unsupervised DBSCAN clustering algorithm with a
contrastive learning strategy to effectively capture fine-grained semantic
representations of traffic. Extensive empirical evaluations are conducted on
the real-world Mawi data set to validate the proposed method in terms of
detection accuracy, robustness, and generalization. Comparative experiments
against several state-of-the-art (SOTA) models demonstrate the superior
performance of our approach. Furthermore, we confirm its applicability and
deployability in real-time scenarios.

</details>


### [23] [A Generalized $χ_n$-Function](https://arxiv.org/abs/2509.20880)
*Cheng Lyu,Mu Yuan,Dabin Zheng,Siwei Sun,Shun Li*

Main category: cs.CR

TL;DR: The paper explores a generalized version of the χₙ mapping (χₙ,m) to enable bijective applications in even-dimensional spaces over 𝔽₂, providing a critical analysis of cryptographic properties and comparisons with existing methods like χχₙ.


<details>
  <summary>Details</summary>
Motivation: The original χₙ mapping is limited to odd dimensions, hindering its applicability in cryptography. This work generalizes it to χₙ,m for even and odd n, expanding its potential use cases in lightweight cryptographic algorithms.

Method: The authors generalize χₙ to χₙ,m and further to θₘ,ₖ by defining new mappings with modular indices and products over ranges avoiding multiples of m. They prove that these mappings form an abelian group isomorphic to the unit group of a specific quotient ring and analyze their algebraic properties.

Result: The key result is the construction of permutations over 𝔽₂ⁿ for any n using the generalized mappings, along with a comprehensive database of cryptographic properties for small n and m. The authors also prove Conjecture~1 from \

Conclusion: The paper concludes that the generalized χₙ,m and θₘ,ₖ mappings offer a versatile framework for constructing permutations in both even and odd dimensions over 𝔽₂, surpassing limitations of prior work and providing a foundational step for future cryptographic research.

Abstract: The mapping $\chi_n$ from $\F_{2}^{n}$ to itself defined by $y=\chi_n(x)$
with $y_i=x_i+x_{i+2}(1+x_{i+1})$, where the indices are computed modulo $n$,
has been widely studied for its applications in lightweight cryptography.
However, $\chi_n $ is bijective on $\F_2^n$ only when $n$ is odd, restricting
its use to odd-dimensional vector spaces over $\F_2$. To address this
limitation, we introduce and analyze the generalized mapping $\chi_{n, m}$
defined by $y=\chi_{n,m}(x)$ with $y_i=x_i+x_{i+m} (x_{i+m-1}+1)(x_{i+m-2}+1)
\cdots (x_{i+1}+1)$, where $m$ is a fixed integer with $m\nmid n$. To
investigate such mappings, we further generalize $\chi_{n,m}$ to $\theta_{m,
k}$, where $\theta_{m, k}$ is given by $y_i=x_{i+mk} \prod_{\substack{j=1,\,\,
m \nmid j}}^{mk-1} \left(x_{i+j}+1\right), \,\,{\rm for }\,\, i\in
\{0,1,\ldots,n-1\}$. We prove that these mappings generate an abelian group
isomorphic to the group of units in $\F_2[z]/(z^{\lfloor n/m\rfloor +1})$. This
structural insight enables us to construct a broad class of permutations over
$\F_2^n$ for any positive integer $n$, along with their inverses. We rigorously
analyze algebraic properties of these mappings, including their iterations,
fixed points, and cycle structures. Additionally, we provide a comprehensive
database of the cryptographic properties for iterates of $\chi_{n,m}$ for small
values of $n$ and $m$. Finally, we conduct a comparative security and
implementation cost analysis among $\chi_{n,m}$, $\chi_n$, $\chi\chi_n$
(EUROCRYPT 2025 \cite{belkheyar2025chi}) and their variants, and prove
Conjecture~1 proposed in~\cite{belkheyar2025chi} as a by-product of our study.
Our results lead to generalizations of $\chi_n$, providing alternatives to
$\chi_n$ and $\chi\chi_n$.

</details>


### [24] [Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools](https://arxiv.org/abs/2509.21011)
*Ping He,Changjiang Li,Binbin Zhao,Tianyu Du,Shouling Ji*

Main category: cs.CR

TL;DR: AutoMalTool automates malicious tool generation for systematic red teaming of LLM agents via MCP protocol, uncovering new security risks through evasion-of-detection attacks.


<details>
  <summary>Details</summary>
Motivation: Existing red teaming methods for MCP tool poisoning remain at proof-of-concept levels, leaving a critical gap in automated and systematic security evaluation of LLM-based agents against these attacks.

Method: The framework employs automated generation of malicious MCP tools to systematically attack and manipulate mainstream LLM-based agents while bypassing detection systems.

Result: Empirical evaluations demonstrate AutoMalTool's effectiveness in generating evasive malicious tools that successfully compromise popular LLM agents while avoiding detection, highlighting previously unseen security weaknesses.

Conclusion: AutoMalTool bridges the gap in automatic red teaming for LLM-based agents under MCP tool poisoning, revealing critical security vulnerabilities in their current implementations.

Abstract: The remarkable capability of large language models (LLMs) has led to the wide
application of LLM-based agents in various domains. To standardize interactions
between LLM-based agents and their environments, model context protocol (MCP)
tools have become the de facto standard and are now widely integrated into
these agents. However, the incorporation of MCP tools introduces the risk of
tool poisoning attacks, which can manipulate the behavior of LLM-based agents.
Although previous studies have identified such vulnerabilities, their red
teaming approaches have largely remained at the proof-of-concept stage, leaving
the automatic and systematic red teaming of LLM-based agents under the MCP tool
poisoning paradigm an open question. To bridge this gap, we propose
AutoMalTool, an automated red teaming framework for LLM-based agents by
generating malicious MCP tools. Our extensive evaluation shows that AutoMalTool
effectively generates malicious MCP tools capable of manipulating the behavior
of mainstream LLM-based agents while evading current detection mechanisms,
thereby revealing new security risks in these agents.

</details>


### [25] [RLCracker: Exposing the Vulnerability of LLM Watermarks with Adaptive RL Attacks](https://arxiv.org/abs/2509.20924)
*Hanbo Huang,Yiran Zhang,Hao Zheng,Xuan Gong,Yihan Li,Lin Liu,Shiyu Liang*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) watermarking has shown promise in detecting
AI-generated content and mitigating misuse, with prior work claiming robustness
against paraphrasing and text editing. In this paper, we argue that existing
evaluations are not sufficiently adversarial, obscuring critical
vulnerabilities and overstating the security. To address this, we introduce
adaptive robustness radius, a formal metric that quantifies watermark
resilience against adaptive adversaries. We theoretically prove that optimizing
the attack context and model parameters can substantially reduce this radius,
making watermarks highly susceptible to paraphrase attacks. Leveraging this
insight, we propose RLCracker, a reinforcement learning (RL)-based adaptive
attack that erases watermarks while preserving semantic fidelity. RLCracker
requires only limited watermarked examples and zero access to the detector.
Despite weak supervision, it empowers a 3B model to achieve 98.5% removal
success and an average 0.92 P-SP score on 1,500-token Unigram-marked texts
after training on only 100 short samples. This performance dramatically exceeds
6.75% by GPT-4o and generalizes across five model sizes over ten watermarking
schemes. Our results confirm that adaptive attacks are broadly effective and
pose a fundamental threat to current watermarking defenses.

</details>


### [26] [CTI Dataset Construction from Telegram](https://arxiv.org/abs/2509.20943)
*Dincy R. Arikkat,Sneha B. T.,Serena Nicolazzo,Antonino Nocera,Vinod P.,Rafidha Rehiman K. A.,Karthika R*

Main category: cs.CR

TL;DR: This paper introduces a Telegram-based CTI pipeline that automates collection, classification, and extraction to generate a 86,509-IoC dataset, enabling more effective cyber threat detection.


<details>
  <summary>Details</summary>
Motivation: High-quality CTI datasets are critical for combating evolving cyber threats, and Telegram offers timely/diverse threat information that outpaces traditional sources.

Method: An end-to-end automated pipeline systematically collects Telegram messages, filters relevant content via a BERT-based classifier, and extracts malicious Indicators of Compromise (IoCs).

Result: Achieved 96.64% classification accuracy, scraped 145,349 messages (yielding 86,509 IoCs including domains/IPs/URLs/hashes/CVEs) from 12 curated Telegram channels.

Conclusion: The proposed pipeline successfully creates a large-scale CTI dataset from Telegram, providing a robust foundation for future cyber threat detection research and applications.

Abstract: Cyber Threat Intelligence (CTI) enables organizations to anticipate, detect,
and mitigate evolving cyber threats. Its effectiveness depends on high-quality
datasets, which support model development, training, evaluation, and
benchmarking. Building such datasets is crucial, as attack vectors and
adversary tactics continually evolve. Recently, Telegram has gained prominence
as a valuable CTI source, offering timely and diverse threat-related
information that can help address these challenges. In this work, we address
these challenges by presenting an end-to-end automated pipeline that
systematically collects and filters threat-related content from Telegram. The
pipeline identifies relevant Telegram channels and scrapes 145,349 messages
from 12 curated channels out of 150 identified sources. To accurately filter
threat intelligence messages from generic content, we employ a BERT-based
classifier, achieving an accuracy of 96.64%. From the filtered messages, we
compile a dataset of 86,509 malicious Indicators of Compromise, including
domains, IPs, URLs, hashes, and CVEs. This approach not only produces a
large-scale, high-fidelity CTI dataset but also establishes a foundation for
future research and operational applications in cyber threat detection.

</details>


### [27] [Dual-Path Phishing Detection: Integrating Transformer-Based NLP with Structural URL Analysis](https://arxiv.org/abs/2509.20972)
*Ibrahim Altan,Abdulla Bachir,Yousuf Parbhulkar,Abdul Muksith Rizvi,Moshiur Farazi*

Main category: cs.CR

TL;DR: A dual-path phishing detection framework combines transformer-based NLP and classical machine learning to improve email security by analyzing text and URLs synergistically.


<details>
  <summary>Details</summary>
Motivation: Traditional phishing detection methods analyze email content or URLs in isolation, failing to address the dual semantic and structural complexity of modern phishing attacks.

Method: The framework integrates fine-tuned DistilBERT for semantic text analysis with character-level TF-IDF vectorization and Random Forest for URL pattern analysis, enabling joint threat detection.

Result: Empirical tests show DistilBERT achieves optimal text classification efficiency, Random Forest outperforms other URL classifiers, and the combined system improves overall detection accuracy compared to single-path approaches.

Conclusion: This dual-path architecture provides a scalable, interpretable solution that balances performance and practicality, establishing a robust defense against evolving phishing techniques.

Abstract: Phishing emails pose a persistent and increasingly sophisticated threat,
undermining email security through deceptive tactics designed to exploit both
semantic and structural vulnerabilities. Traditional detection methods, often
based on isolated analysis of email content or embedded URLs, fail to
comprehensively address these evolving attacks. In this paper, we propose a
dual-path phishing detection framework that integrates transformer-based
natural language processing (NLP) with classical machine learning to jointly
analyze email text and embedded URLs. Our approach leverages the complementary
strengths of semantic analysis using fine-tuned transformer architectures
(e.g., DistilBERT) and structural link analysis via character-level TF-IDF
vectorization paired with classical classifiers (e.g., Random Forest).
Empirical evaluation on representative email and URL datasets demonstrates that
this combined approach significantly improves detection accuracy. Specifically,
the DistilBERT model achieves a near-optimal balance between accuracy and
computational efficiency for textual phishing detection, while Random Forest
notably outperforms other classical classifiers in identifying malicious URLs.
The modular design allows flexibility for standalone deployment or ensemble
integration, facilitating real-world adoption. Collectively, our results
highlight the efficacy and practical value of this dual-path approach,
establishing a scalable, accurate, and interpretable solution capable of
enhancing email security against contemporary phishing threats.

</details>


### [28] [PMark: Towards Robust and Distortion-free Semantic-level Watermarking with Channel Constraints](https://arxiv.org/abs/2509.21057)
*Jiahao Huo,Shuliang Liu,Bin Wang,Junyan Zhang,Yibo Yan,Aiwei Liu,Xuming Hu,Mingxun Zhou*

Main category: cs.CR

TL;DR: Proposes PMark: proxy function-based SWM with theoretical robustness and reduced distortion, enabling better machine-generated text detection.


<details>
  <summary>Details</summary>
Motivation: Existing SWM methods lack strong robustness guarantees and suffer from distribution distortions via reject-sampling; needs improved paraphrasing attack resistance.

Method: Introduces PMark via proxy functions (PFs) for dynamic median estimation of sentence scalars, using multiple channels to strengthen watermarks and an optimized version for sampling efficiency.

Result: Outperforms SWM baselines in text quality/robustness, achieves distortion-free generation with theoretical validity.

Conclusion: PMark offers a more effective paradigm for detecting machine-generated text with theoretical guarantees and improved robustness.

Abstract: Semantic-level watermarking (SWM) for large language models (LLMs) enhances
watermarking robustness against text modifications and paraphrasing attacks by
treating the sentence as the fundamental unit. However, existing methods still
lack strong theoretical guarantees of robustness, and reject-sampling-based
generation often introduces significant distribution distortions compared with
unwatermarked outputs. In this work, we introduce a new theoretical framework
on SWM through the concept of proxy functions (PFs) $\unicode{x2013}$ functions
that map sentences to scalar values. Building on this framework, we propose
PMark, a simple yet powerful SWM method that estimates the PF median for the
next sentence dynamically through sampling while enforcing multiple PF
constraints (which we call channels) to strengthen watermark evidence. Equipped
with solid theoretical guarantees, PMark achieves the desired distortion-free
property and improves the robustness against paraphrasing-style attacks. We
also provide an empirically optimized version that further removes the
requirement for dynamical median estimation for better sampling efficiency.
Experimental results show that PMark consistently outperforms existing SWM
baselines in both text quality and robustness, offering a more effective
paradigm for detecting machine-generated text. Our code will be released at
[this URL](https://github.com/PMark-repo/PMark).

</details>


### [29] [Emerging Paradigms for Securing Federated Learning Systems](https://arxiv.org/abs/2509.21147)
*Amr Akmal Abouelmagd,Amr Hilal*

Main category: cs.CR

TL;DR: This survey reviews emerging solutions (TEEs, QC, etc.) to overcome limitations of traditional privacy methods in FL, evaluates their trade-offs, and proposes a research roadmap for robust FL systems.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the inefficiency and scalability issues of current FL privacy techniques like MPC, HE, and DP, prompting a need for innovative solutions to enhance privacy and efficiency.

Method: The paper conducts a survey of emerging approaches such as TEEs, PUFs, QC, CBE, NC, and SI, analyzing their relevance, strengths, and limitations within the FL pipeline.

Result: The analysis provides a comprehensive evaluation of new paradigms in FL security, identifying trade-offs, practical barriers, and opportunities for advancing secure FL implementations.

Conclusion: The paper concludes by underscoring unresolved challenges in privacy-preserving FL and outlines a roadmap for future research, emphasizing secure and scalable FL systems through emerging technologies.

Abstract: Federated Learning (FL) facilitates collaborative model training while
keeping raw data decentralized, making it a conduit for leveraging the power of
IoT devices while maintaining privacy of the locally collected data. However,
existing privacy- preserving techniques present notable hurdles. Methods such
as Multi-Party Computation (MPC), Homomorphic Encryption (HE), and Differential
Privacy (DP) often incur high compu- tational costs and suffer from limited
scalability. This survey examines emerging approaches that hold promise for
enhancing both privacy and efficiency in FL, including Trusted Execution
Environments (TEEs), Physical Unclonable Functions (PUFs), Quantum Computing
(QC), Chaos-Based Encryption (CBE), Neuromorphic Computing (NC), and Swarm
Intelligence (SI). For each paradigm, we assess its relevance to the FL
pipeline, outlining its strengths, limitations, and practical considerations.
We conclude by highlighting open challenges and prospective research avenues,
offering a detailed roadmap for advancing secure and scalable FL systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [30] [ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation](https://arxiv.org/abs/2509.20380)
*Samyak Jhaveri,Vanessa Klotzmann,Crista Lopes*

Main category: cs.SE

TL;DR: ACCeLLiuM is a fine-tuned LLM that automates expert-level OpenACC directive generation, matching 50% of cases exactly and validly 87% of the time, enabling easier GPU programming through dataset and model sharing.


<details>
  <summary>Details</summary>
Motivation: Existing parallel programming frameworks like OpenACC require significant expertise, and while they simplify GPU programming, there is a need for tools to automate directive generation for data-parallel loops.

Method: The authors fine-tuned open-weight LLMs using a supervised dataset (ACCeLLiuM SFT) composed of 4,033 OpenACC pragma-loop pairs mined from GitHub. The dataset was split into 3,223 training and 810 testing samples.

Result: ACCeLLiuM models achieved 87% accuracy in generating valid pragmas with correct directive types, and 50% exact matching (including clauses and variables). Generated pragmas often included useful clause variations even when not exact.

Conclusion: The study concludes that ACCeLLiuM provides a reproducible benchmark for automating OpenACC pragmas, significantly improves over base LLMs in generating valid directives, and reduces barriers to GPU programming through model and dataset release.

Abstract: The increasing ubiquity of GPUs is accompanied by the increasing complexity
of their hardware and parallel programming frameworks. Directive-based parallel
programming standards like OpenACC simplify GPU programming to some extent by
abstracting away low-level complexities, but a fair amount of expertise is
still required in order to use those directives effectively.
  We introduce ACCeLLiuM, two open weights Large Language Models specifically
fine-tuned for generating expert OpenACC directives for data-parallel loops,
along with the supervised fine-tuning dataset that was used to train them. The
ACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from
public GitHub C/C++ repositories, with 3,223 pairs for training and 810 for
testing. Experimental evaluations show a pronounced performance gap in
generating correct OpenACC pragmas between base LLMs and our fine-tuned
versions. On the held-out test set, base LLMs fail to consistently generate
valid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid
pragmas with the correct directive type for $87\%$ of the data-parallel loops,
and exact pragmas--including directives, clauses, clause order, and clause
variables--for $50\%$ of the cases. Even when not exact, generated pragmas
frequently incorporate the correct clauses in a different order than the
ground-truth label, or include additional clauses that enable finer control
over parallel execution, data movement, and concurrency, offering practical
value beyond strict string-matching. By publicly releasing the code, models,
and dataset as ACCeLLiuM we hope to establish a reproducible benchmark for
LLM-powered OpenACC pragma generation, and lower the barrier to automated GPU
offloading of serially written programs.

</details>


### [31] [State-of-the-Art in Software Security Visualization: A Systematic Review](https://arxiv.org/abs/2509.20385)
*Ishara Devendra,Chaman Wijesiriwardana,Prasad Wimalaratne*

Main category: cs.SE

TL;DR: This paper reviews and categorizes software security visualization techniques into a taxonomy, highlighting key issues, advancements, and future research directions in the field.


<details>
  <summary>Details</summary>
Motivation: As software systems become more complex and cybersecurity threats evolve, traditional text-based methods for analyzing security data are no longer sufficient. There is a need for effective visualization techniques to make complex security information more understandable and actionable.

Method: The authors conduct a systematic review of over 60 recent research papers on software security visualization, analyzing the literature to identify and categorize techniques into four types: graph-based, notation-based, matrix-based, and metaphor-based visualization.

Result: The review results in a comprehensive taxonomy of software security visualization techniques. It highlights two main areas: extensive software development visualization (focused on software architecture) and operational security/cybersecurity visualization. The paper emphasizes the importance of innovative visualization approaches to address current and future security challenges.

Conclusion: Software security visualization is a critical area of research given the growing complexity of systems and evolving threats. The proposed taxonomy provides a framework for understanding existing techniques and guides future work in creating more effective visual solutions to enhance threat detection and response strategies.

Abstract: Software security visualization is an interdisciplinary field that combines
the technical complexity of cybersecurity, including threat intelligence and
compliance monitoring, with visual analytics, transforming complex security
data into easily digestible visual formats. As software systems get more
complex and the threat landscape evolves, traditional text-based and numerical
methods for analyzing and interpreting security concerns become increasingly
ineffective. The purpose of this paper is to systematically review existing
research and create a comprehensive taxonomy of software security visualization
techniques through literature, categorizing these techniques into four types:
graph-based, notation-based, matrix-based, and metaphor-based visualization.
This systematic review explores over 60 recent key research papers in software
security visualization, highlighting its key issues, recent advancements, and
prospective future research directions. From the comprehensive analysis, the
two main areas were distinctly highlighted as extensive software development
visualization, focusing on advanced methods for depicting software
architecture: operational security visualization and cybersecurity
visualization. The findings highlight the necessity for innovative
visualization techniques that adapt to the evolving security landscape, with
practical implications for enhancing threat detection, improving security
response strategies, and guiding future research.

</details>


### [32] [Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments](https://arxiv.org/abs/2509.20386)
*Nishant Gaurav,Adit Akarsh,Ankit Ranjan,Manoj Bajaj*

Main category: cs.SE

TL;DR: Dynamic ReAct enables efficient tool selection for AI agents in large-scale environments, reducing computational overhead without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: Large language models face computational infeasibility when handling hundreds/thousands of tools due to contextual memory limitations, necessitating efficient tool selection strategies.

Method: Five progressively refined architectures are proposed, culminating in a search-and-load mechanism to optimize tool selection from extensive MCP tool sets.

Result: Achieved up to 50% reduction in tool loading while maintaining task completion accuracy through the proposed search-and-load mechanism.

Conclusion: Dynamic ReAct advances the development of general-purpose AI agents by enabling dynamic adaptation to diverse task environments through efficient tool selection.

Abstract: We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef-
ficiently operate with extensive Model Control Protocol (MCP) tool sets that
exceed the contextual memory limitations of large language models. Our approach
addresses the fundamental challenge of tool selection in environments
containing hundreds or thousands of available tools, where loading all tools
simultaneously is computationally infeasible. We propose and evaluate five
distinct architectures that progressively refine the tool selection process,
culminating in a search-and-load mechanism that achieves intelligent tool
selection with minimal computational overhead. Our experimental results
demonstrate that the proposed approach reduces tool loading by up to 50% while
maintaining task completion accuracy, advancing the path towards truly
general-purpose AI agents capable of dynamically adapting to diverse task
environments.

</details>


### [33] [Towards Systematic Specification and Verification of Fairness Requirements: A Position Paper](https://arxiv.org/abs/2509.20387)
*Qusai Ramadan,Jukka Ruohonen,Abhishek Tiwari,Adam Alami,Zeyd Boukhers*

Main category: cs.SE

TL;DR: This paper identifies gaps in fairness requirement specification, proposes a knowledge graph framework to formalize fairness, and presents a roadmap for addressing these challenges in software systems.


<details>
  <summary>Details</summary>
Motivation: Existing studies overlook the critical role of insufficiently specified fairness requirements and their verification, compounded by the implicit nature of experts' fairness knowledge, leading to potential discrimination in software systems.

Method: The paper introduces a knowledge graph-based framework inspired by security engineering practices, aiming to formalize implicit expert knowledge about fairness into verifiable requirements.

Result: The paper outlines challenges, research questions, and a roadmap for developing the proposed framework, establishing a foundation for future work on formal fairness requirements.

Conclusion: The proposed knowledge graph-based framework offers a structured approach to formalize and verify fairness requirements, addressing the lack of explicit mechanisms in current software systems to prevent discrimination.

Abstract: Decisions suggested by improperly designed software systems might be prone to
discriminate against people based on protected characteristics, such as gender
and ethnicity. Previous studies attribute such undesired behavior to flaws in
algorithmic design or biased data. However, these studies ignore that
discrimination is often the result of a lack of well-specified fairness
requirements and their verification. The fact that experts' knowledge about
fairness is often implicit makes the task of specifying precise and verifiable
fairness requirements difficult. In related domains, such as security
engineering, knowledge graphs have been proven to be effective in formalizing
knowledge to assist requirements specification and verification. To address the
lack of formal mechanisms for specifying and verifying fairness requirements,
we propose the development of a knowledge graph-based framework for fairness.
In this paper, we discuss the challenges, research questions, and a road map
towards addressing the research questions.

</details>


### [34] [Online-Optimized RAG for Tool Use and Function Calling](https://arxiv.org/abs/2509.20415)
*Yu Pan,Xiaocheng Li,Hanzhao Wang*

Main category: cs.SE

TL;DR: A framework for RAG systems that continuously adapts query embeddings during deployment using minimal feedback to improve retrieval accuracy and task success.


<details>
  <summary>Details</summary>
Motivation: Current RAG systems often fail due to embedding misalignment caused by imperfect models or noisy descriptions, leading to incorrect function/tool retrieval. The paper addresses this gap by proposing a real-time adaptation framework.

Method: The method employs lightweight online gradient updates to adjust retrieval embeddings using minimal feedback (e.g., task success) from live interactions. It is designed to be plug-and-play, supporting various retrieval configurations and requiring no modifications to the underlying LLM.

Result: Online-Optimized RAG consistently improves tool selection accuracy and end-task success across diverse scenarios, demonstrating robustness and scalability for practical applications.

Conclusion: The study introduces Online-Optimized RAG, which provides a simple and practical framework for self-improving RAG systems by continuously optimizing embeddings during deployment while maintaining compatibility with existing LLMs.

Abstract: In many applications, retrieval-augmented generation (RAG) drives tool use
and function calling by embedding the (user) queries and matching them to
pre-specified tool/function descriptions. In this paper, we address an
embedding misalignment issue that often arises in practical applications due to
imperfect embedding models or noisy descriptions; such misalignment may lead to
incorrect retrieval and task failure. We introduce Online-Optimized RAG, a
deployment-time framework that continually adapts retrieval embeddings from
live interactions using minimal feedback (e.g., task success). Online-Optimized
RAG applies lightweight online gradient updates with negligible per-query
latency and requires no changes to the underlying LLM. The method is
plug-and-play: it supports both single- and multi-hop tool use, dynamic tool
inventories, and $K$-retrieval with re-ranking. We provide a problem-dependent
theoretical analysis that quantifies how the method's performance depends on
the initialization quality of the embeddings and other related quantities.
Across diverse tool-use and document-retrieval scenarios, our Online-Optimized
RAG consistently improves tool selection accuracy and end-task success, thus
providing a simple, practical path to robust, self-improving RAG systems.

</details>


### [35] [Formal Verification of Legal Contracts: A Translation-based Approach](https://arxiv.org/abs/2509.20421)
*Reiner Hähnle,Cosimo Laneve,Adele Veschetti*

Main category: cs.SE

TL;DR: Stipula is a domain-specific language for legal contracts, with a methodology to verify their correctness by translating them into Java with JML annotations and using the KeY verification tool. The approach is fully automatic for contracts with disjoint cycles.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for formal verification of legal contracts involving asset transfers and obligations, ensuring their correctness and enforceable properties.

Method: The authors present a translation methodology that converts Stipula contracts into Java code annotated with Java Modeling Language (JML) specifications. They use the deductive verification tool KeY to verify partial and total correctness, focusing on contracts with disjoint cycles.

Result: The methodology achieves fully automatic verification of a large subset of Stipula contracts (those with disjoint cycles), demonstrating the effectiveness of using general-purpose deductive verification tools via translation.

Conclusion: The work demonstrates that general-purpose deductive verification tools like KeY can be successfully applied to domain-specific languages such as Stipula through translation, enabling reliable verification of legal contracts.

Abstract: Stipula is a domain-specific programming language designed to model legal
contracts with enforceable properties, especially those involving asset
transfers and obligations. This paper presents a methodology to formally verify
the correctness of Stipula contracts through translation into Java code
annotated with Java Modeling Language specifications. As a verification
backend, the deductive verification tool KeY is used. Both, the translation and
the verification of partial and total correctness for a large subset of Stipula
contracts, those with disjoint cycles, is fully automatic. Our work
demonstrates that a general-purpose deductive verification tool can be used
successfully in a translation approach.

</details>


### [36] [AI-Specific Code Smells: From Specification to Detection](https://arxiv.org/abs/2509.20491)
*Brahim Mahmoudi,Naouel Moha,Quentin Stievenert,Florent Avellaneda*

Main category: cs.SE

TL;DR: This paper introduces SpecDetect4AI, a DSL-based static analysis tool detecting 22 AI-specific code smells with 88% precision/recall, validated on 20M lines of code across 826 systems, showcasing scalable AI code quality analysis.


<details>
  <summary>Details</summary>
Motivation: Traditional tools miss AI-specific code smells that impact reproducibility, failure detection, and model generalization in modern AI systems.

Method: The approach combines a DSL for specifying AI-specific code smells with an extensible static analysis tool, validated through 22 smell patterns and large-scale evaluation on 826 AI systems (20M lines of code).

Result: Achieves 88.66% precision and 88.89% recall, outperforming existing tools, with SUS score 81.7/100 demonstrating usability and extensibility for large-scale systems.

Conclusion: SpecDetect4AI effectively detects AI-specific code smells with high precision and recall, demonstrating efficiency, extensibility, and scalability for large-scale AI systems.

Abstract: The rise of Artificial Intelligence (AI) is reshaping how software systems
are developed and maintained. However, AI-based systems give rise to new
software issues that existing detection tools often miss. Among these, we focus
on AI-specific code smells, recurring patterns in the code that may indicate
deeper problems such as unreproducibility, silent failures, or poor model
generalization. We introduce SpecDetect4AI, a tool-based approach for the
specification and detection of these code smells at scale. This approach
combines a high-level declarative Domain-Specific Language (DSL) for rule
specification with an extensible static analysis tool that interprets and
detects these rules for AI-based systems. We specified 22 AI-specific code
smells and evaluated SpecDetect4AI on 826 AI-based systems (20M lines of code),
achieving a precision of 88.66% and a recall of 88.89%, outperforming other
existing detection tools. Our results show that SpecDetect4AI supports the
specification and detection of AI-specific code smells through dedicated rules
and can effectively analyze large AI-based systems, demonstrating both
efficiency and extensibility (SUS 81.7/100).

</details>


### [37] [PromptDebt: A Comprehensive Study of Technical Debt Across LLM Projects](https://arxiv.org/abs/2509.20497)
*Ahmed Aljohani,Hyunsook Do*

Main category: cs.SE

TL;DR: This paper presents a large-scale study on LLM-specific SATD, analyzing 93,142 Python files. It quantifies SATD origins (OpenAI, LangChain), and mitigation strategies, and releases a dataset for reproducibility.


<details>
  <summary>Details</summary>
Motivation: Despite the benefits of using LLMs via APIs, their integration introduces SATD, necessitating an understanding of its characteristics and management strategies.

Method: The authors conducted a large-scale empirical study analyzing 93,142 Python files across major LLM APIs, examining the prevalence, origins, and mitigation of LLM-specific SATD.

Result: 54.49% of SATD instances originated from OpenAI, 12.35% from LangChain, with prompt design being the primary source. Instruction-based and few-shot prompts were found particularly prone to attracting debt.

Conclusion: The paper contributes a comprehensive analysis of LLM-specific SATD and provides a dataset for future research, highlighting the need for systematic prompt design and optimization strategies.

Abstract: Large Language Models (LLMs) are increasingly embedded in software via APIs
like OpenAI, offering powerful AI features without heavy infrastructure. Yet
these integrations bring their own form of self-admitted technical debt (SATD).
In this paper, we present the first large-scale empirical study of LLM-specific
SATD: its origins, prevalence, and mitigation strategies. By analyzing 93,142
Python files across major LLM APIs, we found that 54.49% of SATD instances stem
from OpenAI integrations and 12.35% from LangChain use. Prompt design emerged
as the primary source of LLM-specific SATD, with 6.61% of debt related to
prompt configuration and optimization issues, followed by hyperparameter tuning
and LLM-framework integration. We further explored which prompt techniques
attract the most debt, revealing that instruction-based prompts (38.60%) and
few-shot prompts (18.13%) are particularly vulnerable due to their dependence
on instruction clarity and example quality. Finally, we release a comprehensive
SATD dataset to support reproducibility and offer practical guidance for
managing technical debt in LLM-powered systems.

</details>


### [38] [Enhancing Python Programming Education with an AI-Powered Code Helper: Design, Implementation, and Impact](https://arxiv.org/abs/2509.20518)
*Sayed Mahbub Hasan Amiri,Md Mainul Islam*

Main category: cs.SE

TL;DR: An AI-Python chatbot combining static/dynamic analysis and LLMs improves programming education with 85\% error resolution and 34\@ coding proficiency gains.


<details>
  <summary>Details</summary>
Motivation: Traditional tools lack guided support, while AI assistants prioritize completion over learning; this addresses pedagogical gaps in coding education.

Method: Hybrid architecture using CodeLlama for code embedding, GPT-4 for dialogue, Docker sandboxing, and evaluates 1,500 submissions through mixed-methods analysis.

Result: 85\@ error resolution success (vs. 62\@ pylint\@, 73\@ GPT-4), 59.3\@ debugging time reduction, 34\@ coding proficiency improvement, and positive qualitative feedback with latency critiques.

Conclusion: The chatbot demonstrates AI's potential to enhance educational equity through pedagogically focused design, balancing technical innovation with skill retention.

Abstract: This is the study that presents an AI-Python-based chatbot that helps
students to learn programming by demonstrating solutions to such problems as
debugging errors, solving syntax problems or converting abstract theoretical
concepts to practical implementations. Traditional coding tools like Integrated
Development Environments (IDEs) and static analyzers do not give robotic help
while AI-driven code assistants such as GitHub Copilot focus on getting things
done. To close this gap, our chatbot combines static code analysis, dynamic
execution tracing, and large language models (LLMs) to provide the students
with relevant and practical advice, hence promoting the learning process. The
chatbots hybrid architecture employs CodeLlama for code embedding, GPT-4 for
natural language interactions, and Docker-based sandboxing for secure
execution. Evaluated through a mixed-methods approach involving 1,500 student
submissions, the system demonstrated an 85% error resolution success rate,
outperforming standalone tools like pylint (62%) and GPT-4 (73%). Quantitative
results revealed a 59.3% reduction in debugging time among users, with pre- and
post-test assessments showing a 34% improvement in coding proficiency,
particularly in recursion and exception handling. Qualitative feedback from 120
students highlighted the chatbots clarity, accessibility, and
confidence-building impact, though critiques included occasional latency and
restrictive code sanitization. By balancing technical innovation with
pedagogical empathy, this research provides a blueprint for AI tools that
prioritize educational equity and long-term skill retention over mere code
completion. The chatbot exemplifies how AI can augment human instruction,
fostering deeper conceptual understanding in programming education.

</details>


### [39] [Enhancing LLM-based Fault Localization with a Functionality-Aware Retrieval-Augmented Generation Framework](https://arxiv.org/abs/2509.20552)
*Xinyu Shi,Zhenhao Li,An Ran Chen*

Main category: cs.SE

TL;DR: Introducing FaR-Loc, a framework enhancing fault localization in software debugging by integrating large language models with retrieval-augmented generation, achieving improvements up to 49.0% in Top-1 accuracy over existing methods.


<details>
  <summary>Details</summary>
Motivation: Fault localization is challenging and time-consuming for complex systems where existing large language models may lack the necessary project-specific knowledge and navigation capabilities.

Method: FaR-Loc employs three modules: LLM Functionality Extraction for generating a natural language description of failures, Semantic Dense Retrieval using a pre-trained encoder to find similar methods in a shared semantic space, and LLM Re-ranking to prioritize the most relevant ones for debugging.

Result: FaR-Loc surpasses current state-of-the-art models like SoapFL and AutoFL by 14.6% in Top-1 and 19.2% in Top-5 accuracy. It also performs better than all learning- and spectrum-based models on the Defects4J benchmark.

Conclusion: The framework demonstrates that using LLMs in conjunction with code-structure-aware retrieval methods can significantly enhance fault localization, as supported by the substantial improvements observed with UniXcoder.

Abstract: Fault localization (FL) is a critical but time-consuming task in software
debugging, aiming to identify faulty code elements. While recent advances in
large language models (LLMs) have shown promise for FL, they often struggle
with complex systems due to the lack of project-specific knowledge and the
difficulty of navigating large projects. To address these limitations, we
propose FaR-Loc, a novel framework that enhances method-level FL by integrating
LLMs with retrieval-augmented generation (RAG). FaR-Loc consists of three key
components: LLM Functionality Extraction, Semantic Dense Retrieval, and LLM
Re-ranking. First, given a failed test and its associated stack trace, the LLM
Functionality Extraction module generates a concise natural language
description that captures the failing behavior. Next, the Semantic Dense
Retrieval component leverages a pre-trained code-understanding encoder to embed
both the functionality description (natural language) and the covered methods
(code) into a shared semantic space, enabling the retrieval of methods with
similar functional behavior. Finally, the LLM Re-ranking module reorders the
retrieved methods based on their contextual relevance. Our experiments on the
widely used Defects4J benchmark show that FaR-Loc outperforms state-of-the-art
LLM-based baselines SoapFL and AutoFL, by 14.6% and 9.1% in Top-1 accuracy, by
19.2% and 22.1% in Top-5 accuracy, respectively. It also surpasses all
learning-based and spectrum-based baselines across all Top-N metrics without
requiring re-training. Furthermore, we find that pre-trained code embedding
models that incorporate code structure, such as UniXcoder, can significantly
improve fault localization performance by up to 49.0% in Top-1 accuracy.
Finally, we conduct a case study to illustrate the effectiveness of FaR-Loc and
to provide insights for its practical application.

</details>


### [40] [Design, Implementation and Evaluation of a Novel Programming Language Topic Classification Workflow](https://arxiv.org/abs/2509.20631)
*Michael Zhang,Yuan Tian,Mariam Guizani*

Main category: cs.SE

TL;DR: This paper proposes a novel SVM-based workflow for fine-grained programming language topic classification, achieving strong F1 scores and providing a reusable pipeline for software engineering analysis.


<details>
  <summary>Details</summary>
Motivation: The increasing scale and complexity of software systems necessitate automated tools to understand programming language topic distributions, aiding technical decisions, onboarding, tooling, and education.

Method: A multi-label Support Vector Machine (SVM) combined with a sliding window and voting strategy is employed to detect core language concepts in source code, enabling fine-grained localization of topics.

Result: The model achieves an average F1 score of 0.90 across topics and 0.75 for code-topic highlights when trained on the IBM Project CodeNet dataset.

Conclusion: The paper introduces a reusable pipeline for programming language topic classification, offering empirical insights that benefit researchers and practitioners in code analysis and data-driven software engineering.

Abstract: As software systems grow in scale and complexity, understanding the
distribution of programming language topics within source code becomes
increasingly important for guiding technical decisions, improving onboarding,
and informing tooling and education. This paper presents the design,
implementation, and evaluation of a novel programming language topic
classification workflow. Our approach combines a multi-label Support Vector
Machine (SVM) with a sliding window and voting strategy to enable fine-grained
localization of core language concepts such as operator overloading, virtual
functions, inheritance, and templates. Trained on the IBM Project CodeNet
dataset, our model achieves an average F1 score of 0.90 across topics and 0.75
in code-topic highlight. Our findings contribute empirical insights and a
reusable pipeline for researchers and practitioners interested in code analysis
and data-driven software engineering.

</details>


### [41] [Exploring Engagement in Hybrid Meetings](https://arxiv.org/abs/2509.20780)
*Daniela Grassi,Fabio Calefato,Darja Smite,Nicole Novielli,Filippo Lanubile*

Main category: cs.SE

TL;DR: This paper investigates engagement patterns in hybrid software development meetings via multimodal data, finding comparable engagement between onsite/remote participants with key contextual factors influencing engagement dynamics.


<details>
  <summary>Details</summary>
Motivation: Hybrid work norms post-pandemic reveal collaboration challenges in software teams, with remote workers at risk of disengagement despite widespread adoption of hybrid meetings.

Method: Longitudinal study of three software companies using self-reported questionnaires and biometric devices to measure engagement in hybrid meetings through multimodal data collection.

Result: Remote participants maintain comparable engagement except in long meetings; active roles increase engagement while large meetings and afternoon sessions decrease it, confirmed via regression analysis.

Conclusion: Hybrid meeting engagement depends on temporal factors, participation roles, and meeting duration, offering actionable insights for optimizing hybrid collaboration across knowledge-intensive industries.

Abstract: Background. The widespread adoption of hybrid work following the COVID-19
pandemic has fundamentally transformed software development practices,
introducing new challenges in communication and collaboration as organizations
transition from traditional office-based structures to flexible working
arrangements. This shift has established a new organizational norm where even
traditionally office-first companies now embrace hybrid team structures. While
remote participation in meetings has become commonplace in this new
environment, it may lead to isolation, alienation, and decreased engagement
among remote team members. Aims. This study aims to identify and characterize
engagement patterns in hybrid meetings through objective measurements, focusing
on the differences between co-located and remote participants. Method. We
studied professionals from three software companies over several weeks,
employing a multimodal approach to measure engagement. Data were collected
through self-reported questionnaires and physiological measurements using
biometric devices during hybrid meetings to understand engagement dynamics.
Results. The regression analyses revealed comparable engagement levels between
onsite and remote participants, though remote participants show lower
engagement in long meetings regardless of participation mode. Active roles
positively correlate with higher engagement, while larger meetings and
afternoon sessions are associated with lower engagement. Conclusions. Our
results offer insights into factors associated with engagement and
disengagement in hybrid meetings, as well as potential meeting improvement
recommendations. These insights are potentially relevant not only for software
teams but also for knowledge-intensive organizations across various sectors
facing similar hybrid collaboration challenges.

</details>


### [42] [Verification Limits Code LLM Training](https://arxiv.org/abs/2509.20837)
*Srishti Gureja,Elena Tommasone,Jingyi He,Sara Hooker,Matthias Gallé,Marzieh Fadaee*

Main category: cs.SE

TL;DR: This paper identifies the 'verification ceiling' in code generation models trained on synthetic data, where rigid test suite designs limit data diversity. It demonstrates that tailored verification strategies—prioritizing rich test quality, relaxed pass thresholds with soft verification, and maintaining correct solution diversity—can improve pass@1 metrics by 2-4 points while preserving training value.


<details>
  <summary>Details</summary>
Motivation: Synthetic data enables scalable code generation model training but suffers from verification bottlenecks due to test suite limitations, creating an unexplored gap in training effectiveness and diversity preservation.

Method: The study analyzes three verification dimensions: (i) test suite composition (complexity vs quantity), (ii-iii) verification criteria (100%-pass rigidity vs relaxed thresholds with LLM soft verification), and (iii) solution correctness necessity via human evaluation of formally correct/incorrect examples.

Result: Experiments show richer test suites improve pass@1 by +3 on average. Relaxed verification thresholds with strong test diversity yield 2-4 point gains. Problem-solution diversity (retaining multiple correct variants) consistently enhances generalization.

Conclusion: Verification requires recalibration—not elimination—to balance data quality retention with diversity preservation. Combining calibrated verification and challenging problem sets breaks the verification ceiling, enabling stronger code generation models through strategic synthetic data curation.

Abstract: Large language models for code generation increasingly rely on synthetic
data, where both problem solutions and verification tests are generated by
models. While this enables scalable data creation, it introduces a previously
unexplored bottleneck: the verification ceiling, in which the quality and
diversity of training data are fundamentally constrained by the capabilities of
synthetic verifiers. In this work, we systematically study how verification
design and strategies influence model performance. We investigate (i) what we
verify by analyzing the impact of test complexity and quantity: richer test
suites improve code generation capabilities (on average +3 pass@1), while
quantity alone yields diminishing returns, (ii) how we verify by exploring
relaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By
allowing for relaxed thresholds or incorporating LLM-based soft verification,
we can recover valuable training data, leading to a 2-4 point improvement in
pass@1 performance. However, this benefit is contingent upon the strength and
diversity of the test cases used, and (iii) why verification remains necessary
through controlled comparisons of formally correct versus incorrect solutions
and human evaluation: retaining diverse correct solutions per problem yields
consistent generalization gains. Our results show that Verification as
currently practiced is too rigid, filtering out valuable diversity. But it
cannot be discarded, only recalibrated. By combining calibrated verification
with diverse, challenging problem-solution pairs, we outline a path to break
the verification ceiling and unlock stronger code generation models.

</details>


### [43] [PseudoBridge: Pseudo Code as the Bridge for Better Semantic and Logic Alignment in Code Retrieval](https://arxiv.org/abs/2509.20881)
*Yixuan Li,Xinyi Liu,Weidong Yang,Ben Fei,Shuhao Li,Mingjie Zhou,Lipeng Ma*

Main category: cs.SE

TL;DR: This paper proposes PseudoBridge, a code retrieval framework leveraging pseudo-code to bridge semantic gaps and enhance robustness to code style variations, achieving state-of-the-art results across diverse programming languages.


<details>
  <summary>Details</summary>
Motivation: Existing PLM-based code retrieval methods struggle with a semantic gap between human intent and machine execution logic, as well as fragility to code style variations, necessitating improved alignment strategies for broader generalizability.

Method: PseudoBridge employs a two-stage framework: (1) an LLM synthesizes pseudo-code to align NL queries with semi-structured representations, and (2) a logic-invariant code style augmentation generates diverse yet equivalent code implementations, aligning them with pseudo-code to improve robustness to style variations.

Result: Extensive experiments across 6 programming languages and 10 PLMs show PseudoBridge outperforms baselines, achieving significant improvements in retrieval accuracy and generalization, particularly in zero-shot domain transfer scenarios like Solidity and XLCoST datasets.

Conclusion: PseudoBridge demonstrates effectiveness in enhancing code retrieval by aligning NL intent with PL logic through pseudo-code, offering a robust and generalizable solution.

Abstract: Code search aims to precisely find relevant code snippets that match natural
language queries within massive codebases, playing a vital role in software
development. Recent advances leverage pre-trained language models (PLMs) to
bridge the semantic gap between unstructured natural language (NL) and
structured programming languages (PL), yielding significant improvements over
traditional information retrieval and early deep learning approaches. However,
existing PLM-based methods still encounter key challenges, including a
fundamental semantic gap between human intent and machine execution logic, as
well as limited robustness to diverse code styles. To address these issues, we
propose PseudoBridge, a novel code retrieval framework that introduces
pseudo-code as an intermediate, semi-structured modality to better align NL
semantics with PL logic. Specifically, PseudoBridge consists of two stages.
First, we employ an advanced large language model (LLM) to synthesize
pseudo-code, enabling explicit alignment between NL queries and pseudo-code.
Second, we introduce a logic-invariant code style augmentation strategy and
employ the LLM to generate stylistically diverse yet logically equivalent code
implementations with pseudo-code, then align the code snippets of different
styles with pseudo-code, enhancing model robustness to code style variation. We
build PseudoBridge across 10 different PLMs and evaluate it on 6 mainstream
programming languages. Extensive experiments demonstrate that PseudoBridge
consistently outperforms baselines, achieving significant gains in retrieval
accuracy and generalization, particularly under zero-shot domain transfer
scenarios such as Solidity and XLCoST datasets. These results demonstrate the
effectiveness of explicit logical alignment via pseudo-code and highlight
PseudoBridge's potential as a robust, generalizable solution for code
retrieval.

</details>


### [44] [Designing for Novice Debuggers: A Pilot Study on an AI-Assisted Debugging Tool](https://arxiv.org/abs/2509.21067)
*Oka Kurniawan,Erick Chandra,Christopher M. Poskitt,Yannic Noller,Kenny Tsu Wei Choo,Cyrille Jegourel*

Main category: cs.SE

TL;DR: This paper proposes CodeHinter, a hybrid LLM-debugger tool for novice programmers that balances AI assistance with active engagement, demonstrating improved usability and effectiveness in semantic error resolution through user studies.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based debugging tools risk fostering over-reliance on AI and disengaging students from the debugging process, potentially hindering skill development.

Method: The authors designed CodeHinter, a hybrid debugging assistant combining traditional debugging tools with LLM-based techniques, and evaluated it through a second iteration with undergraduate students.

Result: Students found CodeHinter significantly more effective for semantic error resolution and easier to use compared to the first version, with error localization identified as the most valuable feature.

Conclusion: The paper concludes that AI-assisted debugging tools should be personalized based on student profiles to optimize interaction and learning outcomes.

Abstract: Debugging is a fundamental skill that novice programmers must develop.
Numerous tools have been created to assist novice programmers in this process.
Recently, large language models (LLMs) have been integrated with automated
program repair techniques to generate fixes for students' buggy code. However,
many of these tools foster an over-reliance on AI and do not actively engage
students in the debugging process. In this work, we aim to design an intuitive
debugging assistant, CodeHinter, that combines traditional debugging tools with
LLM-based techniques to help novice debuggers fix semantic errors while
promoting active engagement in the debugging process. We present findings from
our second design iteration, which we tested with a group of undergraduate
students. Our results indicate that the students found the tool highly
effective in resolving semantic errors and significantly easier to use than the
first version. Consistent with our previous study, error localization was the
most valuable feature. Finally, we conclude that any AI-assisted debugging tool
should be personalized based on user profiles to optimize their interactions
with students.

</details>


### [45] [An Improved Quantum Software Challenges Classification Approach using Transfer Learning and Explainable AI](https://arxiv.org/abs/2509.21068)
*Nek Dil Khan,Javed Ali Khan,Mobashir Husain,Muhammad Sohail Khan,Arif Ali Khan,Muhammad Azeem Akbar,Shahid Hussain*

Main category: cs.SE

TL;DR: This study addresses Quantum Software Engineering (QSE) challenges by categorizing Stack Overflow discussions into six challenge types using transformer models (BERT, DistilBERT, RoBERTa) and grounded theory. Transformer-based classifiers outperformed traditional D&ML models with 95% accuracy, aided by SHAP for interpretability, offering insights for improving QSE Q&A organization.


<details>
  <summary>Details</summary>
Motivation: Quantum developers struggle to navigate fragmented Q&A discussions due to technical tagging and lack of concept-based categorization. Systematic classification could identify frequent QSE challenges and improve forum organization.

Method: 1) Extracted 2829 questions using quantum tags from Q&A platforms.
2) Combined content analysis, grounded theory, and human/ChatGPT annotations to build a ground truth dataset.
3) Trained transformer models (BERT, DistilBERT, RoBERTa) and D&ML models (FNN, CNN, LSTM) for classification.
4) Used SHAP values to explain model predictions.

Result: Transformer models achieved 95% accuracy (BERT 95%, DistilBERT 95%) vs. 89-84% for D&ML models without data augmentation. SHAP revealed critical linguistic features while maintaining 6% higher accuracy. Classification categories: Tooling, Theoretical, Learning, Conceptual, Errors, API Usage.

Conclusion: Transformer-based approaches significantly outperform traditional D&ML in QSE challenge classification with inherent model interpretability through SHAP analysis. The framework can improve quantum forum organization, but requires validation through developer-vendor co-creation studies.

Abstract: Quantum Software Engineering (QSE) is a research area practiced by tech
firms. Quantum developers face challenges in optimizing quantum computing and
QSE concepts. They use Stack Overflow (SO) to discuss challenges and label
posts with specialized quantum tags, which often refer to technical aspects
rather than developer posts. Categorizing questions based on quantum concepts
can help identify frequent QSE challenges. We conducted studies to classify
questions into various challenges. We extracted 2829 questions from Q&A
platforms using quantum-related tags. Posts were analyzed to identify frequent
challenges and develop a novel grounded theory. Challenges include Tooling,
Theoretical, Learning, Conceptual, Errors, and API Usage. Through content
analysis and grounded theory, discussions were annotated with common challenges
to develop a ground truth dataset. ChatGPT validated human annotations and
resolved disagreements. Fine-tuned transformer algorithms, including BERT,
DistilBERT, and RoBERTa, classified discussions into common challenges. We
achieved an average accuracy of 95% with BERT DistilBERT, compared to
fine-tuned Deep and Machine Learning (D&ML) classifiers, including Feedforward
Neural Networks (FNN), Convolutional Neural Networks (CNN), and Long Short-Term
Memory networks (LSTM), which achieved accuracies of 89%, 86%, and 84%,
respectively. The Transformer-based approach outperforms the D&ML-based
approach with a 6\% increase in accuracy by processing actual discussions,
i.e., without data augmentation. We applied SHAP (SHapley Additive
exPlanations) for model interpretability, revealing how linguistic features
drive predictions and enhancing transparency in classification. These findings
can help quantum vendors and forums better organize discussions for improved
access and readability. However,empirical evaluation studies with actual
developers and vendors are needed.

</details>


### [46] [Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach](https://arxiv.org/abs/2509.21170)
*Yongda Yu,Guohao Shi,Xianwei Wu,Haochuan He,XueMing Gu,Qianqian Zhao,Kui Liu,Qiushi Wang,Zhao Tian,Haifeng Shen,Guoping Rong*

Main category: cs.SE

TL;DR: MelcotCR improves LLM code review capabilities through structured multi-dimensional training and ME-based reasoning optimization, achieving near-state-of-the-art performance with minimal parameters.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based code review methods underperform human reviewers due to: 1) Limited training data scope 2) Overlooking multi-dimensional analysis required for comprehensive code review 3) Inability to maintain reasoning quality in long COT prompts.

Method: MelcotCR introduces a chain-of-thought fine-tuning framework combining: 1) Structured long COT prompts for multi-dimensional code analysis 2) Maximum Entropy (ME) modeling combined with pre-defined reasoning pathways to address context loss and maintain logical consistency in long reasoning chains.

Result: MelcotCR achieves: - 14B Qwen2.5 fine-tuned with MelcotCR outperforms existing methods in code issue detection (-32% error rate reduction on CodeReviewer dataset) - Matches performance of 671B DeepSeek-R1 model in accuracy for both defect detection and description generation - 22% improvement in multi-dimensional issue detection on MelcotCR's curated dataset

Conclusion: The paper concludes that the MelcotCR approach effectively enhances LLMs' code review capabilities through structured long COT fine-tuning and ME-based reasoning pathway optimization, achieving state-of-the-art performance with significant parameter efficiency.

Abstract: Large Language Models (LLMs) have shown great potential in supporting
automated code review due to their impressive capabilities in context
understanding and reasoning. However, these capabilities are still limited
compared to human-level cognition because they are heavily influenced by the
training data. Recent research has demonstrated significantly improved
performance through fine-tuning LLMs with code review data. However, compared
to human reviewers who often simultaneously analyze multiple dimensions of code
review to better identify issues, the full potential of these methods is
hampered by the limited or vague information used to fine-tune the models. This
paper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that
trains LLMs with an impressive reasoning ability to analyze multiple dimensions
of code review by harnessing long COT techniques to provide rich structured
information. To address context loss and reasoning logic loss issues that
frequently occur when LLMs process long COT prompts, we propose a solution that
combines the Maximum Entropy (ME) modeling principle with pre-defined reasoning
pathways in MelcotCR to enable more effective utilization of in-context
knowledge within long COT prompts while strengthening the logical tightness of
the reasoning process. Empirical evaluations on our curated MelcotCR dataset
and the public CodeReviewer dataset reveal that a low-parameter base model,
such as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art
methods in terms of the accuracy of detecting and describing code issues, with
its performance remarkably on par with that of the 671B DeepSeek-R1 model.

</details>


### [47] [Semantic Clustering of Civic Proposals: A Case Study on Brazil's National Participation Platform](https://arxiv.org/abs/2509.21292)
*Ronivaldo Ferreira,Guilherme da Silva,Carla Rocha,Gustavo Pinto*

Main category: cs.SE

TL;DR: This paper proposes a BERTopic-based approach with seed words and LLM validation to efficiently organize large-scale citizen input on digital platforms, enabling governments to generate actionable policy data with minimal human effort.


<details>
  <summary>Details</summary>
Motivation: Governments face challenges organizing massive citizen engagement data due to scalability issues, expertise requirements, and alignment with official taxonomies, leading to underutilized input.

Method: Combines BERTopic (a neural topic model), seed words for guidance, and LLMs (large language models) for automatic validation of generated topics against institutional criteria.

Result: Initial results show generated topics are coherent, institutionally aligned, and require minimal human intervention compared to manual approaches.

Conclusion: The methodology provides a scalable solution for transforming unstructured citizen feedback into structured insights for public policy decision-making.

Abstract: Promoting participation on digital platforms such as Brasil Participativo has
emerged as a top priority for governments worldwide. However, due to the sheer
volume of contributions, much of this engagement goes underutilized, as
organizing it presents significant challenges: (1) manual classification is
unfeasible at scale; (2) expert involvement is required; and (3) alignment with
official taxonomies is necessary. In this paper, we introduce an approach that
combines BERTopic with seed words and automatic validation by large language
models. Initial results indicate that the generated topics are coherent and
institutionally aligned, with minimal human effort. This methodology enables
governments to transform large volumes of citizen input into actionable data
for public policy.

</details>
