<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 24]
- [cs.SE](#cs.SE) [Total: 6]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [A Robust Pipeline for Differentially Private Federated Learning on Imbalanced Clinical Data using SMOTETomek and FedProx](https://arxiv.org/abs/2508.10017)
*Rodrigo Tertulino*

Main category: cs.CR

TL;DR: This paper proposes an FL framework with DP for cardiovascular risk prediction, addressing class imbalance via SMOTETomek and optimizing non-IID data handling with FedProx, achieving high clinical utility (77% recall) at strong privacy (ε=9.0).


<details>
  <summary>Details</summary>
Motivation: Medical datasets often exhibit severe class imbalance and heterogeneity, while integrating differential privacy with federated learning necessitates resolving the privacy-utility trade-off to enable secure yet effective collaborative health research.

Method: 1) Client-level integration of SMOTETomek for class rebalancing in decentralized medical data scenarios. 2) FedProx algorithm optimization with non-IID data handling tuned for cardiovascular risk prediction tasks.

Result: Standard methods achieved zero recall due to class imbalance. Optimized FedProx outperformed FedAvg across privacy budgets (ε), maintaining over 77% recall at ε=9.0 despite non-linear trade-offs between privacy and model utility.

Conclusion: The study establishes a methodology blueprint for developing diagnostic tools in healthcare settings that simultaneously preserve strong formal privacy guarantees while achieving clinical robustness through FL-embedded class imbalance mitigation.

Abstract: Federated Learning (FL) presents a groundbreaking approach for collaborative
health research, allowing model training on decentralized data while
safeguarding patient privacy. FL offers formal security guarantees when
combined with Differential Privacy (DP). The integration of these technologies,
however, introduces a significant trade-off between privacy and clinical
utility, a challenge further complicated by the severe class imbalance often
present in medical datasets. The research presented herein addresses these
interconnected issues through a systematic, multi-stage analysis. An FL
framework was implemented for cardiovascular risk prediction, where initial
experiments showed that standard methods struggled with imbalanced data,
resulting in a recall of zero. To overcome such a limitation, we first
integrated the hybrid Synthetic Minority Over-sampling Technique with Tomek
Links (SMOTETomek) at the client level, successfully developing a clinically
useful model. Subsequently, the framework was optimized for non-IID data using
a tuned FedProx algorithm. Our final results reveal a clear, non-linear
trade-off between the privacy budget (epsilon) and model recall, with the
optimized FedProx consistently out-performing standard FedAvg. An optimal
operational region was identified on the privacy-utility frontier, where strong
privacy guarantees (with epsilon 9.0) can be achieved while maintaining high
clinical utility (recall greater than 77%). Ultimately, our study provides a
practical methodological blueprint for creating effective, secure, and accurate
diagnostic tools that can be applied to real-world, heterogeneous healthcare
data.

</details>


### [2] [A Comparative Performance Evaluation of Kyber, sntrup761, and FrodoKEM for Post-Quantum Cryptography](https://arxiv.org/abs/2508.10023)
*Samet Ünsal*

Main category: cs.CR

TL;DR: This paper compares leading post-quantum cryptographic algorithms (Kyber, sntrup761, FrodoKEM) and analyzes their security, performance, real-world applicability, and transition challenges toward quantum-safe systems.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the critical need for cryptographic algorithms resistant to quantum computing threats, highlighting transition challenges from classical systems and their industrial impacts.

Method: A comprehensive review and comparative analysis of post-quantum algorithms using theoretical assessments and empirical benchmarks for security, performance, and real-world applicability, with a focus on transitioning from classical systems.

Result: Strengths and weaknesses of Kyber, sntrup761, and FrodoKEM are identified, including their trade-offs between security, computational efficiency, and usability. Transition challenges (e.g., compatibility, key size) and industry-specific implications are detailed.

Conclusion: The study establishes a foundation for post-quantum cryptography adoption, emphasizing the importance of algorithm standardization, practical optimization, and industry collaboration for secure quantum-era systems.

Abstract: Post-quantum cryptography (PQC) aims to develop cryptographic algorithms that
are secure against attacks from quantum computers. This paper compares the
leading postquantum cryptographic algorithms, such as Kyber, sntrup761, and
FrodoKEM, in terms of their security, performance, and real-world
applicability. The review highlights the strengths and weaknesses of each
algorithm and provides insights into future research directions. We also
discuss the challenges of transitioning from classical to post-quantum systems
and the potential impacts on various industries. This paper serves as a
foundation for understanding the current state of post-quantum cryptography and
its future prospects in the quantum computing era.

</details>


### [3] [Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs](https://arxiv.org/abs/2508.10031)
*Jinhwa Kim,Ian G. Harris*

Main category: cs.CR

TL;DR: The study introduces a Context Filtering model to defend LLMs against jailbreak attacks by pre-processing inputs to remove malicious contexts while preserving helpfulness, achieving an 88% reduction in attack success and state-of-the-art safety-helpfulness balance.


<details>
  <summary>Details</summary>
Motivation: Jailbreak attacks leveraging adversarial contexts threaten the safety and ethical alignment of large language models (LLMs), often forcing them to generate harmful responses while reducing their helpfulness to legitimate users.

Method: The proposed Context Filtering model implements an input pre-processing method that filters untrustworthy context and extracts the user's primary intent to detect hidden malicious attacks. It works as a plug-and-play system, applicable to both white-box and black-box LLMs without requiring model fine-tuning.

Result: The model reduced jailbreak attack success rates by up to 88% across six attack scenarios, while maintaining the baseline helpfulness of LLMs, achieving state-of-the-art Safety and Helpfulness Product metrics.

Conclusion: The Context Filtering model offers a universal defense strategy that significantly enhances LLM safety without compromising their performance, providing a practical solution deployable across diverse models and attack scenarios.

Abstract: While Large Language Models (LLMs) have shown significant advancements in
performance, various jailbreak attacks have posed growing safety and ethical
risks. Malicious users often exploit adversarial context to deceive LLMs,
prompting them to generate responses to harmful queries. In this study, we
propose a new defense mechanism called Context Filtering model, an input
pre-processing method designed to filter out untrustworthy and unreliable
context while identifying the primary prompts containing the real user intent
to uncover concealed malicious intent. Given that enhancing the safety of LLMs
often compromises their helpfulness, potentially affecting the experience of
benign users, our method aims to improve the safety of the LLMs while
preserving their original performance. We evaluate the effectiveness of our
model in defending against jailbreak attacks through comparative analysis,
comparing our approach with state-of-the-art defense mechanisms against six
different attacks and assessing the helpfulness of LLMs under these defenses.
Our model demonstrates its ability to reduce the Attack Success Rates of
jailbreak attacks by up to 88% while maintaining the original LLMs'
performance, achieving state-of-the-art Safety and Helpfulness Product results.
Notably, our model is a plug-and-play method that can be applied to all LLMs,
including both white-box and black-box models, to enhance their safety without
requiring any fine-tuning of the models themselves. We will make our model
publicly available for research purposes.

</details>


### [4] [Cognitive Cybersecurity for Artificial Intelligence: Guardrail Engineering with CCS-7](https://arxiv.org/abs/2508.10033)
*Yuksel Aydin*

Main category: cs.CR

TL;DR: The paper introduces CCS-7, a cognitive vulnerability taxonomy for language models, showing that TFVA-style guardrails produce architecture-dependent mitigation outcomes, with source interference risks increasing by 135% in some models while human participants showed consistent 7.9% improvement.


<details>
  <summary>Details</summary>
Motivation: Language models display human-like cognitive vulnerabilities that traditional behavioral alignment methods cannot detect or mitigate, requiring new approaches to ensure security.

Method: 1) Randomized controlled trial with 151 human participants using TFVA lessons (Think First, Verify Always). 2) Evaluation of 12,180 experiments across seven diverse language model architectures using architecture-dependent cognitive security testing.

Result: Human participants showed +7.9% improvement in cognitive security. For models: identity confusion vulnerabilities were almost fully mitigated, source interference vulnerabilities exhibited backfire effects with error rates increasing up to 135% in certain architectures, and other vulnerabilities showed mixed architecture-specific results.

Conclusion: Cognitive safety must be treated as an architecture-specific engineering problem. Interventions effective in one model architecture may fail or harm others, necessitating architecture-aware testing before deployment to avoid unintentional risk amplification.

Abstract: Language models exhibit human-like cognitive vulnerabilities, such as
emotional framing, that escape traditional behavioral alignment. We present
CCS-7 (Cognitive Cybersecurity Suite), a taxonomy of seven vulnerabilities
grounded in human cognitive security research. To establish a human benchmark,
we ran a randomized controlled trial with 151 participants: a "Think First,
Verify Always" (TFVA) lesson improved cognitive security by +7.9% overall. We
then evaluated TFVA-style guardrails across 12,180 experiments on seven diverse
language model architectures. Results reveal architecture-dependent risk
patterns: some vulnerabilities (e.g., identity confusion) are almost fully
mitigated, while others (e.g., source interference) exhibit escalating
backfire, with error rates increasing by up to 135% in certain models. Humans,
in contrast, show consistent moderate improvement. These findings reframe
cognitive safety as a model-specific engineering problem: interventions
effective in one architecture may fail, or actively harm, another, underscoring
the need for architecture-aware cognitive safety testing before deployment.

</details>


### [5] [Neural Network-Based Detection and Multi-Class Classification of FDI Attacks in Smart Grid Home Energy Systems](https://arxiv.org/abs/2508.10035)
*Varsha Sen,Biswash Basnet*

Main category: cs.CR

TL;DR: The paper proposes a machine learning framework combining an ANN for real-time detection and a Bidirectional LSTM for classification of False Data Injection Attacks (FDIAs) in smart grid Home Area Networks (HANs).


<details>
  <summary>Details</summary>
Motivation: FDIAs exploit weak security in HANs to manipulate energy data, threatening grid integrity. Existing defenses fail to detect stealthy attacks without alarms, necessitating scalable edge-based solutions.

Method: 1) Lightweight Artificial Neural Network (ANN) for real-time FDIA detection using features like energy consumption, cost, and time context. 2) Bidirectional LSTM model to classify attack types (normal/trapezoidal/sigmoid) by analyzing sequential data patterns. 3) Synthetic time-series dataset emulating household energy behavior for training and evaluation.

Result: Models successfully detected FDIA anomalies (e.g., consumption manipulation) and distinguished attack shapes with high accuracy, validating their effectiveness through controlled experiments. The framework offers edge-deployable, scalable cybersecurity for residential smart grid endpoints.

Conclusion: This work establishes an intelligent FDIA defense system for HANs using machine learning-based edge computing, addressing vulnerabilities in residential energy data integrity while enabling granular attack classification to improve smart grid resilience.

Abstract: False Data Injection Attacks (FDIAs) pose a significant threat to smart grid
infrastructures, particularly Home Area Networks (HANs), where real-time
monitoring and control are highly adopted. Owing to the comparatively less
stringent security controls and widespread availability of HANs, attackers view
them as an attractive entry point to manipulate aggregated demand patterns,
which can ultimately propagate and disrupt broader grid operations. These
attacks undermine the integrity of smart meter data, enabling malicious actors
to manipulate consumption values without activating conventional alarms,
thereby creating serious vulnerabilities across both residential and
utility-scale infrastructures. This paper presents a machine learning-based
framework for both the detection and classification of FDIAs using residential
energy data. A real-time detection is provided by the lightweight Artificial
Neural Network (ANN), which works by using the most vital features of energy
consumption, cost, and time context. For the classification of different attack
types, a Bidirectional LSTM is trained to recognize normal, trapezoidal, and
sigmoid attack shapes through learning sequential dependencies in the data. A
synthetic time-series dataset was generated to emulate realistic household
behaviour. Experimental results demonstrate that the proposed models are
effective in identifying and classifying FDIAs, offering a scalable solution
for enhancing grid resilience at the edge. This work contributes toward
building intelligent, data-driven defence mechanisms that strengthen smart grid
cybersecurity from residential endpoints.

</details>


### [6] [Certifiably robust malware detectors by design](https://arxiv.org/abs/2508.10038)
*Pierre-Francois Gimenez,Sarath Sivaprasad,Mario Fritz*

Main category: cs.CR

TL;DR: The paper proposes ERDALT, a certifiably robust malware detection framework based on a novel model architecture and structural decomposition of robust detectors, achieving robustness without significant performance loss.


<details>
  <summary>Details</summary>
Motivation: Current static malware analysis using machine learning lacks robustness against adversarial examples where minimal behavioral-preserving modifications evade detection, unlike domains such as computer vision.

Method: 1) Introduces a new model architecture designed for inherent robustness. 2) Demonstrates that robust detectors require specific structural properties through decomposition. 3) Implements ERDALT framework leveraging these properties for robust malware detection.

Result: ERDALT achieves robust detection while maintaining limited reduction in accuracy compared to standard machine learning approaches through validation against adversarial examples.

Conclusion: The structural approach enables certifiable robustness in malware detection and provides a pathway to develop empirically robust detectors even for fragile features, advancing secure static malware analysis.

Abstract: Malware analysis involves analyzing suspicious software to detect malicious
payloads. Static malware analysis, which does not require software execution,
relies increasingly on machine learning techniques to achieve scalability.
Although such techniques obtain very high detection accuracy, they can be
easily evaded with adversarial examples where a few modifications of the sample
can dupe the detector without modifying the behavior of the software. Unlike
other domains, such as computer vision, creating an adversarial example of
malware without altering its functionality requires specific transformations.
We propose a new model architecture for certifiably robust malware detection by
design. In addition, we show that every robust detector can be decomposed into
a specific structure, which can be applied to learn empirically robust malware
detectors, even on fragile features. Our framework ERDALT is based on this
structure. We compare and validate these approaches with machine-learning-based
malware detection methods, allowing for robust detection with limited reduction
of detection performance.

</details>


### [7] [Multi-task Adversarial Attacks against Black-box Model with Few-shot Queries](https://arxiv.org/abs/2508.10039)
*Wenqiang Wang,Yan Xiao,Hao Lin,Yangshijie Zhang,Xiaochun Cao*

Main category: cs.CR

TL;DR: CEMA is a black-box adversarial text attack method that uses a plug-and-play substitute model to attack multi-task systems with limited queries across diverse tasks like classification, translation, and image generation.


<details>
  <summary>Details</summary>
Motivation: Existing multi-task adversarial attacks require excessive internal feature access and queries, limiting their practical effectiveness against black-box systems, commercial APIs, and multi-task deployments with diverse objectives (e.g., classification, translation, summarization).

Method: CEMA trains a deep-level substitute model in a task-agnostic plug-and-play framework. It generates multiple adversarial candidates using diverse text classification methods and selects the most effective one against substitute models, reducing multi-task attacks to simpler classification tasks with minimal query requirements.

Result: CEMA achieves significant attack success across multi-task models with 2-6 tasks in experiments spanning classification, translation, summarization, and text-to-image generation. It can attack commercial APIs (Baidu, Google Translate), ChatGPT 4o, and image-generation models like Stable Diffusion V2 using as few as 100 queries.

Conclusion: CEMA offers a versatile adversarial attack framework for practical multi-task scenarios by leveraging substitute model transferability, minimal query requirements, and cross-domain applicability against commercial and open-source models in both text and image domains.

Abstract: Current multi-task adversarial text attacks rely on abundant access to shared
internal features and numerous queries, often limited to a single task type. As
a result, these attacks are less effective against practical scenarios
involving black-box feedback APIs, limited queries, or multiple task types. To
bridge this gap, we propose \textbf{C}luster and \textbf{E}nsemble
\textbf{M}ulti-task Text Adversarial \textbf{A}ttack (\textbf{CEMA}), an
effective black-box attack that exploits the transferability of adversarial
texts across different tasks. CEMA simplifies complex multi-task scenarios by
using a \textit{deep-level substitute model} trained in a
\textit{plug-and-play} manner for text classification, enabling attacks without
mimicking the victim model. This approach requires only a few queries for
training, converting multi-task attacks into classification attacks and
allowing attacks across various tasks.
  CEMA generates multiple adversarial candidates using different text
classification methods and selects the one that most effectively attacks
substitute models.
  In experiments involving multi-task models with two, three, or six
tasks--spanning classification, translation, summarization, and text-to-image
generation--CEMA demonstrates significant attack success with as few as 100
queries. Furthermore, CEMA can target commercial APIs (e.g., Baidu and Google
Translate), large language models (e.g., ChatGPT 4o), and image-generation
models (e.g., Stable Diffusion V2), showcasing its versatility and
effectiveness in real-world applications.

</details>


### [8] [Quantum Prime Factorization: A Novel Approach Based on Fermat Method](https://arxiv.org/abs/2508.10041)
*Julien Mellaerts*

Main category: cs.CR

TL;DR: The paper proposes a quantum-enhanced Fermat factorization method with two key contributions: a fourfold complexity reduction in classical Fermat method and a novel reformulation for quantum annealers.


<details>
  <summary>Details</summary>
Motivation: To address computational complexity in factoring composite odd numbers and leverage quantum annealers for solving optimization problems inherent in factorization.

Method: 1) Optimized classical Fermat method for reduced complexity; 2) Reformulated factorization as an optimization problem compatible with quantum annealers.

Result: Achieved a fourfold reduction in computational complexity for the classical Fermat method and successfully factorized 8,689,739 using a quantum annealer—the largest number demonstrated with a quantum device.

Conclusion: Demonstrates advancements in both classical and quantum approaches to factorization, establishing a benchmark for quantum factorization capabilities.

Abstract: In this paper, we introduce a novel quantum algorithm for the factorization
of composite odd numbers. This work makes two significant contributions. First,
we present a new improvement to the classical Fermat method, fourfold reducing
the computational complexity of factoring. Second, we reformulate Fermat
factorization method as an optimization problem suitable for Quantum Annealers
which allowed us to factorize 8,689,739, the biggest number ever factorized
using a quantum device to our knowledge.

</details>


### [9] [FIDELIS: Blockchain-Enabled Protection Against Poisoning Attacks in Federated Learning](https://arxiv.org/abs/2508.10042)
*Jane Carney,Kushal Upreti,Gaby G. Dagher,Tim Andersen*

Main category: cs.CR

TL;DR: The paper introduces \Sys, a blockchain-enabled framework for detecting data poisoning attacks in federated learning by decentralizing the global server's role and using a consensus-verified judge model. Empirical results demonstrate its robustness and scalability.


<details>
  <summary>Details</summary>
Motivation: Federated learning (FL) systems face risks from data poisoning attacks that compromise model performance and integrity. Existing detection approaches rely on centralized trust assumptions or lack standardized methods, creating vulnerabilities in collaborative learning environments.

Method: The proposed \Sys framework utilizes blockchain technology to distribute the global server's authority among FL clients. Each client generates a judge model for poisoning detection through a three-phase process: (1) client-side judge model generation, (2) consensus-based model validation, and (3) dynamic model selection using the longest blockchain branch as a confidence indicator.

Result: Implementations validated \Sys's effectiveness against common data poisoning attacks (e.g., label flipping, backdoor). The framework achieved >95% detection accuracy while maintaining model performance and demonstrated linear scalability of the judge model creation process with increasing client numbers.

Conclusion: Blockchain-enabled decentralization of poisoning detection in federated learning presents a viable security solution. \Sys shifts responsibility from a vulnerable central server to distributed validation, with experimental results confirming its potential to protect collaborative learning systems without compromising scalability or model utility.

Abstract: Federated learning enhances traditional deep learning by enabling the joint
training of a model with the use of IoT device's private data. It ensures
privacy for clients, but is susceptible to data poisoning attacks during
training that degrade model performance and integrity. Current poisoning
detection methods in federated learning lack a standardized detection method or
take significant liberties with trust. In this paper, we present \Sys, a novel
blockchain-enabled poison detection framework in federated learning. The
framework decentralizes the role of the global server across participating
clients. We introduce a judge model used to detect data poisoning in model
updates. The judge model is produced by each client and verified to reach
consensus on a single judge model. We implement our solution to show \Sys is
robust against data poisoning attacks and the creation of our judge model is
scalable.

</details>


### [10] [Securing Agentic AI: Threat Modeling and Risk Analysis for Network Monitoring Agentic AI System](https://arxiv.org/abs/2508.10043)
*Pallavi Zambare,Venkata Nikhil Thanikella,Ying Liu*

Main category: cs.CR

TL;DR: This paper analyzes security vulnerabilities in agentic AI systems combining LLMs and autonomous agents using the MAESTRO framework. Two threat cases (resource denial-of-service via traffic replay and memory poisoning via log tampering) were identified and mitigated through a defense-in-depth approach, proving MAESTRO's effectiveness in operational threat mapping and resilient system design.


<details>
  <summary>Details</summary>
Motivation: Integrating LLMs with autonomous agents in network monitoring and decision-making systems introduces security risks that threaten system reliability. This paper aims to systematically expose and mitigate such vulnerabilities in adversarial environments.

Method: The authors implemented a prototype agent system using Python, LangChain, and WebSocket telemetry, integrating inference, memory, parameter tuning, and anomaly detection modules. They applied MAESTRO's 7-layer threat modeling architecture to identify and test vulnerabilities through two practical attack scenarios.

Result: Both threat cases caused measurable performance degradation (delayed telemetry updates, increased computational loads) due to poor system adaptation. Real-time validation of planners and anomaly response systems effectively combated these attacks, while memory isolation prevented log tampering.

Conclusion: MAESTRO enables proactive threat mapping and risk scoring for agentic AI systems. The study confirms the importance of memory integrity enforcement, adaptation logic monitoring, and cross-layer protections to guarantee reliability against sophisticated adversarial attacks.

Abstract: When combining Large Language Models (LLMs) with autonomous agents, used in
network monitoring and decision-making systems, this will create serious
security issues. In this research, the MAESTRO framework consisting of the
seven layers threat modeling architecture in the system was used to expose,
evaluate, and eliminate vulnerabilities of agentic AI. The prototype agent
system was constructed and implemented, using Python, LangChain, and telemetry
in WebSockets, and deployed with inference, memory, parameter tuning, and
anomaly detection modules. Two practical threat cases were confirmed as
follows: (i) resource denial of service by traffic replay denial-of-service,
and (ii) memory poisoning by tampering with the historical log file maintained
by the agent. These situations resulted in measurable levels of performance
degradation, i.e. telemetry updates were delayed, and computational loads were
increased, as a result of poor system adaptations. It was suggested to use a
multilayered defense-in-depth approach with memory isolation, validation of
planners and anomaly response systems in real-time. These findings verify that
MAESTRO is viable in operational threat mapping, prospective risk scoring, and
the basis of the resilient system design. The authors bring attention to the
importance of the enforcement of memory integrity, paying attention to the
adaptation logic monitoring, and cross-layer communication protection that
guarantee the agentic AI reliability in adversarial settings.

</details>


### [11] [Generative AI for Cybersecurity of Energy Management Systems: Methods, Challenges, and Future Directions](https://arxiv.org/abs/2508.10044)
*Aydin Zaboli,Junho Hong*

Main category: cs.CR

TL;DR: The paper proposes a comprehensive security framework for energy management systems (EMSs) using generative AI and a multi-point attack/error model, validated on the IEEE 14-Bus system.


<details>
  <summary>Details</summary>
Motivation: Modern EMSs face evolving cybersecurity vulnerabilities and system problems, necessitating a dynamic framework to address interconnected attack vectors and integrate novel methodologies for robust protection.

Method: 1. A multi-point attack/error model to identify vulnerabilities in the EMS data pipeline (e.g., SE stealth attacks, RTDB corruption, HMI display manipulation). 2. First-time application of generative AI (GenAI)-based anomaly detection systems (ADSs) in power systems. 3. A set-of-mark generative intelligence (SoM-GI) framework combining visual markers, linguistic rules, and GenAI to overcome spatial reasoning limitations.

Result: Validation on the IEEE 14-Bus system demonstrated the framework's effectiveness in handling scenarios and detecting visual anomalies missed by numerical methods.

Conclusion: The integrated framework combining numerical analysis, visual pattern recognition, and linguistic rules effectively addresses cyber threats and system errors in EMSs through novel GenAI-driven methodologies.

Abstract: This paper elaborates on an extensive security framework specifically
designed for energy management systems (EMSs), which effectively tackles the
dynamic environment of cybersecurity vulnerabilities and/or system problems
(SPs), accomplished through the incorporation of novel methodologies. A
comprehensive multi-point attack/error model is initially proposed to
systematically identify vulnerabilities throughout the entire EMS data
processing pipeline, including post state estimation (SE) stealth attacks, EMS
database manipulation, and human-machine interface (HMI) display corruption
according to the real-time database (RTDB) storage. This framework acknowledges
the interconnected nature of modern attack vectors, which utilize various
phases of supervisory control and data acquisition (SCADA) data flow. Then,
generative AI (GenAI)-based anomaly detection systems (ADSs) for EMSs are
proposed for the first time in the power system domain to handle the scenarios.
Further, a set-of-mark generative intelligence (SoM-GI) framework, which
leverages multimodal analysis by integrating visual markers with rules
considering the GenAI capabilities, is suggested to overcome inherent spatial
reasoning limitations. The SoM-GI methodology employs systematic visual
indicators to enable accurate interpretation of segmented HMI displays and
detect visual anomalies that numerical methods fail to identify. Validation on
the IEEE 14-Bus system shows the framework's effectiveness across scenarios,
while visual analysis identifies inconsistencies. This integrated approach
combines numerical analysis with visual pattern recognition and linguistic
rules to protect against cyber threats and system errors.

</details>


### [12] [NetMoniAI: An Agentic AI Framework for Network Security & Monitoring](https://arxiv.org/abs/2508.10052)
*Pallavi Zambare,Venkata Nikhil Thanikella,Nikhil Padmanabh Kottur,Sree Akhil Akula,Ying Liu*

Main category: cs.CR

TL;DR: NetMoniAI is a two-tier AI framework for network monitoring combining autonomous node-level agents with a central coordinator. It achieves scalability, reduced redundancy, and faster response times while maintaining accuracy, with open-source availability for broader adoption.


<details>
  <summary>Details</summary>
Motivation: Existing network monitoring systems face challenges in scalability, resource efficiency, and coordination across distributed environments, necessitating a solution that balances decentralized analysis with centralized control for effective threat detection and situational awareness.

Method: The architecture employs autonomous micro-agents at each node for local traffic analysis and anomaly detection, paired with a central controller aggregating node data to detect coordinated attacks and maintain system-wide visibility. Evaluation used a local micro-testbed and NS-3 simulations.

Result: NetMoniAI demonstrated improved performance through scalable operations under resource limitations, minimized redundant processing, and faster response times without accuracy compromises. Open-source distribution enables replication and extension across diverse network scenarios.

Conclusion: The two-layer agentic-AI design effectively enhances network monitoring by distributing intelligence and centralizing coordination, with open-source availability fostering validation and adaptation to evolving security challenges in heterogeneous infrastructure.

Abstract: In this paper, we present NetMoniAI, an agentic AI framework for automatic
network monitoring and security that integrates decentralized analysis with
lightweight centralized coordination. The framework consists of two layers:
autonomous micro-agents at each node perform local traffic analysis and anomaly
detection. A central controller then aggregates insights across nodes to detect
coordinated attacks and maintain system-wide situational awareness. We
evaluated NetMoniAI on a local micro-testbed and through NS-3 simulations.
Results confirm that the two-tier agentic-AI design scales under resource
constraints, reduces redundancy, and improves response time without
compromising accuracy. To facilitate broader adoption and reproducibility, the
complete framework is available as open source. This enables researchers and
practitioners to replicate, validate, and extend it across diverse network
environments and threat scenarios. Github link:
https://github.com/pzambare3/NetMoniAI

</details>


### [13] [Invisible Watermarks, Visible Gains: Steering Machine Unlearning with Bi-Level Watermarking Design](https://arxiv.org/abs/2508.10065)
*Yuhao Sun,Yihua Zhang,Gaowen Liu,Hongtao Xie,Sijia Liu*

Main category: cs.CR

TL;DR: The paper introduces Water4MU, a digital watermarking-based framework for enhancing machine unlearning (MU) through data-level adjustments. The bi-level optimization approach separately optimizes watermarking and model training to enable precise data removal while maintaining model utility, achieving superior performance in challenging unlearning scenarios.


<details>
  <summary>Details</summary>
Motivation: Machine unlearning algorithms predominantly use in-training weight adjustments, lacking exploration of data-level optimizations. The paper addresses this gap by investigating how watermarking can improve unlearning effectiveness, allowing controlled influence removal of sensitive data without compromising unrelated tasks. Traditional MU approaches struggle with 'challenging forgets' requiring targeted data removal.

Method: Water4MU employs a bi-level optimization (BLO) framework: 1) Upper level optimizes watermarking to minimize unlearning difficulty by strategically embedding modifications that facilitate data influence removal 2) Lower level optimizes model training independent of watermarking signals. Digital watermarking is leveraged to proactively embed unlearning controls within data samples, enabling post-hoc adjustments to remove specific data influences.

Result: Water4MU demonstrates effective machine unlearning across image classification and generation tasks. The approach outperforms existing methods in 'challenging forget' scenarios while preserving model accuracy for non-sensitive data. Watermarked data showed comparable unlearning effectiveness to original datasets, validating the framework's practical value in real-world applications.

Conclusion: The paper establishes digital watermarking as a viable approach for data-centric machine unlearning, with Water4MU providing a novel solution that balances precise data influence removal and model utility preservation. The results suggest this framework resolves critical limitations in existing MU techniques, particularly for difficult data unlearning requirements in regulated environments.

Abstract: With the increasing demand for the right to be forgotten, machine unlearning
(MU) has emerged as a vital tool for enhancing trust and regulatory compliance
by enabling the removal of sensitive data influences from machine learning (ML)
models. However, most MU algorithms primarily rely on in-training methods to
adjust model weights, with limited exploration of the benefits that data-level
adjustments could bring to the unlearning process. To address this gap, we
propose a novel approach that leverages digital watermarking to facilitate MU
by strategically modifying data content. By integrating watermarking, we
establish a controlled unlearning mechanism that enables precise removal of
specified data while maintaining model utility for unrelated tasks. We first
examine the impact of watermarked data on MU, finding that MU effectively
generalizes to watermarked data. Building on this, we introduce an
unlearning-friendly watermarking framework, termed Water4MU, to enhance
unlearning effectiveness. The core of Water4MU is a bi-level optimization (BLO)
framework: at the upper level, the watermarking network is optimized to
minimize unlearning difficulty, while at the lower level, the model itself is
trained independently of watermarking. Experimental results demonstrate that
Water4MU is effective in MU across both image classification and image
generation tasks. Notably, it outperforms existing methods in challenging MU
scenarios, known as "challenging forgets".

</details>


### [14] [An Architecture for Distributed Digital Identities in the Physical World](https://arxiv.org/abs/2508.10185)
*René Mayrhofer,Michael Roland,Tobias Höller,Philipp Hofer,Mario Lins*

Main category: cs.CR

TL;DR: The paper proposes a decentralized digital identity architecture with a Personal Identity Agent (PIA) to address privacy and availability issues in centralized systems, verifying a protocol for security against global adversaries and demonstrating feasibility via a proof-of-concept.


<details>
  <summary>Details</summary>
Motivation: Centralized identity providers create single points of failure and control, making them vulnerable to attacks on technical, organizational, or legal fronts, thus requiring a distributed solution for privacy and resilience.

Method: Composed a distributed architecture integrating sensors, identity authorities, attribute verifiers, and a novel PIA component; developed a protocol and formally verified its security properties under a realistic threat model.

Result: Proof-of-concept implementation shows the architecture and protocol are practically feasible for real-world applications with acceptable latency (a few seconds).

Conclusion: Decentralized identity systems using components like PIA can effectively mediate physical-world transactions while mitigating global threats, paving the way for resilient and privacy-preserving infrastructure.

Abstract: Digital identities are increasingly important for mediating not only digital
but also physical service transactions. Managing such identities through
centralized providers can cause both availability and privacy concerns: single
points of failure and control are ideal targets for global attacks on
technical, organizational, or legal fronts. We design, analyze, and build a
distributed digital identity architecture for physical world transactions in
common scenarios like unlocking doors, public transport, or crossing country
borders. This architecture combines (biometric and other) sensors, (established
and upcoming) identity authorities, attribute verifiers, and a new core
component we call the \emph{Personal Identity Agent (PIA)} that represents
individuals with their identity attributes in the digital domain. All
transactions are conducted in a completely decentralized manner, and the
components for which we currently assume central coordination are optional and
only used for assisting with service discovery and latency reduction. We
present a first protocol between these parties and formally verify that it
achieves relevant security properties based on a realistic threat model
including strong global adversaries. A proof-of-concept implementation
demonstrates practical feasibility of both architecture and initial protocol
for applications that can tolerate end-to-end latencies in the range of a few
seconds.

</details>


### [15] [Detecting Untargeted Attacks and Mitigating Unreliable Updates in Federated Learning for Underground Mining Operations](https://arxiv.org/abs/2508.10212)
*Md Sazedur Rahman,Mohamed Elmahallawy,Sanjay Madria,Samuel Frimpong*

Main category: cs.CR

TL;DR: MineDetect: Defense FL framework for underground mining to address privacy risks and data quality challenges while improving safety.


<details>
  <summary>Details</summary>
Motivation: Centralized transmission of raw sensor data in underground mining creates privacy risks and operational vulnerabilities. Federated Learning (FL) enables collaborative model training with data localization but faces challenges from malicious attacks and low-quality data common in mining environments.

Method: Proposes MineDetect with two innovations: 1) History-aware attack detection using gradient update comparisons between local and global averages; 2) Adversarial influence elimination mechanism for unreliable models caused by poor data quality.

Result: Simulations show MineDetect outperforms existing methods in robustness and accuracy across diverse datasets, particularly in non-IID scenarios with no significant computational overhead.

Conclusion: MineDetect provides critical security and quality improvements for FL in underground mining operations, advancing safety and decision-making while preserving data privacy.

Abstract: Underground mining operations rely on distributed sensor networks to collect
critical data daily, including mine temperature, toxic gas concentrations, and
miner movements for hazard detection and operational decision-making. However,
transmitting raw sensor data to a central server for training deep learning
models introduces significant privacy risks, potentially exposing sensitive
mine-specific information. Federated Learning (FL) offers a transformative
solution by enabling collaborative model training while ensuring that raw data
remains localized at each mine. Despite its advantages, FL in underground
mining faces key challenges: (i) An attacker may compromise a mine's local
model by employing techniques such as sign-flipping attacks or additive noise,
leading to erroneous predictions; (ii) Low-quality (yet potentially valuable)
data, caused by poor lighting conditions or sensor inaccuracies in mines may
degrade the FL training process. In response, this paper proposes MineDetect, a
defense FL framework that detects and isolates the attacked models while
mitigating the impact of mines with low-quality data. MineDetect introduces two
key innovations: (i) Detecting attacked models (maliciously manipulated) by
developing a history-aware mechanism that leverages local and global averages
of gradient updates; (ii) Identifying and eliminating adversarial influences
from unreliable models (generated by clients with poor data quality) on the FL
training process. Comprehensive simulations across diverse datasets demonstrate
that MineDetect outperforms existing methods in both robustness and accuracy,
even in challenging non-IID data scenarios. Its ability to counter adversarial
influences while maintaining lower computational efficiency makes it a vital
advancement for improving safety and operational effectiveness in underground
mining.

</details>


### [16] [BERTector: Intrusion Detection Based on Joint-Dataset Learning](https://arxiv.org/abs/2508.10327)
*Haoyang Hu,Xun Huang,Chenyu Wu,Shiwen Liu,Zhichao Lian,Shuangquan Zhang*

Main category: cs.CR

TL;DR: BERTector is a scalable BERT-based IDS framework that enhances generalization and robustness through joint-dataset training.


<details>
  <summary>Details</summary>
Motivation: Existing intrusion detection systems struggle with generalization and robustness due to network traffic heterogeneity and diverse attack patterns.

Method: BERTector integrates three components: (1) NSS-Tokenizer for traffic-aware semantic tokenization, (2) supervised fine-tuning with hybrid datasets, (3) low-rank adaptation (LoRA) for efficient training.

Result: Achieves state-of-the-art detection accuracy, strong cross-dataset generalization, and excellent robustness against adversarial perturbations.

Conclusion: Provides a unified and efficient solution for modern IDS in complex, dynamic network environments.

Abstract: Intrusion detection systems (IDS) are facing challenges in generalization and
robustness due to the heterogeneity of network traffic and the diversity of
attack patterns. To address this issue, we propose a new joint-dataset training
paradigm for IDS and propose a scalable BERTector framework based on BERT.
BERTector integrates three key components: NSS-Tokenizer for traffic-aware
semantic tokenization, supervised fine-tuning with a hybrid dataset, and
low-rank adaptation (LoRA) for efficient training. Extensive experiments show
that BERTector achieves state-of-the-art detection accuracy, strong
cross-dataset generalization capabilities, and excellent robustness to
adversarial perturbations. This work establishes a unified and efficient
solution for modern IDS in complex and dynamic network environments.

</details>


### [17] [Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based Side-Channel Attacks on Fully Associative Randomized Caches](https://arxiv.org/abs/2508.10431)
*Chris Cao,Gururaj Saileshwar*

Main category: cs.CR

TL;DR: This paper refutes previous claims that MIRAGE cache is vulnerable to occupancy-based AES key attacks due to flaws in the simulation's random seed initialization.


<details>
  <summary>Details</summary>
Motivation: Addressing reports of a purported vulnerability in MIRAGE's security against occupancy-based attacks, this paper investigates the validity of such claims through rigorous analysis.

Method: The authors replicate and critique the simulation methodology of the prior work, identifying a critical flaw in the use of a constant seed for MIRAGE's random number generator. They reconfigure the simulation to randomize the seed for each encryption run, reflecting realistic behavior.

Result: With randomized eviction seeds, the correlation between AES T-table access patterns and attacker runtime observed in the prior work disappears. The simulated attack fails to recover keys under realistic conditions.

Conclusion: The reported key leakage is an artifact of incorrect modeling, not an actual security flaw. MIRAGE's design remains secure against occupancy-based attacks when implemented correctly.

Abstract: Recent work presented at USENIX Security 2025 claims that occupancy-based
attacks can recover AES keys from the MIRAGE randomized cache. In this paper,
we examine these claims and find that they arise from fundamental modeling
flaws. Most critically, the authors' simulation of MIRAGE uses a constant seed
to initialize the random number generator used for global evictions in MIRAGE,
causing every AES encryption they trace to evict the same deterministic
sequence of cache lines. This artificially creates a highly repeatable timing
pattern that is not representative of a realistic implementation of MIRAGE,
where eviction sequences vary randomly between encryptions. When we instead
randomize the eviction seed for each run, reflecting realistic operation, the
correlation between AES T-table accesses and attacker runtimes disappears, and
the attack fails. These findings show that the reported leakage is an artifact
of incorrect modeling, and not an actual vulnerability in MIRAGE.

</details>


### [18] [AlDBaran: Towards Blazingly Fast State Commitments for Blockchains](https://arxiv.org/abs/2508.10493)
*Bernhard Kauer,Aleksandr Petrosyan,Benjamin Livshits*

Main category: cs.CR

TL;DR: AlDBaran is a high-performance authenticated database for blockchains, achieving 50 Gbps throughput and 48 million updates/s, enabling efficient state commitments and historical proofs even in resource-constrained environments.


<details>
  <summary>Details</summary>
Motivation: Existing authenticated database approaches for blockchains struggle to meet demands of multi-million TPS systems, necessitating more efficient solutions for state updates and commitments.

Method: 1) Disk I/O removal from critical path. 2) Prefetching strategies. 3) Merkle tree update mechanism refinement through architectural optimizations.

Result: AlDBaran handles 50 Gbps network throughput, 48 million updates/s on identical machines, 8 million updates/s on portable hardware in-memory, and 5 million updates/s with sub-second snapshots, surpassing prior blockchain throughput records.

Conclusion: AlDBaran provides unprecedented scalability and flexibility for blockchain authenticated databases, supporting historical state proofs while offering performance advantages in both enterprise and consumer-grade systems through its modular design.

Abstract: The fundamental basis for maintaining integrity within contemporary
blockchain systems is provided by authenticated databases. Our analysis
indicates that a significant portion of the approaches applied in this domain
fail to sufficiently meet the stringent requirements of systems processing
transactions at rates of multi-million TPS. AlDBaran signifies a substantial
advancement in authenticated databases. By eliminating disk I/O operations from
the critical path, implementing prefetching strategies, and refining the update
mechanism of the Merkle tree, we have engineered an authenticated data
structure capable of handling state updates efficiently at a network throughput
of 50 Gbps. This throughput capacity significantly surpasses any empirically
documented blockchain throughput, guaranteeing the ability of even the most
high-throughput blockchains to generate state commitments effectively.
  AlDBaran provides support for historical state proofs, which facilitates a
wide array of novel applications. For instance, the deployment of AlDBaran
could enable blockchains that do not currently support state commitments to
offer functionalities for light clients and/or implement rollups.
  When benchmarked against alternative authenticated data structure projects,
AlDBaran exhibits superior performance and simplicity. In particular, AlDBaran
achieves speeds of approximately 48 million updates per second using an
identical machine configuration. This characteristic renders AlDBaran an
attractive solution for resource-limited environments, as its historical data
capabilities can be modularly isolated (and deactivated), which further
enhances performance. On consumer-level portable hardware, it achieves
approximately 8 million updates/s in an in-memory setting and 5 million
updates/s with snapshots at sub-second intervals, illustrating compelling and
cost-effective scalability.

</details>


### [19] [Codes on any Cayley Graph have an Interactive Oracle Proof of Proximity](https://arxiv.org/abs/2508.10510)
*Hugo Delavenne,Louise Lallemand*

Main category: cs.CR

TL;DR: This paper generalizes the Flowering IOPP protocol [DMR25] from specific (2,n)-regular Tanner codes to any code with symbols indexed on Cayley graph edges, preserving low soundness and slight complexity reduction while enabling application to codes with constant rate and minimum distance.


<details>
  <summary>Details</summary>
Motivation: Prior Flowering IOPPs [DMR25] had weak o(1) minimum distance and lower soundness than FRI [BBHR18a]. Generalizing to Cayley graph codes allows maintaining strong security while using efficient codes with constant rate/distance parameters.

Method: Adapt the Flowering protocol framework to leverage Cayley graph expansion properties by encoding symbols on their edges, using their strong connectivity characteristics for proximity testing.

Result: Achieved equivalent soundness parameters with marginal complexity improvements over FRI [BCI+23] and enabled practical deployment on codes with constant relative minimum distance and coding rate.

Conclusion: Cayley graph-based IOPP generalization offers superior tradeoff between security strength and implementation efficiency compared to both traditional FRI and prior Flowering protocols, making SNARK systems more viable for constant-distance code applications.

Abstract: Interactive Oracle Proofs of Proximity (IOPP) are at the heart of code-based
SNARKs, a family of zeroknowledge protocols. The first and most famous one is
the FRI protocol [BBHR18a], that efficiently tests proximity to Reed-Solomon
codes. This paper generalizes the flowering IOPP introduced in [DMR25] for some
specific (2, n)-regular Tanner codes to a much broader variety of codes: any
code with symbols indexed on the edges of a Cayley graph. The flowering
protocol of [DMR25] had a soundness parameter much lower than the FRI protocol
[BCI + 23], and complexity parameters that could compete with the FRI
[BBHR18a]. The lower soundness and the absence of restriction on the base field
may lead to other practical speedups, however the codes considered in [DMR25]
have an o(1) minimum distance. The generalization proposed in this paper
preserves the soundness parameter with a slight decrease of the complexity
parameters, while allowing being applied on codes with constant rate and
constant minimum distance thanks to the good expansion properties of some
families of Cayley graphs.

</details>


### [20] [A Transformer-Based Approach for DDoS Attack Detection in IoT Networks](https://arxiv.org/abs/2508.10636)
*Sandipan Dey,Payal Santosh Kate,Vatsala Upadhyay,Abhishek Vaish*

Main category: cs.CR

TL;DR: The paper proposes using Transformer models for detecting DDoS attacks in IoT networks, overcoming the limitations of traditional methods through self-attention mechanisms and achieving better performance.


<details>
  <summary>Details</summary>
Motivation: Traditional DDoS detection methods struggle with IoT networks due to their resource constraints, dynamic nature, scalability of attacks, protocol diversity (e.g., MQTT, CoAP), high traffic volume, and device behavior variability.

Method: A Transformer-based model is designed to extract network traffic features and process them using self-attention mechanisms, adapting to IoT-specific challenges.

Result: Experiments on real-world IoT data demonstrate superior performance over traditional machine learning methods, with improvements in accuracy, precision, recall, and F1-score.

Conclusion: Transformer models offer an effective and scalable solution for DDoS attack detection in IoT environments, with potential for real-world deployment.

Abstract: DDoS attacks have become a major threat to the security of IoT devices and
can cause severe damage to the network infrastructure. IoT devices suffer from
the inherent problem of resource constraints and are therefore susceptible to
such resource-exhausting attacks. Traditional methods for detecting DDoS
attacks are not efficient enough to cope with the dynamic nature of IoT
networks, as well as the scalability of the attacks, diversity of protocols,
high volume of traffic, and variability in device behavior, and variability of
protocols like MQTT, CoAP, making it hard to implement security across all the
protocols. In this paper, we propose a novel approach, i.e., the use of
Transformer models, which have shown remarkable performance in natural language
processing tasks, for detecting DDoS attacks on IoT devices. The proposed model
extracts features from network traffic data and processes them using a
self-attention mechanism. Experiments conducted on a real-world dataset
demonstrate that the proposed approach outperforms traditional machine learning
techniques, which can be validated by comparing both approaches' accuracy,
precision, recall, and F1-score. The results of this study show that the
Transformer models can be an effective solution for detecting DDoS attacks on
IoT devices and have the potential to be deployed in real-world IoT
environments.

</details>


### [21] [MirGuard: Towards a Robust Provenance-based Intrusion Detection System Against Graph Manipulation Attacks](https://arxiv.org/abs/2508.10639)
*Anyuan Sang,Lu Zhou,Li Yang,Junbo Jia,Huipeng Yang,Pengbin Feng,Jianfeng Ma*

Main category: cs.CR

TL;DR: MirGuard is a robust anomaly detection framework that enhances Learning-based Provenance-based Intrusion Detection Systems (PIDSes) against graph manipulation attacks by using Logic-Aware Noise Injection (LNI) and contrastive representation learning, achieving better robustness while maintaining detection performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Learning-based PIDSes are vulnerable to graph manipulation attacks that exploit structural perturbations, limiting their practical applicability despite their ability to detect unknown attacks. Existing approaches lack a robust detection solution for this issue.

Method: MirGuard introduces Logic-Aware Noise Injection (LNI) to generate semantically valid graph views and employs a Logic-Preserving Contrastive Learning framework to learn representations invariant to benign transformations but sensitive to adversarial inconsistencies.

Result: Comprehensive evaluations showed MirGuard significantly outperforms state-of-the-art detectors in robustness against various graph manipulation attacks, retaining detection performance and efficiency.

Conclusion: MirGuard provides the first targeted and effective solution to enhance PIDSes against adversarial graph manipulation attacks, offering a secure and efficient framework for modern cybersecurity challenges.

Abstract: Learning-based Provenance-based Intrusion Detection Systems (PIDSes) have
become essential tools for anomaly detection in host systems due to their
ability to capture rich contextual and structural information, as well as their
potential to detect unknown attacks. However, recent studies have shown that
these systems are vulnerable to graph manipulation attacks, where attackers
manipulate the graph structure to evade detection. While some previous
approaches have discussed this type of attack, none have fully addressed it
with a robust detection solution, limiting the practical applicability of
PIDSes.
  To address this challenge, we propose MirGuard, a robust anomaly detection
framework that combines logic-aware multi-view augmentation with contrastive
representation learning. Rather than applying arbitrary structural
perturbations, MirGuard introduces Logic-Aware Noise Injection (LNI) to
generate semantically valid graph views, ensuring that all augmentations
preserve the underlying causal semantics of the provenance data. These views
are then used in a Logic-Preserving Contrastive Learning framework, which
encourages the model to learn representations that are invariant to benign
transformations but sensitive to adversarial inconsistencies. Comprehensive
evaluations on multiple provenance datasets demonstrate that MirGuard
significantly outperforms state-of-the-art detectors in robustness against
various graph manipulation attacks without sacrificing detection performance
and efficiency. Our work represents the first targeted study to enhance PIDS
against such adversarial threats, providing a robust and effective solution to
modern cybersecurity challenges.

</details>


### [22] [A Novel Study on Intelligent Methods and Explainable AI for Dynamic Malware Analysis](https://arxiv.org/abs/2508.10652)
*Richa Dasila,Vatsala Upadhyay,Samo Bobek,Abhishek Vaish*

Main category: cs.CR

TL;DR: This paper investigates the integration of Explainable AI (XAI) with deep learning models to enhance transparency and trustworthiness in malware detection, particularly focusing on dynamic analysis of metamorphic malware using MLP, CNN, RNN, and CNN-LSTM architectures.


<details>
  <summary>Details</summary>
Motivation: Deep learning models are powerful for threat detection but lack transparency (black-box nature), creating trust issues and hindering adoption in cybersecurity. Dynamic malware analysis, especially for metamorphic malware, requires interpretable models to understand detection rationale.

Method: The study evaluates four deep learning architectures (MLP, CNN, RNN, CNN-LSTM) for malware classification in dynamic analysis contexts, applying XAI techniques to analyze their decision-making processes and interpretability effectiveness.

Result: Demonstrates how XAI techniques can be applied to these models to enhance transparency while maintaining effectiveness in detecting metamorphic malware, with comparative results on model performance and explainability metrics

Conclusion: Integrating XAI approaches with deep learning models significantly improves interpretability without compromising effectiveness, making them more trustworthy for cybersecurity applications by elucidating their malware detection mechanisms.

Abstract: Deep learning models are one of the security strategies, trained on extensive
datasets, and play a critical role in detecting and responding to these threats
by recognizing complex patterns in malicious code. However, the opaque nature
of these models-often described as "black boxes"-makes their decision-making
processes difficult to understand, even for their creators. This research
addresses these challenges by integrating Explainable AI (XAI) techniques to
enhance the interpretability and trustworthiness of malware detection models.
In this research, the use of Multi-Layer Perceptrons (MLP) for dynamic malware
analysis has been considered, a less explored area, and its efficacy in
detecting Metamorphic Malware, and further the effectiveness and transparency
of MLPs, CNNs, RNNs, and CNN-LSTM models in malware classification, evaluating
these models through the lens of Explainable AI (XAI). This comprehensive
approach aims to demystify the internal workings of deep learning models,
promoting a better understanding and trust in their predictive capabilities in
cybersecurity contexts. Such in-depth analysis and implementation haven't been
done to the best of our knowledge.

</details>


### [23] [Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat Intelligence](https://arxiv.org/abs/2508.10677)
*Amine Tellache,Abdelaziz Amara Korba,Amdjed Mokhtari,Horea Moldovan,Yacine Ghamri-Doudane*

Main category: cs.CR

TL;DR: A RAG-based LLM framework enhances incident response by automating CTI integration, improving alert accuracy, reducing analyst workload, and addressing alert fatigue via context-aware enrichment.


<details>
  <summary>Details</summary>
Motivation: Security teams face alert fatigue, high false-positive rates, and challenges in manually analyzing fragmented CTI documents. The goal is to automate and enrich incident response (IR) processes to improve efficiency and accuracy.

Method: The approach uses a hybrid retrieval mechanism combining NLP-based similarity searches in a CTI vector database and standardized queries to external CTI platforms. An LLM then generates precise mitigation strategies using the enriched CTI.

Result: Empirical validation shows improved accuracy, contextualization, and efficiency in IR through real-world and simulated alerts, with reduced response latency and analyst workload.

Conclusion: LLM-driven CTI fusion demonstrates potential for advancing autonomous security operations and building intelligent, adaptive cybersecurity frameworks, validated through a dual expert-LLM evaluation paradigm.

Abstract: Effective incident response (IR) is critical for mitigating cyber threats,
yet security teams are overwhelmed by alert fatigue, high false-positive rates,
and the vast volume of unstructured Cyber Threat Intelligence (CTI) documents.
While CTI holds immense potential for enriching security operations, its
extensive and fragmented nature makes manual analysis time-consuming and
resource-intensive. To bridge this gap, we introduce a novel
Retrieval-Augmented Generation (RAG)-based framework that leverages Large
Language Models (LLMs) to automate and enhance IR by integrating dynamically
retrieved CTI. Our approach introduces a hybrid retrieval mechanism that
combines NLP-based similarity searches within a CTI vector database with
standardized queries to external CTI platforms, facilitating context-aware
enrichment of security alerts. The augmented intelligence is then leveraged by
an LLM-powered response generation module, which formulates precise,
actionable, and contextually relevant incident mitigation strategies. We
propose a dual evaluation paradigm, wherein automated assessment using an
auxiliary LLM is systematically cross-validated by cybersecurity experts.
Empirical validation on real-world and simulated alerts demonstrates that our
approach enhances the accuracy, contextualization, and efficiency of IR,
alleviating analyst workload and reducing response latency. This work
underscores the potential of LLM-driven CTI fusion in advancing autonomous
security operations and establishing a foundation for intelligent, adaptive
cybersecurity frameworks.

</details>


### [24] [Searching for Privacy Risks in LLM Agents via Simulation](https://arxiv.org/abs/2508.10880)
*Yanzhe Zhang,Diyi Yang*

Main category: cs.CR

TL;DR: This paper proposes a search-based framework to identify and enhance privacy vulnerabilities in LLM-based agents through simulated multi-role interactions, showing escalation in both attack tactics (direct requests → impersonation/consent forgery) and defense mechanisms (rule-based → identity-verification state machines).


<details>
  <summary>Details</summary>
Motivation: Dynamic multi-turn dialogues by malicious LLM agents can extract sensitive information in unpredictable ways, making manual detection of sophisticated vulnerabilities impractical.

Method: Alternating attacker/defender instruction improvements via multi-threaded simulations with three roles (data subject, sender, recipient). Uses LLMs as optimizers with parallel search and cross-thread propagation.

Result: Attack strategies progress from basic to advanced tactics while defenses evolve correspondingly; transferability of findings across scenarios and models validated.

Conclusion: The framework enables systematic exploration of interactive privacy threats and defenses, providing practical tools to build privacy-aware agents through adaptive simulation-based discovery.

Abstract: The widespread deployment of LLM-based agents is likely to introduce a
critical privacy threat: malicious agents that proactively engage others in
multi-turn interactions to extract sensitive information. These dynamic
dialogues enable adaptive attack strategies that can cause severe privacy
violations, yet their evolving nature makes it difficult to anticipate and
discover sophisticated vulnerabilities manually. To tackle this problem, we
present a search-based framework that alternates between improving attacker and
defender instructions by simulating privacy-critical agent interactions. Each
simulation involves three roles: data subject, data sender, and data recipient.
While the data subject's behavior is fixed, the attacker (data recipient)
attempts to extract sensitive information from the defender (data sender)
through persistent and interactive exchanges. To explore this interaction space
efficiently, our search algorithm employs LLMs as optimizers, using parallel
search with multiple threads and cross-thread propagation to analyze simulation
trajectories and iteratively propose new instructions. Through this process, we
find that attack strategies escalate from simple direct requests to
sophisticated multi-turn tactics such as impersonation and consent forgery,
while defenses advance from rule-based constraints to identity-verification
state machines. The discovered attacks and defenses transfer across diverse
scenarios and backbone models, demonstrating strong practical utility for
building privacy-aware agents.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [25] [FormalGrad: Integrating Formal Methods with Gradient-Based LLM Refinement](https://arxiv.org/abs/2508.10059)
*Yueke Zhang,Yifan Zhang,Kevin Leach,Yu Huang*

Main category: cs.SE

TL;DR: FormalGrad is a framework that integrates formal methods into LLM-based code generation by treating code as a differentiable variable, converting constraints into textual gradients for iterative refinement, achieving significant improvements in correctness and robustness over existing baselines.


<details>
  <summary>Details</summary>
Motivation: The limitations of current LLMs in code generation, where they produce code lacking guarantees of correctness, robustness, and efficiency, especially in domains with strict constraints, motivate the development of a system that enforces formal verification during generation.

Method: FormalGrad iteratively treats code as a differentiable variable, generates structured textual pseudo-gradients from formal constraints and verification feedback, and uses these gradients to guide the LLM in refining code solutions through multiple passes of gradient-directed adjustment.

Result: The framework achieves 27% absolute improvement on HumanEval and 41% relative improvement on LiveCodeBench V6 benchmarks. Generated code demonstrates formal proofs of correctness while maintaining functionality, robustness, and efficiency.

Conclusion: FormalGrad demonstrates that formal methods can be practically integrated into LLM code generation, providing verifiable correctness guarantees and significantly better performance on constrained programming tasks compared to baseline approaches.

Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities
in code generation, they often produce solutions that lack guarantees of
correctness, robustness, and efficiency. The limitation is acute in domains
requiring strict constraints. FormalGrad introduces a principled framework that
integrates formal methods directly into an iterative LLM-based generation loop.
It uniquely treats code as a differentiable variable, converting structured
feedback and formal constraints into a textual pseudo-gradient. This gradient
guides the model to iteratively refine solutions, ensuring they are not only
functional but also robust and formally justified. We evaluate FormalGrad on
the HumanEval, HumanEval+, and LiveCodeBench benchmarks. Our implementation
outperforms strong baselines, achieving an absolute improvement of up to 27% on
HumanEval and a 41% relative improvement on the challenging LiveCodeBench V6.
FormalGrad generates formally justified code that is robust and efficient,
paving the way for reliable AI-assisted software development in high-stakes
applications.

</details>


### [26] [SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion](https://arxiv.org/abs/2508.10068)
*Xiaohan Chen,Zhongying Pan,Quan Feng,Yu Tian,Shuqun Yang,Mengru Wang,Lina Gong,Yuxia Geng,Piji Li,Xiang Chen*

Main category: cs.SE

TL;DR: Saracoder introduces a Hierarchical Feature-Optimized retrieval framework for code completion, addressing semantic misalignment, redundancy, and cross-file symbol ambiguity through semantic distillation, structural similarity metrics, and reranking.


<details>
  <summary>Details</summary>
Motivation: Current RAG-based code completion methods rely on superficial text similarity, leading to issues like semantic misguidance, redundancy, homogeneity, and inability to resolve external symbol ambiguity at the repository scale.

Method: Saracoder’s approach combines two modules: (1) Hierarchical Feature Optimization, which refines retrieval candidates by extracting deep semantic relationships, removing duplicates, calculating structural similarity via a topological edit-weighted graph metric, and diversity-aware reranking; (2) External-Aware Identifier Disambiguator, which analyzes dependencies to resolve cross-file symbol ambiguities. These components synergistically enhance retrieval quality at both code-level and repository-level granularities.

Result: Experiments on CrossCodeEval and RepoEval-Updated benchmarks show Saracoder outperforms existing RAG baselines across multiple programming languages and foundation models, achieving higher accuracy and robustness in repository-scale code generation.

Conclusion: Systematic multi-dimensional refinement of retrieval results—capturing semantic depth, structural diversity, and cross-repository dependencies—provides a novel, effective paradigm for building repository-level code completion systems that address critical limitations of text-similarity-based methods.

Abstract: Retrieval-augmented generation (RAG) for repository-level code completion
commonly relies on superficial text similarity, leading to results plagued by
semantic misguidance, redundancy, and homogeneity, while also failing to
resolve external symbol ambiguity. To address these challenges, we introduce
Saracoder, a Hierarchical Feature-Optimized retrieval framework. Its core
Hierarchical Feature Optimization module systematically refines candidates by
distilling deep semantic relationships, pruning exact duplicates, assessing
structural similarity with a novel graph-based metric that weighs edits by
their topological importance, and reranking results to maximize both relevance
and diversity. Furthermore, an External-Aware Identifier Disambiguator module
accurately resolves cross-file symbol ambiguity via dependency analysis.
Extensive experiments on the challenging CrossCodeEval and RepoEval-Updated
benchmarks demonstrate that Saracoder significantly outperforms existing
baselines across multiple programming languages and models. Our work proves
that systematically refining retrieval results across multiple dimensions
provides a new paradigm for building more accurate and robust repository-level
code completion systems.

</details>


### [27] [Next Edit Prediction: Learning to Predict Code Edits from Context and Interaction History](https://arxiv.org/abs/2508.10074)
*Ruofan Lu,Yintong Huo,Meng Zhang,Yichen Li,Michael R. Lyu*

Main category: cs.SE

TL;DR: This paper introduces Next Edit Prediction, a task to proactively predict both the location and content of a developer's subsequent code edit by learning from interaction history. It presents a dataset, benchmark, and evaluation of fine-tuned models that enable anticipatory collaboration between AI and developers.


<details>
  <summary>Details</summary>
Motivation: Existing AI coding assistants either focus on low-latency position-bound suggestions or require context-switching to describe intent via chat-based editing, both failing to proactively predict sequences of related edits. This limits seamless integration into developer workflows.

Method: The authors curate a supervised fine-tuning dataset and evaluation benchmark for Next Edit Prediction. They perform systematic model fine-tuning and compare their approach against baseline models through comprehensive experimentation to validate the task's feasibility.

Result: The evaluations reveal novel findings about model performance in predicting sequential code edits, establishing benchmark metrics and demonstrating potential improvements in edit prediction accuracy through the proposed framework.

Conclusion: The work establishes a foundational framework for proactive code interaction, where coding assistants can collaboratively anticipate and suggest code modifications rather than merely reacting to explicit developer commands, enabling more natural and efficient development workflows.

Abstract: The rapid advancement of large language models (LLMs) has led to the
widespread adoption of AI-powered coding assistants integrated into a
development environment. On one hand, low-latency code completion offers
completion suggestions but is fundamentally constrained to the cursor's current
position. On the other hand, chat-based editing can perform complex
modifications, yet forces developers to stop their work, describe the intent in
natural language, which causes a context-switch away from the code. This
creates a suboptimal user experience, as neither paradigm proactively predicts
the developer's next edit in a sequence of related edits. To bridge this gap
and provide the seamless code edit suggestion, we introduce the task of Next
Edit Prediction, a novel task designed to infer developer intent from recent
interaction history to predict both the location and content of the subsequent
edit. Specifically, we curate a high-quality supervised fine-tuning dataset and
an evaluation benchmark for the Next Edit Prediction task. Then, we conduct
supervised fine-tuning on a series of models and performed a comprehensive
evaluation of both the fine-tuned models and other baseline models, yielding
several novel findings. This work lays the foundation for a new interaction
paradigm that proactively collaborate with developers by anticipating their
following action, rather than merely reacting to explicit instructions.

</details>


### [28] [On the synchronization between Hugging Face pre-trained language models and their upstream GitHub repository](https://arxiv.org/abs/2508.10157)
*Ajibode Adekunle,Abdul Ali Bangash,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: The paper studies coordination challenges in pretrained language model development across GitHub and Hugging Face, identifying eight synchronization patterns that reveal structural disconnects leading to model fragmentation.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address misaligned release timelines, inconsistent versioning, and limited PTLM variant reuse by analyzing how commit activities are coordinated between upstream and downstream platforms.

Method: A mixed-method analysis of 325 PTLM families (904 HF variants) examined commit activities through three dimensions: synchronization lag, type, and intensity, uncovering eight distinct patterns

Result: Identified patterns show issues like isolated changes (non-replicated improvements/fixes) and platform abandonment, exposing users to incomplete/outdated models due to cross-platform coordination failures

Conclusion: Recognition of these synchronization patterns is critical for improving PTLM release workflows through better oversight, traceability, and platform interoperability to mitigate model fragmentation risks

Abstract: Pretrained language models (PTLMs) have advanced natural language processing
(NLP), enabling progress in tasks like text generation and translation. Like
software package management, PTLMs are trained using code and environment
scripts in upstream repositories (e.g., GitHub, GH) and distributed as variants
via downstream platforms like Hugging Face (HF). Coordinating development
between GH and HF poses challenges such as misaligned release timelines,
inconsistent versioning, and limited reuse of PTLM variants. We conducted a
mixed-method study of 325 PTLM families (904 HF variants) to examine how commit
activities are coordinated. Our analysis reveals that GH contributors typically
make changes related to specifying the version of the model, improving code
quality, performance optimization, and dependency management within the
training scripts, while HF contributors make changes related to improving model
descriptions, data set handling, and setup required for model inference.
Furthermore, to understand the synchronization aspects of commit activities
between GH and HF, we examined three dimensions of these activities -- lag
(delay), type of synchronization, and intensity -- which together yielded eight
distinct synchronization patterns. The prevalence of partially synchronized
patterns, such as Disperse synchronization and Sparse synchronization, reveals
structural disconnects in current cross-platform release practices. These
patterns often result in isolated changes -- where improvements or fixes made
on one platform are never replicated on the other -- and in some cases,
indicate an abandonment of one repository in favor of the other. Such
fragmentation risks exposing end users to incomplete, outdated, or behaviorally
inconsistent models. Hence, recognizing these synchronization patterns is
critical for improving oversight and traceability in PTLM release workflows.

</details>


### [29] [Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart Contract Compilation Error Resolution](https://arxiv.org/abs/2508.10517)
*Likai Ye,Mengliang Li,Dehai Zhao,Jiamou Sun,Xiaoxue Ren*

Main category: cs.SE

TL;DR: This paper analyzes challenges in Solidity version migration, evaluates LLM effectiveness for error repair, and proposes SMCFIXER—a framework combining expert knowledge retrieval with LLMs—which achieves 96.97% accuracy and 24.24% improvement over GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Frequent Solidity updates cause widespread compilation errors, migration difficulties, and maintenance issues, necessitating reliable error repair solutions for smart contract evolution.

Method: Conducted empirical studies on version migration errors (81.68% error rate) and evaluated open/closed-source LLMs (LLaMA3, DeepSeek, GPT-4o) for error repair. Introduced SMCFIXER with context-aware slicing, expert knowledge retrieval, and iterative patch generation.

Result: LLMs show limited effectiveness (~13% accuracy improvement) but strong dependence on prompt engineering. SMCFIXER achieved 96.97% accuracy, demonstrating 24.24% improvement over GPT-4o baselines in real-world solidit}

Conclusion: Domain-specific adaptation is critical for LLM-based Solidity error repair. SMCFIXER provides a statistically significant solution through expert knowledge integration and structured framework design.

Abstract: Solidity, the dominant smart contract language for Ethereum, has rapidly
evolved with frequent version updates to enhance security, functionality, and
developer experience. However, these continual changes introduce significant
challenges, particularly in compilation errors, code migration, and
maintenance. Therefore, we conduct an empirical study to investigate the
challenges in the Solidity version evolution and reveal that 81.68% of examined
contracts encounter errors when compiled across different versions, with 86.92%
of compilation errors.
  To mitigate these challenges, we conducted a systematic evaluation of large
language models (LLMs) for resolving Solidity compilation errors during version
migrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek)
and closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these
models exhibit error repair capabilities, their effectiveness diminishes
significantly for semantic-level issues and shows strong dependency on prompt
engineering strategies. This underscores the critical need for domain-specific
adaptation in developing reliable LLM-based repair systems for smart contracts.
  Building upon these insights, we introduce SMCFIXER, a novel framework that
systematically integrates expert knowledge retrieval with LLM-based repair
mechanisms for Solidity compilation error resolution. The architecture
comprises three core phases: (1) context-aware code slicing that extracts
relevant error information; (2) expert knowledge retrieval from official
documentation; and (3) iterative patch generation for Solidity migration.
Experimental validation across Solidity version migrations demonstrates our
approach's statistically significant 24.24% improvement over baseline GPT-4o on
real-world datasets, achieving near-perfect 96.97% accuracy.

</details>


### [30] [EVOSCAT: Exploring Software Change Dynamics in Large-Scale Historical Datasets](https://arxiv.org/abs/2508.10852)
*Souhaila Serbout,Diana Carolina Muñoz Hurtado,Hassan Atwi,Edoardo Riggio,Cesare Pautasso*

Main category: cs.SE

TL;DR: EvoScat is a tool for scalable visualization of software evolution data using interactive density scatterplots, enabling analysis of temporal patterns and artifact characteristics across large datasets.


<details>
  <summary>Details</summary>
Motivation: Researchers need scalable tools to analyze vast historical datasets in software evolution, enabling exploration of artifact aging rates and metric trends that traditional methods cannot handle efficiently.

Method: The authors designed EvoScat with interactive density scatterplots for global overview, and implemented flexible configurations for temporal alignment, artifact sorting, and color mapping to support various analysis tasks on mined datasets.

Result: EvoScat successfully visualizes millions of events from tens of thousands of software artifacts, supported by a gallery of real-world datasets (OpenAPI, GitHub workflows) and case studies on popular projects.

Conclusion: EvoScat provides an effective solution for exploring and comparing software artifact histories at scale, supporting key analysis goals like pace assessment, clone detection, and freshness evaluation in open source projects.

Abstract: Long lived software projects encompass a large number of artifacts, which
undergo many revisions throughout their history. Empirical software engineering
researchers studying software evolution gather and collect datasets with
millions of events, representing changes introduced to specific artifacts. In
this paper, we propose EvoScat, a tool that attempts addressing temporal
scalability through the usage of interactive density scatterplot to provide a
global overview of large historical datasets mined from open source
repositories in a single visualization. EvoScat intents to provide researchers
with a mean to produce scalable visualizations that can help them explore and
characterize evolution datasets, as well as comparing the histories of
individual artifacts, both in terms of 1) observing how rapidly different
artifacts age over multiple-year-long time spans 2) how often metrics
associated with each artifacts tend towards an improvement or worsening. The
paper shows how the tool can be tailored to specific analysis needs (pace of
change comparison, clone detection, freshness assessment) thanks to its support
for flexible configuration of history scaling and alignment along the time
axis, artifacts sorting and interactive color mapping, enabling the analysis of
millions of events obtained by mining the histories of tens of thousands of
software artifacts. We include in this paper a gallery showcasing datasets
gathering specific artifacts (OpenAPI descriptions, GitHub workflow
definitions) across multiple repositories, as well as diving into the history
of specific popular open source projects.

</details>
