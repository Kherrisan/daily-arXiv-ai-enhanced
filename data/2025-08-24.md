<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 4]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [MCPTox: A Benchmark for Tool Poisoning Attack on Real-World MCP Servers](https://arxiv.org/abs/2508.14925)
*Zhiqiang Wang,Yichao Gao,Yanting Wang,Suyuan Liu,Haifeng Sun,Haoran Cheng,Guanquan Shi,Haohua Du,Xiangyang Li*

Main category: cs.CR

TL;DR: Introduces MCPTox, the first benchmark to evaluate LLM agents' vulnerabilities to Tool Poisoning through metadata in MCP environments, revealing widespread susceptibility with 72.8% success rate for o1-mini.


<details>
  <summary>Details</summary>
Motivation: Prior work focused on external tool output injections; this paper investigates a more fundamental vulnerability—Tool Poisoning via metadata—and addresses the lack of systematic, large-scale evaluation.

Method: Constructed MCPTox using 45 real-world MCP servers and 353 tools, generating 1.3k malicious test cases across 10 risk categories via 3 attack templates and few-shot learning. Evaluated 20 leading LLM agents.

Result: All 20 LLM agents demonstrated vulnerability to Tool Poisoning, with o1-mini achieving highest success rate (72.8%). More capable models showed greater susceptibility, and only 3% maximum rejection rate among agents.

Conclusion: Establishes critical empirical baseline for Tool Poisoning vulnerabilities, showing existing safety measures are insufficient against metadata-based attacks. Releases open dataset to advance verification of safer agent systems.

Abstract: By providing a standardized interface for LLM agents to interact with
external tools, the Model Context Protocol (MCP) is quickly becoming a
cornerstone of the modern autonomous agent ecosystem. However, it creates novel
attack surfaces due to untrusted external tools. While prior work has focused
on attacks injected through external tool outputs, we investigate a more
fundamental vulnerability: Tool Poisoning, where malicious instructions are
embedded within a tool's metadata without execution. To date, this threat has
been primarily demonstrated through isolated cases, lacking a systematic,
large-scale evaluation.
  We introduce MCPTox, the first benchmark to systematically evaluate agent
robustness against Tool Poisoning in realistic MCP settings. MCPTox is
constructed upon 45 live, real-world MCP servers and 353 authentic tools. To
achieve this, we design three distinct attack templates to generate a
comprehensive suite of 1312 malicious test cases by few-shot learning, covering
10 categories of potential risks. Our evaluation on 20 prominent LLM agents
setting reveals a widespread vulnerability to Tool Poisoning, with o1-mini,
achieving an attack success rate of 72.8\%. We find that more capable models
are often more susceptible, as the attack exploits their superior
instruction-following abilities. Finally, the failure case analysis reveals
that agents rarely refuse these attacks, with the highest refused rate
(Claude-3.7-Sonnet) less than 3\%, demonstrating that existing safety alignment
is ineffective against malicious actions that use legitimate tools for
unauthorized operation. Our findings create a crucial empirical baseline for
understanding and mitigating this widespread threat, and we release MCPTox for
the development of verifiably safer AI agents. Our dataset is available at an
anonymized repository: \textit{https://anonymous.4open.science/r/AAAI26-7C02}.

</details>


### [2] [A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives](https://arxiv.org/abs/2508.15031)
*Kaixiang Zhao,Lincan Li,Kaize Ding,Neil Zhenqiang Gong,Yue Zhao,Yushun Dong*

Main category: cs.CR

TL;DR: This paper surveys Model Extraction Attacks on MLaaS platforms, introduces a taxonomy, and analyzes attack mechanisms, defenses, and implications for AI security and policy.


<details>
  <summary>Details</summary>
Motivation: Machine-Learning-as-a-Service (MLaaS) platforms have made ML accessible but introduced vulnerabilities like MEAs. This paper addresses the lack of systematic analysis on MEAs, their defenses, and their broader implications, aiming to guide researchers and policymakers in AI security.

Method: The authors propose a taxonomy classifying MEAs by attack mechanisms, defense approaches, and computing environments. They analyze attack techniques, evaluate defenses, and discuss technical, ethical, legal, and societal implications. The work synthesizes existing literature and identifies gaps in addressing the utility-security trade-off.

Result: The survey identifies critical challenges in existing defenses (e.g., balancing utility vs. security) and provides technical and societal insights into MEAs across computing paradigms. It curates an ongoing online repository for literature updates.

Conclusion: The paper provides a comprehensive survey of Model Extraction Attacks (MEAs) and defense strategies, offering a novel taxonomy to classify attacks, evaluate their effectiveness, and highlight challenges in balancing model utility with security. It serves as a reference for AI security research and policy while maintaining an updated online repository for future work.

Abstract: Machine learning (ML) models have significantly grown in complexity and
utility, driving advances across multiple domains. However, substantial
computational resources and specialized expertise have historically restricted
their wide adoption. Machine-Learning-as-a-Service (MLaaS) platforms have
addressed these barriers by providing scalable, convenient, and affordable
access to sophisticated ML models through user-friendly APIs. While this
accessibility promotes widespread use of advanced ML capabilities, it also
introduces vulnerabilities exploited through Model Extraction Attacks (MEAs).
Recent studies have demonstrated that adversaries can systematically replicate
a target model's functionality by interacting with publicly exposed interfaces,
posing threats to intellectual property, privacy, and system security. In this
paper, we offer a comprehensive survey of MEAs and corresponding defense
strategies. We propose a novel taxonomy that classifies MEAs according to
attack mechanisms, defense approaches, and computing environments. Our analysis
covers various attack techniques, evaluates their effectiveness, and highlights
challenges faced by existing defenses, particularly the critical trade-off
between preserving model utility and ensuring security. We further assess MEAs
within different computing paradigms and discuss their technical, ethical,
legal, and societal implications, along with promising directions for future
research. This systematic survey aims to serve as a valuable reference for
researchers, practitioners, and policymakers engaged in AI security and
privacy. Additionally, we maintain an online repository continuously updated
with related literature at https://github.com/kzhao5/ModelExtractionPapers.

</details>


### [3] [MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs](https://arxiv.org/abs/2508.15036)
*Ruyi Ding,Tianhong Xu,Xinyi Shen,Aidong Adam Ding,Yunsi Fei*

Main category: cs.CR

TL;DR: MoEcho reveals critical privacy vulnerabilities in MoE-based transformers by exploiting hardware execution traces, enabling novel side-channel attacks that reconstruct sensitive user inputs. This work calls for urgent security measures to protect data in scalable AI systems.


<details>
  <summary>Details</summary>
Motivation: The increasing adoption of MoE architectures in transformers has created a new attack surface for privacy breaches, as adaptive routing mechanisms leave exploitable temporal and spatial traces in hardware execution.

Method: The authors propose MoEcho, a side-channel analysis framework that introduces four novel architectural side channels (Cache Occupancy Channels, Pageout+Reload, Performance Counter, and TLB Evict+Reload) to exploit input-dependent activation patterns in MoE architectures.

Result: MoEcho enables four privacy-breach attacks (Prompt Inference, Response Reconstruction, Visual Inference, and Visual Reconstruction) against LLMs and VLMs based on MoE architectures, demonstrating the first runtime architecture-level security analysis of this design.

Conclusion: This paper highlights the serious security and privacy threats posed by MoE-based models, urging the development of effective safeguards to protect sensitive user data during the deployment of large-scale AI systems.

Abstract: The transformer architecture has become a cornerstone of modern AI, fueling
remarkable progress across applications in natural language processing,
computer vision, and multimodal learning. As these models continue to scale
explosively for performance, implementation efficiency remains a critical
challenge. Mixture of Experts (MoE) architectures, selectively activating
specialized subnetworks (experts), offer a unique balance between model
accuracy and computational cost. However, the adaptive routing in MoE
architectures, where input tokens are dynamically directed to specialized
experts based on their semantic meaning inadvertently opens up a new attack
surface for privacy breaches. These input-dependent activation patterns leave
distinctive temporal and spatial traces in hardware execution, which
adversaries could exploit to deduce sensitive user data. In this work, we
propose MoEcho, discovering a side channel analysis based attack surface that
compromises user privacy on MoE based systems. Specifically, in MoEcho, we
introduce four novel architectural side channels on different computing
platforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and
Performance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting
these vulnerabilities, we propose four attacks that effectively breach user
privacy in large language models (LLMs) and vision language models (VLMs) based
on MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,
Visual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first
runtime architecture level security analysis of the popular MoE structure
common in modern transformers, highlighting a serious security and privacy
threat and calling for effective and timely safeguards when harnessing MoE
based models for developing efficient large scale AI services.

</details>


### [4] [When Machine Learning Meets Vulnerability Discovery: Challenges and Lessons Learned](https://arxiv.org/abs/2508.15042)
*Sima Arasteh,Christophe Hauser*

Main category: cs.CR

TL;DR: This paper identifies key challenges in applying machine learning to software vulnerability detection (e.g., dataset limitations and model choices) and offers actionable insights from their prior works to improve future methods.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to resolve critical gaps in machine-learning-based vulnerability detection, particularly the lack of dataset standardization and uncertainty about training validity, which hinder reliable evaluation and progress in the field.

Method: The authors analyze challenges through critical evaluation of existing methods and provide insights derived from their prior works (Bin2vec and BinHunter) to inform future research.

Result: The analysis reveals that current evaluation practices lack transparency in dataset statistics and model granularity, while insights from Bin2vec and BinHunter suggest pathways to address these limitations.

Conclusion: The paper highlights the need to address current shortcomings in evaluating machine-learning approaches for vulnerability detection by improving dataset transparency and model training practices, while leveraging insights from Bin2vec and BinHunter to guide future research.

Abstract: In recent years, machine learning has demonstrated impressive results in
various fields, including software vulnerability detection. Nonetheless, using
machine learning to identify software vulnerabilities presents new challenges,
especially regarding the scale of data involved, which was not a factor in
traditional methods. Consequently, in spite of the rise of new
machine-learning-based approaches in that space, important shortcomings persist
regarding their evaluation. First, researchers often fail to provide concrete
statistics about their training datasets, such as the number of samples for
each type of vulnerability. Moreover, many methods rely on training with
semantically similar functions rather than directly on vulnerable programs.
This leads to uncertainty about the suitability of the datasets currently used
for training. Secondly, the choice of a model and the level of granularity at
which models are trained also affect the effectiveness of such vulnerability
discovery approaches.
  In this paper, we explore the challenges of applying machine learning to
vulnerability discovery. We also share insights from our two previous research
papers, Bin2vec and BinHunter, which could enhance future research in this
field.

</details>
