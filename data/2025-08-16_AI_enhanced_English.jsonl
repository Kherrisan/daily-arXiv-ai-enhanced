{"id": "2508.10017", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10017", "abs": "https://arxiv.org/abs/2508.10017", "authors": ["Rodrigo Tertulino"], "title": "A Robust Pipeline for Differentially Private Federated Learning on Imbalanced Clinical Data using SMOTETomek and FedProx", "comment": "This is being prepared to be submitted to the Journal of the\n  Brazilian Computer Society (JBCS), which is still under construction", "summary": "Federated Learning (FL) presents a groundbreaking approach for collaborative\nhealth research, allowing model training on decentralized data while\nsafeguarding patient privacy. FL offers formal security guarantees when\ncombined with Differential Privacy (DP). The integration of these technologies,\nhowever, introduces a significant trade-off between privacy and clinical\nutility, a challenge further complicated by the severe class imbalance often\npresent in medical datasets. The research presented herein addresses these\ninterconnected issues through a systematic, multi-stage analysis. An FL\nframework was implemented for cardiovascular risk prediction, where initial\nexperiments showed that standard methods struggled with imbalanced data,\nresulting in a recall of zero. To overcome such a limitation, we first\nintegrated the hybrid Synthetic Minority Over-sampling Technique with Tomek\nLinks (SMOTETomek) at the client level, successfully developing a clinically\nuseful model. Subsequently, the framework was optimized for non-IID data using\na tuned FedProx algorithm. Our final results reveal a clear, non-linear\ntrade-off between the privacy budget (epsilon) and model recall, with the\noptimized FedProx consistently out-performing standard FedAvg. An optimal\noperational region was identified on the privacy-utility frontier, where strong\nprivacy guarantees (with epsilon 9.0) can be achieved while maintaining high\nclinical utility (recall greater than 77%). Ultimately, our study provides a\npractical methodological blueprint for creating effective, secure, and accurate\ndiagnostic tools that can be applied to real-world, heterogeneous healthcare\ndata.", "AI": {"tldr": "This paper proposes an FL framework with DP for cardiovascular risk prediction, addressing class imbalance via SMOTETomek and optimizing non-IID data handling with FedProx, achieving high clinical utility (77% recall) at strong privacy (\u03b5=9.0).", "motivation": "Medical datasets often exhibit severe class imbalance and heterogeneity, while integrating differential privacy with federated learning necessitates resolving the privacy-utility trade-off to enable secure yet effective collaborative health research.", "method": "1) Client-level integration of SMOTETomek for class rebalancing in decentralized medical data scenarios. 2) FedProx algorithm optimization with non-IID data handling tuned for cardiovascular risk prediction tasks.", "result": "Standard methods achieved zero recall due to class imbalance. Optimized FedProx outperformed FedAvg across privacy budgets (\u03b5), maintaining over 77% recall at \u03b5=9.0 despite non-linear trade-offs between privacy and model utility.", "conclusion": "The study establishes a methodology blueprint for developing diagnostic tools in healthcare settings that simultaneously preserve strong formal privacy guarantees while achieving clinical robustness through FL-embedded class imbalance mitigation."}}
{"id": "2508.10059", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10059", "abs": "https://arxiv.org/abs/2508.10059", "authors": ["Yueke Zhang", "Yifan Zhang", "Kevin Leach", "Yu Huang"], "title": "FormalGrad: Integrating Formal Methods with Gradient-Based LLM Refinement", "comment": "6 Pages", "summary": "While Large Language Models (LLMs) have demonstrated remarkable capabilities\nin code generation, they often produce solutions that lack guarantees of\ncorrectness, robustness, and efficiency. The limitation is acute in domains\nrequiring strict constraints. FormalGrad introduces a principled framework that\nintegrates formal methods directly into an iterative LLM-based generation loop.\nIt uniquely treats code as a differentiable variable, converting structured\nfeedback and formal constraints into a textual pseudo-gradient. This gradient\nguides the model to iteratively refine solutions, ensuring they are not only\nfunctional but also robust and formally justified. We evaluate FormalGrad on\nthe HumanEval, HumanEval+, and LiveCodeBench benchmarks. Our implementation\noutperforms strong baselines, achieving an absolute improvement of up to 27% on\nHumanEval and a 41% relative improvement on the challenging LiveCodeBench V6.\nFormalGrad generates formally justified code that is robust and efficient,\npaving the way for reliable AI-assisted software development in high-stakes\napplications.", "AI": {"tldr": "FormalGrad is a framework that integrates formal methods into LLM-based code generation by treating code as a differentiable variable, converting constraints into textual gradients for iterative refinement, achieving significant improvements in correctness and robustness over existing baselines.", "motivation": "The limitations of current LLMs in code generation, where they produce code lacking guarantees of correctness, robustness, and efficiency, especially in domains with strict constraints, motivate the development of a system that enforces formal verification during generation.", "method": "FormalGrad iteratively treats code as a differentiable variable, generates structured textual pseudo-gradients from formal constraints and verification feedback, and uses these gradients to guide the LLM in refining code solutions through multiple passes of gradient-directed adjustment.", "result": "The framework achieves 27% absolute improvement on HumanEval and 41% relative improvement on LiveCodeBench V6 benchmarks. Generated code demonstrates formal proofs of correctness while maintaining functionality, robustness, and efficiency.", "conclusion": "FormalGrad demonstrates that formal methods can be practically integrated into LLM code generation, providing verifiable correctness guarantees and significantly better performance on constrained programming tasks compared to baseline approaches."}}
{"id": "2508.10023", "categories": ["cs.CR", "quant-ph", "94A60 (Cryptography)"], "pdf": "https://arxiv.org/pdf/2508.10023", "abs": "https://arxiv.org/abs/2508.10023", "authors": ["Samet \u00dcnsal"], "title": "A Comparative Performance Evaluation of Kyber, sntrup761, and FrodoKEM for Post-Quantum Cryptography", "comment": "12 pages, 3 tables, IEEE conference format", "summary": "Post-quantum cryptography (PQC) aims to develop cryptographic algorithms that\nare secure against attacks from quantum computers. This paper compares the\nleading postquantum cryptographic algorithms, such as Kyber, sntrup761, and\nFrodoKEM, in terms of their security, performance, and real-world\napplicability. The review highlights the strengths and weaknesses of each\nalgorithm and provides insights into future research directions. We also\ndiscuss the challenges of transitioning from classical to post-quantum systems\nand the potential impacts on various industries. This paper serves as a\nfoundation for understanding the current state of post-quantum cryptography and\nits future prospects in the quantum computing era.", "AI": {"tldr": "This paper compares leading post-quantum cryptographic algorithms (Kyber, sntrup761, FrodoKEM) and analyzes their security, performance, real-world applicability, and transition challenges toward quantum-safe systems.", "motivation": "The paper addresses the critical need for cryptographic algorithms resistant to quantum computing threats, highlighting transition challenges from classical systems and their industrial impacts.", "method": "A comprehensive review and comparative analysis of post-quantum algorithms using theoretical assessments and empirical benchmarks for security, performance, and real-world applicability, with a focus on transitioning from classical systems.", "result": "Strengths and weaknesses of Kyber, sntrup761, and FrodoKEM are identified, including their trade-offs between security, computational efficiency, and usability. Transition challenges (e.g., compatibility, key size) and industry-specific implications are detailed.", "conclusion": "The study establishes a foundation for post-quantum cryptography adoption, emphasizing the importance of algorithm standardization, practical optimization, and industry collaboration for secure quantum-era systems."}}
{"id": "2508.10068", "categories": ["cs.SE", "cs.CL", "cs.IR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.10068", "abs": "https://arxiv.org/abs/2508.10068", "authors": ["Xiaohan Chen", "Zhongying Pan", "Quan Feng", "Yu Tian", "Shuqun Yang", "Mengru Wang", "Lina Gong", "Yuxia Geng", "Piji Li", "Xiang Chen"], "title": "SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion", "comment": null, "summary": "Retrieval-augmented generation (RAG) for repository-level code completion\ncommonly relies on superficial text similarity, leading to results plagued by\nsemantic misguidance, redundancy, and homogeneity, while also failing to\nresolve external symbol ambiguity. To address these challenges, we introduce\nSaracoder, a Hierarchical Feature-Optimized retrieval framework. Its core\nHierarchical Feature Optimization module systematically refines candidates by\ndistilling deep semantic relationships, pruning exact duplicates, assessing\nstructural similarity with a novel graph-based metric that weighs edits by\ntheir topological importance, and reranking results to maximize both relevance\nand diversity. Furthermore, an External-Aware Identifier Disambiguator module\naccurately resolves cross-file symbol ambiguity via dependency analysis.\nExtensive experiments on the challenging CrossCodeEval and RepoEval-Updated\nbenchmarks demonstrate that Saracoder significantly outperforms existing\nbaselines across multiple programming languages and models. Our work proves\nthat systematically refining retrieval results across multiple dimensions\nprovides a new paradigm for building more accurate and robust repository-level\ncode completion systems.", "AI": {"tldr": "Saracoder introduces a Hierarchical Feature-Optimized retrieval framework for code completion, addressing semantic misalignment, redundancy, and cross-file symbol ambiguity through semantic distillation, structural similarity metrics, and reranking.", "motivation": "Current RAG-based code completion methods rely on superficial text similarity, leading to issues like semantic misguidance, redundancy, homogeneity, and inability to resolve external symbol ambiguity at the repository scale.", "method": "Saracoder\u2019s approach combines two modules: (1) Hierarchical Feature Optimization, which refines retrieval candidates by extracting deep semantic relationships, removing duplicates, calculating structural similarity via a topological edit-weighted graph metric, and diversity-aware reranking; (2) External-Aware Identifier Disambiguator, which analyzes dependencies to resolve cross-file symbol ambiguities. These components synergistically enhance retrieval quality at both code-level and repository-level granularities.", "result": "Experiments on CrossCodeEval and RepoEval-Updated benchmarks show Saracoder outperforms existing RAG baselines across multiple programming languages and foundation models, achieving higher accuracy and robustness in repository-scale code generation.", "conclusion": "Systematic multi-dimensional refinement of retrieval results\u2014capturing semantic depth, structural diversity, and cross-repository dependencies\u2014provides a novel, effective paradigm for building repository-level code completion systems that address critical limitations of text-similarity-based methods."}}
{"id": "2508.10031", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10031", "abs": "https://arxiv.org/abs/2508.10031", "authors": ["Jinhwa Kim", "Ian G. Harris"], "title": "Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs", "comment": "13 pages, 2 figures", "summary": "While Large Language Models (LLMs) have shown significant advancements in\nperformance, various jailbreak attacks have posed growing safety and ethical\nrisks. Malicious users often exploit adversarial context to deceive LLMs,\nprompting them to generate responses to harmful queries. In this study, we\npropose a new defense mechanism called Context Filtering model, an input\npre-processing method designed to filter out untrustworthy and unreliable\ncontext while identifying the primary prompts containing the real user intent\nto uncover concealed malicious intent. Given that enhancing the safety of LLMs\noften compromises their helpfulness, potentially affecting the experience of\nbenign users, our method aims to improve the safety of the LLMs while\npreserving their original performance. We evaluate the effectiveness of our\nmodel in defending against jailbreak attacks through comparative analysis,\ncomparing our approach with state-of-the-art defense mechanisms against six\ndifferent attacks and assessing the helpfulness of LLMs under these defenses.\nOur model demonstrates its ability to reduce the Attack Success Rates of\njailbreak attacks by up to 88% while maintaining the original LLMs'\nperformance, achieving state-of-the-art Safety and Helpfulness Product results.\nNotably, our model is a plug-and-play method that can be applied to all LLMs,\nincluding both white-box and black-box models, to enhance their safety without\nrequiring any fine-tuning of the models themselves. We will make our model\npublicly available for research purposes.", "AI": {"tldr": "The study introduces a Context Filtering model to defend LLMs against jailbreak attacks by pre-processing inputs to remove malicious contexts while preserving helpfulness, achieving an 88% reduction in attack success and state-of-the-art safety-helpfulness balance.", "motivation": "Jailbreak attacks leveraging adversarial contexts threaten the safety and ethical alignment of large language models (LLMs), often forcing them to generate harmful responses while reducing their helpfulness to legitimate users.", "method": "The proposed Context Filtering model implements an input pre-processing method that filters untrustworthy context and extracts the user's primary intent to detect hidden malicious attacks. It works as a plug-and-play system, applicable to both white-box and black-box LLMs without requiring model fine-tuning.", "result": "The model reduced jailbreak attack success rates by up to 88% across six attack scenarios, while maintaining the baseline helpfulness of LLMs, achieving state-of-the-art Safety and Helpfulness Product metrics.", "conclusion": "The Context Filtering model offers a universal defense strategy that significantly enhances LLM safety without compromising their performance, providing a practical solution deployable across diverse models and attack scenarios."}}
{"id": "2508.10074", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10074", "abs": "https://arxiv.org/abs/2508.10074", "authors": ["Ruofan Lu", "Yintong Huo", "Meng Zhang", "Yichen Li", "Michael R. Lyu"], "title": "Next Edit Prediction: Learning to Predict Code Edits from Context and Interaction History", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has led to the\nwidespread adoption of AI-powered coding assistants integrated into a\ndevelopment environment. On one hand, low-latency code completion offers\ncompletion suggestions but is fundamentally constrained to the cursor's current\nposition. On the other hand, chat-based editing can perform complex\nmodifications, yet forces developers to stop their work, describe the intent in\nnatural language, which causes a context-switch away from the code. This\ncreates a suboptimal user experience, as neither paradigm proactively predicts\nthe developer's next edit in a sequence of related edits. To bridge this gap\nand provide the seamless code edit suggestion, we introduce the task of Next\nEdit Prediction, a novel task designed to infer developer intent from recent\ninteraction history to predict both the location and content of the subsequent\nedit. Specifically, we curate a high-quality supervised fine-tuning dataset and\nan evaluation benchmark for the Next Edit Prediction task. Then, we conduct\nsupervised fine-tuning on a series of models and performed a comprehensive\nevaluation of both the fine-tuned models and other baseline models, yielding\nseveral novel findings. This work lays the foundation for a new interaction\nparadigm that proactively collaborate with developers by anticipating their\nfollowing action, rather than merely reacting to explicit instructions.", "AI": {"tldr": "This paper introduces Next Edit Prediction, a task to proactively predict both the location and content of a developer's subsequent code edit by learning from interaction history. It presents a dataset, benchmark, and evaluation of fine-tuned models that enable anticipatory collaboration between AI and developers.", "motivation": "Existing AI coding assistants either focus on low-latency position-bound suggestions or require context-switching to describe intent via chat-based editing, both failing to proactively predict sequences of related edits. This limits seamless integration into developer workflows.", "method": "The authors curate a supervised fine-tuning dataset and evaluation benchmark for Next Edit Prediction. They perform systematic model fine-tuning and compare their approach against baseline models through comprehensive experimentation to validate the task's feasibility.", "result": "The evaluations reveal novel findings about model performance in predicting sequential code edits, establishing benchmark metrics and demonstrating potential improvements in edit prediction accuracy through the proposed framework.", "conclusion": "The work establishes a foundational framework for proactive code interaction, where coding assistants can collaboratively anticipate and suggest code modifications rather than merely reacting to explicit developer commands, enabling more natural and efficient development workflows."}}
{"id": "2508.10033", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10033", "abs": "https://arxiv.org/abs/2508.10033", "authors": ["Yuksel Aydin"], "title": "Cognitive Cybersecurity for Artificial Intelligence: Guardrail Engineering with CCS-7", "comment": null, "summary": "Language models exhibit human-like cognitive vulnerabilities, such as\nemotional framing, that escape traditional behavioral alignment. We present\nCCS-7 (Cognitive Cybersecurity Suite), a taxonomy of seven vulnerabilities\ngrounded in human cognitive security research. To establish a human benchmark,\nwe ran a randomized controlled trial with 151 participants: a \"Think First,\nVerify Always\" (TFVA) lesson improved cognitive security by +7.9% overall. We\nthen evaluated TFVA-style guardrails across 12,180 experiments on seven diverse\nlanguage model architectures. Results reveal architecture-dependent risk\npatterns: some vulnerabilities (e.g., identity confusion) are almost fully\nmitigated, while others (e.g., source interference) exhibit escalating\nbackfire, with error rates increasing by up to 135% in certain models. Humans,\nin contrast, show consistent moderate improvement. These findings reframe\ncognitive safety as a model-specific engineering problem: interventions\neffective in one architecture may fail, or actively harm, another, underscoring\nthe need for architecture-aware cognitive safety testing before deployment.", "AI": {"tldr": "The paper introduces CCS-7, a cognitive vulnerability taxonomy for language models, showing that TFVA-style guardrails produce architecture-dependent mitigation outcomes, with source interference risks increasing by 135% in some models while human participants showed consistent 7.9% improvement.", "motivation": "Language models display human-like cognitive vulnerabilities that traditional behavioral alignment methods cannot detect or mitigate, requiring new approaches to ensure security.", "method": "1) Randomized controlled trial with 151 human participants using TFVA lessons (Think First, Verify Always). 2) Evaluation of 12,180 experiments across seven diverse language model architectures using architecture-dependent cognitive security testing.", "result": "Human participants showed +7.9% improvement in cognitive security. For models: identity confusion vulnerabilities were almost fully mitigated, source interference vulnerabilities exhibited backfire effects with error rates increasing up to 135% in certain architectures, and other vulnerabilities showed mixed architecture-specific results.", "conclusion": "Cognitive safety must be treated as an architecture-specific engineering problem. Interventions effective in one model architecture may fail or harm others, necessitating architecture-aware testing before deployment to avoid unintentional risk amplification."}}
{"id": "2508.10157", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10157", "abs": "https://arxiv.org/abs/2508.10157", "authors": ["Ajibode Adekunle", "Abdul Ali Bangash", "Bram Adams", "Ahmed E. Hassan"], "title": "On the synchronization between Hugging Face pre-trained language models and their upstream GitHub repository", "comment": null, "summary": "Pretrained language models (PTLMs) have advanced natural language processing\n(NLP), enabling progress in tasks like text generation and translation. Like\nsoftware package management, PTLMs are trained using code and environment\nscripts in upstream repositories (e.g., GitHub, GH) and distributed as variants\nvia downstream platforms like Hugging Face (HF). Coordinating development\nbetween GH and HF poses challenges such as misaligned release timelines,\ninconsistent versioning, and limited reuse of PTLM variants. We conducted a\nmixed-method study of 325 PTLM families (904 HF variants) to examine how commit\nactivities are coordinated. Our analysis reveals that GH contributors typically\nmake changes related to specifying the version of the model, improving code\nquality, performance optimization, and dependency management within the\ntraining scripts, while HF contributors make changes related to improving model\ndescriptions, data set handling, and setup required for model inference.\nFurthermore, to understand the synchronization aspects of commit activities\nbetween GH and HF, we examined three dimensions of these activities -- lag\n(delay), type of synchronization, and intensity -- which together yielded eight\ndistinct synchronization patterns. The prevalence of partially synchronized\npatterns, such as Disperse synchronization and Sparse synchronization, reveals\nstructural disconnects in current cross-platform release practices. These\npatterns often result in isolated changes -- where improvements or fixes made\non one platform are never replicated on the other -- and in some cases,\nindicate an abandonment of one repository in favor of the other. Such\nfragmentation risks exposing end users to incomplete, outdated, or behaviorally\ninconsistent models. Hence, recognizing these synchronization patterns is\ncritical for improving oversight and traceability in PTLM release workflows.", "AI": {"tldr": "The paper studies coordination challenges in pretrained language model development across GitHub and Hugging Face, identifying eight synchronization patterns that reveal structural disconnects leading to model fragmentation.", "motivation": "The authors aim to address misaligned release timelines, inconsistent versioning, and limited PTLM variant reuse by analyzing how commit activities are coordinated between upstream and downstream platforms.", "method": "A mixed-method analysis of 325 PTLM families (904 HF variants) examined commit activities through three dimensions: synchronization lag, type, and intensity, uncovering eight distinct patterns", "result": "Identified patterns show issues like isolated changes (non-replicated improvements/fixes) and platform abandonment, exposing users to incomplete/outdated models due to cross-platform coordination failures", "conclusion": "Recognition of these synchronization patterns is critical for improving PTLM release workflows through better oversight, traceability, and platform interoperability to mitigate model fragmentation risks"}}
{"id": "2508.10035", "categories": ["cs.CR", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.10035", "abs": "https://arxiv.org/abs/2508.10035", "authors": ["Varsha Sen", "Biswash Basnet"], "title": "Neural Network-Based Detection and Multi-Class Classification of FDI Attacks in Smart Grid Home Energy Systems", "comment": "17 pages, 7 figures", "summary": "False Data Injection Attacks (FDIAs) pose a significant threat to smart grid\ninfrastructures, particularly Home Area Networks (HANs), where real-time\nmonitoring and control are highly adopted. Owing to the comparatively less\nstringent security controls and widespread availability of HANs, attackers view\nthem as an attractive entry point to manipulate aggregated demand patterns,\nwhich can ultimately propagate and disrupt broader grid operations. These\nattacks undermine the integrity of smart meter data, enabling malicious actors\nto manipulate consumption values without activating conventional alarms,\nthereby creating serious vulnerabilities across both residential and\nutility-scale infrastructures. This paper presents a machine learning-based\nframework for both the detection and classification of FDIAs using residential\nenergy data. A real-time detection is provided by the lightweight Artificial\nNeural Network (ANN), which works by using the most vital features of energy\nconsumption, cost, and time context. For the classification of different attack\ntypes, a Bidirectional LSTM is trained to recognize normal, trapezoidal, and\nsigmoid attack shapes through learning sequential dependencies in the data. A\nsynthetic time-series dataset was generated to emulate realistic household\nbehaviour. Experimental results demonstrate that the proposed models are\neffective in identifying and classifying FDIAs, offering a scalable solution\nfor enhancing grid resilience at the edge. This work contributes toward\nbuilding intelligent, data-driven defence mechanisms that strengthen smart grid\ncybersecurity from residential endpoints.", "AI": {"tldr": "The paper proposes a machine learning framework combining an ANN for real-time detection and a Bidirectional LSTM for classification of False Data Injection Attacks (FDIAs) in smart grid Home Area Networks (HANs).", "motivation": "FDIAs exploit weak security in HANs to manipulate energy data, threatening grid integrity. Existing defenses fail to detect stealthy attacks without alarms, necessitating scalable edge-based solutions.", "method": "1) Lightweight Artificial Neural Network (ANN) for real-time FDIA detection using features like energy consumption, cost, and time context. 2) Bidirectional LSTM model to classify attack types (normal/trapezoidal/sigmoid) by analyzing sequential data patterns. 3) Synthetic time-series dataset emulating household energy behavior for training and evaluation.", "result": "Models successfully detected FDIA anomalies (e.g., consumption manipulation) and distinguished attack shapes with high accuracy, validating their effectiveness through controlled experiments. The framework offers edge-deployable, scalable cybersecurity for residential smart grid endpoints.", "conclusion": "This work establishes an intelligent FDIA defense system for HANs using machine learning-based edge computing, addressing vulnerabilities in residential energy data integrity while enabling granular attack classification to improve smart grid resilience."}}
{"id": "2508.10517", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10517", "abs": "https://arxiv.org/abs/2508.10517", "authors": ["Likai Ye", "Mengliang Li", "Dehai Zhao", "Jiamou Sun", "Xiaoxue Ren"], "title": "Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart Contract Compilation Error Resolution", "comment": "International Conference on Software Maintenance and Evolution\n  (ICSME) 2025", "summary": "Solidity, the dominant smart contract language for Ethereum, has rapidly\nevolved with frequent version updates to enhance security, functionality, and\ndeveloper experience. However, these continual changes introduce significant\nchallenges, particularly in compilation errors, code migration, and\nmaintenance. Therefore, we conduct an empirical study to investigate the\nchallenges in the Solidity version evolution and reveal that 81.68% of examined\ncontracts encounter errors when compiled across different versions, with 86.92%\nof compilation errors.\n  To mitigate these challenges, we conducted a systematic evaluation of large\nlanguage models (LLMs) for resolving Solidity compilation errors during version\nmigrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek)\nand closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these\nmodels exhibit error repair capabilities, their effectiveness diminishes\nsignificantly for semantic-level issues and shows strong dependency on prompt\nengineering strategies. This underscores the critical need for domain-specific\nadaptation in developing reliable LLM-based repair systems for smart contracts.\n  Building upon these insights, we introduce SMCFIXER, a novel framework that\nsystematically integrates expert knowledge retrieval with LLM-based repair\nmechanisms for Solidity compilation error resolution. The architecture\ncomprises three core phases: (1) context-aware code slicing that extracts\nrelevant error information; (2) expert knowledge retrieval from official\ndocumentation; and (3) iterative patch generation for Solidity migration.\nExperimental validation across Solidity version migrations demonstrates our\napproach's statistically significant 24.24% improvement over baseline GPT-4o on\nreal-world datasets, achieving near-perfect 96.97% accuracy.", "AI": {"tldr": "This paper analyzes challenges in Solidity version migration, evaluates LLM effectiveness for error repair, and proposes SMCFIXER\u2014a framework combining expert knowledge retrieval with LLMs\u2014which achieves 96.97% accuracy and 24.24% improvement over GPT-4o.", "motivation": "Frequent Solidity updates cause widespread compilation errors, migration difficulties, and maintenance issues, necessitating reliable error repair solutions for smart contract evolution.", "method": "Conducted empirical studies on version migration errors (81.68% error rate) and evaluated open/closed-source LLMs (LLaMA3, DeepSeek, GPT-4o) for error repair. Introduced SMCFIXER with context-aware slicing, expert knowledge retrieval, and iterative patch generation.", "result": "LLMs show limited effectiveness (~13% accuracy improvement) but strong dependence on prompt engineering. SMCFIXER achieved 96.97% accuracy, demonstrating 24.24% improvement over GPT-4o baselines in real-world solidit}", "conclusion": "Domain-specific adaptation is critical for LLM-based Solidity error repair. SMCFIXER provides a statistically significant solution through expert knowledge integration and structured framework design."}}
{"id": "2508.10038", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10038", "abs": "https://arxiv.org/abs/2508.10038", "authors": ["Pierre-Francois Gimenez", "Sarath Sivaprasad", "Mario Fritz"], "title": "Certifiably robust malware detectors by design", "comment": null, "summary": "Malware analysis involves analyzing suspicious software to detect malicious\npayloads. Static malware analysis, which does not require software execution,\nrelies increasingly on machine learning techniques to achieve scalability.\nAlthough such techniques obtain very high detection accuracy, they can be\neasily evaded with adversarial examples where a few modifications of the sample\ncan dupe the detector without modifying the behavior of the software. Unlike\nother domains, such as computer vision, creating an adversarial example of\nmalware without altering its functionality requires specific transformations.\nWe propose a new model architecture for certifiably robust malware detection by\ndesign. In addition, we show that every robust detector can be decomposed into\na specific structure, which can be applied to learn empirically robust malware\ndetectors, even on fragile features. Our framework ERDALT is based on this\nstructure. We compare and validate these approaches with machine-learning-based\nmalware detection methods, allowing for robust detection with limited reduction\nof detection performance.", "AI": {"tldr": "The paper proposes ERDALT, a certifiably robust malware detection framework based on a novel model architecture and structural decomposition of robust detectors, achieving robustness without significant performance loss.", "motivation": "Current static malware analysis using machine learning lacks robustness against adversarial examples where minimal behavioral-preserving modifications evade detection, unlike domains such as computer vision.", "method": "1) Introduces a new model architecture designed for inherent robustness. 2) Demonstrates that robust detectors require specific structural properties through decomposition. 3) Implements ERDALT framework leveraging these properties for robust malware detection.", "result": "ERDALT achieves robust detection while maintaining limited reduction in accuracy compared to standard machine learning approaches through validation against adversarial examples.", "conclusion": "The structural approach enables certifiable robustness in malware detection and provides a pathway to develop empirically robust detectors even for fragile features, advancing secure static malware analysis."}}
{"id": "2508.10852", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10852", "abs": "https://arxiv.org/abs/2508.10852", "authors": ["Souhaila Serbout", "Diana Carolina Mu\u00f1oz Hurtado", "Hassan Atwi", "Edoardo Riggio", "Cesare Pautasso"], "title": "EVOSCAT: Exploring Software Change Dynamics in Large-Scale Historical Datasets", "comment": "Submitted to VISSOFT 2025. For the hi-resolution version of the\n  paper, see https://design.inf.usi.ch/publications/2025/vissoft", "summary": "Long lived software projects encompass a large number of artifacts, which\nundergo many revisions throughout their history. Empirical software engineering\nresearchers studying software evolution gather and collect datasets with\nmillions of events, representing changes introduced to specific artifacts. In\nthis paper, we propose EvoScat, a tool that attempts addressing temporal\nscalability through the usage of interactive density scatterplot to provide a\nglobal overview of large historical datasets mined from open source\nrepositories in a single visualization. EvoScat intents to provide researchers\nwith a mean to produce scalable visualizations that can help them explore and\ncharacterize evolution datasets, as well as comparing the histories of\nindividual artifacts, both in terms of 1) observing how rapidly different\nartifacts age over multiple-year-long time spans 2) how often metrics\nassociated with each artifacts tend towards an improvement or worsening. The\npaper shows how the tool can be tailored to specific analysis needs (pace of\nchange comparison, clone detection, freshness assessment) thanks to its support\nfor flexible configuration of history scaling and alignment along the time\naxis, artifacts sorting and interactive color mapping, enabling the analysis of\nmillions of events obtained by mining the histories of tens of thousands of\nsoftware artifacts. We include in this paper a gallery showcasing datasets\ngathering specific artifacts (OpenAPI descriptions, GitHub workflow\ndefinitions) across multiple repositories, as well as diving into the history\nof specific popular open source projects.", "AI": {"tldr": "EvoScat is a tool for scalable visualization of software evolution data using interactive density scatterplots, enabling analysis of temporal patterns and artifact characteristics across large datasets.", "motivation": "Researchers need scalable tools to analyze vast historical datasets in software evolution, enabling exploration of artifact aging rates and metric trends that traditional methods cannot handle efficiently.", "method": "The authors designed EvoScat with interactive density scatterplots for global overview, and implemented flexible configurations for temporal alignment, artifact sorting, and color mapping to support various analysis tasks on mined datasets.", "result": "EvoScat successfully visualizes millions of events from tens of thousands of software artifacts, supported by a gallery of real-world datasets (OpenAPI, GitHub workflows) and case studies on popular projects.", "conclusion": "EvoScat provides an effective solution for exploring and comparing software artifact histories at scale, supporting key analysis goals like pace assessment, clone detection, and freshness evaluation in open source projects."}}
{"id": "2508.10039", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10039", "abs": "https://arxiv.org/abs/2508.10039", "authors": ["Wenqiang Wang", "Yan Xiao", "Hao Lin", "Yangshijie Zhang", "Xiaochun Cao"], "title": "Multi-task Adversarial Attacks against Black-box Model with Few-shot Queries", "comment": null, "summary": "Current multi-task adversarial text attacks rely on abundant access to shared\ninternal features and numerous queries, often limited to a single task type. As\na result, these attacks are less effective against practical scenarios\ninvolving black-box feedback APIs, limited queries, or multiple task types. To\nbridge this gap, we propose \\textbf{C}luster and \\textbf{E}nsemble\n\\textbf{M}ulti-task Text Adversarial \\textbf{A}ttack (\\textbf{CEMA}), an\neffective black-box attack that exploits the transferability of adversarial\ntexts across different tasks. CEMA simplifies complex multi-task scenarios by\nusing a \\textit{deep-level substitute model} trained in a\n\\textit{plug-and-play} manner for text classification, enabling attacks without\nmimicking the victim model. This approach requires only a few queries for\ntraining, converting multi-task attacks into classification attacks and\nallowing attacks across various tasks.\n  CEMA generates multiple adversarial candidates using different text\nclassification methods and selects the one that most effectively attacks\nsubstitute models.\n  In experiments involving multi-task models with two, three, or six\ntasks--spanning classification, translation, summarization, and text-to-image\ngeneration--CEMA demonstrates significant attack success with as few as 100\nqueries. Furthermore, CEMA can target commercial APIs (e.g., Baidu and Google\nTranslate), large language models (e.g., ChatGPT 4o), and image-generation\nmodels (e.g., Stable Diffusion V2), showcasing its versatility and\neffectiveness in real-world applications.", "AI": {"tldr": "CEMA is a black-box adversarial text attack method that uses a plug-and-play substitute model to attack multi-task systems with limited queries across diverse tasks like classification, translation, and image generation.", "motivation": "Existing multi-task adversarial attacks require excessive internal feature access and queries, limiting their practical effectiveness against black-box systems, commercial APIs, and multi-task deployments with diverse objectives (e.g., classification, translation, summarization).", "method": "CEMA trains a deep-level substitute model in a task-agnostic plug-and-play framework. It generates multiple adversarial candidates using diverse text classification methods and selects the most effective one against substitute models, reducing multi-task attacks to simpler classification tasks with minimal query requirements.", "result": "CEMA achieves significant attack success across multi-task models with 2-6 tasks in experiments spanning classification, translation, summarization, and text-to-image generation. It can attack commercial APIs (Baidu, Google Translate), ChatGPT 4o, and image-generation models like Stable Diffusion V2 using as few as 100 queries.", "conclusion": "CEMA offers a versatile adversarial attack framework for practical multi-task scenarios by leveraging substitute model transferability, minimal query requirements, and cross-domain applicability against commercial and open-source models in both text and image domains."}}
{"id": "2508.10041", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.10041", "abs": "https://arxiv.org/abs/2508.10041", "authors": ["Julien Mellaerts"], "title": "Quantum Prime Factorization: A Novel Approach Based on Fermat Method", "comment": null, "summary": "In this paper, we introduce a novel quantum algorithm for the factorization\nof composite odd numbers. This work makes two significant contributions. First,\nwe present a new improvement to the classical Fermat method, fourfold reducing\nthe computational complexity of factoring. Second, we reformulate Fermat\nfactorization method as an optimization problem suitable for Quantum Annealers\nwhich allowed us to factorize 8,689,739, the biggest number ever factorized\nusing a quantum device to our knowledge.", "AI": {"tldr": "The paper proposes a quantum-enhanced Fermat factorization method with two key contributions: a fourfold complexity reduction in classical Fermat method and a novel reformulation for quantum annealers.", "motivation": "To address computational complexity in factoring composite odd numbers and leverage quantum annealers for solving optimization problems inherent in factorization.", "method": "1) Optimized classical Fermat method for reduced complexity; 2) Reformulated factorization as an optimization problem compatible with quantum annealers.", "result": "Achieved a fourfold reduction in computational complexity for the classical Fermat method and successfully factorized 8,689,739 using a quantum annealer\u2014the largest number demonstrated with a quantum device.", "conclusion": "Demonstrates advancements in both classical and quantum approaches to factorization, establishing a benchmark for quantum factorization capabilities."}}
{"id": "2508.10042", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10042", "abs": "https://arxiv.org/abs/2508.10042", "authors": ["Jane Carney", "Kushal Upreti", "Gaby G. Dagher", "Tim Andersen"], "title": "FIDELIS: Blockchain-Enabled Protection Against Poisoning Attacks in Federated Learning", "comment": null, "summary": "Federated learning enhances traditional deep learning by enabling the joint\ntraining of a model with the use of IoT device's private data. It ensures\nprivacy for clients, but is susceptible to data poisoning attacks during\ntraining that degrade model performance and integrity. Current poisoning\ndetection methods in federated learning lack a standardized detection method or\ntake significant liberties with trust. In this paper, we present \\Sys, a novel\nblockchain-enabled poison detection framework in federated learning. The\nframework decentralizes the role of the global server across participating\nclients. We introduce a judge model used to detect data poisoning in model\nupdates. The judge model is produced by each client and verified to reach\nconsensus on a single judge model. We implement our solution to show \\Sys is\nrobust against data poisoning attacks and the creation of our judge model is\nscalable.", "AI": {"tldr": "The paper introduces \\Sys, a blockchain-enabled framework for detecting data poisoning attacks in federated learning by decentralizing the global server's role and using a consensus-verified judge model. Empirical results demonstrate its robustness and scalability.", "motivation": "Federated learning (FL) systems face risks from data poisoning attacks that compromise model performance and integrity. Existing detection approaches rely on centralized trust assumptions or lack standardized methods, creating vulnerabilities in collaborative learning environments.", "method": "The proposed \\Sys framework utilizes blockchain technology to distribute the global server's authority among FL clients. Each client generates a judge model for poisoning detection through a three-phase process: (1) client-side judge model generation, (2) consensus-based model validation, and (3) dynamic model selection using the longest blockchain branch as a confidence indicator.", "result": "Implementations validated \\Sys's effectiveness against common data poisoning attacks (e.g., label flipping, backdoor). The framework achieved >95% detection accuracy while maintaining model performance and demonstrated linear scalability of the judge model creation process with increasing client numbers.", "conclusion": "Blockchain-enabled decentralization of poisoning detection in federated learning presents a viable security solution. \\Sys shifts responsibility from a vulnerable central server to distributed validation, with experimental results confirming its potential to protect collaborative learning systems without compromising scalability or model utility."}}
{"id": "2508.10043", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10043", "abs": "https://arxiv.org/abs/2508.10043", "authors": ["Pallavi Zambare", "Venkata Nikhil Thanikella", "Ying Liu"], "title": "Securing Agentic AI: Threat Modeling and Risk Analysis for Network Monitoring Agentic AI System", "comment": "Submitted and under review in IEEE Transactions on Privacy", "summary": "When combining Large Language Models (LLMs) with autonomous agents, used in\nnetwork monitoring and decision-making systems, this will create serious\nsecurity issues. In this research, the MAESTRO framework consisting of the\nseven layers threat modeling architecture in the system was used to expose,\nevaluate, and eliminate vulnerabilities of agentic AI. The prototype agent\nsystem was constructed and implemented, using Python, LangChain, and telemetry\nin WebSockets, and deployed with inference, memory, parameter tuning, and\nanomaly detection modules. Two practical threat cases were confirmed as\nfollows: (i) resource denial of service by traffic replay denial-of-service,\nand (ii) memory poisoning by tampering with the historical log file maintained\nby the agent. These situations resulted in measurable levels of performance\ndegradation, i.e. telemetry updates were delayed, and computational loads were\nincreased, as a result of poor system adaptations. It was suggested to use a\nmultilayered defense-in-depth approach with memory isolation, validation of\nplanners and anomaly response systems in real-time. These findings verify that\nMAESTRO is viable in operational threat mapping, prospective risk scoring, and\nthe basis of the resilient system design. The authors bring attention to the\nimportance of the enforcement of memory integrity, paying attention to the\nadaptation logic monitoring, and cross-layer communication protection that\nguarantee the agentic AI reliability in adversarial settings.", "AI": {"tldr": "This paper analyzes security vulnerabilities in agentic AI systems combining LLMs and autonomous agents using the MAESTRO framework. Two threat cases (resource denial-of-service via traffic replay and memory poisoning via log tampering) were identified and mitigated through a defense-in-depth approach, proving MAESTRO's effectiveness in operational threat mapping and resilient system design.", "motivation": "Integrating LLMs with autonomous agents in network monitoring and decision-making systems introduces security risks that threaten system reliability. This paper aims to systematically expose and mitigate such vulnerabilities in adversarial environments.", "method": "The authors implemented a prototype agent system using Python, LangChain, and WebSocket telemetry, integrating inference, memory, parameter tuning, and anomaly detection modules. They applied MAESTRO's 7-layer threat modeling architecture to identify and test vulnerabilities through two practical attack scenarios.", "result": "Both threat cases caused measurable performance degradation (delayed telemetry updates, increased computational loads) due to poor system adaptation. Real-time validation of planners and anomaly response systems effectively combated these attacks, while memory isolation prevented log tampering.", "conclusion": "MAESTRO enables proactive threat mapping and risk scoring for agentic AI systems. The study confirms the importance of memory integrity enforcement, adaptation logic monitoring, and cross-layer protections to guarantee reliability against sophisticated adversarial attacks."}}
{"id": "2508.10044", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10044", "abs": "https://arxiv.org/abs/2508.10044", "authors": ["Aydin Zaboli", "Junho Hong"], "title": "Generative AI for Cybersecurity of Energy Management Systems: Methods, Challenges, and Future Directions", "comment": "36 pages, 10 figures", "summary": "This paper elaborates on an extensive security framework specifically\ndesigned for energy management systems (EMSs), which effectively tackles the\ndynamic environment of cybersecurity vulnerabilities and/or system problems\n(SPs), accomplished through the incorporation of novel methodologies. A\ncomprehensive multi-point attack/error model is initially proposed to\nsystematically identify vulnerabilities throughout the entire EMS data\nprocessing pipeline, including post state estimation (SE) stealth attacks, EMS\ndatabase manipulation, and human-machine interface (HMI) display corruption\naccording to the real-time database (RTDB) storage. This framework acknowledges\nthe interconnected nature of modern attack vectors, which utilize various\nphases of supervisory control and data acquisition (SCADA) data flow. Then,\ngenerative AI (GenAI)-based anomaly detection systems (ADSs) for EMSs are\nproposed for the first time in the power system domain to handle the scenarios.\nFurther, a set-of-mark generative intelligence (SoM-GI) framework, which\nleverages multimodal analysis by integrating visual markers with rules\nconsidering the GenAI capabilities, is suggested to overcome inherent spatial\nreasoning limitations. The SoM-GI methodology employs systematic visual\nindicators to enable accurate interpretation of segmented HMI displays and\ndetect visual anomalies that numerical methods fail to identify. Validation on\nthe IEEE 14-Bus system shows the framework's effectiveness across scenarios,\nwhile visual analysis identifies inconsistencies. This integrated approach\ncombines numerical analysis with visual pattern recognition and linguistic\nrules to protect against cyber threats and system errors.", "AI": {"tldr": "The paper proposes a comprehensive security framework for energy management systems (EMSs) using generative AI and a multi-point attack/error model, validated on the IEEE 14-Bus system.", "motivation": "Modern EMSs face evolving cybersecurity vulnerabilities and system problems, necessitating a dynamic framework to address interconnected attack vectors and integrate novel methodologies for robust protection.", "method": "1. A multi-point attack/error model to identify vulnerabilities in the EMS data pipeline (e.g., SE stealth attacks, RTDB corruption, HMI display manipulation). 2. First-time application of generative AI (GenAI)-based anomaly detection systems (ADSs) in power systems. 3. A set-of-mark generative intelligence (SoM-GI) framework combining visual markers, linguistic rules, and GenAI to overcome spatial reasoning limitations.", "result": "Validation on the IEEE 14-Bus system demonstrated the framework's effectiveness in handling scenarios and detecting visual anomalies missed by numerical methods.", "conclusion": "The integrated framework combining numerical analysis, visual pattern recognition, and linguistic rules effectively addresses cyber threats and system errors in EMSs through novel GenAI-driven methodologies."}}
{"id": "2508.10052", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10052", "abs": "https://arxiv.org/abs/2508.10052", "authors": ["Pallavi Zambare", "Venkata Nikhil Thanikella", "Nikhil Padmanabh Kottur", "Sree Akhil Akula", "Ying Liu"], "title": "NetMoniAI: An Agentic AI Framework for Network Security & Monitoring", "comment": "Accepted in IEEE 3rd International Conference on Artificial\n  Intelligence, Blockchain, and Internet of Things (AIBThings 2025)", "summary": "In this paper, we present NetMoniAI, an agentic AI framework for automatic\nnetwork monitoring and security that integrates decentralized analysis with\nlightweight centralized coordination. The framework consists of two layers:\nautonomous micro-agents at each node perform local traffic analysis and anomaly\ndetection. A central controller then aggregates insights across nodes to detect\ncoordinated attacks and maintain system-wide situational awareness. We\nevaluated NetMoniAI on a local micro-testbed and through NS-3 simulations.\nResults confirm that the two-tier agentic-AI design scales under resource\nconstraints, reduces redundancy, and improves response time without\ncompromising accuracy. To facilitate broader adoption and reproducibility, the\ncomplete framework is available as open source. This enables researchers and\npractitioners to replicate, validate, and extend it across diverse network\nenvironments and threat scenarios. Github link:\nhttps://github.com/pzambare3/NetMoniAI", "AI": {"tldr": "NetMoniAI is a two-tier AI framework for network monitoring combining autonomous node-level agents with a central coordinator. It achieves scalability, reduced redundancy, and faster response times while maintaining accuracy, with open-source availability for broader adoption.", "motivation": "Existing network monitoring systems face challenges in scalability, resource efficiency, and coordination across distributed environments, necessitating a solution that balances decentralized analysis with centralized control for effective threat detection and situational awareness.", "method": "The architecture employs autonomous micro-agents at each node for local traffic analysis and anomaly detection, paired with a central controller aggregating node data to detect coordinated attacks and maintain system-wide visibility. Evaluation used a local micro-testbed and NS-3 simulations.", "result": "NetMoniAI demonstrated improved performance through scalable operations under resource limitations, minimized redundant processing, and faster response times without accuracy compromises. Open-source distribution enables replication and extension across diverse network scenarios.", "conclusion": "The two-layer agentic-AI design effectively enhances network monitoring by distributing intelligence and centralizing coordination, with open-source availability fostering validation and adaptation to evolving security challenges in heterogeneous infrastructure."}}
{"id": "2508.10065", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10065", "abs": "https://arxiv.org/abs/2508.10065", "authors": ["Yuhao Sun", "Yihua Zhang", "Gaowen Liu", "Hongtao Xie", "Sijia Liu"], "title": "Invisible Watermarks, Visible Gains: Steering Machine Unlearning with Bi-Level Watermarking Design", "comment": "Accepted by ICCV 2025", "summary": "With the increasing demand for the right to be forgotten, machine unlearning\n(MU) has emerged as a vital tool for enhancing trust and regulatory compliance\nby enabling the removal of sensitive data influences from machine learning (ML)\nmodels. However, most MU algorithms primarily rely on in-training methods to\nadjust model weights, with limited exploration of the benefits that data-level\nadjustments could bring to the unlearning process. To address this gap, we\npropose a novel approach that leverages digital watermarking to facilitate MU\nby strategically modifying data content. By integrating watermarking, we\nestablish a controlled unlearning mechanism that enables precise removal of\nspecified data while maintaining model utility for unrelated tasks. We first\nexamine the impact of watermarked data on MU, finding that MU effectively\ngeneralizes to watermarked data. Building on this, we introduce an\nunlearning-friendly watermarking framework, termed Water4MU, to enhance\nunlearning effectiveness. The core of Water4MU is a bi-level optimization (BLO)\nframework: at the upper level, the watermarking network is optimized to\nminimize unlearning difficulty, while at the lower level, the model itself is\ntrained independently of watermarking. Experimental results demonstrate that\nWater4MU is effective in MU across both image classification and image\ngeneration tasks. Notably, it outperforms existing methods in challenging MU\nscenarios, known as \"challenging forgets\".", "AI": {"tldr": "The paper introduces Water4MU, a digital watermarking-based framework for enhancing machine unlearning (MU) through data-level adjustments. The bi-level optimization approach separately optimizes watermarking and model training to enable precise data removal while maintaining model utility, achieving superior performance in challenging unlearning scenarios.", "motivation": "Machine unlearning algorithms predominantly use in-training weight adjustments, lacking exploration of data-level optimizations. The paper addresses this gap by investigating how watermarking can improve unlearning effectiveness, allowing controlled influence removal of sensitive data without compromising unrelated tasks. Traditional MU approaches struggle with 'challenging forgets' requiring targeted data removal.", "method": "Water4MU employs a bi-level optimization (BLO) framework: 1) Upper level optimizes watermarking to minimize unlearning difficulty by strategically embedding modifications that facilitate data influence removal 2) Lower level optimizes model training independent of watermarking signals. Digital watermarking is leveraged to proactively embed unlearning controls within data samples, enabling post-hoc adjustments to remove specific data influences.", "result": "Water4MU demonstrates effective machine unlearning across image classification and generation tasks. The approach outperforms existing methods in 'challenging forget' scenarios while preserving model accuracy for non-sensitive data. Watermarked data showed comparable unlearning effectiveness to original datasets, validating the framework's practical value in real-world applications.", "conclusion": "The paper establishes digital watermarking as a viable approach for data-centric machine unlearning, with Water4MU providing a novel solution that balances precise data influence removal and model utility preservation. The results suggest this framework resolves critical limitations in existing MU techniques, particularly for difficult data unlearning requirements in regulated environments."}}
{"id": "2508.10185", "categories": ["cs.CR", "cs.CY", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.10185", "abs": "https://arxiv.org/abs/2508.10185", "authors": ["Ren\u00e9 Mayrhofer", "Michael Roland", "Tobias H\u00f6ller", "Philipp Hofer", "Mario Lins"], "title": "An Architecture for Distributed Digital Identities in the Physical World", "comment": null, "summary": "Digital identities are increasingly important for mediating not only digital\nbut also physical service transactions. Managing such identities through\ncentralized providers can cause both availability and privacy concerns: single\npoints of failure and control are ideal targets for global attacks on\ntechnical, organizational, or legal fronts. We design, analyze, and build a\ndistributed digital identity architecture for physical world transactions in\ncommon scenarios like unlocking doors, public transport, or crossing country\nborders. This architecture combines (biometric and other) sensors, (established\nand upcoming) identity authorities, attribute verifiers, and a new core\ncomponent we call the \\emph{Personal Identity Agent (PIA)} that represents\nindividuals with their identity attributes in the digital domain. All\ntransactions are conducted in a completely decentralized manner, and the\ncomponents for which we currently assume central coordination are optional and\nonly used for assisting with service discovery and latency reduction. We\npresent a first protocol between these parties and formally verify that it\nachieves relevant security properties based on a realistic threat model\nincluding strong global adversaries. A proof-of-concept implementation\ndemonstrates practical feasibility of both architecture and initial protocol\nfor applications that can tolerate end-to-end latencies in the range of a few\nseconds.", "AI": {"tldr": "The paper proposes a decentralized digital identity architecture with a Personal Identity Agent (PIA) to address privacy and availability issues in centralized systems, verifying a protocol for security against global adversaries and demonstrating feasibility via a proof-of-concept.", "motivation": "Centralized identity providers create single points of failure and control, making them vulnerable to attacks on technical, organizational, or legal fronts, thus requiring a distributed solution for privacy and resilience.", "method": "Composed a distributed architecture integrating sensors, identity authorities, attribute verifiers, and a novel PIA component; developed a protocol and formally verified its security properties under a realistic threat model.", "result": "Proof-of-concept implementation shows the architecture and protocol are practically feasible for real-world applications with acceptable latency (a few seconds).", "conclusion": "Decentralized identity systems using components like PIA can effectively mediate physical-world transactions while mitigating global threats, paving the way for resilient and privacy-preserving infrastructure."}}
{"id": "2508.10212", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.10212", "abs": "https://arxiv.org/abs/2508.10212", "authors": ["Md Sazedur Rahman", "Mohamed Elmahallawy", "Sanjay Madria", "Samuel Frimpong"], "title": "Detecting Untargeted Attacks and Mitigating Unreliable Updates in Federated Learning for Underground Mining Operations", "comment": null, "summary": "Underground mining operations rely on distributed sensor networks to collect\ncritical data daily, including mine temperature, toxic gas concentrations, and\nminer movements for hazard detection and operational decision-making. However,\ntransmitting raw sensor data to a central server for training deep learning\nmodels introduces significant privacy risks, potentially exposing sensitive\nmine-specific information. Federated Learning (FL) offers a transformative\nsolution by enabling collaborative model training while ensuring that raw data\nremains localized at each mine. Despite its advantages, FL in underground\nmining faces key challenges: (i) An attacker may compromise a mine's local\nmodel by employing techniques such as sign-flipping attacks or additive noise,\nleading to erroneous predictions; (ii) Low-quality (yet potentially valuable)\ndata, caused by poor lighting conditions or sensor inaccuracies in mines may\ndegrade the FL training process. In response, this paper proposes MineDetect, a\ndefense FL framework that detects and isolates the attacked models while\nmitigating the impact of mines with low-quality data. MineDetect introduces two\nkey innovations: (i) Detecting attacked models (maliciously manipulated) by\ndeveloping a history-aware mechanism that leverages local and global averages\nof gradient updates; (ii) Identifying and eliminating adversarial influences\nfrom unreliable models (generated by clients with poor data quality) on the FL\ntraining process. Comprehensive simulations across diverse datasets demonstrate\nthat MineDetect outperforms existing methods in both robustness and accuracy,\neven in challenging non-IID data scenarios. Its ability to counter adversarial\ninfluences while maintaining lower computational efficiency makes it a vital\nadvancement for improving safety and operational effectiveness in underground\nmining.", "AI": {"tldr": "MineDetect: Defense FL framework for underground mining to address privacy risks and data quality challenges while improving safety.", "motivation": "Centralized transmission of raw sensor data in underground mining creates privacy risks and operational vulnerabilities. Federated Learning (FL) enables collaborative model training with data localization but faces challenges from malicious attacks and low-quality data common in mining environments.", "method": "Proposes MineDetect with two innovations: 1) History-aware attack detection using gradient update comparisons between local and global averages; 2) Adversarial influence elimination mechanism for unreliable models caused by poor data quality.", "result": "Simulations show MineDetect outperforms existing methods in robustness and accuracy across diverse datasets, particularly in non-IID scenarios with no significant computational overhead.", "conclusion": "MineDetect provides critical security and quality improvements for FL in underground mining operations, advancing safety and decision-making while preserving data privacy."}}
{"id": "2508.10327", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.10327", "abs": "https://arxiv.org/abs/2508.10327", "authors": ["Haoyang Hu", "Xun Huang", "Chenyu Wu", "Shiwen Liu", "Zhichao Lian", "Shuangquan Zhang"], "title": "BERTector: Intrusion Detection Based on Joint-Dataset Learning", "comment": null, "summary": "Intrusion detection systems (IDS) are facing challenges in generalization and\nrobustness due to the heterogeneity of network traffic and the diversity of\nattack patterns. To address this issue, we propose a new joint-dataset training\nparadigm for IDS and propose a scalable BERTector framework based on BERT.\nBERTector integrates three key components: NSS-Tokenizer for traffic-aware\nsemantic tokenization, supervised fine-tuning with a hybrid dataset, and\nlow-rank adaptation (LoRA) for efficient training. Extensive experiments show\nthat BERTector achieves state-of-the-art detection accuracy, strong\ncross-dataset generalization capabilities, and excellent robustness to\nadversarial perturbations. This work establishes a unified and efficient\nsolution for modern IDS in complex and dynamic network environments.", "AI": {"tldr": "BERTector is a scalable BERT-based IDS framework that enhances generalization and robustness through joint-dataset training.", "motivation": "Existing intrusion detection systems struggle with generalization and robustness due to network traffic heterogeneity and diverse attack patterns.", "method": "BERTector integrates three components: (1) NSS-Tokenizer for traffic-aware semantic tokenization, (2) supervised fine-tuning with hybrid datasets, (3) low-rank adaptation (LoRA) for efficient training.", "result": "Achieves state-of-the-art detection accuracy, strong cross-dataset generalization, and excellent robustness against adversarial perturbations.", "conclusion": "Provides a unified and efficient solution for modern IDS in complex, dynamic network environments."}}
{"id": "2508.10431", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.10431", "abs": "https://arxiv.org/abs/2508.10431", "authors": ["Chris Cao", "Gururaj Saileshwar"], "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based Side-Channel Attacks on Fully Associative Randomized Caches", "comment": null, "summary": "Recent work presented at USENIX Security 2025 claims that occupancy-based\nattacks can recover AES keys from the MIRAGE randomized cache. In this paper,\nwe examine these claims and find that they arise from fundamental modeling\nflaws. Most critically, the authors' simulation of MIRAGE uses a constant seed\nto initialize the random number generator used for global evictions in MIRAGE,\ncausing every AES encryption they trace to evict the same deterministic\nsequence of cache lines. This artificially creates a highly repeatable timing\npattern that is not representative of a realistic implementation of MIRAGE,\nwhere eviction sequences vary randomly between encryptions. When we instead\nrandomize the eviction seed for each run, reflecting realistic operation, the\ncorrelation between AES T-table accesses and attacker runtimes disappears, and\nthe attack fails. These findings show that the reported leakage is an artifact\nof incorrect modeling, and not an actual vulnerability in MIRAGE.", "AI": {"tldr": "This paper refutes previous claims that MIRAGE cache is vulnerable to occupancy-based AES key attacks due to flaws in the simulation's random seed initialization.", "motivation": "Addressing reports of a purported vulnerability in MIRAGE's security against occupancy-based attacks, this paper investigates the validity of such claims through rigorous analysis.", "method": "The authors replicate and critique the simulation methodology of the prior work, identifying a critical flaw in the use of a constant seed for MIRAGE's random number generator. They reconfigure the simulation to randomize the seed for each encryption run, reflecting realistic behavior.", "result": "With randomized eviction seeds, the correlation between AES T-table access patterns and attacker runtime observed in the prior work disappears. The simulated attack fails to recover keys under realistic conditions.", "conclusion": "The reported key leakage is an artifact of incorrect modeling, not an actual security flaw. MIRAGE's design remains secure against occupancy-based attacks when implemented correctly."}}
{"id": "2508.10493", "categories": ["cs.CR", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.10493", "abs": "https://arxiv.org/abs/2508.10493", "authors": ["Bernhard Kauer", "Aleksandr Petrosyan", "Benjamin Livshits"], "title": "AlDBaran: Towards Blazingly Fast State Commitments for Blockchains", "comment": null, "summary": "The fundamental basis for maintaining integrity within contemporary\nblockchain systems is provided by authenticated databases. Our analysis\nindicates that a significant portion of the approaches applied in this domain\nfail to sufficiently meet the stringent requirements of systems processing\ntransactions at rates of multi-million TPS. AlDBaran signifies a substantial\nadvancement in authenticated databases. By eliminating disk I/O operations from\nthe critical path, implementing prefetching strategies, and refining the update\nmechanism of the Merkle tree, we have engineered an authenticated data\nstructure capable of handling state updates efficiently at a network throughput\nof 50 Gbps. This throughput capacity significantly surpasses any empirically\ndocumented blockchain throughput, guaranteeing the ability of even the most\nhigh-throughput blockchains to generate state commitments effectively.\n  AlDBaran provides support for historical state proofs, which facilitates a\nwide array of novel applications. For instance, the deployment of AlDBaran\ncould enable blockchains that do not currently support state commitments to\noffer functionalities for light clients and/or implement rollups.\n  When benchmarked against alternative authenticated data structure projects,\nAlDBaran exhibits superior performance and simplicity. In particular, AlDBaran\nachieves speeds of approximately 48 million updates per second using an\nidentical machine configuration. This characteristic renders AlDBaran an\nattractive solution for resource-limited environments, as its historical data\ncapabilities can be modularly isolated (and deactivated), which further\nenhances performance. On consumer-level portable hardware, it achieves\napproximately 8 million updates/s in an in-memory setting and 5 million\nupdates/s with snapshots at sub-second intervals, illustrating compelling and\ncost-effective scalability.", "AI": {"tldr": "AlDBaran is a high-performance authenticated database for blockchains, achieving 50 Gbps throughput and 48 million updates/s, enabling efficient state commitments and historical proofs even in resource-constrained environments.", "motivation": "Existing authenticated database approaches for blockchains struggle to meet demands of multi-million TPS systems, necessitating more efficient solutions for state updates and commitments.", "method": "1) Disk I/O removal from critical path. 2) Prefetching strategies. 3) Merkle tree update mechanism refinement through architectural optimizations.", "result": "AlDBaran handles 50 Gbps network throughput, 48 million updates/s on identical machines, 8 million updates/s on portable hardware in-memory, and 5 million updates/s with sub-second snapshots, surpassing prior blockchain throughput records.", "conclusion": "AlDBaran provides unprecedented scalability and flexibility for blockchain authenticated databases, supporting historical state proofs while offering performance advantages in both enterprise and consumer-grade systems through its modular design."}}
{"id": "2508.10510", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.10510", "abs": "https://arxiv.org/abs/2508.10510", "authors": ["Hugo Delavenne", "Louise Lallemand"], "title": "Codes on any Cayley Graph have an Interactive Oracle Proof of Proximity", "comment": null, "summary": "Interactive Oracle Proofs of Proximity (IOPP) are at the heart of code-based\nSNARKs, a family of zeroknowledge protocols. The first and most famous one is\nthe FRI protocol [BBHR18a], that efficiently tests proximity to Reed-Solomon\ncodes. This paper generalizes the flowering IOPP introduced in [DMR25] for some\nspecific (2, n)-regular Tanner codes to a much broader variety of codes: any\ncode with symbols indexed on the edges of a Cayley graph. The flowering\nprotocol of [DMR25] had a soundness parameter much lower than the FRI protocol\n[BCI + 23], and complexity parameters that could compete with the FRI\n[BBHR18a]. The lower soundness and the absence of restriction on the base field\nmay lead to other practical speedups, however the codes considered in [DMR25]\nhave an o(1) minimum distance. The generalization proposed in this paper\npreserves the soundness parameter with a slight decrease of the complexity\nparameters, while allowing being applied on codes with constant rate and\nconstant minimum distance thanks to the good expansion properties of some\nfamilies of Cayley graphs.", "AI": {"tldr": "This paper generalizes the Flowering IOPP protocol [DMR25] from specific (2,n)-regular Tanner codes to any code with symbols indexed on Cayley graph edges, preserving low soundness and slight complexity reduction while enabling application to codes with constant rate and minimum distance.", "motivation": "Prior Flowering IOPPs [DMR25] had weak o(1) minimum distance and lower soundness than FRI [BBHR18a]. Generalizing to Cayley graph codes allows maintaining strong security while using efficient codes with constant rate/distance parameters.", "method": "Adapt the Flowering protocol framework to leverage Cayley graph expansion properties by encoding symbols on their edges, using their strong connectivity characteristics for proximity testing.", "result": "Achieved equivalent soundness parameters with marginal complexity improvements over FRI [BCI+23] and enabled practical deployment on codes with constant relative minimum distance and coding rate.", "conclusion": "Cayley graph-based IOPP generalization offers superior tradeoff between security strength and implementation efficiency compared to both traditional FRI and prior Flowering protocols, making SNARK systems more viable for constant-distance code applications."}}
{"id": "2508.10636", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.10636", "abs": "https://arxiv.org/abs/2508.10636", "authors": ["Sandipan Dey", "Payal Santosh Kate", "Vatsala Upadhyay", "Abhishek Vaish"], "title": "A Transformer-Based Approach for DDoS Attack Detection in IoT Networks", "comment": null, "summary": "DDoS attacks have become a major threat to the security of IoT devices and\ncan cause severe damage to the network infrastructure. IoT devices suffer from\nthe inherent problem of resource constraints and are therefore susceptible to\nsuch resource-exhausting attacks. Traditional methods for detecting DDoS\nattacks are not efficient enough to cope with the dynamic nature of IoT\nnetworks, as well as the scalability of the attacks, diversity of protocols,\nhigh volume of traffic, and variability in device behavior, and variability of\nprotocols like MQTT, CoAP, making it hard to implement security across all the\nprotocols. In this paper, we propose a novel approach, i.e., the use of\nTransformer models, which have shown remarkable performance in natural language\nprocessing tasks, for detecting DDoS attacks on IoT devices. The proposed model\nextracts features from network traffic data and processes them using a\nself-attention mechanism. Experiments conducted on a real-world dataset\ndemonstrate that the proposed approach outperforms traditional machine learning\ntechniques, which can be validated by comparing both approaches' accuracy,\nprecision, recall, and F1-score. The results of this study show that the\nTransformer models can be an effective solution for detecting DDoS attacks on\nIoT devices and have the potential to be deployed in real-world IoT\nenvironments.", "AI": {"tldr": "The paper proposes using Transformer models for detecting DDoS attacks in IoT networks, overcoming the limitations of traditional methods through self-attention mechanisms and achieving better performance.", "motivation": "Traditional DDoS detection methods struggle with IoT networks due to their resource constraints, dynamic nature, scalability of attacks, protocol diversity (e.g., MQTT, CoAP), high traffic volume, and device behavior variability.", "method": "A Transformer-based model is designed to extract network traffic features and process them using self-attention mechanisms, adapting to IoT-specific challenges.", "result": "Experiments on real-world IoT data demonstrate superior performance over traditional machine learning methods, with improvements in accuracy, precision, recall, and F1-score.", "conclusion": "Transformer models offer an effective and scalable solution for DDoS attack detection in IoT environments, with potential for real-world deployment."}}
{"id": "2508.10639", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.10639", "abs": "https://arxiv.org/abs/2508.10639", "authors": ["Anyuan Sang", "Lu Zhou", "Li Yang", "Junbo Jia", "Huipeng Yang", "Pengbin Feng", "Jianfeng Ma"], "title": "MirGuard: Towards a Robust Provenance-based Intrusion Detection System Against Graph Manipulation Attacks", "comment": null, "summary": "Learning-based Provenance-based Intrusion Detection Systems (PIDSes) have\nbecome essential tools for anomaly detection in host systems due to their\nability to capture rich contextual and structural information, as well as their\npotential to detect unknown attacks. However, recent studies have shown that\nthese systems are vulnerable to graph manipulation attacks, where attackers\nmanipulate the graph structure to evade detection. While some previous\napproaches have discussed this type of attack, none have fully addressed it\nwith a robust detection solution, limiting the practical applicability of\nPIDSes.\n  To address this challenge, we propose MirGuard, a robust anomaly detection\nframework that combines logic-aware multi-view augmentation with contrastive\nrepresentation learning. Rather than applying arbitrary structural\nperturbations, MirGuard introduces Logic-Aware Noise Injection (LNI) to\ngenerate semantically valid graph views, ensuring that all augmentations\npreserve the underlying causal semantics of the provenance data. These views\nare then used in a Logic-Preserving Contrastive Learning framework, which\nencourages the model to learn representations that are invariant to benign\ntransformations but sensitive to adversarial inconsistencies. Comprehensive\nevaluations on multiple provenance datasets demonstrate that MirGuard\nsignificantly outperforms state-of-the-art detectors in robustness against\nvarious graph manipulation attacks without sacrificing detection performance\nand efficiency. Our work represents the first targeted study to enhance PIDS\nagainst such adversarial threats, providing a robust and effective solution to\nmodern cybersecurity challenges.", "AI": {"tldr": "MirGuard is a robust anomaly detection framework that enhances Learning-based Provenance-based Intrusion Detection Systems (PIDSes) against graph manipulation attacks by using Logic-Aware Noise Injection (LNI) and contrastive representation learning, achieving better robustness while maintaining detection performance and efficiency.", "motivation": "Learning-based PIDSes are vulnerable to graph manipulation attacks that exploit structural perturbations, limiting their practical applicability despite their ability to detect unknown attacks. Existing approaches lack a robust detection solution for this issue.", "method": "MirGuard introduces Logic-Aware Noise Injection (LNI) to generate semantically valid graph views and employs a Logic-Preserving Contrastive Learning framework to learn representations invariant to benign transformations but sensitive to adversarial inconsistencies.", "result": "Comprehensive evaluations showed MirGuard significantly outperforms state-of-the-art detectors in robustness against various graph manipulation attacks, retaining detection performance and efficiency.", "conclusion": "MirGuard provides the first targeted and effective solution to enhance PIDSes against adversarial graph manipulation attacks, offering a secure and efficient framework for modern cybersecurity challenges."}}
{"id": "2508.10652", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.10652", "abs": "https://arxiv.org/abs/2508.10652", "authors": ["Richa Dasila", "Vatsala Upadhyay", "Samo Bobek", "Abhishek Vaish"], "title": "A Novel Study on Intelligent Methods and Explainable AI for Dynamic Malware Analysis", "comment": null, "summary": "Deep learning models are one of the security strategies, trained on extensive\ndatasets, and play a critical role in detecting and responding to these threats\nby recognizing complex patterns in malicious code. However, the opaque nature\nof these models-often described as \"black boxes\"-makes their decision-making\nprocesses difficult to understand, even for their creators. This research\naddresses these challenges by integrating Explainable AI (XAI) techniques to\nenhance the interpretability and trustworthiness of malware detection models.\nIn this research, the use of Multi-Layer Perceptrons (MLP) for dynamic malware\nanalysis has been considered, a less explored area, and its efficacy in\ndetecting Metamorphic Malware, and further the effectiveness and transparency\nof MLPs, CNNs, RNNs, and CNN-LSTM models in malware classification, evaluating\nthese models through the lens of Explainable AI (XAI). This comprehensive\napproach aims to demystify the internal workings of deep learning models,\npromoting a better understanding and trust in their predictive capabilities in\ncybersecurity contexts. Such in-depth analysis and implementation haven't been\ndone to the best of our knowledge.", "AI": {"tldr": "This paper investigates the integration of Explainable AI (XAI) with deep learning models to enhance transparency and trustworthiness in malware detection, particularly focusing on dynamic analysis of metamorphic malware using MLP, CNN, RNN, and CNN-LSTM architectures.", "motivation": "Deep learning models are powerful for threat detection but lack transparency (black-box nature), creating trust issues and hindering adoption in cybersecurity. Dynamic malware analysis, especially for metamorphic malware, requires interpretable models to understand detection rationale.", "method": "The study evaluates four deep learning architectures (MLP, CNN, RNN, CNN-LSTM) for malware classification in dynamic analysis contexts, applying XAI techniques to analyze their decision-making processes and interpretability effectiveness.", "result": "Demonstrates how XAI techniques can be applied to these models to enhance transparency while maintaining effectiveness in detecting metamorphic malware, with comparative results on model performance and explainability metrics", "conclusion": "Integrating XAI approaches with deep learning models significantly improves interpretability without compromising effectiveness, making them more trustworthy for cybersecurity applications by elucidating their malware detection mechanisms."}}
{"id": "2508.10677", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10677", "abs": "https://arxiv.org/abs/2508.10677", "authors": ["Amine Tellache", "Abdelaziz Amara Korba", "Amdjed Mokhtari", "Horea Moldovan", "Yacine Ghamri-Doudane"], "title": "Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat Intelligence", "comment": null, "summary": "Effective incident response (IR) is critical for mitigating cyber threats,\nyet security teams are overwhelmed by alert fatigue, high false-positive rates,\nand the vast volume of unstructured Cyber Threat Intelligence (CTI) documents.\nWhile CTI holds immense potential for enriching security operations, its\nextensive and fragmented nature makes manual analysis time-consuming and\nresource-intensive. To bridge this gap, we introduce a novel\nRetrieval-Augmented Generation (RAG)-based framework that leverages Large\nLanguage Models (LLMs) to automate and enhance IR by integrating dynamically\nretrieved CTI. Our approach introduces a hybrid retrieval mechanism that\ncombines NLP-based similarity searches within a CTI vector database with\nstandardized queries to external CTI platforms, facilitating context-aware\nenrichment of security alerts. The augmented intelligence is then leveraged by\nan LLM-powered response generation module, which formulates precise,\nactionable, and contextually relevant incident mitigation strategies. We\npropose a dual evaluation paradigm, wherein automated assessment using an\nauxiliary LLM is systematically cross-validated by cybersecurity experts.\nEmpirical validation on real-world and simulated alerts demonstrates that our\napproach enhances the accuracy, contextualization, and efficiency of IR,\nalleviating analyst workload and reducing response latency. This work\nunderscores the potential of LLM-driven CTI fusion in advancing autonomous\nsecurity operations and establishing a foundation for intelligent, adaptive\ncybersecurity frameworks.", "AI": {"tldr": "A RAG-based LLM framework enhances incident response by automating CTI integration, improving alert accuracy, reducing analyst workload, and addressing alert fatigue via context-aware enrichment.", "motivation": "Security teams face alert fatigue, high false-positive rates, and challenges in manually analyzing fragmented CTI documents. The goal is to automate and enrich incident response (IR) processes to improve efficiency and accuracy.", "method": "The approach uses a hybrid retrieval mechanism combining NLP-based similarity searches in a CTI vector database and standardized queries to external CTI platforms. An LLM then generates precise mitigation strategies using the enriched CTI.", "result": "Empirical validation shows improved accuracy, contextualization, and efficiency in IR through real-world and simulated alerts, with reduced response latency and analyst workload.", "conclusion": "LLM-driven CTI fusion demonstrates potential for advancing autonomous security operations and building intelligent, adaptive cybersecurity frameworks, validated through a dual expert-LLM evaluation paradigm."}}
{"id": "2508.10880", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10880", "abs": "https://arxiv.org/abs/2508.10880", "authors": ["Yanzhe Zhang", "Diyi Yang"], "title": "Searching for Privacy Risks in LLM Agents via Simulation", "comment": "Preprint", "summary": "The widespread deployment of LLM-based agents is likely to introduce a\ncritical privacy threat: malicious agents that proactively engage others in\nmulti-turn interactions to extract sensitive information. These dynamic\ndialogues enable adaptive attack strategies that can cause severe privacy\nviolations, yet their evolving nature makes it difficult to anticipate and\ndiscover sophisticated vulnerabilities manually. To tackle this problem, we\npresent a search-based framework that alternates between improving attacker and\ndefender instructions by simulating privacy-critical agent interactions. Each\nsimulation involves three roles: data subject, data sender, and data recipient.\nWhile the data subject's behavior is fixed, the attacker (data recipient)\nattempts to extract sensitive information from the defender (data sender)\nthrough persistent and interactive exchanges. To explore this interaction space\nefficiently, our search algorithm employs LLMs as optimizers, using parallel\nsearch with multiple threads and cross-thread propagation to analyze simulation\ntrajectories and iteratively propose new instructions. Through this process, we\nfind that attack strategies escalate from simple direct requests to\nsophisticated multi-turn tactics such as impersonation and consent forgery,\nwhile defenses advance from rule-based constraints to identity-verification\nstate machines. The discovered attacks and defenses transfer across diverse\nscenarios and backbone models, demonstrating strong practical utility for\nbuilding privacy-aware agents.", "AI": {"tldr": "This paper proposes a search-based framework to identify and enhance privacy vulnerabilities in LLM-based agents through simulated multi-role interactions, showing escalation in both attack tactics (direct requests \u2192 impersonation/consent forgery) and defense mechanisms (rule-based \u2192 identity-verification state machines).", "motivation": "Dynamic multi-turn dialogues by malicious LLM agents can extract sensitive information in unpredictable ways, making manual detection of sophisticated vulnerabilities impractical.", "method": "Alternating attacker/defender instruction improvements via multi-threaded simulations with three roles (data subject, sender, recipient). Uses LLMs as optimizers with parallel search and cross-thread propagation.", "result": "Attack strategies progress from basic to advanced tactics while defenses evolve correspondingly; transferability of findings across scenarios and models validated.", "conclusion": "The framework enables systematic exploration of interactive privacy threats and defenses, providing practical tools to build privacy-aware agents through adaptive simulation-based discovery."}}
