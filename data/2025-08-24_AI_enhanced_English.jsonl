{"id": "2508.14925", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.14925", "abs": "https://arxiv.org/abs/2508.14925", "authors": ["Zhiqiang Wang", "Yichao Gao", "Yanting Wang", "Suyuan Liu", "Haifeng Sun", "Haoran Cheng", "Guanquan Shi", "Haohua Du", "Xiangyang Li"], "title": "MCPTox: A Benchmark for Tool Poisoning Attack on Real-World MCP Servers", "comment": null, "summary": "By providing a standardized interface for LLM agents to interact with\nexternal tools, the Model Context Protocol (MCP) is quickly becoming a\ncornerstone of the modern autonomous agent ecosystem. However, it creates novel\nattack surfaces due to untrusted external tools. While prior work has focused\non attacks injected through external tool outputs, we investigate a more\nfundamental vulnerability: Tool Poisoning, where malicious instructions are\nembedded within a tool's metadata without execution. To date, this threat has\nbeen primarily demonstrated through isolated cases, lacking a systematic,\nlarge-scale evaluation.\n  We introduce MCPTox, the first benchmark to systematically evaluate agent\nrobustness against Tool Poisoning in realistic MCP settings. MCPTox is\nconstructed upon 45 live, real-world MCP servers and 353 authentic tools. To\nachieve this, we design three distinct attack templates to generate a\ncomprehensive suite of 1312 malicious test cases by few-shot learning, covering\n10 categories of potential risks. Our evaluation on 20 prominent LLM agents\nsetting reveals a widespread vulnerability to Tool Poisoning, with o1-mini,\nachieving an attack success rate of 72.8\\%. We find that more capable models\nare often more susceptible, as the attack exploits their superior\ninstruction-following abilities. Finally, the failure case analysis reveals\nthat agents rarely refuse these attacks, with the highest refused rate\n(Claude-3.7-Sonnet) less than 3\\%, demonstrating that existing safety alignment\nis ineffective against malicious actions that use legitimate tools for\nunauthorized operation. Our findings create a crucial empirical baseline for\nunderstanding and mitigating this widespread threat, and we release MCPTox for\nthe development of verifiably safer AI agents. Our dataset is available at an\nanonymized repository: \\textit{https://anonymous.4open.science/r/AAAI26-7C02}.", "AI": {"tldr": "Introduces MCPTox, the first benchmark to evaluate LLM agents' vulnerabilities to Tool Poisoning through metadata in MCP environments, revealing widespread susceptibility with 72.8% success rate for o1-mini.", "motivation": "Prior work focused on external tool output injections; this paper investigates a more fundamental vulnerability\u2014Tool Poisoning via metadata\u2014and addresses the lack of systematic, large-scale evaluation.", "method": "Constructed MCPTox using 45 real-world MCP servers and 353 tools, generating 1.3k malicious test cases across 10 risk categories via 3 attack templates and few-shot learning. Evaluated 20 leading LLM agents.", "result": "All 20 LLM agents demonstrated vulnerability to Tool Poisoning, with o1-mini achieving highest success rate (72.8%). More capable models showed greater susceptibility, and only 3% maximum rejection rate among agents.", "conclusion": "Establishes critical empirical baseline for Tool Poisoning vulnerabilities, showing existing safety measures are insufficient against metadata-based attacks. Releases open dataset to advance verification of safer agent systems."}}
{"id": "2508.15031", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15031", "abs": "https://arxiv.org/abs/2508.15031", "authors": ["Kaixiang Zhao", "Lincan Li", "Kaize Ding", "Neil Zhenqiang Gong", "Yue Zhao", "Yushun Dong"], "title": "A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives", "comment": null, "summary": "Machine learning (ML) models have significantly grown in complexity and\nutility, driving advances across multiple domains. However, substantial\ncomputational resources and specialized expertise have historically restricted\ntheir wide adoption. Machine-Learning-as-a-Service (MLaaS) platforms have\naddressed these barriers by providing scalable, convenient, and affordable\naccess to sophisticated ML models through user-friendly APIs. While this\naccessibility promotes widespread use of advanced ML capabilities, it also\nintroduces vulnerabilities exploited through Model Extraction Attacks (MEAs).\nRecent studies have demonstrated that adversaries can systematically replicate\na target model's functionality by interacting with publicly exposed interfaces,\nposing threats to intellectual property, privacy, and system security. In this\npaper, we offer a comprehensive survey of MEAs and corresponding defense\nstrategies. We propose a novel taxonomy that classifies MEAs according to\nattack mechanisms, defense approaches, and computing environments. Our analysis\ncovers various attack techniques, evaluates their effectiveness, and highlights\nchallenges faced by existing defenses, particularly the critical trade-off\nbetween preserving model utility and ensuring security. We further assess MEAs\nwithin different computing paradigms and discuss their technical, ethical,\nlegal, and societal implications, along with promising directions for future\nresearch. This systematic survey aims to serve as a valuable reference for\nresearchers, practitioners, and policymakers engaged in AI security and\nprivacy. Additionally, we maintain an online repository continuously updated\nwith related literature at https://github.com/kzhao5/ModelExtractionPapers.", "AI": {"tldr": "This paper surveys Model Extraction Attacks on MLaaS platforms, introduces a taxonomy, and analyzes attack mechanisms, defenses, and implications for AI security and policy.", "motivation": "Machine-Learning-as-a-Service (MLaaS) platforms have made ML accessible but introduced vulnerabilities like MEAs. This paper addresses the lack of systematic analysis on MEAs, their defenses, and their broader implications, aiming to guide researchers and policymakers in AI security.", "method": "The authors propose a taxonomy classifying MEAs by attack mechanisms, defense approaches, and computing environments. They analyze attack techniques, evaluate defenses, and discuss technical, ethical, legal, and societal implications. The work synthesizes existing literature and identifies gaps in addressing the utility-security trade-off.", "result": "The survey identifies critical challenges in existing defenses (e.g., balancing utility vs. security) and provides technical and societal insights into MEAs across computing paradigms. It curates an ongoing online repository for literature updates.", "conclusion": "The paper provides a comprehensive survey of Model Extraction Attacks (MEAs) and defense strategies, offering a novel taxonomy to classify attacks, evaluate their effectiveness, and highlight challenges in balancing model utility with security. It serves as a reference for AI security research and policy while maintaining an updated online repository for future work."}}
{"id": "2508.15036", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15036", "abs": "https://arxiv.org/abs/2508.15036", "authors": ["Ruyi Ding", "Tianhong Xu", "Xinyi Shen", "Aidong Adam Ding", "Yunsi Fei"], "title": "MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs", "comment": "This paper will appear in CCS 2025", "summary": "The transformer architecture has become a cornerstone of modern AI, fueling\nremarkable progress across applications in natural language processing,\ncomputer vision, and multimodal learning. As these models continue to scale\nexplosively for performance, implementation efficiency remains a critical\nchallenge. Mixture of Experts (MoE) architectures, selectively activating\nspecialized subnetworks (experts), offer a unique balance between model\naccuracy and computational cost. However, the adaptive routing in MoE\narchitectures, where input tokens are dynamically directed to specialized\nexperts based on their semantic meaning inadvertently opens up a new attack\nsurface for privacy breaches. These input-dependent activation patterns leave\ndistinctive temporal and spatial traces in hardware execution, which\nadversaries could exploit to deduce sensitive user data. In this work, we\npropose MoEcho, discovering a side channel analysis based attack surface that\ncompromises user privacy on MoE based systems. Specifically, in MoEcho, we\nintroduce four novel architectural side channels on different computing\nplatforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and\nPerformance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting\nthese vulnerabilities, we propose four attacks that effectively breach user\nprivacy in large language models (LLMs) and vision language models (VLMs) based\non MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,\nVisual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first\nruntime architecture level security analysis of the popular MoE structure\ncommon in modern transformers, highlighting a serious security and privacy\nthreat and calling for effective and timely safeguards when harnessing MoE\nbased models for developing efficient large scale AI services.", "AI": {"tldr": "MoEcho reveals critical privacy vulnerabilities in MoE-based transformers by exploiting hardware execution traces, enabling novel side-channel attacks that reconstruct sensitive user inputs. This work calls for urgent security measures to protect data in scalable AI systems.", "motivation": "The increasing adoption of MoE architectures in transformers has created a new attack surface for privacy breaches, as adaptive routing mechanisms leave exploitable temporal and spatial traces in hardware execution.", "method": "The authors propose MoEcho, a side-channel analysis framework that introduces four novel architectural side channels (Cache Occupancy Channels, Pageout+Reload, Performance Counter, and TLB Evict+Reload) to exploit input-dependent activation patterns in MoE architectures.", "result": "MoEcho enables four privacy-breach attacks (Prompt Inference, Response Reconstruction, Visual Inference, and Visual Reconstruction) against LLMs and VLMs based on MoE architectures, demonstrating the first runtime architecture-level security analysis of this design.", "conclusion": "This paper highlights the serious security and privacy threats posed by MoE-based models, urging the development of effective safeguards to protect sensitive user data during the deployment of large-scale AI systems."}}
{"id": "2508.15042", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15042", "abs": "https://arxiv.org/abs/2508.15042", "authors": ["Sima Arasteh", "Christophe Hauser"], "title": "When Machine Learning Meets Vulnerability Discovery: Challenges and Lessons Learned", "comment": null, "summary": "In recent years, machine learning has demonstrated impressive results in\nvarious fields, including software vulnerability detection. Nonetheless, using\nmachine learning to identify software vulnerabilities presents new challenges,\nespecially regarding the scale of data involved, which was not a factor in\ntraditional methods. Consequently, in spite of the rise of new\nmachine-learning-based approaches in that space, important shortcomings persist\nregarding their evaluation. First, researchers often fail to provide concrete\nstatistics about their training datasets, such as the number of samples for\neach type of vulnerability. Moreover, many methods rely on training with\nsemantically similar functions rather than directly on vulnerable programs.\nThis leads to uncertainty about the suitability of the datasets currently used\nfor training. Secondly, the choice of a model and the level of granularity at\nwhich models are trained also affect the effectiveness of such vulnerability\ndiscovery approaches.\n  In this paper, we explore the challenges of applying machine learning to\nvulnerability discovery. We also share insights from our two previous research\npapers, Bin2vec and BinHunter, which could enhance future research in this\nfield.", "AI": {"tldr": "This paper identifies key challenges in applying machine learning to software vulnerability detection (e.g., dataset limitations and model choices) and offers actionable insights from their prior works to improve future methods.", "motivation": "The paper seeks to resolve critical gaps in machine-learning-based vulnerability detection, particularly the lack of dataset standardization and uncertainty about training validity, which hinder reliable evaluation and progress in the field.", "method": "The authors analyze challenges through critical evaluation of existing methods and provide insights derived from their prior works (Bin2vec and BinHunter) to inform future research.", "result": "The analysis reveals that current evaluation practices lack transparency in dataset statistics and model granularity, while insights from Bin2vec and BinHunter suggest pathways to address these limitations.", "conclusion": "The paper highlights the need to address current shortcomings in evaluating machine-learning approaches for vulnerability detection by improving dataset transparency and model training practices, while leveraging insights from Bin2vec and BinHunter to guide future research."}}
