{"id": "2508.15135", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15135", "abs": "https://arxiv.org/abs/2508.15135", "authors": ["Sumudu Liyanage", "Sherlock A. Licorish", "Markus Wagner", "Stephen G. MacDonell"], "title": "On the need to perform comprehensive evaluations of automated program repair benchmarks: Sorald case study", "comment": null, "summary": "In supporting the development of high-quality software, especially necessary\nin the era of LLMs, automated program repair (APR) tools aim to improve code\nquality by automatically addressing violations detected by static analysis\nprofilers. Previous research tends to evaluate APR tools only for their ability\nto clear violations, neglecting their potential introduction of new (sometimes\nsevere) violations, changes to code functionality and degrading of code\nstructure. There is thus a need for research to develop and assess\ncomprehensive evaluation frameworks for APR tools. This study addresses this\nresearch gap, and evaluates Sorald (a state-of-the-art APR tool) as a proof of\nconcept. Sorald's effectiveness was evaluated in repairing 3,529 SonarQube\nviolations across 30 rules within 2,393 Java code snippets extracted from Stack\nOverflow. Outcomes show that while Sorald fixes specific rule violations, it\nintroduced 2,120 new faults (32 bugs, 2088 code smells), reduced code\nfunctional correctness--as evidenced by a 24% unit test failure rate--and\ndegraded code structure, demonstrating the utility of our framework. Findings\nemphasize the need for evaluation methodologies that capture the full spectrum\nof APR tool effects, including side effects, to ensure their safe and effective\nadoption.", "AI": {"tldr": "This paper addresses the need for comprehensive evaluation frameworks for Automated Program Repair (APR) tools by analyzing Sorald's effectiveness in fixing and introducing code violations, impacting functionality and code structure.", "motivation": "Previous APR tool evaluations focused only on clearing violations, neglecting new faults, functionality changes, and code structure degradation, necessitating a broader assessment framework.", "method": "The study evaluated Sorald on 3,529 SonarQube violations in 2,393 Java code snippets from Stack Overflow, analyzing new faults, test failures, and code structure changes.", "result": "Sorald introduced 2,120 new faults (32 bugs, 2088 code smells), caused 24% unit test failures, and degraded code structure despite fixing original violations.", "conclusion": "The research emphasizes developing evaluation methodologies that capture all APR tool effects, including side effects, to ensure safe and effective adoption."}}
{"id": "2508.15411", "categories": ["cs.SE", "cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.15411", "abs": "https://arxiv.org/abs/2508.15411", "authors": ["Frederik Vandeputte"], "title": "Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems", "comment": null, "summary": "Generative AI (GenAI) has emerged as a transformative technology,\ndemonstrating remarkable capabilities across diverse application domains.\nHowever, GenAI faces several major challenges in developing reliable and\nefficient GenAI-empowered systems due to its unpredictability and inefficiency.\nThis paper advocates for a paradigm shift: future GenAI-native systems should\nintegrate GenAI's cognitive capabilities with traditional software engineering\nprinciples to create robust, adaptive, and efficient systems.\n  We introduce foundational GenAI-native design principles centered around five\nkey pillars -- reliability, excellence, evolvability, self-reliance, and\nassurance -- and propose architectural patterns such as GenAI-native cells,\norganic substrates, and programmable routers to guide the creation of resilient\nand self-evolving systems. Additionally, we outline the key ingredients of a\nGenAI-native software stack and discuss the impact of these systems from\ntechnical, user adoption, economic, and legal perspectives, underscoring the\nneed for further validation and experimentation. Our work aims to inspire\nfuture research and encourage relevant communities to implement and refine this\nconceptual framework.", "AI": {"tldr": "This paper proposes merging Generative AI with traditional software engineering to build reliable systems. It introduces design principles, architectural patterns, and software stack components for GenAI-native systems while highlighting technical and societal impacts requiring further research.", "motivation": "GenAI's unpredictability and inefficiency hinder the development of reliable systems. Existing approaches fail to address the need for resilient, self-evolving architectures. The paper seeks to bridge this gap by advocating a paradigm shift that merges GenAI's cognitive capabilities with traditional software engineering fundamentals.", "method": "The authors propose foundational GenAI-native design principles centered on five pillars (reliability, excellence, evolvability, self-reliance, and assurance) and introduce architectural patterns such as GenAI-native cells, organic substrates, and programmable routers. They also outline the components of a GenAI-native software stack and analyze system impacts from multiple perspectives.", "result": "The paper delivers a conceptual framework for GenAI-native systems, including design principles, architectural patterns, and software stack requirements. It provides a multidimensional analysis of technical feasibility, user adoption barriers, economic implications, and legal considerations, setting the stage for future research and implementation.", "conclusion": "The paper concludes that integrating Generative AI (GenAI) with traditional software engineering principles can lead to the development of robust, adaptive, and efficient systems. This conceptual framework requires further validation, experimentation, and refinement by the research community to address technical, user, economic, and legal challenges."}}
{"id": "2508.15423", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15423", "abs": "https://arxiv.org/abs/2508.15423", "authors": ["Ruiqi Wang", "Zezhou Yang", "Cuiyun Gao", "Xin Xia", "Qing Liao"], "title": "An Empirical Study of Knowledge Distillation for Code Understanding Tasks", "comment": "Accepted by ICSE 2026 (Cycle 1)", "summary": "Pre-trained language models (PLMs) have emerged as powerful tools for code\nunderstanding. However, deploying these PLMs in large-scale applications faces\npractical challenges due to their computational intensity and inference\nlatency. Knowledge distillation (KD), a promising model compression and\nacceleration technique, addresses these limitations by transferring knowledge\nfrom large teacher models to compact student models, enabling efficient\ninference while preserving most of the teacher models' capabilities. While this\ntechnique has shown remarkable success in natural language processing and\ncomputer vision domains, its potential for code understanding tasks remains\nlargely underexplored.\n  In this paper, we systematically investigate the effectiveness and usage of\nKD in code understanding tasks. Our study encompasses two popular types of KD\nmethods, i.e., logit-based and feature-based KD methods, experimenting across\neight student models and two teacher PLMs from different domains on three\ndownstream tasks. The experimental results indicate that KD consistently offers\nnotable performance boosts across student models with different sizes compared\nwith standard fine-tuning. Notably, code-specific PLM demonstrates better\neffectiveness as the teacher model. Among all KD methods, the latest\nfeature-based KD methods exhibit superior performance, enabling student models\nto retain up to 98% teacher performance with merely 5% parameters. Regarding\nstudent architecture, our experiments reveal that similarity with teacher\narchitecture does not necessarily lead to better performance. We further\ndiscuss the efficiency and behaviors in the KD process and inference, summarize\nthe implications of findings, and identify promising future directions.", "AI": {"tldr": "This paper demonstrates that knowledge distillation (KD) improves code understanding model efficiency, with feature-based KD and code-specific teachers achieving 98% performance at 5% model size. Architecture similarity doesn't guarantee better results, and KD significantly outperforms fine-tuning.", "motivation": "Pre-trained language models (PLMs) for code understanding face deployment challenges due to computational costs. Knowledge distillation (KD) is proposed as a compression technique to create efficient student models that retain most of the original PLMs' capabilities, but its application to code understanding remains underexplored.", "method": "The study evaluates two KD types (logit-based and feature-based) using eight student models and two teacher PLMs (from code and cross-domains) across three downstream tasks. Experiments compare performance, efficiency, and architectural choices during KD and inference.", "result": "KD consistently outperforms standard fine-tuning across student models of varying sizes. Feature-based KD achieves up to 98% of the teacher model's performance with only 5% of its parameters. Code-specific teacher models deliver better effectiveness than cross-domain ones, and student-teacher architecture similarity is not a performance determinant.", "conclusion": "The paper concludes that knowledge distillation (KD) is effective for code understanding tasks, with feature-based methods and code-specific teacher models yielding the best performance. Student model architecture similarity to the teacher does not correlate with performance, and KD enables significant parameter reduction while retaining high performance. The study provides insights into KD strategies and future research directions."}}
{"id": "2508.15495", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15495", "abs": "https://arxiv.org/abs/2508.15495", "authors": ["Dongjun Yu", "Xiao Yan", "Zhenrui Li", "Jipeng Xiao", "Haochuan He", "Yongda Yu", "Hao Zhang", "Guoping Rong", "Xiaobo Huang"], "title": "SynthCoder: A Synthetical Strategy to Tune LLMs for Code Completion", "comment": null, "summary": "Code completion is a prominent application of Large Language Models (LLMs) in\nsoftware engineering. Due to the near real-time response requirements of this\ntask, base models with small to medium-sized parameters are typically employed,\nsupplemented by various optimization and post-training techniques. However,\nthese optimization methods often have trade-offs, leading to a seesaw effect\nwhere performance improvements on certain datasets or metrics are accompanied\nby degradations on others -- sometimes even falling below the baseline model's\nperformance. This paper proposes SynthCoder, a model that integrates leading\nindustry practices to achieve state-of-the-art performance on the\nFill-in-the-Middle (FIM) code completion task. In specific, we first construct\na diverse dataset by combining Abstract Syntax Tree (AST) node extraction with\nheuristics that simulate developer behavior. Then we enrich our training corpus\nwith cross-file contextual information using the BM25 algorithm and call\ngraphs, enhancing the model's ability to perform code completion in both\nfile-level and repository-level scenarios. As the last step, we employ a\ntwo-stage training process using the Seed-Coder-8B-Base as the base model.\nFirst, we fine-tune the model using Curriculum Learning technology. Following\nthis, we perform alignment using Direct Preference Optimization (DPO) with\npreference pairs generated through Rejection Sampling. Experimental results\ndemonstrate that our final model excels on mainstream repository-level code\ncompletion benchmarks, including aiXcoder, ExecRepoBench, CrossCodeEval, and\nCoLT. Furthermore, our carefully curated training set effectively mitigates the\nmodel's tendency to just repeat existing code, a common issue existing in\nvarious code completion models.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.14925", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.14925", "abs": "https://arxiv.org/abs/2508.14925", "authors": ["Zhiqiang Wang", "Yichao Gao", "Yanting Wang", "Suyuan Liu", "Haifeng Sun", "Haoran Cheng", "Guanquan Shi", "Haohua Du", "Xiangyang Li"], "title": "MCPTox: A Benchmark for Tool Poisoning Attack on Real-World MCP Servers", "comment": null, "summary": "By providing a standardized interface for LLM agents to interact with\nexternal tools, the Model Context Protocol (MCP) is quickly becoming a\ncornerstone of the modern autonomous agent ecosystem. However, it creates novel\nattack surfaces due to untrusted external tools. While prior work has focused\non attacks injected through external tool outputs, we investigate a more\nfundamental vulnerability: Tool Poisoning, where malicious instructions are\nembedded within a tool's metadata without execution. To date, this threat has\nbeen primarily demonstrated through isolated cases, lacking a systematic,\nlarge-scale evaluation.\n  We introduce MCPTox, the first benchmark to systematically evaluate agent\nrobustness against Tool Poisoning in realistic MCP settings. MCPTox is\nconstructed upon 45 live, real-world MCP servers and 353 authentic tools. To\nachieve this, we design three distinct attack templates to generate a\ncomprehensive suite of 1312 malicious test cases by few-shot learning, covering\n10 categories of potential risks. Our evaluation on 20 prominent LLM agents\nsetting reveals a widespread vulnerability to Tool Poisoning, with o1-mini,\nachieving an attack success rate of 72.8\\%. We find that more capable models\nare often more susceptible, as the attack exploits their superior\ninstruction-following abilities. Finally, the failure case analysis reveals\nthat agents rarely refuse these attacks, with the highest refused rate\n(Claude-3.7-Sonnet) less than 3\\%, demonstrating that existing safety alignment\nis ineffective against malicious actions that use legitimate tools for\nunauthorized operation. Our findings create a crucial empirical baseline for\nunderstanding and mitigating this widespread threat, and we release MCPTox for\nthe development of verifiably safer AI agents. Our dataset is available at an\nanonymized repository: \\textit{https://anonymous.4open.science/r/AAAI26-7C02}.", "AI": {"tldr": "This paper introduces MCPTox, the first benchmark for evaluating LLM agent robustness against Tool Poisoning in MCP environments. Systematic testing reveals widespread vulnerabilities in leading models, particularly more capable ones, and exposes the ineffectiveness of current safety alignments against such threats.", "motivation": "Existing research on MCP vulnerabilities focused on attacks through tool outputs, while Tool Poisoning\u2014a threat involving malicious metadata within tools without execution\u2014lacks systematic evaluation.", "method": "The researchers developed MCPTox, a benchmark consisting of 45 real-world MCP servers and 353 authentic tools, employing three attack templates to generate 1312 malicious test cases via few-shot learning. They evaluated 20 prominent LLM agents to assess their vulnerability to Tool Poisoning.", "result": "High vulnerability to Tool Poisoning was identified in LLM agents, with o1-mini achieving a 72.8% attack success rate. More capable models were more susceptible due to enhanced instruction-following abilities, while existing safety measures failed to detect malicious unauthorized operations, with refusal rates as low as 3%.", "conclusion": "The study establishes an essential empirical baseline for understanding and mitigating the Tool Poisoning threat, emphasizing the necessity for improved safety mechanisms in autonomous agent systems."}}
{"id": "2508.15496", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15496", "abs": "https://arxiv.org/abs/2508.15496", "authors": ["Elena Masserini", "Diego Clerissi", "Daniela Micucci", "Jo\u00e3o R. Campos", "Leonardo Mariani"], "title": "Towards the Assessment of Task-based Chatbots: From the TOFU-R Snapshot to the BRASATO Curated Dataset", "comment": "10 pages, 10 figure, Accepted at IEEE International Symposium on\n  Software Reliability Engineering (ISSRE) 2025", "summary": "Task-based chatbots are increasingly being used to deliver real services, yet\nassessing their reliability, security, and robustness remains underexplored,\nalso due to the lack of large-scale, high-quality datasets. The emerging\nautomated quality assessment techniques targeting chatbots often rely on\nlimited pools of subjects, such as custom-made toy examples, or outdated, no\nlonger available, or scarcely popular agents, complicating the evaluation of\nsuch techniques. In this paper, we present two datasets and the tool support\nnecessary to create and maintain these datasets. The first dataset is RASA\nTASK-BASED CHATBOTS FROM GITHUB (TOFU-R), which is a snapshot of the Rasa\nchatbots available on GitHub, representing the state of the practice in\nopen-source chatbot development with Rasa. The second dataset is BOT RASA\nCOLLECTION (BRASATO), a curated selection of the most relevant chatbots for\ndialogue complexity, functional complexity, and utility, whose goal is to ease\nreproducibility and facilitate research on chatbot reliability.", "AI": {"tldr": "This paper introduces two datasets (TOFU-R, BRASATO) and tools to address the lack of quality data for chatbot evaluation, enabling systematic study of reliability and reproducibility.", "motivation": "Current automated quality assessment techniques for chatbots face evaluation challenges due to reliance on small-scale, outdated, or impractical datasets, hindering progress in ensuring reliability, security, and robustness.", "method": "The authors created two datasets: (1) TOFU-R, a snapshot of open-source Rasa chatbots on GitHub, and (2) BRASATO, a curated collection prioritizing dialogue/functional complexity and utility. They also provided tools for maintaining these datasets.", "result": "TOFU-R captures real-world open-source practices, while BRASATO offers a focused resource for studying chatbot quality. These datasets facilitate reproducibility and benchmarking in reliability research.", "conclusion": "The paper concludes that the presented datasets (TOFU-R and BRASATO) address the critical need for large-scale, high-quality resources, enabling robust evaluation and reproducibility in chatbot reliability research."}}
{"id": "2508.15031", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15031", "abs": "https://arxiv.org/abs/2508.15031", "authors": ["Kaixiang Zhao", "Lincan Li", "Kaize Ding", "Neil Zhenqiang Gong", "Yue Zhao", "Yushun Dong"], "title": "A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives", "comment": null, "summary": "Machine learning (ML) models have significantly grown in complexity and\nutility, driving advances across multiple domains. However, substantial\ncomputational resources and specialized expertise have historically restricted\ntheir wide adoption. Machine-Learning-as-a-Service (MLaaS) platforms have\naddressed these barriers by providing scalable, convenient, and affordable\naccess to sophisticated ML models through user-friendly APIs. While this\naccessibility promotes widespread use of advanced ML capabilities, it also\nintroduces vulnerabilities exploited through Model Extraction Attacks (MEAs).\nRecent studies have demonstrated that adversaries can systematically replicate\na target model's functionality by interacting with publicly exposed interfaces,\nposing threats to intellectual property, privacy, and system security. In this\npaper, we offer a comprehensive survey of MEAs and corresponding defense\nstrategies. We propose a novel taxonomy that classifies MEAs according to\nattack mechanisms, defense approaches, and computing environments. Our analysis\ncovers various attack techniques, evaluates their effectiveness, and highlights\nchallenges faced by existing defenses, particularly the critical trade-off\nbetween preserving model utility and ensuring security. We further assess MEAs\nwithin different computing paradigms and discuss their technical, ethical,\nlegal, and societal implications, along with promising directions for future\nresearch. This systematic survey aims to serve as a valuable reference for\nresearchers, practitioners, and policymakers engaged in AI security and\nprivacy. Additionally, we maintain an online repository continuously updated\nwith related literature at https://github.com/kzhao5/ModelExtractionPapers.", "AI": {"tldr": "This paper surveys model extraction attacks against MLaaS platforms, introduces a taxonomy for categorizing attacks/defenses, and analyzes their technical and societal impacts, providing a resource hub for the AI security community.", "motivation": "The adoption of MLaaS platforms, while democratizing ML access, introduces vulnerabilities via MEAs that threaten intellectual property, privacy, and security. This work addresses the need for structured analysis of MEAs and defenses to guide secure ML deployment.", "method": "The authors conduct a systematic survey of MEAs, propose a taxonomy classifying attacks by mechanisms, defenses, and computing environments, evaluate attack effectiveness, and analyze trade-offs between model utility and security across paradigms.", "result": "The study categorizes MEAs through its novel taxonomy, highlights limitations of existing defenses (particularly utility-security trade-offs), and discusses multidimensional implications. An actively updated online repository centralizes related research.", "conclusion": "This paper provides a comprehensive survey of Model Extraction Attacks (MEAs) and defense strategies, offering a novel taxonomy and analyzing technical, ethical, and societal implications. It serves as a reference for AI security researchers and policymakers, supported by an openly maintained literature repository."}}
{"id": "2508.15503", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15503", "abs": "https://arxiv.org/abs/2508.15503", "authors": ["Sebastian Baltes", "Florian Angermeir", "Chetan Arora", "Marvin Mu\u00f1oz Bar\u00f3n", "Chunyang Chen", "Lukas B\u00f6hme", "Fabio Calefato", "Neil Ernst", "Davide Falessi", "Brian Fitzgerald", "Davide Fucci", "Marcos Kalinowski", "Stefano Lambiase", "Daniel Russo", "Mircea Lungu", "Lutz Prechelt", "Paul Ralph", "Christoph Treude", "Stefan Wagner"], "title": "Evaluation Guidelines for Empirical Studies in Software Engineering involving LLMs", "comment": "Draft of evaluation guidelines for empirical studies in software\n  engineering involving LLMs (see also llm-guidelines.org)", "summary": "Large language models (LLMs) are increasingly being integrated into software\nengineering (SE) research and practice, yet their non-determinism, opaque\ntraining data, and evolving architectures complicate the reproduction and\nreplication of empirical studies. We present a community effort to scope this\nspace, introducing a taxonomy of LLM-based study types together with eight\nguidelines for designing and reporting empirical studies involving LLMs. The\nguidelines present essential (must) criteria as well as desired (should)\ncriteria and target transparency throughout the research process. Our\nrecommendations, contextualized by our study types, are: (1) to declare LLM\nusage and role; (2) to report model versions, configurations, and fine-tuning;\n(3) to document tool architectures; (4) to disclose prompts and interaction\nlogs; (5) to use human validation; (6) to employ an open LLM as a baseline; (7)\nto report suitable baselines, benchmarks, and metrics; and (8) to openly\narticulate limitations and mitigations. Our goal is to enable reproducibility\nand replicability despite LLM-specific barriers to open science. We maintain\nthe study types and guidelines online as a living resource for the community to\nuse and shape (llm-guidelines.org).", "AI": {"tldr": "This paper introduces a taxonomy and eight guidelines for ensuring reproducibility and transparency in empirical studies using Large Language Models (LLMs) in software engineering, addressing LLM-specific barriers like non-determinism and opaque training data.", "motivation": "Integration of LLMs in software engineering research creates challenges for reproducibility and replication due to their non-determinism, lack of transparent training data, and rapidly changing architectures.", "method": "Presented a community-derived taxonomy of LLM-based study types along with eight guidelines. The guidelines combine essential (must) and desired (should) criteria, covering aspects like LLM declaration, version reporting, documentation, and benchmarking.", "result": "The proposed taxonomy and guidelines, hosted as a living online resource (llm-guidelines.org), provide a framework for designing, reporting, and evaluating LLM-driven empirical research in SE, focusing on transparency and baselines.", "conclusion": "The study advocates for a structured approach to LLM-based SE research to overcome barriers to open science, emphasizing that declaring LLM usage, disclosing details, and articulating limitations can improve reproducibility and replicability despite the challenges posed by LLMs."}}
{"id": "2508.15036", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15036", "abs": "https://arxiv.org/abs/2508.15036", "authors": ["Ruyi Ding", "Tianhong Xu", "Xinyi Shen", "Aidong Adam Ding", "Yunsi Fei"], "title": "MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs", "comment": "This paper will appear in CCS 2025", "summary": "The transformer architecture has become a cornerstone of modern AI, fueling\nremarkable progress across applications in natural language processing,\ncomputer vision, and multimodal learning. As these models continue to scale\nexplosively for performance, implementation efficiency remains a critical\nchallenge. Mixture of Experts (MoE) architectures, selectively activating\nspecialized subnetworks (experts), offer a unique balance between model\naccuracy and computational cost. However, the adaptive routing in MoE\narchitectures, where input tokens are dynamically directed to specialized\nexperts based on their semantic meaning inadvertently opens up a new attack\nsurface for privacy breaches. These input-dependent activation patterns leave\ndistinctive temporal and spatial traces in hardware execution, which\nadversaries could exploit to deduce sensitive user data. In this work, we\npropose MoEcho, discovering a side channel analysis based attack surface that\ncompromises user privacy on MoE based systems. Specifically, in MoEcho, we\nintroduce four novel architectural side channels on different computing\nplatforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and\nPerformance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting\nthese vulnerabilities, we propose four attacks that effectively breach user\nprivacy in large language models (LLMs) and vision language models (VLMs) based\non MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,\nVisual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first\nruntime architecture level security analysis of the popular MoE structure\ncommon in modern transformers, highlighting a serious security and privacy\nthreat and calling for effective and timely safeguards when harnessing MoE\nbased models for developing efficient large scale AI services.", "AI": {"tldr": "MoEcho exposes hidden security risks in MoE-based AI models: adversarial side-channel attacks can extract user data from hardware traces, demanding urgent safeguards for scalable AI systems.", "motivation": "As MoE architectures scale for performance, their input-dependent routing mechanisms create exploitable side channels that could leak sensitive data, posing significant privacy risks in applications like LLMs and VLMs.", "method": "The authors propose MoEcho, a runtime analysis framework that identifies four novel architectural side channels (Cache Occupancy, Pageout+Reload, Performance Counter, and TLB Evict+Reload) on CPUs/GPUs. They demonstrate four attacks exploiting these channels to breach privacy in MoE-based models.", "result": "The four MoEcho attacks successfully infer user prompts, reconstruct responses, and extract visual information from MoE-based models, proving the feasibility of breaching privacy via hardware execution traces.", "conclusion": "This paper is the first to analyze the security risks of MoE architectures in transformers, identifying critical privacy threats through side-channel attacks and urging immediate mitigation strategies for secure large-scale AI deployment."}}
