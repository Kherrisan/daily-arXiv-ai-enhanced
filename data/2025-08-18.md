<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 10]
- [cs.SE](#cs.SE) [Total: 10]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [MCP-Guard: A Defense Framework for Model Context Protocol Integrity in Large Language Model Applications](https://arxiv.org/abs/2508.10991)
*Wenpeng Xing,Zhonghao Qi,Yupeng Qin,Yilin Li,Caini Chang,Jiahui Yu,Changting Lin,Zhenzhen Xie,Meng Han*

Main category: cs.CR

TL;DR: This paper introduces MCP-Guard, a defense architecture for securing LLM-tool interactions against threats like prompt injection, and proposes MCP-AttackBench, a benchmark of 70,000+ samples for training/evaluating such defenses.


<details>
  <summary>Details</summary>
Motivation: The integration of LLMs with external tools via protocols like Model Context Protocol (MCP) creates significant security risks, including prompt injection and data exfiltration. These threats require systematic protection mechanisms to maintain trust and safety in LLM-based systems.

Method: A three-stage defense pipeline: (1) lightweight static scanning for obvious threats, (2) a fine-tuned E5-based deep neural detector for semantic attacks, and (3) a lightweight LLM arbitrator for final validation. Combined with MCP-AttackBench - a benchmark containing 70,000+ GPT-4 augmented attack samples in MCP format.

Result: The E5-based model achieves 96.01% accuracy in detecting adversarial prompts, while the layered approach effectively reduces false positives through multiple detection stages and LLM validation.

Conclusion: MCP-Guard provides a robust, practical solution for securing LLM-tool ecosystems against both syntactic and semantic attacks, while MCP-AttackBench establishes a foundational benchmark for future security research in this domain.

Abstract: The integration of Large Language Models (LLMs) with external tools via
protocols such as the Model Context Protocol (MCP) introduces critical security
vulnerabilities, including prompt injection, data exfiltration, and other
threats. To counter these challenges, we propose MCP-Guard, a robust, layered
defense architecture designed for LLM--tool interactions. MCP-Guard employs a
three-stage detection pipeline that balances efficiency with accuracy: it
progresses from lightweight static scanning for overt threats and a deep neural
detector for semantic attacks, to our fine-tuned E5-based model achieves
(96.01) accuracy in identifying adversarial prompts. Finally, a lightweight LLM
arbitrator synthesizes these signals to deliver the final decision while
minimizing false positives. To facilitate rigorous training and evaluation, we
also introduce MCP-AttackBench, a comprehensive benchmark of over 70,000
samples. Sourced from public datasets and augmented by GPT-4, MCP-AttackBench
simulates diverse, real-world attack vectors in the MCP format, providing a
foundation for future research into securing LLM-tool ecosystems.

</details>


### [2] [A Constant-Time Hardware Architecture for the CSIDH Key-Exchange Protocol](https://arxiv.org/abs/2508.11082)
*Sina Bagheri,Masoud Kaveh,Francisco Hernando-Gallego,Diego Martín,Nuria Serrano*

Main category: cs.CR

TL;DR: This paper presents the first comprehensive hardware study of the CSIDH post-quantum key exchange protocol, providing performance baselines on both FPGA and ASIC platforms. The design uses a pipelined ALU and parallelized schoolbook multiplier to accelerate 512-bit field operations, achieving key generation latencies of 515 ms (FPGA) and 591 ms (ASIC).


<details>
  <summary>Details</summary>
Motivation: CSIDH is recognized for its small key sizes in post-quantum cryptography but faces challenges in computational efficiency and constant-time implementation to avoid side-channel attacks. No prior hardware benchmarks exist for CSIDH, necessitating a study to establish performance baselines for future isogeny-based cryptography accelerators.

Method: The authors developed a unified hardware architecture featuring a finite state machine (FSM) to manage a deeply pipelined arithmetic logic unit (ALU). The ALU leverages a parallelized schoolbook multiplier optimized for 512-bit finite field operations, supporting both FPGA and ASIC platforms. Key design metrics include completing 512×512-bit multiplications in 22 cycles and Montgomery modular multiplication in 87 cycles.

Result: On a Xilinx Zynq UltraScale+ FPGA (200 MHz), the design achieves a key generation latency of 515 ms. In an ASIC implementation (180nm process) with similar clocking, the latency is 591 ms. Key generation requires 1.03×10⁸ and 1.065×10⁸ clock cycles respectively, offering the first public hardware performance metrics for CSIDH.

Conclusion: By delivering the first FPGA and ASIC baselines for CSIDH, this work establishes critical benchmarks for optimizing isogeny-based post-quantum cryptography hardware. The results address the computational complexity of CSIDH while emphasizing secure, constant-time implementation requirements for practical adoption.

Abstract: The commutative supersingular isogeny Diffie-Hellman (CSIDH) algorithm is a
promising post-quantum key exchange protocol, notable for its exceptionally
small key sizes, but hindered by computationally intensive key generation.
Furthermore, practical implementations must operate in constant time to
mitigate side-channel vulnerabilities, which presents an additional performance
challenge. This paper presents, to our knowledge, the first comprehensive
hardware study of CSIDH, establishing a performance baseline with a unified
architecture on both field-programmable gate array (FPGA) and
application-specific integrated circuit (ASIC) platforms. The architecture
features a top-level finite state machine (FSM) that orchestrates a deeply
pipelined arithmetic logic unit (ALU) to accelerate the underlying 512-bit
finite field operations. The ALU employs a parallelized schoolbook multiplier,
completing a 512$\times$512-bit multiplication in 22 clock cycles and enabling
a full Montgomery modular multiplication in 87 cycles. The constant-time
CSIDH-512 design requires $1.03\times10^{8}$ clock cycles per key generation.
When implemented on a Xilinx Zynq UltraScale+ FPGA, the architecture achieves a
200 MHz clock frequency, corresponding to a 515 ms latency. For ASIC
implementation in a 180nm process, the design requires $1.065\times10^{8}$
clock cycles and achieves a \textasciitilde 180 MHz frequency, resulting in a
key generation latency of 591 ms. By providing the first public hardware
performance metrics for CSIDH on both FPGA and ASIC platforms, this work
delivers a crucial benchmark for future isogeny-based post-quantum cryptography
(PQC) accelerators.

</details>


### [3] [HEIR: A Universal Compiler for Homomorphic Encryption](https://arxiv.org/abs/2508.11095)
*Asra Ali,Jaeho Choi,Bryant Gipson,Shruthi Gorantala,Jeremy Kun,Wouter Legiest,Lawrence Lim,Alexander Viand,Meron Zerihun Demissie,Hongren Zheng*

Main category: cs.CR

TL;DR: HEIR is a unified framework for building homomorphic encryption compilers using MLIR, enabling integration of diverse HE techniques and optimizations for research and industry applications.


<details>
  <summary>Details</summary>
Motivation: Current HE optimization techniques are fragmented and difficult to combine or compare, necessitating a platform for exploration and benchmarking across the entire HE stack.

Method: Built on MLIR, HEIR introduces HE-specific abstraction layers that facilitate implementation of existing optimizations and novel research ideas, with support for multiple frontends like Python.

Result: HEIR's design was validated by porting extensive HE research literature, demonstrating its capacity to handle more complex programs than prior works, and showing adoption as a de facto compiler.

Conclusion: HEIR establishes itself as the emerging standard for HE compiler development, bridging academic research and industry implementation through its flexible, extensible architecture.

Abstract: This work presents Homomorphic Encryption Intermediate Representation (HEIR),
a unified approach to building homomorphic encryption (HE) compilers. HEIR aims
to support all mainstream techniques in homomorphic encryption, integrate with
all major software libraries and hardware accelerators, and advance the field
by providing a platform for research and benchmarking. Built on the MLIR
compiler framework, HEIR introduces HE-specific abstraction layers at which
existing optimizations and new research ideas may be easily implemented.
Although many HE optimization techniques have been proposed, it remains
difficult to combine or compare them effectively. HEIR provides a means to
effectively explore the space of HE optimizations. HEIR addresses the entire HE
stack and includes support for various frontends, including Python. The
contribution of this work includes: (1) We introduce HEIR as a framework for
building HE compilers. (2) We validate HEIR's design by porting a large
fraction of the HE literature to HEIR, and we argue that HEIR can tackle more
complicated and diverse programs than prior literature. (3) We provide evidence
that HEIR is emerging as the de facto HE compiler for academic research and
industry development.

</details>


### [4] [Salty Seagull: A VSAT Honeynet to Follow the Bread Crumb of Attacks in Ship Networks](https://arxiv.org/abs/2508.11325)
*Georgios Michail Makrakis,Jeroen Pijpker,Remco Hassing,Rob Loves,Stephen McCombie*

Main category: cs.CR

TL;DR: The paper introduces Salty Seagull, a honeynet simulating a VSAT system for maritime cybersecurity research. Exposed to the internet for 30 days, it observed many generic attacks but only one attacker exploited its specialized vulnerabilities without fully accessing the system.


<details>
  <summary>Details</summary>
Motivation: The maritime industry faces growing cyber threats due to ships' specialized, interconnected network infrastructures and legacy systems, creating urgent needs for innovative methods to analyze attacker behaviors and identify vulnerabilities.

Method: The authors designed Salty Seagull as a VSAT simulation honeynet that integrates known maritime system vulnerabilities. They implemented dual interaction channels (Web dashboard and CLI) and monitored network activity for 30 days to collect attack data.

Result: Despite 30 days of exposure, only one attacker demonstrated awareness of the honeynet's maritime-specific setup. While multiple generic attacks occurred, no incidents reached deeply simulated legacy components suggesting limited advanced attacker engagement.

Conclusion: The deployment demonstrates honeynets' potential to detect and analyze maritime-specific cyber threats, but also reveals the relative obscurity of ship systems to most attackers despite their strategic importance to global trade.

Abstract: Cyber threats against the maritime industry have increased notably in recent
years, highlighting the need for innovative cybersecurity approaches. Ships, as
critical assets, possess highly specialized and interconnected network
infrastructures, where their legacy systems and operational constraints further
exacerbate their vulnerability to cyberattacks. To better understand this
evolving threat landscape, we propose the use of cyber-deception techniques and
in particular honeynets, as a means to gather valuable insights into ongoing
attack campaigns targeting the maritime sector.
  In this paper we present Salty Seagull, a honeynet conceived to simulate a
VSAT system for ships. This environment mimics the operations of a functional
VSAT system onboard and, at the same time, enables a user to interact with it
through a Web dashboard and a CLI environment. Furthermore, based on existing
vulnerabilities, we purposefully integrate them into our system to increase
attacker engagement. We exposed our honeynet for 30 days to the Internet to
assess its capability and measured the received interaction. Results show that
while numerous generic attacks have been attempted, only one curious attacker
with knowledge of the nature of the system and its vulnerabilities managed to
access it, without however exploring its full potential.

</details>


### [5] [RMSL: Weakly-Supervised Insider Threat Detection with Robust Multi-sphere Learning](https://arxiv.org/abs/2508.11472)
*Yang Wang,Yaxin Zhao,Xinyu Jiao,Sihan Xu,Xiangrui Cai,Ying Zhang,Xiaojie Yuan*

Main category: cs.CR

TL;DR: This paper proposes Robust Multi-sphere Learning (RMSL), a weakly supervised framework for behavior-level insider threat detection that uses sequence-level labels to address annotation challenges and improve discriminative feature learning through multiple hyper-spheres and adaptive self-training.


<details>
  <summary>Details</summary>
Motivation: Traditional unsupervised insider threat detection struggles with high false positives due to ambiguity in behavior-level anomalies, while fully supervised approaches require costly fine-grained annotations. Sequence-level annotations provide a scalable intermediate solution.

Method: RMSL combines (1) one-class classification as initial anomaly-free learning, (2) multiple instance learning to capture sequence patterns, and (3) adaptive behavior-level self-training using confidence scores to refine hyper-sphere representations of normal behaviors within weakly labeled sequences.

Result: Experiments show RMSL significantly enhances behavior-level detection performance by effectively distinguishing anomalies from normal behaviors using sequence-level supervision, outperforming unsupervised and conventional weakly supervised approaches.

Conclusion: The RMSL framework demonstrates that weak sequence-level labels can effectively train models to detect behavior-level anomalies, overcoming limitations of annotation costs and ambiguity through innovative hyper-sphere refinement strategies.

Abstract: Insider threat detection aims to identify malicious user behavior by
analyzing logs that record user interactions. Due to the lack of fine-grained
behavior-level annotations, detecting specific behavior-level anomalies within
user behavior sequences is challenging. Unsupervised methods face high false
positive rates and miss rates due to the inherent ambiguity between normal and
anomalous behaviors. In this work, we instead introduce weak labels of behavior
sequences, which have lower annotation costs, i.e., the training labels
(anomalous or normal) are at sequence-level instead of behavior-level, to
enhance the detection capability for behavior-level anomalies by learning
discriminative features. To achieve this, we propose a novel framework called
Robust Multi-sphere Learning (RMSL). RMSL uses multiple hyper-spheres to
represent the normal patterns of behaviors. Initially, a one-class classifier
is constructed as a good anomaly-supervision-free starting point. Building on
this, using multiple instance learning and adaptive behavior-level
self-training debiasing based on model prediction confidence, the framework
further refines hyper-spheres and feature representations using weak
sequence-level labels. This approach enhances the model's ability to
distinguish between normal and anomalous behaviors. Extensive experiments
demonstrate that RMSL significantly improves the performance of behavior-level
insider threat detection.

</details>


### [6] [KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value Estimation](https://arxiv.org/abs/2508.11495)
*Jingnan Xu,Leixia Wang,Xiaofeng Meng*

Main category: cs.CR

TL;DR: The paper proposes KV-Auditor, a framework for auditing local differential privacy (LDP) key-value estimation protocols by deriving empirical privacy lower bounds, addressing gaps in existing LDP auditing methods that focus mainly on discrete frequency estimation.


<details>
  <summary>Details</summary>
Motivation: Existing LDP auditing methods are limited to discrete data frequency estimation, neglecting correlated key-value data requiring both discrete key and continuous value analysis, creating a need for robust auditing frameworks in practical implementations.

Method: KV-Auditor analyzes unbounded output distributions instead of binary predictions, classifies key-value mechanisms into interactive/non-interactive categories, and introduces horizontal/vertical auditing for non-interactive cases (depending on domain size) and iterative segmentation for interactive ones.

Result: Extensive experiments validated KV-Auditor's effectiveness in auditing LDP mechanisms, providing actionable insights to optimize key-value estimation protocols that handle both discrete keys and continuous values under LDP constraints.

Conclusion: KV-Auditor enables comprehensive auditing of LDP key-value mechanisms by addressing domain-specific challenges through tailored empirical privacy bounds, offering a practical solution to verify real-world LDP implementations across diverse data types.

Abstract: To protect privacy for data-collection-based services, local differential
privacy (LDP) is widely adopted due to its rigorous theoretical bound on
privacy loss. However, mistakes in complex theoretical analysis or subtle
implementation errors may undermine its practical guarantee. To address this,
auditing is crucial to confirm that LDP protocols truly protect user data.
However, existing auditing methods, though, mainly target machine learning and
federated learning tasks based on centralized differentially privacy (DP), with
limited attention to LDP. Moreover, the few studies on LDP auditing focus
solely on simple frequency estimation task for discrete data, leaving
correlated key-value data - which requires both discrete frequency estimation
for keys and continuous mean estimation for values - unexplored.
  To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based
key-value estimation mechanisms by estimating their empirical privacy lower
bounds. Rather than traditional LDP auditing methods that relies on binary
output predictions, KV-Auditor estimates this lower bound by analyzing
unbounded output distributions, supporting continuous data. Specifically, we
classify state-of-the-art LDP key-value mechanisms into interactive and
non-interactive types. For non-interactive mechanisms, we propose horizontal
KV-Auditor for small domains with sufficient samples and vertical KV-Auditor
for large domains with limited samples. For interactive mechanisms, we design a
segmentation strategy to capture incremental privacy leakage across iterations.
Finally, we perform extensive experiments to validate the effectiveness of our
approach, offering insights for optimizing LDP-based key-value estimators.

</details>


### [7] [Copyright Protection for Large Language Models: A Survey of Methods, Challenges, and Trends](https://arxiv.org/abs/2508.11548)
*Zhenhua Xu,Xubin Yue,Zhebo Wang,Qichen Liu,Xixiang Zhao,Jingxuan Zhang,Wenjun Zeng,Wengpeng Xing,Dezhang Kong,Changting Lin,Meng Han*

Main category: cs.CR

TL;DR: This work provides a comprehensive survey of LLM copyright protection, bridging gaps in model fingerprinting research by clarifying terminology, analyzing techniques, and presenting novel approaches like fingerprint transfer/removal.


<details>
  <summary>Details</summary>
Motivation: Need for protecting costly, proprietary LLMs from misuse; existing surveys focus only on text watermarking, neglecting model-level protection methods (watermarking/fingerprinting) despite their critical importance.

Method: Systematic analysis covering (1) terminology unification of text/model watermarking/fingerprinting, (2) comparative review of text watermarking techniques usable for model fingerprinting, (3) categorization of LLM model fingerprinting approaches, (4) introduction of fingerprint transfer/removal techniques, (5) evaluation metrics framework.

Result: First clarification of conceptual relationships between text and model fingerprinting; comprehensive comparison of techniques; novel methods for fingerprint manipulation (transfer, removal); structured evaluation criteria for effectiveness/robustness/stealthiness.

Conclusion: Establishes foundational understanding of LLM copyright protection technologies to guide future research in model fingerprinting and enhance commercial IP protection for LLMs.

Abstract: Copyright protection for large language models is of critical importance,
given their substantial development costs, proprietary value, and potential for
misuse. Existing surveys have predominantly focused on techniques for tracing
LLM-generated content-namely, text watermarking-while a systematic exploration
of methods for protecting the models themselves (i.e., model watermarking and
model fingerprinting) remains absent. Moreover, the relationships and
distinctions among text watermarking, model watermarking, and model
fingerprinting have not been comprehensively clarified. This work presents a
comprehensive survey of the current state of LLM copyright protection
technologies, with a focus on model fingerprinting, covering the following
aspects: (1) clarifying the conceptual connection from text watermarking to
model watermarking and fingerprinting, and adopting a unified terminology that
incorporates model watermarking into the broader fingerprinting framework; (2)
providing an overview and comparison of diverse text watermarking techniques,
highlighting cases where such methods can function as model fingerprinting; (3)
systematically categorizing and comparing existing model fingerprinting
approaches for LLM copyright protection; (4) presenting, for the first time,
techniques for fingerprint transfer and fingerprint removal; (5) summarizing
evaluation metrics for model fingerprints, including effectiveness,
harmlessness, robustness, stealthiness, and reliability; and (6) discussing
open challenges and future research directions. This survey aims to offer
researchers a thorough understanding of both text watermarking and model
fingerprinting technologies in the era of LLMs, thereby fostering further
advances in protecting their intellectual property.

</details>


### [8] [Pushing the Limits of Frequency Analysis in Leakage Abuse Attacks](https://arxiv.org/abs/2508.11563)
*Nathaniel Moyer,Charalampos Papamanthou,Evgenios Kornaropoulos*

Main category: cs.CR

TL;DR: The paper introduces a new framework, LAMA, for frequency analysis attacks on searchable encryption schemes with access-pattern leakage, demonstrating its effectiveness on high-dimensional data and identifying optimal query distributions for mitigation.


<details>
  <summary>Details</summary>
Motivation: Existing cryptanalyses of encrypted range queries assume either uniformly random queries or search-pattern leakage. This work addresses the unknown impact of access-pattern leakage combined with query distribution knowledge, aiming to understand reconstruction limits and potential mitigations.

Method: The authors generalize frequency analysis via leakage-abuse attacks using LAMA, a parameterized framework that optimally matches retrieval frequencies with plaintext probabilities. They prove LAMA's limits for any convex query class (not just axis-aligned) and implement it for multi-dimensional queries.

Result: LAMA successfully reconstructs plaintext data from encrypted range queries in up to 4 dimensions. The analysis identifies query distributions (like uniform) that maximize entropy for encrypted range queries, complicating attacker reconstruction.

Conclusion: LAMA provides a comprehensive model for frequency analysis attacks on SE schemes, proving theoretical limits and practical capabilities. The work shows how carefully designed query distributions can resist frequency-based reconstruction, offering immediate design guidance for more secure encrypted search systems.

Abstract: Searchable encryption (SE) is the most scalable cryptographic primitive for
searching on encrypted data. Typical SE constructions often allow
access-pattern leakage, revealing which encrypted records are retrieved in the
server's responses. All the known generic cryptanalyses assume either that the
queries are issued uniformly at random or that the attacker observes the
search-pattern leakage. It remains unclear what can be reconstructed when using
only the access-pattern leakage and knowledge of the query distribution. In
this work, we focus on the cryptanalytic technique of frequency analysis in the
context of leakage-abuse attacks on schemes that support encrypted range
queries. Frequency analysis matches the frequency of retrieval of an encrypted
record with a plaintext value based on its probability of retrieval that
follows from the knowledge of the query distribution. We generalize this
underexplored cryptanalytic technique and introduce a generic attack framework
called Leakage-Abuse via Matching (LAMA) that works even on high-dimensional
encrypted data. We identify a parameterization of LAMA that brings frequency
analysis to its limit -- that is, we prove that there is no additional
frequency matching that an attacker can perform to refine the result.
Furthermore, we show that our results hold for any class of convex queries, and
not just axis-aligned rectangles, which is the assumption in all other attacks
on range schemes. Using these results, we identify query distributions that
make frequency analysis challenging for the attacker and, thus, can act as a
mitigation mechanism. Finally, we implement and benchmark LAMA and reconstruct,
for the first time, plaintext data from encrypted range queries spanning up to
four dimensions.

</details>


### [9] [Activate Me!: Designing Efficient Activation Functions for Privacy-Preserving Machine Learning with Fully Homomorphic Encryption](https://arxiv.org/abs/2508.11575)
*Nges Brian Njungle,Michel A. Kinsy*

Main category: cs.CR

TL;DR: This paper designs and evaluates FHE-compatible activation functions for neural networks, demonstrating trade-offs between accuracy and inference time in encrypted ML models.


<details>
  <summary>Details</summary>
Motivation: Machine learning in sensitive domains requires privacy-preserving techniques like FHE, which supports only linear operations but necessitates non-linear activation functions for modern neural networks.

Method: The authors implemented Square and ReLU functions on LeNet-5 and ResNet-20 using OpenFHE's CKKS scheme, evaluating a traditional polynomial approximation versus a novel scheme-switching approach for ReLU.

Result: Square achieved 99.4% accuracy at 128s/image for LeNet-5; ReLU using polynomial approximation yielded 83.8% accuracy at 1,145s/image for ResNet-20, while scheme-switching improved accuracy to 89.8% at 1,697s/image.

Conclusion: The study reveals a critical efficiency-accuracy trade-off in FHE-based ML, with shallow networks favoring fast Square functions and deeper models requiring careful selection of ReLU implementations to optimize performance.

Abstract: The growing adoption of machine learning in sensitive areas such as
healthcare and defense introduces significant privacy and security challenges.
These domains demand robust data protection, as models depend on large volumes
of sensitive information for both training and inference. Fully Homomorphic
Encryption (FHE) presents a compelling solution by enabling computations
directly on encrypted data, maintaining confidentiality across the entire
machine learning workflow. However, FHE inherently supports only linear
operations, making it difficult to implement non-linear activation functions,
essential components of modern neural networks. This work focuses on designing,
implementing, and evaluating activation functions tailored for FHE-based
machine learning. We investigate two commonly used functions: the Square
function and Rectified Linear Unit (ReLU), using LeNet-5 and ResNet-20
architectures with the CKKS scheme from the OpenFHE library. For ReLU, we
assess two methods: a conventional low-degree polynomial approximation and a
novel scheme-switching technique that securely evaluates ReLU under FHE
constraints. Our findings show that the Square function performs well in
shallow networks like LeNet-5, achieving 99.4% accuracy with 128 seconds per
image. In contrast, deeper models like ResNet-20 benefit more from ReLU. The
polynomial approximation yields 83.8% accuracy with 1,145 seconds per image,
while our scheme-switching method improves accuracy to 89.8%, albeit with a
longer inference time of 1,697 seconds. These results underscore a critical
trade-off in FHE-based ML: faster activation functions often reduce accuracy,
whereas those preserving accuracy demand greater computational resources.

</details>


### [10] [CryptoScope: Utilizing Large Language Models for Automated Cryptographic Logic Vulnerability Detection](https://arxiv.org/abs/2508.11599)
*Zhihao Li,Zimo Ji,Tao Zheng,Hao Ren,Xiao Lan*

Main category: cs.CR

TL;DR: CryptoScope is an LLM-based framework that enhances cryptographic vulnerability detection through CoT prompting and RAG, outperforming existing models and discovering 9 new flaws in open-source projects.


<details>
  <summary>Details</summary>
Motivation: Cryptographic algorithm implementations often contain hard-to-detect logic flaws, necessitating automated detection methods to improve security.

Method: CryptoScope integrates Chain-of-Thought (CoT) prompting with Retrieval-Augmented Generation (RAG), leveraging a cryptographic knowledge base with 12,000+ entries to analyze code across 11 programming languages.

Result: Achieved 11.62% improvement over DeepSeek-V3, 20.28% over GPT-4o-mini, and 28.69% over GLM-4-Flash on the LLM-CLVA benchmark (92 real-world/synthetic CVE cases). Identified 9 undisclosed vulnerabilities in popular open-source cryptographic projects.

Conclusion: CryptoScope demonstrates significant performance gains over state-of-the-art LLM baselines in detecting cryptographic vulnerabilities, validating its effectiveness through both benchmark results and real-world security discoveries.

Abstract: Cryptographic algorithms are fundamental to modern security, yet their
implementations frequently harbor subtle logic flaws that are hard to detect.
We introduce CryptoScope, a novel framework for automated cryptographic
vulnerability detection powered by Large Language Models (LLMs). CryptoScope
combines Chain-of-Thought (CoT) prompting with Retrieval-Augmented Generation
(RAG), guided by a curated cryptographic knowledge base containing over 12,000
entries. We evaluate CryptoScope on LLM-CLVA, a benchmark of 92 cases primarily
derived from real-world CVE vulnerabilities, complemented by cryptographic
challenges from major Capture The Flag (CTF) competitions and synthetic
examples across 11 programming languages. CryptoScope consistently improves
performance over strong LLM baselines, boosting DeepSeek-V3 by 11.62%,
GPT-4o-mini by 20.28%, and GLM-4-Flash by 28.69%. Additionally, it identifies 9
previously undisclosed flaws in widely used open-source cryptographic projects.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [11] [The Impact of Large Language Models (LLMs) on Code Review Process](https://arxiv.org/abs/2508.11034)
*Antonio Collante,Samuel Abedu,SayedHassan Khatoonabadi,Ahmad Abdellatif,Ebube Alor,Emad Shihab*

Main category: cs.SE

TL;DR: This paper examines the impact of GPT-assisted tools on GitHub pull request (PR) workflows, revealing significant reductions in resolution time and phase-specific improvements in code review efficiency.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the lack of research on how LLMs like GPT affect specific phases of the code review process, building on the broader context of LLMs enhancing software development productivity.

Method: The researchers analyzed 25,473 PRs from 9,254 GitHub projects to identify GPT-assisted PRs using keyword detection, regex filtering, and manual verification (95% accuracy). Statistical methods, including multiple linear regression and Mann-Whitney U tests, were applied to assess performance differences across PR phases.

Result: GPT-assisted PRs reduced median resolution time by 60% (9 vs. 23 hours), with 33% shorter review times and 87% less waiting time before acceptance. Analysis of 300 PRs showed developers primarily used GPT for code optimization (60%), bug fixing (26%), and documentation (12%).

Conclusion: Early integration of GPT in code reviews enhances efficiency at multiple stages. The findings highlight actionable benefits for software teams, such as time savings and improved error resolution, and emphasize GPT's role in optimizing workflows and collaboration.

Abstract: Large language models (LLMs) have recently gained prominence in the field of
software development, significantly boosting productivity and simplifying
teamwork. Although prior studies have examined task-specific applications, the
phase-specific effects of LLM assistance on the efficiency of code review
processes remain underexplored. This research investigates the effect of GPT on
GitHub pull request (PR) workflows, with a focus on reducing resolution time,
optimizing phase-specific performance, and assisting developers. We curated a
dataset of 25,473 PRs from 9,254 GitHub projects and identified GPT-assisted
PRs using a semi-automated heuristic approach that combines keyword-based
detection, regular expression filtering, and manual verification until
achieving 95% labeling accuracy. We then applied statistical modeling,
including multiple linear regression and Mann-Whitney U test, to evaluate
differences between GPT-assisted and non-assisted PRs, both at the overall
resolution level and across distinct review phases. Our research has revealed
that early adoption of GPT can substantially boost the effectiveness of the PR
process, leading to considerable time savings at various stages. Our findings
suggest that GPT-assisted PRs reduced median resolution time by more than 60%
(9 hours compared to 23 hours for non-assisted PRs). We discovered that
utilizing GPT can reduce the review time by 33% and the waiting time before
acceptance by 87%. Analyzing a sample dataset of 300 GPT-assisted PRs, we
discovered that developers predominantly use GPT for code optimization (60%),
bug fixing (26%), and documentation updates (12%). This research sheds light on
the impact of the GPT model on the code review process, offering actionable
insights for software teams seeking to enhance workflows and promote seamless
collaboration.

</details>


### [12] [Diffusion is a code repair operator and generator](https://arxiv.org/abs/2508.11110)
*Mukul Singh,Gust Verbruggen,Vu Le,Sumit Gulwani*

Main category: cs.SE

TL;DR: This paper explores using pre-trained code diffusion models for last-mile repair tasks through two applications: resuming diffusion on noisy broken code and generating synthetic training data. Experiments span Python, Excel, and PowerShell domains.


<details>
  <summary>Details</summary>
Motivation: Code diffusion models iteratively denoise code latents, with later steps resembling small edits typical of last-mile repairs. Leveraging existing models avoids costly retraining while potentially improving repair efficiency.

Method: 1) Noise-based repair: Add noise to broken code then resume diffusion for repairs. 2) Data generation: Sample intermediate (input) and final (output) program states from diffusion process for synthetic training pairs. Evaluated across three code domains.

Result: Demonstrated applications of diffusion models to last-mile repair through experiments in three domains, establishing their potential for 1) direct repair by resuming diffusion and 2) generating efficient synthetic training data for repair tasks.

Conclusion: Pre-trained code diffusion models can effectively address last-mile repairs through noise resumption and synthetic data generation, offering computationally efficient solutions that repurpose existing model capabilities without task-specific training.

Abstract: Code diffusion models generate code by iteratively removing noise from the
latent representation of a code snippet. During later steps of the diffusion
process, when the code snippet has almost converged, differences between
discrete representations of these snippets look like last-mile repairs applied
to broken or incomplete code. We evaluate the extent to which this resemblance
can be exploited to leverage pre-trained code diffusion models for the problem
of last-mile repair by considering two applications with significant potential.
First, we can leverage the diffusion model for last-mile repair by adding noise
to a broken code snippet and resuming the diffusion process. Second, we can
leverage the diffusion model to generate arbitrary amount of training data for
last-mile repair tasks (that are computationally more efficient) by sampling an
intermediate program (input) and the final program (output) from the diffusion
process. We perform experiments on 3 domains (Python, Excel and PowerShell) to
evaluate applications, as well as analyze properties.

</details>


### [13] [AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities](https://arxiv.org/abs/2508.11126)
*Huanting Wang,Jingzhi Gong,Huawei Zhang,Zheng Wang*

Main category: cs.SE

TL;DR: This survey reviews AI agentic programming, a paradigm using large language models to autonomously plan, execute, and iteratively refine software development tasks through interaction with external tools, examining techniques, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: The rapid emergence of AI agentic programming necessitates defining its scope, consolidating technical foundations, and identifying open research challenges to guide future development and ensure reliable, trustworthy systems.

Method: The study provides a taxonomy of agent behaviors and architectures, analyzes core techniques (planning, memory/context management, tool integration, execution monitoring), evaluates benchmarks, and assesses current methodologies for coding agent performance.

Result: Identified key challenges in long-context handling, memory persistence across tasks, and system safety/alignment with human collaboration issues; highlighted opportunities for improving reliability, adaptability, and transparency in agentic systems.

Conclusion: This survey synthesizes advancements and outlines future research directions to establish foundational frameworks for next-generation AI agentic programming systems that are both capable and trustworthy.

Abstract: AI agentic programming is an emerging paradigm in which large language models
(LLMs) autonomously plan, execute, and interact with external tools like
compilers, debuggers, and version control systems to iteratively perform
complex software development tasks. Unlike conventional code generation tools,
agentic systems are capable of decomposing high-level goals, coordinating
multi-step processes, and adapting their behavior based on intermediate
feedback. These capabilities are transforming the software development
practice. As this emerging field evolves rapidly, there is a need to define its
scope, consolidate its technical foundations, and identify open research
challenges. This survey provides a comprehensive and timely review of AI
agentic programming. We introduce a taxonomy of agent behaviors and system
architectures, and examine core techniques including planning, memory and
context management, tool integration, and execution monitoring. We also analyze
existing benchmarks and evaluation methodologies used to assess coding agent
performance. Our study identifies several key challenges, including limitations
in handling long context, a lack of persistent memory across tasks, and
concerns around safety, alignment with user intent, and collaboration with
human developers. We discuss emerging opportunities to improve the reliability,
adaptability, and transparency of agentic systems. By synthesizing recent
advances and outlining future directions, this survey aims to provide a
foundation for research and development in building the next generation of
intelligent and trustworthy AI coding agents.

</details>


### [14] [From Feedback to Failure: Automated Android Performance Issue Reproduction](https://arxiv.org/abs/2508.11147)
*Zhengquan Li,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: RevPerf is a tool that uses Google Play app reviews to reproduce performance issues, achieving a 70% success rate through enhanced review data, command execution, and multifaceted issue detection.


<details>
  <summary>Details</summary>
Motivation: Performance issues in mobile apps are challenging to detect in development environments due to subtle manifestations and diagnostic difficulties, necessitating a reliable reproduction method.

Method: RevPerf enriches app reviews with performance issue details using prompt engineering, then employs an execution agent to automate command generation and execution. It uses Android logs, GUI observation, and resource monitoring for detection.

Result: The framework successfully reproduced performance issues in 70% of cases on a manually validated dataset, demonstrating its effectiveness in automating issue reproduction.

Conclusion: RevPerf offers a practical framework for leveraging user-reported reviews to reproduce and detect performance issues, significantly improving diagnosis success rates in app development.

Abstract: Mobile application performance is a vital factor for user experience. Yet,
performance issues are notoriously difficult to detect within development
environments, where their manifestations are often less conspicuous and
diagnosis proves more challenging. To address this limitation, we propose
RevPerf, an advanced performance issue reproduction tool that leverages app
reviews from Google Play to acquire pertinent information. RevPerf employs
relevant reviews and prompt engineering to enrich the original review with
performance issue details. An execution agent is then employed to generate and
execute commands to reproduce the issue. After executing all necessary steps,
the system incorporates multifaceted detection methods to identify performance
issues by monitoring Android logs, GUI changes, and system resource utilization
during the reproduction process. Experimental results demonstrate that our
proposed framework achieves a 70\% success rate in reproducing performance
issues on the dataset we constructed and manually validated.

</details>


### [15] [PTMPicker: Facilitating Efficient Pretrained Model Selection for Application Developers](https://arxiv.org/abs/2508.11179)
*Pei Liu,Terry Zhuo,Jiawei Deng,Zhenchang Xing,Qinghua Lu,Xiaoning Du,Hongyu Zhan*

Main category: cs.SE

TL;DR: PTMPicker addresses the limitations of keyword-based PTM selection by using a structured template and embedding similarities alongside constraint evaluations, achieving 85% top-10 accuracy with 15,207 synthesized search requests and 543,949 Hugging Face models.


<details>
  <summary>Details</summary>
Motivation: Traditional keyword-based PTM searches fail to capture user intent beyond function descriptions due to limited consideration of bias mitigation, hardware requirements, and license compliance in selection criteria.

Method: PTMPicker employs a structured template for PTM representation, embedding similarities for function-related attributes, and custom prompts to evaluate special constraints (e.g., license, hardware). 543,949 models were scraped from Hugging Face and 15,207 search requests synthesized using metadata and prompting techniques.

Result: Experimental evaluation on the curated PTM dataset and synthetic search requests showed PTMPicker successfully identifies suitable models for 85% of sampled requests within the top-10 ranked candidates.

Conclusion: PTMPicker's structured approach improves PTM selection effectiveness by bridging the gap between model descriptions and nuanced user requirements, including functional and non-functional constraints, with strong empirical validation.

Abstract: The rapid emergence of pretrained models (PTMs) has attracted significant
attention from both Deep Learning (DL) researchers and downstream application
developers. However, selecting appropriate PTMs remains challenging because
existing methods typically rely on keyword-based searches in which the keywords
are often derived directly from function descriptions. This often fails to
fully capture user intent and makes it difficult to identify suitable models
when developers also consider factors such as bias mitigation, hardware
requirements, or license compliance. To address the limitations of
keyword-based model search, we propose PTMPicker to accurately identify
suitable PTMs. We first define a structured template composed of common and
essential attributes for PTMs and then PTMPicker represents both candidate
models and user-intended features (i.e., model search requests) in this unified
format. To determine whether candidate models satisfy user requirements, it
computes embedding similarities for function-related attributes and uses
well-crafted prompts to evaluate special constraints such as license compliance
and hardware requirements. We scraped a total of 543,949 pretrained models from
Hugging Face to prepare valid candidates for selection. PTMPicker then
represented them in the predefined structured format by extracting their
associated descriptions. Guided by the extracted metadata, we synthesized a
total of 15,207 model search requests with carefully designed prompts, as no
such search requests are readily available. Experiments on the curated PTM
dataset and the synthesized model search requests show that PTMPicker can help
users effectively identify models,with 85% of the sampled requests successfully
locating appropriate PTMs within the top-10 ranked candidates.

</details>


### [16] [ORFuzz: Fuzzing the "Other Side" of LLM Safety -- Testing Over-Refusal](https://arxiv.org/abs/2508.11222)
*Haonan Zhang,Dongxia Wang,Yi Liu,Kexin Chen,Jiashui Wang,Xinlei Ying,Long Liu,Wenhai Wang*

Main category: cs.SE

TL;DR: This paper introduces ORFuzz, the first evolutionary testing framework for detecting LLM over-refusals, which improves testing by 6.98% and creates a benchmark with 1,855 test cases achieving 63.56% over-refusal rate across models.


<details>
  <summary>Details</summary>
Motivation: Current testing methods for LLM over-refusals are inadequate due to flawed benchmarks and limited test generation, evidenced by a user study highlighting issues in validity and conservativeness.

Method: ORFuzz integrates three components: safety category-aware seed selection for coverage, adaptive mutator optimization using reasoning LLMs for effective test generation, and OR-Judge, a human-aligned model for validating test cases.

Result: ORFuzz achieved a 6.98% average over-refusal detection rate (double leading baselines) and generates the ORFuzzSet benchmark with 1,855 test cases, achieving 63.56% average over-refusal rate across 10 LLMs.

Conclusion: ORFuzz and ORFuzzSet provide a robust framework and community resource for improving the reliability and trustworthiness of LLM-based systems through systematic over-refusal detection and analysis.

Abstract: Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously
rejecting benign queries due to overly conservative safety measures - a
critical functional flaw that undermines their reliability and usability.
Current methods for testing this behavior are demonstrably inadequate,
suffering from flawed benchmarks and limited test generation capabilities, as
highlighted by our empirical user study. To the best of our knowledge, this
paper introduces the first evolutionary testing framework, ORFuzz, for the
systematic detection and analysis of LLM over-refusals. ORFuzz uniquely
integrates three core components: (1) safety category-aware seed selection for
comprehensive test coverage, (2) adaptive mutator optimization using reasoning
LLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge
model validated to accurately reflect user perception of toxicity and refusal.
Our extensive evaluations demonstrate that ORFuzz generates diverse, validated
over-refusal instances at a rate (6.98% average) more than double that of
leading baselines, effectively uncovering vulnerabilities. Furthermore,
ORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly
transferable test cases that achieves a superior 63.56% average over-refusal
rate across 10 diverse LLMs, significantly outperforming existing datasets.
ORFuzz and ORFuzzSet provide a robust automated testing framework and a
valuable community resource, paving the way for developing more reliable and
trustworthy LLM-based software systems.

</details>


### [17] [Hallucination in LLM-Based Code Generation: An Automotive Case Study](https://arxiv.org/abs/2508.11257)
*Marc Pavel,Nenad Petrovic,Lukasz Mazur,Vahid Zolfaghari,Fengjunjie Pan,Alois Knoll*

Main category: cs.SE

TL;DR: This paper investigates hallucinations in code generation for the automotive domain, comparing various LLMs under different prompting strategies and highlighting the need for mitigation techniques.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) face limitations in code generation due to hallucinations, particularly in safety-critical domains like automotive software systems.

Method: The authors conducted a case study evaluating GPT-4.1, Codex, and GPT-4o using three prompting complexities: one-liner prompts, VSS-context prompts, and context-rich prompts with code skeletons.

Result: GPT-4.1 and GPT-4o produced correct solutions only with the most context-rich prompts. Simpler strategies failed despite refinement, revealing common issues like syntax violations, invalid references, and API conflicts.

Conclusion: The study underscores the necessity of reliable mitigation techniques to ensure safe use of LLM-generated code in critical domains, as current models struggle with context-poor prompts and real-world complexity.

Abstract: Large Language Models (LLMs) have shown significant potential in automating
code generation tasks offering new opportunities across software engineering
domains. However, their practical application remains limited due to
hallucinations - outputs that appear plausible but are factually incorrect,
unverifiable or nonsensical. This paper investigates hallucination phenomena in
the context of code generation with a specific focus on the automotive domain.
A case study is presented that evaluates multiple code LLMs for three different
prompting complexities ranging from a minimal one-liner prompt to a prompt with
Covesa Vehicle Signal Specifications (VSS) as additional context and finally to
a prompt with an additional code skeleton. The evaluation reveals a high
frequency of syntax violations, invalid reference errors and API knowledge
conflicts in state-of-the-art models GPT-4.1, Codex and GPT-4o. Among the
evaluated models, only GPT-4.1 and GPT-4o were able to produce a correct
solution when given the most context-rich prompt. Simpler prompting strategies
failed to yield a working result, even after multiple refinement iterations.
These findings highlight the need for effective mitigation techniques to ensure
the safe and reliable use of LLM generated code, especially in safety-critical
domains such as automotive software systems.

</details>


### [18] [Defects4Log: Benchmarking LLMs for Logging Code Defect Detection and Reasoning](https://arxiv.org/abs/2508.11305)
*Xin Wang,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: The paper presents a comprehensive study on logging code defects, introducing a taxonomy of seven defect patterns with 14 scenarios, a benchmark dataset of 164 real-world defects, and an automated LLM-based detection framework. It highlights that LLMs improve defect detection accuracy (10.9%) when domain-specific knowledge is incorporated.


<details>
  <summary>Details</summary>
Motivation: Previous research on logging defects has prioritized limited patterns and source code-based analysis, while LLMs' potential for detecting such defects remains underexplored despite their generalization capabilities in code tasks.

Method: The work derives a defect taxonomy through systematic analysis, constructs a developer-verified dataset, and evaluates LLM performance using prompting strategies combined with contextual defect pattern knowledge.

Result: LLMs achieve improved detection accuracy (10.9% increase) when provided with detailed defect pattern scenarios, but perform poorly with source code alone. The taxonomy and dataset are validated through experimental analysis.

Conclusion: The proposed taxonomy and benchmark enable practitioners to avoid common logging defects while improving LLM-based detection frameworks. Future work should further integrate domain-specific knowledge to enhance LLM performance in this domain.

Abstract: Logging code is written by developers to capture system runtime behavior and
plays a vital role in debugging, performance analysis, and system monitoring.
However, defects in logging code can undermine the usefulness of logs and lead
to misinterpretations. Although prior work has identified several logging
defect patterns and provided valuable insights into logging practices, these
studies often focus on a narrow range of defect patterns derived from limited
sources (e.g., commit histories) and lack a systematic and comprehensive
analysis. Moreover, large language models (LLMs) have demonstrated promising
generalization and reasoning capabilities across a variety of code-related
tasks, yet their potential for detecting logging code defects remains largely
unexplored.
  In this paper, we derive a comprehensive taxonomy of logging code defects,
which encompasses seven logging code defect patterns with 14 detailed
scenarios. We further construct a benchmark dataset, \dataset, consisting of
164 developer-verified real-world logging defects. Then we propose an automated
framework that leverages various prompting strategies and contextual
information to evaluate LLMs' capability in detecting and reasoning logging
code defects. Experimental results reveal that LLMs generally struggle to
accurately detect and reason logging code defects based on the source code
only. However, incorporating proper knowledge (e.g., detailed scenarios of
defect patterns) can lead to 10.9\% improvement in detection accuracy. Overall,
our findings provide actionable guidance for practitioners to avoid common
defect patterns and establish a foundation for improving LLM-based reasoning in
logging code defect detection.

</details>


### [19] [TRACY: Benchmarking Execution Efficiency of LLM-Based Code Translation](https://arxiv.org/abs/2508.11468)
*Zhihao Gong,Zeyu Sun,Dong Huang,Qingyuan Liang,Jie M. Zhang,Dan Hao*

Main category: cs.SE

TL;DR: TRACY is a new benchmark evaluating execution efficiency of LLM-translated code, revealing that top models struggle with efficiency despite good correctness. It identifies algorithmic flaws and resource handling as major issues.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based code translation prioritizes correctness over execution efficiency, leaving a critical gap in performance evaluation and optimization.

Method: LLM-driven two-stage pipeline: stress test generation to amplify performance differences, followed by efficiency-oriented pruning to isolate distinguishing tasks, creating 1,011 C++/Java/Python tasks with 22.1 verified references and 10 demanding tests each.

Result: Top-tier LLMs fail to produce consistent efficient code (Claude-4-think ranks 8th in time efficiency). Algorithmic flaws cause 5.6× median slowdown, improper resource handling leads to 12.0× memory increase.

Conclusion: Future LLM development must address both correctness and efficiency in code translation, with TRACY providing a foundational benchmark for this dual metric evaluation.

Abstract: Automatic code translation is a fundamental task in modern software
development. While the advent of Large Language Models (LLMs) has significantly
improved the correctness of code translation, the critical dimension of
execution efficiency remains overlooked. To address this gap, we introduce
TRACY, the first comprehensive benchmark designed to evaluate the execution
efficiency of LLM-translated code. TRACY is constructed through an LLM-driven
two-stage pipeline: an initial stage generates a suite of stress tests to
amplify performance differences, followed by an efficiency-oriented task
pruning stage that isolates the efficiency-distinguishing tasks. The resulting
benchmark comprises 1,011 code translation tasks across C++, Java, and Python,
each accompanied by an average of 22.1 verified reference translations and 10
computationally demanding tests. Our extensive evaluation of 26 representative
LLMs reveals that even top-tier LLMs struggle to consistently produce efficient
code translations. For instance, Claude-4-think, the leading model for
correctness, ranks eighth overall when time efficiency is taken into account,
surpassed by several smaller open-source models. We further pinpoint that
algorithmic flaws and improper resource handling are the most detrimental,
causing a median time slowdown of 5.6$\times$ and memory increase of
12.0$\times$, respectively. Our work underscores the necessity of jointly
optimizing for correctness and efficiency in future LLM-based code translation.

</details>


### [20] [Temporal Network Analysis of Microservice Architectural Degradation](https://arxiv.org/abs/2508.11571)
*Alexander Bakhtin*

Main category: cs.SE

TL;DR: This paper explores the application of temporal network analysis to microservice architectures, highlighting challenges in data collection and analysis limitations due to small-scale datasets.


<details>
  <summary>Details</summary>
Motivation: Microservice systems evolve over time, necessitating temporal network methods to capture dynamic dependencies for effective monitoring and development.

Method: The author models microservice architectures as temporal service dependency graphs using tracing data and release history, then examines analytical challenges in such networks.

Result: A case study demonstrates the feasibility of the approach but reveals significant limitations due to small network size (7 time instances, 42 microservices).

Conclusion: Temporal network analysis offers valuable insights into microservice systems, but requires larger-scale datasets to overcome current analytical limitations.

Abstract: Microservice architecture can be modeled as a network of microservices making
calls to each other, commonly known as the service dependency graph. Network
Science can provide methods to study such networks. In particular, temporal
network analysis is a branch of Network Science that analyzes networks evolving
with time. In microservice systems, temporal networks can arise if we examine
the architecture of the system across releases or monitor a deployed system
using tracing.
  In this research summary paper, I discuss the challenges in obtaining
temporal networks from microservice systems and analyzing them with the
temporal network methods. In particular, the most complete temporal network
that we could obtain contains 7 time instances and 42 microservices, which
limits the potential analysis that could be applied.

</details>
