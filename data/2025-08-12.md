<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 19]
- [cs.SE](#cs.SE) [Total: 15]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Symbolic Execution in Practice: A Survey of Applications in Vulnerability, Malware, Firmware, and Protocol Analysis](https://arxiv.org/abs/2508.06643)
*Joshua Bailey,Charles Nicholas*

Main category: cs.CR

TL;DR: This paper presents a taxonomy for symbolic execution strategies to address path explosion, surveys their applications across domains, and identifies future research opportunities.


<details>
  <summary>Details</summary>
Motivation: Symbolic execution's practical application is hindered by path explosion, necessitating effective strategies for managing state proliferation in complex systems.

Method: The authors develop a systematic taxonomy categorizing path explosion solutions into 'Scope Reduction' (limiting analysis to code subsections) and 'Guidance Heuristics' (path prioritization), then use this framework to analyze domain-specific applications.

Result: The taxonomy enables clearer understanding of path explosion strategies across domains like vulnerability analysis and malware research, while highlighting research gaps in real-time systems and type-safe languages.

Conclusion: The taxonomy provides a foundation for advancing symbolic execution in challenging areas, with recommended research focusing on real-time operating systems and modern programming languages with type safety.

Abstract: Symbolic execution is a powerful program analysis technique that allows for
the systematic exploration of all program paths. Path explosion, where the
number of states to track becomes unwieldy, is one of the biggest challenges
hindering symbolic execution's practical application. To combat this,
researchers have employed various strategies to enable symbolic execution on
complex software systems. This paper introduces a systematic taxonomy of these
strategies, categorizing them into two primary approaches: Scope Reduction,
which aims to reduce the scope of symbolic execution to manageable portions of
code, and Guidance Heuristics, which steer the symbolic execution engine toward
promising paths. Using this taxonomy as a lens, we survey applications of
symbolic executions in several domains such as vulnerability analysis, malware
analysis, firmware re-hosting, and network protocol analysis. Finally, we
identify promising directions for future research, including the application of
symbolic execution to real-time operating systems and modern, type-safe
languages.

</details>


### [2] [Mitigating Distribution Shift in Graph-Based Android Malware Classification via Function Metadata and LLM Embeddings](https://arxiv.org/abs/2508.06734)
*Ngoc N. Tran,Anwar Said,Waseem Abbas,Tyler Derr,Xenofon D. Koutsoukos*

Main category: cs.CR

TL;DR: The paper identifies a gap in graph-based malware classification accuracy for unseen variants and proposes a semantic enrichment framework to address this, demonstrating improved performance via new benchmarks and experiments.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based malware classifiers experience significant accuracy drops when facing unseen malware variants from the same family, indicating poor generalization due to limited semantic representation.

Method: A semantic enrichment framework was developed to enhance function call graphs with contextual features, including function-level metadata and code embeddings from large language models, adaptable to varied feature availability.

Result: The method improved classification accuracy by up to 8% under distribution shifts and increased robustness when combined with adaptation-based approaches, evaluated on new cross-family benchmarks (MalNet-Tiny-Common/Distinct).

Conclusion: The framework provides a practical solution for resilient malware detection in evolving threat environments by incorporating semantic signals into graph representations.

Abstract: Graph-based malware classifiers can achieve over 94% accuracy on standard
Android datasets, yet we find they suffer accuracy drops of up to 45% when
evaluated on previously unseen malware variants from the same family - a
scenario where strong generalization would typically be expected. This
highlights a key limitation in existing approaches: both the model
architectures and their structure-only representations often fail to capture
deeper semantic patterns. In this work, we propose a robust semantic enrichment
framework that enhances function call graphs with contextual features,
including function-level metadata and, when available, code embeddings derived
from large language models. The framework is designed to operate under
real-world constraints where feature availability is inconsistent, and supports
flexible integration of semantic signals. To evaluate generalization under
realistic domain and temporal shifts, we introduce two new benchmarks:
MalNet-Tiny-Common and MalNet-Tiny-Distinct, constructed using malware family
partitioning to simulate cross-family generalization and evolving threat
behavior. Experiments across multiple graph neural network backbones show that
our method improves classification performance by up to 8% under distribution
shift and consistently enhances robustness when integrated with
adaptation-based methods. These results offer a practical path toward building
resilient malware detection systems in evolving threat environments.

</details>


### [3] [Label Inference Attacks against Federated Unlearning](https://arxiv.org/abs/2508.06789)
*Wei Wang,Xiangyun Tang,Yajie Wang,Yijing Lin,Tao Zhang,Meng Shen,Dusit Niyato,Liehuang Zhu*

Main category: cs.CR

TL;DR: This paper introduces ULIA, a novel label inference attack method for Federated Unlearning (FU) that can accurately infer unlearning data labels across three FU levels using gradient variations.


<details>
  <summary>Details</summary>
Motivation: Existing FU methods risk exposing client data information through parameter variations, but prior label inference attacks against FU have not been explored, highlighting the need for understanding this privacy threat.

Method: ULIA employs a gradient-label mapping mechanism to establish relationships between model parameter variations during unlearning and the corresponding labels, enabling inference even with accumulated variations.

Result: Experiments show ULIA achieves 100% Attack Success Rate (ASR) on IID data under class- and client-level unlearning, and maintains 93-62.3% ASR even when only 1% of a user's data is forgotten, with performance validated on both IID and non-IID settings.

Conclusion: ULIA demonstrates that model parameter variations in federation unlearning can leak unlearning data labels, raising critical privacy concerns and necessitating improved safeguards for compliance with data erasure rights.

Abstract: Federated Unlearning (FU) has emerged as a promising solution to respond to
the right to be forgotten of clients, by allowing clients to erase their data
from global models without compromising model performance. Unfortunately,
researchers find that the parameter variations of models induced by FU expose
clients' data information, enabling attackers to infer the label of unlearning
data, while label inference attacks against FU remain unexplored. In this
paper, we introduce and analyze a new privacy threat against FU and propose a
novel label inference attack, ULIA, which can infer unlearning data labels
across three FU levels. To address the unique challenges of inferring labels
via the models variations, we design a gradient-label mapping mechanism in ULIA
that establishes a relationship between gradient variations and unlearning
labels, enabling inferring labels on accumulated model variations. We evaluate
ULIA on both IID and non-IID settings. Experimental results show that in the
IID setting, ULIA achieves a 100% Attack Success Rate (ASR) under both
class-level and client-level unlearning. Even when only 1% of a user's local
data is forgotten, ULIA still attains an ASR ranging from 93% to 62.3%.

</details>


### [4] [Towards Practical Data-Dependent Memory-Hard Functions with Optimal Sustained Space Trade-offs in the Parallel Random Oracle Model](https://arxiv.org/abs/2508.06795)
*Jeremiah Blocki,Blake Holman*

Main category: cs.CR

TL;DR: The paper introduces EGSample, a new Memory-Hard Function (MHF) that avoids expensive combinatorial constructions, achieving strong sequential and memory complexity tradeoffs in both the dynamic pebbling model and the Parallel Random Oracle Model (PROM).


<details>
  <summary>Details</summary>
Motivation: Existing MHF constructions like [BH22] are impractical due to reliance on combinatorial graphs and lack formal justification for the dynamic pebbling game in the PROM model.

Method: The authors design EGSample using heuristic approaches (e.g., dynamic pebbling) for analysis, then propose new techniques to establish provable security bounds for cumulative memory costs (CMC) and sustained-space complexity (SSC) in the PROM model.

Result: In the dynamic pebbling model, EGSample guarantees Ω(N) memory for Ω(N) steps or Ω(N^{3−ε}) CMC. In the PROM model, it proves any adversary must either lock up Ω(N) memory for Ω(N) steps or incur Ω(N^{2.5−ε}) CMC.

Conclusion: EGSample provides a practical MHF with provably strong SSC/CMC tradeoffs, addressing limitations of prior work by eliminating combinatorial graph reliance and establishing theoretical security in the PROM model.

Abstract: Memory-Hard Functions (MHF) are a useful cryptographic primitive to build
egalitarian proofs-of-work and to help protect low entropy secrets (e.g., user
passwords) against brute-forces attacks. Ideally, we would like for a MHF to
have the property that (1) an honest party can evaluate the function in
sequential time $\Omega(N)$, and (2) any parallel party that evaluates the
function is forced to lockup $\Omega(N)$ memory for $\Omega(N)$ sequential
steps. Unfortunately, this goal is not quite achievable, so prior work of
Blocki and Holman [BH22] focused on designing MHFs with strong tradeoff
guarantees between sustained-space complexity (SSC) and cumulative memory costs
(CMC). However, their theoretical construction is not suitable for practical
deployment due to the reliance on expensive constructions of combinatorial
graphs. Furthermore, there is no formal justification for the heuristic use of
the dynamic pebbling game in MHF analysis so we cannot rule out the possibility
that there are more efficient attacks in the Parallel Random Oracle Model
(PROM). Towards the goal of developing a practical MHF with provably strong
SSC/CMC tradeoffs we develop a new MHF called EGSample which does not rely on
expensive combinatorial constructions like [BH22]. In the dynamic pebbling
model, we prove equivalent SSC/CMC tradeoffs for EGSample i.e., any the dynamic
pebbling strategy either (1) locks up $\Omega(N)$ memory for $\Omega(N)$ steps,
or (2) incurs cumulative memory cost at least $\Omega(N^{3-\epsilon})$. We also
develop new techniques to directly establish SSC/CMC tradeoffs in the parallel
random oracle model. In particular, we prove that {\em any} PROM algorithm
evaluating our MHF either (1) locks up $\Omega(N)$ blocks of memory for
$\Omega(N)$ steps or (2) incurs cumulative memory cost at least
$\Omega(N^{2.5-\epsilon})$.

</details>


### [5] [Towards Effective Prompt Stealing Attack against Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.06837)
*Shiqian Zhao,Chong Wang,Yiming Li,Yihao Huang,Wenjie Qu,Siew-Kei Lam,Yi Xie,Kangjie Chen,Jie Zhang,Tianwei Zhang*

Main category: cs.CR

TL;DR: Prometheus introduces a training-free prompt-stealing attack for text-to-image models using dynamic modifiers and a proxy model, achieving a 25% higher attack success rate compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing prompt-stealing attacks suffer from limited adaptability to diverse diffusion models and target images, and rely on model-specific training while exposing valuable prompts in practical AI art markets.

Method: Prometheus employs dynamic modifiers (natively generated via NLP analysis), contextual matching to filter relevant modifiers, and a greedy search algorithm using proxy model feedback to iteratively reverse-engineer prompts without requiring training.

Result: The attack achieves a 25.0% improvement in Attack Success Rate (ASR) against Midjourney, Leonardo.ai, and DALL$\cdot$E platforms, demonstrating robustness against potential defense mechanisms.

Conclusion: Prometheus establishes a practical and severe threat to prompt security in text-to-image systems, outperforming previous methods while maintaining compatibility with diverse models without training overhead.

Abstract: Text-to-Image (T2I) models, represented by DALL$\cdot$E and Midjourney, have
gained huge popularity for creating realistic images. The quality of these
images relies on the carefully engineered prompts, which have become valuable
intellectual property. While skilled prompters showcase their AI-generated art
on markets to attract buyers, this business incidentally exposes them to
\textit{prompt stealing attacks}. Existing state-of-the-art attack techniques
reconstruct the prompts from a fixed set of modifiers (i.e., style
descriptions) with model-specific training, which exhibit restricted
adaptability and effectiveness to diverse showcases (i.e., target images) and
diffusion models.
  To alleviate these limitations, we propose Prometheus, a training-free,
proxy-in-the-loop, search-based prompt-stealing attack, which reverse-engineers
the valuable prompts of the showcases by interacting with a local proxy model.
It consists of three innovative designs. First, we introduce dynamic modifiers,
as a supplement to static modifiers used in prior works. These dynamic
modifiers provide more details specific to the showcases, and we exploit NLP
analysis to generate them on the fly. Second, we design a contextual matching
algorithm to sort both dynamic and static modifiers. This offline process helps
reduce the search space of the subsequent step. Third, we interact with a local
proxy model to invert the prompts with a greedy search algorithm. Based on the
feedback guidance, we refine the prompt to achieve higher fidelity. The
evaluation results show that Prometheus successfully extracts prompts from
popular platforms like PromptBase and AIFrog against diverse victim models,
including Midjourney, Leonardo.ai, and DALL$\cdot$E, with an ASR improvement of
25.0\%. We also validate that Prometheus is resistant to extensive potential
defenses, further highlighting its severity in practice.

</details>


### [6] [SPARE: Securing Progressive Web Applications Against Unauthorized Replications](https://arxiv.org/abs/2508.07053)
*Sajib Talukder,Nur Imtiazul Haque,Khandakar Ashrafi Akbar*

Main category: cs.CR

TL;DR: This paper proposes a query parameter-based security solution to protect Progressive Web Applications (PWAs) from unauthorized replication by malicious developers who create counterfeit native apps using PWA links. The approach embeds device identifiers and Unix timestamps into query parameters, validated through a prototype and realistic attack simulations.


<details>
  <summary>Details</summary>
Motivation: Malicious actors can exploit the open nature of PWAs by duplicating their web links to develop fake native apps that divert users for financial gain, creating risks for both users and legitimate developers.

Method: The authors developed a system using query parameters containing Unix timestamps and device identifiers. They analyze vulnerabilities, design advanced measures to address weaknesses, and create a Zipfian distribution-based dataset to simulate real-world mobile app usage patterns for evaluation.

Result: The prototype successfully mitigates replication attacks through the detection of tampered query parameters. Evaluation against simulated attack scenarios confirms its feasibility, but real-world implementation limitations are noted.

Conclusion: The paper contributes a practical defense framework against PWA link duplication but acknowledges challenges in maintaining robustness against sophisticated attacks, providing future research directions

Abstract: WebView applications are widely used in mobile applications to display web
content directly within the app, enhancing user engagement by eliminating the
need to open an external browser and providing a seamless experience.
Progressive Web Applications (PWAs) further improve usability by combining the
accessibility of web apps with the speed, offline capabilities, and
responsiveness of native applications. However, malicious developers can
exploit this technology by duplicating PWA web links to create counterfeit
native apps, monetizing through user diversion. This unethical practice poses
significant risks to users and the original application developers,
underscoring the need for robust security measures to prevent unauthorized
replication. Considering the one-way communication of Trusted Web Activity (a
method for integrating web content into Android applications) and PWAs, we
propose a query parameter-based practical security solution to defend against
or mitigate such attacks. We analyze the vulnerabilities of our proposed
security solution to assess its effectiveness and introduce advanced measures
to address any identified weaknesses, presenting a comprehensive defense
framework. As part of our work, we developed a prototype web application that
secures PWAs from replication by embedding a combination of Unix timestamps and
device identifiers into the query parameters. We evaluate the effectiveness of
this defense strategy by simulating an advanced attack scenario. Additionally,
we created a realistic dataset reflecting mobile app user behavior, modeled
using a Zipfian distribution, to validate our framework.

</details>


### [7] [ScamDetect: Towards a Robust, Agnostic Framework to Uncover Threats in Smart Contracts](https://arxiv.org/abs/2508.07094)
*Pasquale De Rosa,Pascal Felber,Valerio Schiavoni*

Main category: cs.CR

TL;DR: This paper introduces ScamDetect, a platform-agnostic framework for smart contract malware detection that evolves from PhishingHook by addressing obfuscation via graph neural networks (GNNs) and expanding to WASM runtimes, aiming for scalable blockchain security.


<details>
  <summary>Details</summary>
Motivation: Smart contracts face growing threats like phishing and exploits, while traditional detection methods expose user data, and existing static analysis approaches struggle with obfuscated bytecode and cross-platform adaptability.

Method: ScamDetect employs graph neural networks (GNNs) to analyze control flow graphs (CFGs) of Ethereum Virtual Machine (EVM) bytecode, overcoming limitations of static analysis by capturing structural patterns beyond opcode sequences, followed by generalization to WASM-based environments.

Result: The paper outlines ScamDetect's two-stage development plan over 2.5 years, transitioning from EVM bytecode obfuscation analysis to platform-agnostic WASM adoption, though empirical results are not yet presented.

Conclusion: ScamDetect aims to provide a robust, modular, and cross-platform solution for proactive smart contract security, addressing evasion techniques and enabling future-proof decentralized ecosystem protection.

Abstract: Smart contracts have transformed decentralized finance by enabling
programmable, trustless transactions. However, their widespread adoption and
growing financial significance have attracted persistent and sophisticated
threats, such as phishing campaigns and contract-level exploits. Traditional
transaction-based threat detection methods often expose sensitive user data and
interactions, raising privacy and security concerns. In response, static
bytecode analysis has emerged as a proactive mitigation strategy, identifying
malicious contracts before they execute harmful actions.Building on this
approach, we introduced PhishingHook, the first machine-learning-based
framework for detecting phishing activities in smart contracts via static
bytecode and opcode analysis, achieving approximately 90% detection accuracy.
Nevertheless, two pressing challenges remain: (1) the increasing use of
sophisticated bytecode obfuscation techniques designed to evade static
analysis, and (2) the heterogeneity of blockchain environments requiring
platform-agnostic solutions.This paper presents a vision for ScamDetect (Smart
Contract Agnostic Malware Detector), a robust, modular, and platform-agnostic
framework for smart contract malware detection. Over the next 2.5 years,
ScamDetect will evolve in two stages: first, by tackling obfuscated Ethereum
Virtual Machine (EVM) bytecode through graph neural network (GNN) analysis of
control flow graphs (CFGs), leveraging GNNs' ability to capture complex
structural patterns beyond opcode sequences; and second, by generalizing
detection capabilities to emerging runtimes such as WASM. ScamDetect aims to
enable proactive, scalable security for the future of decentralized ecosystems.

</details>


### [8] [A Real-Time, Self-Tuning Moderator Framework for Adversarial Prompt Detection](https://arxiv.org/abs/2508.07139)
*Ivan Zhang*

Main category: cs.CR

TL;DR: The paper introduces RTST, a real-time, self-tuning moderator framework for defending LLMs against adversarial attacks and jailbreaking, offering scalability and minimal performance degradation.


<details>
  <summary>Details</summary>
Motivation: Existing defenses for LLMs fail to adapt quickly to new attacks, degrade model outputs, or hinder scalability, necessitating a more effective solution for AI alignment in critical information security contexts.

Method: A lightweight training approach: RTST uses real-time self-adjustment mechanisms evaluated on Google's Gemini models through modern jailbreak attacks.

Result: RTST outperforms traditional fine-tuning/classifier-based methods by being adaptive to novel attacks while preserving benign prompt performance.

Conclusion: RTST provides a scalable, minimally intrusive defense framework for LLMs that effectively addresses modern jailbreaking threats without compromising core model functionality.

Abstract: Ensuring LLM alignment is critical to information security as AI models
become increasingly widespread and integrated in society. Unfortunately, many
defenses against adversarial attacks and jailbreaking on LLMs cannot adapt
quickly to new attacks, degrade model responses to benign prompts, or introduce
significant barriers to scalable implementation. To mitigate these challenges,
we introduce a real-time, self-tuning (RTST) moderator framework to defend
against adversarial attacks while maintaining a lightweight training footprint.
We empirically evaluate its effectiveness using Google's Gemini models against
modern, effective jailbreaks. Our results demonstrate the advantages of an
adaptive, minimally intrusive framework for jailbreak defense over traditional
fine-tuning or classifier models.

</details>


### [9] [Understanding NFTs from EIP Standards](https://arxiv.org/abs/2508.07190)
*Minfeng Qi,Qin Wang,Guangsheng Yu,Ruiqiang Li,Victor Zhou,Shiping Chen*

Main category: cs.CR

TL;DR: This paper systematically analyzes NFT technical foundations using Ethereum-related standards and discussions, highlighting interoperability issues and security risks from functional complexity.


<details>
  <summary>Details</summary>
Motivation: Prior research on NFTs has focused on market dynamics and user behavior, lacking systematic analysis of standards underpinning their functionality.

Method: Empirical analysis of 191 NFT-related EIPs and 10K+ Ethereum Magicians discussions (as of July 2025) combined with Solidity interface parsing, inheritance graph modeling, contributor profiling, and discussion data mining.

Result: Foundational vs. emerging standards were distinguished; cross-version interoperability deficiencies were exposed; growing functional complexity significantly heightens security risks.

Conclusion: Current NFT standards require better cross-version interoperability and risk management for functional complexity to ensure secure and scalable blockchain applications.

Abstract: We argue that the technical foundations of non-fungible tokens (NFTs) remain
inadequately understood. Prior research has focused on market dynamics, user
behavior, and isolated security incidents, yet systematic analysis of the
standards underpinning NFT functionality is largely absent.
  We present the first study of NFTs through the lens of Ethereum Improvement
Proposals (EIPs). We conduct a large-scale empirical analysis of 191
NFT-related EIPs and 10K+ Ethereum Magicians discussions (as of July, 2025). We
integrate multi-dimensional analyses including the automated parsing of
Solidity interfaces, graph-based modeling of inheritance structures,
contributor profiling, and mining of community discussion data. We distinguish
foundational from emerging standards, expose poor cross-version
interoperability, and show that growing functional complexity heightens
security risks.

</details>


### [10] [Fading the Digital Ink: A Universal Black-Box Attack Framework for 3DGS Watermarking Systems](https://arxiv.org/abs/2508.07263)
*Qingyuan Zeng,Shu Jiang,Jiajing Lin,Zhenzhong Wang,Kay Chen Tan,Min Jiang*

Main category: cs.CR

TL;DR: This paper proposes the Group-based Multi-objective Evolutionary Attack (GMEA), a first universal black-box attack framework for removing watermarks from 3D Gaussian Splatting (3DGS) models while preserving visual quality.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS watermarking techniques lack robustness against attacks, and the risks to copyright protection schemes require systematic investigation. The authors aim to expose vulnerabilities in current systems and promote the development of more secure methods.

Method: The attack is formulated as a large-scale multi-objective optimization problem, using an indirect objective function with convolutional networks to minimize feature map standard deviation and a group-based strategy to partition the 3DGS search space into independent sub-problems.

Result: GMEA successfully removes both 1D bitstream and 2D image watermarks from major 3DGS watermarking approaches while maintaining high visual fidelity in experimental tests.

Conclusion: The study reveals significant security flaws in current 3DGS watermarking techniques, demonstrating that even basic attacks can effectively bypass them. This highlights the urgent need for more attack-resistant copyright protection methods in 3DGS.

Abstract: With the rise of 3D Gaussian Splatting (3DGS), a variety of digital
watermarking techniques, embedding either 1D bitstreams or 2D images, are used
for copyright protection. However, the robustness of these watermarking
techniques against potential attacks remains underexplored. This paper
introduces the first universal black-box attack framework, the Group-based
Multi-objective Evolutionary Attack (GMEA), designed to challenge these
watermarking systems. We formulate the attack as a large-scale multi-objective
optimization problem, balancing watermark removal with visual quality. In a
black-box setting, we introduce an indirect objective function that blinds the
watermark detector by minimizing the standard deviation of features extracted
by a convolutional network, thus rendering the feature maps uninformative. To
manage the vast search space of 3DGS models, we employ a group-based
optimization strategy to partition the model into multiple, independent
sub-optimization problems. Experiments demonstrate that our framework
effectively removes both 1D and 2D watermarks from mainstream 3DGS watermarking
methods while maintaining high visual fidelity. This work reveals critical
vulnerabilities in existing 3DGS copyright protection schemes and calls for the
development of more robust watermarking systems.

</details>


### [11] [SRAM-based Physically Unclonable Function using Lightweight Hamming-Code Fuzzy Extractor for Energy Harvesting Beat Sensors](https://arxiv.org/abs/2508.07510)
*Hoang-Long Pham,Duy-Hieu Bui,Xuan-Tu Tran,Orazio Aiello*

Main category: cs.CR

TL;DR: The paper proposes an SRAM-based PUF system with bit selection and error-correcting code for secure key generation in batteryless beat sensors, enabling encryption without power consumption.


<details>
  <summary>Details</summary>
Motivation: Batteryless IoT sensors like beat sensors lack security mechanisms, especially for cryptographic key generation, as they are often powered off after transmitting data, requiring a power-efficient and reliable solution.

Method: An SRAM-based Physically Unclonable Function (PUF) is designed by integrating a high-reliability bit selection algorithm and lightweight error-correcting code to generate stable keys when the microcontroller is powered off, utilizing its inherent SRAM random value feature.

Result: The system was validated on STM32 Cortex M0+ microcontrollers, demonstrating effective secure key generation for beat sensors while aligning with their power-off requirements and maintaining low energy consumption.

Conclusion: The proposed PUF approach successfully addresses security needs in batteryless IoT sensors, offering a practical and energy-efficient method to ensure data protection without requiring active power for key generation.

Abstract: Batteryless energy harvesting IoT sensor nodes such as beat sensors can be
deployed in millions without the need to replace batteries. They are
ultra-low-power and cost-effective wireless sensor nodes without the
maintenance cost and can work for 24 hours/365 days. However, they were not
equipped with security mechanisms to protect user data. Data encryption and
authentication can be used to secure beat sensor applications, but generating a
secure cryptographic key is challenging. In this paper, we proposed an
SRAM-based Physically Unclonable Function (PUF) combining a high-reliability
bit selection algorithm with a lightweight error-correcting code to generate
reliable secure keys for data encryption. The system employs a feature of beat
sensors, in which the microcontroller is powered on to transmit the ID signals
and then powered off. This fits the SRAM-based PUF requirement, which needs the
SRAM to be powered off to read out its random values. The proposed system has
been evaluated on STM32 Cortex M0+ microcontrollers and has been implemented to
protect important data on beat sensors.

</details>


### [12] [Chimera: Harnessing Multi-Agent LLMs for Automatic Insider Threat Simulation](https://arxiv.org/abs/2508.07745)
*Jiongchi Yu,Xiaofei Xie,Qiang Hu,Yuhan Ma,Ziming Zhao*

Main category: cs.CR

TL;DR: The paper introduces ChimeraLog, a diverse and realistic insider threat detection dataset generated by an LLM-based multi-agent framework (Chimera) that simulates benign and malicious employee behaviors across enterprise environments, addressing limitations of existing data sources.


<details>
  <summary>Details</summary>
Motivation: High-quality data for machine learning-based insider threat detection is scarce due to sensitive nature of enterprise logs and limitations of public datasets (either small-scale or unrealistic synthetic data).

Method: Chimera models employees as role-specific agents with modules for group meetings, pairwise interactions, and autonomous scheduling to capture genuine organizational dynamics. It simulates 15 insider attack types (e.g., IP theft) across three sensitive domains.

Result: ChimeraLog shows diversity and realism through human evaluation and quantitative analysis. Existing ITD methods achieve 0.83 average F1-score vs. 0.99 on the CERT dataset, proving higher difficulty and authenticity.

Conclusion: ChimeraLog provides a challenging, scalable benchmark with real-world semantics, enabling more robust insider threat detection research due to its realistic simulation and threat pattern explainability.

Abstract: Insider threats, which can lead to severe losses, remain a major security
concern. While machine learning-based insider threat detection (ITD) methods
have shown promising results, their progress is hindered by the scarcity of
high-quality data. Enterprise data is sensitive and rarely accessible, while
publicly available datasets, when limited in scale due to cost, lack sufficient
real-world coverage; and when purely synthetic, they fail to capture rich
semantics and realistic user behavior. To address this, we propose Chimera, the
first large language model (LLM)-based multi-agent framework that automatically
simulates both benign and malicious insider activities and collects diverse
logs across diverse enterprise environments. Chimera models each employee with
agents that have role-specific behavior and integrates modules for group
meetings, pairwise interactions, and autonomous scheduling, capturing realistic
organizational dynamics. It incorporates 15 types of insider attacks (e.g., IP
theft, system sabotage) and has been deployed to simulate activities in three
sensitive domains: technology company, finance corporation, and medical
institution, producing a new dataset, ChimeraLog. We assess ChimeraLog via
human studies and quantitative analysis, confirming its diversity, realism, and
presence of explainable threat patterns. Evaluations of existing ITD methods
show an average F1-score of 0.83, which is significantly lower than 0.99 on the
CERT dataset, demonstrating ChimeraLog's higher difficulty and utility for
advancing ITD research.

</details>


### [13] [A Comparative Analysis of Lightweight Hash Functions Using AVR ATXMega128 and ChipWhisperer](https://arxiv.org/abs/2508.07840)
*Mohsin Khan,Dag Johansen,Håvard Dagenborg*

Main category: cs.CR

TL;DR: This paper compares 22 lightweight hash functions for embedded/IoT systems using a new benchmark method on AVR ATXMega128 microcontroller and ChipWhisperer platform, evaluating execution speed, memory, energy consumption, and introducing E-RANK trade-off metrics.


<details>
  <summary>Details</summary>
Motivation: Lightweight hash functions are critical for security in resource-constrained embedded and IoT systems, yet existing comparisons lack comprehensive trade-off analysis across speed, memory, and energy dimensions.

Method: Benchmark methodology combining AVR ATXMega128 microcontroller with ChipWhisperer platform to measure 22 hash functions across execution speed (Cycles per Byte), memory footprint, energy consumption, and introduces composite E-RANK metric for trade-off quantification.

Result: Quantitative evaluation results showing performance trade-offs between speed, memory, and energy consumption for each hash function, with E-RANK providing structured comparison insights not achievable with single metrics.

Conclusion: The study provides actionable insights for system developers selecting lightweight hash functions by offering a novel composite metric (E-RANK) that clarifies security-performance trade-offs in embedded/IoT environments.

Abstract: Lightweight hash functions have become important building blocks for security
in embedded and IoT systems. A plethora of algorithms have been proposed and
standardized, providing a wide range of performance trade-off options for
developers to choose from. This paper presents a comparative analysis of 22 key
software-based lightweight hash functions, including the finalist from the
SHA-3 competition. We use a novel benchmark methodology that combines an AVR
ATXMega128 microcontroller with the ChipWhisperer cryptanalysis platform and
evaluate and compare the various hash functions along several dimensions,
including execution speed, % measured in Cycles per Byte (CpB), memory
footprint, and energy consumption. Using the composite E-RANK metric, we
provide new insight into the various trade-offs each hash function offers to
system developers.

</details>


### [14] [EFU: Enforcing Federated Unlearning via Functional Encryption](https://arxiv.org/abs/2508.07873)
*Samaneh Mohammadi,Vasileios Tsouvalas,Iraklis Symeonidis,Ali Balador,Tanir Ozcelebi,Francesco Flammini,Nirvana Meratnia*

Main category: cs.CR

TL;DR: EFU is a framework for Federated Unlearning that allows clients to remove their data's influence from collaborative models without revealing unlearning intentions to the server, using cryptographic techniques like functional encryption and auxiliary losses to maintain performance and privacy.


<details>
  <summary>Details</summary>
Motivation: Existing federated unlearning (FU) methods often require server-side cooperation, which exposes clients' unlearning intent and identity, compromising both client autonomy and unlearning privacy. There is a need for a secure, unlearning framework that conceals these details while ensuring reliable performance.

Method: EFU employs functional encryption to bind encrypted updates to specific aggregation functions on the server side. This prevents the server from performing unauthorized computations or detecting unlearning requests. To further obscure behavioral and parameter changes in the model, the framework incorporates adversarial example-based and parameter importance regularization losses.

Result: EFU achieves almost random accuracy on forgotten data with performance comparable to full retraining across diverse datasets and architectures. It successfully hides unlearning intent from the server and supports a broad range of client-side unlearning mechanisms as a function-agnostic solution.

Conclusion: EFU addresses critical privacy and autonomy issues in Federated Unlearning by cryptographically enforcing data removal confidentiality and maintaining model performance, enabling secure, function-hiding, and certifiable unlearning for various client-side strategies without modifying their core unlearning pipelines.

Abstract: Federated unlearning (FU) algorithms allow clients in federated settings to
exercise their ''right to be forgotten'' by removing the influence of their
data from a collaboratively trained model. Existing FU methods maintain data
privacy by performing unlearning locally on the client-side and sending
targeted updates to the server without exposing forgotten data; yet they often
rely on server-side cooperation, revealing the client's intent and identity
without enforcement guarantees - compromising autonomy and unlearning privacy.
In this work, we propose EFU (Enforced Federated Unlearning), a
cryptographically enforced FU framework that enables clients to initiate
unlearning while concealing its occurrence from the server. Specifically, EFU
leverages functional encryption to bind encrypted updates to specific
aggregation functions, ensuring the server can neither perform unauthorized
computations nor detect or skip unlearning requests. To further mask behavioral
and parameter shifts in the aggregated model, we incorporate auxiliary
unlearning losses based on adversarial examples and parameter importance
regularization. Extensive experiments show that EFU achieves near-random
accuracy on forgotten data while maintaining performance comparable to full
retraining across datasets and neural architectures - all while concealing
unlearning intent from the server. Furthermore, we demonstrate that EFU is
agnostic to the underlying unlearning algorithm, enabling secure,
function-hiding, and verifiable unlearning for any client-side FU mechanism
that issues targeted updates.

</details>


### [15] [Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data Manipulation Attacks](https://arxiv.org/abs/2508.08029)
*Thusitha Dayaratne,Ngoc Duy Pham,Viet Vo,Shangqi Lai,Sharif Abuadbba,Hajime Suzuki,Xingliang Yuan,Carsten Rudolph*

Main category: cs.CR

TL;DR: The paper explores using Large Language Models (LLMs) for secure anomaly detection in O-RAN by addressing Unicode-wise data manipulation attacks (hypoglyphs) from traditional ML-based systems.


<details>
  <summary>Details</summary>
Motivation: 5G and O-RAN architectures introduce novel security challenges, particularly vulnerability to hypoglyph attacks via malicious xApps in the Shared Data Layer (SDL). Traditional ML-based anomaly detection methods (e.g., AutoEncoders) fail under these subtle Unicode manipulations, risking system crashes and undetected threats.

Method: The authors evaluate LLM-based xApps as an alternative by testing their robustness against hypoglyphed input data. They leverage LLMs' adaptability via prompt engineering and assess detection latency for Near-Real-Time RIC compatibility.

Result: LLMs demonstrated robustness to hypoglyph attacks without crashing, achieved low detection latency (<0.07s), and showed potential for improved accuracy through prompt engineering, though refinement is needed.

Conclusion: LLMs offer a promising solution for O-RAN security, providing resilience against adversarial Unicode manipulations and low latency for real-time applications. Further research into enhancing detection accuracy via prompt engineering is required for practical deployment.

Abstract: The introduction of 5G and the Open Radio Access Network (O-RAN) architecture
has enabled more flexible and intelligent network deployments. However, the
increased complexity and openness of these architectures also introduce novel
security challenges, such as data manipulation attacks on the semi-standardised
Shared Data Layer (SDL) within the O-RAN platform through malicious xApps. In
particular, malicious xApps can exploit this vulnerability by introducing
subtle Unicode-wise alterations (hypoglyphs) into the data that are being used
by traditional machine learning (ML)-based anomaly detection methods. These
Unicode-wise manipulations can potentially bypass detection and cause failures
in anomaly detection systems based on traditional ML, such as AutoEncoders,
which are unable to process hypoglyphed data without crashing. We investigate
the use of Large Language Models (LLMs) for anomaly detection within the O-RAN
architecture to address this challenge. We demonstrate that LLM-based xApps
maintain robust operational performance and are capable of processing
manipulated messages without crashing. While initial detection accuracy
requires further improvements, our results highlight the robustness of LLMs to
adversarial attacks such as hypoglyphs in input data. There is potential to use
their adaptability through prompt engineering to further improve the accuracy,
although this requires further research. Additionally, we show that LLMs
achieve low detection latency (under 0.07 seconds), making them suitable for
Near-Real-Time (Near-RT) RIC deployments.

</details>


### [16] [IPBA: Imperceptible Perturbation Backdoor Attack in Federated Self-Supervised Learning](https://arxiv.org/abs/2508.08031)
*Jiayao Wang,Yang Song,Zhendong Zhao,Jiale Zhang,Qilin Wu,Junwu Zhu,Dongfang Zhao*

Main category: cs.CR

TL;DR: The paper proposes IPBA, an imperceptible backdoor attack for Federated Self-Supervised Learning (FSSL), addressing challenges like transferability and feature entanglement through decoupling techniques and Sliced-Wasserstein distance.


<details>
  <summary>Details</summary>
Motivation: Traditional backdoor attacks in FSSL rely on visually obvious triggers, failing to meet real-world requirements for stealth and practicality.

Method: IPBA decouples feature distributions of backdoor/aggregated samples and optimizes trigger generation using Sliced-Wasserstein distance to mitigate out-of-distribution issues and entanglement.

Result: IPBA outperforms existing backdoor methods in performance and remains robust against various defense mechanisms across multiple FSSL scenarios and datasets.

Conclusion: IPBA provides a more effective and stealthy backdoor attack framework for FSSL by addressing key limitations of prior approaches with novel feature decoupling and optimization strategies.

Abstract: Federated self-supervised learning (FSSL) combines the advantages of
decentralized modeling and unlabeled representation learning, serving as a
cutting-edge paradigm with strong potential for scalability and privacy
preservation. Although FSSL has garnered increasing attention, research
indicates that it remains vulnerable to backdoor attacks. Existing methods
generally rely on visually obvious triggers, which makes it difficult to meet
the requirements for stealth and practicality in real-world deployment. In this
paper, we propose an imperceptible and effective backdoor attack method against
FSSL, called IPBA. Our empirical study reveals that existing imperceptible
triggers face a series of challenges in FSSL, particularly limited
transferability, feature entanglement with augmented samples, and
out-of-distribution properties. These issues collectively undermine the
effectiveness and stealthiness of traditional backdoor attacks in FSSL. To
overcome these challenges, IPBA decouples the feature distributions of backdoor
and augmented samples, and introduces Sliced-Wasserstein distance to mitigate
the out-of-distribution properties of backdoor samples, thereby optimizing the
trigger generation process. Our experimental results on several FSSL scenarios
and datasets show that IPBA significantly outperforms existing backdoor attack
methods in performance and exhibits strong robustness under various defense
mechanisms.

</details>


### [17] [False Reality: Uncovering Sensor-induced Human-VR Interaction Vulnerability](https://arxiv.org/abs/2508.08043)
*Yancheng Jiang,Yan Jiang,Ruochen Zhou,Yi-Chao Chen,Xiaoyu Ji,Wenyuan Xu*

Main category: cs.CR

TL;DR: This paper introduces False Reality, a new physical attack threat to VR devices that manipulates sensor measurements to spoof user perception and induce harmful actions without modifying software.


<details>
  <summary>Details</summary>
Motivation: Existing VR attacks require deep software access and technical expertise, leaving physical vulnerabilities unexplored. The paper addresses gaps in VR security by demonstrating novel physical attacks that exploit sensor data manipulation to directly affect users' experiences and safety.

Method: The authors formalize False Reality through an attack pathway framework, validate three specific attack scenarios using physical experiments and user studies on five commercial VR headsets, and develop a prototype defense system to detect and mitigate these threats.

Result: Three attack pathways were successfully tested, showing that sensor tampering can disrupt VR services, induce perceptual illusions (like dizziness), and cause physical harm (e.g., collisions with obstacles) across major VR platforms.

Conclusion: False Reality attacks reveal critical physical security vulnerabilities in VR systems. The proposed defense mechanisms and attack framework provide foundational approaches for improving hardware-level security and safeguarding user safety in immersive technologies.

Abstract: Virtual Reality (VR) techniques, serving as the bridge between the real and
virtual worlds, have boomed and are widely used in manufacturing, remote
healthcare, gaming, etc. Specifically, VR systems offer users immersive
experiences that include both perceptions and actions. Various studies have
demonstrated that attackers can manipulate VR software to influence users'
interactions, including perception and actions. However, such attacks typically
require strong access and specialized expertise. In this paper, we are the
first to present a systematic analysis of physical attacks against VR systems
and introduce False Reality, a new attack threat to VR devices without
requiring access to or modification of their software. False Reality disturbs
VR system services by tampering with sensor measurements, and further spoofing
users' perception even inducing harmful actions, e.g., inducing dizziness or
causing users to crash into obstacles, by exploiting perceptual and
psychological effects. We formalize these threats through an attack pathway
framework and validate three representative pathways via physical experiments
and user studies on five commercial VR devices. Finally, we further propose a
defense prototype to mitigate such threats. Our findings shall provide valuable
insights for enhancing the security and resilience of future VR systems.

</details>


### [18] [Fully-Fluctuating Participation in Sleepy Consensus](https://arxiv.org/abs/2508.08068)
*Yuval Efron,Joachim Neu,Toniann Pitassi*

Main category: cs.CR

TL;DR: This paper introduces an 'external adversary' model for proof-of-stake consensus protocols in the sleepy model, enabling security against fully fluctuating participation without compromising efficiency or corruption resilience, overcoming theoretical barriers from prior work.


<details>
  <summary>Details</summary>
Motivation: Current proof-of-stake protocols in the sleepy model struggle to match Bitcoin's proof-of-work robustness against participation fluctuations while maintaining efficiency and corruption resilience, relying on restrictive assumptions.

Method: Proposes a new adversary model where corrupt nodes in proof-of-stake protocols retain secrecy of their secret keys, allowing protocols to enforce security constraints even as participation dynamically changes.

Result: Protocols leveraging the external adversary model achieve theoretical security guarantees under fully fluctuating participation, circumventing limitations of traditional adversary models and prior PoS protocol designs.

Conclusion: The external adversary model provides a natural and theoretically stronger framework for analyzing proof-of-stake consensus security, enabling protocols to maintain robustness across arbitrary participation fluctuations as effectively as proof-of-work systems.

Abstract: Proof-of-work allows Bitcoin to boast security amidst arbitrary fluctuations
in participation of miners throughout time, so long as, at any point in time, a
majority of hash power is honest. In recent years, however, the pendulum has
shifted in favor of proof-of-stake-based consensus protocols. There, the sleepy
model is the most prominent model for handling fluctuating participation of
nodes. However, to date, no protocol in the sleepy model rivals Bitcoin in its
robustness to drastic fluctuations in participation levels, with
state-of-the-art protocols making various restrictive assumptions. In this
work, we present a new adversary model, called external adversary. Intuitively,
in our model, corrupt nodes do not divulge information about their secret keys.
In this model, we show that protocols in the sleepy model can meaningfully
claim to remain secure against fully fluctuating participation, without
compromising efficiency or corruption resilience. Our adversary model is quite
natural, and arguably naturally captures the process via which malicious
behavior arises in protocols, as opposed to traditional worst-case modeling. On
top of which, the model is also theoretically appealing, circumventing a
barrier established in a recent work of Malkhi, Momose, and Ren.

</details>


### [19] [Differential Privacy for Regulatory Compliance in Cyberattack Detection on Critical Infrastructure Systems](https://arxiv.org/abs/2508.08190)
*Paritosh Ramanan,H. M. Mohaimanul Islam,Abhiram Reddy Alugula*

Main category: cs.CR

TL;DR: This paper proposes a differentially private cyberattack detection framework to balance regulatory compliance and stakeholder privacy in critical infrastructure networks.


<details>
  <summary>Details</summary>
Motivation: Regulatory oversight of CINs requires honest data sharing for security monitoring while stakeholders face privacy risks from disclosing sensitive sensor/control data.

Method: A two-phase privacy scheme protecting both covariance matrices and sensor-driven test statistics using DP hypothesis tests for alarm generation.

Result: Theoretical analysis demonstrates misclassification rates comparable to non-DP approaches. Real-world experiments validate reliability across interdependent stakeholder attack scenarios.

Conclusion: The framework achieves robust privacy protections without compromising attack detection effectiveness, enabling regulatory compliance in multi-stakeholder CIN environments.

Abstract: Industrial control systems are a fundamental component of critical
infrastructure networks (CIN) such as gas, water and power. With the growing
risk of cyberattacks, regulatory compliance requirements are also increasing
for large scale critical infrastructure systems comprising multiple utility
stakeholders. The primary goal of regulators is to ensure overall system
stability with recourse to trustworthy stakeholder attack detection. However,
adhering to compliance requirements requires stakeholders to also disclose
sensor and control data to regulators raising privacy concerns. In this paper,
we present a cyberattack detection framework that utilizes differentially
private (DP) hypothesis tests geared towards enhancing regulatory confidence
while alleviating privacy concerns of CIN stakeholders. The hallmark of our
approach is a two phase privacy scheme that protects the privacy of covariance,
as well as the associated sensor driven test statistics computed as a means to
generate alarms. Theoretically, we show that our method induces a
misclassification error rate comparable to the non-DP cases while delivering
robust privacy guarantees. With the help of real-world datasets, we show the
reliability of our DP-detection outcomes for a wide variety of attack scenarios
for interdependent stakeholders.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [20] [Refactoring-Aware Patch Integration Across Structurally Divergent Java Forks](https://arxiv.org/abs/2508.06718)
*Daniel Ogenrwot,John Businge*

Main category: cs.SE

TL;DR: RePatch addresses challenges in integrating bug-fix patches across long-lived divergent forks in Java projects by using refactoring-aware asymmetric patch transfer, successfully fixing 52.8% of cases failing with Git cherry-pick.


<details>
  <summary>Details</summary>
Motivation: Existing syntax-based tools like Git cherry-pick frequently fail (64.4% failure rate) when propagating bug fixes between divergent software variants due to structural drift from code refactorings that rename/reorganize code elements.

Method: RePatch extends the RefMerge framework with asymmetric patch transfer by (1) inverting refactorings in both source and target repositories to realign code structure, (2) applying the patch in the normalized state, and (3) replaying original refactorings to restore variant-specific semantics.

Result: In evaluation with 478 bug-fix pull requests, RePatch successfully integrates 52.8% of patches that failed using Git cherry-pick, demonstrating improvement over syntax-based approaches through semantic refactoring analysis.

Conclusion: This study shows semantic reasoning via refactoring-aware systems like RePatch is essential for variant-aware patch propagation, addressing the limitations of standard version control tools in handling structural code evolution.

Abstract: While most forks on platforms like GitHub are short-lived and used for social
collaboration, a smaller but impactful subset evolve into long-lived forks,
referred to here as variants, that maintain independent development
trajectories. Integrating bug-fix patches across such divergent variants poses
challenges due to structural drift, including refactorings that rename,
relocate, or reorganize code elements and obscure semantic correspondence. This
paper presents an empirical study of patch integration failures in 14 divergent
pair of variants and introduces RePatch, a refactoring-aware integration system
for Java repositories. RePatch extends the RefMerge framework, originally
designed for symmetric merges, by supporting asymmetric patch transfer. RePatch
inverts refactorings in both the source and target to realign the patch
context, applies the patch, and replays the transformations to preserve the
intent of the variant. In our evaluation of 478 bug-fix pull requests, Git
cherry-pick fails in 64.4% of cases due to structural misalignments, while
RePatch successfully integrates 52.8% of the previously failing patches. These
results highlight the limitations of syntax-based tools and the need for
semantic reasoning in variant-aware patch propagation.

</details>


### [21] [Quo Vadis, Code Review? Exploring the Future of Code Review](https://arxiv.org/abs/2508.06879)
*Michael Dorner,Andreas Bauer,Darja Šmite,Lukas Thode,Daniel Mendez,Ricardo Britto,Stephan Lukasczyk,Ehsan Zabardast,Michael Kormann*

Main category: cs.SE

TL;DR: The paper examines current code review practices among software engineers and predicts future changes while analyzing their long-term risks.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand how code review, a fundamental collaborative practice in software engineering, is perceived to evolve and the implications of these changes.

Method: Qualitative analysis of practitioners' reflections through surveys or interviews to identify trends and potential risks in code review practices.

Result: Practitioners anticipate shifts in code review approaches, suggesting possible areas of concern for maintaining collaborative integrity and effectiveness.

Conclusion: Anticipated changes in code review risk undermining its role in collaborative software engineering if long-term consequences are not carefully managed.

Abstract: Code review has long been a core practice in collaborative software
engineering. In this research, we explore how practitioners reflect on code
review today and what changes they anticipate in the near future. We then
discuss the potential long-term risks of these anticipated changes for the
evolution of code review and its role in collaborative software engineering.

</details>


### [22] [Multi-Modal Requirements Data-based Acceptance Criteria Generation using LLMs](https://arxiv.org/abs/2508.06888)
*Fanyu Wang,Chetan Arora,Yonghui Liu,Kaicheng Huang,Chakkrit Tantithamthavorn,Aldeida Aleti,Dishan Sambathkumar,David Lo*

Main category: cs.SE

TL;DR: RAGcceptance M2RE is a novel approach that uses Retrieval-Augmented Generation (RAG) with multi-modal data (textual and visual UI) to automate and improve acceptance criteria generation, validated in an industrial case study.


<details>
  <summary>Details</summary>
Motivation: Manual creation of accurate, comprehensive, and unambiguous acceptance criteria (ACs) for user interface (UI)-intensive applications is challenging due to reliance on domain-specific knowledge and visual context not captured by textual requirements alone. Current methods lack synergy between visual and textual data, leading to inefficiency and potential oversights.

Method: The paper proposes RAGcceptance M2RE, a system that integrates Retrieval-Augmented Generation (RAG) with multi-modal information from textual documentation and visual UI mockups/data. It systematically processes both data types to generate ACs, leveraging context-aware retrieval of relevant input and generation adapted to UI-specific requirements.

Result: In an industrial case study involving a UI-intensive education system with ~100,000 users, the multi-modal RAG approach improved AC relevance, correctness, and comprehensibility by ~25% compared to text-only methods. Practitioner evaluations confirmed a 40% reduction in manual effort and revealed that the model captured 30% more nuanced stakeholder intent while identifying criteria often overlooked by domain experts.

Conclusion: RAGcceptance M2RE demonstrates that multi-modal RAG techniques can significantly enhance AC generation, reduce manual effort, and uncover overlooked requirements in UI-intensive software. The research supports industry adoption of such methods for more efficient and comprehensive software validation, with open-sourced implementation and dataset to enable further exploration.

Abstract: Acceptance criteria (ACs) play a critical role in software development by
clearly defining the conditions under which a software feature satisfies
stakeholder expectations. However, manually creating accurate, comprehensive,
and unambiguous acceptance criteria is challenging, particularly in user
interface-intensive applications, due to the reliance on domain-specific
knowledge and visual context that is not always captured by textual
requirements alone. To address these challenges, we propose RAGcceptance M2RE,
a novel approach that leverages Retrieval-Augmented Generation (RAG) to
generate acceptance criteria from multi-modal requirements data, including both
textual documentation and visual UI information. We systematically evaluated
our approach in an industrial case study involving an education-focused
software system used by approximately 100,000 users. The results indicate that
integrating multi-modal information significantly enhances the relevance,
correctness, and comprehensibility of the generated ACs. Moreover, practitioner
evaluations confirm that our approach effectively reduces manual effort,
captures nuanced stakeholder intent, and provides valuable criteria that domain
experts may overlook, demonstrating practical utility and significant potential
for industry adoption. This research underscores the potential of multi-modal
RAG techniques in streamlining software validation processes and improving
development efficiency. We also make our implementation and a dataset
available.

</details>


### [23] [Integrating Rules and Semantics for LLM-Based C-to-Rust Translation](https://arxiv.org/abs/2508.06926)
*Feng Luo,Kexing Ji,Cuiyun Gao,Shuzheng Gao,Jia Feng,Kui Liu,Xin Xia,Michael R. Lyu*

Main category: cs.SE

TL;DR: IRENE is an LLM-based framework for translating C code to Rust that integrates rule patterns and semantic understanding to address limitations in prior methods, evaluated on two datasets with eight LLMs for accuracy and safety.


<details>
  <summary>Details</summary>
Motivation: Legacy C code lacks memory safety, and existing translation approaches either rely on limited static rules with poor Rust syntactic compliance or use direct LLM prompting that fails to maintain semantic consistency in complex code scenarios.

Method: IRENE consists of three modules: rule-augmented retrieval using static analysis-generated patterns, structured summarization for semantic context, and compiler diagnostic-error driven iterative translation refinement.

Result: IRENE demonstrated improved translation accuracy and Rust code safety compared to previous methods on xCodeEval (academic) and HW-Bench (industrial) datasets across multiple LLMs.

Conclusion: By combining static rules with semantic understanding and error feedback, IRENE advances LLM-based code translation for safer, semantically consistent Rust code generation from C.

Abstract: Automated translation of legacy C code into Rust aims to ensure memory safety
while reducing the burden of manual migration. Early approaches in code
translation rely on static rule-based methods, but they suffer from limited
coverage due to dependence on predefined rule patterns. Recent works regard the
task as a sequence-to-sequence problem by leveraging large language models
(LLMs). Although these LLM-based methods are capable of reducing unsafe code
blocks, the translated code often exhibits issues in following Rust rules and
maintaining semantic consistency. On one hand, existing methods adopt a direct
prompting strategy to translate the C code, which struggles to accommodate the
syntactic rules between C and Rust. On the other hand, this strategy makes it
difficult for LLMs to accurately capture the semantics of complex code. To
address these challenges, we propose IRENE, an LLM-based framework that
Integrates RulEs aNd sEmantics to enhance translation. IRENE consists of three
modules: 1) a rule-augmented retrieval module that selects relevant translation
examples based on rules generated from a static analyzer developed by us,
thereby improving the handling of Rust rules; 2) a structured summarization
module that produces a structured summary for guiding LLMs to enhance the
semantic understanding of C code; 3) an error-driven translation module that
leverages compiler diagnostics to iteratively refine translations. We evaluate
IRENE on two datasets (xCodeEval, a public dataset, and HW-Bench, an industrial
dataset provided by Huawei) and eight LLMs, focusing on translation accuracy
and safety.

</details>


### [24] [When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust "APIs'' for Human-AI Interaction](https://arxiv.org/abs/2508.06942)
*Zhenchang Xing,Yang Liu,Zhuo Cheng,Qing Huang,Dehai Zhao,Daniel Sun,Chenhua Liu*

Main category: cs.SE

TL;DR: The paper proposes Controlled Natural Language for Prompting (CNL-P), combining prompt engineering and software engineering principles to reduce ambiguity in natural language prompts. It introduces an NL2CNL-P conversion tool and a linting tool based on static analysis, demonstrating improved LLM response quality through systematic experimentation.


<details>
  <summary>Details</summary>
Motivation: Existing natural language (NL) prompts for LLMs suffer from ambiguity, while established prompt engineering (PE) guidelines and templates have limitations. This work aims to integrate SE's precise grammar and semantic norms into NL prompting to enhance consistency and quality of LLM outputs.

Method: 1) Introduces CNL-P framework with strict grammar structures and semantic norms
2) Develops LLM-based NL2CNL-P conversion tool
3) Implements static analysis-powered linting tool for syntactic/semantic checks
4) Conducts systematic evaluation of CNL-P effectiveness

Result: Experiments show CNL-P significantly improves LLM response quality compared to traditional NL prompts. The conversion tool lowers the learning barrier for structured prompting, and the linting tool successfully identifies and corrects prompt structure/semantic issues through novel static analysis application.

Conclusion: CNL-P bridges prompt engineering and software engineering paradigms, enabling a more systematic and reliable approach to human-LLM interaction. This lays the groundwork for a natural language-centric programming paradigm with demonstrated benefits for output consistency and quality.

Abstract: With the growing capabilities of large language models (LLMs), they are
increasingly applied in areas like intelligent customer service, code
generation, and knowledge management. Natural language (NL) prompts act as the
``APIs'' for human-LLM interaction. To improve prompt quality, best practices
for prompt engineering (PE) have been developed, including writing guidelines
and templates. Building on this, we propose Controlled NL for Prompt (CNL-P),
which not only incorporates PE best practices but also draws on key principles
from software engineering (SE). CNL-P introduces precise grammar structures and
strict semantic norms, further eliminating NL's ambiguity, allowing for a
declarative but structured and accurate expression of user intent. This helps
LLMs better interpret and execute the prompts, leading to more consistent and
higher-quality outputs. We also introduce an NL2CNL-P conversion tool based on
LLMs, enabling users to write prompts in NL, which are then transformed into
CNL-P format, thus lowering the learning curve of CNL-P. In particular, we
develop a linting tool that checks CNL-P prompts for syntactic and semantic
accuracy, applying static analysis techniques to NL for the first time.
Extensive experiments demonstrate that CNL-P enhances the quality of LLM
responses through the novel and organic synergy of PE and SE. We believe that
CNL-P can bridge the gap between emerging PE and traditional SE, laying the
foundation for a new programming paradigm centered around NL.

</details>


### [25] [An Empirical Study on Method-Level Performance Evolution in Open-Source Java Projects](https://arxiv.org/abs/2508.07084)
*Kaveh Shahedi,Nana Gyambrah,Heng Li,Maxime Lamothe,Foutse Khomh*

Main category: cs.SE

TL;DR: The paper analyzes how method-level code changes affect software performance in 15 open-source Java projects, finding that 32.7% of changes cause measurable performance impacts with more regressions (1.3x) than improvements. Conventional assumptions about code change categories are challenged, and algorithmic changes show both high improvement potential and regression risks. Senior developers produce more stable changes, and code complexity correlates with regression likelihood.


<details>
  <summary>Details</summary>
Motivation: Developers often make assumptions about code changes' performance impacts without fine-grained empirical validation. This study aims to quantify the significance and magnitude of method-level code changes to software performance evolution.

Method: A large-scale empirical study of 739 commits (1,499 method changes) in 15 open-source Java projects using JMH benchmarking, bytecode instrumentation for execution metrics, and statistical analysis of four aspects: temporal patterns, code change types, developer expertise, and domain-size interactions.

Result: 32.7% of method changes had measurable impacts; no significant differences across code change categories (contrary to expectations); algorithmic changes offered highest improvement potential but carried 2.4x more regression risk than refactorings; senior developers made 40% more stable changes; complex code (CBO>7) had 1.8x higher regression likelihood; web server + small projects showed highest instability (28.6% ±8.9% variation).

Conclusion: Automated performance testing in CI pipelines is empirically justified. Current risk-stratified development strategies may need revision since no significant differences exist in impact distributions across code change types. Project domain and size (particularly web-server small projects) are critical factors for performance instability.

Abstract: Performance is a critical quality attribute in software development, yet the
impact of method-level code changes on performance evolution remains poorly
understood. While developers often make intuitive assumptions about which types
of modifications are likely to cause performance regressions or improvements,
these beliefs lack empirical validation at a fine-grained level. We conducted a
large-scale empirical study analyzing performance evolution in 15 mature
open-source Java projects hosted on GitHub. Our analysis encompassed 739
commits containing 1,499 method-level code changes, using Java Microbenchmark
Harness (JMH) for precise performance measurement and rigorous statistical
analysis to quantify both the significance and magnitude of performance
variations. We employed bytecode instrumentation to capture method-specific
execution metrics and systematically analyzed four key aspects: temporal
performance patterns, code change type correlations, developer and complexity
factors, and domain-size interactions. Our findings reveal that 32.7% of
method-level changes result in measurable performance impacts, with regressions
occurring 1.3 times more frequently than improvements. Contrary to conventional
wisdom, we found no significant differences in performance impact distributions
across code change categories, challenging risk-stratified development
strategies. Algorithmic changes demonstrate the highest improvement potential
but carry substantial regression risk. Senior developers produce more stable
changes with fewer extreme variations, while code complexity correlates with
increased regression likelihood. Domain-size interactions reveal significant
patterns, with web server + small projects exhibiting the highest performance
instability. Our study provides empirical evidence for integrating automated
performance testing into continuous integration pipelines.

</details>


### [26] [From Noise to Knowledge: Interactive Summaries for Developer Alerts](https://arxiv.org/abs/2508.07169)
*Burak Yetiştiren,Hong Jin Kang,Miryung Kim*

Main category: cs.SE

TL;DR: CLARITY is an interactive warning sensemaking tool that groups related bug reports through active rule inference feedback, enabling faster and more confident root cause identification in Java projects.


<details>
  <summary>Details</summary>
Motivation: programmers review each bug-finding tool warning individually without leveraging patterns or relationships, limiting cognitive efficiency during sensemaking; users require customizable interpretation frameworks to accommodate varying preferences

Method: interactive inquiry system with rule-level feedback during warning annotation, automatically surfacing structural similarities through algorithmic analysis of containment, subtyping, method invocation, field access, and expression patterns

Result: 14-participant within-subject study found increased speed and confidence in root cause analysis for recurring warning types; simulation showed 40% fewer interactions (11.8 vs 17.8) when using rule-level feedback alignment

Conclusion: active learning-based summarization with customizable feedback significantly improves warning sensemaking performance while addressing individual user variation in grouping preferences

Abstract: Programmers using bug-finding tools often review their reported warnings one
by one. Based on the insight that identifying recurring themes and
relationships can enhance the cognitive process of sensemaking, we propose
CLARITY, which supports interpreting tool-generated warnings through
interactive inquiry. CLARITY derives summary rules for custom grouping of
related warnings with active feedback. As users mark warnings as interesting or
uninteresting, CLARITY's rule inference algorithm surfaces common symptoms,
highlighting structural similarities in containment, subtyping, invoked
methods, accessed fields, and expressions.
  We demonstrate CLARITY on Infer and SpotBugs warnings across two mature Java
projects. In a within-subject user study with 14 participants, users
articulated root causes for similar uninteresting warnings faster and with more
confidence using CLARITY. We observed significant individual variation in
desired grouping, reinforcing the need for customizable sensemaking. Simulation
shows that with rule-level feedback, only 11.8 interactions are needed on
average to align all inferred rules with a simulated user's labels (vs. 17.8
without). Our evaluation suggests that CLARITY's active learning-based
summarization enhances interactive warning sensemaking.

</details>


### [27] [Dynamic Benchmark Construction for Evaluating Large Language Models on Real-World Codes](https://arxiv.org/abs/2508.07180)
*Zhe Zhang,Runlin Liu,Aishan Liu,Xingyu Liu,Xiang Gao,Hailong Sun*

Main category: cs.SE

TL;DR: The paper proposes CODE2BENCH, a dynamic benchmarking pipeline for LLMs in code generation tasks. It introduces three innovations: Automated Dynamism for contamination control, Scope Graph-based dependency analysis to categorize self-contained (SC) and weakly self-contained (WSC) tasks, and Property-Based Testing for rigorous verification. CODE2BENCH-2505 benchmark (from 880 Python projects) shows consistent LLM struggles with SC tasks requiring non-standard logic and cross-language transfer, compared to Python-specific WSC tasks.


<details>
  <summary>Details</summary>
Motivation: Existing code generation benchmarks suffer from data contamination (models memorizing training data) and insufficient test rigor, limiting their ability to reveal LLM failures in real-world software development scenarios.

Method: CODE2BENCH implements a three-pronged approach: 1) Periodically updates training data using recent GitHub code to reduce contamination. 2) Uses Scope Graphs to analyze function-level dependencies, creating SC tasks (cross-language evaluation) and WSC tasks (Python-specific with library usage). 3) Integrates Property-Based Testing to automatically generate comprehensive test suites for functional verification.

Result: CODE2BENCH-2505 benchmark achieved 100% average branch coverage across 1,163 tasks. Evaluations showed systematic LLM struggles with: a) SC tasks needing non-idiomatic logic, b) Cross-language code generation tasks. In contrast, Python-specific WSC tasks had better performance across 16 tested models. The methodology's robustness reduced contamination artifacts compared to static benchmarks.

Conclusion: CODE2BENCH establishes a contamination-resistant, language-agnostic framework for benchmarking LLMs in code generation. The findings highlight persistent challenges in handling self-contained problems and cross-language transfers, while demonstrating practical effectiveness for Python-centric WSC tasks. This introduces a more rigorous and real-world aligned evaluation methodology for LLM research.

Abstract: As large language models LLMs) become increasingly integrated into software
development workflows, rigorously evaluating their performance on complex,
real-world code generation tasks has become essential. However, existing
benchmarks often suffer from data contamination and limited test rigor,
constraining their ability to reveal model failures effectively. To address
these, we present CODE2BENCH, a end-to-end pipeline for dynamically
constructing robust and contamination-resistant benchmarks from real-world
GitHub repositories. Specifically, CODE2BENCH introduces three key innovations:
(1) Automated Dynamism, achieved through periodic ingestion of recent code to
minimize training data contamination; (2) Scope Graph-based dependency
analysis, which enables structured classification of functions into benchmark
instances with controlled dependency levels (distinguishing between
Self-Contained (SC) tasks for cross-language evaluation and Weakly
Self-Contained (WSC) tasks involving permitted library usage); and (3)
Property-Based Testing (PBT) for the automated synthesis of rigorous test
suites to enable thorough functional verification. Using this pipeline, we
construct CODE2BENCH-2505, the first benchmark derived from 880 recent Python
projects spanning diverse domains, comprising 1,163 code generation tasks with
100% average branch coverage on ground-truth implementations. Extensive
evaluation of 16 LLMs using CODE2BENCH-2505 reveals that models consistently
struggle with SC tasks requiring complex, non-standard logic and cross-language
transfer, while showing relatively stronger performance on WSC tasks in Python.
Our work introduces a contamination-resistant, language-agnostic methodology
for dynamic benchmark construction, offering a principled foundation for the
comprehensive and realistic evaluation of LLMs on real-world software
development tasks.

</details>


### [28] [TraceLens: Question-Driven Debugging for Taint Flow Understanding](https://arxiv.org/abs/2508.07198)
*Burak Yetiştiren,Hong Jin Kang,Miryung Kim*

Main category: cs.SE

TL;DR: TraceLens is a QA-style debugging interface for taint analysis that addresses limitations in existing tools by enabling users to ask why, why-not, and what-if questions, improving flow reasoning and reducing cognitive load.


<details>
  <summary>Details</summary>
Motivation: Current taint analysis tools lack end-user debugging capabilities to investigate unexpected flows, missing flows, and global connectivity impacts, hindering effective security analysis.

Method: The paper proposes TraceLens, an interface supporting speculative what-if analysis through QA-style interactions, allowing developers to explore the impact of source/sink configurations and 3rd-party library models on taint flows.

Result: A user study with 12 participants showed TraceLens achieved 21% higher accuracy than CodeQL average, 45% reduction in mental demand (NASA-TLX), and higher confidence in identifying relevant flows.

Conclusion: TraceLens enhances end-user taint analysis by providing structured debugging through QA mechanisms, demonstrating improved accuracy, reduced cognitive load, and better handling of complex flow connectivity across sources and sinks.

Abstract: Taint analysis is a security analysis technique used to track the flow of
potentially dangerous data through an application and its dependent libraries.
Investigating why certain unexpected flows appear and why expected flows are
missing is an important sensemaking process during end-user taint analysis.
Existing taint analysis tools often do not provide this end-user debugging
capability, where developers can ask why, why-not, and what-if questions about
dataflows and reason about the impact of configuring sources and sinks, and
models of 3rd-party libraries that abstract permissible and impermissible data
flows. Furthermore, a tree-view or a list-view used in existing
taint-analyzer's visualization makes it difficult to reason about the global
impact on connectivity between multiple sources and sinks.
  Inspired by the insight that sensemaking tool-generated results can be
significantly improved by a QA inquiry process, we propose TraceLens, a first
end-user question-answer style debugging interface for taint analysis. It
enables a user to ask why, why-not, and what-if questions to investigate the
existence of suspicious flows, the non-existence of expected flows, and the
global impact of third-party library models. TraceLens performs speculative
what-if analysis, to help a user in debugging how different connectivity
assumptions affect overall results. A user study with 12 participants shows
that participants using TraceLens achieved 21% higher accuracy on average,
compared to CodeQL. They also reported a 45% reduction in mental demand
(NASA-TLX) and rated higher confidence in identifying relevant flows using
TraceLens.

</details>


### [29] [AutoAssert 1: A LoRA Fine-Tuned LLM Model for Efficient Automated Assertion Generation](https://arxiv.org/abs/2508.07371)
*Yi Zhong,Hongchao Liu,Di ZHao*

Main category: cs.SE

TL;DR: The paper introduces a lightweight HDL-based assertion generation method using an adjustable LLM and Unsloth platform to reduce training costs while maintaining accuracy in hardware logic testing.


<details>
  <summary>Details</summary>
Motivation: Increasing software system complexity demands efficient automated testing tools. Existing methods may be costly or lack generalization. This work addresses the urgent need for a scalable, accurate solution.

Method: A parameter-adjustable large language model (LLM) is integrated with the Unsloth platform to automatically generate test cases. The framework balances cost reduction with preservation of accuracy and generalization in hardware logic assertion generation.

Result: Empirical evaluation demonstrates the method's ability to efficiently generate assertions that strictly conform to hardware logic specifications, outperforming traditional approaches in cost efficiency.

Conclusion: The proposed framework offers a robust, flexible solution for modern software testing challenges by leveraging parameter-adjustable LLMs, reducing costs while maintaining accuracy. Implementation is available at linked repositories.

Abstract: As the complexity of software systems continues to increase, the demand for
automated testing and maintenance tools is growing exponentially. To meet this
urgent need, we propose a new assertion generation method based on Hardware
Description Language (HDL). This method combines a lightweight,
parameter-adjustable large language model (LLM) with the Unsloth platform to
automatically generate test cases, thereby significantly reducing training
costs without sacrificing accuracy or generalization performance. Empirical
evaluation shows that our method can efficiently generate assertions that
strictly conform to the hardware logic. This framework provides a robust and
flexible solution to modern software testing and maintenance challenges.
https://github.com/liusu-orange/AutoAssert-1 and
https://gitee.com/OpenBPU/auto-assert1 are the locations of the source code.

</details>


### [30] [Extracting Overlapping Microservices from Monolithic Code via Deep Semantic Embeddings and Graph Neural Network-Based Soft Clustering](https://arxiv.org/abs/2508.07486)
*Morteza Ziabakhsh,Kiyan Rezaee,Sadegh Eskandari,Seyed Amir Hossein Tabatabaei,Mohammad M. Ghassemi*

Main category: cs.SE

TL;DR: This paper introduces Mo2oM, a framework for microservice extraction using soft clustering to allow components to belong probabilistically to multiple microservices, achieving significant improvements in modularity and communication overhead compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Hard clustering methods in microservice extraction increase inter-service coupling and reduce intra-service cohesion, contrasting with expert practices that intentionally replicate components for efficiency. The paper aims to address these limitations by enabling overlapping microservices.

Method: Mo2oM combines deep semantic embeddings with structural dependencies from method-call graphs to capture functional and architectural relationships. It employs a graph neural network-based soft clustering algorithm to generate probabilistic microservice assignments.

Result: Evaluation on four open-source benchmarks showed Mo2oM outperformed eight baselines with up to 40.97% better structural modularity, 58% reduction in inter-service calls, 26.16% fewer interfaces, and improved service size balance.

Conclusion: Mo2oM effectively bridges the gap between automated and expert-driven microservice decomposition by leveraging soft clustering, enhancing modularity and deprioritizing communication overhead in large-scale systems.

Abstract: Modern software systems are increasingly shifting from monolithic
architectures to microservices to enhance scalability, maintainability, and
deployment flexibility. Existing microservice extraction methods typically rely
on hard clustering, assigning each software component to a single microservice.
This approach often increases inter-service coupling and reduces intra-service
cohesion. We propose Mo2oM (Monolithic to Overlapping Microservices), a
framework that formulates microservice extraction as a soft clustering problem,
allowing components to belong probabilistically to multiple microservices. This
approach is inspired by expert-driven decompositions, where practitioners
intentionally replicate certain software components across services to reduce
communication overhead. Mo2oM combines deep semantic embeddings with structural
dependencies extracted from methodcall graphs to capture both functional and
architectural relationships. A graph neural network-based soft clustering
algorithm then generates the final set of microservices. We evaluate Mo2oM on
four open-source monolithic benchmarks and compare it against eight
state-of-the-art baselines. Our results demonstrate that Mo2oM achieves
improvements of up to 40.97% in structural modularity (balancing cohesion and
coupling), 58% in inter-service call percentage (communication overhead),
26.16% in interface number (modularity and decoupling), and 38.96% in
non-extreme distribution (service size balance) across all benchmarks.

</details>


### [31] [Adopting Road-Weather Open Data in Route Recommendation Engine](https://arxiv.org/abs/2508.07881)
*Henna Tammia,Benjamin Kämä,Ella Peltonen*

Main category: cs.SE

TL;DR: This paper analyzes challenges in using Finland's DigiTraffic road data API for a personalized routing recommendation system, including preprocessing steps and machine learning validation for three driver profiles.


<details>
  <summary>Details</summary>
Motivation: While DigiTraffic provides extensive real-time road data, practical applications require structured preprocessing methods and tailored ML tools to overcome data quality issues and variability.

Method: The authors examine road-weather attributes from DigiTraffic as a case study, develop a methodology for efficient data usage, and implement a recommendation engine validated through real-world driver profiles.

Result: Three distinct driver profiles achieved personalized route recommendations using the proposed preprocessing and ML framework, demonstrating the system's effectiveness in real conditions.

Conclusion: The study provides actionable insights for leveraging large-scale road data APIs through structured preprocessing and algorithmic approaches adaptable to driver-specific preferences.

Abstract: Digitraffic, Finland's open road data interface, provides access to
nationwide road sensors with more than 2,300 real-time attributes from 1,814
stations. However, efficiently utilizing such a versatile data API for a
practical application requires a deeper understanding of the data qualities,
preprocessing phases, and machine learning tools. This paper discusses the
challenges of large-scale road weather and traffic data. We go through the
road-weather-related attributes from DigiTraffic as a practical example of
processes required to work with such a dataset. In addition, we provide a
methodology for efficient data utilization for the target application, a
personalized road recommendation engine based on a simple routing application.
We validate our solution based on real-world data, showing we can efficiently
identify and recommend personalized routes for three different driver profiles.

</details>


### [32] [SHIELDA: Structured Handling of Exceptions in LLM-Driven Agentic Workflows](https://arxiv.org/abs/2508.07935)
*Jingwen Zhou,Jieshan Chen,Qinghua Lu,Dehai Zhao,Liming Zhu*

Main category: cs.SE

TL;DR: This paper introduces SHIELDA, a modular framework for structured exception handling in LLM-powered agentic workflows by linking exceptions to their root reasoning-phase causes and enabling composable recovery strategies across 12 agent artifacts.


<details>
  <summary>Details</summary>
Motivation: Current LLM agentic systems struggle with exceptions due to (1) superficial handling of execution-phase errors without identifying root reasoning-phase causes, and (2) fragile recovery logic lacking escalation pathways for failed initial attempts.

Method: 1) Constructed a taxonomy of 36 exception types across 12 agent artifacts. 2) Designed SHIELDA with three components: exception classifier, handling pattern registry, and structured executor with local handling, flow control, and state recovery modules.

Result: Case study on AutoPR agent demonstrated SHIELDA's ability to recover from a reasoning-induced exception through phase-aware handling, outperforming existing solutions in completeness and robustness.

Conclusion: SHIELDA provides a systemic solution to multi-phase exception handling in LLM agents by combining structured error classification with composable recovery patterns, establishing a foundation for more robust autonomous workflows.

Abstract: Large Language Model (LLM) agentic systems are software systems powered by
LLMs that autonomously reason, plan, and execute multi-step workflows to
achieve human goals, rather than merely executing predefined steps. During
execution, these workflows frequently encounter exceptions. Existing exception
handling solutions often treat exceptions superficially, failing to trace
execution-phase exceptions to their reasoning-phase root causes. Furthermore,
their recovery logic is brittle, lacking structured escalation pathways when
initial attempts fail. To tackle these challenges, we first present a
comprehensive taxonomy of 36 exception types across 12 agent artifacts.
Building on this, we propose SHIELDA (Structured Handling of Exceptions in
LLM-Driven Agentic Workflows), a modular runtime exception handling framework
for LLM agentic workflows. SHIELDA uses an exception classifier to select a
predefined exception handling pattern from a handling pattern registry. These
patterns are then executed via a structured handling executor, comprising local
handling, flow control, and state recovery, to enable phase-aware recovery by
linking exceptions to their root causes and facilitating composable strategies.
We validate SHIELDA's effectiveness through a case study on the AutoPR agent,
demonstrating effective, cross-phase recovery from a reasoning-induced
exception.

</details>


### [33] [Exploring the Challenges and Opportunities of AI-assisted Codebase Generation](https://arxiv.org/abs/2508.07966)
*Philipp Eibl,Sadra Sabouri,Souti Chattopadhyay*

Main category: cs.SE

TL;DR: This paper investigates how developers interact with codebase AI assistants (CBAs), identifies six challenges and five barriers to their adoption, and proposes design opportunities based on a user study with 16 participants and an analysis of 21 commercial CBAs.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand why CBAs remain underadopted despite their potential, by examining developers' interactions, dissatisfaction factors, and barriers to integration into workflows.

Method: A counterbalanced user study and interviews with 16 students/developers, followed by a survey of 21 commercial CBAs to compare capabilities and identify design opportunities.

Result: Participants reported low satisfaction (mean=2.8, median=3), with 77% dissatisfaction due to functionality, 42% due to code quality, and 25% due to communication issues. Six key challenges and five barriers were identified.

Conclusion: Current CBAs fail to meet developer expectations in functionality and quality, highlighting the need for improved communication and system design to enhance adoption and usefulness.

Abstract: Recent AI code assistants have significantly improved their ability to
process more complex contexts and generate entire codebases based on a textual
description, compared to the popular snippet-level generation. These codebase
AI assistants (CBAs) can also extend or adapt codebases, allowing users to
focus on higher-level design and deployment decisions. While prior work has
extensively studied the impact of snippet-level code generation, this new class
of codebase generation models is relatively unexplored. Despite initial
anecdotal reports of excitement about these agents, they remain less frequently
adopted compared to snippet-level code assistants. To utilize CBAs better, we
need to understand how developers interact with CBAs, and how and why CBAs fall
short of developers' needs. In this paper, we explored these gaps through a
counterbalanced user study and interview with (n = 16) students and developers
working on coding tasks with CBAs. We found that participants varied the
information in their prompts, like problem description (48% of prompts),
required functionality (98% of prompts), code structure (48% of prompts), and
their prompt writing process. Despite various strategies, the overall
satisfaction score with generated codebases remained low (mean = 2.8, median =
3, on a scale of one to five). Participants mentioned functionality as the most
common factor for dissatisfaction (77% of instances), alongside poor code
quality (42% of instances) and communication issues (25% of instances). We
delve deeper into participants' dissatisfaction to identify six underlying
challenges that participants faced when using CBAs, and extracted five barriers
to incorporating CBAs into their workflows. Finally, we surveyed 21 commercial
CBAs to compare their capabilities with participant challenges and present
design opportunities for more efficient and useful CBAs.

</details>


### [34] [PyVeritas: On Verifying Python via LLM-Based Transpilation and Bounded Model Checking for C](https://arxiv.org/abs/2508.08171)
*Pedro Orvalho,Marta Kwiatkowska*

Main category: cs.SE

TL;DR: PyVeritas uses LLMs to transpile Python to C for formal verification, enabling assertion-based checks and fault localization with C tools, achieving 80-90% accuracy on benchmarks.


<details>
  <summary>Details</summary>
Motivation: The abstract highlights the lack of robust formal verification tools for Python compared to C, and the challenges posed by existing transpilers (e.g., Cython) due to their low-level nature and verbosity. This motivates the development of a high-level, LLM-driven transpilation approach to enable mature verification techniques for Python programs.

Method: PyVeritas employs Large Language Models (LLMs) to perform high-level transpilation of Python code to C. The LLM-derived C code is then subjected to bounded model checking and MaxSAT-based fault localisation using existing C model checkers like CBMC. This two-step process bypasses Python's verification limitations by leveraging C's mature verification ecosystem.

Result: The framework achieves 80–90% transpilation accuracy on selected Python benchmarks, demonstrating its effectiveness in creating a verification and bug-localization environment for small but meaningful Python programs through LLMs.

Conclusion: PyVeritas presents a promising solution to Python's formal verification challenges by combining LLMs and C tools. The results suggest that LLM-based transpilation can provide an accurate and practical path for assertion-driven verification and interpretable debugging in Python, though focus remains on non-trivial but small-scale programs.

Abstract: Python has become the dominant language for general-purpose programming, yet
it lacks robust tools for formal verification. In contrast, programmers working
in languages such as C benefit from mature model checkers, for example CBMC,
which enable exhaustive symbolic reasoning and fault localisation. The inherent
complexity of Python, coupled with the verbosity and low-level nature of
existing transpilers (e.g., Cython), have historically limited the
applicability of formal verification to Python programs.
  In this paper, we propose PyVeritas, a novel framework that leverages Large
Language Models (LLMs) for high-level transpilation from Python to C, followed
by bounded model checking and MaxSAT-based fault localisation in the generated
C code. PyVeritas enables verification and bug localisation for Python code
using existing model checking tools for C. Our empirical evaluation on two
Python benchmarks demonstrates that LLM-based transpilation can achieve a high
degree of accuracy, up to 80--90% for some LLMs, enabling effective development
environment that supports assertion-based verification and interpretable fault
diagnosis for small yet non-trivial Python programs.

</details>
