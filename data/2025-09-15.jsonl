{"id": "2509.09853", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09853", "abs": "https://arxiv.org/abs/2509.09853", "authors": ["Zhiyu Fan", "Kirill Vasilevski", "Dayi Lin", "Boyuan Chen", "Yihao Chen", "Zhiqing Zhong", "Jie M. Zhang", "Pinjia He", "Ahmed E. Hassan"], "title": "SWE-Effi: Re-Evaluating Software AI Agent System Effectiveness Under Resource Constraints", "comment": null, "summary": "The advancement of large language models (LLMs) and code agents has\ndemonstrated significant potential to assist software engineering (SWE) tasks,\nsuch as autonomous issue resolution and feature addition. Existing AI for\nsoftware engineering leaderboards (e.g., SWE-bench) focus solely on solution\naccuracy, ignoring the crucial factor of effectiveness in a\nresource-constrained world. This is a universal problem that also exists beyond\nsoftware engineering tasks: any AI system should be more than correct - it must\nalso be cost-effective. To address this gap, we introduce SWE-Effi, a set of\nnew metrics to re-evaluate AI systems in terms of holistic effectiveness\nscores. We define effectiveness as the balance between the accuracy of outcome\n(e.g., issue resolve rate) and the resources consumed (e.g., token and time).\nIn this paper, we specifically focus on the software engineering scenario by\nre-ranking popular AI systems for issue resolution on a subset of the SWE-bench\nbenchmark using our new multi-dimensional metrics. We found that AI system's\neffectiveness depends not just on the scaffold itself, but on how well it\nintegrates with the base model, which is key to achieving strong performance in\na resource-efficient manner. We also identified systematic challenges such as\nthe \"token snowball\" effect and, more significantly, a pattern of \"expensive\nfailures\". In these cases, agents consume excessive resources while stuck on\nunsolvable tasks - an issue that not only limits practical deployment but also\ndrives up the cost of failed rollouts during RL training. Lastly, we observed a\nclear trade-off between effectiveness under the token budget and effectiveness\nunder the time budget, which plays a crucial role in managing project budgets\nand enabling scalable reinforcement learning, where fast responses are\nessential."}
{"id": "2509.09873", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09873", "abs": "https://arxiv.org/abs/2509.09873", "authors": ["James Jewitt", "Hao Li", "Bram Adams", "Gopi Krishnan Rajbahadur", "Ahmed E. Hassan"], "title": "From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI Ecosystem", "comment": "9 pages, 4 figures, 5 tables, pre-print", "summary": "Hidden license conflicts in the open-source AI ecosystem pose serious legal\nand ethical risks, exposing organizations to potential litigation and users to\nundisclosed risk. However, the field lacks a data-driven understanding of how\nfrequently these conflicts occur, where they originate, and which communities\nare most affected. We present the first end-to-end audit of licenses for\ndatasets and models on Hugging Face, as well as their downstream integration\ninto open-source software applications, covering 364 thousand datasets, 1.6\nmillion models, and 140 thousand GitHub projects. Our empirical analysis\nreveals systemic non-compliance in which 35.5% of model-to-application\ntransitions eliminate restrictive license clauses by relicensing under\npermissive terms. In addition, we prototype an extensible rule engine that\nencodes almost 200 SPDX and model-specific clauses for detecting license\nconflicts, which can solve 86.4% of license conflicts in software applications.\nTo support future research, we release our dataset and the prototype engine.\nOur study highlights license compliance as a critical governance challenge in\nopen-source AI and provides both the data and tools necessary to enable\nautomated, AI-aware compliance at scale."}
{"id": "2509.09917", "categories": ["cs.SE", "D.2.4"], "pdf": "https://arxiv.org/pdf/2509.09917", "abs": "https://arxiv.org/abs/2509.09917", "authors": ["Zehan Chen", "Long Zhang", "Zhiwei Zhang", "JingJing Zhang", "Ruoyu Zhou", "Yulong Shen", "JianFeng Ma", "Lin Yang"], "title": "SLD-Spec: Enhancement LLM-assisted Specification Generation for Complex Loop Functions via Program Slicing and Logical Deletion", "comment": "22 pages, 2 figures, conference", "summary": "Automatically generating formal specifications from program code can greatly\nenhance the efficiency of program verification and enable end-to-end automation\nfrom requirements to reliable software. However, existing LLM-based approaches\noften struggle with programs that include complex loop structures, leading to\nirrelevant specifications. Moreover, the rigorous proof obligations and design\nconstraints imposed by verification tools can further result in incomplete and\nambiguous specifications. To address these challenges, we propose SLD-Spec, an\nLLM-assisted specification generation method tailored for programs with complex\nloop constructs. SLD-Spec introduces two novel phases into the traditional\nspecification generation framework: (1) A slicing phase, which decomposes each\nfunction into code fragments containing independent loop structures, thereby\nreducing the complexity of specification generation; and (2) A logical deletion\nphase, which applies LLM-based reasoning to filter out incorrect candidate\nspecifications--especially those not easily identified by verification\ntool--while retaining valid ones. Experimental results show that on the simple\ndataset, SLD-Spec successfully verifies five more programs than the\nstate-of-the-art AutoSpec and reduces runtime by 23.73%. To address the\nlimitations of existing research, we manually construct a dataset comprising\nfour categories of complex loop programs. On this dataset, SLD-Spec\nsignificantly improves the correctness, relevance, and completeness of\ngenerated specifications compared to baseline methods, enabling 95.1% of\nassertions and 90.91% of programs to pass verification. Ablation studies\nfurther reveal that logical deletion is critical for enhancing specification\ncorrectness and relevance, while program slicing contributes significantly to\nspecification completeness. Our code and data are publicly available."}
{"id": "2509.09918", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09918", "abs": "https://arxiv.org/abs/2509.09918", "authors": ["Seyed Moein Abtahi", "Akramul Azim"], "title": "WALL: A Web Application for Automated Quality Assurance using Large Language Models", "comment": null, "summary": "As software projects become increasingly complex, the volume and variety of\nissues in code files have grown substantially. Addressing this challenge\nrequires efficient issue detection, resolution, and evaluation tools. This\npaper presents WALL, a web application that integrates SonarQube and large\nlanguage models (LLMs) such as GPT-3.5 Turbo and GPT-4o to automate these\ntasks. WALL comprises three modules: an issue extraction tool, code issues\nreviser, and code comparison tool. Together, they enable a seamless pipeline\nfor detecting software issues, generating automated code revisions, and\nevaluating the accuracy of revisions. Our experiments, conducted on 563 files\nwith over 7,599 issues, demonstrate WALL's effectiveness in reducing human\neffort while maintaining high-quality revisions. Results show that employing a\nhybrid approach of cost-effective and advanced LLMs can significantly lower\ncosts and improve revision rates. Future work aims to enhance WALL's\ncapabilities by integrating open-source LLMs and eliminating human\nintervention, paving the way for fully automated code quality management."}
{"id": "2509.09706", "categories": ["cs.CR", "cs.AI", "cs.CL", "I.2; H.3.3"], "pdf": "https://arxiv.org/pdf/2509.09706", "abs": "https://arxiv.org/abs/2509.09706", "authors": ["Taniya Gidatkar", "Oluwaseun Ajao", "Matthew Shardlow"], "title": "Differential Robustness in Transformer Language Models: Empirical Evaluation Under Adversarial Text Attacks", "comment": "8 pages, 4 tables, to appear in proceedings of Recent Advances in\n  Natural Language Processing (RANLP 2025) and ACL Anthology", "summary": "This study evaluates the resilience of large language models (LLMs) against\nadversarial attacks, specifically focusing on Flan-T5, BERT, and RoBERTa-Base.\nUsing systematically designed adversarial tests through TextFooler and\nBERTAttack, we found significant variations in model robustness. RoBERTa-Base\nand FlanT5 demonstrated remarkable resilience, maintaining accuracy even when\nsubjected to sophisticated attacks, with attack success rates of 0%. In\ncontrast. BERT-Base showed considerable vulnerability, with TextFooler\nachieving a 93.75% success rate in reducing model accuracy from 48% to just 3%.\nOur research reveals that while certain LLMs have developed effective defensive\nmechanisms, these safeguards often require substantial computational resources.\nThis study contributes to the understanding of LLM security by identifying\nexisting strengths and weaknesses in current safeguarding approaches and\nproposes practical recommendations for developing more efficient and effective\ndefensive strategies."}
{"id": "2509.09947", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.09947", "abs": "https://arxiv.org/abs/2509.09947", "authors": ["Humza Ashraf", "Syed Muhammad Danish", "Zeeshan Sattar"], "title": "Toward Green Code: Prompting Small Language Models for Energy-Efficient Code Generation", "comment": null, "summary": "There is a growing concern about the environmental impact of large language\nmodels (LLMs) in software development, particularly due to their high energy\nuse and carbon footprint. Small Language Models (SLMs) offer a more sustainable\nalternative, requiring fewer computational resources while remaining effective\nfor fundamental programming tasks. In this study, we investigate whether prompt\nengineering can improve the energy efficiency of SLMs in code generation. We\nevaluate four open-source SLMs, StableCode-Instruct-3B,\nQwen2.5-Coder-3B-Instruct, CodeLlama-7B-Instruct, and Phi-3-Mini-4K-Instruct,\nacross 150 Python problems from LeetCode, evenly distributed into easy, medium,\nand hard categories. Each model is tested under four prompting strategies: role\nprompting, zero-shot, few-shot, and chain-of-thought (CoT). For every generated\nsolution, we measure runtime, memory usage, and energy consumption, comparing\nthe results with a human-written baseline. Our findings show that CoT prompting\nprovides consistent energy savings for Qwen2.5-Coder and StableCode-3B, while\nCodeLlama-7B and Phi-3-Mini-4K fail to outperform the baseline under any\nprompting strategy. These results highlight that the benefits of prompting are\nmodel-dependent and that carefully designed prompts can guide SLMs toward\ngreener software development."}
{"id": "2509.09787", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09787", "abs": "https://arxiv.org/abs/2509.09787", "authors": ["Nojan Sheybani", "Alessandro Pegoraro", "Jonathan Knauer", "Phillip Rieger", "Elissa Mollakuqe", "Farinaz Koushanfar", "Ahmad-Reza Sadeghi"], "title": "ZORRO: Zero-Knowledge Robustness and Privacy for Split Learning (Full Version)", "comment": "Full version of CCS 2025 paper", "summary": "Split Learning (SL) is a distributed learning approach that enables\nresource-constrained clients to collaboratively train deep neural networks\n(DNNs) by offloading most layers to a central server while keeping in- and\noutput layers on the client-side. This setup enables SL to leverage server\ncomputation capacities without sharing data, making it highly effective in\nresource-constrained environments dealing with sensitive data. However, the\ndistributed nature enables malicious clients to manipulate the training\nprocess. By sending poisoned intermediate gradients, they can inject backdoors\ninto the shared DNN. Existing defenses are limited by often focusing on\nserver-side protection and introducing additional overhead for the server. A\nsignificant challenge for client-side defenses is enforcing malicious clients\nto correctly execute the defense algorithm.\n  We present ZORRO, a private, verifiable, and robust SL defense scheme.\nThrough our novel design and application of interactive zero-knowledge proofs\n(ZKPs), clients prove their correct execution of a client-located defense\nalgorithm, resulting in proofs of computational integrity attesting to the\nbenign nature of locally trained DNN portions. Leveraging the frequency\nrepresentation of model partitions enables ZORRO to conduct an in-depth\ninspection of the locally trained models in an untrusted environment, ensuring\nthat each client forwards a benign checkpoint to its succeeding client. In our\nextensive evaluation, covering different model architectures as well as various\nattack strategies and data scenarios, we show ZORRO's effectiveness, as it\nreduces the attack success rate to less than 6\\% while causing even for models\nstoring \\numprint{1000000} parameters on the client-side an overhead of less\nthan 10 seconds."}
{"id": "2509.09975", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.09975", "abs": "https://arxiv.org/abs/2509.09975", "authors": ["Takasaburo Fukuda", "Takao Nakagawa", "Keisuke Miyazaki", "Susumu Tokumoto"], "title": "Development of Automated Software Design Document Review Methods Using Large Language Models", "comment": "SANER 2025", "summary": "In this study, we explored an approach to automate the review process of\nsoftware design documents by using LLM. We first analyzed the review methods of\ndesign documents and organized 11 review perspectives. Additionally, we\nanalyzed the issues of utilizing LLMs for these 11 review perspectives and\ndetermined which perspectives can be reviewed by current general-purpose LLMs\ninstead of humans. For the reviewable perspectives, we specifically developed\nnew techniques to enable LLMs to comprehend complex design documents that\ninclude table data. For evaluation, we conducted experiments using GPT to\nassess the consistency of design items and descriptions across different design\ndocuments in the design process used in actual business operations. Our results\nconfirmed that LLMs can be utilized to identify inconsistencies in software\ndesign documents during the review process."}
{"id": "2509.09942", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.09942", "abs": "https://arxiv.org/abs/2509.09942", "authors": ["Lei Yu", "Jingyuan Zhang", "Xin Wang", "Jiajia Ma", "Li Yang", "Fengjun Zhang"], "title": "SmartCoder-R1: Towards Secure and Explainable Smart Contract Generation with Security-Aware Group Relative Policy Optimization", "comment": null, "summary": "Smart contracts automate the management of high-value assets, where\nvulnerabilities can lead to catastrophic financial losses. This challenge is\namplified in Large Language Models (LLMs) by two interconnected failures: they\noperate as unauditable \"black boxes\" lacking a transparent reasoning process,\nand consequently, generate code riddled with critical security vulnerabilities.\nTo address both issues, we propose SmartCoder-R1 (based on Qwen2.5-Coder-7B), a\nnovel framework for secure and explainable smart contract generation. It begins\nwith Continual Pre-training (CPT) to specialize the model. We then apply Long\nChain-of-Thought Supervised Fine-Tuning (L-CoT SFT) on 7,998 expert-validated\nreasoning-and-code samples to train the model to emulate human security\nanalysis. Finally, to directly mitigate vulnerabilities, we employ\nSecurity-Aware Group Relative Policy Optimization (S-GRPO), a reinforcement\nlearning phase that refines the generation policy by optimizing a weighted\nreward signal for compilation success, security compliance, and format\ncorrectness. Evaluated against 17 baselines on a benchmark of 756 real-world\nfunctions, SmartCoder-R1 establishes a new state of the art, achieving top\nperformance across five key metrics: a ComPass of 87.70%, a VulRate of 8.60%, a\nSafeAval of 80.16%, a FuncRate of 53.84%, and a FullRate of 50.53%. This\nFullRate marks a 45.79% relative improvement over the strongest baseline,\nDeepSeek-R1. Crucially, its generated reasoning also excels in human\nevaluations, achieving high-quality ratings for Functionality (82.7%), Security\n(85.3%), and Clarity (90.7%)."}
{"id": "2509.10085", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.10085", "abs": "https://arxiv.org/abs/2509.10085", "authors": ["Philipp Zech", "Irdin Pekaric"], "title": "Sustaining Research Software: A Fitness Function Approach", "comment": null, "summary": "The long-term sustainability of research software is a critical challenge, as\nit usually suffers from poor maintainability, lack of adaptability, and\neventual obsolescence. This paper proposes a novel approach to addressing this\nissue by leveraging the concept of fitness functions from evolutionary\narchitecture. Fitness functions are automated, continuously evaluated metrics\ndesigned to ensure that software systems meet desired non-functional,\narchitectural qualities over time. We define a set of fitness functions\ntailored to the unique requirements of research software, focusing on\nfindability, accessibility, interoperability and reusability (FAIR). These\nfitness functions act as proactive safeguards, promoting practices such as\nmodular design, comprehensive documentation, version control, and compatibility\nwith evolving technological ecosystems. By integrating these metrics into the\ndevelopment life cycle, we aim to foster a culture of sustainability within the\nresearch community. Case studies and experimental results demonstrate the\npotential of this approach to enhance the long-term FAIR of research software,\nbridging the gap between ephemeral project-based development and enduring\nscientific impact."}
{"id": "2509.09950", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09950", "abs": "https://arxiv.org/abs/2509.09950", "authors": ["Pouneh Nikkhah Bahrami", "Dylan Cutler", "Igor Bilogrevic"], "title": "Byte by Byte: Unmasking Browser Fingerprinting at the Function Level Using V8 Bytecode Transformers", "comment": null, "summary": "Browser fingerprinting enables persistent cross-site user tracking via subtle\ntechniques that often evade conventional defenses or cause website breakage\nwhen script-level blocking countermeasures are applied. Addressing these\nchallenges requires detection methods offering both function-level precision to\nminimize breakage and inherent robustness against code obfuscation and URL\nmanipulation.\n  We introduce ByteDefender, the first system leveraging V8 engine bytecode to\ndetect fingerprinting operations specifically at the JavaScript function level.\nA Transformer-based classifier, trained offline on bytecode sequences,\naccurately identifies functions exhibiting fingerprinting behavior. We develop\nand evaluate light-weight signatures derived from this model to enable\nlow-overhead, on-device matching against function bytecode during compilation\nbut prior to execution, which only adds a 4% (average) latency to the page load\ntime. This mechanism facilitates targeted, real-time prevention of\nfingerprinting function execution, thereby preserving legitimate script\nfunctionality. Operating directly on bytecode ensures inherent resilience\nagainst common code obfuscation and URL-based evasion. Our evaluation on the\ntop 100k websites demonstrates high detection accuracy at both function- and\nscript-level, with substantial improvements over state-of-the-art AST-based\nmethods, particularly in robustness against obfuscation. ByteDefender offers a\npractical framework for effective, precise, and robust fingerprinting\nmitigation."}
{"id": "2509.10099", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10099", "abs": "https://arxiv.org/abs/2509.10099", "authors": ["Radu Apsan", "Vincenzo Stoico", "Michel Albonico", "Rudra Dhar", "Karthik Vaidhyanathan", "Ivano Malavolta"], "title": "Generating Energy-Efficient Code via Large-Language Models -- Where are we now?", "comment": null, "summary": "Context. The rise of Large Language Models (LLMs) has led to their widespread\nadoption in development pipelines. Goal. We empirically assess the energy\nefficiency of Python code generated by LLMs against human-written code and code\ndeveloped by a Green software expert. Method. We test 363 solutions to 9 coding\nproblems from the EvoEval benchmark using 6 widespread LLMs with 4 prompting\ntechniques, and comparing them to human-developed solutions. Energy consumption\nis measured on three different hardware platforms: a server, a PC, and a\nRaspberry Pi for a total of ~881h (36.7 days). Results. Human solutions are 16%\nmore energy-efficient on the server and 3% on the Raspberry Pi, while LLMs\noutperform human developers by 25% on the PC. Prompting does not consistently\nlead to energy savings, where the most energy-efficient prompts vary by\nhardware platform. The code developed by a Green software expert is\nconsistently more energy-efficient by at least 17% to 30% against all LLMs on\nall hardware platforms. Conclusions. Even though LLMs exhibit relatively good\ncode generation capabilities, no LLM-generated code was more energy-efficient\nthan that of an experienced Green software developer, suggesting that as of\ntoday there is still a great need of human expertise for developing\nenergy-efficient Python code."}
{"id": "2509.09970", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09970", "abs": "https://arxiv.org/abs/2509.09970", "authors": ["Seyed Moein Abtahi", "Akramul Azim"], "title": "Securing LLM-Generated Embedded Firmware through AI Agent-Driven Validation and Patching", "comment": null, "summary": "Large Language Models (LLMs) show promise in generating firmware for embedded\nsystems, but often introduce security flaws and fail to meet real-time\nperformance constraints. This paper proposes a three-phase methodology that\ncombines LLM-based firmware generation with automated security validation and\niterative refinement in a virtualized environment. Using structured prompts,\nmodels like GPT-4 generate firmware for networking and control tasks, deployed\non FreeRTOS via QEMU. These implementations are tested using fuzzing, static\nanalysis, and runtime monitoring to detect vulnerabilities such as buffer\noverflows (CWE-120), race conditions (CWE-362), and denial-of-service threats\n(CWE-400). Specialized AI agents for Threat Detection, Performance\nOptimization, and Compliance Verification collaborate to improve detection and\nremediation. Identified issues are categorized using CWE, then used to prompt\ntargeted LLM-generated patches in an iterative loop. Experiments show a 92.4\\%\nVulnerability Remediation Rate (37.3\\% improvement), 95.8\\% Threat Model\nCompliance, and 0.87 Security Coverage Index. Real-time metrics include 8.6ms\nworst-case execution time and 195{\\mu}s jitter. This process enhances firmware\nsecurity and performance while contributing an open-source dataset for future\nresearch."}
{"id": "2509.10236", "categories": ["cs.SE", "cs.PF", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.10236", "abs": "https://arxiv.org/abs/2509.10236", "authors": ["Mingyi Li", "Junmin Xiao", "Siyan Chen", "Hui Ma", "Xi Chen", "Peihua Bao", "Liang Yuan", "Guangming Tan"], "title": "Stencil-Lifting: Hierarchical Recursive Lifting System for Extracting Summary of Stencil Kernel in Legacy Codes", "comment": "33 pages, 12 figures. Submitted to OOPSLA2'25", "summary": "We introduce Stencil-Lifting, a novel system for automatically converting\nstencil kernels written in low-level languages in legacy code into semantically\nequivalent Domain-Specific Language (DSL) implementations. Targeting the\nefficiency bottlenecks of existing verified lifting systems, Stencil-Lifting\nachieves scalable stencil kernel abstraction through two key innovations.\nFirst, we propose a hierarchical recursive lifting theory that represents\nstencil kernels, structured as nested loops, using invariant subgraphs, which\nare customized data dependency graphs that capture loop-carried computation and\nstructural invariants. Each vertex in the invariant subgraph is associated with\na predicate-based summary, encoding its computational semantics. By enforcing\nself-consistency across these summaries, Stencil-Lifting ensures the derivation\nof correct loop invariants and postconditions for nested loops, eliminating the\nneed for external verification. Second, we develop a hierarchical recursive\nlifting algorithm that guarantees termination through a convergent recursive\nprocess, avoiding the inefficiencies of search-based synthesis. The algorithm\nefficiently derives the valid summaries of stencil kernels, and its\ncompleteness is formally proven. We evaluate Stencil-Lifting on diverse stencil\nbenchmarks from two different suites and on four real-world applications.\nExperimental results demonstrate that Stencil-Lifting achieves 31.62$\\times$\nand 5.8$\\times$ speedups compared to the state-of-the-art verified lifting\nsystems STNG and Dexter, respectively, while maintaining full semantic\nequivalence. Our work significantly enhances the translation efficiency of\nlow-level stencil kernels to DSL implementations, effectively bridging the gap\nbetween legacy optimization techniques and modern DSL-based paradigms."}
{"id": "2509.09989", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09989", "abs": "https://arxiv.org/abs/2509.09989", "authors": ["Priyanka Rushikesh Chaudhary", "Manan Gupta", "Jabez Christopher", "Putrevu Venkata Sai Charan", "Rajib Ranjan Maiti"], "title": "rCamInspector: Building Reliability and Trust on IoT (Spy) Camera Detection using XAI", "comment": null, "summary": "The classification of network traffic using machine learning (ML) models is\none of the primary mechanisms to address the security issues in IoT networks\nand/or IoT devices. However, the ML models often act as black-boxes that create\na roadblock to take critical decision based on the model output. To address\nthis problem, we design and develop a system, called rCamInspector, that\nemploys Explainable AI (XAI) to provide reliable and trustworthy explanations\nto model output. rCamInspector adopts two classifiers, Flow Classifier -\ncategorizes a flow into one of four classes, IoTCam, Conf, Share and Others,\nand SmartCam Classifier - classifies an IoTCam flow into one of six classes,\nNetatmo, Spy Clock, Canary, D3D, Ezviz, V380 Spy Bulb; both are IP address and\ntransport port agnostic. rCamInspector is evaluated using 38GB of network\ntraffic and our results show that XGB achieves the highest accuracy of 92% and\n99% in the Flow and SmartCam classifiers respectively among eight supervised ML\nmodels. We analytically show that the traditional mutual information (MI) based\nfeature importance cannot provide enough reliability on the model output of XGB\nin either classifiers. Using SHAP and LIME, we show that a separate set of\nfeatures can be picked up to explain a correct prediction of XGB. For example,\nthe feature Init Bwd Win Byts turns out to have the highest SHAP values to\nsupport the correct prediction of both IoTCam in Flow Classifier and Netatmo\nclass in SmartCam Classifier. To evaluate the faithfulness of the explainers on\nour dataset, we show that both SHAP and LIME have a consistency of more than\n0.7 and a sufficiency of 1.0. Comparing with existing works, we show that\nrCamInspector achieves a better accuracy (99%), precision (99%), and false\nnegative rate (0.7%)."}
{"id": "2509.10279", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10279", "abs": "https://arxiv.org/abs/2509.10279", "authors": ["Pavel Plyusnin", "Aleksey Antonov", "Vasilii Ermakov", "Aleksandr Khaybriev", "Margarita Kikot", "Ilseyar Alimova", "Stanislav Moiseev"], "title": "Targeted Test Selection Approach in Continuous Integration", "comment": "Accepted at ICSME 2025", "summary": "In modern software development change-based testing plays a crucial role.\nHowever, as codebases expand and test suites grow, efficiently managing the\ntesting process becomes increasingly challenging, especially given the high\nfrequency of daily code commits. We propose Targeted Test Selection (T-TS), a\nmachine learning approach for industrial test selection. Our key innovation is\na data representation that represent commits as Bags-of-Words of changed files,\nincorporates cross-file and additional predictive features, and notably avoids\nthe use of coverage maps. Deployed in production, T-TS was comprehensively\nevaluated against industry standards and recent methods using both internal and\npublic datasets, measuring time efficiency and fault detection. On live\nindustrial data, T-TS selects only 15% of tests, reduces execution time by\n$5.9\\times$, accelerates the pipeline by $5.6\\times$, and detects over 95% of\ntest failures. The implementation is publicly available to support further\nresearch and practical adoption."}
{"id": "2509.10165", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.10165", "abs": "https://arxiv.org/abs/2509.10165", "authors": ["Matthew J. Schneider", "James Bailie", "Dawn Iacobucci"], "title": "Why Data Anonymization Has Not Taken Off", "comment": "15 pages", "summary": "Companies are looking to data anonymization research $\\unicode{x2013}$\nincluding differential private and synthetic data methods $\\unicode{x2013}$ for\nsimple and straightforward compliance solutions. But data anonymization has not\ntaken off in practice because it is anything but simple to implement. For one,\nit requires making complex choices which are case dependent, such as the domain\nof the dataset to anonymize; the units to protect; the scope where the data\nprotection should extend to; and the standard of protection. Each variation of\nthese choices changes the very meaning, as well as the practical implications,\nof differential privacy (or of any other measure of data anonymization). Yet\ndifferential privacy is frequently being branded as the same privacy guarantee\nregardless of variations in these choices. Some data anonymization methods can\nbe effective, but only when the insights required are much larger than the unit\nof protection. Given that businesses care about profitability, any solution\nmust preserve the patterns between a firm's data and that profitability. As a\nresult, data anonymization solutions usually need to be bespoke and\ncase-specific, which reduces their scalability. Companies should not expect\neasy wins, but rather recognize that anonymization is just one approach to data\nprivacy with its own particular advantages and drawbacks, while the best\nstrategies jointly leverage the full range of approaches to data privacy and\nsecurity in combination."}
{"id": "2509.10402", "categories": ["cs.SE", "D.2.0; D.2.7"], "pdf": "https://arxiv.org/pdf/2509.10402", "abs": "https://arxiv.org/abs/2509.10402", "authors": ["Suzhen Zhong", "Ying Zou", "Bram Adams"], "title": "Developer-LLM Conversations: An Empirical Study of Interactions and Generated Code Quality", "comment": null, "summary": "Large Language Models (LLMs) are becoming integral to modern software\ndevelopment workflows, assisting developers with code generation, API\nexplanation, and iterative problem-solving through natural language\nconversations. Despite widespread adoption, there is limited understanding of\nhow developers interact with LLMs in practice and how these conversational\ndynamics influence task outcomes, code quality, and software engineering\nworkflows. To address this, we leverage CodeChat, a large dataset comprising\n82,845 real-world developer-LLM conversations, containing 368,506 code snippets\ngenerated across over 20 programming languages, derived from the WildChat\ndataset. We find that LLM responses are substantially longer than developer\nprompts, with a median token-length ratio of 14:1. Multi-turn conversations\naccount for 68% of the dataset and often evolve due to shifting requirements,\nincomplete prompts, or clarification requests. Topic analysis identifies web\ndesign (9.6% of conversations) and neural network training (8.7% of\nconversations) as the most frequent LLM-assisted tasks. Evaluation across five\nlanguages (i.e., Python, JavaScript, C++, Java, and C#) reveals prevalent and\nlanguage-specific issues in LLM-generated code: generated Python and JavaScript\ncode often include undefined variables (83.4% and 75.3% of code snippets,\nrespectively); Java code lacks required comments (75.9%); C++ code frequently\nomits headers (41.1%) and C# code shows unresolved namespaces (49.2%). During a\nconversation, syntax and import errors persist across turns; however,\ndocumentation quality in Java improves by up to 14.7%, and import handling in\nPython improves by 3.7% over 5 turns. Prompts that point out mistakes in code\ngenerated in prior turns and explicitly request a fix are most effective for\nresolving errors."}
{"id": "2509.10206", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10206", "abs": "https://arxiv.org/abs/2509.10206", "authors": ["Federica Uccello", "Simin Nadjm-Tehrani"], "title": "Investigating Feature Attribution for 5G Network Intrusion Detection", "comment": null, "summary": "With the rise of fifth-generation (5G) networks in critical applications, it\nis urgent to move from detection of malicious activity to systems capable of\nproviding a reliable verdict suitable for mitigation. In this regard,\nunderstanding and interpreting machine learning (ML) models' security alerts is\ncrucial for enabling actionable incident response orchestration. Explainable\nArtificial Intelligence (XAI) techniques are expected to enhance trust by\nproviding insights into why alerts are raised. A dominant approach\nstatistically associates feature sets that can be correlated to a given alert.\nThis paper starts by questioning whether such attribution is relevant for\nfuture generation communication systems, and investigates its merits in\ncomparison with an approach based on logical explanations. We extensively study\ntwo methods, SHAP and VoTE-XAI, by analyzing their interpretations of alerts\ngenerated by an XGBoost model in three different use cases with several 5G\ncommunication attacks. We identify three metrics for assessing explanations:\nsparsity, how concise they are; stability, how consistent they are across\nsamples from the same attack type; and efficiency, how fast an explanation is\ngenerated. As an example, in a 5G network with 92 features, 6 were deemed\nimportant by VoTE-XAI for a Denial of Service (DoS) variant, ICMPFlood, while\nSHAP identified over 20. More importantly, we found a significant divergence\nbetween features selected by SHAP and VoTE-XAI. However, none of the top-ranked\nfeatures selected by SHAP were missed by VoTE-XAI. When it comes to efficiency\nof providing interpretations, we found that VoTE-XAI is significantly more\nresponsive, e.g. it provides a single explanation in under 0.002 seconds, in a\nhigh-dimensional setting (478 features)."}
{"id": "2509.09942", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.09942", "abs": "https://arxiv.org/abs/2509.09942", "authors": ["Lei Yu", "Jingyuan Zhang", "Xin Wang", "Jiajia Ma", "Li Yang", "Fengjun Zhang"], "title": "SmartCoder-R1: Towards Secure and Explainable Smart Contract Generation with Security-Aware Group Relative Policy Optimization", "comment": null, "summary": "Smart contracts automate the management of high-value assets, where\nvulnerabilities can lead to catastrophic financial losses. This challenge is\namplified in Large Language Models (LLMs) by two interconnected failures: they\noperate as unauditable \"black boxes\" lacking a transparent reasoning process,\nand consequently, generate code riddled with critical security vulnerabilities.\nTo address both issues, we propose SmartCoder-R1 (based on Qwen2.5-Coder-7B), a\nnovel framework for secure and explainable smart contract generation. It begins\nwith Continual Pre-training (CPT) to specialize the model. We then apply Long\nChain-of-Thought Supervised Fine-Tuning (L-CoT SFT) on 7,998 expert-validated\nreasoning-and-code samples to train the model to emulate human security\nanalysis. Finally, to directly mitigate vulnerabilities, we employ\nSecurity-Aware Group Relative Policy Optimization (S-GRPO), a reinforcement\nlearning phase that refines the generation policy by optimizing a weighted\nreward signal for compilation success, security compliance, and format\ncorrectness. Evaluated against 17 baselines on a benchmark of 756 real-world\nfunctions, SmartCoder-R1 establishes a new state of the art, achieving top\nperformance across five key metrics: a ComPass of 87.70%, a VulRate of 8.60%, a\nSafeAval of 80.16%, a FuncRate of 53.84%, and a FullRate of 50.53%. This\nFullRate marks a 45.79% relative improvement over the strongest baseline,\nDeepSeek-R1. Crucially, its generated reasoning also excels in human\nevaluations, achieving high-quality ratings for Functionality (82.7%), Security\n(85.3%), and Clarity (90.7%)."}
{"id": "2509.10213", "categories": ["cs.CR", "D.4.6; K.6.5"], "pdf": "https://arxiv.org/pdf/2509.10213", "abs": "https://arxiv.org/abs/2509.10213", "authors": ["Ming Zhou", "Xupu Hu", "Zhihao Wang", "Haining Wang", "Hui Wen", "Limin Sun", "Peng Zhang"], "title": "Dynamic Vulnerability Patching for Heterogeneous Embedded Systems Using Stack Frame Reconstruction", "comment": "Accepted/To be published in ACM CCS 2025", "summary": "Existing dynamic vulnerability patching techniques are not well-suited for\nembedded devices, especially mission-critical ones such as medical equipment,\nas they have limited computational power and memory but uninterrupted service\nrequirements. Those devices often lack sufficient idle memory for dynamic\npatching, and the diverse architectures of embedded systems further complicate\nthe creation of patch triggers that are compatible across various system\nkernels and hardware platforms. To address these challenges, we propose a hot\npatching framework called StackPatch that facilitates patch development based\non stack frame reconstruction. StackPatch introduces different triggering\nstrategies to update programs stored in memory units. We leverage the\nexception-handling mechanisms commonly available in embedded processors to\nenhance StackPatch's adaptability across different processor architectures for\ncontrol flow redirection. We evaluated StackPatch on embedded devices featuring\nthree major microcontroller (MCU) architectures: ARM, RISC-V, and Xtensa. In\nthe experiments, we used StackPatch to successfully fix 102 publicly disclosed\nvulnerabilities in real-time operating systems (RTOS). We applied patching to\nmedical devices, soft programmable logic controllers (PLCs), and network\nservices, with StackPatch consistently completing each vulnerability\nremediation in less than 260 MCU clock cycles."}
{"id": "2509.10320", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.10320", "abs": "https://arxiv.org/abs/2509.10320", "authors": ["Davide Corradini", "Mariano Ceccato", "Mohammad Ghafari"], "title": "Automated Testing of Broken Authentication Vulnerabilities in Web APIs with AuthREST", "comment": null, "summary": "We present AuthREST, an open-source security testing tool targeting broken\nauthentication, one of the most prevalent API security risks in the wild.\nAuthREST automatically tests web APIs for credential stuffing, password brute\nforcing, and unchecked token authenticity. Empirical results show that AuthREST\nis effective in improving web API security. Notably, it uncovered previously\nunknown authentication vulnerabilitiesin in four public APIs."}
{"id": "2509.10224", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.10224", "abs": "https://arxiv.org/abs/2509.10224", "authors": ["Reynaldo Gil-Pons", "Sjouke Mauw", "Rolando Trujillo-Rasua"], "title": "Empirical Evaluation of Memory-Erasure Protocols", "comment": "Published at SECRYPT 2025", "summary": "Software-based memory-erasure protocols are two-party communication protocols\nwhere a verifier instructs a computational device to erase its memory and send\na proof of erasure. They aim at guaranteeing that low-cost IoT devices are free\nof malware by putting them back into a safe state without requiring secure\nhardware or physical manipulation of the device. Several software-based\nmemory-erasure protocols have been introduced and theoretically analysed. Yet,\nmany of them have not been tested for their feasibility, performance and\nsecurity on real devices, which hinders their industry adoption. This article\nreports on the first empirical analysis of software-based memory-erasure\nprotocols with respect to their security, erasure guarantees, and performance.\nThe experimental setup consists of 3 modern IoT devices with different\ncomputational capabilities, 7 protocols, 6 hash-function implementations, and\nvarious performance and security criteria. Our results indicate that existing\nsoftware-based memory-erasure protocols are feasible, although slow devices may\ntake several seconds to erase their memory and generate a proof of erasure. We\nfound that no protocol dominates across all empirical settings, defined by the\ncomputational power and memory size of the device, the network speed, and the\nrequired level of security. Interestingly, network speed and hidden constants\nwithin the protocol specification played a more prominent role in the\nperformance of these protocols than anticipated based on the related\nliterature. We provide an evaluation framework that, given a desired level of\nsecurity, determines which protocols offer the best trade-off between\nperformance and erasure guarantees."}
{"id": "2509.10413", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.10413", "abs": "https://arxiv.org/abs/2509.10413", "authors": ["Guojun Tang", "Carylyne Chan", "Ning Nan", "Spencer Yang", "Jiayu Zhou", "Henry Leung", "Mohammad Mamun", "Steve Drew"], "title": "Bitcoin Cross-Chain Bridge: A Taxonomy and Its Promise in Artificial Intelligence of Things", "comment": "Blockchain Cross-Chain Bridge Survey", "summary": "Bitcoin's limited scripting capabilities and lack of native interoperability\nmechanisms have constrained its integration into the broader blockchain\necosystem, especially decentralized finance (DeFi) and multi-chain\napplications. This paper presents a comprehensive taxonomy of Bitcoin\ncross-chain bridge protocols, systematically analyzing their trust assumptions,\nperformance characteristics, and applicability to the Artificial Intelligence\nof Things (AIoT) scenarios. We categorize bridge designs into three main types:\nnaive token swapping, pegged-asset bridges, and arbitrary-message bridges. Each\ncategory is evaluated across key metrics such as trust model, latency, capital\nefficiency, and DeFi composability. Emerging innovations like BitVM and\nrecursive sidechains are highlighted for their potential to enable secure,\nscalable, and programmable Bitcoin interoperability. Furthermore, we explore\npractical use cases of cross-chain bridges in AIoT applications, including\ndecentralized energy trading, healthcare data integration, and supply chain\nautomation. This taxonomy provides a foundational framework for researchers and\npractitioners seeking to design secure and efficient cross-chain\ninfrastructures in AIoT systems."}
{"id": "2509.10252", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.10252", "abs": "https://arxiv.org/abs/2509.10252", "authors": ["Yifan Jia", "Ye Tian", "Yanbin Wang", "Jianguo Sun", "Haitao Xu"], "title": "ExDoS: Expert-Guided Dual-Focus Cross-Modal Distillation for Smart Contract Vulnerability Detection", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "The success of smart contracts has made them a target for attacks, but their\nclosed-source nature often forces vulnerability detection to work on bytecode,\nwhich is inherently more challenging than source-code-based analysis. While\nrecent studies try to align source and bytecode embeddings during training to\ntransfer knowledge, current methods rely on graph-level alignment that obscures\nfine-grained structural and semantic correlations between the two modalities.\nMoreover, the absence of precise vulnerability patterns and granular\nannotations in bytecode leads to depriving the model of crucial supervisory\nsignals for learning discriminant features. We propose ExDoS to transfer rich\nsemantic knowledge from source code to bytecode, effectively supplementing the\nsource code prior in practical settings. Specifically, we construct semantic\ngraphs from source code and control-flow graphs from bytecode. To address\nobscured local signals in graph-level contract embeddings, we propose a\nDual-Attention Graph Network introducing a novel node attention aggregation\nmodule to enhance local pattern capture in graph embeddings. Furthermore, by\nsummarizing existing source code vulnerability patterns and designing a\ncorresponding set of bytecode-level patterns for each, we construct the first\ndataset of vulnerability pattern annotations aligned with source code\ndefinitions to facilitate fine-grained cross-modal alignment and the capture of\nfunction-level vulnerability signals. Finally, we propose a dual-focus\nobjective for our cross-modal distillation framework, comprising: a Global\nSemantic Distillation Loss for transferring graph-level knowledge and a Local\nSemantic Distillation Loss enabling expert-guided, fine-grained\nvulnerability-specific distillation. Experiments on real-world contracts\ndemonstrate that our method achieves consistent F1-score improvements\n(3\\%--6\\%) over strong baselines."}
{"id": "2509.10287", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.10287", "abs": "https://arxiv.org/abs/2509.10287", "authors": ["Ye Tian", "Yifan Jia", "Yanbin Wang", "Jianguo Sun", "Zhiquan Liu", "Xiaowen Ling"], "title": "URL2Graph++: Unified Semantic-Structural-Character Learning for Malicious URL Detection", "comment": null, "summary": "Malicious URL detection remains a major challenge in cybersecurity, primarily\ndue to two factors: (1) the exponential growth of the Internet has led to an\nimmense diversity of URLs, making generalized detection increasingly difficult;\nand (2) attackers are increasingly employing sophisticated obfuscation\ntechniques to evade detection. We advocate that addressing these challenges\nfundamentally requires: (1) obtaining semantic understanding to improve\ngeneralization across vast and diverse URL sets, and (2) accurately modeling\ncontextual relationships within the structural composition of URLs. In this\npaper, we propose a novel malicious URL detection method combining\nmulti-granularity graph learning with semantic embedding to jointly capture\nsemantic, character-level, and structural features for robust URL analysis. To\nmodel internal dependencies within URLs, we first construct dual-granularity\nURL graphs at both subword and character levels, where nodes represent URL\ntokens/characters and edges encode co-occurrence relationships. To obtain\nfine-grained embeddings, we initialize node representations using a\ncharacter-level convolutional network. The two graphs are then processed\nthrough jointly trained Graph Convolutional Networks to learn consistent\ngraph-level representations, enabling the model to capture complementary\nstructural features that reflect co-occurrence patterns and character-level\ndependencies. Furthermore, we employ BERT to derive semantic representations of\nURLs for semantically aware understanding. Finally, we introduce a gated\ndynamic fusion network to combine the semantically enriched BERT\nrepresentations with the jointly optimized graph vectors, further enhancing\ndetection performance. We extensively evaluate our method across multiple\nchallenging dimensions. Results show our method exceeds SOTA performance,\nincluding against large language models."}
{"id": "2509.10313", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.10313", "abs": "https://arxiv.org/abs/2509.10313", "authors": ["Hamish Alsop", "Leandros Maglaras", "Helge Janicke", "Iqbal H. Sarker", "Mohamed Amine Ferrag"], "title": "Innovating Augmented Reality Security: Recent E2E Encryption Approaches", "comment": null, "summary": "End-to-end encryption (E2EE) has emerged as a fundamental element of modern\ndigital communication, protecting data from unauthorized access during\ntransmission. By design, E2EE ensures that only the intended recipient can\ndecrypt the information, making it inaccessible even to service providers. Yet,\nthis powerful safeguard of individual privacy and digital trust also introduces\na paradox: it can simultaneously prevent law enforcement efforts by hiding\npotential malicious activities. This paper examines the dual role of E2EE, its\ncritical importance to privacy, the challenges it"}
{"id": "2509.10320", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.10320", "abs": "https://arxiv.org/abs/2509.10320", "authors": ["Davide Corradini", "Mariano Ceccato", "Mohammad Ghafari"], "title": "Automated Testing of Broken Authentication Vulnerabilities in Web APIs with AuthREST", "comment": null, "summary": "We present AuthREST, an open-source security testing tool targeting broken\nauthentication, one of the most prevalent API security risks in the wild.\nAuthREST automatically tests web APIs for credential stuffing, password brute\nforcing, and unchecked token authenticity. Empirical results show that AuthREST\nis effective in improving web API security. Notably, it uncovered previously\nunknown authentication vulnerabilitiesin in four public APIs."}
{"id": "2509.10413", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.10413", "abs": "https://arxiv.org/abs/2509.10413", "authors": ["Guojun Tang", "Carylyne Chan", "Ning Nan", "Spencer Yang", "Jiayu Zhou", "Henry Leung", "Mohammad Mamun", "Steve Drew"], "title": "Bitcoin Cross-Chain Bridge: A Taxonomy and Its Promise in Artificial Intelligence of Things", "comment": "Blockchain Cross-Chain Bridge Survey", "summary": "Bitcoin's limited scripting capabilities and lack of native interoperability\nmechanisms have constrained its integration into the broader blockchain\necosystem, especially decentralized finance (DeFi) and multi-chain\napplications. This paper presents a comprehensive taxonomy of Bitcoin\ncross-chain bridge protocols, systematically analyzing their trust assumptions,\nperformance characteristics, and applicability to the Artificial Intelligence\nof Things (AIoT) scenarios. We categorize bridge designs into three main types:\nnaive token swapping, pegged-asset bridges, and arbitrary-message bridges. Each\ncategory is evaluated across key metrics such as trust model, latency, capital\nefficiency, and DeFi composability. Emerging innovations like BitVM and\nrecursive sidechains are highlighted for their potential to enable secure,\nscalable, and programmable Bitcoin interoperability. Furthermore, we explore\npractical use cases of cross-chain bridges in AIoT applications, including\ndecentralized energy trading, healthcare data integration, and supply chain\nautomation. This taxonomy provides a foundational framework for researchers and\npractitioners seeking to design secure and efficient cross-chain\ninfrastructures in AIoT systems."}
