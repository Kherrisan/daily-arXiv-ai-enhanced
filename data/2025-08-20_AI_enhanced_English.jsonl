{"id": "2508.13214", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13214", "abs": "https://arxiv.org/abs/2508.13214", "authors": ["Xuyang Guo", "Zekai Huang", "Zhao Song", "Jiahao Zhang"], "title": "Too Easily Fooled? Prompt Injection Breaks LLMs on Frustratingly Simple Multiple-Choice Questions", "comment": null, "summary": "Large Language Models (LLMs) have recently demonstrated strong emergent\nabilities in complex reasoning and zero-shot generalization, showing\nunprecedented potential for LLM-as-a-judge applications in education, peer\nreview, and data quality evaluation. However, their robustness under prompt\ninjection attacks, where malicious instructions are embedded into the content\nto manipulate outputs, remains a significant concern. In this work, we explore\na frustratingly simple yet effective attack setting to test whether LLMs can be\neasily misled. Specifically, we evaluate LLMs on basic arithmetic questions\n(e.g., \"What is 3 + 2?\") presented as either multiple-choice or true-false\njudgment problems within PDF files, where hidden prompts are injected into the\nfile. Our results reveal that LLMs are indeed vulnerable to such hidden prompt\ninjection attacks, even in these trivial scenarios, highlighting serious\nrobustness risks for LLM-as-a-judge applications.", "AI": {"tldr": "This paper investigates the vulnerability of Large Language Models (LLMs) to hidden prompt injection attacks in LLM-as-a-judge applications, demonstrating that even simple tasks like arithmetic questions can be manipulated through maliciously embedded instructions in PDFs.", "motivation": "LLMs are increasingly used as evaluators in critical domains (e.g., education, peer review), but their susceptibility to prompt injection attacks threatens their reliability and security in these roles.", "method": "The authors design a minimalistic attack by embedding malicious instructions into PDF files containing basic arithmetic questions (multiple-choice or true-false formats) to assess LLMs' robustness against adversarial content.", "result": "Experiments reveal that LLMs are significantly vulnerable to hidden prompt injection attacks, failing to resist manipulation even for trivial arithmetic problems presented in document files.", "conclusion": "The findings expose serious robustness gaps in LLM-as-a-judge systems, emphasizing the need for defensive mechanisms to prevent adversarial interference in practical deployment scenarios."}}
{"id": "2508.13220", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13220", "abs": "https://arxiv.org/abs/2508.13220", "authors": ["Yixuan Yang", "Daoyuan Wu", "Yufan Chen"], "title": "MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols", "comment": "This is a technical report from Lingnan University, Hong Kong", "summary": "Large Language Models (LLMs) are increasingly integrated into real-world\napplications via the Model Context Protocol (MCP), a universal, open standard\nfor connecting AI agents with data sources and external tools. While MCP\nenhances the capabilities of LLM-based agents, it also introduces new security\nrisks and expands their attack surfaces. In this paper, we present the first\nsystematic taxonomy of MCP security, identifying 17 attack types across 4\nprimary attack surfaces. We introduce MCPSecBench, a comprehensive security\nbenchmark and playground that integrates prompt datasets, MCP servers, MCP\nclients, and attack scripts to evaluate these attacks across three major MCP\nproviders. Our benchmark is modular and extensible, allowing researchers to\nincorporate custom implementations of clients, servers, and transport protocols\nfor systematic security assessment. Experimental results show that over 85% of\nthe identified attacks successfully compromise at least one platform, with core\nvulnerabilities universally affecting Claude, OpenAI, and Cursor, while\nprompt-based and tool-centric attacks exhibit considerable variability across\ndifferent hosts and models. Overall, MCPSecBench standardizes the evaluation of\nMCP security and enables rigorous testing across all MCP layers.", "AI": {"tldr": "The paper introduces MCPSecBench, a security benchmark for evaluating Model Context Protocol (MCP) vulnerabilities in LLM-based systems, identifying 17 attack types across 4 surfaces and demonstrating platform-specific risks.", "motivation": "LLMs are increasingly used in real-world applications via MCP, which introduces new security risks and attack surfaces requiring systematic evaluation.", "method": "Systematic taxonomy of 17 MCP attack types across 4 surfaces; development of MCPSecBench with integrated prompt datasets, MCP servers/clients, and attack scripts for modular, extensible security assessment across three major providers.", "result": "85% of attacks succeed in compromising at least one platform; core vulnerabilities affect all providers (Claude, OpenAI, Cursor), while prompt-based and tool-centric attacks vary significantly across hosts and models.", "conclusion": "MCPSecBench standardizes MCP security evaluation, enabling rigorous cross-layer testing and exposing platform-specific security risks in LLM integration workflows."}}
{"id": "2508.13240", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13240", "abs": "https://arxiv.org/abs/2508.13240", "authors": ["Soham Hans", "Nikolos Gurney", "Stacy Marsella", "Sofia Hirschmann"], "title": "Quantifying Loss Aversion in Cyber Adversaries via LLM Analysis", "comment": null, "summary": "Understanding and quantifying human cognitive biases from empirical data has\nlong posed a formidable challenge, particularly in cybersecurity, where\ndefending against unknown adversaries is paramount. Traditional cyber defense\nstrategies have largely focused on fortification, while some approaches attempt\nto anticipate attacker strategies by mapping them to cognitive vulnerabilities,\nyet they fall short in dynamically interpreting attacks in progress. In\nrecognition of this gap, IARPA's ReSCIND program seeks to infer, defend\nagainst, and even exploit attacker cognitive traits. In this paper, we present\na novel methodology that leverages large language models (LLMs) to extract\nquantifiable insights into the cognitive bias of loss aversion from hacker\nbehavior. Our data are collected from an experiment in which hackers were\nrecruited to attack a controlled demonstration network. We process the hacker\ngenerated notes using LLMs using it to segment the various actions and\ncorrelate the actions to predefined persistence mechanisms used by hackers. By\ncorrelating the implementation of these mechanisms with various operational\ntriggers, our analysis provides new insights into how loss aversion manifests\nin hacker decision-making. The results demonstrate that LLMs can effectively\ndissect and interpret nuanced behavioral patterns, thereby offering a\ntransformative approach to enhancing cyber defense strategies through\nreal-time, behavior-based analysis.", "AI": {"tldr": "This paper proposes a methodology using large language models (LLMs) to analyze hacker behavior and infer cognitive biases, particularly loss aversion, enabling real-time cyber defense improvements.", "motivation": "Traditional cyber defense strategies lack dynamic interpretation of attacker behavior during ongoing attacks, creating a critical need to understand cognitive biases like loss aversion in real-time.", "method": "The approach involves processing hacker-generated notes with LLMs to segment actions and correlate them with predefined persistence mechanisms, analyzing how loss aversion manifests through operational triggers in attack scenarios.", "result": "Experimental results demonstrate that LLMs effectively dissect nuanced behavioral patterns, revealing quantifiable insights into loss aversion in hackers and offering a new capability for real-time behavioral analysis.", "conclusion": "This methodology transforms cyber defense by leveraging LLMs to analyze cognitive traits during attacks, enabling adaptive strategies that can infer, defend against, and potentially exploit hacker biases."}}
{"id": "2508.13246", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13246", "abs": "https://arxiv.org/abs/2508.13246", "authors": ["Yangyang Guo", "Yangyan Li", "Mohan Kankanhalli"], "title": "Involuntary Jailbreak", "comment": "We plan to temporarily restrict access to the github code due to\n  potential risks of malicious use. But in the meantime, you can try using the\n  prompt, provided it hasn't been banned", "summary": "In this study, we disclose a worrying new vulnerability in Large Language\nModels (LLMs), which we term \\textbf{involuntary jailbreak}. Unlike existing\njailbreak attacks, this weakness is distinct in that it does not involve a\nspecific attack objective, such as generating instructions for \\textit{building\na bomb}. Prior attack methods predominantly target localized components of the\nLLM guardrail. In contrast, involuntary jailbreaks may potentially compromise\nthe entire guardrail structure, which our method reveals to be surprisingly\nfragile. We merely employ a single universal prompt to achieve this goal. In\nparticular, we instruct LLMs to generate several questions that would typically\nbe rejected, along with their corresponding in-depth responses (rather than a\nrefusal). Remarkably, this simple prompt strategy consistently jailbreaks the\nmajority of leading LLMs, including Claude Opus 4.1, Grok 4, Gemini 2.5 Pro,\nand GPT 4.1. We hope this problem can motivate researchers and practitioners to\nre-evaluate the robustness of LLM guardrails and contribute to stronger safety\nalignment in future.", "AI": {"tldr": "This paper introduces 'involuntary jailbreak', a new vulnerability in LLMs that bypasses guardrails using a simple universal prompt strategy.", "motivation": "The study highlights the surprising fragility of current LLM guardrail mechanisms against non-objective-specific attacks.", "method": "A universal prompt instructs LLMs to generate rejected questions and detailed responses, revealing vulnerabilities without direct attack objectives.", "result": "The method successfully jailbreaks multiple leading LLMs (Claude Opus 4.1, Grok 4, Gemini 2.5 Pro, GPT 4.1) by compromising their entire guardrail structure.", "conclusion": "The work urges re-evaluation of LLM guardrail robustness to achieve stronger safety alignment in future systems."}}
{"id": "2508.13396", "categories": ["cs.SE", "68P20 (Primary), 68M14 (Secondary)"], "pdf": "https://arxiv.org/pdf/2508.13396", "abs": "https://arxiv.org/abs/2508.13396", "authors": ["Dinesh Eswararaj", "Ajay Babu Nellipudi", "Vandana Kollati"], "title": "A Comparative Study of Delta Parquet, Iceberg, and Hudi for Automotive Data Engineering Use Cases", "comment": "Published in SSRG International Journal of Computer Science and\n  Engineering (IJCSE), July 2025. This is the authors accepted manuscript. The\n  final published version is available", "summary": "The automotive industry generates vast amounts of data from sensors,\ntelemetry, diagnostics, and real-time operations. Efficient data engineering is\ncritical to handle challenges of latency, scalability, and consistency. Modern\ndata lakehouse formats Delta Parquet, Apache Iceberg, and Apache Hudi offer\nfeatures such as ACID transactions, schema enforcement, and real-time\ningestion, combining the strengths of data lakes and warehouses to support\ncomplex use cases. This study presents a comparative analysis of Delta Parquet,\nIceberg, and Hudi using real-world time-series automotive telemetry data with\nfields such as vehicle ID, timestamp, location, and event metrics. The\nevaluation considers modeling strategies, partitioning, CDC support, query\nperformance, scalability, data consistency, and ecosystem maturity. Key\nfindings show Delta Parquet provides strong ML readiness and governance,\nIceberg delivers high performance for batch analytics and cloud-native\nworkloads, while Hudi is optimized for real-time ingestion and incremental\nprocessing. Each format exhibits tradeoffs in query efficiency, time-travel,\nand update semantics. The study offers insights for selecting or combining\nformats to support fleet management, predictive maintenance, and route\noptimization. Using structured datasets and realistic queries, the results\nprovide practical guidance for scaling data pipelines and integrating machine\nlearning models in automotive applications.", "AI": {"tldr": "This paper compares Delta Parquet, Apache Iceberg, and Apache Hudi for automotive telemetry data engineering, analyzing their performance in ML readiness, real-time ingestion, scalability, and other critical factors to guide format selection.", "motivation": "The automotive industry faces data engineering challenges with large-scale telemetry data requiring low latency, scalability, and consistency. Choosing the right data format is essential for managing use cases like fleet management and predictive maintenance.", "method": "The study evaluated the three formats using real-world time-series automotive telemetry data (vehicle ID, timestamp, location, event metrics) across modeling strategies, partitioning, CDC support, query performance, scalability, consistency, and ecosystem maturity.", "result": "Delta Parquet excels in ML readiness and governance; Iceberg shows high performance for batch analytics and cloud-native workloads; Hudi is strongest for real-time ingestion and incremental processing. Tradeoffs exist in query efficiency, time-travel, and update semantics.", "conclusion": "The analysis provides actionable insights for selecting or combining Delta, Iceberg, and Hudi to optimize automotive data pipelines for applications like predictive maintenance, route optimization, and fleet analytics, balancing performance, scalability, and functional requirements."}}
{"id": "2508.13357", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.13357", "abs": "https://arxiv.org/abs/2508.13357", "authors": ["Zhuoran Li", "Hanieh Totonchi Asl", "Ebrahim Nouri", "Yifei Cai", "Danella Zhao"], "title": "Silentflow: Leveraging Trusted Execution for Resource-Limited MPC via Hardware-Algorithm Co-design", "comment": null, "summary": "Secure Multi-Party Computation (MPC) offers a practical foundation for\nprivacy-preserving machine learning at the edge, with MPC commonly employed to\nsupport nonlinear operations. These MPC protocols fundamentally rely on\nOblivious Transfer (OT), particularly Correlated OT (COT), to generate\ncorrelated randomness essential for secure computation. Although COT generation\nis efficient in conventional two-party settings with resource-rich\nparticipants, it becomes a critical bottleneck in real-world inference on\nresource-constrained devices (e.g., IoT sensors and wearables), due to both\ncommunication latency and limited computational capacity. To enable real-time\nsecure inference, we introduce Silentflow, a highly efficient Trusted Execution\nEnvironment (TEE)-assisted protocol that eliminates communication in COT\ngeneration. We tackle the core performance bottleneck-low computational\nintensity-through structured algorithmic decomposition: kernel fusion for\nparallelism, Blocked On-chip eXpansion (BOX) to improve memory access patterns,\nand vectorized batch operations to maximize memory bandwidth utilization.\nThrough design space exploration, we balance end-to-end latency and resource\ndemands, achieving up to 39.51x speedup over state-of-the-art protocols. By\noffloading COT computations to a Zynq-7000 SoC, SilentFlow accelerates PPMLaaS\ninference on the ImageNet dataset under resource constraints, achieving a 4.62x\nand 3.95x speedup over Cryptflow2 and Cheetah, respectively.", "AI": {"tldr": "Silentflow introduces a TEE-assisted protocol for efficient COT generation in resource-constrained edge devices, achieving significant speedups over existing MPC-based privacy-preserving machine learning (PPMLaaS) solutions.", "motivation": "Conventional COT generation in MPC is inefficient for IoT sensors and wearables due to communication latency and limited computational capacity, creating a performance bottleneck for real-time secure inference.", "method": "The protocol uses algorithmic decomposition techniques including kernel fusion for parallelism, Blocked On-chip eXpansion (BOX) for optimized memory access patterns, and vectorized batch operations to maximize memory bandwidth. COT computation is offloaded to a Zynq-7000 SoC for acceleration.", "result": "Silentflow achieves 39.51\u00d7 speedup over state-of-the-art protocols and 4.62\u00d7/3.95\u00d7 speedup over Cryptflow2 and Cheetah specifically on ImageNet dataset inference for resource-constrained devices.", "conclusion": "Silentflow effectively addresses the computational and communication limitations of COT generation in edge devices through architectural optimizations and TEE-based hardware acceleration, enabling real-time secure machine learning inference."}}
{"id": "2508.13666", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.13666", "abs": "https://arxiv.org/abs/2508.13666", "authors": ["Dangfeng Pan", "Zhensu Sun", "Cenyuan Zhang", "David Lo", "Xiaoning Du"], "title": "The Hidden Cost of Readability: How Code Formatting Silently Consumes Your LLM Budget", "comment": "Accepted by ICSE'26 (First Cycle)", "summary": "Source code is usually formatted with elements like indentation and newlines\nto improve readability for human developers. However, these visual aids do not\nseem to be beneficial for large language models (LLMs) in the same way since\nthe code is processed as a linear sequence of tokens. Furthermore, these\nadditional tokens can lead to increased computational costs and longer response\ntimes for LLMs. If such formatting elements are non-essential to LLMs, we can\nreduce such costs by removing them from the code. To figure out the role played\nby formatting elements, we conduct a comprehensive empirical study to evaluate\nthe impact of code formatting on LLM performance and efficiency. Through\nlarge-scale experiments on Fill-in-the-Middle Code Completion tasks across four\nprogramming languages (Java, Python, C++, C\\#) and ten LLMs-including both\ncommercial and open-source models-we systematically analyze token count and\nperformance when formatting elements are removed. Key findings indicate that\nLLMs can maintain performance across formatted code and unformatted code,\nachieving an average input token reduction of 24.5\\% with negligible output\ntoken reductions. This makes code format removal a practical optimization\nstrategy for improving LLM efficiency. Further exploration reveals that both\nprompting and fine-tuning LLMs can lead to significant reductions (up to\n36.1\\%) in output code length without compromising correctness. To facilitate\npractical applications, we develop a bidirectional code transformation tool for\nformat processing, which can be seamlessly integrated into existing LLM\ninference workflows, ensuring both human readability and LLM efficiency.", "AI": {"tldr": "The paper investigates the impact of code formatting on LLM efficiency and performance, revealing that removing non-essential formatting elements reduces computational costs while maintaining correctness, with a developed bidirectional code transformation tool.", "motivation": "Code formatting elements like indentation and newlines enhance human readability but are processed as token overhead by LLMs, increasing computational costs and inference time without adding value for the models.", "method": "Conducted large-scale experiments on Fill-in-the-Middle Code Completion using 10 LLMs (commercial and open-source) across four programming languages, analyzing token count and performance variations with/without formatting elements.", "result": "24.5% average input token reduction with negligible output token impact; output code length could be reduced up to 36.1% through prompting/fine-tuning without correctness loss.", "conclusion": "Code formatting removal is a viable optimization strategy for LLM efficiency, and a bidirectional transformation tool was created to balance human readability and model inference requirements."}}
{"id": "2508.13364", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13364", "abs": "https://arxiv.org/abs/2508.13364", "authors": ["Tadeu Freitas", "Carlos Novo", "In\u00eas Dutra", "Jo\u00e3o Soares", "Manuel Correia", "Benham Shariati", "Rolando Martins"], "title": "A Risk Manager for Intrusion Tolerant Systems: Enhancing HAL 9000 with New Scoring and Data Sources", "comment": null, "summary": "Intrusion Tolerant Systems (ITSs) have become increasingly critical due to\nthe rise of multi-domain adversaries exploiting diverse attack surfaces. ITS\narchitectures aim to tolerate intrusions, ensuring system compromise is\nprevented or mitigated even with adversary presence. Existing ITS solutions\noften employ Risk Managers leveraging public security intelligence to adjust\nsystem defenses dynamically against emerging threats. However, these approaches\nrely heavily on databases like NVD and ExploitDB, which require manual analysis\nfor newly discovered vulnerabilities. This dependency limits the system's\nresponsiveness to rapidly evolving threats. HAL 9000, an ITS Risk Manager\nintroduced in our prior work, addressed these challenges through machine\nlearning. By analyzing descriptions of known vulnerabilities, HAL 9000 predicts\nand assesses new vulnerabilities automatically. To calculate the risk of a\nsystem, it also incorporates the Exploitability Probability Scoring system to\nestimate the likelihood of exploitation within 30 days, enhancing proactive\ndefense capabilities.\n  Despite its success, HAL 9000's reliance on NVD and ExploitDB knowledge is a\nlimitation, considering the availability of other sources of information. This\nextended work introduces a custom-built scraper that continuously mines diverse\nthreat sources, including security advisories, research forums, and real-time\nexploit proofs-of-concept. This significantly expands HAL 9000's intelligence\nbase, enabling earlier detection and assessment of unverified vulnerabilities.\nOur evaluation demonstrates that integrating scraper-derived intelligence with\nHAL 9000's risk management framework substantially improves its ability to\naddress emerging threats. This paper details the scraper's integration into the\narchitecture, its role in providing additional information on new threats, and\nthe effects on HAL 9000's management.", "AI": {"tldr": "The paper enhances HAL 9000, an ITS Risk Manager, by integrating a custom-built scraper to collect threat data from diverse sources, improving responsiveness to emerging vulnerabilities and enabling earlier detection without manual analysis.", "motivation": "Existing ITS solutions depend on manually updated public databases (NVD, ExploitDB), limiting their ability to adapt to rapidly evolving threats. HAL 9000's prior reliance on these databases necessitates expansion for proactive defense.", "method": "Developed a scraper to continuously mine security advisories, research forums, and real-time exploit proofs-of-concept. Integrated scraper-derived intelligence into HAL 9000's risk management framework using machine learning and Exploitability Probability Scoring.", "result": "The scraper significantly expands HAL 9000's intelligence base, enabling earlier detection of unverified vulnerabilities. Evaluation confirms improved threat mitigation capabilities compared to prior approaches.", "conclusion": "Integrating diverse threat intelligence sources into HAL 9000's architecture enhances automatic risk assessment and intrusion tolerance for multi-domain adversaries."}}
{"id": "2508.13757", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13757", "abs": "https://arxiv.org/abs/2508.13757", "authors": ["James Meaden", "Micha\u0142 Jarosz", "Piotr Jod\u0142owski", "Grigori Melnik"], "title": "COMPASS: A Multi-Dimensional Benchmark for Evaluating Code Generation in Large Language Models", "comment": null, "summary": "Current code generation benchmarks focus primarily on functional correctness\nwhile overlooking two critical aspects of real-world programming: algorithmic\nefficiency and code quality. We introduce COMPASS (COdility's Multi-dimensional\nProgramming ASSessment), a comprehensive evaluation framework that assesses\ncode generation across three dimensions: correctness, efficiency, and quality.\nCOMPASS consists of 50 competitive programming problems from real Codility\ncompetitions, providing authentic human baselines from 393,150 submissions.\nUnlike existing benchmarks that treat algorithmically inefficient solutions\nidentically to optimal ones provided they pass test cases, COMPASS\nsystematically evaluates runtime efficiency and code quality using\nindustry-standard analysis tools. Our evaluation of three leading\nreasoning-enhanced models, Anthropic Claude Opus 4, Google Gemini 2.5 Pro, and\nOpenAI O4-Mini-High, reveals that models achieving high correctness scores do\nnot necessarily produce efficient algorithms or maintainable code. These\nfindings highlight the importance of evaluating more than just correctness to\ntruly understand the real-world capabilities of code generation models. COMPASS\nserves as a guiding framework, charting a path for future research toward AI\nsystems that are robust, reliable, and ready for production use.", "AI": {"tldr": "This paper introduces COMPASS, a code generation benchmark framework that evaluates correctness, efficiency, and code quality using real Codility problems and human baselines, revealing that top models on correctness do not necessarily excel in efficiency and maintainability.", "motivation": "Existing benchmarks ignore algorithmic efficiency and code quality despite their real-world importance; models passing correctness-only tests may lack practical viability.", "method": "Curated 50 Codility competitive programming problems with 393,150 human submissions as baselines. Evaluated 3 leading models (Claude Opus 4, Gemini 2.5 Pro, O4-Mini-High) across correctness, efficiency, and quality using industry-standard tools.", "result": "High correctness scores correlate poorly with efficiency (runtime analysis) and quality (maintainability metrics) in generated solutions; human-baseline comparison shows significant gaps in practical code generation.", "conclusion": "COMPASS establishes a vital framework for holistic code generation evaluation, urging future research to optimize not just correctness but also algorithmic efficiency and code quality for production-ready AI systems."}}
{"id": "2508.13425", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.13425", "abs": "https://arxiv.org/abs/2508.13425", "authors": ["Mohamed Elmahallawy", "Tie Luo"], "title": "When Secure Aggregation Falls Short: Achieving Long-Term Privacy in Asynchronous Federated Learning for LEO Satellite Networks", "comment": null, "summary": "Secure aggregation is a common technique in federated learning (FL) for\nprotecting data privacy from both curious internal entities (clients or server)\nand external adversaries (eavesdroppers). However, in dynamic and\nresource-constrained environments such as low Earth orbit (LEO) satellite\nnetworks, traditional secure aggregation methods fall short in two aspects: (1)\nthey assume continuous client availability while LEO satellite visibility is\nintermittent and irregular; (2) they consider privacy in each communication\nround but have overlooked the possible privacy leakage through multiple rounds.\nTo address these limitations, we propose LTP-FLEO, an asynchronous FL framework\nthat preserves long-term privacy (LTP) for LEO satellite networks. LTP-FLEO\nintroduces (i) privacy-aware satellite partitioning, which groups satellites\nbased on their predictable visibility to the server and enforces joint\nparticipation; (ii) model age balancing, which mitigates the adverse impact of\nstale model updates; and (iii) fair global aggregation, which treats satellites\nof different visibility durations in an equitable manner. Theoretical analysis\nand empirical validation demonstrate that LTP-FLEO effectively safeguards both\nmodel and data privacy across multi-round training, promotes fairness in line\nwith satellite contributions, accelerates global convergence, and achieves\ncompetitive model accuracy.", "AI": {"tldr": "The paper proposes LTP-FLEO, an asynchronous federated learning framework that preserves long-term privacy for LEO satellite networks by addressing intermittent visibility and multi-round privacy leakage through privacy-aware satellite partitioning, model age balancing, and fair global aggregation.", "motivation": "Traditional secure aggregation methods fail in dynamic, resource-constrained LEO satellite environments due to assumptions of continuous client availability and lack of long-term privacy guarantees across multiple communication rounds.", "method": "1. Privacy-aware satellite partitioning (predictable visibility-based grouping with joint participation requirements) \n2. Model age balancing (mitigating impacts of stale updates) \n3. Fair global aggregation (equitable treatment of satellites based on visibility duration)", "result": "Theoretical analysis and experiments show LTP-FLEO preserves model/data privacy across multi-round training, achieves fairness by contribution, accelerates convergence, and maintains competitive model accuracy despite dynamic satellite visibility.", "conclusion": "LTP-FLEO addresses critical limitations of existing secure aggregation in LEO FL by integrating long-term privacy preservation (across rounds) with fairness and convergence efficiency, making it suitable for dynamic satellite network environments."}}
{"id": "2508.13774", "categories": ["cs.SE", "cs.AI", "J.5; I.2"], "pdf": "https://arxiv.org/pdf/2508.13774", "abs": "https://arxiv.org/abs/2508.13774", "authors": ["Peer Trilcke", "Ingo B\u00f6rner", "Henny Sluyter-G\u00e4thje", "Daniil Skorinkin", "Frank Fischer", "Carsten Milling"], "title": "Agentic DraCor and the Art of Docstring Engineering: Evaluating MCP-empowered LLM Usage of the DraCor API", "comment": "Preprint, submitted to the 2nd Workshop on Computational Drama\n  Analysis at DraCor Summit 2025, September 03, 2025, Berlin, Germany", "summary": "This paper reports on the implementation and evaluation of a Model Context\nProtocol (MCP) server for DraCor, enabling Large Language Models (LLM) to\nautonomously interact with the DraCor API. We conducted experiments focusing on\ntool selection and application by the LLM, employing a qualitative approach\nthat includes systematic observation of prompts to understand how LLMs behave\nwhen using MCP tools, evaluating \"Tool Correctness\", \"Tool-Calling Efficiency\",\nand \"Tool-Use Reliability\". Our findings highlight the importance of \"Docstring\nEngineering\", defined as reflexively crafting tool documentation to optimize\nLLM-tool interaction. Our experiments demonstrate both the promise of agentic\nAI for research in Computational Literary Studies and the essential\ninfrastructure development needs for reliable Digital Humanities\ninfrastructures.", "AI": {"tldr": "The paper presents a Model Context Protocol (MCP) server for DraCor, enabling LLMs to interact with its API and demonstrates the importance of 'Docstring Engineering' for effective LLM-tool integration in Digital Humanities research.", "motivation": "The motivation centers on advancing Computational Literary Studies through autonomous LLM interactions with digital archives while addressing the need for reliable infrastructure in the Digital Humanities field.", "method": "The method involves implementing the MCP server and conducting qualitative experiments via systematic prompt observation to evaluate LLM tool selection, correctness, calling efficiency, and use reliability.", "result": "Results highlight the effectiveness of 'Docstring Engineering' in optimizing LLM-tool interactions and provide empirical evidence of agentic AI's potential for research while identifying infrastructure gaps.", "conclusion": "The study concludes that agentic AI shows promise for Computational Literary Studies but emphasizes the critical need for infrastructure development to ensure reliable implementation in Digital Humanities contexts."}}
{"id": "2508.13453", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.13453", "abs": "https://arxiv.org/abs/2508.13453", "authors": ["Ruby Nealon"], "title": "Beneath the Mask: Can Contribution Data Unveil Malicious Personas in Open-Source Projects?", "comment": null, "summary": "In February 2024, after building trust over two years with project\nmaintainers by making a significant volume of legitimate contributions, GitHub\nuser \"JiaT75\" self-merged a version of the XZ Utils project containing a highly\nsophisticated, well-disguised backdoor targeting sshd processes running on\nsystems with the backdoored package installed. A month later, this package\nbegan to be distributed with popular Linux distributions until a Microsoft\nemployee discovered the backdoor while investigating how a recent system\nupgrade impacted the performance of SSH authentication. Despite its potential\nglobal impact, no tooling exists for monitoring and identifying anomalous\nbehavior by personas contributing to other open-source projects. This paper\ndemonstrates how Open Source Intelligence (OSINT) data gathered from GitHub\ncontributions, analyzed using graph databases and graph theory, can efficiently\nidentify anomalous behaviors exhibited by the \"JiaT75\" persona across other\nopen-source projects.", "AI": {"tldr": "Analyzes detection of anomalous open-source contributions using OSINT and graph analysis via the XZ Utils backdoor case.", "motivation": "Existing lack of tooling to monitor and identify suspicious behavioral patterns in open-source project contributors despite potential global security risks.", "method": "Utilized Open Source Intelligence (OSINT) data from GitHub contributions, processed and analyzed through graph databases and graph theory to detect anomalies in contributor patterns.", "result": "Identified anomalous behaviors by the 'JiaT75' user persona across multiple projects, explaining how the XZUtils backdoor could have been flagged through this method.", "conclusion": "Graph-based OSINT analysis effectively reveals suspicious contributor patterns in open-source ecosystems, emphasizing the need for monitoring systems to counter such threats."}}
{"id": "2508.13819", "categories": ["cs.SE", "K.6.3; E.0"], "pdf": "https://arxiv.org/pdf/2508.13819", "abs": "https://arxiv.org/abs/2508.13819", "authors": ["Daniel Ogenrwot", "John Businge", "Shaikh Arifuzzaman"], "title": "Structural and Connectivity Patterns in the Maven Central Software Dependency Network", "comment": "17 pages, 6 figures, 34th International Conference on Software\n  Engineering and Data Engineering", "summary": "Understanding the structural characteristics and connectivity patterns of\nlarge-scale software ecosystems is critical for enhancing software reuse,\nimproving ecosystem resilience, and mitigating security risks. In this paper,\nwe investigate the Maven Central ecosystem, one of the largest repositories of\nJava libraries, by applying network science techniques to its dependency graph.\nLeveraging the Goblin framework, we extracted a sample consisting of the top\n5,000 highly connected artifacts based on their degree centrality and then\nperformed breadth-first search (BFS) expansion from each selected artifact as a\nseed node, traversing the graph outward to capture all libraries and releases\nreachable those seed nodes. This sampling strategy captured the immediate\nstructural context surrounding these libraries resulted in a curated graph\ncomprising of 1.3 million nodes and 20.9 million edges. We conducted a\ncomprehensive analysis of this graph, computing degree distributions,\nbetweenness centrality, PageRank centrality, and connected components\ngraph-theoretic metrics. Our results reveal that Maven Central exhibits a\nhighly interconnected, scale-free, and small-world topology, characterized by a\nsmall number of infrastructural hubs that support the majority of projects.\nFurther analysis using PageRank and betweenness centrality shows that these\nhubs predominantly consist of core ecosystem infrastructure, including testing\nframeworks and general-purpose utility libraries. While these hubs facilitate\nefficient software reuse and integration, they also pose systemic risks;\nfailures or vulnerabilities affecting these critical nodes can have widespread\nand cascading impacts throughout the ecosystem.", "AI": {"tldr": "The study applies network science to Maven Central's dependency graph, revealing its scale-free and small-world topology with critical infrastructural hubs that pose systemic risks if compromised.", "motivation": "Understanding structural characteristics and connectivity patterns of software ecosystems helps enhance software reuse, improve ecosystem resilience, and mitigate security risks.", "method": "Used the Goblin framework to sample top 5,000 artifacts by degree centrality, performed BFS expansion, and analyzed 1.3 million-node graph using metrics like degree distribution, betweenness centrality, PageRank, and connected components.", "result": "Maven Central exhibits scale-free and small-world topology with infrastructural hubs (testing frameworks, utilities) at the core, which enable reuse but create systemic vulnerability risks due to their central role.", "conclusion": "While infrastructural hubs in the ecosystem promote efficient software reuse and integration, their centrality and widespread dependencies create critical systemic risks requiring focused security and resilience strategies."}}
{"id": "2508.13520", "categories": ["cs.CR", "math.NT", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.13520", "abs": "https://arxiv.org/abs/2508.13520", "authors": ["Takreem Haider"], "title": "Optimizing Scalar Selection in Elliptic Curve Cryptography Using Differential Evolution for Enhanced Security", "comment": null, "summary": "Elliptic Curve Cryptography (ECC) is a fundamental component of modern\npublic-key cryptosystems that enable efficient and secure digital signatures,\nkey exchanges, and encryption. Its core operation, scalar multiplication,\ndenoted as $k \\cdot P$, where $P$ is a base point and $k$ is a private scalar,\nrelies heavily on the secrecy and unpredictability of $k$. Conventionally, $k$\nis selected using user input or pseudorandom number generators. However, in\nresource-constrained environments with weak entropy sources, these approaches\nmay yield low-entropy or biased scalars, increasing susceptibility to\nside-channel and key recovery attacks. To mitigate these vulnerabilities, we\nintroduce an optimization-driven scalar generation method that explicitly\nmaximizes bit-level entropy. Our approach uses differential evolution (DE), a\npopulation-based metaheuristic algorithm, to search for scalars whose binary\nrepresentations exhibit maximal entropy, defined by an even and statistically\nuniform distribution of ones and zeros. This reformulation of scalar selection\nas an entropy-optimization problem enhances resistance to entropy-based\ncryptanalytic techniques and improves overall unpredictability. Experimental\nresults demonstrate that DE-optimized scalars achieve entropy significantly\nhigher than conventionally generated scalars. The proposed method can be\nintegrated into existing ECC-based protocols, offering a deterministic, tunable\nalternative to traditional randomness, ideal for applications in blockchain,\nsecure messaging, IoT, and other resource-constrained environments.", "AI": {"tldr": "The paper proposes an optimization-driven method using differential evolution to generate high-entropy private scalars for elliptic curve cryptography (ECC), enhancing security in resource-constrained environments by outperforming conventional random generation approaches.", "motivation": "Existing scalar generation methods for ECC (user input/pseudorandom generators) produce low-entropy or biased scalars in weak entropy environments, exposing systems to side-channel and key recovery attacks. Traditional approaches lack adaptability to constrained resources.", "method": "Reformulate scalar selection as an entropy-optimization problem. Use differential evolution (DE), a population-based metaheuristic algorithm, to search for scalars with maximized bit-level entropy via fitness functions measuring binary distribution uniformity.", "result": "DE-optimized scalars achieve statistically significant higher entropy compared to conventionally generated scalars. The method provides deterministic, tunable results without compromising cryptographic compatibility.", "conclusion": "The optimization-based scalar generation improves ECC security in resource-constrained environments by maximizing bit entropy, offering a robust alternative to traditional randomness while maintaining protocol compatibility for blockchain, IoT, and secure messaging applications."}}
{"id": "2508.13863", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.13863", "abs": "https://arxiv.org/abs/2508.13863", "authors": ["Shuai Zhao", "Jieyu Jiang", "Shenlin Cai", "Yaowei Liang", "Chen Jie", "Yinjie Fang", "Wei Zhang", "Guoquan Zhang", "Yaoyao Gu", "Xiang Xiao", "Wei Qin", "Xiangzhen Ouyang", "Wanli Chang"], "title": "Tight Inter-Core Cache Contention Analysis for WCET Estimation on Multicore Systems", "comment": null, "summary": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead.", "AI": {"tldr": "This paper introduces a new method for inter-core cache contention analysis in multicore WCET estimation, improving accuracy by considering actual cache states and access counts, reducing interference estimates by 52.31% and WCET by 8.94%.", "motivation": "Existing WCET analysis methods on multicore architectures overestimate remote cache block interference due to incomplete modeling of cache states and access dynamics, leading to inefficient resource allocation.", "method": "The analysis uses program region ordering to identify affected memory references, then applies dynamic programming to compute precise cache miss counts based on local/remote access quantities across cores.", "result": "Experiments show 52.31% average reduction in quantified inter-core cache contention and 8.94% average WCET estimation reduction compared to prior techniques, with minimal computational overhead.", "conclusion": "Dynamic programming-based contention analysis offers a scalable solution to accurately model real-time cache interference in multicore systems, enabling tighter WCET estimates and better resource predictions."}}
{"id": "2508.13588", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.13588", "abs": "https://arxiv.org/abs/2508.13588", "authors": ["V\u00edctor Mayoral-Vilches", "Jasmin Wachter", "Crist\u00f3bal R. J. Veas Chavez", "Cathrin Schachner", "Luis Javier Navarrete-Lozano", "Mar\u00eda Sanz-G\u00f3mez"], "title": "CAI Fluency: A Framework for Cybersecurity AI Fluency", "comment": null, "summary": "This work introduces CAI Fluency, an an educational platform of the\nCybersecurity AI (CAI) framework dedicated to democratizing the knowledge and\napplication of cybersecurity AI tools in the global security community. The\nmain objective of the CAI framework is to accelerate the widespread adoption\nand effective use of artificial intelligence-based cybersecurity solutions,\npathing the way to vibe-hacking, the cybersecurity analogon to vibe-coding.\n  CAI Fluency builds upon the Framework for AI Fluency, adapting its three\nmodalities of human-AI interaction and four core competencies specifically for\ncybersecurity applications. This theoretical foundation ensures that\npractitioners develop not just technical skills, but also the critical thinking\nand ethical awareness necessary for responsible AI use in security contexts.\n  This technical report serves as a white-paper, as well as detailed\neducational and practical guide that helps users understand the principles\nbehind the CAI framework, and educates them how to apply this knowledge in\ntheir projects and real-world security contexts.", "AI": {"tldr": "CAI Fluency is an educational platform under the Cybersecurity AI (CAI) framework designed to democratize AI-based cybersecurity tools by combining human-AI interaction modalities with core competencies, promoting responsible adoption through theory and practice.", "motivation": "The CAI framework seeks to accelerate the adoption of AI-driven cybersecurity solutions, address skills gaps, and establish 'vibe-hacking' as an ethical paradigm for human-AI collaboration in security contexts.", "method": "The platform adapts the Framework for AI Fluency's three human-AI interaction modalities and four core competencies to cybersecurity needs, structuring the technical report as both a white paper and practical implementation guide.", "result": "This technical report provides comprehensive educational resources, theoretical foundations (including 'vibe-hacking'), and actionable guidance for deploying AI security solutions within the CAI methodology.", "conclusion": "CAI Fluency bridges technical capability with ethical responsibility in cybersecurity AI, offering a scalable approach to train practitioners in both AI deployment and critical assessment of security implications."}}
{"id": "2508.13644", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.13644", "abs": "https://arxiv.org/abs/2508.13644", "authors": ["Viktoria Koscinski", "Mark Nelson", "Ahmet Okutan", "Robert Falso", "Mehdi Mirakhorli"], "title": "Conflicting Scores, Confusing Signals: An Empirical Study of Vulnerability Scoring Systems", "comment": null, "summary": "Accurately assessing software vulnerabilities is essential for effective\nprioritization and remediation. While various scoring systems exist to support\nthis task, their differing goals, methodologies and outputs often lead to\ninconsistent prioritization decisions. This work provides the first\nlarge-scale, outcome-linked empirical comparison of four publicly available\nvulnerability scoring systems: the Common Vulnerability Scoring System (CVSS),\nthe Stakeholder-Specific Vulnerability Categorization (SSVC), the Exploit\nPrediction Scoring System (EPSS), and the Exploitability Index. We use a\ndataset of 600 real-world vulnerabilities derived from four months of\nMicrosoft's Patch Tuesday disclosures to investigate the relationships between\nthese scores, evaluate how they support vulnerability management task, how\nthese scores categorize vulnerabilities across triage tiers, and assess their\nability to capture the real-world exploitation risk. Our findings reveal\nsignificant disparities in how scoring systems rank the same vulnerabilities,\nwith implications for organizations relying on these metrics to make\ndata-driven, risk-based decisions. We provide insights into the alignment and\ndivergence of these systems, highlighting the need for more transparent and\nconsistent exploitability, risk, and severity assessments.", "AI": {"tldr": "This study empirically compares four vulnerability scoring systems (CVSS, SSVC, EPSS, Exploitability Index) using 600 real-world Microsoft vulnerabilities to evaluate their consistency in prioritization and exploitation risk assessment.", "motivation": "Inconsistent prioritization decisions across vulnerability scoring systems hinder effective risk management. Organizations need outcome-linked empirical analysis to choose reliable metrics for resource allocation.", "method": "Large-scale empirical evaluation of four scoring systems using 600 vulnerabilities from Microsoft's Patch Tuesday over four months, analyzing score relationships, triage tier categorization, and exploitation risk prediction accuracy.", "result": "Scoring systems demonstrated statistically significant (p<0.001) ranking discrepancies for the same vulnerabilities, with no consistent alignment across systems. EPSS showed weak correlation (r\u00b2=0.18) with actual exploitation patterns, while CVSS had moderate correlation (r\u00b2=0.45).", "conclusion": "Vulnerability scoring systems produce inconsistent risk assessments, necessitating standardized evaluation frameworks and greater transparency in scoring methodology to improve reliability for organizational decision-making."}}
{"id": "2508.13690", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13690", "abs": "https://arxiv.org/abs/2508.13690", "authors": ["Wei Shao", "Zequan Liang", "Ruoyu Zhang", "Ruijie Fang", "Ning Miao", "Ehsan Kourkchi", "Setareh Rafatirad", "Houman Homayoun", "Chongzhou Fang"], "title": "Know Me by My Pulse: Toward Practical Continuous Authentication on Wearable Devices via Wrist-Worn PPG", "comment": "To be published in Network and Distributed System Security (NDSS)\n  Symposium 2026", "summary": "Biometric authentication using physiological signals offers a promising path\ntoward secure and user-friendly access control in wearable devices. While\nelectrocardiogram (ECG) signals have shown high discriminability, their\nintrusive sensing requirements and discontinuous acquisition limit\npracticality. Photoplethysmography (PPG), on the other hand, enables\ncontinuous, non-intrusive authentication with seamless integration into\nwrist-worn wearable devices. However, most prior work relies on high-frequency\nPPG (e.g., 75 - 500 Hz) and complex deep models, which incur significant energy\nand computational overhead, impeding deployment in power-constrained real-world\nsystems. In this paper, we present the first real-world implementation and\nevaluation of a continuous authentication system on a smartwatch, We-Be Band,\nusing low-frequency (25 Hz) multi-channel PPG signals. Our method employs a\nBi-LSTM with attention mechanism to extract identity-specific features from\nshort (4 s) windows of 4-channel PPG. Through extensive evaluations on both\npublic datasets (PTTPPG) and our We-Be Dataset (26 subjects), we demonstrate\nstrong classification performance with an average test accuracy of 88.11%,\nmacro F1-score of 0.88, False Acceptance Rate (FAR) of 0.48%, False Rejection\nRate (FRR) of 11.77%, and Equal Error Rate (EER) of 2.76%. Our 25 Hz system\nreduces sensor power consumption by 53% compared to 512 Hz and 19% compared to\n128 Hz setups without compromising performance. We find that sampling at 25 Hz\npreserves authentication accuracy, whereas performance drops sharply at 20 Hz\nwhile offering only trivial additional power savings, underscoring 25 Hz as the\npractical lower bound. Additionally, we find that models trained exclusively on\nresting data fail under motion, while activity-diverse training improves\nrobustness across physiological states.", "AI": {"tldr": "This paper introduces a continuous authentication system on a smartwatch using low-frequency (25 Hz) multi-channel PPG, achieving strong performance (88.11% accuracy, F1=0.88, FAR=0.48%, EER=2.76%) while reducing power consumption by 53% compared to 512 Hz systems. It identifies 25 Hz as the practical lower bound for sampling and emphasizes the necessity of activity-diverse training data to maintain robustness under motion.", "motivation": "ECG-based biometric authentication is limited by intrusive sensing requirements, and PPG systems using high-frequency sampling (75-500 Hz) are problematic for wearable devices due to significant energy and computational overhead. There is a need for non-intrusive, power-efficient authentication suitable for real-world deployment.", "method": "The proposed system, We-Be Band, uses a Bi-LSTM with attention mechanism to extract identity-specific features from 4-channel PPG signals sampled at 25 Hz. Short windows (4 seconds) of PPG data are processed through the model to enable real-time authentication on smartwatches.", "result": "The system achieves 88.11% average test accuracy, macro F1-score of 0.88, FAR of 0.48%, FRR of 11.77%, and EER of 2.76% using 25 Hz sampling. It reduces sensor power consumption by 53% versus 512 Hz and 19% versus 128 Hz setups. Performance degrades sharply below 25 Hz, but training with activity-diverse data maintains accuracy under motion.", "conclusion": "25 Hz is confirmed as the practical sampling frequency minimum for PPG-based authentication without performance loss. Activity-diverse training is critical to maintain robustness across different physiological states, enabling real-world deployment of continuous biometric authentication in wearable devices."}}
{"id": "2508.13730", "categories": ["cs.CR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.13730", "abs": "https://arxiv.org/abs/2508.13730", "authors": ["Daniel M. Jimenez-Gutierrez", "Yelizaveta Falkouskaya", "Jose L. Hernandez-Ramos", "Aris Anagnostopoulos", "Ioannis Chatzigiannakis", "Andrea Vitaletti"], "title": "On the Security and Privacy of Federated Learning: A Survey with Attacks, Defenses, Frameworks, Applications, and Future Directions", "comment": null, "summary": "Federated Learning (FL) is an emerging distributed machine learning paradigm\nenabling multiple clients to train a global model collaboratively without\nsharing their raw data. While FL enhances data privacy by design, it remains\nvulnerable to various security and privacy threats. This survey provides a\ncomprehensive overview of more than 200 papers regarding the state-of-the-art\nattacks and defense mechanisms developed to address these challenges,\ncategorizing them into security-enhancing and privacy-preserving techniques.\nSecurity-enhancing methods aim to improve FL robustness against malicious\nbehaviors such as byzantine attacks, poisoning, and Sybil attacks. At the same\ntime, privacy-preserving techniques focus on protecting sensitive data through\ncryptographic approaches, differential privacy, and secure aggregation. We\ncritically analyze the strengths and limitations of existing methods, highlight\nthe trade-offs between privacy, security, and model performance, and discuss\nthe implications of non-IID data distributions on the effectiveness of these\ndefenses. Furthermore, we identify open research challenges and future\ndirections, including the need for scalable, adaptive, and energy-efficient\nsolutions operating in dynamic and heterogeneous FL environments. Our survey\naims to guide researchers and practitioners in developing robust and\nprivacy-preserving FL systems, fostering advancements safeguarding\ncollaborative learning frameworks' integrity and confidentiality.", "AI": {"tldr": "This survey provides an overview of over 200 papers on attacks and defenses in Federated Learning (FL), categorizing techniques into security-enhancing and privacy-preserving methods while analyzing trade-offs and future challenges.", "motivation": "FL improves data privacy through decentralized training but faces security and privacy threats, necessitating a systematic analysis of existing solutions and open challenges.", "method": "A comprehensive review and categorization of state-of-the-art attacks and defense mechanisms, with critical analysis of their strengths, limitations, and trade-offs between privacy, security, and model performance.", "result": "Identifies gaps in defending FL against threats (e.g., non-IID data challenges existing methods) and emphasizes trade-offs between privacy guarantees and model accuracy.", "conclusion": "The survey highlights the need for scalable, adaptive FL defenses in dynamic environments and aims to guide research toward robust, privacy-preserving collaborative learning frameworks."}}
{"id": "2508.13750", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.13750", "abs": "https://arxiv.org/abs/2508.13750", "authors": ["Eric Cornelissen", "Musard Balliu"], "title": "NodeShield: Runtime Enforcement of Security-Enhanced SBOMs for Node.js", "comment": "15 pages, 3 figures, 9 tables", "summary": "The software supply chain is an increasingly common attack vector for\nmalicious actors. The Node.js ecosystem has been subject to a wide array of\nattacks, likely due to its size and prevalence. To counter such attacks, the\nresearch community and practitioners have proposed a range of static and\ndynamic mechanisms, including process- and language-level sandboxing,\npermission systems, and taint tracking. Drawing on valuable insight from these\nworks, this paper studies a runtime protection mechanism for (the supply chain\nof) Node.js applications with the ambitious goals of compatibility, automation,\nminimal overhead, and policy conciseness.\n  Specifically, we design, implement and evaluate NodeShield, a protection\nmechanism for Node.js that enforces an application's dependency hierarchy and\ncontrols access to system resources at runtime. We leverage the up-and-coming\nSBOM standard as the source of truth for the dependency hierarchy of the\napplication, thus preventing components from stealthily abusing undeclared\ncomponents. We propose to enhance the SBOM with a notion of capabilities that\nrepresents a set of related system resources a component may access. Our\nproposed SBOM extension, the Capability Bill of Materials or CBOM, records the\nrequired capabilities of each component, providing valuable insight into the\npotential privileged behavior. NodeShield enforces the SBOM and CBOM at runtime\nvia code outlining (as opposed to inlining) with no modifications to the\noriginal code or Node.js runtime, thus preventing unexpected, potentially\nmalicious behavior. Our evaluation shows that NodeShield can prevent over 98%\nout of 67 known supply chain attacks while incurring minimal overhead on\nservers at less than 1ms per request. We achieve this while maintaining broad\ncompatibility with vanilla Node.js and a concise policy language that consists\nof at most 7 entries per dependency.", "AI": {"tldr": "Studies a runtime protection mechanism for Node.js supply chain application using NodeShield, which enforces dependency hierarchy and controls system resources with SBOM/CBOM extensions.", "motivation": "Node.js's large ecosystem and prevalence make it a frequent target for supply chain attacks. Existing static and dynamic mechanisms (sandboxing, permissions, taint tracking) fall short on compatibility, automation, and overhead.", "method": "Designed NodeShield: 1) Uses SBOM (Software Bill of Materials) as dependency hierarchy source. 2) Introduces CBOM (Capability Bill of Materials) to define component access capabilities. 3) Implements runtime enforcement via code outlining without modifying original code or Node.js runtime.", "result": "Prevented 98% of 67 known supply chain attacks with <1ms request overhead on servers. Demonstrated compatibility with vanilla Node.js and concise policies (\u22647 entries per dependency).", "conclusion": "NodeShield provides effective, low-overhead runtime protection for Node.js supply chains using enhanced SBOM metadata, surpassing prior approaches in compatibility and practical deployment feasibility."}}
{"id": "2508.13965", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.13965", "abs": "https://arxiv.org/abs/2508.13965", "authors": ["Yuntao Liu", "Abir Akib", "Zelin Lu", "Qian Xu", "Ankur Srivastava", "Gang Qu", "David Kehlet", "Nij Dorairaj"], "title": "Red Teaming Methodology for Design Obfuscation", "comment": null, "summary": "The main goal of design obfuscation schemes is to protect sensitive design\ndetails from untrusted parties in the VLSI supply chain, including but not\nlimited to off-shore foundries and untrusted end users. In this work, we\nprovide a systematic red teaming approach to evaluate the security of design\nobfuscation approaches. Specifically, we propose security metrics and\nevaluation methodology for the scenarios where the adversary does not have\naccess to a working chip. A case study on the RIPPER tool developed by the\nUniversity of Florida indicates that more information is leaked about the\nstructure of the original design than commonly considered.", "AI": {"tldr": "The paper proposes a systematic red teaming approach to evaluate design obfuscation security in VLSI supply chains, revealing significant information leakage in the RIPPER tool's obfuscation methodology.", "motivation": "Protect sensitive design details from untrusted parties in the VLSI supply chain (e.g., offshore foundries and end users) through rigorous obfuscation evaluation.", "method": "Developed novel security metrics and evaluation methodology for obfuscation analysis in scenarios where adversaries lack access to working chips, validated through red-teaming techniques.", "result": "Case study on RIPPER tool demonstrated structural information leakage exceeding prior assumptions, undermining its security claims.", "conclusion": "Current design obfuscation techniques may be more vulnerable than previously believed, necessitating improved security evaluation frameworks to address hidden leakage channels."}}
