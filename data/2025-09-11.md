<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 4]
- [cs.SE](#cs.SE) [Total: 7]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Establishing a Baseline of Software Supply Chain Security Task Adoption by Software Organizations](https://arxiv.org/abs/2509.08083)
*Laurie Williams,Sammy Migues*

Main category: cs.CR

TL;DR: The study identifies that prioritizing adoption of security tasks targeting novel software supply chain attack vectors (components and build infrastructure) is critical as these are less implemented despite existing frameworks having more mature but already widely adopted tasks.


<details>
  <summary>Details</summary>
Motivation: The exponential increase in software supply chain attacks since 2020 and the infeasibility of exhaustively adopting all risk management framework tasks necessitate prioritization guidance for organizations.

Method: An interview study with 61 software practitioners across nine organizations focused on their implementation of software supply chain risk management tasks.

Result: Organizations predominantly implemented tasks that were already adopted beforehand (e.g., human-focused tasks), while tasks addressing newer attack vectors through software components and build infrastructure remain in early adoption stages.

Conclusion: Software organizations should prioritize adopting security tasks that mitigate novel software-based attack vectors in supply chains to address immature implementation areas.

Abstract: Software supply chain attacks have increased exponentially since 2020. The
primary attack vectors for supply chain attacks are through: (1) software
components; (2) the build infrastructure; and (3) humans (a.k.a software
practitioners). Software supply chain risk management frameworks provide a list
of tasks that an organization can adopt to reduce software supply chain risk.
Exhaustively adopting all the tasks of these frameworks is infeasible,
necessitating the prioritized adoption of tasks. Software organizations can
benefit from being guided in this prioritization by learning what tasks other
teams have adopted. The goal of this study is to aid software development
organizations in understanding the adoption of security tasks that reduce
software supply chain risk through an interview study of software practitioners
engaged in software supply chain risk management efforts. An interview study
was conducted with 61 practitioners at nine software development organizations
that have focused efforts on reducing software supply chain risk. The results
of the interviews indicate that organizations had implemented the most adopted
software tasks before the focus on software supply chain security. Therefore,
their implementation in organizations is more mature. The tasks that mitigate
the novel attack vectors through software components and the build
infrastructure are in the early stages of adoption. Adoption of these tasks
should be prioritized.

</details>


### [2] [SAGE: Sample-Aware Guarding Engine for Robust Intrusion Detection Against Adversarial Attacks](https://arxiv.org/abs/2509.08091)
*Jing Chen,Onat Gungor,Zhengli Shang,Tajana Rosing*

Main category: cs.CR

TL;DR: SAGE, a sample-aware defense engine for IoT intrusion detection, uses active learning and data reduction to achieve robust adversarial resilience. It outperforms existing methods by 201% in F1-score, minimizes the Oracle gap, and reduces computational costs by 29×.


<details>
  <summary>Details</summary>
Motivation: Current ML-based intrusion detection systems (ML-IDS) struggle with adversarial attacks due to the absence of a systematic defense selection approach. This creates vulnerabilities in IoT ecosystems, necessitating more efficient and resilient defense mechanisms.

Method: SAGE employs an active learning mechanism to identify the most informative input samples and their optimal defense labels, which are then used to train a second-level learner for defense selection. This integrates targeted data reduction, enabling efficient learning of robust defense strategies.

Result: SAGE achieves a 201% average F1-score improvement over state-of-the-art defenses, reduces the performance gap to the Oracle by 3.8%, and decreases computational overhead by up to 29× while maintaining robustness and generalizability across datasets.

Conclusion: SAGE effectively enhances intrusion detection by combining active learning with targeted data reduction, significantly improving performance and efficiency compared to existing methods, while narrowing the gap to an ideal Oracle model.

Abstract: The rapid proliferation of the Internet of Things (IoT) continues to expose
critical security vulnerabilities, necessitating the development of efficient
and robust intrusion detection systems (IDS). Machine learning-based intrusion
detection systems (ML-IDS) have significantly improved threat detection
capabilities; however, they remain highly susceptible to adversarial attacks.
While numerous defense mechanisms have been proposed to enhance ML-IDS
resilience, a systematic approach for selecting the most effective defense
against a specific adversarial attack remains absent. To address this
challenge, we previously proposed DYNAMITE, a dynamic defense selection
approach that identifies the most suitable defense against adversarial attacks
through an ML-driven selection mechanism. Building on this foundation, we
propose SAGE (Sample-Aware Guarding Engine), a substantially improved defense
algorithm that integrates active learning with targeted data reduction. It
employs an active learning mechanism to selectively identify the most
informative input samples and their corresponding optimal defense labels, which
are then used to train a second-level learner responsible for selecting the
most effective defense. This targeted sampling improves computational
efficiency, exposes the model to diverse adversarial strategies during
training, and enhances robustness, stability, and generalizability. As a
result, SAGE demonstrates strong predictive performance across multiple
intrusion detection datasets, achieving an average F1-score improvement of 201%
over the state-of-the-art defenses. Notably, SAGE narrows the performance gap
to the Oracle to just 3.8%, while reducing computational overhead by up to 29x.

</details>


### [3] [Accelerating AI Development with Cyber Arenas](https://arxiv.org/abs/2509.08200)
*William Cashman,Chasen Milner,Michael Houle,Michael Jones,Hayden Jananthan,Jeremy Kepner,Peter Michaleas,Alex Pentland*

Main category: cs.CR

TL;DR: This paper evaluates the use of cyber arenas for AI testing by deploying a specialized network sensor during a National Guard exercise, demonstrating their potential to simulate real-world AI performance and adaptability.


<details>
  <summary>Details</summary>
Motivation: AI development requires high-fidelity testing environments to bridge the gap between laboratory settings and operational deployment. Cyber arenas provide flexible, evolving frameworks to expose AI capabilities to real-world complexities and user interactions.

Method: The paper leverages the MIT/IEEE/Amazon Graph Challenge Anonymized Network Sensor, deploying it within a cyber arena during a National Guard exercise to evaluate its performance and adaptability in real-world operational contexts.

Result: The experiment successfully integrated the anonymized network sensor into a dynamic, mission-driven exercise, validating the feasibility of using cyber arenas to test and refine AI systems under realistic conditions.

Conclusion: The deployment of the MIT/IEEE/Amazon Graph Challenge Anonymized Network Sensor in a cyber arena during a National Guard exercise demonstrates the effectiveness of cyber arenas for rapidly testing and integrating AI capabilities in real-world scenarios.

Abstract: AI development requires high fidelity testing environments to effectively
transition from the laboratory to operations. The flexibility offered by cyber
arenas presents a novel opportunity to test new artificial intelligence (AI)
capabilities with users. Cyber arenas are designed to expose end-users to
real-world situations and must rapidly incorporate evolving capabilities to
meet their core objectives. To explore this concept the MIT/IEEE/Amazon Graph
Challenge Anonymized Network Sensor was deployed in a cyber arena during a
National Guard exercise.

</details>


### [4] [Unlocking Reproducibility: Automating re-Build Process for Open-Source Software](https://arxiv.org/abs/2509.08204)
*Behnaz Hassanshahi,Trong Nhan Mai,Benjamin Selwyn Smith,Nicholas Allen*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Software ecosystems like Maven Central play a crucial role in modern software
supply chains by providing repositories for libraries and build plugins.
However, the separation between binaries and their corresponding source code in
Maven Central presents a significant challenge, particularly when it comes to
linking binaries back to their original build environment. This lack of
transparency poses security risks, as approximately 84% of the top 1200
commonly used artifacts are not built using a transparent CI/CD pipeline.
Consequently, users must place a significant amount of trust not only in the
source code but also in the environment in which these artifacts are built.
  Rebuilding software artifacts from source provides a robust solution to
improve supply chain security. This approach allows for a deeper review of
code, verification of binary-source equivalence, and control over dependencies.
However, challenges arise due to variations in build environments, such as JDK
versions and build commands, which can lead to build failures. Additionally,
ensuring that all dependencies are rebuilt from source across large and complex
dependency graphs further complicates the process. In this paper, we introduce
an extension to Macaron, an industry-grade open-source supply chain security
framework, to automate the rebuilding of Maven artifacts from source. Our
approach improves upon existing tools, by offering better performance in source
code detection and automating the extraction of build specifications from
GitHub Actions workflows. We also present a comprehensive root cause analysis
of build failures in Java projects and propose a scalable solution to automate
the rebuilding of artifacts, ultimately enhancing security and transparency in
the open-source supply chain.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [ChatGPT for Code Refactoring: Analyzing Topics, Interaction, and Effective Prompts](https://arxiv.org/abs/2509.08090)
*Eman Abdullah AlOmar,Luo Xu,Sofia Martinez,Anthony Peruma,Mohamed Wiem Mkaouer,Christian D. Newman,Ali Ouni*

Main category: cs.SE

TL;DR: This paper examines developer-chats with ChatGPT for refactoring, analyzing 715 interactions to uncover how needs are expressed and met, enhancing LLMs' role in code improvement.


<details>
  <summary>Details</summary>
Motivation: Developers' interaction patterns with ChatGPT for refactoring remain underexplored, despite LLMs being widely used in software engineering tasks like refactoring. This study aims to bridge this gap.

Method: The researchers conducted text mining of 715 refactoring-related interactions from 29,778 ChatGPT prompts/responses, analyzing developers' explicit refactoring intentions and how ChatGPT addresses them.

Result: The analysis reveals key patterns in how developers articulate refactoring needs and how ChatGPT interprets and satisfies these requests, offering practical implications for refining LLM support in code improvement tasks.

Conclusion: The study highlights the importance of understanding how developers and ChatGPT interact during refactoring tasks, providing actionable insights for improving LLM-based tools in software engineering.

Abstract: Large Language Models (LLMs), such as ChatGPT, have become widely popular and
widely used in various software engineering tasks such as refactoring, testing,
code review, and program comprehension. Although recent studies have examined
the effectiveness of LLMs in recommending and suggesting refactoring, there is
a limited understanding of how developers express their refactoring needs when
interacting with ChatGPT. In this paper, our goal is to explore interactions
related to refactoring between developers and ChatGPT to better understand how
developers identify areas for improvement in code, and how ChatGPT addresses
developers' needs. Our approach involves text mining 715 refactoring-related
interactions from 29,778 ChatGPT prompts and responses, as well as the analysis
of developers' explicit refactoring intentions.

</details>


### [6] [Safety Factories -- a Manifesto](https://arxiv.org/abs/2509.08285)
*Carmen Cârlan,Daniel Ratiu,Michael Wagner*

Main category: cs.SE

TL;DR: Bridging software-safety gap via automated, formal safety engineering ("safety factories").


<details>
  <summary>Details</summary>
Motivation: Modern cyber-physical systems require rapid software iterations for competitive advantage, but existing methods create a disconnect between software development and safety engineering, necessitating disciplined, automated solutions for safety-critical systems.

Method: The paper proposes capturing safety work products in semantically rich, machine-processable models, implementing automatic consistency checks, automating documentation generation, and adopting best practices from software development for safety engineering.

Result: The proposed 'safety factories' framework integrates safety tooling with software pipelines, enabling automated safety analysis and documentation generation during development.

Conclusion: The paper advocates for integrating safety engineering tools and methods into software development pipelines to address the gap between rapid development and safety-critical requirements, emphasizing the need for formal models and automation.

Abstract: Modern cyber-physical systems are operated by complex software that
increasingly takes over safety-critical functions. Software enables rapid
iterations and continuous delivery of new functionality that meets the
ever-changing expectations of users. As high-speed development requires
discipline, rigor, and automation, software factories are used. These entail
methods and tools used for software development, such as build systems and
pipelines. To keep up with the rapid evolution of software, we need to bridge
the disconnect in methods and tools between software development and safety
engineering today. We need to invest more in formality upfront - capturing
safety work products in semantically rich models that are machine-processable,
defining automatic consistency checks, and automating the generation of
documentation - to benefit later. Transferring best practices from software to
safety engineering is worth exploring. We advocate for safety factories, which
integrate safety tooling and methods into software development pipelines.

</details>


### [7] [The Impact of Team Diversity in Agile Development Education](https://arxiv.org/abs/2509.08389)
*Marco Torchiano,Riccardo Coppola,Antonio Vetro',Xhoi Musaj*

Main category: cs.SE

TL;DR: Gender diversity improves project success in software engineering education, nationality diversity has minimal negative impact, and teams with both gender and nationality diversity may face slight performance challenges due to communication barriers—promoting diversity remains educationally beneficial.


<details>
  <summary>Details</summary>
Motivation: This work addresses the understudied impact of nationality diversity and its interaction with gender diversity in software engineering education, aiming to clarify whether promoting diversity affects educational outcomes and project quality in agile teams.

Method: The study analyzed 51 teams across three academic years in an agile software development course. Three diversity indexes (gender, nationality, and their co-presence) were calculated for each team and correlated with project outcomes using statistical analysis.

Result: Gender diversity showed a moderate positive correlation with project success, nationality diversity had a negligible negative effect, and combined diversity (gender+nationality) had a slight negative impact likely due to communication barriers and cultural differences.

Conclusion: Promoting diversity in software engineering education, particularly in gender and nationality, does not negatively impact team performance and is important for achieving educational goals. However, combined diversity (gender and nationality) may introduce communication barriers and cultural differences that hinder project success.

Abstract: Software Engineering is mostly a male-dominated sector, where gender
diversity is a key feature for improving equality of opportunities,
productivity, and innovation. Other diversity aspects, including but not
limited to nationality and ethnicity, are often understudied.In this work we
aim to assess the impact of team diversity, focusing mainly on gender and
nationality, in the context of an agile software development project-based
course. We analyzed 51 teams over three academic years, measuring three
different Diversity indexes - regarding Gender, Nationality and their
co-presence - to examine how different aspects of diversity impact the quality
of team project outcomes.Statistical analysis revealed a moderate,
statistically significant correlation between gender diversity and project
success, aligning with existing literature. Diversity in nationality showed a
negative but negligible effect on project results, indicating that promoting
these aspects does not harm students' performance. Analyzing their co-presence
within a team, gender and nationality combined had a negative impact, likely
due to increased communication barriers and differing cultural norms.This study
underscores the importance of considering multiple diversity dimensions and
their interactions in educational settings. Our findings, overall, show that
promoting diversity in teams does not negatively impact their performance and
achievement of educational goals.

</details>


### [8] [AutoStub: Genetic Programming-Based Stub Creation for Symbolic Execution](https://arxiv.org/abs/2509.08524)
*Felix Mächtle,Nils Loose,Jan-Niclas Serr,Jonas Sander,Thomas Eisenbarth*

Main category: cs.SE

TL;DR: This paper proposes a GP-based method to generate symbolic stubs for external functions during symbolic execution without manual intervention, achieving high accuracy in function approximation.


<details>
  <summary>Details</summary>
Motivation: Symbolic execution is limited by the need for context, expensive solvers, or manual intervention when dealing with external functions like native methods or third-party libraries.

Method: AutoStub automatically generates symbolic stubs by executing external functions on random inputs, collecting outputs, and using Genetic Programming to derive expressions that approximate their behavior.

Result: The method achieved over 90% accuracy for 55% of evaluated functions and successfully inferred language-specific behaviors to reveal edge cases important for testing.

Conclusion: AutoStub provides an effective, automated solution to approximate external functions during symbolic execution, improving test coverage and reducing manual effort.

Abstract: Symbolic execution is a powerful technique for software testing, but suffers
from limitations when encountering external functions, such as native methods
or third-party libraries. Existing solutions often require additional context,
expensive SMT solvers, or manual intervention to approximate these functions
through symbolic stubs. In this work, we propose a novel approach to
automatically generate symbolic stubs for external functions during symbolic
execution that leverages Genetic Programming. When the symbolic executor
encounters an external function, AutoStub generates training data by executing
the function on randomly generated inputs and collecting the outputs. Genetic
Programming then derives expressions that approximate the behavior of the
function, serving as symbolic stubs. These automatically generated stubs allow
the symbolic executor to continue the analysis without manual intervention,
enabling the exploration of program paths that were previously intractable. We
demonstrate that AutoStub can automatically approximate external functions with
over 90% accuracy for 55% of the functions evaluated, and can infer
language-specific behaviors that reveal edge cases crucial for software
testing.

</details>


### [9] [Beyond the Binary: The System of All-round Evaluation of Research and Its Practices in China](https://arxiv.org/abs/2509.08546)
*Yu Zhu,Jiyuan Ye*

Main category: cs.SE

TL;DR: The paper addresses the lack of a systematic evaluation theory in global research reform by introducing SAER, a framework integrating three evaluation dimensions and six elements. It bridges qualitative-quantitative divides, rooted in Chinese dialectical theory, to advance global evaluation practices.


<details>
  <summary>Details</summary>
Motivation: The study responds to stagnant research evaluation reforms due to fragmented binary approaches (qualitative vs. quantitative methods) and the absence of a unifying macro-level theory to guide systematic implementation.

Method: The authors review research evaluation history to reveal binary oppositions, then propose the SAER system—combining form, content, and utility evaluations through six elements—to transcending methodological divides via a trinity of evaluation dimensions.

Result: SAER provides a theoretical breakthrough by harmonizing evaluation dimensions and elements, offering a practical foundation for academic evaluators to resolve methodological contradictions and advance holistic research assessment.

Conclusion: By embedding Chinese dialectical principles, SAER demonstrates how to transcend qualitative-quantitative binaries, delivering a globally relevant framework for research evaluation reform while preserving cultural epistemological contributions.

Abstract: The lack of a macro-level, systematic evaluation theory to guide the
implementation of evaluation practices has become a key bottleneck in the
reform of global research evaluation systems. By reviewing the historical
development of research evaluation, this paper highlights the current binary
opposition between qualitative and quantitative methods in evaluation
practices. This paper introduces the System of All-round Evaluation of Research
(SAER), a framework that integrates form, content, and utility evaluations with
six key elements. SAER offers a theoretical breakthrough by transcending the
binary, providing a comprehensive foundation for global evaluation reforms. The
comprehensive system proposes a trinity of three evaluation dimensions,
combined with six evaluation elements, which would help academic evaluators and
researchers reconcile binary oppositions in evaluation methods. The system
highlights the dialectical wisdom and experience embedded in Chinese research
evaluation theory, offering valuable insights and references for the reform and
advancement of global research evaluation systems.

</details>


### [10] [Minimal Data, Maximum Clarity: A Heuristic for Explaining Optimization](https://arxiv.org/abs/2509.08667)
*Amirali Rayegan,Tim Menzies*

Main category: cs.SE

TL;DR: EZR is a label-efficient, interpretable multi-objective optimization framework combining active learning and decision trees to achieve 90+% of state-of-the-art performance with minimal data.


<details>
  <summary>Details</summary>
Motivation: Software engineering faces challenges of vast configuration spaces and costly labeling, requiring optimization methods that balance effectiveness with interpretability.

Method: EZR uses Naive Bayes-based active learning to prioritize informative samples, then distills optimization policies into decision trees for transparent explanations.

Result: Experiments on 60 datasets show EZR matches 90+% of best-known performance while outperforming LIME/SHAP in explanation clarity with 80-95–% label reduction.

Conclusion: Prioritizing informative data through active learning enables high-performance optimization with minimal labels, advancing 'minimum data maximum clarity' in software systems.

Abstract: Efficient, interpretable optimization is a critical but underexplored
challenge in software engineering, where practitioners routinely face vast
configuration spaces and costly, error-prone labeling processes. This paper
introduces EZR, a novel and modular framework for multi-objective optimization
that unifies active sampling, learning, and explanation within a single,
lightweight pipeline. Departing from conventional wisdom, our Maximum Clarity
Heuristic demonstrates that using less (but more informative) data can yield
optimization models that are both effective and deeply understandable. EZR
employs an active learning strategy based on Naive Bayes sampling to
efficiently identify high-quality configurations with a fraction of the labels
required by fully supervised approaches. It then distills optimization logic
into concise decision trees, offering transparent, actionable explanations for
both global and local decision-making. Extensive experiments across 60
real-world datasets establish that EZR reliably achieves over 90% of the
best-known optimization performance in most cases, while providing clear,
cohort-based rationales that surpass standard attribution-based explainable AI
(XAI) methods (LIME, SHAP, BreakDown) in clarity and utility. These results
endorse "less but better"; it is both possible and often preferable to use
fewer (but more informative) examples to generate label-efficient optimization
and explanations in software systems. To support transparency and
reproducibility, all code and experimental materials are publicly available at
https://github.com/amiiralii/Minimal-Data-Maximum-Clarity.

</details>


### [11] [SWE-Mirror: Scaling Issue-Resolving Datasets by Mirroring Issues Across Repositories](https://arxiv.org/abs/2509.08724)
*Junhao Wang,Daoguang Zan,Shulin Xin,Siyao Liu,Yurong Wu,Kai Shen*

Main category: cs.SE

TL;DR: This paper introduces SWE-Mirror, a pipeline that transforms real GitHub issues into verifiable Gym environments, creating a large-scale dataset that improves coding agent performance by over 46% for large models.


<details>
  <summary>Details</summary>
Motivation: Existing methods for creating verifiable training datasets for issue-resolution face limitations in success rates, computational overhead, and failure to utilize authentic human-reported issues from repositories like GitHub, necessitating a scalable and effective solution.

Method: The SWE-Mirror pipeline distills semantic essence from GitHub issues, mirrors them into configurable Gym environments, and generates verifiable tasks by reusing existing infrastructure. This approach bridges the gap between synthetic environment setups and real-world problem data.

Result: Generated 60,671 verified tasks across 40 repositories and 4 languages, achieving +21.8% resolve rate improvement for 7B models and +46.0% for 32B models on SWE-Bench-Verified datasets, establishing a new SOTA with 12,000 high-quality trajectories.

Conclusion: SWE-Mirror effectively constructs a large-scale verifiable dataset by leveraging real GitHub issues and Gym environments, significantly improving issue-resolution capabilities of coding agents with substantial performance gains over existing methods.

Abstract: Creating large-scale verifiable training datasets for issue-resolving tasks
is a critical yet notoriously difficult challenge. Existing methods on
automating the Gym environment setup process for real-world issues suffer from
low success rates and high overhead. Meanwhile, synthesizing new tasks within
existing Gym environments leaves the vast pool of authentic, human-reported
problems untapped. To maximize the utilization of existing Gym environments and
also the rich data of issue-resolving history on GitHub, we introduce
SWE-Mirror, a pipeline that distills a real-world issue's semantic essence,
mirrors it into another repository with a configured Gym environment, and
re-animates it as a verifiable issue-resolving task. SWE-Mirror reuses existing
Gym environments along with the vast pool of issue-resolving history hosted on
GitHub to construct a large-scale dataset of mirrored authentic and verifiable
tasks. Applying SWE-Mirror to 40 repositories across 4 languages, we have
curated a dataset with 60,671 issue-resolving tasks and demonstrated the value
of our dataset by training and evaluating coding agents at various scale.
Post-training experiments show that models trained with the dataset exhibit
improvements in issue-resolving capabilities. Furthermore, by extending the
dataset size to over 12,000 high-quality trajectories, we established a new
state-of-the-art (SOTA) among Qwen2.5-Coder-Instruct based LLMs on the
OpenHands agent framework, which increases the resolve rate on
SWE-Bench-Verified by +21.8% for the 7B model and +46.0% for the 32B model and
validates the effectiveness of our approach.

</details>
