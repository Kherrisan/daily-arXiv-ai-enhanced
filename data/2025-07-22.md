<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 31]
- [cs.SE](#cs.SE) [Total: 35]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [DM-RSA: An Extension of RSA with Dual Modulus](https://arxiv.org/abs/2507.14197)
*Andriamifidisoa Ramamonjy,Rufine Marius Lalasoa*

Main category: cs.CR

TL;DR: DM-RSA is a new RSA variant using two distinct moduli and the Chinese Remainder Theorem (CRT) for decryption to improve security against side-channel attacks while retaining classical RSA efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional RSA is vulnerable to side-channel attacks that compromise private key information. The paper addresses this by proposing a cryptosystem that resists partial modulus exposure and maintains performance through symmetric use of two moduli.

Method: DM-RSA employs two unique moduli symmetrically. During decryption, it utilizes the non-CRT method for one modulus and the CRT for the other, combining results to achieve enhanced security while preserving efficient key operations.

Result: Experiments show DM-RSA increases robustness against partial modulus attacks and maintains classical RSA's efficiency. The dual modulus approach makes it more secure in environments with side-channel threats without significant performance overhead.

Conclusion: DM-RSA offers superior security in side-channel attack scenarios while being compatible with existing RSA infrastructures. The symmetric dual modulus design provides practical defense mechanisms without compromising the speed of traditional RSA implementations.

Abstract: We introduce DM-RSA (Dual Modulus RSA), a variant of the RSA cryptosystem
that employs two distinct moduli symmetrically to enhance security. By
leveraging the Chinese Remainder Theorem (CRT) for decryption, DM-RSA provides
increased robustness against side-channel attacks while preserving the
efficiency of classical RSA. This approach improves resistance to partial
compromise of a modulus and integrates easily into existing infrastructures.

</details>


### [2] [ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation](https://arxiv.org/abs/2507.14201)
*Yiran Wu,Mauricio Velazco,Andrew Zhao,Manuel Raúl Meléndez Luján,Srisuma Movva,Yogesh K Roy,Quang Nguyen,Roberto Rodriguez,Qingyun Wu,Michael Albada,Julia Kiseleva,Anand Mudgerikar*

Main category: cs.CR

TL;DR: This paper introduces ExCyTIn-Bench, the first benchmark for evaluating LLM agents in cyber threat investigation using explicit graph-based security questions.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based threat investigation lacks standardized evaluation metrics to quantify how agents process heterogeneous alert signals, follow multi-hop evidence chains, and generate validated incident reports.

Method: The authors constructed a dataset from a controlled Azure tenant with 8 simulated multi-step attacks and 57 log tables from Microsoft Sentinel. They used expert detection logic to build investigation graphs, then generated 589 questions through paired nodes (context nodes as question backgrounds and end nodes as answers).

Result: In base experiments, all models showed low average rewards (0.249 max) with the best model achieving 0.368 reward, demonstrating remaining performance gaps compared to human analysts.

Conclusion: ExCyTIn-Bench provides a reusable, extensible framework enabling automatic question generation and reinforcement learning training while highlighting the challenge of threat investigation tasks through its low baseline scores.

Abstract: We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on
the task of Cyber Threat Investigation through security questions derived from
investigation graphs. Real-world security analysts must sift through a large
number of heterogeneous alert signals and security logs, follow multi-hop
chains of evidence, and compile an incident report. With the developments of
LLMs, building LLM-based agents for automatic thread investigation is a
promising direction. To assist the development and evaluation of LLM agents, we
construct a dataset from a controlled Azure tenant that covers 8 simulated
real-world multi-step attacks, 57 log tables from Microsoft Sentinel and
related services, and 589 automatically generated questions. We leverage
security logs extracted with expert-crafted detection logic to build threat
investigation graphs, and then generate questions with LLMs using paired nodes
on the graph, taking the start node as background context and the end node as
answer. Anchoring each question to these explicit nodes and edges not only
provides automatic, explainable ground truth answers but also makes the
pipeline reusable and readily extensible to new logs. This also enables the
automatic generation of procedural tasks with verifiable rewards, which can be
naturally extended to training agents via reinforcement learning. Our
comprehensive experiments with different models confirm the difficulty of the
task: with the base setting, the average reward across all evaluated models is
0.249, and the best achieved is 0.368, leaving substantial headroom for future
research. Code and data are coming soon!

</details>


### [3] [PRM-Free Security Alignment of Large Models via Red Teaming and Adversarial Training](https://arxiv.org/abs/2507.14202)
*Pengfei Du*

Main category: cs.CR

TL;DR: Proposes PRM-free security alignment framework for LLMs using automated red teaming and adversarial training, reducing computational costs by 61% while improving robustness against adversaries.


<details>
  <summary>Details</summary>
Motivation: Current PRM-based security alignment methods for LLMs require significant computational resources and face scalability challenges, limiting their practical deployment in critical security domains.

Method: 1) Automated red teaming through genetic algorithm optimization, multi-agent simulation, and prompt mutation techniques 2) Adversarial training with curriculum learning and adaptive regularization mechanisms 3) Transparent reporting and continuous audit frameworks

Result:  Achieved superior security alignment performance compared to PRM-based approaches on five SOTA LLMs while decreasing computational costs by 61%. Demonstrated effectiveness against diverse adversarial strategies through systematic vulnerability identification.

Conclusion: The PRM-free framework enables scalable, cost-effective security alignment for resource-constrained organizations while maintaining strong adversarial robustness. Provides foundations for evolving security against emerging threats without sacrificing computational efficiency.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse applications, yet they pose significant security risks that threaten
their safe deployment in critical domains. Current security alignment
methodologies predominantly rely on Process Reward Models (PRMs) to evaluate
intermediate reasoning steps, introducing substantial computational overhead
and scalability constraints. This paper presents a novel PRM-free security
alignment framework that leverages automated red teaming and adversarial
training to achieve robust security guarantees while maintaining computational
efficiency. Our approach systematically identifies vulnerabilities through
sophisticated attack strategies including genetic algorithm optimization,
multi-agent simulation, and advanced prompt mutation techniques. The framework
enhances model robustness via targeted adversarial training with curriculum
learning and adaptive regularization mechanisms. Comprehensive experimental
evaluation across five state-of-the-art LLMs demonstrates that our method
achieves superior security alignment performance compared to PRM-based
approaches while reducing computational costs by 61\%. The framework
incorporates transparent reporting and continuous audit mechanisms that enable
iterative security improvement and regulatory compliance. Our contributions
advance the field of efficient LLM security alignment by democratizing access
to robust security measures for resource-constrained organizations and
providing a scalable foundation for addressing evolving adversarial threats.

</details>


### [4] [Mitigating Trojanized Prompt Chains in Educational LLM Use Cases: Experimental Findings and Detection Tool Design](https://arxiv.org/abs/2507.14207)
*Richard M. Charles,James H. Curry,Richard B. Charles*

Main category: cs.CR

TL;DR: This paper investigates risks in using LLMs for K--12 education, demonstrating how students can Trojanize prompts to bypass safety mechanisms in GPT-3.5 and GPT-4, and proposes a prototype detection tool called TrojanPromptGuard.


<details>
  <summary>Details</summary>
Motivation: LLM integration in education presents transformative potential but also unsafe outputs when students manipulate prompts to evade content moderation systems, necessitating robust safety measures for safe deployment.

Method: The study conducted systematic experiments using simulated K--12 queries and multi-turn dialogues to identify vulnerabilities in GPT models' safety guardrails, then developed TrojanPromptGuard as an automated detection tool.

Result: Key vulnerabilities were exposed in GPT-3.5 and GPT-4's educational safety mechanisms, including successful generation of unsafe outputs via Trojanized prompts, with TPG prototype showing initial efficacy in detection.

Conclusion: The work highlights critical security gaps in educational LLM usage and provides tangible solutions through TPG, offering actionable insights for both AI safety researchers and educational technologists to improve system trustworthiness.

Abstract: The integration of Large Language Models (LLMs) in K--12 education offers
both transformative opportunities and emerging risks. This study explores how
students may Trojanize prompts to elicit unsafe or unintended outputs from
LLMs, bypassing established content moderation systems with safety guardrils.
Through a systematic experiment involving simulated K--12 queries and
multi-turn dialogues, we expose key vulnerabilities in GPT-3.5 and GPT-4. This
paper presents our experimental design, detailed findings, and a prototype
tool, TrojanPromptGuard (TPG), to automatically detect and mitigate Trojanized
educational prompts. These insights aim to inform both AI safety researchers
and educational technologists on the safe deployment of LLMs for educators.

</details>


### [5] [Secure Goal-Oriented Communication: Defending against Eavesdropping Timing Attacks](https://arxiv.org/abs/2507.14212)
*Federico Mason,Federico Chiariotti,Pietro Talli,Andrea Zanella*

Main category: cs.CR

TL;DR: The paper introduces a new paradigm called Goal-oriented Communication (GoC) for reducing system communication by transmitting data only when necessary for the receiver to achieve objectives. However, this approach creates a timing-based side channel vulnerability exploitable by eavesdroppers to infer system states in monitoring and control scenarios. The authors analyze this attack and propose two heuristic defenses that reduce information leakage while maintaining most of the communication efficiency benefits.


<details>
  <summary>Details</summary>
Motivation: GoC reduces communication frequency but may expose timing-based side channels, which can be exploited by attackers to infer sensitive system states despite secure data content. This work addresses the critical need to understand and mitigate such vulnerabilities in remote monitoring and control systems.

Method: The authors develop a theoretical framework to quantify eavesdropping effectiveness against pull-based GoC in Markov processes and design two practical heuristic defenses. These heuristics balance communication efficiency gains with the reduction of timing-based information leakage by introducing adaptive transmission timing patterns and probabilistic update intervals.

Result: Experiments demonstrate that a naive GoC scheduler allows eavesdroppers to guess the system state 60% of the time, while the proposed heuristic defenses reduce this success rate to 30% with minimal performance degradation (e.g., 5-10% fewer transmissions saved). This indicates significant improvement in security with acceptable trade-offs for communication efficiency.

Conclusion: The study confirms that timing-based side channels in GoC are a tangible threat, even to information-theoretic secure systems, but that strategically designed heuristics for update scheduling can effectively mitigate these risks while preserving most of the communication savings.

Abstract: Goal-oriented Communication (GoC) is a new paradigm that plans data
transmission to occur only when it is instrumental for the receiver to achieve
a certain goal. This leads to the advantage of reducing the frequency of
transmissions significantly while maintaining adherence to the receiver's
objectives. However, GoC scheduling also opens a timing-based side channel that
an eavesdropper can exploit to obtain information about the state of the
system. This type of attack sidesteps even information-theoretic security, as
it exploits the timing of updates rather than their content. In this work, we
study such an eavesdropping attack against pull-based goal-oriented scheduling
for remote monitoring and control of Markov processes. We provide a theoretical
framework for defining the effectiveness of the attack and propose possible
countermeasures, including two practical heuristics that provide a balance
between the performance gains offered by GoC and the amount of leaked
information. Our results show that, while a naive goal-oriented scheduler
allows the eavesdropper to correctly guess the system state about 60% of the
time, our heuristic defenses can halve the leakage with a marginal reduction of
the benefits of goal-oriented approaches.

</details>


### [6] [Magneto-Ionic Hardware Security Primitives: Embedding Data Protection at the Material Level](https://arxiv.org/abs/2507.14213)
*Irena Spasojevic,Federica Celegato,Alessandro Magni,Paola Tiberto,Jordi Sort*

Main category: cs.CR

TL;DR: This paper introduces a magneto-ionic approach to enhance hardware security using FeCoN dots, enabling voltage-controlled N3- ion migration to create tunable ferromagnetic sublayers with stochastic magnetic properties for secure primitives like true random number generators and physical unclonable functions.


<details>
  <summary>Details</summary>
Motivation: The rise of Big Data necessitates robust, energy-efficient security hardware to counter sophisticated cyber threats, as traditional encryption methods are resource-intensive and vulnerable to attacks.

Method: Utilizes fully selective voltage-controlled N3- ion migration within paramagnetic FeCoN dots to generate ferromagnetic sublayers with controllable thickness, inducing either deterministic (single-domain/vortex) or probabilistic (coexisting magnetic configurations) states with stochastic orientation and chirality.

Result: Enables self-protected hardware primitives including true random number generators, physical unclonable functions, and in-memory probabilistic inference. The architecture demonstrates tamper resistance, low energy consumption, and scalability through reconfigurable magnetic properties.

Conclusion: This innovation represents a significant advancement toward next-generation hardware security by leveraging emergent magnetic phenomena, offering enhanced protection against hacking and counterfeiting while maintaining energy efficiency and scalability.

Abstract: The Big Data revolution has heightened the demand for robust,
energy-efficient security hardware capable of withstanding increasingly
sophisticated cyber threats. Conventional encryption schemes, reliant on
complex algorithms, are resource-intensive and remain vulnerable. To fortify
sensitive information, society needs innovative anti-hacking and
anti-counterfeiting technologies that exploit new materials and designs. Here,
we present a magneto-ionic strategy for hardware-level security based on fully
selective voltage-controlled N3- ion migration within pre-defined, initially
paramagnetic FeCoN dots. This process generates ferromagnetic sublayers of
tuneable thickness, resulting in either deterministic (single-domain or vortex)
or probabilistic states (with coexisting magnetic configurations and
voltage-adjustable probabilities), each exhibiting stochastic orientation and
chirality, thereby providing a rich platform for magnetic fingerprinting. This
approach enables self-protected primitives, including true random number
generators, physical unclonable functions, and in-memory probabilistic
inference. The resulting reconfigurable architecture combines tamper
resistance, low energy consumption, and scalability, marking a significant leap
toward next-generation hardware security rooted in emergent magnetic phenomena.

</details>


### [7] [GPU-Accelerated Interpretable Generalization for Rapid Cyberattack Detection and Forensics](https://arxiv.org/abs/2507.14222)
*Shu-Ting Huang,Wen-Cheng Chung,Hao-Ting Pai*

Main category: cs.CR

TL;DR: IG-GPU accelerates the Interpretable Generalization mechanism for intrusion detection by offloading operations to GPUs, achieving 116x speed-up over CPU implementations and enabling robust real-time performance.


<details>
  <summary>Details</summary>
Motivation: The original IG mechanism's cubic-time complexity and memory demands make it impractical for large datasets on CPUs, necessitating a solution for scalable, real-time cyber-defense.

Method: Re-architecture of IG using PyTorch to offload pairwise intersections and subset evaluations to commodity GPUs, optimized for parallel processing.

Result: On NSL-KDD (15k/148k records), IG-GPU achieves 116x CPU speed-up, 18-minute runtime for full data, and improved metrics (Recall 0.957, Precision 0.973, AUC 0.961) without down-sampling.

Conclusion: IG-GPU bridges the gap between interpretability and real-time performance for intrusion detection, enabling scalable cyber-defense systems and future optimizations via GPU hardware compatibility.

Abstract: The Interpretable Generalization (IG) mechanism recently published in IEEE
Transactions on Information Forensics and Security delivers state-of-the-art,
evidence-based intrusion detection by discovering coherent normal and attack
patterns through exhaustive intersect-and-subset operations-yet its cubic-time
complexity and large intermediate bitsets render full-scale datasets
impractical on CPUs. We present IG-GPU, a PyTorch re-architecture that offloads
all pairwise intersections and subset evaluations to commodity GPUs.
Implemented on a single NVIDIA RTX 4070 Ti, in the 15k-record NSL-KDD dataset,
IG-GPU shows a 116-fold speed-up over the multi-core CPU implementation of IG.
In the full size of NSL-KDD (148k-record), given small training data (e.g.,
10%-90% train-test split), IG-GPU runs in 18 minutes with Recall 0.957,
Precision 0.973, and AUC 0.961, whereas IG required down-sampling to
15k-records to avoid memory exhaustion and obtained Recall 0.935, Precision
0.942, and AUC 0.940. The results confirm that IG-GPU is robust across scales
and could provide millisecond-level per-flow inference once patterns are
learned. IG-GPU thus bridges the gap between rigorous interpretability and
real-time cyber-defense, offering a portable foundation for future work on
hardware-aware scheduling, multi-GPU sharding, and dataset-specific sparsity
optimizations.

</details>


### [8] [Multi-Granular Discretization for Interpretable Generalization in Precise Cyberattack Identification](https://arxiv.org/abs/2507.14223)
*Wen-Cheng Chung,Shu-Ting Huang,Hao-Ting Pai*

Main category: cs.CR

TL;DR: The paper introduces Multi-Granular Discretization (IG-MD), an enhancement to the Interpretable Generalization (IG) mechanism, to improve precision in explainable intrusion detection systems while maintaining rule-based interpretability and domain adaptability.


<details>
  <summary>Details</summary>
Motivation: Current explainable intrusion detection systems (IDS) often rely on post-hoc explanations from opaque classifiers, resulting in incomplete or misleading insights. This hinders effective analysis and decision-making in mission-critical networks.

Method: IG-MD combines the IG mechanism's coherent pattern learning (converting feature combinations into auditable rules) with Multi-Granular Discretization, which models continuous features at multiple Gaussian-based resolutions to capture nuanced traffic characteristics.

Result: On UKM-IDS20, IG-MD achieves precision gains of ≥4% across all train-test splits with near-perfect recall (≈1.0). It maintains strong performance on NSL-KDD and UNSW-NB15 at 10% training data utilization.

Conclusion: IG-MD demonstrates that high-precision, transparent intrusion detection can be achieved through multi-resolution feature modeling, enabling a single interpretation-ready model to adapt across domains without manual tuning.

Abstract: Explainable intrusion detection systems (IDS) are now recognized as essential
for mission-critical networks, yet most "XAI" pipelines still bolt an
approximate explainer onto an opaque classifier, leaving analysts with partial
and sometimes misleading insights. The Interpretable Generalization (IG)
mechanism, published in IEEE Transactions on Information Forensics and
Security, eliminates that bottleneck by learning coherent patterns - feature
combinations unique to benign or malicious traffic - and turning them into
fully auditable rules. IG already delivers outstanding precision, recall, and
AUC on NSL-KDD, UNSW-NB15, and UKM-IDS20, even when trained on only 10% of the
data. To raise precision further without sacrificing transparency, we introduce
Multi-Granular Discretization (IG-MD), which represents every continuous
feature at several Gaussian-based resolutions. On UKM-IDS20, IG-MD lifts
precision by greater than or equal to 4 percentage points across all nine
train-test splits while preserving recall approximately equal to 1.0,
demonstrating that a single interpretation-ready model can scale across domains
without bespoke tuning.

</details>


### [9] [Using Modular Arithmetic Optimized Neural Networks To Crack Affine Cryptographic Schemes Efficiently](https://arxiv.org/abs/2507.14229)
*Vanja Stojanović,Žiga Lesar,CIril Bohak*

Main category: cs.CR

TL;DR: A hybrid neural network combining modular arithmetic-aware and statistical feature-based learning outperforms purely statistical approaches for affine cipher cryptanalysis on short/medium ciphertexts, but struggles with long texts.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need to improve affine cipher cryptanalysis by integrating interpretable modular arithmetic capabilities with statistical frequency analysis, inspired by recent advancements in neural cryptanalysis and modular arithmetic modeling.

Method: The architecture employs a dual-branch system: a modular branch processing raw ciphertext sequences with arithmetic operations and a statistical branch using letter frequency features. These branches are trained jointly for key recovery.

Result: The hybrid model achieves high key recovery accuracy for short (e.g., 80-100 characters) and moderate-length ciphertexts, while purely statistical baselines fail to match performance in these settings. However, accuracy degrades for very long ciphertexts.

Conclusion: The hybrid approach demonstrates advantages for affine ciphers where arithmetic properties are critical, but exposes limitations in model generalization for extended sequences. This highlights the potential of neural networks with domain-knowledge integration for classical cryptanalysis while indicating challenges in scalable performance.

Abstract: We investigate the cryptanalysis of affine ciphers using a hybrid neural
network architecture that combines modular arithmetic-aware and statistical
feature-based learning. Inspired by recent advances in interpretable neural
networks for modular arithmetic and neural cryptanalysis of classical ciphers,
our approach integrates a modular branch that processes raw ciphertext
sequences and a statistical branch that leverages letter frequency features.
Experiments on datasets derived from natural English text demonstrate that the
hybrid model attains high key recovery accuracy for short and moderate
ciphertexts, outperforming purely statistical approaches for the affine cipher.
However, performance degrades for very long ciphertexts, highlighting
challenges in model generalization.

</details>


### [10] [Breaking the Illusion of Security via Interpretation: Interpretable Vision Transformer Systems under Attack](https://arxiv.org/abs/2507.14248)
*Eldor Abdukhamidov,Mohammed Abuhamad,Simon S. Woo,Hyoungshick Kim,Tamer Abuhmed*

Main category: cs.CR

TL;DR: AdViT adversarial examples degrade both ViT model and interpretability, achieving 100% success rates in white-box/black-box scenarios with up to 98%/76% misclassification confidence.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial attacks on Vision Transformers (ViT) ignore the effect on model interpolations despite their role in detection. Transformer-interpreter combinations are presumed secure, but this assumption warrants reevaluation.

Method: Proposed AdViT attacks that optimize perturbations to simultaneously mislead ViT models and their coupled interpreters through a unified loss function combining classification and interpretation features.

Result: Achieved 100% attack success rate across 4 ViT variants and two interpreters (GradCAM and Class Activation Mapping), maintaining high confidence (98% white-box / 76% black-box) while producing naturalistic, undetectable perturbations. Perturbations caused consistent misclassification across model/interpreter pairs.

Conclusion: Even when paired with interpretation models, ViT systems remain vulnerable to adversaries that manipulate both visual classification and interpretability patterns, suggesting current security assumptions for transformer models are insufficient and detection methods need stronger robustness analysis.

Abstract: Vision transformer (ViT) models, when coupled with interpretation models, are
regarded as secure and challenging to deceive, making them well-suited for
security-critical domains such as medical applications, autonomous vehicles,
drones, and robotics. However, successful attacks on these systems can lead to
severe consequences. Recent research on threats targeting ViT models primarily
focuses on generating the smallest adversarial perturbations that can deceive
the models with high confidence, without considering their impact on model
interpretations. Nevertheless, the use of interpretation models can effectively
assist in detecting adversarial examples. This study investigates the
vulnerability of transformer models to adversarial attacks, even when combined
with interpretation models. We propose an attack called "AdViT" that generates
adversarial examples capable of misleading both a given transformer model and
its coupled interpretation model. Through extensive experiments on various
transformer models and two transformer-based interpreters, we demonstrate that
AdViT achieves a 100% attack success rate in both white-box and black-box
scenarios. In white-box scenarios, it reaches up to 98% misclassification
confidence, while in black-box scenarios, it reaches up to 76%
misclassification confidence. Remarkably, AdViT consistently generates accurate
interpretations in both scenarios, making the adversarial examples more
difficult to detect.

</details>


### [11] [Quantum-Safe Identity Verification using Relativistic Zero-Knowledge Proof Systems](https://arxiv.org/abs/2507.14324)
*Yao Ma,Wen Yu Kon,Jefferson Chu,Kevin Han Yong Loh,Kaushik Chakraborty,Charles Lim*

Main category: cs.CR

TL;DR: This paper advances relativistic zero-knowledge proof (RZKP) for secure identity verification by relaxing distance constraints, enhancing stability/scalability for near-term use, and proposing a three-prover configuration with upper bounds for multi-round protocols.


<details>
  <summary>Details</summary>
Motivation: Current password/PIN identity systems are vulnerable to phishing/skimming attacks. RZKPs enable proving identity without disclosing secrets, but existing implementations face stability, scalability, and long-term security challenges against quantum adversaries.

Method: 1. Relaxed relativistic constraints from 60m to 30m
2. Modified 2-prover graph coloring RZKP protocol for improved stability/scalability
3. Established soundness parameter bounds for the modified protocol
4. Extended the protocol to a three-prover configuration and analyzed its security against entangled malicious provers

Result: Demonstrated improved practicality of RZKPs through experimental enhancements (lower constraints + 30% stability/15x scalability improvements). The three-prover setup achieves security in multi-round protocols with ω(G) ≤ 3/2^k for k rounds, while maintaining comparable protocol costs.

Conclusion: The work provides critical engineering advancements for immediate RZKP deployment and establishes a theoretical framework to adapt to future quantum threats. The three-prover extension validates the robustness of relativistic principles in securing collaborative verification against sophisticated adversarial entanglements.

Abstract: Identity verification is the process of confirming an individual's claimed
identity, which is essential in sectors like finance, healthcare, and online
services to ensure security and prevent fraud. However, current
password/PIN-based identity solutions are susceptible to phishing or skimming
attacks, where malicious intermediaries attempt to steal credentials using fake
identification portals. Alikhani et al. [Nature, 2021] began exploring identity
verification through graph coloring-based relativistic zero-knowledge proofs
(RZKPs), a key cryptographic primitive that enables a prover to demonstrate
knowledge of secret credentials to a verifier without disclosing any
information about the secret. Our work advances this field and addresses
unresolved issues: From an engineering perspective, we relax further the
relativistic constraints from 60m to 30m, and significantly enhance the
stability and scalability of the experimental demonstration of the 2-prover
graph coloring-based RZKP protocol for near-term use cases. At the same time,
for long-term security against entangled malicious provers, we propose a
modified protocol with comparable computation and communication costs, we
establish an upper bound on the soundness parameter for this modified protocol.
On the other hand, we extend the two-prover, two-verifier setup to a
three-prover configuration, demonstrating the security of such relativistic
protocols against entangled malicious provers.

</details>


### [12] [Towards Efficient Privacy-Preserving Machine Learning: A Systematic Review from Protocol, Model, and System Perspectives](https://arxiv.org/abs/2507.14519)
*Wenxuan Zeng,Tianshi Xu,Yi Chen,Yifan Zhou,Mingzhe Zhang,Jin Tan,Cheng Hong,Meng Li*

Main category: cs.CR

TL;DR: This survey systematically reviews recent PPML research focusing on cross-level optimizations (protocol, model, system), provides comparative insights, and highlights integration of optimizations for efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing PPML protocols face significant efficiency and scalability challenges due to computational overheads compared to plaintext alternatives.

Method: Categorization and analysis of PPML studies into three optimization levels (protocol, model, system) with qualitative and quantitative comparisons.

Result: Technical insights on optimization progress, comparative analysis of existing works, and identification of research opportunities through multi-level integration.

Conclusion: Achieving efficient PPML requires coordinated optimizations across protocol, model, and system layers, supported by ongoing tracking through provided GitHub repository (https://github.com/PKU-SEC-Lab/Awesome-PPML-Papers).

Abstract: Privacy-preserving machine learning (PPML) based on cryptographic protocols
has emerged as a promising paradigm to protect user data privacy in cloud-based
machine learning services. While it achieves formal privacy protection, PPML
often incurs significant efficiency and scalability costs due to orders of
magnitude overhead compared to the plaintext counterpart. Therefore, there has
been a considerable focus on mitigating the efficiency gap for PPML. In this
survey, we provide a comprehensive and systematic review of recent PPML studies
with a focus on cross-level optimizations. Specifically, we categorize existing
papers into protocol level, model level, and system level, and review progress
at each level. We also provide qualitative and quantitative comparisons of
existing works with technical insights, based on which we discuss future
research directions and highlight the necessity of integrating optimizations
across protocol, model, and system levels. We hope this survey can provide an
overarching understanding of existing approaches and potentially inspire future
breakthroughs in the PPML field. As the field is evolving fast, we also provide
a public GitHub repository to continuously track the developments, which is
available at https://github.com/PKU-SEC-Lab/Awesome-PPML-Papers.

</details>


### [13] [FORTA: Byzantine-Resilient FL Aggregation via DFT-Guided Krum](https://arxiv.org/abs/2507.14588)
*Usayd Shahul,J. Harshan*

Main category: cs.CR

TL;DR: FORTA is a real-domain Byzantine-resilient secure aggregation framework that uses DFT codes for privacy and refines Krum outlier detection with feedback, achieving improved robustness and accuracy compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Finite-field based secure aggregation schemes (like that of Jinhyun So et al.) suffer from numerical errors and overflows when handling real-valued model updates, limiting their effectiveness for practical machine learning applications.

Method: We propose FORTA which operates entirely in the real domain, combining Discrete Fourier Transform (DFT) codes for privacy preservation with a feedback-modified Krum algorithm that improves Byzantine user detection by incorporating information from the DFT decoding process.

Result: Theoretical analysis and experiments demonstrate that our feedback-enhanced Krum variant provides better robustness and produces more accurate aggregations than standard Krum in real-domain settings.

Conclusion: FORTA addresses critical limitations of finite-field approaches while maintaining security against both privacy breaches and Byzantine attacks, establishing a more practical and effective secure aggregation framework for federated learning.

Abstract: Secure federated learning enables collaborative model training across
decentralized users while preserving data privacy. A key component is secure
aggregation, which keeps individual updates hidden from both the server and
users, while also defending against Byzantine users who corrupt the
aggregation. To this end, Jinhyun So et al. recently developed a
Byzantine-resilient secure aggregation scheme using a secret-sharing strategy
over finite-field arithmetic. However, such an approach can suffer from
numerical errors and overflows when applied to real-valued model updates,
motivating the need for secure aggregation methods that operate directly over
the real domain. We propose FORTA, a Byzantine-resilient secure aggregation
framework that operates entirely in the real domain. FORTA leverages Discrete
Fourier Transform (DFT) codes for privacy and employs Krum-based outlier
detection for robustness. While DFT decoder is error-free under infinite
precision, finite precision introduces numerical perturbations that can distort
distance estimates and allow malicious updates to evade detection. To address
this, FORTA refines Krum using feedback from DFT decoder, improving the
selection of trustworthy updates. Theoretical analysis and experiments show
that our modification of Krum offers improved robustness and more accurate
aggregation than standard Krum.

</details>


### [14] [Hybrid Classical-Quantum Rainbow Table Attack on Human Passwords](https://arxiv.org/abs/2507.14600)
*MA. Khajeian*

Main category: cs.CR

TL;DR: A hybrid attack combines classical rainbow tables with efficient quantum search to break long human-generated passwords.


<details>
  <summary>Details</summary>
Motivation: Long human-generated passwords have irregular structures and large search spaces, challenging classical and quantum attacks. Existing solutions lack efficiency in modeling user behavior and noise resilience.

Method: 1) Build dictionary-based rainbow tables with transformation rules to simulate human password patterns. 2) Bucket organization for faster lookup. 3) Use distributed exact Grover algorithm to reduce circuit depth and ensure deterministic results.

Result: Achieved shallower quantum circuits with lower noise vulnerability (especially against depolarizing channels) while improving password recovery efficiency through classical-quantum hybridization.

Conclusion: Proposed a robust hybrid framework that models user behavior better and enhances password recovery effectiveness for long human-generated passwords in noisy quantum environments.

Abstract: Passwords that are long and human-generated pose a challenge for both
classical and quantum attacks due to their irregular structure and large search
space. In this work, we present an enhanced classical-quantum hybrid attack
tailored to this scenario. We build rainbow tables using dictionary-based
password generation with transformation rules to better model real user
behavior. These tables are then organized into buckets, enabling faster lookup
and reduced space complexity. To perform quantum search within each bucket, we
use a distributed exact variant of Grover's algorithm, which offers lower
circuit depth and deterministic success. As a result, the overall quantum
circuit is shallower and more robust against noise, particularly from
depolarizing channels commonly found in near-term quantum devices. Through this
work, Overall, we propose a hybrid framework that combines structured rainbow
tables with efficient quantum search to enhance password recovery.

</details>


### [15] [VTarbel: Targeted Label Attack with Minimal Knowledge on Detector-enhanced Vertical Federated Learning](https://arxiv.org/abs/2507.14625)
*Juntao Tan,Anran Li,Quanchao Liu,Peng Ran,Lan Zhang*

Main category: cs.CR

TL;DR: This paper introduces VTarbel, a two-stage attack framework targeting label security in vertical federated learning (VFL) that outperforms existing methods by evading real-world anomaly detectors, revealing critical vulnerabilities in current VFL deployments.


<details>
  <summary>Details</summary>
Motivation: Existing VFL security attacks rely on unrealistic assumptions (e.g., model output access) and ignore practical anomaly detection systems, creating a need for minimal-knowledge attacks that mimic real-world scenarios.

Method: The attack consists of: 1) Preparation stage selecting high-expressiveness samples using MMD to train local pseudo-models via VFL outputs 2) Attack stage using gradient-based perturbations guided by estimated models to craft adversarial samples while evading detectors.

Result: VTarbel achieves superior attack success rates (vs four SOTA baselines) across 4 model architectures, 7 multimodal datasets, and 2 detectors, maintaining effectiveness against three privacy-preserving defenses without increasing detection rates.

Conclusion: The work exposes significant security blind spots in VFL systems with detectors, stressing the urgent need for robust, attack-aware defense mechanisms in practical federated learning deployments.

Abstract: Vertical federated learning (VFL) enables multiple parties with disjoint
features to collaboratively train models without sharing raw data. While
privacy vulnerabilities of VFL are extensively-studied, its security
threats-particularly targeted label attacks-remain underexplored. In such
attacks, a passive party perturbs inputs at inference to force
misclassification into adversary-chosen labels. Existing methods rely on
unrealistic assumptions (e.g., accessing VFL-model's outputs) and ignore
anomaly detectors deployed in real-world systems. To bridge this gap, we
introduce VTarbel, a two-stage, minimal-knowledge attack framework explicitly
designed to evade detector-enhanced VFL inference. During the preparation
stage, the attacker selects a minimal set of high-expressiveness samples (via
maximum mean discrepancy), submits them through VFL protocol to collect
predicted labels, and uses these pseudo-labels to train estimated detector and
surrogate model on local features. In attack stage, these models guide
gradient-based perturbations of remaining samples, crafting adversarial
instances that induce targeted misclassifications and evade detection. We
implement VTarbel and evaluate it against four model architectures, seven
multimodal datasets, and two anomaly detectors. Across all settings, VTarbel
outperforms four state-of-the-art baselines, evades detection, and retains
effective against three representative privacy-preserving defenses. These
results reveal critical security blind spots in current VFL deployments and
underscore urgent need for robust, attack-aware defenses.

</details>


### [16] [VMask: Tunable Label Privacy Protection for Vertical Federated Learning via Layer Masking](https://arxiv.org/abs/2507.14629)
*Juntao Tan,Lan Zhang,Zhonghao Hu,Kai Yang,Peng Ran,Bo Li*

Main category: cs.CR

TL;DR: This paper proposes VMask, a novel label privacy protection framework for vertical federated learning (VFL) that disrupts the correlation between input data and intermediate outputs through layer masking with secret sharing. It achieves a strong privacy-utility trade-off, reduces label inference accuracy to random guessing levels, and improves runtime efficiency compared to cryptography-based methods.


<details>
  <summary>Details</summary>
Motivation: Current VFL systems are vulnerable to powerful label inference attacks like model completion (MC), while existing defenses either degrade model accuracy or require impractical computational resources.

Method: VMask applies secret sharing (SS) to mask only strategically selected critical layers in the attacker's model rather than the entire architecture, enabling tunable privacy budgets for flexible defense strength.

Result: Extensive experiments on 5 model architectures and 13 datasets demonstrate VMask reduces MC attack accuracy to random guessing levels with 0.09% average accuracy loss in Transformer models. It is 60,846× faster than cryptography-based methods and has 1.8× higher runtime than standard VFL in large models.

Conclusion: VMask represents a practical and effective solution for protecting label privacy in VFL by combining layer masking with secret sharing, achieving state-of-the-art privacy-utility trade-offs with tunable parameters and significant computational advantages over prior approaches.

Abstract: Though vertical federated learning (VFL) is generally considered to be
privacy-preserving, recent studies have shown that VFL system is vulnerable to
label inference attacks originating from various attack surfaces. Among these
attacks, the model completion (MC) attack is currently the most powerful one.
Existing defense methods against it either sacrifice model accuracy or incur
impractical computational overhead. In this paper, we propose VMask, a novel
label privacy protection framework designed to defend against MC attack from
the perspective of layer masking. Our key insight is to disrupt the strong
correlation between input data and intermediate outputs by applying the secret
sharing (SS) technique to mask layer parameters in the attacker's model. We
devise a strategy for selecting critical layers to mask, reducing the overhead
that would arise from naively applying SS to the entire model. Moreover, VMask
is the first framework to offer a tunable privacy budget to defenders, allowing
for flexible control over the levels of label privacy according to actual
requirements. We built a VFL system, implemented VMask on it, and extensively
evaluated it using five model architectures and 13 datasets with different
modalities, comparing it to 12 other defense methods. The results demonstrate
that VMask achieves the best privacy-utility trade-off, successfully thwarting
the MC attack (reducing the label inference accuracy to a random guessing
level) while preserving model performance (e.g., in Transformer-based model,
the averaged drop of VFL model accuracy is only 0.09%). VMask's runtime is up
to 60,846 times faster than cryptography-based methods, and it only marginally
exceeds that of standard VFL by 1.8 times in a large Transformer-based model,
which is generally acceptable.

</details>


### [17] [CANDoSA: A Hardware Performance Counter-Based Intrusion Detection System for DoS Attacks on Automotive CAN bus](https://arxiv.org/abs/2507.14739)
*Franco Oberti,Stefano Di Carlo,Alessandro Savino*

Main category: cs.CR

TL;DR: A new Intrusion Detection System using Hardware Performance Counters is proposed to enhance CAN security in autonomous vehicles.


<details>
  <summary>Details</summary>
Motivation: The CAN protocol's lack of built-in security features poses increasing cybersecurity risks, especially in autonomous vehicles, as traditional measures like encryption and authentication only provide partial protection.

Method: The researchers simulated a RISC-V-based CAN receiver using gem5, implemented AES-128 encrypted payloads as FreeRTOS tasks, and optimized HPC features through data extraction and correlation analysis for anomaly detection.

Result: Experimental results demonstrate the approach's potential to significantly improve CAN security and address emerging automotive cybersecurity challenges.

Conclusion: This work advances CAN intrusion detection by effectively utilizing hardware performance metrics for abnormal behavior identification in encrypted automotive communication environments.

Abstract: The Controller Area Network (CAN) protocol, essential for automotive embedded
systems, lacks inherent security features, making it vulnerable to cyber
threats, especially with the rise of autonomous vehicles. Traditional security
measures offer limited protection, such as payload encryption and message
authentication. This paper presents a novel Intrusion Detection System (IDS)
designed for the CAN environment, utilizing Hardware Performance Counters
(HPCs) to detect anomalies indicative of cyber attacks. A RISC-V-based CAN
receiver is simulated using the gem5 simulator, processing CAN frame payloads
with AES-128 encryption as FreeRTOS tasks, which trigger distinct HPC
responses. Key HPC features are optimized through data extraction and
correlation analysis to enhance classification efficiency. Results indicate
that this approach could significantly improve CAN security and address
emerging challenges in automotive cybersecurity.

</details>


### [18] [Careful Whisper: Attestation for peer-to-peer Confidential Computing networks](https://arxiv.org/abs/2507.14796)
*Ceren Kocaoğullar,Gustavo Petri,Dominic P. Mulligan,Derek Miller,Hugo J. M. Vincent,Shale Xiong,Alastair R. Beresford*

Main category: cs.CR

TL;DR: This paper introduces Careful Whisper, a gossip-based protocol reducing TEE attestation overhead from quadratic to linear in dynamic peer-to-peer networks while enabling transitive trust and supporting offline nodes.


<details>
  <summary>Details</summary>
Motivation: Vehicular ad hoc networks require secure collaboration without compromising confidentiality, but direct TEE attestation between all nodes leads to inefficient quadratic communication overhead, especially with frequent node joins/leaves.

Method: Careful Whisper uses gossip-based dissemination of attestation evidence for transitive trust, relayed attestations for offline nodes, and evaluates performance via a custom discrete-event simulator across various network topologies.

Result: Careful Whisper achieves linear attestation complexity, propagates trust faster/wider than naive methods, uses ~21.5 KiB of data (200-node network) with 0.158s per round, and maintains resilience to attestation failures across diverse topologies.

Conclusion: The protocol effectively addresses TEE attestation inefficiencies in dynamic networks by providing resource-efficient, wide-reaching trust establishment with fault tolerance, enabling scalable secure collaboration in heterogeneous environments.

Abstract: Trusted Execution Environments (TEEs) are designed to protect the privacy and
integrity of data in use. They enable secure data processing and sharing in
peer-to-peer networks, such as vehicular ad hoc networks of autonomous
vehicles, without compromising confidentiality. In these networks, nodes must
establish mutual trust to collaborate securely. TEEs can achieve this through
remote attestation, where a prover presents evidence of its trustworthiness to
a verifier, which then decides whether or not to trust the prover. However, a
naive peer-to-peer attestation approach, where every TEE directly attests every
other TEE, results in quadratic communication overhead. This is inefficient in
dynamic environments, where nodes frequently join and leave the network.
  To address this, we present Careful Whisper, a gossip-based protocol that
disseminates trust efficiently, reducing attestation overhead to linear
complexity under ideal conditions. It enables interoperability by enabling
transitive trust across heterogeneous networks, and supports trust
establishment with offline nodes via relayed attestations. Using a custom
discrete-event simulator, we show that Careful Whisper propagates trust both
faster and more widely than naive approaches across various network topologies.
Our results demonstrate that our protocol is resource efficient, sending ~21.5
KiB and requiring 0.158 seconds per round in a 200-node network, and that our
protocol is resilient to attestation failures across various network
topologies.

</details>


### [19] [Manipulating LLM Web Agents with Indirect Prompt Injection Attack via HTML Accessibility Tree](https://arxiv.org/abs/2507.14799)
*Sam Johnson,Viet Pham,Thai Le*

Main category: cs.CR

TL;DR: This paper shows that LLM-based web navigation agents are vulnerable to Indirect Prompt Injection (IPI) attacks through adversarial HTML triggers, demonstrating high success rates in hijacking agent behavior for malicious actions like credential theft and forced ad clicks.


<details>
  <summary>Details</summary>
Motivation: The research highlights critical security risks associated with the growing adoption of LLM-driven autonomous web agents, emphasizing the need for robust defenses against potential adversarial manipulations.

Method: The authors use the Greedy Coordinate Gradient (GCG) algorithm to craft universal adversarial triggers in webpage HTML. These triggers exploit accessibility tree parsing mechanisms in agents, particularly testing with a Browser Gym agent powered by Llama-3.1.

Result: Empirical results demonstrate high attack success rates on real websites, showing effectiveness of both targeted (e.g., credential exfiltration) and general attacks (e.g., forced ad clicks) using LLM-based agents with standard parsing tools.

Conclusion: The study reveals significant vulnerabilities in LLM-driven web agents due to IPI attacks through HTML manipulation. The authors release their system software and demo website to promote awareness and encourage development of stronger defensive mechanisms.

Abstract: This work demonstrates that LLM-based web navigation agents offer powerful
automation capabilities but are vulnerable to Indirect Prompt Injection (IPI)
attacks. We show that adversaries can embed universal adversarial triggers in
webpage HTML to hijack agent behavior that utilizes the accessibility tree to
parse HTML, causing unintended or malicious actions. Using the Greedy
Coordinate Gradient (GCG) algorithm and a Browser Gym agent powered by
Llama-3.1, our system demonstrates high success rates across real websites in
both targeted and general attacks, including login credential exfiltration and
forced ad clicks. Our empirical results highlight critical security risks and
the need for stronger defenses as LLM-driven autonomous web agents become more
widely adopted. The system software
(https://github.com/sej2020/manipulating-web-agents) is released under the MIT
License, with an accompanying publicly available demo website
(http://lethaiq.github.io/attack-web-llm-agent).

</details>


### [20] [Quantum Skyshield: Quantum Key Distribution and Post-Quantum Authentication for Low-Altitude Wireless Networks in Adverse Skies](https://arxiv.org/abs/2507.14822)
*Zeeshan Kaleem,Misha Urooj Khan,Ahmad Suleman,Waqas Khalid,Kai-Kit Wong,Chau Yuen*

Main category: cs.CR

TL;DR: The paper proposes Quantum Skyshield, a quantum-secure architecture for low-altitude wireless networks (LAWNs) that combines BB84 QKD with post-quantum authentication to address security vulnerabilities and environmental challenges in FSO links.


<details>
  <summary>Details</summary>
Motivation: LAWNs, crucial for the low-altitude economy with UAVs/HAPs, face security risks (interception/spoofing) and reliability issues (turbulence, misalignment, weather attenuation) in traditional RF and FSO communication as data demands grow.

Method: Quantum Skyshield integrates BB84 quantum key distribution (QKD) for symmetric key generation, Lamport one-time signatures and HMAC for authentication, and a Grover-inspired threat detection mechanism for anomaly identification.

Result: Simulations validate a 128-bit symmetric key generation at QBER <11%; authentication ensures message integrity with post-quantum methods; Grover-based detection identifies anomalies with ~89% probability in real-time trust evaluation.

Conclusion: Quantum Skyshield demonstrates feasibility for secure LAWN communication but highlights open challenges (e.g., improving detection probability, practical deployment hurdles) to drive future research.

Abstract: Recently, low-altitude wireless networks (LAWNs) have emerged as a critical
backbone for supporting the low-altitude economy, particularly with the
densification of unmanned aerial vehicles (UAVs) and high-altitude platforms
(HAPs). To meet growing data demands, some LAWN deployments incorporate
free-space optical (FSO) links, which offer exceptional bandwidth and beam
directivity. However, without strong security measures in place, both
conventional radio frequency channels and FSO beams remain vulnerable to
interception and spoofing and FSO in particular can suffer from turbulence,
misalignment, and weather-related attenuation. To address these challenges in
the quantum era, a quantum-secure architecture called Quantum Skyshield is
proposed to enable reliable communication between the base transceiver station
(BTS) and LAWN. The proposed design integrates BB84 quantum key distribution
(QKD) with post-quantum authentication mechanisms. Simulation results confirm
the reliable generation of a 128-bit symmetric key when the quantum bit error
rate (QBER) remains below the threshold of 11%. Authentication is enforced
using Lamport one-time signatures and hash-based message authentication codes
(HMAC) to ensure message integrity. A Grover-inspired threat detection
mechanism identifies anomalies with up to 89% probability in a single
iteration, enabling real-time trust evaluation. Lastly, future research
challenges have also been identified and discussed to guide further development
in this area.

</details>


### [21] [A Privacy-Centric Approach: Scalable and Secure Federated Learning Enabled by Hybrid Homomorphic Encryption](https://arxiv.org/abs/2507.14853)
*Khoa Nguyen,Tanveer Khan,Antonis Michalas*

Main category: cs.CR

TL;DR: This paper proposes integrating Hybrid Homomorphic Encryption (HHE) with Federated Learning (FL) to reduce communication overhead while enhancing privacy, addressing limitations of traditional encryption methods. Motivation: FL is privacy-prioritizing but faces high costs in communication and computation due to existing privacy-preserving techniques like Homomorphic Encryption (HE). Method: A hybrid approach combining symmetric encryption (for efficiency) with HE (for security) is explored within FL's collaborative framework. Result: Demonstrates HHE's effectiveness in mitigating both communication and privacy challenges in FL. Conclusion: HHE enables scalable, secure decentralized learning by maintaining confidentiality while minimizing computational/communication burdens.


<details>
  <summary>Details</summary>
Motivation: Federated Learning (FL) is crucial for privacy-sensitive domains but hindered by communication/privacy trade-offs. Traditional HE-based privacy-preserving techniques, while secure, introduce high computational and communication overhead, limiting real-world deployment.

Method: The paper investigates integrating Hybrid Homomorphic Encryption (HHE), which combines symmetric encryption's efficiency with homomorphic encryption's privacy guarantees, into FL's collaborative model training architecture.

Result: HHE significantly reduces encryption-related costs compared to HE while preserving data privacy, enabling FL systems to achieve practical scalability without compromising security.

Conclusion: HHE presents a viable solution to reconcile FL's communication and privacy challenges, advancing the feasibility of secure decentralized learning in high-stakes environments.

Abstract: Federated Learning (FL) enables collaborative model training without sharing
raw data, making it a promising approach for privacy-sensitive domains. Despite
its potential, FL faces significant challenges, particularly in terms of
communication overhead and data privacy. Privacy-preserving Techniques (PPTs)
such as Homomorphic Encryption (HE) have been used to mitigate these concerns.
However, these techniques introduce substantial computational and communication
costs, limiting their practical deployment. In this work, we explore how Hybrid
Homomorphic Encryption (HHE), a cryptographic protocol that combines symmetric
encryption with HE, can be effectively integrated with FL to address both
communication and privacy challenges, paving the way for scalable and secure
decentralized learning system.

</details>


### [22] [A Compact Post-quantum Strong Designated Verifier Signature Scheme from Isogenies](https://arxiv.org/abs/2507.14893)
*Farzin Renan*

Main category: cs.CR

TL;DR: This paper introduces CSI-SDVS, a compact isogeny-based post-quantum secure Strong Designated Verifier Signature (SDVS) scheme. It addresses the need for privacy in applications like e-voting and digital cash by using ideal class group actions from CSIDH and techniques from CSI-FiSh, achieving SUF-CMA, NT, and PSI security while reducing key and signature sizes from O(λ²) to O(λ).


<details>
  <summary>Details</summary>
Motivation: Traditional SDVS schemes relying on number-theoretic assumptions are vulnerable to quantum attacks, while existing post-quantum alternatives (e.g., lattice-based) suffer from large key and signature sizes. Privacy-sensitive applications require both quantum resistance and compact verification models.

Method: Leverages the ideal class group action framework from CSIDH and signature techniques from CSI-FiSh. Security is based on the Multi-Target Group Action Inverse Problem (MT-GAIP) hardness assumption.

Result: Achieves SUF-CMA, Non-Transferability (NT), and Privacy of Signer's Identity (PSI) in the random oracle model. Key and signature sizes are O(λ), a significant improvement over existing post-quantum SDVS schemes which require O(λ²) size.

Conclusion: CSI-SDVS is the first isogeny-based post-quantum secure SDVS scheme with compact keys and signatures, making it the most efficient PQC-based SDVS to date.

Abstract: Digital signatures are essential cryptographic tools that provide
authentication and integrity in digital communications. However,
privacy-sensitive applications, such as e-voting and digital cash, require more
restrictive verification models to ensure confidentiality and control. Strong
Designated Verifier Signature (SDVS) schemes address this need by enabling the
signer to designate a specific verifier, ensuring that only this party can
validate the signature. Existing SDVS constructions are primarily based on
number-theoretic assumptions and are therefore vulnerable to quantum attacks.
Although post-quantum alternatives, particularly those based on lattices, have
been proposed, they often entail large key and signature sizes. In this work,
we introduce $\mathsf{CSI\text{-}SDVS}$, a novel isogeny-based SDVS scheme that
offers a compact, quantum-resistant alternative. Our construction builds on the
ideal class group action framework of CSIDH and the signature techniques of
CSI-FiSh, and relies on the hardness of the Multi-Target Group Action Inverse
Problem (MT-GAIP). $\mathsf{CSI\text{-}SDVS}$ achieves strong security
guarantees; namely, Strong Unforgeability under Chosen-Message Attacks
(SUF-CMA), Non-Transferability (NT), and Privacy of Signer's Identity (PSI), in
the random oracle model. Remarkably, both the keys and signatures in
$\mathsf{CSI\text{-}SDVS}$ are of size $\mathcal{O}(\lambda)$, representing a
significant improvement over the typical $\mathcal{O}(\lambda^2)$ bounds in
existing post-quantum SDVS schemes, thereby making it among the most compact
PQC-based SDVS schemes and the only post-quantum secure construction based on
isogenies.

</details>


### [23] [Metaverse Security and Privacy Research: A Systematic Review](https://arxiv.org/abs/2507.14985)
*Argianto Rahartomo,Leonel Merino,Mohammad Ghafari*

Main category: cs.CR

TL;DR: This paper systematically reviews 2013-2024 literature on metaverse security and privacy, identifying rising research focus on practical/user-centered approaches but highlighting critical gaps in policy compliance and infrastructure security.


<details>
  <summary>Details</summary>
Motivation: Metaverse technologies' sociotechnical complexity and pervasive data collection create urgent security/privacy risks requiring structured academic analysis to guide future research and practice.

Method: The authors conducted a systematic literature review, categorizing 128 studies by methodology (benchmarking/human experimentation/qualitative), analyzed security properties (authentication, unobservability), and evaluation strategies.

Result: Found 128% annual growth in metaverse security research from 2019-2023, with 75% using practical/user-centered methods. 62% focus on authentication/unobservability, but only 12% address policy compliance, 9% accessibility, and 5% infrastructure security.

Conclusion: The review underscores the need for integrated technical/social research approaches to address metaverse security gaps, particularly in policy mechanisms, accessibility design, and backend infrastructure protections to ensure trustworthy immersive environments.

Abstract: The rapid growth of metaverse technologies, including virtual worlds,
augmented reality, and lifelogging, has accelerated their adoption across
diverse domains. This rise exposes users to significant new security and
privacy challenges due to sociotechnical complexity, pervasive connectivity,
and extensive user data collection in immersive environments. We present a
systematic review of the literature published between 2013 and 2024, offering a
comprehensive analysis of how the research community has addressed
metaverse-related security and privacy issues over the past decade. We organize
the studies by method, examined the security and privacy properties, immersive
components, and evaluation strategies. Our investigation reveals a sharp
increase in research activity in the last five years, a strong focus on
practical and user-centered approaches, and a predominant use of benchmarking,
human experimentation, and qualitative methods. Authentication and
unobservability are the most frequently studied properties. However, critical
gaps remain in areas such as policy compliance, accessibility,
interoperability, and back-end infrastructure security. We emphasize the
intertwined technical complexity and human factors of the metaverse and call
for integrated, interdisciplinary approaches to securing inclusive and
trustworthy immersive environments.

</details>


### [24] [LibLMFuzz: LLM-Augmented Fuzz Target Generation for Black-box Libraries](https://arxiv.org/abs/2507.15058)
*Ian Hardgrove,John D. Hastings*

Main category: cs.CR

TL;DR: LibLMFuzz addresses the challenge of fuzzing closed-source libraries by integrating Large Language Models (LLMs) with a lightweight toolchain to automatically analyze binaries, create fuzz drivers, and self-repair errors, significantly reducing costs.


<details>
  <summary>Details</summary>
Motivation: Fuzzing closed-source/proprietary software is difficult due to high setup/maintenance costs and lack of source code access, motivating the need for automated solutions to improve efficiency and reduce human intervention.

Method: The framework combines an agentic LLM with a disassembler/compiler/fuzzer toolchain to autonomously analyze stripped binaries, plan fuzz strategies, generate drivers, and iteratively resolve build/runtime errors without human oversight.

Result: LibLMFuzz achieved 100% API coverage for 558 fuzzable functions in four Linux libraries, generating syntactically correct drivers at 100% and achieving 75.52% nominal correctness on first execution across 1601 drivers.

Conclusion: LLM-augmented middleware demonstrates cost-reduction potential for fuzzing black-box components, establishing a foundation for future research in branch coverage optimization and autonomous vulnerability discovery.

Abstract: A fundamental problem in cybersecurity and computer science is determining
whether a program is free of bugs and vulnerabilities. Fuzzing, a popular
approach to discovering vulnerabilities in programs, has several advantages
over alternative strategies, although it has investment costs in the form of
initial setup and continuous maintenance. The choice of fuzzing is further
complicated when only a binary library is available, such as the case of
closed-source and proprietary software. In response, we introduce LibLMFuzz, a
framework that reduces costs associated with fuzzing closed-source libraries by
pairing an agentic Large Language Model (LLM) with a lightweight tool-chain
(disassembler/compiler/fuzzer) to autonomously analyze stripped binaries, plan
fuzz strategies, generate drivers, and iteratively self-repair build or runtime
errors. Tested on four widely-used Linux libraries, LibLMFuzz produced
syntactically correct drivers for all 558 fuzz-able API functions, achieving
100% API coverage with no human intervention. Across the 1601 synthesized
drivers, 75.52% were nominally correct on first execution. The results show
that LLM-augmented middleware holds promise in reducing the costs of fuzzing
black box components and provides a foundation for future research efforts.
Future opportunities exist for research in branch coverage.

</details>


### [25] [PromptArmor: Simple yet Effective Prompt Injection Defenses](https://arxiv.org/abs/2507.15219)
*Tianneng Shi,Kaijie Zhu,Zhun Wang,Yuqi Jia,Will Cai,Weida Liang,Haonan Wang,Hend Alzahrani,Joshua Lu,Kenji Kawaguchi,Basel Alomair,Xuandong Zhao,William Yang Wang,Neil Gong,Wenbo Guo,Dawn Song*

Main category: cs.CR

TL;DR: PromptArmor effectively detects and removes prompt injection attacks in LLM agents, achieving under 1% false positive/negative rates and reducing attack success rates to near-zero.


<details>
  <summary>Details</summary>
Motivation: LLM agents are vulnerable to prompt injection attacks where malicious inputs override user-specified tasks. Developing robust defenses against this threat is critical to ensure agent reliability and security.

Method: PromptArmor leverages an off-the-shelf LLM (e.g., GPT-4o, o4-mini) to scan inputs pre-processing, identifying and removing potential injected prompts through strategic prompting techniques.

Result: Evaluation on the AgentDojo benchmark shows PromptArmor achieves <1% false positive and false negative rates when using three models, with attack success rates dropping to below 1% after prompt removal. The system remains effective against adaptive attacks.

Conclusion: PromptArmor demonstrates high efficacy as a simple defense mechanism against prompt injection, warranting its adoption as a standard baseline for future evaluation of LLM agent security defenses.

Abstract: Despite their potential, recent research has demonstrated that LLM agents are
vulnerable to prompt injection attacks, where malicious prompts are injected
into the agent's input, causing it to perform an attacker-specified task rather
than the intended task provided by the user. In this paper, we present
PromptArmor, a simple yet effective defense against prompt injection attacks.
Specifically, PromptArmor prompts an off-the-shelf LLM to detect and remove
potential injected prompts from the input before the agent processes it. Our
results show that PromptArmor can accurately identify and remove injected
prompts. For example, using GPT-4o, GPT-4.1, or o4-mini, PromptArmor achieves
both a false positive rate and a false negative rate below 1% on the AgentDojo
benchmark. Moreover, after removing injected prompts with PromptArmor, the
attack success rate drops to below 1%. We also demonstrate PromptArmor's
effectiveness against adaptive attacks and explore different strategies for
prompting an LLM. We recommend that PromptArmor be adopted as a standard
baseline for evaluating new defenses against prompt injection attacks.

</details>


### [26] [The Matrix Subcode Equivalence problem and its application to signature with MPC-in-the-Head](https://arxiv.org/abs/2507.15377)
*Magali Bardet,Charles Brion,Philippe Gaborit,Mercedes Haiech,Romaric Neveu*

Main category: cs.CR

TL;DR: This paper introduces Matrix Subcode Equivalence and Permuted Kernel Problems for developing post-quantum signature schemes with reduced size compared to existing systems like SPHINCS+ and MEDS.


<details>
  <summary>Details</summary>
Motivation: The authors aim to enhance cryptography by introducing new hard problems in matrix codes for post-quantum signatures, addressing the lack of subcode equivalence studies and improving efficiency over current schemes with compact key/signature sizes.

Method: The Matrix Subcode Equivalence and Permuted Kernel Problems are defined, reduced to existing NP-Complete problems, and analyzed. The MPCitH paradigm is applied to build a signature scheme, with algorithmic complexity adaptations for subcode equivalence.

Result: The new signature scheme achieves ~4800 Byte signatures and ~275 Byte public keys, outperforming SPHINCS+ (smaller key/signature total size), MEDS, and CROSS in compactness while maintaining equivalent algorithmic hardness to their Hamming counterparts.

Conclusion: The proposed scheme establishes a novel class of post-quantum signatures with competitive size advantages over state-of-the-art systems, demonstrating viability through computational analysis and concrete parameter choices.

Abstract: Nowadays, equivalence problems are widely used in cryptography, most notably
to establish cryptosystems such as digital signatures, with MEDS, LESS, PERK as
the most recent ones. However, in the context of matrix codes, only the code
equivalence problem has been studied, while the subcode equivalence is
well-defined in the Hamming metric. In this work, we introduce two new
problems: the Matrix Subcode Equivalence Problem and the Matrix Code Permuted
Kernel Problem, to which we apply the MPCitH paradigm to build a signature
scheme. These new problems, closely related to the Matrix Code Equivalence
problem, ask to find an isometry given a code $C$ and a subcode $D$.
Furthermore, we prove that the Matrix Subcode Equivalence problem reduces to
the Hamming Subcode Equivalence problem, which is known to be NP-Complete, thus
introducing the matrix code version of the Permuted Kernel Problem. We also
adapt the combinatorial and algebraic algorithms for the Matrix Code
Equivalence problem to the subcode case, and we analyze their complexities. We
find with this analysis that the algorithms perform much worse than in the code
equivalence case, which is the same as what happens in the Hamming metric.
Finally, our analysis of the attacks allows us to take parameters much smaller
than in the Matrix Code Equivalence case. Coupled with the effectiveness of
\textit{Threshold-Computation-in-the-Head} or \textit{VOLE-in-the-Head}, we
obtain a signature size of $\approx$ 4 800 Bytes, with a public key of
$\approx$ 275 Bytes. We thus obtain a reasonable signature size, which brings
diversity in the landscape of post-quantum signature schemes, by relying on a
new hard problem. In particular, this new signature scheme performs better than
SPHINCS+, with a smaller size of public key + signature. Our signature compares
also well with other signature schemes: compared to MEDS, the signature is
smaller, and we reduced the size of the sum of signature and public key by a
factor close to 5. We also obtain a signature size that is almost half the size
of the CROSS signature scheme.

</details>


### [27] [PiMRef: Detecting and Explaining Ever-evolving Spear Phishing Emails with Knowledge Base Invariants](https://arxiv.org/abs/2507.15393)
*Ruofan Liu,Yun Lin,Silas Yeo Shuen Yu,Xiwen Teoh,Zhenkai Liang,Jin Song Dong*

Main category: cs.CR

TL;DR: This paper introduces PiMRef, a reference-based phishing email detector that leverages knowledge-based invariants to detect contradictions in identity claims. It outperforms existing methods in precision and efficiency on both benchmarks and real-world data.


<details>
  <summary>Details</summary>
Motivation: Traditional phishing detectors struggle with the evolving nature of phishing attacks, especially those enhanced by large language models (LLMs) which generate highly persuasive emails at low cost, making existing methods insufficient for accurate detection.

Method: PiMRef focuses on identity fact-checking through three steps: (i) extracting the sender's claimed identity from emails, (ii) verifying the domain legitimacy against a knowledge base, and (iii) identifying call-to-action prompts that encourage user engagement to flag contradictory claims as phishing indicators.

Result: PiMRef improves precision by 8.8% without compromising recall on standard phishing datasets (Nazario and PhishPot). In real-world evaluation (10,183 emails over 3 years), it achieves 92.1% precision, 87.9% recall, and 0.05s median runtime, surpassing state-of-the-art approaches in accuracy and speed.

Conclusion: PiMRef demonstrates a novel defense strategy against LLM-powered phishing by identifying contradictions in sender identity claims, providing both high effectiveness (precision/recall) and efficiency (speed), while offering explainable results through detectable factual inconsistencies.

Abstract: Phishing emails are a critical component of the cybercrime kill chain due to
their wide reach and low cost. Their ever-evolving nature renders traditional
rule-based and feature-engineered detectors ineffective in the ongoing arms
race between attackers and defenders. The rise of large language models (LLMs)
further exacerbates the threat, enabling attackers to craft highly convincing
phishing emails at minimal cost.
  This work demonstrates that LLMs can generate psychologically persuasive
phishing emails tailored to victim profiles, successfully bypassing nearly all
commercial and academic detectors. To defend against such threats, we propose
PiMRef, the first reference-based phishing email detector that leverages
knowledge-based invariants. Our core insight is that persuasive phishing emails
often contain disprovable identity claims, which contradict real-world facts.
PiMRef reframes phishing detection as an identity fact-checking task. Given an
email, PiMRef (i) extracts the sender's claimed identity, (ii) verifies the
legitimacy of the sender's domain against a predefined knowledge base, and
(iii) detects call-to-action prompts that push user engagement. Contradictory
claims are flagged as phishing indicators and serve as human-understandable
explanations.
  Compared to existing methods such as D-Fence, HelpHed, and ChatSpamDetector,
PiMRef boosts precision by 8.8% with no loss in recall on standard benchmarks
like Nazario and PhishPot. In a real-world evaluation of 10,183 emails across
five university accounts over three years, PiMRef achieved 92.1% precision,
87.9% recall, and a median runtime of 0.05s, outperforming the state-of-the-art
in both effectiveness and efficiency.

</details>


### [28] [PhishIntentionLLM: Uncovering Phishing Website Intentions through Multi-Agent Retrieval-Augmented Generation](https://arxiv.org/abs/2507.15419)
*Wenhao Li,Selvakumar Manickam,Yung-wey Chong,Shankar Karuppayah*

Main category: cs.CR

TL;DR: PhishIntentionLLM is a multi-agent RAG framework for identifying phishing intentions from website screenshots, achieving a micro-precision of 0.7895 with GPT-4o and a 95% improvement over single-agent baselines while releasing datasets of 2K and 9K labeled samples.


<details>
  <summary>Details</summary>
Motivation: Current phishing detection methods ignore the deeper malicious intention identification, which is critical for effective cybersecurity response and understanding threat landscapes.

Method: A retrieval-augmented generation (RAG) framework using visual-language LLMs to analyze website screenshots and classify phishing intentions into four categories: Credential Theft, Financial Fraud, Malware Distribution, and Personal Information Harvesting, supported by a ~2K ground truth dataset and evaluation across four commercial LLMs.

Result: Micro-precision of 0.7895 with GPT-4o, ~95% improvement over single-agent baselines, 0.8545 precision for Credential Theft (4% better than prior work), and a ~9K dataset for sector-level profiling.

Conclusion: PhishIntentionLLM provides scalable, interpretable intention-aware phishing analysis through multi-agent RAG, enabling more precise threat mitigation and large-scale behavioral profiling across sectors.

Abstract: Phishing websites remain a major cybersecurity threat, yet existing methods
primarily focus on detection, while the recognition of underlying malicious
intentions remains largely unexplored. To address this gap, we propose
PhishIntentionLLM, a multi-agent retrieval-augmented generation (RAG) framework
that uncovers phishing intentions from website screenshots. Leveraging the
visual-language capabilities of large language models (LLMs), our framework
identifies four key phishing objectives: Credential Theft, Financial Fraud,
Malware Distribution, and Personal Information Harvesting. We construct and
release the first phishing intention ground truth dataset (~2K samples) and
evaluate the framework using four commercial LLMs. Experimental results show
that PhishIntentionLLM achieves a micro-precision of 0.7895 with GPT-4o and
significantly outperforms the single-agent baseline with a ~95% improvement in
micro-precision. Compared to the previous work, it achieves 0.8545 precision
for credential theft, marking a ~4% improvement. Additionally, we generate a
larger dataset of ~9K samples for large-scale phishing intention profiling
across sectors. This work provides a scalable and interpretable solution for
intention-aware phishing analysis.

</details>


### [29] [Cryptanalysis of a multivariate CCZ scheme](https://arxiv.org/abs/2507.15449)
*Alessio Caminata,Elisa Gorla,Madison Mabe,Martina Vigorito,Irene Villa*

Main category: cs.CR

TL;DR: The paper challenges the security of the Pesto multivariate scheme by efficiently reducing its degree 4 public polynomials to quadratic systems, suggesting the CCZ transformation may not provide significant security benefits as previously believed.


<details>
  <summary>Details</summary>
Motivation: Initial assumptions about the security of Pesto's CCZ-transformed quadratic polynomials encouraged exploration of their vulnerabilities, aiming to reassess the transformation's effectiveness.

Method: The authors analyze the public polynomial system of Pesto and demonstrate a method to efficiently convert degree 4 polynomials into a quadratic system through structural manipulation.

Result: They successfully reduced the complexity of Pesto's public key system, revealing potential weaknesses in the CCZ transformation's security contribution.

Conclusion: While the CCZ transformation was thought to enhance security, this work indicates it may be insufficient, highlighting the need for further investigation into robust multivariate encryption methods.

Abstract: We consider the multivariate scheme Pesto, which was introduced by Calderini,
Caminata, and Villa. In this scheme, the public polynomials are obtained by
applying a CCZ transformation to a set of quadratic secret polynomials. As a
consequence, the public key consists of polynomials of degree 4. In this work,
we show that the public degree 4 polynomial system can be efficiently reduced
to a system of quadratic polynomials. This seems to suggest that the CCZ
transformation may not offer a significant increase in security, contrary to
what was initially believed.

</details>


### [30] [Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems](https://arxiv.org/abs/2507.15613)
*Andrii Balashov,Olena Ponomarova,Xiaohua Zhai*

Main category: cs.CR

TL;DR: This paper studies multi-stage prompt inference attacks on enterprise LLMs (e.g., Microsoft 365 Copilot) and proposes defenses including anomaly detection, access control, prompt sanitization, and architectural modifications. It emphasizes moving from single-turn to multi-stage security analysis.


<details>
  <summary>Details</summary>
Motivation: Enterprise LLMs with access to private data face novel security threats like multi-stage prompt inference attacks, which bypass standard safety measures by chaining benign-seeming queries. Current defenses are insufficient against these sophisticated, persistent threats.

Method: Formal threat modeling using probability theory, optimization frameworks, and information-theoretic leakage bounds. Experimental simulations with real attack scenarios involving multi-turn interactions. Evaluates four defense approaches (anomaly detection, access control, sanitization, spotlighting), including derivation of differential privacy leakage bounds and attack success reduction metrics.

Result: Demonstrated reliable exfiltration of SharePoint documents and emails even with existing safety measures. Anomaly detection achieved high AUC, spotlighting reduced attack success by 90%, and defense-in-depth strategy showed empirical validation of combined approach effectiveness.

Conclusion: Securing enterprise LLMs requires multi-stage threat modeling and layered defenses beyond basic prompt filtering. The work establishes theoretical foundations and practical methods for defending against sophisticated prompt inference attacks across multiple interaction steps.

Abstract: Large Language Models (LLMs) deployed in enterprise settings (e.g., as
Microsoft 365 Copilot) face novel security challenges. One critical threat is
prompt inference attacks: adversaries chain together seemingly benign prompts
to gradually extract confidential data. In this paper, we present a
comprehensive study of multi-stage prompt inference attacks in an enterprise
LLM context. We simulate realistic attack scenarios where an attacker uses
mild-mannered queries and indirect prompt injections to exploit an LLM
integrated with private corporate data. We develop a formal threat model for
these multi-turn inference attacks and analyze them using probability theory,
optimization frameworks, and information-theoretic leakage bounds. The attacks
are shown to reliably exfiltrate sensitive information from the LLM's context
(e.g., internal SharePoint documents or emails), even when standard safety
measures are in place.
  We propose and evaluate defenses to counter such attacks, including
statistical anomaly detection, fine-grained access control, prompt sanitization
techniques, and architectural modifications to LLM deployment. Each defense is
supported by mathematical analysis or experimental simulation. For example, we
derive bounds on information leakage under differential privacy-based training
and demonstrate an anomaly detection method that flags multi-turn attacks with
high AUC. We also introduce an approach called "spotlighting" that uses input
transformations to isolate untrusted prompt content, reducing attack success by
an order of magnitude. Finally, we provide a formal proof of concept and
empirical validation for a combined defense-in-depth strategy. Our work
highlights that securing LLMs in enterprise settings requires moving beyond
single-turn prompt filtering toward a holistic, multi-stage perspective on both
attacks and defenses.

</details>


### [31] [Cyber security of Mega Events: A Case Study of Securing the Digital Infrastructure for MahaKumbh 2025 -- A 45 days Mega Event of 600 Million Footfalls](https://arxiv.org/abs/2507.15660)
*Rohit Negi,Amit Negi,Manish Sharma,S. Venkatesan,Prem Kumar,Sandeep K. Shukla*

Main category: cs.CR

TL;DR: This paper presents a cybersecurity assessment and risk management approach for securing MahaKumbh 2025, a massive 45-day event with 600 million footfall in Prayagraj, India, using temporary infrastructure and emphasizing proactive defense strategies.


<details>
  <summary>Details</summary>
Motivation: Digitalized mega-event infrastructures are vulnerable to cyber threats due to their temporary nature and expedited construction, requiring distinct security approaches compared to permanent organizations to prevent chaos and reputational damage.

Method: The authors implemented a cybersecurity strategy focused on assessments, risk management, monitoring of mobile/web applications, networking, CCTV systems, control rooms, and multi-layered defensive mechanisms tailored to the event's temporary and high-traffic characteristics.

Result: All 45 days of MahaKumbh 2025 were secured successfully, with no cyberattacks exploiting vulnerabilities or disrupting operations, demonstrating the efficacy of the team's approach.

Conclusion: The methodology used for MahaKumbh 2025 provides a framework for securing large, temporary digital infrastructures, with lessons learned highlighting potential improvements for future mega events.

Abstract: Mega events such as the Olympics, World Cup tournaments, G-20 Summit,
religious events such as MahaKumbh are increasingly digitalized. From event
ticketing, vendor booth or lodging reservations, sanitation, event scheduling,
customer service, crime reporting, media streaming and messaging on digital
display boards, surveillance, crowd control, traffic control and many other
services are based on mobile and web applications, wired and wireless
networking, network of Closed-Circuit Television (CCTV) cameras, specialized
control room with network and video-feed monitoring. Consequently, cyber
threats directed at such digital infrastructure are common. Starting from hobby
hackers, hacktivists, cyber crime gangs, to the nation state actors, all target
such infrastructure to unleash chaos on an otherwise smooth operation, and
often the cyber threat actors attempt to embarrass the organizing country or
the organizers. Unlike long-standing organizations such as a corporate or a
government department, the infrastructure of mega-events is temporary,
constructed over a short time span in expediency, and often shortcuts are taken
to make the deadline for the event. As a result, securing such an elaborate yet
temporary infrastructure requires a different approach than securing a standard
organizational digital infrastructure. In this paper, we describe our approach
to securing MahaKumbh 2025, a 600 million footfall event for 45 days in
Prayagraj, India, as a cyber security assessment and risk management oversight
team. We chronicle the scope, process, methodology, and outcome of our team's
effort to secure this mega event. It should be noted that none of the cyber
attacks during the 45-day event was successful. Our goal is to put on record
the methodology and discuss what we would do differently in case we work on
similar future mega event.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [32] [Impact of Code Context and Prompting Strategies on Automated Unit Test Generation with Modern General-Purpose Large Language Models](https://arxiv.org/abs/2507.14256)
*Jakub Walczak,Piotr Tomalak,Artur Laskowski*

Main category: cs.SE

TL;DR: This paper evaluates how code context and prompting strategies affect the quality of unit tests generated by different LLMs. Chain-of-thought prompting achieves the highest performance (96.3% branch coverage, 57% average mutation score), with M5 (Gemini 2.5 Pro) leading in mutation score and compilation success rate. Code and tests are available at https://github.com/peetery/LLM-analysis.


<details>
  <summary>Details</summary>
Motivation: The testing pyramid emphasizes unit tests as the foundation of reliable software development, yet creating schematic tests with minimal domain expertise remains labor-intensive. Automating unit test generation can significantly improve productivity for developers.

Method: The study compares multiple LLM families by varying code context (e.g., docstrings, full implementations) and prompting strategies. Metrics like branch coverage, mutation score, and compilation success rate are measured to assess test quality and robustness.

Result: Incorporating docstrings boosts code adequacy, while full contextual extensions provide smaller benefits. Chain-of-thought prompting outperforms alternatives, achieving nearly perfect compilation success rates and the highest mutation scores (up to 57% and 96.3% branch coverage). M5 (Gemini 2.5 Pro) consistently excels across metrics.

Conclusion: Chain-of-thought prompting combined with selective contextual enrichment (e.g., docstrings) delivers optimal unit test generation for LLMs. The M5 model remains superior, and open-source collaboration through shared codebases is encouraged to advance reliable AI-powered testing methodologies.

Abstract: Generative AI is gaining increasing attention in software engineering, where
testing remains an indispensable reliability mechanism. According to the widely
adopted testing pyramid, unit tests constitute the majority of test cases and
are often schematic, requiring minimal domain expertise. Automatically
generating such tests under the supervision of software engineers can
significantly enhance productivity during the development phase of the software
lifecycle.
  This paper investigates the impact of code context and prompting strategies
on the quality and adequacy of unit tests generated by various large language
models (LLMs) across several families. The results show that including
docstrings notably improves code adequacy, while further extending context to
the full implementation yields definitely smaller gains. Notably, the
chain-of-thought prompting strategy -- applied even to 'reasoning' models --
achieves the best results, with up to 96.3\% branch coverage, a 57\% average
mutation score, and near-perfect compilation success rate. Among the evaluated
models, M5 (Gemini 2.5 Pro) demonstrated superior performance in both mutation
score and branch coverage being still in top in terms of compilation success
rate.
  All the code and resulting test suites are publicly available at
https://github.com/peetery/LLM-analysis.

</details>


### [33] [Leveraging LLMs for Formal Software Requirements -- Challenges and Prospects](https://arxiv.org/abs/2507.14330)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: This position paper addresses the challenge of generating formal verifiable specifications from ambiguous natural language requirements through approaches combining NLP, ontologies, artifact reuse, and LLMs, summarizing literature gaps and research directions.


<details>
  <summary>Details</summary>
Motivation: Formal verification requires precise specifications but faces challenges due to informal and ambiguous natural language requirements, leading to inconsistent verification outcomes in safety-critical domains.

Method: Preliminary literature synthesis using natural language processing, ontology-based domain modeling, reusable artefact frameworks, and large language model integration to identify spec-generation patterns and challenges.

Result: Identified common challenges in informal requirement parsing and proposed research directions for automated/semi-automated spec-generation techniques using interdisciplinary methods.

Conclusion: Automated specification generation from natural language requires hybrid approaches combining NLP, domain ontologies, and LLMs, with future work needed on tool integration and validation workflows in formal verification.

Abstract: Software correctness is ensured mathematically through formal verification,
which involves the resources of generating formal requirement specifications
and having an implementation that must be verified. Tools such as
model-checkers and theorem provers ensure software correctness by verifying the
implementation against the specification. Formal methods deployment is
regularly enforced in the development of safety-critical systems e.g.
aerospace, medical devices and autonomous systems. Generating these
specifications from informal and ambiguous natural language requirements
remains the key challenge. Our project, VERIFAI^{1}, aims to investigate
automated and semi-automated approaches to bridge this gap, using techniques
from Natural Language Processing (NLP), ontology-based domain modelling,
artefact reuse, and large language models (LLMs). This position paper presents
a preliminary synthesis of relevant literature to identify recurring challenges
and prospective research directions in the generation of verifiable
specifications from informal requirements.

</details>


### [34] [Developing Shared Vocabulary System For Collaborative Software Engineering](https://arxiv.org/abs/2507.14396)
*Carey Lai Zheng Hui,Johnson Britto Jessia Esther Leena,Kumuthini Subramanian,Zhao Chenyu,Shubham Rajeshkumar Jariwala*

Main category: cs.SE

TL;DR: This paper introduces a structured methodology for collaborative vocabulary development in software engineering using Design Science Research, showing that shared vocabulary systems improve communication and collaboration efficiency despite initial adoption overhead.


<details>
  <summary>Details</summary>
Motivation: Communication gaps in software engineering cause persistent issues like misunderstandings, inefficiencies, and defects, particularly through ambiguous messaging, misaligned documentation, inconsistent code review feedback, and API integration problems.

Method: The study employs a Design Science Research (DSR) framework with three iterative phases: problem identification (thematic analysis and interviews), method development (Grounded Theory-based collaboration methodology), and empirical validation (controlled experiments tracking communication outcomes).

Result: Controlled experiments revealed increased information density, documentation clarity, and collaboration efficiency over time, though initial system adoption introduced measurable overhead.

Conclusion: Shared vocabulary systems offer actionable improvements for software engineering collaboration practices, but require balancing adoption costs with long-term benefits while addressing identified limitations through future research.

Abstract: Effective communication is a critical factor in successful software
engineering collaboration. However, communication gaps remain a persistent
challenge, often leading to misunderstandings, inefficiencies, and defects.
This research investigates the technical factors contributing to such
misunderstandings and explores the measurable benefits of establishing shared
vocabulary systems within software documentation and codebases. Using a Design
Science Research (DSR) framework, the study was structured into three iterative
phases: problem identification, method development, and empirical validation.
The problem identification phase involved thematic analysis of communication
data and semi-structured interviews, revealing key factors such as ambiguous
messaging, misalignment in documentation, inconsistent code review feedback,
and API integration miscommunication. Grounded Theory principles were employed
to design a structured methodology for collaborative vocabulary development.
Empirical validation through controlled experiments demonstrated that while
initial adoption introduced overhead, the shared vocabulary system
significantly improved information density, documentation clarity, and
collaboration efficiency over time. Findings offer actionable insights for
improving communication practices in software engineering, while also
identifying limitations and directions for future research.

</details>


### [35] [On the Effect of Token Merging on Pre-trained Models for Code](https://arxiv.org/abs/2507.14423)
*Mootez Saad,Hao Li,Tushar Sharma,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: This paper explores merging subtoken representations in code language models to reduce computational overhead and improve downstream performance across three tasks.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency caused by longer subtoken sequences in code language models compared to traditional compilers.

Method: Proposes averaging subtoken representations and a learning-based strategy for merging, compatible with existing code models like CodeBERT and CodeT5+. Conducts experiments on six models across three tasks.

Result: Reduces FLOPs by 1–19%. Vulnerability detection F1 score drops by 1.82 points, while code translation gains 2.47 CodeBLEU improvement. Mixed performance trade-offs observed.

Conclusion: Merging subtoken representations offers efficiency gains with task-dependent performance trade-offs, advancing code language models in both computational speed and accuracy dimensions.

Abstract: Tokenization is a fundamental component of language models for code. It
involves breaking down the input into units that are later passed to the
language model stack to learn high-dimensional representations used in various
contexts, from classification to generation. However, the output of these
tokenizers is often longer than that traditionally used in compilers and
interpreters. This could result in undesirable effects, such as increased
computational overhead. In this work, we investigate the effect of merging the
hidden representations of subtokens that belong to the same semantic unit, such
as subtokens that form a single identifier. We propose two strategies: one
based on averaging the representations and another that leverages a
learning-based approach. Both methods can be seamlessly integrated with
existing language models for code. We conduct experiments using six language
models for code: CodeBERT, GraphCodeBERT, UniXCoder, CdoeT5, CodeT5+ (220M),
and CodeT5+ (770M), across three software engineering tasks: vulnerability
detection, code classification, and code translation. Results show that these
strategies can reduce the number of floating-point operations by $1\%$ to
$19\%$. Regarding downstream performance, the most significant degradation was
observed in the vulnerability detection task, where the F1 score decreased by
$1.82$ points compared to the baseline. In contrast, for code translation, we
observed an improvement of $2.47$ points in CodeBLEU. This work contributes to
the broader effort of improving language models for code across multiple
dimensions, including both computational efficiency and downstream performance.

</details>


### [36] [Architectural Degradation: Definition, Motivations, Measurement and Remediation Approaches](https://arxiv.org/abs/2507.14547)
*Noman Ahmad,Ruoyu Su,Matteo Esposito,Andrea Janes,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: This paper unifies understanding of architectural degradation by analyzing 108 studies, identifying definitions, causes, metrics, and tools, and highlights gaps in continuous remediation strategies.


<details>
  <summary>Details</summary>
Motivation: Current literature on architectural degradation has fragmented definitions, metrics, and remediation strategies, necessitating a unified framework to improve system maintainability and adaptability.

Method: Multivocal literature review of 108 academic and gray literature sources to extract definitions, causes, metrics, measurement approaches, tools, and remediation strategies, synthesizing into a taxonomy of architectural, code, and process debt.

Result: The study uncovers evolving definitions (emphasizing socio-technical aspects), 54 metrics (focusing on smells, cohesion/coupling, and evolution), 31 measurement techniques, and a taxonomy revealing missed integration between metrics, tools, and repair logic.

Conclusion: Architectural degradation requires holistic, proactive strategies combining technical and organizational solutions, as current tools focus on detection rather than continuous remediation.

Abstract: Architectural degradation, also known as erosion, decay, or aging, impacts
system quality, maintainability, and adaptability. Although widely
acknowledged, current literature shows fragmented definitions, metrics, and
remediation strategies. Our study aims to unify understanding of architectural
degradation by identifying its definitions, causes, metrics, tools, and
remediation approaches across academic and gray literature. We conducted a
multivocal literature review of 108 studies extracting definitions, causes,
metrics, measurement approaches, tools, and remediation strategies. We
developed a taxonomy encompassing architectural, code, and process debt to
explore definition evolution, methodological trends, and research gaps.
Architectural degradation has shifted from a low-level issue to a
socio-technical concern. Definitions now address code violations, design drift,
and structural decay. Causes fall under architectural (e.g., poor
documentation), code (e.g., hasty fixes), and process debt (e.g., knowledge
loss). We identified 54 metrics and 31 measurement techniques, focused on
smells, cohesion/coupling, and evolution. Yet, most tools detect issues but
rarely support ongoing or preventive remediation. Degradation is both technical
and organizational. While detection is well-studied, continuous remediation
remains lacking. Our study reveals missed integration between metrics, tools,
and repair logic, urging holistic, proactive strategies for sustainable
architecture.

</details>


### [37] [Emerging Trends in Software Architecture from the Practitioners Perspective: A Five Year Review](https://arxiv.org/abs/2507.14554)
*Ruoyu Su,Noman ahmad,Matteo Esposito,Andrea Janes,Davide Taibi,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: This paper analyzes trends in software architecture across eight industry conferences, revealing the dominance of Kubernetes, Cloud Native, Serverless, and Containers, while highlighting gaps in early DevOps stages and the need for research to address holistic architectural evolution.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand shifts in architectural practices driven by cloud computing, microservices, and containers, providing insights into their adoption motivations and contexts to guide practitioners and researchers.

Method: Authors analyzed 5,677 conference talks using large language models and expert validation, extracting technology usage patterns, purposes, deployment pipeline integration, and community cohesion through network analysis.

Result: Identified 450 technologies with Kubernetes/Cloud Native/Serverless/Containers as central hubs; mapped five distinct technology communities; revealed hybrid deployment prevalence and limited focus on early DevOps stages like planning/coding.

Conclusion: Current architecture practice increasingly relies on standardized core technologies concentrated in deployment/runtime phases, suggesting opportunities for academic research to improve architectural design quality and address underexplored early-stage needs.

Abstract: Software architecture plays a central role in the design, development, and
maintenance of software systems. With the rise of cloud computing,
microservices, and containers, architectural practices have diversified.
Understanding these shifts is vital. This study analyzes software architecture
trends across eight leading industry conferences over five years. We
investigate the evolution of software architecture by analyzing talks from top
practitioner conferences, focusing on the motivations and contexts driving
technology adoption. We analyzed 5,677 talks from eight major industry
conferences, using large language models and expert validation to extract
technologies, their purposes, and usage contexts. We also explored how
technologies interrelate and fit within DevOps and deployment pipelines. Among
450 technologies, Kubernetes, Cloud Native, Serverless, and Containers dominate
by frequency and centrality. Practitioners present technology mainly related to
deployment, communication, AI, and observability. We identify five technology
communities covering automation, coordination, cloud AI, monitoring, and
cloud-edge. Most technologies span multiple DevOps stages and support hybrid
deployment. Our study reveals that a few core technologies, like Kubernetes and
Serverless, dominate the contemporary software architecture practice. These are
mainly applied in later DevOps stages, with limited focus on early phases like
planning and coding. We also show how practitioners frame technologies by
purpose and context, reflecting evolving industry priorities. Finally, we
observe how only research can provide a more holistic lens on architectural
design, quality, and evolution.

</details>


### [38] [Harnessing LLMs for Document-Guided Fuzzing of OpenCV Library](https://arxiv.org/abs/2507.14558)
*Bin Duan,Tarek Mahmud,Meiru Che,Yan Yan,Naipeng Dong,Dan Dongseong Kim,Guowei Yang*

Main category: cs.SE

TL;DR: VISTAFUZZ is a novel technique using LLMs to enhance OpenCV library testing by parsing documentation, extracting parameter constraints, and generating inputs to detect 17 new bugs with 5 confirmed fixes.


<details>
  <summary>Details</summary>
Motivation: Bugs in the widely used OpenCV library can disrupt critical computer vision applications, necessitating robust testing approaches to ensure reliability.

Method: LLMs parse OpenCV API documentation to extract standardized info and parameter constraints/dependencies. Generated inputs based on constraints systematically test APIs through fuzzing.

Result: Testing 330 APIs revealed 17 new bugs (10 confirmed, 5 fixed) demonstrating VISTAFUZZ's effectiveness in identifying real issues in OpenCV.

Conclusion: Document-guided LLM fuzzing is a promising approach for detecting bugs in foundational libraries like OpenCV, highlighting LLMs' potential in improving software reliability through structured documentation analysis.

Abstract: The combination of computer vision and artificial intelligence is
fundamentally transforming a broad spectrum of industries by enabling machines
to interpret and act upon visual data with high levels of accuracy. As the
biggest and by far the most popular open-source computer vision library, OpenCV
library provides an extensive suite of programming functions supporting
real-time computer vision. Bugs in the OpenCV library can affect the downstream
computer vision applications, and it is critical to ensure the reliability of
the OpenCV library. This paper introduces VISTAFUZZ, a novel technique for
harnessing large language models (LLMs) for document-guided fuzzing of the
OpenCV library. VISTAFUZZ utilizes LLMs to parse API documentation and obtain
standardized API information. Based on this standardized information, VISTAFUZZ
extracts constraints on individual input parameters and dependencies between
these. Using these constraints and dependencies, VISTAFUZZ then generates new
input values to systematically test each target API. We evaluate the
effectiveness of VISTAFUZZ in testing 330 APIs in the OpenCV library, and the
results show that VISTAFUZZ detected 17 new bugs, where 10 bugs have been
confirmed, and 5 of these have been fixed.

</details>


### [39] [A first look at License Variants in the PyPI Ecosystem](https://arxiv.org/abs/2507.14594)
*Weiwei Xu,Hengzhi Ye,Kai Gao,Minghui Zhou*

Main category: cs.SE

TL;DR: First comprehensive empirical study of license variants in PyPI, revealing 2% substantial modifications causing 10.7% incompatibility, and introducing LV-Parser (93.6% accuracy, 30% cost reduction) and LV-Compat (5.2x more incompatible packages detected with 98% precision) for license analysis.\n\n


<details>
  <summary>Details</summary>
Motivation: License variants create compliance complexities in software systems, with existing tools failing to address their legal implications and impact on dependency networks, leading to significant risks for developers and organizations.\n\n

Method: 1) Conducted empirical analysis of 12K PyPI packages to characterize license variants\n2) Developed LV-Parser using diff-based techniques and LLMs for variant classification\n3) Created LV-Compat for automated dependency-level incompatibility detection in software ecosystems\n\n

Result: 1) 2% of license variants contain legal modifications\n2) 10.7% of downstream dependencies face license incompatibility\n3) LV-Parser achieves 93.6% accuracy with 30% lower computational cost\n4) LV-Compat detects 5.2x more incompatible packages than prior approaches while maintaining 98% precision\n\n

Conclusion: This work establishes the first systematic understanding of license variants in open-source ecosystems and demonstrates how specialized tools like LV-Parser and LV-Compat can significantly improve compliance analysis accuracy and effectiveness for dependency management.\n\n

Abstract: Open-source licenses establish the legal foundation for software reuse, yet
license variants, including both modified standard licenses and custom-created
alternatives, introduce significant compliance complexities. Despite their
prevalence and potential impact, these variants are poorly understood in modern
software systems, and existing tools do not account for their existence,
leading to significant challenges in both effectiveness and efficiency of
license analysis. To fill this knowledge gap, we conduct a comprehensive
empirical study of license variants in the PyPI ecosystem. Our findings show
that textual variations in licenses are common, yet only 2% involve substantive
modifications. However, these license variants lead to significant compliance
issues, with 10.7% of their downstream dependencies found to be
license-incompatible.
  Inspired by our findings, we introduce LV-Parser, a novel approach for
efficient license variant analysis leveraging diff-based techniques and large
language models, along with LV-Compat, an automated pipeline for detecting
license incompatibilities in software dependency networks. Our evaluation
demonstrates that LV-Parser achieves an accuracy of 0.936 while reducing
computational costs by 30%, and LV-Compat identifies 5.2 times more
incompatible packages than existing methods with a precision of 0.98.
  This work not only provides the first empirical study into license variants
in software packaging ecosystem but also equips developers and organizations
with practical tools for navigating the complex landscape of open-source
licensing.

</details>


### [40] [An Efficient Algorithm for Generating Minimal Unique-Cause MC/DC Test cases for Singular Boolean Expressions](https://arxiv.org/abs/2507.14687)
*Robin Lee,Youngho Nam*

Main category: cs.SE

TL;DR: The paper introduces Robin's Rule, a deterministic algorithm for optimal test generation in Singular Boolean Expressions (SBEs), achieving 100% Unique-Cause MC/DC with N + 1 test cases and outperforming commercial tools in efficiency.


<details>
  <summary>Details</summary>
Motivation: Unique-Cause MC/DC is critical for safety but lacks efficient test generation, despite 99.7% of avionics decisions being SBEs. This paper addresses the practical need for a minimal, deterministic approach to meet rigorous safety standards.

Method: Robin's Rule directly constructs a minimal test set of N + 1 cases for SBEs, avoiding full truth table generation. The algorithm was validated using a benchmark derived from TCAS-II SBEs and compared against a certified commercial tool through verification runs.

Result: Empirical results demonstrate Robin's Rule consistently achieves 100% Unique-Cause MC/DC coverage with the theoretical minimum test cases while being computationally more efficient than industry-standard tools on the TCAS-II benchmark.

Conclusion: Robin's Rule offers a provably optimal solution for MC/DC verification in safety-critical systems, proving efficacy with minimal test cases and superior efficiency, making it a practical replacement for existing methods.

Abstract: Modified Condition/Decision Coverage (MC/DC) is a mandatory structural
coverage criterion for ensuring the reliability and safety of critical systems.
While its strictest form, Unique-Cause MC/DC, offers the highest assurance,
research on its efficient test generation has been lacking. This gap is
particularly significant, as an analysis of large-scale avionics systems shows
that 99.7% of all conditional decisions are, in fact, Singular Boolean
Expressions (SBEs) the ideal structure for applying Unique-Cause MC/DC. This
paper proposes 'Robin's Rule', a deterministic algorithm that directly
constructs a minimal test set of N + 1 cases to guarantee 100% Unique-Cause
MC/DC for SBEs with N conditions, without generating a full truth table. To
validate our approach, we constructed a benchmark by reformulating the TCAS-II
specifications into SBEs and verified the results using an industry-standard,
certified commercial tool. The results confirm that our method consistently
achieves 100% coverage with the theoretical minimum number of tests and is more
efficient than the commercial tool. This work provides a practical and provably
optimal solution for verifying safety-critical systems, ensuring both rigor and
efficiency.

</details>


### [41] [HistoryFinder: Advancing Method-Level Source Code History Generation with Accurate Oracles and Enhanced Algorithm](https://arxiv.org/abs/2507.14716)
*Shahidul Islam,Ashik Aowal,Md Sharif Uddin,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: The paper introduces HistoryFinder, a new method history generation tool that outperforms existing tools in accuracy and runtime efficiency by using more reliable ground truth oracles combining automated and manual validation.


<details>
  <summary>Details</summary>
Motivation: Existing method history tools like CodeShovel and CodeTracker have evaluations limited by inaccurate ground truth oracles, which motivates the need for more reliable validation methods and improved tools.

Method: The authors systematically created two new oracles (corrected CodeShovel and HistoryFinder) through automated analysis with expert-guided manual validation, and developed HistoryFinder to enhance accuracy, completeness, and runtime performance.

Result: HistoryFinder outperformed CodeShovel, CodeTracker, IntelliJ, and Git-based baselines across all metrics (precision, recall, F1 score) with 400 evaluated methods from 40 open-source repositories, achieving the lowest mean and median execution times among research-based tools.

Conclusion: HistoryFinder is positions as the best overall solution when both accuracy and efficiency are critical, offering faster runtime than research tools while surpassing Git-based methods in precision.

Abstract: Reconstructing a method's change history efficiently and accurately is
critical for many software engineering tasks, including maintenance,
refactoring, and comprehension. Despite the availability of method history
generation tools such as CodeShovel and CodeTracker, existing evaluations of
their effectiveness are limited by inaccuracies in the ground truth oracles
used. In this study, we systematically construct two new oracles -- the
corrected CodeShovel oracle and a newly developed HistoryFinder oracle -- by
combining automated analysis with expert-guided manual validation. We also
introduce HistoryFinder, a new method history generation tool designed to
improve not only the accuracy and completeness of method change histories but
also to offer competitive runtime performance. Through extensive evaluation
across 400 methods from 40 open-source repositories, we show that HistoryFinder
consistently outperforms CodeShovel, CodeTracker, IntelliJ, and Git-based
baselines in terms of precision, recall, and F1 score. Moreover, HistoryFinder
achieves competitive runtime performance, offering the lowest mean and median
execution times among all the research-based tools.
  While Git-based tools exhibit the fastest runtimes, this efficiency comes at
the cost of significantly lower precision and recall -- leaving HistoryFinder
as the best overall choice when both accuracy and efficiency are important. To
facilitate adoption, we provide a web interface, CLI, and Java library for
flexible usage.

</details>


### [42] [Investigating the Role of LLMs Hyperparameter Tuning and Prompt Engineering to Support Domain Modeling](https://arxiv.org/abs/2507.14735)
*Vladyslav Bulhakov,Giordano d'Aloisio,Claudio Di Sipio,Antinisca Di Marco,Davide Di Ruscio*

Main category: cs.SE

TL;DR: The paper investigates how hyperparameter tuning and prompt engineering enhance the Llama 3.1 model's accuracy in generating domain models from text, showing improved results across multiple domains.


<details>
  <summary>Details</summary>
Motivation: General-purpose LLMs for domain modeling face limitations in accuracy, and fine-tuning is resource-intensive and prone to catastrophic forgetting. This study aims to enhance LLM performance for domain modeling through alternative optimization techniques.

Method: The authors employ search-based hyperparameter tuning for the Llama 3.1 model on a medical data model, followed by testing optimized parameters across ten application domains, combined with prompt engineering techniques.

Result: Hyperparameter tuning with prompt engineering improves model quality in a medical case and achieves enhanced results in almost all tested domains, though not universally effective.

Conclusion: Combining hyperparameter tuning and prompt engineering can effectively improve domain model generation accuracy for LLMs like Llama 3.1 across diverse application areas.

Abstract: The introduction of large language models (LLMs) has enhanced automation in
software engineering tasks, including in Model Driven Engineering (MDE).
However, using general-purpose LLMs for domain modeling has its limitations.
One approach is to adopt fine-tuned models, but this requires significant
computational resources and can lead to issues like catastrophic forgetting.
  This paper explores how hyperparameter tuning and prompt engineering can
improve the accuracy of the Llama 3.1 model for generating domain models from
textual descriptions. We use search-based methods to tune hyperparameters for a
specific medical data model, resulting in a notable quality improvement over
the baseline LLM. We then test the optimized hyperparameters across ten diverse
application domains.
  While the solutions were not universally applicable, we demonstrate that
combining hyperparameter tuning with prompt engineering can enhance results
across nearly all examined domain models.

</details>


### [43] [Toward Inclusive AI-Driven Development: Exploring Gender Differences in Code Generation Tool Interactions](https://arxiv.org/abs/2507.14770)
*Manaal Basha,Ivan Beschastnikh,Gema Rodriguez-Perez,Cleidson R. B. de Souza*

Main category: cs.SE

TL;DR: The study investigates gender differences in the use of Code Generation Tools (CGTs) among developers to address fairness, inclusivity, and cognitive load in programming workflows.


<details>
  <summary>Details</summary>
Motivation: CGTs like Copilot are reshaping programming but lack investigation into how their effectiveness varies across user groups, particularly gender, while prior research shows gender differences in technology use and cognitive processing.

Method: A mixed-subjects design with 54 gender-balanced participants completing two programming tasks under CGT-only and internet-only conditions. Data includes cognitive load surveys, screen recordings, and performance metrics (time, correctness, interaction behaviors).

Result: Pending (study proposed), expected to uncover gender-based differences in CGT usage patterns and task performance outcomes.

Conclusion: The proposed study aims to advance fairness, accountability, and transparency in CGT design, supporting inclusive AI practices through better understanding of gender-influenced interactions.

Abstract: Context: The increasing reliance on Code Generation Tools (CGTs), such as
Windsurf and GitHub Copilot, are revamping programming workflows and raising
critical questions about fairness and inclusivity. While CGTs offer potential
productivity enhancements, their effectiveness across diverse user groups have
not been sufficiently investigated. Objectives: We hypothesize that developers'
interactions with CGTs vary based on gender, influencing task outcomes and
cognitive load, as prior research suggests that gender differences can affect
technology use and cognitive processing. Methods: The study will employ a
mixed-subjects design with 54 participants, evenly divided by gender for a
counterbalanced design. Participants will complete two programming tasks
(medium to hard difficulty) with only CGT assistance and then with only
internet access. Task orders and conditions will be counterbalanced to mitigate
order effects. Data collection will include cognitive load surveys, screen
recordings, and task performance metrics such as completion time, code
correctness, and CGT interaction behaviors. Statistical analyses will be
conducted to identify statistically significant differences in CGT usage.
Expected Contributions: Our work can uncover gender differences in CGT
interaction and performance among developers. Our findings can inform future
CGT designs and help address usability and potential disparities in interaction
patterns across diverse user groups. Conclusion: While results are not yet
available, our proposal lays the groundwork for advancing fairness,
accountability, transparency, and ethics (FATE) in CGT design. The outcomes are
anticipated to contribute to inclusive AI practices and equitable tool
development for all users.

</details>


### [44] [VeriOpt: PPA-Aware High-Quality Verilog Generation via Multi-Role LLMs](https://arxiv.org/abs/2507.14776)
*Kimia Tasnia,Alexander Garcia,Tasnuva Farheen,Sazadur Rahman*

Main category: cs.SE

TL;DR: VeriOpt is a framework that enhances LLM-generated Verilog code by integrating PPA-aware optimization and role-based prompting, achieving both high functional correctness and significant Power/Performance/Area (PPA) improvements validated with industry tools.


<details>
  <summary>Details</summary>
Motivation: Current LLM applications in hardware design prioritize functional correctness but overlook critical PPA metrics necessary for industrial-grade implementations, creating a gap between AI-generated designs and production requirements.

Method: VeriOpt structures LLM interactions through specialized roles (Planner, Programmer, Reviewer, Evaluator) to mimic human workflows, combines multi-modal feedback (synthesis reports, timing diagrams) with PPA-aware prompting, and integrates PPA constraints directly into the prompting pipeline.

Result: Experimental results show 88% lower power, 76% smaller area, 73% better timing closure compared to baseline LLM-generated RTL, while maintaining 86% functionality success rate as verified by industry-standard EDA tools.

Conclusion: VeriOpt advances AI-driven hardware design by addressing the PPA-functional correctness disconnect, enabling scalable and reliable LLM adoption for production-grade digital circuit synthesis.

Abstract: The rapid adoption of large language models(LLMs) in hardware design has
primarily focused on generating functionally correct Verilog code, overlooking
critical Power Performance-Area(PPA) metrics essential for industrial-grade
designs. To bridge this gap, we propose VeriOpt, a novel framework that
leverages role-based prompting and PPA-aware optimization to enable LLMs to
produce high-quality, synthesizable Verilog. VeriOpt structures LLM
interactions into specialized roles (e.g., Planner, Programmer, Reviewer,
Evaluator) to emulate human design workflows, while integrating PPA constraints
directly into the prompting pipeline. By combining multi-modal feedback (e.g.,
synthesis reports, timing diagrams) with PPA aware prompting, VeriOpt achieves
PPA-efficient code generation without sacrificing functional correctness.
Experimental results demonstrate up to 88% reduction in power, 76% reduction in
area and 73% improvement in timing closure compared to baseline LLM-generated
RTL, validated using industry standard EDA tools. At the same time achieves 86%
success rate in functionality evaluation. Our work advances the
state-of-the-art AI-driven hardware design by addressing the critical gap
between correctness and quality, paving the way for reliable LLM adoption in
production workflows.

</details>


### [45] [Enhancing Repository-Level Code Generation with Call Chain-Aware Multi-View Context](https://arxiv.org/abs/2507.14791)
*Yang Liu,Li Zhang,Fang Liu,Zhuohang Wang,Donglin Wei,Zhishuo Yang,Kechi Zhang,Jia Li,Lin Shi*

Main category: cs.SE

TL;DR: RepoScope is a novel repository-level code generation framework that leverages call chain-aware multi-view context and structural semantic graphs to outperform existing RAG-based methods by 36.35% in pass@1 scores without extra training.


<details>
  <summary>Details</summary>
Motivation: Current repository-level code generation approaches using RAG struggle with semantic richness of contextual information and fail to account for structural relationships in retrieved code, leading to narrow contextual perspectives and inaccurate LLM interpretations.

Method: 1. Constructs a Repository Structural Semantic Graph (RSSG) to enable comprehensive four-view context retrieval
2. Implements a call chain prediction method using structural semantics
3. Develops a structure-preserving serialization algorithm for prompt construction
4. Relys solely on static analysis without requiring additional training or multiple LLM queries

Result: Achieved up to 36.35% relative improvement in pass@1 scores on CoderEval and DevEval benchmarks. Demonstrated cross-task effectiveness and seamless integration with existing approaches through experiments.

Conclusion: RepoScope's structural analysis approach offers both efficiency and generalizability advantages over previous methods, making it a valuable advancement in repository-level code generation through better semantic preservation and structural awareness.

Abstract: Repository-level code generation aims to generate code within the context of
a specified repository. Existing approaches typically employ
retrieval-augmented generation (RAG) techniques to provide LLMs with relevant
contextual information extracted from the repository. However, these approaches
often struggle with effectively identifying truly relevant contexts that
capture the rich semantics of the repository, and their contextual perspectives
remains narrow. Moreover, most approaches fail to account for the structural
relationships in the retrieved code during prompt construction, hindering the
LLM's ability to accurately interpret the context. To address these issues, we
propose RepoScope, which leverages call chain-aware multi-view context for
repository-level code generation. RepoScope constructs a Repository Structural
Semantic Graph (RSSG) and retrieves a comprehensive four-view context,
integrating both structural and similarity-based contexts. We propose a novel
call chain prediction method that utilizes the repository's structural
semantics to improve the identification of callees in the target function.
Additionally, we present a structure-preserving serialization algorithm for
prompt construction, ensuring the coherence of the context for the LLM.
Notably, RepoScope relies solely on static analysis, eliminating the need for
additional training or multiple LLM queries, thus ensuring both efficiency and
generalizability. Evaluation on widely-used repository-level code generation
benchmarks (CoderEval and DevEval) demonstrates that RepoScope outperforms
state-of-the-art methods, achieving up to a 36.35% relative improvement in
pass@1 scores. Further experiments emphasize RepoScope's potential to improve
code generation across different tasks and its ability to integrate effectively
with existing approaches.

</details>


### [46] [Think Like an Engineer: A Neuro-Symbolic Collaboration Agent for Generative Software Requirements Elicitation and Self-Review](https://arxiv.org/abs/2507.14969)
*Sai Zhang,Zhenchang Xing,Jieshan Chen,Dehai Zhao,Zizhong Zhu,Xiaowang Zhang,Zhiyong Feng,Xiaohong Li*

Main category: cs.SE

TL;DR: RequireCEG is a neuro-symbolic agent for improving end-user software requirements through self-healing causal-effect graphs, enhancing Gherkin scenario generation with 87% coverage and 51.88% higher diversity.


<details>
  <summary>Details</summary>
Motivation: End-user software engineering faces challenges due to ambiguous natural language requirements. Existing tools like Gherkin fail to express causal logic between preconditions and behaviors, necessitating better requirement elicitation.

Method: RequireCEG uses (1) hierarchical feature trees to structure requirements, (2) self-healing causal-effect graphs (CEGs) to model causal relationships between atomic preconditions and behaviors, and (3) CEG analysis to review/optimize Gherkin scenarios.

Result: RGPair experiments show RequireCEG achieves 87% requirement coverage and 51.88% increased diversity in generated scenarios compared to existing approaches.

Conclusion: Causal-effect graph integration provides a robust neuro-symbolic framework for natural language requirements, enabling consistent Gherkin scenarios through explicit causal modeling and hierarchical organization.

Abstract: The vision of End-User Software Engineering (EUSE) is to empower
non-professional users with full control over the software development
lifecycle. It aims to enable users to drive generative software development
using only natural language requirements. However, since end-users often lack
knowledge of software engineering, their requirement descriptions are
frequently ambiguous, raising significant challenges to generative software
development. Although existing approaches utilize structured languages like
Gherkin to clarify user narratives, they still struggle to express the causal
logic between preconditions and behavior actions. This paper introduces
RequireCEG, a requirement elicitation and self-review agent that embeds
causal-effect graphs (CEGs) in a neuro-symbolic collaboration architecture.
RequireCEG first uses a feature tree to analyze user narratives hierarchically,
clearly defining the scope of software components and their system behavior
requirements. Next, it constructs the self-healing CEGs based on the elicited
requirements, capturing the causal relationships between atomic preconditions
and behavioral actions. Finally, the constructed CEGs are used to review and
optimize Gherkin scenarios, ensuring consistency between the generated Gherkin
requirements and the system behavior requirements elicited from user
narratives. To evaluate our method, we created the RGPair benchmark dataset and
conducted extensive experiments. It achieves an 87% coverage rate and raises
diversity by 51.88%.

</details>


### [47] [The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering](https://arxiv.org/abs/2507.15003)
*Hao Li,Haoxiang Zhang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: This paper introduces AIDev, the first large-scale dataset capturing autonomous coding agents' operations in real-world software development, enabling empirical research into AI-native workflows and human-AI collaboration.


<details>
  <summary>Details</summary>
Motivation: Previous research on AI-native software engineering has been largely theoretical, lacking empirical data to study autonomous teammates' actual impacts on code quality, collaboration dynamics, and adoption challenges.

Method: AIDev compiles over 456,000 pull requests from five leading agents (Codex, Devin, Copilot, Cursor, Claude Code) across 61,000 repositories, 47,000 developers, and 3+ years. It includes structured metadata on PR content, authorship, review timelines, code complexity metrics, and integration outcomes.

Result: Despite faster code submission by agents (e.g., 3 years' worth of PRs submitted in 3 days), their proposals show lower acceptance rates (trust/quality gap) and produce simpler code structures compared to developers. The dataset reveals patterns in agent behavior and collaboration challenges in real-world contexts.

Conclusion: AIDev provides a public, extensible foundation for SE 3.0 research, enabling benchmarking, scalability analysis, governance studies, and optimization of agentic systems. It supports understanding trust gaps, structural limitations, and opportunities for symbiotic human-AI collaboration in software engineering.

Abstract: The future of software engineering--SE 3.0--is unfolding with the rise of AI
teammates: autonomous, goal-driven systems collaborating with human developers.
Among these, autonomous coding agents are especially transformative, now
actively initiating, reviewing, and evolving code at scale. This paper
introduces AIDev, the first large-scale dataset capturing how such agents
operate in the wild. Spanning over 456,000 pull requests by five leading
agents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across
61,000 repositories and 47,000 developers, AIDev provides an unprecedented
empirical foundation for studying autonomous teammates in software development.
  Unlike prior work that has largely theorized the rise of AI-native software
engineering, AIDev offers structured, open data to support research in
benchmarking, agent readiness, optimization, collaboration modeling, and AI
governance. The dataset includes rich metadata on PRs, authorship, review
timelines, code changes, and integration outcomes--enabling exploration beyond
synthetic benchmarks like SWE-bench. For instance, although agents often
outperform humans in speed, their PRs are accepted less frequently, revealing a
trust and utility gap. Furthermore, while agents accelerate code
submission--one developer submitted as many PRs in three days as they had in
three years--these are structurally simpler (via code complexity metrics).
  We envision AIDev as a living resource: extensible, analyzable, and ready for
the SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev
enables a new generation of research into AI-native workflows and supports
building the next wave of symbiotic human-AI collaboration. The dataset is
publicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.
  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering
Agent

</details>


### [48] [Survey of GenAI for Automotive Software Development: From Requirements to Executable Code](https://arxiv.org/abs/2507.15025)
*Nenad Petrovic,Vahid Zolfaghari,Andre Schamschurko,Sven Kirchner,Fengjunjie Pan,Chengdng Wu,Nils Purschke,Aleksei Velsh,Krzysztof Lebioda,Yinglei Song,Yi Zhang,Lukasz Mazur,Alois Knoll*

Main category: cs.SE

TL;DR: This paper explores GenAI adoption in automotive software development, focusing on requirements handling, compliance, and code generation. It reviews LLMs, RAG, and VLMs, presents a generalized workflow, and summarizes a survey on industry tool usage.


<details>
  <summary>Details</summary>
Motivation: Automotive software development is time-consuming and costly due to complex requirements and strict standards. GenAI adoption aims to reduce human effort and streamline these processes.

Method: Literature review of GenAI technologies (LLMs, RAG, VLMs) and prompting techniques, combined with a survey of automotive industry partners to assess real-world GenAI tool implementation.

Result: Derived a GenAI-aided workflow for automotive software development and provided insights from industry partner surveys regarding current GenAI tool usage patterns.

Conclusion: GenAI offers significant potential for automotive software development but requires tailored workflows and further refinement of tools to ensure compliance while maximizing efficiency in this highly standardized domain.

Abstract: Adoption of state-of-art Generative Artificial Intelligence (GenAI) aims to
revolutionize many industrial areas by reducing the amount of human
intervention needed and effort for handling complex underlying processes.
Automotive software development is considered to be a significant area for
GenAI adoption, taking into account lengthy and expensive procedures, resulting
from the amount of requirements and strict standardization. In this paper, we
explore the adoption of GenAI for various steps of automotive software
development, mainly focusing on requirements handling, compliance aspects and
code generation. Three GenAI-related technologies are covered within the
state-of-art: Large Language Models (LLMs), Retrieval Augmented Generation
(RAG), Vision Language Models (VLMs), as well as overview of adopted prompting
techniques in case of code generation. Additionally, we also derive a
generalized GenAI-aided automotive software development workflow based on our
findings from this literature review. Finally, we include a summary of a survey
outcome, which was conducted among our automotive industry partners regarding
the type of GenAI tools used for their daily work activities.

</details>


### [49] [Can LLMs Generate User Stories and Assess Their Quality?](https://arxiv.org/abs/2507.15157)
*Giovanni Quattrocchi,Liliana Pasquale,Paola Spoletini,Luciano Baresi*

Main category: cs.SE

TL;DR: This paper investigates the use of large language models (LLMs) for automating requirements elicitation in agile development by generating and evaluating user stories (US), with results showing the potential and limitations of LLMs.


<details>
  <summary>Details</summary>
Motivation: Requirements elicitation is challenging due to complex translations of customer needs into concrete requirements, and manual assessment of semantic quality is time-consuming despite syntactic checks by automated tools.

Method: The study utilized 10 state-of-the-art LLMs to generate US by emulating customer interviews and compared their quality with human-generated US from experts and students. It also evaluated LLMs' ability to assess the semantic quality of US.

Result: LLMs generated US comparable to humans in coverage and stylistic quality but showed lower diversity and creativity. They frequently failed to meet acceptance criteria despite model size, and could assess semantic quality reliably when provided with clear criteria.

Conclusion: LLMs demonstrate potential to automate US generation and evaluation within agile frameworks, but their creativity and ability to meet acceptance criteria require improvement. Clear evaluation criteria enable reliable semantic quality assessment, reducing human effort in large-scale processes.

Abstract: Requirements elicitation is still one of the most challenging activities of
the requirements engineering process due to the difficulty requirements
analysts face in understanding and translating complex needs into concrete
requirements. In addition, specifying high-quality requirements is crucial, as
it can directly impact the quality of the software to be developed. Although
automated tools allow for assessing the syntactic quality of requirements,
evaluating semantic metrics (e.g., language clarity, internal consistency)
remains a manual and time-consuming activity. This paper explores how LLMs can
help automate requirements elicitation within agile frameworks, where
requirements are defined as user stories (US). We used 10 state-of-the-art LLMs
to investigate their ability to generate US automatically by emulating customer
interviews. We evaluated the quality of US generated by LLMs, comparing it with
the quality of US generated by humans (domain experts and students). We also
explored whether and how LLMs can be used to automatically evaluate the
semantic quality of US. Our results indicate that LLMs can generate US similar
to humans in terms of coverage and stylistic quality, but exhibit lower
diversity and creativity. Although LLM-generated US are generally comparable in
quality to those created by humans, they tend to meet the acceptance quality
criteria less frequently, regardless of the scale of the LLM model. Finally,
LLMs can reliably assess the semantic quality of US when provided with clear
evaluation criteria and have the potential to reduce human effort in
large-scale assessments.

</details>


### [50] [Deep Learning Framework Testing via Heuristic Guidance Based on Multiple Model Measurements](https://arxiv.org/abs/2507.15181)
*Yinglong Zou,Juan Zhai,Chunrong Fang,Yanzhou Mu,Jiawei Liu,Zhenyu Chen*

Main category: cs.SE

TL;DR: The paper proposes DLMMM, a novel deep learning framework testing method that addresses three limitations of existing approaches by quantitatively measuring operator combination variety, execution time, and their correlations in heuristic guidance.


<details>
  <summary>Details</summary>
Motivation: Current testing methods for deep learning frameworks use heuristic indicators to measure model bug detection effectiveness but suffer from three critical shortcomings: inadequate quantification of operator combination variety, neglect of execution time as a test input metric, and overlooking correlations between measurements. This leads to missed opportunities for identifying critical bugs and inefficient testing.

Method: DLMMM first quantitatively evaluates three model aspects (bug detection effectiveness, operator combination variety, and execution time). Then, it fuses these measurements using their correlations to achieve trade-offs through multi-level heuristic guidance during test input generation.

Result: The abstract does not specify quantitative results, but it claims DLMMM improves testing effectiveness by addressing the identified limitations of existing methods through multi-indicator measurement fusion.

Conclusion: DLMMM is the first method to systematically incorporate multiple model measurements and their trade-offs in heuristic guidance for deep learning framework testing, leading to more comprehensive and efficient bug detection.

Abstract: Deep learning frameworks serve as the foundation for developing and deploying
deep learning applications. To enhance the quality of deep learning frameworks,
researchers have proposed numerous testing methods using deep learning models
as test inputs. However, existing methods predominantly measure model bug
detection effectiveness as heuristic indicators, presenting three critical
limitations: Firstly, existing methods fail to quantitatively measure model's
operator combination variety, potentially missing critical operator
combinations that could trigger framework bugs. Secondly, existing methods
neglect measuring model execution time, resulting in the omission of numerous
models potential for detecting more framework bugs within limited testing time.
Thirdly, existing methods overlook correlation between different model
measurements, relying simply on single-indicator heuristic guidance without
considering their trade-offs. To overcome these limitations, we propose DLMMM,
the first deep learning framework testing method to include multiple model
measurements into heuristic guidance and fuse these measurements to achieve
their trade-off. DLMMM firstly quantitatively measures model's bug detection
performance, operator combination variety, and model execution time. After
that, DLMMM fuses the above measurements based on their correlation to achieve
their trade-off. To further enhance testing effectiveness, DLMMM designs
multi-level heuristic guidance for test input model generation.

</details>


### [51] [Cultural Impact on Requirements Engineering Activities: Bangladeshi Practitioners' View](https://arxiv.org/abs/2507.15188)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: This paper explores how Bangladeshi national culture influences Requirements Engineering (RE) processes in its growing IT sector, emphasizing the need to address cultural factors for effective RE practices.


<details>
  <summary>Details</summary>
Motivation: Software development projects now involve diverse stakeholders, and cultural misunderstandings in RE can lead to conflicts. Bangladesh's unique socio-cultural context lacks representation in RE research, requiring insights to future-proof practices and promote inclusion.

Method: The study investigates the adoption of RE processes in Bangladesh and identifies cultural influences affecting these activities through case studies, interviews, or observations (specific methods to be detailed in the full paper).

Result: The paper reveals specific cultural factors (to be identified) shaping RE activities in Bangladesh, contributing to understanding how localization impacts requirements gathering in under-researched regions.

Conclusion: Cultural awareness is critical for RE practitioners in Bangladesh to avoid conflicts and enhance inclusion. The study underscores the importance of tailoring RE methods to socio-cultural contexts in emerging IT sectors.

Abstract: Requirements Engineering (RE) is one of the most interaction-intensive phases
of software development. This means that RE activities might be especially
impacted by stakeholders' national culture. Software development projects
increasingly have a very diverse range of stakeholders. To future-proof RE
activities, we need to help RE practitioners avoid misunderstandings and
conflicts that might arise from not understanding potential Cultural Influences
(CIs). Moreover, an awareness of CIs supports diversity and inclusion in the IT
profession. Bangladesh has a growing IT sector with some unique socio-cultural
characteristics, and has been largely overlooked in this research field. In
this study, we aim to investigate how the RE process is adopted in the context
of Bangladeshi culture and what cultural influences impact overall RE
activities.

</details>


### [52] [Towards Using Personas in Requirements Engineering: What Has Been Changed Recently?](https://arxiv.org/abs/2507.15197)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: This SMS analyzes 22 RE papers (2023-2025) on personas and Generative AI trends, revealing increased AI use in construction/validation, popularity of template-based personas, and growing validation research focus.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to map recent RE persona research against Generative AI advancements to understand evolving practices and identify gaps in the literature.

Method: Systematic mapping study covering 22 publications from April 2023 - April 2025, analyzing persona representation, construction, validation, and RE activities.

Result: Evidence of AI-based persona construction/validation, template-based persona popularity rise, and doubling of validation-focused studies compared to prior periods.

Conclusion: The study provides insights into current persona methodologies in RE, highlighting AI integration and shifting validation priorities as critical trends for future research directions.

Abstract: In requirements engineering (RE), personas are now being used to represent
user expectations and needs. This systematic mapping study (SMS) aims to
explore the most recent studies and to cover recent changes in trends,
especially related to the recent evolution of Generative AI approaches. Our SMS
covers the period between April 2023 and April 2025. We identified 22 relevant
publications and analysed persona representation, construction, validation, as
well as RE activities covered by personas. We identified that a number of
studies applied AI-based solutions for persona construction and validation. We
observed that template-based personas are becoming more popular nowadays. We
also observed an increase in the proportion of studies covering validation
aspects.

</details>


### [53] [SimdBench: Benchmarking Large Language Models for SIMD-Intrinsic Code Generation](https://arxiv.org/abs/2507.15224)
*Yibo He,Shuoran Zhao,Jiaming Huang,Yingjie Fu,Hao Yu,Cunjian Huang,Tao Xie*

Main category: cs.SE

TL;DR: The paper introduces SimdBench, the first benchmark for evaluating SIMD-intrinsic code generation by Large Language Models (LLMs), revealing a universal performance drop in LLM-generated vectorized code compared to scalar code.


<details>
  <summary>Details</summary>
Motivation: Existing code-generation benchmarks lack SIMD-intrinsic focused tasks despite their importance in performance-critical applications. It is unclear how LLMs handle vectorized code generation, motivating the creation of SimdBench for systematic evaluation.

Method: 136 tasks targeting five SIMD architectures (SSE, AVX, Neon, SVE, RVV) were designed as a benchmark. 18 LLMs were evaluated using correctness (pass@k) and performance metrics through a structured assessment framework.

Result: LLMs show significantly lower pass@k rates for SIMD-intrinsic code generation than scalar code. Performance analysis highlights challenges in handling vectorized operations, with insights into specific failing patterns and instruction set dependencies.

Conclusion: The study establishes that LLMs struggle with maintaining code correctness in SIMD intrinsic generation. It proposes avenues for LLM improvements through SimdBench's open-source release to advance research in domain-specific code generation.

Abstract: SIMD (Single Instruction Multiple Data) instructions and their compiler
intrinsics are widely supported by modern processors to accelerate
performance-critical tasks. SIMD intrinsic programming, a trade-off between
coding productivity and high performance, is widely used in the development of
mainstream performance-critical libraries and daily computing tasks. Large
Language Models (LLMs), which have demonstrated strong and comprehensive
capabilities in code generation, show promise in assisting programmers with the
challenges of SIMD intrinsic programming. However, existing code-generation
benchmarks focus on only scalar code, and it is unclear how LLMs perform in
generating vectorized code using SIMD intrinsics. To fill this gap, we propose
SimdBench, the first code benchmark specifically designed for SIMD-intrinsic
code generation, comprising 136 carefully crafted tasks and targeting five
representative SIMD intrinsics: SSE (x86 Streaming SIMD Extension), AVX (x86
Advanced Vector Extension), Neon (ARM Advanced SIMD Extension), SVE (ARM
Scalable Vector Extension), and RVV (RISC-V Vector Extension). We conduct a
systematic evaluation (measuring both correctness and performance) of 18
representative LLMs on SimdBench, resulting in a series of novel and insightful
findings. Our evaluation results demonstrate that LLMs exhibit a universal
decrease in pass@k during SIMD-intrinsic code generation compared to
scalar-code generation. Our in-depth analysis highlights promising directions
for the further advancement of LLMs in the challenging domain of SIMD-intrinsic
code generation. SimdBench is fully open source at
https://anonymous.4open.science/r/SimdBench-1B3F/ to benefit the broader
research community.

</details>


### [54] [Code Clone Detection via an AlphaFold-Inspired Framework](https://arxiv.org/abs/2507.15226)
*Changguo Jia,Yi Zhan,Tianqi Zhao,Hengzhi Ye,Minghui Zhou*

Main category: cs.SE

TL;DR: AlphaCC adapts AlphaFold's sequence-to-structure modeling for code clone detection, enabling multi-language semantic analysis with competitive efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing code clone detection methods either lack semantic understanding or depend on language-specific analyzers, creating limitations in universal applicability and accuracy.

Method: The framework converts code to token sequences, constructs multi-sequence alignments (MSA) for enhanced context, employs a modified AlphaFold attention encoder for semantic modeling, and computes similarity scores via late interaction for binary classification of clones.

Result: Outperforms baselines in semantic clone detection across languages using three language-diverse datasets, with efficiency suitable for large-scale analysis.

Conclusion: AlphaCC demonstrates strong cross-language semantic understanding and practicality for real-world code clone detection tasks by extending AlphaFold's architectural innovations.

Abstract: Code clone detection, which aims to identify functionally equivalent code
fragments, plays a critical role in software maintenance and vulnerability
analysis. Substantial methods have been proposed to detect code clones, but
they fall short in capturing code semantics or relying on language-specific
analyzers. Inspired by the remarkable success of AlphaFold in predicting
three-dimensional protein structures from protein sequences, in this paper, we
leverage AlphaFold for code clone detection based on the insight that protein
sequences and token sequences share a common linear sequential structure. In
particular, we propose AlphaCC, which represents code fragments as token
sequences to ensure multi-language applicability and adapts AlphaFold's
sequence-to-structure modeling capability to infer code semantics. The pipeline
of AlphaCC goes through three steps. First, AlphaCC transforms each input code
fragment into a token sequence and, motivated by AlphaFold's use of multiple
sequence alignment (MSA) to enhance contextual understanding, constructs an MSA
from lexically similar token sequences. Second, AlphaCC adopts a modified
attention-based encoder based on AlphaFold to model dependencies within and
across token sequences. Finally, unlike AlphaFold's protein structure
prediction task, AlphaCC computes similarity scores between token sequences
through a late interaction strategy and performs binary classification to
determine code clone pairs. Comprehensive evaluations on three language-diverse
datasets demonstrate AlphaCC's applicability across multiple programming
languages. On two semantic clone detection datasets, it consistently
outperforms all baselines, showing strong semantic understanding. Moreover,
AlphaCC maintains competitive efficiency, enabling practical usage in
large-scale clone detection tasks.

</details>


### [55] [FaultLine: Automated Proof-of-Vulnerability Generation Using LLM Agents](https://arxiv.org/abs/2507.15241)
*Vikram Nitin,Baishakhi Ray,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: The paper introduces FaultLine, an LLM agent workflow that automatically generates proof-of-vulnerability (PoV) test cases by combining reasoning steps inspired by static/dynamic analysis, achieving a 77% improvement over existing methods on a multi-lingual dataset of 100 vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Software vulnerability reports often lack PoV tests, which are essential for validating fixes and preventing regressions. Generating these tests is challenging due to the need to reason about complex program flows across nested levels in multiple programming languages.

Method: FaultLine employs an LLM-based agent workflow with three key steps: (1) tracing input flow from API sources to vulnerability sinks, (2) analyzing branch conditions for test requirements, and (3) iteratively generating test cases using feedback loops. It avoids language-specific analysis components for cross-language generality.

Result: FaultLine successfully generated PoV tests for 16/100 vulnerabilities in Java/C/C++ projects, outperforming CodeAct 2.1 (9/100) with a 77% relative improvement. Evaluation shows hierarchical reasoning enhances LLM agents' performance in this domain.

Conclusion: While hierarchical reasoning significantly improves LLM agent test generation for program vulnerabilities, the task remains difficult across language diversities. The authors open-source their code and dataset to advance further research in automated vulnerability testing.

Abstract: Despite the critical threat posed by software security vulnerabilities,
reports are often incomplete, lacking the proof-of-vulnerability (PoV) tests
needed to validate fixes and prevent regressions. These tests are crucial not
only for ensuring patches work, but also for helping developers understand how
vulnerabilities can be exploited. Generating PoV tests is a challenging
problem, requiring reasoning about the flow of control and data through deeply
nested levels of a program.
  We present FaultLine, an LLM agent workflow that uses a set of carefully
designed reasoning steps, inspired by aspects of traditional static and dynamic
program analysis, to automatically generate PoV test cases. Given a software
project with an accompanying vulnerability report, FaultLine 1) traces the flow
of an input from an externally accessible API ("source") to the "sink"
corresponding to the vulnerability, 2) reasons about the conditions that an
input must satisfy in order to traverse the branch conditions encountered along
the flow, and 3) uses this reasoning to generate a PoV test case in a
feedback-driven loop. FaultLine does not use language-specific static or
dynamic analysis components, which enables it to be used across programming
languages.
  To evaluate FaultLine, we collate a challenging multi-lingual dataset of 100
known vulnerabilities in Java, C and C++ projects. On this dataset, FaultLine
is able to generate PoV tests for 16 projects, compared to just 9 for CodeAct
2.1, a popular state-of-the-art open-source agentic framework. Thus, FaultLine
represents a 77% relative improvement over the state of the art. Our findings
suggest that hierarchical reasoning can enhance the performance of LLM agents
on PoV test generation, but the problem in general remains challenging. We make
our code and dataset publicly available in the hope that it will spur further
research in this area.

</details>


### [56] [Input Reduction Enhanced LLM-based Program Repair](https://arxiv.org/abs/2507.15251)
*Boyang Yang,Luyao Ren,Xin Yin,Jiadong Ren,Haoye Tian,Shunfu Jin*

Main category: cs.SE

TL;DR: ReduceFix is an LLM-based APR approach that automatically reduces failure-inducing test inputs to address the 'lost-in-the-middle' issue, improving repair performance. It introduces LFTBench, a new long-input APR benchmark, and demonstrates significant improvements in input reduction and repair success rates.


<details>
  <summary>Details</summary>
Motivation: Test inputs are critical for diagnosing failure causes in APR, but LLMs lose key information with long prompts, causing the 'lost-in-the-middle' problem and reducing repair effectiveness.

Method: We designed ReduceFix with an automatic test input reducer that eliminates redundancy while preserving failure-inducing behavior. The reduced inputs guide LLM-driven patch generation, supported by LFTBench (a 1 MB median input size APR benchmark) for evaluation.

Result: ReduceFix achieves 89.1% average input reduction and boosts pass@10 by 53.8% compared to original test prompts, and 17.6% over no test inputs. Adding the reduction step to ChatRepair improves its fix rate by 21.3%. Ablation studies confirm the positive impact of reduced inputs and compressed failure information.

Conclusion: Automated test input reduction using ReduceFix is a practical enhancement for LLM-based APR, resolving scalability issues with long prompts and significantly improving repair success rates.

Abstract: Large Language Models (LLMs) have shown great potential in Automated Program
Repair (APR). Test inputs, being crucial for reasoning the root cause of
failures, are always included in the prompt for LLM-based APR. Unfortunately,
LLMs struggle to retain key information in long prompts. When the test inputs
are extensive in the prompt, this may trigger the "lost-in-the-middle" issue,
compromising repair performance. To address this, we propose ReduceFix, an
LLM-based APR approach with a built-in component that automatically reduces
test inputs while retaining their failure-inducing behavior. ReduceFix prompts
an LLM to generate a reducer that minimizes failure-inducing test inputs
without human effort, and then feeds the reduced failure-inducing inputs to
guide patch generation.
  For targeted evaluation, we constructed LFTBench, the first long-input APR
benchmark with 200 real bugs from 20 programming tasks, each paired with a
failure-inducing input whose median size is 1 MB. On this benchmark, ReduceFix
shrinks inputs by 89.1% on average and improves overall pass@10 by up to 53.8%
relative to a prompt that includes the original test, and by 17.6% compared
with omitting the test entirely. Adding the same reduction step to ChatRepair
increases its fix rate by 21.3% without other changes. Ablation studies further
highlight the impact of input length and compressed failure information on
repair success. These results underscore that automatically reducing failing
inputs is a practical and powerful complement to LLM-based APR, significantly
improving its scalability and effectiveness.

</details>


### [57] [Butterfly Effects in Toolchains: A Comprehensive Analysis of Failed Parameter Filling in LLM Tool-Agent Systems](https://arxiv.org/abs/2507.15296)
*Qian Xiong,Yuekai Huang,Ziyou Jiang,Zhiyuan Chang,Yujia Zheng,Tianhao Li,Mingyang Li*

Main category: cs.SE

TL;DR: This paper investigates parameter failure in tool agent -LLM systems, proposes a taxonomy of five failure types, identifies input source issues as a key cause, and suggests format standardization, enhanced error feedback, and parameter consistency to improve reliability.


<details>
  <summary>Details</summary>
Motivation: Expanding LLM capabilities via tool agents is limited by execution parameter failures. Existing paradigms lack systematic failure analysis and solutions for complex tasks.

Method: 1) Construct parameter failure taxonomy by analyzing invocation chains of mainstream tool agents. 2) Apply 15 input perturbation methods across three input sources. 3) Quantitatively evaluate failure patterns.

Result: 1) 5 distinct failure categories identified. 2) Parameter name hallucination attributed to inherent LLM issues. 3) 78.3% of non-hallucination failures trace to input source problems. 4) Perturbation experiments isolate failure causality.

Conclusion: Improving tool agent reliability requires: standardized return formats, context-aware error feedback systems, and parameter validation protocols to address source-induced failures and inherent LLM limitations.

Abstract: The emergence of the tool agent paradigm has broadened the capability
boundaries of the Large Language Model (LLM), enabling it to complete more
complex tasks. However, the effectiveness of this paradigm is limited due to
the issue of parameter failure during its execution. To explore this phenomenon
and propose corresponding suggestions, we first construct a parameter failure
taxonomy in this paper. We derive five failure categories from the invocation
chain of a mainstream tool agent. Then, we explore the correlation between
three different input sources and failure categories by applying 15 input
perturbation methods to the input. Experimental results show that parameter
name hallucination failure primarily stems from inherent LLM limitations, while
issues with input sources mainly cause other failure patterns. To improve the
reliability and effectiveness of tool-agent interactions, we propose
corresponding improvement suggestions, including standardizing tool return
formats, improving error feedback mechanisms, and ensuring parameter
consistency.

</details>


### [58] [StackTrans: From Large Language Model to Large Pushdown Automata Model](https://arxiv.org/abs/2507.15343)
*Kechi Zhang,Ge Li,Jia Li,Huangzhao Zhang,Yihong Dong,Jia Li,Jingjing Xu,Zhi Jin*

Main category: cs.SE

TL;DR: StackTrans is a novel architectural enhancement to the Transformer that integrates differentiable hidden state stacks to better capture Chomsky hierarchy properties, outperforming both standard Transformers and larger LLMs on structured and natural language benchmarks.


<details>
  <summary>Details</summary>
Motivation: The Transformer architecture's inherent limitations in modeling sequential dependencies beyond context-free grammars (e.g. failing to properly handle regular expressions) prevent natural integration with formal language structures, limiting its linguistic modeling capabilities.

Method: We introduce hidden state stacks between standard Transformer blocks, enabling differentiable push/pop operations on computational memory. Stack operations are implemented as learnable differentiable modules that integrate seamlessly with attention mechanisms and maintain compatibility with flash-attention optimizations.

Result: StackTrans achieves state-of-the-art performance on both formal grammar (Chomsky hierarchy) and natural language benchmarks. The 360M parameter model outperforms multiple open-source LLMs 2-3× bigger on parameter efficiency metrics, showing comparable performance with fewer parameters.

Conclusion: StackTrans demonstrates that stack-based memory augmentation is a viable path to improve parameter efficiency and structured reasoning capabilities in large language models, maintaining full compatibility with existing Transformer tooling and frameworks.

Abstract: The Transformer architecture has emerged as a landmark advancement within the
broad field of artificial intelligence, effectively catalyzing the advent of
large language models (LLMs). However, despite its remarkable capabilities and
the substantial progress it has facilitated, the Transformer architecture still
has some limitations. One such intrinsic limitation is its inability to
effectively capture the Chomsky hierarchy, such as regular expressions or
deterministic context-free grammars. Drawing inspiration from pushdown
automata, which efficiently resolve deterministic context-free grammars using
stacks, we propose StackTrans to address the aforementioned issue within LLMs.
Unlike previous approaches that modify the attention computation, StackTrans
explicitly incorporates hidden state stacks between Transformer layers. This
design maintains compatibility with existing frameworks like flash-attention.
Specifically, our design features stack operations -- such as pushing and
popping hidden states -- that are differentiable and can be learned in an
end-to-end manner. Our comprehensive evaluation spans benchmarks for both
Chomsky hierarchies and large-scale natural languages. Across these diverse
tasks, StackTrans consistently outperforms standard Transformer models and
other baselines. We have successfully scaled StackTrans up from 360M to 7B
parameters. In particular, our from-scratch pretrained model StackTrans-360M
outperforms several larger open-source LLMs with 2-3x more parameters,
showcasing its superior efficiency and reasoning capability.

</details>


### [59] [Applying the Chinese Wall Reverse Engineering Technique to Large Language Model Code Editing](https://arxiv.org/abs/2507.15599)
*Manatsawin Hanmongkolchai*

Main category: cs.SE

TL;DR: The paper proposes using the "Chinese Wall" technique to enhance ethical code LLMs by leveraging detailed instructions from stronger models, achieving performance boosts on benchmarks despite training data limitations.


<details>
  <summary>Details</summary>
Motivation: Top code LLMs use undisclosed training data, raising copyright concerns. Existing ethical models with curated, licensed data (e.g., Comma, Pleias) are less competitive due to limited training data.

Method: Apply reverse-engineered "Chinese Wall" approach: a high-quality model generates fine-grained instructions for a weaker model to execute complex tasks ethically.

Result: Comma v0.1 1T's CanItEdit benchmark performance improved by 66%, and Starcoder2 Instruct by 20% when using Chinese Wall instructions, outperforming standalone execution.

Conclusion: Chinese Wall technique shows promise for improving ethical code LLMs, but practical adoption hinges on wider availability of open-access, copyright-free training models.

Abstract: Large language models for code (Code LLM) are increasingly utilized in
programming environments. Despite their utility, the training datasets for top
LLM remain undisclosed, raising concerns about potential copyright violations.
Some models, such as Pleias and Comma put emphasis on data curation and
licenses, however, with limited training data these models are not competitive
and only serve as proof of concepts. To improve the utility of these models, we
propose an application of the "Chinese Wall" technique, inspired by the reverse
engineering technique of the same name -- a high quality model is used to
generate detailed instructions for a weaker model. By doing so, a weaker but
ethically aligned model may be used to perform complicated tasks that,
otherwise, can only be completed by more powerful models. In our evaluation,
we've found that this technique improves Comma v0.1 1T's performance in
CanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%
compared to when running the same model on the benchmark alone. The practical
application of this technique today, however, may be limited due to the lack of
models trained on public domain content without copyright restrictions.

</details>


### [60] [Hot Topics and Common Challenges: an Empirical Study of React Discussions on Stack Overflow](https://arxiv.org/abs/2507.15624)
*Yusuf Sulistyo Nugroho,Ganno Tribuana Kurniaji,Syful Islam,Mohammed Humayun Kabir,Vanesya Aura Ardity,Md. Kamal Uddin*

Main category: cs.SE

TL;DR: This study analyzes React-related Stack Overflow questions to identify common challenges and errors, revealing top keywords and algorithmic errors as the primary issues faced by mid-reputation users


<details>
  <summary>Details</summary>
Motivation: Despite React's popularity, specific user challenges remain underexplored, motivating this analysis to inform community support and future research

Method: Exploratory data analysis of React-related Stack Overflow questions, focusing on keyword frequency, error classification, and novel reputation-based error correlations

Result: Top 8 keywords included code/link-related terms; algorithmic errors (55.77% from mid-reputation users) were most prevalent category; comprehensive error typology established

Conclusion: Highlights need for targeted algorithmic error guidance resources and provides foundational insights for improving React adoption support through early implementation assistance

Abstract: React is a JavaScript library used to build user interfaces for single-page
applications. Although recent studies have shown the popularity and advantages
of React in web development, the specific challenges users face remain unknown.
Thus, this study aims to analyse the React-related questions shared on Stack
Overflow. The study utilizes an exploratory data analysis to investigate the
most frequently discussed keywords, error classification, and user
reputation-based errors, which is the novelty of this work. The results show
the top eight most frequently used keywords on React-related questions, namely,
code, link, vir, href, connect, azure, windows, and website. The error
classification of questions from the sample shows that algorithmic error is the
most frequent issue faced by all groups of users, where mid-reputation users
contribute the most, accounting for 55.77%. This suggests the need for the
community to provide guidance materials in solving algorithm-related problems.
We expect that the results of this study will provide valuable insight into
future research to support the React community during the early stages of
implementation, facilitating their ability to effectively overcome challenges
to adoption.

</details>


### [61] [SustainDiffusion: Optimising the Social and Environmental Sustainability of Stable Diffusion Models](https://arxiv.org/abs/2507.15663)
*Giordano d'Aloisio,Tosin Fadahunsi,Jay Choy,Rebecca Moussa,Federica Sarro*

Main category: cs.SE

TL;DR: SustainDiffusion is a search-based method to reduce gender/ethnic bias and energy consumption in Stable Diffusion (SD) text-to-image generation while maintaining comparable image quality.


<details>
  <summary>Details</summary>
Motivation: Widespread use of SD models raises social and environmental sustainability concerns due to potential biases and high energy consumption associated with their operation.

Method: The approach employs hyperparameter and prompt structure optimization through search algorithms to systematically reduce bias metrics and energy consumption during image generation, avoiding model retraining.

Result: Empirical testing on 56 prompts shows SustainDiffusion reduces gender bias in SD3 by 68%, ethnic bias by 59%, and energy consumption by 48%, with consistent results across multiple runs and prompt generalization.

Conclusion: Demonstrates the feasibility of improving social-environmental sustainability in text-to-image generation models without architectural changes to the base model.

Abstract: Background: Text-to-image generation models are widely used across numerous
domains. Among these models, Stable Diffusion (SD) - an open-source
text-to-image generation model - has become the most popular, producing over 12
billion images annually. However, the widespread use of these models raises
concerns regarding their social and environmental sustainability.
  Aims: To reduce the harm that SD models may have on society and the
environment, we introduce SustainDiffusion, a search-based approach designed to
enhance the social and environmental sustainability of SD models.
  Method: SustainDiffusion searches the optimal combination of hyperparameters
and prompt structures that can reduce gender and ethnic bias in generated
images while also lowering the energy consumption required for image
generation. Importantly, SustainDiffusion maintains image quality comparable to
that of the original SD model.
  Results: We conduct a comprehensive empirical evaluation of SustainDiffusion,
testing it against six different baselines using 56 different prompts. Our
results demonstrate that SustainDiffusion can reduce gender bias in SD3 by 68%,
ethnic bias by 59%, and energy consumption (calculated as the sum of CPU and
GPU energy) by 48%. Additionally, the outcomes produced by SustainDiffusion are
consistent across multiple runs and can be generalised to various prompts.
  Conclusions: With SustainDiffusion, we demonstrate how enhancing the social
and environmental sustainability of text-to-image generation models is possible
without fine-tuning or changing the model's architecture.

</details>


### [62] [Modeling CubeSat Storage Battery Discharge: Equivalent Circuit Versus Machine Learning Approaches](https://arxiv.org/abs/2507.15666)
*Igor Turkin,Lina Volobuieva,Andriy Chukhray,Oleksandr Liubimov*

Main category: cs.SE

TL;DR: The paper compares analytical equivalent circuit and machine learning models for CubeSat battery discharge, concluding that machine learning offers higher accuracy and adaptability despite lower transparency.


<details>
  <summary>Details</summary>
Motivation: Reliable power system modeling in CubeSats is critical for predicting the impact of autonomous power system disconnections and ensuring in-orbit fault tolerance, making this research relevant for satellite missions.

Method: The study uses orbital power system data from CubeSats (voltage, current, solar panel temperatures) to analyze two modeling approaches: analytical modeling based on physical laws and machine learning using empirical data.

Result: Equivalent circuit models provided transparent parameter-based insights but struggled with environmental variability, whereas machine learning models delivered superior accuracy by adapting to actual mission conditions, even when they departed from theoretical expectations.

Conclusion: Machine learning models are recommended over analytical methods for CubeSat battery discharge prediction due to their superior accuracy and adaptability to real-world conditions, despite being less interpretable.

Abstract: The subject of the article is the study and comparison of two approaches to
modelling the battery discharge of a CubeSat satellite: analytical using
equivalent circuit and machine learning. The article aims to make a reasoned
choice of the approach to modelling the battery discharge of a CubeSat
satellite. Modelling the battery discharge of a satellite will enable the
prediction of the consequences of disconnecting the autonomous power system and
ensure the fault tolerance of equipment in orbit. Therefore, the selected study
is relevant and promising. This study focuses on the analysis of CubeSat
satellite data, based explicitly on orbital data samples of the power system,
which include data available at the time of the article publication. The
dataset contains data on the voltage, current, and temperature of the battery
and solar panels attached to the five sides of the satellite. In this context,
two approaches are considered: analytical modelling based on physical laws and
machine learning, which uses empirical data to create a predictive model.
Results: A comparative analysis of the modeling results reveals that the
equivalent circuit approach has the advantage of transparency, as it identifies
possible parameters that facilitate understanding of the relationships.
However, the model is less flexible to environmental changes or non-standard
satellite behavior. The machine learning model demonstrated more accurate
results, as it can account for complex dependencies and adapt to actual
conditions, even when they deviate from theoretical assumptions.

</details>


### [63] [BugScope: Learn to Find Bugs Like Human](https://arxiv.org/abs/2507.15671)
*Jinyao Guo,Chengpeng Wang,Dominic Deluca,Jinjie Liu,Zhuo Zhang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: BugScope is an LLM-driven multi-agent system that improves software bug detection by emulating human auditors' learning from examples. It synthesizes retrieval strategies via program slicing and tailored prompts, achieving high precision and recall on real-world bugs and uncovering previously unknown issues in large-scale projects.


<details>
  <summary>Details</summary>
Motivation: Traditional static analysis tools lack coverage and adaptability for diverse bug patterns, while existing LLM-based methods struggle with sophistication and limited analysis contexts. There is a need for a system that can dynamically learn and apply new human-like auditing strategies.

Method: The system uses program slicing to extract relevant contexts from example bug cases and constructs customized detection prompts that guide LLM reasoning. By imitating how human auditors learn from examples, it enhances detection accuracy for both known and novel bug patterns.

Result: Demonstrated 87.04% precision and 90.00% recall on 40 real-world bugs from 21 projects. Discovered 141 unknown bugs in systems like the Linux kernel, with 78 fixed and 7 confirmed by developers, surpassing industrial tools in F1 score by 0.44.

Conclusion: BugScope's ability to learn from examples and generate tailored analysis workflows enables high-performance, scalable bug detection that outperforms state-of-the-art tools while achieving significant real-world bug discovery impact.

Abstract: Detecting software bugs remains a fundamental challenge due to the extensive
diversity of real-world defects. Traditional static analysis tools often rely
on symbolic workflows, which restrict their coverage and hinder adaptability to
customized bugs with diverse anti-patterns. While recent advances incorporate
large language models (LLMs) to enhance bug detection, these methods continue
to struggle with sophisticated bugs and typically operate within limited
analysis contexts. To address these challenges, we propose BugScope, an
LLM-driven multi-agent system that emulates how human auditors learn new bug
patterns from representative examples and apply that knowledge during code
auditing. Given a set of examples illustrating both buggy and non-buggy
behaviors, BugScope synthesizes a retrieval strategy to extract relevant
detection contexts via program slicing and then constructs a tailored detection
prompt to guide accurate reasoning by the LLM. Our evaluation on a curated
dataset of 40 real-world bugs drawn from 21 widely-used open-source projects
demonstrates that BugScope achieves 87.04% precision and 90.00% recall,
surpassing state-of-the-art industrial tools by 0.44 in F1 score. Further
testing on large-scale open-source systems, including the Linux kernel,
uncovered 141 previously unknown bugs, of which 78 have been fixed and 7
confirmed by developers, highlighting BugScope's substantial practical impact.

</details>


### [64] [Do AI models help produce verified bug fixes?](https://arxiv.org/abs/2507.15822)
*Li Huang,Ilgiz Mustafin,Marco Piccioni,Alessandro Schena,Reto Weber,Bertrand Meyer*

Main category: cs.SE

TL;DR: This paper investigates whether Large Language Models (LLMs) improve Automatic Program Repair (APR) in practice by comparing two groups of programmers with/without LLM access during debugging tasks, and identifies seven distinct LLM usage patterns with validated guidance for effective APR integration. The study uses a goal-based methodology with formal proof validation.


<details>
  <summary>Details</summary>
Motivation: The study addresses the need to understand if AI/LLM integration into APR delivers practical benefits, and how programmers leverage LLMs to augment their skills when available.

Method: A controlled experiment comparing programmers with/without LLM access using a Goal-Query-Metric framework, where both groups proved correctness of their fixes. Full-session recordings enabled fine-grained analysis of behavior and LLM usage patterns.

Result: Surprising results showed varying LLM effectiveness. The study revealed seven distinct usage patterns, demonstrated the potential of LLMs in APR, and provided validated guidance for their optimal use. However, the limited sample size restricts generalization.

Conclusion: The work establishes foundational methodology for assessing LLM use in APR, highlights unexpected practical roles for AI in debugging, and defines actionable patterns and advice for integrating LLMs into repair workflows. It represents an early exploration toward guaranteed-correct automated repairs.

Abstract: Among areas of software engineering where AI techniques -- particularly,
Large Language Models -- seem poised to yield dramatic improvements, an
attractive candidate is Automatic Program Repair (APR), the production of
satisfactory corrections to software bugs. Does this expectation materialize in
practice? How do we find out, making sure that proposed corrections actually
work? If programmers have access to LLMs, how do they actually use them to
complement their own skills?
  To answer these questions, we took advantage of the availability of a
program-proving environment, which formally determines the correctness of
proposed fixes, to conduct a study of program debugging with two randomly
assigned groups of programmers, one with access to LLMs and the other without,
both validating their answers through the proof tools. The methodology relied
on a division into general research questions (Goals in the Goal-Query-Metric
approach), specific elements admitting specific answers (Queries), and
measurements supporting these answers (Metrics). While applied so far to a
limited sample size, the results are a first step towards delineating a proper
role for AI and LLMs in providing guaranteed-correct fixes to program bugs.
  These results caused surprise as compared to what one might expect from the
use of AI for debugging and APR. The contributions also include: a detailed
methodology for experiments in the use of LLMs for debugging, which other
projects can reuse; a fine-grain analysis of programmer behavior, made possible
by the use of full-session recording; a definition of patterns of use of LLMs,
with 7 distinct categories; and validated advice for getting the best of LLMs
for debugging and Automatic Program Repair.

</details>


### [65] [Investigating the Use of LLMs for Evidence Briefings Generation in Software Engineering](https://arxiv.org/abs/2507.15828)
*Mauro Marcelino,Marcos Alves,Bianca Trinkenreich,Bruno Cartaxo,Sérgio Soares,Simone D. J. Barbosa,Marcos Kalinowski*

Main category: cs.SE

TL;DR: This paper designs an experiment to compare the effectiveness of LLM-generated evidence briefings against human-created ones in terms of content fidelity, ease of understanding, and usefulness for secondary studies.


<details>
  <summary>Details</summary>
Motivation: Evidence briefings are labor-intensive to produce manually, limiting their adoption despite their value to industry software engineers.

Method: A RAG-based LLM tool was built to auto-generate previous human-created briefings, followed by a controlled experiment to evaluate LLM vs. human briefings using perception metrics.

Result: Results depend on post-experiment analysis (not yet available).

Conclusion: Conclusion will depend on the experimental results comparing LLM and human-generated briefings.

Abstract: [Context] An evidence briefing is a concise and objective transfer medium
that can present the main findings of a study to software engineers in the
industry. Although practitioners and researchers have deemed Evidence Briefings
useful, their production requires manual labor, which may be a significant
challenge to their broad adoption. [Goal] The goal of this registered report is
to describe an experimental protocol for evaluating LLM-generated evidence
briefings for secondary studies in terms of content fidelity, ease of
understanding, and usefulness, as perceived by researchers and practitioners,
compared to human-made briefings. [Method] We developed an RAG-based LLM tool
to generate evidence briefings. We used the tool to automatically generate two
evidence briefings that had been manually generated in previous research
efforts. We designed a controlled experiment to evaluate how the LLM-generated
briefings compare to the human-made ones regarding perceived content fidelity,
ease of understanding, and usefulness. [Results] To be reported after the
experimental trials. [Conclusion] Depending on the experiment results.

</details>


### [66] [Observing Fine-Grained Changes in Jupyter Notebooks During Development Time](https://arxiv.org/abs/2507.15831)
*Sergey Titov,Konstantin Grotov,Cristina Sarasua,Yaroslav Golubev,Dhivyabharathi Ramasamy,Alberto Bacchelli,Abraham Bernstein,Timofey Bryksin*

Main category: cs.SE

TL;DR: This study introduces a toolset to analyze changes in Jupyter notebooks during data science development, revealing patterns of small iterative fixes and debugging behaviors. A dataset of 9,207 cell executions from 20 developers is presented, along with insights and future research directions.


<details>
  <summary>Details</summary>
Motivation: Fine-grained log analysis has enabled advancements in software engineering domains but remains unexplored in computational notebooks used for data science projects, despite their popularity for development and exploration.

Method: The researchers (1) developed a toolset to collect Jupyter notebook code changes during development, (2) captured over 100 hours of real-world data science work from 20 diverse developers producing 2,655 cells and 9,207 executions, and (3) analyzed these data to characterize notebook development dynamics.

Result: Analysis found that 28% of cell executions were followed by code modifications within cells, with 64% involving minor fixes like syntax errors, variable name updates, and small logic changes (median 3 lines changed). This shows notebooks are frequently used for iterative refinement and debugging beyond their core development/analysis role.

Conclusion: Notebooks enable rapid code iteration and debugging, which has implications for tool design in data science environments. The paper provides the first empirical characterization of Jupyter notebook development practices and suggests future studies to explore how these patterns affect collaboration, reproducibility, and workflow optimization.

Abstract: In software engineering, numerous studies have focused on the analysis of
fine-grained logs, leading to significant innovations in areas such as
refactoring, security, and code completion. However, no similar studies have
been conducted for computational notebooks in the context of data science.
  To help bridge this research gap, we make three scientific contributions: we
(1) introduce a toolset for collecting code changes in Jupyter notebooks during
development time; (2) use it to collect more than 100 hours of work related to
a data analysis task and a machine learning task (carried out by 20 developers
with different levels of expertise), resulting in a dataset containing 2,655
cells and 9,207 cell executions; and (3) use this dataset to investigate the
dynamic nature of the notebook development process and the changes that take
place in the notebooks.
  In our analysis of the collected data, we classified the changes made to the
cells between executions and found that a significant number of these changes
were relatively small fixes and code iteration modifications. This suggests
that notebooks are used not only as a development and exploration tool but also
as a debugging tool. We report a number of other insights and propose potential
future research directions on the novel data.

</details>
