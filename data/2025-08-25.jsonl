{"id": "2508.15941", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15941", "abs": "https://arxiv.org/abs/2508.15941", "authors": ["Imen Trabelsi", "Brahim Mahmoudi", "Jean Baptiste Minani", "Naouel Moha", "Yann-Gaël Guéhéneuc"], "title": "A Systematic Literature Review of Machine Learning Approaches for Migrating Monolithic Systems to Microservices", "comment": null, "summary": "Scalability and maintainability challenges in monolithic systems have led to\nthe adoption of microservices, which divide systems into smaller, independent\nservices. However, migrating existing monolithic systems to microservices is a\ncomplex and resource-intensive task, which can benefit from machine learning\n(ML) to automate some of its phases. Choosing the right ML approach for\nmigration remains challenging for practitioners. Previous works studied\nseparately the objectives, artifacts, techniques, tools, and benefits and\nchallenges of migrating monolithic systems to microservices. No work has yet\ninvestigated systematically existing ML approaches for this migration to\nunderstand the \\revised{automated migration phases}, inputs used, ML techniques\napplied, evaluation processes followed, and challenges encountered. We present\na systematic literature review (SLR) that aggregates, synthesises, and\ndiscusses the approaches and results of 81 primary studies (PSs) published\nbetween 2015 and 2024. We followed the Preferred Reporting Items for Systematic\nReview and Meta-Analysis (PRISMA) statement to report our findings and answer\nour research questions (RQs). We extract and analyse data from these PSs to\nanswer our RQs. We synthesise the findings in the form of a classification that\nshows the usage of ML techniques in migrating monolithic systems to\nmicroservices. The findings reveal that some phases of the migration process,\nsuch as monitoring and service identification, are well-studied, while others,\nlike packaging microservices, remain unexplored. Additionally, the findings\nhighlight key challenges, including limited data availability, scalability and\ncomplexity constraints, insufficient tool support, and the absence of\nstandardized benchmarking, emphasizing the need for more holistic solutions."}
{"id": "2508.16025", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16025", "abs": "https://arxiv.org/abs/2508.16025", "authors": ["Saba Naqvi", "Mohammad Baqar"], "title": "Breaking Barriers in Software Testing: The Power of AI-Driven Automation", "comment": "10 Pages", "summary": "Software testing remains critical for ensuring reliability, yet traditional\napproaches are slow, costly, and prone to gaps in coverage. This paper presents\nan AI-driven framework that automates test case generation and validation using\nnatural language processing (NLP), reinforcement learning (RL), and predictive\nmodels, embedded within a policy-driven trust and fairness model. The approach\ntranslates natural language requirements into executable tests, continuously\noptimizes them through learning, and validates outcomes with real-time analysis\nwhile mitigating bias. Case studies demonstrate measurable gains in defect\ndetection, reduced testing effort, and faster release cycles, showing that\nAI-enhanced testing improves both efficiency and reliability. By addressing\nintegration and scalability challenges, the framework illustrates how AI can\nshift testing from a reactive, manual process to a proactive, adaptive system\nthat strengthens software quality in increasingly complex environments."}
{"id": "2508.16053", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16053", "abs": "https://arxiv.org/abs/2508.16053", "authors": ["Shadikur Rahman", "Umme Ayman Koana", "Hasibul Karim Shanto", "Mahmuda Akter", "Chitra Roy", "Aras M. Ismael"], "title": "Measuring the effectiveness of code review comments in GitHub repositories: A machine learning approach", "comment": null, "summary": "This paper illustrates an empirical study of the working efficiency of\nmachine learning techniques in classifying code review text by semantic\nmeaning. The code review comments from the source control repository in GitHub\nwere extracted for development activity from the existing year for three\nopen-source projects. Apart from that, programmers need to be aware of their\ncode and point out their errors. In that case, it is a must to classify the\nsentiment polarity of the code review comments to avoid an error. We manually\nlabelled 13557 code review comments generated by three open source projects in\nGitHub during the existing year. In order to recognize the sentiment polarity\n(or sentiment orientation) of code reviews, we use seven machine learning\nalgorithms and compare those results to find the better ones. Among those\nLinear Support Vector Classifier(SVC) classifier technique achieves higher\naccuracy than others. This study will help programmers to make any solution\nbased on code reviews by avoiding misconceptions."}
{"id": "2508.16071", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16071", "abs": "https://arxiv.org/abs/2508.16071", "authors": ["Mahinthan Chandramohan", "Jovan Jancic", "Yuntong Zhang", "Padmanabhan Krishnan"], "title": "From Benchmark Data To Applicable Program Repair: An Experience Report", "comment": null, "summary": "This paper describes our approach to automated program repair. We combine\nvarious techniques from the literature to achieve this. Our experiments show\nthat our approach performs better than other techniques on standard benchmarks.\nHowever, on closer inspection, none of these techniques work on realistic\ndefects that we see in industry.\n  We find that augmenting code with formal specifications enables LLMs to\ngenerate higher-quality unit tests, especially for complex production code with\nimproved coverage of edge cases and exception handling. However, specifications\nadd little value for well-understood errors (e.g., null pointer, index out of\nbounds), but are beneficial for logic and string manipulation errors. Despite\nencouraging benchmark results, real-world adoption is limited since passing\ntests do not guarantee correct patches. Current challenges include insufficient\nexpressiveness of the JML specification language, necessitating advanced\nverification tools and richer predicates. Our ongoing work is exploring\ncontract automata, programming by example, and testcase repair, with a focus on\nintegrating human feedback and measuring productivity gains - highlighting the\ngap between academic benchmarks and practical industry needs"}
{"id": "2508.15776", "categories": ["cs.CR", "cs.CE"], "pdf": "https://arxiv.org/pdf/2508.15776", "abs": "https://arxiv.org/abs/2508.15776", "authors": ["Saeid Ghasemshirazi", "Ghazaleh Shirvani", "Marziye Ranjbar Tavakoli", "Bahar Ghaedi", "Mohammad Amin Langarizadeh"], "title": "Implementing Zero Trust Architecture to Enhance Security and Resilience in the Pharmaceutical Supply Chain", "comment": null, "summary": "The pharmaceutical supply chain faces escalating cybersecurity challenges\nthreatening patient safety and operational continuity. This paper examines the\ntransformative potential of zero trust architecture for enhancing security and\nresilience within this critical ecosystem. We explore the challenges posed by\ndata breaches, counterfeiting, and disruptions and introduce the principles of\ncontinuous verification, least-privilege access, and data-centric security\ninherent in zero trust. Real-world case studies illustrate successful\nimplementations. Benefits include heightened security, data protection, and\nadaptable resilience. As recognized by researchers and industrialists, a\nreliable drug tracing system is crucial for ensuring drug safety throughout the\npharmaceutical production process. One of the most pivotal domains within the\npharmaceutical industry and its associated supply chains where zero trust can\nbe effectively implemented is in the management of narcotics, high-health-risk\ndrugs, and abusable substances. By embracing zero trust, the pharmaceutical\nindustry fortifies its supply chain against constantly changing cyber threats,\nensuring the trustworthiness of critical medical operations."}
{"id": "2508.16104", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.16104", "abs": "https://arxiv.org/abs/2508.16104", "authors": ["Arturo Miguel Russell Bernal", "Maureen Petterson", "Pedro Antonio Alarcon Granadeno", "Michael Murphy", "James Mason", "Jane Cleland-Huang"], "title": "Validating Terrain Models in Digital Twins for Trustworthy sUAS Operations", "comment": "Submitted to EDTconf 2025", "summary": "With the increasing deployment of small Unmanned Aircraft Systems (sUAS) in\nunfamiliar and complex environments, Environmental Digital Twins (EDT) that\ncomprise weather, airspace, and terrain data are critical for safe flight\nplanning and for maintaining appropriate altitudes during search and\nsurveillance operations. With the expansion of sUAS capabilities through edge\nand cloud computing, accurate EDT are also vital for advanced sUAS\ncapabilities, like geolocation. However, real-world sUAS deployment introduces\nsignificant sources of uncertainty, necessitating a robust validation process\nfor EDT components. This paper focuses on the validation of terrain models, one\nof the key components of an EDT, for real-world sUAS tasks. These models are\nconstructed by fusing U.S. Geological Survey (USGS) datasets and satellite\nimagery, incorporating high-resolution environmental data to support mission\ntasks. Validating both the terrain models and their operational use by sUAS\nunder real-world conditions presents significant challenges, including limited\ndata granularity, terrain discontinuities, GPS and sensor inaccuracies, visual\ndetection uncertainties, as well as onboard resources and timing constraints.\nWe propose a 3-Dimensions validation process grounded in software engineering\nprinciples, following a workflow across granularity of tests, simulation to\nreal world, and the analysis of simple to edge conditions. We demonstrate our\napproach using a multi-sUAS platform equipped with a Terrain-Aware Digital\nShadow."}
{"id": "2508.15778", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15778", "abs": "https://arxiv.org/abs/2508.15778", "authors": ["Yifan Liao", "Yuxin Cao", "Yedi Zhang", "Wentao He", "Yan Xiao", "Xianglong Du", "Zhiyong Huang", "Jin Song Dong"], "title": "Towards Stealthy and Effective Backdoor Attacks on Lane Detection: A Naturalistic Data Poisoning Approach", "comment": "12 pages,7 figures", "summary": "Deep learning-based lane detection (LD) plays a critical role in autonomous\ndriving and advanced driver assistance systems. However, its vulnerability to\nbackdoor attacks presents a significant security concern. Existing backdoor\nattack methods on LD often exhibit limited practical utility due to the\nartificial and conspicuous nature of their triggers. To address this limitation\nand investigate the impact of more ecologically valid backdoor attacks on LD\nmodels, we examine the common data poisoning attack and introduce DBALD, a\nnovel diffusion-based data poisoning framework for generating naturalistic\nbackdoor triggers. DBALD comprises two key components: optimal trigger position\nfinding and stealthy trigger generation. Given the insight that attack\nperformance varies depending on the trigger position, we propose a\nheatmap-based method to identify the optimal trigger location, with gradient\nanalysis to generate attack-specific heatmaps. A region-based editing diffusion\nprocess is then applied to synthesize visually plausible triggers within the\nmost susceptible regions identified previously. Furthermore, to ensure scene\nintegrity and stealthy attacks, we introduce two loss strategies: one for\npreserving lane structure and another for maintaining the consistency of the\ndriving scene. Consequently, compared to existing attack methods, DBALD\nachieves both a high attack success rate and superior stealthiness. Extensive\nexperiments on 4 mainstream LD models show that DBALD exceeds state-of-the-art\nmethods, with an average success rate improvement of +10.87% and significantly\nenhanced stealthiness. The experimental results highlight significant practical\nchallenges in ensuring model robustness against real-world backdoor threats in\nLD."}
{"id": "2508.16131", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16131", "abs": "https://arxiv.org/abs/2508.16131", "authors": ["Zoe Kotti", "Konstantina Dritsa", "Diomidis Spinellis", "Panos Louridas"], "title": "The Fools are Certain; the Wise are Doubtful: Exploring LLM Confidence in Code Completion", "comment": "30 pages, 10 figures", "summary": "Code completion entails the task of providing missing tokens given a\nsurrounding context. It can boost developer productivity while providing a\npowerful code discovery tool. Following the Large Language Model (LLM) wave,\ncode completion has been approached with diverse LLMs fine-tuned on code (code\nLLMs). The performance of code LLMs can be assessed with downstream and\nintrinsic metrics. Downstream metrics are usually employed to evaluate the\npractical utility of a model, but can be unreliable and require complex\ncalculations and domain-specific knowledge. In contrast, intrinsic metrics such\nas perplexity, entropy, and mutual information, which measure model confidence\nor uncertainty, are simple, versatile, and universal across LLMs and tasks, and\ncan serve as proxies for functional correctness and hallucination risk in\nLLM-generated code. Motivated by this, we evaluate the confidence of LLMs when\ngenerating code by measuring code perplexity across programming languages,\nmodels, and datasets using various LLMs, and a sample of 1008 files from 657\nGitHub projects. We find that strongly-typed languages exhibit lower perplexity\nthan dynamically typed languages. Scripting languages also demonstrate higher\nperplexity. Perl appears universally high in perplexity, whereas Java appears\nlow. Code perplexity depends on the employed LLM, but not on the code dataset.\nAlthough code comments often increase perplexity, the language ranking based on\nperplexity is barely affected by their presence. LLM researchers, developers,\nand users can employ our findings to assess the benefits and suitability of\nLLM-based code completion in specific software projects based on how language,\nmodel choice, and code characteristics impact model confidence."}
{"id": "2508.15808", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15808", "abs": "https://arxiv.org/abs/2508.15808", "authors": ["Benjamin Murphy", "Twm Stone"], "title": "Uplifted Attackers, Human Defenders: The Cyber Offense-Defense Balance for Trailing-Edge Organizations", "comment": null, "summary": "Advances in AI are widely understood to have implications for cybersecurity.\nArticles have emphasized the effect of AI on the cyber offense-defense balance,\nand commentators can be found arguing either that cyber will privilege\nattackers or defenders. For defenders, arguments are often made that AI will\nenable solutions like formal verification of all software--and for some\nwell-equipped companies, this may be true. This conversation, however, does not\nmatch the reality for most companies. \"Trailing-edge organizations,\" as we term\nthem, rely heavily on legacy software, poorly staff security roles, and\nstruggle to implement best practices like rapid deployment of security patches.\nThese decisions may be the result of corporate inertia, but may also be the\nresult of a seemingly-rational calculation that attackers may not bother\ntargeting a firm due to lack of economic incentives, and as a result,\nunderinvestment in defense will not be punished.\n  This approach to security may have been sufficient prior to the development\nof AI systems, but it is unlikely to remain viable in the near future. We argue\nthat continuing improvements in AI's capabilities poses additional risks on two\nfronts: First, increased usage of AI will alter the economics of the marginal\ncyberattack and expose these trailing-edge organizations to more attackers,\nmore frequently. Second, AI's advances will enable attackers to develop\nexploits and launch attacks earlier than they can today--meaning that it is\ninsufficient for these companies to attain parity with today's leading\ndefenders, but must instead aim for faster remediation timelines and more\nresilient software. The situation today portends a dramatically increased\nnumber of attacks in the near future. Moving forward, we offer a range of\nsolutions for both organizations and governments to improve the defensive\nposture of firms which lag behind their peers today."}
{"id": "2508.16165", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.16165", "abs": "https://arxiv.org/abs/2508.16165", "authors": ["Sebastian Lubos", "Alexander Felfernig", "Gerhard Leitner", "Julian Schwazer"], "title": "Towards Recommending Usability Improvements with Multimodal Large Language Models", "comment": null, "summary": "Usability describes a set of essential quality attributes of user interfaces\n(UI) that influence human-computer interaction. Common evaluation methods, such\nas usability testing and inspection, are effective but resource-intensive and\nrequire expert involvement. This makes them less accessible for smaller\norganizations. Recent advances in multimodal LLMs offer promising opportunities\nto automate usability evaluation processes partly by analyzing textual, visual,\nand structural aspects of software interfaces. To investigate this possibility,\nwe formulate usability evaluation as a recommendation task, where multimodal\nLLMs rank usability issues by severity. We conducted an initial\nproof-of-concept study to compare LLM-generated usability improvement\nrecommendations with usability expert assessments. Our findings indicate the\npotential of LLMs to enable faster and more cost-effective usability\nevaluation, which makes it a practical alternative in contexts with limited\nexpert resources."}
{"id": "2508.15839", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15839", "abs": "https://arxiv.org/abs/2508.15839", "authors": ["Yuksel Aydin"], "title": "CIA+TA Risk Assessment for AI Reasoning Vulnerabilities", "comment": null, "summary": "As AI systems increasingly influence critical decisions, they face threats\nthat exploit reasoning mechanisms rather than technical infrastructure. We\npresent a framework for cognitive cybersecurity, a systematic protection of AI\nreasoning processes from adversarial manipulation. Our contributions are\nthreefold. First, we establish cognitive cybersecurity as a discipline\ncomplementing traditional cybersecurity and AI safety, addressing\nvulnerabilities where legitimate inputs corrupt reasoning while evading\nconventional controls. Second, we introduce the CIA+TA, extending traditional\nConfidentiality, Integrity, and Availability triad with Trust (epistemic\nvalidation) and Autonomy (human agency preservation), requirements unique to\nsystems generating knowledge claims and mediating decisions. Third, we present\na quantitative risk assessment methodology with empirically-derived\ncoefficients, enabling organizations to measure cognitive security risks. We\nmap our framework to OWASP LLM Top 10 and MITRE ATLAS, facilitating operational\nintegration. Validation through previously published studies (151 human\nparticipants; 12,180 AI trials) reveals strong architecture dependence:\nidentical defenses produce effects ranging from 96% reduction to 135%\namplification of vulnerabilities. This necessitates pre-deployment Cognitive\nPenetration Testing as a governance requirement for trustworthy AI deployment."}
{"id": "2508.16181", "categories": ["cs.SE", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.16181", "abs": "https://arxiv.org/abs/2508.16181", "authors": ["Zirui Li", "Stephan Husung", "Haoze Wang"], "title": "LLM-Assisted Semantic Alignment and Integration in Collaborative Model-Based Systems Engineering Using SysML v2", "comment": "Accepted by IEEE ISSE 2025, DOI pending", "summary": "Cross-organizational collaboration in Model-Based Systems Engineering (MBSE)\nfaces many challenges in achieving semantic alignment across independently\ndeveloped system models. SysML v2 introduces enhanced structural modularity and\nformal semantics, offering a stronger foundation for interoperable modeling.\nMeanwhile, GPT-based Large Language Models (LLMs) provide new capabilities for\nassisting model understanding and integration. This paper proposes a\nstructured, prompt-driven approach for LLM-assisted semantic alignment of SysML\nv2 models. The core contribution lies in the iterative development of an\nalignment approach and interaction prompts, incorporating model extraction,\nsemantic matching, and verification. The approach leverages SysML v2 constructs\nsuch as alias, import, and metadata extensions to support traceable, soft\nalignment integration. It is demonstrated with a GPT-based LLM through an\nexample of a measurement system. Benefits and limitations are discussed."}
{"id": "2508.15840", "categories": ["cs.CR", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.15840", "abs": "https://arxiv.org/abs/2508.15840", "authors": ["Robert Dilworth"], "title": "Unveiling Unicode's Unseen Underpinnings in Undermining Authorship Attribution", "comment": null, "summary": "When using a public communication channel -- whether formal or informal, such\nas commenting or posting on social media -- end users have no expectation of\nprivacy: they compose a message and broadcast it for the world to see. Even if\nan end user takes utmost precautions to anonymize their online presence --\nusing an alias or pseudonym; masking their IP address; spoofing their\ngeolocation; concealing their operating system and user agent; deploying\nencryption; registering with a disposable phone number or email; disabling\nnon-essential settings; revoking permissions; and blocking cookies and\nfingerprinting -- one obvious element still lingers: the message itself.\nAssuming they avoid lapses in judgment or accidental self-exposure, there\nshould be little evidence to validate their actual identity, right? Wrong. The\ncontent of their message -- necessarily open for public consumption -- exposes\nan attack vector: stylometric analysis, or author profiling. In this paper, we\ndissect the technique of stylometry, discuss an antithetical counter-strategy\nin adversarial stylometry, and devise enhancements through Unicode\nsteganography."}
{"id": "2508.16273", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16273", "abs": "https://arxiv.org/abs/2508.16273", "authors": ["Maria Teresa Rossi", "Martina De Sanctis", "Ludovico Iovino", "Manuel Wimmer"], "title": "A Systematic Mapping Study on Smart Cities Modeling Approaches", "comment": null, "summary": "The Smart City concept was introduced to define an idealized city\ncharacterized by automation and connection. It then evolved rapidly by\nincluding further aspects, such as economy, environment. Since then, many\npublications have explored various aspects of Smart Cities across different\napplication domains and research communities, acknowledging the\ninterdisciplinary nature of this subject. In particular, our interest focuses\non how smart cities are designed and modeled, as a whole or as regards with\ntheir subsystems, when dealing with the accomplishment of the research goals in\nthis complex and heterogeneous domain. To this aim, we performed a systematic\nmapping study on smart cities modeling approaches identifying the relevant\ncontributions (i) to get an overview of existing research approaches, (ii) to\nidentify whether there are any publication trends, and (iii) to identify\npossible future research directions. We followed the guidelines for conducting\nsystematic mapping studies by Petersen et al. to analyze smart cities modeling\npublications. Our analysis revealed the following main findings: (i) smart\ngovernance is the most investigated and modeled smart city dimension; (ii) the\nmost used modeling approaches are business, architectural, and ontological\nmodeling approaches, spanning multiple application fields; (iii) the great\nmajority of existing technologies for modeling smart cities are not yet proven\nin operational environments; (iv) diverse research communities publish their\nresults in a multitude of different venues which further motivates the\npresented literature study. Researchers can use our results for better\nunderstanding the state-of-the-art in modeling smart cities, and as a\nfoundation for further analysis of specific approaches about smart cities\nmodeling. Lastly, we also discuss the impact of our analysis for the\nModel-Driven Engineering community."}
{"id": "2508.15848", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15848", "abs": "https://arxiv.org/abs/2508.15848", "authors": ["Yinghan Zhou", "Juan Wen", "Wanli Peng", "Zhengxian Wu", "Ziwei Zhang", "Yiming Xue"], "title": "Self-Disguise Attack: Induce the LLM to disguise itself for AIGT detection evasion", "comment": null, "summary": "AI-generated text (AIGT) detection evasion aims to reduce the detection\nprobability of AIGT, helping to identify weaknesses in detectors and enhance\ntheir effectiveness and reliability in practical applications. Although\nexisting evasion methods perform well, they suffer from high computational\ncosts and text quality degradation. To address these challenges, we propose\nSelf-Disguise Attack (SDA), a novel approach that enables Large Language Models\n(LLM) to actively disguise its output, reducing the likelihood of detection by\nclassifiers. The SDA comprises two main components: the adversarial feature\nextractor and the retrieval-based context examples optimizer. The former\ngenerates disguise features that enable LLMs to understand how to produce more\nhuman-like text. The latter retrieves the most relevant examples from an\nexternal knowledge base as in-context examples, further enhancing the\nself-disguise ability of LLMs and mitigating the impact of the disguise process\non the diversity of the generated text. The SDA directly employs prompts\ncontaining disguise features and optimized context examples to guide the LLM in\ngenerating detection-resistant text, thereby reducing resource consumption.\nExperimental results demonstrate that the SDA effectively reduces the average\ndetection accuracy of various AIGT detectors across texts generated by three\ndifferent LLMs, while maintaining the quality of AIGT."}
{"id": "2508.16307", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16307", "abs": "https://arxiv.org/abs/2508.16307", "authors": ["Jinsheng Ba", "Yuancheng Jiang", "Manuel Rigger"], "title": "Metamorphic Coverage", "comment": null, "summary": "Metamorphic testing is a widely used methodology that examines an expected\nrelation between pairs of executions to automatically find bugs, such as\ncorrectness bugs. We found that code coverage cannot accurately measure the\nextent to which code is validated and mutation testing is computationally\nexpensive for evaluating metamorphic testing methods. In this work, we propose\nMetamorphic Coverage (MC), a coverage metric that examines the distinct code\nexecuted by pairs of test inputs within metamorphic testing. Our intuition is\nthat, typically, a bug can be observed if the corresponding code is executed\nwhen executing either test input but not the other one, so covering more\ndifferential code covered by pairs of test inputs might be more likely to\nexpose bugs. While most metamorphic testing methods have been based on this\ngeneral intuition, our work defines and systematically evaluates MC on five\nwidely used metamorphic testing methods for testing database engines,\ncompilers, and constraint solvers. The code measured by MC overlaps with the\nbug-fix locations of 50 of 64 bugs found by metamorphic testing methods, and MC\nhas a stronger positive correlation with bug numbers than line coverage. MC is\n4x more sensitive than line coverage in distinguishing testing methods'\neffectiveness, and the average value of MC is 6x smaller than line coverage\nwhile still capturing the part of the program that is being tested. MC required\n359x less time than mutation testing. Based on a case study for an automated\ndatabase system testing approach, we demonstrate that when used for feedback\nguidance, MC significantly outperforms code coverage, by finding 41\\% more\nbugs. Consequently, this work might have broad applications for assessing\nmetamorphic testing methods and improving test-case generation."}
{"id": "2508.15850", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15850", "abs": "https://arxiv.org/abs/2508.15850", "authors": ["Ziyu Wang", "Elahe Khatibi", "Farshad Firouzi", "Sanaz Rahimi Mousavi", "Krishnendu Chakrabarty", "Amir M. Rahmani"], "title": "Linkage Attacks Expose Identity Risks in Public ECG Data Sharing", "comment": null, "summary": "The increasing availability of publicly shared electrocardiogram (ECG) data\nraises critical privacy concerns, as its biometric properties make individuals\nvulnerable to linkage attacks. Unlike prior studies that assume idealized\nadversarial capabilities, we evaluate ECG privacy risks under realistic\nconditions where attackers operate with partial knowledge. Using data from 109\nparticipants across diverse real-world datasets, our approach achieves 85%\naccuracy in re-identifying individuals in public datasets while maintaining a\n14.2% overall misclassification rate at an optimal confidence threshold, with\n15.6% of unknown individuals misclassified as known and 12.8% of known\nindividuals misclassified as unknown. These results highlight the inadequacy of\nsimple anonymization techniques in preventing re-identification, demonstrating\nthat even limited adversarial knowledge enables effective identity linkage. Our\nfindings underscore the urgent need for privacy-preserving strategies, such as\ndifferential privacy, access control, and encrypted computation, to mitigate\nre-identification risks while ensuring the utility of shared biosignal data in\nhealthcare applications."}
{"id": "2508.16318", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16318", "abs": "https://arxiv.org/abs/2508.16318", "authors": ["Juan C. Alonso", "Alberto Martin-Lopez", "Sergio Segura", "Gabriele Bavota", "Antonio Ruiz-Cortés"], "title": "SATORI: Static Test Oracle Generation for REST APIs", "comment": "Accepted for publication at 40th IEEE/ACM International Conference on\n  Automated Software Engineering, ASE 2025", "summary": "REST API test case generation tools are evolving rapidly, with growing\ncapabilities for the automated generation of complex tests. However, despite\ntheir strengths in test data generation, these tools are constrained by the\ntypes of test oracles they support, often limited to crashes, regressions, and\nnoncompliance with API specifications or design standards. This paper\nintroduces SATORI (Static API Test ORacle Inference), a black-box approach for\ngenerating test oracles for REST APIs by analyzing their OpenAPI Specification.\nSATORI uses large language models to infer the expected behavior of an API by\nanalyzing the properties of the response fields of its operations, such as\ntheir name and descriptions. To foster its adoption, we extended the\nPostmanAssertify tool to automatically convert the test oracles reported by\nSATORI into executable assertions. Evaluation results on 17 operations from 12\nindustrial APIs show that SATORI can automatically generate up to hundreds of\nvalid test oracles per operation. SATORI achieved an F1-score of 74.3%,\noutperforming the state-of-the-art dynamic approach AGORA+ (69.3%)-which\nrequires executing the API-when generating comparable oracle types. Moreover,\nour findings show that static and dynamic oracle inference methods are\ncomplementary: together, SATORI and AGORA+ found 90% of the oracles in our\nannotated ground-truth dataset. Notably, SATORI uncovered 18 bugs in popular\nAPIs (Amadeus Hotel, Deutschebahn, FDIC, GitLab, Marvel, OMDb and Vimeo)\nleading to documentation updates by the API maintainers."}
{"id": "2508.15865", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15865", "abs": "https://arxiv.org/abs/2508.15865", "authors": ["Julia Boone", "Fatemeh Afghah"], "title": "Securing Swarms: Cross-Domain Adaptation for ROS2-based CPS Anomaly Detection", "comment": "Accepted for publication in MILCOM 2025. 6 pages, 2 figures", "summary": "Cyber-physical systems (CPS) are being increasingly utilized for critical\napplications. CPS combines sensing and computing elements, often having\nmulti-layer designs with networking, computational, and physical interfaces,\nwhich provide them with enhanced capabilities for a variety of application\nscenarios. However, the combination of physical and computational elements also\nmakes CPS more vulnerable to attacks compared to network-only systems, and the\nresulting impacts of CPS attacks can be substantial. Intelligent intrusion\ndetection systems (IDS) are an effective mechanism by which CPS can be secured,\nbut the majority of current solutions often train and validate on network\ntraffic-only datasets, ignoring the distinct attacks that may occur on other\nsystem layers. In order to address this, we develop an adaptable CPS anomaly\ndetection model that can detect attacks within CPS without the need for\npreviously labeled data. To achieve this, we utilize domain adaptation\ntechniques that allow us to transfer known attack knowledge from a network\ntraffic-only environment to a CPS environment. We validate our approach using a\nstate-of-the-art CPS intrusion dataset that combines network, operating system\n(OS), and Robot Operating System (ROS) data. Through this dataset, we are able\nto demonstrate the effectiveness of our model across network traffic-only and\nCPS environments with distinct attack types and its ability to outperform other\nanomaly detection methods."}
{"id": "2508.16341", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16341", "abs": "https://arxiv.org/abs/2508.16341", "authors": ["Sebastian Copei", "Oliver Hohlfeld", "Jens Kosiol"], "title": "The (C)omprehensive (A)rchitecture (P)attern (I)ntegration method: Navigating the sea of technology", "comment": null, "summary": "The technological landscape changes daily, making it nearly impossible for a\nsingle person to be aware of all trends or available tools that may or may not\nbe suitable for their software project. This makes tool selection and\narchitectural design decisions a complex problem, especially for large-scale\nsoftware systems. To tackle this issue, we introduce CAPI, the Comprehensive\nArchitecture Pattern Integration method that uses a diagnostic decision tree to\nsuggest architectural patterns depending on user needs. By suggesting patterns\ninstead of tools, the overall complexity for further decisions is lower as\nthere are fewer architectural patterns than tools due to the abstract nature of\npatterns. Moreover, since tools implement patterns, each non-proposed pattern\nreduces the number of tools to choose from, reducing complexity. We iteratively\ndeveloped CAPI, evaluating its understandability and usability in small studies\nwith academic participants. When satisfied with the outcome, we performed a\nuser-study with industry representatives to investigate the state-of-the-art in\ntechnology selection and the effectiveness of our proposed method. We find that\ntechnology selection is largely performed via trial and error, that CAPI is\nuniformly perceived as helpful, and that CAPI is able to reproduce the\nproductive architectural environments of our participants."}
{"id": "2508.15917", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15917", "abs": "https://arxiv.org/abs/2508.15917", "authors": ["Xiaoli Zhuo", "Xuehu Yan", "Lintao Liu", "Wei Yan"], "title": "Evolving k-Threshold Visual Cryptography Schemes", "comment": null, "summary": "In evolving access structures, the number of participants is countably\ninfinite with no predetermined upper bound. While such structures have been\nrealized in secret sharing, research in secret image sharing has primarily\nfocused on visual cryptography schemes (VCS). However, there exists no\nconstruction for $(k,\\infty)$ VCS that applies to arbitrary $k$ values without\npixel expansion currently, and the contrast requires enhancement. In this\npaper, we first present a formal mathematical definition of $(k,\\infty)$ VCS.\nThen, propose a $(k,\\infty)$ VCS based on random grids that works for arbitrary\n$k$. In addition, to further improve contrast, we develop optimized\n$(k,\\infty)$ VCS for $k=2$ and $3$, along with contrast enhancement strategies\nfor $k\\geq 4$. Theoretical analysis and experimental results demonstrate the\nsuperiority of our proposed schemes."}
{"id": "2508.16402", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16402", "abs": "https://arxiv.org/abs/2508.16402", "authors": ["Zihan Wang", "Jiaze Chen", "Zhicheng Liu", "Markus Mak", "Yidi Du", "Geonsik Moon", "Luoqi Xu", "Aaron Tua", "Kunshuo Peng", "Jiayi Lu", "Mingfei Xia", "Boqian Zou", "Chenyang Ran", "Guang Tian", "Shoutai Zhu", "Yeheng Duan", "Zhenghui Kang", "Zhenxing Lin", "Shangshu Li", "Qiang Luo", "Qingshen Long", "Zhiyong Chen", "Yihan Xiao", "Yurong Wu", "Daoguang Zan", "Yuyi Fu", "Mingxuan Wang", "Ming Ding"], "title": "AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions", "comment": "15 pages", "summary": "Competitive programming has emerged as a critical benchmark for evaluating\nthe reasoning and coding capabilities of Large Language Models (LLMs). Despite\nimpressive progress on existing benchmarks, we argue that current evaluations\noverstate model proficiency, masking a substantial gap between LLMs and elite\nhuman programmers. This gap arises from two key limitations: insufficient\ndifficulty and scope of benchmark problems, and evaluation bias from\nlow-quality test cases. To address these shortcomings, we present AetherCode, a\nnew benchmark that draws problems from premier programming competitions such as\nIOI and ICPC, offering broader coverage and higher difficulty. AetherCode\nfurther incorporates comprehensive, expert-validated test suites built through\na hybrid of automated generation and human curation, ensuring rigorous and\nreliable assessment. By combining challenging problem design with robust\nevaluation, AetherCode provides a more faithful measure of LLM capabilities and\nsets a new standard for future research in code reasoning."}
{"id": "2508.15934", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15934", "abs": "https://arxiv.org/abs/2508.15934", "authors": ["Onur Alp Kirci", "M. Emre Gursoy"], "title": "Strategic Sample Selection for Improved Clean-Label Backdoor Attacks in Text Classification", "comment": null, "summary": "Backdoor attacks pose a significant threat to the integrity of text\nclassification models used in natural language processing. While several\ndirty-label attacks that achieve high attack success rates (ASR) have been\nproposed, clean-label attacks are inherently more difficult. In this paper, we\npropose three sample selection strategies to improve attack effectiveness in\nclean-label scenarios: Minimum, Above50, and Below50. Our strategies identify\nthose samples which the model predicts incorrectly or with low confidence, and\nby injecting backdoor triggers into such samples, we aim to induce a stronger\nassociation between the trigger patterns and the attacker-desired target label.\nWe apply our methods to clean-label variants of four canonical backdoor attacks\n(InsertSent, WordInj, StyleBkd, SynBkd) and evaluate them on three datasets\n(IMDB, SST2, HateSpeech) and four model types (LSTM, BERT, DistilBERT,\nRoBERTa). Results show that the proposed strategies, particularly the Minimum\nstrategy, significantly improve the ASR over random sample selection with\nlittle or no degradation in the model's clean accuracy. Furthermore,\nclean-label attacks enhanced by our strategies outperform BITE, a state of the\nart clean-label attack method, in many configurations."}
{"id": "2508.16419", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16419", "abs": "https://arxiv.org/abs/2508.16419", "authors": ["Akshay Mhatre", "Noujoud Nader", "Patrick Diehl", "Deepti Gupta"], "title": "LLM-GUARD: Large Language Model-Based Detection and Repair of Bugs and Security Vulnerabilities in C++ and Python", "comment": null, "summary": "Large Language Models (LLMs) such as ChatGPT-4, Claude 3, and LLaMA 4 are\nincreasingly embedded in software/application development, supporting tasks\nfrom code generation to debugging. Yet, their real-world effectiveness in\ndetecting diverse software bugs, particularly complex, security-relevant\nvulnerabilities, remains underexplored. This study presents a systematic,\nempirical evaluation of these three leading LLMs using a benchmark of\nfoundational programming errors, classic security flaws, and advanced,\nproduction-grade bugs in C++ and Python. The dataset integrates real code from\nSEED Labs, OpenSSL (via the Suresoft GLaDOS database), and PyBugHive, validated\nthrough local compilation and testing pipelines. A novel multi-stage,\ncontext-aware prompting protocol simulates realistic debugging scenarios, while\na graded rubric measures detection accuracy, reasoning depth, and remediation\nquality. Our results show that all models excel at identifying syntactic and\nsemantic issues in well-scoped code, making them promising for educational use\nand as first-pass reviewers in automated code auditing. Performance diminishes\nin scenarios involving complex security vulnerabilities and large-scale\nproduction code, with ChatGPT-4 and Claude 3 generally providing more nuanced\ncontextual analyses than LLaMA 4. This highlights both the promise and the\npresent constraints of LLMs in serving as reliable code analysis tools."}
{"id": "2508.15987", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15987", "abs": "https://arxiv.org/abs/2508.15987", "authors": ["Andreas D. Kellas", "Neophytos Christou", "Wenxin Jiang", "Penghui Li", "Laurent Simon", "Yaniv David", "Vasileios P. Kemerlis", "James C. Davis", "Junfeng Yang"], "title": "PickleBall: Secure Deserialization of Pickle-based Machine Learning Models", "comment": "To be published in the proceedings of 2025 ACM CCS", "summary": "Machine learning model repositories such as the Hugging Face Model Hub\nfacilitate model exchanges. However, bad actors can deliver malware through\ncompromised models. Existing defenses such as safer model formats, restrictive\n(but inflexible) loading policies, and model scanners have shortcomings: 44.9%\nof popular models on Hugging Face still use the insecure pickle format, 15% of\nthese cannot be loaded by restrictive loading policies, and model scanners have\nboth false positives and false negatives. Pickle remains the de facto standard\nfor model exchange, and the ML community lacks a tool that offers transparent\nsafe loading.\n  We present PickleBall to help machine learning engineers load pickle-based\nmodels safely. PickleBall statically analyzes the source code of a given\nmachine learning library and computes a custom policy that specifies a safe\nload-time behavior for benign models. PickleBall then dynamically enforces the\npolicy during load time as a drop-in replacement for the pickle module.\nPickleBall generates policies that correctly load 79.8% of benign pickle-based\nmodels in our dataset, while rejecting all (100%) malicious examples in our\ndataset. In comparison, evaluated model scanners fail to identify known\nmalicious models, and the state-of-art loader loads 22% fewer benign models\nthan PickleBall. PickleBall removes the threat of arbitrary function invocation\nfrom malicious pickle-based models, raising the bar for attackers to depend on\ncode reuse techniques."}
{"id": "2508.16445", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16445", "abs": "https://arxiv.org/abs/2508.16445", "authors": ["Sonia Nicoletti", "Paolo Ciancarini"], "title": "Using LLMs and Essence to Support Software Practice Adoption", "comment": null, "summary": "Recent advancements in natural language processing (NLP) have enabled the\ndevelopment of automated tools that support various domains, including software\nengineering. However, while NLP and artificial intelligence (AI) research has\nextensively focused on tasks such as code generation, less attention has been\ngiven to automating support for the adoption of best practices, the evolution\nof ways of working, and the monitoring of process health. This study addresses\nthis gap by exploring the integration of Essence, a standard and thinking\nframework for managing software engineering practices, with large language\nmodels (LLMs). To this end, a specialised chatbot was developed to assist\nstudents and professionals in understanding and applying Essence. The chatbot\nemploys a retrieval-augmented generation (RAG) system to retrieve relevant\ncontextual information from a curated knowledge base. Four different LLMs were\nused to create multiple chatbot configurations, each evaluated both as a base\nmodel and augmented with the RAG system. The system performance was evaluated\nthrough both the relevance of retrieved context and the quality of generated\nresponses. Comparative analysis against the general-purpose LLMs demonstrated\nthat the proposed system consistently outperforms its baseline counterpart in\ndomain-specific tasks. By facilitating access to structured software\nengineering knowledge, this work contributes to bridging the gap between\ntheoretical frameworks and practical application, potentially improving process\nmanagement and the adoption of software development practices. While further\nvalidation through user studies is required, these findings highlight the\npotential of LLM-based automation to enhance learning and decision-making in\nsoftware engineering."}
{"id": "2508.16078", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.16078", "abs": "https://arxiv.org/abs/2508.16078", "authors": ["Nadeem Ahmed", "Lei Zhang", "Aryya Gangopadhyay"], "title": "A Survey of Post-Quantum Cryptography Support in Cryptographic Libraries", "comment": "To be published in IEEE International Conference on Quantum Computing\n  and Engineering (QCE) 2025", "summary": "The rapid advancement of quantum computing poses a significant threat to\nmodern cryptographic systems, necessitating the transition to Post-Quantum\nCryptography (PQC). This study evaluates the support for PQC algorithms within\nnine widely used open-source cryptographic libraries -- OpenSSL, wolfSSL,\nBoringSSL, LibreSSL, Bouncy Castle, libsodium, Crypto++, Botan, and MbedTLS --\nfocusing on their implementation of the NIST-selected PQC finalists:\nCRYSTALS-Kyber, CRYSTALS-Dilithium, FALCON, and SPHINCS+. Our analysis, based\non the latest available documentation, release notes, and industry reports as\nof early 2025, reveals a varied state of readiness across these libraries.\nWhile some libraries have integrated PQC support or have clear implementation\nroadmaps, others lag behind, creating potential security risks as quantum\nthreats become more imminent. We discuss key challenges, including performance\ntrade-offs, implementation security, and adoption hurdles in real-world\ncryptographic applications. Our findings highlight the urgent need for\ncontinued research, standardization efforts, and coordinated adoption\nstrategies to ensure a secure transition to the quantum-resistant cryptographic\nlandscape."}
{"id": "2508.16499", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16499", "abs": "https://arxiv.org/abs/2508.16499", "authors": ["Kazuki Kusama", "Honglin Shu", "Masanari Kondo", "Yasutaka Kamei"], "title": "How Small is Enough? Empirical Evidence of Quantized Small Language Models for Automated Program Repair", "comment": null, "summary": "Background: Large language models (LLMs) have greatly improved the accuracy\nof automated program repair (APR) methods. However, LLMs are constrained by\nhigh computational resource requirements. Aims: We focus on small language\nmodels (SLMs), which perform well even with limited computational resources\ncompared to LLMs. We aim to evaluate whether SLMs can achieve competitive\nperformance in APR tasks. Method: We conducted experiments on the QuixBugs\nbenchmark to compare the bug-fixing accuracy of SLMs and LLMs. We also analyzed\nthe impact of int8 quantization on APR performance. Results: The latest SLMs\ncan fix bugs as accurately as--or even more accurately than--LLMs. Also, int8\nquantization had minimal effect on APR accuracy while significantly reducing\nmemory requirements. Conclusions: SLMs present a viable alternative to LLMs for\nAPR, offering competitive accuracy with lower computational costs, and\nquantization can further enhance their efficiency without compromising\neffectiveness."}
{"id": "2508.16133", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16133", "abs": "https://arxiv.org/abs/2508.16133", "authors": ["Shilin Xiao", "Wenjun Zhu", "Yan Jiang", "Kai Wang", "Peiwang Wang", "Chen Yan", "Xiaoyu Ji", "Wenyuan Xu"], "title": "SoK: Understanding the Fundamentals and Implications of Sensor Out-of-band Vulnerabilities", "comment": "Accepted by NDSS 2026", "summary": "Sensors are fundamental to cyber-physical systems (CPS), enabling perception\nand control by transducing physical stimuli into digital measurements. However,\ndespite growing research on physical attacks on sensors, our understanding of\nsensor hardware vulnerabilities remains fragmented due to the ad-hoc nature of\nthis field. Moreover, the infinite attack signal space further complicates\nthreat abstraction and defense. To address this gap, we propose a\nsystematization framework, termed sensor out-of-band (OOB) vulnerabilities,\nthat for the first time provides a comprehensive abstraction for sensor attack\nsurfaces based on underlying physical principles. We adopt a bottom-up\nsystematization methodology that analyzes OOB vulnerabilities across three\nlevels. At the component level, we identify the physical principles and\nlimitations that contribute to OOB vulnerabilities. At the sensor level, we\ncategorize known attacks and evaluate their practicality. At the system level,\nwe analyze how CPS features such as sensor fusion, closed-loop control, and\nintelligent perception impact the exposure and mitigation of OOB threats. Our\nfindings offer a foundational understanding of sensor hardware security and\nprovide guidance and future directions for sensor designers, security\nresearchers, and system developers aiming to build more secure sensors and CPS."}
{"id": "2508.16517", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.16517", "abs": "https://arxiv.org/abs/2508.16517", "authors": ["Bingkun Yao", "Ning Wang", "Xiangfeng Liu", "Yuxin Du", "Yuchen Hu", "Hong Gao", "Zhe Jiang", "Nan Guan"], "title": "ARSP: Automated Repair of Verilog Designs via Semantic Partitioning", "comment": null, "summary": "Debugging functional Verilog bugs consumes a significant portion of front-end\ndesign time. While Large Language Models (LLMs) have demonstrated great\npotential in mitigating this effort, existing LLM-based automated debugging\nmethods underperform on industrial-scale modules. A major reason for this is\nbug signal dilution in long contexts, where a few bug-relevant tokens are\noverwhelmed by hundreds of unrelated lines, diffusing the model's attention. To\naddress this issue, we introduce ARSP, a two-stage system that mitigates\ndilution via semantics-guided fragmentation. A Partition LLM splits a module\ninto semantically tight fragments; a Repair LLM patches each fragment; edits\nare merged without altering unrelated logic. A synthetic data framework\ngenerates fragment-level training pairs spanning bug types, design styles, and\nscales to supervise both models. Experiments show that ARSP achieves 77.92%\npass@1 and 83.88% pass@5, outperforming mainstream commercial LLMs including\nClaude-3.7 and SOTA automated Verilog debugging tools Strider and MEIC. Also,\nsemantic partitioning improves pass@1 by 11.6% and pass@5 by 10.2% over\nwhole-module debugging, validating the effectiveness of fragment-level scope\nreduction in LLM-based Verilog debugging."}
{"id": "2508.16150", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16150", "abs": "https://arxiv.org/abs/2508.16150", "authors": ["Aristeidis Sidiropoulos", "Christos Chrysanthos Nikolaidis", "Theodoros Tsiolakis", "Nikolaos Pavlidis", "Vasilis Perifanis", "Pavlos S. Efraimidis"], "title": "Evaluating the Defense Potential of Machine Unlearning against Membership Inference Attacks", "comment": null, "summary": "Membership Inference Attacks (MIAs) pose a significant privacy risk, as they\nenable adversaries to determine whether a specific data point was included in\nthe training dataset of a model. While Machine Unlearning is primarily designed\nas a privacy mechanism to efficiently remove private data from a machine\nlearning model without the need for full retraining, its impact on the\nsusceptibility of models to MIA remains an open question. In this study, we\nsystematically assess the vulnerability of models to MIA after applying\nstate-of-art Machine Unlearning algorithms. Our analysis spans four diverse\ndatasets (two from the image domain and two in tabular format), exploring how\ndifferent unlearning approaches influence the exposure of models to membership\ninference. The findings highlight that while Machine Unlearning is not\ninherently a countermeasure against MIA, the unlearning algorithm and data\ncharacteristics can significantly affect a model's vulnerability. This work\nprovides essential insights into the interplay between Machine Unlearning and\nMIAs, offering guidance for the design of privacy-preserving machine learning\nsystems."}
{"id": "2508.16189", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16189", "abs": "https://arxiv.org/abs/2508.16189", "authors": ["Aparna Singh", "Geetanjali Rathee", "Chaker Abdelaziz Kerrache", "Mohamed Chahine Ghanem"], "title": "A Relay-Chain-Powered Ciphertext-Policy Attribute-Based Encryption in Intelligent Transportation Systems", "comment": null, "summary": "The very high growth of Intelligent Transportation Systems (ITS) has\ngenerated an urgent requirement for secure, effective, and context-aware data\nsharing mechanisms, especially over heterogeneous and geographically dispersed\nsettings. This work suggests a new architecture that combines a relay\nchain-driven encryption system with a modified Ciphertext-Policy\nAttribute-Based Encryption (CP-ABE) scheme to tackle the double impediment of\ndynamic access and low-latency communication. The model proposes a\ncontext-aware smart contract on a worldwide relay chain that checks against\ndata properties, including event type, time, and geographical region, to\nspecify the suitable level of encryption policy. From such relay-directed\njudgment, On-Board Units (OBUs) encrypt data end-to-end by utilising CP-ABE and\nstore ciphertext inside localised regional blockchains, preventing dependence\non symmetric encryption or off-chain storage. High-sensitivity events are\nsecured with firm, multi-attribute access rules, whereas common updates use\nlight policies to help reduce processing burdens. The crypto system also adds\ntraceability and low-latency revocation, with global enforcement managed\nthrough the relay chain. This distributed, scalable model provides a proper\nbalance between responsiveness in real time and security and is extremely apt\nfor next-gen vehicular networks that function across multi-jurisdictional\ndomains."}
{"id": "2508.16202", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16202", "abs": "https://arxiv.org/abs/2508.16202", "authors": ["Shu-Jie Cao", "Dongning Guo"], "title": "How to Beat Nakamoto in the Race", "comment": "Accepted for presentation at the 2025 ACM Conference on Computer and\n  Communications Security (CCS)", "summary": "This paper studies proof-of-work Nakamoto consensus under bounded network\ndelays, settling two long-standing questions in blockchain security: How can an\nadversary most effectively attack block safety under a given block confirmation\nlatency? And what is the resulting probability of safety violation? A Markov\ndecision process (MDP) framework is introduced to precise characterize the\nsystem state (including the tree and timings of all blocks mined), the\nadversary's potential actions, and the state transitions due to the adversarial\naction and the random block arrival processes. An optimal attack, called\nbait-and-switch, is proposed and proved to maximize the adversary's chance of\nviolating block safety by \"beating Nakamoto in the race\". The exact probability\nof this violation is calculated for any confirmation depth using Markov chain\nanalysis, offering fresh insights into the interplay of network delay,\nconfirmation rules, and blockchain security."}
{"id": "2508.16347", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16347", "abs": "https://arxiv.org/abs/2508.16347", "authors": ["Yu Yan", "Sheng Sun", "Zhe Wang", "Yijun Lin", "Zenghao Duan", "zhifei zheng", "Min Liu", "Zhiyi yin", "Jianping Zhang"], "title": "Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and Investigating the Real Misuse Threat of LLMs", "comment": null, "summary": "With the development of Large Language Models (LLMs), numerous efforts have\nrevealed their vulnerabilities to jailbreak attacks. Although these studies\nhave driven the progress in LLMs' safety alignment, it remains unclear whether\nLLMs have internalized authentic knowledge to deal with real-world crimes, or\nare merely forced to simulate toxic language patterns. This ambiguity raises\nconcerns that jailbreak success is often attributable to a hallucination loop\nbetween jailbroken LLM and judger LLM. By decoupling the use of jailbreak\ntechniques, we construct knowledge-intensive Q\\&A to investigate the misuse\nthreats of LLMs in terms of dangerous knowledge possession, harmful task\nplanning utility, and harmfulness judgment robustness. Experiments reveal a\nmismatch between jailbreak success rates and harmful knowledge possession in\nLLMs, and existing LLM-as-a-judge frameworks tend to anchor harmfulness\njudgments on toxic language patterns. Our study reveals a gap between existing\nLLM safety assessments and real-world threat potential."}
{"id": "2508.16405", "categories": ["cs.CR", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.16405", "abs": "https://arxiv.org/abs/2508.16405", "authors": ["Min Wang", "Chuanpeng Jiang", "Zhaohao Wang", "Zhengyi Hou", "Zhongkui Zhang", "Yuanfu Zhao", "Hongxi Liu", "Weisheng Zhao"], "title": "Temperature-Resilient Reconfigurable PUF with Dual-Pulse Modulation based on SOT-MRAM Chip", "comment": null, "summary": "In the Internet of Things (IoT) era, hardware-based security solutions have\nbecome an emerging choice for enhancing end-terminal information security. As\none of the hardware technologies, physical unclonable functions (PUFs) utilize\nthe inherent variations in the manufacturing process to generate cryptographic\nkeys. Reconfigurable PUFs (rPUFs), characterized by updating cryptographic\nkeys, offer enhanced security ability for protecting massive amounts of data in\ndynamic operational scenarios. The core challenge lies in achieving real-time\nreconfiguration independent of environmental conditions, particularly operating\ntemperature, which has rarely been investigated and addressed. In this study,\nwe propose a dual-pulse reconfiguration strategy based on SOT-MRAM carriers,\nwhich effectively widens the operating window and exhibits excellent PUF\nmetrics. Experimental results demonstrate that our design achieves real-time\nreconfiguration across industrial-grade operating temperature ranges, without\nthe need for dynamic feedback of real-time temperature. The proposed SOT-MRAM\nrPUF design lays a solid foundation for next-generation IoT protection\narchitectures."}
{"id": "2508.16406", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16406", "abs": "https://arxiv.org/abs/2508.16406", "authors": ["Guangyu Yang", "Jinghong Chen", "Jingbiao Mei", "Weizhe Lin", "Bill Byrne"], "title": "Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak Prevention for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) remain vulnerable to jailbreak attacks, which\nattempt to elicit harmful responses from LLMs. The evolving nature and\ndiversity of these attacks pose many challenges for defense systems, including\n(1) adaptation to counter emerging attack strategies without costly retraining,\nand (2) control of the trade-off between safety and utility. To address these\nchallenges, we propose Retrieval-Augmented Defense (RAD), a novel framework for\njailbreak detection that incorporates a database of known attack examples into\nRetrieval-Augmented Generation, which is used to infer the underlying,\nmalicious user query and jailbreak strategy used to attack the system. RAD\nenables training-free updates for newly discovered jailbreak strategies and\nprovides a mechanism to balance safety and utility. Experiments on StrongREJECT\nshow that RAD substantially reduces the effectiveness of strong jailbreak\nattacks such as PAP and PAIR while maintaining low rejection rates for benign\nqueries. We propose a novel evaluation scheme and show that RAD achieves a\nrobust safety-utility trade-off across a range of operating points in a\ncontrollable manner."}
