<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 19]
- [cs.SE](#cs.SE) [Total: 17]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Implementing Zero Trust Architecture to Enhance Security and Resilience in the Pharmaceutical Supply Chain](https://arxiv.org/abs/2508.15776)
*Saeid Ghasemshirazi,Ghazaleh Shirvani,Marziye Ranjbar Tavakoli,Bahar Ghaedi,Mohammad Amin Langarizadeh*

Main category: cs.CR

TL;DR: This paper advocates adopting Zero Trust Architecture to strengthen pharmaceutical supply chains against cyber threats, with real-world case studies showing enhanced security and resilience, especially for critical drug management.


<details>
  <summary>Details</summary>
Motivation: The pharmaceutical supply chain faces escalating cybersecurity threats (e.g., data breaches, counterfeiting, operational disruptions) that endanger patient safety and operational continuity, necessitating robust security frameworks like ZTA.

Method: The study explores ZTA through the principles of continuous verification, least-privilege access, and data-centric security, supported by real-world case studies demonstrating its implementation in pharmaceutical supply chains.

Result: Implementing ZTA enhances security, data protection, and adaptable resilience. Case studies validate its success in managing narcotics and high-health-risk drugs, aligning with industry-recognized needs for reliable drug tracing systems.

Conclusion: The paper concludes that adopting Zero Trust Architecture (ZTA) can effectively fortify pharmaceutical supply chains against evolving cyber threats, ensuring the trustworthiness of critical medical operations, particularly in high-risk areas like narcotics and drug tracing.

Abstract: The pharmaceutical supply chain faces escalating cybersecurity challenges
threatening patient safety and operational continuity. This paper examines the
transformative potential of zero trust architecture for enhancing security and
resilience within this critical ecosystem. We explore the challenges posed by
data breaches, counterfeiting, and disruptions and introduce the principles of
continuous verification, least-privilege access, and data-centric security
inherent in zero trust. Real-world case studies illustrate successful
implementations. Benefits include heightened security, data protection, and
adaptable resilience. As recognized by researchers and industrialists, a
reliable drug tracing system is crucial for ensuring drug safety throughout the
pharmaceutical production process. One of the most pivotal domains within the
pharmaceutical industry and its associated supply chains where zero trust can
be effectively implemented is in the management of narcotics, high-health-risk
drugs, and abusable substances. By embracing zero trust, the pharmaceutical
industry fortifies its supply chain against constantly changing cyber threats,
ensuring the trustworthiness of critical medical operations.

</details>


### [2] [Towards Stealthy and Effective Backdoor Attacks on Lane Detection: A Naturalistic Data Poisoning Approach](https://arxiv.org/abs/2508.15778)
*Yifan Liao,Yuxin Cao,Yedi Zhang,Wentao He,Yan Xiao,Xianglong Du,Zhiyong Huang,Jin Song Dong*

Main category: cs.CR

TL;DR: DBALD introduces stealthy, realistic backdoor attacks for lane detection models using diffusion-based techniques, exposing critical vulnerabilities in autonomous driving safety.


<details>
  <summary>Details</summary>
Motivation: Existing backdoor attacks on lane detection models use artificial triggers that lack real-world plausibility, limiting their practical threat. The authors aim to address this gap by introducing ecologically valid, stealthy attack methods for more realistic security threats.

Method: The paper proposes DBALD, a diffusion-based data poisoning framework with two components: heatmap-based optimal trigger positioning (using gradient analysis) and visually plausible trigger generation via region-based editing diffusion processes, complemented by lane structure preservation and driving scene consistency losses.

Result: DBALD outperforms existing methods across four LD models by +10.87% average success rate while achieving significantly improved stealthiness, validated through extensive experiments demonstrating both effectiveness and scene coherence.

Conclusion: The paper highlights the urgent need for robust deep learning-based lane detection models against real-world backdoor threats, as demonstrated by the effectiveness of DBALD in evading detection while maintaining high attack success rates.

Abstract: Deep learning-based lane detection (LD) plays a critical role in autonomous
driving and advanced driver assistance systems. However, its vulnerability to
backdoor attacks presents a significant security concern. Existing backdoor
attack methods on LD often exhibit limited practical utility due to the
artificial and conspicuous nature of their triggers. To address this limitation
and investigate the impact of more ecologically valid backdoor attacks on LD
models, we examine the common data poisoning attack and introduce DBALD, a
novel diffusion-based data poisoning framework for generating naturalistic
backdoor triggers. DBALD comprises two key components: optimal trigger position
finding and stealthy trigger generation. Given the insight that attack
performance varies depending on the trigger position, we propose a
heatmap-based method to identify the optimal trigger location, with gradient
analysis to generate attack-specific heatmaps. A region-based editing diffusion
process is then applied to synthesize visually plausible triggers within the
most susceptible regions identified previously. Furthermore, to ensure scene
integrity and stealthy attacks, we introduce two loss strategies: one for
preserving lane structure and another for maintaining the consistency of the
driving scene. Consequently, compared to existing attack methods, DBALD
achieves both a high attack success rate and superior stealthiness. Extensive
experiments on 4 mainstream LD models show that DBALD exceeds state-of-the-art
methods, with an average success rate improvement of +10.87% and significantly
enhanced stealthiness. The experimental results highlight significant practical
challenges in ensuring model robustness against real-world backdoor threats in
LD.

</details>


### [3] [Uplifted Attackers, Human Defenders: The Cyber Offense-Defense Balance for Trailing-Edge Organizations](https://arxiv.org/abs/2508.15808)
*Benjamin Murphy,Twm Stone*

Main category: cs.CR

TL;DR: AI is making cyberattacks cheaper and more frequent, putting under-invested firms at higher risk. Old-school companies need to speed up fixes and build better software resilience, not just match todayâ€™s standards.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the mismatch between the theoretical benefits of AI for cybersecurity and the realities of under-resourced 'trailing-edge organizations,' which face heightened risks due to legacy systems and cost-based underinvestment in defenses.

Method: The paper combines analysis of AI's role in cyber economics with an examination of organizational cybersecurity practices, evaluating how underinvestment in defense interacts with AI-driven attack capabilities.

Result: The analysis shows that AI reduces the marginal cost of cyberattacks, exposes trailing-edge organizations to higher attack rates, and requires these entities to outpace current defensive benchmarks to remain secure.

Conclusion: The paper concludes that AI advancements will increase cyberattack frequency and effectiveness, necessitating trailing-edge organizations to adopt faster remediation and more resilient systems, with proposed solutions for both organizational and governmental actions to mitigate risks.

Abstract: Advances in AI are widely understood to have implications for cybersecurity.
Articles have emphasized the effect of AI on the cyber offense-defense balance,
and commentators can be found arguing either that cyber will privilege
attackers or defenders. For defenders, arguments are often made that AI will
enable solutions like formal verification of all software--and for some
well-equipped companies, this may be true. This conversation, however, does not
match the reality for most companies. "Trailing-edge organizations," as we term
them, rely heavily on legacy software, poorly staff security roles, and
struggle to implement best practices like rapid deployment of security patches.
These decisions may be the result of corporate inertia, but may also be the
result of a seemingly-rational calculation that attackers may not bother
targeting a firm due to lack of economic incentives, and as a result,
underinvestment in defense will not be punished.
  This approach to security may have been sufficient prior to the development
of AI systems, but it is unlikely to remain viable in the near future. We argue
that continuing improvements in AI's capabilities poses additional risks on two
fronts: First, increased usage of AI will alter the economics of the marginal
cyberattack and expose these trailing-edge organizations to more attackers,
more frequently. Second, AI's advances will enable attackers to develop
exploits and launch attacks earlier than they can today--meaning that it is
insufficient for these companies to attain parity with today's leading
defenders, but must instead aim for faster remediation timelines and more
resilient software. The situation today portends a dramatically increased
number of attacks in the near future. Moving forward, we offer a range of
solutions for both organizations and governments to improve the defensive
posture of firms which lag behind their peers today.

</details>


### [4] [CIA+TA Risk Assessment for AI Reasoning Vulnerabilities](https://arxiv.org/abs/2508.15839)
*Yuksel Aydin*

Main category: cs.CR

TL;DR: The paper introduces cognitive cybersecurity as a framework to protect AI reasoning processes, extending traditional security models with epistemic Trust and human Autonomy. Validation shows defenses' effectiveness depends heavily on system architecture, necessitating pre-deployment testing.


<details>
  <summary>Details</summary>
Motivation: AI systems face adversarial threats targeting reasoning mechanisms rather than infrastructure, creating vulnerabilities where legitimate inputs corrupt reasoning while evading conventional security controls. This necessitates systematic protection of knowledge-generating AI systems and human-mediated decisions.

Method: The authors propose a three-part framework: (1) defining cognitive cybersecurity as a distinct discipline addressing reasoning vulnerabilities, (2) extending the CIA triad with Trust and Autonomy (CIA+TA), and (3) a quantitative risk assessment methodology using empirically-derived coefficients. Validation integrates with OWASP LLM Top 10 and MITRE ATLAS for operational feasibility.

Result: Empirical validation with 151 human participants and 12,180 AI trials demonstrated architecture-dependent outcomes: identical defenses varied between 96% risk reduction and 135% vulnerability amplification, underscoring the need for pre-deployment Cognitive Penetration Testing.

Conclusion: This work establishes cognitive cybersecurity as a critical discipline to safeguard AI reasoning processes, emphasizing the necessity of pre-deployment Cognitive Penetration Testing to ensure trustworthy AI deployment in the face of architecture-dependent vulnerabilities.

Abstract: As AI systems increasingly influence critical decisions, they face threats
that exploit reasoning mechanisms rather than technical infrastructure. We
present a framework for cognitive cybersecurity, a systematic protection of AI
reasoning processes from adversarial manipulation. Our contributions are
threefold. First, we establish cognitive cybersecurity as a discipline
complementing traditional cybersecurity and AI safety, addressing
vulnerabilities where legitimate inputs corrupt reasoning while evading
conventional controls. Second, we introduce the CIA+TA, extending traditional
Confidentiality, Integrity, and Availability triad with Trust (epistemic
validation) and Autonomy (human agency preservation), requirements unique to
systems generating knowledge claims and mediating decisions. Third, we present
a quantitative risk assessment methodology with empirically-derived
coefficients, enabling organizations to measure cognitive security risks. We
map our framework to OWASP LLM Top 10 and MITRE ATLAS, facilitating operational
integration. Validation through previously published studies (151 human
participants; 12,180 AI trials) reveals strong architecture dependence:
identical defenses produce effects ranging from 96% reduction to 135%
amplification of vulnerabilities. This necessitates pre-deployment Cognitive
Penetration Testing as a governance requirement for trustworthy AI deployment.

</details>


### [5] [Unveiling Unicode's Unseen Underpinnings in Undermining Authorship Attribution](https://arxiv.org/abs/2508.15840)
*Robert Dilworth*

Main category: cs.CR

TL;DR: Public messages unavoidably expose author identities through stylometrics, but the paper shows new countermeasures using adversarial text manipulation and steganography can effectively enhance user privacy.


<details>
  <summary>Details</summary>
Motivation: Despite extensive anonymization measures, the content of public messages inherently exposes users to deanonymization through stylometric analysis, making privacy protection in communication a critical challenge.

Method: The authors dissect stylometry techniques, develop adversarial stylometry methods, and introduce Unicode steganography as a way to obfuscate authorship while maintaining message integrity.

Result: The proposed adversarial approaches successfully introduce noise into text analysis, making accurate author attribution significantly less feasible while preserving the intended message's meaning and function.

Conclusion: The paper concludes that content-based stylometric analysis poses a significant threat to user anonymity, and the proposed counter-strategies using adversarial stylometry and Unicode steganography can effectively mitigate this risk.

Abstract: When using a public communication channel -- whether formal or informal, such
as commenting or posting on social media -- end users have no expectation of
privacy: they compose a message and broadcast it for the world to see. Even if
an end user takes utmost precautions to anonymize their online presence --
using an alias or pseudonym; masking their IP address; spoofing their
geolocation; concealing their operating system and user agent; deploying
encryption; registering with a disposable phone number or email; disabling
non-essential settings; revoking permissions; and blocking cookies and
fingerprinting -- one obvious element still lingers: the message itself.
Assuming they avoid lapses in judgment or accidental self-exposure, there
should be little evidence to validate their actual identity, right? Wrong. The
content of their message -- necessarily open for public consumption -- exposes
an attack vector: stylometric analysis, or author profiling. In this paper, we
dissect the technique of stylometry, discuss an antithetical counter-strategy
in adversarial stylometry, and devise enhancements through Unicode
steganography.

</details>


### [6] [Self-Disguise Attack: Induce the LLM to disguise itself for AIGT detection evasion](https://arxiv.org/abs/2508.15848)
*Yinghan Zhou,Juan Wen,Wanli Peng,Zhengxian Wu,Ziwei Zhang,Yiming Xue*

Main category: cs.CR

TL;DR: This paper proposes SDA, an LLM attack method that reduces AI text detection rates through adversarial feature extraction and knowledge-based optimization, maintaining text quality with lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing evasion methods for AIGT detection are computationally expensive and degrade text quality, creating a need for efficient, effective approaches to improve detector reliability in practical applications.

Method: SDA combines an adversarial feature extractor (to generate disguise features for human-like text production) and a retrieval-based context examples optimizer (to enhance disguise using external knowledge). These components guide LLMs via optimized prompts to create detection-resistant text.

Result: SDA reduced average detection accuracy of multiple AIGT detectors across three LLMs (Qwen, GPT-3.5, LLaMA2) while preserving text quality metrics, demonstrating resource-efficient performance improvements.

Conclusion: The proposed Self-Disguise Attack (SDA) effectively reduces detection rates of AI-generated text while maintaining text quality, demonstrating a practical approach to enhancing detector robustness through LLM self-disguise capabilities.

Abstract: AI-generated text (AIGT) detection evasion aims to reduce the detection
probability of AIGT, helping to identify weaknesses in detectors and enhance
their effectiveness and reliability in practical applications. Although
existing evasion methods perform well, they suffer from high computational
costs and text quality degradation. To address these challenges, we propose
Self-Disguise Attack (SDA), a novel approach that enables Large Language Models
(LLM) to actively disguise its output, reducing the likelihood of detection by
classifiers. The SDA comprises two main components: the adversarial feature
extractor and the retrieval-based context examples optimizer. The former
generates disguise features that enable LLMs to understand how to produce more
human-like text. The latter retrieves the most relevant examples from an
external knowledge base as in-context examples, further enhancing the
self-disguise ability of LLMs and mitigating the impact of the disguise process
on the diversity of the generated text. The SDA directly employs prompts
containing disguise features and optimized context examples to guide the LLM in
generating detection-resistant text, thereby reducing resource consumption.
Experimental results demonstrate that the SDA effectively reduces the average
detection accuracy of various AIGT detectors across texts generated by three
different LLMs, while maintaining the quality of AIGT.

</details>


### [7] [Linkage Attacks Expose Identity Risks in Public ECG Data Sharing](https://arxiv.org/abs/2508.15850)
*Ziyu Wang,Elahe Khatibi,Farshad Firouzi,Sanaz Rahimi Mousavi,Krishnendu Chakrabarty,Amir M. Rahmani*

Main category: cs.CR

TL;DR: ECG data anonymity is easily breached with realistic attacks; robust privacy measures are urgently needed.


<details>
  <summary>Details</summary>
Motivation: Publicly shared ECG data poses significant privacy risks due to its biometric uniqueness, which prior studies have overlooked by assuming idealized adversarial capabilities. This paper addresses the gap by assessing realistic attack scenarios.

Method: The research evaluates ECG privacy risks using data from 109 participants across real-world datasets, achieving 85% re-identification accuracy with specific misclassification rates, to demonstrate the inadequacy of basic anonymization techniques under realistic adversarial conditions.

Result: The approach achieves 85% accuracy in re-identifying individuals in public datasets, with a 14.2% overall misclassification rate (15.6% false positives and 12.8% false negatives), demonstrating effective identity linkage with limited adversarial knowledge.

Conclusion: The study emphasizes the necessity for advanced privacy-preserving strategies like differential privacy, access control, and encrypted computation to protect ECG data from re-identification risks while maintaining its utility for healthcare applications.

Abstract: The increasing availability of publicly shared electrocardiogram (ECG) data
raises critical privacy concerns, as its biometric properties make individuals
vulnerable to linkage attacks. Unlike prior studies that assume idealized
adversarial capabilities, we evaluate ECG privacy risks under realistic
conditions where attackers operate with partial knowledge. Using data from 109
participants across diverse real-world datasets, our approach achieves 85%
accuracy in re-identifying individuals in public datasets while maintaining a
14.2% overall misclassification rate at an optimal confidence threshold, with
15.6% of unknown individuals misclassified as known and 12.8% of known
individuals misclassified as unknown. These results highlight the inadequacy of
simple anonymization techniques in preventing re-identification, demonstrating
that even limited adversarial knowledge enables effective identity linkage. Our
findings underscore the urgent need for privacy-preserving strategies, such as
differential privacy, access control, and encrypted computation, to mitigate
re-identification risks while ensuring the utility of shared biosignal data in
healthcare applications.

</details>


### [8] [Securing Swarms: Cross-Domain Adaptation for ROS2-based CPS Anomaly Detection](https://arxiv.org/abs/2508.15865)
*Julia Boone,Fatemeh Afghah*

Main category: cs.CR

TL;DR: The paper presents an adaptable anomaly detection model for CPS using domain adaptation without labeled data, validated on a dataset combining network, OS, and ROS data, demonstrating superior performance across attack types.


<details>
  <summary>Details</summary>
Motivation: Current IDS solutions for CPS rely on network traffic-only datasets, neglecting vulnerabilities in other system layers (e.g., physical/operational). This limits their ability to detect cross-layer attacks in real-world CPS applications.

Method: Developed a domain adaptation-based CPS anomaly detection model that transfers attack knowledge from network traffic environments to CPS, leveraging a dataset integrating network traffic, OS logs, and ROS data for validation.

Result: The model outperforms traditional anomaly detection methods on the dataset, effectively detecting distinct attack types (e.g., physical, computational) across network-only and CPS settings.

Conclusion: The proposed domain adaptation framework offers a promising approach for securing multi-layer CPS environments by enabling cross-layer attack detection without requiring pre-labeled CPS-specific data.

Abstract: Cyber-physical systems (CPS) are being increasingly utilized for critical
applications. CPS combines sensing and computing elements, often having
multi-layer designs with networking, computational, and physical interfaces,
which provide them with enhanced capabilities for a variety of application
scenarios. However, the combination of physical and computational elements also
makes CPS more vulnerable to attacks compared to network-only systems, and the
resulting impacts of CPS attacks can be substantial. Intelligent intrusion
detection systems (IDS) are an effective mechanism by which CPS can be secured,
but the majority of current solutions often train and validate on network
traffic-only datasets, ignoring the distinct attacks that may occur on other
system layers. In order to address this, we develop an adaptable CPS anomaly
detection model that can detect attacks within CPS without the need for
previously labeled data. To achieve this, we utilize domain adaptation
techniques that allow us to transfer known attack knowledge from a network
traffic-only environment to a CPS environment. We validate our approach using a
state-of-the-art CPS intrusion dataset that combines network, operating system
(OS), and Robot Operating System (ROS) data. Through this dataset, we are able
to demonstrate the effectiveness of our model across network traffic-only and
CPS environments with distinct attack types and its ability to outperform other
anomaly detection methods.

</details>


### [9] [Evolving k-Threshold Visual Cryptography Schemes](https://arxiv.org/abs/2508.15917)
*Xiaoli Zhuo,Xuehu Yan,Lintao Liu,Wei Yan*

Main category: cs.CR

TL;DR: This paper introduces a novel (k,âˆž) VCS framework that overcomes pixel expansion and contrast limitations in secret image sharing for evolving access structures, validated through rigorous analysis and optimization strategies.


<details>
  <summary>Details</summary>
Motivation: Current visual cryptography schemes for infinite participants suffer from pixel expansion constraints and insufficient contrast, especially for arbitrary access structures, necessitating novel mathematical frameworks for secure and efficient image sharing.

Method: The work introduces a formal (k,âˆž) VCS definition, develops a random-grid-based scheme for arbitrary k values, and implements contrast-enhancement strategies through algorithmic optimizations for specific k thresholds.

Result: Theoretical analysis and experiments confirm that the proposed schemes achieve optimal pixel efficiency and significantly enhance contrast compared to existing methods, particularly for k â‰¥ 2.

Conclusion: The proposed (k,âˆž) VCS framework addresses critical limitations in existing secret image sharing methods by enabling flexible access structures with improved efficiency and contrast.

Abstract: In evolving access structures, the number of participants is countably
infinite with no predetermined upper bound. While such structures have been
realized in secret sharing, research in secret image sharing has primarily
focused on visual cryptography schemes (VCS). However, there exists no
construction for $(k,\infty)$ VCS that applies to arbitrary $k$ values without
pixel expansion currently, and the contrast requires enhancement. In this
paper, we first present a formal mathematical definition of $(k,\infty)$ VCS.
Then, propose a $(k,\infty)$ VCS based on random grids that works for arbitrary
$k$. In addition, to further improve contrast, we develop optimized
$(k,\infty)$ VCS for $k=2$ and $3$, along with contrast enhancement strategies
for $k\geq 4$. Theoretical analysis and experimental results demonstrate the
superiority of our proposed schemes.

</details>


### [10] [Strategic Sample Selection for Improved Clean-Label Backdoor Attacks in Text Classification](https://arxiv.org/abs/2508.15934)
*Onur Alp Kirci,M. Emre Gursoy*

Main category: cs.CR

TL;DR: New sample selection strategies improve clean-label backdoor attacks by focusing on low-confidence samples, achieving higher attack success rates and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Clean-label backdoor attacks are more challenging than dirty-label attacks. Prior methods achieve limited success, necessitating improved strategies to increase attack effectiveness while preserving clean data performance.

Method: Three sample selection strategies (Minimum, Above50, Below50) are introduced to select uncertain or misclassified samples for injecting backdoor triggers. These strategies are integrated into four existing clean-label backdoor attack methods (InsertSent, WordInj, StyleBkd, SynBkd) to strengthen trigger-target label associations.

Result: The proposed strategies, particularly Minimum, significantly boost attack success rates (ASR) compared to random selection across three datasets (IMDB, SST2, HateSpeech) and four models (LSTM, BERT, DistilBERT, RoBERTa). Enhanced attacks outperform BITE in many configurations without notable clean accuracy degradation.

Conclusion: The proposed sample selection strategies effectively enhance clean-label backdoor attacks, with the Minimum strategy showing the most significant improvements, outperforming existing methods like BITE while maintaining clean data accuracy.

Abstract: Backdoor attacks pose a significant threat to the integrity of text
classification models used in natural language processing. While several
dirty-label attacks that achieve high attack success rates (ASR) have been
proposed, clean-label attacks are inherently more difficult. In this paper, we
propose three sample selection strategies to improve attack effectiveness in
clean-label scenarios: Minimum, Above50, and Below50. Our strategies identify
those samples which the model predicts incorrectly or with low confidence, and
by injecting backdoor triggers into such samples, we aim to induce a stronger
association between the trigger patterns and the attacker-desired target label.
We apply our methods to clean-label variants of four canonical backdoor attacks
(InsertSent, WordInj, StyleBkd, SynBkd) and evaluate them on three datasets
(IMDB, SST2, HateSpeech) and four model types (LSTM, BERT, DistilBERT,
RoBERTa). Results show that the proposed strategies, particularly the Minimum
strategy, significantly improve the ASR over random sample selection with
little or no degradation in the model's clean accuracy. Furthermore,
clean-label attacks enhanced by our strategies outperform BITE, a state of the
art clean-label attack method, in many configurations.

</details>


### [11] [PickleBall: Secure Deserialization of Pickle-based Machine Learning Models](https://arxiv.org/abs/2508.15987)
*Andreas D. Kellas,Neophytos Christou,Wenxin Jiang,Penghui Li,Laurent Simon,Yaniv David,Vasileios P. Kemerlis,James C. Davis,Junfeng Yang*

Main category: cs.CR

TL;DR: PickleBall secures ML model loading through code analysis-enforced policies, safely loading 79.8% of benign models while blocking 100% of malicious ones, outperforming existing defenses.


<details>
  <summary>Details</summary>
Motivation: Current defenses against malicious ML models (e.g., safer formats, restrictive loading policies, scanners) have critical limitations: 44.9% of Hugging Face models use insecure pickle, 15% of these are incompatible with restrictive policies, and scanners exhibit high false positive/negative rates. The ML community lacks a tool enabling transparent, safe pickle loading that balances security and usability.

Method: PickleBall employs static analysis of machine learning library source code to generate custom security policies, which are then dynamically enforced during model loading as a drop-in replacement for the pickle module. This two-phase approach ensures safe behavior for benign models while blocking malicious ones.

Result: PickleBall achieves 79.8% success in loading benign pickle models while blocking all malicious examples in evaluation datasets. It outperforms existing loaders by enabling correct loading of 22% more benign models than the state-of-the-art solution, while maintaining 100% detection accuracy against known attacks.

Conclusion: PickleBall significantly enhances the security of loading pickle-based machine learning models by introducing a static analysis and dynamic enforcement approach, outperforming existing scanners and restrictive loading policies. It sets a higher security bar for attackers by eliminating arbitrary function invocation risks.

Abstract: Machine learning model repositories such as the Hugging Face Model Hub
facilitate model exchanges. However, bad actors can deliver malware through
compromised models. Existing defenses such as safer model formats, restrictive
(but inflexible) loading policies, and model scanners have shortcomings: 44.9%
of popular models on Hugging Face still use the insecure pickle format, 15% of
these cannot be loaded by restrictive loading policies, and model scanners have
both false positives and false negatives. Pickle remains the de facto standard
for model exchange, and the ML community lacks a tool that offers transparent
safe loading.
  We present PickleBall to help machine learning engineers load pickle-based
models safely. PickleBall statically analyzes the source code of a given
machine learning library and computes a custom policy that specifies a safe
load-time behavior for benign models. PickleBall then dynamically enforces the
policy during load time as a drop-in replacement for the pickle module.
PickleBall generates policies that correctly load 79.8% of benign pickle-based
models in our dataset, while rejecting all (100%) malicious examples in our
dataset. In comparison, evaluated model scanners fail to identify known
malicious models, and the state-of-art loader loads 22% fewer benign models
than PickleBall. PickleBall removes the threat of arbitrary function invocation
from malicious pickle-based models, raising the bar for attackers to depend on
code reuse techniques.

</details>


### [12] [A Survey of Post-Quantum Cryptography Support in Cryptographic Libraries](https://arxiv.org/abs/2508.16078)
*Nadeem Ahmed,Lei Zhang,Aryya Gangopadhyay*

Main category: cs.CR

TL;DR: This paper evaluates PQC library support (as of 2025), revealing inconsistent adoption of NIST finalists and highlighting urgent challenges in securing a quantum-resistant cryptographic transition.


<details>
  <summary>Details</summary>
Motivation: Quantum computing's advancements threaten modern cryptography, necessitating a transition to quantum-resistant algorithms. The study aims to assess the current state of PQC library adoption to identify gaps and risks.

Method: The evaluation focused on nine open-source cryptographic libraries (OpenSSL, wolfSSL, etc.), analyzing their implementation of NIST-selected PQC finalists (CRYSTALS-Kyber, CRYSTALS-Dilithium, FALCON, SPHINCS+) using documentation, release notes, and industry reports (as of early 2025).

Result: Divergent readiness among libraries: some libraries have integrated PQC or defined roadmaps, while others lag. Key challenges include performance trade-offs, implementation security concerns, and adoption barriers in real-world applications.

Conclusion: The study underscores the urgent need for continued research, standardization, and coordinated adoption strategies to address the varied readiness of cryptographic libraries in supporting Post-Quantum Cryptography (PQC) as quantum threats become imminent.

Abstract: The rapid advancement of quantum computing poses a significant threat to
modern cryptographic systems, necessitating the transition to Post-Quantum
Cryptography (PQC). This study evaluates the support for PQC algorithms within
nine widely used open-source cryptographic libraries -- OpenSSL, wolfSSL,
BoringSSL, LibreSSL, Bouncy Castle, libsodium, Crypto++, Botan, and MbedTLS --
focusing on their implementation of the NIST-selected PQC finalists:
CRYSTALS-Kyber, CRYSTALS-Dilithium, FALCON, and SPHINCS+. Our analysis, based
on the latest available documentation, release notes, and industry reports as
of early 2025, reveals a varied state of readiness across these libraries.
While some libraries have integrated PQC support or have clear implementation
roadmaps, others lag behind, creating potential security risks as quantum
threats become more imminent. We discuss key challenges, including performance
trade-offs, implementation security, and adoption hurdles in real-world
cryptographic applications. Our findings highlight the urgent need for
continued research, standardization efforts, and coordinated adoption
strategies to ensure a secure transition to the quantum-resistant cryptographic
landscape.

</details>


### [13] [SoK: Understanding the Fundamentals and Implications of Sensor Out-of-band Vulnerabilities](https://arxiv.org/abs/2508.16133)
*Shilin Xiao,Wenjun Zhu,Yan Jiang,Kai Wang,Peiwang Wang,Chen Yan,Xiaoyu Ji,Wenyuan Xu*

Main category: cs.CR

TL;DR: This paper systematizes sensor hardware vulnerabilities in CPS using a novel framework that analyzes vulnerabilities at component, sensor, and system levels, offering foundational insights and design guidelines for secure CPS.


<details>
  <summary>Details</summary>
Motivation: Current research on sensor security is fragmented, with ad-hoc methodologies and an infinite attack signal space complicating threat abstraction and defense. This creates a need for a comprehensive framework to systematically address sensor hardware vulnerabilities.

Method: The study employs a bottom-up systematization approach, analyzing OOB vulnerabilities at three levels: (1) component level (physical principles and limitations), (2) sensor level (attack categorization and practicality evaluation), and (3) system level (analysis of how CPS features like sensor fusion and closed-loop control affect vulnerability exposure).

Result: The framework identifies OOB vulnerabilities across hardware and system layers, categorizes practical attacks, and elucidates how CPS design features influence vulnerability dynamics. It provides actionable guidance for securing sensor hardware and CPS architectures.

Conclusion: The paper introduces a systematic framework for understanding sensor hardware vulnerabilities in cyber-physical systems, offering a structured approach to analyze, categorize, and mitigate out-of-band (OOB) attacks through a three-level methodology.

Abstract: Sensors are fundamental to cyber-physical systems (CPS), enabling perception
and control by transducing physical stimuli into digital measurements. However,
despite growing research on physical attacks on sensors, our understanding of
sensor hardware vulnerabilities remains fragmented due to the ad-hoc nature of
this field. Moreover, the infinite attack signal space further complicates
threat abstraction and defense. To address this gap, we propose a
systematization framework, termed sensor out-of-band (OOB) vulnerabilities,
that for the first time provides a comprehensive abstraction for sensor attack
surfaces based on underlying physical principles. We adopt a bottom-up
systematization methodology that analyzes OOB vulnerabilities across three
levels. At the component level, we identify the physical principles and
limitations that contribute to OOB vulnerabilities. At the sensor level, we
categorize known attacks and evaluate their practicality. At the system level,
we analyze how CPS features such as sensor fusion, closed-loop control, and
intelligent perception impact the exposure and mitigation of OOB threats. Our
findings offer a foundational understanding of sensor hardware security and
provide guidance and future directions for sensor designers, security
researchers, and system developers aiming to build more secure sensors and CPS.

</details>


### [14] [Evaluating the Defense Potential of Machine Unlearning against Membership Inference Attacks](https://arxiv.org/abs/2508.16150)
*Aristeidis Sidiropoulos,Christos Chrysanthos Nikolaidis,Theodoros Tsiolakis,Nikolaos Pavlidis,Vasilis Perifanis,Pavlos S. Efraimidis*

Main category: cs.CR

TL;DR: This paper evaluates whether Machine Unlearning mitigates Membership Inference Attacks (MIAs) and finds that while unlearning isnâ€™t inherently a privacy countermeasure, its effectiveness depends on the algorithm and data type, offering practical guidance for designing privacy-preserving systems.


<details>
  <summary>Details</summary>
Motivation: Machine Unlearning is intended to enhance privacy by removing private data from models, but its effectiveness in mitigating Membership Inference Attacks (MIAs) remained unclear. This study addresses this gap to clarify the role of unlearning in privacy-preserving systems.

Method: The paper systematically evaluates the vulnerability of models to MIAs after applying state-of-the-art Machine Unlearning algorithms across four datasets (two image and two tabular), analyzing how unlearning approaches and data properties influence model exposure.

Result: Findings indicate that Machine Unlearning does not inherently reduce MIA risk but highlights that the specific unlearning algorithm and data characteristics (e.g., image vs. tabular) significantly modify a model's susceptibility to membership inference.

Conclusion: The study provides essential insights into the interplay between Machine Unlearning and Membership Inference Attacks (MIAs), demonstrating that while unlearning is not inherently a countermeasure to MIA, the choice of algorithms and data characteristics significantly impacts a model's vulnerability. This offers guidance for designing privacy-preserving machine learning systems.

Abstract: Membership Inference Attacks (MIAs) pose a significant privacy risk, as they
enable adversaries to determine whether a specific data point was included in
the training dataset of a model. While Machine Unlearning is primarily designed
as a privacy mechanism to efficiently remove private data from a machine
learning model without the need for full retraining, its impact on the
susceptibility of models to MIA remains an open question. In this study, we
systematically assess the vulnerability of models to MIA after applying
state-of-art Machine Unlearning algorithms. Our analysis spans four diverse
datasets (two from the image domain and two in tabular format), exploring how
different unlearning approaches influence the exposure of models to membership
inference. The findings highlight that while Machine Unlearning is not
inherently a countermeasure against MIA, the unlearning algorithm and data
characteristics can significantly affect a model's vulnerability. This work
provides essential insights into the interplay between Machine Unlearning and
MIAs, offering guidance for the design of privacy-preserving machine learning
systems.

</details>


### [15] [A Relay-Chain-Powered Ciphertext-Policy Attribute-Based Encryption in Intelligent Transportation Systems](https://arxiv.org/abs/2508.16189)
*Aparna Singh,Geetanjali Rathee,Chaker Abdelaziz Kerrache,Mohamed Chahine Ghanem*

Main category: cs.CR

TL;DR: This paper introduces a relay chain and CP-ABE-based architecture for secure ITS data sharing, offering context-aware encryption, dynamic access control, and scalability across distributed, multi-jurisdictional networks.


<details>
  <summary>Details</summary>
Motivation: The urgent need for secure, effective, and context-aware data sharing in heterogeneous, geographically dispersed ITS environments necessitates solutions to dynamic access control and low-latency communication challenges.

Method: The model integrates a relay chain with context-aware smart contracts to evaluate data properties (event type, time, geography) for dynamic encryption policies. On-Board Units (OBUs) use CP-ABE for end-to-end encryption, storing ciphertext in regional blockchains. High-sensitivity events use strict multi-attribute rules, while common updates apply light policies to reduce computational load.

Result: The system enables traceable, low-latency revocation managed globally via the relay chain, while regional blockchains eliminate reliance on symmetric encryption or off-chain storage. It provides real-time responsiveness with robust security, suitable for next-generation vehicular networks.

Conclusion: The proposed architecture effectively addresses the challenges of secure, context-aware data sharing in Intelligent Transportation Systems by combining relay chain-driven encryption with modified CP-ABE, achieving a scalable balance between real-time responsiveness and security across multi-jurisdictional domains.

Abstract: The very high growth of Intelligent Transportation Systems (ITS) has
generated an urgent requirement for secure, effective, and context-aware data
sharing mechanisms, especially over heterogeneous and geographically dispersed
settings. This work suggests a new architecture that combines a relay
chain-driven encryption system with a modified Ciphertext-Policy
Attribute-Based Encryption (CP-ABE) scheme to tackle the double impediment of
dynamic access and low-latency communication. The model proposes a
context-aware smart contract on a worldwide relay chain that checks against
data properties, including event type, time, and geographical region, to
specify the suitable level of encryption policy. From such relay-directed
judgment, On-Board Units (OBUs) encrypt data end-to-end by utilising CP-ABE and
store ciphertext inside localised regional blockchains, preventing dependence
on symmetric encryption or off-chain storage. High-sensitivity events are
secured with firm, multi-attribute access rules, whereas common updates use
light policies to help reduce processing burdens. The crypto system also adds
traceability and low-latency revocation, with global enforcement managed
through the relay chain. This distributed, scalable model provides a proper
balance between responsiveness in real time and security and is extremely apt
for next-gen vehicular networks that function across multi-jurisdictional
domains.

</details>


### [16] [How to Beat Nakamoto in the Race](https://arxiv.org/abs/2508.16202)
*Shu-Jie Cao,Dongning Guo*

Main category: cs.CR

TL;DR: Paper introduces MDP framework to mathematically solve for optimal blockchain attacks under delay constraints, calculating exact safety violation probabilities for Nakamoto consensus


<details>
  <summary>Details</summary>
Motivation: The paper addresses critical gaps in blockchain security by determining the most effective adversarial attack strategy and quantifying safety risks under real-world network delay constraints.

Method: The authors use a Markov decision process (MDP) framework to model system states, adversarial actions, and state transitions, combined with Markov chain analysis to compute violation probabilities for any confirmation depth.

Result: The bait-and-switch attack strategy is proven optimal, with exact violation probability calculations revealing the precise relationship between network delay, confirmation rules, and blockchain security thresholds.

Conclusion: This paper settles two longstanding questions in blockchain security by introducing an MDP framework to analyze optimal adversarial strategies and deriving exact probabilities of safety violations under bounded network delays.

Abstract: This paper studies proof-of-work Nakamoto consensus under bounded network
delays, settling two long-standing questions in blockchain security: How can an
adversary most effectively attack block safety under a given block confirmation
latency? And what is the resulting probability of safety violation? A Markov
decision process (MDP) framework is introduced to precise characterize the
system state (including the tree and timings of all blocks mined), the
adversary's potential actions, and the state transitions due to the adversarial
action and the random block arrival processes. An optimal attack, called
bait-and-switch, is proposed and proved to maximize the adversary's chance of
violating block safety by "beating Nakamoto in the race". The exact probability
of this violation is calculated for any confirmation depth using Markov chain
analysis, offering fresh insights into the interplay of network delay,
confirmation rules, and blockchain security.

</details>


### [17] [Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and Investigating the Real Misuse Threat of LLMs](https://arxiv.org/abs/2508.16347)
*Yu Yan,Sheng Sun,Zhe Wang,Yijun Lin,Zenghao Duan,zhifei zheng,Min Liu,Zhiyi yin,Jianping Zhang*

Main category: cs.CR

TL;DR: This paper exposes the limitations of current LLM safety evaluations by demonstrating that jailbreak success doesn't equate to real-world threat capability, with existing frameworks misjudging harmfulness through superficial toxic language analysis rather than true knowledge understanding.


<details>
  <summary>Details</summary>
Motivation: Prior studies show LLMs are vulnerable to jailbreak attacks, but it remains ambiguous whether these models genuinely understand real-world crimes or merely mimic toxic patterns, raising concerns about hallucination loops between jailbroken models and judger models.

Method: Constructs knowledge-intensive Q&A to examine three key areas of LLM misuse: dangerous knowledge possession, harmful task planning utility, and harmfulness judgment robustness through decoupled jailbreak technique analysis.

Result: Experiments reveal (1) mismatch between jailbreak success rates and actual harmful knowledge possession, and (2) LLM-as-a-judge frameworks over-rely on surface-level toxic language patterns rather than deep harmfulness analysis.

Conclusion: Existing safety assessments of LLMs based on jailbreak success rates and toxic language patterns fail to capture real-world threat potential, revealing a critical gap in evaluating harmful knowledge possession and task planning robustness.

Abstract: With the development of Large Language Models (LLMs), numerous efforts have
revealed their vulnerabilities to jailbreak attacks. Although these studies
have driven the progress in LLMs' safety alignment, it remains unclear whether
LLMs have internalized authentic knowledge to deal with real-world crimes, or
are merely forced to simulate toxic language patterns. This ambiguity raises
concerns that jailbreak success is often attributable to a hallucination loop
between jailbroken LLM and judger LLM. By decoupling the use of jailbreak
techniques, we construct knowledge-intensive Q\&A to investigate the misuse
threats of LLMs in terms of dangerous knowledge possession, harmful task
planning utility, and harmfulness judgment robustness. Experiments reveal a
mismatch between jailbreak success rates and harmful knowledge possession in
LLMs, and existing LLM-as-a-judge frameworks tend to anchor harmfulness
judgments on toxic language patterns. Our study reveals a gap between existing
LLM safety assessments and real-world threat potential.

</details>


### [18] [Temperature-Resilient Reconfigurable PUF with Dual-Pulse Modulation based on SOT-MRAM Chip](https://arxiv.org/abs/2508.16405)
*Min Wang,Chuanpeng Jiang,Zhaohao Wang,Zhengyi Hou,Zhongkui Zhang,Yuanfu Zhao,Hongxi Liu,Weisheng Zhao*

Main category: cs.CR

TL;DR: This paper addresses temperature-dependent limitations in reconfigurable PUFs by proposing a dual-pulse SOT-MRAM strategy that enables temperature-resilient real-time key reconfiguration for IoT security.


<details>
  <summary>Details</summary>
Motivation: Current reconfigurable PUFs lack temperature-independent real-time reconfiguration capabilities, leaving gaps in securing IoT systems against dynamic threats and environmental changes.

Method: Introduces a dual-pulse reconfiguration strategy using SOT-MRAM carriers to expand operating windows and maintain PUF performance across temperature variations without dynamic temperature feedback.

Result: Demonstrates real-time, temperature-resilient reconfiguration over industrial-grade thermal ranges with strong PUF metrics, eliminating the need for real-time temperature monitoring.

Conclusion: The proposed SOT-MRAM-based dual-pulse rPUF design provides a robust foundation for next-generation IoT security architectures, particularly in dynamic environments with temperature fluctuations.

Abstract: In the Internet of Things (IoT) era, hardware-based security solutions have
become an emerging choice for enhancing end-terminal information security. As
one of the hardware technologies, physical unclonable functions (PUFs) utilize
the inherent variations in the manufacturing process to generate cryptographic
keys. Reconfigurable PUFs (rPUFs), characterized by updating cryptographic
keys, offer enhanced security ability for protecting massive amounts of data in
dynamic operational scenarios. The core challenge lies in achieving real-time
reconfiguration independent of environmental conditions, particularly operating
temperature, which has rarely been investigated and addressed. In this study,
we propose a dual-pulse reconfiguration strategy based on SOT-MRAM carriers,
which effectively widens the operating window and exhibits excellent PUF
metrics. Experimental results demonstrate that our design achieves real-time
reconfiguration across industrial-grade operating temperature ranges, without
the need for dynamic feedback of real-time temperature. The proposed SOT-MRAM
rPUF design lays a solid foundation for next-generation IoT protection
architectures.

</details>


### [19] [Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak Prevention for Large Language Models](https://arxiv.org/abs/2508.16406)
*Guangyu Yang,Jinghong Chen,Jingbiao Mei,Weizhe Lin,Bill Byrne*

Main category: cs.CR

TL;DR: A defense system called RAD protects LLMs from jailbreak attacks by using a database of known attacks to detect threats without retraining, balancing safety and utility effectively.


<details>
  <summary>Details</summary>
Motivation: The evolving and diverse nature of jailbreak attacks on LLMs creates two major challenges for defense systems: (1) adapting to new attacks without costly retraining, and (2) maintaining a balance between safety (preventing harmful responses) and utility (avoiding over-rejection of benign queries).

Method: RAD leverages a database of known attack examples within Retrieval-Augmented Generation to infer malicious user intent and jailbreak strategies. It enables training-free updates for emerging attacks and uses a retrieval-based mechanism to maintain safety while minimizing utility losses.

Result: Experiments on StrongREJECT show RAD significantly reduces the effectiveness of strong jailbreak attacks (e.g., PAP, PAIR) while keeping low rejection rates for benign queries. A novel evaluation framework demonstrates RAD's ability to achieve a robust and controllable safety-utility trade-off across different operating points.

Conclusion: The paper introduces Retrieval-Augmented Defense (RAD) as an effective framework to counter jailbreak attacks on LLMs. RAD addresses the challenges of adapting to new attacks without retraining and balancing safety-utility, achieving a robust and controllable trade-off between these aspects through a novel evaluation scheme.

Abstract: Large Language Models (LLMs) remain vulnerable to jailbreak attacks, which
attempt to elicit harmful responses from LLMs. The evolving nature and
diversity of these attacks pose many challenges for defense systems, including
(1) adaptation to counter emerging attack strategies without costly retraining,
and (2) control of the trade-off between safety and utility. To address these
challenges, we propose Retrieval-Augmented Defense (RAD), a novel framework for
jailbreak detection that incorporates a database of known attack examples into
Retrieval-Augmented Generation, which is used to infer the underlying,
malicious user query and jailbreak strategy used to attack the system. RAD
enables training-free updates for newly discovered jailbreak strategies and
provides a mechanism to balance safety and utility. Experiments on StrongREJECT
show that RAD substantially reduces the effectiveness of strong jailbreak
attacks such as PAP and PAIR while maintaining low rejection rates for benign
queries. We propose a novel evaluation scheme and show that RAD achieves a
robust safety-utility trade-off across a range of operating points in a
controllable manner.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [20] [A Systematic Literature Review of Machine Learning Approaches for Migrating Monolithic Systems to Microservices](https://arxiv.org/abs/2508.15941)
*Imen Trabelsi,Brahim Mahmoudi,Jean Baptiste Minani,Naouel Moha,Yann-GaÃ«l GuÃ©hÃ©neuc*

Main category: cs.SE

TL;DR: 81 studies reviewed: ML helps automate monolithic-to-microservices migration (e.g., service identification), but faces data scarcity and lacks tools. Packaging microservices is underexplored.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the gap in systematically investigating ML approaches for monolithic-to-microservices migration, where prior works separately analyzed objectives and tools but lacked synthesis of ML-specific techniques, inputs, and challenges.

Method: A systematic literature review (SLR) of 81 primary studies (2015â€“2024) using PRISMA methodology to analyze ML techniques applied to monolithic-to-microservices migration, focusing on automated phases, inputs, evaluation methods, and challenges.

Result: Findings show ML is extensively used for monitoring and service identification but absent in packaging microservices. Key challenges include limited data availability (29% of studies), scalability/complexity constraints, insufficient tooling (54% of studies), and absence of standardized benchmarks.

Conclusion: The review concludes that while ML aids monolithic-to-microservices migration by automating tasks like monitoring and service identification, critical phases like packaging remain unexplored. Addressing data scarcity, scalability challenges, and standardization gaps is vital for developing holistic migration solutions.

Abstract: Scalability and maintainability challenges in monolithic systems have led to
the adoption of microservices, which divide systems into smaller, independent
services. However, migrating existing monolithic systems to microservices is a
complex and resource-intensive task, which can benefit from machine learning
(ML) to automate some of its phases. Choosing the right ML approach for
migration remains challenging for practitioners. Previous works studied
separately the objectives, artifacts, techniques, tools, and benefits and
challenges of migrating monolithic systems to microservices. No work has yet
investigated systematically existing ML approaches for this migration to
understand the \revised{automated migration phases}, inputs used, ML techniques
applied, evaluation processes followed, and challenges encountered. We present
a systematic literature review (SLR) that aggregates, synthesises, and
discusses the approaches and results of 81 primary studies (PSs) published
between 2015 and 2024. We followed the Preferred Reporting Items for Systematic
Review and Meta-Analysis (PRISMA) statement to report our findings and answer
our research questions (RQs). We extract and analyse data from these PSs to
answer our RQs. We synthesise the findings in the form of a classification that
shows the usage of ML techniques in migrating monolithic systems to
microservices. The findings reveal that some phases of the migration process,
such as monitoring and service identification, are well-studied, while others,
like packaging microservices, remain unexplored. Additionally, the findings
highlight key challenges, including limited data availability, scalability and
complexity constraints, insufficient tool support, and the absence of
standardized benchmarking, emphasizing the need for more holistic solutions.

</details>


### [21] [Breaking Barriers in Software Testing: The Power of AI-Driven Automation](https://arxiv.org/abs/2508.16025)
*Saba Naqvi,Mohammad Baqar*

Main category: cs.SE

TL;DR: This paper introduces an AI-powered testing framework using NLP and RL to automate test generation, optimize validation, and address bias. Results show enhanced defect detection and efficiency in real-world case studies, positioning AI as a key enabler for proactive, scalable software quality assurance.


<details>
  <summary>Details</summary>
Motivation: Traditional software testing approaches are slow, costly, and prone to coverage gaps, necessitating a shift toward automated, adaptive solutions that improve efficiency and reliability without compromising fairness.

Method: The method involves an AI-driven framework combining NLP, reinforcement learning (RL), and predictive models. It translates natural language requirements into executable tests, optimizes them via continuous learning, validates outcomes with real-time analysis, and embeds policy-driven trust and fairness mechanisms.

Result: Case studies demonstrate significant improvements in defect detection rates, reduced testing effort, faster release cycles, and successful mitigation of bias in test outcomes.

Conclusion: The paper concludes that integrating AI techniques into software testing can transform it from a reactive process to a proactive system, enhancing software quality in complex environments through automation, adaptability, and bias mitigation.

Abstract: Software testing remains critical for ensuring reliability, yet traditional
approaches are slow, costly, and prone to gaps in coverage. This paper presents
an AI-driven framework that automates test case generation and validation using
natural language processing (NLP), reinforcement learning (RL), and predictive
models, embedded within a policy-driven trust and fairness model. The approach
translates natural language requirements into executable tests, continuously
optimizes them through learning, and validates outcomes with real-time analysis
while mitigating bias. Case studies demonstrate measurable gains in defect
detection, reduced testing effort, and faster release cycles, showing that
AI-enhanced testing improves both efficiency and reliability. By addressing
integration and scalability challenges, the framework illustrates how AI can
shift testing from a reactive, manual process to a proactive, adaptive system
that strengthens software quality in increasingly complex environments.

</details>


### [22] [Measuring the effectiveness of code review comments in GitHub repositories: A machine learning approach](https://arxiv.org/abs/2508.16053)
*Shadikur Rahman,Umme Ayman Koana,Hasibul Karim Shanto,Mahmuda Akter,Chitra Roy,Aras M. Ismael*

Main category: cs.SE

TL;DR: This paper evaluates seven ML algorithms for classifying sentiment polarity in GitHub code reviews using 13,557 labeled comments, finding Linear SVC as the most accurate method to improve developer error detection.


<details>
  <summary>Details</summary>
Motivation: The motivation addresses the need for programmers to accurately interpret code review comments to avoid errors, highlighting the importance of classifying sentiment polarity in developer collaboration and workflow efficiency.

Method: The method involves manually labeling 13,557 code review comments from three GitHub open-source projects and applying seven machine learning algorithms to classify sentiment polarity, with performance comparison to identify the most effective model.

Result: The results show that the Linear Support Vector Classifier (SVC) outperformed other algorithms in classifying code review sentiment, achieving the highest accuracy among the tested methods.

Conclusion: The study concludes that using the Linear SVC algorithm enhances the accuracy of sentiment classification in code reviews, aiding programmers in avoiding errors and misconceptions by better understanding review feedback.

Abstract: This paper illustrates an empirical study of the working efficiency of
machine learning techniques in classifying code review text by semantic
meaning. The code review comments from the source control repository in GitHub
were extracted for development activity from the existing year for three
open-source projects. Apart from that, programmers need to be aware of their
code and point out their errors. In that case, it is a must to classify the
sentiment polarity of the code review comments to avoid an error. We manually
labelled 13557 code review comments generated by three open source projects in
GitHub during the existing year. In order to recognize the sentiment polarity
(or sentiment orientation) of code reviews, we use seven machine learning
algorithms and compare those results to find the better ones. Among those
Linear Support Vector Classifier(SVC) classifier technique achieves higher
accuracy than others. This study will help programmers to make any solution
based on code reviews by avoiding misconceptions.

</details>


### [23] [From Benchmark Data To Applicable Program Repair: An Experience Report](https://arxiv.org/abs/2508.16071)
*Mahinthan Chandramohan,Jovan Jancic,Yuntong Zhang,Padmanabhan Krishnan*

Main category: cs.SE

TL;DR: This paper analyzes automated repair techniques that excel in benchmarks but fail for real-world defects. Formal specs enhance test generation for complex code but are ineffective for simple errors. Challenges include limited JML expressiveness and the need for richer tools. Ongoing work targets contract automata, programming by example, and human feedback to bridge academic-industry gaps.


<details>
  <summary>Details</summary>
Motivation: The motivation addresses the disconnect between academic benchmark success and real-world effectiveness in automated repair. Industry defects often evade existing techniques, and passing tests do not ensure correct patches. The paper seeks to improve practical reliability by leveraging formal specifications and exploring methods like contract automata to bridge this gap.

Method: The approach combines techniques from the literature, utilizing formal specifications to enhance LLM-generated unit tests. It focuses on improving coverage for edge cases and exceptions in complex code while acknowledging the limitations of this strategy for well-known error types. The methodology also emphasizes the need for advanced verification tools and richer predicates.

Result: Experiments show improved benchmark performance on standard tests, particularly in generating high-quality unit tests for complex code. However, the approach struggles with realistic industrial defects, and formal specifications provide limited value for common errors. Pass rates in benchmarks do not translate to real-world reliability, revealing challenges in specification expressiveness and tooling.

Conclusion: The paper highlights a significant gap between academic benchmarks and practical industry needs in automated program repair. While combining existing techniques and augmenting code with formal specifications improves test generation for complex production code, real-world adoption remains limited due to the insufficiency of current specification languages and the lack of guarantees for correct patches. Ongoing efforts focus on enhancing expressiveness, integrating human feedback, and addressing industry-specific challenges.

Abstract: This paper describes our approach to automated program repair. We combine
various techniques from the literature to achieve this. Our experiments show
that our approach performs better than other techniques on standard benchmarks.
However, on closer inspection, none of these techniques work on realistic
defects that we see in industry.
  We find that augmenting code with formal specifications enables LLMs to
generate higher-quality unit tests, especially for complex production code with
improved coverage of edge cases and exception handling. However, specifications
add little value for well-understood errors (e.g., null pointer, index out of
bounds), but are beneficial for logic and string manipulation errors. Despite
encouraging benchmark results, real-world adoption is limited since passing
tests do not guarantee correct patches. Current challenges include insufficient
expressiveness of the JML specification language, necessitating advanced
verification tools and richer predicates. Our ongoing work is exploring
contract automata, programming by example, and testcase repair, with a focus on
integrating human feedback and measuring productivity gains - highlighting the
gap between academic benchmarks and practical industry needs

</details>


### [24] [Validating Terrain Models in Digital Twins for Trustworthy sUAS Operations](https://arxiv.org/abs/2508.16104)
*Arturo Miguel Russell Bernal,Maureen Petterson,Pedro Antonio Alarcon Granadeno,Michael Murphy,James Mason,Jane Cleland-Huang*

Main category: cs.SE

TL;DR: This paper proposes a 3D validation framework for terrain models in Environmental Digital Twins, addressing real-world sUAS challenges via simulation-to-reality testing principles, ensuring safe and reliable autonomous operations in complex environments.


<details>
  <summary>Details</summary>
Motivation: The deployment of sUAS in complex environments requires accurate Environmental Digital Twins (EDT) for safe operations. However, real-world uncertainties like data granularity, sensor inaccuracies, and hardware limitations demand robust validation of EDT componentsâ€”specifically terrain modelsâ€”to ensure mission reliability.

Method: The proposed 3D validation process follows software engineering principles, progressing through test granularity levels, transitioning from simulation to real-world scenarios, and analyzing simple to edge conditions. It utilizes a multi-sUAS platform with a Terrain-Aware Digital Shadow for practical demonstration.

Result: The 3D validation framework successfully validated terrain models under various conditions, showcasing its ability to address uncertainties and support mission-critical tasks like geolocation and altitude maintenance in real-world sUAS operations.

Conclusion: The paper demonstrates that a structured 3D validation process, combining simulation and real-world testing, effectively addresses the challenges of terrain model validation for sUAS, enhancing the reliability of Environmental Digital Twins in complex environments.

Abstract: With the increasing deployment of small Unmanned Aircraft Systems (sUAS) in
unfamiliar and complex environments, Environmental Digital Twins (EDT) that
comprise weather, airspace, and terrain data are critical for safe flight
planning and for maintaining appropriate altitudes during search and
surveillance operations. With the expansion of sUAS capabilities through edge
and cloud computing, accurate EDT are also vital for advanced sUAS
capabilities, like geolocation. However, real-world sUAS deployment introduces
significant sources of uncertainty, necessitating a robust validation process
for EDT components. This paper focuses on the validation of terrain models, one
of the key components of an EDT, for real-world sUAS tasks. These models are
constructed by fusing U.S. Geological Survey (USGS) datasets and satellite
imagery, incorporating high-resolution environmental data to support mission
tasks. Validating both the terrain models and their operational use by sUAS
under real-world conditions presents significant challenges, including limited
data granularity, terrain discontinuities, GPS and sensor inaccuracies, visual
detection uncertainties, as well as onboard resources and timing constraints.
We propose a 3-Dimensions validation process grounded in software engineering
principles, following a workflow across granularity of tests, simulation to
real world, and the analysis of simple to edge conditions. We demonstrate our
approach using a multi-sUAS platform equipped with a Terrain-Aware Digital
Shadow.

</details>


### [25] [The Fools are Certain; the Wise are Doubtful: Exploring LLM Confidence in Code Completion](https://arxiv.org/abs/2508.16131)
*Zoe Kotti,Konstantina Dritsa,Diomidis Spinellis,Panos Louridas*

Main category: cs.SE

TL;DR: LLM code completion perplexity reveals language and model performance patterns: Java is easiest (low perplexity), Perl hardest. Model choice matters, but not datasets. Use perplexity to guide practical code generation decisions.


<details>
  <summary>Details</summary>
Motivation: Despite LLMs' potential for code completion, existing downstream metrics are impractical for evaluating model confidence. Intrinsic metrics like perplexity offer a universal, cost-effective proxy for functional correctness and hallucination risk.

Method: The authors measured code perplexity across 657 GitHub projects (1008 files), comparing programming languages, LLMs, and datasets. They analyzed perplexity variations influenced by language typing, model architecture, and code comments.

Result: Strongly-typed languages (e.g., Java) show lower perplexity than dynamically-typed/scripting languages (e.g., Perl). Perplexity depends on the LLM but not datasets. Code comments increase perplexity without altering language rankings.

Conclusion: The study demonstrates that intrinsic metrics like perplexity effectively assess model confidence in LLM-based code completion. Findings help stakeholders choose models and understand risks based on language, model selection, and code characteristics.

Abstract: Code completion entails the task of providing missing tokens given a
surrounding context. It can boost developer productivity while providing a
powerful code discovery tool. Following the Large Language Model (LLM) wave,
code completion has been approached with diverse LLMs fine-tuned on code (code
LLMs). The performance of code LLMs can be assessed with downstream and
intrinsic metrics. Downstream metrics are usually employed to evaluate the
practical utility of a model, but can be unreliable and require complex
calculations and domain-specific knowledge. In contrast, intrinsic metrics such
as perplexity, entropy, and mutual information, which measure model confidence
or uncertainty, are simple, versatile, and universal across LLMs and tasks, and
can serve as proxies for functional correctness and hallucination risk in
LLM-generated code. Motivated by this, we evaluate the confidence of LLMs when
generating code by measuring code perplexity across programming languages,
models, and datasets using various LLMs, and a sample of 1008 files from 657
GitHub projects. We find that strongly-typed languages exhibit lower perplexity
than dynamically typed languages. Scripting languages also demonstrate higher
perplexity. Perl appears universally high in perplexity, whereas Java appears
low. Code perplexity depends on the employed LLM, but not on the code dataset.
Although code comments often increase perplexity, the language ranking based on
perplexity is barely affected by their presence. LLM researchers, developers,
and users can employ our findings to assess the benefits and suitability of
LLM-based code completion in specific software projects based on how language,
model choice, and code characteristics impact model confidence.

</details>


### [26] [Towards Recommending Usability Improvements with Multimodal Large Language Models](https://arxiv.org/abs/2508.16165)
*Sebastian Lubos,Alexander Felfernig,Gerhard Leitner,Julian Schwazer*

Main category: cs.SE

TL;DR: This research proposes using multimodal LLMs for automated usability assessment. Through a proof-of-concept study, they show LLM-generated recommendations match expert evaluations in identifying issues, offering faster and cheaper alternatives for organizations lacking expert resources.


<details>
  <summary>Details</summary>
Motivation: Traditional usability evaluation methods (testing and inspection) are resource-intensive and require expert involvement, making them inaccessible to smaller organizations. Multimodal LLMs present an opportunity to automate these processes.

Method: The authors formulated usability evaluation as a task for ranking usability issues by severity using multimodal LLMs. They conducted a proof-of-concept study comparing LLM-generated recommendations with usability expert assessments.

Result: The findings demonstrate that LLMs can generate usability improvement recommendations comparable to expert assessments, indicating potential for automating usability evaluation with reduced time and cost.

Conclusion: The study concludes that multimodal LLMs can effectively automate usability evaluation processes, offering faster and more cost-effective solutions compared to traditional expert-driven methods, particularly beneficial for organizations with limited resources.

Abstract: Usability describes a set of essential quality attributes of user interfaces
(UI) that influence human-computer interaction. Common evaluation methods, such
as usability testing and inspection, are effective but resource-intensive and
require expert involvement. This makes them less accessible for smaller
organizations. Recent advances in multimodal LLMs offer promising opportunities
to automate usability evaluation processes partly by analyzing textual, visual,
and structural aspects of software interfaces. To investigate this possibility,
we formulate usability evaluation as a recommendation task, where multimodal
LLMs rank usability issues by severity. We conducted an initial
proof-of-concept study to compare LLM-generated usability improvement
recommendations with usability expert assessments. Our findings indicate the
potential of LLMs to enable faster and more cost-effective usability
evaluation, which makes it a practical alternative in contexts with limited
expert resources.

</details>


### [27] [LLM-Assisted Semantic Alignment and Integration in Collaborative Model-Based Systems Engineering Using SysML v2](https://arxiv.org/abs/2508.16181)
*Zirui Li,Stephan Husung,Haoze Wang*

Main category: cs.SE

TL;DR: This paper introduces an LLM-assisted method for aligning SysML v2 models in collaborative MBSE, using structured prompts and system extensions to improve interoperability.


<details>
  <summary>Details</summary>
Motivation: Cross-organizational MBSE faces challenges in semantic alignment between independently developed models. SysML v2â€™s formal semantics and modularity provide an opportunity, while LLMs enable new automation for model understanding.

Method: The authors develop a prompt-driven framework using GPT-based LLMs, leveraging SysML v2 features like aliases, imports, and metadata extensions. The method iteratively implements model extraction, semantic matching, and verification for traceable alignment.

Result: The approach is validated through a measurement system example using a GPT-based LLM, demonstrating effective semantic matching and alignment capabilities. Limitations include reliance on LLM accuracy and computational resources.

Conclusion: The paper concludes that the proposed LLM-assisted approach enhances semantic alignment in SysML v2 models, offering a traceable framework for cross-organizational MBSE collaboration. Future work includes refining the method for broader applicability.

Abstract: Cross-organizational collaboration in Model-Based Systems Engineering (MBSE)
faces many challenges in achieving semantic alignment across independently
developed system models. SysML v2 introduces enhanced structural modularity and
formal semantics, offering a stronger foundation for interoperable modeling.
Meanwhile, GPT-based Large Language Models (LLMs) provide new capabilities for
assisting model understanding and integration. This paper proposes a
structured, prompt-driven approach for LLM-assisted semantic alignment of SysML
v2 models. The core contribution lies in the iterative development of an
alignment approach and interaction prompts, incorporating model extraction,
semantic matching, and verification. The approach leverages SysML v2 constructs
such as alias, import, and metadata extensions to support traceable, soft
alignment integration. It is demonstrated with a GPT-based LLM through an
example of a measurement system. Benefits and limitations are discussed.

</details>


### [28] [A Systematic Mapping Study on Smart Cities Modeling Approaches](https://arxiv.org/abs/2508.16273)
*Maria Teresa Rossi,Martina De Sanctis,Ludovico Iovino,Manuel Wimmer*

Main category: cs.SE

TL;DR: This systematic mapping study analyzes smart cities modeling approaches, revealing that governance is the most studied area while modeling technologies lack practical validation. Researchers can use these insights to navigate state-of-the-art methods and identify gaps for future work.


<details>
  <summary>Details</summary>
Motivation: Smart cities are inherently interdisciplinary, and understanding their design and modeling is critical for achieving research goals. The study addresses the need to consolidate existing approaches, identify trends, and guide future research directions.

Method: The authors conducted a systematic mapping study following Petersen et al.'s guidelines, analyzing existing publications to identify trends and gaps in smart cities modeling research.

Result: Key findings include: (1) smart governance is the most modeled dimension; (2) business, architectural, and ontological modeling approaches dominate; (3) most technologies remain unproven at scale; (4) diverse publication venues reflect fragmented research communities.

Conclusion: The study provides a comprehensive overview of smart cities modeling approaches, highlighting the need for further research, especially in validating technologies in real-world environments. It serves as a foundation for future studies and emphasizes the relevance for the Model-Driven Engineering community.

Abstract: The Smart City concept was introduced to define an idealized city
characterized by automation and connection. It then evolved rapidly by
including further aspects, such as economy, environment. Since then, many
publications have explored various aspects of Smart Cities across different
application domains and research communities, acknowledging the
interdisciplinary nature of this subject. In particular, our interest focuses
on how smart cities are designed and modeled, as a whole or as regards with
their subsystems, when dealing with the accomplishment of the research goals in
this complex and heterogeneous domain. To this aim, we performed a systematic
mapping study on smart cities modeling approaches identifying the relevant
contributions (i) to get an overview of existing research approaches, (ii) to
identify whether there are any publication trends, and (iii) to identify
possible future research directions. We followed the guidelines for conducting
systematic mapping studies by Petersen et al. to analyze smart cities modeling
publications. Our analysis revealed the following main findings: (i) smart
governance is the most investigated and modeled smart city dimension; (ii) the
most used modeling approaches are business, architectural, and ontological
modeling approaches, spanning multiple application fields; (iii) the great
majority of existing technologies for modeling smart cities are not yet proven
in operational environments; (iv) diverse research communities publish their
results in a multitude of different venues which further motivates the
presented literature study. Researchers can use our results for better
understanding the state-of-the-art in modeling smart cities, and as a
foundation for further analysis of specific approaches about smart cities
modeling. Lastly, we also discuss the impact of our analysis for the
Model-Driven Engineering community.

</details>


### [29] [Metamorphic Coverage](https://arxiv.org/abs/2508.16307)
*Jinsheng Ba,Yuancheng Jiang,Manuel Rigger*

Main category: cs.SE

TL;DR: This paper proposes Metamorphic Coverage (MC), a superior metric for metamorphic testing, showing it outperforms code coverage and mutation testing in bug detection efficiency and effectiveness.


<details>
  <summary>Details</summary>
Motivation: The study addresses the limitations of existing metrics like code coverage (inaccuracy) and mutation testing (computational expense) in the context of metamorphic testing.

Method: The paper defines MC as a coverage metric that analyzes the distinct code executed by pairs of test inputs in metamorphic testing and systematically evaluates it across five methods for database engines, compilers, and solvers.

Result: MC demonstrates higher sensitivity to testing effectiveness, a stronger correlation with bug counts, 359x faster computation than mutation testing, and 41% more bug detection in feedback-guided testing.

Conclusion: Metamorphic Coverage (MC) offers a more effective and efficient metric for evaluating metamorphic testing methods, with broader applications in test-case generation and bug detection.

Abstract: Metamorphic testing is a widely used methodology that examines an expected
relation between pairs of executions to automatically find bugs, such as
correctness bugs. We found that code coverage cannot accurately measure the
extent to which code is validated and mutation testing is computationally
expensive for evaluating metamorphic testing methods. In this work, we propose
Metamorphic Coverage (MC), a coverage metric that examines the distinct code
executed by pairs of test inputs within metamorphic testing. Our intuition is
that, typically, a bug can be observed if the corresponding code is executed
when executing either test input but not the other one, so covering more
differential code covered by pairs of test inputs might be more likely to
expose bugs. While most metamorphic testing methods have been based on this
general intuition, our work defines and systematically evaluates MC on five
widely used metamorphic testing methods for testing database engines,
compilers, and constraint solvers. The code measured by MC overlaps with the
bug-fix locations of 50 of 64 bugs found by metamorphic testing methods, and MC
has a stronger positive correlation with bug numbers than line coverage. MC is
4x more sensitive than line coverage in distinguishing testing methods'
effectiveness, and the average value of MC is 6x smaller than line coverage
while still capturing the part of the program that is being tested. MC required
359x less time than mutation testing. Based on a case study for an automated
database system testing approach, we demonstrate that when used for feedback
guidance, MC significantly outperforms code coverage, by finding 41\% more
bugs. Consequently, this work might have broad applications for assessing
metamorphic testing methods and improving test-case generation.

</details>


### [30] [SATORI: Static Test Oracle Generation for REST APIs](https://arxiv.org/abs/2508.16318)
*Juan C. Alonso,Alberto Martin-Lopez,Sergio Segura,Gabriele Bavota,Antonio Ruiz-CortÃ©s*

Main category: cs.SE

TL;DR: SATORI generates robust REST API test oracles using OpenAPI specs and LLMs. Outperforms dynamic approaches and finds practical bugs without API execution.


<details>
  <summary>Details</summary>
Motivation: Existing REST API testing tools are limited to crash/regression oracles and specification compliance checks, lacking expressive test oracles capturing expected API behavior.

Method: SATORI employs large language models to analyze OpenAPI Specification response field properties (names and descriptions) for static test oracle inference. Integrates with PostmanAssertify for execution.

Result: Achieved 74.3% F1-score (vs. 69.3% for AGORA+), generated hundreds of oracles per operation. 18 bugs identified in real-world APIs leading to documentation fixes.

Conclusion: Static and dynamic oracle inference methods (SATORI and AGORA+) are complementary, achieving 90% coverage of annotated oracles. SATORI demonstrates practical value by uncovering 18 bugs in widely-used APIs.

Abstract: REST API test case generation tools are evolving rapidly, with growing
capabilities for the automated generation of complex tests. However, despite
their strengths in test data generation, these tools are constrained by the
types of test oracles they support, often limited to crashes, regressions, and
noncompliance with API specifications or design standards. This paper
introduces SATORI (Static API Test ORacle Inference), a black-box approach for
generating test oracles for REST APIs by analyzing their OpenAPI Specification.
SATORI uses large language models to infer the expected behavior of an API by
analyzing the properties of the response fields of its operations, such as
their name and descriptions. To foster its adoption, we extended the
PostmanAssertify tool to automatically convert the test oracles reported by
SATORI into executable assertions. Evaluation results on 17 operations from 12
industrial APIs show that SATORI can automatically generate up to hundreds of
valid test oracles per operation. SATORI achieved an F1-score of 74.3%,
outperforming the state-of-the-art dynamic approach AGORA+ (69.3%)-which
requires executing the API-when generating comparable oracle types. Moreover,
our findings show that static and dynamic oracle inference methods are
complementary: together, SATORI and AGORA+ found 90% of the oracles in our
annotated ground-truth dataset. Notably, SATORI uncovered 18 bugs in popular
APIs (Amadeus Hotel, Deutschebahn, FDIC, GitLab, Marvel, OMDb and Vimeo)
leading to documentation updates by the API maintainers.

</details>


### [31] [The (C)omprehensive (A)rchitecture (P)attern (I)ntegration method: Navigating the sea of technology](https://arxiv.org/abs/2508.16341)
*Sebastian Copei,Oliver Hohlfeld,Jens Kosiol*

Main category: cs.SE

TL;DR: CAPI uses pattern-based decision trees to reduce architectural complexity, validated by academic and industry evaluations showing improved decision-making efficiency.


<details>
  <summary>Details</summary>
Motivation: Rapid technological changes and overwhelming tool choices create complexity in software architecture decisions for large systems, motivating the need for structured decision support frameworks.

Method: The authors developed CAPI, a diagnostic decision tree-driven method that suggests architectural patterns based on user needs, and evaluated it through iterative academic/industry experiments.

Result: User studies showed CAPI is perceived as helpful, replicates productive environments, and reveals industry reliance on trial-and-error for technology selection.

Conclusion: CAPI effectively simplifies architectural decision-making by focusing on patterns rather than tools, reducing complexity and supporting productive environments confirmed by industry participants.

Abstract: The technological landscape changes daily, making it nearly impossible for a
single person to be aware of all trends or available tools that may or may not
be suitable for their software project. This makes tool selection and
architectural design decisions a complex problem, especially for large-scale
software systems. To tackle this issue, we introduce CAPI, the Comprehensive
Architecture Pattern Integration method that uses a diagnostic decision tree to
suggest architectural patterns depending on user needs. By suggesting patterns
instead of tools, the overall complexity for further decisions is lower as
there are fewer architectural patterns than tools due to the abstract nature of
patterns. Moreover, since tools implement patterns, each non-proposed pattern
reduces the number of tools to choose from, reducing complexity. We iteratively
developed CAPI, evaluating its understandability and usability in small studies
with academic participants. When satisfied with the outcome, we performed a
user-study with industry representatives to investigate the state-of-the-art in
technology selection and the effectiveness of our proposed method. We find that
technology selection is largely performed via trial and error, that CAPI is
uniformly perceived as helpful, and that CAPI is able to reproduce the
productive architectural environments of our participants.

</details>


### [32] [AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions](https://arxiv.org/abs/2508.16402)
*Zihan Wang,Jiaze Chen,Zhicheng Liu,Markus Mak,Yidi Du,Geonsik Moon,Luoqi Xu,Aaron Tua,Kunshuo Peng,Jiayi Lu,Mingfei Xia,Boqian Zou,Chenyang Ran,Guang Tian,Shoutai Zhu,Yeheng Duan,Zhenghui Kang,Zhenxing Lin,Shangshu Li,Qiang Luo,Qingshen Long,Zhiyong Chen,Yihan Xiao,Yurong Wu,Daoguang Zan,Yuyi Fu,Mingxuan Wang,Ming Ding*

Main category: cs.SE

TL;DR: This paper introduces AetherCode, a new competitive programming benchmark for LLMs combining high-difficulty competition problems with expert-validated test cases, addressing flaws in existing evaluations to deliver more accurate code reasoning assessments.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for LLM code evaluation inadequately reflect elite human performance due to (1) insufficient problem difficulty/scope and (2) evaluation bias from low-quality test cases, overstating model capabilities.

Method: The authors created AetherCode by curating high-difficulty programming problems from premier competitions (IOI, ICPC) and developing comprehensive test suites through automated generation combined with expert validation, ensuring both problem quality and evaluation reliability.

Result: AetherCode offers broader problem coverage and higher difficulty than existing benchmarks, with robust evaluation via hybrid-automated test suites that reduce bias and provide more reliable LLM performance measurement.

Conclusion: AetherCode sets a new standard for evaluating LLM code reasoning by addressing gaps in existing benchmarks with challenging problems and rigorous assessments, providing a more accurate measure of model proficiency compared to elite human programmers.

Abstract: Competitive programming has emerged as a critical benchmark for evaluating
the reasoning and coding capabilities of Large Language Models (LLMs). Despite
impressive progress on existing benchmarks, we argue that current evaluations
overstate model proficiency, masking a substantial gap between LLMs and elite
human programmers. This gap arises from two key limitations: insufficient
difficulty and scope of benchmark problems, and evaluation bias from
low-quality test cases. To address these shortcomings, we present AetherCode, a
new benchmark that draws problems from premier programming competitions such as
IOI and ICPC, offering broader coverage and higher difficulty. AetherCode
further incorporates comprehensive, expert-validated test suites built through
a hybrid of automated generation and human curation, ensuring rigorous and
reliable assessment. By combining challenging problem design with robust
evaluation, AetherCode provides a more faithful measure of LLM capabilities and
sets a new standard for future research in code reasoning.

</details>


### [33] [LLM-GUARD: Large Language Model-Based Detection and Repair of Bugs and Security Vulnerabilities in C++ and Python](https://arxiv.org/abs/2508.16419)
*Akshay Mhatre,Noujoud Nader,Patrick Diehl,Deepti Gupta*

Main category: cs.SE

TL;DR: LLMs like ChatGPT-4 and Claude 3 perform well in basic code analysis but underperform in complex security bug detection, showing promise for education but limitations for real-world auditing.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the gap in understanding how effectively LLMs (ChatGPT-4, Claude 3, LLaMA 4) detect diverse software bugs, particularly complex security-relevant vulnerabilities, which are critical for ensuring robust software development.

Method: The research employed a systematic empirical evaluation using a benchmark of foundational programming errors, security flaws, and production-grade bugs in C++ and Python. Datasets were validated through compilation and testing pipelines, with a multi-stage, context-aware prompting protocol and a graded rubric for measuring detection accuracy, reasoning depth, and remediation quality.

Result: LLMs excel at identifying syntactic/semantic issues in well-scoped code but struggle with complex security vulnerabilities and large-scale production code. ChatGPT-4 and Claude 3 demonstrate more nuanced contextual analysis compared to LLaMA 4, suggesting their potential as educational tools and first-pass code reviewers but highlighting constraints in real-world reliability.

Conclusion: The study highlights that while LLMs like ChatGPT-4 and Claude 3 show promise for code analysis, their limitations in handling complex security vulnerabilities and production-grade bugs underscore the need for further refining their reliability in real-world software development.

Abstract: Large Language Models (LLMs) such as ChatGPT-4, Claude 3, and LLaMA 4 are
increasingly embedded in software/application development, supporting tasks
from code generation to debugging. Yet, their real-world effectiveness in
detecting diverse software bugs, particularly complex, security-relevant
vulnerabilities, remains underexplored. This study presents a systematic,
empirical evaluation of these three leading LLMs using a benchmark of
foundational programming errors, classic security flaws, and advanced,
production-grade bugs in C++ and Python. The dataset integrates real code from
SEED Labs, OpenSSL (via the Suresoft GLaDOS database), and PyBugHive, validated
through local compilation and testing pipelines. A novel multi-stage,
context-aware prompting protocol simulates realistic debugging scenarios, while
a graded rubric measures detection accuracy, reasoning depth, and remediation
quality. Our results show that all models excel at identifying syntactic and
semantic issues in well-scoped code, making them promising for educational use
and as first-pass reviewers in automated code auditing. Performance diminishes
in scenarios involving complex security vulnerabilities and large-scale
production code, with ChatGPT-4 and Claude 3 generally providing more nuanced
contextual analyses than LLaMA 4. This highlights both the promise and the
present constraints of LLMs in serving as reliable code analysis tools.

</details>


### [34] [Using LLMs and Essence to Support Software Practice Adoption](https://arxiv.org/abs/2508.16445)
*Sonia Nicoletti,Paolo Ciancarini*

Main category: cs.SE

TL;DR: This study enhances software engineering process management by developing a RAG-powered chatbot integrating Essence with LLMs, which outperforms standard models in applying domain-specific knowledge to improve learning and decision-making.


<details>
  <summary>Details</summary>
Motivation: NLP/AI research has prioritized code generation over automating best practice adoption and process health monitoring in software engineering, creating a need for tools that operationalize theoretical frameworks like Essence.

Method: Developed a specialized RAG-augmented chatbot integrating Essence with LLMs, testing four configurations across base models and RAG-enhanced variants, evaluated through contextual retrieval relevance and response quality metrics.

Result: The RAG-enhanced system consistently outperformed baseline LLMs and general-purpose models in domain-specific tasks, demonstrating improved effectiveness in retrieving relevant software engineering knowledge and generating accurate responses.

Conclusion: This work bridges the gap between theoretical frameworks and practical application in software engineering by demonstrating that an LLM-based chatbot with RAG outperforms baselines in domain-specific tasks, enhancing learning and decision-making processes. Further user validation is needed but highlights the potential for improving process management.

Abstract: Recent advancements in natural language processing (NLP) have enabled the
development of automated tools that support various domains, including software
engineering. However, while NLP and artificial intelligence (AI) research has
extensively focused on tasks such as code generation, less attention has been
given to automating support for the adoption of best practices, the evolution
of ways of working, and the monitoring of process health. This study addresses
this gap by exploring the integration of Essence, a standard and thinking
framework for managing software engineering practices, with large language
models (LLMs). To this end, a specialised chatbot was developed to assist
students and professionals in understanding and applying Essence. The chatbot
employs a retrieval-augmented generation (RAG) system to retrieve relevant
contextual information from a curated knowledge base. Four different LLMs were
used to create multiple chatbot configurations, each evaluated both as a base
model and augmented with the RAG system. The system performance was evaluated
through both the relevance of retrieved context and the quality of generated
responses. Comparative analysis against the general-purpose LLMs demonstrated
that the proposed system consistently outperforms its baseline counterpart in
domain-specific tasks. By facilitating access to structured software
engineering knowledge, this work contributes to bridging the gap between
theoretical frameworks and practical application, potentially improving process
management and the adoption of software development practices. While further
validation through user studies is required, these findings highlight the
potential of LLM-based automation to enhance learning and decision-making in
software engineering.

</details>


### [35] [How Small is Enough? Empirical Evidence of Quantized Small Language Models for Automated Program Repair](https://arxiv.org/abs/2508.16499)
*Kazuki Kusama,Honglin Shu,Masanari Kondo,Yasutaka Kamei*

Main category: cs.SE

TL;DR: This paper shows small language models (SLMs) achieve comparable or superior bug-fixing accuracy to large models at lower costs, with quantization further improving efficiency without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: LLMs have high computational resource requirements, creating a need for alternatives like SLMs that perform well with limited resources in APR tasks.

Method: Conducted experiments on the QuixBugs benchmark to compare bug-fixing accuracy of SLMs and LLMs, and analyzed the impact of int8 quantization on APR performance.

Result: Latest SLMs fix bugs as accurately as or more than LLMs. Int8 quantization minimally affects APR accuracy while significantly reducing memory requirements.

Conclusion: SLMs present a viable alternative to LLMs for APR, offering competitive accuracy with lower computational costs, and quantization can further enhance their efficiency without compromising effectiveness.

Abstract: Background: Large language models (LLMs) have greatly improved the accuracy
of automated program repair (APR) methods. However, LLMs are constrained by
high computational resource requirements. Aims: We focus on small language
models (SLMs), which perform well even with limited computational resources
compared to LLMs. We aim to evaluate whether SLMs can achieve competitive
performance in APR tasks. Method: We conducted experiments on the QuixBugs
benchmark to compare the bug-fixing accuracy of SLMs and LLMs. We also analyzed
the impact of int8 quantization on APR performance. Results: The latest SLMs
can fix bugs as accurately as--or even more accurately than--LLMs. Also, int8
quantization had minimal effect on APR accuracy while significantly reducing
memory requirements. Conclusions: SLMs present a viable alternative to LLMs for
APR, offering competitive accuracy with lower computational costs, and
quantization can further enhance their efficiency without compromising
effectiveness.

</details>


### [36] [ARSP: Automated Repair of Verilog Designs via Semantic Partitioning](https://arxiv.org/abs/2508.16517)
*Bingkun Yao,Ning Wang,Xiangfeng Liu,Yuxin Du,Yuchen Hu,Hong Gao,Zhe Jiang,Nan Guan*

Main category: cs.SE

TL;DR: ARSP mitigates bug signal dilution in Verilog debugging via a two-stage system that partitions modules into semantic fragments. Synthetic data trains specialized LLMs to repair fragments effectively. This approach outperforms existing tools by 11.6% in accuracy, enabling scalable LLM-based debugging for large industrial modules.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based debugging methods underperform on industrial-scale Verilog modules due to bug signal dilution in long contexts, where model attention is overwhelmed by unrelated lines of code. This limits scalability and debugging efficiency for complex designs.

Method: ARSP is a two-stage system that first partitions a Verilog module into semantically coherent fragments using a Partition LLM, then patches each fragment with a Repair LLM. Edits are merged to avoid altering unrelated logic. A synthetic data framework generates diverse training pairs to supervise both models across bug types and design styles.

Result: ARSP achieves 77.92% pass@1 and 83.88% pass@5 in debugging tasks, outperforming commercial LLMs and state-of-the-art tools like Strider and MEIC. Semantic partitioning yields a 11.6% improvement in pass@1 and 10.2% in pass@5 compared to whole-module debugging, confirming the effectiveness of fragment-level scope reduction.

Conclusion: The paper concludes that the proposed ARSP system effectively addresses the issue of bug signal dilution in Verilog debugging for industrial-scale modules. By reducing the context scope through semantic fragmentation, ARSP demonstrates significant improvements in debugging accuracy, validating the importance of fragment-level approach for scalable LLM-based debugging.

Abstract: Debugging functional Verilog bugs consumes a significant portion of front-end
design time. While Large Language Models (LLMs) have demonstrated great
potential in mitigating this effort, existing LLM-based automated debugging
methods underperform on industrial-scale modules. A major reason for this is
bug signal dilution in long contexts, where a few bug-relevant tokens are
overwhelmed by hundreds of unrelated lines, diffusing the model's attention. To
address this issue, we introduce ARSP, a two-stage system that mitigates
dilution via semantics-guided fragmentation. A Partition LLM splits a module
into semantically tight fragments; a Repair LLM patches each fragment; edits
are merged without altering unrelated logic. A synthetic data framework
generates fragment-level training pairs spanning bug types, design styles, and
scales to supervise both models. Experiments show that ARSP achieves 77.92%
pass@1 and 83.88% pass@5, outperforming mainstream commercial LLMs including
Claude-3.7 and SOTA automated Verilog debugging tools Strider and MEIC. Also,
semantic partitioning improves pass@1 by 11.6% and pass@5 by 10.2% over
whole-module debugging, validating the effectiveness of fragment-level scope
reduction in LLM-based Verilog debugging.

</details>
