<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 11]
- [cs.SE](#cs.SE) [Total: 7]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Cross-Service Token: Finding Attacks in 5G Core Networks](https://arxiv.org/abs/2509.08992)
*Anqi Chen,Riccardo Preatoni,Alessandro Brighente,Mauro Conti,Cristina Nita-Rotaru*

Main category: cs.CR

TL;DR: FivGeeFuzz, a fuzzing framework for 5G core interfaces, discovers 8 critical security vulnerabilities in free5GC, highlighting risks in service-based architectures and validating the framework's effectiveness.


<details>
  <summary>Details</summary>
Motivation: The transition to 5G's Service-Based Architecture (SBA) introduces new security challenges due to modularized Network Functions (NFs) deployed in vulnerable cloud environments, necessitating tools to detect insider threats and unauthorized access risks.

Method: The authors developed FivGeeFuzz, a grammar-based fuzzing framework that automatically derives grammars from 3GPP API specifications to generate malformed inputs, integrates automated bug detection, and supports manual validation and root-cause analysis for 5G core SBIs.

Result: FivGeeFuzz identified 8 novel vulnerabilities in free5GC, including a severe Cross-Service Token Attack, with 7 patched and 1 under resolution. These flaws caused crashes, improper error handling, and unauthorized resource access.

Conclusion: This paper demonstrates the effectiveness of FivGeeFuzz in uncovering critical security vulnerabilities in 5G core Service-Based Interfaces, emphasizing the necessity of rigorous security testing for cloud-native, service-oriented architectures.

Abstract: 5G marks a major departure from previous cellular architectures, by
transitioning from a monolithic design of the core network to a Service-Based
Architecture (SBA) where services are modularized as Network Functions (NFs)
which communicate with each other via standard-defined HTTP-based APIs called
Service-Based Interfaces (SBIs). These NFs are deployed in private and public
cloud infrastructure, and an access control framework based on OAuth restricts
how they communicate with each other and obtain access to resources. Given the
increased vulnerabilities of clouds to insiders, it is important to study the
security of the 5G Core services for vulnerabilities that allow attackers to
use compromised NFs to obtain unauthorized access to resources.
  We present FivGeeFuzz, a grammar-based fuzzing framework designed to uncover
security flaws in 5G core SBIs. FivGeeFuzz automatically derives grammars from
3GPP API specifications to generate malformed, unexpected, or semantically
inconsistent inputs, and it integrates automated bug detection with manual
validation and root-cause analysis. We evaluate our approach on free5GC, the
only open-source 5G core implementing Release 17-compliant SBIs with an access
control mechanism. Using FivGeeFuzz, we discovered 8 previously unknown
vulnerabilities in free5GC, leading to runtime crashes, improper error
handling, and unauthorized access to resources, including a very severe attack
we call Cross-Service Token Attack. All bugs were confirmed by the free5GC
team, 7 have already been patched, and the remaining one has a patch under
development.

</details>


### [2] [When FinTech Meets Privacy: Securing Financial LLMs with Differential Private Fine-Tuning](https://arxiv.org/abs/2509.08995)
*Sichen Zhu,Hoyeung Leung,Xiaoyi Wang,Jia Wei,Honghui Xu*

Main category: cs.CR

TL;DR: DPFinLLM is a privacy-enhanced, lightweight Large Language Model (LLM) designed for on-device financial applications, combining differential privacy with a streamlined architecture to balance data security and performance on complex tasks.


<details>
  <summary>Details</summary>
Motivation: The growing adoption of AI in FinTech has raised significant privacy concerns, particularly when deploying models on edge devices, where sensitive financial data is processed locally and exposed to potential breaches.

Method: The paper introduces DPFinLLM, a model that integrates a robust differential privacy (DP) mechanism with a compact, efficient architecture inspired by state-of-the-art LLMs. This approach allows the model to securely process financial data on edge devices while maintaining performance.

Result: Extensive experiments on financial sentiment datasets demonstrate that DPFinLLM achieves performance comparable to fully fine-tuned models under strict privacy constraints.

Conclusion: DPFinLLM offers a viable solution for privacy-preserving, high-performance LLM deployment in financial technology, particularly for edge devices, by effectively balancing security and efficiency through its design.

Abstract: The integration of Large Language Models (LLMs) into financial technology
(FinTech) has revolutionized the analysis and processing of complex financial
data, driving advancements in real-time decision-making and analytics. With the
growing trend of deploying AI models on edge devices for financial
applications, ensuring the privacy of sensitive financial data has become a
significant challenge. To address this, we propose DPFinLLM, a
privacy-enhanced, lightweight LLM specifically designed for on-device financial
applications. DPFinLLM combines a robust differential privacy mechanism with a
streamlined architecture inspired by state-of-the-art models, enabling secure
and efficient processing of financial data. This proposed DPFinLLM can not only
safeguard user data from privacy breaches but also ensure high performance
across diverse financial tasks. Extensive experiments on multiple financial
sentiment datasets validate the effectiveness of DPFinLLM, demonstrating its
ability to achieve performance comparable to fully fine-tuned models, even
under strict privacy constraints.

</details>


### [3] [Beyond Tag Collision: Cluster-based Memory Management for Tag-based Sanitizers](https://arxiv.org/abs/2509.09089)
*Mengfei Xie,Yan Lin,Hongtao Wu,Jianming Fu,Chenke Luo,Guojun Peng*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Tag-based sanitizers attach a small "key" to each pointer and a matching
"lock" tag to its target memory object, enabling runtime verification of
pointer-object consistency and helping developers to detect potential memory
violations. However, the limited tag encoding space challenges existing studies
in assigning distinct tags to memory objects across temporal and spatial
dimensions, leading to potential tag collisions. In this paper, we present
ClusterTag, a novel cluster-based memory allocator aimed at simultaneously
mitigating tag collisions in both temporal and spatial dimensions. The core
design of ClusterTag effectively balances the significant mismatch between tag
encoding space and memory objects: it divides memory objects into multiple
independent clusters, thereby limiting tag collisions to finite chunks within
each cluster. To mitigate tag collisions across clusters, we design a
cluster-grained heap randomization scheme. This approach introduces random
address intervals between clusters and further breaks the entropy limitation of
the tag space. ClusterTag has been implemented as an independent memory
allocator that seamlessly integrates with tag-based sanitizers such as HWASan,
and maintains comparable performance overhead (within 1%) at various
randomization densities. Security evaluations on the Juliet dataset indicate
that ClusterTag exhibits deterministic results across 500 repeated tests (5,652
reported and 1,530 missed), while the existing three types of tag assignment
strategies all exhibit probabilistic false negatives due to tag collisions.
Quantitative analysis across three tag collision distance metrics-minimum,
average, and unpredictability-demonstrates that ClusterTag achieves balanced
improvements across all three, whereas prior tag assignment schemes (random,
staggered, fixed) show significant trade-offs in at least one metric.

</details>


### [4] [Towards Confidential and Efficient LLM Inference with Dual Privacy Protection](https://arxiv.org/abs/2509.09091)
*Honglan Yu,Yibin Wang,Feifei Dai,Dong Liu,Haihui Fan,Xiaoyan Gu*

Main category: cs.CR

TL;DR: CMIF addresses private LLM inference by partitioning model layers across TEEs and GPUs, with optimized privacy mechanisms to reduce overhead while preserving performance and confidentiality.


<details>
  <summary>Details</summary>
Motivation: CPU-based TEEs suffer from high latency for nonlinear LLM layers, and DP-based methods degrade performance. Existing partitioning approaches cannot efficiently handle the communication overhead of large models.

Method: CMIF confidentially deploys the embedding layer in a client-side TEE and offloads subsequent layers to GPU servers. It also optimizes the Report-Noisy-Max mechanism to enhance privacy with minimal performance loss.

Result: Experiments on Llama-series models show CMIF reduces TEE inference overhead while maintaining data privacy, achieving both efficiency and security.

Conclusion: CMIF offers an efficient and confidential solution for private LLM inference, balancing performance and security effectively.

Abstract: CPU-based trusted execution environments (TEEs) and differential privacy (DP)
have gained wide applications for private inference. Due to high inference
latency in TEEs, researchers use partition-based approaches that offload linear
model components to GPUs. However, dense nonlinear layers of large language
models (LLMs) result in significant communication overhead between TEEs and
GPUs. DP-based approaches apply random noise to protect data privacy, but this
compromises LLM performance and semantic understanding. To overcome the above
drawbacks, this paper proposes CMIF, a Confidential and efficient Model
Inference Framework. CMIF confidentially deploys the embedding layer in the
client-side TEE and subsequent layers on GPU servers. Meanwhile, it optimizes
the Report-Noisy-Max mechanism to protect sensitive inputs with a slight
decrease in model performance. Extensive experiments on Llama-series models
demonstrate that CMIF reduces additional inference overhead in TEEs while
preserving user data privacy.

</details>


### [5] [DP-FedLoRA: Privacy-Enhanced Federated Fine-Tuning for On-Device Large Language Models](https://arxiv.org/abs/2509.09097)
*Honghui Xu,Shiva Shrestha,Wei Chen,Zhiyuan Li,Zhipeng Cai*

Main category: cs.CR

TL;DR: DP-FedLoRA addresses privacy risks in federated LLM fine-tuning by combining LoRA adapters with differential privacy, achieving robust performance and privacy through Gaussian noise and unbiased update analysis.


<details>
  <summary>Details</summary>
Motivation: Federated fine-tuning of on-device LLMs poses significant privacy risks due to sensitive user data processing within federated learning frameworks, necessitating privacy-enhanced solutions.

Method: Integrates LoRA-based adaptation with differential privacy, using Gaussian noise to perturb local LoRA matrices, along with theoretical analysis of update bias and variance bounds for privacy-budget calibration.

Result: Delivers competitive performance across mainstream benchmarks while satisfying ($\epsilon$, $\delta$)-differential privacy, demonstrating strong privacy guarantees without compromising scalability.

Conclusion: DP-FedLoRA is a scalable, privacy-preserving framework for on-device LLM deployment, enabling secure federated fine-tuning with strong privacy guarantees while maintaining competitive performance.

Abstract: As on-device large language model (LLM) systems become increasingly
prevalent, federated fine-tuning enables advanced language understanding and
generation directly on edge devices; however, it also involves processing
sensitive, user-specific data, raising significant privacy concerns within the
federated learning framework. To address these challenges, we propose
DP-FedLoRA, a privacy-enhanced federated fine-tuning framework that integrates
LoRA-based adaptation with differential privacy in a communication-efficient
setting. Each client locally clips and perturbs its LoRA matrices using
Gaussian noise to satisfy ($\epsilon$, $\delta$)-differential privacy. We
further provide a theoretical analysis demonstrating the unbiased nature of the
updates and deriving bounds on the variance introduced by noise, offering
practical guidance for privacy-budget calibration. Experimental results across
mainstream benchmarks show that DP-FedLoRA delivers competitive performance
while offering strong privacy guarantees, paving the way for scalable and
privacy-preserving LLM deployment in on-device environments.

</details>


### [6] [AgriSentinel: Privacy-Enhanced Embedded-LLM Crop Disease Alerting System](https://arxiv.org/abs/2509.09103)
*Chanti Raju Mylay,Bobin Deng,Zhipeng Cai,Honghui Xu*

Main category: cs.CR

TL;DR: The paper introduces AgriSentinel, the first Privacy-Enhanced Embedded-LLM Crop Disease Alerting System. It combines differential privacy for data protection with a personalized, on-device large language model to provide farmers with characteristics regarding early detection of recommendation and crop disease intellectual, ensuring both privacy and actionable insights.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the significant impact of crop diseases on agriculture and the limitations of current AI alert systems in terms of data privacy, market pricing protection, and usability for farmers. There is a need for a system that ensures privacy without compromising classification accuracy and usability.

Method: AgriSentinel employs a light-weight deep learning-based crop disease classification model optimized for mobile devices and integrates an on-device large language model (LLM) for recommendations. A data privacy protection process provides personalized, private disease management strategies and a market pricing mechanism.

Result: Experiments show that AgriSentinel can protect data privacy, maintain high classification performance, and deliver practical disease management solutions. It demonstrates effectiveness and showcases high performance in managing crop diseases with privacy and usability together.

Conclusion: AgriSentinel successfully addresses major gaps in crop disease management, offering a robust solution for privacy-enhanced AI systems in agriculture. This can lead to improved agricultural decision-making and increased productivity for greater awareness.

Abstract: Crop diseases pose significant threats to global food security, agricultural
productivity, and sustainable farming practices, directly affecting farmers'
livelihoods and economic stability. To address the growing need for effective
crop disease management, AI-based disease alerting systems have emerged as
promising tools by providing early detection and actionable insights for timely
intervention. However, existing systems often overlook critical aspects such as
data privacy, market pricing power, and farmer-friendly usability, leaving
farmers vulnerable to privacy breaches and economic exploitation. To bridge
these gaps, we propose AgriSentinel, the first Privacy-Enhanced Embedded-LLM
Crop Disease Alerting System. AgriSentinel incorporates a differential privacy
mechanism to protect sensitive crop image data while maintaining classification
accuracy. Its lightweight deep learning-based crop disease classification model
is optimized for mobile devices, ensuring accessibility and usability for
farmers. Additionally, the system includes a fine-tuned, on-device large
language model (LLM) that leverages a curated knowledge pool to provide farmers
with specific, actionable suggestions for managing crop diseases, going beyond
simple alerting. Comprehensive experiments validate the effectiveness of
AgriSentinel, demonstrating its ability to safeguard data privacy, maintain
high classification performance, and deliver practical, actionable disease
management strategies. AgriSentinel offers a robust, farmer-friendly solution
for automating crop disease alerting and management, ultimately contributing to
improved agricultural decision-making and enhanced crop productivity.

</details>


### [7] [CryptGNN: Enabling Secure Inference for Graph Neural Networks](https://arxiv.org/abs/2509.09107)
*Pritam Sen,Yao Ma,Cristian Borcea*

Main category: cs.CR

TL;DR: CryptGNN is a secure GNN inference framework for MLaaS in the cloud, using SMPC to protect data, graph structure, and model parameters. It achieves provable security against P-1 colluding parties without requiring trusted servers.


<details>
  <summary>Details</summary>
Motivation: Clients using cloud-based GNN models face privacy risks: cloud providers could leak input data/graph structures, and model owners could expose parameters. Existing solutions fail to address collusion resistance or require trusted components.

Method: CryptGNN implements secure message passing/feature transformation via distributed SMPC protocols. It distributes computation across multiple parties, ensures confidentiality through cryptographic guarantees, and requires no trusted server. Security holds under P-1-party collusion.

Result: Theoretical analysis proves security against collusion. Empirical evaluation shows practical efficiency for GNN inference, demonstrating feasibility of secure MLaaS for graph data without performance bottlenecks.

Conclusion: CryptGNN establishes a robust, collusion-resilient framework for secure GNN inference in untrusted cloud environments, enabling privacy-preserving MLaaS for graph-structured data without sacrificing model availability or requiring hardware trust assumptions.

Abstract: We present CryptGNN, a secure and effective inference solution for
third-party graph neural network (GNN) models in the cloud, which are accessed
by clients as ML as a service (MLaaS). The main novelty of CryptGNN is its
secure message passing and feature transformation layers using distributed
secure multi-party computation (SMPC) techniques. CryptGNN protects the
client's input data and graph structure from the cloud provider and the
third-party model owner, and it protects the model parameters from the cloud
provider and the clients. CryptGNN works with any number of SMPC parties, does
not require a trusted server, and is provably secure even if P-1 out of P
parties in the cloud collude. Theoretical analysis and empirical experiments
demonstrate the security and efficiency of CryptGNN.

</details>


### [8] [Character-Level Perturbations Disrupt LLM Watermarks](https://arxiv.org/abs/2509.09112)
*Zhaoxi Zhang,Xiaomei Zhang,Yanjun Zhang,He Zhang,Shirui Pan,Bo Liu,Asif Qumer Gill,Leo Yu Zhang*

Main category: cs.CR

TL;DR: This paper demonstrates that LLM watermarks are vulnerable to simple, character-level perturbations and GA-guided attacks, showing current defenses can be bypassed with minimal edits. It calls for new robust watermarking techniques to secure AI-generated content.


<details>
  <summary>Details</summary>
Motivation: Previous watermark removal attacks were deemed ineffective due to suboptimal methods, creating a misconception that large perturbations or powerful adversaries are required. This study challenges that view by demonstrating efficient removal with minimal changes under realistic constraints.

Method: The authors formalize threat models for LLM watermarking, analyze perturbation types (focusing on character-level attacks), and propose a Genetic Algorithm (GA)-guided removal method optimized with a reference detector. They also introduce an adaptive compound attack to counter defenses.

Result: Experiments confirm character-level perturbations disrupt tokenization more effectively than other methods. The GA-based attacks achieve strong removal performance with limited detector queries. The proposed adaptive attacks successfully bypass defensive strategies.

Conclusion: The paper highlights vulnerabilities in existing LLM watermarking schemes, showing that character-level perturbations and guided attacks can effectively remove watermarks under realistic constraints. It emphasizes the need for robust, adaptive watermarking mechanisms to address these security gaps.

Abstract: Large Language Model (LLM) watermarking embeds detectable signals into
generated text for copyright protection, misuse prevention, and content
detection. While prior studies evaluate robustness using watermark removal
attacks, these methods are often suboptimal, creating the misconception that
effective removal requires large perturbations or powerful adversaries.
  To bridge the gap, we first formalize the system model for LLM watermark, and
characterize two realistic threat models constrained on limited access to the
watermark detector. We then analyze how different types of perturbation vary in
their attack range, i.e., the number of tokens they can affect with a single
edit. We observe that character-level perturbations (e.g., typos, swaps,
deletions, homoglyphs) can influence multiple tokens simultaneously by
disrupting the tokenization process. We demonstrate that character-level
perturbations are significantly more effective for watermark removal under the
most restrictive threat model. We further propose guided removal attacks based
on the Genetic Algorithm (GA) that uses a reference detector for optimization.
Under a practical threat model with limited black-box queries to the watermark
detector, our method demonstrates strong removal performance. Experiments
confirm the superiority of character-level perturbations and the effectiveness
of the GA in removing watermarks under realistic constraints. Additionally, we
argue there is an adversarial dilemma when considering potential defenses: any
fixed defense can be bypassed by a suitable perturbation strategy. Motivated by
this principle, we propose an adaptive compound character-level attack.
Experimental results show that this approach can effectively defeat the
defenses. Our findings highlight significant vulnerabilities in existing LLM
watermark schemes and underline the urgency for the development of new robust
mechanisms.

</details>


### [9] [IoTFuzzSentry: A Protocol Guided Mutation Based Fuzzer for Automatic Vulnerability Testing in Commercial IoT Devices](https://arxiv.org/abs/2509.09158)
*Priyanka Rushikesh Chaudhary,Rajib Ranjan Maiti*

Main category: cs.CR

TL;DR: The paper introduces IoTFuzzSentry, a fuzzing tool for uncovering IoT security flaws. It identifies four vulnerability types in commercial devices, publishes CVEs, and demonstrates real-world exploits, showing potential for scalable IoT security improvement.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the critical need to uncover security vulnerabilities in IoT devices during their operational phase, where implementation flaws in security mechanisms can lead to unauthorized access and data leakage.

Method: The authors developed IoTFuzzSentry, a mutation-based protocol fuzzing tool, integrating it with Cotopaxi to test commercial IoT devices. They injected crafted packets to expose vulnerabilities in transport and application-layer security mechanisms.

Result: The evaluation revealed four exploit categories (IoT Access Credential Leakage, Sneak IoT Live Video Stream, Creep IoT Live Image, IoT Command Injection) and demonstrated real-world exploits. Two CVEs were published (CVE-2024-41623, CVE-2024-42531), and six additional IoT devices showed similar vulnerabilities due to common application protocols.

Conclusion: The paper concludes that IoTFuzzSentry effectively identifies unconventional security threats in IoT devices with minimal overhead, enabling vendors to enhance device security automatically.

Abstract: Protocol fuzzing is a scalable and cost-effective technique for identifying
security vulnerabilities in deployed Internet of Things devices. During their
operational phase, IoT devices often run lightweight servers to handle user
interactions, such as video streaming or image capture in smart cameras.
Implementation flaws in transport or application-layer security mechanisms can
expose IoT devices to a range of threats, including unauthorized access and
data leakage. This paper addresses the challenge of uncovering such
vulnerabilities by leveraging protocol fuzzing techniques that inject crafted
transport and application-layer packets into IoT communications. We present a
mutation-based fuzzing tool, named IoTFuzzSentry, to identify specific
non-trivial vulnerabilities in commercial IoT devices. We further demonstrate
how these vulnerabilities can be exploited in real-world scenarios. We
integrated our fuzzing tool into a well-known testing tool Cotopaxi and
evaluated it with commercial-off-the-shelf IoT devices such as IP cameras and
Smart Plug. Our evaluation revealed vulnerabilities categorized into 4 types
(IoT Access Credential Leakage, Sneak IoT Live Video Stream, Creep IoT Live
Image, IoT Command Injection) and we show their exploits using three IoT
devices. We have responsibly disclosed all these vulnerabilities to the
respective vendors. So far, we have published two CVEs, CVE-2024-41623 and
CVE-2024-42531, and one is awaiting. To extend the applicability, we have
investigated the traffic of six additional IoT devices and our analysis shows
that these devices can have similar vulnerabilities, due to the presence of a
similar set of application protocols. We believe that IoTFuzzSentry has the
potential to discover unconventional security threats and allow IoT vendors to
strengthen the security of their commercialized IoT devices automatically with
negligible overhead.

</details>


### [10] [Enhancing Cyber Threat Hunting -- A Visual Approach with the Forensic Visualization Toolkit](https://arxiv.org/abs/2509.09185)
*Jihane Najar,Marinos Tsantekidis,Aris Sotiropoulos,Vassilis Prevelakis*

Main category: cs.CR

TL;DR: Introduces FVT: a digital forensics toolkit with visualizations to boost proactive threat hunting, validated through EU research projects.


<details>
  <summary>Details</summary>
Motivation: Dynamic cyber threat landscapes require proactive threat hunting beyond passive security systems to detect advanced threats that evade traditional defenses.

Method: Development of the Forensic Visualization Toolkit (FVT), featuring digital forensics investigations, interactive visualizations, and iterative application in EU-funded research projects to refine its capabilities.

Result: FVT demonstrates effectiveness in real-world scenarios for identifying, analyzing, and responding to cyber threats, validated through integration into EU-funded research programs.

Conclusion: The Forensic Visualization Toolkit (FVT) enhances cybersecurity situational awareness through advanced digital forensics, integrates with EU-funded research, and empowers analysts to actively hunt advanced threats.

Abstract: In today's dynamic cyber threat landscape, organizations must take proactive
steps to bolster their cybersecurity defenses. Cyber threat hunting is a
proactive and iterative process aimed at identifying and mitigating advanced
threats that may go undetected by traditional security measures. Rather than
waiting for automated security systems to flag potential threats, threat
hunting involves actively searching for signs of malicious activity within an
organization's network. In this paper, we present the Forensic Visualization
Toolkit, a powerful tool designed for digital forensics investigations,
analysis of digital evidence, and advanced visualizations to enhance
cybersecurity situational awareness and risk management and empower security
analysts with an intuitive and interactive tool. Through practical, real-world
scenarios, we demonstrate how FVT significantly amplifies the capabilities of
cybersecurity professionals, enabling them to effectively identify, analyze,
and respond to threats. Furthermore, it is important to highlight that FVT has
been integrated into, utilized, and continually enhanced within various
EU-funded research projects over recent years.

</details>


### [11] [Shell or Nothing: Real-World Benchmarks and Memory-Activated Agents for Automated Penetration Testing](https://arxiv.org/abs/2509.09207)
*Wuyuao Mai,Geng Hong,Qi Liu,Jinsong Chen,Jiarun Dai,Xudong Pan,Yuan Zhang,Min Yang*

Main category: cs.CR

TL;DR: This work pioneers real-world AI pentesting by creating TermiBench benchmark and TermiAgent framework with memory activation & structured code analysis, outperforming existing agents in practical penetration testing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Traditional pentesting relies on expensive human experts; AI agents are evaluated in oversimplified CTF environments that don't reflect real-world complexity. Existing systems fail to achieve full system control in realistic scenarios.

Method: 1) Introduced TermiBench: First real-world pentesting benchmark with 510 hosts across 25 services/30 CVEs
2) Developed TermiAgent: Multi-agent framework with
• Located Memory Activation for long-context retention
• Structured code understanding for exploit arsenal development

Result: 1) Existing agents struggle with TermiBench: Can't obtain system shells in realistic environments
2) TermiAgent outperforms state-of-the-art agents:
• Stronger penetration testing capability
• 58% time reduction and financial cost savings
• Laptop-scale deployment feasibility

Conclusion: The paper introduces the first real-world open-source benchmark for autonomous pentesting, TermiBench, and a novel AI-driven framework TermiAgent, establishing a milestone in AI-powered penetration testing by addressing limitations of existing methods.

Abstract: Penetration testing is critical for identifying and mitigating security
vulnerabilities, yet traditional approaches remain expensive, time-consuming,
and dependent on expert human labor. Recent work has explored AI-driven
pentesting agents, but their evaluation relies on oversimplified
capture-the-flag (CTF) settings that embed prior knowledge and reduce
complexity, leading to performance estimates far from real-world practice. We
close this gap by introducing the first real-world, agent-oriented pentesting
benchmark, TermiBench, which shifts the goal from 'flag finding' to achieving
full system control. The benchmark spans 510 hosts across 25 services and 30
CVEs, with realistic environments that require autonomous reconnaissance,
discrimination between benign and exploitable services, and robust exploit
execution. Using this benchmark, we find that existing systems can hardly
obtain system shells under realistic conditions.
  To address these challenges, we propose TermiAgent, a multi-agent penetration
testing framework. TermiAgent mitigates long-context forgetting with a Located
Memory Activation mechanism and builds a reliable exploit arsenal via
structured code understanding rather than naive retrieval. In evaluations, our
work outperforms state-of-the-art agents, exhibiting stronger penetration
testing capability, reducing execution time and financial cost, and
demonstrating practicality even on laptop-scale deployments. Our work delivers
both the first open-source benchmark for real-world autonomous pentesting and a
novel agent framework that establishes a milestone for AI-driven penetration
testing.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [12] [Pattern-Based File and Data Access with Python Glob: A Comprehensive Guide for Computational Research](https://arxiv.org/abs/2509.08843)
*Sidney Shapiro*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Pattern-based file access is a fundamental but often under-documented aspect
of computational research. The Python glob module provides a simple yet
powerful way to search, filter, and ingest files using wildcard patterns,
enabling scalable workflows across disciplines. This paper introduces glob as a
versatile tool for data science, business analytics, and artificial
intelligence applications. We demonstrate use cases including large-scale data
ingestion, organizational data analysis, AI dataset construction, and
reproducible research practices. Through concrete Python examples with widely
used libraries such as pandas,scikit-learn, and matplotlib, we show how glob
facilitates efficient file traversal and integration with analytical pipelines.
By situating glob within the broader context of reproducible research and data
engineering, we highlight its role as a methodological building block. Our goal
is to provide researchers and practitioners with a concise reference that
bridges foundational concepts and applied practice, making glob a default
citation for file pattern matching in Python-based research workflows.

</details>


### [13] [A Systematic Mapping Study on Chatbots in Programming Education](https://arxiv.org/abs/2509.08857)
*Marcelino Garcia,Renato Garcia,Arthur Parizotto,Andre Mendes,Pedro Valle,Ricardo Vilela,Renato Balancieri,Williamson Silva*

Main category: cs.SE

TL;DR: This SMS analyzes 54 studies to map educational chatbots in programming education, revealing Python’s prominence, foundational content, and pedagogical diversity while highlighting research gaps to guide future tool development.


<details>
  <summary>Details</summary>
Motivation: Educational chatbots are increasingly used in programming education, necessitating an analysis of their development and application to guide future tool design.

Method: A Systematic Mapping Study (SMS) was conducted, analyzing 54 studies out of 3,216 publications using five research subquestions related to chatbot types, languages, interaction models, and application contexts.

Result: Most chatbots target Python instruction, focus on foundational programming concepts, and utilize diverse pedagogical strategies and architectures. Key trends and gaps were identified.

Conclusion: The study provides insights into trends and gaps in educational chatbots for programming, aiming to inform the development of new educational tools.

Abstract: Educational chatbots have gained prominence as support tools for teaching
programming, particularly in introductory learning contexts. This paper
presents a Systematic Mapping Study (SMS) that investigated how such agents
have been developed and applied in programming education. From an initial set
of 3,216 publications, 54 studies were selected and analyzed based on five
research subquestions, addressing chatbot types, programming languages used,
educational content covered, interaction models, and application contexts. The
results reveal a predominance of chatbots designed for Python instruction,
focusing on fundamental programming concepts, and employing a wide variety of
pedagogical approaches and technological architectures. In addition to
identifying trends and gaps in the literature, this study provides insights to
inform the development of new educational tools for programming instruction.

</details>


### [14] [GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation](https://arxiv.org/abs/2509.08863)
*Qianqian Luo,Liuchang Xu,Qingming Lin,Sensen Wu,Ruichen Mao,Chao Wang,Hailin Feng,Bo Huang,Zhenhong Du*

Main category: cs.SE

TL;DR: This paper proposes GeoJSON Agents, a multi-agent LLM architecture that enhances GIS automation by converting natural language tasks into structured GeoJSON operations using Function Calling and Code Generation techniques, achieving significant performance improvements over general-purpose models.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) face limitations in GIS tasks without specialized expertise, necessitating a framework to bridge this gap and improve automation capabilities for spatial data processing.

Method: The architecture comprises three modules: (1) a Planner agent parsing tasks into GeoJSON commands, (2) Worker agents executing spatial analysis via Function Calling or Code Generation, and (3) integrating results into standards-compliant GeoJSON files. Evaluated using a 70-task benchmark and GPT-4o.

Result: Function Calling achieved 85.71% accuracy, Code Generation 97.14%, both surpassing general-purpose models (48.57%). Code Generation shows greater flexibility while Function Calling provides stable execution.

Conclusion: This work introduces the first LLM multi-agent framework for GeoJSON data, systematically comparing Function Calling and Code Generation approaches, offering novel insights for optimizing GeoAI system performance.

Abstract: LLMs have made substantial progress in task automation and natural language
understanding.However,without expertise in GIS,they continue to encounter
limitations.To address these issues, we propose GeoJSON Agents-a multi-agent
LLM architecture.This framework transforms natural language tasks into
structured GeoJSON operation commands and processes spatial data using two
widely adopted LLM enhancement techniques:Function Calling and Code
Generation.The architecture consists of three components-task parsing,agent
collaboration,and result integration-aimed at enhancing both the performance
and scalability of GIS automation.The Planner agent interprets natural language
tasks into structured GeoJSON commands.Then,specialized Worker agents
collaborate according to assigned roles to perform spatial data processing and
analysis,either by invoking predefined function APIs or by dynamically
generating and executing Python-based spatial analysis code.Finally,the system
integrates the outputs from multiple execution rounds into
reusable,standards-compliant GeoJSON files.To systematically evaluate the
performance of the two approaches,we constructed a benchmark dataset of 70
tasks with varying complexity and conducted experiments using OpenAI's GPT-4o
as the core model.Results indicate that the Function Calling-based GeoJSON
Agent achieved an accuracy of 85.71%,while the Code Generation-based agent
reached 97.14%,both significantly outperforming the best-performing
general-purpose model (48.57%).Further analysis reveals that the Code
Generation provides greater flexibility,whereas the Function Calling approach
offers more stable execution.This study is the first to introduce an LLM
multi-agent framework for GeoJSON data and to compare the strengths and
limitations of two mainstream LLM enhancement methods,offering new perspectives
for improving GeoAI system performance.

</details>


### [15] [TraceRAG: A LLM-Based Framework for Explainable Android Malware Detection and Behavior Analysis](https://arxiv.org/abs/2509.08865)
*Guangyu Zhang,Xixuan Wang,Shiyu Sun,Peiyan Xiao,Kun Sun,Yanhai Xiong*

Main category: cs.SE

TL;DR: TraceRAG introduces a RAG-based framework for explainable Android malware detection by connecting natural language queries to code semantics, achieving state-of-the-art accuracy and producing actionable threat reports through multi-step analysis.


<details>
  <summary>Details</summary>
Motivation: Malicious Android apps use evasion tactics to hide within legitimate code, while traditional analysis methods lack deep behavioral visibility and fail to provide human-readable explanations for their determinations.

Method: TraceRAG generates method-level Java code summaries indexed in a vector database, retrieves semantically relevant snippets for behavioral analysis, and synthesizes multi-turn inspection results into explainable reports through LLM-based reasoning.

Result: 96% malware detection accuracy and 83.81% behavior identification accuracy validated through VirusTotal scans and manual verification, with expert-validated report utility for real-world malware analysis.

Conclusion: TraceRAG effectively addresses the limitations of traditional malware analysis by combining retrieval-augmented generation with code semantics, achieving high detection accuracy and producing human-readable reports for transparent threat analysis.

Abstract: Sophisticated evasion tactics in malicious Android applications, combined
with their intricate behavioral semantics, enable attackers to conceal
malicious logic within legitimate functions, underscoring the critical need for
robust and in-depth analysis frameworks. However, traditional analysis
techniques often fail to recover deeply hidden behaviors or provide
human-readable justifications for their decisions. Inspired by advances in
large language models (LLMs), we introduce TraceRAG, a retrieval-augmented
generation (RAG) framework that bridges natural language queries and Java code
to deliver explainable malware detection and analysis. First, TraceRAG
generates summaries of method-level code snippets, which are indexed in a
vector database. At query time, behavior-focused questions retrieve the most
semantically relevant snippets for deeper inspection. Finally, based on the
multi-turn analysis results, TraceRAG produces human-readable reports that
present the identified malicious behaviors and their corresponding code
implementations. Experimental results demonstrate that our method achieves 96\%
malware detection accuracy and 83.81\% behavior identification accuracy based
on updated VirusTotal (VT) scans and manual verification. Furthermore, expert
evaluation confirms the practical utility of the reports generated by TraceRAG.

</details>


### [16] [Benchmarking Energy Efficiency of Large Language Models Using vLLM](https://arxiv.org/abs/2509.08867)
*K. Pronk,Q. Zhao*

Main category: cs.SE

TL;DR: To address gaps in LLM energy efficiency benchmarks, the paper introduces a production-focused benchmark using vLLM, showing how model design and usage patterns affect sustainability and offering tools for developers to optimize energy consumption.


<details>
  <summary>Details</summary>
Motivation: Current energy efficiency benchmarks for LLMs fail to simulate realistic production scenarios, necessitating practical tools to guide developers in reducing the environmental impact of their AI systems.

Method: The authors developed the LLM Efficiency Benchmark using vLLM, a production-ready backend, to evaluate energy efficiency under factors like model size, architecture, and concurrent request volume.

Result: Findings demonstrate that the proposed benchmark effectively simulates real-world deployment conditions, providing developers with data to improve the energy efficiency of LLMs.

Conclusion: The study concludes that creating energy efficiency benchmarks for LLMs that reflect real-world deployment conditions is feasible, offering actionable insights for developers to build sustainable AI systems.

Abstract: The prevalence of Large Language Models (LLMs) is having an growing impact on
the climate due to the substantial energy required for their deployment and
use. To create awareness for developers who are implementing LLMs in their
products, there is a strong need to collect more information about the energy
efficiency of LLMs. While existing research has evaluated the energy efficiency
of various models, these benchmarks often fall short of representing realistic
production scenarios. In this paper, we introduce the LLM Efficiency Benchmark,
designed to simulate real-world usage conditions. Our benchmark utilizes vLLM,
a high-throughput, production-ready LLM serving backend that optimizes model
performance and efficiency. We examine how factors such as model size,
architecture, and concurrent request volume affect inference energy efficiency.
Our findings demonstrate that it is possible to create energy efficiency
benchmarks that better reflect practical deployment conditions, providing
valuable insights for developers aiming to build more sustainable AI systems.

</details>


### [17] [CLARA: A Developer's Companion for Code Comprehension and Analysis](https://arxiv.org/abs/2509.09072)
*Ahmed Adnan,Mushfiqur Rahman,Saad Sakib Noor,Kazi Sakib*

Main category: cs.SE

TL;DR: CLARA is an open-source browser extension using AI to streamline code comprehension, refactoring, and quality analysis, validated by evaluations and user testing as effective and practical.


<details>
  <summary>Details</summary>
Motivation: Existing code analysis tools require manual setup, lack context awareness, and involve significant manual effort, necessitating a more practical solution.

Method: The authors developed CLARA, a browser extension leveraging a state-of-the-art inference model, and evaluated its performance using existing datasets and a user study with 10 participants.

Result: CLARA demonstrated usefulness, accuracy, and practicality in code comprehension tasks, with positive feedback from the user study.

Conclusion: CLARA effectively addresses the shortcomings of existing tools by providing a context-aware, easy-to-use solution for code comprehension and analysis, validated through user studies and evaluations.

Abstract: Code comprehension and analysis of open-source project codebases is a task
frequently performed by developers and researchers. However, existing tools
that practitioners use for assistance with such tasks often require prior
project setup, lack context-awareness, and involve significant manual effort.
To address this, we present CLARA, a browser extension that utilizes a
state-of-the-art inference model to assist developers and researchers in: (i)
comprehending code files and code fragments, (ii) code refactoring, and (iii)
code quality attribute detection. We qualitatively evaluated CLARA's inference
model using existing datasets and methodology, and performed a comprehensive
user study with 10 developers and academic researchers to assess its usability
and usefulness. The results show that CLARA is useful, accurate, and practical
in code comprehension and analysis tasks. CLARA is an open-source tool
available at https://github.com/SaadNoor555/CLARA_tool_demo. A video showing
the full capabilities of CLARA can be found at
https://youtu.be/VDKVXvIH41Q?si=qBFsmS_Y4m_9x3YH.

</details>


### [18] [Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset](https://arxiv.org/abs/2509.09192)
*Doha Nam,Taehyoun Kim,Duksan Ryu,Jongmoon Baik*

Main category: cs.SE

TL;DR: This paper addresses noisy labels in JIT-SDP via ReDef dataset and evaluates PLM's ability to understand code modifications.


<details>
  <summary>Details</summary>
Motivation: Existing JIT-SDP datasets suffer from unreliable labels and poor precision in identifying bug-inducing commits, hindering robust model evaluation.

Method: 1) Constructed ReDef dataset using revert commits for defective cases and post-hoc checks for clean cases. 2 Conducted GPT-assisted filtering to remove ambiguity. 3 Evaluated PLMs (CodeBERT, CodeT5+, UniXcoder) across 5 encoding strategies with counterfactual perturbations.

Result: 1) ReDef achieves high-label quality with 3,164 defective and 10,268 clean modifications. 2 Diff-style encodings outperform whole-function formats for all PLMs. 3 Counterfactual tests reveal models rely on surface-level cues rather than semantic understanding.

Conclusion: Current PLMs exhibit limited semantic comprehension of code modifications in JIT-SDP despite apparent robustness, highlighting the need for improved modeling approaches.

Abstract: Just-in-Time software defect prediction (JIT-SDP) plays a critical role in
prioritizing risky code changes during code review and continuous integration.
However, existing datasets often suffer from noisy labels and low precision in
identifying bug-inducing commits. To address this, we present ReDef
(Revert-based Defect dataset), a high-confidence benchmark of function-level
modifications curated from 22 large-scale C/C++ projects. Defective cases are
anchored by revert commits, while clean cases are validated through post-hoc
history checks. Ambiguous instances are conservatively filtered out via a
GPT-assisted triage process involving multiple votes and audits. This pipeline
yields 3,164 defective and 10,268 clean modifications, offering substantially
more reliable labels than prior existing resources. Beyond dataset
construction, we provide the first systematic evaluation of how pre-trained
language models (PLMs) reason about code modifications -- specifically, which
input encodings most effectively expose change information, and whether models
genuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder
under five encoding strategies, and further probe their sensitivity through
counterfactual perturbations that swap added/deleted blocks, invert diff
polarity, or inject spurious markers. Our results show that compact diff-style
encodings consistently outperform whole-function formats across all PLMs, with
statistical tests confirming large, model-independent effects. However, under
counterfactual tests, performance degrades little or not at all -- revealing
that what appears to be robustness in fact reflects reliance on superficial
cues rather than true semantic understanding. These findings indicate that,
unlike in snapshot-based tasks, current PLMs remain limited in their ability to
genuinely comprehend code modifications.

</details>
