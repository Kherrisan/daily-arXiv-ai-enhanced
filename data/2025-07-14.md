<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 7]
- [cs.SE](#cs.SE) [Total: 10]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Beyond the Worst Case: Extending Differential Privacy Guarantees to Realistic Adversaries](https://arxiv.org/abs/2507.08158)
*Marika Swanberg,Meenatchi Sundaram Muthu Selva Annamalai,Jamie Hayes,Borja Balle,Adam Smith*

Main category: cs.CR

TL;DR: This paper establishes a flexible framework to derive high-probability bounds for differential privacy mechanisms in both unexplored and real-world attack scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing differential privacy analyses focus on worst-case guarantees, but quantitative tradeoffs between adversarial models and privacy protections remain poorly understood.

Method: Developing a generalized analytical framework through careful reconstruction risk analysis and empirical validation against state-of-the-art attacks in two critical domains

Result: The framework quantitatively demonstrates how DP protections against multi-column reconstruction attacks and sensitive information extraction from language models depend on adversarial prior probability, with empirical validation showing its effectiveness

Conclusion: The high-probability bounds enable nuanced privacy risk assessment across diverse attack settings, complementing traditional DP guarantees while addressing practical limitations

Abstract: Differential Privacy (DP) is a family of definitions that bound the
worst-case privacy leakage of a mechanism. One important feature of the
worst-case DP guarantee is it naturally implies protections against adversaries
with less prior information, more sophisticated attack goals, and complex
measures of a successful attack. However, the analytical tradeoffs between the
adversarial model and the privacy protections conferred by DP are not well
understood thus far. To that end, this work sheds light on what the worst-case
guarantee of DP implies about the success of attackers that are more
representative of real-world privacy risks.
  In this paper, we present a single flexible framework that generalizes and
extends the patchwork of bounds on DP mechanisms found in prior work. Our
framework allows us to compute high-probability guarantees for DP mechanisms on
a large family of natural attack settings that previous bounds do not capture.
One class of such settings is the approximate reconstruction of multiple
individuals' data, such as inferring nearly entire columns of a tabular data
set from noisy marginals and extracting sensitive information from DP-trained
language models.
  We conduct two empirical case studies to illustrate the versatility of our
bounds and compare them to the success of state-of-the-art attacks.
Specifically, we study attacks that extract non-uniform PII from a DP-trained
language model, as well as multi-column reconstruction attacks where the
adversary has access to some columns in the clear and attempts to reconstruct
the remaining columns for each person's record. We find that the absolute
privacy risk of attacking non-uniform data is highly dependent on the
adversary's prior probability of success. Our high probability bounds give us a
nuanced understanding of the privacy leakage of DP mechanisms in a variety of
previously understudied attack settings.

</details>


### [2] [GPUHammer: Rowhammer Attacks on GPU Memories are Practical](https://arxiv.org/abs/2507.08166)
*Chris S. Lin,Joyce Qu,Gururaj Saileshwar*

Main category: cs.CR

TL;DR: GPUHammer demonstrates the first Rowhammer attack on NVIDIA GDDR6 GPUs, enabling ML model tampering with up to 80% accuracy degradation.


<details>
  <summary>Details</summary>
Motivation: Rowhammer vulnerabilities in CPU DRAMs are well studied, but GPU GDDR memories' security implications for ML applications remain unexplored.

Method: Reverse-engineered GDDR6 row mappings and optimized GPU memory access to bypass hardware mitigations through proprietary attack techniques.

Result: Successfully induced 8 bit-flips across 4 banks on NVIDIA A6000 GPUs, proving ML model manipulation feasibility with 80% accuracy drop demonstrations.

Conclusion: Exposes critical GDDR6 security gaps in GPU architecture, challenging assumptions about proprietary memory protections and emphasizing ML infrastructure vulnerabilities.

Abstract: Rowhammer is a read disturbance vulnerability in modern DRAM that causes
bit-flips, compromising security and reliability. While extensively studied on
Intel and AMD CPUs with DDR and LPDDR memories, its impact on GPUs using GDDR
memories, critical for emerging machine learning applications, remains
unexplored. Rowhammer attacks on GPUs face unique challenges: (1) proprietary
mapping of physical memory to GDDR banks and rows, (2) high memory latency and
faster refresh rates that hinder effective hammering, and (3) proprietary
mitigations in GDDR memories, difficult to reverse-engineer without FPGA-based
test platforms. We introduce GPUHammer, the first Rowhammer attack on NVIDIA
GPUs with GDDR6 DRAM. GPUHammer proposes novel techniques to reverse-engineer
GDDR DRAM row mappings, and employs GPU-specific memory access optimizations to
amplify hammering intensity and bypass mitigations. Thus, we demonstrate the
first successful Rowhammer attack on a discrete GPU, injecting up to 8
bit-flips across 4 DRAM banks on an NVIDIA A6000 with GDDR6 memory. We also
show how an attacker can use these to tamper with ML models, causing
significant accuracy drops (up to 80%).

</details>


### [3] [TruChain: A Multi-Layer Architecture for Trusted, Verifiable, and Immutable Open Banking Data](https://arxiv.org/abs/2507.08286)
*Aufa Nasywa Rahman,Bimo Sunarfri Hantono,Guntur Dharma Putra*

Main category: cs.CR

TL;DR: The paper addresses technological risks in open banking by proposing a three-layered architecture using decentralized identity, cryptographic signing, and Tangle for trustworthiness, demonstrating efficient scalability and strong data integrity.


<details>
  <summary>Details</summary>
Motivation: Current open banking standards face severe risks like unverified data sources, inconsistent integrity, and lack of immutability, threatening secure data sharing and regulatory compliance.

Method: A layered architecture: Layer 1 ensures source validation via decentralized identity and verifiable presentations; Layer 2 guarantees data authenticity using cryptographic signing; Layer 3 ensures immutability through the Tangle DAG ledger.

Result: Proof-of-concept shows linear scalability, 100% validation rate, <35% CPU usage, 350 MiB memory consumption, and significantly reduced latency compared to real-world implementations with robust data integrity.

Conclusion: The proposed solution provides a practical, efficient, and compliant system for secure open banking data sharing, mitigating critical risks while maintaining high performance and trust.

Abstract: Open banking framework enables third party providers to access financial data
across banking institutions, leading to unprecedented innovations in the
financial sector. However, some open banking standards remain susceptible to
severe technological risks, including unverified data sources, inconsistent
data integrity, and lack of immutability. In this paper, we propose a layered
architecture that provides assurance in data trustworthiness with three
distinct levels of trust, covering source validation, data-level
authentication, and tamper-proof storage. The first layer guarantees the source
legitimacy using decentralized identity and verifiable presentation, while the
second layer verifies data authenticity and consistency using cryptographic
signing. Lastly, the third layer guarantees data immutability through the
Tangle, a directed acyclic graph distributed ledger. We implemented a
proof-of-concept implementation of our solution to evaluate its performance,
where the results demonstrate that the system scales linearly with a stable
throughput, exhibits a 100% validation rate, and utilizes under 35% of CPU and
350 MiB memory. Compared to a real-world open banking implementation, our
solution offers significantly reduced latency and stronger data integrity
assurance. Overall, our solution offers a practical and efficient system for
secure data sharing in financial ecosystems while maintaining regulatory
compliance.

</details>


### [4] [Invariant-based Robust Weights Watermark for Large Language Models](https://arxiv.org/abs/2507.08288)
*Qingxiao Guo,Xinjie Zhu,Yilong Ma,Hui Jin,Yunhao Wang,Weifeng Zhang,Xiaobing Guo*

Main category: cs.CR

TL;DR: The paper proposes a post-training watermarking method for transformer models (LLMs) that generates per-user watermarks through linear constrained model invariants, offering robustness against IP theft and collusion attacks without requiring retraining.


<details>
  <summary>Details</summary>
Motivation: With billions of edge devices hosting LLMs, protecting intellectual property from malicious IP theft (including collusion attacks among multiple users) has become critical. Existing watermarking techniques often require retraining or fine-tuning, which is computationally expensive and impractical for distributed scenarios.

Method: 1) Generates unique user keys for watermarking
2) Derives stable watermarks by solving linear constraints based on model invariants
3) Implements a noise mechanism to obscure watermark locations in multi-user settings
4) Requires no retraining or model modification

Result: Demonstrated strong robustness against:
- Fine-tuning, pruning, and quantization
- Structural attacks (permutation, scaling, reversible matrix)
- Collusion attacks
Evaluated on Llama3, Phi3, and Gemma models showing consistent performance

Conclusion: This watermarking approach provides a practical solution for securing distributed LLMs with minimal overhead, maintaining watermark integrity under diverse attacks and multi-user collusion scenarios while avoiding the need for model retraining.

Abstract: Watermarking technology has gained significant attention due to the
increasing importance of intellectual property (IP) rights, particularly with
the growing deployment of large language models (LLMs) on billions
resource-constrained edge devices. To counter the potential threats of IP theft
by malicious users, this paper introduces a robust watermarking scheme without
retraining or fine-tuning for transformer models. The scheme generates a unique
key for each user and derives a stable watermark value by solving linear
constraints constructed from model invariants. Moreover, this technology
utilizes noise mechanism to hide watermark locations in multi-user scenarios
against collusion attack. This paper evaluates the approach on three popular
models (Llama3, Phi3, Gemma), and the experimental results confirm the strong
robustness across a range of attack methods (fine-tuning, pruning,
quantization, permutation, scaling, reversible matrix and collusion attacks).

</details>


### [5] [Evaluating Post-Quantum Cryptographic Algorithms on Resource-Constrained Devices](https://arxiv.org/abs/2507.08312)
*Jesus Lopez,Viviana Cadena,Mohammad Saidur Rahman*

Main category: cs.CR

TL;DR: This paper evaluates post-quantum cryptography (PQC) algorithms BIKE, CRYSTALS-Kyber, and HQC on Raspberry Pi-based IoT devices, confirming their practical deployment despite constrained resources through performance analysis.


<details>
  <summary>Details</summary>
Motivation: Quantum computing threatens classical cryptographic methods (RSA, ECC) in IoT devices, which rely on secure but resource-constrained communication systems. Transitioning to quantum-secure protocols is urgently needed.

Method: 1. Implemented BIKE, CRYSTALS-Kyber, HQC on lightweight IoT devices using Raspberry Pi hardware. 2. Integrated Open Quantum Safe (liboqs) with mbedTLS. 3. Evaluated computational overhead, memory usage, and energy consumption for key exchange protocols.

Result: All three PQC algorithms achieved practical performance on constrained hardware: - Computational overhead, memory usage, and energy consumption measurements captured - Protocols demonstrated functional feasibility in real-world IoT environments - Direct comparison of resource requirements between BIKE, Kyber, and HQC

Conclusion: Quantum-secure cryptographic implementation is achievable in IoT devices despite resource limitations. Next-generation IoT systems require immediate integration of PQC frameworks to ensure long-term security against quantum threats.

Abstract: The rapid advancement of quantum computing poses a critical threat to
classical cryptographic algorithms such as RSA and ECC, particularly in
Internet of Things (IoT) devices, where secure communication is essential but
often constrained by limited computational resources. This paper investigates
the feasibility of deploying post-quantum cryptography (PQC) algorithms on
resource-constrained devices. In particular, we implement three PQC algorithms
-- BIKE, CRYSTALS-Kyber, and HQC -- on a lightweight IoT platform built with
Raspberry Pi devices. Leveraging the Open Quantum Safe (\texttt{liboqs})
library in conjunction with \texttt{mbedTLS}, we develop quantum-secure key
exchange protocols, and evaluate their performance in terms of computational
overhead, memory usage, and energy consumption for quantum secure
communication. Experimental results demonstrate that the integration of PQC
algorithms on constrained hardware is practical, reinforcing the urgent need
for quantum-resilient cryptographic frameworks in next-generation IoT devices.
The implementation of this paper is available at
https://iqsec-lab.github.io/PQC-IoT/.

</details>


### [6] [Qualcomm Trusted Application Emulation for Fuzzing Testing](https://arxiv.org/abs/2507.08331)
*Chun-I Fan,Li-En Chang,Cheng-Han Shie*

Main category: cs.CR

TL;DR: This paper proposes a novel lightweight emulator for Qualcomm Trusted Execution Environment (TEE) applications, combining reverse engineering and fuzzing testing to identify vulnerabilities efficiently.


<details>
  <summary>Details</summary>
Motivation: Vulnerabilities in TEEs pose significant risks to user data and privacy. Existing security testing methods for Qualcomm TAs rely on complex full-system simulations, necessitating a more practical and resource-efficient approach.

Method: The authors use reverse engineering techniques to analyze Qualcomm TAs and construct a partial emulation environment. They further integrate fuzzing techniques into the emulator to systematically detect vulnerabilities.

Result: The developed emulator successfully identifies real-world security flaws in Qualcomm TAs and provides implementation details and source code, marking the first such open solution for Qualcomm TEE applications.

Conclusion: The research establishes a practical, scalable framework for TEE security testing, offering both a reference implementation and a convenient alternative to traditional simulation-based approaches.

Abstract: In recent years, the increasing awareness of cybersecurity has led to a
heightened focus on information security within hardware devices and products.
Incorporating Trusted Execution Environments (TEEs) into product designs has
become a standard practice for safeguarding sensitive user information.
However, vulnerabilities within these components present significant risks, if
exploited by attackers, these vulnerabilities could lead to the leakage of
sensitive data, thereby compromising user privacy and security. This research
centers on trusted applications (TAs) within the Qualcomm TEE and introduces a
novel emulator specifically designed for these applications. Through reverse
engineering techniques, we thoroughly analyze Qualcomm TAs and develop a
partial emulation environment that accurately emulates their behavior.
Additionally, we integrate fuzzing testing techniques into the emulator to
systematically uncover potential vulnerabilities within Qualcomm TAs,
demonstrating its practical effectiveness in identifying real-world security
flaws. This research makes a significant contribution by being the first to
provide both the implementation methods and source codes for a Qualcomm TAs
emulator, offering a valuable reference for future research efforts. Unlike
previous approaches that relied on complex and resource-intensive full-system
simulations, our approach is lightweight and effective, making security testing
of TA more convenient.

</details>


### [7] [White-Basilisk: A Hybrid Model for Code Vulnerability Detection](https://arxiv.org/abs/2507.08540)
*Ioannis Lamprou,Alexander Shevtsov,Ioannis Arapakis,Sotiris Ioannidis*

Main category: cs.CR

TL;DR: White-Basilisk is a 200M-parameter vulnerability detection model using Mamba layers, linear self-attention, and Mixture of Experts to process long code sequences efficiently, outperforming current LLMs in specialized tasks.


<details>
  <summary>Details</summary>
Motivation: Software vulnerabilities require improved detection methods. Existing LLMs struggle with long code contexts and have overestimated scaling assumptions.

Method: Combines Mamba sequence layers, linear self-attention for long-range analysis, and a Mixture of Experts framework, enabling 200M-parameter efficient processing of extensive codebases.

Result: Achieves state-of-the-art vulnerability detection performance, outperforms LLMs in imbalanced real-world datasets, and processes long code sequences in one pass.

Conclusion: Demonstrates compact model designs can surpass larger models in domain-specific cybersecurity tasks, providing empirical evidence to rethink AI scaling assumptions for security applications.

Abstract: The proliferation of software vulnerabilities presents a significant
challenge to cybersecurity, necessitating more effective detection
methodologies. We introduce White-Basilisk, a novel approach to vulnerability
detection that demonstrates superior performance while challenging prevailing
assumptions in AI model scaling. Utilizing an innovative architecture that
integrates Mamba layers, linear self-attention, and a Mixture of Experts
framework, White-Basilisk achieves state-of-the-art results in vulnerability
detection tasks with a parameter count of only 200M. The model's capacity to
process sequences of unprecedented length enables comprehensive analysis of
extensive codebases in a single pass, surpassing the context limitations of
current Large Language Models (LLMs). White-Basilisk exhibits robust
performance on imbalanced, real-world datasets, while maintaining computational
efficiency that facilitates deployment across diverse organizational scales.
This research not only establishes new benchmarks in code security but also
provides empirical evidence that compact, efficiently designed models can
outperform larger counterparts in specialized tasks, potentially redefining
optimization strategies in AI development for domain-specific applications.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [8] [The State of Computational Science in Fission and Fusion Energy](https://arxiv.org/abs/2507.08061)
*Andrea Morales Coto,Aditi Verma*

Main category: cs.SE

TL;DR: The 2024 survey of 103 computational scientists in nuclear energy reveals a shift toward modern programming languages (e.g., Python, C++), open-source, modular software, and growing development budgets (up to $50M) in fusion and fission energy engineering.


<details>
  <summary>Details</summary>
Motivation: Software tools are critical in fusion/fission energy engineering, as they shape what is engineerable and remain underexplored in industry trends.

Method: Survey of 103 computational scientists from US national labs and universities, analyzing their tool preferences and developer experiences.

Result: Rising adoption of Python/C++ over FORTRAN, increased use of modular/multiphysics codes, open-source software popularity, and substantial development budgets (up to $50M).

Conclusion: Nuclear energy codes are evolving toward modular, compute-efficient, and industry-prioritized designs, reflecting cutting-edge trends in software-driven engineering.

Abstract: The tools used to engineer something are just as important as the thing that
is actually being engineered. In fact, in many cases, the tools can indeed
determine what is engineerable. In fusion and fission1 energy engineering,
software has become the dominant tool for design. For that reason, in 2024, for
the first time ever, we asked 103 computational scientists developing the codes
used in fusion and fission energy about the problems they are attempting to
solve with their codes, the tools available to them to solve them, and their
end to end developer experience with said tools.
  The results revealed a changing tide in software tools in fusion and fission,
with more and more computational scientists preferring modern programming
languages, open-source codes, and modular software. These trends represent a
peek into what will happen 5 to 10 years in the future of nuclear engineering.
Since the majority of our respondents belonged to US national labs and
universities, these results hint at the most cutting-edge trends in the
industry. The insights included in the State of Computational Science in
Fission and Fusion Energy indicate a dramatic shift toward multiphysics codes,
a drop-off in the use of FORTRAN in favor of more modern languages like Python
and C++, and ever-rising budgets for code development, at times reaching $50M
in a single organization.
  Our survey paints a future of nuclear engineering codes that is modular in
nature, small in terms of compute, and increasingly prioritized by
organizations. Access to our results in web form are available online.

</details>


### [9] [Code with Me or for Me? How Increasing AI Automation Transforms Developer Workflows](https://arxiv.org/abs/2507.08149)
*Valerie Chen,Ameet Talwalkar,Robert Brennan,Graham Neubig*

Main category: cs.SE

TL;DR: This paper examines how autonomous coding agents impact developer productivity and experience compared to copilots like GitHub Copilot, identifying both benefits (task completion beyond human capability, reduced effort) and challenges (agent behavior understanding) in their adoption through empirical evaluation involving real developers.


<details>
  <summary>Details</summary>
Motivation: While prior research has explored developer interactions with copilots, autonomous coding agents remain largely unexamined in human-in-the-loop studies. The paper motivates research by highlighting the gap in understanding how agentic AI tools alter developer workflows and how their broader adoption might be facilitated.

Method: An empirical comparison of two systems: GitHub Copilot (copilot-like assistant) and OpenHands (agentic assistant). Participants regularly using GitHub Copilot were recruited to evaluate both tools, focusing on task completion, productivity, and user experience through direct human interaction rather than static benchmarks.

Result: Agents outperformed copilots in enabling task completion (e.g., achieving results developers couldn't achieve independently) and reducing human effort. However, challenges in understanding agent behaviors emerged as a critical barrier to adoption. The study characterizes behavioral patterns and identifies fundamental differences in how developers interact with agentic vs. copilot tools.

Conclusion: The work reveals significant opportunities for coding agents to enhance software development while emphasizing critical adoption challenges that must be addressed. Key insights include the need for better agent transparency, workflow adaptation requirements, and a framework of recommendations for building future agentic systems that bridge current capability-expectation gaps among developers.

Abstract: Developers now have access to a growing array of increasingly autonomous AI
tools to support software development. While numerous studies have examined
developer use of copilots, which can provide chat assistance or code
completions, evaluations of coding agents, which can automatically write files
and run code, still largely rely on static benchmarks without
humans-in-the-loop. In this work, we conduct the first academic study to
explore developer interactions with coding agents and characterize how more
autonomous AI tools affect user productivity and experience, compared to
existing copilots. We evaluate two leading copilot and agentic coding
assistants, GitHub Copilot and OpenHands, recruiting participants who regularly
use the former. Our results show agents have the potential to assist developers
in ways that surpass copilots (e.g., completing tasks that humans might not
have accomplished before) and reduce the user effort required to complete
tasks. However, there are challenges involved in enabling their broader
adoption, including how to ensure users have an adequate understanding of agent
behaviors. Our results not only provide insights into how developer workflows
change as a result of coding agents but also highlight how user interactions
with agents differ from those with existing copilots, motivating a set of
recommendations for researchers building new agents. Given the broad set of
developers who still largely rely on copilot-like systems, our work highlights
key challenges of adopting more agentic systems into developer workflows.

</details>


### [10] [The Impact of Generative AI on Code Expertise Models: An Exploratory Study](https://arxiv.org/abs/2507.08160)
*Otávio Cury,Guilherme Avelino*

Main category: cs.SE

TL;DR: The paper explores how generative AI (GenAI) tools in code generation may reduce developer understanding of code, affecting expertise metrics like knowledge models and the Truck Factor algorithm.


<details>
  <summary>Details</summary>
Motivation: Developers' over-reliance on GenAI tools like ChatGPT poses risks to maintainable software development by creating knowledge gaps, yet the impact on expertise measurement remains understudied.

Method: The researchers collected GitHub integration data for ChatGPT-generated code and simulated GenAI contribution levels to assess their effects on expertise metrics.

Result: Simulations showed measurable impacts on current expertise metrics, indicating their sensitivity to decreased code understanding from GenAI reliance.

Conclusion: As GenAI integration grows, current methods for evaluating developer expertise may become less reliable, requiring updated metrics.

Abstract: Generative Artificial Intelligence (GenAI) tools for source code generation
have significantly boosted productivity in software development. However, they
also raise concerns, particularly the risk that developers may rely heavily on
these tools, reducing their understanding of the generated code. We hypothesize
that this loss of understanding may be reflected in source code knowledge
models, which are used to identify developer expertise. In this work, we
present an exploratory analysis of how a knowledge model and a Truck Factor
algorithm built upon it can be affected by GenAI usage. To investigate this, we
collected statistical data on the integration of ChatGPT-generated code into
GitHub projects and simulated various scenarios by adjusting the degree of
GenAI contribution. Our findings reveal that most scenarios led to measurable
impacts, indicating the sensitivity of current expertise metrics. This suggests
that as GenAI becomes more integrated into development workflows, the
reliability of such metrics may decrease.

</details>


### [11] [Leveraging Large Language Models for Classifying App Users' Feedback](https://arxiv.org/abs/2507.08250)
*Yasaman Abedini,Abbas Heydarnoori*

Main category: cs.SE

TL;DR: This paper evaluates four large language models (GPT-3.5-Turbo, GPT-4, Flan-T5, Llama3-70b) as annotation tools to augment user feedback datasets and improve BERT-based classification models when labeled data is limited.


<details>
  <summary>Details</summary>
Motivation: Creating large, accurately labeled user feedback datasets for supervised machine learning is time-consuming and resource-intensive, necessitating more generalizable solutions for feedback classification.

Method: The authors conducted experiments on eight well-labeled user feedback datasets (including app store reviews, X posts, and public forums) to assess LLM performance in coarse-grained and fine-grained classification tasks. LLM-generated labels were used to augment training data for state-of-the-art BERT models due to computational constraints and the high volume of user feedback.

Result: LLMs demonstrated effectiveness in coarse-grained classification when using well-crafted prompts. Augmenting datasets with LLM-labeled data using consensus improved classifier performance significantly, particularly under limited labeled data conditions.

Conclusion: Advanced LLMs can serve as viable annotation tools to address dataset limitations in user feedback classification. When leveraged strategically with prompt engineering and consensus-based labeling, they enhance the performance of BERT-based models with reduced dependency on large human-labeled datasets.

Abstract: In recent years, significant research has been conducted into classifying
application (app) user feedback, primarily relying on supervised machine
learning algorithms. However, fine-tuning more generalizable classifiers based
on existing labeled datasets remains an important challenge, as creating large
and accurately labeled datasets often requires considerable time and resources.
In this paper, we evaluate the capabilities of four advanced LLMs, including
GPT-3.5-Turbo, GPT-4, Flan-T5, and Llama3-70b, to enhance user feedback
classification and address the challenge of the limited labeled dataset. To
achieve this, we conduct several experiments on eight datasets that have been
meticulously labeled in prior research. These datasets include user reviews
from app stores, posts from the X platform, and discussions from the public
forums, widely recognized as representative sources of app user feedback. We
analyze the performance of various LLMs in identifying both fine-grained and
coarse-grained user feedback categories. Given the substantial volume of daily
user feedback and the computational limitations of LLMs, we leverage these
models as an annotation tool to augment labeled datasets with general and
app-specific data. This augmentation aims to enhance the performance of
state-of-the-art BERT-based classification models. Our findings indicate that
LLMs when guided by well-crafted prompts, can effectively classify user
feedback into coarse-grained categories. Moreover, augmenting the training
dataset with datasets labeled using the consensus of LLMs can significantly
enhance classifier performance.

</details>


### [12] [Computing Floating-Point Errors by Injecting Perturbations](https://arxiv.org/abs/2507.08467)
*Youshuai Tan,Zhanwei Zhang,Jinfu Chen,Zishuo Ding,Jifeng Xuan,Weiyi Shang*

Main category: cs.SE

TL;DR: The paper proposes PI-detector, a new method for efficiently computing floating-point errors in programs by perturbing atomic operations and comparing results, addressing limitations of existing tools like ATOMU and FPCC that face false positives or slow performance.


<details>
  <summary>Details</summary>
Motivation: Floating-point errors in scientific and engineering applications can have severe consequences, yet current error detection methods relying on high-precision oracles are either inaccurate (false positives) or computationally expensive, necessitating a more effective and efficient solution.

Method: PI-detector injects small perturbations into operands of atomic operations (addition/subtraction) in the program, then computes and compares the program's original output against the perturbed version to identify floating-point errors.

Result: Experimental evaluations on ATOMU, HSED datasets, and a complex linear system-solving program demonstrate that PI-detector achieves efficient and accurate floating-point error computation without the limitations of previous tools.

Conclusion: PI-detector effectively addresses floating-point error detection by leveraging operand perturbation in atomic operations, offering a faster and more precise alternative to existing solutions.

Abstract: Floating-point programs form the foundation of modern science and
engineering, providing the essential computational framework for a wide range
of applications, such as safety-critical systems, aerospace engineering, and
financial analysis. Floating-point errors can lead to severe consequences.
Although floating-point errors widely exist, only a subset of inputs may
trigger significant errors in floating-point programs. Therefore, it is crucial
to determine whether a given input could produce such errors. Researchers tend
to take the results of high-precision floating-point programs as oracles for
detecting floating-point errors, which introduces two main limitations: (1)
difficulty of implementation and (2) prolonged execution time. The two recent
tools, ATOMU and FPCC, can partially address these issues. However, ATOMU
suffers from false positives; while FPCC, though eliminating false positives,
operates at a considerably slower speed.
  To address these two challenges, we propose a novel approach named
PI-detector to computing floating-point errors effectively and efficiently. Our
approach is based on the observation that floating-point errors stem from large
condition numbers in atomic operations (such as addition and subtraction),
which then propagate and accumulate. PI-detector injects small perturbations
into the operands of individual atomic operations within the program and
compares the outcomes of the original program with the perturbed version to
compute floating-point errors. We evaluate PI-detector with datasets from ATOMU
and HSED, as well as a complex linear system-solving program. Experimental
results demonstrate that PI-detector can perform efficient and accurate
floating-point error computation.

</details>


### [13] [InferLog: Accelerating LLM Inference for Online Log Parsing via ICL-oriented Prefix Caching](https://arxiv.org/abs/2507.08523)
*Yilun Wang,Pengfei Chen,Haiyu Huang,Zilong He,Gou Tan,Chuanfu Zhang,Jingkai He,Zibin Zheng*

Main category: cs.SE

TL;DR: InferLog is an LLM inference optimization method for online log parsing that addresses privacy and latency issues in production environments without compromising accuracy.


<details>
  <summary>Details</summary>
Motivation: Commercial LLM log parsers face deployment challenges due to privacy risks and latency/throughput limitations. Existing optimization methods reduce query counts but overlook per-invocation latency exacerbating performance under concurrent workloads.

Method: 1) Prefix-aware ICL Refinement policy to enhance prefix caching efficiency via example refinement and permutation. 2) Meta-learning pipeline for task-specific configuration tuning to find optimal scheduling parameters for dynamic workloads.

Result: Experiments on Loghub and vLLM show InferLog outperforms existing optimization methods by significantly accelerating state-of-the-art LLM-based parsing while maintaining accuracy.

Conclusion: InferLog identifies inference efficiency as the critical bottleneck in online log parsing, offering an effective optimization framework that makes LLM deployment practical for high-volume log streams with real-time constraints.

Abstract: Modern software systems generate massive volumes of runtime logs,
necessitating efficient and accurate log parsing to enable critical downstream
tasks such as anomaly detection and root cause analysis. Recently, large
language models (LLMs) have achieved advanced accuracy on log parsing, but
their deployment in production environments faces two major limitations: (1)
the privacy risks associated with commercial LLMs, driving the adoption of
local deployment, and (2) the stringent latency and throughput requirements
imposed by high-volume log streams, which existing LLM-based parsers fail to
meet. Although recent efforts have reduced the number of LLM queries, they
overlook the high latency of the LLM invocations, where concurrent log parsing
requests can cause serve performance degradation of LLM inference system.
  In this study, we present InferLog, the first LLM inference optimization
method for online log parsing. Our key insight is that the inference efficiency
emerges as the vital bottleneck in LLM-based online log parsing, rather than
parsing accuracy. InferLog accelerates inference by designing (1) A
Prefix-aware ICL Refinement policy to refine the examples and permutation of
in-context learning to improve the prefix caching efficiency. (2) A rapid and
task-specific configuration tuning pipeline based on meta-learning to find the
optimal LLM scheduling-related configuration for dynamic log parsing workloads.
The experimental results based on Loghub dataset and vLLM demonstrate that
InferLog significantly outperforms existing inference optimization methods and
markedly accelerates the state-of-the-art LLM-based log parser without
compromising parsing accuracy.

</details>


### [14] [Generating Proto-Personas through Prompt Engineering: A Case Study on Efficiency, Effectiveness and Empathy](https://arxiv.org/abs/2507.08594)
*Fernando Ayach,Vitor Lameirão,Raul Leão,Jerfferson Felizardo,Rafael Sobrinho,Vanessa Borges,Patrícia Matsubara,Awdren Fontão*

Main category: cs.SE

TL;DR: This paper proposes a prompt engineering-based Generative AI (GenAI) approach for creating proto-personas in software Product Discovery, validated through a case study with 19 participants in a real Lean Inception.


<details>
  <summary>Details</summary>
Motivation: Manual proto-persona creation for Product Discovery is time-intensive, cognitively demanding, and prone to bias, motivating the need for a GenAI-supported approach to enhance efficiency and objectivity.

Method: The authors developed and evaluated a GenAI-based proto-persona generation approach using prompt engineering, employing mixed methods (qualitative and quantitative) in a real-world case study during Lean Inception with 19 participants.

Result: The GenAI approach reduced creation time/effort, improved persona quality and reusability for downstream tasks (MVP scoping, feature refinement), achieved high user acceptance (usefulness/ease of use), but showed limitations in generalization and domain specificity. Empathy outcomes were mixed, with cognitive empathy strongly supported but affective/behavioral empathy varying.

Conclusion: GenAI can effectively enhance Product Discovery by generating proto-personas more efficiently, though challenges remain in generalization and balanced empathy elicitation. The study provides empirical guidance for hybrid AI-driven design processes.

Abstract: Proto-personas are commonly used during early-stage Product Discovery, such
as Lean Inception, to guide product definition and stakeholder alignment.
However, the manual creation of proto-personas is often time-consuming,
cognitively demanding, and prone to bias. In this paper, we propose and
empirically investigate a prompt engineering-based approach to generate
proto-personas with the support of Generative AI (GenAI). Our goal is to
evaluate the approach in terms of efficiency, effectiveness, user acceptance,
and the empathy elicited by the generated personas. We conducted a case study
with 19 participants embedded in a real Lean Inception, employing a qualitative
and quantitative methods design. The results reveal the approach's efficiency
by reducing time and effort and improving the quality and reusability of
personas in later discovery phases, such as Minimum Viable Product (MVP)
scoping and feature refinement. While acceptance was generally high, especially
regarding perceived usefulness and ease of use, participants noted limitations
related to generalization and domain specificity. Furthermore, although
cognitive empathy was strongly supported, affective and behavioral empathy
varied significantly across participants. These results contribute novel
empirical evidence on how GenAI can be effectively integrated into software
Product Discovery practices, while also identifying key challenges to be
addressed in future iterations of such hybrid design processes.

</details>


### [15] [NL in the Middle: Code Translation with LLMs and Intermediate Representations](https://arxiv.org/abs/2507.08627)
*Chi-en Amy Tai,Pengyu Nie,Lukasz Golab,Alexander Wong*

Main category: cs.SE

TL;DR: The paper investigates leveraging intermediate representations (natural language summaries and abstract syntax trees) in code translation LLMs via prompt engineering. CoT prompting with natural language summaries improved translation success rates by 13.8% and 6.7% on CodeNet and AVATAR benchmarks.


<details>
  <summary>Details</summary>
Motivation: LLMs frequently generate buggy code translations, suggesting a need for architectural or prompting innovations that provide structured guidance during translation.

Method: Experimental analysis of one-shot vs chain-of-thought (CoT) prompting methods with Open Gpt4 8X7B, StarCoder, and CodeGen models on CodeNet and AVATAR code translation benchmarks.

Result: CoT with intermediate natural language summaries achieved highest gains, with Open Gpt4 8X7B showing 13.8% and 6.7% increases in successful translations on CodeNet and AVATAR compared to zero-shot baselines.

Conclusion: Prompt engineering that incorporates intermediate representations, particularly natural language summaries in chain-of-thought format, significantly improves code translation accuracy for large language models.

Abstract: Studies show that large language models (LLMs) produce buggy code
translations. One avenue to improve translation accuracy is through
intermediate representations, which could provide structured insights to guide
the model's understanding. We explore whether code translation using LLMs can
benefit from intermediate representations via natural language (NL) and
abstract syntax trees (ASTs). Since prompt engineering greatly affects LLM
performance, we consider several ways to integrate these representations, from
one-shot to chain-of-thought (CoT) prompting. Using Open Gpt4 8X7B and
specialized StarCoder and CodeGen models on popular code translation benchmarks
(CodeNet and AVATAR), we find that CoT with an intermediate NL summary performs
best, with an increase of 13.8% and 6.7%, respectively, in successful
translations for the best-performing model (Open Gpt4 8X7B) compared to the
zero-shot prompt.

</details>


### [16] [LLMCup: Ranking-Enhanced Comment Updating with LLMs](https://arxiv.org/abs/2507.08671)
*Hua Ge,Juan Zhai,Minxue Pan,Fusen He,Ziyue Tan*

Main category: cs.SE

TL;DR: LLMCup is a novel comment updating framework using diverse LLM prompts and a ranking model (CupRank) to improve accuracy and quality, outperforming CUP/HebCup by 49.0%-116.9% in accuracy with human evaluation surpassing in some cases.


<details>
  <summary>Details</summary>
Motivation: Existing comment-updating methods (e.g., CUP, HebCup) often miss/misinterpret code logic during updates, leading to inaccurate documentation for complex software changes.

Method: 1) Generates diverse candidate comments via LLM using multiple prompt strategies 2) Applies CupRank ranking model to select optimal output

Result: 49.0%-116.9% accuracy improvement over SOTA baselines, 10.8%-20% BLEU-4 gain, and user studies showing AI-generated comments outperforming human-written updates in certain cases

Conclusion: LLMs with strategic prompting can surpass existing methods in comment updating, but require effective diversity/ranking mechanisms to maximize utility while emphasizing the need for human evaluation in quality assessment

Abstract: While comments are essential for enhancing code readability and
maintainability in modern software projects, developers are often motivated to
update code but not comments, leading to outdated or inconsistent documentation
that hinders future understanding and maintenance. Recent approaches such as
CUP and HebCup have attempted automatic comment updating using neural
sequence-to-sequence models and heuristic rules, respectively. However, these
methods can miss or misinterpret crucial information during comment updating,
resulting in inaccurate comments, and they often struggle with complex update
scenarios. Given these challenges, a promising direction lies in leveraging
large language models (LLMs), which have shown impressive performance in
software engineering tasks such as comment generation, code synthesis, and
program repair. This suggests their strong potential to capture the logic
behind code modifications - an ability that is crucial for the task of comment
updating. Nevertheless, selecting an appropriate prompt strategy for an LLM on
each update case remains challenging. To address this, we propose a novel
comment updating framework, LLMCup, which first uses multiple prompt strategies
to provide diverse candidate updated comments via an LLM, and then employs a
ranking model, CupRank, to select the best candidate as final updated comment.
Experimental results demonstrate the effectiveness of LLMCup, with improvements
over state-of-the-art baselines (CUP and HebCup) by 49.0%-116.9% in Accuracy,
10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in
SentenceBert similarity. Furthermore, a user study shows that comments updated
by LLMCup sometimes surpass human-written updates, highlighting the importance
of incorporating human evaluation in comment quality assessment.

</details>


### [17] [Dually Hierarchical Drift Adaptation for Online Configuration Performance Learning](https://arxiv.org/abs/2507.08730)
*Zezhen Xiang,Jingzhi Gong,Tao Chen*

Main category: cs.SE

TL;DR: DHDA is an online configuration performance learning framework that addresses global and local concept drifts in dynamic environments using dually hierarchical adaptation, achieving 2x accuracy improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing offline/transfer learning approaches struggle with real-time adaptation to global (entire performance landscape) and local (sub-region) concept drifts caused by workload changes, hardware updates, and system dynamics.

Method: Dually hierarchical framework: upper-level periodically re-divides data into divisions for global drift adaptation, while lower-level local models handle local drifts via asynchronous updates. Combines incremental learning with periodic full retraining to balance efficiency.

Result: Outperforms state-of-the-art methods on eight software systems by 2x in accuracy, adapts effectively to drifts with reasonable overhead, and demonstrates improved local model handling of concept drift.

Conclusion: DHDA successfully addresses multi-level concept drifts in configuration performance learning through hierarchical adaptation, offering superior accuracy and adaptability while maintaining computational efficiency in dynamic environments.

Abstract: Modern configurable software systems need to learn models that correlate
configuration and performance. However, when the system operates in dynamic
environments, the workload variations, hardware changes, and system updates
will inevitably introduce concept drifts at different levels - global drifts,
which reshape the performance landscape of the entire configuration space; and
local drifts, which only affect certain sub-regions of that space. As such,
existing offline and transfer learning approaches can struggle to adapt to
these implicit and unpredictable changes in real-time, rendering configuration
performance learning challenging. To address this, we propose DHDA, an online
configuration performance learning framework designed to capture and adapt to
these drifts at different levels. The key idea is that DHDA adapts to both the
local and global drifts using dually hierarchical adaptation: at the upper
level, we redivide the data into different divisions, within each of which the
local model is retrained, to handle global drifts only when necessary. At the
lower level, the local models of the divisions can detect local drifts and
adapt themselves asynchronously. To balance responsiveness and efficiency, DHDA
combines incremental updates with periodic full retraining to minimize
redundant computation when no drifts are detected. Through evaluating eight
software systems and against state-of-the-art approaches, we show that DHDA
achieves considerably better accuracy and can effectively adapt to drifts with
up to 2x improvements, while incurring reasonable overhead and is able to
improve different local models in handling concept drift.

</details>
