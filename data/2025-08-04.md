<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 11]
- [cs.SE](#cs.SE) [Total: 19]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [ranDecepter: Real-time Identification and Deterrence of Ransomware Attacks](https://arxiv.org/abs/2508.00293)
*Md Sajidul Islam Sajid,Jinpeng Wei,Ehab Al-Shaer*

Main category: cs.CR

TL;DR: ranDecepter combines active cyber deception and real-time analysis to effectively combat ransomware by depleting attacker resources through false data transmission.


<details>
  <summary>Details</summary>
Motivation: Ransomware is a significant threat requiring proactive countermeasures, and active deception can reveal malicious behaviors while depleting attackers' resources by exploiting the communication channel between them and the system.

Method: ranDecepter identifies ransomware in real-time, isolates it in a deceptive environment, creates code-based loop mechanisms, and repeatedly restarts malware to transmit counterfeit encryption information and secret keys to attackers.

Result: Achieved 100% ransomware identification accuracy with no false positives and negligible response time using 1,134 malware samples and 12 benign applications. 50 agents generated 9.223 million database entries for attackers within 24 hours.

Conclusion: By autonomously creating resource-depleting loops through deception, ranDecepter demonstrates a powerful strategy to disrupt ransomware attacks and limit their propagation.

Abstract: Ransomware (RW) presents a significant and widespread threat in the digital
landscape, necessitating effective countermeasures. Active cyber deception is a
promising strategy to thwart RW and limiting its propagation by misleading it
with false information and revealing its true behaviors. Furthermore, RW often
acts as a communication conduit between attackers and defenders, allowing
deception to return false data to attackers and deplete their resources. This
paper introduces ranDecepter, a novel approach that combines active cyber
deception with real-time analysis to enhance defenses against RW attacks. The
ranDecepter identifies RW in real-time and isolates it within a deceptive
environment, autonomously identifying critical elements in the RW code to
create a loop mechanism. By repeatedly restarting the malware and transmitting
counterfeit encryption information and secret keys to the attacker, it forces
the attacker to store these fabricated details for each victim, thereby
depleting their resources. Our comprehensive evaluation of ranDecepter,
conducted using 1,134 real-world malware samples and twelve benign
applications, demonstrates a remarkable 100% accuracy in RW identification,
with no false positives and minimal impact on response times. Furthermore,
within 24-hours, ranDecepter generates up to 9,223K entries in the attacker's
database using 50 agents, showcasing its potential to undermine attacker
resources.

</details>


### [2] [Cryptanalysis of Isogeny-Based Quantum Money with Rational Points](https://arxiv.org/abs/2508.00351)
*Hyeonhak Kim,Donghoe Heo,Seokhie Hong*

Main category: cs.CR

TL;DR: This paper presents a cryptanalysis method for elliptic curve quantum money schemes using division polynomials and quadratic twists, achieving a O(log^4p) speedup over brute-force attacks while maintaining exponential time complexity. It also develops a more efficient verification procedure for these systems.


<details>
  <summary>Details</summary>
Motivation: The work aims to evaluate security risks of Montgomery and Sharif's quantum money scheme based on class group actions, while exploring potential optimizations in verification processes inherent to quantum cryptography.

Method: The approach leverages efficient division polynomial evaluations with rational points coordinates and utilizes quadratic twists to enhance verification efficiency. The algorithm exploits structural properties of the class group action and superposition cardinality verification through these mathematical techniques.

Result: Demonstrated a O(log^4p) speedup factor in cryptanalysis compared to brute-force methods, while maintaining exponential time requirements for practical security. Achieved improved verification efficiency using quadratic twist properties for elliptic curve superpositions.

Conclusion: The research provides foundational insights into elliptic curve quantum cryptography security, showing both practical cryptanalytic improvements and verification optimizations. The approach contributes to understanding inherent trade-offs between security and efficiency in quantum money systems.

Abstract: Quantum money is the cryptographic application of the quantum no-cloning
theorem. It has recently been instantiated by Montgomery and Sharif (Asiacrypt
'24) from class group actions on elliptic curves. In this work, we propose a
concrete cryptanalysis by leveraging the efficiency of evaluating division
polynomials with the coordinates of rational points, offering a speedup of
O(log^4p) compared to the brute-force attack. Since our attack still requires
exponential time, it remains impractical to forge a quantum banknote.
Interestingly, due to the inherent properties of quantum money, our attack
method also results in a more efficient verification procedure. Our algorithm
leverages the properties of quadratic twists to utilize rational points in
verifying the cardinality of the superposition of elliptic curves. We expect
this approach to contribute to future research on elliptic-curve-based quantum
cryptography.

</details>


### [3] [Preliminary Investigation into Uncertainty-Aware Attack Stage Classification](https://arxiv.org/abs/2508.00368)
*Alessandro Gaudenzi,Lorenzo Nodari,Lance Kaplan,Alessandra Russo,Murat Sensoy,Federico Cerutti*

Main category: cs.CR

TL;DR: This paper proposes an evidential deep learning approach for APT attack stage inference and out-of-distribution detection, leveraging Dirichlet distributions to model predictive uncertainty.


<details>
  <summary>Details</summary>
Motivation: Traditional binary classification methods in intrusion detection fail to capture attack progression dynamics, yet response strategies require accurate stage inference (reconnaissance vs. exploitation vs. exfiltration) and robustness to adversarial tactic changes.

Method: The paper introduces a classification framework using Evidential Deep Learning (EDL) to output Dirichlet distribution parameters over attack stages, enabling simultaneous stage prediction and uncertainty quantification while detecting inputs outside the training distribution.

Result: Preliminary experiments in simulated environments demonstrate accurate stage inference with well-calibrated confidence scores, and effective detection of out-of-distribution inputs indicating evolving attacker tactics.

Conclusion: Uncertainty-aware models like EDL enable robust staged threat detection in dynamic adversarial environments, providing defenders with both attack stage inference and confidence calibration capabilities for more effective response decisions.

Abstract: Advanced Persistent Threats (APTs) represent a significant challenge in
cybersecurity due to their prolonged, multi-stage nature and the sophistication
of their operators. Traditional detection systems typically focus on
identifying malicious activity in binary terms (benign or malicious) without
accounting for the progression of an attack. However, effective response
strategies depend on accurate inference of the attack's current stage, as
countermeasures must be tailored to whether an adversary is in the early
reconnaissance phase or actively conducting exploitation or exfiltration. This
work addresses the problem of attack stage inference under uncertainty, with a
focus on robustness to out-of-distribution (OOD) inputs. We propose a
classification approach based on Evidential Deep Learning (EDL), which models
predictive uncertainty by outputting parameters of a Dirichlet distribution
over possible stages. This allows the system not only to predict the most
likely stage of an attack but also to indicate when it is uncertain or the
input lies outside the training distribution. Preliminary experiments in a
simulated environment demonstrate that the proposed model can accurately infer
the stage of an attack with calibrated confidence while effectively detecting
OOD inputs, which may indicate changes in the attackers' tactics. These results
support the feasibility of deploying uncertainty-aware models for staged threat
detection in dynamic and adversarial environments.

</details>


### [4] [Accurate Latent Inversion for Generative Image Steganography via Rectified Flow](https://arxiv.org/abs/2508.00434)
*Yuqi Qian,Yun Cao,Meiyang Lv,Haocheng Fu*

Main category: cs.CR

TL;DR: RF-Stego introduces PCLI and RF samplers to improve latent inversion accuracy in diffusion model-based steganography, achieving better performance than existing methods across multiple metrics.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion model steganography methods suffer from inaccurate latent inversion causing discrepancies between original and reconstructed latents, making message extraction unreliable.

Method: 1. PCLI imposes formal constraints to enforce path consistency between forward generation and latent inversion. 2. Replaces traditional unstable samplers with Rectified Flow (RF) to ensure theoretical reversibility and numerical stability during inversion.

Result: Experiments demonstrate RF-Stego surpasses state-of-the-art approaches in extraction accuracy, image quality (PSNR/SSIM), robustness, security, and generation efficiency on benchmark datasets.

Conclusion: RF-Stego effectively addresses latent inversion challenges in diffusion-based steganography through path consistency enforcement and stable sampling, establishing a new standard for accuracy and reliability in generative image steganography.

Abstract: Steganography based on diffusion models has attracted increasing attention
due to its ability to generate high-quality images and exhibit strong
robustness. In such approaches, the secret message is first embedded into the
initial latent variable, and then the stego image is generated through the
forward process. To extract the message, an inversion process is required to
reconstruct the latent variables from the received image. However, inaccurate
latent inversion leads to significant discrepancies between the reconstructed
and original latent variables, rendering message extraction infeasible. To
address this issue, we propose \textbf{RF-Stego}, a novel generative image
steganography method that enables accurate latent inversion and significantly
improves extraction accuracy. First, we develop the \textbf{P}ath
\textbf{C}onsistency \textbf{L}inear \textbf{I}nversion (\textbf{PCLI}), which
imposes formal constraints on the inversion process. By explicitly aligning it
with the forward generation path and modeling both directions along a shared
linear path, PCLI eliminates path mismatch and ensures path consistency
throughout the steganographic process. Second, through rigorous theoretical
proof, we demonstrate that \textbf{R}ectified \textbf{F}low \textbf{(RF)}
offers both theoretical reversibility and numerical stability in the inversion
process. Based on this, we replace traditional unstable samplers with RF
sampler which effectively improves the numerical precision of the inversion
process. Experimental results show RF-Stego outperforms state-of-the-art
methods in terms of extraction accuracy, image quality, robustness, security
and generation efficiency.

</details>


### [5] [CyGATE: Game-Theoretic Cyber Attack-Defense Engine for Patch Strategy Optimization](https://arxiv.org/abs/2508.00478)
*Yuning Jiang,Nay Oo,Qiaoran Meng,Lu Lin,Dusit Niyato,Zehui Xiong,Hoon Wei Lim,Biplab Sikdar*

Main category: cs.CR

TL;DR: CyGATE is a game-theoretic framework that uses large language models and threat intelligence for dynamic cyber attack and defense modeling, enhancing adaptability through real-time data and strategic foresight.


<details>
  <summary>Details</summary>
Motivation: Existing game-theoretic models for cyber defense rely on static assumptions and lack integration with real-time threat intelligence, reducing their ability to adapt to evolving attack scenarios and prioritize mitigations effectively.

Method: CyGATE models attacker-defender interactions as a partially observable stochastic game (POSG) over Cyber Kill Chain stages, incorporating LLMs with retrieval-augmented generation (RAG) for tactic selection and patch prioritization. Belief states represent uncertainty for both agents, enabling dynamic adaptation to observed adversary behavior and risk assessments.

Result: Evaluations in a dynamic patch scheduling scenario show CyGATE improves vulnerability prioritization by integrating time-critical threat intelligence, enables strategic foresight through attacker move anticipation, and optimizes attacker and defender resource usage efficiency.

Conclusion: The flexible architecture of CyGATE addresses limitations of static game-theoretic models by incorporating real-time threat intelligence and LLMs, demonstrating adaptability in cyber defense through three key advantages: dynamic threat integration, strategic foresight, and resource optimization efficiency.

Abstract: Modern cyber attacks unfold through multiple stages, requiring defenders to
dynamically prioritize mitigations under uncertainty. While game-theoretic
models capture attacker-defender interactions, existing approaches often rely
on static assumptions and lack integration with real-time threat intelligence,
limiting their adaptability. This paper presents CyGATE, a game-theoretic
framework modeling attacker-defender interactions, using large language models
(LLMs) with retrieval-augmented generation (RAG) to enhance tactic selection
and patch prioritization. Applied to a two-agent scenario, CyGATE frames cyber
conflicts as a partially observable stochastic game (POSG) across Cyber Kill
Chain stages. Both agents use belief states to navigate uncertainty, with the
attacker adapting tactics and the defender re-prioritizing patches based on
evolving risks and observed adversary behavior. The framework's flexible
architecture enables extension to multi-agent scenarios involving coordinated
attackers, collaborative defenders, or complex enterprise environments with
multiple stakeholders. Evaluated in a dynamic patch scheduling scenario, CyGATE
effectively prioritizes high-risk vulnerabilities, enhancing adaptability
through dynamic threat integration, strategic foresight by anticipating
attacker moves under uncertainty, and efficiency by optimizing resource use.

</details>


### [6] [Activation-Guided Local Editing for Jailbreaking Attacks](https://arxiv.org/abs/2508.00555)
*Jiecong Wang,Haoran Li,Hao Peng,Ziqian Zeng,Zihao Wang,Haohua Du,Zhengtao Yu*

Main category: cs.CR

TL;DR: AGILE is a two-stage jailbreaking framework that improves over existing methods by combining scenario-based context generation and hidden state-guided fine-grained edits, achieving state-of-the-art attack success rates and demonstrating strong transferability to black-box models.


<details>
  <summary>Details</summary>
Motivation: Existing jailbreak methods (token/prompt-level) suffer from incoherent input generation, poor transferability, or heavy manual effort. There's a need for scalable and effective techniques to robustly evaluate and improve model security.

Method: Stage 1: Scenario-based generation of context to rephrase malicious queries and obscure harmful intent. Stage 2: Model-based analysis of hidden states to guide precise input edits that steer internal representations toward benignness.

Result: 37.74% gain in attack success rate over strongest baselines with high transferability (black-box model compatibility) and maintained effectiveness against major defense mechanisms.

Conclusion: AGILE outperforms existing jailbreak approaches while uncovering critical limitations in current model defenses, providing valuable insights for developing more robust safety mechanisms in AI systems.

Abstract: Jailbreaking is an essential adversarial technique for red-teaming these
models to uncover and patch security flaws. However, existing jailbreak methods
face significant drawbacks. Token-level jailbreak attacks often produce
incoherent or unreadable inputs and exhibit poor transferability, while
prompt-level attacks lack scalability and rely heavily on manual effort and
human ingenuity. We propose a concise and effective two-stage framework that
combines the advantages of these approaches. The first stage performs a
scenario-based generation of context and rephrases the original malicious query
to obscure its harmful intent. The second stage then utilizes information from
the model's hidden states to guide fine-grained edits, effectively steering the
model's internal representation of the input from a malicious toward a benign
one. Extensive experiments demonstrate that this method achieves
state-of-the-art Attack Success Rate, with gains of up to 37.74% over the
strongest baseline, and exhibits excellent transferability to black-box models.
Our analysis further demonstrates that AGILE maintains substantial
effectiveness against prominent defense mechanisms, highlighting the
limitations of current safeguards and providing valuable insights for future
defense development. Our code is available at
https://github.com/yunsaijc/AGILE.

</details>


### [7] [LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection and Leakage Attacks](https://arxiv.org/abs/2508.00602)
*Francesco Panebianco,Stefano Bonfanti,Francesco Trovò,Michele Carminati*

Main category: cs.CR

TL;DR: The paper addresses security threats in Large Language Models (LLMs), such as jailbreaking and data leakage, by introducing a methodology to create topic-categorized usage maps and a model-agnostic framework called LeakSealer. LeakSealer combines static analysis and dynamic defenses in a Human-In-The-Loop pipeline to detect adversarial interactions and PII leakage. Evaluations show high precision/recall for prompt injection detection and an AUPRC of 0.97 for PII leakage, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: The deployment of LLMs has introduced security risks like jailbreaking and data leakage, particularly when using Retrieval Augmented Generation (RAG). These vulnerabilities compromise sensitive information and require proactive mitigation strategies.

Method: 1) A methodology for analyzing historical LLM interaction data to generate topic-based usage maps and track adversarial interaction patterns. 2) LeakSealer, a model-agnostic framework combining static analysis (for forensic insights) and dynamic defenses (in HITL pipelines) to identify high-risk topic groups and detect anomalies.

Result: LeakSealer achieved: 
- Highest precision and recall on ToxicChat dataset for static prompt injection detection
- 0.97 AUPRC for dynamic PII leakage detection
- Outperformed baselines like Llama Guard in both metrics

Conclusion: The paper provides a comprehensive security analysis framework for LLMs. LeakSealer demonstrates strong effectiveness as a hybrid static-dynamic defense system, achieving state-of-the-art performance in identifying adversarial behavior and preventing data leakage through continuous monitoring and human oversight.

Abstract: The generalization capabilities of Large Language Models (LLMs) have led to
their widespread deployment across various applications. However, this
increased adoption has introduced several security threats, notably in the
forms of jailbreaking and data leakage attacks. Additionally, Retrieval
Augmented Generation (RAG), while enhancing context-awareness in LLM responses,
has inadvertently introduced vulnerabilities that can result in the leakage of
sensitive information. Our contributions are twofold. First, we introduce a
methodology to analyze historical interaction data from an LLM system, enabling
the generation of usage maps categorized by topics (including adversarial
interactions). This approach further provides forensic insights for tracking
the evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a
model-agnostic framework that combines static analysis for forensic insights
with dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique
identifies topic groups and detects anomalous patterns, allowing for proactive
defense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1)
jailbreak attempts, employing a public benchmark dataset, and (2) PII leakage,
supported by a curated dataset of labeled LLM interactions. In the static
setting, LeakSealer achieves the highest precision and recall on the ToxicChat
dataset when identifying prompt injection. In the dynamic setting, PII leakage
detection achieves an AUPRC of $0.97$, significantly outperforming baselines
such as Llama Guard.

</details>


### [8] [FedGuard: A Diverse-Byzantine-Robust Mechanism for Federated Learning with Major Malicious Clients](https://arxiv.org/abs/2508.00636)
*Haocheng Jiang,Hua Shen,Jixin Zhang,Willy Susilo,Mingwu Zhang*

Main category: cs.CR

TL;DR: The paper proposes FedGuard, a novel federated learning mechanism that addresses vulnerabilities to Byzantine attacks by using membership inference sensitivity to model bias, significantly outperforming existing defenses under high non-IID data and majority-malicious scenarios.


<details>
  <summary>Details</summary>
Motivation: Federated learning frameworks are susceptible to Byzantine attacks, especially when datasets are non-IID or over 50% of clients are malicious. Current defenses are often attack-specific, limiting their effectiveness against diverse attack types.

Method: FedGuard requires clients to train on an additional server-specified mini-batch, exploiting the fact that poisoned models exhibit a significant drop in confidence on this data. This approach leverages membership inference's sensitivity to model bias to detect and exclude malicious updates.

Result: FedGuard achieves superior robustness on three highly non-IID datasets with 90% Byzantine clients and seven distinct attack types per round, outperforming existing schemes in mitigating various Byzantine attacks.

Conclusion: FedGuard provides a general, effective defense against Byzantine attacks in federated learning, particularly in environments with high data heterogeneity and majority-malicious client participation, without relying on attack-specific assumptions.

Abstract: Federated learning is a distributed training framework vulnerable to
Byzantine attacks, particularly when over 50% of clients are malicious or when
datasets are highly non-independent and identically distributed (non-IID).
Additionally, most existing defense mechanisms are designed for specific attack
types (e.g., gradient similarity-based schemes can only defend against outlier
model poisoning), limiting their effectiveness. In response, we propose
FedGuard, a novel federated learning mechanism. FedGuard cleverly addresses the
aforementioned issues by leveraging the high sensitivity of membership
inference to model bias. By requiring clients to include an additional
mini-batch of server-specified data in their training, FedGuard can identify
and exclude poisoned models, as their confidence in the mini-batch will drop
significantly. Our comprehensive evaluation unequivocally shows that, under
three highly non-IID datasets, with 90% of clients being Byzantine and seven
different types of Byzantine attacks occurring in each round, FedGuard
significantly outperforms existing robust federated learning schemes in
mitigating various types of Byzantine attacks.

</details>


### [9] [Demo: TOSense -- What Did You Just Agree to?](https://arxiv.org/abs/2508.00659)
*Xinzhang Chen,Hassan Ali,Arash Shaghaghi,Salil S. Kanhere,Sanjay Jha*

Main category: cs.CR

TL;DR: This paper introduces TOSense, a Chrome extension for real-time natural language question answering about Terms of Service (ToS) using a crawling system and lightweight language model pipeline. A novel QA evaluation method called QEP is proposed to assess answer quality without manual annotation, validated through experiments on five major platforms.


<details>
  <summary>Details</summary>
Motivation: Terms of Service (ToS) for online services are lengthy and complex, creating information asymmetry and legal risks for users who lack the time or expertise to fully understand them.

Method: The system combines (i) a crawler (tos-crawl) for automated ToS content extraction, and (ii) a dual language model pipeline: MiniLM for semantic retrieval and BART-encoder for answer relevance verification. A synthetic QA generation approach (QEP) is introduced to evaluate answers through clustered topic matching without manual ground truth.

Result: Experiments on Apple, Google, X, Microsoft, and Netflix ToS achieve up to 44.5% accuracy across varying topic cluster configurations. The real-time interactive QA interface demonstrates seamless functionality and instant indexing for new websites during live demonstrations.

Conclusion: TOSense offers a scalable solution for simplifying ToS comprehension through automated information extraction, synthetic QA generation, and real-time natural language interaction, significantly reducing legal risks from complex service agreements while maintaining low computational overhead.

Abstract: Online services often require users to agree to lengthy and obscure Terms of
Service (ToS), leading to information asymmetry and legal risks. This paper
proposes TOSense-a Chrome extension that allows users to ask questions about
ToS in natural language and get concise answers in real time. The system
combines (i) a crawler "tos-crawl" that automatically extracts ToS content, and
(ii) a lightweight large language model pipeline: MiniLM for semantic retrieval
and BART-encoder for answer relevance verification. To avoid expensive manual
annotation, we present a novel Question Answering Evaluation Pipeline (QEP)
that generates synthetic questions and verifies the correctness of answers
using clustered topic matching. Experiments on five major platforms, Apple,
Google, X (formerly Twitter), Microsoft, and Netflix, show the effectiveness of
TOSense (with up to 44.5% accuracy) across varying number of topic clusters.
During the demonstration, we will showcase TOSense in action. Attendees will be
able to experience seamless extraction, interactive question answering, and
instant indexing of new sites.

</details>


### [10] [Unveiling Dynamic Binary Instrumentation Techniques](https://arxiv.org/abs/2508.00682)
*Oscar Llorente-Vazquez,Xabier Ugarte-Pedrero,Igor Santos-Grueiro,Pablo Garcia Bringas*

Main category: cs.CR

TL;DR: The paper provides an analysis and comparison of different Dynamic Binary Instrumentation approaches, highlighting their strengths, weaknesses, and performance trade-offs across various run-time instrumentation tasks.


<details>
  <summary>Details</summary>
Motivation: DBI is critical for security and system analysis, but existing approaches have diverse limitations, motivating the need for a comprehensive comparison to guide suitable application scenarios.

Method: The authors review process-level and whole-system DBI approaches, analyze their instrumentation techniques, evaluate performance on varying primitives/events, and compare capabilities through structured analysis.

Result: Experimental evaluations show no single DBI technique dominates all scenarios; different approaches suit specific goals due to inherent design trade-offs in instrumentation capabilities and performance overheads.

Conclusion: DBI methods require careful selection based on task requirements as no universal solution exists, emphasizing the importance of matching technique characteristics to specific instrumentation needs across processes or entire systems.

Abstract: Dynamic Binary Instrumentation (DBI) is the set of techniques that enable
instrumentation of programs at run-time, making it possible to monitor and
modify the execution of compiled binaries or entire systems. DBI is used for
countless security applications and analyses, and is extensively used across
many fields in both industry and academia. Over the years, several DBI
approaches have been proposed based on different technologies and implementing
diverse techniques. Every solution tries to overcome certain limitations, but
they sometimes bring other shortcomings. Some are specialized for one
particular domain or task, while others have a wider scope.
  In this paper, we shed light into the labyrinth of DBI, bringing together
process-level and whole-system approaches. We depict their building blocks and
analyze the underlying instrumentation techniques, comparing their ability to
instrument different primitives and run-time events. Then, we evaluate their
performance when implementing each primitive, and highlight relevant
observations. Our results show that no single technique is better than the rest
in all circumstances.

</details>


### [11] [LeakyCLIP: Extracting Training Data from CLIP](https://arxiv.org/abs/2508.00756)
*Yunhao Chen,Shujie Wang,Xin Wang,Xingjun Ma*

Main category: cs.CR

TL;DR: This paper introduces LeakyCLIP, a novel attack framework to reconstruct training images from CLIP embeddings, revealing significant memorization and privacy leakage risks in multimodal models. It achieves over 358% SSIM improvement on ViT-B-16 and demonstrates data membership inference via low-fidelity metrics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate memorization and privacy leakage vulnerabilities in CLIP to ensure the security of multimodal models, as prior studies showed similar risks in diffusion models.

Method: LeakyCLIP addresses three challenges in CLIP inversion: 1) non-robust features via adversarial fine-tuning, 2) limited visual semantics in text embeddings using linear transformation-based alignment, and 3) low reconstruction fidelity with Stable Diffusion-based refinement.

Result: LeakyCLIP achieves 358% SSIM improvement over baselines on ViT-B-16 using LAION-2B. It also demonstrates successful training data membership inference from low-fidelity reconstruction metrics.

Conclusion: The work provides a practical CLIP inversion method and highlights pervasive privacy risks in multimodal models, offering novel insights into the scope and nature of these vulnerabilities.

Abstract: Understanding the memorization and privacy leakage risks in Contrastive
Language--Image Pretraining (CLIP) is critical for ensuring the security of
multimodal models. Recent studies have demonstrated the feasibility of
extracting sensitive training examples from diffusion models, with conditional
diffusion models exhibiting a stronger tendency to memorize and leak
information. In this work, we investigate data memorization and extraction
risks in CLIP through the lens of CLIP inversion, a process that aims to
reconstruct training images from text prompts. To this end, we introduce
\textbf{LeakyCLIP}, a novel attack framework designed to achieve high-quality,
semantically accurate image reconstruction from CLIP embeddings. We identify
three key challenges in CLIP inversion: 1) non-robust features, 2) limited
visual semantics in text embeddings, and 3) low reconstruction fidelity. To
address these challenges, LeakyCLIP employs 1) adversarial fine-tuning to
enhance optimization smoothness, 2) linear transformation-based embedding
alignment, and 3) Stable Diffusion-based refinement to improve fidelity.
Empirical results demonstrate the superiority of LeakyCLIP, achieving over 358%
improvement in Structural Similarity Index Measure (SSIM) for ViT-B-16 compared
to baseline methods on LAION-2B subset. Furthermore, we uncover a pervasive
leakage risk, showing that training data membership can even be successfully
inferred from the metrics of low-fidelity reconstructions. Our work introduces
a practical method for CLIP inversion while offering novel insights into the
nature and scope of privacy risks in multimodal models.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [12] [Git Context Controller: Manage the Context of LLM-based Agents like Git](https://arxiv.org/abs/2508.00031)
*Junde Wu*

Main category: cs.SE

TL;DR: GCC is a Git-inspired context management framework for agents that improves long-horizon task performance through versioned memory operations (COMMIT/BRANCH/MERGE/CONTEXT), achieving 48% bug resolution and tripling self-replication task performance.


<details>
  <summary>Details</summary>
Motivation: Long-horizon agent workflows (e.g. large-scale software projects) face critical context management challenges as existing agents struggle to maintain and organize evolving knowledge states over extended interaction sequences.

Method: Structures agent memory as a version-controlled file system using Git-like operations: COMMIT for checkpointing progress, BRANCH for isolating experiments, MERGE for consolidating paths, and CONTEXT for selecting active memories, enabling milestone tracking and structured planning.

Result: SOTA on SWE-Bench-Lite (48.00% bug resolution vs. 26 systems), 40.7 task resolution in self-replication case study (3.5× improvement over non-GCC agents), with code repository provided for replication.

Conclusion: Versioned context management through GCC framework enables agents to successfully handle complex long-term goals by systematically organizing knowledge, isolating experimental variations, and facilitating memory handoff between sessions/agents.

Abstract: Large language model (LLM) based agents have shown impressive capabilities by
interleaving internal reasoning with external tool use. However, as these
agents are deployed in long-horizon workflows, such as coding for a big,
long-term project, context management becomes a critical bottleneck. We
introduce Git-Context-Controller (GCC), a structured context management
framework inspired by software version control systems. GCC elevates context as
versioned memory hierarchy like Git. It structures agent memory as a persistent
file system with explicit operations: COMMIT, BRANCH, MERGE, and CONTEXT,
enabling milestone-based checkpointing, exploration of alternative plans, and
structured reflection. Our approach empowers agents to manage long-term goals,
isolate architectural experiments, and recover or hand off memory across
sessions and agents. Empirically, agents equipped with GCC achieve
state-of-the-art performance on the SWE-Bench-Lite benchmark, resolving 48.00
of software bugs, outperforming 26 competitive systems. In a self-replication
case study, a GCC-augmented agent builds a new CLI agent from scratch,
achieving 40.7 task resolution, compared to only 11.7 without GCC. The code is
released at: https://github.com/theworldofagents/GCC

</details>


### [13] [GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries](https://arxiv.org/abs/2508.00033)
*Nuno Fachada,Daniel Fernandes,Carlos M. Fernandes,Bruno D. Ferreira-Saraiva,João P. Matos-Carvalho*

Main category: cs.SE

TL;DR: This paper benchmarks state-of-the-art LLMs in generating executable Python code for scientific tasks without in-context examples, finding that only GPT-4.1 consistently succeeds across both conversational data analysis (ParShift) and synthetic data generation/clustering (pyclugen+scikit-learn) tasks.


<details>
  <summary>Details</summary>
Motivation: Scientific research increasingly requires Python API usage for complex computational experiments, but LLM capabilities in this domain - particularly with zero-shot reasoning about unfamiliar libraries - remain poorly understood.

Method: Used structured zero-shot prompts on two tasks: 1) conversational data analysis with ParShift, and 2) synthetic data generation and clustering using pyclugen and scikit-learn. Evaluated code functionality, prompt compliance across multiple model runs, and analyzed execution errors.

Result: Only GPT-4.1 achieved 100% success rate in both tasks. Other models showed significant variability. Evaluation revealed LLMs require more robust prompting strategies and highlights issues in third-party libraries (e.g., documentation gaps, obscure bugs).

Conclusion: Current LLM capabilities are limited for end-to-end scientific automation. Highlights the need for: 1) improved zero-shot prompt design, 2) better library documentation, and 3) continued LLM development to handle complex computational workflows.

Abstract: Large Language Models (LLMs) have advanced rapidly as tools for automating
code generation in scientific research, yet their ability to interpret and use
unfamiliar Python APIs for complex computational experiments remains poorly
characterized. This study systematically benchmarks a selection of
state-of-the-art LLMs in generating functional Python code for two increasingly
challenging scenarios: conversational data analysis with the \textit{ParShift}
library, and synthetic data generation and clustering using \textit{pyclugen}
and \textit{scikit-learn}. Both experiments use structured, zero-shot prompts
specifying detailed requirements but omitting in-context examples. Model
outputs are evaluated quantitatively for functional correctness and prompt
compliance over multiple runs, and qualitatively by analyzing the errors
produced when code execution fails. Results show that only a small subset of
models consistently generate correct, executable code, with GPT-4.1 standing
out as the only model to always succeed in both tasks. In addition to
benchmarking LLM performance, this approach helps identify shortcomings in
third-party libraries, such as unclear documentation or obscure implementation
bugs. Overall, these findings highlight current limitations of LLMs for
end-to-end scientific automation and emphasize the need for careful prompt
design, comprehensive library documentation, and continued advances in language
model capabilities.

</details>


### [14] [Machine Learning Pipeline for Software Engineering: A Systematic Literature Review](https://arxiv.org/abs/2508.00045)
*Samah Kansab*

Main category: cs.SE

TL;DR: This paper reviews ML pipelines in software engineering (SE), highlighting best practices, challenges, and the importance of robust preprocessing and ensemble methods for improved model reliability.


<details>
  <summary>Details</summary>
Motivation: SE systems are becoming increasingly complex, leading to inefficiencies in quality assurance. Traditional methods fail to scale, necessitating improved ML pipeline frameworks for tasks like defect prediction and code review.

Method: Systematic literature review (SLR) to consolidate state-of-the-art ML pipelining practices, evaluate their components (preprocessing, model selection, validation), and identify existing gaps in SE applications.

Result: Key findings include: SMOTE/SZZ-based preprocessing enhances model reliability, Random Forest/Gradient Boosting dominate performance, Naive Bayes offers efficiency/interpretability, AUC/F1-precision metrics are widely used, and bootstrapping validation is critical for stability.

Conclusion: Well-designed ML pipelines are essential for addressing SE challenges. The study provides actionable guidelines for researchers/practitioners to optimize ML adoption in software development, while mapping existing gaps for future innovation.

Abstract: The rapid advancement of software development practices has introduced
challenges in ensuring quality and efficiency across the software engineering
(SE) lifecycle. As SE systems grow in complexity, traditional approaches often
fail to scale, resulting in longer debugging times, inefficient defect
detection, and resource-heavy development cycles. Machine Learning (ML) has
emerged as a key solution, enabling automation in tasks such as defect
prediction, code review, and release quality estimation. However, the
effectiveness of ML in SE depends on the robustness of its pipeline, including
data collection, preprocessing, feature engineering, algorithm selection,
validation, and evaluation.
  This systematic literature review (SLR) examines state-of-the-art ML
pipelines designed for SE, consolidating best practices, challenges, and gaps.
Our findings show that robust preprocessing, such as SMOTE for data balancing
and SZZ-based algorithms for feature selection, improves model reliability.
Ensemble methods like Random Forest and Gradient Boosting dominate performance
across tasks, while simpler models such as Naive Bayes remain valuable for
efficiency and interpretability. Evaluation metrics including AUC, F1-score,
and precision are most common, with new metrics like Best Arithmetic Mean (BAM)
emerging in niche applications. Validation techniques such as bootstrapping are
widely used to ensure model stability and generalizability.
  This SLR highlights the importance of well-designed ML pipelines for
addressing SE challenges and provides actionable insights for researchers and
practitioners seeking to optimize software quality and efficiency. By
identifying gaps and trends, this study sets a foundation for advancing ML
adoption and fostering innovation in increasingly complex development
environments.

</details>


### [15] [A Survey on Code Generation with LLM-based Agents](https://arxiv.org/abs/2508.00083)
*Yihong Dong,Xue Jiang,Jiaru Qian,Tian Wang,Kechi Zhang,Zhi Jin,Ge Li*

Main category: cs.SE

TL;DR: This paper systematically surveys LLM-based code generation agents, categorizing their core techniques (single-agent/multi-agent), SDLC applications, evaluation benchmarks, and proposes future research directions based on primary challenges.


<details>
  <summary>Details</summary>
Motivation: Highlights the transformative impact of code generation agents on software development by addressing autonomy, expanded task scope beyond code snippets, and focus on practical engineering challenges.

Method: Conducts a systematic review to trace technological development, classify architectural approaches, and analyze existing tools and evaluation frameworks in the domain.

Result: Comprehensive analysis of LLM-based agents across software development lifecycle, identified mainstream benchmarks/metrics, and categorized key implementation patterns (single vs. multi-agent systems).

Conclusion: Elucidates foundational challenges in the field and suggests long-term research trajectories focusing on system reliability, process management, and tool integration within practical software engineering contexts.

Abstract: Code generation agents powered by large language models (LLMs) are
revolutionizing the software development paradigm. Distinct from previous code
generation techniques, code generation agents are characterized by three core
features. 1) Autonomy: the ability to independently manage the entire workflow,
from task decomposition to coding and debugging. 2) Expanded task scope:
capabilities that extend beyond generating code snippets to encompass the full
software development lifecycle (SDLC). 3) Enhancement of engineering
practicality: a shift in research emphasis from algorithmic innovation toward
practical engineering challenges, such as system reliability, process
management, and tool integration. This domain has recently witnessed rapid
development and an explosion in research, demonstrating significant application
potential. This paper presents a systematic survey of the field of LLM-based
code generation agents. We trace the technology's developmental trajectory from
its inception and systematically categorize its core techniques, including both
single-agent and multi-agent architectures. Furthermore, this survey details
the applications of LLM-based agents across the full SDLC, summarizes
mainstream evaluation benchmarks and metrics, and catalogs representative
tools. Finally, by analyzing the primary challenges, we identify and propose
several foundational, long-term research directions for the future work of the
field.

</details>


### [16] [How Quantization Impacts Privacy Risk on LLMs for Code?](https://arxiv.org/abs/2508.00128)
*Md Nazmul Haque,Hua Yang,Zhou Yang,Bowen Xu*

Main category: cs.SE

TL;DR: This paper investigates the impact of quantization on privacy risks and performance tradeoffs in code-specific large language models, finding that quantization reduces privacy exposure while maintaining a performance-benefit balance.


<details>
  <summary>Details</summary>
Motivation: LLMs4Code face privacy risks from sensitive training data, and quantization's effect on these risks remains unclear despite its adoption for cost reduction.

Method: Empirical evaluation of static/dynamic quantization on Pythia, CodeGen, and GPTNeo models using membership inference attacks and performance metrics.

Result: Quantization significantly reduces privacy risk compared to original models; larger quantized models outperform smaller full-precision ones in accuracy-risk balance; positive correlation between performance and privacy risk confirmed.

Conclusion: Quantization offers practical privacy safeguards for compressed LLMs4Code deployments while maintaining usable performance, with architecture-specific considerations for optimal implementation.

Abstract: Large language models for code (LLMs4Code) rely heavily on massive training
data, including sensitive data, such as cloud service credentials of the
projects and personal identifiable information of the developers, raising
serious privacy concerns. Membership inference (MI) has recently emerged as an
effective tool for assessing privacy risk by identifying whether specific data
belong to a model's training set. In parallel, model compression techniques,
especially quantization, have gained traction for reducing computational costs
and enabling the deployment of large models. However, while quantized models
still retain knowledge learned from the original training data, it remains
unclear whether quantization affects their ability to retain and expose privacy
information. Answering this question is of great importance to understanding
privacy risks in real-world deployments. In this work, we conduct the first
empirical study on how quantization influences task performance and privacy
risk simultaneously in LLMs4Code. To do this, we implement widely used
quantization techniques (static and dynamic) to three representative model
families, namely Pythia, CodeGen, and GPTNeo. Our results demonstrate that
quantization has a significant impact on reducing the privacy risk relative to
the original model. We also uncover a positive correlation between task
performance and privacy risk, indicating an underlying tradeoff. Moreover, we
reveal the possibility that quantizing larger models could yield better balance
than using full-precision small models. Finally, we demonstrate that these
findings generalize across different architectures, model sizes and MI methods,
offering practical guidance for safeguarding privacy when deploying compressed
LLMs4Code.

</details>


### [17] [Testing the Untestable? An Empirical Study on the Testing Process of LLM-Powered Software Systems](https://arxiv.org/abs/2508.00198)
*Cleyton Magalhaes,Italo Santos,Brody Stuart-Verner,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: This paper investigates testing practices for LLM-powered systems in real-world applications using 99 student project reports. It identifies strategies like manual/automated testing and challenges such as hallucinations and prompt sensitivity.


<details>
  <summary>Details</summary>
Motivation: As LLM-integrated software becomes widespread, existing research has overlooked how complete LLM-powered systems are verified during development, necessitating practical insights into their testing approaches.

Method: An exploratory case study analyzed 99 student-written reports through thematic analysis and structured coding, examining how LLM applications were tested in academic settings.

Result: Testing methods combined manual verification (exploratory testing, prompt iteration) and automated techniques (unit tests), with frequent problems in model behavior evaluation, integration stability, and output reliability.

Conclusion: Testing LLM systems requires hybrid approaches that combine traditional software verification with behavior-focused strategies, offering practical guidance for evaluating generative components in software engineering contexts.

Abstract: Background: Software systems powered by large language models are becoming a
routine part of everyday technologies, supporting applications across a wide
range of domains. In software engineering, many studies have focused on how
LLMs support tasks such as code generation, debugging, and documentation.
However, there has been limited focus on how full systems that integrate LLMs
are tested during development. Aims: This study explores how LLM-powered
systems are tested in the context of real-world application development.
Method: We conducted an exploratory case study using 99 individual reports
written by students who built and deployed LLM-powered applications as part of
a university course. Each report was independently analyzed using thematic
analysis, supported by a structured coding process. Results: Testing strategies
combined manual and automated methods to evaluate both system logic and model
behavior. Common practices included exploratory testing, unit testing, and
prompt iteration. Reported challenges included integration failures,
unpredictable outputs, prompt sensitivity, hallucinations, and uncertainty
about correctness. Conclusions: Testing LLM-powered systems required
adaptations to traditional verification methods, blending source-level
reasoning with behavior-aware evaluations. These findings provide evidence on
the practical context of testing generative components in software systems.

</details>


### [18] [Functional vs. Object-Oriented: Comparing How Programming Paradigms Affect the Architectural Characteristics of Systems](https://arxiv.org/abs/2508.00244)
*Briza Mel Dias de Sousa,Renato Cordeiro Ferreira,Alfredo Goldman*

Main category: cs.SE

TL;DR: This study compares object-oriented (Kotlin) and functional (Scala) programming paradigms in a Digital Wallet system using qualitative/quantitative analyses to evaluate architectural impacts and developer perceptions.


<details>
  <summary>Details</summary>
Motivation: The software industry is observing a shift from object-oriented programming (OOP) to functional programming (FP), creating a need to understand how these paradigms affect software architecture and developer experience.

Method: The paper examines a Digital Wallet implementation in Kotlin (OOP) and Scala (FP) via self-ethnographic qualitative analysis (comparative implementation) and survey-based quantitative analysis (developer perception study).

Result: Qualitative findings reveal differences in architectural structure between paradigms, while quantitative results demonstrate varying developer perceptions when working with or reviewing code in each paradigm.

Conclusion: The empirical comparison between OOP and FP implementations provides actionable insights for developers and organizations choosing the optimal paradigm for future projects based on architectural characteristics and readability considerations.

Abstract: After decades of dominance by object-oriented programming (OOP), functional
programming (FP) is gaining increasing attention in the software industry. This
study compares the impact of OOP and FP on the architectural characteristics of
software systems. For that, it examines the design and implementation of a
Digital Wallet system, developed in Kotlin (representing OOP) and Scala
(representing FP). The comparison is made through both qualitative and
quantitative analyses to explore how each paradigm influences the system's
architectural characteristics. The self-ethnographic qualitative analysis
provides a side-by-side comparison of both implementations, revealing the
perspective of those writing such code. The survey-based quantitative analysis
gathers feedback from developers with diverse backgrounds, showing their
impressions of those reading this code. Hopefully, these results may be useful
for developers or organizations seeking to make more informed decisions about
which paradigm is best suited for their next project.

</details>


### [19] [Leveraging Large Language Model for Information Retrieval-based Bug Localization](https://arxiv.org/abs/2508.00253)
*Moumita Asad,Rafed Muhammad Yasir,Armin Geramirad,Sam Malek*

Main category: cs.SE

TL;DR: GenLoc is an LLM-based iterative bug localization method that surpasses existing approaches by resolving vocabulary mismatch through code exploration and semantic retrieval, achieving ~60% improvement in accuracy@1 on 9,000+ Java bug reports.


<details>
  <summary>Details</summary>
Motivation: Traditional IR-based bug localization methods struggle with vocabulary mismatch between bug reports and code, and existing models fail to adequately capture semantic context. This limits their effectiveness in identifying buggy files.

Method: GenLoc uses a Large Language Model with code-exploration functions to iteratively analyze the codebase for bug reports. It optionally employs vector embeddings to retrieve semantically related files, enriching contextual understanding of the code-retrieval process.

Result: GenLoc outperforms five state-of-the-art techniques across multiple metrics, showing >60% average improvement in Accuracy@1 when tested on 9,000+ real-world Java bug reports from six large-scale projects.

Conclusion: LLMs enhanced with iterative code exploration and strategic contextual retrieval (via embeddings) significantly improve bug localization effectiveness. GenLoc's semantic flexibility overcomes vocabulary mismatch limitations present in conventional approaches.

Abstract: Information Retrieval-based Bug Localization aims to identify buggy source
files for a given bug report. While existing approaches -- ranging from vector
space models to deep learning models -- have shown potential in this domain,
their effectiveness is often limited by the vocabulary mismatch between bug
reports and source code. To address this issue, we propose a novel Large
Language Model (LLM) based bug localization approach, called GenLoc. Given a
bug report, GenLoc leverages an LLM equipped with code-exploration functions to
iteratively analyze the code base and identify potential buggy files. To gather
better context, GenLoc may optionally retrieve semantically relevant files
using vector embeddings. GenLoc has been evaluated on over 9,000 real-world bug
reports from six large-scale Java projects. Experimental results show that
GenLoc outperforms five state-of-the-art bug localization techniques across
multiple metrics, achieving an average improvement of more than 60\% in
Accuracy@1.

</details>


### [20] [Accurate and Consistent Graph Model Generation from Text with Large Language Models](https://arxiv.org/abs/2508.00255)
*Boqi Chen,Ou Wei,Bingzhou Zheng,Gunter Mussbacher*

Main category: cs.SE

TL;DR: A novel abstraction-concretization framework improves LLM-generated graph models by addressing syntax violations, constraint inconsistencies, and hallucination via probabilistic aggregation and constraint-refined concrete models.


<details>
  <summary>Details</summary>
Motivation: LLM-based graph model generation produces partially correct models with unresolved issues in constraint adherence and accuracy despite existing techniques addressing syntax violations.

Method: Proposes a probabilistic framework that aggregates multiple LLM outputs into a partial model and refines it into a concrete model satisfying all constraints through abstraction-concretization cycles.

Result: Evaluation on open-source and closed-source LLMs across diverse datasets shows significant improvements in model consistency and quality compared to baseline approaches.

Conclusion: The abstraction-concretization framework effectively overcomes persistent limitations in LLM-based graph modeling, particularly constraint consistency and factual accuracy issues.

Abstract: Graph model generation from natural language description is an important task
with many applications in software engineering. With the rise of large language
models (LLMs), there is a growing interest in using LLMs for graph model
generation. Nevertheless, LLM-based graph model generation typically produces
partially correct models that suffer from three main issues: (1) syntax
violations: the generated model may not adhere to the syntax defined by its
metamodel, (2) constraint inconsistencies: the structure of the model might not
conform to some domain-specific constraints, and (3) inaccuracy: due to the
inherent uncertainty in LLMs, the models can include inaccurate, hallucinated
elements. While the first issue is often addressed through techniques such as
constraint decoding or filtering, the latter two remain largely unaddressed.
Motivated by recent self-consistency approaches in LLMs, we propose a novel
abstraction-concretization framework that enhances the consistency and quality
of generated graph models by considering multiple outputs from an LLM. Our
approach first constructs a probabilistic partial model that aggregates all
candidate outputs and then refines this partial model into the most appropriate
concrete model that satisfies all constraints. We evaluate our framework on
several popular open-source and closed-source LLMs using diverse datasets for
model generation tasks. The results demonstrate that our approach significantly
improves both the consistency and quality of the generated graph models.

</details>


### [21] [Benchmarking LLMs for Unit Test Generation from Real-World Functions](https://arxiv.org/abs/2508.00408)
*Dong Huang,Jie M. Zhang,Mark Harman,Qianru Zhang,Mingzhe Du,See-Kiong Ng*

Main category: cs.SE

TL;DR: The paper introduces ULT, a new benchmark for evaluating LLMs' unit test generation capabilities by addressing data contamination and structural simplicity in existing benchmarks. It also provides PLT for controlled analysis of memorization vs reasoning, showing ULT is significantly more challenging.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLM unit test generation suffer from data contamination and structurally simple function code, leading to biased conclusions in empirical studies. The authors aim to create a more realistic benchmark to assess LLMs' true test generation capabilities without these limitations.

Method: The authors developed ULT using a multi-stage curation process ensuring high cyclomatic complexity and preventing test case contamination. A paired benchmark PLT is introduced with leaked tests to differentiate between LLM memorization and reasoning during test generation.

Result: ULT achieves lower performance metrics (41.32% accuracy, 45.10% statement coverage, 30.22% branch coverage, 40.21% mutation score) compared to TestEval (91.79%, 92.18%, 82.04%, 49.69%) and PLT (47.07%, 55.13%, 40.07%, 50.80%), demonstrating its higher difficulty.

Conclusion: ULT is a significantly more challenging and realistic benchmark for function-level unit test generation, as using pre-contaminated benchmarks can lead to overstated performance results. Future studies should adopt such benchmarks to better evaluate LLMs' test generation capabilities in real-world scenarios.

Abstract: Recently, large language models (LLMs) have shown great promise in automating
unit test generation, significantly reducing the manual effort required by
developers. To effectively evaluate the capabilities of LLMs in this domain, it
is crucial to have a well-designed benchmark that accurately reflects
real-world scenarios and mitigates common pitfalls. Existing LLM test
generation benchmarks are limited by two critical drawbacks: data contamination
and structurally simple function code. As a result, we often cannot rely on the
validity of scientific conclusions drawn from empirical studies using these
limited benchmarks. The empirical evidence presented may be biased due to
contamination and may fail to generalize beyond toy programs due to structural
simplicity.
  To address these problems, we introduce ULT (UnLeakedTestbench), a new
benchmark specifically designed for function-level unit test generation from
real-world Python functions. ULT is constructed through a multi-stage curation
process that ensures high cyclomatic complexity and mitigates test case
contamination. With 3,909 carefully selected function-level tasks, ULT provides
a more realistic and challenging evaluation of LLMs' test generation
capabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT
with leaked tests designed to enable a controlled analysis of memorization
versus reasoning in test generation. Our evaluation results demonstrate that
ULT is significantly more challenging. For example, test cases generated by
LLMs only achieve 41.32\%, 45.10\%, 30.22\%, and 40.21\% for accuracy,
statement coverage, branch coverage, and mutation score on average for all
LLMs, respectively. These results are substantially lower than the
corresponding metrics on TestEval (91.79\%, 92.18\%, 82.04\%, and 49.69\%) and
PLT (47.07\%, 55.13\%, 40.07\%, and 50.80\%).

</details>


### [22] [Managing Power Gaps as a Topic of Pair Programming Skill: A Grounded Theory](https://arxiv.org/abs/2508.00462)
*Linus Ververs,Lutz Prechelt*

Main category: cs.SE

TL;DR: This paper examines power dynamics in industrial pair programming through grounded theory, identifying Power Gaps and providing strategies to mitigate them.


<details>
  <summary>Details</summary>
Motivation: To address unexamined power-related behaviors in professional pair programming that impact collaboration effectiveness and outcomes.

Method: Analyzed 22 industrial pair programming sessions using Grounded Theory Methodology, validated with a survey of 292 practitioners.

Result: Identified 'Power Gap' as a key phenomenon caused by hierarchical behaviors and mitigated by equalizing behaviors, validated through survey frequency and indirect observational evidence.

Conclusion: Avoiding Hierarchical Behavior and implementing Equalizing Behavior are critical skills for effective pair programming in industry.

Abstract: Context: Pair Programming as a work mode is used (occasionally or frequently)
throughout professional software development. Objective: Understand what
power-related phenomena occur in pair programming as it is used in industry;
give advice to practitioners on how to do better pair programming. Method:
Analyze 22 industrial pair programming sessions using Grounded Theory
Methodology. Formulate a Grounded Theory on power-related behaviors. Run a
survey with 292 participants about that theory. Use it to demonstrate that the
phenomena are common. Results: Our theory describes the phenomenon of Power
Gap: a perceived difference in participation opportunities. The theory shows
the behaviors that create a Power Gap or result from it. Power Gaps tend to
damage knowledge transfer, code quality, and process effi ciency. The survey
results show that all concepts from our theory are frequent in practice. They
also provide more grounding for concepts that are observable only indirectly.
Conclusions: It is a valuable component of pair programming skill to be able to
avoid Power Gaps. Specifically, pair partners need to avoid Hierarchical
Behavior (which tends to create or increase a Power Gap) and should perform
enough Equalizing Behavior (which prevents or reduces a Power Gap).

</details>


### [23] [Desyan: A Platform for Seamless Value-Flow and Symbolic Analysis](https://arxiv.org/abs/2508.00508)
*Panagiotis Diamantakis,Thanassis Avgerinos,Yannis Smaragdakis*

Main category: cs.SE

TL;DR: Desyan is a platform integrating value-flow (Datalog) and symbolic (SMT) analyses, enabling efficient hybrid static analysis by extending Soufflé with SMT solvers and Datalog-native symbolic reasoning constructs.


<details>
  <summary>Details</summary>
Motivation: Value-flow analysis (e.g., data-flow) and symbolic analysis (e.g., symbolic execution) are dominant but traditionally separate paradigms due to lack of an integrated platform. Combining these approaches could improve program analysis but requires overcoming performance and integration challenges.

Method: Desyan extends Soufflé, a high-performance Datalog engine, with full SMT solver integration and introduces a bottom-up algebraic module for Datalog-native symbolic reasoning. It features patterns and optimizations tailored for program analysis, enabling seamless switching between reasoning styles while maintaining technology agnosticism.

Result: Desyan achieves: (1) Best-in-class Datalog execution speed for value-flow tasks (20x+ faster in some cases), (2) 2x+ speedup for lightweight symbolic evaluation versus external SMT solvers, and (3) enables application of symbolic techniques to complement traditional value-flow analysis workflows.

Conclusion: Desyan creates a unifying platform that preserves the strengths of Datalog and SMT-based analysis while enabling efficient hybrid reasoning. This advances static analysis capabilities by making cross-paradigm analysis more accessible and performant for complex program analysis tasks.

Abstract: Over the past two decades, two different types of static analyses have
emerged as dominant paradigms both in academia and industry: value-flow
analysis (e.g., data-flow analysis or points-to analysis) and symbolic analysis
(e.g., symbolic execution). Despite their individual successes in numerous
application fields, the two approaches have remained largely separate; an
artifact of the simple reality that there is no broadly adopted unifying
platform for effortless and efficient integration of symbolic techniques with
high-performance data-flow reasoning.
  To bridge this gap, we introduce Desyan: a platform for writing program
analyses with seamless integration of value-flow and symbolic reasoning. Desyan
expands a production-ready Datalog fixpoint engine (Souffl\'e) with
full-fledged SMT solving invoking industry-leading SMT engines. Desyan provides
constructs for automatically (and efficiently!) handling typical patterns that
come up in program analysis. At the same time, the integration is agnostic with
respect to the solving technology, and supports Datalog-native symbolic
reasoning, via a bottom-up algebraic reasoning module.
  The result is an engine that allows blending different kinds of reasoning, as
needed for the underlying analysis. For value-flow analysis, the engine is the
best-in-class Datalog evaluator (often by a factor of over 20x in execution
time); for applications that require full SMT (e.g., a concolic execution
engine or other symbolic evaluator that needs to solve arbitrarily complex
conditions), the engine is leveraging the leading SMT solvers; for lightweight
symbolic evaluation (e.g., solving simple conditionals in the context of a
path-sensitive analysis), the engine can use Datalog-native symbolic reasoning,
achieving large speedups (often of over 2x) compared to eagerly appealing to an
SMT solver.

</details>


### [24] [SPENCER: Self-Adaptive Model Distillation for Efficient Code Retrieval](https://arxiv.org/abs/2508.00546)
*Wenchao Gu,Zongyi Lyu,Yanlin Wang,Hongyu Zhang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: SPENCER improves code retrieval by combining dual-encoder efficiency with cross-encoder accuracy through model distillation, achieving 98% performance retention and 70% faster inference.


<details>
  <summary>Details</summary>
Motivation: Dual-encoders limit code retrieval performance due to lack of code-query interaction during training, while cross-encoders enhance accuracy at the cost of efficiency. A hybrid approach is needed to balance both.

Method: First, a dual-encoder narrows the search space for efficiency. Then, a cross-encoder refines accuracy. SPENCER introduces a Self-AdaPtive Model Distillation technique to distill the dual-encoder's performance while reducing inference time, combined with an adaptive teaching assistant selection strategy.

Result: Experiments show SPENCER outperforms dual-encoder-only models by combining both architectures. The distillation technique achieves 70% faster dual-encoder inference with over 98% retention of performance.

Conclusion: SPENCER effectively bridges the efficiency-accuracy gap in code retrieval by integrating dual- and cross-encoders with model distillation, offering both improved performance and significant speed gains.

Abstract: Code retrieval aims to provide users with desired code snippets based on
users' natural language queries. With the development of deep learning
technologies, adopting pre-trained models for this task has become mainstream.
Considering the retrieval efficiency, most of the previous approaches adopt a
dual-encoder for this task, which encodes the description and code snippet into
representation vectors, respectively. However, the model structure of the
dual-encoder tends to limit the model's performance, since it lacks the
interaction between the code snippet and description at the bottom layer of the
model during training. To improve the model's effectiveness while preserving
its efficiency, we propose a framework, which adopts Self-AdaPtive Model
Distillation for Efficient CodE Retrieval, named SPENCER. SPENCER first adopts
the dual-encoder to narrow the search space and then adopts the cross-encoder
to improve accuracy. To improve the efficiency of SPENCER, we propose a novel
model distillation technique, which can greatly reduce the inference time of
the dual-encoder while maintaining the overall performance. We also propose a
teaching assistant selection strategy for our model distillation, which can
adaptively select the suitable teaching assistant models for different
pre-trained models during the model distillation to ensure the model
performance. Extensive experiments demonstrate that the combination of
dual-encoder and cross-encoder improves overall performance compared to solely
dual-encoder-based models for code retrieval. Besides, our model distillation
technique retains over 98% of the overall performance while reducing the
inference time of the dual-encoder by 70%.

</details>


### [25] [Can User Feedback Help Issue Detection? An Empirical Study on a One-billion-user Online Service System](https://arxiv.org/abs/2508.00593)
*Shuyao Jiang,Jiazhen Gu,Wujie Zheng,Yangfan Zhou,Michael R. Lyu*

Main category: cs.SE

TL;DR: This paper analyzes user feedback in large-scale online systems to improve issue detection, finding challenges in filtering irrelevant information and demonstrating the viability of machine learning approaches despite undetectable severe issues via feedback alone.


<details>
  <summary>Details</summary>
Motivation: Large-scale systems receive vast amounts of user feedback, but identifying severe issues remains challenging. The study aims to understand feedback characteristics to build effective detection methods.

Method: Empirical analysis of 50M+ feedback items from six services, focusing on content relevance, issue indicator features, and ML's effectiveness for temporal topic distribution patterns.

Result: 1) High proportion of irrelevant feedback necessitates filtering. 2) Some severe issues elude detection via feedback characteristics alone. 3) Consistent topic distributions over time validate ML-based analysis potential.

Conclusion: Provides empirical foundation for ML-driven feedback-based issue detection in large systems, guiding practical implementation despite limitations in feedback completeness.

Abstract: Background: It has long been suggested that user feedback, typically written
in natural language by end-users, can help issue detection. However, for
large-scale online service systems that receive a tremendous amount of
feedback, it remains a challenging task to identify severe issues from user
feedback. Aims: To develop a better feedback-based issue detection approach, it
is crucial first to gain a comprehensive understanding of the characteristics
of user feedback in real production systems. Method: In this paper, we conduct
an empirical study on 50,378,766 user feedback items from six real-world
services in a one-billion-user online service system. We first study what users
provide in their feedback. We then examine whether certain features of feedback
items can be good indicators of severe issues. Finally, we investigate whether
adopting machine learning techniques to analyze user feedback is reasonable.
Results: Our results show that a large proportion of user feedback provides
irrelevant information about system issues. As a result, it is crucial to
filter out issue-irrelevant information when processing user feedback.
Moreover, we find severe issues that cannot be easily detected based solely on
user feedback characteristics. Finally, we find that the distributions of the
feedback topics in different time intervals are similar. This confirms that
designing machine learning-based approaches is a viable direction for better
analyzing user feedback. Conclusions: We consider that our findings can serve
as an empirical foundation for feedback-based issue detection in large-scale
service systems, which sheds light on the design and implementation of
practical issue detection approaches.

</details>


### [26] [MCeT: Behavioral Model Correctness Evaluation using Large Language Models](https://arxiv.org/abs/2508.00630)
*Khaled Ahmed,Jialing Song,Boqi Chen,Ou Wei,Bingzhou Zheng*

Main category: cs.SE

TL;DR: The paper introduces MCeT, the first fully automated tool for evaluating sequence diagram correctness via requirements text using LLMs with a multi-perspective approach that enhances issue detection by 90% compared to direct comparison methods.


<details>
  <summary>Details</summary>
Motivation: Manual verification of behavioral models (e.g., sequence diagrams) against requirements is labor-intensive. Current LLM-based automated approaches detect <35% of issues identified by expert engineers, necessitating improved strategies for AI-generated model evaluation.

Method: MCeT employs a fine-grained atomic decomposition strategy: 1) Dissects diagrams into indivisible interactions 2) Parses requirements into atomic items 3) Uses multi-perspective cross-checking between atomic components 4) Applies self-consistency validation to mitigate hallucinations by integrating conflicting LLM responses.

Result: MCeT achieves 81% precision (up from 58% in direct comparison) on real-world requirements, identifies 90% more issues than human experts using direct comparison, and discovers an average of 6 novel issues per diagram in its dataset.

Conclusion: The atomic decomposition and multi-perspective validation approach significantly improves LLM-based model correctness evaluation for sequence diagrams. MCeT's self-consistency mechanism reduces hallucinations by 35%, enabling robust automated assessment crucial for AI-assisted diagram generation workflows.

Abstract: Behavioral model diagrams, e.g., sequence diagrams, are an essential form of
documentation that are typically designed by system engineers from requirements
documentation, either fully manually or assisted by design tools. With the
growing use of Large Language Models (LLM) as AI modeling assistants, more
automation will be involved in generating diagrams. This necessitates the
advancement of automatic model correctness evaluation tools. Such a tool can be
used to evaluate both manually and AI automatically generated models; to
provide feedback to system engineers, and enable AI assistants to self-evaluate
and self-enhance their generated models.
  In this paper, we propose MCeT, the first fully automated tool to evaluate
the correctness of a behavioral model, sequence diagrams in particular, against
its corresponding requirements text and produce a list of issues that the model
has. We utilize LLMs for the correctness evaluation tasks as they have shown
outstanding natural language understanding ability. However, we show that
directly asking an LLM to compare a diagram to requirements finds less than 35%
of issues that experienced engineers can find. We propose to supplement the
direct check with a fine-grained, multi-perspective approach; we split the
diagram into atomic, non-divisible interactions, and split the requirements
text into atomic, self-contained items. We compare the diagram with atomic
requirements and each diagram-atom with the requirements. We also propose a
self-consistency checking approach that combines perspectives to mitigate LLM
hallucinated issues. Our combined approach improves upon the precision of the
direct approach from 0.58 to 0.81 in a dataset of real requirements. Moreover,
the approach finds 90% more issues that the experienced engineers found than
the direct approach, and reports an average of 6 new issues per diagram.

</details>


### [27] [Is LLM-Generated Code More Maintainable \& Reliable than Human-Written Code?](https://arxiv.org/abs/2508.00700)
*Alfred Santa Molison,Marcia Moraes,Glaucia Melo,Fabio Santos,Wesley K. G. Assuncao*

Main category: cs.SE

TL;DR: This study compares the internal quality of LLM-generated and human-written code using SonarQube metrics across Python tasks of varying difficulty. LLM code shows fewer bugs and lower fix efforts but introduces structural issues in complex problems.


<details>
  <summary>Details</summary>
Motivation: To evaluate how well LLMs generate code solutions in terms of software quality (maintainability, reliability) and compare them to human-written code under a systematic analysis framework.

Method: Empirical analysis combining coding tasks datasets, three LLM configurations (zero-shot, few-shot, fine-tuning), and SonarQube quality metrics. Python code solutions were evaluated across introductory, interview, and competition difficulty levels.

Result: LLM-generated code has fewer bugs and lower issue-resolution effort but may introduce new structural problems in complex tasks. Fine-tuning reduces high-severity issues but lowers overall performance.

Conclusion: LLM-generated code demonstrates strong quality attributes overall, but structural weaknesses in complex scenarios necessitate careful validation. The study deepens understanding of LLM capabilities and limitations in code generation.

Abstract: Background: The rise of Large Language Models (LLMs) in software development
has opened new possibilities for code generation. Despite the widespread use of
this technology, it remains unclear how well LLMs generate code solutions in
terms of software quality and how they compare to human-written code. Aims:
This study compares the internal quality attributes of LLM-generated and
human-written code. Method: Our empirical study integrates datasets of coding
tasks, three LLM configurations (zero-shot, few-shot, and fine-tuning), and
SonarQube to assess software quality. The dataset comprises Python code
solutions across three difficulty levels: introductory, interview, and
competition. We analyzed key code quality metrics, including maintainability
and reliability, and the estimated effort required to resolve code issues.
Results: Our analysis shows that LLM-generated code has fewer bugs and requires
less effort to fix them overall. Interestingly, fine-tuned models reduced the
prevalence of high-severity issues, such as blocker and critical bugs, and
shifted them to lower-severity categories, but decreased the model's
performance. In competition-level problems, the LLM solutions sometimes
introduce structural issues that are not present in human-written code.
Conclusion: Our findings provide valuable insights into the quality of
LLM-generated code; however, the introduction of critical issues in more
complex scenarios highlights the need for a systematic evaluation and
validation of LLM solutions. Our work deepens the understanding of the
strengths and limitations of LLMs for code generation.

</details>


### [28] [Tool-Assisted Conformance Checking to Reference Process Models](https://arxiv.org/abs/2508.00738)
*Bernhard Rumpe,Max Stachon,Sebastian Stüber,Valdes Voufo*

Main category: cs.SE

TL;DR: This paper introduces an automated method using causal dependency analysis to check process models against reference frameworks, improving conformance verification accuracy and flexibility.


<details>
  <summary>Details</summary>
Motivation: Existing conformance checking methods rely solely on execution traces, lacking semantic expressiveness and automation for meaningful comparison against reference models, which are essential for ensuring quality and consistency across processes.

Method: The authors integrate their approach into a semantic framework, propose an algorithm for reference model conformance checking, and validate it through a case study while analyzing strengths and limitations.

Result: Evaluations demonstrate enhanced accuracy and flexibility in verifying process model conformance, offering a tool-assisted solution to address semantic comparison challenges previously unresolved by trace-based methods.

Conclusion: The research presents a viable algorithmic approach within a semantic framework for robust reference model conformance checking, advancing beyond prior techniques by enabling causal dependency analysis of tasks and events.

Abstract: Reference models convey best practices and standards. The reference
frameworks necessitate conformance checks to ensure adherence to established
guidelines and principles, which is crucial for maintaining quality and
consistency in various processes. This paper explores automated conformance
checks for concrete process models against reference models using causal
dependency analysis of tasks and events. Existing notions of conformance
checking for process models focus on verifying process execution traces and
lack the expressiveness and automation needed for semantic model comparison,
leaving this question unresolved. We integrate our approach into a broader
semantic framework for defining reference model conformance. We outline an
algorithm for reference process model conformance checking, evaluate it through
a case study, and discuss its strengths and limitations. Our research provides
a tool-assisted solution enhancing accuracy and flexibility in process model
conformance verification.

</details>


### [29] [Dynamic Symbolic Execution for Semantic Difference Analysis of Component and Connector Architectures](https://arxiv.org/abs/2508.00749)
*Johanna Grahl,Bernhard Rumpe,Max Stachon,Sebastian Stüber*

Main category: cs.SE

TL;DR: This paper explores using Dynamic Symbolic Execution (DSE) for semantic difference analysis in component-and-connector architectures (MontiArc) by enhancing execution data collection and evaluating strategies under constraints.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the critical need for ensuring correctness and consistency of evolving models in model-driven development, where traditional methods may fall short.

Method: The authors improved a MontiArc-to-Java generator to capture symbolic/concrete runtime data (transition conditions, states, automata variables) and evaluate DSE strategies across efficiency, minimality, and completeness criteria.

Result: DSE effectively identifies significant execution traces for system behavior insights, but scalability issues limit its practical use in large systems.

Conclusion: While DSE shows potential for semantic difference analysis in architectures, research is needed to overcome scalability limitations for broader application.

Abstract: In the context of model-driven development, ensuring the correctness and
consistency of evolving models is paramount. This paper investigates the
application of Dynamic Symbolic Execution (DSE) for semantic difference
analysis of component-and-connector architectures, specifically utilizing
MontiArc models. We have enhanced the existing MontiArc-to-Java generator to
gather both symbolic and concrete execution data at runtime, encompassing
transition conditions, visited states, and internal variables of automata. This
data facilitates the identification of significant execution traces that
provide critical insights into system behavior. We evaluate various execution
strategies based on the criteria of runtime efficiency, minimality, and
completeness, establishing a framework for assessing the applicability of DSE
in semantic difference analysis. Our findings indicate that while DSE shows
promise for analyzing component and connector architectures, scalability
remains a primary limitation, suggesting further research is needed to enhance
its practical utility in larger systems.

</details>


### [30] [From Code to Career: Assessing Competitive Programmers for Industry Placement](https://arxiv.org/abs/2508.00772)
*Md Imranur Rahman Akib,Fathima Binthe Muhammed,Umit Saha,Md Fazlul Karim Patwary,Mehrin Anannya,Md Alomgeer Hussein,Md Biplob Hosen*

Main category: cs.SE

TL;DR: This paper proposes a system to predict software engineering job readiness for Codeforces users using machine learning, categorizing them into four employability levels via a Random Forest classifier.


<details>
  <summary>Details</summary>
Motivation: The tech industry requires tools to assess programmers' job readiness based on coding performance, motivating the analysis of competitive programming activity's correlation with job opportunity levels.

Method: Data collection through Codeforces API, processing performance metrics, training a Random Forest classifier to predict job readiness, and implementing the system with Flask for real-time deployment.

Result: The model successfully differentiates employability levels (entry-level to top-tier jobs) using coding proficiency and participation metrics, validated through system evaluation.

Conclusion: The work establishes a machine learning foundation for career assessment in software engineering and suggests extensibility to other technical domains.

Abstract: In today's fast-paced tech industry, there is a growing need for tools that
evaluate a programmer's job readiness based on their coding performance. This
study focuses on predicting the potential of Codeforces users to secure various
levels of software engineering jobs. The primary objective is to analyze how a
user's competitive programming activity correlates with their chances of
obtaining positions, ranging from entry-level roles to jobs at major tech
companies. We collect user data using the Codeforces API, process key
performance metrics, and build a prediction model using a Random Forest
classifier. The model categorizes users into four levels of employability,
ranging from those needing further development to those ready for top-tier tech
jobs. The system is implemented using Flask and deployed on Render for
real-time predictions. Our evaluation demonstrates that the approach
effectively distinguishes between different skill levels based on coding
proficiency and participation. This work lays a foundation for the use of
machine learning in career assessment and could be extended to predict job
readiness in broader technical fields.

</details>
