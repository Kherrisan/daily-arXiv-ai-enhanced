{"id": "2509.20382", "categories": ["cs.CR", "cs.AI", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.20382", "abs": "https://arxiv.org/abs/2509.20382", "authors": ["Dilli Hang Rai", "Sabin Kafley"], "title": "Lightweight MobileNetV1+GRU for ECG Biometric Authentication: Federated and Adversarial Evaluation", "comment": "5 pages, 7 figures, 5 tables", "summary": "ECG biometrics offer a unique, secure authentication method, yet their\ndeployment on wearable devices faces real-time processing, privacy, and\nspoofing vulnerability challenges. This paper proposes a lightweight deep\nlearning model (MobileNetV1+GRU) for ECG-based authentication, injection of\n20dB Gaussian noise & custom preprocessing. We simulate wearable conditions and\nedge deployment using the ECGID, MIT-BIH, CYBHi, and PTB datasets, achieving\naccuracies of 99.34%, 99.31%, 91.74%, and 98.49%, F1-scores of 0.9869, 0.9923,\n0.9125, and 0.9771, Precision of 0.9866, 0.9924, 0.9180 and 0.9845, Recall of\n0.9878, 0.9923, 0.9129, and 0.9756, equal error rates (EER) of 0.0009, 0.00013,\n0.0091, and 0.0009, and ROC-AUC values of 0.9999, 0.9999, 0.9985, and 0.9998,\nwhile under FGSM adversarial attacks, accuracy drops from 96.82% to as low as\n0.80%. This paper highlights federated learning, adversarial testing, and the\nneed for diverse wearable physiological datasets to ensure secure and scalable\nbiometrics.", "AI": {"tldr": "The paper proposes a lightweight deep learning model for secure ECG authentication on wearables, achieving high accuracy but showing vulnerabilities to adversarial attacks.", "motivation": "The motivation addresses the need for real-time ECG biometric processing on wearable devices, considering challenges like processing time, privacy concerns, and susceptibility to spoofing attacks.", "method": "The method involves a lightweight model combining MobileNetV1 and GRU, custom preprocessing with added Gaussian noise to simulate wearable conditions, and uses ECGID, MIT-BIH, CYBHi, and PTB datasets for evaluation.", "result": "Results demonstrate strong performance with high accuracy, F1-scores, and low EER across datasets, but the model's accuracy drops severely under FGSM adversarial attacks.", "conclusion": "The conclusion underscores the importance of federated learning, adversarial testing, and diverse datasets to enhance security and scalability in ECG biometrics for wearables."}}
{"id": "2509.20383", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20383", "abs": "https://arxiv.org/abs/2509.20383", "authors": ["Wei Wan", "Yuxuan Ning", "Zhicong Huang", "Cheng Hong", "Shengshan Hu", "Ziqi Zhou", "Yechao Zhang", "Tianqing Zhu", "Wanlei Zhou", "Leo Yu Zhang"], "title": "MARS: A Malignity-Aware Backdoor Defense in Federated Learning", "comment": "NeurIPS 2025", "summary": "Federated Learning (FL) is a distributed paradigm aimed at protecting\nparticipant data privacy by exchanging model parameters to achieve high-quality\nmodel training. However, this distributed nature also makes FL highly\nvulnerable to backdoor attacks. Notably, the recently proposed state-of-the-art\n(SOTA) attack, 3DFed (SP2023), uses an indicator mechanism to determine whether\nthe backdoor models have been accepted by the defender and adaptively optimizes\nbackdoor models, rendering existing defenses ineffective. In this paper, we\nfirst reveal that the failure of existing defenses lies in the employment of\nempirical statistical measures that are loosely coupled with backdoor attacks.\nMotivated by this, we propose a Malignity-Aware backdooR defenSe (MARS) that\nleverages backdoor energy (BE) to indicate the malicious extent of each neuron.\nTo amplify malignity, we further extract the most prominent BE values from each\nmodel to form a concentrated backdoor energy (CBE). Finally, a novel\nWasserstein distance-based clustering method is introduced to effectively\nidentify backdoor models. Extensive experiments demonstrate that MARS can\ndefend against SOTA backdoor attacks and significantly outperforms existing\ndefenses.", "AI": {"tldr": "This paper proposes MARS, a malignity-aware defense mechanism for federated learning to counter backdoor attacks, demonstrating superior performance against existing methods.", "motivation": "The multiplicative weights method in FL is insufficient for backdoor attack prevention as it relies on weakly-correlated statistical measures.", "method": "MARS employs backdoor energy (BE) and concentrated backdoor energy (CBE) to detect malicious neurons, using a Wasserstein distance-based clustering method to identify backdoor models.", "result": "MARS effectively defends against SOTA backdoor attacks and outperforms existing defenses in experiments.", "conclusion": "MARS provides a robust solution for detecting and mitigating backdoor attacks in FL by focusing on backdoor energy and novel clustering techniques."}}
{"id": "2509.20384", "categories": ["cs.CR", "cs.AI", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20384", "abs": "https://arxiv.org/abs/2509.20384", "authors": ["Jiayi Lin", "Liangcai Su", "Junzhe Li", "Chenxiong Qian"], "title": "R1-Fuzz: Specializing Language Models for Textual Fuzzing via Reinforcement Learning", "comment": null, "summary": "Fuzzing is effective for vulnerability discovery but struggles with complex\ntargets such as compilers, interpreters, and database engines, which accept\ntextual input that must satisfy intricate syntactic and semantic constraints.\nAlthough language models (LMs) have attracted interest for this task due to\ntheir vast latent knowledge and reasoning potential, their practical adoption\nhas been limited. The major challenges stem from insufficient exploration of\ndeep program logic among real-world codebases, and the high cost of leveraging\nlarger models. To overcome these challenges, we propose R1-Fuzz, the first\nframework that leverages reinforcement learning (RL) to specialize\ncost-efficient LMs and integrate them for complex textual fuzzing input\ngeneration. R1-Fuzz introduces two key designs: coverage-slicing-based question\nconstruction and a distance-based reward calculation. Through RL-based\npost-training of a model with our constructed dataset, R1-Fuzz designs a\nfuzzing workflow that tightly integrates LMs to reason deep program semantics\nduring fuzzing. Evaluations on diverse real-world targets show that our design\nenables a small model, named R1-Fuzz-7B, to rival or even outperform much\nlarger models in real-world fuzzing. Notably, R1-Fuzz achieves up to 75\\%\nhigher coverage than state-of-the-art fuzzers and discovers 29 previously\nunknown vulnerabilities, demonstrating its practicality.", "AI": {"tldr": "R1-Fuzz enables efficient, effective fuzzing of complex systems via RL-specialized LMs, achieving 75% coverage gains and discovering 29 new vulnerabilities using a cost-effective small model.", "motivation": "Traditional fuzzing struggles with complex targets due to syntactic/semantic constraints, while existing LM-based approaches face (1) Inadequate exploration of deep program logic and (2) High computational costs of large models. This limits practical adoption despite LMs' potential for code understanding.", "method": "The framework introduces: (1) Coverage-slicing-based question construction to guide LM training, (2) Distance-based reward calculation for RL post-training, and (3) An integrated fuzzing workflow that leverages LMs for semantic reasoning during input generation. This enables cost-effective specialization of language models for syntactic/semantic constraints of complex systems.", "result": "Evaluations show R1-Fuzz achieves 75% higher code coverage than state-of-the-art fuzzers. Using a 7B parameter model (R1-Fuzz-7B), it discovers 29 new vulnerabilities in real-world targets, outperforming significantly larger LM-based approaches while maintaining cost-efficiency.", "conclusion": "R1-Fuzz addresses challenges in real-world codebase fuzzing by combining reinforcement learning with language models, achieving state-of-the-art coverage and vulnerability discovery while maintaining cost-efficiency. The approach demonstrates that smaller specialized LMs can match or exceed larger models' performance in complex fuzzing tasks."}}
{"id": "2509.20388", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20388", "abs": "https://arxiv.org/abs/2509.20388", "authors": ["Amir AL-Maamari"], "title": "Can You Trust Your Copilot? A Privacy Scorecard for AI Coding Assistants", "comment": null, "summary": "The rapid integration of AI-powered coding assistants into developer\nworkflows has raised significant privacy and trust concerns. As developers\nentrust proprietary code to services like OpenAI's GPT, Google's Gemini, and\nGitHub Copilot, the unclear data handling practices of these tools create\nsecurity and compliance risks. This paper addresses this challenge by\nintroducing and applying a novel, expert-validated privacy scorecard. The\nmethodology involves a detailed analysis of four document types; from legal\npolicies to external audits; to score five leading assistants against 14\nweighted criteria. A legal expert and a data protection officer refined these\ncriteria and their weighting. The results reveal a distinct hierarchy of\nprivacy protections, with a 20-point gap between the highest- and lowest-ranked\ntools. The analysis uncovers common industry weaknesses, including the\npervasive use of opt-out consent for model training and a near-universal\nfailure to filter secrets from user prompts proactively. The resulting\nscorecard provides actionable guidance for developers and organizations,\nenabling evidence-based tool selection. This work establishes a new benchmark\nfor transparency and advocates for a shift towards more user-centric privacy\nstandards in the AI industry.", "AI": {"tldr": "Paper evaluates AI coding assistants' privacy practices using an expert-scored framework, uncovering risks and promoting evidence-based tool selection to drive better privacy standards.", "motivation": "The motivation stems from growing privacy and trust concerns as developers entrust proprietary code to AI coding assistants, where unclear data handling practices pose security and compliance risks.", "method": "The paper introduces a privacy scorecard evaluated by a legal expert and data protection officer, analyzing four document types against 14 weighted criteria across five leading AI assistants.", "result": "Results reveal a 20-point gap between top and bottom performers, with common weaknesses including opt-out consent models and failure to proactively filter secrets from user prompts.", "conclusion": "This paper concludes that a novel privacy scorecard establishes a new benchmark for transparency in AI-powered coding assistants, advocating for user-centric privacy standards and providing actionable guidance for developers to select tools based on evidence."}}
{"id": "2509.20380", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.20380", "abs": "https://arxiv.org/abs/2509.20380", "authors": ["Samyak Jhaveri", "Vanessa Klotzmann", "Crista Lopes"], "title": "ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation", "comment": null, "summary": "The increasing ubiquity of GPUs is accompanied by the increasing complexity\nof their hardware and parallel programming frameworks. Directive-based parallel\nprogramming standards like OpenACC simplify GPU programming to some extent by\nabstracting away low-level complexities, but a fair amount of expertise is\nstill required in order to use those directives effectively.\n  We introduce ACCeLLiuM, two open weights Large Language Models specifically\nfine-tuned for generating expert OpenACC directives for data-parallel loops,\nalong with the supervised fine-tuning dataset that was used to train them. The\nACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from\npublic GitHub C/C++ repositories, with 3,223 pairs for training and 810 for\ntesting. Experimental evaluations show a pronounced performance gap in\ngenerating correct OpenACC pragmas between base LLMs and our fine-tuned\nversions. On the held-out test set, base LLMs fail to consistently generate\nvalid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid\npragmas with the correct directive type for $87\\%$ of the data-parallel loops,\nand exact pragmas--including directives, clauses, clause order, and clause\nvariables--for $50\\%$ of the cases. Even when not exact, generated pragmas\nfrequently incorporate the correct clauses in a different order than the\nground-truth label, or include additional clauses that enable finer control\nover parallel execution, data movement, and concurrency, offering practical\nvalue beyond strict string-matching. By publicly releasing the code, models,\nand dataset as ACCeLLiuM we hope to establish a reproducible benchmark for\nLLM-powered OpenACC pragma generation, and lower the barrier to automated GPU\noffloading of serially written programs.", "AI": {"tldr": "ACCeLLiuM is a fine-tuned LLM that automates expert-level OpenACC directive generation, matching 50% of cases exactly and validly 87% of the time, enabling easier GPU programming through dataset and model sharing.", "motivation": "Existing parallel programming frameworks like OpenACC require significant expertise, and while they simplify GPU programming, there is a need for tools to automate directive generation for data-parallel loops.", "method": "The authors fine-tuned open-weight LLMs using a supervised dataset (ACCeLLiuM SFT) composed of 4,033 OpenACC pragma-loop pairs mined from GitHub. The dataset was split into 3,223 training and 810 testing samples.", "result": "ACCeLLiuM models achieved 87% accuracy in generating valid pragmas with correct directive types, and 50% exact matching (including clauses and variables). Generated pragmas often included useful clause variations even when not exact.", "conclusion": "The study concludes that ACCeLLiuM provides a reproducible benchmark for automating OpenACC pragmas, significantly improves over base LLMs in generating valid directives, and reduces barriers to GPU programming through model and dataset release."}}
{"id": "2509.20391", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20391", "abs": "https://arxiv.org/abs/2509.20391", "authors": ["Md. Alamgir Hossain", "Waqas Ishtiaq", "Md. Samiul Islam"], "title": "A Comparative Analysis of Ensemble-Based Machine Learning Approaches with Explainable AI for Multi-Class Intrusion Detection in Drone Networks", "comment": "27 pages, 18 figures, 10 tables", "summary": "The growing integration of drones into civilian, commercial, and defense\nsectors introduces significant cybersecurity concerns, particularly with the\nincreased risk of network-based intrusions targeting drone communication\nprotocols. Detecting and classifying these intrusions is inherently challenging\ndue to the dynamic nature of drone traffic and the presence of multiple\nsophisticated attack vectors such as spoofing, injection, replay, and\nman-in-the-middle (MITM) attacks. This research aims to develop a robust and\ninterpretable intrusion detection framework tailored for drone networks, with a\nfocus on handling multi-class classification and model explainability. We\npresent a comparative analysis of ensemble-based machine learning models,\nnamely Random Forest, Extra Trees, AdaBoost, CatBoost, and XGBoost, trained on\na labeled dataset comprising benign traffic and nine distinct intrusion types.\nComprehensive data preprocessing was performed, including missing value\nimputation, scaling, and categorical encoding, followed by model training and\nextensive evaluation using metrics such as macro F1-score, ROC AUC, Matthews\nCorrelation Coefficient, and Log Loss. Random Forest achieved the highest\nperformance with a macro F1-score of 0.9998 and ROC AUC of 1.0000. To validate\nthe superiority of the models, statistical tests, including Friedmans test, the\nWilcoxon signed-rank test with Holm correction, and bootstrapped confidence\nintervals, were applied. Furthermore, explainable AI methods, SHAP and LIME,\nwere integrated to interpret both global and local feature importance,\nenhancing model transparency and decision trustworthiness. The proposed\napproach not only delivers near-perfect accuracy but also ensures\ninterpretability, making it highly suitable for real-time and safety-critical\ndrone operations.", "AI": {"tldr": "This study addresses cybersecurity threats in drone networks by developing a high-performing intrusion detection framework using ensemble machine learning models, emphasizing interpretability for real-time applications.", "motivation": "Drones are increasingly integrated into multiple sectors, creating vulnerabilities to sophisticated network-based intrusions. Existing intrusion detection solutions lack the necessary robustness and interpretability for such dynamic environments.", "method": "The authors conduct a comparative analysis of ensemble-based machine learning models (Random Forest, Extra Trees, AdaBoost, CatBoost, XGBoost) using a labeled dataset with benign traffic and nine intrusion types. They apply comprehensive data preprocessing and evaluate their results using statistical tests (Friedman's test, Wilcoxon signed-rank test with Holm correction) and performance metrics (macro F1-score, ROC AUC, etc.). For explainability,they use SHAP and LIME to interpret global and local feature importance.", "result": "Random Forest outperforms other models with a macro F1-score of 0.9998 and ROC AUC of 1.0000. Statistical validation confirms the effectiveness of the ensemble approach. SHAP and LIME successfully identify critical features for intrusion detection.", "conclusion": "The proposed framework provides a robust, accurate, and interpretable solution for securing drone networks against diverse cyber threats, making it suitable for real-time applications in safety-critical environments."}}
{"id": "2509.20385", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20385", "abs": "https://arxiv.org/abs/2509.20385", "authors": ["Ishara Devendra", "Chaman Wijesiriwardana", "Prasad Wimalaratne"], "title": "State-of-the-Art in Software Security Visualization: A Systematic Review", "comment": null, "summary": "Software security visualization is an interdisciplinary field that combines\nthe technical complexity of cybersecurity, including threat intelligence and\ncompliance monitoring, with visual analytics, transforming complex security\ndata into easily digestible visual formats. As software systems get more\ncomplex and the threat landscape evolves, traditional text-based and numerical\nmethods for analyzing and interpreting security concerns become increasingly\nineffective. The purpose of this paper is to systematically review existing\nresearch and create a comprehensive taxonomy of software security visualization\ntechniques through literature, categorizing these techniques into four types:\ngraph-based, notation-based, matrix-based, and metaphor-based visualization.\nThis systematic review explores over 60 recent key research papers in software\nsecurity visualization, highlighting its key issues, recent advancements, and\nprospective future research directions. From the comprehensive analysis, the\ntwo main areas were distinctly highlighted as extensive software development\nvisualization, focusing on advanced methods for depicting software\narchitecture: operational security visualization and cybersecurity\nvisualization. The findings highlight the necessity for innovative\nvisualization techniques that adapt to the evolving security landscape, with\npractical implications for enhancing threat detection, improving security\nresponse strategies, and guiding future research.", "AI": {"tldr": "This paper reviews and categorizes software security visualization techniques into a taxonomy, highlighting key issues, advancements, and future research directions in the field.", "motivation": "As software systems become more complex and cybersecurity threats evolve, traditional text-based methods for analyzing security data are no longer sufficient. There is a need for effective visualization techniques to make complex security information more understandable and actionable.", "method": "The authors conduct a systematic review of over 60 recent research papers on software security visualization, analyzing the literature to identify and categorize techniques into four types: graph-based, notation-based, matrix-based, and metaphor-based visualization.", "result": "The review results in a comprehensive taxonomy of software security visualization techniques. It highlights two main areas: extensive software development visualization (focused on software architecture) and operational security/cybersecurity visualization. The paper emphasizes the importance of innovative visualization approaches to address current and future security challenges.", "conclusion": "Software security visualization is a critical area of research given the growing complexity of systems and evolving threats. The proposed taxonomy provides a framework for understanding existing techniques and guides future work in creating more effective visual solutions to enhance threat detection and response strategies."}}
{"id": "2509.20395", "categories": ["cs.CR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20395", "abs": "https://arxiv.org/abs/2509.20395", "authors": ["Noam Schmitt", "Marc Antoine Lacoste"], "title": "Centralized vs. Decentralized Security for Space AI Systems? A New Look", "comment": "IEEE HPEC 2025 - 29th Annual IEEE High Performance Extreme Computing\n  Virtual Conference, MIT Lincoln Laboratory, Sep 2025, Boston (MA), United\n  States", "summary": "This paper investigates the trade-off between centralized and decentralized\nsecurity management in constellations of satellites to balance security and\nperformance. We highlight three key AI architectures for automated security\nmanagement: (a) centralized, (b) distributed and (c) federated. The centralized\narchitecture is the best option short term, providing fast training, despite\nthe hard challenge of the communication latency overhead across space.\nDecentralized architectures are better alternatives in the longer term,\nproviding enhanced scalability and security.", "AI": {"tldr": "The paper compares centralized, distributed, and federated AI architectures for satellite constellation security management, finding centralized systems optimal short-term for fast training but hindered by communication latency, while decentralized approaches offer better long-term scalability and security.", "motivation": "Satellite constellations require balancing security and performance as systems scale. Traditional centralized security management introduces latency, while decentralized approaches remain underexplored.", "method": "Analysis of three AI architectures (centralized, distributed, federated), evaluating their trade-offs in training speed, communication latency, scalability, and security for space systems.", "result": "Centralized architectures enable rapid training but face latency challenges. Decentralized architectures provide superior long-term scalability and security but require addressing coordination complexities.", "conclusion": "Decentralized security management is positioned as the optimal long-term solution for satellite constellations despite short-term technical challenges, emphasizing the need for adaptive architectural choices based on deployment timelines."}}
{"id": "2509.20386", "categories": ["cs.SE", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20386", "abs": "https://arxiv.org/abs/2509.20386", "authors": ["Nishant Gaurav", "Adit Akarsh", "Ankit Ranjan", "Manoj Bajaj"], "title": "Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments", "comment": null, "summary": "We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef-\nficiently operate with extensive Model Control Protocol (MCP) tool sets that\nexceed the contextual memory limitations of large language models. Our approach\naddresses the fundamental challenge of tool selection in environments\ncontaining hundreds or thousands of available tools, where loading all tools\nsimultaneously is computationally infeasible. We propose and evaluate five\ndistinct architectures that progressively refine the tool selection process,\nculminating in a search-and-load mechanism that achieves intelligent tool\nselection with minimal computational overhead. Our experimental results\ndemonstrate that the proposed approach reduces tool loading by up to 50% while\nmaintaining task completion accuracy, advancing the path towards truly\ngeneral-purpose AI agents capable of dynamically adapting to diverse task\nenvironments.", "AI": {"tldr": "Dynamic ReAct enables efficient tool selection for AI agents in large-scale environments, reducing computational overhead without sacrificing performance.", "motivation": "Large language models face computational infeasibility when handling hundreds/thousands of tools due to contextual memory limitations, necessitating efficient tool selection strategies.", "method": "Five progressively refined architectures are proposed, culminating in a search-and-load mechanism to optimize tool selection from extensive MCP tool sets.", "result": "Achieved up to 50% reduction in tool loading while maintaining task completion accuracy through the proposed search-and-load mechanism.", "conclusion": "Dynamic ReAct advances the development of general-purpose AI agents by enabling dynamic adaptation to diverse task environments through efficient tool selection."}}
{"id": "2509.20399", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20399", "abs": "https://arxiv.org/abs/2509.20399", "authors": ["Birk Torpmann-Hagen", "Michael A. Riegler", "P\u00e5l Halvorsen", "Dag Johansen"], "title": "Defending against Stegomalware in Deep Neural Networks with Permutation Symmetry", "comment": null, "summary": "Deep neural networks are being utilized in a growing number of applications,\nboth in production systems and for personal use. Network checkpoints are as a\nconsequence often shared and distributed on various platforms to ease the\ndevelopment process. This work considers the threat of neural network\nstegomalware, where malware is embedded in neural network checkpoints at a\nnegligible cost to network accuracy. This constitutes a significant security\nconcern, but is nevertheless largely neglected by the deep learning\npractitioners and security specialists alike. We propose the first effective\ncountermeasure to these attacks. In particular, we show that state-of-the-art\nneural network stegomalware can be efficiently and effectively neutralized\nthrough shuffling the column order of the weight- and bias-matrices, or\nequivalently the channel-order of convolutional layers. We show that this\neffectively corrupts payloads that have been embedded by state-of-the-art\nmethods in neural network steganography at no cost to network accuracy,\noutperforming competing methods by a significant margin. We then discuss\npossible means by which to bypass this defense, additional defense methods, and\nadvocate for continued research into the security of machine learning systems.", "AI": {"tldr": "This paper introduces an effective defense against stegomalware in neural networks through weight matrix shuffling, neutralizing malware without accuracy loss and exceeding existing methods' performance.", "motivation": "The work highlights the security risk of neural network stegomalware - malware embedded in shared checkpoints with negligible accuracy loss - which is neglected by both deep learning practitioners and security specialists despite its significant threat potential.", "method": "The authors propose shuffling the column order of weight-/bias-matrices (or channel-order in convolutional layers) to corrupt embedded stegomalware payloads while preserving network accuracy. They experimentally demonstrate this defense's effectiveness against state-of-the-art stegomalware.", "result": "The proposed defense outperforms existing methods by a significant margin in neutralizing stegomalware while maintaining accuracy. The paper also analyzes potential bypasses and suggests additional defense strategies.", "conclusion": "The paper concludes that shuffling weight and bias matrix columns in neural networks effectively neutralizes stegal malware without compromising accuracy, addressing a critical yet overlooked security threat in deep learning. Continued research into machine learning security is advocated."}}
{"id": "2509.20387", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20387", "abs": "https://arxiv.org/abs/2509.20387", "authors": ["Qusai Ramadan", "Jukka Ruohonen", "Abhishek Tiwari", "Adam Alami", "Zeyd Boukhers"], "title": "Towards Systematic Specification and Verification of Fairness Requirements: A Position Paper", "comment": "Accepted at the 2025 IEEE 33rd International Requirements Engineering\n  Conference Workshops", "summary": "Decisions suggested by improperly designed software systems might be prone to\ndiscriminate against people based on protected characteristics, such as gender\nand ethnicity. Previous studies attribute such undesired behavior to flaws in\nalgorithmic design or biased data. However, these studies ignore that\ndiscrimination is often the result of a lack of well-specified fairness\nrequirements and their verification. The fact that experts' knowledge about\nfairness is often implicit makes the task of specifying precise and verifiable\nfairness requirements difficult. In related domains, such as security\nengineering, knowledge graphs have been proven to be effective in formalizing\nknowledge to assist requirements specification and verification. To address the\nlack of formal mechanisms for specifying and verifying fairness requirements,\nwe propose the development of a knowledge graph-based framework for fairness.\nIn this paper, we discuss the challenges, research questions, and a road map\ntowards addressing the research questions.", "AI": {"tldr": "This paper identifies gaps in fairness requirement specification, proposes a knowledge graph framework to formalize fairness, and presents a roadmap for addressing these challenges in software systems.", "motivation": "Existing studies overlook the critical role of insufficiently specified fairness requirements and their verification, compounded by the implicit nature of experts' fairness knowledge, leading to potential discrimination in software systems.", "method": "The paper introduces a knowledge graph-based framework inspired by security engineering practices, aiming to formalize implicit expert knowledge about fairness into verifiable requirements.", "result": "The paper outlines challenges, research questions, and a roadmap for developing the proposed framework, establishing a foundation for future work on formal fairness requirements.", "conclusion": "The proposed knowledge graph-based framework offers a structured approach to formalize and verify fairness requirements, addressing the lack of explicit mechanisms in current software systems to prevent discrimination."}}
{"id": "2509.20405", "categories": ["cs.CR", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.20405", "abs": "https://arxiv.org/abs/2509.20405", "authors": ["Visar Berisha", "Prad Kadambi", "Isabella Lenz"], "title": "Why Speech Deepfake Detectors Won't Generalize: The Limits of Detection in an Open World", "comment": null, "summary": "Speech deepfake detectors are often evaluated on clean, benchmark-style\nconditions, but deployment occurs in an open world of shifting devices,\nsampling rates, codecs, environments, and attack families. This creates a\n``coverage debt\" for AI-based detectors: every new condition multiplies with\nexisting ones, producing data blind spots that grow faster than data can be\ncollected. Because attackers can target these uncovered regions, worst-case\nperformance (not average benchmark scores) determines security. To demonstrate\nthe impact of the coverage debt problem, we analyze results from a recent\ncross-testing framework. Grouping performance by bona fide domain and spoof\nrelease year, two patterns emerge: newer synthesizers erase the legacy\nartifacts detectors rely on, and conversational speech domains\n(teleconferencing, interviews, social media) are consistently the hardest to\nsecure. These findings show that detection alone should not be relied upon for\nhigh-stakes decisions. Detectors should be treated as auxiliary signals within\nlayered defenses that include provenance, personhood credentials, and policy\nsafeguards.", "AI": {"tldr": "Speech deepfake detectors face 'coverage debt' due to real-world deployment challenges like device shifts and new attack methods, creating blind spots that weaken security. New synthesizers eliminate detectable artifacts while conversational speech domains remain especially insecure, arguing detectors must be part of multi-layered defenses.", "motivation": "Existing evaluations focus on controlled conditions, but real-world deployment encounters diverse devices/codecs/environments, which create exponentially-growing data blind spots. Security risks arise since attackers can specifically exploit these uncovered regions.", "method": "Analyzed cross-testing framework results by grouping performance across 1. bona fide domains (e.g., teleconferencing) and 2. spoof release years to identify patterns in detection vulnerabilities.", "result": "1. New synthesizers remove detectable artifacts of older methods | 2. Conversational speech domains consistently exhibit lowest detection accuracy | 3. Worst-case performance reveals critical security gaps", "conclusion": "Reliance on detection alone is insufficient for high-stakes applications. Detection should be augmented with layered security approaches combining provenance tracking, personhood verification, and policy safeguards to address coverage debt."}}
{"id": "2509.20415", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20415", "abs": "https://arxiv.org/abs/2509.20415", "authors": ["Yu Pan", "Xiaocheng Li", "Hanzhao Wang"], "title": "Online-Optimized RAG for Tool Use and Function Calling", "comment": null, "summary": "In many applications, retrieval-augmented generation (RAG) drives tool use\nand function calling by embedding the (user) queries and matching them to\npre-specified tool/function descriptions. In this paper, we address an\nembedding misalignment issue that often arises in practical applications due to\nimperfect embedding models or noisy descriptions; such misalignment may lead to\nincorrect retrieval and task failure. We introduce Online-Optimized RAG, a\ndeployment-time framework that continually adapts retrieval embeddings from\nlive interactions using minimal feedback (e.g., task success). Online-Optimized\nRAG applies lightweight online gradient updates with negligible per-query\nlatency and requires no changes to the underlying LLM. The method is\nplug-and-play: it supports both single- and multi-hop tool use, dynamic tool\ninventories, and $K$-retrieval with re-ranking. We provide a problem-dependent\ntheoretical analysis that quantifies how the method's performance depends on\nthe initialization quality of the embeddings and other related quantities.\nAcross diverse tool-use and document-retrieval scenarios, our Online-Optimized\nRAG consistently improves tool selection accuracy and end-task success, thus\nproviding a simple, practical path to robust, self-improving RAG systems.", "AI": {"tldr": "A framework for RAG systems that continuously adapts query embeddings during deployment using minimal feedback to improve retrieval accuracy and task success.", "motivation": "Current RAG systems often fail due to embedding misalignment caused by imperfect models or noisy descriptions, leading to incorrect function/tool retrieval. The paper addresses this gap by proposing a real-time adaptation framework.", "method": "The method employs lightweight online gradient updates to adjust retrieval embeddings using minimal feedback (e.g., task success) from live interactions. It is designed to be plug-and-play, supporting various retrieval configurations and requiring no modifications to the underlying LLM.", "result": "Online-Optimized RAG consistently improves tool selection accuracy and end-task success across diverse scenarios, demonstrating robustness and scalability for practical applications.", "conclusion": "The study introduces Online-Optimized RAG, which provides a simple and practical framework for self-improving RAG systems by continuously optimizing embeddings during deployment while maintaining compatibility with existing LLMs."}}
{"id": "2509.20411", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20411", "abs": "https://arxiv.org/abs/2509.20411", "authors": ["Tharcisse Ndayipfukamiye", "Jianguo Ding", "Doreen Sebastian Sarwatt", "Adamu Gaston Philipo", "Huansheng Ning"], "title": "Adversarial Defense in Cybersecurity: A Systematic Review of GANs for Threat Detection and Mitigation", "comment": "35 pages, 10 tables, 4figures", "summary": "Machine learning-based cybersecurity systems are highly vulnerable to\nadversarial attacks, while Generative Adversarial Networks (GANs) act as both\npowerful attack enablers and promising defenses. This survey systematically\nreviews GAN-based adversarial defenses in cybersecurity (2021--August 31,\n2025), consolidating recent progress, identifying gaps, and outlining future\ndirections. Using a PRISMA-compliant systematic literature review protocol, we\nsearched five major digital libraries. From 829 initial records, 185\npeer-reviewed studies were retained and synthesized through quantitative trend\nanalysis and thematic taxonomy development. We introduce a four-dimensional\ntaxonomy spanning defensive function, GAN architecture, cybersecurity domain,\nand adversarial threat model. GANs improve detection accuracy, robustness, and\ndata utility across network intrusion detection, malware analysis, and IoT\nsecurity. Notable advances include WGAN-GP for stable training, CGANs for\ntargeted synthesis, and hybrid GAN models for improved resilience. Yet,\npersistent challenges remain such as instability in training, lack of\nstandardized benchmarks, high computational cost, and limited explainability.\nGAN-based defenses demonstrate strong potential but require advances in stable\narchitectures, benchmarking, transparency, and deployment. We propose a roadmap\nemphasizing hybrid models, unified evaluation, real-world integration, and\ndefenses against emerging threats such as LLM-driven cyberattacks. This survey\nestablishes the foundation for scalable, trustworthy, and adaptive GAN-powered\ndefenses.", "AI": {"tldr": "This survey examines GAN-based adversarial defenses in cybersecurity from 2021 to 2025, highlighting their benefits and current challenges. It introduces a taxonomy, reviews techniques like WGAN-GP, and suggests future research directions.", "motivation": "The motivation of this paper is to comprehensively review the advancements and challenges in GAN-based adversarial defenses for cybersecurity, given GANs' dual role as both powerful attack enablers and strong defenses against adversarial threats.", "method": "The method used in this paper involves a PRISMA-compliant systematic literature review protocol across five major digital libraries. Data synthesis included quantitative trend analysis and the development of a thematic taxonomy from 185 peer-reviewed studies selected from 829 initial records.", "result": "The result of the survey reveals that GANs have enhanced detection accuracy, robustness, and data utility in areas like network intrusion detection and malware analysis. Techniques such as WGAN-GP, CGANs, and hybrid GAN models have shown notable improvements, but challenges in training stability and benchmarking persist.", "conclusion": "The paper concludes GAN-based defenses hold significant potential for cybersecurity but require improvements in training stability, benchmarking standards, and deployment strategies to achieve scalable and adaptive solutions against emerging threats, such as LLM-driven attacks."}}
{"id": "2509.20421", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20421", "abs": "https://arxiv.org/abs/2509.20421", "authors": ["Reiner H\u00e4hnle", "Cosimo Laneve", "Adele Veschetti"], "title": "Formal Verification of Legal Contracts: A Translation-based Approach", "comment": null, "summary": "Stipula is a domain-specific programming language designed to model legal\ncontracts with enforceable properties, especially those involving asset\ntransfers and obligations. This paper presents a methodology to formally verify\nthe correctness of Stipula contracts through translation into Java code\nannotated with Java Modeling Language specifications. As a verification\nbackend, the deductive verification tool KeY is used. Both, the translation and\nthe verification of partial and total correctness for a large subset of Stipula\ncontracts, those with disjoint cycles, is fully automatic. Our work\ndemonstrates that a general-purpose deductive verification tool can be used\nsuccessfully in a translation approach.", "AI": {"tldr": "Stipula is a domain-specific language for legal contracts, with a methodology to verify their correctness by translating them into Java with JML annotations and using the KeY verification tool. The approach is fully automatic for contracts with disjoint cycles.", "motivation": "The paper addresses the need for formal verification of legal contracts involving asset transfers and obligations, ensuring their correctness and enforceable properties.", "method": "The authors present a translation methodology that converts Stipula contracts into Java code annotated with Java Modeling Language (JML) specifications. They use the deductive verification tool KeY to verify partial and total correctness, focusing on contracts with disjoint cycles.", "result": "The methodology achieves fully automatic verification of a large subset of Stipula contracts (those with disjoint cycles), demonstrating the effectiveness of using general-purpose deductive verification tools via translation.", "conclusion": "The work demonstrates that general-purpose deductive verification tools like KeY can be successfully applied to domain-specific languages such as Stipula through translation, enabling reliable verification of legal contracts."}}
{"id": "2509.20418", "categories": ["cs.CR", "cs.AI", "cs.ET", "K.6.5; I.2.0"], "pdf": "https://arxiv.org/pdf/2509.20418", "abs": "https://arxiv.org/abs/2509.20418", "authors": ["Grace Billiris", "Asif Gill", "Madhushi Bandara"], "title": "A Taxonomy of Data Risks in AI and Quantum Computing (QAI) - A Systematic Review", "comment": "11 pages, 2 figures, 2 tables", "summary": "Quantum Artificial Intelligence (QAI), the integration of Artificial\nIntelligence (AI) and Quantum Computing (QC), promises transformative advances,\nincluding AI-enabled quantum cryptography and quantum-resistant encryption\nprotocols. However, QAI inherits data risks from both AI and QC, creating\ncomplex privacy and security vulnerabilities that are not systematically\nstudied. These risks affect the trustworthiness and reliability of AI and QAI\nsystems, making their understanding critical. This study systematically reviews\n67 privacy- and security-related studies to expand understanding of QAI data\nrisks. We propose a taxonomy of 22 key data risks, organised into five\ncategories: governance, risk assessment, control implementation, user\nconsiderations, and continuous monitoring. Our findings reveal vulnerabilities\nunique to QAI and identify gaps in holistic risk assessment. This work\ncontributes to trustworthy AI and QAI research and provides a foundation for\ndeveloping future risk assessment tools.", "AI": {"tldr": "This study creates a 22-risk taxonomy for QAI, revealing unique vulnerabilities and gaps, offering a framework for future risk assessment tools.", "motivation": "Quantum AI inherits data risks from both AI and QC, creating under-studied privacy/security vulnerabilities that impact system trustworthiness, necessitating systematic understanding for reliable development.", "method": "Systematic review of 67 privacy/security-related studies, resulting in a taxonomy of 22 key data risks organized into five categories (governance, risk assessment, control implementation, user considerations, and continuous monitoring).", "result": "Revealed QAI-specific vulnerabilities, identified gaps in holistic risk assessment, and established a structured risk taxonomy, contributing to the foundation of future risk assessment tools.", "conclusion": "The study provides a foundational framework for trustworthy AI and QAI research, advancing future risk assessment tool development by systematically identifying QAI-specific data vulnerabilities and assessment gaps."}}
{"id": "2509.20491", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20491", "abs": "https://arxiv.org/abs/2509.20491", "authors": ["Brahim Mahmoudi", "Naouel Moha", "Quentin Stievenert", "Florent Avellaneda"], "title": "AI-Specific Code Smells: From Specification to Detection", "comment": null, "summary": "The rise of Artificial Intelligence (AI) is reshaping how software systems\nare developed and maintained. However, AI-based systems give rise to new\nsoftware issues that existing detection tools often miss. Among these, we focus\non AI-specific code smells, recurring patterns in the code that may indicate\ndeeper problems such as unreproducibility, silent failures, or poor model\ngeneralization. We introduce SpecDetect4AI, a tool-based approach for the\nspecification and detection of these code smells at scale. This approach\ncombines a high-level declarative Domain-Specific Language (DSL) for rule\nspecification with an extensible static analysis tool that interprets and\ndetects these rules for AI-based systems. We specified 22 AI-specific code\nsmells and evaluated SpecDetect4AI on 826 AI-based systems (20M lines of code),\nachieving a precision of 88.66% and a recall of 88.89%, outperforming other\nexisting detection tools. Our results show that SpecDetect4AI supports the\nspecification and detection of AI-specific code smells through dedicated rules\nand can effectively analyze large AI-based systems, demonstrating both\nefficiency and extensibility (SUS 81.7/100).", "AI": {"tldr": "This paper introduces SpecDetect4AI, a DSL-based static analysis tool detecting 22 AI-specific code smells with 88% precision/recall, validated on 20M lines of code across 826 systems, showcasing scalable AI code quality analysis.", "motivation": "Traditional tools miss AI-specific code smells that impact reproducibility, failure detection, and model generalization in modern AI systems.", "method": "The approach combines a DSL for specifying AI-specific code smells with an extensible static analysis tool, validated through 22 smell patterns and large-scale evaluation on 826 AI systems (20M lines of code).", "result": "Achieves 88.66% precision and 88.89% recall, outperforming existing tools, with SUS score 81.7/100 demonstrating usability and extensibility for large-scale systems.", "conclusion": "SpecDetect4AI effectively detects AI-specific code smells with high precision and recall, demonstrating efficiency, extensibility, and scalability for large-scale AI systems."}}
{"id": "2509.20460", "categories": ["cs.CR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.20460", "abs": "https://arxiv.org/abs/2509.20460", "authors": ["Andrew Campbell", "Anna Scaglione", "Hang Liu", "Victor Elvira", "Sean Peisert", "Daniel Arnold"], "title": "Differential Privacy of Network Parameters from a System Identification Perspective", "comment": null, "summary": "This paper addresses the problem of protecting network information from\nprivacy system identification (SI) attacks when sharing cyber-physical system\nsimulations. We model analyst observations of networked states as time-series\noutputs of a graph filter driven by differentially private (DP) nodal\nexcitations, with the analyst aiming to infer the underlying graph shift\noperator (GSO). Unlike traditional SI, which estimates system parameters, we\nstudy the inverse problem: what assumptions prevent adversaries from\nidentifying the GSO while preserving utility for legitimate analysis. We show\nthat applying DP mechanisms to inputs provides formal privacy guarantees for\nthe GSO, linking the $(\\epsilon,\\delta)$-DP bound to the spectral properties of\nthe graph filter and noise covariance. More precisely, for DP Gaussian signals,\nthe spectral characteristics of both the filter and noise covariance determine\nthe privacy bound, with smooth filters and low-condition-number covariance\nyielding greater privacy.", "AI": {"tldr": "This paper investigates privacy protection against graph structure identification attacks in cyber-physical system simulations by applying differential privacy (DP) to input signals, linking privacy guarantees to the spectral properties of graph filters and noise covariance.", "motivation": "Traditional privacy-preserving methods focus on parameter estimation, but this work addresses the inverse problem: preventing adversaries from inferring the graph shift operator (GSO) while maintaining utility for legitimate analysis. Such a gap exists as prior work lacks formal privacy bounds for graph structure protection in simulation data sharing scenarios.", "method": "The authors model the analyst's observations as time-series outputs of a graph filter driven by DP nodal excitation signals. They mathematically analyze how spectral characteristics of the filter and covariance matrix of the DP noise influence the $(\\epsilon,\\delta)$-DP guarantees for the GSO, focusing on Gaussian signal scenarios.", "result": "They derive formal privacy bounds for the GSO based on the interplay between filter smoothness, noise covariance condition number, and DP parameters. Results show smoother filters and low-condition-number noise covariances yield stronger privacy protection without compromising utility.", "conclusion": "The paper demonstrates that input-level DP mechanisms can preserve graph structure privacy in simulations while maintaining analytical utility, providing actionable insights into designing DP systems by optimizing spectral characteristics of graph filters and noise structures."}}
{"id": "2509.20497", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20497", "abs": "https://arxiv.org/abs/2509.20497", "authors": ["Ahmed Aljohani", "Hyunsook Do"], "title": "PromptDebt: A Comprehensive Study of Technical Debt Across LLM Projects", "comment": "Accepted at Proceedings of the 2025 Evaluation and Assessment in\n  Software Engineering (EASE '25)", "summary": "Large Language Models (LLMs) are increasingly embedded in software via APIs\nlike OpenAI, offering powerful AI features without heavy infrastructure. Yet\nthese integrations bring their own form of self-admitted technical debt (SATD).\nIn this paper, we present the first large-scale empirical study of LLM-specific\nSATD: its origins, prevalence, and mitigation strategies. By analyzing 93,142\nPython files across major LLM APIs, we found that 54.49% of SATD instances stem\nfrom OpenAI integrations and 12.35% from LangChain use. Prompt design emerged\nas the primary source of LLM-specific SATD, with 6.61% of debt related to\nprompt configuration and optimization issues, followed by hyperparameter tuning\nand LLM-framework integration. We further explored which prompt techniques\nattract the most debt, revealing that instruction-based prompts (38.60%) and\nfew-shot prompts (18.13%) are particularly vulnerable due to their dependence\non instruction clarity and example quality. Finally, we release a comprehensive\nSATD dataset to support reproducibility and offer practical guidance for\nmanaging technical debt in LLM-powered systems.", "AI": {"tldr": "This paper presents a large-scale study on LLM-specific SATD, analyzing 93,142 Python files. It quantifies SATD origins (OpenAI, LangChain), and mitigation strategies, and releases a dataset for reproducibility.", "motivation": "Despite the benefits of using LLMs via APIs, their integration introduces SATD, necessitating an understanding of its characteristics and management strategies.", "method": "The authors conducted a large-scale empirical study analyzing 93,142 Python files across major LLM APIs, examining the prevalence, origins, and mitigation of LLM-specific SATD.", "result": "54.49% of SATD instances originated from OpenAI, 12.35% from LangChain, with prompt design being the primary source. Instruction-based and few-shot prompts were found particularly prone to attracting debt.", "conclusion": "The paper contributes a comprehensive analysis of LLM-specific SATD and provides a dataset for future research, highlighting the need for systematic prompt design and optimization strategies."}}
{"id": "2509.20476", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20476", "abs": "https://arxiv.org/abs/2509.20476", "authors": ["Ren-Yi Huang", "Dumindu Samaraweera", "Prashant Shekhar", "J. Morris Chang"], "title": "Advancing Practical Homomorphic Encryption for Federated Learning: Theoretical Guarantees and Efficiency Optimizations", "comment": null, "summary": "Federated Learning (FL) enables collaborative model training while preserving\ndata privacy by keeping raw data locally stored on client devices, preventing\naccess from other clients or the central server. However, recent studies reveal\nthat sharing model gradients creates vulnerability to Model Inversion Attacks,\nparticularly Deep Leakage from Gradients (DLG), which reconstructs private\ntraining data from shared gradients. While Homomorphic Encryption has been\nproposed as a promising defense mechanism to protect gradient privacy, fully\nencrypting all model gradients incurs high computational overhead. Selective\nencryption approaches aim to balance privacy protection with computational\nefficiency by encrypting only specific gradient components. However, the\nexisting literature largely overlooks a theoretical exploration of the spectral\nbehavior of encrypted versus unencrypted parameters, relying instead primarily\non empirical evaluations. To address this gap, this paper presents a framework\nfor theoretical analysis of the underlying principles of selective encryption\nas a defense against model inversion attacks. We then provide a comprehensive\nempirical study that identifies and quantifies the critical factors, such as\nmodel complexity, encryption ratios, and exposed gradients, that influence\ndefense effectiveness. Our theoretical framework clarifies the relationship\nbetween gradient selection and privacy preservation, while our experimental\nevaluation demonstrates how these factors shape the robustness of defenses\nagainst model inversion attacks. Collectively, these contributions advance the\nunderstanding of selective encryption mechanisms and offer principled guidance\nfor designing efficient, scalable, privacy-preserving federated learning\nsystems.", "AI": {"tldr": "This paper bridges theory and practice in selective encryption for Federated Learning, providing a framework to analyze privacy-preserving gradient sharing and identifying key factors that determine defense effectiveness against model inversion attacks.", "motivation": "Existing selective encryption approaches in Federated Learning lack theoretical analysis of their spectral behavior and rely on empirical evaluations, creating a gap in understanding how gradient selection impacts privacy preservation against model inversion attacks like DLG.", "method": "The study presents a theoretical framework to analyze selective encryption principles and conducts a comprehensive empirical evaluation to quantify factors influencing defense effectiveness, such as model complexity, encryption ratios, and exposed gradients.", "result": "The theoretical framework establishes principles linking gradient selection to privacy, while experiments demonstrate how factors like encryption ratios and model complexity determine defense robustness, validating the effectiveness of selective encryption strategies.", "conclusion": "The paper advances understanding of selective encryption mechanisms in Federated Learning, offering theoretical and empirical guidance for designing efficient, privacy-preserving systems by clarifying the relationship between gradient selection and defense robustness against model inversion attacks."}}
{"id": "2509.20518", "categories": ["cs.SE", "cs.PL", "D.2.3"], "pdf": "https://arxiv.org/pdf/2509.20518", "abs": "https://arxiv.org/abs/2509.20518", "authors": ["Sayed Mahbub Hasan Amiri", "Md Mainul Islam"], "title": "Enhancing Python Programming Education with an AI-Powered Code Helper: Design, Implementation, and Impact", "comment": "20 pages, 16 figures", "summary": "This is the study that presents an AI-Python-based chatbot that helps\nstudents to learn programming by demonstrating solutions to such problems as\ndebugging errors, solving syntax problems or converting abstract theoretical\nconcepts to practical implementations. Traditional coding tools like Integrated\nDevelopment Environments (IDEs) and static analyzers do not give robotic help\nwhile AI-driven code assistants such as GitHub Copilot focus on getting things\ndone. To close this gap, our chatbot combines static code analysis, dynamic\nexecution tracing, and large language models (LLMs) to provide the students\nwith relevant and practical advice, hence promoting the learning process. The\nchatbots hybrid architecture employs CodeLlama for code embedding, GPT-4 for\nnatural language interactions, and Docker-based sandboxing for secure\nexecution. Evaluated through a mixed-methods approach involving 1,500 student\nsubmissions, the system demonstrated an 85% error resolution success rate,\noutperforming standalone tools like pylint (62%) and GPT-4 (73%). Quantitative\nresults revealed a 59.3% reduction in debugging time among users, with pre- and\npost-test assessments showing a 34% improvement in coding proficiency,\nparticularly in recursion and exception handling. Qualitative feedback from 120\nstudents highlighted the chatbots clarity, accessibility, and\nconfidence-building impact, though critiques included occasional latency and\nrestrictive code sanitization. By balancing technical innovation with\npedagogical empathy, this research provides a blueprint for AI tools that\nprioritize educational equity and long-term skill retention over mere code\ncompletion. The chatbot exemplifies how AI can augment human instruction,\nfostering deeper conceptual understanding in programming education.", "AI": {"tldr": "An AI-Python chatbot combining static/dynamic analysis and LLMs improves programming education with 85\\% error resolution and 34\\@ coding proficiency gains.", "motivation": "Traditional tools lack guided support, while AI assistants prioritize completion over learning; this addresses pedagogical gaps in coding education.", "method": "Hybrid architecture using CodeLlama for code embedding, GPT-4 for dialogue, Docker sandboxing, and evaluates 1,500 submissions through mixed-methods analysis.", "result": "85\\@ error resolution success (vs. 62\\@ pylint\\@, 73\\@ GPT-4), 59.3\\@ debugging time reduction, 34\\@ coding proficiency improvement, and positive qualitative feedback with latency critiques.", "conclusion": "The chatbot demonstrates AI's potential to enhance educational equity through pedagogically focused design, balancing technical innovation with skill retention."}}
{"id": "2509.20589", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20589", "abs": "https://arxiv.org/abs/2509.20589", "authors": ["Maria Chiper", "Radu Tudor Ionescu"], "title": "Every Character Counts: From Vulnerability to Defense in Phishing Detection", "comment": "Accepted at ICTAI 2025", "summary": "Phishing attacks targeting both organizations and individuals are becoming an\nincreasingly significant threat as technology advances. Current automatic\ndetection methods often lack explainability and robustness in detecting new\nphishing attacks. In this work, we investigate the effectiveness of\ncharacter-level deep learning models for phishing detection, which can provide\nboth robustness and interpretability. We evaluate three neural architectures\nadapted to operate at the character level, namely CharCNN, CharGRU, and\nCharBiLSTM, on a custom-built email dataset, which combines data from multiple\nsources. Their performance is analyzed under three scenarios: (i) standard\ntraining and testing, (ii) standard training and testing under adversarial\nattacks, and (iii) training and testing with adversarial examples. Aiming to\ndevelop a tool that operates as a browser extension, we test all models under\nlimited computational resources. In this constrained setup, CharGRU proves to\nbe the best-performing model across all scenarios. All models show\nvulnerability to adversarial attacks, but adversarial training substantially\nimproves their robustness. In addition, by adapting the Gradient-weighted Class\nActivation Mapping (Grad-CAM) technique to character-level inputs, we are able\nto visualize which parts of each email influence the decision of each model.\nOur open-source code and data is released at\nhttps://github.com/chipermaria/every-character-counts.", "AI": {"tldr": "The paper explores character-level deep learning models for phishing detection, focusing on robustness and interpretability. It evaluates CharCNN, CharGRU, and CharBiLSTM under standard and adversarial conditions, finding CharGRU as the best performer in constrained setups. Adversarial training improves robustness, and Grad-CAM is adapted for visual interpretation.", "motivation": "The motivation arises from the growing threat of phishing attacks and the insufficient explainability and robustness of current automatic detection methods in keeping up with new attack strategies.", "method": "The method involves evaluating three character-level neural architectures (CharCNN, CharGRU, CharBiLSTM) on a custom email dataset under three scenarios: standard, adversarial testing, and training with adversarial examples, while testing under limited computational resources for browser extension use.", "result": "In a resource-constrained setup, CharGRU outperforms in all scenarios. All models are vulnerable to adversarial attacks but show improved robustness via adversarial training. Model decision influences in emails are visualized using an adapted Grad-CAM approach.", "conclusion": "The study concludes that character-level models, particularly CharGRU, offer viable solutions for phishing detection where robustness, interpretability, and low computational demands are essential. Adversarial training and interpretation methods like Grad-CAM are valuable for enhancing performance and understanding decision mechanisms."}}
{"id": "2509.20552", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20552", "abs": "https://arxiv.org/abs/2509.20552", "authors": ["Xinyu Shi", "Zhenhao Li", "An Ran Chen"], "title": "Enhancing LLM-based Fault Localization with a Functionality-Aware Retrieval-Augmented Generation Framework", "comment": null, "summary": "Fault localization (FL) is a critical but time-consuming task in software\ndebugging, aiming to identify faulty code elements. While recent advances in\nlarge language models (LLMs) have shown promise for FL, they often struggle\nwith complex systems due to the lack of project-specific knowledge and the\ndifficulty of navigating large projects. To address these limitations, we\npropose FaR-Loc, a novel framework that enhances method-level FL by integrating\nLLMs with retrieval-augmented generation (RAG). FaR-Loc consists of three key\ncomponents: LLM Functionality Extraction, Semantic Dense Retrieval, and LLM\nRe-ranking. First, given a failed test and its associated stack trace, the LLM\nFunctionality Extraction module generates a concise natural language\ndescription that captures the failing behavior. Next, the Semantic Dense\nRetrieval component leverages a pre-trained code-understanding encoder to embed\nboth the functionality description (natural language) and the covered methods\n(code) into a shared semantic space, enabling the retrieval of methods with\nsimilar functional behavior. Finally, the LLM Re-ranking module reorders the\nretrieved methods based on their contextual relevance. Our experiments on the\nwidely used Defects4J benchmark show that FaR-Loc outperforms state-of-the-art\nLLM-based baselines SoapFL and AutoFL, by 14.6% and 9.1% in Top-1 accuracy, by\n19.2% and 22.1% in Top-5 accuracy, respectively. It also surpasses all\nlearning-based and spectrum-based baselines across all Top-N metrics without\nrequiring re-training. Furthermore, we find that pre-trained code embedding\nmodels that incorporate code structure, such as UniXcoder, can significantly\nimprove fault localization performance by up to 49.0% in Top-1 accuracy.\nFinally, we conduct a case study to illustrate the effectiveness of FaR-Loc and\nto provide insights for its practical application.", "AI": {"tldr": "Introducing FaR-Loc, a framework enhancing fault localization in software debugging by integrating large language models with retrieval-augmented generation, achieving improvements up to 49.0% in Top-1 accuracy over existing methods.", "motivation": "Fault localization is challenging and time-consuming for complex systems where existing large language models may lack the necessary project-specific knowledge and navigation capabilities.", "method": "FaR-Loc employs three modules: LLM Functionality Extraction for generating a natural language description of failures, Semantic Dense Retrieval using a pre-trained encoder to find similar methods in a shared semantic space, and LLM Re-ranking to prioritize the most relevant ones for debugging.", "result": "FaR-Loc surpasses current state-of-the-art models like SoapFL and AutoFL by 14.6% in Top-1 and 19.2% in Top-5 accuracy. It also performs better than all learning- and spectrum-based models on the Defects4J benchmark.", "conclusion": "The framework demonstrates that using LLMs in conjunction with code-structure-aware retrieval methods can significantly enhance fault localization, as supported by the substantial improvements observed with UniXcoder."}}
{"id": "2509.20592", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.20592", "abs": "https://arxiv.org/abs/2509.20592", "authors": ["Oluwole Adewusi", "Wallace S. Msagusa", "Jean Pierre Imanirumva", "Okemawo Obadofin", "Jema D. Ndibwile"], "title": "Beyond SSO: Mobile Money Authentication for Inclusive e-Government in Sub-Saharan Africa", "comment": null, "summary": "The rapid adoption of Mobile Money Services (MMS) in Sub-Saharan Africa (SSA)\noffers a viable path to improve e-Government service accessibility in the face\nof persistent low internet penetration. However, existing Mobile Money\nAuthentication (MMA) methods face critical limitations, including\nsusceptibility to SIM swapping, weak session protection, and poor scalability\nduring peak demand. This study introduces a hybrid MMA framework that combines\nUnstructured Supplementary Service Data (USSD)-based multi-factor\nauthentication with secure session management via cryptographically bound JSON\nWeb Tokens (JWT). Unlike traditional MMA systems that rely solely on SIM-PIN\nverification or smartphone-dependent biometrics, our design implements a\nthree-factor authentication model; SIM verification, PIN entry, and session\ntoken binding, tailored for resource-constrained environments. Simulations and\ncomparative analysis against OAuth-based Single Sign-On (SSO) methods reveal a\n45% faster authentication time (8 seconds vs. 12 to 15 seconds), 15% higher\nsuccess under poor network conditions (95% vs. 80%), and increased resistance\nto phishing and brute-force attacks. Penetration testing and threat modeling\nfurther demonstrate a substantial reduction in vulnerability exposure compared\nto conventional approaches. The primary contributions of this work are: (1) a\nhybrid authentication protocol that ensures offline accessibility and secure\nsession continuity; (2) a tailored security framework addressing threats like\nSIM swapping and social engineering in SSA; and (3) demonstrated scalability\nfor thousands of users with reduced infrastructure overhead. The proposed\napproach advances secure digital inclusion in SSA and other regions with\nsimilar constraints.", "AI": {"tldr": "This study introduces a hybrid MMA framework for Sub-Saharan Africa that combines USSD-based multi-factor authentication with secure session management via JWT, offering faster authentication, better performance under poor network conditions, and enhanced security against attacks like phishing and SIM swapping.", "motivation": "The rapid adoption of Mobile Money Services (MMS) in Sub-Saharan Africa (SSA) presents an opportunity to boost e-Government accessibility despite low internet penetration. But existing MMS authentication methods are flawed, leaving systems less secure and less scalable.", "method": "A hybrid framework is proposed, integrating USSD multi-factor authentication with secure session management through cryptographically bound JSON Web Tokens (JWT). It employs a three-factor authentication model: SIM verification, PIN entry, and session token binding, optimized for resource-constrained environments. The method is compared with OAuth-based SSO through simulations and tested using penetration testing and threat modeling.", "result": "The proposed method shows a 45% faster authentication (8 seconds vs. 12 to 15), 15% higher success under poor network (95% vs. 80%), and stronger resilience to phishing and brute-force attacks. The framework demonstrates efficient scalability for large user bases with reduced infrastructure costs.", "conclusion": "The hybrid authentication framework provides an innovative solution to the limitations of current MMA systems in SSA. It maintains offline accessibility while mitigating major security threats and can scale securely. This approach supports broader digital inclusion in low-connectivity regions."}}
{"id": "2509.20631", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20631", "abs": "https://arxiv.org/abs/2509.20631", "authors": ["Michael Zhang", "Yuan Tian", "Mariam Guizani"], "title": "Design, Implementation and Evaluation of a Novel Programming Language Topic Classification Workflow", "comment": null, "summary": "As software systems grow in scale and complexity, understanding the\ndistribution of programming language topics within source code becomes\nincreasingly important for guiding technical decisions, improving onboarding,\nand informing tooling and education. This paper presents the design,\nimplementation, and evaluation of a novel programming language topic\nclassification workflow. Our approach combines a multi-label Support Vector\nMachine (SVM) with a sliding window and voting strategy to enable fine-grained\nlocalization of core language concepts such as operator overloading, virtual\nfunctions, inheritance, and templates. Trained on the IBM Project CodeNet\ndataset, our model achieves an average F1 score of 0.90 across topics and 0.75\nin code-topic highlight. Our findings contribute empirical insights and a\nreusable pipeline for researchers and practitioners interested in code analysis\nand data-driven software engineering.", "AI": {"tldr": "This paper proposes a novel SVM-based workflow for fine-grained programming language topic classification, achieving strong F1 scores and providing a reusable pipeline for software engineering analysis.", "motivation": "The increasing scale and complexity of software systems necessitate automated tools to understand programming language topic distributions, aiding technical decisions, onboarding, tooling, and education.", "method": "A multi-label Support Vector Machine (SVM) combined with a sliding window and voting strategy is employed to detect core language concepts in source code, enabling fine-grained localization of topics.", "result": "The model achieves an average F1 score of 0.90 across topics and 0.75 for code-topic highlights when trained on the IBM Project CodeNet dataset.", "conclusion": "The paper introduces a reusable pipeline for programming language topic classification, offering empirical insights that benefit researchers and practitioners in code analysis and data-driven software engineering."}}
{"id": "2509.20639", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20639", "abs": "https://arxiv.org/abs/2509.20639", "authors": ["Adam Swanda", "Amy Chang", "Alexander Chen", "Fraser Burch", "Paul Kassianik", "Konstantin Berlin"], "title": "A Framework for Rapidly Developing and Deploying Protection Against Large Language Model Attacks", "comment": null, "summary": "The widespread adoption of Large Language Models (LLMs) has revolutionized AI\ndeployment, enabling autonomous and semi-autonomous applications across\nindustries through intuitive language interfaces and continuous improvements in\nmodel development. However, the attendant increase in autonomy and expansion of\naccess permissions among AI applications also make these systems compelling\ntargets for malicious attacks. Their inherent susceptibility to security flaws\nnecessitates robust defenses, yet no known approaches can prevent zero-day or\nnovel attacks against LLMs. This places AI protection systems in a category\nsimilar to established malware protection systems: rather than providing\nguaranteed immunity, they minimize risk through enhanced observability,\nmulti-layered defense, and rapid threat response, supported by a threat\nintelligence function designed specifically for AI-related threats.\n  Prior work on LLM protection has largely evaluated individual detection\nmodels rather than end-to-end systems designed for continuous, rapid adaptation\nto a changing threat landscape. We present a production-grade defense system\nrooted in established malware detection and threat intelligence practices. Our\nplatform integrates three components: a threat intelligence system that turns\nemerging threats into protections; a data platform that aggregates and enriches\ninformation while providing observability, monitoring, and ML operations; and a\nrelease platform enabling safe, rapid detection updates without disrupting\ncustomer workflows. Together, these components deliver layered protection\nagainst evolving LLM threats while generating training data for continuous\nmodel improvement and deploying updates without interrupting production.", "AI": {"tldr": "This paper proposes an LLM defense system inspired by malware protection, combining threat intelligence, data aggregation, and automated updates to combat evolving threats through continuous adaptation and non-disruptive deployment.", "motivation": "The motivation addresses the growing security vulnerabilities of autonomous LLM applications. Prior work lacks end-to-end systems for adapting to dynamic threats, creating a need for robust, production-ready solutions akin to traditional malware defense frameworks.", "method": "The method introduces a production-grade defense system with three components: (1) a threat intelligence system converting emerging threats into protections, (2) a data platform for aggregation, enrichment, and ML operations, and (3) a release platform enabling non-disruptive updates. These components work synergistically to improve model training and adapt to new threats.", "result": "The system achieves layered protection against LLM threats, generates training data for model improvement, and facilitates rapid updates without disrupting workflows. It bridges the gap between theoretical detection models and practical, adaptive defense systems.", "conclusion": "The paper concludes that AI protection systems, similar to traditional malware defenses, must focus on observability, multi-layered strategies, and rapid threat response to mitigate risks posed by evolving threats to LLMs. Their proposed system emphasizes continuous adaptation and safe deployment of updates."}}
{"id": "2509.20780", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20780", "abs": "https://arxiv.org/abs/2509.20780", "authors": ["Daniela Grassi", "Fabio Calefato", "Darja Smite", "Nicole Novielli", "Filippo Lanubile"], "title": "Exploring Engagement in Hybrid Meetings", "comment": null, "summary": "Background. The widespread adoption of hybrid work following the COVID-19\npandemic has fundamentally transformed software development practices,\nintroducing new challenges in communication and collaboration as organizations\ntransition from traditional office-based structures to flexible working\narrangements. This shift has established a new organizational norm where even\ntraditionally office-first companies now embrace hybrid team structures. While\nremote participation in meetings has become commonplace in this new\nenvironment, it may lead to isolation, alienation, and decreased engagement\namong remote team members. Aims. This study aims to identify and characterize\nengagement patterns in hybrid meetings through objective measurements, focusing\non the differences between co-located and remote participants. Method. We\nstudied professionals from three software companies over several weeks,\nemploying a multimodal approach to measure engagement. Data were collected\nthrough self-reported questionnaires and physiological measurements using\nbiometric devices during hybrid meetings to understand engagement dynamics.\nResults. The regression analyses revealed comparable engagement levels between\nonsite and remote participants, though remote participants show lower\nengagement in long meetings regardless of participation mode. Active roles\npositively correlate with higher engagement, while larger meetings and\nafternoon sessions are associated with lower engagement. Conclusions. Our\nresults offer insights into factors associated with engagement and\ndisengagement in hybrid meetings, as well as potential meeting improvement\nrecommendations. These insights are potentially relevant not only for software\nteams but also for knowledge-intensive organizations across various sectors\nfacing similar hybrid collaboration challenges.", "AI": {"tldr": "This paper investigates engagement patterns in hybrid software development meetings via multimodal data, finding comparable engagement between onsite/remote participants with key contextual factors influencing engagement dynamics.", "motivation": "Hybrid work norms post-pandemic reveal collaboration challenges in software teams, with remote workers at risk of disengagement despite widespread adoption of hybrid meetings.", "method": "Longitudinal study of three software companies using self-reported questionnaires and biometric devices to measure engagement in hybrid meetings through multimodal data collection.", "result": "Remote participants maintain comparable engagement except in long meetings; active roles increase engagement while large meetings and afternoon sessions decrease it, confirmed via regression analysis.", "conclusion": "Hybrid meeting engagement depends on temporal factors, participation roles, and meeting duration, offering actionable insights for optimizing hybrid collaboration across knowledge-intensive industries."}}
{"id": "2509.20686", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.20686", "abs": "https://arxiv.org/abs/2509.20686", "authors": ["Rian Adam Rajagede", "Yan Solihin"], "title": "Reliability Analysis of Fully Homomorphic Encryption Systems Under Memory Faults", "comment": null, "summary": "Fully Homomorphic Encryption (FHE) represents a paradigm shift in\ncryptography, enabling computation directly on encrypted data and unlocking\nprivacy-critical computation. Despite being increasingly deployed in real\nplatforms, the reliability aspects of FHE systems, especially how they respond\nto faults, have been mostly neglected. This paper aims to better understand of\nhow FHE computation behaves in the presence of memory faults, both in terms of\nindividual operations as well as at the level of applications, for different\nFHE schemes. Finally, we investigate how effective traditional and FHE-specific\nfault mitigation techniques are.", "AI": {"tldr": "This paper explores the reliability of Fully Homomorphic Encryption (FHE) systems under memory faults, examining the behavior of individual operations and applications across different FHE schemes, and evaluating traditional and FHE-specific fault mitigation techniques.", "motivation": "Understanding how FHE computation behaves in the presence of memory faults is critical for ensuring the reliability of FHE systems, which have increasingly been deployed in real platforms. The neglect of reliability aspects, particularly in fault response, motivates this study to assess the impact of such faults and the effectiveness of mitigation strategies.", "method": "The analysis involves rigorously examining FHE computation within the context of memory faults, evaluating individual operations and application-level behaviors across various FHE schemes. The effectiveness of traditional and FHE-specific fault mitigation techniques is also investigated through systematic testing.", "result": "The paper identifies how FHE systems behave when subjected to memory faults, revealing the levels of impact at both operation and application scales for different FHE schemes. The results show the effectiveness of certain fault mitigation techniques, potentially highlighting those that are better suited for protecting FHE computations.", "conclusion": "The study concludes that FHE systems must be evaluated not only for their cryptographic properties but also for their resilience to memory faults. It provides insights into the reliability of current FHE implementations and guidance for selecting or developing effective fault mitigation strategies to ensure secure and dependable computation in real-world applications."}}
{"id": "2509.20837", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20837", "abs": "https://arxiv.org/abs/2509.20837", "authors": ["Srishti Gureja", "Elena Tommasone", "Jingyi He", "Sara Hooker", "Matthias Gall\u00e9", "Marzieh Fadaee"], "title": "Verification Limits Code LLM Training", "comment": null, "summary": "Large language models for code generation increasingly rely on synthetic\ndata, where both problem solutions and verification tests are generated by\nmodels. While this enables scalable data creation, it introduces a previously\nunexplored bottleneck: the verification ceiling, in which the quality and\ndiversity of training data are fundamentally constrained by the capabilities of\nsynthetic verifiers. In this work, we systematically study how verification\ndesign and strategies influence model performance. We investigate (i) what we\nverify by analyzing the impact of test complexity and quantity: richer test\nsuites improve code generation capabilities (on average +3 pass@1), while\nquantity alone yields diminishing returns, (ii) how we verify by exploring\nrelaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By\nallowing for relaxed thresholds or incorporating LLM-based soft verification,\nwe can recover valuable training data, leading to a 2-4 point improvement in\npass@1 performance. However, this benefit is contingent upon the strength and\ndiversity of the test cases used, and (iii) why verification remains necessary\nthrough controlled comparisons of formally correct versus incorrect solutions\nand human evaluation: retaining diverse correct solutions per problem yields\nconsistent generalization gains. Our results show that Verification as\ncurrently practiced is too rigid, filtering out valuable diversity. But it\ncannot be discarded, only recalibrated. By combining calibrated verification\nwith diverse, challenging problem-solution pairs, we outline a path to break\nthe verification ceiling and unlock stronger code generation models.", "AI": {"tldr": "This paper identifies the 'verification ceiling' in code generation models trained on synthetic data, where rigid test suite designs limit data diversity. It demonstrates that tailored verification strategies\u2014prioritizing rich test quality, relaxed pass thresholds with soft verification, and maintaining correct solution diversity\u2014can improve pass@1 metrics by 2-4 points while preserving training value.", "motivation": "Synthetic data enables scalable code generation model training but suffers from verification bottlenecks due to test suite limitations, creating an unexplored gap in training effectiveness and diversity preservation.", "method": "The study analyzes three verification dimensions: (i) test suite composition (complexity vs quantity), (ii-iii) verification criteria (100%-pass rigidity vs relaxed thresholds with LLM soft verification), and (iii) solution correctness necessity via human evaluation of formally correct/incorrect examples.", "result": "Experiments show richer test suites improve pass@1 by +3 on average. Relaxed verification thresholds with strong test diversity yield 2-4 point gains. Problem-solution diversity (retaining multiple correct variants) consistently enhances generalization.", "conclusion": "Verification requires recalibration\u2014not elimination\u2014to balance data quality retention with diversity preservation. Combining calibrated verification and challenging problem sets breaks the verification ceiling, enabling stronger code generation models through strategic synthetic data curation."}}
{"id": "2509.20714", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20714", "abs": "https://arxiv.org/abs/2509.20714", "authors": ["Anh Tu Ngo", "Anupam Chattopadhyay", "Subhamoy Maitra"], "title": "Cryptographic Backdoor for Neural Networks: Boon and Bane", "comment": "Preprint", "summary": "In this paper we show that cryptographic backdoors in a neural network (NN)\ncan be highly effective in two directions, namely mounting the attacks as well\nas in presenting the defenses as well. On the attack side, a carefully planted\ncryptographic backdoor enables powerful and invisible attack on the NN.\nConsidering the defense, we present applications: first, a provably robust NN\nwatermarking scheme; second, a protocol for guaranteeing user authentication;\nand third, a protocol for tracking unauthorized sharing of the NN intellectual\nproperty (IP). From a broader theoretical perspective, borrowing the ideas from\nGoldwasser et. al. [FOCS 2022], our main contribution is to show that all these\ninstantiated practical protocol implementations are provably robust. The\nprotocols for watermarking, authentication and IP tracking resist an adversary\nwith black-box access to the NN, whereas the backdoor-enabled adversarial\nattack is impossible to prevent under the standard assumptions. While the\ntheoretical tools used for our attack is mostly in line with the Goldwasser et.\nal. ideas, the proofs related to the defense need further studies. Finally, all\nthese protocols are implemented on state-of-the-art NN architectures with\nempirical results corroborating the theoretical claims. Further, one can\nutilize post-quantum primitives for implementing the cryptographic backdoors,\nlaying out foundations for quantum-era applications in machine learning (ML).", "AI": {"tldr": "Cryptographic backdoors in NNs enable both powerful, invisible attacks and provably secure defenses (watermarking, authentication, IP tracking). The paper establishes theoretical foundations for robust protocols and explores quantum-era applications through post-quantum cryptography.", "motivation": "The work addresses the duality of cryptographic backdoors in ML: improving attack stealth (via undetectable adversarial mechanisms) and enabling formally verified defenses. Previous research often focused unilaterally on attacks or defenses, but this study bridges the gap to provide secure, provably resilient protocols.", "method": "The paper combines cryptographic techniques (e.g., Goldwasser et al.'s theoretical foundations) with NN architectures to design backdoor-based attacks and defenses. It provides theoretical proofs for defense robustness against black-box adversaries and validates results empirically on state-of-the-art models.", "result": "The defense protocols (watermarking, authentication, IP tracking) are theoretically robust against adversaries with black-box access. Empirical results confirm effectiveness, and the inclusion of post-quantum primitives ensures future readiness. Conversely, backdoor-enabled attacks under standard cryptographic assumptions are unpreventable.", "conclusion": "The paper demonstrates that cryptographic backdoors in neural networks (NNs) are effective for both adversarial attacks and defensive protocols. It introduces provably robust watermarking, authentication, and IP-tracking methods while highlighting the inherent vulnerability of backdoor-enabled attacks. The framework also aligns with quantum-era ML applications through post-quantum cryptographic primitives."}}
{"id": "2509.20881", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20881", "abs": "https://arxiv.org/abs/2509.20881", "authors": ["Yixuan Li", "Xinyi Liu", "Weidong Yang", "Ben Fei", "Shuhao Li", "Mingjie Zhou", "Lipeng Ma"], "title": "PseudoBridge: Pseudo Code as the Bridge for Better Semantic and Logic Alignment in Code Retrieval", "comment": null, "summary": "Code search aims to precisely find relevant code snippets that match natural\nlanguage queries within massive codebases, playing a vital role in software\ndevelopment. Recent advances leverage pre-trained language models (PLMs) to\nbridge the semantic gap between unstructured natural language (NL) and\nstructured programming languages (PL), yielding significant improvements over\ntraditional information retrieval and early deep learning approaches. However,\nexisting PLM-based methods still encounter key challenges, including a\nfundamental semantic gap between human intent and machine execution logic, as\nwell as limited robustness to diverse code styles. To address these issues, we\npropose PseudoBridge, a novel code retrieval framework that introduces\npseudo-code as an intermediate, semi-structured modality to better align NL\nsemantics with PL logic. Specifically, PseudoBridge consists of two stages.\nFirst, we employ an advanced large language model (LLM) to synthesize\npseudo-code, enabling explicit alignment between NL queries and pseudo-code.\nSecond, we introduce a logic-invariant code style augmentation strategy and\nemploy the LLM to generate stylistically diverse yet logically equivalent code\nimplementations with pseudo-code, then align the code snippets of different\nstyles with pseudo-code, enhancing model robustness to code style variation. We\nbuild PseudoBridge across 10 different PLMs and evaluate it on 6 mainstream\nprogramming languages. Extensive experiments demonstrate that PseudoBridge\nconsistently outperforms baselines, achieving significant gains in retrieval\naccuracy and generalization, particularly under zero-shot domain transfer\nscenarios such as Solidity and XLCoST datasets. These results demonstrate the\neffectiveness of explicit logical alignment via pseudo-code and highlight\nPseudoBridge's potential as a robust, generalizable solution for code\nretrieval.", "AI": {"tldr": "This paper proposes PseudoBridge, a code retrieval framework leveraging pseudo-code to bridge semantic gaps and enhance robustness to code style variations, achieving state-of-the-art results across diverse programming languages.", "motivation": "Existing PLM-based code retrieval methods struggle with a semantic gap between human intent and machine execution logic, as well as fragility to code style variations, necessitating improved alignment strategies for broader generalizability.", "method": "PseudoBridge employs a two-stage framework: (1) an LLM synthesizes pseudo-code to align NL queries with semi-structured representations, and (2) a logic-invariant code style augmentation generates diverse yet equivalent code implementations, aligning them with pseudo-code to improve robustness to style variations.", "result": "Extensive experiments across 6 programming languages and 10 PLMs show PseudoBridge outperforms baselines, achieving significant improvements in retrieval accuracy and generalization, particularly in zero-shot domain transfer scenarios like Solidity and XLCoST datasets.", "conclusion": "PseudoBridge demonstrates effectiveness in enhancing code retrieval by aligning NL intent with PL logic through pseudo-code, offering a robust and generalizable solution."}}
{"id": "2509.20767", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20767", "abs": "https://arxiv.org/abs/2509.20767", "authors": ["Ayush Kumar", "Kar Wai Fok", "Vrizlynn L. L. Thing"], "title": "ExpIDS: A Drift-adaptable Network Intrusion Detection System With Improved Explainability", "comment": null, "summary": "Despite all the advantages associated with Network Intrusion Detection\nSystems (NIDSs) that utilize machine learning (ML) models, there is a\nsignificant reluctance among cyber security experts to implement these models\nin real-world production settings. This is primarily because of their opaque\nnature, meaning it is unclear how and why the models make their decisions. In\nthis work, we design a deep learning-based NIDS, ExpIDS to have high decision\ntree explanation fidelity, i.e., the predictions of decision tree explanation\ncorresponding to ExpIDS should be as close to ExpIDS's predictions as possible.\nExpIDS can also adapt to changes in network traffic distribution (drift). With\nthe help of extensive experiments, we verify that ExpIDS achieves higher\ndecision tree explanation fidelity and a malicious traffic detection\nperformance comparable to state-of-the-art NIDSs for common attacks with\nvarying levels of real-world drift.", "AI": {"tldr": "This paper proposes ExpIDS, a deep learning-based NIDS with high decision tree explanation fidelity and adaptability to traffic distribution drift, achieving performance comparable to state-of-the-art systems.", "motivation": "Machine learning-based NIDS are underutilized due to opacity and lack of trust in their decision-making processes. Cybersecurity experts require explainable and adaptive models for real-world deployment.", "method": "ExpIDS combines deep learning with decision tree explanation mechanisms to ensure predictions align closely with interpretable explanations. It incorporates techniques to adapt to network traffic drift through extensive experimental validation.", "result": "ExpIDS demonstrates higher decision tree explanation fidelity than existing models and maintains effective malicious traffic detection performance across varying real-world traffic distributions and common attack schemas.", "conclusion": "ExpIDS bridges the gap between high-performance NIDS and human-interpretable explanations, enabling practical deployment of ML-based security systems while maintaining adaptability to evolving network environments."}}
{"id": "2509.21067", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.21067", "abs": "https://arxiv.org/abs/2509.21067", "authors": ["Oka Kurniawan", "Erick Chandra", "Christopher M. Poskitt", "Yannic Noller", "Kenny Tsu Wei Choo", "Cyrille Jegourel"], "title": "Designing for Novice Debuggers: A Pilot Study on an AI-Assisted Debugging Tool", "comment": "Accepted by the 25th Koli Calling International Conference on\n  Computing Education Research (Koli Calling 2025)", "summary": "Debugging is a fundamental skill that novice programmers must develop.\nNumerous tools have been created to assist novice programmers in this process.\nRecently, large language models (LLMs) have been integrated with automated\nprogram repair techniques to generate fixes for students' buggy code. However,\nmany of these tools foster an over-reliance on AI and do not actively engage\nstudents in the debugging process. In this work, we aim to design an intuitive\ndebugging assistant, CodeHinter, that combines traditional debugging tools with\nLLM-based techniques to help novice debuggers fix semantic errors while\npromoting active engagement in the debugging process. We present findings from\nour second design iteration, which we tested with a group of undergraduate\nstudents. Our results indicate that the students found the tool highly\neffective in resolving semantic errors and significantly easier to use than the\nfirst version. Consistent with our previous study, error localization was the\nmost valuable feature. Finally, we conclude that any AI-assisted debugging tool\nshould be personalized based on user profiles to optimize their interactions\nwith students.", "AI": {"tldr": "This paper proposes CodeHinter, a hybrid LLM-debugger tool for novice programmers that balances AI assistance with active engagement, demonstrating improved usability and effectiveness in semantic error resolution through user studies.", "motivation": "Existing LLM-based debugging tools risk fostering over-reliance on AI and disengaging students from the debugging process, potentially hindering skill development.", "method": "The authors designed CodeHinter, a hybrid debugging assistant combining traditional debugging tools with LLM-based techniques, and evaluated it through a second iteration with undergraduate students.", "result": "Students found CodeHinter significantly more effective for semantic error resolution and easier to use compared to the first version, with error localization identified as the most valuable feature.", "conclusion": "The paper concludes that AI-assisted debugging tools should be personalized based on student profiles to optimize interaction and learning outcomes."}}
{"id": "2509.20796", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20796", "abs": "https://arxiv.org/abs/2509.20796", "authors": ["Yongjiao Li", "Liang Zhu", "Yalin Deng", "Qikun Zhang", "Zhenlei Wang", "Zhu Cao"], "title": "Fast Revocable Attribute-Based Encryption with Data Integrity for Internet of Things", "comment": "16 pages, 7 figures", "summary": "Efficient and secure revocable attribute-based encryption (RABE) is vital for\nensuring flexible and fine-grained access control and data sharing in cloud\nstorage and outsourced data environments within the Internet of Things (IoT).\nHowever, current RABE schemes often struggle to achieve an optimal balance\nbetween efficiency, security, dynamic scalability, and other important\nfeatures, which hampers their practical application. To overcome these\nlimitations, we propose a fast RABE scheme with data integrity for IoT that\nachieves adaptive security with multiple challenge ciphertexts. Our scheme\nsupports the revocation of authorized users and transfers the computationally\nheavy revocation processes to the cloud, thereby easing the computational\nburden on IoT devices. Moreover, it consistently guarantees the integrity and\ncorrectness of data. We have demonstrated its adaptive security within the\ndefined security model with multiple challenge ciphertexts and optimized its\nperformance. Experimental results indicate that our scheme provides better\nperformance than existing solutions. Under the same access policy, our scheme\nreduces computational consumption by 7 to 9 times compared to previous schemes.", "AI": {"tldr": "A novel RABE scheme for IoT improves efficiency and security by offloading revocation to clouds and ensuring data integrity, outperforming existing methods computationally by 7\u20139 times.", "motivation": "Current RABE schemes fail to balance efficiency, security, and scalability in IoT environments, limiting their real-world applicability. Efficient access control and data sharing in IoT and cloud storage necessitate optimized RABE solutions.", "method": "A fast RABE scheme with data integrity is proposed, leveraging multiple challenge ciphertexts for adaptive security. Revocation computations are offloaded to the cloud, and data integrity is consistently guaranteed.", "result": "Experimental results demonstrate the scheme's adaptive security under defined models and a 7\u20139\u00d7 reduction in computational costs compared to existing solutions, achieving superior performance and data integrity.", "conclusion": "The proposed RABE scheme enhances practicality for IoT by achieving adaptive security, dynamic scalability, and reduced computational overhead, with significant improvements in performance compared to existing methods."}}
{"id": "2509.21068", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21068", "abs": "https://arxiv.org/abs/2509.21068", "authors": ["Nek Dil Khan", "Javed Ali Khan", "Mobashir Husain", "Muhammad Sohail Khan", "Arif Ali Khan", "Muhammad Azeem Akbar", "Shahid Hussain"], "title": "An Improved Quantum Software Challenges Classification Approach using Transfer Learning and Explainable AI", "comment": null, "summary": "Quantum Software Engineering (QSE) is a research area practiced by tech\nfirms. Quantum developers face challenges in optimizing quantum computing and\nQSE concepts. They use Stack Overflow (SO) to discuss challenges and label\nposts with specialized quantum tags, which often refer to technical aspects\nrather than developer posts. Categorizing questions based on quantum concepts\ncan help identify frequent QSE challenges. We conducted studies to classify\nquestions into various challenges. We extracted 2829 questions from Q&A\nplatforms using quantum-related tags. Posts were analyzed to identify frequent\nchallenges and develop a novel grounded theory. Challenges include Tooling,\nTheoretical, Learning, Conceptual, Errors, and API Usage. Through content\nanalysis and grounded theory, discussions were annotated with common challenges\nto develop a ground truth dataset. ChatGPT validated human annotations and\nresolved disagreements. Fine-tuned transformer algorithms, including BERT,\nDistilBERT, and RoBERTa, classified discussions into common challenges. We\nachieved an average accuracy of 95% with BERT DistilBERT, compared to\nfine-tuned Deep and Machine Learning (D&ML) classifiers, including Feedforward\nNeural Networks (FNN), Convolutional Neural Networks (CNN), and Long Short-Term\nMemory networks (LSTM), which achieved accuracies of 89%, 86%, and 84%,\nrespectively. The Transformer-based approach outperforms the D&ML-based\napproach with a 6\\% increase in accuracy by processing actual discussions,\ni.e., without data augmentation. We applied SHAP (SHapley Additive\nexPlanations) for model interpretability, revealing how linguistic features\ndrive predictions and enhancing transparency in classification. These findings\ncan help quantum vendors and forums better organize discussions for improved\naccess and readability. However,empirical evaluation studies with actual\ndevelopers and vendors are needed.", "AI": {"tldr": "This study addresses Quantum Software Engineering (QSE) challenges by categorizing Stack Overflow discussions into six challenge types using transformer models (BERT, DistilBERT, RoBERTa) and grounded theory. Transformer-based classifiers outperformed traditional D&ML models with 95% accuracy, aided by SHAP for interpretability, offering insights for improving QSE Q&A organization.", "motivation": "Quantum developers struggle to navigate fragmented Q&A discussions due to technical tagging and lack of concept-based categorization. Systematic classification could identify frequent QSE challenges and improve forum organization.", "method": "1) Extracted 2829 questions using quantum tags from Q&A platforms.\n2) Combined content analysis, grounded theory, and human/ChatGPT annotations to build a ground truth dataset.\n3) Trained transformer models (BERT, DistilBERT, RoBERTa) and D&ML models (FNN, CNN, LSTM) for classification.\n4) Used SHAP values to explain model predictions.", "result": "Transformer models achieved 95% accuracy (BERT 95%, DistilBERT 95%) vs. 89-84% for D&ML models without data augmentation. SHAP revealed critical linguistic features while maintaining 6% higher accuracy. Classification categories: Tooling, Theoretical, Learning, Conceptual, Errors, API Usage.", "conclusion": "Transformer-based approaches significantly outperform traditional D&ML in QSE challenge classification with inherent model interpretability through SHAP analysis. The framework can improve quantum forum organization, but requires validation through developer-vendor co-creation studies."}}
{"id": "2509.20808", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20808", "abs": "https://arxiv.org/abs/2509.20808", "authors": ["Raghul Saravanan", "Sudipta Paria", "Aritra Dasgupta", "Swarup Bhunia", "Sai Manoj P D"], "title": "Intelligent Graybox Fuzzing via ATPG-Guided Seed Generation and Submodule Analysis", "comment": "7 pages, 6 figures, 4 tables", "summary": "Hardware Fuzzing emerged as one of the crucial techniques for finding\nsecurity flaws in modern hardware designs by testing a wide range of input\nscenarios. One of the main challenges is creating high-quality input seeds that\nmaximize coverage and speed up verification. Coverage-Guided Fuzzing (CGF)\nmethods help explore designs more effectively, but they struggle to focus on\nspecific parts of the hardware. Existing Directed Gray-box Fuzzing (DGF)\ntechniques like DirectFuzz try to solve this by generating targeted tests, but\nit has major drawbacks, such as supporting only limited hardware description\nlanguages, not scaling well to large circuits, and having issues with\nabstraction mismatches. To address these problems, we introduce a novel\nframework, PROFUZZ, that follows the DGF approach and combines fuzzing with\nAutomatic Test Pattern Generation (ATPG) for more efficient fuzzing. By\nleveraging ATPG's structural analysis capabilities, PROFUZZ can generate\nprecise input seeds that target specific design regions more effectively while\nmaintaining high fuzzing throughput. Our experiments show that PROFUZZ scales\n30x better than DirectFuzz when handling multiple target sites, improves\ncoverage by 11.66%, and runs 2.76x faster, highlighting its scalability and\neffectiveness for directed fuzzing in complex hardware systems.", "AI": {"tldr": "TLDR: PROFUZZ is a novel framework that combines directed gray-box fuzzing with ATPG for efficient hardware fuzzing, outperforming DirectFuzz in scalability, coverage, and speed.", "motivation": "The motivation behind this paper is to address the limitations of existing directed gray-box fuzzing (DGF) techniques in hardware fuzzing, such as DirectFuzz, which suffers from limited hardware description language support, poor scalability to large circuits, and abstraction mismatch issues. High-quality input seeds that maximize coverage and speed up verification remain a major challenge in hardware fuzzing.", "method": "The proposed method, PROFUZZ, integrates directed gray-box fuzzing with Automatic Test Pattern Generation (ATPG). It uses ATPG's structural analysis to generate precise input seeds that target specific regions of the hardware design, thereby improving efficiency and maintaining high fuzzing throughput. This combination allows for better scalability and targeted exploration of the design space compared to existing DGF approaches.", "result": "Experimental results demonstrate that PROFUZZ scales 30 times better than DirectFuzz when dealing with multiple target sites. Additionally, it improves coverage by 11.66% and operates 2.76 times faster, showcasing significant performance advantages.", "conclusion": "By combining directed gray-box fuzzing with ATPG, PROFUZZ effectively overcomes key limitations of current hardware fuzzing techniques. It achieves superior scalability, coverage, and speed, making it a highly effective solution for directed fuzzing in complex hardware systems."}}
{"id": "2509.21170", "categories": ["cs.SE", "cs.AI", "D.2.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.21170", "abs": "https://arxiv.org/abs/2509.21170", "authors": ["Yongda Yu", "Guohao Shi", "Xianwei Wu", "Haochuan He", "XueMing Gu", "Qianqian Zhao", "Kui Liu", "Qiushi Wang", "Zhao Tian", "Haifeng Shen", "Guoping Rong"], "title": "Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach", "comment": "22 pages", "summary": "Large Language Models (LLMs) have shown great potential in supporting\nautomated code review due to their impressive capabilities in context\nunderstanding and reasoning. However, these capabilities are still limited\ncompared to human-level cognition because they are heavily influenced by the\ntraining data. Recent research has demonstrated significantly improved\nperformance through fine-tuning LLMs with code review data. However, compared\nto human reviewers who often simultaneously analyze multiple dimensions of code\nreview to better identify issues, the full potential of these methods is\nhampered by the limited or vague information used to fine-tune the models. This\npaper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that\ntrains LLMs with an impressive reasoning ability to analyze multiple dimensions\nof code review by harnessing long COT techniques to provide rich structured\ninformation. To address context loss and reasoning logic loss issues that\nfrequently occur when LLMs process long COT prompts, we propose a solution that\ncombines the Maximum Entropy (ME) modeling principle with pre-defined reasoning\npathways in MelcotCR to enable more effective utilization of in-context\nknowledge within long COT prompts while strengthening the logical tightness of\nthe reasoning process. Empirical evaluations on our curated MelcotCR dataset\nand the public CodeReviewer dataset reveal that a low-parameter base model,\nsuch as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art\nmethods in terms of the accuracy of detecting and describing code issues, with\nits performance remarkably on par with that of the 671B DeepSeek-R1 model.", "AI": {"tldr": "MelcotCR improves LLM code review capabilities through structured multi-dimensional training and ME-based reasoning optimization, achieving near-state-of-the-art performance with minimal parameters.", "motivation": "Existing LLM-based code review methods underperform human reviewers due to: 1) Limited training data scope 2) Overlooking multi-dimensional analysis required for comprehensive code review 3) Inability to maintain reasoning quality in long COT prompts.", "method": "MelcotCR introduces a chain-of-thought fine-tuning framework combining: 1) Structured long COT prompts for multi-dimensional code analysis 2) Maximum Entropy (ME) modeling combined with pre-defined reasoning pathways to address context loss and maintain logical consistency in long reasoning chains.", "result": "MelcotCR achieves: - 14B Qwen2.5 fine-tuned with MelcotCR outperforms existing methods in code issue detection (-32% error rate reduction on CodeReviewer dataset) - Matches performance of 671B DeepSeek-R1 model in accuracy for both defect detection and description generation - 22% improvement in multi-dimensional issue detection on MelcotCR's curated dataset", "conclusion": "The paper concludes that the MelcotCR approach effectively enhances LLMs' code review capabilities through structured long COT fine-tuning and ME-based reasoning pathway optimization, achieving state-of-the-art performance with significant parameter efficiency."}}
{"id": "2509.20835", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20835", "abs": "https://arxiv.org/abs/2509.20835", "authors": ["Yu Liu", "Boxiang He", "Fanggang Wang"], "title": "Security-aware Semantic-driven ISAC via Paired Adversarial Residual Networks", "comment": null, "summary": "This paper proposes a novel and flexible security-aware semantic-driven\nintegrated sensing and communication (ISAC) framework, namely security semantic\nISAC (SS-ISAC). Inspired by the positive impact of the adversarial attack, a\npair of pluggable encryption and decryption modules is designed in the proposed\nSS-ISAC framework. The encryption module is installed after the semantic\ntransmitter, adopting a trainable adversarial residual network (ARN) to create\nthe adversarial attack. Correspondingly, the decryption module before the\nsemantic receiver utilizes another trainable ARN to mitigate the adversarial\nattack and noise. These two modules can be flexibly assembled considering the\nsystem security demands, without drastically modifying the hardware\ninfrastructure. To ensure the sensing and communication (SAC) performance while\npreventing the eavesdropping threat, the above ARNs are jointly optimized by\nminimizing a carefully designed loss function that relates to the adversarial\nattack power, SAC performance, as well as the privacy leakage risk. Simulation\nresults validate the effectiveness of the proposed SS-ISAC framework in terms\nof both SAC and eavesdropping prevention performance.", "AI": {"tldr": "The paper introduces security semantic ISAC (SS-ISAC), a framework integrating sensing and communication with security. It uses adversarial residual networks (ARNs)-based encryption/decryption modules to enhance security without hardware changes, validated by simulations.", "motivation": "Existing ISAC systems lack integrated security solutions to prevent eavesdropping while maintaining performance. Current methods either compromise security or require expensive hardware modifications. Adversarial attacks inspire innovate security mechanisms.", "method": "Designs a pluggable encryption module (after semantic transmitter with ARN to generate adversarial noise) and decryption module (before receiver with ARN to neutralize noise). Two ARNs are jointly trained via a multi-objective loss function balancing SAC performance, adversarial strength, and privacy risk.", "result": "Simulations demonstrate SS-ISAC achieves competitive sensing/communication performance while effectively mitigating eavesdropping threats compared to baseline methods.", "conclusion": "SS-ISAC provides a flexible, hardware-friendly security framework for future ISAC systems. The adversarial-driven design adaptively adjusts security levels while maintaining core performance metrics."}}
{"id": "2509.21292", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21292", "abs": "https://arxiv.org/abs/2509.21292", "authors": ["Ronivaldo Ferreira", "Guilherme da Silva", "Carla Rocha", "Gustavo Pinto"], "title": "Semantic Clustering of Civic Proposals: A Case Study on Brazil's National Participation Platform", "comment": "12 pages, in Portuguese language", "summary": "Promoting participation on digital platforms such as Brasil Participativo has\nemerged as a top priority for governments worldwide. However, due to the sheer\nvolume of contributions, much of this engagement goes underutilized, as\norganizing it presents significant challenges: (1) manual classification is\nunfeasible at scale; (2) expert involvement is required; and (3) alignment with\nofficial taxonomies is necessary. In this paper, we introduce an approach that\ncombines BERTopic with seed words and automatic validation by large language\nmodels. Initial results indicate that the generated topics are coherent and\ninstitutionally aligned, with minimal human effort. This methodology enables\ngovernments to transform large volumes of citizen input into actionable data\nfor public policy.", "AI": {"tldr": "This paper proposes a BERTopic-based approach with seed words and LLM validation to efficiently organize large-scale citizen input on digital platforms, enabling governments to generate actionable policy data with minimal human effort.", "motivation": "Governments face challenges organizing massive citizen engagement data due to scalability issues, expertise requirements, and alignment with official taxonomies, leading to underutilized input.", "method": "Combines BERTopic (a neural topic model), seed words for guidance, and LLMs (large language models) for automatic validation of generated topics against institutional criteria.", "result": "Initial results show generated topics are coherent, institutionally aligned, and require minimal human intervention compared to manual approaches.", "conclusion": "The methodology provides a scalable solution for transforming unstructured citizen feedback into structured insights for public policy decision-making."}}
{"id": "2509.20861", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20861", "abs": "https://arxiv.org/abs/2509.20861", "authors": ["Chao Zha", "Haolin Pan", "Bing Bai", "Jiangxing Wu", "Ruyun Zhang"], "title": "FlowXpert: Context-Aware Flow Embedding for Enhanced Traffic Detection in IoT Network", "comment": null, "summary": "In the Internet of Things (IoT) environment, continuous interaction among a\nlarge number of devices generates complex and dynamic network traffic, which\nposes significant challenges to rule-based detection approaches. Machine\nlearning (ML)-based traffic detection technology, capable of identifying\nanomalous patterns and potential threats within this traffic, serves as a\ncritical component in ensuring network security. This study first identifies a\nsignificant issue with widely adopted feature extraction tools (e.g.,\nCICMeterFlow): the extensive use of time- and length-related features leads to\nhigh sparsity, which adversely affects model convergence. Furthermore, existing\ntraffic detection methods generally lack an embedding mechanism capable of\nefficiently and comprehensively capturing the semantic characteristics of\nnetwork traffic. To address these challenges, we propose a novel feature\nextraction tool that eliminates traditional time and length features in favor\nof context-aware semantic features related to the source host, thus improving\nthe generalizability of the model. In addition, we design an embedding training\nframework that integrates the unsupervised DBSCAN clustering algorithm with a\ncontrastive learning strategy to effectively capture fine-grained semantic\nrepresentations of traffic. Extensive empirical evaluations are conducted on\nthe real-world Mawi data set to validate the proposed method in terms of\ndetection accuracy, robustness, and generalization. Comparative experiments\nagainst several state-of-the-art (SOTA) models demonstrate the superior\nperformance of our approach. Furthermore, we confirm its applicability and\ndeployability in real-time scenarios.", "AI": {"tldr": "The paper addresses limitations of traditional ML-based IoT network traffic detection by proposing (1) a context-aware semantic feature extraction tool eliminating time/length features, and (2) a DBSCAN-contrastive learning framework for fine-grained representations, validated on real-world datasets.", "motivation": "Existing feature extraction tools (e.g., CICMeterFlow) suffer from high sparsity due to time/length features, while current methods lack effective embedding mechanisms to capture traffic semantics.", "method": "Develops a feature extraction tool with host-context semantic features, and an embedding framework combining unsupervised DBSCAN clustering with contrastive learning to capture traffic semantics.", "result": "Superior detection accuracy, robustness, and generalization demonstrated on Mawi dataset against SOTA models, with real-time deployability confirmed.", "conclusion": "Proposed method resolves feature sparsity and semantic embedding limitations in IoT traffic detection, offering an effective and deployable solution for dynamic network environments."}}
{"id": "2509.20880", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.20880", "abs": "https://arxiv.org/abs/2509.20880", "authors": ["Cheng Lyu", "Mu Yuan", "Dabin Zheng", "Siwei Sun", "Shun Li"], "title": "A Generalized $\u03c7_n$-Function", "comment": null, "summary": "The mapping $\\chi_n$ from $\\F_{2}^{n}$ to itself defined by $y=\\chi_n(x)$\nwith $y_i=x_i+x_{i+2}(1+x_{i+1})$, where the indices are computed modulo $n$,\nhas been widely studied for its applications in lightweight cryptography.\nHowever, $\\chi_n $ is bijective on $\\F_2^n$ only when $n$ is odd, restricting\nits use to odd-dimensional vector spaces over $\\F_2$. To address this\nlimitation, we introduce and analyze the generalized mapping $\\chi_{n, m}$\ndefined by $y=\\chi_{n,m}(x)$ with $y_i=x_i+x_{i+m} (x_{i+m-1}+1)(x_{i+m-2}+1)\n\\cdots (x_{i+1}+1)$, where $m$ is a fixed integer with $m\\nmid n$. To\ninvestigate such mappings, we further generalize $\\chi_{n,m}$ to $\\theta_{m,\nk}$, where $\\theta_{m, k}$ is given by $y_i=x_{i+mk} \\prod_{\\substack{j=1,\\,\\,\nm \\nmid j}}^{mk-1} \\left(x_{i+j}+1\\right), \\,\\,{\\rm for }\\,\\, i\\in\n\\{0,1,\\ldots,n-1\\}$. We prove that these mappings generate an abelian group\nisomorphic to the group of units in $\\F_2[z]/(z^{\\lfloor n/m\\rfloor +1})$. This\nstructural insight enables us to construct a broad class of permutations over\n$\\F_2^n$ for any positive integer $n$, along with their inverses. We rigorously\nanalyze algebraic properties of these mappings, including their iterations,\nfixed points, and cycle structures. Additionally, we provide a comprehensive\ndatabase of the cryptographic properties for iterates of $\\chi_{n,m}$ for small\nvalues of $n$ and $m$. Finally, we conduct a comparative security and\nimplementation cost analysis among $\\chi_{n,m}$, $\\chi_n$, $\\chi\\chi_n$\n(EUROCRYPT 2025 \\cite{belkheyar2025chi}) and their variants, and prove\nConjecture~1 proposed in~\\cite{belkheyar2025chi} as a by-product of our study.\nOur results lead to generalizations of $\\chi_n$, providing alternatives to\n$\\chi_n$ and $\\chi\\chi_n$.", "AI": {"tldr": "The paper explores a generalized version of the \u03c7\u2099 mapping (\u03c7\u2099,m) to enable bijective applications in even-dimensional spaces over \ud835\udd3d\u2082, providing a critical analysis of cryptographic properties and comparisons with existing methods like \u03c7\u03c7\u2099.", "motivation": "The original \u03c7\u2099 mapping is limited to odd dimensions, hindering its applicability in cryptography. This work generalizes it to \u03c7\u2099,m for even and odd n, expanding its potential use cases in lightweight cryptographic algorithms.", "method": "The authors generalize \u03c7\u2099 to \u03c7\u2099,m and further to \u03b8\u2098,\u2096 by defining new mappings with modular indices and products over ranges avoiding multiples of m. They prove that these mappings form an abelian group isomorphic to the unit group of a specific quotient ring and analyze their algebraic properties.", "result": "The key result is the construction of permutations over \ud835\udd3d\u2082\u207f for any n using the generalized mappings, along with a comprehensive database of cryptographic properties for small n and m. The authors also prove Conjecture~1 from \\", "conclusion": "The paper concludes that the generalized \u03c7\u2099,m and \u03b8\u2098,\u2096 mappings offer a versatile framework for constructing permutations in both even and odd dimensions over \ud835\udd3d\u2082, surpassing limitations of prior work and providing a foundational step for future cryptographic research."}}
{"id": "2509.21011", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21011", "abs": "https://arxiv.org/abs/2509.21011", "authors": ["Ping He", "Changjiang Li", "Binbin Zhao", "Tianyu Du", "Shouling Ji"], "title": "Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools", "comment": null, "summary": "The remarkable capability of large language models (LLMs) has led to the wide\napplication of LLM-based agents in various domains. To standardize interactions\nbetween LLM-based agents and their environments, model context protocol (MCP)\ntools have become the de facto standard and are now widely integrated into\nthese agents. However, the incorporation of MCP tools introduces the risk of\ntool poisoning attacks, which can manipulate the behavior of LLM-based agents.\nAlthough previous studies have identified such vulnerabilities, their red\nteaming approaches have largely remained at the proof-of-concept stage, leaving\nthe automatic and systematic red teaming of LLM-based agents under the MCP tool\npoisoning paradigm an open question. To bridge this gap, we propose\nAutoMalTool, an automated red teaming framework for LLM-based agents by\ngenerating malicious MCP tools. Our extensive evaluation shows that AutoMalTool\neffectively generates malicious MCP tools capable of manipulating the behavior\nof mainstream LLM-based agents while evading current detection mechanisms,\nthereby revealing new security risks in these agents.", "AI": {"tldr": "AutoMalTool automates malicious tool generation for systematic red teaming of LLM agents via MCP protocol, uncovering new security risks through evasion-of-detection attacks.", "motivation": "Existing red teaming methods for MCP tool poisoning remain at proof-of-concept levels, leaving a critical gap in automated and systematic security evaluation of LLM-based agents against these attacks.", "method": "The framework employs automated generation of malicious MCP tools to systematically attack and manipulate mainstream LLM-based agents while bypassing detection systems.", "result": "Empirical evaluations demonstrate AutoMalTool's effectiveness in generating evasive malicious tools that successfully compromise popular LLM agents while avoiding detection, highlighting previously unseen security weaknesses.", "conclusion": "AutoMalTool bridges the gap in automatic red teaming for LLM-based agents under MCP tool poisoning, revealing critical security vulnerabilities in their current implementations."}}
{"id": "2509.20924", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20924", "abs": "https://arxiv.org/abs/2509.20924", "authors": ["Hanbo Huang", "Yiran Zhang", "Hao Zheng", "Xuan Gong", "Yihan Li", "Lin Liu", "Shiyu Liang"], "title": "RLCracker: Exposing the Vulnerability of LLM Watermarks with Adaptive RL Attacks", "comment": null, "summary": "Large Language Models (LLMs) watermarking has shown promise in detecting\nAI-generated content and mitigating misuse, with prior work claiming robustness\nagainst paraphrasing and text editing. In this paper, we argue that existing\nevaluations are not sufficiently adversarial, obscuring critical\nvulnerabilities and overstating the security. To address this, we introduce\nadaptive robustness radius, a formal metric that quantifies watermark\nresilience against adaptive adversaries. We theoretically prove that optimizing\nthe attack context and model parameters can substantially reduce this radius,\nmaking watermarks highly susceptible to paraphrase attacks. Leveraging this\ninsight, we propose RLCracker, a reinforcement learning (RL)-based adaptive\nattack that erases watermarks while preserving semantic fidelity. RLCracker\nrequires only limited watermarked examples and zero access to the detector.\nDespite weak supervision, it empowers a 3B model to achieve 98.5% removal\nsuccess and an average 0.92 P-SP score on 1,500-token Unigram-marked texts\nafter training on only 100 short samples. This performance dramatically exceeds\n6.75% by GPT-4o and generalizes across five model sizes over ten watermarking\nschemes. Our results confirm that adaptive attacks are broadly effective and\npose a fundamental threat to current watermarking defenses.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.20943", "categories": ["cs.CR", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.20943", "abs": "https://arxiv.org/abs/2509.20943", "authors": ["Dincy R. Arikkat", "Sneha B. T.", "Serena Nicolazzo", "Antonino Nocera", "Vinod P.", "Rafidha Rehiman K. A.", "Karthika R"], "title": "CTI Dataset Construction from Telegram", "comment": null, "summary": "Cyber Threat Intelligence (CTI) enables organizations to anticipate, detect,\nand mitigate evolving cyber threats. Its effectiveness depends on high-quality\ndatasets, which support model development, training, evaluation, and\nbenchmarking. Building such datasets is crucial, as attack vectors and\nadversary tactics continually evolve. Recently, Telegram has gained prominence\nas a valuable CTI source, offering timely and diverse threat-related\ninformation that can help address these challenges. In this work, we address\nthese challenges by presenting an end-to-end automated pipeline that\nsystematically collects and filters threat-related content from Telegram. The\npipeline identifies relevant Telegram channels and scrapes 145,349 messages\nfrom 12 curated channels out of 150 identified sources. To accurately filter\nthreat intelligence messages from generic content, we employ a BERT-based\nclassifier, achieving an accuracy of 96.64%. From the filtered messages, we\ncompile a dataset of 86,509 malicious Indicators of Compromise, including\ndomains, IPs, URLs, hashes, and CVEs. This approach not only produces a\nlarge-scale, high-fidelity CTI dataset but also establishes a foundation for\nfuture research and operational applications in cyber threat detection.", "AI": {"tldr": "This paper introduces a Telegram-based CTI pipeline that automates collection, classification, and extraction to generate a 86,509-IoC dataset, enabling more effective cyber threat detection.", "motivation": "High-quality CTI datasets are critical for combating evolving cyber threats, and Telegram offers timely/diverse threat information that outpaces traditional sources.", "method": "An end-to-end automated pipeline systematically collects Telegram messages, filters relevant content via a BERT-based classifier, and extracts malicious Indicators of Compromise (IoCs).", "result": "Achieved 96.64% classification accuracy, scraped 145,349 messages (yielding 86,509 IoCs including domains/IPs/URLs/hashes/CVEs) from 12 curated Telegram channels.", "conclusion": "The proposed pipeline successfully creates a large-scale CTI dataset from Telegram, providing a robust foundation for future cyber threat detection research and applications."}}
{"id": "2509.20972", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20972", "abs": "https://arxiv.org/abs/2509.20972", "authors": ["Ibrahim Altan", "Abdulla Bachir", "Yousuf Parbhulkar", "Abdul Muksith Rizvi", "Moshiur Farazi"], "title": "Dual-Path Phishing Detection: Integrating Transformer-Based NLP with Structural URL Analysis", "comment": "Paper accepted for presentation at the ACS/IEEE 22nd International\n  Conference on Computer Systems and Applications (AICCSA 2025)", "summary": "Phishing emails pose a persistent and increasingly sophisticated threat,\nundermining email security through deceptive tactics designed to exploit both\nsemantic and structural vulnerabilities. Traditional detection methods, often\nbased on isolated analysis of email content or embedded URLs, fail to\ncomprehensively address these evolving attacks. In this paper, we propose a\ndual-path phishing detection framework that integrates transformer-based\nnatural language processing (NLP) with classical machine learning to jointly\nanalyze email text and embedded URLs. Our approach leverages the complementary\nstrengths of semantic analysis using fine-tuned transformer architectures\n(e.g., DistilBERT) and structural link analysis via character-level TF-IDF\nvectorization paired with classical classifiers (e.g., Random Forest).\nEmpirical evaluation on representative email and URL datasets demonstrates that\nthis combined approach significantly improves detection accuracy. Specifically,\nthe DistilBERT model achieves a near-optimal balance between accuracy and\ncomputational efficiency for textual phishing detection, while Random Forest\nnotably outperforms other classical classifiers in identifying malicious URLs.\nThe modular design allows flexibility for standalone deployment or ensemble\nintegration, facilitating real-world adoption. Collectively, our results\nhighlight the efficacy and practical value of this dual-path approach,\nestablishing a scalable, accurate, and interpretable solution capable of\nenhancing email security against contemporary phishing threats.", "AI": {"tldr": "A dual-path phishing detection framework combines transformer-based NLP and classical machine learning to improve email security by analyzing text and URLs synergistically.", "motivation": "Traditional phishing detection methods analyze email content or URLs in isolation, failing to address the dual semantic and structural complexity of modern phishing attacks.", "method": "The framework integrates fine-tuned DistilBERT for semantic text analysis with character-level TF-IDF vectorization and Random Forest for URL pattern analysis, enabling joint threat detection.", "result": "Empirical tests show DistilBERT achieves optimal text classification efficiency, Random Forest outperforms other URL classifiers, and the combined system improves overall detection accuracy compared to single-path approaches.", "conclusion": "This dual-path architecture provides a scalable, interpretable solution that balances performance and practicality, establishing a robust defense against evolving phishing techniques."}}
{"id": "2509.21057", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21057", "abs": "https://arxiv.org/abs/2509.21057", "authors": ["Jiahao Huo", "Shuliang Liu", "Bin Wang", "Junyan Zhang", "Yibo Yan", "Aiwei Liu", "Xuming Hu", "Mingxun Zhou"], "title": "PMark: Towards Robust and Distortion-free Semantic-level Watermarking with Channel Constraints", "comment": null, "summary": "Semantic-level watermarking (SWM) for large language models (LLMs) enhances\nwatermarking robustness against text modifications and paraphrasing attacks by\ntreating the sentence as the fundamental unit. However, existing methods still\nlack strong theoretical guarantees of robustness, and reject-sampling-based\ngeneration often introduces significant distribution distortions compared with\nunwatermarked outputs. In this work, we introduce a new theoretical framework\non SWM through the concept of proxy functions (PFs) $\\unicode{x2013}$ functions\nthat map sentences to scalar values. Building on this framework, we propose\nPMark, a simple yet powerful SWM method that estimates the PF median for the\nnext sentence dynamically through sampling while enforcing multiple PF\nconstraints (which we call channels) to strengthen watermark evidence. Equipped\nwith solid theoretical guarantees, PMark achieves the desired distortion-free\nproperty and improves the robustness against paraphrasing-style attacks. We\nalso provide an empirically optimized version that further removes the\nrequirement for dynamical median estimation for better sampling efficiency.\nExperimental results show that PMark consistently outperforms existing SWM\nbaselines in both text quality and robustness, offering a more effective\nparadigm for detecting machine-generated text. Our code will be released at\n[this URL](https://github.com/PMark-repo/PMark).", "AI": {"tldr": "Proposes PMark: proxy function-based SWM with theoretical robustness and reduced distortion, enabling better machine-generated text detection.", "motivation": "Existing SWM methods lack strong robustness guarantees and suffer from distribution distortions via reject-sampling; needs improved paraphrasing attack resistance.", "method": "Introduces PMark via proxy functions (PFs) for dynamic median estimation of sentence scalars, using multiple channels to strengthen watermarks and an optimized version for sampling efficiency.", "result": "Outperforms SWM baselines in text quality/robustness, achieves distortion-free generation with theoretical validity.", "conclusion": "PMark offers a more effective paradigm for detecting machine-generated text with theoretical guarantees and improved robustness."}}
{"id": "2509.21147", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21147", "abs": "https://arxiv.org/abs/2509.21147", "authors": ["Amr Akmal Abouelmagd", "Amr Hilal"], "title": "Emerging Paradigms for Securing Federated Learning Systems", "comment": null, "summary": "Federated Learning (FL) facilitates collaborative model training while\nkeeping raw data decentralized, making it a conduit for leveraging the power of\nIoT devices while maintaining privacy of the locally collected data. However,\nexisting privacy- preserving techniques present notable hurdles. Methods such\nas Multi-Party Computation (MPC), Homomorphic Encryption (HE), and Differential\nPrivacy (DP) often incur high compu- tational costs and suffer from limited\nscalability. This survey examines emerging approaches that hold promise for\nenhancing both privacy and efficiency in FL, including Trusted Execution\nEnvironments (TEEs), Physical Unclonable Functions (PUFs), Quantum Computing\n(QC), Chaos-Based Encryption (CBE), Neuromorphic Computing (NC), and Swarm\nIntelligence (SI). For each paradigm, we assess its relevance to the FL\npipeline, outlining its strengths, limitations, and practical considerations.\nWe conclude by highlighting open challenges and prospective research avenues,\noffering a detailed roadmap for advancing secure and scalable FL systems.", "AI": {"tldr": "This survey reviews emerging solutions (TEEs, QC, etc.) to overcome limitations of traditional privacy methods in FL, evaluates their trade-offs, and proposes a research roadmap for robust FL systems.", "motivation": "The paper addresses the inefficiency and scalability issues of current FL privacy techniques like MPC, HE, and DP, prompting a need for innovative solutions to enhance privacy and efficiency.", "method": "The paper conducts a survey of emerging approaches such as TEEs, PUFs, QC, CBE, NC, and SI, analyzing their relevance, strengths, and limitations within the FL pipeline.", "result": "The analysis provides a comprehensive evaluation of new paradigms in FL security, identifying trade-offs, practical barriers, and opportunities for advancing secure FL implementations.", "conclusion": "The paper concludes by underscoring unresolved challenges in privacy-preserving FL and outlines a roadmap for future research, emphasizing secure and scalable FL systems through emerging technologies."}}
