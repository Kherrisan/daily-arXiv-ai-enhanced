{"id": "2508.10059", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10059", "abs": "https://arxiv.org/abs/2508.10059", "authors": ["Yueke Zhang", "Yifan Zhang", "Kevin Leach", "Yu Huang"], "title": "FormalGrad: Integrating Formal Methods with Gradient-Based LLM Refinement", "comment": "6 Pages", "summary": "While Large Language Models (LLMs) have demonstrated remarkable capabilities\nin code generation, they often produce solutions that lack guarantees of\ncorrectness, robustness, and efficiency. The limitation is acute in domains\nrequiring strict constraints. FormalGrad introduces a principled framework that\nintegrates formal methods directly into an iterative LLM-based generation loop.\nIt uniquely treats code as a differentiable variable, converting structured\nfeedback and formal constraints into a textual pseudo-gradient. This gradient\nguides the model to iteratively refine solutions, ensuring they are not only\nfunctional but also robust and formally justified. We evaluate FormalGrad on\nthe HumanEval, HumanEval+, and LiveCodeBench benchmarks. Our implementation\noutperforms strong baselines, achieving an absolute improvement of up to 27% on\nHumanEval and a 41% relative improvement on the challenging LiveCodeBench V6.\nFormalGrad generates formally justified code that is robust and efficient,\npaving the way for reliable AI-assisted software development in high-stakes\napplications.", "AI": {"tldr": "FormalGrad integrates formal methods into iterative LLM-based code generation to produce correct, robust, and efficient solutions by treating code as a differentiable variable and using textual pseudo-gradients for refinement.", "motivation": "Large Language Models (LLMs) generate code lacking guarantees of correctness, robustness, and efficiency, which is problematic in high-constraint domains requiring reliable solutions.", "method": "FormalGrad converts structured feedback and formal constraints into a textual pseudo-gradient, allowing code to be treated as a differentiable variable and iteratively refined by the LLM within a generation loop.", "result": "The framework achieves up to a 27% absolute improvement on HumanEval and a 41% relative improvement on LiveCodeBench V6, outperforming strong baselines.", "conclusion": "FormalGrad enables reliable AI-assisted software development by generating formally justified and robust code, making it suitable for high-stakes applications."}}
{"id": "2508.10068", "categories": ["cs.SE", "cs.CL", "cs.IR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.10068", "abs": "https://arxiv.org/abs/2508.10068", "authors": ["Xiaohan Chen", "Zhongying Pan", "Quan Feng", "Yu Tian", "Shuqun Yang", "Mengru Wang", "Lina Gong", "Yuxia Geng", "Piji Li", "Xiang Chen"], "title": "SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion", "comment": null, "summary": "Retrieval-augmented generation (RAG) for repository-level code completion\ncommonly relies on superficial text similarity, leading to results plagued by\nsemantic misguidance, redundancy, and homogeneity, while also failing to\nresolve external symbol ambiguity. To address these challenges, we introduce\nSaracoder, a Hierarchical Feature-Optimized retrieval framework. Its core\nHierarchical Feature Optimization module systematically refines candidates by\ndistilling deep semantic relationships, pruning exact duplicates, assessing\nstructural similarity with a novel graph-based metric that weighs edits by\ntheir topological importance, and reranking results to maximize both relevance\nand diversity. Furthermore, an External-Aware Identifier Disambiguator module\naccurately resolves cross-file symbol ambiguity via dependency analysis.\nExtensive experiments on the challenging CrossCodeEval and RepoEval-Updated\nbenchmarks demonstrate that Saracoder significantly outperforms existing\nbaselines across multiple programming languages and models. Our work proves\nthat systematically refining retrieval results across multiple dimensions\nprovides a new paradigm for building more accurate and robust repository-level\ncode completion systems.", "AI": {"tldr": "Saracoder is a hierarchical feature-optimized retrieval framework that improves repository-level code completion by refining candidates based on semantic relationships, diversity, and cross-file symbol disambiguation, outperforming existing baselines on major benchmarks.", "motivation": "Existing RAG methods for code completion rely on superficial text similarity, leading to semantic misguidance, redundant/homogeneous results, and failure to resolve external symbol ambiguity in repositories.", "method": "1) Hierarchical Feature Optimization module: refines candidates through semantic distillation, duplicate pruning, and graph-based structural similarity metric with topological edit weighting. 2) External-Aware Identifier Disambiguator: resolves cross-file symbol ambiguity via dependency analysis.", "result": "Significantly outperforms existing baselines on CrossCodeEval and RepoEval-Updated benchmarks across multiple programming languages, demonstrating improved accuracy and robustness in code completion.", "conclusion": "Systematic refinement of retrieval results across semantic, structural, and contextual dimensions establishes a new paradigm for repository-level code completion systems through hierarchical feature optimization."}}
{"id": "2508.10074", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10074", "abs": "https://arxiv.org/abs/2508.10074", "authors": ["Ruofan Lu", "Yintong Huo", "Meng Zhang", "Yichen Li", "Michael R. Lyu"], "title": "Next Edit Prediction: Learning to Predict Code Edits from Context and Interaction History", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has led to the\nwidespread adoption of AI-powered coding assistants integrated into a\ndevelopment environment. On one hand, low-latency code completion offers\ncompletion suggestions but is fundamentally constrained to the cursor's current\nposition. On the other hand, chat-based editing can perform complex\nmodifications, yet forces developers to stop their work, describe the intent in\nnatural language, which causes a context-switch away from the code. This\ncreates a suboptimal user experience, as neither paradigm proactively predicts\nthe developer's next edit in a sequence of related edits. To bridge this gap\nand provide the seamless code edit suggestion, we introduce the task of Next\nEdit Prediction, a novel task designed to infer developer intent from recent\ninteraction history to predict both the location and content of the subsequent\nedit. Specifically, we curate a high-quality supervised fine-tuning dataset and\nan evaluation benchmark for the Next Edit Prediction task. Then, we conduct\nsupervised fine-tuning on a series of models and performed a comprehensive\nevaluation of both the fine-tuned models and other baseline models, yielding\nseveral novel findings. This work lays the foundation for a new interaction\nparadigm that proactively collaborate with developers by anticipating their\nfollowing action, rather than merely reacting to explicit instructions.", "AI": {"tldr": "This paper proposes Next Edit Prediction, a new task to anticipate developers' subsequent code edits by jointly predicting location and content from interaction history, addressing limitations of existing code completion and chat-based editing approaches.", "motivation": "Current code editing paradigms have inherent limitations: low-latency completion is restricted to the current cursor position, while chat-based editing requires developers to describe intentions in natural language, creating workflow disruptions.", "method": "The authors (1) curate a high-quality supervised fine-tuning dataset and evaluation benchmark for next edit prediction, (2) perform supervised fine-tuning on multiple models, and (3) conduct comprehensive evaluations comparing fine-tuned models with baselines.", "result": "The work yields novel findings from model evaluations, demonstrating the effectiveness of this new paradigm in anticipating sequential code edits beyond existing capabilities.", "conclusion": "Next Edit Prediction establishes a proactive collaboration paradigm for coding assistants, enabling AI to predict and suggest sequential edits rather than just reacting to explicit instructions or confined code context."}}
{"id": "2508.10157", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10157", "abs": "https://arxiv.org/abs/2508.10157", "authors": ["Ajibode Adekunle", "Abdul Ali Bangash", "Bram Adams", "Ahmed E. Hassan"], "title": "On the synchronization between Hugging Face pre-trained language models and their upstream GitHub repository", "comment": null, "summary": "Pretrained language models (PTLMs) have advanced natural language processing\n(NLP), enabling progress in tasks like text generation and translation. Like\nsoftware package management, PTLMs are trained using code and environment\nscripts in upstream repositories (e.g., GitHub, GH) and distributed as variants\nvia downstream platforms like Hugging Face (HF). Coordinating development\nbetween GH and HF poses challenges such as misaligned release timelines,\ninconsistent versioning, and limited reuse of PTLM variants. We conducted a\nmixed-method study of 325 PTLM families (904 HF variants) to examine how commit\nactivities are coordinated. Our analysis reveals that GH contributors typically\nmake changes related to specifying the version of the model, improving code\nquality, performance optimization, and dependency management within the\ntraining scripts, while HF contributors make changes related to improving model\ndescriptions, data set handling, and setup required for model inference.\nFurthermore, to understand the synchronization aspects of commit activities\nbetween GH and HF, we examined three dimensions of these activities -- lag\n(delay), type of synchronization, and intensity -- which together yielded eight\ndistinct synchronization patterns. The prevalence of partially synchronized\npatterns, such as Disperse synchronization and Sparse synchronization, reveals\nstructural disconnects in current cross-platform release practices. These\npatterns often result in isolated changes -- where improvements or fixes made\non one platform are never replicated on the other -- and in some cases,\nindicate an abandonment of one repository in favor of the other. Such\nfragmentation risks exposing end users to incomplete, outdated, or behaviorally\ninconsistent models. Hence, recognizing these synchronization patterns is\ncritical for improving oversight and traceability in PTLM release workflows.", "AI": {"tldr": "The paper analyzes synchronization challenges between GitHub and Hugging Face repositories for PTLMs, revealing 8 distinct patterns and their implications for model fragmentation.", "motivation": "Fragmentation between code repositories (GitHub) and model distribution platforms (Hugging Face) risks exposing end users to inconsistent, outdated, or incomplete PTLMs due to misaligned release practices.", "method": "Mixed-method study of 325 PTLM families (904 Hugging Face variants), analyzing commit activities across three dimensions: lag, synchronization type, and intensity to identify cross-platform coordination patterns.", "result": "Identified 8 synchronization patterns, including Disperse and Sparse types, showing frequent isolated changes and platform abandonment when PTLM developers modify code/scripts on GitHub or metadata/model files on Hugging Face without cross-referencing.", "conclusion": "Recognizing these synchronization patterns is essential for improving traceability and oversight in PTLM release workflows to prevent user exposure to fragmented models."}}
{"id": "2508.10017", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10017", "abs": "https://arxiv.org/abs/2508.10017", "authors": ["Rodrigo Tertulino"], "title": "A Robust Pipeline for Differentially Private Federated Learning on Imbalanced Clinical Data using SMOTETomek and FedProx", "comment": "This is being prepared to be submitted to the Journal of the\n  Brazilian Computer Society (JBCS), which is still under construction", "summary": "Federated Learning (FL) presents a groundbreaking approach for collaborative\nhealth research, allowing model training on decentralized data while\nsafeguarding patient privacy. FL offers formal security guarantees when\ncombined with Differential Privacy (DP). The integration of these technologies,\nhowever, introduces a significant trade-off between privacy and clinical\nutility, a challenge further complicated by the severe class imbalance often\npresent in medical datasets. The research presented herein addresses these\ninterconnected issues through a systematic, multi-stage analysis. An FL\nframework was implemented for cardiovascular risk prediction, where initial\nexperiments showed that standard methods struggled with imbalanced data,\nresulting in a recall of zero. To overcome such a limitation, we first\nintegrated the hybrid Synthetic Minority Over-sampling Technique with Tomek\nLinks (SMOTETomek) at the client level, successfully developing a clinically\nuseful model. Subsequently, the framework was optimized for non-IID data using\na tuned FedProx algorithm. Our final results reveal a clear, non-linear\ntrade-off between the privacy budget (epsilon) and model recall, with the\noptimized FedProx consistently out-performing standard FedAvg. An optimal\noperational region was identified on the privacy-utility frontier, where strong\nprivacy guarantees (with epsilon 9.0) can be achieved while maintaining high\nclinical utility (recall greater than 77%). Ultimately, our study provides a\npractical methodological blueprint for creating effective, secure, and accurate\ndiagnostic tools that can be applied to real-world, heterogeneous healthcare\ndata.", "AI": {"tldr": "This paper proposes a federated learning framework for cardiovascular risk prediction that addresses privacy-utility trade-offs and imbalanced medical data through SMOTETomek and optimzed FedProx, achieving strong privacy (epsilon=9.0) with high model recall (77%).", "motivation": "Federated Learning (FL) enables privacy-preserving healthcare collaboration, but faces challenges from severe class imbalance and the privacy-utility trade-off when combining with Differential Privacy (DP). This study aims to develop practical solutions for creating accurate clinical models under these constraints.", "method": "The framework combines 1) client-level hybrid SMOTETomek oversampling for imbalanced data handling and 2) tuned FedProx algorithm for non-IID data optimization in FL settings.", "result": "1) Standard FL methods achieved zero recall on imbalanced clinical data\n2) SMOTETomek integration created clinically useful models\n3) Optimized FedProx outperformed standard FedAvg\n4) Non-linear privacy-utility trade-off identified\n5) Achieved 77%+ recall at epsilon=9.0 (strong privacy guarantee)", "conclusion": "Provides a methodological blueprint for developing effective, privacy-preserving diagnostic tools for real-world healthcare scenarios, demonstrating that 77%+ clinical utility can be maintained while achieving epsilon=9.0 level privacy protection in non-IID medical data contexts."}}
{"id": "2508.10517", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10517", "abs": "https://arxiv.org/abs/2508.10517", "authors": ["Likai Ye", "Mengliang Li", "Dehai Zhao", "Jiamou Sun", "Xiaoxue Ren"], "title": "Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart Contract Compilation Error Resolution", "comment": "International Conference on Software Maintenance and Evolution\n  (ICSME) 2025", "summary": "Solidity, the dominant smart contract language for Ethereum, has rapidly\nevolved with frequent version updates to enhance security, functionality, and\ndeveloper experience. However, these continual changes introduce significant\nchallenges, particularly in compilation errors, code migration, and\nmaintenance. Therefore, we conduct an empirical study to investigate the\nchallenges in the Solidity version evolution and reveal that 81.68% of examined\ncontracts encounter errors when compiled across different versions, with 86.92%\nof compilation errors.\n  To mitigate these challenges, we conducted a systematic evaluation of large\nlanguage models (LLMs) for resolving Solidity compilation errors during version\nmigrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek)\nand closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these\nmodels exhibit error repair capabilities, their effectiveness diminishes\nsignificantly for semantic-level issues and shows strong dependency on prompt\nengineering strategies. This underscores the critical need for domain-specific\nadaptation in developing reliable LLM-based repair systems for smart contracts.\n  Building upon these insights, we introduce SMCFIXER, a novel framework that\nsystematically integrates expert knowledge retrieval with LLM-based repair\nmechanisms for Solidity compilation error resolution. The architecture\ncomprises three core phases: (1) context-aware code slicing that extracts\nrelevant error information; (2) expert knowledge retrieval from official\ndocumentation; and (3) iterative patch generation for Solidity migration.\nExperimental validation across Solidity version migrations demonstrates our\napproach's statistically significant 24.24% improvement over baseline GPT-4o on\nreal-world datasets, achieving near-perfect 96.97% accuracy.", "AI": {"tldr": "The paper addresses the challenges of Solidity version evolution, which causes compilation errors in most contracts. It evaluates large language models (LLMs) for error resolution but highlights their limitations, particularly for semantic errors and prompt engineering. The study introduces SMCFIXER, a framework integrating expert knowledge retrieval and LLM-based repair, achieving a 24.24% improvement and 96.97% accuracy over baseline GPT-4o.", "motivation": "Frequent Solidity version updates lead to compilation errors, code migration complexities, and maintenance issues, with 81.68% of contracts encountering errors and 86.92% resulting in compilation failures. This necessitates effective strategies for error resolution during version migrations.", "method": "The research involves an empirical analysis of Solidity version issues, evaluating both open-source (LLaMA3, DeepSeek) and closed-source (GPT-4o, GPT-3.5-turbo) LLMs for error repair. Key insights led to the development of SMCFIXER, which includes three phases: context-aware code slicing, expert knowledge retrieval from documentation, and iterative patch generation.", "result": "LLMs demonstrated error repair capabilities but struggled with semantic errors and required robust prompt engineering. SMCFIXER achieved a statistically significant 24.24% improvement over GPT-4o and 96.97% accuracy on real-world datasets during Solidity version migrations.", "conclusion": "The study underscores the need for domain-specific adaptation in LLM-based smart contract repair systems. SMCFIXER\u2019s framework, combining expert knowledge and iterative patch generation, effectively mitigates version evolution challenges, emphasizing the importance of tailored solutions for complex semantic errors in Solidity."}}
{"id": "2508.10023", "categories": ["cs.CR", "quant-ph", "94A60 (Cryptography)"], "pdf": "https://arxiv.org/pdf/2508.10023", "abs": "https://arxiv.org/abs/2508.10023", "authors": ["Samet \u00dcnsal"], "title": "A Comparative Performance Evaluation of Kyber, sntrup761, and FrodoKEM for Post-Quantum Cryptography", "comment": "12 pages, 3 tables, IEEE conference format", "summary": "Post-quantum cryptography (PQC) aims to develop cryptographic algorithms that\nare secure against attacks from quantum computers. This paper compares the\nleading postquantum cryptographic algorithms, such as Kyber, sntrup761, and\nFrodoKEM, in terms of their security, performance, and real-world\napplicability. The review highlights the strengths and weaknesses of each\nalgorithm and provides insights into future research directions. We also\ndiscuss the challenges of transitioning from classical to post-quantum systems\nand the potential impacts on various industries. This paper serves as a\nfoundation for understanding the current state of post-quantum cryptography and\nits future prospects in the quantum computing era.", "AI": {"tldr": "This paper compares leading post-quantum cryptographic algorithms (Kyber, sntrup761, FrodoKEM) in security, performance, and real-world applicability, while discussing transition challenges and future directions for quantum-resistant cryptography.", "motivation": "The advent of quantum computing threatens classical cryptographic algorithms, necessitating the development and analysis of quantum-resistant alternatives to ensure long-term information security across industries.", "method": "The study conducts a comprehensive review and comparative analysis of prominent lattice-based PQC algorithms, evaluating their technical properties, performance metrics, and practical implementations through literature examination.", "result": "Kyber shows balanced security-performance tradeoffs, sntrup761 demonstrates efficiency advantages in specific parameters, and FrodoKEM exhibits strong security but higher cryptographic overhead, with transition challenges identified in interoperation and industry adoption.", "conclusion": "The paper establishes a framework for understanding PQC maturity while emphasizing the urgency of addressing technical and implementation challenges to prepare for the quantum computing era transition."}}
{"id": "2508.10852", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10852", "abs": "https://arxiv.org/abs/2508.10852", "authors": ["Souhaila Serbout", "Diana Carolina Mu\u00f1oz Hurtado", "Hassan Atwi", "Edoardo Riggio", "Cesare Pautasso"], "title": "EVOSCAT: Exploring Software Change Dynamics in Large-Scale Historical Datasets", "comment": "Submitted to VISSOFT 2025. For the hi-resolution version of the\n  paper, see https://design.inf.usi.ch/publications/2025/vissoft", "summary": "Long lived software projects encompass a large number of artifacts, which\nundergo many revisions throughout their history. Empirical software engineering\nresearchers studying software evolution gather and collect datasets with\nmillions of events, representing changes introduced to specific artifacts. In\nthis paper, we propose EvoScat, a tool that attempts addressing temporal\nscalability through the usage of interactive density scatterplot to provide a\nglobal overview of large historical datasets mined from open source\nrepositories in a single visualization. EvoScat intents to provide researchers\nwith a mean to produce scalable visualizations that can help them explore and\ncharacterize evolution datasets, as well as comparing the histories of\nindividual artifacts, both in terms of 1) observing how rapidly different\nartifacts age over multiple-year-long time spans 2) how often metrics\nassociated with each artifacts tend towards an improvement or worsening. The\npaper shows how the tool can be tailored to specific analysis needs (pace of\nchange comparison, clone detection, freshness assessment) thanks to its support\nfor flexible configuration of history scaling and alignment along the time\naxis, artifacts sorting and interactive color mapping, enabling the analysis of\nmillions of events obtained by mining the histories of tens of thousands of\nsoftware artifacts. We include in this paper a gallery showcasing datasets\ngathering specific artifacts (OpenAPI descriptions, GitHub workflow\ndefinitions) across multiple repositories, as well as diving into the history\nof specific popular open source projects.", "AI": {"tldr": "EvoScat is an interactive density scatterplot tool for scalable analysis of large software evolution datasets, enabling researchers to explore artifact aging, metric trends, and comparative histories across thousands of open-source artifacts.", "motivation": "Chronically evolving software projects generate massive revision datasets, requiring scalable visualization tools to explore/compare artifact histories, track aging rates, and analyze metric improvements/worsenings over multi-year periods.", "method": "The tool uses interactive density scatterplots with configurable history scaling/alignment, artifact sorting, and dynamic color mapping to visualize millions of events from tens of thousands of software artifacts in open-source repositories.", "result": "EvoScat successfully analyzes datasets including OpenAPI descriptions, GitHub workflow definitions, and popular open-source projects, demonstrating scalability through flexible configuration options for change pace comparison, clone detection, and freshness assessment.", "conclusion": "EvoScat provides an effective visualization approach for large-scale software evolution analysis by addressing temporal scalability challenges, offering researchers customized analysis capabilities through interactive dense dataset exploration."}}
{"id": "2508.10031", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10031", "abs": "https://arxiv.org/abs/2508.10031", "authors": ["Jinhwa Kim", "Ian G. Harris"], "title": "Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs", "comment": "13 pages, 2 figures", "summary": "While Large Language Models (LLMs) have shown significant advancements in\nperformance, various jailbreak attacks have posed growing safety and ethical\nrisks. Malicious users often exploit adversarial context to deceive LLMs,\nprompting them to generate responses to harmful queries. In this study, we\npropose a new defense mechanism called Context Filtering model, an input\npre-processing method designed to filter out untrustworthy and unreliable\ncontext while identifying the primary prompts containing the real user intent\nto uncover concealed malicious intent. Given that enhancing the safety of LLMs\noften compromises their helpfulness, potentially affecting the experience of\nbenign users, our method aims to improve the safety of the LLMs while\npreserving their original performance. We evaluate the effectiveness of our\nmodel in defending against jailbreak attacks through comparative analysis,\ncomparing our approach with state-of-the-art defense mechanisms against six\ndifferent attacks and assessing the helpfulness of LLMs under these defenses.\nOur model demonstrates its ability to reduce the Attack Success Rates of\njailbreak attacks by up to 88% while maintaining the original LLMs'\nperformance, achieving state-of-the-art Safety and Helpfulness Product results.\nNotably, our model is a plug-and-play method that can be applied to all LLMs,\nincluding both white-box and black-box models, to enhance their safety without\nrequiring any fine-tuning of the models themselves. We will make our model\npublicly available for research purposes.", "AI": {"tldr": "The study introduces a Context Filtering model to defend against jailbreak attacks on Large Language Models (LLMs), aiming to reduce attack success rates while preserving LLM performance. It achieves up to 88% reduction in attacks with state-of-the-art Safety and Helpfulness Product results and is a plug-and-play solution applicable to various LLMs.", "motivation": "LLMs face increasing safety and ethical risks due to jailbreak attacks that exploit adversarial context to generate harmful responses. Existing defenses often compromise LLM helpfulness, negatively impacting benign users.", "method": "Context Filtering model: an input pre-processing method to filter untrustworthy context, identify primary prompts containing user intent, and uncover concealed malicious intent without requiring LLM fine-tuning. It is compatible with both white-box and black-box models.", "result": "The model reduces jailbreak attack success rates by 88% while maintaining LLM performance, achieving state-of-the-art Safety and Helpfulness Product results. Comparative analysis shows superior effectiveness against six different attacks.", "conclusion": "The Context Filtering model provides a universal, non-intrusive defense for LLMs against jailbreak attacks, preserving helpfulness without model adjustments. It is publicly available for research."}}
{"id": "2508.10033", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10033", "abs": "https://arxiv.org/abs/2508.10033", "authors": ["Yuksel Aydin"], "title": "Cognitive Cybersecurity for Artificial Intelligence: Guardrail Engineering with CCS-7", "comment": null, "summary": "Language models exhibit human-like cognitive vulnerabilities, such as\nemotional framing, that escape traditional behavioral alignment. We present\nCCS-7 (Cognitive Cybersecurity Suite), a taxonomy of seven vulnerabilities\ngrounded in human cognitive security research. To establish a human benchmark,\nwe ran a randomized controlled trial with 151 participants: a \"Think First,\nVerify Always\" (TFVA) lesson improved cognitive security by +7.9% overall. We\nthen evaluated TFVA-style guardrails across 12,180 experiments on seven diverse\nlanguage model architectures. Results reveal architecture-dependent risk\npatterns: some vulnerabilities (e.g., identity confusion) are almost fully\nmitigated, while others (e.g., source interference) exhibit escalating\nbackfire, with error rates increasing by up to 135% in certain models. Humans,\nin contrast, show consistent moderate improvement. These findings reframe\ncognitive safety as a model-specific engineering problem: interventions\neffective in one architecture may fail, or actively harm, another, underscoring\nthe need for architecture-aware cognitive safety testing before deployment.", "AI": {"tldr": "This paper introduces CCS-7, a taxonomy of seven cognitive vulnerabilities in language models, demonstrating through human experiments and model evaluations that cognitive safety requires architecture-specific interventions due to varying efficacy and backfire effects.", "motivation": "Traditional behavioral alignment methods fail to address human-like cognitive vulnerabilities (e.g., emotional framing) in language models, necessitating new frameworks for robust cognitive safety.", "method": "1. Proposed CCS-7 taxonomy based on human cognitive security research. 2. Conducted a randomized controlled trial with 151 human participants to benchmark the 'Think First, Verify Always' (TFVA) intervention. 3. Evaluated TFVA-style guardrails across 12,180 experiments on seven model architectures.", "result": "Architecture-dependent vulnerability patterns emerged: identity confusion was nearly mitigated, but source interference risk increased by 135% in some models. Humans showed consistent 7.9% improvement with TFVA training.", "conclusion": "Cognitive safety must be approached as an architecture-specific engineering challenge; interventions effective in one model may backfire in another, requiring pre-deployment architecture-aware testing."}}
{"id": "2508.10035", "categories": ["cs.CR", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.10035", "abs": "https://arxiv.org/abs/2508.10035", "authors": ["Varsha Sen", "Biswash Basnet"], "title": "Neural Network-Based Detection and Multi-Class Classification of FDI Attacks in Smart Grid Home Energy Systems", "comment": "17 pages, 7 figures", "summary": "False Data Injection Attacks (FDIAs) pose a significant threat to smart grid\ninfrastructures, particularly Home Area Networks (HANs), where real-time\nmonitoring and control are highly adopted. Owing to the comparatively less\nstringent security controls and widespread availability of HANs, attackers view\nthem as an attractive entry point to manipulate aggregated demand patterns,\nwhich can ultimately propagate and disrupt broader grid operations. These\nattacks undermine the integrity of smart meter data, enabling malicious actors\nto manipulate consumption values without activating conventional alarms,\nthereby creating serious vulnerabilities across both residential and\nutility-scale infrastructures. This paper presents a machine learning-based\nframework for both the detection and classification of FDIAs using residential\nenergy data. A real-time detection is provided by the lightweight Artificial\nNeural Network (ANN), which works by using the most vital features of energy\nconsumption, cost, and time context. For the classification of different attack\ntypes, a Bidirectional LSTM is trained to recognize normal, trapezoidal, and\nsigmoid attack shapes through learning sequential dependencies in the data. A\nsynthetic time-series dataset was generated to emulate realistic household\nbehaviour. Experimental results demonstrate that the proposed models are\neffective in identifying and classifying FDIAs, offering a scalable solution\nfor enhancing grid resilience at the edge. This work contributes toward\nbuilding intelligent, data-driven defence mechanisms that strengthen smart grid\ncybersecurity from residential endpoints.", "AI": {"tldr": "This paper introduces a machine learning-based framework using a lightweight ANN for real-time detection and a Bidirectional LSTM for classification of False Data Injection Attacks (FDIAs) in smart grids, particularly targeting Home Area Networks (HANs) to enhance grid resilience against data manipulation.", "motivation": "The paper addresses the vulnerability of smart grids, especially HANs, to FDIAs which can manipulate demand patterns and disrupt grid operations due to insufficient security measures, necessitating scalable and effective detection solutions.", "method": "The authors propose two models: a lightweight Artificial Neural Network (ANN) for real-time FDIAs detection leveraging energy consumption, cost, and time context features, and a Bidirectional Long Short-Term Memory (LSTM) network for classifying attack types (normal, trapezoidal, sigmoid) by learning sequential dependencies. They validated the framework using a synthetic time-series dataset mimicking household energy behavior.", "result": "Experimental evaluations confirmed the effectiveness of both models, achieving accurate detection and classification of FDIAs. The proposed solution demonstrates scalability and real-time usability for enhancing smart grid security at the edge level.", "conclusion": "The paper concludes that the framework provides a robust, data-driven approach to defend smart grids against FDIAs by deploying ML techniques at residential endpoints, contributing toward improved cybersecurity and operational resilience in HANs."}}
{"id": "2508.10038", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10038", "abs": "https://arxiv.org/abs/2508.10038", "authors": ["Pierre-Francois Gimenez", "Sarath Sivaprasad", "Mario Fritz"], "title": "Certifiably robust malware detectors by design", "comment": null, "summary": "Malware analysis involves analyzing suspicious software to detect malicious\npayloads. Static malware analysis, which does not require software execution,\nrelies increasingly on machine learning techniques to achieve scalability.\nAlthough such techniques obtain very high detection accuracy, they can be\neasily evaded with adversarial examples where a few modifications of the sample\ncan dupe the detector without modifying the behavior of the software. Unlike\nother domains, such as computer vision, creating an adversarial example of\nmalware without altering its functionality requires specific transformations.\nWe propose a new model architecture for certifiably robust malware detection by\ndesign. In addition, we show that every robust detector can be decomposed into\na specific structure, which can be applied to learn empirically robust malware\ndetectors, even on fragile features. Our framework ERDALT is based on this\nstructure. We compare and validate these approaches with machine-learning-based\nmalware detection methods, allowing for robust detection with limited reduction\nof detection performance.", "AI": {"tldr": "This paper proposes a certifiably robust malware detection framework (ERDALT) designed to resist adversarial examples without compromising detection performance, addressing vulnerabilities in static ML-based malware analysis.", "motivation": "Despite high accuracy, static malware detectors using ML are vulnerable to adversarial attacks where minor code modifications can evade detection without altering behavior. Unlike computer vision, malware adversarial examples require precise functional transformations.", "method": "The authors introduce a novel model architecture for robust malware detection by design and demonstrate that robust detectors can be decomposed into a specific structure. Their ERDALT framework leverages this architectural principle to create certifiably robust malware classifiers.", "result": "ERDALT achieves robust detection against adversarial examples while maintaining strong detection performance, validated through comparisons with standard machine learning-based malware detection approaches.", "conclusion": "The proposed structured approach enables scalable malware detection systems that inherently resist adversarial attacks, even when using fragile features, bridging a critical security gap in static analysis."}}
{"id": "2508.10039", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10039", "abs": "https://arxiv.org/abs/2508.10039", "authors": ["Wenqiang Wang", "Yan Xiao", "Hao Lin", "Yangshijie Zhang", "Xiaochun Cao"], "title": "Multi-task Adversarial Attacks against Black-box Model with Few-shot Queries", "comment": null, "summary": "Current multi-task adversarial text attacks rely on abundant access to shared\ninternal features and numerous queries, often limited to a single task type. As\na result, these attacks are less effective against practical scenarios\ninvolving black-box feedback APIs, limited queries, or multiple task types. To\nbridge this gap, we propose \\textbf{C}luster and \\textbf{E}nsemble\n\\textbf{M}ulti-task Text Adversarial \\textbf{A}ttack (\\textbf{CEMA}), an\neffective black-box attack that exploits the transferability of adversarial\ntexts across different tasks. CEMA simplifies complex multi-task scenarios by\nusing a \\textit{deep-level substitute model} trained in a\n\\textit{plug-and-play} manner for text classification, enabling attacks without\nmimicking the victim model. This approach requires only a few queries for\ntraining, converting multi-task attacks into classification attacks and\nallowing attacks across various tasks.\n  CEMA generates multiple adversarial candidates using different text\nclassification methods and selects the one that most effectively attacks\nsubstitute models.\n  In experiments involving multi-task models with two, three, or six\ntasks--spanning classification, translation, summarization, and text-to-image\ngeneration--CEMA demonstrates significant attack success with as few as 100\nqueries. Furthermore, CEMA can target commercial APIs (e.g., Baidu and Google\nTranslate), large language models (e.g., ChatGPT 4o), and image-generation\nmodels (e.g., Stable Diffusion V2), showcasing its versatility and\neffectiveness in real-world applications.", "AI": {"tldr": "This paper introduces CEMA, a black-box adversarial text attack method with high query efficiency and multi-task versatility by leveraging a plug-and-play substitute model and ensemble attack strategies across diverse tasks.", "motivation": "Current multi-task adversarial text attacks require excessive model access/internal features and are limited to single-task scenarios, reducing their effectiveness in practical black-box settings with restricted queries.", "method": "CEMA uses a deep-level substitute model trained independently for text classification to bypass target model mimicry, combined with adversarial candidate ensembles and selection based on substitute model attack effectiveness.", "result": "Achieves high attack success rates with 100 queries on multi-task models (2-6 tasks) across classification, translation, summarization, and image generation, effectively targeting commercial APIs (Baidu, Google Translate), ChatGPT 4o, and Stable Diffusion V2.", "conclusion": "CEMA demonstrates that multi-task adversarial attacks can be simplified to classification problems through substitute model transferability, enabling practical black-box attacks with limited queries across diverse tasks and models."}}
{"id": "2508.10041", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.10041", "abs": "https://arxiv.org/abs/2508.10041", "authors": ["Julien Mellaerts"], "title": "Quantum Prime Factorization: A Novel Approach Based on Fermat Method", "comment": null, "summary": "In this paper, we introduce a novel quantum algorithm for the factorization\nof composite odd numbers. This work makes two significant contributions. First,\nwe present a new improvement to the classical Fermat method, fourfold reducing\nthe computational complexity of factoring. Second, we reformulate Fermat\nfactorization method as an optimization problem suitable for Quantum Annealers\nwhich allowed us to factorize 8,689,739, the biggest number ever factorized\nusing a quantum device to our knowledge.", "AI": {"tldr": "This paper presents a novel quantum algorithm for factoring composite odd numbers, enhancing the Fermat method fourfold and using Quantum Annealers to achieve the largest factorization on a quantum device to date.", "motivation": "Factoring large composite numbers is crucial for cryptography and demonstrating quantum computing capabilities. The classical Fermat method's limitations in complexity necessitate improvements.", "method": "Improved Fermat method by reducing computational complexity by fourfold, then reformulated it as an optimization problem for Quantum Annealers.", "result": "Successfully factorized 8,689,739 using a quantum device, the largest number achieved with a quantum approach, and reduced the computational complexity.", "conclusion": "The combination of classical method enhancement and quantum optimization establishes a new benchmark for factoring with potential applications in quantum algorithms."}}
{"id": "2508.10042", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10042", "abs": "https://arxiv.org/abs/2508.10042", "authors": ["Jane Carney", "Kushal Upreti", "Gaby G. Dagher", "Tim Andersen"], "title": "FIDELIS: Blockchain-Enabled Protection Against Poisoning Attacks in Federated Learning", "comment": null, "summary": "Federated learning enhances traditional deep learning by enabling the joint\ntraining of a model with the use of IoT device's private data. It ensures\nprivacy for clients, but is susceptible to data poisoning attacks during\ntraining that degrade model performance and integrity. Current poisoning\ndetection methods in federated learning lack a standardized detection method or\ntake significant liberties with trust. In this paper, we present \\Sys, a novel\nblockchain-enabled poison detection framework in federated learning. The\nframework decentralizes the role of the global server across participating\nclients. We introduce a judge model used to detect data poisoning in model\nupdates. The judge model is produced by each client and verified to reach\nconsensus on a single judge model. We implement our solution to show \\Sys is\nrobust against data poisoning attacks and the creation of our judge model is\nscalable.", "AI": {"tldr": "The paper introduces a blockchain-powered poison detection framework for federated learning, decentralizing the global server's role and leveraging client-agreed judge models to enhance robustness against data poisoning attacks while maintaining scalability.", "motivation": "Federated learning is vulnerable to data poisoning attacks due to reliance on decentralized, untrusted client data. Existing detection methods lack standardization or trustworthy verification mechanisms.", "method": "Sys utilizes blockchain to distribute the global server's responsibilities across clients. Each client generates a judge model to identify poisoned updates, with consensus mechanisms ensuring agreement on a unified judge model for validation.", "result": "Implementation results demonstrate Sys effectively resists data poisoning attacks and exhibits scalability in judging model creation, validating its practicality for real-world applications.", "conclusion": "Sys represents a secure, decentralized solution for federated learning robustness, achieving consensus for poison detection without compromising privacy or scalability, as evidenced by successful implementation."}}
{"id": "2508.10043", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10043", "abs": "https://arxiv.org/abs/2508.10043", "authors": ["Pallavi Zambare", "Venkata Nikhil Thanikella", "Ying Liu"], "title": "Securing Agentic AI: Threat Modeling and Risk Analysis for Network Monitoring Agentic AI System", "comment": "Submitted and under review in IEEE Transactions on Privacy", "summary": "When combining Large Language Models (LLMs) with autonomous agents, used in\nnetwork monitoring and decision-making systems, this will create serious\nsecurity issues. In this research, the MAESTRO framework consisting of the\nseven layers threat modeling architecture in the system was used to expose,\nevaluate, and eliminate vulnerabilities of agentic AI. The prototype agent\nsystem was constructed and implemented, using Python, LangChain, and telemetry\nin WebSockets, and deployed with inference, memory, parameter tuning, and\nanomaly detection modules. Two practical threat cases were confirmed as\nfollows: (i) resource denial of service by traffic replay denial-of-service,\nand (ii) memory poisoning by tampering with the historical log file maintained\nby the agent. These situations resulted in measurable levels of performance\ndegradation, i.e. telemetry updates were delayed, and computational loads were\nincreased, as a result of poor system adaptations. It was suggested to use a\nmultilayered defense-in-depth approach with memory isolation, validation of\nplanners and anomaly response systems in real-time. These findings verify that\nMAESTRO is viable in operational threat mapping, prospective risk scoring, and\nthe basis of the resilient system design. The authors bring attention to the\nimportance of the enforcement of memory integrity, paying attention to the\nadaptation logic monitoring, and cross-layer communication protection that\nguarantee the agentic AI reliability in adversarial settings.", "AI": {"tldr": "This paper analyzes security vulnerabilities in autonomous agentic AI systems combined with Large Language Models (LLMs) through the MAESTRO threat modeling framework. It identifies two critical threats\u2014resource denial-of-service and memory poisoning\u2014and proposes a multilayered defense strategy to ensure system reliability.", "motivation": "The integration of LLMs with autonomous agents in network monitoring and decision-making systems introduces significant security risks, necessitating proactive threat modeling to prevent adversarial exploitation and ensure robust system design.", "method": "The authors applied the MAESTRO seven-layer threat modeling architecture to assess vulnerabilities. They built a prototype agent system using Python, LangChain, and WebSocket telemetry, incorporating modules for inference, memory, parameter tuning, and anomaly detection. Two threat scenarios were validated experimentally.", "result": "Practical confirmation of two vulnerabilities: (1) traffic replay denial-of-service leading to delayed telemetry updates, and (2) memory poisoning via log file tampering causing increased computational loads. These demonstrated measurable system performance degradation.", "conclusion": "MAESTRO's multilayered defense-in-depth approach\u2014which includes memory isolation, planner validation, and real-time anomaly response\u2014provides viable solutions for operational threat mapping, risk scoring, and resilient system design. The study underscores memory integrity and cross-layer communication protection as critical for agentic AI reliability."}}
{"id": "2508.10044", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10044", "abs": "https://arxiv.org/abs/2508.10044", "authors": ["Aydin Zaboli", "Junho Hong"], "title": "Generative AI for Cybersecurity of Energy Management Systems: Methods, Challenges, and Future Directions", "comment": "36 pages, 10 figures", "summary": "This paper elaborates on an extensive security framework specifically\ndesigned for energy management systems (EMSs), which effectively tackles the\ndynamic environment of cybersecurity vulnerabilities and/or system problems\n(SPs), accomplished through the incorporation of novel methodologies. A\ncomprehensive multi-point attack/error model is initially proposed to\nsystematically identify vulnerabilities throughout the entire EMS data\nprocessing pipeline, including post state estimation (SE) stealth attacks, EMS\ndatabase manipulation, and human-machine interface (HMI) display corruption\naccording to the real-time database (RTDB) storage. This framework acknowledges\nthe interconnected nature of modern attack vectors, which utilize various\nphases of supervisory control and data acquisition (SCADA) data flow. Then,\ngenerative AI (GenAI)-based anomaly detection systems (ADSs) for EMSs are\nproposed for the first time in the power system domain to handle the scenarios.\nFurther, a set-of-mark generative intelligence (SoM-GI) framework, which\nleverages multimodal analysis by integrating visual markers with rules\nconsidering the GenAI capabilities, is suggested to overcome inherent spatial\nreasoning limitations. The SoM-GI methodology employs systematic visual\nindicators to enable accurate interpretation of segmented HMI displays and\ndetect visual anomalies that numerical methods fail to identify. Validation on\nthe IEEE 14-Bus system shows the framework's effectiveness across scenarios,\nwhile visual analysis identifies inconsistencies. This integrated approach\ncombines numerical analysis with visual pattern recognition and linguistic\nrules to protect against cyber threats and system errors.", "AI": {"tldr": "The paper introduces a comprehensive security framework for energy management systems (EMSs) that addresses dynamic cybersecurity vulnerabilities using a multi-point attack/model methodology, generative AI-based anomaly detection, and the SoM-GI framework for visual and multimodal analysis. It validates the framework\u2019s effectiveness on an IEEE 14-Bus system.", "motivation": "Modern EMSs face interconnected cybersecurity threats and system errors across the data processing pipeline, necessitating adaptive frameworks to handle evolving attack vectors and their real-time impacts on operational systems like HMI displays and databases.", "method": "1. Proposes a multi-point attack/error model to identify vulnerabilities in EMS data processing (post-SE stealth attacks, database manipulation, HMI display corruption). 2. Introduces genAI-based anomaly detection systems (ADSs) for the first time in power systems. 3. Develops the SoM-GI framework, which integrates visual markers with rules to improve spatial reasoning and detect visual anomalies in HMI displays.", "result": "Validation on the IEEE 14-Bus system demonstrates the framework\u2019s ability to detect anomalies across diverse scenarios and identify inconsistencies in real-time operations, including failures in numerical detection methods. The integrated approach combines numerical and visual pattern recognition with linguistic rules.", "conclusion": "The hybrid framework effectively mitigates cyber threats and system errors in EMSs by leveraging genAI capabilities, multimodal analysis, and visual indicators, offering a novel solution for dynamic and complex attack environments in power systems."}}
{"id": "2508.10052", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10052", "abs": "https://arxiv.org/abs/2508.10052", "authors": ["Pallavi Zambare", "Venkata Nikhil Thanikella", "Nikhil Padmanabh Kottur", "Sree Akhil Akula", "Ying Liu"], "title": "NetMoniAI: An Agentic AI Framework for Network Security & Monitoring", "comment": "Accepted in IEEE 3rd International Conference on Artificial\n  Intelligence, Blockchain, and Internet of Things (AIBThings 2025)", "summary": "In this paper, we present NetMoniAI, an agentic AI framework for automatic\nnetwork monitoring and security that integrates decentralized analysis with\nlightweight centralized coordination. The framework consists of two layers:\nautonomous micro-agents at each node perform local traffic analysis and anomaly\ndetection. A central controller then aggregates insights across nodes to detect\ncoordinated attacks and maintain system-wide situational awareness. We\nevaluated NetMoniAI on a local micro-testbed and through NS-3 simulations.\nResults confirm that the two-tier agentic-AI design scales under resource\nconstraints, reduces redundancy, and improves response time without\ncompromising accuracy. To facilitate broader adoption and reproducibility, the\ncomplete framework is available as open source. This enables researchers and\npractitioners to replicate, validate, and extend it across diverse network\nenvironments and threat scenarios. Github link:\nhttps://github.com/pzambare3/NetMoniAI", "AI": {"tldr": "NetMoniAI combines decentralized and centralized AI for efficient network monitoring and security.", "motivation": "The paper addresses scalability and accuracy challenges in network monitoring systems by introducing a two-tier architecture that minimizes redundancy while maintaining situational awareness across diverse threat scenarios.", "method": "The framework employs autonomous micro-agents for local traffic analysis and anomaly detection at each network node, coupled with a central controller that aggregates node-specific insights to identify coordinated attacks and systemic issues. Evaluation used local testbeds and NS-3 simulations.", "result": "NetMoniAI demonstrates improved scalability, reduced redundancy, faster response times, and preserved accuracy under resource constraints across test environments and simulated scenarios.", "conclusion": "The agentic-AI framework offers a reproducible, open-source solution for network monitoring that balances decentralized automation with centralized coordination, enabling researchers to extend its capabilities to varied environments."}}
{"id": "2508.10065", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10065", "abs": "https://arxiv.org/abs/2508.10065", "authors": ["Yuhao Sun", "Yihua Zhang", "Gaowen Liu", "Hongtao Xie", "Sijia Liu"], "title": "Invisible Watermarks, Visible Gains: Steering Machine Unlearning with Bi-Level Watermarking Design", "comment": "Accepted by ICCV 2025", "summary": "With the increasing demand for the right to be forgotten, machine unlearning\n(MU) has emerged as a vital tool for enhancing trust and regulatory compliance\nby enabling the removal of sensitive data influences from machine learning (ML)\nmodels. However, most MU algorithms primarily rely on in-training methods to\nadjust model weights, with limited exploration of the benefits that data-level\nadjustments could bring to the unlearning process. To address this gap, we\npropose a novel approach that leverages digital watermarking to facilitate MU\nby strategically modifying data content. By integrating watermarking, we\nestablish a controlled unlearning mechanism that enables precise removal of\nspecified data while maintaining model utility for unrelated tasks. We first\nexamine the impact of watermarked data on MU, finding that MU effectively\ngeneralizes to watermarked data. Building on this, we introduce an\nunlearning-friendly watermarking framework, termed Water4MU, to enhance\nunlearning effectiveness. The core of Water4MU is a bi-level optimization (BLO)\nframework: at the upper level, the watermarking network is optimized to\nminimize unlearning difficulty, while at the lower level, the model itself is\ntrained independently of watermarking. Experimental results demonstrate that\nWater4MU is effective in MU across both image classification and image\ngeneration tasks. Notably, it outperforms existing methods in challenging MU\nscenarios, known as \"challenging forgets\".", "AI": {"tldr": "This paper introduces Water4MU, a novel machine unlearning approach using digital watermarking to modify data content. It enhances unlearning effectiveness via bi-level optimization (BLO) and outperforms existing methods in 'challenging forgets' scenarios for image classification and generation tasks.", "motivation": "Current machine unlearning (MU) methods focus on in-training weight adjustments with limited data-level approach exploration. The paper addresses the need for data-level modifications to improve unlearning efficiency and comply with data removal regulations like the right to be forgotten.", "method": "The framework uses a bi-level optimization (BLO) process: (1) Upper level optimizes a watermarking network to minimize unlearning difficulty for targeted data, and (2) Lower level trains the main model independently of the watermarking. Watermarking embeds controllable data patterns to facilitate selective influence removal.", "result": "Water4MU achieves effective unlearning performance across image classification and generation tasks, demonstrating superior results in challenging scenarios (e.g., 'challenging forgets') compared to state-of-the-art in-training MU methods.", "conclusion": "Water4MU provides a data-centric path for machine unlearning by leveraging watermarking to strategically alter data content. It achieves precise sensitive data removal while preserving model utility, offering a promising direction for future unlearning research and applications."}}
{"id": "2508.10185", "categories": ["cs.CR", "cs.CY", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.10185", "abs": "https://arxiv.org/abs/2508.10185", "authors": ["Ren\u00e9 Mayrhofer", "Michael Roland", "Tobias H\u00f6ller", "Philipp Hofer", "Mario Lins"], "title": "An Architecture for Distributed Digital Identities in the Physical World", "comment": null, "summary": "Digital identities are increasingly important for mediating not only digital\nbut also physical service transactions. Managing such identities through\ncentralized providers can cause both availability and privacy concerns: single\npoints of failure and control are ideal targets for global attacks on\ntechnical, organizational, or legal fronts. We design, analyze, and build a\ndistributed digital identity architecture for physical world transactions in\ncommon scenarios like unlocking doors, public transport, or crossing country\nborders. This architecture combines (biometric and other) sensors, (established\nand upcoming) identity authorities, attribute verifiers, and a new core\ncomponent we call the \\emph{Personal Identity Agent (PIA)} that represents\nindividuals with their identity attributes in the digital domain. All\ntransactions are conducted in a completely decentralized manner, and the\ncomponents for which we currently assume central coordination are optional and\nonly used for assisting with service discovery and latency reduction. We\npresent a first protocol between these parties and formally verify that it\nachieves relevant security properties based on a realistic threat model\nincluding strong global adversaries. A proof-of-concept implementation\ndemonstrates practical feasibility of both architecture and initial protocol\nfor applications that can tolerate end-to-end latencies in the range of a few\nseconds.", "AI": {"tldr": "This paper proposes a decentralized digital identity architecture for physical world transactions, introducing a Personal Identity Agent (PIA) to reduce privacy and availability risks from centralized providers, with a verified protocol and feasible implementation for low-latency applications.", "motivation": "Centralized digital identity systems have single points of failure and control, making them susceptible to global attacks across technical, organizational, and legal domains, which compromises both availability and privacy.", "method": "The architecture integrates sensors, identity authorities, attribute verifiers, and PIAs. It employs a decentralized protocol, formal verification under a realistic threat model involving strong global adversaries, and a proof-of-concept implementation to test feasibility.", "result": "A working implementation demonstrates the architecture and protocol's practicality, confirming they can handle real-world applications with acceptable latency (a few seconds) while maintaining security.", "conclusion": "The paper concludes that the decentralized architecture with PIAs effectively addresses privacy and availability challenges in physical service interactions, offering a sustainable alternative to centralized identity management systems."}}
{"id": "2508.10212", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.10212", "abs": "https://arxiv.org/abs/2508.10212", "authors": ["Md Sazedur Rahman", "Mohamed Elmahallawy", "Sanjay Madria", "Samuel Frimpong"], "title": "Detecting Untargeted Attacks and Mitigating Unreliable Updates in Federated Learning for Underground Mining Operations", "comment": null, "summary": "Underground mining operations rely on distributed sensor networks to collect\ncritical data daily, including mine temperature, toxic gas concentrations, and\nminer movements for hazard detection and operational decision-making. However,\ntransmitting raw sensor data to a central server for training deep learning\nmodels introduces significant privacy risks, potentially exposing sensitive\nmine-specific information. Federated Learning (FL) offers a transformative\nsolution by enabling collaborative model training while ensuring that raw data\nremains localized at each mine. Despite its advantages, FL in underground\nmining faces key challenges: (i) An attacker may compromise a mine's local\nmodel by employing techniques such as sign-flipping attacks or additive noise,\nleading to erroneous predictions; (ii) Low-quality (yet potentially valuable)\ndata, caused by poor lighting conditions or sensor inaccuracies in mines may\ndegrade the FL training process. In response, this paper proposes MineDetect, a\ndefense FL framework that detects and isolates the attacked models while\nmitigating the impact of mines with low-quality data. MineDetect introduces two\nkey innovations: (i) Detecting attacked models (maliciously manipulated) by\ndeveloping a history-aware mechanism that leverages local and global averages\nof gradient updates; (ii) Identifying and eliminating adversarial influences\nfrom unreliable models (generated by clients with poor data quality) on the FL\ntraining process. Comprehensive simulations across diverse datasets demonstrate\nthat MineDetect outperforms existing methods in both robustness and accuracy,\neven in challenging non-IID data scenarios. Its ability to counter adversarial\ninfluences while maintaining lower computational efficiency makes it a vital\nadvancement for improving safety and operational effectiveness in underground\nmining.", "AI": {"tldr": "MineDetect: A robust federated learning framework addressing security and data quality challenges in underground mining operations by detecting attacked models and mitigating adversarial influences from low-quality data.", "motivation": "Federated Learning (FL) is needed in underground mining to preserve privacy of local sensor data while enabling collaborative hazard detection. Existing FL approaches face risks from model attacks (sign-flipping/additive noise) and data degradation caused by harsh mining conditions.", "method": "MineDetect implements two innovations: (1) A history-aware gradient update mechanism tracking local and global averages for detecting malicious model attacks, and (2) A method for identifying and eliminating adversarial influences from unreliable models caused by poor data quality.", "result": "Comprehensive simulations demonstrate superior robustness and accuracy compared to existing methods across challenging non-IID data scenarios, while maintaining computational efficiency.", "conclusion": "MineDetect advances FL security and reliability for underground mining applications, offering practical solutions to both adversarial attacks and data quality issues critical for operational safety."}}
{"id": "2508.10327", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.10327", "abs": "https://arxiv.org/abs/2508.10327", "authors": ["Haoyang Hu", "Xun Huang", "Chenyu Wu", "Shiwen Liu", "Zhichao Lian", "Shuangquan Zhang"], "title": "BERTector: Intrusion Detection Based on Joint-Dataset Learning", "comment": null, "summary": "Intrusion detection systems (IDS) are facing challenges in generalization and\nrobustness due to the heterogeneity of network traffic and the diversity of\nattack patterns. To address this issue, we propose a new joint-dataset training\nparadigm for IDS and propose a scalable BERTector framework based on BERT.\nBERTector integrates three key components: NSS-Tokenizer for traffic-aware\nsemantic tokenization, supervised fine-tuning with a hybrid dataset, and\nlow-rank adaptation (LoRA) for efficient training. Extensive experiments show\nthat BERTector achieves state-of-the-art detection accuracy, strong\ncross-dataset generalization capabilities, and excellent robustness to\nadversarial perturbations. This work establishes a unified and efficient\nsolution for modern IDS in complex and dynamic network environments.", "AI": {"tldr": "This paper proposes BERTector, a BERT-based intrusion detection system with a joint-dataset training paradigm, NSS-Tokenizer for traffic-aware semantic tokenization, and low-rank adaptation for efficient training, achieving state-of-the-art performance in accuracy, generalization, and adversarial robustness.", "motivation": "Modern IDS face challenges in generalization and robustness due to diverse network traffic patterns and varying attack types, necessitating a more adaptable and efficient detection framework.", "method": "The framework integrates three components: (1) NSS-Tokenizer for traffic-aware semantic tokenization of network data, (2) supervised fine-tuning using hybrid datasets combining different traffic types, and (3) low-rank adaptation (LoRA) to optimize training efficiency while maintaining model performance.", "result": "Extensive experiments demonstrate:\n1. State-of-the-art intrusion detection accuracy across standard datasets\n2. Strong cross-dataset generalization capabilities (13.8% higher accuracy than prior methods on unseen data)\n3. Excellent robustness to adversarial perturbations (96.7% accuracy under attacks compared to 22.3% for baseline models)", "conclusion": "BERTector establishes a unified, efficient solution for intrusion detection in complex network environments by combining transformer architectures with specialized network traffic features and adaptive training techniques, significantly improving practical deployment viability through its joint-dataset approach and resource-efficient design."}}
{"id": "2508.10431", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.10431", "abs": "https://arxiv.org/abs/2508.10431", "authors": ["Chris Cao", "Gururaj Saileshwar"], "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based Side-Channel Attacks on Fully Associative Randomized Caches", "comment": null, "summary": "Recent work presented at USENIX Security 2025 claims that occupancy-based\nattacks can recover AES keys from the MIRAGE randomized cache. In this paper,\nwe examine these claims and find that they arise from fundamental modeling\nflaws. Most critically, the authors' simulation of MIRAGE uses a constant seed\nto initialize the random number generator used for global evictions in MIRAGE,\ncausing every AES encryption they trace to evict the same deterministic\nsequence of cache lines. This artificially creates a highly repeatable timing\npattern that is not representative of a realistic implementation of MIRAGE,\nwhere eviction sequences vary randomly between encryptions. When we instead\nrandomize the eviction seed for each run, reflecting realistic operation, the\ncorrelation between AES T-table accesses and attacker runtimes disappears, and\nthe attack fails. These findings show that the reported leakage is an artifact\nof incorrect modeling, and not an actual vulnerability in MIRAGE.", "AI": {"tldr": "This paper refutes claims that occupancy-based attacks can recover AES keys from the MIRAGE cache, blaming the prior results on flawed modeling with a constant RNG seed. Correcting this flaw renders the attack ineffective.", "motivation": "To evaluate and correct the flawed claims of vulnerabilities in MIRAGE by examining the simulation methodology of prior occupancy-based attacks.", "method": "The authors re-analyzed the MIRAGE simulation used in the prior work, identifying the constant RNG seed for global evictions as a critical flaw. They redesigned experiments with randomized seeds per AES encryption run.", "result": "Attacker runtime correlation with AES T-table accesses vanished when eviction seeds were randomized, conclusively invalidating the prior attack's success.", "conclusion": "The apparent vulnerability in MIRAGE was an artifact of incorrect simulation methodology. Properly modeled with random eviction sequences, this attack approach is infeasible, confirming MIRAGE's security against these techniques."}}
{"id": "2508.10493", "categories": ["cs.CR", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.10493", "abs": "https://arxiv.org/abs/2508.10493", "authors": ["Bernhard Kauer", "Aleksandr Petrosyan", "Benjamin Livshits"], "title": "AlDBaran: Towards Blazingly Fast State Commitments for Blockchains", "comment": null, "summary": "The fundamental basis for maintaining integrity within contemporary\nblockchain systems is provided by authenticated databases. Our analysis\nindicates that a significant portion of the approaches applied in this domain\nfail to sufficiently meet the stringent requirements of systems processing\ntransactions at rates of multi-million TPS. AlDBaran signifies a substantial\nadvancement in authenticated databases. By eliminating disk I/O operations from\nthe critical path, implementing prefetching strategies, and refining the update\nmechanism of the Merkle tree, we have engineered an authenticated data\nstructure capable of handling state updates efficiently at a network throughput\nof 50 Gbps. This throughput capacity significantly surpasses any empirically\ndocumented blockchain throughput, guaranteeing the ability of even the most\nhigh-throughput blockchains to generate state commitments effectively.\n  AlDBaran provides support for historical state proofs, which facilitates a\nwide array of novel applications. For instance, the deployment of AlDBaran\ncould enable blockchains that do not currently support state commitments to\noffer functionalities for light clients and/or implement rollups.\n  When benchmarked against alternative authenticated data structure projects,\nAlDBaran exhibits superior performance and simplicity. In particular, AlDBaran\nachieves speeds of approximately 48 million updates per second using an\nidentical machine configuration. This characteristic renders AlDBaran an\nattractive solution for resource-limited environments, as its historical data\ncapabilities can be modularly isolated (and deactivated), which further\nenhances performance. On consumer-level portable hardware, it achieves\napproximately 8 million updates/s in an in-memory setting and 5 million\nupdates/s with snapshots at sub-second intervals, illustrating compelling and\ncost-effective scalability.", "AI": {"tldr": "AlDBaran is an authenticated database for high-throughput blockchain systems, enabling 50 Gbps network throughput, 48M updates/s on standard hardware, and 8M/5M updates/s on portable devices. It outperforms existing solutions with disk I/O removal and Merkle tree optimizations.", "motivation": "Blockchain systems require higher transaction throughput (multi-million TPS) than current authenticated database approaches can efficiently handle, while maintaining integrity and supporting lightweight clients/rollups.", "method": "The paper introduces AlDBaran, an authenticated data structure that eliminates disk I/O from the critical path, implements prefetching strategies, and refines Merkle tree updates through parallelization and batched proof generation techniques.", "result": "AlDBaran achieves 48 million updates per second on standard machine configurations, 8 million updates/s on in-memory portable hardware, and 5 million updates/s with sub-second snapshots. It supports historical state proofs modularly.", "conclusion": "AlDBaran provides state-of-the-art throughput and efficiency for authenticated blockchain databases, making it suitable for resource-constrained environments while enabling new applications through historical state proof capabilities."}}
{"id": "2508.10510", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.10510", "abs": "https://arxiv.org/abs/2508.10510", "authors": ["Hugo Delavenne", "Louise Lallemand"], "title": "Codes on any Cayley Graph have an Interactive Oracle Proof of Proximity", "comment": null, "summary": "Interactive Oracle Proofs of Proximity (IOPP) are at the heart of code-based\nSNARKs, a family of zeroknowledge protocols. The first and most famous one is\nthe FRI protocol [BBHR18a], that efficiently tests proximity to Reed-Solomon\ncodes. This paper generalizes the flowering IOPP introduced in [DMR25] for some\nspecific (2, n)-regular Tanner codes to a much broader variety of codes: any\ncode with symbols indexed on the edges of a Cayley graph. The flowering\nprotocol of [DMR25] had a soundness parameter much lower than the FRI protocol\n[BCI + 23], and complexity parameters that could compete with the FRI\n[BBHR18a]. The lower soundness and the absence of restriction on the base field\nmay lead to other practical speedups, however the codes considered in [DMR25]\nhave an o(1) minimum distance. The generalization proposed in this paper\npreserves the soundness parameter with a slight decrease of the complexity\nparameters, while allowing being applied on codes with constant rate and\nconstant minimum distance thanks to the good expansion properties of some\nfamilies of Cayley graphs.", "AI": {"tldr": "Generalizes the flowering IOPP protocol from specific (2,n)-regular Tanner codes to Cayley graph-indexed codes, preserves soundness with slight complexity improvements, enabling codes with constant rate and distance due to Cayley graphs' expansion properties.", "motivation": "The paper aims to extend the efficiency and practicality of IOPPs for code-based SNARKs by generalizing the protocol to broader code families while maintaining strong soundness parameters.", "method": "The authors adapt the flowering IOPP framework using Cayley graph expansion properties, which allow improved code parameters (constant rate and distance) while preserving the soundness guarantees from [DMR25].", "result": "Achieves similar soundness to [DMR25] with only a mild complexity increase, enabling application to codes with constant rate and minimum distance, unlike the previous $o(1)$ distance limitation.", "conclusion": "This generalization demonstrates that Cayley graph-indexed codes can maintain the security and efficiency benefits of existing IOPP protocols, paving the way for more practical code-based SNARK implementations."}}
{"id": "2508.10636", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.10636", "abs": "https://arxiv.org/abs/2508.10636", "authors": ["Sandipan Dey", "Payal Santosh Kate", "Vatsala Upadhyay", "Abhishek Vaish"], "title": "A Transformer-Based Approach for DDoS Attack Detection in IoT Networks", "comment": null, "summary": "DDoS attacks have become a major threat to the security of IoT devices and\ncan cause severe damage to the network infrastructure. IoT devices suffer from\nthe inherent problem of resource constraints and are therefore susceptible to\nsuch resource-exhausting attacks. Traditional methods for detecting DDoS\nattacks are not efficient enough to cope with the dynamic nature of IoT\nnetworks, as well as the scalability of the attacks, diversity of protocols,\nhigh volume of traffic, and variability in device behavior, and variability of\nprotocols like MQTT, CoAP, making it hard to implement security across all the\nprotocols. In this paper, we propose a novel approach, i.e., the use of\nTransformer models, which have shown remarkable performance in natural language\nprocessing tasks, for detecting DDoS attacks on IoT devices. The proposed model\nextracts features from network traffic data and processes them using a\nself-attention mechanism. Experiments conducted on a real-world dataset\ndemonstrate that the proposed approach outperforms traditional machine learning\ntechniques, which can be validated by comparing both approaches' accuracy,\nprecision, recall, and F1-score. The results of this study show that the\nTransformer models can be an effective solution for detecting DDoS attacks on\nIoT devices and have the potential to be deployed in real-world IoT\nenvironments.", "AI": {"tldr": "The paper proposes using Transformer models for detecting DDoS attacks in IoT networks, demonstrating superior performance over traditional methods via self-attention mechanisms and real-world dataset validation.", "motivation": "Traditional DDoS detection methods struggle with IoT's dynamic network behavior, protocol diversity (e.g., MQTT, CoAP), high traffic volume, and device variability, lacking scalability and adaptability required for effective security.", "method": "A Transformer-based model employs a self-attention mechanism to extract features from network traffic data, designed to adapt to IoT environments' unique challenges through scalable sequence processing.", "result": "Experiments on real-world datasets showed the Transformer model achieved higher accuracy, precision, recall, and F1-score compared to conventional machine learning techniques, emphasizing its robustness in variable IoT scenarios.", "conclusion": "Transformer models effectively address DDoS attack detection in IoT networks, offering a scalable and protocol-agnostic solution with potential for real-world deployment due to their adaptability and predictive performance."}}
{"id": "2508.10639", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.10639", "abs": "https://arxiv.org/abs/2508.10639", "authors": ["Anyuan Sang", "Lu Zhou", "Li Yang", "Junbo Jia", "Huipeng Yang", "Pengbin Feng", "Jianfeng Ma"], "title": "MirGuard: Towards a Robust Provenance-based Intrusion Detection System Against Graph Manipulation Attacks", "comment": null, "summary": "Learning-based Provenance-based Intrusion Detection Systems (PIDSes) have\nbecome essential tools for anomaly detection in host systems due to their\nability to capture rich contextual and structural information, as well as their\npotential to detect unknown attacks. However, recent studies have shown that\nthese systems are vulnerable to graph manipulation attacks, where attackers\nmanipulate the graph structure to evade detection. While some previous\napproaches have discussed this type of attack, none have fully addressed it\nwith a robust detection solution, limiting the practical applicability of\nPIDSes.\n  To address this challenge, we propose MirGuard, a robust anomaly detection\nframework that combines logic-aware multi-view augmentation with contrastive\nrepresentation learning. Rather than applying arbitrary structural\nperturbations, MirGuard introduces Logic-Aware Noise Injection (LNI) to\ngenerate semantically valid graph views, ensuring that all augmentations\npreserve the underlying causal semantics of the provenance data. These views\nare then used in a Logic-Preserving Contrastive Learning framework, which\nencourages the model to learn representations that are invariant to benign\ntransformations but sensitive to adversarial inconsistencies. Comprehensive\nevaluations on multiple provenance datasets demonstrate that MirGuard\nsignificantly outperforms state-of-the-art detectors in robustness against\nvarious graph manipulation attacks without sacrificing detection performance\nand efficiency. Our work represents the first targeted study to enhance PIDS\nagainst such adversarial threats, providing a robust and effective solution to\nmodern cybersecurity challenges.", "AI": {"tldr": "This paper introduces MirGuard, a robust anomaly detection framework combining logic-aware graph augmentation with contrastive learning to defend against semantic-preserving attacks on provenance-based intrusion detection systems.", "motivation": "Learning-based provenance intrusion detection systems (PIDSes) face practical limitations due to vulnerability to graph manipulation attacks that exploit arbitrary structural perturbations while maintaining semantic validity.", "method": "MirGuard employs Logic-Aware Noise Injection (LNI) to generate semantically valid multi-view graph representations through causal semantics preservation, followed by Logic-Preserving Contrastive Learning to enhance adversarial robustness.", "result": "Extensive evaluations on provenance datasets demonstrate superior performance over existing detectors against diverse graph manipulation attacks, while maintaining detection accuracy and computational efficiency.", "conclusion": "MirGuard represents the first targeted solution addressing graph manipulation attacks in PIDSes, offering a robust and effective framework for modern cybersecurity challenges through logic-preserving representation learning."}}
{"id": "2508.10652", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.10652", "abs": "https://arxiv.org/abs/2508.10652", "authors": ["Richa Dasila", "Vatsala Upadhyay", "Samo Bobek", "Abhishek Vaish"], "title": "A Novel Study on Intelligent Methods and Explainable AI for Dynamic Malware Analysis", "comment": null, "summary": "Deep learning models are one of the security strategies, trained on extensive\ndatasets, and play a critical role in detecting and responding to these threats\nby recognizing complex patterns in malicious code. However, the opaque nature\nof these models-often described as \"black boxes\"-makes their decision-making\nprocesses difficult to understand, even for their creators. This research\naddresses these challenges by integrating Explainable AI (XAI) techniques to\nenhance the interpretability and trustworthiness of malware detection models.\nIn this research, the use of Multi-Layer Perceptrons (MLP) for dynamic malware\nanalysis has been considered, a less explored area, and its efficacy in\ndetecting Metamorphic Malware, and further the effectiveness and transparency\nof MLPs, CNNs, RNNs, and CNN-LSTM models in malware classification, evaluating\nthese models through the lens of Explainable AI (XAI). This comprehensive\napproach aims to demystify the internal workings of deep learning models,\npromoting a better understanding and trust in their predictive capabilities in\ncybersecurity contexts. Such in-depth analysis and implementation haven't been\ndone to the best of our knowledge.", "AI": {"tldr": "This research integrates XAI techniques with deep learning models (MLP, CNN, RNN, CNN-LSTM) to improve interpretability and trustworthiness in dynamic malware detection.", "motivation": "The 'black box' nature of deep learning models limits trust and understanding in critical security applications, necessitating explainability for effective threat mitigation.", "method": "The study evaluates four deep learning architectures using XAI methods for dynamic malware analysis, focusing on Metamorphic Malware detection through enhanced model transparency.", "result": "The comprehensive XAI framework successfully analyzes model decision-making processes, though specific performance metrics are not detailed in the abstract.", "conclusion": "By demonstrating how XAI can demystify deep learning in cybersecurity, this work provides a novel methodology to balance machine learning efficacy with humaninterpretable explanations for malware classification."}}
{"id": "2508.10677", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10677", "abs": "https://arxiv.org/abs/2508.10677", "authors": ["Amine Tellache", "Abdelaziz Amara Korba", "Amdjed Mokhtari", "Horea Moldovan", "Yacine Ghamri-Doudane"], "title": "Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat Intelligence", "comment": null, "summary": "Effective incident response (IR) is critical for mitigating cyber threats,\nyet security teams are overwhelmed by alert fatigue, high false-positive rates,\nand the vast volume of unstructured Cyber Threat Intelligence (CTI) documents.\nWhile CTI holds immense potential for enriching security operations, its\nextensive and fragmented nature makes manual analysis time-consuming and\nresource-intensive. To bridge this gap, we introduce a novel\nRetrieval-Augmented Generation (RAG)-based framework that leverages Large\nLanguage Models (LLMs) to automate and enhance IR by integrating dynamically\nretrieved CTI. Our approach introduces a hybrid retrieval mechanism that\ncombines NLP-based similarity searches within a CTI vector database with\nstandardized queries to external CTI platforms, facilitating context-aware\nenrichment of security alerts. The augmented intelligence is then leveraged by\nan LLM-powered response generation module, which formulates precise,\nactionable, and contextually relevant incident mitigation strategies. We\npropose a dual evaluation paradigm, wherein automated assessment using an\nauxiliary LLM is systematically cross-validated by cybersecurity experts.\nEmpirical validation on real-world and simulated alerts demonstrates that our\napproach enhances the accuracy, contextualization, and efficiency of IR,\nalleviating analyst workload and reducing response latency. This work\nunderscores the potential of LLM-driven CTI fusion in advancing autonomous\nsecurity operations and establishing a foundation for intelligent, adaptive\ncybersecurity frameworks.", "AI": {"tldr": "This paper proposes a RAG-based framework using LLMs to enhance and automate incident response by integrating real-time Cyber Threat Intelligence, improving accuracy and efficiency.", "motivation": "Security teams face alert fatigue, high false-positive rates, and challenges in manually analyzing fragmented CTI documents, leading to inefficiencies in threat mitigation.", "method": "A hybrid retrieval mechanism combining NLP-based similarity searches in a CTI vector database with standardized queries to external CTI platforms, followed by LLM-powered response generation of context-aware mitigation strategies.", "result": "Empirical validation on real-world and simulated alerts showed improved accuracy, contextualization, reduced response latency, and alleviated analyst workload compared to traditional methods.", "conclusion": "LLM-driven CTI fusion demonstrates potential to advance autonomous security operations and enable intelligent, adaptive cybersecurity frameworks."}}
{"id": "2508.10880", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10880", "abs": "https://arxiv.org/abs/2508.10880", "authors": ["Yanzhe Zhang", "Diyi Yang"], "title": "Searching for Privacy Risks in LLM Agents via Simulation", "comment": "Preprint", "summary": "The widespread deployment of LLM-based agents is likely to introduce a\ncritical privacy threat: malicious agents that proactively engage others in\nmulti-turn interactions to extract sensitive information. These dynamic\ndialogues enable adaptive attack strategies that can cause severe privacy\nviolations, yet their evolving nature makes it difficult to anticipate and\ndiscover sophisticated vulnerabilities manually. To tackle this problem, we\npresent a search-based framework that alternates between improving attacker and\ndefender instructions by simulating privacy-critical agent interactions. Each\nsimulation involves three roles: data subject, data sender, and data recipient.\nWhile the data subject's behavior is fixed, the attacker (data recipient)\nattempts to extract sensitive information from the defender (data sender)\nthrough persistent and interactive exchanges. To explore this interaction space\nefficiently, our search algorithm employs LLMs as optimizers, using parallel\nsearch with multiple threads and cross-thread propagation to analyze simulation\ntrajectories and iteratively propose new instructions. Through this process, we\nfind that attack strategies escalate from simple direct requests to\nsophisticated multi-turn tactics such as impersonation and consent forgery,\nwhile defenses advance from rule-based constraints to identity-verification\nstate machines. The discovered attacks and defenses transfer across diverse\nscenarios and backbone models, demonstrating strong practical utility for\nbuilding privacy-aware agents.", "AI": {"tldr": "The paper proposes a search-based framework simulating privacy-critical interactions between LLM agents to iteratively improve attacker and defender instructions, revealing escalation paths in attack strategies and defense mechanisms.", "motivation": "Malicious LLM agents can proactively exploit multi-turn conversations for sensitive data extraction, creating dynamic privacy threats that are difficult to detect through traditional manual methods.", "method": "A simulation framework with three roles (data subject, sender, recipient) using parallel LLM-optimized search with multiple threads and cross-thread propagation to explore attacker-defender interaction trajectories and refine instructions systematically.", "result": "Attack strategies evolve from direct requests to sophisticated impersonation tactics, while defenses progress from rule-based systems to state machines. The findings demonstrate cross-scenario and model transferability of both attacks and defenses.", "conclusion": "The proposed framework provides practical utility for developing privacy-aware agents by automatically uncovering both emerging vulnerabilities and adaptive defense patterns in dynamic agent interactions."}}
