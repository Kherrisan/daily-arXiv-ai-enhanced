{"id": "2507.07210", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07210", "abs": "https://arxiv.org/abs/2507.07210", "authors": ["Nils Rollshausen", "Alexander Heinrich", "Matthias Hollick", "Jiska Classen"], "title": "WatchWitch: Interoperability, Privacy, and Autonomy for the Apple Watch", "comment": "To appear in \"Proceedings on Privacy Enhancing Technologies\"", "summary": "Smartwatches such as the Apple Watch collect vast amounts of intimate health\nand fitness data as we wear them. Users have little choice regarding how this\ndata is processed: The Apple Watch can only be used with Apple's iPhones, using\ntheir software and their cloud services. We are the first to publicly\nreverse-engineer the watch's wireless protocols, which led to discovering\nmultiple security issues in Apple's proprietary implementation. With\nWatchWitch, our custom Android reimplementation, we break out of Apple's walled\ngarden -- demonstrating practical interoperability with enhanced privacy\ncontrols and data autonomy. We thus pave the way for more consumer choice in\nthe smartwatch ecosystem, offering users more control over their devices.", "AI": {"tldr": "This paper explores the security vulnerabilities in Apple Watch's proprietary system and introduces WatchWitch, an Android-based alternative, to enhance user privacy and interoperability in the smartwatch market.", "motivation": "Apple Watch's restricted ecosystem limits user control over health/fitness data and locks users into Apple's proprietary software/cloud services, creating privacy and choice concerns.", "method": "Public reverse-engineering of Apple Watch's wireless protocols to identify security flaws, followed by development of WatchWitch - a privacy-focused Android reimplementation with interoperability.", "result": "1) Multiple security issues discovered in Apple's implementation  2) Created functional Android reimplementation (WatchWitch) 3) Demonstrated practical interoperability 4) Achieved enhanced privacy controls", "conclusion": "The research establishes precedent for breaking proprietary smartwatch ecosystems while maintaining functionality, enabling users to prioritize privacy through choice in platform and data handling."}}
{"id": "2507.07244", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07244", "abs": "https://arxiv.org/abs/2507.07244", "authors": ["Faissal Ahmadou", "Sepehr Ghaffarzadegan", "Boubakr Nour", "Makan Pourzandi", "Mourad Debbabi", "Chadi Assi"], "title": "Automated Attack Testflow Extraction from Cyber Threat Report using BERT for Contextual Analysis", "comment": null, "summary": "In the ever-evolving landscape of cybersecurity, the rapid identification and\nmitigation of Advanced Persistent Threats (APTs) is crucial. Security\npractitioners rely on detailed threat reports to understand the tactics,\ntechniques, and procedures (TTPs) employed by attackers. However, manually\nextracting attack testflows from these reports requires elusive knowledge and\nis time-consuming and prone to errors. This paper proposes FLOWGUARDIAN, a\nnovel solution leveraging language models (i.e., BERT) and Natural Language\nProcessing (NLP) techniques to automate the extraction of attack testflows from\nunstructured threat reports. FLOWGUARDIAN systematically analyzes and\ncontextualizes security events, reconstructs attack sequences, and then\ngenerates comprehensive testflows. This automated approach not only saves time\nand reduces human error but also ensures comprehensive coverage and robustness\nin cybersecurity testing. Empirical validation using public threat reports\ndemonstrates FLOWGUARDIAN's accuracy and efficiency, significantly enhancing\nthe capabilities of security teams in proactive threat hunting and incident\nresponse.", "AI": {"tldr": "FLOWGUARDIAN automates extraction of attack testflows from unstructured threat reports using BERT and NLP, enabling efficient and accurate cybersecurity threat analysis and incident response.", "motivation": "Manual extraction of attack testflows from threat reports is time-consuming, error-prone, and requires specialized knowledge, limiting the efficiency of cybersecurity testing and response.", "method": "FLOWGUARDIAN leverages BERT language models and NLP techniques to systematically analyze security event context, reconstruct attack sequences, and generate structured testflows from unstructured reports.", "result": "Empirical validation shows FLOWGUARDIAN achieves high accuracy and efficiency in testflow extraction, demonstrably improving security teams' capabilities in proactive threat hunting and incident response.", "conclusion": "Automated testflow extraction via FLOWGUARDIAN reduces manual effort, minimizes errors, and enhances comprehensive coverage in cybersecurity testing, with promising practical applications for security operations."}}
{"id": "2507.07246", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07246", "abs": "https://arxiv.org/abs/2507.07246", "authors": ["Peicheng Wang", "Monika Santra", "Mingyu Liu", "Cong Sun", "Dongrui Zeng", "Gang Tan"], "title": "Disa: Accurate Learning-based Static Disassembly with Attentions", "comment": "To appear at ACM CCS 2025", "summary": "For reverse engineering related security domains, such as vulnerability\ndetection, malware analysis, and binary hardening, disassembly is crucial yet\nchallenging. The fundamental challenge of disassembly is to identify\ninstruction and function boundaries. Classic approaches rely on file-format\nassumptions and architecture-specific heuristics to guess the boundaries,\nresulting in incomplete and incorrect disassembly, especially when the binary\nis obfuscated. Recent advancements of disassembly have demonstrated that deep\nlearning can improve both the accuracy and efficiency of disassembly. In this\npaper, we propose Disa, a new learning-based disassembly approach that uses the\ninformation of superset instructions over the multi-head self-attention to\nlearn the instructions' correlations, thus being able to infer function\nentry-points and instruction boundaries. Disa can further identify instructions\nrelevant to memory block boundaries to facilitate an advanced block-memory\nmodel based value-set analysis for an accurate control flow graph (CFG)\ngeneration. Our experiments show that Disa outperforms prior deep-learning\ndisassembly approaches in function entry-point identification, especially\nachieving 9.1% and 13.2% F1-score improvement on binaries respectively\nobfuscated by the disassembly desynchronization technique and popular\nsource-level obfuscator. By achieving an 18.5% improvement in the memory block\nprecision, Disa generates more accurate CFGs with a 4.4% reduction in Average\nIndirect Call Targets (AICT) compared with the state-of-the-art heuristic-based\napproach.", "AI": {"tldr": "This paper proposes Disa, a deep learning-based disassembly method that improves function entry-point identification and control flow graph accuracy by leveraging superset instructions with multi-head self-attention, outperforming prior approaches on obfuscated binaries.", "motivation": "Traditional disassembly methods using file-format assumptions and architecture-specific heuristics fail to handle obfuscated binaries, leading to incomplete/incorrect instruction boundary identification crucial for vulnerability detection, malware analysis, and binary hardening.", "method": "Disa uses multi-head self-attention with superset instruction information to learn instruction correlations, identifying function entry-points and instruction boundaries. It incorporates memory block boundary detection to enhance control flow graph accuracy through advanced value-set analysis.", "result": "Disa achieves 9.1% and 13.2% F1-score improvements on binaries obfuscated by desynchronizing techniques and source-level obfuscators respectively. It improves memory block precision by 18.5%, reducing Average Indirect Call Targets (AICT) by 4.4% compared to heuristic-based approaches.", "conclusion": "Deep learning-based Disa effectively addresses disassembly challenges in obfuscated binaries, outperforming both prior deep learning and state-of-the-art heuristic methods in function boundary identification and control flow graph generation precision."}}
{"id": "2507.07250", "categories": ["cs.CR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.07250", "abs": "https://arxiv.org/abs/2507.07250", "authors": ["Jordi Serra-Ruiz", "David Meg\u00edas"], "title": "Semi-fragile watermarking of remote sensing images using DWT, vector quantization and automatic tiling", "comment": null, "summary": "A semi-fragile watermarking scheme for multiple band images is presented in\nthis article. We propose to embed a mark into remote sensing images applying a\ntree-structured vector quantization approach to the pixel signatures instead of\nprocessing each band separately. The signature of the multispectral or\nhyperspectral image is used to embed the mark in it order to detect any\nsignificant modification of the original image. The image is segmented into\nthree-dimensional blocks, and a tree-structured vector quantizer is built for\neach block. These trees are manipulated using an iterative algorithm until the\nresulting block satisfies a required criterion, which establishes the embedded\nmark. The method is shown to be able to preserve the mark under lossy\ncompression (above a given threshold) but, at the same time, it detects\npossibly forged blocks and their position in the whole image.", "AI": {"tldr": "This paper proposes a semi-fragile watermarking scheme for multi-band images using a tree-structured vector quantization approach to embed marks into pixel signatures, enabling detection of significant modifications with lossy compression robustness.", "motivation": "Traditional separation-based processing of multi-band remote sensing images may inadequately address integrity verification needs, motivated by the demand for watermarking schemes that can distinguish between acceptable compressions and malicious forgeries while leveraging image signature characteristics.", "method": "The method involves 1) segmenting images into 3D blocks containing all bands, 2) constructing tree-structured vector quantizers for each block using pixel signatures, and 3) applying an iterative algorithm to manipulate these trees until watermark embedding criteria are met while maintaining signature correlations.", "result": "Experimental results demonstrate the scheme successfully preserves watermarks under lossy compression (above a defined threshold) and simultaneously detects forged blocks by identifying non-compliant signature deviations in multi-band images.", "conclusion": "The proposed semi-fragile watermarking approach for multi-band images enables effective authentication while accommodating natural compressive losses, with potential applications in remote sensing data integrity verification where multi-spectral/hyper-spectral analysis requires robust yet forgery-sensitive security mechanisms."}}
{"id": "2507.07325", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07325", "abs": "https://arxiv.org/abs/2507.07325", "authors": ["Martin Obaidi", "Marc Herrmann", "Elisa Schmid", "Raymond Ochsner", "Kurt Schneider", "Jil Kl\u00fcnder"], "title": "A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering", "comment": "This paper has been accepted at the 33rd IEEE International\n  Requirements Engineering Workshop (REW 2025)", "summary": "Sentiment analysis is an essential technique for investigating the emotional\nclimate within developer teams, contributing to both team productivity and\nproject success. Existing sentiment analysis tools in software engineering\nprimarily rely on English or non-German gold-standard datasets. To address this\ngap, our work introduces a German dataset of 5,949 unique developer statements,\nextracted from the German developer forum Android-Hilfe.de. Each statement was\nannotated with one of six basic emotions, based on the emotion model by Shaver\net al., by four German-speaking computer science students. Evaluation of the\nannotation process showed high interrater agreement and reliability. These\nresults indicate that the dataset is sufficiently valid and robust to support\nsentiment analysis in the German-speaking software engineering community.\nEvaluation with existing German sentiment analysis tools confirms the lack of\ndomain-specific solutions for software engineering. We also discuss approaches\nto optimize annotation and present further use cases for the dataset.", "AI": {"tldr": "This paper introduces a German dataset of 5,949 developer statements annotated with six basic emotions to address the lack of domain-specific sentiment analysis tools in German software engineering contexts. The dataset shows high reliability and is intended to support German-speaking communities.", "motivation": "Current sentiment analysis tools in software engineering rely on English or non-German gold-standard datasets, leaving a gap for German-speaking developer communities. This limits the availability of accurate, context-specific emotional analysis in German contexts.", "method": "The dataset was collected from the German developer forum Android-Hilfe.de. Developer statements were annotated with six basic emotions (Shaver et al.'s model) by four German-speaking computer science students. Inter-rater agreement and reliability were evaluated to validate the dataset.", "result": "The German dataset exhibited high inter-rater agreement and reliability, confirming its validity. Evaluations revealed that existing German sentiment analysis tools lack domain specificity for software engineering.", "conclusion": "The presented dataset provides a reliable foundation for sentiment analysis in German software engineering contexts. It highlights the need for domain-specific tools and proposes approaches to optimize annotation processes along with additional use cases."}}
{"id": "2507.07258", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07258", "abs": "https://arxiv.org/abs/2507.07258", "authors": ["Rami Darwish", "Mahmoud Abdelsalam", "Sajad Khorsandroo", "Kaushik Roy"], "title": "FedP3E: Privacy-Preserving Prototype Exchange for Non-IID IoT Malware Detection in Cross-Silo Federated Learning", "comment": null, "summary": "As IoT ecosystems continue to expand across critical sectors, they have\nbecome prominent targets for increasingly sophisticated and large-scale malware\nattacks. The evolving threat landscape, combined with the sensitive nature of\nIoT-generated data, demands detection frameworks that are both\nprivacy-preserving and resilient to data heterogeneity. Federated Learning (FL)\noffers a promising solution by enabling decentralized model training without\nexposing raw data. However, standard FL algorithms such as FedAvg and FedProx\noften fall short in real-world deployments characterized by class imbalance and\nnon-IID data distributions -- particularly in the presence of rare or disjoint\nmalware classes. To address these challenges, we propose FedP3E\n(Privacy-Preserving Prototype Exchange), a novel FL framework that supports\nindirect cross-client representation sharing while maintaining data privacy.\nEach client constructs class-wise prototypes using Gaussian Mixture Models\n(GMMs), perturbs them with Gaussian noise, and transmits only these compact\nsummaries to the server. The aggregated prototypes are then distributed back to\nclients and integrated into local training, supported by SMOTE-based\naugmentation to enhance representation of minority malware classes. Rather than\nrelying solely on parameter averaging, our prototype-driven mechanism enables\nclients to enrich their local models with complementary structural patterns\nobserved across the federation -- without exchanging raw data or gradients.\nThis targeted strategy reduces the adverse impact of statistical heterogeneity\nwith minimal communication overhead. We evaluate FedP3E on the N-BaIoT dataset\nunder realistic cross-silo scenarios with varying degrees of data imbalance.", "AI": {"tldr": "The paper introduces FedP3E, a privacy-preserving federated learning framework for IoT malware detection, addressing class imbalance and non-IID data challenges through prototype-based representation sharing and SMOTE augmentation.", "motivation": "IoT ecosystems face sophisticated malware attacks, requiring privacy-preserving frameworks robust to data heterogeneity. Standard FL methods like FedAvg/FedProx fail in real-world scenarios with class imbalance and disjoint malware classes.", "method": "FedP3E uses Gaussian Mixture Models to generate class-wise prototypes, adds Gaussian noise to them for privacy, shares compact summaries via aggregation, and integrates SMOTE-based data augmentation to improve training on imbalanced and non-IID client data.", "result": "FedP3E is evaluated on N-BaIoT datasets with cross-silo scenarios and varying data imbalances, demonstrating its effectiveness with reduced communication overhead.", "conclusion": "FedP3E outperforms existing FL methods in IoT malware detection under non-IID and imbalanced data conditions, achieving privacy-preservation and robustness through indirect representation sharing and structural pattern enrichment."}}
{"id": "2507.07344", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07344", "abs": "https://arxiv.org/abs/2507.07344", "authors": ["Martin Obaidi", "Jannik Fischbach", "Jakob Droste", "Hannah Deters", "Marc Herrmann", "Jil Kl\u00fcnder", "Steffen Kr\u00e4tzig", "Hugo Villamizar", "Kurt Schneider"], "title": "Automatic Generation of Explainability Requirements and Software Explanations From User Reviews", "comment": "This paper has been accepted at the 33rd IEEE International\n  Requirements Engineering Workshop (REW 2025)", "summary": "Explainability has become a crucial non-functional requirement to enhance\ntransparency, build user trust, and ensure regulatory compliance. However,\ntranslating explanation needs expressed in user feedback into structured\nrequirements and corresponding explanations remains challenging. While existing\nmethods can identify explanation-related concerns in user reviews, there is no\nestablished approach for systematically deriving requirements and generating\naligned explanations. To contribute toward addressing this gap, we introduce a\ntool-supported approach that automates this process. To evaluate its\neffectiveness, we collaborated with an industrial automation manufacturer to\ncreate a dataset of 58 user reviews, each annotated with manually crafted\nexplainability requirements and explanations. Our evaluation shows that while\nAI-generated requirements often lack relevance and correctness compared to\nhuman-created ones, the AI-generated explanations are frequently preferred for\ntheir clarity and style. Nonetheless, correctness remains an issue,\nhighlighting the importance of human validation. This work contributes to the\nadvancement of explainability requirements in software systems by (1)\nintroducing an automated approach to derive requirements from user reviews and\ngenerate corresponding explanations, (2) providing empirical insights into the\nstrengths and limitations of automatically generated artifacts, and (3)\nreleasing a curated dataset to support future research on the automatic\ngeneration of explainability requirements.", "AI": {"tldr": "This paper introduces an automated tool-supported approach to derive explainability requirements from user reviews and generate aligned explanations, while highlighting the tradeoff between AI-generated clarity/style vs human validation for correctness.", "motivation": "Explainability requirements are critical for transparency and trust but current methods lack systematic approaches to translate user feedback into structured requirements and explanations.", "method": "Developed an industry-validated tool to automatically parse 58 annotated user reviews, extract requirements, and generate explanations while benchmarking AI output against human-crafted ones", "result": "AI-generated explanations were preferred for clarity/style (78% acceptance) but requirements lacked relevance/correctness (62% acceptable). Human validation remains essential for correctness despite AI benefits. Dataset released for future research.", "conclusion": "The work presents three contributions: 1) Automated explainability requirements derivation pipeline, 2) Empirical analysis of AI vs human artifacts, 3) Publicly available annotated dataset enabling research in this area."}}
{"id": "2507.07401", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07401", "abs": "https://arxiv.org/abs/2507.07401", "authors": ["Fupei Chen", "Liyao Xiang", "Haoxiang Sun", "Hei Victor Cheng", "Kaiming Shen"], "title": "Shuffling for Semantic Secrecy", "comment": null, "summary": "Deep learning draws heavily on the latest progress in semantic\ncommunications. The present paper aims to examine the security aspect of this\ncutting-edge technique from a novel shuffling perspective. Our goal is to\nimprove upon the conventional secure coding scheme to strike a desirable\ntradeoff between transmission rate and leakage rate. To be more specific, for a\nwiretap channel, we seek to maximize the transmission rate while minimizing the\nsemantic error probability under the given leakage rate constraint. Toward this\nend, we devise a novel semantic security communication system wherein the\nrandom shuffling pattern plays the role of the shared secret key. Intuitively,\nthe permutation of feature sequences via shuffling would distort the semantic\nessence of the target data to a sufficient extent so that eavesdroppers cannot\naccess it anymore. The proposed random shuffling method also exhibits its\nflexibility in working for the existing semantic communication system as a\nplugin. Simulations demonstrate the significant advantage of the proposed\nmethod over the benchmark in boosting secure transmission, especially when\nchannels are prone to strong noise and unpredictable fading.", "AI": {"tldr": "This paper proposes a semantic security communication system using random shuffling patterns as shared keys to distort data semantics and enhance secure transmission, particularly in noisy channels.", "motivation": "The motivation stems from the need for improving conventional secure coding schemes by achieving a better tradeoff between transmission rate and leakage rate in wiretap channels while minimizing semantic error probability.", "method": "The authors devised a novel system utilizing random shuffling patterns as shared secret keys to permute feature sequences, obscuring semantic information from eavesdroppers. This approach is flexible enough to integrate as a plugin into existing semantic communication frameworks.", "result": "Simulation results demonstrate that the proposed shuffling method significantly outperforms benchmarks in secure transmission, especially in channels with strong noise and unpredictable fading conditions.", "conclusion": "The conclusion highlights that the random shuffling-based security framework effectively protects data semantics against eavesdropping, offering a practical and adaptable solution for enhancing security in semantic communication systems."}}
{"id": "2507.07468", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07468", "abs": "https://arxiv.org/abs/2507.07468", "authors": ["Sten Gr\u00fcner", "Nafise Eskandani"], "title": "Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN", "comment": "7 pages, 7 figures, Accepted at IFAC EAAS 2025\n  (https://j3c.org/eaas.php)", "summary": "The integration of Industry 4.0 technologies into engineering workflows is an\nessential step toward automating and optimizing plant and process engineering\nprocesses. The Asset Administration Shell (AAS) serves as a key enabler for\ncreating interoperable Digital Twins that facilitate engineering data exchange\nand automation. This paper explores the use of AAS within engineering\nworkflows, particularly in combination with Business Process Model and Notation\n(BPMN) to define structured and automated processes. We propose a distributed\nAAS copy-on-write infrastructure that enhances security and scalability while\nenabling seamless cross organizational collaboration. We also introduce a\nworkflow management prototype automating AAS operations and engineering\nworkflows, improving efficiency and traceability.", "AI": {"tldr": "This paper proposes a distributed AAS copy-on-write infrastructure combined with BPMN workflows to enhance secure, scalable, and automated engineering processes across organizations.", "motivation": "Industry 4.0 demands automated plant/process engineering workflows enabled by interoperable Digital Twins, but existing solutions lack robust security, scalability, and cross-organizational collaboration capabilities.", "method": "The authors develop a distributed copy-on-write AAS infrastructure to address security and scalability challenges, while implementing a workflow management prototype driven by BPMN to automate AAS operations.", "result": "The proposed infrastructure enables seamless AAS engineering automation, cross-organizational collaboration, and improved process traceability, validated through a workflow management prototype demonstrating enhanced efficiency.", "conclusion": "The integration of distributed AAS infrastructure with BPMN workflows offers a promising approach to advancing interoperable, automated engineering processes in Industry 4.0 contexts."}}
{"id": "2507.07406", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07406", "abs": "https://arxiv.org/abs/2507.07406", "authors": ["Jikesh Thapa", "Gurrehmat Chahal", "Serban Voinea Gabreanu", "Yazan Otoum"], "title": "Phishing Detection in the Gen-AI Era: Quantized LLMs vs Classical Models", "comment": "8 Pages, IEEE Conference", "summary": "Phishing attacks are becoming increasingly sophisticated, underscoring the\nneed for detection systems that strike a balance between high accuracy and\ncomputational efficiency. This paper presents a comparative evaluation of\ntraditional Machine Learning (ML), Deep Learning (DL), and quantized\nsmall-parameter Large Language Models (LLMs) for phishing detection. Through\nexperiments on a curated dataset, we show that while LLMs currently\nunderperform compared to ML and DL methods in terms of raw accuracy, they\nexhibit strong potential for identifying subtle, context-based phishing cues.\nWe also investigate the impact of zero-shot and few-shot prompting strategies,\nrevealing that LLM-rephrased emails can significantly degrade the performance\nof both ML and LLM-based detectors. Our benchmarking highlights that models\nlike DeepSeek R1 Distill Qwen 14B (Q8_0) achieve competitive accuracy, above\n80%, using only 17GB of VRAM, supporting their viability for cost-efficient\ndeployment. We further assess the models' adversarial robustness and\ncost-performance tradeoffs, and demonstrate how lightweight LLMs can provide\nconcise, interpretable explanations to support real-time decision-making. These\nfindings position optimized LLMs as promising components in phishing defence\nsystems and offer a path forward for integrating explainable, efficient AI into\nmodern cybersecurity frameworks.", "AI": {"tldr": "This paper compares traditional Machine Learning (ML), Deep Learning (DL), and quantized LLMs for phishing detection. While LLMs lag in raw accuracy, they excel at identifying context-based phishing cues and lightweight LLMs like Q8_0 models offer cost-efficient deployment options with over 80% accuracy at 17GB VRAM. LLM-based methods also provide interpretable explanations but suffer from performance degradation when handling rephrased emails.", "motivation": "Phishing attacks are becoming more sophisticated, necessitating detection systems that balance accuracy with computational efficiency. Traditional AI approaches may lack interpretability, while unoptimized LLMs face high resource constraints. The study explores optimized LLM applications to address these challenges in cybersecurity contexts.", "method": "Experiments were conducted on a curated phishing detection dataset, comparing traditional ML/DL models against quantized LLMs (e.g., DeepSeek R1 Distill Qwen 14B Q8_0). The evaluation assessed accuracy, VRAM requirements, adversarial robustness, cost-performance tradeoffs, and the impact of zero-shot/few-shot prompting strategies using LLM-rephrased emails as adversarial examples.", "result": "1) Quantized LLMs achieved >80% accuracy using 17GB VRAM. 2) LLMs outperformed ML/DL for subtle contextual phishing cues while underperforming in direct accuracy comparisons. 3) Zero-shot/few-shot prompting significantly reduced detection accuracy for both ML and LLM detectors. 4) Lightweight LLMs provided concise, human-interpretable explanations for phishing decisions.", "conclusion": "While traditionally behind ML/DL approaches in accuracy, optimized LLMs demonstrate sufficient phishing detection accuracy with major advantages in interpretability and cost efficiency. The findings suggest lightweight LLMs can be strategically integrated into multi-layered cybersecurity systems as explainable, resource-conscious detectors for context-aware phishing threats."}}
{"id": "2507.07548", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07548", "abs": "https://arxiv.org/abs/2507.07548", "authors": ["Jonathan Ullrich", "Matthias Koch", "Andreas Vogelsang"], "title": "From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering", "comment": "This paper has been accepted for publication at the 33rd IEEE\n  International Requirements Engineering (RE) conference", "summary": "With the advent of generative LLMs and their advanced code generation\ncapabilities, some people already envision the end of traditional software\nengineering, as LLMs may be able to produce high-quality code based solely on\nthe requirements a domain expert feeds into the system. The feasibility of this\nvision can be assessed by understanding how developers currently incorporate\nrequirements when using LLMs for code generation-a topic that remains largely\nunexplored. We interviewed 18 practitioners from 14 companies to understand how\nthey (re)use information from requirements and other design artifacts to feed\nLLMs when generating code. Based on our findings, we propose a theory that\nexplains the processes developers employ and the artifacts they rely on. Our\ntheory suggests that requirements, as typically documented, are too abstract\nfor direct input into LLMs. Instead, they must first be manually decomposed\ninto programming tasks, which are then enriched with design decisions and\narchitectural constraints before being used in prompts. Our study highlights\nthat fundamental RE work is still necessary when LLMs are used to generate\ncode. Our theory is important for contextualizing scientific approaches to\nautomating requirements-centric SE tasks.", "AI": {"tldr": "This paper investigates how developers use requirements and design artifacts with LLMs for code generation.", "motivation": "The shift in envisioning traditional software engineering due to advanced LLM code generation capabilities necessitates understanding current developer practices in incorporating requirements-a largely unexplored area.", "method": "Qualitative analysis through interviews with 18 practitioners from 14 companies about their use of requirements and design artifacts in LLM code generation.", "result": "A theory proposing that 'requirements must be manually decomposed into programming tasks and enriched with design decisions/architectural constraints before being used for LLM prompting'.", "conclusion": "While LLMs enable new workflows, fundamental requirements engineering work remains essential for effective code generation, highlighting ongoing needs for RE-AI integration research."}}
{"id": "2507.07413", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07413", "abs": "https://arxiv.org/abs/2507.07413", "authors": ["Mohammad F. Al-Hammouri", "Yazan Otoum", "Rasha Atwa", "Amiya Nayak"], "title": "Hybrid LLM-Enhanced Intrusion Detection for Zero-Day Threats in IoT Networks", "comment": "6 pages, IEEE conference", "summary": "This paper presents a novel approach to intrusion detection by integrating\ntraditional signature-based methods with the contextual understanding\ncapabilities of the GPT-2 Large Language Model (LLM). As cyber threats become\nincreasingly sophisticated, particularly in distributed, heterogeneous, and\nresource-constrained environments such as those enabled by the Internet of\nThings (IoT), the need for dynamic and adaptive Intrusion Detection Systems\n(IDSs) becomes increasingly urgent. While traditional methods remain effective\nfor detecting known threats, they often fail to recognize new and evolving\nattack patterns. In contrast, GPT-2 excels at processing unstructured data and\nidentifying complex semantic relationships, making it well-suited to uncovering\nsubtle, zero-day attack vectors. We propose a hybrid IDS framework that merges\nthe robustness of signature-based techniques with the adaptability of\nGPT-2-driven semantic analysis. Experimental evaluations on a representative\nintrusion dataset demonstrate that our model enhances detection accuracy by\n6.3%, reduces false positives by 9.0%, and maintains near real-time\nresponsiveness. These results affirm the potential of language model\nintegration to build intelligent, scalable, and resilient cybersecurity\ndefences suited for modern connected environments.", "AI": {"tldr": "This paper introduces a hybrid Intrusion Detection System (IDS) that combines signature-based methods with GPT-2's semantic analysis, achieving 6.3% better detection accuracy and 9.0% fewer false positives while maintaining near real-time performance.", "motivation": "To address rising cyber threats in IoT and other complex environments, where traditional signature-based IDSs struggle with zero-day attacks but machine learning models like GPT-2 can provide contextual threat understanding through unstructured data analysis.", "method": "Proposed a hybrid IDS framework merging signature-based threat detection with GPT-2-powered semantic analysis for anomaly detection in network traffic, using experiments on a benchmark intrusion dataset to validate performance improvements.", "result": "Demonstrated 6.3% increased detection accuracy, 9.0% reduced false-positive rate, and maintained near real-time responsiveness compared to traditional methods in experimental evaluations.", "conclusion": "Integration of large language models with conventional IDS approaches enhances threat detection capabilities while maintaining efficiency, suggesting a viable path toward intelligent, adaptive cybersecurity systems for modern distributed environments."}}
{"id": "2507.07682", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07682", "abs": "https://arxiv.org/abs/2507.07682", "authors": ["Kaicheng Huang", "Fanyu Wang", "Yutan Huang", "Chetan Arora"], "title": "Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap", "comment": null, "summary": "Advancements in large language models (LLMs) have led to a surge of prompt\nengineering (PE) techniques that can enhance various requirements engineering\n(RE) tasks. However, current LLMs are often characterized by significant\nuncertainty and a lack of controllability. This absence of clear guidance on\nhow to effectively prompt LLMs acts as a barrier to their trustworthy\nimplementation in the RE field. We present the first roadmap-oriented\nsystematic literature review of Prompt Engineering for RE (PE4RE). Following\nKitchenham's and Petersen's secondary-study protocol, we searched six digital\nlibraries, screened 867 records, and analyzed 35 primary studies. To bring\norder to a fragmented landscape, we propose a hybrid taxonomy that links\ntechnique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented\nRE roles (elicitation, validation, traceability). Two research questions, with\nfive sub-questions, map the tasks addressed, LLM families used, and prompt\ntypes adopted, and expose current limitations and research gaps. Finally, we\noutline a step-by-step roadmap showing how today's ad-hoc PE prototypes can\nevolve into reproducible, practitioner-friendly workflows.", "AI": {"tldr": "This paper presents a systematic review and roadmap for integrating prompt engineering in requirements engineering, addressing current fragmentation and offering a taxonomy of techniques.", "motivation": "The paper addresses the lack of controllability and uncertainty in LLM applications for RE tasks, highlighting the need for structured guidance to enable trustworthy implementation in RE.", "method": "Conducted a systematic literature review following Kitchenham and Petersen's protocol, screening 867 records across six digital libraries and analyzing 35 primary studies. Developed a hybrid taxonomy and two research questions with five sub-questions.", "result": "The review proposes a hybrid taxonomy linking prompt engineering techniques to RE tasks and identifies current limitations and research gaps. A roadmap is outlined to transition from ad-hoc PE prototypes to structured workflows.", "conclusion": "The authors emphasize the importance of transitioning from ad-hoc prompt engineering methods to reproducible, practitioner-friendly workflows as a foundational step for LLM adoption in requirements engineering."}}
{"id": "2507.07416", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07416", "abs": "https://arxiv.org/abs/2507.07416", "authors": ["Jenifer Paulraj", "Brindha Raghuraman", "Nagarani Gopalakrishnan", "Yazan Otoum"], "title": "Autonomous AI-based Cybersecurity Framework for Critical Infrastructure: Real-Time Threat Mitigation", "comment": "7 pages, IEEE conference", "summary": "Critical infrastructure systems, including energy grids, healthcare\nfacilities, transportation networks, and water distribution systems, are\npivotal to societal stability and economic resilience. However, the increasing\ninterconnectivity of these systems exposes them to various cyber threats,\nincluding ransomware, Denial-of-Service (DoS) attacks, and Advanced Persistent\nThreats (APTs). This paper examines cybersecurity vulnerabilities in critical\ninfrastructure, highlighting the threat landscape, attack vectors, and the role\nof Artificial Intelligence (AI) in mitigating these risks. We propose a hybrid\nAI-driven cybersecurity framework to enhance real-time vulnerability detection,\nthreat modelling, and automated remediation. This study also addresses the\ncomplexities of adversarial AI, regulatory compliance, and integration. Our\nfindings provide actionable insights to strengthen the security and resilience\nof critical infrastructure systems against emerging cyber threats.", "AI": {"tldr": "This paper analyzes cybersecurity vulnerabilities in interconnected critical infrastructure systems and proposes a hybrid AI-based framework for real-time threat detection and mitigation, addressing adversarial AI and regulatory challenges to enhance security resilience.", "motivation": "Critical infrastructure systems are increasingly exposed to cyber threats due to interconnectivity, necessitating improved security measures to ensure societal stability and economic resilience.", "method": "The study proposes a hybrid AI-driven cybersecurity framework integrating real-time vulnerability detection, threat modeling, and automated remediation capabilities.", "result": "The findings provide actionable insights for strengthening critical infrastructure security against emerging threats through AI implementation.", "conclusion": "The hybrid AI framework offers a robust solution to address evolving cyber threats in critical infrastructure while managing adversarial AI risks and compliance complexities."}}
{"id": "2507.07689", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07689", "abs": "https://arxiv.org/abs/2507.07689", "authors": ["Chetan Arora", "Fanyu Wang", "Chakkrit Tantithamthavorn", "Aldeida Aleti", "Shaun Kenyon"], "title": "From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry", "comment": null, "summary": "Requirements engineering (RE) in the space industry is inherently complex,\ndemanding high precision, alignment with rigorous standards, and adaptability\nto mission-specific constraints. Smaller space organisations and new entrants\noften struggle to derive actionable requirements from extensive, unstructured\ndocuments such as mission briefs, interface specifications, and regulatory\nstandards. In this innovation opportunity paper, we explore the potential of\nRetrieval-Augmented Generation (RAG) models to support and (semi-)automate\nrequirements generation in the space domain. We present a modular, AI-driven\napproach that preprocesses raw space mission documents, classifies them into\nsemantically meaningful categories, retrieves contextually relevant content\nfrom domain standards, and synthesises draft requirements using large language\nmodels (LLMs). We apply the approach to a real-world mission document from the\nspace domain to demonstrate feasibility and assess early outcomes in\ncollaboration with our industry partner, Starbound Space Solutions. Our\npreliminary results indicate that the approach can reduce manual effort,\nimprove coverage of relevant requirements, and support lightweight compliance\nalignment. We outline a roadmap toward broader integration of AI in RE\nworkflows, intending to lower barriers for smaller organisations to participate\nin large-scale, safety-critical missions.", "AI": {"tldr": "This paper proposes using Retrieval-Augmented Generation (RAG) and large language models (LLMs) to semi-automate requirements engineering (RE) in the space industry, reducing manual effort and improving compliance for smaller organizations.", "motivation": "Smaller space organizations face challenges in deriving actionable requirements from unstructured documents due to complexity, standards alignment, and mission-specific constraints, necessitating automation.", "method": "A modular AI-driven approach that preprocesses mission documents, classifies content, retrieves relevant standards, and generates draft requirements using LLMs.", "result": "Preliminary results show reduced manual effort, improved requirement coverage, and enhanced compliance alignment. A practical implementation was tested with industry partner Starbound Space Solutions.", "conclusion": "The authors outline a roadmap for integrating AI into RE workflows to lower barriers for smaller space organizations in safety-critical missions."}}
{"id": "2507.07417", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07417", "abs": "https://arxiv.org/abs/2507.07417", "authors": ["Nishit V. Pandya", "Andrey Labunets", "Sicun Gao", "Earlence Fernandes"], "title": "May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks", "comment": null, "summary": "A popular class of defenses against prompt injection attacks on large\nlanguage models (LLMs) relies on fine-tuning the model to separate instructions\nand data, so that the LLM does not follow instructions that might be present\nwith data. There are several academic systems and production-level\nimplementations of this idea. We evaluate the robustness of this class of\nprompt injection defenses in the whitebox setting by constructing strong\noptimization-based attacks and showing that the defenses do not provide the\nclaimed security properties. Specifically, we construct a novel attention-based\nattack algorithm for text-based LLMs and apply it to two recent whitebox\ndefenses SecAlign (CCS 2025) and StruQ (USENIX Security 2025), showing attacks\nwith success rates of up to 70% with modest increase in attacker budget in\nterms of tokens. Our findings make fundamental progress towards understanding\nthe robustness of prompt injection defenses in the whitebox setting. We release\nour code and attacks at https://github.com/nishitvp/better_opts_attacks", "AI": {"tldr": "This paper demonstrates that existing fine-tuning defenses against prompt injection attacks in LLMs (SecAlign and StruQ) are vulnerable in the whitebox setting via novel attention-based attacks achieving ~70% success rates with minimal token budget increases. The analysis highlights gaps in claimed security properties and provides public code for evaluated attacks.", "motivation": "Current instruction separation defenses for LLMs lack thorough whitebox attack evaluation, creating uncertainty about their security guarantees. Understanding these weaknesses is critical for developing robust mitigation strategies.", "method": "Developed an attention-based optimization attack framework that exploits visibility patterns between instruction tokens and model internals. Applied this method to analyze SecAlign and StruQ through iterative adversarial perturbation with token budget constraints.", "result": "Achieved up to 70% successful prompt injection attack rates against both defenses with only a 10-20% token budget increase from baseline attacks, demonstrating significant vulnerability despite architectural modifications.", "conclusion": "Popular instruction separation defenses are fundamentally vulnerable to whitebox optimization-based attacks requiring minimal resources. This work establishes a benchmark for evaluating LLM injection defenses and emphasizes the need for stronger architecture-level solutions."}}
{"id": "2507.07732", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07732", "abs": "https://arxiv.org/abs/2507.07732", "authors": ["Giovanni Gambigliani Zoccoli", "Filip Valgimigli", "Dario Stabili", "Mirco Marchetti"], "title": "RADAR: a Radio-based Analytics for Dynamic Association and Recognition of pseudonyms in VANETs", "comment": "7 pages, 4 figures, accepted for publication at the 2025 IEEE 102nd\n  Vehicular Technology Conference: VTC2025-Fall", "summary": "This paper presents RADAR, a tracking algorithm for vehicles participating in\nCooperative Intelligent Transportation Systems (C-ITS) that exploits multiple\nradio signals emitted by a modern vehicle to break privacy-preserving pseudonym\nschemes deployed in VANETs. This study shows that by combining Dedicated Short\nRange Communication (DSRC) and Wi-Fi probe request messages broadcast by the\nvehicle, it is possible to improve tracking over standard de-anonymization\napproaches that only leverage DSRC, especially in realistic scenarios where the\nattacker does not have full coverage of the entire vehicle path. The\nexperimental evaluation compares three different metrics for pseudonym and\nWi-Fi probe identifier association (Count, Statistical RSSI, and Pearson RSSI),\ndemonstrating that the Pearson RSSI metric is better at tracking vehicles under\npseudonym-changing schemes in all scenarios and against previous works. As an\nadditional contribution to the state-of-the-art, we publicly release all\nimplementations and simulation scenarios used in this work.", "AI": {"tldr": "RADAR is a vehicle tracking algorithm that combines DSRC and Wi-Fi signals to improve de-anonymization in VANETs by exploiting pseudonym-breaking opportunities when attacker coverage is limited.", "motivation": "Existing pseudonym schemes in VANETs preserve location privacy by rotating identifiers, but standard de-anonymization methods relying solely on DSRC signals fail under partial observation scenarios common in real-world C-ITS implementations.", "method": "The approach combines Dedicated Short Range Communication (DSRC) with Wi-Fi probe request messages using a three-metric evaluation framework (Count, Statistical RSSI, and Pearson RSSI) to associate pseudonyms and Wi-Fi identifiers despite pseudonym rotation.", "result": "Experimental evaluation across multiple scenarios demonstrates Pearson RSSI metric outperforms previous methods in tracking performance, establishing robust advantages over standard Count and Statistical RSSI approaches.", "conclusion": "This work advances vehicle tracking capabilities in C-ITS through multi-signal correlation while contributing to research transparency by publicly releasing all implementations and simulation scenarios."}}
{"id": "2507.07773", "categories": ["cs.CR", "cs.CV", "B.8; I.4"], "pdf": "https://arxiv.org/pdf/2507.07773", "abs": "https://arxiv.org/abs/2507.07773", "authors": ["Youqian Zhang", "Xinyu Ji", "Zhihao Wang", "Qinhong Jiang"], "title": "Rainbow Artifacts from Electromagnetic Signal Injection Attacks on Image Sensors", "comment": "5 pages, 4 figures", "summary": "Image sensors are integral to a wide range of safety- and security-critical\nsystems, including surveillance infrastructure, autonomous vehicles, and\nindustrial automation. These systems rely on the integrity of visual data to\nmake decisions. In this work, we investigate a novel class of electromagnetic\nsignal injection attacks that target the analog domain of image sensors,\nallowing adversaries to manipulate raw visual inputs without triggering\nconventional digital integrity checks. We uncover a previously undocumented\nattack phenomenon on CMOS image sensors: rainbow-like color artifacts induced\nin images captured by image sensors through carefully tuned electromagnetic\ninterference. We further evaluate the impact of these attacks on\nstate-of-the-art object detection models, showing that the injected artifacts\npropagate through the image signal processing pipeline and lead to significant\nmispredictions. Our findings highlight a critical and underexplored\nvulnerability in the visual perception stack, highlighting the need for more\nrobust defenses against physical-layer attacks in such systems.", "AI": {"tldr": "The paper identifies and evaluates a novel electromagnetic interference attack on CMOS image sensors, which introduces rainbow-like color artifacts in raw visual data and leads to significant mispredictions in object detection systems. This highlights a critical vulnerability in the physical layer of image sensors used in safety/industrial applications.", "motivation": "Image sensors are crucial in safety- and security-critical systems (e.g., autonomous vehicles, surveillance infrastructure), and their data integrity is essential for valid system decisions. Current digital integrity checks fail to detect analog-domain attacks. The paper aims to uncover physical-layer vulnerabilities to improve security.", "method": "The researchers inject electromagnetic signals into the analog domain of CMOS image sensors to manipulate raw visual input. They exploit undocumented attack phenomena by tuning electromagnetic interference to induce controlled rainbow-like color artifacts. These artifacts are then analyzed to determine their impact on object detection pipelines.", "result": "The injected artifacts bypass conventional digital integrity checks, propagate through image signal processing pipelines, and cause significant mispredictions in state-of-the-art object detection models (e.g., misclassifying objects, introducing false positives). This demonstrates vulnerability to physical-layer attacks in the visual perception stack.", "conclusion": "The paper concludes that physical-layer attacks targeting the analog domain of image sensors represent a critical and underexplored vulnerability. It emphasizes the need to develop robust defenses for the visual perception stack to prevent adversarial manipulation of raw visual data in safety-critical systems."}}
{"id": "2507.07871", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07871", "abs": "https://arxiv.org/abs/2507.07871", "authors": ["Toluwani Aremu", "Noor Hussein", "Munachiso Nwadike", "Samuele Poppi", "Jie Zhang", "Karthik Nandakumar", "Neil Gong", "Nils Lukas"], "title": "Mitigating Watermark Stealing Attacks in Generative Models via Multi-Key Watermarking", "comment": null, "summary": "Watermarking offers a promising solution for GenAI providers to establish the\nprovenance of their generated content. A watermark is a hidden signal embedded\nin the generated content, whose presence can later be verified using a secret\nwatermarking key. A threat to GenAI providers are \\emph{watermark stealing}\nattacks, where users forge a watermark into content that was \\emph{not}\ngenerated by the provider's models without access to the secret key, e.g., to\nfalsely accuse the provider. Stealing attacks collect \\emph{harmless}\nwatermarked samples from the provider's model and aim to maximize the expected\nsuccess rate of generating \\emph{harmful} watermarked samples. Our work focuses\non mitigating stealing attacks while treating the underlying watermark as a\nblack-box. Our contributions are: (i) Proposing a multi-key extension to\nmitigate stealing attacks that can be applied post-hoc to any watermarking\nmethod across any modality. (ii) We provide theoretical guarantees and\ndemonstrate empirically that our method makes forging substantially less\neffective across multiple datasets, and (iii) we formally define the threat of\nwatermark forging as the task of generating harmful, watermarked content and\nmodel this threat via security games.", "AI": {"tldr": "The paper proposes a multi-key watermarking strategy to prevent unauthorized forging of watermarks in generative AI outputs, offering theoretical/empirical guarantees and modeling threats via security games.", "motivation": "GenAI providers face risks from watermark stealing attacks where users forge watermarks without access to secret keys to misattribute harmful content, undermining provenance verification.", "method": "A multi-key extension with black-box watermarking compatibility is introduced, enabling dynamic key selection during verification. Threats are formalized as security games to evaluate model vulnerabilities.", "result": "The method significantly reduces forging success rates across datasets with theoretical security bounds validated experimentally, demonstrating robustness against simulated stealing attacks.", "conclusion": "The proposed framework provides a practical defense against watermark stealing by decoupling key management from watermark design, while establishing a game-theoretic model for future attack/defense research."}}
{"id": "2507.07901", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07901", "abs": "https://arxiv.org/abs/2507.07901", "authors": ["Sree Bhargavi Balija", "Rekha Singal", "Abhishek Singh", "Ramesh Raskar", "Erfan Darzi", "Raghu Bala", "Thomas Hardjono", "Ken Huang"], "title": "The Trust Fabric: Decentralized Interoperability and Economic Coordination for the Agentic Web", "comment": null, "summary": "The fragmentation of AI agent ecosystems has created urgent demands for\ninteroperability, trust, and economic coordination that current protocols --\nincluding MCP (Hou et al., 2025), A2A (Habler et al., 2025), ACP (Liu et al.,\n2025), and Cisco's AGP (Edwards, 2025) -- cannot address at scale. We present\nthe Nanda Unified Architecture, a decentralized framework built around three\ncore innovations: fast DID-based agent discovery through distributed\nregistries, semantic agent cards with verifiable credentials and composability\nprofiles, and a dynamic trust layer that integrates behavioral attestations\nwith policy compliance. The system introduces X42/H42 micropayments for\neconomic coordination and MAESTRO, a security framework incorporating\nSynergetics' patented AgentTalk protocol (US Patent 12,244,584 B1) and secure\ncontainerization. Real-world deployments demonstrate 99.9 percent compliance in\nhealthcare applications and substantial monthly transaction volumes with strong\nprivacy guarantees. By unifying MIT's trust research with production\ndeployments from Cisco and Synergetics, we show how cryptographic proofs and\npolicy-as-code transform agents into trust-anchored participants in a\ndecentralized economy (Lakshmanan, 2025; Sha, 2025). The result enables a\nglobally interoperable Internet of Agents where trust becomes the native\ncurrency of collaboration across both enterprise and Web3 ecosystems.", "AI": {"tldr": "Nanda addresses AI agent ecosystem fragmentation via DID discovery, semantic agent cards, dynamic trust layers, X42 H42 micropayments, and MAESTRO security for a trust-anchored IoA.", "motivation": "Current protocols (MCP, A2A, ACP, AGP) fail to address interoperability, trust, and economic coordination challenges in fragmented AI agent ecosystems at scale.", "method": "Architecture features: 1) DID-based distributed registries for fast agent discovery 2) Semantic agent cards with verifiable credentials/composability profiles 3) Trust layer combining behavioral attestations/policy compliance 4) X42/H42 micropayment coordination 5) MAESTRO security (AgentTalk protocol + secure containers).", "result": "Real-world deployments show 99.9% healthcare compliance and strong monthly transaction volumes with privacy guarantees, demonstrating transactional viability.", "conclusion": "Unifies MIT's trust research with production deployments to create a trust-anchored, globally interoperable Internet of Agents (IoA) across enterprise/Web3 ecosystems using cryptographic proofs and policy-as-code."}}
{"id": "2507.07916", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.07916", "abs": "https://arxiv.org/abs/2507.07916", "authors": ["Federico Maria Cau", "Giuseppe Desolda", "Francesco Greco", "Lucio Davide Spano", "Luca Vigan\u00f2"], "title": "Can Large Language Models Improve Phishing Defense? A Large-Scale Controlled Experiment on Warning Dialogue Explanations", "comment": null, "summary": "Phishing has become a prominent risk in modern cybersecurity, often used to\nbypass technological defences by exploiting predictable human behaviour.\nWarning dialogues are a standard mitigation measure, but the lack of\nexplanatory clarity and static content limits their effectiveness. In this\npaper, we report on our research to assess the capacity of Large Language\nModels (LLMs) to generate clear, concise, and scalable explanations for\nphishing warnings. We carried out a large-scale between-subjects user study (N\n= 750) to compare the influence of warning dialogues supplemented with manually\ngenerated explanations against those generated by two LLMs, Claude 3.5 Sonnet\nand Llama 3.3 70B. We investigated two explanatory styles (feature-based and\ncounterfactual) for their effects on behavioural metrics (click-through rate)\nand perceptual outcomes (e.g., trust, risk, clarity). The results indicate that\nwell-constructed LLM-generated explanations can equal or surpass manually\ncrafted explanations in reducing susceptibility to phishing; Claude-generated\nwarnings exhibited particularly robust performance. Feature-based explanations\nwere more effective for genuine phishing attempts, whereas counterfactual\nexplanations diminished false-positive rates. Other variables such as workload,\ngender, and prior familiarity with warning dialogues significantly moderated\nwarning effectiveness. These results indicate that LLMs can be used to\nautomatically build explanations for warning users against phishing, and that\nsuch solutions are scalable, adaptive, and consistent with human-centred\nvalues.", "AI": {"tldr": "This paper evaluates using Large Language Models (LLMs) to generate phishing warning explanations, demonstrating LLMs can match or exceed manual explanations in reducing phishing susceptibility while being scalable and adaptive.", "motivation": "Traditional phishing warnings with static content and limited clarity fail to effectively mitigate risks by addressing human factors, necessitating scalable, context-aware solutions.", "method": "A large-scale between-subjects user study (N=750) comparing manually generated vs. LLM (Claude 3.5 Sonnet, Llama 3.3 70B) generated explanations using two styles (feature-based and counterfactual) to measure click-through rates and user perceptions like trust/risk.", "result": "LLM-generated explanations equalled/exceeded manual ones in reducing phishing susceptibility; feature-based explanations outperformed counterfactual for genuine phishing attempts, while counterfactual reduced false-positive rates. Variables like workload, gender, and prior familiarity with warnings significantly influenced outcomes.", "conclusion": "LLMs enable the automatic creation of scalable, context-sensitive phishing explanations that maintain human-centred effectiveness while offering adaptability, demonstrating potential to enhance cybersecurity defenses through personalized user interaction."}}
{"id": "2507.07927", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07927", "abs": "https://arxiv.org/abs/2507.07927", "authors": ["Jenny Blessing", "Ross J. Anderson", "Alastair R. Beresford"], "title": "KeyDroid: A Large-Scale Analysis of Secure Key Storage in Android Apps", "comment": null, "summary": "Most contemporary mobile devices offer hardware-backed storage for\ncryptographic keys, user data, and other sensitive credentials. Such hardware\nprotects credentials from extraction by an adversary who has compromised the\nmain operating system, such as a malicious third-party app. Since 2011, Android\napp developers can access trusted hardware via the Android Keystore API. In\nthis work, we conduct the first comprehensive survey of hardware-backed key\nstorage in Android devices. We analyze 490 119 Android apps, collecting data on\nhow trusted hardware is used by app developers (if used at all) and\ncross-referencing our findings with sensitive user data collected by each app,\nas self-reported by developers via the Play Store's data safety labels.\n  We find that despite industry-wide initiatives to encourage adoption, 56.3%\nof apps self-reporting as processing sensitive user data do not use Android's\ntrusted hardware capabilities at all, while just 5.03% of apps collecting some\nform of sensitive data use the strongest form of trusted hardware, a secure\nelement distinct from the main processor. To better understand the potential\ndownsides of using secure hardware, we conduct the first empirical analysis of\ntrusted hardware performance in mobile devices, measuring the runtime of common\ncryptographic operations across both software- and hardware-backed keystores.\nWe find that while hardware-backed key storage using a coprocessor is viable\nfor most common cryptographic operations, secure elements capable of preventing\nmore advanced attacks make performance infeasible for symmetric encryption with\nnon-negligible payloads and any kind of asymmetric encryption.", "AI": {"tldr": "This paper analyzes the adoption and performance of Android's hardware-backed key storage across 490,119 apps. 56.3% of apps handling sensitive data don't use trusted hardware, and only 5.03% use the strongest secure element form. Secure elements show performance limitations for large symmetric encryption and all asymmetric encryption.", "motivation": "Hardware-backed storage protects sensitive data from OS-level attacks, but adoption patterns and performance implications remain poorly understood despite industry initiatives.", "method": "Examined 490,119 Android apps for hardware usage, cross-referencing with self-reported data safety labels. Conducted empirical performance analysis of cryptographic operations in hardware vs software keystores across 24 devices.", "result": "56.3% of sensitive data processing apps used no hardware protection. 5.03% used secure elements. Coprocessor-based storage was viable for common operations, but secure elements showed 5-20x slower symmetric encryption for large payloads and 50-100x slower asymmetric encryption.", "conclusion": "Industry efforts to promote secure hardware adoption have limited success. Security vs performance tradeoffs in existing implementations (especially secure elements) create challenges for practical deployment of defense-in-depth strategies."}}
{"id": "2507.07972", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07972", "abs": "https://arxiv.org/abs/2507.07972", "authors": ["Karthik Garimella", "Austin Ebel", "Brandon Reagen"], "title": "EinHops: Einsum Notation for Expressive Homomorphic Operations on RNS-CKKS Tensors", "comment": "11 pages, 7 figures, 1 table", "summary": "Fully Homomorphic Encryption (FHE) is an encryption scheme that allows for\ncomputation to be performed directly on encrypted data, effectively closing the\nloop on secure and outsourced computing. Data is encrypted not only during rest\nand transit, but also during processing. However, FHE provides a limited\ninstruction set: SIMD addition, SIMD multiplication, and cyclic rotation of 1-D\nvectors. This restriction makes performing multi-dimensional tensor operations\nchallenging. Practitioners must pack these tensors into 1-D vectors and map\ntensor operations onto this one-dimensional layout rather than their\ntraditional nested structure. And while prior systems have made significant\nstrides in automating this process, they often hide critical packing decisions\nbehind layers of abstraction, making debugging, optimizing, and building on top\nof these systems difficult.\n  In this work, we approach multi-dimensional tensor operations in FHE through\nEinstein summation (einsum) notation. Einsum notation explicitly encodes\ndimensional structure and operations in its syntax, naturally exposing how\ntensors should be packed and transformed. We decompose einsum expressions into\na fixed set of FHE-friendly operations. We implement our design and present\nEinHops, a minimalist system that factors einsum expressions into a fixed\nsequence of FHE operations. EinHops enables developers to perform encrypted\ntensor operations using FHE while maintaining full visibility into the\nunderlying packing strategy. We evaluate EinHops on a range of tensor\noperations from a simple transpose to complex multi-dimensional contractions.\nWe show that the explicit nature of einsum notation allows us to build an FHE\ntensor system that is simple, general, and interpretable. We open-source\nEinHops at the following repository: https://github.com/baahl-nyu/einhops.", "AI": {"tldr": "EinHops addresses FHE limitations in multi-dimensional tensor operations by leveraging Einstein summation notation, providing a transparent and general system for encrypted tensor computations.", "motivation": "FHE's 1-D operation constraints force tensor packing into vectors, with existing systems hiding critical decisions in abstractions. This hinders debugging and optimization, necessitating a transparent approach to tensor operations.", "method": "The authors decompose einsum expressions into FHE-compatible operations, implementing EinHops to explicitly expose packing strategies through einsum's dimensional syntax. This avoids abstracted packing decisions in prior systems.", "result": "EinHops was evaluated on diverse tensor operations, showing it achieves simplicity, generality, and interpretability. The open-source implementation provides a foundation for further encrypted computing advancements.", "conclusion": "EinHops demonstrates that einsum's explicitness enables simple, interpretable FHE tensor systems with full visibility into packing strategies, validated across operations from transposes to contractions."}}
{"id": "2507.07974", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07974", "abs": "https://arxiv.org/abs/2507.07974", "authors": ["Sizhe Chen", "Yizhu Wang", "Nicholas Carlini", "Chawin Sitawarin", "David Wagner"], "title": "Defending Against Prompt Injection With a Few DefensiveTokens", "comment": null, "summary": "When large language model (LLM) systems interact with external data to\nperform complex tasks, a new attack, namely prompt injection, becomes a\nsignificant threat. By injecting instructions into the data accessed by the\nsystem, the attacker is able to override the initial user task with an\narbitrary task directed by the attacker. To secure the system, test-time\ndefenses, e.g., defensive prompting, have been proposed for system developers\nto attain security only when needed in a flexible manner. However, they are\nmuch less effective than training-time defenses that change the model\nparameters. Motivated by this, we propose DefensiveToken, a test-time defense\nwith prompt injection robustness comparable to training-time alternatives.\nDefensiveTokens are newly inserted as special tokens, whose embeddings are\noptimized for security. In security-sensitive cases, system developers can\nappend a few DefensiveTokens before the LLM input to achieve security with a\nminimal utility drop. In scenarios where security is less of a concern,\ndevelopers can simply skip DefensiveTokens; the LLM system remains the same as\nthere is no defense, generating high-quality responses. Thus, DefensiveTokens,\nif released alongside the model, allow a flexible switch between the\nstate-of-the-art (SOTA) utility and almost-SOTA security at test time. The code\nis available at https://github.com/Sizhe-Chen/DefensiveToken.", "AI": {"tldr": "Introduces DefensiveTokens for test-time LLM security against prompt injection attacks, balancing high security and utility without retraining.", "motivation": "Existing test-time LLM defenses are ineffective compared to retraining-based approaches. There is a need for effective test-time security with minimal utility loss that can be optionally activated.", "method": "Proposes DefensiveTokens: special tokens with optimized embeddings inserted before LLM inputs. These tokens are designed to neutralize prompt injection attacks by contextually overriding malicious inputs without altering model parameters.", "result": "Achieves security comparable to training-time defenses when using DefensiveTokens, while maintaining low utility impact when security is disabled. Developers can dynamically enable/disable security.", "conclusion": "DefensiveTokens provide a test-time defense against prompt injection attacks, enabling LLM systems to switch between SOTA utility and near-SOTA security without model parameter changes, offering flexibility for developers. The method's availability via GitHub demonstrates its practical implementation."}}
