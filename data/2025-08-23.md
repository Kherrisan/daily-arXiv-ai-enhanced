<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 3]
- [cs.SE](#cs.SE) [Total: 6]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [MCPTox: A Benchmark for Tool Poisoning Attack on Real-World MCP Servers](https://arxiv.org/abs/2508.14925)
*Zhiqiang Wang,Yichao Gao,Yanting Wang,Suyuan Liu,Haifeng Sun,Haoran Cheng,Guanquan Shi,Haohua Du,Xiangyang Li*

Main category: cs.CR

TL;DR: This paper introduces MCPTox, the first benchmark for evaluating LLM agent robustness against Tool Poisoning in MCP environments. Systematic testing reveals widespread vulnerabilities in leading models, particularly more capable ones, and exposes the ineffectiveness of current safety alignments against such threats.


<details>
  <summary>Details</summary>
Motivation: Existing research on MCP vulnerabilities focused on attacks through tool outputs, while Tool Poisoning—a threat involving malicious metadata within tools without execution—lacks systematic evaluation.

Method: The researchers developed MCPTox, a benchmark consisting of 45 real-world MCP servers and 353 authentic tools, employing three attack templates to generate 1312 malicious test cases via few-shot learning. They evaluated 20 prominent LLM agents to assess their vulnerability to Tool Poisoning.

Result: High vulnerability to Tool Poisoning was identified in LLM agents, with o1-mini achieving a 72.8% attack success rate. More capable models were more susceptible due to enhanced instruction-following abilities, while existing safety measures failed to detect malicious unauthorized operations, with refusal rates as low as 3%.

Conclusion: The study establishes an essential empirical baseline for understanding and mitigating the Tool Poisoning threat, emphasizing the necessity for improved safety mechanisms in autonomous agent systems.

Abstract: By providing a standardized interface for LLM agents to interact with
external tools, the Model Context Protocol (MCP) is quickly becoming a
cornerstone of the modern autonomous agent ecosystem. However, it creates novel
attack surfaces due to untrusted external tools. While prior work has focused
on attacks injected through external tool outputs, we investigate a more
fundamental vulnerability: Tool Poisoning, where malicious instructions are
embedded within a tool's metadata without execution. To date, this threat has
been primarily demonstrated through isolated cases, lacking a systematic,
large-scale evaluation.
  We introduce MCPTox, the first benchmark to systematically evaluate agent
robustness against Tool Poisoning in realistic MCP settings. MCPTox is
constructed upon 45 live, real-world MCP servers and 353 authentic tools. To
achieve this, we design three distinct attack templates to generate a
comprehensive suite of 1312 malicious test cases by few-shot learning, covering
10 categories of potential risks. Our evaluation on 20 prominent LLM agents
setting reveals a widespread vulnerability to Tool Poisoning, with o1-mini,
achieving an attack success rate of 72.8\%. We find that more capable models
are often more susceptible, as the attack exploits their superior
instruction-following abilities. Finally, the failure case analysis reveals
that agents rarely refuse these attacks, with the highest refused rate
(Claude-3.7-Sonnet) less than 3\%, demonstrating that existing safety alignment
is ineffective against malicious actions that use legitimate tools for
unauthorized operation. Our findings create a crucial empirical baseline for
understanding and mitigating this widespread threat, and we release MCPTox for
the development of verifiably safer AI agents. Our dataset is available at an
anonymized repository: \textit{https://anonymous.4open.science/r/AAAI26-7C02}.

</details>


### [2] [A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives](https://arxiv.org/abs/2508.15031)
*Kaixiang Zhao,Lincan Li,Kaize Ding,Neil Zhenqiang Gong,Yue Zhao,Yushun Dong*

Main category: cs.CR

TL;DR: This paper surveys model extraction attacks against MLaaS platforms, introduces a taxonomy for categorizing attacks/defenses, and analyzes their technical and societal impacts, providing a resource hub for the AI security community.


<details>
  <summary>Details</summary>
Motivation: The adoption of MLaaS platforms, while democratizing ML access, introduces vulnerabilities via MEAs that threaten intellectual property, privacy, and security. This work addresses the need for structured analysis of MEAs and defenses to guide secure ML deployment.

Method: The authors conduct a systematic survey of MEAs, propose a taxonomy classifying attacks by mechanisms, defenses, and computing environments, evaluate attack effectiveness, and analyze trade-offs between model utility and security across paradigms.

Result: The study categorizes MEAs through its novel taxonomy, highlights limitations of existing defenses (particularly utility-security trade-offs), and discusses multidimensional implications. An actively updated online repository centralizes related research.

Conclusion: This paper provides a comprehensive survey of Model Extraction Attacks (MEAs) and defense strategies, offering a novel taxonomy and analyzing technical, ethical, and societal implications. It serves as a reference for AI security researchers and policymakers, supported by an openly maintained literature repository.

Abstract: Machine learning (ML) models have significantly grown in complexity and
utility, driving advances across multiple domains. However, substantial
computational resources and specialized expertise have historically restricted
their wide adoption. Machine-Learning-as-a-Service (MLaaS) platforms have
addressed these barriers by providing scalable, convenient, and affordable
access to sophisticated ML models through user-friendly APIs. While this
accessibility promotes widespread use of advanced ML capabilities, it also
introduces vulnerabilities exploited through Model Extraction Attacks (MEAs).
Recent studies have demonstrated that adversaries can systematically replicate
a target model's functionality by interacting with publicly exposed interfaces,
posing threats to intellectual property, privacy, and system security. In this
paper, we offer a comprehensive survey of MEAs and corresponding defense
strategies. We propose a novel taxonomy that classifies MEAs according to
attack mechanisms, defense approaches, and computing environments. Our analysis
covers various attack techniques, evaluates their effectiveness, and highlights
challenges faced by existing defenses, particularly the critical trade-off
between preserving model utility and ensuring security. We further assess MEAs
within different computing paradigms and discuss their technical, ethical,
legal, and societal implications, along with promising directions for future
research. This systematic survey aims to serve as a valuable reference for
researchers, practitioners, and policymakers engaged in AI security and
privacy. Additionally, we maintain an online repository continuously updated
with related literature at https://github.com/kzhao5/ModelExtractionPapers.

</details>


### [3] [MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs](https://arxiv.org/abs/2508.15036)
*Ruyi Ding,Tianhong Xu,Xinyi Shen,Aidong Adam Ding,Yunsi Fei*

Main category: cs.CR

TL;DR: MoEcho exposes hidden security risks in MoE-based AI models: adversarial side-channel attacks can extract user data from hardware traces, demanding urgent safeguards for scalable AI systems.


<details>
  <summary>Details</summary>
Motivation: As MoE architectures scale for performance, their input-dependent routing mechanisms create exploitable side channels that could leak sensitive data, posing significant privacy risks in applications like LLMs and VLMs.

Method: The authors propose MoEcho, a runtime analysis framework that identifies four novel architectural side channels (Cache Occupancy, Pageout+Reload, Performance Counter, and TLB Evict+Reload) on CPUs/GPUs. They demonstrate four attacks exploiting these channels to breach privacy in MoE-based models.

Result: The four MoEcho attacks successfully infer user prompts, reconstruct responses, and extract visual information from MoE-based models, proving the feasibility of breaching privacy via hardware execution traces.

Conclusion: This paper is the first to analyze the security risks of MoE architectures in transformers, identifying critical privacy threats through side-channel attacks and urging immediate mitigation strategies for secure large-scale AI deployment.

Abstract: The transformer architecture has become a cornerstone of modern AI, fueling
remarkable progress across applications in natural language processing,
computer vision, and multimodal learning. As these models continue to scale
explosively for performance, implementation efficiency remains a critical
challenge. Mixture of Experts (MoE) architectures, selectively activating
specialized subnetworks (experts), offer a unique balance between model
accuracy and computational cost. However, the adaptive routing in MoE
architectures, where input tokens are dynamically directed to specialized
experts based on their semantic meaning inadvertently opens up a new attack
surface for privacy breaches. These input-dependent activation patterns leave
distinctive temporal and spatial traces in hardware execution, which
adversaries could exploit to deduce sensitive user data. In this work, we
propose MoEcho, discovering a side channel analysis based attack surface that
compromises user privacy on MoE based systems. Specifically, in MoEcho, we
introduce four novel architectural side channels on different computing
platforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and
Performance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting
these vulnerabilities, we propose four attacks that effectively breach user
privacy in large language models (LLMs) and vision language models (VLMs) based
on MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,
Visual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first
runtime architecture level security analysis of the popular MoE structure
common in modern transformers, highlighting a serious security and privacy
threat and calling for effective and timely safeguards when harnessing MoE
based models for developing efficient large scale AI services.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [On the need to perform comprehensive evaluations of automated program repair benchmarks: Sorald case study](https://arxiv.org/abs/2508.15135)
*Sumudu Liyanage,Sherlock A. Licorish,Markus Wagner,Stephen G. MacDonell*

Main category: cs.SE

TL;DR: This paper addresses the need for comprehensive evaluation frameworks for Automated Program Repair (APR) tools by analyzing Sorald's effectiveness in fixing and introducing code violations, impacting functionality and code structure.


<details>
  <summary>Details</summary>
Motivation: Previous APR tool evaluations focused only on clearing violations, neglecting new faults, functionality changes, and code structure degradation, necessitating a broader assessment framework.

Method: The study evaluated Sorald on 3,529 SonarQube violations in 2,393 Java code snippets from Stack Overflow, analyzing new faults, test failures, and code structure changes.

Result: Sorald introduced 2,120 new faults (32 bugs, 2088 code smells), caused 24% unit test failures, and degraded code structure despite fixing original violations.

Conclusion: The research emphasizes developing evaluation methodologies that capture all APR tool effects, including side effects, to ensure safe and effective adoption.

Abstract: In supporting the development of high-quality software, especially necessary
in the era of LLMs, automated program repair (APR) tools aim to improve code
quality by automatically addressing violations detected by static analysis
profilers. Previous research tends to evaluate APR tools only for their ability
to clear violations, neglecting their potential introduction of new (sometimes
severe) violations, changes to code functionality and degrading of code
structure. There is thus a need for research to develop and assess
comprehensive evaluation frameworks for APR tools. This study addresses this
research gap, and evaluates Sorald (a state-of-the-art APR tool) as a proof of
concept. Sorald's effectiveness was evaluated in repairing 3,529 SonarQube
violations across 30 rules within 2,393 Java code snippets extracted from Stack
Overflow. Outcomes show that while Sorald fixes specific rule violations, it
introduced 2,120 new faults (32 bugs, 2088 code smells), reduced code
functional correctness--as evidenced by a 24% unit test failure rate--and
degraded code structure, demonstrating the utility of our framework. Findings
emphasize the need for evaluation methodologies that capture the full spectrum
of APR tool effects, including side effects, to ensure their safe and effective
adoption.

</details>


### [5] [Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems](https://arxiv.org/abs/2508.15411)
*Frederik Vandeputte*

Main category: cs.SE

TL;DR: This paper proposes merging Generative AI with traditional software engineering to build reliable systems. It introduces design principles, architectural patterns, and software stack components for GenAI-native systems while highlighting technical and societal impacts requiring further research.


<details>
  <summary>Details</summary>
Motivation: GenAI's unpredictability and inefficiency hinder the development of reliable systems. Existing approaches fail to address the need for resilient, self-evolving architectures. The paper seeks to bridge this gap by advocating a paradigm shift that merges GenAI's cognitive capabilities with traditional software engineering fundamentals.

Method: The authors propose foundational GenAI-native design principles centered on five pillars (reliability, excellence, evolvability, self-reliance, and assurance) and introduce architectural patterns such as GenAI-native cells, organic substrates, and programmable routers. They also outline the components of a GenAI-native software stack and analyze system impacts from multiple perspectives.

Result: The paper delivers a conceptual framework for GenAI-native systems, including design principles, architectural patterns, and software stack requirements. It provides a multidimensional analysis of technical feasibility, user adoption barriers, economic implications, and legal considerations, setting the stage for future research and implementation.

Conclusion: The paper concludes that integrating Generative AI (GenAI) with traditional software engineering principles can lead to the development of robust, adaptive, and efficient systems. This conceptual framework requires further validation, experimentation, and refinement by the research community to address technical, user, economic, and legal challenges.

Abstract: Generative AI (GenAI) has emerged as a transformative technology,
demonstrating remarkable capabilities across diverse application domains.
However, GenAI faces several major challenges in developing reliable and
efficient GenAI-empowered systems due to its unpredictability and inefficiency.
This paper advocates for a paradigm shift: future GenAI-native systems should
integrate GenAI's cognitive capabilities with traditional software engineering
principles to create robust, adaptive, and efficient systems.
  We introduce foundational GenAI-native design principles centered around five
key pillars -- reliability, excellence, evolvability, self-reliance, and
assurance -- and propose architectural patterns such as GenAI-native cells,
organic substrates, and programmable routers to guide the creation of resilient
and self-evolving systems. Additionally, we outline the key ingredients of a
GenAI-native software stack and discuss the impact of these systems from
technical, user adoption, economic, and legal perspectives, underscoring the
need for further validation and experimentation. Our work aims to inspire
future research and encourage relevant communities to implement and refine this
conceptual framework.

</details>


### [6] [An Empirical Study of Knowledge Distillation for Code Understanding Tasks](https://arxiv.org/abs/2508.15423)
*Ruiqi Wang,Zezhou Yang,Cuiyun Gao,Xin Xia,Qing Liao*

Main category: cs.SE

TL;DR: This paper demonstrates that knowledge distillation (KD) improves code understanding model efficiency, with feature-based KD and code-specific teachers achieving 98% performance at 5% model size. Architecture similarity doesn't guarantee better results, and KD significantly outperforms fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Pre-trained language models (PLMs) for code understanding face deployment challenges due to computational costs. Knowledge distillation (KD) is proposed as a compression technique to create efficient student models that retain most of the original PLMs' capabilities, but its application to code understanding remains underexplored.

Method: The study evaluates two KD types (logit-based and feature-based) using eight student models and two teacher PLMs (from code and cross-domains) across three downstream tasks. Experiments compare performance, efficiency, and architectural choices during KD and inference.

Result: KD consistently outperforms standard fine-tuning across student models of varying sizes. Feature-based KD achieves up to 98% of the teacher model's performance with only 5% of its parameters. Code-specific teacher models deliver better effectiveness than cross-domain ones, and student-teacher architecture similarity is not a performance determinant.

Conclusion: The paper concludes that knowledge distillation (KD) is effective for code understanding tasks, with feature-based methods and code-specific teacher models yielding the best performance. Student model architecture similarity to the teacher does not correlate with performance, and KD enables significant parameter reduction while retaining high performance. The study provides insights into KD strategies and future research directions.

Abstract: Pre-trained language models (PLMs) have emerged as powerful tools for code
understanding. However, deploying these PLMs in large-scale applications faces
practical challenges due to their computational intensity and inference
latency. Knowledge distillation (KD), a promising model compression and
acceleration technique, addresses these limitations by transferring knowledge
from large teacher models to compact student models, enabling efficient
inference while preserving most of the teacher models' capabilities. While this
technique has shown remarkable success in natural language processing and
computer vision domains, its potential for code understanding tasks remains
largely underexplored.
  In this paper, we systematically investigate the effectiveness and usage of
KD in code understanding tasks. Our study encompasses two popular types of KD
methods, i.e., logit-based and feature-based KD methods, experimenting across
eight student models and two teacher PLMs from different domains on three
downstream tasks. The experimental results indicate that KD consistently offers
notable performance boosts across student models with different sizes compared
with standard fine-tuning. Notably, code-specific PLM demonstrates better
effectiveness as the teacher model. Among all KD methods, the latest
feature-based KD methods exhibit superior performance, enabling student models
to retain up to 98% teacher performance with merely 5% parameters. Regarding
student architecture, our experiments reveal that similarity with teacher
architecture does not necessarily lead to better performance. We further
discuss the efficiency and behaviors in the KD process and inference, summarize
the implications of findings, and identify promising future directions.

</details>


### [7] [SynthCoder: A Synthetical Strategy to Tune LLMs for Code Completion](https://arxiv.org/abs/2508.15495)
*Dongjun Yu,Xiao Yan,Zhenrui Li,Jipeng Xiao,Haochuan He,Yongda Yu,Hao Zhang,Guoping Rong,Xiaobo Huang*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Code completion is a prominent application of Large Language Models (LLMs) in
software engineering. Due to the near real-time response requirements of this
task, base models with small to medium-sized parameters are typically employed,
supplemented by various optimization and post-training techniques. However,
these optimization methods often have trade-offs, leading to a seesaw effect
where performance improvements on certain datasets or metrics are accompanied
by degradations on others -- sometimes even falling below the baseline model's
performance. This paper proposes SynthCoder, a model that integrates leading
industry practices to achieve state-of-the-art performance on the
Fill-in-the-Middle (FIM) code completion task. In specific, we first construct
a diverse dataset by combining Abstract Syntax Tree (AST) node extraction with
heuristics that simulate developer behavior. Then we enrich our training corpus
with cross-file contextual information using the BM25 algorithm and call
graphs, enhancing the model's ability to perform code completion in both
file-level and repository-level scenarios. As the last step, we employ a
two-stage training process using the Seed-Coder-8B-Base as the base model.
First, we fine-tune the model using Curriculum Learning technology. Following
this, we perform alignment using Direct Preference Optimization (DPO) with
preference pairs generated through Rejection Sampling. Experimental results
demonstrate that our final model excels on mainstream repository-level code
completion benchmarks, including aiXcoder, ExecRepoBench, CrossCodeEval, and
CoLT. Furthermore, our carefully curated training set effectively mitigates the
model's tendency to just repeat existing code, a common issue existing in
various code completion models.

</details>


### [8] [Towards the Assessment of Task-based Chatbots: From the TOFU-R Snapshot to the BRASATO Curated Dataset](https://arxiv.org/abs/2508.15496)
*Elena Masserini,Diego Clerissi,Daniela Micucci,João R. Campos,Leonardo Mariani*

Main category: cs.SE

TL;DR: This paper introduces two datasets (TOFU-R, BRASATO) and tools to address the lack of quality data for chatbot evaluation, enabling systematic study of reliability and reproducibility.


<details>
  <summary>Details</summary>
Motivation: Current automated quality assessment techniques for chatbots face evaluation challenges due to reliance on small-scale, outdated, or impractical datasets, hindering progress in ensuring reliability, security, and robustness.

Method: The authors created two datasets: (1) TOFU-R, a snapshot of open-source Rasa chatbots on GitHub, and (2) BRASATO, a curated collection prioritizing dialogue/functional complexity and utility. They also provided tools for maintaining these datasets.

Result: TOFU-R captures real-world open-source practices, while BRASATO offers a focused resource for studying chatbot quality. These datasets facilitate reproducibility and benchmarking in reliability research.

Conclusion: The paper concludes that the presented datasets (TOFU-R and BRASATO) address the critical need for large-scale, high-quality resources, enabling robust evaluation and reproducibility in chatbot reliability research.

Abstract: Task-based chatbots are increasingly being used to deliver real services, yet
assessing their reliability, security, and robustness remains underexplored,
also due to the lack of large-scale, high-quality datasets. The emerging
automated quality assessment techniques targeting chatbots often rely on
limited pools of subjects, such as custom-made toy examples, or outdated, no
longer available, or scarcely popular agents, complicating the evaluation of
such techniques. In this paper, we present two datasets and the tool support
necessary to create and maintain these datasets. The first dataset is RASA
TASK-BASED CHATBOTS FROM GITHUB (TOFU-R), which is a snapshot of the Rasa
chatbots available on GitHub, representing the state of the practice in
open-source chatbot development with Rasa. The second dataset is BOT RASA
COLLECTION (BRASATO), a curated selection of the most relevant chatbots for
dialogue complexity, functional complexity, and utility, whose goal is to ease
reproducibility and facilitate research on chatbot reliability.

</details>


### [9] [Evaluation Guidelines for Empirical Studies in Software Engineering involving LLMs](https://arxiv.org/abs/2508.15503)
*Sebastian Baltes,Florian Angermeir,Chetan Arora,Marvin Muñoz Barón,Chunyang Chen,Lukas Böhme,Fabio Calefato,Neil Ernst,Davide Falessi,Brian Fitzgerald,Davide Fucci,Marcos Kalinowski,Stefano Lambiase,Daniel Russo,Mircea Lungu,Lutz Prechelt,Paul Ralph,Christoph Treude,Stefan Wagner*

Main category: cs.SE

TL;DR: This paper introduces a taxonomy and eight guidelines for ensuring reproducibility and transparency in empirical studies using Large Language Models (LLMs) in software engineering, addressing LLM-specific barriers like non-determinism and opaque training data.


<details>
  <summary>Details</summary>
Motivation: Integration of LLMs in software engineering research creates challenges for reproducibility and replication due to their non-determinism, lack of transparent training data, and rapidly changing architectures.

Method: Presented a community-derived taxonomy of LLM-based study types along with eight guidelines. The guidelines combine essential (must) and desired (should) criteria, covering aspects like LLM declaration, version reporting, documentation, and benchmarking.

Result: The proposed taxonomy and guidelines, hosted as a living online resource (llm-guidelines.org), provide a framework for designing, reporting, and evaluating LLM-driven empirical research in SE, focusing on transparency and baselines.

Conclusion: The study advocates for a structured approach to LLM-based SE research to overcome barriers to open science, emphasizing that declaring LLM usage, disclosing details, and articulating limitations can improve reproducibility and replicability despite the challenges posed by LLMs.

Abstract: Large language models (LLMs) are increasingly being integrated into software
engineering (SE) research and practice, yet their non-determinism, opaque
training data, and evolving architectures complicate the reproduction and
replication of empirical studies. We present a community effort to scope this
space, introducing a taxonomy of LLM-based study types together with eight
guidelines for designing and reporting empirical studies involving LLMs. The
guidelines present essential (must) criteria as well as desired (should)
criteria and target transparency throughout the research process. Our
recommendations, contextualized by our study types, are: (1) to declare LLM
usage and role; (2) to report model versions, configurations, and fine-tuning;
(3) to document tool architectures; (4) to disclose prompts and interaction
logs; (5) to use human validation; (6) to employ an open LLM as a baseline; (7)
to report suitable baselines, benchmarks, and metrics; and (8) to openly
articulate limitations and mitigations. Our goal is to enable reproducibility
and replicability despite LLM-specific barriers to open science. We maintain
the study types and guidelines online as a living resource for the community to
use and shape (llm-guidelines.org).

</details>
