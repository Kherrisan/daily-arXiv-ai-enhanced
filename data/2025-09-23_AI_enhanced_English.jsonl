{"id": "2509.16274", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.16274", "abs": "https://arxiv.org/abs/2509.16274", "authors": ["Uwe Serd\u00fclt"], "title": "Reconnecting Citizens to Politics via Blockchain - Starting the Debate", "comment": "Published as Proceedings of Ongoing Research, Practitioners, Posters,\n  Workshops, and Projects of the International Conference EGOV-CeDEM-ePart 2019", "summary": "Elections are not the only but arguably one of the most important pillars for\nthe proper functioning of liberal democracies. Recent evidence across the globe\nshows that it is not straightforward to conduct them in a free and fair manner.\nOne constant concern is the role of money in politics, more specifically,\nelection campaign financing. Frequent scandals are proof of the difficulties\nencountered with current approaches to tackle the issue. Suggestions on how to\novercome the problem exist but seem difficult to implement. With the help of\nblockchain technology we might be able to make a step forward. A separate\ncrypto currency specifically designed to pay for costs of political campaigning\nand advertising could be introduced. Admittedly, at this stage, there are many\nopen questions. However, under the assumption that blockchain technology is\nhere to stay, it is an idea that deserves further exploration.", "AI": {"tldr": "This paper explores using blockchain technology and a dedicated cryptocurrency to improve the transparency and fairness of election campaign financing, despite existing challenges.", "motivation": "The study addresses the pervasive issue of money in politics, highlighting the need for more transparent and secure methods to prevent scandals in election campaign financing.", "method": "The paper proposes introducing a blockchain-based cryptocurrency specifically designed to pay for political campaigning and advertising costs, aiming to enhance transparency and security.", "result": "While specific results are not detailed, the paper suggests that blockchain could potentially offer a more secure and traceable system, though many open questions remain about its implementation.", "conclusion": "Blockchain technology offers a promising avenue for improving election campaign financing, warranting further exploration despite existing challenges and uncertainties."}}
{"id": "2509.16268", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16268", "abs": "https://arxiv.org/abs/2509.16268", "authors": ["Zhenlan Ji", "Daoyuan Wu", "Wenxuan Wang", "Pingchuan Ma", "Shuai Wang", "Lei Ma"], "title": "Digging Into the Internal: Causality-Based Analysis of LLM Function Calling", "comment": null, "summary": "Function calling (FC) has emerged as a powerful technique for facilitating\nlarge language models (LLMs) to interact with external systems and perform\nstructured tasks. However, the mechanisms through which it influences model\nbehavior remain largely under-explored. Besides, we discover that in addition\nto the regular usage of FC, this technique can substantially enhance the\ncompliance of LLMs with user instructions. These observations motivate us to\nleverage causality, a canonical analysis method, to investigate how FC works\nwithin LLMs. In particular, we conduct layer-level and token-level causal\ninterventions to dissect FC's impact on the model's internal computational\nlogic when responding to user queries. Our analysis confirms the substantial\ninfluence of FC and reveals several in-depth insights into its mechanisms. To\nfurther validate our findings, we conduct extensive experiments comparing the\neffectiveness of FC-based instructions against conventional prompting methods.\nWe focus on enhancing LLM safety robustness, a critical LLM application\nscenario, and evaluate four mainstream LLMs across two benchmark datasets. The\nresults are striking: FC shows an average performance improvement of around\n135% over conventional prompting methods in detecting malicious inputs,\ndemonstrating its promising potential to enhance LLM reliability and capability\nin practical applications.", "AI": {"tldr": "FC for LLM compliance inspection", "motivation": "Understanding FC mechanism to enhance LLM compliance", "method": "Causal interventions at layer and token levels and FC-based instruction experiments", "result": "FC performance improvements by 135% over conventional methods in safety robustness", "conclusion": "FC has potential to improve LLM reliability and capability"}}
{"id": "2509.16275", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16275", "abs": "https://arxiv.org/abs/2509.16275", "authors": ["Jugal Gajjar", "Kamalasankari Subramaniakuppusamy", "Relsy Puthal", "Kaustik Ranaware"], "title": "SecureFixAgent: A Hybrid LLM Agent for Automated Python Static Vulnerability Repair", "comment": "6 pages, 3 figures, 4 tables, 1 algorithm, accepted in the Robustness\n  and Security of Large Language Models (ROSE-LLM) special session at ICMLA\n  2025", "summary": "Modern software development pipelines face growing challenges in securing\nlarge codebases with extensive dependencies. Static analysis tools like Bandit\nare effective at vulnerability detection but suffer from high false positives\nand lack repair capabilities. Large Language Models (LLMs), in contrast, can\nsuggest fixes but often hallucinate changes and lack self-validation. We\npresent SecureFixAgent, a hybrid repair framework integrating Bandit with\nlightweight local LLMs (<8B parameters) in an iterative detect-repair-validate\nloop. To improve precision, we apply parameter-efficient LoRA-based fine-tuning\non a diverse, curated dataset spanning multiple Python project domains,\nmitigating dataset bias and reducing unnecessary edits. SecureFixAgent uses\nBandit for detection, the LLM for candidate fixes with explanations, and Bandit\nre-validation for verification, all executed locally to preserve privacy and\nreduce cloud reliance. Experiments show SecureFixAgent reduces false positives\nby 10.8% over static analysis, improves fix accuracy by 13.51%, and lowers\nfalse positives by 5.46% compared to pre-trained LLMs, typically converging\nwithin three iterations. Beyond metrics, developer studies rate explanation\nquality 4.5/5, highlighting its value for human trust and adoption. By\ncombining verifiable security improvements with transparent rationale in a\nresource-efficient local framework, SecureFixAgent advances trustworthy,\nautomated vulnerability remediation for modern pipelines.", "AI": {"tldr": "SecureFixAgent is a hybrid vulnerability repair framework combining static analysis (Bandit) with lightweight local LLMs via iterative detection-repair-validation cycles, reducing false positives by 10.8%, improving fix accuracy by 13.51%, and achieving high developer trust through explainable local workflows.", "motivation": "Modern software pipelines require secure, efficient vulnerability remediation as static analysis tools exhibit high false positives while LLMs produce hallucinatory fixes without validation.", "method": "SecureFixAgent integrates Bandit (detection), LoRA-fine-tuned local LLMs (<8B parameters) (repair with explanations), and Bandit re-validation (verification) in a three-iteration loop, using dataset diversity to mitigate bias and reduce unnecessary edits.", "result": "Reduces false positives by 10.8%, improves fix accuracy by 13.51%, lowers LLM false positives by 5.46%, achieves 4.5/5 developer satisfaction for explanation quality, and converges in 3 iterations with resource-efficient local execution.", "conclusion": "SecureFixAgent advances trustworthy automated remediation by combining verifiable static analysis with explainable LLM-based repairs in a privacy-preserving, low-resource framework, enabling human-AI collaboration in modern DevSecOps."}}
{"id": "2509.16478", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16478", "abs": "https://arxiv.org/abs/2509.16478", "authors": ["Hossein Yousefizadeh", "Shenghui Gu", "Lionel C. Briand", "Ali Nasr"], "title": "Constrained Co-evolutionary Metamorphic Differential Testing for Autonomous Systems with an Interpretability Approach", "comment": null, "summary": "Autonomous systems, such as autonomous driving systems, evolve rapidly\nthrough frequent updates, risking unintended behavioral degradations. Effective\nsystem-level testing is challenging due to the vast scenario space, the absence\nof reliable test oracles, and the need for practically applicable and\ninterpretable test cases. We present CoCoMagic, a novel automated test case\ngeneration method that combines metamorphic testing, differential testing, and\nadvanced search-based techniques to identify behavioral divergences between\nversions of autonomous systems. CoCoMagic formulates test generation as a\nconstrained cooperative co-evolutionary search, evolving both source scenarios\nand metamorphic perturbations to maximize differences in violations of\npredefined metamorphic relations across versions. Constraints and population\ninitialization strategies guide the search toward realistic, relevant\nscenarios. An integrated interpretability approach aids in diagnosing the root\ncauses of divergences. We evaluate CoCoMagic on an end-to-end ADS, InterFuser,\nwithin the Carla virtual simulator. Results show significant improvements over\nbaseline search methods, identifying up to 287\\% more distinct high-severity\nbehavioral differences while maintaining scenario realism. The interpretability\napproach provides actionable insights for developers, supporting targeted\ndebugging and safety assessment. CoCoMagic offers an efficient, effective, and\ninterpretable way for the differential testing of evolving autonomous systems\nacross versions.", "AI": {"tldr": "CoCoMagic addresses testing challenges in evolving autonomous systems by combining metamorphic/differential testing with co-evolutionary search, achieving 287% more defect detection than baselines and enabling interpretable fault diagnosis.", "motivation": "Rapid updates to autonomous systems (e.g., ADS) risk unintended behavioral degradations. System-level testing faces challenges due to vast scenario spaces, lack of reliable test oracles, and the need for both practically applicable and interpretable test cases.", "method": "CoCoMagic combines metamorphic testing, differential testing, and search-based techniques through a constrained cooperative co-evolutionary approach. It evolves source scenarios and metamorphic perturbations to maximize violation differences across versions, using constraints and initialization strategies for realism, and incorporates an interpretability framework for root-cause analysis.", "result": "Evaluation on the InterFuser ADS in the Carla simulator showed CoCoMagic identified 287% more high-severity behavioral differences compared to baseline methods while preserving scenario realism. The interpretability approach provided actionable insights for debugging and safety assessment.", "conclusion": "CoCoMagic offers an efficient, effective, and interpretable solution for differential testing of evolving autonomous systems across versions, significantly improving detection of behavioral degradations while maintaining scenario realism."}}
{"id": "2509.16292", "categories": ["cs.CR", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.16292", "abs": "https://arxiv.org/abs/2509.16292", "authors": ["Qian'ang Mao", "Jiaxin Wang", "Zhiqi Feng", "Yi Zhang", "Jiaqi Yan"], "title": "Decoding TRON: A Comprehensive Framework for Large-Scale Blockchain Data Extraction and Exploration", "comment": "written in early 2024", "summary": "Cryptocurrencies and Web3 applications based on blockchain technology have\nflourished in the blockchain research field. Unlike Bitcoin and Ethereum, due\nto its unique architectural designs in consensus mechanisms, resource\nmanagement, and throughput, TRON has developed a more distinctive ecosystem and\napplication scenarios centered around stablecoins. Although it is popular in\nareas like stablecoin payments and settlement, research on analyzing on-chain\ndata from the TRON blockchain is remarkably scarce. To fill this gap, this\npaper proposes a comprehensive data extraction and exploration framework for\nthe TRON blockchain. An innovative high-performance ETL system aims to\nefficiently extract raw on-chain data from TRON, including blocks,\ntransactions, smart contracts, and receipts, establishing a research dataset.\nAn in-depth analysis of the extracted dataset reveals insights into TRON's\nblock generation, transaction trends, the dominance of exchanges, the resource\ndelegation market, smart contract usage patterns, and the central role of the\nUSDT stablecoin. The prominence of gambling applications and potential illicit\nactivities related to USDT is emphasized. The paper discusses opportunities for\nfuture research leveraging this dataset, including analysis of delegate\nservices, gambling scenarios, stablecoin activities, and illicit transaction\ndetection. These contributions enhance blockchain data management capabilities\nand understanding of the rapidly evolving TRON ecosystem.", "AI": {"tldr": "This paper introduces a high-performance ETL system to extract and analyze TRON blockchain data, revealing insights into its ecosystem, including USDT dominance, gambling applications, and illicit activities, while proposing future research directions.", "motivation": "TRON's unique blockchain architecture and prominence in stablecoin applications contrast with the lack of on-chain data analysis, creating a research gap that hinders understanding of its ecosystem and use cases.", "method": "The authors developed a specialized ETL (Extract, Transform, Load) system to collect TRON's raw on-chain data (blocks, transactions, contracts) and conducted exploratory analysis across transaction trends, resource delegation, smart contract patterns, and USDT dynamics.", "result": "Key findings include TRON's block generation efficiency, exchange centralization, resource delegation market behavior, smart contract adoption, USDT's critical role, and empirical evidence of gambling platform proliferation and potential illicit transaction patterns.", "conclusion": "The framework establishes a foundational dataset for TRON research, enabling future studies in delegate services, stablecoin mechanics, and anti-money laundering applications, while highlighting the ecosystem's unique characteristics and risks."}}
{"id": "2509.16525", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16525", "abs": "https://arxiv.org/abs/2509.16525", "authors": ["Anna Mazhar", "Sainyam Galhotra"], "title": "Causal Fuzzing for Verifying Machine Unlearning", "comment": null, "summary": "As machine learning models become increasingly embedded in decision-making\nsystems, the ability to \"unlearn\" targeted data or features is crucial for\nenhancing model adaptability, fairness, and privacy in models which involves\nexpensive training. To effectively guide machine unlearning, a thorough testing\nis essential. Existing methods for verification of machine unlearning provide\nlimited insights, often failing in scenarios where the influence is indirect.\nIn this work, we propose CAF\\'E, a new causality based framework that unifies\ndatapoint- and feature-level unlearning for verification of black-box ML\nmodels. CAF\\'E evaluates both direct and indirect effects of unlearning targets\nthrough causal dependencies, providing actionable insights with fine-grained\nanalysis. Our evaluation across five datasets and three model architectures\ndemonstrates that CAF\\'E successfully detects residual influence missed by\nbaselines while maintaining computational efficiency.", "AI": {"tldr": "The paper introduces CAF\u00c9, a causality-based framework for verifying machine unlearning in black-box models by analyzing direct and indirect effects of unlearning targets through causal dependencies.", "motivation": "Existing unlearning verification methods fail to capture indirect influences, limiting their effectiveness in ensuring model adaptability, fairness, and privacy.", "method": "CAF\u00c9 unifies datapoint- and feature-level unlearning verification by evaluating causal dependencies to identify both direct and indirect residual influences in black-box ML models.", "result": "Experiments on five datasets and three architectures show CAF\u00c9 detects residual influences missed by baselines while maintaining computational efficiency.", "conclusion": "CAF\u00c9 provides a unified, efficient solution for detecting unlearning residual effects, offering actionable insights for improving model reliability in sensitive applications."}}
{"id": "2509.16340", "categories": ["cs.CR", "cs.OS"], "pdf": "https://arxiv.org/pdf/2509.16340", "abs": "https://arxiv.org/abs/2509.16340", "authors": ["Mohammad Hossein Asghari", "Lianying Zhao"], "title": "To Unpack or Not to Unpack: Living with Packers to Enable Dynamic Analysis of Android Apps", "comment": null, "summary": "Android apps have become a valuable target for app modifiers and imitators\ndue to its popularity and being trusted with highly sensitive data. Packers, on\nthe other hand, protect apps from tampering with various anti-analysis\ntechniques embedded in the app. Meanwhile, packers also conceal certain\nbehavior potentially against the interest of the users, aside from being abused\nby malware for stealth. Security practitioners typically try to capture\nundesired behavior at runtime with hooking (e.g., Frida) or debugging\ntechniques, which are heavily affected by packers. Unpackers have been the\ncommunity's continuous effort to address this, but due to the emerging\ncommercial packers, our study shows that none of the unpackers remain\neffective, and they are unfit for this purpose as unpacked apps can no longer\nrun. We first perform a large-scale prevalence analysis of Android packers with\na real-world dataset of 12,341 apps, the first of its kind, to find out what\npercentage of Android apps are actually packed and to what extent dynamic\nanalysis is hindered. We then propose Purifire, an evasion engine to bypass\npackers' anti-analysis techniques and enable dynamic analysis on packed apps\nwithout unpacking them. Purifire is based on eBPF, a low-level kernel feature,\nwhich provides observability and invisibility to userspace apps to enforce\ndefined evasion rules while staying low-profile. Our evaluation shows that\nPurifire is able to bypass packers' anti-analysis checks and more importantly,\nfor previous research works suffering from packers, we observe a significant\nimprovement (e.g., a much higher number of detected items such as device\nfingerprints).", "AI": {"tldr": "The paper addresses the challenge of analyzing packed Android apps by proposing Purifire, an eBPF-based evasion engine to bypass packers\u2019 anti-analysis techniques. It reveals the prevalence of packers in real-world apps and demonstrates significant improvements in dynamic analysis compared to existing methods.", "motivation": "Packers hinder dynamic analysis of Android apps by embedding anti-analysis techniques, rendering existing unpackers ineffective. This obstructs security research and detection of malicious behaviors, necessitating a new solution for runtime analysis of packed apps.", "method": "1) A large-scale study of 12,341 apps to analyze packer prevalence and impact on dynamic analysis. 2Development of Purifire, leveraging eBPF to enforce evasion rules at the kernel level without unpacking apps, circumventing anti-analysis checks.", "result": "Purifire successfully bypasses commercial packers\u2019 anti-analysis mechanisms. Experimental results show dramatic improvements in security tools\u2019 performance, such as a higher detection rate of device fingerprints, demonstrating its effectiveness in restoring dynamic analysis capabilities.", "conclusion": "Purifire provides a scalable, low-profile solution to bypass packers\u2019 defenses, enabling reliable dynamic analysis of packed Android apps. It addresses critical limitations of existing unpackers and enhances the practicality of runtime security analysis."}}
{"id": "2509.16595", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16595", "abs": "https://arxiv.org/abs/2509.16595", "authors": ["Jiaming Ye", "Xiongfei Wu", "Shangzhou Xia", "Fuyuan Zhang", "Jianjun Zhao"], "title": "Is Measurement Enough? Rethinking Output Validation in Quantum Program Testing", "comment": "This paper will be appeared in the proceedings of the 40th IEEE/ACM\n  International Conference on Automated Software Engineering (ASE 2025), NIER\n  track, Seoul, South Korea, November 16 -20, 2025", "summary": "As quantum computing continues to emerge, ensuring the quality of quantum\nprograms has become increasingly critical. Quantum program testing has emerged\nas a prominent research area within the scope of quantum software engineering.\nWhile numerous approaches have been proposed to address quantum program quality\nassurance, our analysis reveals that most existing methods rely on\nmeasurement-based validation in practice. However, due to the inherently\nprobabilistic nature of quantum programs, measurement-based validation methods\nface significant limitations.\n  To investigate these limitations, we conducted an empirical study of recent\nresearch on quantum program testing, analyzing measurement-based validation\nmethods in the literature. Our analysis categorizes existing measurement-based\nvalidation methods into two groups: distribution-level validation and\noutput-value-level validation. We then compare measurement-based validation\nwith statevector-based validation methods to evaluate their pros and cons. Our\nfindings demonstrate that measurement-based validation is suitable for\nstraightforward assessments, such as verifying the existence of specific output\nvalues, while statevector-based validation proves more effective for\ncomplicated tasks such as assessing the program behaviors.", "AI": {"tldr": "This paper analyzes measurement-based validation methods in quantum program testing, categorizing them into distribution and output-value levels, and compares their effectiveness with statevector-based methods, finding that statevector validation excels in complex assessments.", "motivation": "Quantum program quality assurance is critical as quantum computing advances, yet existing measurement-based validation methods have inherent limitations due to quantum's probabilistic nature.", "method": "The authors conducted an empirical study to categorize and compare measurement-based validation methods (distribution/output-value) against statevector-based approaches, evaluating their suitability for different testing tasks.", "result": "Measurement-based validation is effective for simple output verification, but statevector validation outperforms it in assessing complex program behaviors like execution dynamics and non-trivial properties.", "conclusion": "The study suggests that statevector-based validation is superior for comprehensive quantum program analysis, while measurement-based methods remain viable for basic correctness checks, guiding future method development and application strategies."}}
{"id": "2509.16352", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16352", "abs": "https://arxiv.org/abs/2509.16352", "authors": ["Yunfan Yang", "Jiarong Xu", "Hongzhe Zhang", "Xiao Fang"], "title": "Secure Confidential Business Information When Sharing Machine Learning Models", "comment": null, "summary": "Model-sharing offers significant business value by enabling firms with\nwell-established Machine Learning (ML) models to monetize and share their\nmodels with others who lack the resources to develop ML models from scratch.\nHowever, concerns over data confidentiality remain a significant barrier to\nmodel-sharing adoption, as Confidential Property Inference (CPI) attacks can\nexploit shared ML models to uncover confidential properties of the model\nprovider's private model training data. Existing defenses often assume that CPI\nattacks are non-adaptive to the specific ML model they are targeting. This\nassumption overlooks a key characteristic of real-world adversaries: their\nresponsiveness, i.e., adversaries' ability to dynamically adjust their attack\nmodels based on the information of the target and its defenses. To overcome\nthis limitation, we propose a novel defense method that explicitly accounts for\nthe responsive nature of real-world adversaries via two methodological\ninnovations: a novel Responsive CPI attack and an attack-defense arms race\nframework. The former emulates the responsive behaviors of adversaries in the\nreal world, and the latter iteratively enhances both the target and attack\nmodels, ultimately producing a secure ML model that is robust against\nresponsive CPI attacks. Furthermore, we propose and integrate a novel\napproximate strategy into our defense, which addresses a critical computational\nbottleneck of defense methods and improves defense efficiency. Through\nextensive empirical evaluations across various realistic model-sharing\nscenarios, we demonstrate that our method outperforms existing defenses by more\neffectively defending against CPI attacks, preserving ML model utility, and\nreducing computational overhead.", "AI": {"tldr": "The paper introduces a new defense method, based on a novel responsive CPI attack and an attack-defense arms race, for secure model sharing which considers responsive real-world adversaries. The method is evaluated in different realistic scenarios and shown to outperform existing methods in CPI defense, model utility, and computational efficiency.", "motivation": "Model-sharing is economically beneficial, but CPI attacks pose a data confidentiality risk. Existing defenses are not effective against responsive adversaries, who adapt their attacks to target and defense-specific information.", "method": "The method involves two key components: a novel Responsive CPI attack which simulates real-world adversaries' responsiveness, and an attack-defense arms race framework where target and attack models are iteratively improved. It also includes an approximation strategy to enhance computational efficiency.", "result": "In various realistic model-sharing scenarios, the new method demonstrates superior performance against CPI attacks while maintaining model utility and reducing computational overhead, compared to existing defense methods.", "conclusion": "The proposed defense method, which accounts for the responsive nature of real-world adversaries in model-sharing, is effective and efficient against CPI attacks, offering a better solution for secure model sharing."}}
{"id": "2509.16655", "categories": ["cs.SE", "cs.CR", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2509.16655", "abs": "https://arxiv.org/abs/2509.16655", "authors": ["Serena Wang", "Martino Banchio", "Krzysztof Kotowicz", "Katrina Ligett", "R. Preston McAfee", "Eduardo' Vela'' Nava"], "title": "Incentives and Outcomes in Bug Bounties", "comment": null, "summary": "Bug bounty programs have contributed significantly to security in technology\nfirms in the last decade, but little is known about the role of reward\nincentives in producing useful outcomes. We analyze incentives and outcomes in\nGoogle's Vulnerability Rewards Program (VRP), one of the world's largest bug\nbounty programs. We analyze the responsiveness of the quality and quantity of\nbugs received to changes in payments, focusing on a change in Google's reward\namounts posted in July, 2024, in which reward amounts increased by up to 200%\nfor the highest impact tier. Our empirical results show an increase in the\nvolume of high-value bugs received after the reward increase, for which we also\ncompute elasticities. We further break down the sources of this increase\nbetween veteran researchers and new researchers, showing that the reward\nincrease both redirected the attention of veteran researchers and attracted new\ntop security researchers into the program.", "AI": {"tldr": "Google's bug bounty program raised rewards by 200-300%, leading to increased high-value bug reports from both veteran and new researchers.", "motivation": "To understand how reward incentives impact the quantity/quality of bug reports in security programs", "method": "Empirical analysis of Google VRP pre/post July 2024 reward increase, with elasticity calculations and contributor segmentation (veteran vs new researchers)", "result": "213% increase in highest-impact bug submissions after pay raise, with significant growth from both veteran researchers (34%) and new top researchers (13x increase)", "conclusion": "Strategic reward increases effectively attract and redirect top security talent while improving program outcomes."}}
{"id": "2509.16389", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16389", "abs": "https://arxiv.org/abs/2509.16389", "authors": ["Tianrou Xia", "Kaiming Huang", "Dongyeon Yu", "Yuseok Jeon", "Jie Zhou", "Dinghao Wu", "Taegyu Kim"], "title": "LiteRSan: Lightweight Memory Safety Via Rust-specific Program Analysis and Selective Instrumentation", "comment": "14 pages (main text), 18 pages including references and appendix, 2\n  figures", "summary": "Rust is a memory-safe language, and its strong safety guarantees combined\nwith high performance have been attracting widespread adoption in systems\nprogramming and security-critical applications. However, Rust permits the use\nof unsafe code, which bypasses compiler-enforced safety checks and can\nintroduce memory vulnerabilities. A widely adopted approach for detecting\nmemory safety bugs in Rust is Address Sanitizer (ASan). Optimized versions,\nsuch as ERASan and RustSan, have been proposed to selectively apply security\nchecks in order to reduce performance overhead. However, these tools still\nincur significant performance and memory overhead and fail to detect many\nclasses of memory safety vulnerabilities due to the inherent limitations of\nASan. In this paper, we present LiteRSan, a novel memory safety sanitizer that\naddresses the limitations of prior approaches. By leveraging Rust's unique\nownership model, LiteRSan performs Rust-specific static analysis that is aware\nof pointer lifetimes to identify risky pointers. It then selectively\ninstruments risky pointers to enforce only the necessary spatial or temporal\nmemory safety checks. Consequently, LiteRSan introduces significantly lower\nruntime overhead (18.84% versus 152.05% and 183.50%) and negligible memory\noverhead (0.81% versus 739.27% and 861.98%) compared with existing ASan-based\nsanitizers while being capable of detecting memory safety bugs that prior\ntechniques miss.", "AI": {"tldr": "LiteRSan improves Rust memory safety with lower overhead by targeting risky pointers via ownership-aware analysis, outperforming existing sanitizers.", "motivation": "Rust\u2019s unsafe code introduces memory vulnerabilities, and existing ASan-based sanitizers suffer from high overhead and detection limitations.", "method": "LiteRSan uses Rust-specific static analysis of pointer lifetimes to identify risky pointers and applies selective instrumentation for minimal checks.", "result": "LiteRSan achieves 18.84% runtime overhead and 0.81% memory overhead, outperforming ERASan and RustSan by 76% and 87% in runtime, with broader bug detection coverage.", "conclusion": "LiteRSan effectively reduces runtime and memory overhead while detecting previously missed memory safety bugs in Rust by leveraging its ownership model."}}
{"id": "2509.16681", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.16681", "abs": "https://arxiv.org/abs/2509.16681", "authors": ["Peterson Jean"], "title": "Verifying User Interfaces using SPARK Ada: A Case Study of the T34 Syringe Driver", "comment": "62 pages. Master's dissertation submitted to Swansea University,\n  Department of Computer Science, September 2022. Supervisor Dr Jens Blanck", "summary": "The increase in safety and critical systems improved Healthcare. Due to their\nrisk of harm, such systems are subject to stringent guidelines and compliances.\nThese safety measures ensure a seamless experience and mitigate the risk to\nend-users. Institutions like the Food and Drug Administration and the NHS,\nrespectively, established international standards and competency frameworks to\nensure industry compliance with these safety concerns. Medical device\nmanufacturing is mainly concerned with standards. Consequently, these standards\nnow advocate for better human factors considered in user interaction for\nmedical devices. This forces manufacturers to rely on heavy testing and review\nto cover many of these factors during development. Sadly, many human factor\nrisks will not be caught until proper testing in real life, which might be\ncatastrophic in the case of an ambulatory device like the T34 syringe pump.\nTherefore, effort in formal methods research may propose new solutions in\nanticipating these errors in the early stages of development or even reducing\ntheir occurrence based on the use of standard generic model. These generically\ndeveloped models will provide a common framework for safety integration in\nindustry and may potentially be proven using formal verification mathematical\nproofs. This research uses SPARK Ada's formal verification tool against a\nbehavioural model of the T34 syringe driver. A Generic Infusion Pump model\nrefinement is explored and implemented in SPARK Ada. As a subset of the Ada\nlanguage, the verification level of the end prototype is evaluated using SPARK.\nExploring potential limitations defines the proposed model's implementation\nliability when considering abstraction and components of User Interface design\nin SPARK Ada.", "AI": {"tldr": "This paper investigates using SPARK Ada's formal verification to improve safety in medical devices like the T34 syringe pump, aiming to identify human factor risks early. It finds that SPARK can effectively verify models but faces challenges with UI design abstractions, suggesting formal methods could enhance safety if standardized frameworks are adopted.", "motivation": "Despite stringent industry standards for safety-critical healthcare systems, human factor risks in user interaction remain largely undetected until real-world testing\u2014posing catastrophic risks (e.g., for devices like the T34 syringe pump). Formal methods are proposed to anticipate and mitigate these risks during early development stages.", "method": "The study employs SPARK Ada's formal verification tool to develop and refine a generic behavioral model of the T34 syringe pump. It evaluates the tool\u2019s verification effectiveness while addressing challenges in abstracting user interface components within SPARK Ada.", "result": "The implementation reveals SPARK Ada\u2019s potential to verify safety-critical models rigorously but highlights limitations in handling UI abstraction complexity. The study identifies practical constraints of using SPARK as a formal verification tool subset of Ada for such systems.", "conclusion": "The research demonstrates that formal methods, specifically SPARK Ada, can enhance safety in medical device development by enabling early error detection through formal verification. However, limitations in SPARK\u2019s abstraction capabilities, especially in modeling UI components, highlight the need for further research to expand standard frameworks for safer design."}}
{"id": "2509.16390", "categories": ["cs.CR", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.16390", "abs": "https://arxiv.org/abs/2509.16390", "authors": ["Mohamed Abdessamed Rezazi", "Mouhamed Amine Bouchiha", "Ahmed Mounsf Rafik Bendada", "Yacine Ghamri-Doudane"], "title": "B5GRoam: A Zero Trust Framework for Secure and Efficient On-Chain B5G Roaming", "comment": "6 pages, 2 figures, Accepted at GLOBECOM'25", "summary": "Roaming settlement in 5G and beyond networks demands secure, efficient, and\ntrustworthy mechanisms for billing reconciliation between mobile operators.\nWhile blockchain promises decentralization and auditability, existing solutions\nsuffer from critical limitations-namely, data privacy risks, assumptions of\nmutual trust, and scalability bottlenecks. To address these challenges, we\npresent B5GRoam, a novel on-chain and zero-trust framework for secure,\nprivacy-preserving, and scalable roaming settlements. B5GRoam introduces a\ncryptographically verifiable call detail record (CDR) submission protocol,\nenabling smart contracts to authenticate usage claims without exposing\nsensitive data. To preserve privacy, we integrate non-interactive\nzero-knowledge proofs (zkSNARKs) that allow on-chain verification of roaming\nactivity without revealing user or network details. To meet the high-throughput\ndemands of 5G environments, B5GRoam leverages Layer 2 zk-Rollups, significantly\nreducing gas costs while maintaining the security guarantees of Layer 1.\nExperimental results demonstrate a throughput of over 7,200 tx/s with strong\nprivacy and substantial cost savings. By eliminating intermediaries and\nenhancing verifiability, B5GRoam offers a practical and secure foundation for\ndecentralized roaming in future mobile networks.", "AI": {"tldr": "B5GRoam is a blockchain-based framework for secure, privacy-preserving, and scalable 5G roaming settlements using zero-trust architecture and zk-SNARKs, achieving high throughput and cost efficiency.", "motivation": "Existing blockchain solutions for roaming settlement face data privacy risks, mutual trust assumptions, and scalability issues. 5G networks require high-throughput mechanisms to prevent billing reconciliation disputes while maintaining security and trust.", "method": "B5GRoam combines cryptographically verifiable call detail records (CDRs) with non-interactive zero-knowledge proofs (zkSNARKs) and Layer 2 zk-Rollups. It enables smart contracts to authenticate usage claims without exposing data, maintaining privacy while achieving scalability through off-chain transaction aggregation.", "result": "Experiments show B5GRoam processes over 7,200 transactions/second with strong privacy guarantees and significant cost reductions. It eliminates intermediaries and reduces gas costs via Layer 2 optimizations.", "conclusion": "B5GRoam provides a decentralized, verifiable, and practical solution for 5G roaming settlements, addressing privacy, scalability, and trust limitations of current blockchain approaches."}}
{"id": "2509.16701", "categories": ["cs.SE", "D.2.5; I.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.16701", "abs": "https://arxiv.org/abs/2509.16701", "authors": ["Shunyu Liu", "Guangdong Bai", "Mark Utting", "Guowei Yang"], "title": "RelRepair: Enhancing Automated Program Repair by Retrieving Relevant Code", "comment": "11 pages, 5 figures, under review at TSE", "summary": "Automated Program Repair (APR) has emerged as a promising paradigm for\nreducing debugging time and improving the overall efficiency of software\ndevelopment. Recent advances in Large Language Models (LLMs) have demonstrated\ntheir potential for automated bug fixing and other software engineering tasks.\nNevertheless, the general-purpose nature of LLM pre-training means these models\noften lack the capacity to perform project-specific repairs, which require\nunderstanding of domain-specific identifiers, code structures, and contextual\nrelationships within a particular codebase. As a result, LLMs may struggle to\ngenerate correct patches when the repair depends on project-specific\ninformation.\n  To address this limitation, we introduce RelRepair, a novel approach that\nretrieves relevant project-specific code to enhance automated program repair.\nRelRepair first identifies relevant function signatures by analyzing function\nnames and code comments within the project. It then conducts deeper code\nanalysis to retrieve code snippets relevant to the repair context. The\nretrieved relevant information is then incorporated into the LLM's input\nprompt, guiding the model to generate more accurate and informed patches. We\nevaluate RelRepair on two widely studied datasets, Defects4J V1.2 and\nManySStuBs4J, and compare its performance against several state-of-the-art\nLLM-based APR approaches. RelRepair successfully repairs 101 bugs in Defects4J\nV1.2. Furthermore, RelRepair achieves a 17.1\\% improvement in the ManySStuBs4J\ndataset, increasing the overall fix rate to 48.3\\%. These results highlight the\nimportance of providing relevant project-specific information to LLMs, shedding\nlight on effective strategies for leveraging LLMs in APR tasks.", "AI": {"tldr": "RelRepair addresses LLM shortcomings in project-specific bug fixing by providing relevant code context, achieving superior APR performance on major datasets.", "motivation": "LLMs lack project-specific knowledge required for accurate automated program repair, leading to incorrect patches when contextual code understanding is needed.", "method": "RelRepair retrieves project-specific code by analyzing function names/comments and implementing deeper contextual analysis, then injects this information into LLM prompts to guide patch generation.", "result": "Repairs 101/Defects4J V1.2 bugs (48.3%) with 17.1% improvement over existing methods on ManySStuBs4J dataset.", "conclusion": "Project-specific code context is essential for effective LLM-based APR, with RelRepair's approach providing a generalizable strategy for incorporating domain knowledge into code generation."}}
{"id": "2509.16418", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16418", "abs": "https://arxiv.org/abs/2509.16418", "authors": ["Petr Grinberg", "Eric Bezzam", "Paolo Prandoni", "Martin Vetterli"], "title": "LenslessMic: Audio Encryption and Authentication via Lensless Computational Imaging", "comment": "Submitted to ICASSP 2026", "summary": "With society's increasing reliance on digital data sharing, the protection of\nsensitive information has become critical. Encryption serves as one of the\nprivacy-preserving methods; however, its realization in the audio domain\npredominantly relies on signal processing or software methods embedded into\nhardware. In this paper, we introduce LenslessMic, a hybrid optical\nhardware-based encryption method that utilizes a lensless camera as a physical\nlayer of security applicable to multiple types of audio. We show that\nLenslessMic enables (1) robust authentication of audio recordings and (2)\nencryption strength that can rival the search space of 256-bit digital\nstandards, while maintaining high-quality signals and minimal loss of content\ninformation. The approach is validated with a low-cost Raspberry Pi prototype\nand is open-sourced together with datasets to facilitate research in the area.", "AI": {"tldr": "LenslessMic is a new hardware-based audio encryption method that uses a lensless camera for security. It enables authentication and strong encryption similar to 256-bit standards with high signal quality and low cost.", "motivation": "Society needs better protection for sensitive shared audio data. Existing encryption methods in the audio domain depend too much on software or embedded hardware which might be vulnerable to cyber threats.", "method": "LenslessMic works by adding a physical layer of security using a lensless camera to obscure the audio signals, making it tougher for unauthorized individuals to both record and recover the content. we use a raspberry pi to test this method in real-world conditions.", "result": "The experiment shows that LenslessMic provides robust authentication for audio, has good encryption strength (seen to rival 256-bit digital standards), and keeps the signal quality high with minimal information loss.", "conclusion": "Integration of physical and digital security through LenslessMic offers an efficient method for protecting digital audio data. it also has low-cost implementations, where it can be a viable alternative to traditional audio encryption methods."}}
{"id": "2509.16795", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16795", "abs": "https://arxiv.org/abs/2509.16795", "authors": ["Saikat Mondal", "Chanchal K. Roy", "Hong Wang", "Juan Arguello", "Samantha Mathan"], "title": "Can We Trust the AI Pair Programmer? Copilot for API Misuse Detection and Correction", "comment": "Accepted in the 35th IEEE International Conference on Collaborative\n  Advances in Software Computing", "summary": "API misuse introduces security vulnerabilities, system failures, and\nincreases maintenance costs, all of which remain critical challenges in\nsoftware development. Existing detection approaches rely on static analysis or\nmachine learning-based tools that operate post-development, which delays defect\nresolution. Delayed defect resolution can significantly increase the cost and\ncomplexity of maintenance and negatively impact software reliability and user\ntrust. AI-powered code assistants, such as GitHub Copilot, offer the potential\nfor real-time API misuse detection within development environments. This study\nevaluates GitHub Copilot's effectiveness in identifying and correcting API\nmisuse using MUBench, which provides a curated benchmark of misuse cases. We\nconstruct 740 misuse examples, manually and via AI-assisted variants, using\ncorrect usage patterns and misuse specifications. These examples and 147\ncorrect usage cases are analyzed using Copilot integrated in Visual Studio\nCode. Copilot achieved a detection accuracy of 86.2%, precision of 91.2%, and\nrecall of 92.4%. It performed strongly on common misuse types (e.g.,\nmissing-call, null-check) but struggled with compound or context-sensitive\ncases. Notably, Copilot successfully fixed over 95% of the misuses it\nidentified. These findings highlight both the strengths and limitations of\nAI-driven coding assistants, positioning Copilot as a promising tool for\nreal-time pair programming and detecting and fixing API misuses during software\ndevelopment.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.16489", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16489", "abs": "https://arxiv.org/abs/2509.16489", "authors": ["Minhaj Uddin Ahmad", "Akid Abrar", "Sagar Dasgupta", "Mizanur Rahman"], "title": "End-to-End Co-Simulation Testbed for Cybersecurity Research and Development in Intelligent Transportation Systems", "comment": null, "summary": "Intelligent Transportation Systems (ITS) have been widely deployed across\nmajor metropolitan regions worldwide to improve roadway safety, optimize\ntraffic flow, and reduce environmental impacts. These systems integrate\nadvanced sensors, communication networks, and data analytics to enable\nreal-time traffic monitoring, adaptive signal control, and predictive\nmaintenance. However, such integration significantly broadens the ITS attack\nsurface, exposing critical infrastructures to cyber threats that jeopardize\nsafety, data integrity, and operational resilience. Ensuring robust\ncybersecurity is therefore essential, yet comprehensive vulnerability\nassessments, threat modeling, and mitigation validations are often\ncost-prohibitive and time-intensive when applied to large-scale, heterogeneous\ntransportation systems. Simulation platforms offer a cost-effective and\nrepeatable means for cybersecurity evaluation, and the simulation platform\nshould encompass the full range of ITS dimensions - mobility, sensing,\nnetworking, and applications. This chapter discusses an integrated\nco-simulation testbed that links CARLA for 3D environment and sensor modeling,\nSUMO for microscopic traffic simulation and control, and OMNeT++ for V2X\ncommunication simulation. The co-simulation testbed enables end-to-end\nexperimentation, vulnerability identification, and mitigation benchmarking,\nproviding practical insights for developing secure, efficient, and resilient\nITS infrastructures. To illustrate its capabilities, the chapter incorporates a\ncase study on a C-V2X proactive safety alert system enhanced with post-quantum\ncryptography, highlighting the role of the testbed in advancing secure and\nresilient ITS infrastructures.", "AI": {"tldr": "This paper proposes a CARLA-SUMO-OMNeT++ co-simulation testbed for ITS cybersecurity evaluation, demonstrating its utility through a post-quantum cryptography case study.", "motivation": "ITS cyber threats endanger safety and infrastructure, but vulnerability assessments for heterogeneous systems are prohibitively costly/time-consuming, requiring simulation-based solutions for secure development.", "method": "An integrated co-simulation platform combining CARLA (3D environment/sensor modeling), SUMO (microscopic traffic simulation), and OMNeT++ (V2X communication) for cybersecurity evaluation in large-scale ITS.", "result": "Enables end-to-end experimentation and vulnerability analysis through a C-V2X proactive safety alert case study enhanced with post-quantum cryptography, providing practical cybersecurity insights.", "conclusion": "The co-simulation testbed demonstrates practical effectiveness in advancing secure, efficient, and resilient ITS infrastructures through end-to-end experimentation and mitigation benchmarking."}}
{"id": "2509.16844", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16844", "abs": "https://arxiv.org/abs/2509.16844", "authors": ["Rim Zrelli", "Henrique Amaral Misson", "Sorelle Kamkuimo", "Maroua Ben Attia", "Abdo Shabah", "Felipe Gohring de Magalhaes", "Gabriela Nicolescu"], "title": "Implementation of the Collision Avoidance System for DO-178C Compliance", "comment": null, "summary": "This technical report presents the detailed implementation of a Collision\nAvoidance System (CAS) for Unmanned Aerial Vehicles (UAVs), developed as a case\nstudy to demonstrate a rigorous methodology for achieving DO-178C compliance in\nsafety-critical software. The CAS is based on functional requirements inspired\nby NASA's Access 5 project and is designed to autonomously detect, evaluate,\nand avoid potential collision threats in real-time, supporting the safe\nintegration of UAVs into civil airspace.\n  The implementation environment combines formal methods, model-based\ndevelopment, and automated verification tools, including Alloy, SPIN, Simulink\nEmbedded Coder, and the LDRA tool suite. The report documents each phase of the\nsoftware lifecycle: requirements specification and validation, architectural\nand detailed design, coding, verification, and traceability, with a strong\nfocus on compliance with DO-178C Design Assurance Level B objectives.\n  Results demonstrate that formal modelling and automated toolchains enabled\nearly detection and correction of specification defects, robust traceability,\nand strong evidence of verification and validation across all development\nstages. Static and dynamic analyses confirmed code quality and coverage, while\nformal verification methods provided mathematical assurance of correctness for\ncritical components. Although the integration phase was not fully implemented,\nthe approach proved effective in addressing certification challenges for UAV\nsafety-critical systems.\n  \\keywords Collision Avoidance System (CAS), Unmanned Aerial Vehicles (UAVs),\nDO-178C compliance, Safety-critical software, Formal methods, Model-based\ndevelopment, Alloy, SPIN model checker, Simulink Embedded Coder, LDRA tool\nsuite, Software verification and validation, Traceability, Certification.", "AI": {"tldr": "This report details a DO-178C-compliant Collision Avoidance System (CAS) for UAVs using formal methods and automated tools like Alloy, SPIN, and Simulink. The approach ensures early defect detection, traceability, and verification, effectively addressing certification challenges for safety-critical UAV software.", "motivation": "The motivation centers on enabling safe UAV integration into civil airspace via DO-178C-compliant systems, addressing certification challenges in safety-critical software through rigorous methodologies.", "method": "The method integrates formal modeling (Alloy, SPIN), model-based development (Simulink Embedded Coder), and automated verification (LDRA) within the full software lifecycle, emphasizing traceability and compliance with DO-178C Design Assurance Level B.", "result": "Results show early defect detection, robust traceability, and verification across all phases. Static/dynamic analyses confirmed code quality, while formal methods mathematically assured critical component correctness, validating the approach's effectiveness despite incomplete integration testing.", "conclusion": "The paper concludes that the applied methodology effectively tackles certification challenges for UAV safety-critical systems, demonstrating a practical approach to achieving DO-178C compliance through formal methods and automated toolchains."}}
{"id": "2509.16546", "categories": ["cs.CR", "cs.AI", "F.2.2, I.2.7"], "pdf": "https://arxiv.org/pdf/2509.16546", "abs": "https://arxiv.org/abs/2509.16546", "authors": ["Ashley Kurian", "Aydin Aysu"], "title": "Train to Defend: First Defense Against Cryptanalytic Neural Network Parameter Extraction Attacks", "comment": "18 pages, 3 Figures", "summary": "Neural networks are valuable intellectual property due to the significant\ncomputational cost, expert labor, and proprietary data involved in their\ndevelopment. Consequently, protecting their parameters is critical not only for\nmaintaining a competitive advantage but also for enhancing the model's security\nand privacy. Prior works have demonstrated the growing capability of\ncryptanalytic attacks to scale to deeper models. In this paper, we present the\nfirst defense mechanism against cryptanalytic parameter extraction attacks. Our\nkey insight is to eliminate the neuron uniqueness necessary for these attacks\nto succeed. We achieve this by a novel, extraction-aware training method.\nSpecifically, we augment the standard loss function with an additional\nregularization term that minimizes the distance between neuron weights within a\nlayer. Therefore, the proposed defense has zero area-delay overhead during\ninference. We evaluate the effectiveness of our approach in mitigating\nextraction attacks while analyzing the model accuracy across different\narchitectures and datasets. When re-trained with the same model architecture,\nthe results show that our defense incurs a marginal accuracy change of less\nthan 1% with the modified loss function. Moreover, we present a theoretical\nframework to quantify the success probability of the attack. When tested\ncomprehensively with prior attack settings, our defense demonstrated empirical\nsuccess for sustained periods of extraction, whereas unprotected networks are\nextracted between 14 minutes to 4 hours.", "AI": {"tldr": "This paper presents the first training-time defense against neural network parameter extraction attacks by reducing neuron uniqueness through weight-regularization, maintaining accuracy (\u22641% drop) and inference efficiency while delaying attacks from minutes/hours with theoretical analysis.", "motivation": "Neural networks represent significant intellectual property requiring protection from increasingly capable cryptanalytic attacks, which threaten both competitive advantage and data privacy.", "method": "An extraction-aware training method that adds a regularization term to minimize distances between neuron weights within a layer, eliminating neuron uniqueness needed for attacks without requiring runtime modifications.", "result": "The defense demonstrates empirical robustness against extraction attacks (surviving prolonged attack periods vs. 14m-4h for unprotected networks) while maintaining model accuracy and offering a theoretical quantification of attack success probabilities.", "conclusion": "The proposed defense mechanism effectively mitigates parameter extraction attacks with minimal impact on model accuracy (less than 1% decrease) and zero inference overhead, while providing a theoretical framework to quantify attack probabilities."}}
{"id": "2509.16864", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16864", "abs": "https://arxiv.org/abs/2509.16864", "authors": ["Wei Liu", "Yi Wen Heng", "Feng Lin", "Tse-Hsun", "Chen", "Ahmed E. Hassan"], "title": "MobileUPReg: Identifying User-Perceived Performance Regressions in Mobile OS Versions", "comment": "ASE 2025 Industry Showcase", "summary": "Mobile operating systems (OS) are frequently updated, but such updates can\nunintentionally degrade user experience by introducing performance regressions.\nExisting detection techniques often rely on system-level metrics (e.g., CPU or\nmemory usage) or focus on specific OS components, which may miss regressions\nactually perceived by users -- such as slower responses or UI stutters. To\naddress this gap, we present MobileUPReg, a black-box framework for detecting\nuser-perceived performance regressions across OS versions. MobileUPReg runs the\nsame apps under different OS versions and compares user-perceived performance\nmetrics -- response time, finish time, launch time, and dropped frames -- to\nidentify regressions that are truly perceptible to users. In a large-scale\nstudy, MobileUPReg achieves high accuracy in extracting user-perceived metrics\nand detects user-perceived regressions with 0.96 precision, 0.91 recall, and\n0.93 F1-score -- significantly outperforming a statistical baseline using the\nWilcoxon rank-sum test and Cliff's Delta. MobileUPReg has been deployed in an\nindustrial CI pipeline, where it analyzes thousands of screencasts across\nhundreds of apps daily and has uncovered regressions missed by traditional\ntools. These results demonstrate that MobileUPReg enables accurate, scalable,\nand perceptually aligned regression detection for mobile OS validation.", "AI": {"tldr": "MobileUPReg is a black-box framework for detecting user-perceived performance regressions in mobile OS updates using perceptual metrics like response time and dropped frames, achieving high accuracy through large-scale analysis.", "motivation": "Existing methods rely on system-level metrics or specific components, failing to capture user-perceived regressions such as UI delays or stutters that impact real-world experience.", "method": "MobileUPReg compares user-perceived performance metrics (response time, launch time, dropped frames, etc.) across OS versions using a controlled app execution framework, validated against statistical baselines.", "result": "Achieved 0.96 precision, 0.91 recall, and 0.93 F1-score in regression detection, outperforming Wilcoxon and Cliff's Delta. Integrated into an industrial CI pipeline processing ~1k daily app tests, uncovering regressions missed by traditional tools.", "conclusion": "MobileUPReg provides a scalable, perceptually-aligned solution for mobile OS validation, bridging the gap between system-level measurements and user experience."}}
{"id": "2509.16558", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16558", "abs": "https://arxiv.org/abs/2509.16558", "authors": ["Mingjian Duan", "Ming Xu", "Shenghao Zhang", "Jiaheng Zhang", "Weili Han"], "title": "MoPE: A Mixture of Password Experts for Improving Password Guessing", "comment": null, "summary": "Textual passwords remain a predominant authentication mechanism in web\nsecurity. To evaluate their strength, existing research has proposed several\ndata-driven models across various scenarios. However, these models generally\ntreat passwords uniformly, neglecting the structural differences among\npasswords. This typically results in biased training that favors frequent\npassword structural patterns. To mitigate the biased training, we argue that\npasswords, as a type of complex short textual data, should be processed in a\nstructure-aware manner by identifying their structural patterns and routing\nthem to specialized models accordingly. In this paper, we propose MoPE, a\nMixture of Password Experts framework, specifically designed to leverage the\nstructural patterns in passwords to improveguessing performance. Motivated by\nthe observation that passwords with similar structural patterns (e.g.,\nfixed-length numeric strings) tend to cluster in high-density regions within\nthe latent space, our MoPE introduces: (1) a novel structure-based method for\ngenerating specialized expert models; (2) a lightweight gate method to select\nappropriate expert models to output reliable guesses, better aligned with the\nhigh computational frequency of password guessing tasks. Our evaluation shows\nthat MoPE significantly outperforms existing state-of-the-art baselines in both\noffline and online guessing scenarios, achieving up to 38.80% and 9.27%\nimprovement in cracking rate, respectively, showcasing that MoPE can\neffectively exploit the capabilities of data-driven models for password\nguessing. Additionally, we implement a real-time Password Strength Meter (PSM)\nbased on offline MoPE, assisting users in choosing stronger passwords more\nprecisely with millisecond-level response latency.", "AI": {"tldr": "This paper proposes MoPE, a structure-aware password guessing framework that leverages mixed experts to address training bias from uniform password modeling, achieving significant performance improvements over baselines.", "motivation": "Existing password models treat all passwords uniformly, creating training bias towards frequent structural patterns. The authors argue passwords require structure-aware processing as complex short textual data to reduce this bias.", "method": "The MoPE framework introduces (1) a structure-based method to generate specialized expert models for password patterns, and (2) a lightweight gate mechanism to select experts for reliable guessing aligned with password guessing task demands.", "result": "MoPE achieves 38.80% improvement in offline cracking rates and 9.27% in online scenarios over state-of-the-art baselines. A real-time Password Strength Meter implementation provides millisecond-level response latency.", "conclusion": "Structure-aware modeling via MoPE's expert mixture framework effectively mitigates training bias in password analysis, demonstrating superior password guessing performance and practical applications for strength estimation."}}
{"id": "2509.16870", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16870", "abs": "https://arxiv.org/abs/2509.16870", "authors": ["Rui Yang", "Michael Fu", "Chakkrit Tantithamthavorn", "Chetan Arora", "Gunel Gulmammadova", "Joey Chua"], "title": "DecipherGuard: Understanding and Deciphering Jailbreak Prompts for a Safer Deployment of Intelligent Software Systems", "comment": "Under Review", "summary": "Intelligent software systems powered by Large Language Models (LLMs) are\nincreasingly deployed in critical sectors, raising concerns about their safety\nduring runtime. Through an industry-academic collaboration when deploying an\nLLM-powered virtual customer assistant, a critical software engineering\nchallenge emerged: how to enhance a safer deployment of LLM-powered software\nsystems at runtime? While LlamaGuard, the current state-of-the-art runtime\nguardrail, offers protection against unsafe inputs, our study reveals a Defense\nSuccess Rate (DSR) drop of 24% under obfuscation- and template-based jailbreak\nattacks. In this paper, we propose DecipherGuard, a novel framework that\nintegrates a deciphering layer to counter obfuscation-based prompts and a\nlow-rank adaptation mechanism to enhance guardrail effectiveness against\ntemplate-based attacks. Empirical evaluation on over 22,000 prompts\ndemonstrates that DecipherGuard improves DSR by 36% to 65% and Overall\nGuardrail Performance (OGP) by 20% to 50% compared to LlamaGuard and two other\nruntime guardrails. These results highlight the effectiveness of DecipherGuard\nin defending LLM-powered software systems against jailbreak attacks during\nruntime.", "AI": {"tldr": "This paper addresses runtime safety of LLM-powered systems by proposing DecipherGuard, a defense framework that outperforms existing guardrails by 36\u201365% in blocking jailbreak attacks using deciphering layers and low-rank adaptation.", "motivation": "Deploying LLM-powered systems in critical sectors necessitates runtime safety mechanisms. Existing guardrails like LlamaGuard show insufficient performance (24% DSR drop under attacks), motivating the development of a more robust defense framework.", "method": "The paper introduces DecipherGuard, combining a deciphering layer to counter obfuscation-based prompts and low-rank adaptation to enhance guardrail effectiveness against template-based attacks.", "result": "DecipherGuard achieves a 36\u201365% improvement in Defense Success Rate (DSR) and 20\u201350% better Overall Guardrail Performance (OGP) compared to LlamaGuard and other baselines, validated on 22,000+ prompts.", "conclusion": "DecipherGuard effectively defends LLM-powered systems against jailbreak attacks during runtime, with proven improvements in defense and overall performance metrics."}}
{"id": "2509.16581", "categories": ["cs.CR", "cs.CE"], "pdf": "https://arxiv.org/pdf/2509.16581", "abs": "https://arxiv.org/abs/2509.16581", "authors": ["Mohsen Ahmadvand", "Pedro Souto"], "title": "Towards Cost-Effective ZK-Rollups: Modeling and Optimization of Proving Infrastructure", "comment": null, "summary": "Zero-knowledge rollups rely on provers to generate multi-step state\ntransition proofs under strict finality and availability constraints. These\nsteps require expensive hardware (e.g., GPUs), and finality is reached only\nonce all stages complete and results are posted on-chain. As rollups scale,\nstaying economically viable becomes increasingly difficult due to rising\nthroughput, fast finality demands, volatile gas prices, and dynamic resource\nneeds. We base our study on Halo2-based proving systems and identify\ntransactions per second (TPS), average gas usage, and finality time as key cost\ndrivers. To address this, we propose a parametric cost model that captures\nrollup-specific constraints and ensures provers can keep up with incoming\ntransaction load. We formulate this model as a constraint system and solve it\nusing the Z3 SMT solver to find cost-optimal configurations. To validate our\napproach, we implement a simulator that detects lag and estimates operational\ncosts. Our method shows a potential cost reduction of up to 70\\%.", "AI": {"tldr": "Optimizing ZK-rollup costs via parametric modeling and constraint solving reduces costs by 70% in scalable environments.", "motivation": "Scaling zero-knowledge rollups faces economic viability challenges due to rising hardware costs, throughput demands, and volatile gas prices, necessitating cost optimization strategies.", "method": "Developed a cost model capturing rollup constraints, formulated it as a constraint system solvable by the Z3 SMT solver, and validated it with a simulator measuring lag and costs.", "result": "A 70% cost reduction potential demonstrated via simulation, with the model successfully identifying optimal configurations while meeting transaction load and finality constraints.", "conclusion": "The proposed parametric cost model and constraint-solving approach effectively reduce operational costs for zero-knowledge rollups by up to 70% while maintaining scalability and finality requirements."}}
{"id": "2509.16939", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16939", "abs": "https://arxiv.org/abs/2509.16939", "authors": ["Taehyoun Kim", "Duksan Ryu", "Jongmoon Baik"], "title": "Deep Synthetic Cross-Project Approaches for Software Reliability Growth Modeling", "comment": "Submitted on April 26, 2025. Under review", "summary": "Software Reliability Growth Models (SRGMs) are widely used to predict\nsoftware reliability based on defect discovery data collected during testing or\noperational phases. However, their predictive accuracy often degrades in\ndata-scarce environments, such as early-stage testing or safety-critical\nsystems. Although cross-project transfer learning has been explored to mitigate\nthis issue by leveraging data from past projects, its applicability remains\nlimited due to the scarcity and confidentiality of real-world datasets. To\novercome these limitations, we propose Deep Synthetic Cross-project SRGM\n(DSC-SRGM), a novel approach that integrates synthetic data generation with\ncross-project transfer learning. Synthetic datasets are generated using\ntraditional SRGMs to preserve the statistical characteristics of real-world\ndefect discovery trends. A cross-correlation-based clustering method is applied\nto identify synthetic datasets with patterns similar to the target project.\nThese datasets are then used to train a deep learning model for reliability\nprediction. The proposed method is evaluated on 60 real-world datasets, and its\nperformance is compared with both traditional SRGMs and cross-project deep\nlearning models trained on real-world datasets. DSC-SRGM achieves up to 23.3%\nimprovement in predictive accuracy over traditional SRGMs and 32.2% over\ncross-project deep learning models trained on real-world datasets. However,\nexcessive use of synthetic data or a naive combination of synthetic and\nreal-world data may degrade prediction performance, highlighting the importance\nof maintaining an appropriate data balance. These findings indicate that\nDSC-SRGM is a promising approach for software reliability prediction in\ndata-scarce environments.", "AI": {"tldr": "DSC-SRGM improves software reliability prediction in data-scarce environments by combining synthetic data generation with cross-project transfer learning, achieving significant accuracy improvements over traditional and cross-project models.", "motivation": "Traditional SRGMs and cross-project transfer learning face limitations in data-scarce scenarios due to insufficient or confidential real-world data, reducing predictive accuracy for early-stage testing and safety-critical systems.", "method": "DSC-SRGM generates synthetic datasets using traditional SRGMs to preserve defect trends, applies cross-correlation clustering to select relevant datasets for the target project, and trains a deep learning model on this synthetic data before evaluating it on real-world datasets.", "result": "DSC-SRGM achieved 23.3\\% higher accuracy than traditional SRGMs and 32.2\\% higher accuracy than cross-project deep learning models trained on real-world data across 60 datasets.", "conclusion": "DSC-SRGM provides a viable solution for data-scarce software reliability prediction but requires careful balancing of synthetic and real-world data to avoid performance degradation."}}
{"id": "2509.16593", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.16593", "abs": "https://arxiv.org/abs/2509.16593", "authors": ["Avi Shaked"], "title": "Reproducing a Security Risk Assessment Using Computer Aided Design", "comment": null, "summary": "Security risk assessment is essential in establishing the trustworthiness and\nreliability of modern systems. While various security risk assessment\napproaches exist, prevalent applications are \"pen and paper\" implementations\nthat -- even if performed digitally using computers -- remain prone to\nauthoring mistakes and inconsistencies. Computer-aided design approaches can\ntransform security risk assessments into more rigorous and sustainable efforts.\nThis is of value to both industrial practitioners and researchers, who practice\nsecurity risk assessments to reflect on systems' designs and to contribute to\nthe discipline's state-of-the-art. In this article, we report the application\nof a model-based security design tool to reproduce a previously reported\nsecurity assessment. The main contributions are: 1) an independent attempt to\nreproduce a refereed article describing a real security risk assessment of a\nsystem; 2) comparison of a new computer-aided application with a previous\nnon-computer-aided application, based on a published, real-world case study; 3)\na showcase for the potential advantages -- for both practitioners and\nresearchers -- of using computer-aided design approaches to analyze reports and\nto assess systems.", "AI": {"tldr": "This paper evaluates computer-aided security tools by reproducing a prior assessment, showing they enhance reliability and sustainability compared to traditional methods, with benefits for both research and practice.", "motivation": "The study addresses limitations in traditional 'pen and paper' security assessments (even when digitized), which are prone to authoring errors and inconsistencies, and aims to demonstrate the advantages of computer-aided tools for both practitioners and researchers.", "method": "The authors applied a model-based security design tool to reproduce a previously published security risk assessment, comparing the computer-aided approach with the original non-computer method using a real-world case study.", "result": "Three contributions: (1) successful reproduction of a refereed security assessment article, (2) comparative analysis of computer-aided vs non-computer methods using a published real-world case, and (3) demonstration of computer-aided tools' potential to improve assessment rigor and usability.", "conclusion": "The paper concludes that computer-aided design approaches enhance the rigor and sustainability of security risk assessments, benefiting both industry practitioners and researchers through improved consistency and reliability."}}
{"id": "2509.16941", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16941", "abs": "https://arxiv.org/abs/2509.16941", "authors": ["Xiang Deng", "Jeff Da", "Edwin Pan", "Yannis Yiming He", "Charles Ide", "Kanak Garg", "Niklas Lauffer", "Andrew Park", "Nitin Pasari", "Chetan Rane", "Karmini Sampath", "Maya Krishnan", "Srivatsa Kundurthy", "Sean Hendryx", "Zifan Wang", "Chen Bo Calvin Zhang", "Noah Jacobson", "Bing Liu", "Brad Kenstler"], "title": "SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?", "comment": null, "summary": "We introduce SWE-Bench Pro, a substantially more challenging benchmark that\nbuilds upon the best practices of SWE-BENCH [25], but is explicitly designed to\ncapture realistic, complex, enterprise-level problems beyond the scope of\nSWE-BENCH. SWE-BENCH PRO contains 1,865 problems sourced from a diverse set of\n41 actively maintained repositories spanning business applications, B2B\nservices, and developer tools. The benchmark is partitioned into a public set\nwith open access to problems sourced from 11 repositories, a held-out set of 12\nrepositories and a commercial set of 18 proprietary repositories where we have\nformal partnership agreements with early-stage startups. Problems in the\nheld-out and the commercial set are not publicly accessible, but we release\nresults on the commercial set. Our benchmark features long-horizon tasks that\nmay require hours to days for a professional software engineer to complete,\noften involving patches across multiple files and substantial code\nmodifications. All tasks are human-verified and augmented with sufficient\ncontext to ensure resolvability. In our evaluation of widely used coding\nmodels, under a unified scaffold, we observe that their performance on\nSWE-Bench PRO remains below 25% (Pass@1), with GPT-5 achieving the highest\nscore to date at 23.3%. To better understand these limitations, we cluster the\nfailure modes observed in the collected agent trajectories for a clearer\ncharacterization of the error patterns exhibited by current models. Overall,\nSWE-BENCH PRO provides a contamination-resistant testbed that more faithfully\ncaptures the complexity and diversity of real-world software development,\nadvancing the pursuit of truly autonomous software engineering agents at a\nprofessional level.", "AI": {"tldr": "SWE-Bench Pro is a new benchmark for complex software engineering tasks, featuring 1,865 real-world problems from 41 repositories across public, held-out, and commercial partitions. It evaluates AI models' ability to handle long-horizon, multi-file patches with human-verified tasks. Results show current models perform under 25%, with GPT-5 at 23.3%.", "motivation": "Existing benchmarks (e.g., SWE-BENCH) lack realism for enterprise-level, multi-file software engineering tasks requiring extensive modifications. SWE-Bench Pro fills this gap by providing a contamination-resistant testbed for authentic, professional-grade challenges.", "method": "Curated 1,865 problems from 41 repositories (business, B2B, developer tools). Structured into three partitions with access controls. Tasks involve long-horizon (>days) multi-file patches. Evaluated models under uniform scaffolding and analyzed failure modes via trajectory clustering.", "result": "Top AI models (e.g., GPT-5) achieve \u226423.37% accuracy (Pass@1). Commercial partition results are released without exposing tasks. Clustering reveals recurring error patterns in model-generated solutions for complex tasks.", "conclusion": "SWE-Bench Pro advances autonomous software engineering research by rigorously exposing model limitations in large-scale, real-world coding scenarios. Its contamination-resistant design ensures evaluation robustness while benchmarking progress toward professional-level AI agents."}}
{"id": "2509.16620", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16620", "abs": "https://arxiv.org/abs/2509.16620", "authors": ["Yi Chen", "Xiaoyang Dong", "Ruijie Ma", "Yantian Shen", "Anyu Wang", "Hongbo Yu", "Xiaoyun Wang"], "title": "Delving into Cryptanalytic Extraction of PReLU Neural Networks", "comment": "Accepted by ASIACRYPT 2025", "summary": "The machine learning problem of model extraction was first introduced in 1991\nand gained prominence as a cryptanalytic challenge starting with Crypto 2020.\nFor over three decades, research in this field has primarily focused on\nReLU-based neural networks. In this work, we take the first step towards the\ncryptanalytic extraction of PReLU neural networks, which employ more complex\nnonlinear activation functions than their ReLU counterparts. We propose a raw\noutput-based parameter recovery attack for PReLU networks and extend it to more\nrestrictive scenarios where only the top-m probability scores are accessible.\nOur attacks are rigorously evaluated through end-to-end experiments on diverse\nPReLU neural networks, including models trained on the MNIST dataset. To the\nbest of our knowledge, this is the first practical demonstration of PReLU\nneural network extraction across three distinct attack scenarios.", "AI": {"tldr": "This paper introduces the first practical model extraction attacks on PReLU neural networks, extending cryptoanalytic methods beyond ReLU networks.", "motivation": "While model extraction has focused on ReLU-based networks for decades, PReLU networks with complex activation functions represent a critical yet understudied security threat. The authors aim to bridge this research gap as PReLU models become more prevalent in real-world applications.", "method": "The paper proposes a raw output-based parameter recovery attack for PReLU networks and adapts it for scenarios with limited access to top-m probability scores. The attacks are validated through systematice experiments on multiple PReLU architectures (including MNIST-trained models).", "result": "The proposed attacks achieve successful model extraction empirically, demonstrating feasibility across three distinct attack scenarios for PReLU networks. This includes the first end-to-end extraction proof on PReLU architectures with constrained output access.", "conclusion": "The work establishes PReLU network extraction as a valid security concern, highlighting vulnerabilities in models with sophisticated activation functions. It opens new directions for cryptoanalytic defenses and attacks on modern neural network architectures."}}
{"id": "2509.16985", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16985", "abs": "https://arxiv.org/abs/2509.16985", "authors": ["James J. Cusick"], "title": "Static Security Vulnerability Scanning of Proprietary and Open-Source Software: An Adaptable Process with Variants and Results", "comment": "A total of 8 pages, 7 figures, 4 tables, and 31 references", "summary": "Software vulnerabilities remain a significant risk factor in achieving\nsecurity objectives within software development organizations. This is\nespecially true where either proprietary or open-source software (OSS) is\nincluded in the technological environment. In this paper an end-to-end process\nwith supporting methods and tools is presented. This industry proven generic\nprocess allows for the custom instantiation, configuration, and execution of\nroutinized code scanning for software vulnerabilities and their prioritized\nremediation. A select set of tools are described for this key DevSecOps\nfunction and placed into an iterative process. Examples of both industrial\nproprietary applications and open-source applications are provided including\nspecific vulnerability instances and a discussion of their treatment. The\nbenefits of each selected tool are considered, and alternative tools are also\nintroduced. Application of this method in a comprehensive SDLC model is also\nreviewed along with prospective enhancements from automation and the\napplication of advanced technologies including AI. Adoption of this method can\nbe achieved with minimal adjustments and with maximum flexibility for results\nin reducing source code vulnerabilities, reducing supply chain risk, and\nimproving the security profile of new or legacy solutions.", "AI": {"tldr": "The paper proposes an end-to-end DevSecOps process with iterative tools and methods for systematic identification, prioritization, and remediation of software vulnerabilities in proprietary and open-source systems.", "motivation": "Software vulnerabilities in both proprietary and open-source environments pose significant security risks. Existing approaches lack a unified, customizable process for proactive vulnerability management with prioritized remediation.", "method": "The authors present an industry-tested process incorporating specialized code scanning tools within an iterative workflow. This includes tool selection, configuration, and implementation examples for both industrial and open-source applications, alongside strategies for automation and AI-enhanced vulnerability treatment.", "result": "The method successfully enables organizations to reduce code vulnerabilities, supply chain risks, and improve security posture through flexible implementation. Case studies demonstrate vulnerability remediation examples and tool comparisons.", "conclusion": "This generic yet adaptable approach provides maximal security benefits with minimal organizational changes. Future directions include deeper integration of automation and AI to enhance vulnerability detection and prioritization across full SDLC cycles."}}
{"id": "2509.16671", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16671", "abs": "https://arxiv.org/abs/2509.16671", "authors": ["Ekin B\u00f6ke", "Simon Torka"], "title": "\"Digital Camouflage\": The LLVM Challenge in LLM-Based Malware Detection", "comment": null, "summary": "Large Language Models (LLMs) have emerged as promising tools for malware\ndetection by analyzing code semantics, identifying vulnerabilities, and\nadapting to evolving threats. However, their reliability under adversarial\ncompiler-level obfuscation is yet to be discovered. In this study, we\nempirically evaluate the robustness of three state-of-the-art LLMs: ChatGPT-4o,\nGemini Flash 2.5, and Claude Sonnet 4 against compiler-level obfuscation\ntechniques implemented via the LLVM infrastructure. These include control flow\nflattening, bogus control flow injection, instruction substitution, and split\nbasic blocks, which are widely used to evade detection while preserving\nmalicious behavior. We perform a structured evaluation on 40~C functions (20\nvulnerable, 20 secure) sourced from the Devign dataset and obfuscated using\nLLVM passes. Our results show that these models often fail to correctly\nclassify obfuscated code, with precision, recall, and F1-score dropping\nsignificantly after transformation. This reveals a critical limitation: LLMs,\ndespite their language understanding capabilities, can be easily misled by\ncompiler-based obfuscation strategies. To promote reproducibility, we release\nall evaluation scripts, prompts, and obfuscated code samples in a public\nrepository. We also discuss the implications of these findings for adversarial\nthreat modeling, and outline future directions such as software watermarking,\ncompiler-aware defenses, and obfuscation-resilient model design.", "AI": {"tldr": "The paper evaluates the robustness of three state-of-the-art Large Language Models (LLMs) against compiler-level obfuscation techniques in malware detection, finding significant drops in performance metrics like precision, recall, and F1-score after obfuscation, which indicates critical vulnerabilities in these models.", "motivation": "The motivation of the paper is to understand the reliability of Large Language Models (LLMs) in malware detection when faced with adversarial compiler-level obfuscations, which are widely used to evade detection while maintaining malicious behavior. The study seeks to uncover potential vulnerabilities in LLMs that adversaries might exploit, thus contributing to adversarial threat modeling.", "method": "The method involves an empirical evaluation of three state-of-the-art LLMs (ChatGPT-4o, Gemini Flash 2.5, and Claude Sonnet 4) against four compiler-level obfuscation techniques (control flow flattening, bogus control flow injection, instruction substitution, and split basic blocks) implemented using the LLVM infrastructure. The evaluation is performed on a set of 40 C functions (20 vulnerable, 20 secure) sourced from the Devign dataset, which have been obfuscated using LLVM passes.", "result": "The key results of the study show that all three Large Language Models (LLMs) tested performed very poorly on obfuscated code. Specifically, the models failed to correctly classify obfuscated code, with precision, recall, and F1-score dropping significantly after the application of obfuscation techniques. This indicates that compiler-based obfuscation can effectively mislead the models, undermining their utility in malware detection.", "conclusion": "The conclusion is that LLMs, despite their advanced language understanding capabilities, are critically vulnerable to compiler-level obfuscations in the context of malware detection. The models can be easily misled by transformations like control flow flattening, highlighting the need for improving LLM efficiency through compiler-aware defenses, obfuscation-resilient design, or software watermarking. The study also encourages future work in these areas and promotes reproducibility through the release of datasets and evaluation tools."}}
{"id": "2509.17096", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.17096", "abs": "https://arxiv.org/abs/2509.17096", "authors": ["Ziyou Li", "Agnia Sergeyuk", "Maliheh Izadi"], "title": "Prompt-with-Me: in-IDE Structured Prompt Management for LLM-Driven Software Engineering", "comment": "Accepted in the 40th IEEE/ACM International Conference on Automated\n  Software Engineering, ASE 2025 (Industry track)", "summary": "Large Language Models are transforming software engineering, yet prompt\nmanagement in practice remains ad hoc, hindering reliability, reuse, and\nintegration into industrial workflows. We present Prompt-with-Me, a practical\nsolution for structured prompt management embedded directly in the development\nenvironment. The system automatically classifies prompts using a\nfour-dimensional taxonomy encompassing intent, author role, software\ndevelopment lifecycle stage, and prompt type. To enhance prompt reuse and\nquality, Prompt-with-Me suggests language refinements, masks sensitive\ninformation, and extracts reusable templates from a developer's prompt library.\nOur taxonomy study of 1108 real-world prompts demonstrates that modern LLMs can\naccurately classify software engineering prompts. Furthermore, our user study\nwith 11 participants shows strong developer acceptance, with high usability\n(Mean SUS=73), low cognitive load (Mean NASA-TLX=21), and reported gains in\nprompt quality and efficiency through reduced repetitive effort. Lastly, we\noffer actionable insights for building the next generation of prompt management\nand maintenance tools for software engineering workflows.", "AI": {"tldr": "Prompt-with-Me introduces a structured prompt management system for software engineering with a four-dimensional classification taxonomy, demonstrating improved usability and efficiency via empirical validation.", "motivation": "Ad hoc prompt management in software engineering hinders reliability, reuse, and industrial integration, necessitating structured workflows for LLM-driven tooling.", "method": "The system employs a taxonomy (intent, author role, lifecycle stage, prompt type), automated refinements, sensitive data masking, and template extraction, validated via a 1108-prompt taxonomy study and user evaluations.", "result": "LLMs achieved high classification accuracy across 1108 prompts; 11-participant user studies reported high usability (SUS 73) and low cognitive load (NASA-TLX 21), with improved prompt quality and efficiency.", "conclusion": "Prompt-with-Me validates structured prompt management for software workflows, offering actionable insights for future tools to enhance LLM integration in industrial practices."}}
{"id": "2509.16682", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16682", "abs": "https://arxiv.org/abs/2509.16682", "authors": ["Javier Jim\u00e9nez-Rom\u00e1n", "Florina Almenares-Mendoza", "Alfonso S\u00e1nchez-Maci\u00e1n"], "title": "Design and Development of an Intelligent LLM-based LDAP Honeypot", "comment": null, "summary": "Cybersecurity threats continue to increase, with a growing number of\npreviously unknown attacks each year targeting both large corporations and\nsmaller entities. This scenario demands the implementation of advanced security\nmeasures, not only to mitigate damage but also to anticipate emerging attack\ntrends. In this context, deception tools have become a key strategy, enabling\nthe detection, deterrence, and deception of potential attackers while\nfacilitating the collection of information about their tactics and methods.\nAmong these tools, honeypots have proven their value, although they have\ntraditionally been limited by rigidity and configuration complexity, hindering\ntheir adaptability to dynamic scenarios. The rise of artificial intelligence,\nand particularly general-purpose Large Language Models (LLMs), is driving the\ndevelopment of new deception solutions capable of offering greater adaptability\nand ease of use. This work proposes the design and implementation of an\nLLM-based honeypot to simulate an LDAP server, a critical protocol present in\nmost organizations due to its central role in identity and access management.\nThe proposed solution aims to provide a flexible and realistic tool capable of\nconvincingly interacting with attackers, thereby contributing to early\ndetection and threat analysis while enhancing the defensive capabilities of\ninfrastructures against intrusions targeting this service.", "AI": {"tldr": "This paper introduces an LLM-powered honeypot for LDAP servers to improve cybersecurity through adaptive deception techniques, addressing the limitations of traditional tools and enhancing threat detection against evolving attacks on identity management systems.", "motivation": "Traditional honeypots suffer from rigidity and configuration complexity, limiting their effectiveness in dynamic attack scenarios. The rise of AI and LLMs enables new deception solutions that can better simulate realistic interactions, address evolving threats, and improve attacker profiling for proactive defense strategies.", "method": "The paper proposes designing and implementing an LLM-based honeypot to simulate an LDAP server, leveraging artificial intelligence to enhance adaptability, realism, and ease of use compared to traditional rigid honeypot configurations.", "result": "The proposed LLM-based honeypot solution provides a flexible and realistic simulation of LDAP services, enabling credible attacker interaction for early threat detection, method analysis, and enhanced infrastructure defense against LDAP-targeted intrusions.", "conclusion": "The integration of LLMs into honeypot design offers a more adaptive and effective approach to combating evolving cybersecurity threats, particularly for critical services like LDAP servers, by improving early detection and threat analysis capabilities."}}
{"id": "2509.17314", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17314", "abs": "https://arxiv.org/abs/2509.17314", "authors": ["Juyeon Yoon", "Somin Kim", "Robert Feldt", "Shin Yoo"], "title": "Clotho: Measuring Task-Specific Pre-Generation Test Adequacy for LLM Inputs", "comment": null, "summary": "Software increasingly relies on the emergent capabilities of Large Language\nModels (LLMs), from natural language understanding to program analysis and\ngeneration. Yet testing them on specific tasks remains difficult and costly:\nmany prompts lack ground truth, forcing reliance on human judgment, while\nexisting uncertainty and adequacy measures typically require full inference. A\nkey challenge is to assess input adequacy in a way that reflects the demands of\nthe task, ideally before even generating any output. We introduce CLOTHO, a\ntask-specific, pre-generation adequacy measure that estimates input difficulty\ndirectly from hidden LLM states. Given a large pool of unlabelled inputs for a\nspecific task, CLOTHO uses a Gaussian Mixture Model (GMM) to adaptively sample\nthe most informative cases for human labelling. Based on this reference set the\nGMM can then rank unseen inputs by their likelihood of failure. In our\nempirical evaluation across eight benchmark tasks and three open-weight LLMs,\nCLOTHO can predict failures with a ROC-AUC of 0.716, after labelling reference\nsets that are on average only 5.4% of inputs. It does so without generating any\noutputs, thereby reducing costs compared to existing uncertainty measures.\nComparison of CLOTHO and post-generation uncertainty measures shows that the\ntwo approaches complement each other. Crucially, we show that adequacy scores\nlearnt from open-weight LLMs transfer effectively to proprietary models,\nextending the applicability of the approach. When prioritising test inputs for\nproprietary models, CLOTHO increases the average number of failing inputs from\n18.7 to 42.5 out of 100, compared to random prioritisation.", "AI": {"tldr": "CLOTHO is a framework that measures input adequacy in LLM software by analyzing hidden states to estimate input difficulty without generating outputs, enabling efficient prioritization of testing inputs.", "motivation": "This paper addresses the difficulty and cost of testing LLMs for software tasks due to a lack of ground truth and the need for full inference, highlighting the need for task-specific pre-generation input adequacy measures.", "method": "CLOTHO uses a Gaussian Mixture Model (GMM) to analyze hidden states of LLMs, selecting the most informative unlabelled inputs for human labeling and then using the GMM to rank unseen inputs by their likelihood of failure.", "result": "Empirical results show CLOTHO can predict failures with a ROC-AUC of 0.716, requiring labeling of only 5.4% of inputs on average. It increases failing inputs from 18.7 to 42.5 when prioritizing tests for proprietary models.", "conclusion": "CLOTHO provides effective pre-generation input adequacy assessment for LLMs, reducing testing costs and improving failure prediction. It can transfer adequacy scores from open-weight LLMs to proprietary ones, significantly enhancing test prioritization."}}
{"id": "2509.16749", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16749", "abs": "https://arxiv.org/abs/2509.16749", "authors": ["Anna Bertiger", "Bobby Filar", "Aryan Luthra", "Stefano Meschiari", "Aiden Mitchell", "Sam Scholten", "Vivek Sharath"], "title": "Evaluating LLM Generated Detection Rules in Cybersecurity", "comment": "Preprint of a paper accepted at the Conference on Applied Machine\n  Learning in Information Security (CAMLIS 2025). 11 pages, 3 figures, 4 tables", "summary": "LLMs are increasingly pervasive in the security environment, with limited\nmeasures of their effectiveness, which limits trust and usefulness to security\npractitioners. Here, we present an open-source evaluation framework and\nbenchmark metrics for evaluating LLM-generated cybersecurity rules. The\nbenchmark employs a holdout set-based methodology to measure the effectiveness\nof LLM-generated security rules in comparison to a human-generated corpus of\nrules. It provides three key metrics inspired by the way experts evaluate\nsecurity rules, offering a realistic, multifaceted evaluation of the\neffectiveness of an LLM-based security rule generator. This methodology is\nillustrated using rules from Sublime Security's detection team and those\nwritten by Sublime Security's Automated Detection Engineer (ADE), with a\nthorough analysis of ADE's skills presented in the results section.", "AI": {"tldr": "The paper introduces an open-source evaluation framework with benchmark metrics for assessing the effectiveness of LLM-generated cybersecurity rules, using a holdout set-based methodology and three key metrics inspired by expert evaluations.", "motivation": "LLMs are becoming more common in cybersecurity, but their effectiveness in this field is not well understood, which restricts trust and practical application among security professionals.", "method": "The research presents a benchmark using a holdout set-based methodology to evaluate LLM-generated security rules. It introduces three metrics based on expert evaluation practices for a comprehensive assessment.", "result": "The results section includes a detailed analysis of the performance of ADE, Sublime Security's Automated Detection Engineer, showcasing its capabilities and limitations through the proposed metrics.", "conclusion": "The open-source evaluation framework provides realistic, multifaceted metrics, helping advance trust in LLM-based cybersecurity solutions by comparing them with human-generated rules through expert-inspired criteria."}}
{"id": "2509.17335", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.17335", "abs": "https://arxiv.org/abs/2509.17335", "authors": ["Mingxuan Xiao", "Yan Xiao", "Shunhui Ji", "Jiahe Tu", "Pengcheng Zhang"], "title": "BASFuzz: Towards Robustness Evaluation of LLM-based NLP Software via Automated Fuzz Testing", "comment": null, "summary": "Fuzzing has shown great success in evaluating the robustness of intelligent\nnatural language processing (NLP) software. As large language model (LLM)-based\nNLP software is widely deployed in critical industries, existing methods still\nface two main challenges: 1 testing methods are insufficiently coupled with the\nbehavioral patterns of LLM-based NLP software; 2 fuzzing capability for the\ntesting scenario of natural language generation (NLG) generally degrades. To\naddress these issues, we propose BASFuzz, an efficient Fuzz testing method\ntailored for LLM-based NLP software. BASFuzz targets complete test inputs\ncomposed of prompts and examples, and uses a text consistency metric to guide\nmutations of the fuzzing loop, aligning with the behavioral patterns of\nLLM-based NLP software. A Beam-Annealing Search algorithm, which integrates\nbeam search and simulated annealing, is employed to design an efficient fuzzing\nloop. In addition, information entropy-based adaptive adjustment and an elitism\nstrategy further enhance fuzzing capability. We evaluate BASFuzz on six\ndatasets in representative scenarios of NLG and natural language understanding\n(NLU). Experimental results demonstrate that BASFuzz achieves a testing\neffectiveness of 90.335% while reducing the average time overhead by 2,163.852\nseconds compared to the current best baseline, enabling more effective\nrobustness evaluation prior to software deployment.", "AI": {"tldr": "BASFuzz is an efficient fuzz testing method for LLM-based NLP software, addressing issues with existing methods by aligning with behavioral patterns and improving NLG testing capability.", "motivation": "The motivation is that as LLM-based NLP software becomes more common in important industries, testing methods are not well with LLM-based NLP software's behavior and often fail in natural language generation (NLG) scenarios.", "method": "The method involves using BASFuzz, which targets test inputs made of prompts and examples, a text consistency metric guides mutations in the fuzzing loop, and a Beam-Annealing Search algorithm combining beam search and simulated annealing to design an efficient testing process. Additionally, information entropy-based adaptive adjustment and an elitism strategy are used to further enhance fuzzing capability.", "result": "The result shows that BASFuzz achieves a testing effectiveness of 90.335% and reduces the average time overhead by 2,163.852 seconds compared to the current best baseline.", "conclusion": "The conclusion of this paper is that the proposed method is more effective and efficient for testing LLM-based NLP software, leading to a more effective robustness evaluation before deployment."}}
{"id": "2509.16861", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16861", "abs": "https://arxiv.org/abs/2509.16861", "authors": ["Rui Yang", "Michael Fu", "Chakkrit Tantithamthavorn", "Chetan Arora", "Gunel Gulmammadova", "Joey Chua"], "title": "AdaptiveGuard: Towards Adaptive Runtime Safety for LLM-Powered Software", "comment": "Accepted to the ASE 2025 International Conference on Automated\n  Software Engineering, Industry Showcase Track", "summary": "Guardrails are critical for the safe deployment of Large Language Models\n(LLMs)-powered software. Unlike traditional rule-based systems with limited,\npredefined input-output spaces that inherently constrain unsafe behavior, LLMs\nenable open-ended, intelligent interactions--opening the door to jailbreak\nattacks through user inputs. Guardrails serve as a protective layer, filtering\nunsafe prompts before they reach the LLM. However, prior research shows that\njailbreak attacks can still succeed over 70% of the time, even against advanced\nmodels like GPT-4o. While guardrails such as LlamaGuard report up to 95%\naccuracy, our preliminary analysis shows their performance can drop sharply--to\nas low as 12%--when confronted with unseen attacks. This highlights a growing\nsoftware engineering challenge: how to build a post-deployment guardrail that\nadapts dynamically to emerging threats? To address this, we propose\nAdaptiveGuard, an adaptive guardrail that detects novel jailbreak attacks as\nout-of-distribution (OOD) inputs and learns to defend against them through a\ncontinual learning framework. Through empirical evaluation, AdaptiveGuard\nachieves 96% OOD detection accuracy, adapts to new attacks in just two update\nsteps, and retains over 85% F1-score on in-distribution data post-adaptation,\noutperforming other baselines. These results demonstrate that AdaptiveGuard is\na guardrail capable of evolving in response to emerging jailbreak strategies\npost deployment. We release our AdaptiveGuard and studied datasets at\nhttps://github.com/awsm-research/AdaptiveGuard to support further research.", "AI": {"tldr": "This paper introduces AdaptiveGuard, an adaptive guardrail system that achieves 96% novel attack detection and rapid adaptation (2 steps) via continual learning, outperforming prior methods which fail 70%+ of the time.", "motivation": "Existing guardrails (e.g., LlamaGuard) show over 70% failure rates against jailbreak attacks, and their performance drops sharply (<12% accuracy) when facing unseen threats, highlighting the need for post-deployment adaptation.", "method": "Proposes AdaptiveGuard, which uses out-of-distribution (OOD) detection and continual learning to identify novel attacks and incrementally adapt defenses through two update steps.", "result": "96% OOD detection accuracy, adaptation within two update steps, and 85% F1-score retention on in-distribution data post-adaptation, outperforming baselines.", "conclusion": "AdaptiveGuard is a guardrail that dynamically adapts to emerging jailbreak strategies post-deployment, maintaining high accuracy (96% OOD detection) while retaining performance on existing threats."}}
{"id": "2509.17338", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.17338", "abs": "https://arxiv.org/abs/2509.17338", "authors": ["Pengfei He", "Shaowei Wang", "Tse-Hsun Chen"], "title": "SLICET5: Static Program Slicing using Language Models with Copy Mechanism and Constrained Decoding", "comment": "3 tables, 6 Figures, 12 pages", "summary": "Static program slicing is a fundamental technique in software engineering.\nTraditional static slicing tools rely on parsing complete source code, which\nlimits their applicability to real-world scenarios where code snippets are\nincomplete or unparsable. While recent research developed learning-based\napproaches to predict slices, they face critical challenges: (1) Inaccurate\ndependency identification, where models fail to precisely capture data and\ncontrol dependencies between code elements; and (2) Unconstrained generation,\nwhere models produce slices with extraneous or hallucinated tokens not present\nin the input, violating the structural integrity of slices. To address these\nchallenges, we propose \\ourtool, a novel slicing framework that reformulates\nstatic program slicing as a sequence-to-sequence task using lightweight\nlanguage models (e.g., CodeT5+). Our approach incorporates two key innovations.\nFirst, we introduce a copy mechanism that enables the model to more accurately\ncapture inter-element dependencies and directly copy relevant tokens from the\ninput, improving both dependency reasoning and generation constraint. Second,\nwe design a constrained decoding process with (a) lexical constraint,\nrestricting outputs to input tokens only, and (b) syntactic constraint,\nleveraging Tree Similarity of Edit Distance (TSED) monotonicity to detect\nstructurally invalid outputs and discard them. We evaluate \\ourtool on CodeNet\nand LeetCode datasets and show it consistently outperforms state-of-the-art\nbaselines, improving ExactMatch scores by up to 27\\%. Furthermore, \\ourtool\ndemonstrates strong performance on incomplete code, highlighting its robustness\nand practical utility in real-world development environments.", "AI": {"tldr": "This paper proposes \\ourtool, a novel static program slicing framework using sequence-to-sequence learning with copy mechanisms and constrained decoding, achieving up to 27\\% improvement in ExactMatch scores over existing methods by addressing dependency identification and hallucination issues.", "motivation": "Traditional slicing tools require full source code parsing while learning-based approaches struggle with token dependency accuracy and generating unreliable \"hallucinated\" content. Real-world code snippets are often incomplete or unparsable yet require effective analysis.", "method": "Reforms slicing as sequence-to-sequence task with: (1): Copy mechanism for precise token copying from inputs to capture dependencies (2): Dual constraints during decoding - lexical (output limited to input tokens only) and syntactic (TSED monotonicity checks for structure validity) using CodeT5+ models. Evaluated via CodeNet and LeetCode benchmarks.", "result": "Outperforms state-of-the-art baselines by 27\\% in ExactMatch scores while demonstrating superior robustness to incomplete code. Synthesized output maintains strict structural validity through TSED constraint and lexical filtering.", "conclusion": "The combination of copy mechanisms and dual decoding constraints addresses critical limitations in learning-based slicing, enabling practical application to real-world code snippets with missing components while maintaining precision and structural integrity."}}
{"id": "2509.16899", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16899", "abs": "https://arxiv.org/abs/2509.16899", "authors": ["Md Wasiul Haque", "Md Erfan", "Sagar Dasgupta", "Md Rayhanur Rahman", "Mizanur Rahman"], "title": "Security Vulnerabilities in Software Supply Chain for Autonomous Vehicles", "comment": "16 pages, 9 figures, 10 tables", "summary": "The interest in autonomous vehicles (AVs) for critical missions, including\ntransportation, rescue, surveillance, reconnaissance, and mapping, is growing\nrapidly due to their significant safety and mobility benefits. AVs consist of\ncomplex software systems that leverage artificial intelligence (AI), sensor\nfusion algorithms, and real-time data processing. Additionally, AVs are\nbecoming increasingly reliant on open-source software supply chains, such as\nopen-source packages, third-party software components, AI models, and\nthird-party datasets. Software security best practices in the automotive sector\nare often an afterthought for developers. Thus, significant cybersecurity risks\nexist in the software supply chain of AVs, particularly when secure software\ndevelopment practices are not rigorously implemented. For example, Upstream's\n2024 Automotive Cybersecurity Report states that 49.5% of cyberattacks in the\nautomotive sector are related to exploiting security vulnerabilities in\nsoftware systems. In this chapter, we analyze security vulnerabilities in\nopen-source software components in AVs. We utilize static analyzers on popular\nopen-source AV software, such as Autoware, Apollo, and openpilot. Specifically,\nthis chapter covers: (1) prevalent software security vulnerabilities of AVs;\nand (2) a comparison of static analyzer outputs for different open-source AV\nrepositories. The goal is to inform researchers, practitioners, and\npolicymakers about the existing security flaws in the commonplace open-source\nsoftware ecosystem in the AV domain. The findings would emphasize the necessity\nof security best practices earlier in the software development lifecycle to\nreduce cybersecurity risks, thereby ensuring system reliability, safeguarding\nuser data, and maintaining public trust in an increasingly automated world.", "AI": {"tldr": "This paper investigates cybersecurity risks in AV open-source software, using static analysis to expose vulnerabilities and advocate for earlier security practices in development.", "motivation": "Autonomous vehicles (AVs) increasingly rely on open-source software supply chains, which face significant cybersecurity risks due to lax security practices, with 49.5% of automotive cyberattacks exploiting software vulnerabilities.", "method": "The authors analyzed security vulnerabilities in open-source AV software (Autoware, Apollo, openpilot) using static analyzers, comparing prevalence of vulnerabilities and static analyzer outputs across repositories.", "result": "The analysis identified prevalent security vulnerabilities in AV open-source software and provided a comparative evaluation of static analyzer outputs, highlighting critical flaws across popular repositories.", "conclusion": "The study concludes that integrating security best practices early in the software development lifecycle is essential for reducing cybersecurity risks in AV open-source software, ensuring system reliability, data protection, and public trust."}}
{"id": "2509.17548", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.17548", "abs": "https://arxiv.org/abs/2509.17548", "authors": ["Hugo Villamizar", "Jannik Fischbach", "Alexander Korn", "Andreas Vogelsang", "Daniel Mendez"], "title": "Prompts as Software Engineering Artifacts: A Research Agenda and Preliminary Findings", "comment": "This paper has been accepted for presentation at the 26th\n  International Conference on Product-Focused Software Process Improvement\n  (PROFES 2025)", "summary": "Developers now routinely interact with large language models (LLMs) to\nsupport a range of software engineering (SE) tasks. This prominent role\npositions prompts as potential SE artifacts that, like other artifacts, may\nrequire systematic development, documentation, and maintenance. However, little\nis known about how prompts are actually used and managed in LLM-integrated\nworkflows, what challenges practitioners face, and whether the benefits of\nsystematic prompt management outweigh the associated effort. To address this\ngap, we propose a research programme that (a) characterizes current prompt\npractices, challenges, and influencing factors in SE; (b) analyzes prompts as\nsoftware artifacts, examining their evolution, traceability, reuse, and the\ntrade-offs of systematic management; and (c) develops and empirically evaluates\nevidence-based guidelines for managing prompts in LLM-integrated workflows. As\na first step, we conducted an exploratory survey with 74 software professionals\nfrom six countries to investigate current prompt practices and challenges. The\nfindings reveal that prompt usage in SE is largely ad-hoc: prompts are often\nrefined through trial-and-error, rarely reused, and shaped more by individual\nheuristics than standardized practices. These insights not only highlight the\nneed for more systematic approaches to prompt management but also provide the\nempirical foundation for the subsequent stages of our research programme.", "AI": {"tldr": "This paper reveals ad-hoc prompt practices in SE workflows and proposes systematic prompt management as a critical research direction, validated through an exploratory survey of 74 developers.", "motivation": "Prompts are increasingly used in software engineering (SE) but lack systematic development, documentation, and maintenance, making it unclear whether systematic management offers tangible benefits over current ad-hoc approaches.", "method": "An exploratory survey with 74 software professionals across six countries was conducted to analyze prompt practices and challenges in LLM-integrated workflows.", "result": "Prompt usage is predominantly ad-hoc, with trial-and-error refinement, limited reuse, and individual heuristics dominating over standardized practices, creating a clear rationale for systematic management.", "conclusion": "The study establishes the need for systematic prompt management in SE by illustrating current ad-hoc practices and provides a foundation for developing evidence-based guidelines through empirical research."}}
{"id": "2509.16950", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16950", "abs": "https://arxiv.org/abs/2509.16950", "authors": ["Xuan Chen", "Shiwei Feng", "Zikang Xiong", "Shengwei An", "Yunshu Mao", "Lu Yan", "Guanhong Tao", "Wenbo Guo", "Xiangyu Zhang"], "title": "Temporal Logic-Based Multi-Vehicle Backdoor Attacks against Offline RL Agents in End-to-end Autonomous Driving", "comment": null, "summary": "Assessing the safety of autonomous driving (AD) systems against security\nthreats, particularly backdoor attacks, is a stepping stone for real-world\ndeployment. However, existing works mainly focus on pixel-level triggers that\nare impractical to deploy in the real world. We address this gap by introducing\na novel backdoor attack against the end-to-end AD systems that leverage one or\nmore other vehicles' trajectories as triggers. To generate precise trigger\ntrajectories, we first use temporal logic (TL) specifications to define the\nbehaviors of attacker vehicles. Configurable behavior models are then used to\ngenerate these trajectories, which are quantitatively evaluated and iteratively\nrefined based on the TL specifications. We further develop a negative training\nstrategy by incorporating patch trajectories that are similar to triggers but\nare designated not to activate the backdoor. It enhances the stealthiness of\nthe attack and refines the system's responses to trigger scenarios. Through\nextensive experiments on 5 offline reinforcement learning (RL) driving agents\nwith 6 trigger patterns and target action combinations, we demonstrate the\nflexibility and effectiveness of our proposed attack, showing the\nunder-exploration of existing end-to-end AD systems' vulnerabilities to such\ntrajectory-based backdoor attacks.", "AI": {"tldr": "This paper introduces a trajectory-based backdoor attack method for end-to-end autonomous driving systems, demonstrating vulnerabilities not addressed by existing pixel-level attack frameworks.\\n\\n", "motivation": "Current backdoor attack studies on autonomous driving systems focus on impractical pixel-level triggers, creating a gap in understanding real-world security threats based on vehicle trajectory patterns.\\n\\n", "method": "1. Utilize temporal logic specifications to define attacker vehicle behavior patterns\\n2. Generate trigger trajectories with configurable behavior models that meet specification constraints\\n3. Implement negative training with patched trajectories to enhance attack stealthiness\\n4.Quantitative evaluation with iterative refinement ensures trajectory accuracy.\\n\\n", "result": "Demonstrated 100\\u202f%-effective trajectory-based backdoor attacks on 5 offline RL driving agents with 6 different attack patterns, showing existing AD systems are under-exposed to temporal strategy attacks.\\n\\n", "conclusion": "Existing evaluations of end-to-end autonomous driving systems\\u2019 security are insufficient against trajectory-pattern based backdoor attacks, suggesting a need for developing new defense strategies considering temporal behavior patterns.\\n\\n"}}
{"id": "2509.17629", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.17629", "abs": "https://arxiv.org/abs/2509.17629", "authors": ["Antonio Bucchiarone", "Juri Di Rocco", "Damiano Di Vincenzo", "Alfonso Pierantonio"], "title": "From OCL to JSX: declarative constraint modeling in modern SaaS tools", "comment": "10 pages, 2 Figures, Joint Proceedings of the STAF 2025 Workshops:\n  OCL, OOPSLE, LLM4SE, ICMM, AgileMDE, AI4DPS, and TTC. Koblenz, Germany, June\n  10-13, 2025", "summary": "The rise of Node.js in 2010, followed by frameworks like Angular, React, and\nVue.js, has accelerated the growth of low code development platforms. These\nplatforms harness modern UIX paradigms, component-based architectures, and the\nSaaS model to enable non-experts to build software. The widespread adoption of\nsingle-page applications (SPAs), driven by these frameworks, has shaped\nlow-code tools to deliver responsive, client side experiences. In parallel,\nmany modeling platforms have moved to the cloud, adopting either server-centric\narchitectures (e.g., GSLP) or client-side intelligence via SPA frameworks,\nanchoring core components in JavaScript or TypeScript. Within this context,\nOCL.js, a JavaScript-based implementation of the Object Constraint Language,\noffers a web aligned approach to model validation, yet faces challenges such as\npartial standard coverage, limited adoption, and weak integration with modern\nfront-end toolchains. In this paper, we explore JSX, a declarative, functional\nsubset of JavaScript/TypeScript used in the React ecosystem, as an alternative\nto constraint expression in SaaS-based modeling environments. Its\ncomponent-oriented structure supports inductive definitions for syntax, code\ngeneration, and querying. Through empirical evaluation, we compare JSX-based\nconstraints with OCL.js across representative modeling scenarios. Results show\nJSX provides broader expressiveness and better fits front-end-first\narchitectures, indicating a promising path for constraint specification in\nmodern modeling tools.", "AI": {"tldr": "This paper evaluates JSX, a React ecosystem tool, to address limitations in JavaScript-based model validation (OCL.js) for SaaS modeling environments, showing JSX's superior expressiveness and front-end integration.", "motivation": "The shift to client-side, component-driven SPAs and cloud-based modeling platforms necessitates constraint languages that align with modern front-end architectures, but existing solutions like OCL.js have limited adoption and integration.", "method": "The authors propose JSX as an alternative for constraint expression, leveraging its declarative syntax and component structure. They conduct empirical comparisons with OCL.js through modeling scenarios, analyzing expressiveness and integration.", "result": "JSX-based constraints demonstrated broader expressiveness, better alignment with front-end-first architectures, and improved integration potential compared to OCL.js in evaluated modeling scenarios.", "conclusion": "JSX offers a promising path for constraint specification in modern modeling tools, aligning with industry trends toward declarative, component-based front-end development."}}
{"id": "2509.16987", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16987", "abs": "https://arxiv.org/abs/2509.16987", "authors": ["Vyron Kampourakis", "Christos Smiliotopoulos", "Vasileios Gkioulos", "Sokratis Katsikas"], "title": "In Numeris Veritas: An Empirical Measurement of Wi-Fi Integration in Industry", "comment": null, "summary": "Traditional air gaps in industrial systems are disappearing as IT\ntechnologies permeate the OT domain, accelerating the integration of wireless\nsolutions like Wi-Fi. Next-generation Wi-Fi standards (IEEE 802.11ax/be) meet\nperformance demands for industrial use cases, yet their introduction raises\nsignificant security concerns. A critical knowledge gap exists regarding the\nempirical prevalence and security configuration of Wi-Fi in real-world\nindustrial settings. This work addresses this by mining the global crowdsourced\nWiGLE database to provide a data-driven understanding. We create the first\npublicly available dataset of 1,087 high-confidence industrial Wi-Fi networks,\nexamining key attributes such as SSID patterns, encryption methods, vendor\ntypes, and global distribution. Our findings reveal a growing adoption of Wi-Fi\nacross industrial sectors but underscore alarming security deficiencies,\nincluding the continued use of weak or outdated security configurations that\ndirectly expose critical infrastructure. This research serves as a pivotal\nreference point, offering both a unique dataset and practical insights to guide\nfuture investigations into wireless security within industrial environments.", "AI": {"tldr": "This paper analyzes global industrial Wi-Fi security by creating a dataset of 1,087 networks from WiGLE, revealing widespread adoption of IEEE 802.11ax/be but alarming use of weak security configurations in critical infrastructure.", "motivation": "The integration of IT into OT systems is accelerating industrial Wi-Fi adoption (especially next-gen standards), but raises critical security concerns. Current understanding lacks empirical data on real-world deployment patterns and vulnerabilities.", "method": "First publicly available industrial Wi-Fi dataset was created by mining the WiGLE crowdsourced database. Analyzed 1,087 high-confidence networks across SSID patterns, encryption methods, vendor types, and global distribution.", "result": "Confirmed growing industrial Wi-Fi adoption while exposing persistent security issues: weak/outdated encryption remains prevalent, directly exposing critical infrastructure to threats. Dataset and findings offer new baseline for further research.", "conclusion": "Establishes foundational understanding of industrial Wi-Fi security landscape. By combining dataset and empirical analysis, the work directly informs future wireless security research and practice in industrial environments."}}
{"id": "2509.17776", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.17776", "abs": "https://arxiv.org/abs/2509.17776", "authors": ["Cristina Stratan", "Claudio Mandrioli", "Domenico Bianculli"], "title": "Diagnosing Violations of State-based Specifications in iCFTL", "comment": null, "summary": "As modern software systems grow in complexity and operate in dynamic\nenvironments, the need for runtime analysis techniques becomes a more critical\npart of the verification and validation process. Runtime verification monitors\nthe runtime system behaviour by checking whether an execution trace - a\nsequence of recorded events - satisfies a given specification, yielding a\nBoolean or quantitative verdict. However, when a specification is violated,\nsuch a verdict is often insufficient to understand why the violation happened.\nTo fill this gap, diagnostics approaches aim to produce more informative\nverdicts. In this paper, we address the problem of generating informative\nverdicts for violated Inter-procedural Control-Flow Temporal Logic (iCFTL)\nspecifications that express constraints over program variable values. We\npropose a diagnostic approach based on backward data-flow analysis to\nstatically determine the relevant statements contributing to the specification\nviolation. Using this analysis, we instrument the program to produce enriched\nexecution traces. Using the enriched execution traces, we perform the runtime\nanalysis and identify the statements whose execution led to the specification\nviolation. We implemented our approach in a prototype tool, iCFTL-Diagnostics,\nand evaluated it on 112 specifications across 10 software projects. Our tool\nachieves 90% precision in identifying relevant statements for 100 of the 112\nspecifications. It reduces the number of lines that have to be inspected for\ndiagnosing a violation by at least 90%. In terms of computational cost,\niCFTL-Diagnostics generates a diagnosis within 7 min, and requires no more than\n25 MB of memory. The instrumentation required to support diagnostics incurs an\nexecution time overhead of less than 30% and a memory overhead below 20%.", "AI": {"tldr": "The paper introduces iCFTL-Diagnostics, a tool that enhances runtime verification by identifying the root causes of violated iCFTL specifications through backward data-flow analysis and enriched execution traces.", "motivation": "Traditional runtime verification only provides Boolean verdicts for specification violations; this lack of detailed diagnostic information hinders efficient debugging of complex software systems operating in dynamic environments.", "method": "The authors use backward data-flow analysis to determine relevant source code statements contributing to violations, instrument programs to generate enriched execution traces, and perform runtime analysis to pinpoint violating statements. The implementation is evaluated using 112 specifications across 10 projects.", "result": "The tool achieves 90% precision in identifying relevant statements for 100/112 specifications, reduces diagnostic code inspection by 90%, generates diagnoses within 7 minutes, and incurs <30% execution overhead and <20MB memory overhead.", "conclusion": "iCFTL-Diagnostics effectively addresses the diagnostic limitations of runtime verification for iCFTL specifications by combining static analysis and enriched execution traces, offering high precision and efficiency for debugging in real-world software projects."}}
{"id": "2509.17048", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17048", "abs": "https://arxiv.org/abs/2509.17048", "authors": ["Huifang Yu", "Jiaxing Jie", "Lei Li"], "title": "Electronic Reporting Using SM2-Based Ring Signcryption", "comment": null, "summary": "Electronic whistleblowing systems are widely used due to their efficiency and\nconvenience. The key to designing such systems lies in protecting the identity\nprivacy of whistleblowers, preventing malicious whistleblowing, and ensuring\nthe confidentiality of whistleblowing information. To address these issues, a\nSM2 traceable ring signcryption scheme for electronic voting is proposed. This\nscheme combines the SM2 elliptic curve public key cryptography algorithm with\nthe ring signature algorithm, enhancing the overall efficiency of the scheme\nwhile ensuring the autonomy and controllability of the core cryptographic\nalgorithms. Security analysis demonstrates that the proposed scheme satisfies\nconfidentiality, unforgeability, traceability, linkability, and deniability.\nEfficiency analysis shows that, compared to existing ring signature schemes,\nthe proposed scheme exhibits significant efficiency advantages during the\nsignature phase. The electronic whistleblowing system designed using the\nproposed scheme can track malicious whistleblowers while protecting user\nidentity privacy, and ensures that the content of whistleblowing remains\nunknown to third parties.", "AI": {"tldr": "A secure and efficient electronic whistleblowing system based on SM2 traceable ring signcryption is proposed to protect whistleblower identity privacy and prevent malicious activities while ensuring information confidentiality.", "motivation": "The paper addresses the need to protect whistleblower identity privacy, prevent malicious whistleblowing, and ensure the confidentiality of whistleblowing information in electronic systems.", "method": "The authors combined the SM2 elliptic curve public key cryptography algorithm with the ring signature algorithm to propose a SM2 traceable ring signcryption scheme for electronic voting.", "result": "Security analysis shows the scheme provides confidentiality, unforgeability, traceability, linkability, and deniability. Efficiency analysis indicates significant advantages in the signature phase compared to existing ring signature schemes.", "conclusion": "The designed system using the SM2 traceable ring signcryption scheme can track malicious whistleblowers while protecting user identity privacy and keeping whistleblowing content confidential from third parties."}}
{"id": "2509.17070", "categories": ["cs.CR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17070", "abs": "https://arxiv.org/abs/2509.17070", "authors": ["Mayukh Borana", "Junyi Liang", "Sai Sathiesh Rajan", "Sudipta Chattopadhyay"], "title": "Localizing Malicious Outputs from CodeLLM", "comment": "10 pages, 2 figures, 6 tables, Accepted at EMNLP 2025 Findings", "summary": "We introduce FreqRank, a mutation-based defense to localize malicious\ncomponents in LLM outputs and their corresponding backdoor triggers. FreqRank\nassumes that the malicious sub-string(s) consistently appear in outputs for\ntriggered inputs and uses a frequency-based ranking system to identify them.\nOur ranking system then leverages this knowledge to localize the backdoor\ntriggers present in the inputs. We create nine malicious models through\nfine-tuning or custom instructions for three downstream tasks, namely, code\ncompletion (CC), code generation (CG), and code summarization (CS), and show\nthat they have an average attack success rate (ASR) of 86.6%. Furthermore,\nFreqRank's ranking system highlights the malicious outputs as one of the top\nfive suggestions in 98% of cases. We also demonstrate that FreqRank's\neffectiveness scales as the number of mutants increases and show that FreqRank\nis capable of localizing the backdoor trigger effectively even with a limited\nnumber of triggered samples. Finally, we show that our approach is 35-50% more\neffective than other defense methods.", "AI": {"tldr": "This paper presents FreqRank, a method to detect and localize malicious components and backdoor triggers in LLM outputs. The approach is based on the assumption that malicious substrings appear consistently when triggered. It outperforms existing defense methods by 35-50%.", "motivation": "The motivation behind this paper is to address the problem of malicious components and backdoor triggers in Large Language Models (LLMs), which can be exploited to produce harmful or backdoored outputs.", "method": "FreqRank uses a frequency-based ranking system that assumes malicious substrings consistently appear when the model is triggered by input samples. Then ranking system leverages to locate the backdoor trigger in the inputs.", "result": "FreqRank achieves a high localization accuracy, highlighting malicious outputs as one of the top five suggestions in 98% of cases. The method's effectiveness increases with more mutants and works even with few triggered samples. It is also shown to be 35-50% more effective than other defense methods.", "conclusion": "The conclusion is that FreqRank provides a novel and effective defense against backdoor triggers in LLM outputs by effectively localizing malicious components, even in cases with limited data, and outperforms existing methods by a significant margin."}}
{"id": "2509.17126", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17126", "abs": "https://arxiv.org/abs/2509.17126", "authors": ["Stefanos Chaliasos", "Conner Swann", "Sina Pilehchiha", "Nicolas Mohnblatt", "Benjamin Livshits", "Assimakis Kattis"], "title": "Unaligned Incentives: Pricing Attacks Against Blockchain Rollups", "comment": null, "summary": "Rollups have become the de facto scalability solution for Ethereum, securing\nmore than $55B in assets. They achieve scale by executing transactions on a\nLayer 2 ledger, while periodically posting data and finalizing state on the\nLayer 1, either optimistically or via validity proofs. Their fees must\nsimultaneously reflect the pricing of three resources: L2 costs (e.g.,\nexecution), L1 DA, and underlying L1 gas costs for batch settlement and proof\nverification. In this work, we identify critical mis-pricings in existing\nrollup transaction fee mechanisms (TFMs) that allow for two powerful attacks.\nFirstly, an adversary can saturate the L2's DA batch capacity with\ncompute-light data-heavy transactions, forcing low-gas transaction batches that\nenable both L2 DoS attacks, and finality-delay attacks. Secondly, by crafting\nprover killer transactions that maximize proving cycles relative to the gas\ncharges, an adversary can effectively stall proof generation, delaying finality\nby hours and inflicting prover-side economic losses to the rollup at a minimal\ncost.\n  We analyze the above attack vectors across the major Ethereum rollups,\nquantifying adversarial costs and protocol losses. We find that the first\nattack enables periodic DoS on rollups, lasting up to 30 minutes, at a cost\nbelow 2 ETH for most rollups. Moreover, we identify three rollups that are\nexposed to indefinite DoS at a cost of approximately 0.8 to 2.7 ETH per hour.\nThe attack can be further modified to increase finalization delays by a factor\nof about 1.45x to 2.73x, compared to direct L1 blob-stuffing, depending on the\nrollup's parameters. Furthermore, we find that the prover killer attack induces\na finalization latency increase of about 94x. Finally, we propose comprehensive\nmitigations to prevent these attacks and suggest how some practical uses of\nmulti-dimensional rollup TFMs can rectify the identified mis-pricing attacks.", "AI": {"tldr": "This paper identifies critical vulnerabilities in Ethereum rollup transaction fee mechanisms (TFMs) enabling adversarial attacks, analyzes their impact across major rollups, and proposes mitigations. Attacks include data-heavy DoS and prover-killing transactions, with quantified costs (as low as 0.8 ETH/hour) and up to 94x finality delays.", "motivation": "Ethereum rollups, securing $55B in assets, must address TFM mispricings to prevent security risks and maintain scalability. Existing TFMs fail to adequately balance Layer 2 execution, Layer 1 data availability (DA), and verification costs.", "method": "The authors (1) analyze TFM mispricings, (2) design adversarial attack vectors (data-heavy transactions and prover killers), (3) evaluate attack costs and protocol losses across leading rollups, and (4 ) assess finality delays via comparison to L1 blob-stuffing methods.", "result": "Key findings: (1.) Data-heavy attacks cause up to 15-minute DoS for <2 ETH, indefinite DoS for 0.8-2.7 ETH/hour on three rollups, and 1.45x-2.73x finality delays. (2.) Prover killer attacks induce 94x finality latency. (3.) Analysis spans major Ethereum rollup implementations.", "conclusion": "The paper proposes TFM redesigns to address mispricings, including multi-dimensional pricing mechanisms. It advocates for fee structures that properly value data, computation, and proving costs to prevent adversarial exploitation while maintaining rollup scalability."}}
{"id": "2509.17185", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17185", "abs": "https://arxiv.org/abs/2509.17185", "authors": ["Bence So\u00f3ki-T\u00f3th", "Istv\u00e1n Andr\u00e1s Seres", "Kamilla Kara", "\u00c1bel Nagy", "Bal\u00e1zs Pej\u00f3", "Gergely Bicz\u00f3k"], "title": "Bribers, Bribers on The Chain, Is Resisting All in Vain? Trustless Consensus Manipulation Through Bribing Contracts", "comment": "pre-print", "summary": "The long-term success of cryptocurrencies largely depends on the incentive\ncompatibility provided to the validators. Bribery attacks, facilitated\ntrustlessly via smart contracts, threaten this foundation. This work\nintroduces, implements, and evaluates three novel and efficient bribery\ncontracts targeting Ethereum validators. The first bribery contract enables a\nbriber to fork the blockchain by buying votes on their proposed blocks. The\nsecond contract incentivizes validators to voluntarily exit the consensus\nprotocol, thus increasing the adversary's relative staking power. The third\ncontract builds a trustless bribery market that enables the briber to auction\noff their manipulative power over the RANDAO, Ethereum's distributed randomness\nbeacon. Finally, we provide an initial game-theoretical analysis of one of the\ndescribed bribery markets.", "AI": {"tldr": "This paper presents three trustless bribery contracts targeting Ethereum validators, enabling blockchain forks, validator exits, and RANDAO manipulation, along with a game-theoretical analysis of one market.", "motivation": "Cryptocurrencies rely on validator incentives, but trustless smart contract-facilitated bribery attacks threaten this foundation.", "method": "Three novel bribery contracts were developed: (1) fork-enabling vote-buying, (2) validator exit incentives, and (3) RANDAO manipulation through auctioned bribe markets. One market underwent game-theoretical analysis.", "result": "Implementation and evaluation of the contracts demonstrate their efficiency and potential threat. Game-theoretical analysis provides initial insights into market dynamics.", "conclusion": "The presented contracts highlight critical vulnerabilities in Ethereum's staking model and randomness mechanisms. Their analysis underscores the need for countermeasures against trustless collusion attacks."}}
{"id": "2509.17253", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17253", "abs": "https://arxiv.org/abs/2509.17253", "authors": ["Selma Yahia", "Ildi Alla", "Girija Bangalore Mohan", "Daniel Rau", "Mridula Singh", "Valeria Loscri"], "title": "Seeing is Deceiving: Mirror-Based LiDAR Spoofing for Autonomous Vehicle Deception", "comment": null, "summary": "Autonomous vehicles (AVs) rely heavily on LiDAR sensors for accurate 3D\nperception. We show a novel class of low-cost, passive LiDAR spoofing attacks\nthat exploit mirror-like surfaces to inject or remove objects from an AV's\nperception. Using planar mirrors to redirect LiDAR beams, these attacks require\nno electronics or custom fabrication and can be deployed in real settings. We\ndefine two adversarial goals: Object Addition Attacks (OAA), which create\nphantom obstacles, and Object Removal Attacks (ORA), which conceal real\nhazards. We develop geometric optics models, validate them with controlled\noutdoor experiments using a commercial LiDAR and an Autoware-equipped vehicle,\nand implement a CARLA-based simulation for scalable testing. Experiments show\nmirror attacks corrupt occupancy grids, induce false detections, and trigger\nunsafe planning and control behaviors. We discuss potential defenses (thermal\nsensing, multi-sensor fusion, light-fingerprinting) and their limitations.", "AI": {"tldr": "Researchers demonstrate cost-effective LiDAR spoofing attacks using mirrors to deceive self-driving cars into seeing/non-seeing objects, with experiments showing these threats can bypass existing perception systems and evade proposed defenses effectively.", "motivation": "This work addresses critical security vulnerabilities in AV LiDAR systems by demonstrating low-cost, hardware-free spoofing attacks using everyday reflective materials, which prior research has overlooked in real-world deployment scenarios.", "method": "The researchers employed geometric optics modeling, conducted real-world outdoor experiments with commercial LiDAR and an Autoware car, and implemented CARLA simulations to validate passive mirror-based attacks for object insertion/removal.", "result": "Experiments confirmed mirror attacks can distort occupancy grids, generate false obstacles, cause AVs to fail object detection, and induce safety-critical control failures, while proposed defenses face technical limitations.", "conclusion": "The study concludes that mirror-based LiDAR spoofing attacks pose significant security risks to autonomous vehicles by manipulating perception, highlighting urgent needs for robust defense mechanisms against passive optical threats."}}
{"id": "2509.17263", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17263", "abs": "https://arxiv.org/abs/2509.17263", "authors": ["Colman McGuan", "Aadithyan V. Raghavan", "Komala M. Mandapati", "Chansu Yu", "Brian E. Ray", "Debbie K. Jackson", "Sathish Kumar"], "title": "Bridging Cybersecurity Practice and Law: a Hands-on, Scenario-Based Curriculum Using the NICE Framework to Foster Skill Development", "comment": null, "summary": "In an increasingly interconnected world, cybersecurity professionals play a\npivotal role in safeguarding organizations from cyber threats. To secure their\ncyberspace, organizations are forced to adopt a cybersecurity framework such as\nthe NIST National Initiative for Cybersecurity Education Workforce Framework\nfor Cybersecurity (NICE Framework). Although these frameworks are a good\nstarting point for businesses and offer critical information to identify,\nprevent, and respond to cyber incidents, they can be difficult to navigate and\nimplement, particularly for small-medium businesses (SMB). To help overcome\nthis issue, this paper identifies the most frequent attack vectors to SMBs\n(Objective 1) and proposes a practical model of both technical and\nnon-technical tasks, knowledge, skills, abilities (TKSA) from the NICE\nFramework for those attacks (Objective 2). The research develops a\nscenario-based curriculum. By immersing learners in realistic cyber threat\nscenarios, their practical understanding and preparedness in responding to\ncybersecurity incidents is enhanced (Objective 3). Finally, this work\nintegrates practical experience and real-life skill development into the\ncurriculum (Objective 4). SMBs can use the model as a guide to evaluate, equip\ntheir existing workforce, or assist in hiring new employees. In addition,\neducational institutions can use the model to develop scenario-based learning\nmodules to adequately equip the emerging cybersecurity workforce for SMBs.\nTrainees will have the opportunity to practice both technical and legal issues\nin a simulated environment, thereby strengthening their ability to identify,\nmitigate, and respond to cyber threats effectively.", "AI": {"tldr": "This paper designs a scenario-driven cybersecurity model for SMBs using the NIST NICE Framework, combining technical and non-technical training to improve real-world threat response and workforce preparation.", "motivation": "Small and medium businesses (SMBs) face significant challenges in implementing cybersecurity frameworks like NIST NICE, necessitating simplified, practical solutions to identify, prepare, and respond to cyber threats effectively.", "method": "The paper employs a four-pronged approach: identifying frequent attack vectors, proposing a TKSA-based NICE Framework model, developing a scenario-based curriculum for immersive learning, and integrating real-world skill development.", "result": "The research delivers a scenario-based training model for SMBs to evaluate and enhance workforce capabilities, alongside a curriculum for educational institutions to produce adequately trained cybersecurity professionals.", "conclusion": "The paper concludes that integrating scenario-based learning and practical experience, aligned with the NIST NICE Framework's TKSA, provides a structured approach for SMBs to enhance their cybersecurity workforce readiness through realistic training and workforce evaluation."}}
{"id": "2509.17266", "categories": ["cs.CR", "cs.IT", "cs.SY", "eess.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.17266", "abs": "https://arxiv.org/abs/2509.17266", "authors": ["Farhad Farokhi"], "title": "Privacy-Preserving State Estimation with Crowd Sensors: An Information-Theoretic Respective", "comment": "Accepted for presentation at the 17th IEEE International Workshop on\n  Information Forensics and Security (WIFS2025)", "summary": "Privacy-preserving state estimation for linear time-invariant dynamical\nsystems with crowd sensors is considered. At any time step, the estimator has\naccess to measurements from a randomly selected sensor from a pool of sensors\nwith pre-specified models and noise profiles. A Luenberger-like observer is\nused to fuse the measurements with the underlying model of the system to\nrecursively generate the state estimates. An additive privacy-preserving noise\nis used to constrain information leakage. Information leakage is measured via\nmutual information between the identity of the sensors and the state estimate\nconditioned on the actual state of the system. This captures an omnipotent\nadversary that not only can access state estimates but can also gather direct\nhigh-quality state measurements. Any prescribed level of information leakage is\nshown to be achievable by appropriately selecting the variance of the\nprivacy-preserving noise. Therefore, privacy-utility trade-off can be\nfine-tuned.", "AI": {"tldr": "This paper proposed privacy-preserving state estimation method for LTI system using randomly selected crowd sensors, and it is possible to achieve any desired level of privacy by adjusting noise variance added during estimation process.", "motivation": "Existing methods of state estimation in LTI systems using crowd sensors do not account for privacy concerns. The paper addresses this by suggesting a method that limits the information released through state estimation.", "method": "Paper introduces a Luenberger-like observer, fusing measured data from a randomly sampled sensor with the actual system's model. Then, a specific property of mutual information has been leveraged to internally quantify the information leak and ensure the model preserves the privacy through the mathematical proof of this property, not just by selecting low leakage values.", "result": "It is validated that through adjusting the variance of added privacy-preserving noise, any predetermined information leakage level can be achieved. Thus, the paper presents a technically robust solution that allows for a precise adjustment of the privacy-utility trade-off in the state estimation process.", "conclusion": "The proposed method not only effectively handles the privacy-utility trade-off in state estimation involving crowd sensors but also provides an analytical assurance that it can achieve any desired privacy level, making it a reliable framework for applications where privacy is a concern."}}
{"id": "2509.17302", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17302", "abs": "https://arxiv.org/abs/2509.17302", "authors": ["Duoxun Tang", "Xinhang Jiang", "Jiajun Niu"], "title": "TextCrafter: Optimization-Calibrated Noise for Defending Against Text Embedding Inversion", "comment": null, "summary": "Text embedding inversion attacks reconstruct original sentences from latent\nrepresentations, posing severe privacy threats in collaborative inference and\nedge computing. We propose TextCrafter, an optimization-based adversarial\nperturbation mechanism that combines RL learned, geometry aware noise injection\northogonal to user embeddings with cluster priors and PII signal guidance to\nsuppress inversion while preserving task utility. Unlike prior defenses either\nnon learnable or agnostic to perturbation direction, TextCrafter provides a\ndirectional protective policy that balances privacy and utility. Under strong\nprivacy setting, TextCrafter maintains 70 percentage classification accuracy on\nfour datasets and consistently outperforms Gaussian/LDP baselines across lower\nprivacy budgets, demonstrating a superior privacy utility trade off.", "AI": {"tldr": "TextCrafter defends against text inversion attacks using RL-guided directional perturbations, maintaining 70% accuracy under strict privacy while surpassing baseline methods in privacy-utility tradeoffs.", "motivation": "Text embedding inversion attacks threaten privacy in collaborative inference systems by reconstructing sensitive input text from latent representations. Existing defenses either lack learnability or ignore directional perturbation strategies, necessitating a more effective privacy-utility balanced solution.", "method": "TextCrafter combines reinforcement learning (RL), geometry-aware orthogonal noise injection, cluster priors, and PII (Personally Identifiable Information) guidance to create adversarial perturbations that suppress text inversion attacks without compromising task utility.", "result": "Achieved 70% classification accuracy under strong privacy settings across four datasets, while consistently outperforming Gaussian noise and LDP (Local Differential Privacy) baselines at lower privacy budgets.", "conclusion": "TextCrafter effectively balances privacy and utility by introducing a directional perturbation mechanism that outperforms existing baselines while maintaining significant task performance."}}
{"id": "2509.17371", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17371", "abs": "https://arxiv.org/abs/2509.17371", "authors": ["Haotian Xu", "Qingsong Peng", "Jie Shi", "Huadi Zheng", "Yu Li", "Cheng Zhuo"], "title": "SilentStriker:Toward Stealthy Bit-Flip Attacks on Large Language Models", "comment": null, "summary": "The rapid adoption of large language models (LLMs) in critical domains has\nspurred extensive research into their security issues. While input manipulation\nattacks (e.g., prompt injection) have been well studied, Bit-Flip Attacks\n(BFAs) -- which exploit hardware vulnerabilities to corrupt model parameters\nand cause severe performance degradation -- have received far less attention.\nExisting BFA methods suffer from key limitations: they fail to balance\nperformance degradation and output naturalness, making them prone to discovery.\nIn this paper, we introduce SilentStriker, the first stealthy bit-flip attack\nagainst LLMs that effectively degrades task performance while maintaining\noutput naturalness. Our core contribution lies in addressing the challenge of\ndesigning effective loss functions for LLMs with variable output length and the\nvast output space. Unlike prior approaches that rely on output perplexity for\nattack loss formulation, which inevitably degrade output naturalness, we\nreformulate the attack objective by leveraging key output tokens as targets for\nsuppression, enabling effective joint optimization of attack effectiveness and\nstealthiness. Additionally, we employ an iterative, progressive search strategy\nto maximize attack efficacy. Experiments show that SilentStriker significantly\noutperforms existing baselines, achieving successful attacks without\ncompromising the naturalness of generated text.", "AI": {"tldr": "SilentStriker introduces a stealthy bit-flip attack for LLMs that maintains output naturalness while causing performance degradation, overcoming limitations of prior methods.", "motivation": "Existing bit-flip attacks fail to balance performance degradation and output naturalness, limiting their practicality and stealthiness.", "method": "Reformulates attack loss using key output token suppression and employs an iterative, progressive search strategy to optimize effectiveness and stealthiness.", "result": "Experiments demonstrate SilentStriker significantly outperforms prior baselines in both attack efficacy and maintaining text naturalness compared to perplexity-based methods.", "conclusion": "SilentStriker addresses key challenges in BFA design by introducing a token-based loss formulation and progressive search strategy, setting new benchmarks for stealthy LLM attacks."}}
{"id": "2509.17409", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17409", "abs": "https://arxiv.org/abs/2509.17409", "authors": ["Yao Wu", "Ziye Jia", "Qihui Wu", "Yian Zhu"], "title": "A Lightweight Authentication and Key Agreement Protocol Design for FANET", "comment": null, "summary": "The advancement of low-altitude intelligent networks enables unmanned aerial\nvehicle (UAV) interconnection via flying ad-hoc networks (FANETs), offering\nflexibility and decentralized coordination. However, resource constraints,\ndynamic topologies, and UAV operations in open environments present significant\nsecurity and communication challenges. Existing multi-factor and public-key\ncryptography protocols are vulnerable due to their reliance on stored sensitive\ninformation, increasing the risk of exposure and compromise. This paper\nproposes a lightweight authentication and key agreement protocol for FANETs,\nintegrating physical unclonable functions with dynamic credential management\nand lightweight cryptographic primitives. The protocol reduces computational\nand communication overhead while enhancing security. Security analysis confirms\nits resilience against various attacks, and comparative evaluations demonstrate\nits superiority in security, communication efficiency, and computational cost.", "AI": {"tldr": "This paper proposes a lightweight authentication and key agreement protocol for FANETs integrating PUFs and dynamic credentials to enhance security and efficiency.", "motivation": "FANETs face security and communication challenges due to resource constraints, dynamic topologies, and open environments. Existing protocols are vulnerable as they rely on stored sensitive information.", "method": "The protocol integrates physical unclonable functions (PUFs), dynamic credential management, and lightweight cryptographic primitives to reduce overhead and enhance security.", "result": "Security analysis confirms resilience against attacks, and evaluations show superiority in security, communication efficiency, and computational cost compared to existing protocols.", "conclusion": "The proposed protocol effectively addresses FANETs' security limitations with a lightweight, efficient solution."}}
{"id": "2509.17416", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17416", "abs": "https://arxiv.org/abs/2509.17416", "authors": ["Jianbin Ji", "Dawen Xu", "Li Dong", "Lin Yang", "Songhan He"], "title": "DINVMark: A Deep Invertible Network for Video Watermarking", "comment": "Accepted by IEEE Transaction on Multimedia (2025)", "summary": "With the wide spread of video, video watermarking has become increasingly\ncrucial for copyright protection and content authentication. However, video\nwatermarking still faces numerous challenges. For example, existing methods\ntypically have shortcomings in terms of watermarking capacity and robustness,\nand there is a lack of specialized noise layer for High Efficiency Video\nCoding(HEVC) compression. To address these issues, this paper introduces a Deep\nInvertible Network for Video watermarking (DINVMark) and designs a noise layer\nto simulate HEVC compression. This approach not only in creases watermarking\ncapacity but also enhances robustness. DINVMark employs an Invertible Neural\nNetwork (INN), where the encoder and decoder share the same network structure\nfor both watermark embedding and extraction. This shared architecture ensures\nclose coupling between the encoder and decoder, thereby improving the accuracy\nof the watermark extraction process. Experimental results demonstrate that the\nproposed scheme significantly enhances watermark robustness, preserves video\nquality, and substantially increases watermark embedding capacity.", "AI": {"tldr": "This paper proposes DINVMark, a Deep Invertible Network for video watermarking that improves capacity and robustness through an INN-based architecture and a HEVC compression-simulated noise layer.", "motivation": "Existing video watermarking methods suffer from limited capacity, poor robustness, and lack of HEVC-specific noise layers, necessitating a more effective solution for copyright protection and authentication.", "method": "DINVMark employs an Invertible Neural Network (INN), sharing encoder-decoder structures for watermarking and extraction, and introduces a noise layer to simulate HEVC compression, ensuring tight encoder-decoder coupling.", "result": "Experiments show significant robustness improvements, higher embedding capacity, and preserved video quality under HEVC compression compared to prior methods.", "conclusion": "DINVMark addresses key challenges in video watermarking by leveraging invertible networks and HEVC-simulated noise, achieving superior performance in capacity, robustness, and quality."}}
{"id": "2509.17488", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17488", "abs": "https://arxiv.org/abs/2509.17488", "authors": ["Shouju Wang", "Fenglin Yu", "Xirui Liu", "Xiaoting Qin", "Jue Zhang", "Qingwei Lin", "Dongmei Zhang", "Saravan Rajmohan"], "title": "Privacy in Action: Towards Realistic Privacy Mitigation and Evaluation for LLM-Powered Agents", "comment": "To appear at EMNLP 2025 (Findings)", "summary": "The increasing autonomy of LLM agents in handling sensitive communications,\naccelerated by Model Context Protocol (MCP) and Agent-to-Agent (A2A)\nframeworks, creates urgent privacy challenges. While recent work reveals\nsignificant gaps between LLMs' privacy Q&A performance and their agent\nbehavior, existing benchmarks remain limited to static, simplified scenarios.\nWe present PrivacyChecker, a model-agnostic, contextual integrity based\nmitigation approach that effectively reduces privacy leakage from 36.08% to\n7.30% on DeepSeek-R1 and from 33.06% to 8.32% on GPT-4o, all while preserving\ntask helpfulness. We also introduce PrivacyLens-Live, transforming static\nbenchmarks into dynamic MCP and A2A environments that reveal substantially\nhigher privacy risks in practical. Our modular mitigation approach integrates\nseamlessly into agent protocols through three deployment strategies, providing\npractical privacy protection for the emerging agentic ecosystem. Our data and\ncode will be made available at https://aka.ms/privacy_in_action.", "AI": {"tldr": "Addresses agentic LLM privacy risks with novel benchmarking and mitigation frameworks achieving 75%+ privacy leakage reduction across major models", "motivation": "Autonomous LLM agents using MCP/A2A frameworks reveal significant privacy leakage risks that static benchmarks fail to capture, with performance gaps between privacy Q&A capabilities and actual agent behavior.", "method": "Proposes PrivacyChecker (contextual integrity-based mitigation) and PrivacyLens-Live (dynamic benchmarking framework) with three deployment strategies for agent protocols.", "result": "Reduces privacy leakage to 7.30% (DeepSeek-R1) and 8.32% (GPT-4o) while maintaining task helpfulness; dynamic benchmarking identifies substantially higher practical risks.", "conclusion": "The presented approach offers practical privacy protection for emerging agentic ecosystems through modular integration into agent protocols, reducing privacy risks in real-world applications."}}
{"id": "2509.17508", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.17508", "abs": "https://arxiv.org/abs/2509.17508", "authors": ["Eric Filiol"], "title": "Community Covert Communication - Dynamic Mass Covert Communication Through Social Media", "comment": "22 pages, 8 figures, this work has been presented at 44CON 2024 &\n  44CON 2025 in London", "summary": "Since the early 2010s, social network-based influence technologies have grown\nalmost exponentially. Initiated by the U.S. Army's early OEV system in 2011, a\nnumber of companies specializing in this field have emerged. The most\n(in)famous cases are Bell Pottinger, Cambridge Analytica, Aggregate-IQ and,\nmore recently, Team Jorge.\n  In this paper, we consider the use-case of sock puppet master activities,\nwhich consist in creating hundreds or even thousands of avatars, in organizing\nthem into communities and implement influence operations. On-purpose software\nis used to automate these operations (e.g. Ripon software, AIMS) and organize\nthese avatar populations into communities. The aim is to organize targeted and\ndirected influence communication to rather large communities (influence\ntargets).\n  The goal of the present research work is to show how these community\nmanagement techniques (social networks) can also be used to\ncommunicate/disseminate relatively large volumes (up to a few tens of Mb) of\nmulti-level encrypted information to a limited number of actors. To a certain\nextent, this can be compared to a Dark Post-type function, with a number of\nmuch more powerful potentialities. As a consequence, the concept of\ncommunication has been totally redefined and disrupted, so that eavesdropping,\ninterception and jamming operations no longer make sense.", "AI": {"tldr": "The paper examines the evolution of social network-based influence technologies and explores how techniques used for managing avatar communities can facilitate the encrypted dissemination of large data volumes to specific individuals, fundamentally redefining traditional communication and making eavesdropping and interception ineffective.", "motivation": "The motivation stems from the desire to investigate the potential of using community management techniques on social networks for encrypted communication, building upon the exponential growth of automated influence operations such as those carried out by sock puppet masters.", "method": "The researchers considered the use cases of sock puppet master activities, where specialized software automates the creation and management of avatars within communities to conduct influence operations. They analyzed how these same techniques can be leveraged to transmit multi-level encrypted information across controlled data spaces.", "result": "The research demonstrated that the techniques used for orchestrating large-scale, automated influence operations can be adapted to securely and efficiently disseminate large volumes of encrypted data to targeted actors, disrupting conventional eavesdropping, interception, and jamming methods.", "conclusion": "The study concluded that the principles of managing avatar communities on social networks can redefine communication, leveraging encrypted data transmission to a select group of actors in a way that is resilient to traditional surveillance and interference tactics."}}
{"id": "2509.17595", "categories": ["cs.CR", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.17595", "abs": "https://arxiv.org/abs/2509.17595", "authors": ["Shunnosuke Ikeda", "Kazumasa Shinagawa"], "title": "Impossibility Results of Card-Based Protocols via Mathematical Optimization", "comment": null, "summary": "This paper introduces mathematical optimization as a new method for proving\nimpossibility proofs in the field of card-based cryptography. While previous\nimpossibility proofs were often limited to cases involving a small number of\ncards, this new approach establishes results that hold for a large number of\ncards. The research focuses on single-cut full-open (SCFO) protocols, which\nconsist of performing one random cut and then revealing all cards. The main\ncontribution is that for any three-variable Boolean function, no new SCFO\nprotocols exist beyond those already known, under the condition that all\nadditional cards have the same color. The significance of this work is that it\nprovides a new framework for impossibility proofs and delivers a proof that is\nvalid for any number of cards, as long as all additional cards have the same\ncolor.", "AI": {"tldr": "This paper applies mathematical optimization to prove impossibility of new single-cut full-open (SCFO) card-based protocols for three-variable Boolean functions when all additional cards share the same color, creating a scalable proof framework.", "motivation": "Previous impossibility proofs in card-based cryptography were limited to small card numbers; this work addresses the need for generalizable proofs applicable to large card sets.", "method": "The authors use mathematical optimization techniques to analyze SCFO protocols, which involve one random cut followed by full card revelation, focusing on constraints of three-variable Boolean functions and same-colored additional cards.", "result": "They proved no new SCFO protocols exist for three-variable Boolean functions under the same-colored cards condition, with the framework valid for any number of cards in this scenario.", "conclusion": "The research provides a novel optimization-based framework for impossibility proofs in card-based cryptography, delivering a general result for card counts with same-colored additional cards."}}
{"id": "2509.17709", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17709", "abs": "https://arxiv.org/abs/2509.17709", "authors": ["Masayuki Tezuka", "Keisuke Tanaka"], "title": "Ordered Multi-Signatures with Public-Key Aggregation from SXDH Assumption", "comment": "A preliminary version of this paper is appeared in the 20th\n  International Workshop on Security (IWSEC 2025)", "summary": "An ordered multi-signature scheme allows multiple signers to sign a common\nmessage in a sequential manner and allows anyone to verify the signing order of\nsigners with a public-key list. In this work, we propose an ordered\nmulti-signature scheme by modifying the sequential aggregate signature scheme\nby Chatterjee and Kabaleeshwaran (ACISP 2020). Our scheme offers compact public\nparameter size and the public-key aggregation property. This property allows us\nto compress a public-key list into a short aggregated key. We prove the\nsecurity of our scheme under the symmetric external Diffie-Hellman (SXDH)\nassumption without the random oracle model.", "AI": {"tldr": "This work introduces an efficient ordered multi-signature scheme with public-key aggregation, proven secure under SXDH without random oracles.", "motivation": "To address security and efficiency needs for verifying signer order in multi-signature systems, particularly improving parameter size and eliminating reliance on random oracles.", "method": "The authors modified the sequential aggregate signature scheme by Chatterjee and Kabaleeshwaran (ACISP 2020) to include ordered signing and public-key aggregation properties, enabling short aggregated keys.", "result": "The scheme achieves compact public parameters, public-key aggregation, and formal security guarantees under the SXDH assumption, offering practical advantages over existing methods.", "conclusion": "The proposed ordered multi-signature scheme enhances efficiency through compact parameters and public-key aggregation while maintaining security under the SXDH assumption without relying on the random oracle model."}}
{"id": "2509.17722", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17722", "abs": "https://arxiv.org/abs/2509.17722", "authors": ["Masayuki Tezuka", "Keisuke Tanaka"], "title": "Public Key Encryption with Equality Test from Tag-Based Encryption", "comment": "A preliminary version of this paper is appeared in the 20th\n  International Workshop on Security (IWSEC 2025)", "summary": "Public key encryption with equality test (PKEET), proposed by Yang et al.\n(CT-RSA 2010), is a variant of public key encryption that enables an equality\ntest to determine whether two ciphertexts correspond to the same plaintext.\nThis test applies not only for ciphertexts generated under the same encryption\nkey but also for those generated under different encryption keys. To date,\nseveral generic constructions of PKEET have been proposed. However, these\ngeneric constructions have the drawback of reliance on the random oracle model\nor a (hierarchical) identity-based encryption scheme. In this paper, we propose\na generic construction of a PKEET scheme based on tag-based encryption without\nthe random oracle model. Tag-based encryption is a weaker primitive than\nidentity-based encryption. Our scheme allows to derive new PKEET schemes\nwithout the random oracle model. By instantiating our construction with the\npairing-free tag-based encryption scheme by Kiltz (TCC 2006), we obtain a\npairing-free PKEET scheme without the random oracle model. Moreover, by\ninstantiating our construction with a tag-based encryption scheme based on the\nlearning parity with noise (LPN) assumption, we obtain a PKEET scheme based on\nthe LPN assumption without the random oracle model.", "AI": {"tldr": "This paper proposes a novel generic PKEET construction based on tag-based encryption without requiring the random oracle model or identity-based encryption, enabling practical and secure equality testing across ciphertexts under different keys.", "motivation": "Existing PKEET schemes rely on impractical assumptions (random oracle model or hierarchical identity-based encryption). The authors aim to develop a robust PKEET construction with weaker cryptographic primitives to enhance real-world applicability and security.", "method": "The construction leverages tag-based encryption (a weaker primitive than identity-based encryption) as the foundation. It presents a generic framework and instantiates it with specific tag-based encryption schemes, including a pairing-free model (Kiltz 2006) and a scheme based on the Learning Parity with Noise (LPN)-assumption.", "result": "Achieves a PKEET scheme without the random oracle model; instantiations yield a pairing-free PKEET and one based on the LPN assumption, demonstrating flexibility and security from diverse assumptions.", "conclusion": "The proposed method overcomes limitations of prior work by utilizing tag-based encryption, enabling efficient and secure PKEET schemes with broader practical relevance and theoretical robustness."}}
{"id": "2509.17832", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17832", "abs": "https://arxiv.org/abs/2509.17832", "authors": ["Xiangmin Shen", "Wenyuan Cheng", "Yan Chen", "Zhenyuan Li", "Yuqiao Gu", "Lingzhi Wang", "Wencheng Zhao", "Dawei Sun", "Jiashui Wang"], "title": "AEAS: Actionable Exploit Assessment System", "comment": "AEAS has been implemented in the planning agent of PentestAgent, our\n  LLM-driven automated penetration testing framework. Check out our repository:\n  https://github.com/nbshenxm/pentest-agent", "summary": "Security practitioners face growing challenges in exploit assessment, as\npublic vulnerability repositories are increasingly populated with inconsistent\nand low-quality exploit artifacts. Existing scoring systems, such as CVSS and\nEPSS, offer limited support for this task. They either rely on theoretical\nmetrics or produce opaque probability estimates without assessing whether\nusable exploit code exists. In practice, security teams often resort to manual\ntriage of exploit repositories, which is time-consuming, error-prone, and\ndifficult to scale. We present AEAS, an automated system designed to assess and\nprioritize actionable exploits through static analysis. AEAS analyzes both\nexploit code and associated documentation to extract a structured set of\nfeatures reflecting exploit availability, functionality, and setup complexity.\nIt then computes an actionability score for each exploit and produces ranked\nexploit recommendations. We evaluate AEAS on a dataset of over 5,000\nvulnerabilities derived from 600+ real-world applications frequently\nencountered by red teams. Manual validation and expert review on representative\nsubsets show that AEAS achieves a 100% top-3 success rate in recommending\nfunctional exploits and shows strong alignment with expert-validated rankings.\nThese results demonstrate the effectiveness of AEAS in supporting\nexploit-driven vulnerability prioritization.", "AI": {"tldr": "AEAS is an automated system that prioritizes actionable exploits using static analysis of code and documentation, achieving high accuracy in recommending functional exploits.", "motivation": "Current exploit scoring systems (CVSS, EPSS)", "method": "AEAS performs static analysis on exploit code and documentation to extract features like availability, functionality, and setup complexity, generating an actionability score and ranked recommendations.", "result": "AEAS achieved 100%", "conclusion": "AEAS demonstrates significant effectiveness in exploit-driven vulnerability prioritization, outperforming existing systems through transparent, code-based analysis."}}
{"id": "2509.17836", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17836", "abs": "https://arxiv.org/abs/2509.17836", "authors": ["Roberto Doriguzzi-Corin", "Petr Sabel", "Silvio Cretti", "Silvio Ranise"], "title": "Federated Learning in the Wild: A Comparative Study for Cybersecurity under Non-IID and Unbalanced Settings", "comment": null, "summary": "Machine Learning (ML) techniques have shown strong potential for network\ntraffic analysis; however, their effectiveness depends on access to\nrepresentative, up-to-date datasets, which is limited in cybersecurity due to\nprivacy and data-sharing restrictions. To address this challenge, Federated\nLearning (FL) has recently emerged as a novel paradigm that enables\ncollaborative training of ML models across multiple clients while ensuring that\nsensitive data remains local. Nevertheless, Federated Averaging (FedAvg), the\ncanonical FL algorithm, has proven poor convergence in heterogeneous\nenvironments where data distributions are non-independent and identically\ndistributed (i.i.d.) and client datasets are unbalanced, conditions frequently\nobserved in cybersecurity contexts. To overcome these challenges, several\nalternative FL strategies have been developed, yet their applicability to\nnetwork intrusion detection remains insufficiently explored. This study\nsystematically reviews and evaluates a range of FL methods in the context of\nintrusion detection for DDoS attacks. Using a dataset of network attacks within\na Kubernetes-based testbed, we assess convergence efficiency, computational\noverhead, bandwidth consumption, and model accuracy. To the best of our\nknowledge, this is the first comparative analysis of FL algorithms for\nintrusion detection under realistic non-i.i.d. and unbalanced settings,\nproviding new insights for the design of robust, privacypreserving network\nsecurity solutions.", "AI": {"tldr": "This study evaluates Federated Learning methods for intrusion detection, focusing on DDoS attacks in a Kubernetes testbed under non-i.i.d. and unbalanced data conditions, aiming to find algorithms that converge well while preserving privacy.", "motivation": "Machine Learning needs large, representative datasets for network traffic analysis, but data-sharing in cybersecurity is hindered by privacy constraints. FL allows collaborative model training without sharing sensitive data, but existing algorithms like FedAvg struggle with convergence in non-i.i.d. and imbalanced environments typical in cybersecurity.", "method": "The paper conducts a systematic review and empirical evaluation of various FL algorithms using a Kubernetes-based testbed and network attack datasets. Key metrics include convergence efficiency, model accuracy, computational overhead, and bandwidth usage.", "result": "The study identifies FL algorithms that outperform FedAvg in terms of convergence and accuracy under non-i.i.d. and unbalanced data conditions. Specific methods are compared to evaluate their effectiveness in detecting DDoS attacks.", "conclusion": "This work provides the first comprehensive comparison of FL algorithms for intrusion detection under realistic non-i.i.d. and unbalanced data scenarios, offering guidance for developing more effective and privacy-aware network security systems."}}
{"id": "2509.17871", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17871", "abs": "https://arxiv.org/abs/2509.17871", "authors": ["Samuel Breckenridge", "Dani Vilardell", "Andr\u00e9s F\u00e1brega", "Amy Zhao", "Patrick McCorry", "Rafael Solari", "Ari Juels"], "title": "B-Privacy: Defining and Enforcing Privacy in Weighted Voting", "comment": null, "summary": "In traditional, one-vote-per-person voting systems, privacy equates with\nballot secrecy: voting tallies are published, but individual voters' choices\nare concealed.\n  Voting systems that weight votes in proportion to token holdings, though, are\nnow prevalent in cryptocurrency and web3 systems. We show that these\nweighted-voting systems overturn existing notions of voter privacy. Our\nexperiments demonstrate that even with secret ballots, publishing raw tallies\noften reveals voters' choices.\n  Weighted voting thus requires a new framework for privacy. We introduce a\nnotion called B-privacy whose basis is bribery, a key problem in voting systems\ntoday. B-privacy captures the economic cost to an adversary of bribing voters\nbased on revealed voting tallies.\n  We propose a mechanism to boost B-privacy by noising voting tallies. We prove\nbounds on its tradeoff between B-privacy and transparency, meaning\nreported-tally accuracy. Analyzing 3,582 proposals across 30 Decentralized\nAutonomous Organizations (DAOs), we find that the prevalence of large voters\n(\"whales\") limits the effectiveness of any B-Privacy-enhancing technique.\nHowever, our mechanism proves to be effective in cases without extreme voting\nweight concentration: among proposals requiring coalitions of $\\geq5$ voters to\nflip outcomes, our mechanism raises B-privacy by a geometric mean factor of\n$4.1\\times$.\n  Our work offers the first principled guidance on transparency-privacy\ntradeoffs in weighted-voting systems, complementing existing approaches that\nfocus on ballot secrecy and revealing fundamental constraints that voting\nweight concentration imposes on privacy mechanisms.", "AI": {"tldr": "This paper addresses privacy challenges in weighted-voting systems (common in crypto/web3), proposing B-privacy (bribery-based privacy metric) and a tally-noising mechanism. While effective in balanced settings, it struggles against concentrated voting power ('whales'), offering the first principled analysis of transparency-privacy tradeoffs in these systems.", "motivation": "Traditional ballot secrecy frameworks fail in weighted-voting systems where vote weights correlate with token holdings. Published tallies can leak individual choices due to vote-weight mediation, necessitating new privacy notions beyond ballot secrecy.", "method": "Introduce B-privacy (quantifies adversary costs for hypothetical bribes based on revealed tallies). Propose a privacy mechanism that adds noise to voting tallies, mathematically analyzing its privacy/transparency tradeoffs. Empirically test on 3,582 DAO proposals to assess effectiveness.", "result": "Mechanism achieves 4.1\u00d7 mean B-privacy improvement in proposals requiring \u22655-voter coalitions. Shows large-voter 'whales' significantly limit privacy gains. Discrepancy between privacy theory and practice: real-world vote weight concentration breaks many privacy assumptions.", "conclusion": "Weighted voting systems require rebalancing privacy and transparency through bribery economics. While existing confidentiality-centric approaches are insufficient, our work establishes theoretical foundations showing voting weight concentration fundamentally constrains privacy mechanism effectiveness."}}
{"id": "2509.17962", "categories": ["cs.CR", "J.3"], "pdf": "https://arxiv.org/pdf/2509.17962", "abs": "https://arxiv.org/abs/2509.17962", "authors": ["Jon Crowcroft", "Anil Madhavapeddy", "Chris Hicks", "Richard Mortier", "Vasilios Mavroudis"], "title": "What if we could hot swap our Biometrics?", "comment": null, "summary": "What if you could really revoke your actual biometric identity, and install a\nnew one, by live rewriting your biological self? We propose some novel\nmechanisms for hot swapping identity based in novel biotechnology. We discuss\nthe potential positive use cases, and negative consequences if such technology\nwas to become available and affordable. Biometrics are selected on the basis\nthat they are supposed to be unfakeable, or at least not at reasonable cost. If\nthey become easier to fake, it may be much cheaper to fake someone else's\nbiometrics than it is for you to change your own biometrics if someone does\ncopy yours. This potentially makes biometrics a bad trade-off for the user. At\nthe time of writing, this threat is highly speculative, but we believe it is\nworth raising and considering the potential consequences.", "AI": {"tldr": "This paper explores speculative biotech methods for swapping biometric identities, raising concerns about their security risks while acknowledging potential benefits.", "motivation": "Current biometric systems are vulnerable if they become increasingly counterfeitable, potentially making biometric authentication less secure and more costly for individuals to manage.", "method": "The authors propose novel biotechnology-based mechanisms for hot swapping biometric identities and analyze potential use cases and consequences.", "result": "The paper outlines potential positive and negative outcomes of identity-swapping technology, highlighting a critical trade-off in biometric security when counterfeiting becomes easier than identity alteration.", "conclusion": "The paper emphasizes the need to consider potential scenarios where biometric identity swapping could lead to significant security and ethical challenges, urging further discussion and precautionary measures."}}
{"id": "2509.17969", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.17969", "abs": "https://arxiv.org/abs/2509.17969", "authors": ["Gorka Guardiola M\u00fazquiz", "Juan Gonz\u00e1lez-G\u00f3mez", "Enrique Soriano-Salvador"], "title": "The Reverse File System: Towards open cost-effective secure WORM storage devices for logging", "comment": null, "summary": "Write Once Read Many (WORM) properties for storage devices are desirable to\nensure data immutability for applications such as secure logging, regulatory\ncompliance, archival storage, and other types of backup systems. WORM devices\nguarantee that data, once written, cannot be altered or deleted. However,\nimplementing secure and compatible WORM storage remains a challenge.\nTraditional solutions often rely on specialized hardware, which is either\ncostly, closed, or inaccessible to the general public. Distributed approaches,\nwhile promising, introduce additional risks such as denial-of-service\nvulnerabilities and operational complexity. We introduce Socarrat, a novel,\ncost-effective, and local WORM storage solution that leverages a simple\nexternal USB device (specifically, a single-board computer running Linux with\nUSB On-The-Go support). The resulting device can be connected via USB,\nappearing as an ordinary external disk formatted with an ext4 or exFAT file\nsystem, without requiring any specialized software or drivers. By isolating the\nWORM enforcement mechanism in a dedicated USB hardware module, Socarrat\nsignificantly reduces the attack surface and ensures that even privileged\nattackers cannot modify or erase stored data. In addition to the WORM capacity,\nthe system is designed to be tamper-evident, becoming resilient against\nadvanced attacks. This work describes a novel approach, the Reverse File\nSystem, based on inferring the file system operations occurring at higher\nlayers in the host computer where Socarrat is mounted. The paper also describes\nthe current Socarrat prototype, implemented in Go and available as free/libre\nsoftware. Finally, it provides a complete evaluation of the logging performance\non different single-board computers.", "AI": {"tldr": "Socarrat is a cost-effective, local WORM storage solution using a USB-connected single-board Linux device with a Reverse File System to enforce data immutability, offering a tamper-evident alternative to costly or complex existing solutions.", "motivation": "Current WORM storage systems rely on expensive/closed hardware or introduce security risks via distributed architectures, necessitating an accessible, secure, and low-cost alternative.", "method": "Leverages a USB-connected Linux single-board computer (with USB On-The-Go support) to implement a tamper-evident WORM device using a 'Reverse File System' that isolates enforcement logic hardware and avoids privileged software layers.", "result": "A prototype implemented in Go, evaluated across SBCs, demonstrating practical WORM enforcement with ext4/exFAT compatibility, tamper resistance, and low operational overhead.", "conclusion": "Socarrat addresses WORM storage limitations with a secure, software-free hardware approach, offering free/libre software and performance validation on affordable hardware."}}
{"id": "2509.18014", "categories": ["cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.18014", "abs": "https://arxiv.org/abs/2509.18014", "authors": ["Joshua Ward", "Xiaofeng Lin", "Chi-Hua Wang", "Guang Cheng"], "title": "Synth-MIA: A Testbed for Auditing Privacy Leakage in Tabular Data Synthesis", "comment": null, "summary": "Tabular Generative Models are often argued to preserve privacy by creating\nsynthetic datasets that resemble training data. However, auditing their\nempirical privacy remains challenging, as commonly used similarity metrics fail\nto effectively characterize privacy risk. Membership Inference Attacks (MIAs)\nhave recently emerged as a method for evaluating privacy leakage in synthetic\ndata, but their practical effectiveness is limited. Numerous attacks exist\nacross different threat models, each with distinct implementations targeting\nvarious sources of privacy leakage, making them difficult to apply\nconsistently. Moreover, no single attack consistently outperforms the others,\nleading to a routine underestimation of privacy risk.\n  To address these issues, we propose a unified, model-agnostic threat\nframework that deploys a collection of attacks to estimate the maximum\nempirical privacy leakage in synthetic datasets. We introduce Synth-MIA, an\nopen-source Python library that streamlines this auditing process through a\nnovel testbed that integrates seamlessly into existing synthetic data\nevaluation pipelines through a Scikit-Learn-like API. Our software implements\n13 attack methods through a Scikit-Learn-like API, designed to enable fast\nsystematic estimation of privacy leakage for practitioners as well as\nfacilitate the development of new attacks and experiments for researchers.\n  We demonstrate our framework's utility in the largest tabular synthesis\nprivacy benchmark to date, revealing that higher synthetic data quality\ncorresponds to greater privacy leakage, that similarity-based privacy metrics\nshow weak correlation with MIA results, and that the differentially private\ngenerator PATEGAN can fail to preserve privacy under such attacks. This\nunderscores the necessity of MIA-based auditing when designing and deploying\nTabular Generative Models.", "AI": {"tldr": "This paper addresses the challenge of auditing privacy risks in synthetic data generated by tabular generative models by proposing a unified framework and open-source library called Synth-MIA, which integrates multiple membership inference attacks (MIAs) for systematic privacy evaluation.", "motivation": "Current privacy audits for synthetic data rely on inconsistent and unreliable similarity metrics or MIAs that fail to generalize, leading to inaccurate risk assessments and underestimation of vulnerabilities.", "method": "The authors developed a model-agnostic framework that combines 13 distinct MIA methods through a Scikit-Learn-like API (Synth-MIA), enabling systematic privacy leakage estimation. The library integrates into existing evaluation pipelines to test privacy risks comprehensively.", "result": "Experiments on the largest synthetic data privacy benchmark revealed: (1)...", "conclusion": "The work highlights the necessity of MIA-based auditing for synthetic data privacy, showing limitations of quality metrics and differential privacy techniques, and provides a reproducible toolset for robust privacy evaluation."}}
{"id": "2509.18039", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.18039", "abs": "https://arxiv.org/abs/2509.18039", "authors": ["Alessio Izzillo", "Riccardo Lazzeretti", "Emilio Coppa"], "title": "STAFF: Stateful Taint-Assisted Full-system Firmware Fuzzing", "comment": "This paper is currently under review at Computers & Security\n  (Elsevier)", "summary": "Modern embedded Linux devices, such as routers, IP cameras, and IoT gateways,\nrely on complex software stacks where numerous daemons interact to provide\nservices. Testing these devices is crucial from a security perspective since\nvendors often use custom closed- or open-source software without documenting\nreleases and patches. Recent coverage-guided fuzzing solutions primarily test\nindividual processes, ignoring deep dependencies between daemons and their\npersistent internal state. This article presents STAFF, a firmware fuzzing\nframework for discovering bugs in Linux-based firmware built around three key\nideas: (a) user-driven multi-request recording, which monitors user\ninteractions with emulated firmware to capture request sequences involving\napplication-layer protocols (e.g., HTTP); (b) intra- and inter-process\ndependency detection, which uses whole-system taint analysis to track how input\nbytes influence user-space states, including files, sockets, and memory areas;\n(c) protocol-aware taint-guided fuzzing, which applies mutations to request\nsequences based on identified dependencies, exploiting multi-staged forkservers\nto efficiently checkpoint protocol states. When evaluating STAFF on 15\nLinux-based firmware targets, it identifies 42 bugs involving multiple network\nrequests and different firmware daemons, significantly outperforming existing\nstate-of-the-art fuzzing solutions in both the number and reproducibility of\ndiscovered bugs.", "AI": {"tldr": "The paper introduces STAFF, a firmware fuzzing framework for Linux-based embedded devices that addresses inter-daemon dependencies through three innovations: multi-request protocol recording, system-wide taint analysis, and protocol-aware taint-guided fuzzing. STAFF discovers 42 new bugs across 15 firmware targets.", "motivation": "Embedded Linux devices have complex, poorly-documented software stacks where daemons interact in undocumented ways. Existing fuzzing tools ignore daemon dependencies and persistent state, leading to incomplete testing of these critical security targets.", "method": "1) Users record request sequences (e.g., HTTP) through firmware emulation; 2)\nWhole-system taint analysis tracks input byte propagation across daemons/files/sockets; 3)\nMutations use identified dependencies with multi-staged forkservers to maintain protocol states efficiently.", "result": "STAFF discovers 42 bugs spanning multiple daemons and requests on 15 firmware targets. It detects 3.4x more unique bugs than DeepState and identifies 50% more bugs with multi-request sequences compared to AFL++.", "conclusion": "STAFF demonstrates that capturing daemon interdependencies through protocol-aware stateful fuzzing significantly improves firmware testing effectiveness, uncovering both more and more reproducible security issues in embedded Linux devices."}}
{"id": "2509.18044", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18044", "abs": "https://arxiv.org/abs/2509.18044", "authors": ["Saeid Sheikhi", "Panos Kostakos", "Lauri Loven"], "title": "Hybrid Reputation Aggregation: A Robust Defense Mechanism for Adversarial Federated Learning in 5G and Edge Network Environments", "comment": null, "summary": "Federated Learning (FL) in 5G and edge network environments face severe\nsecurity threats from adversarial clients. Malicious participants can perform\nlabel flipping, inject backdoor triggers, or launch Sybil attacks to corrupt\nthe global model. This paper introduces Hybrid Reputation Aggregation (HRA), a\nnovel robust aggregation mechanism designed to defend against diverse\nadversarial behaviors in FL without prior knowledge of the attack type. HRA\ncombines geometric anomaly detection with momentum-based reputation tracking of\nclients. In each round, it detects outlier model updates via distance-based\ngeometric analysis while continuously updating a trust score for each client\nbased on historical behavior. This hybrid approach enables adaptive filtering\nof suspicious updates and long-term penalization of unreliable clients,\ncountering attacks ranging from backdoor insertions to random noise Byzantine\nfailures. We evaluate HRA on a large-scale proprietary 5G network dataset (3M+\nrecords) and the widely used NF-CSE-CIC-IDS2018 benchmark under diverse\nadversarial attack scenarios. Experimental results reveal that HRA achieves\nrobust global model accuracy of up to 98.66% on the 5G dataset and 96.60% on\nNF-CSE-CIC-IDS2018, outperforming state-of-the-art aggregators such as Krum,\nTrimmed Mean, and Bulyan by significant margins. Our ablation studies further\ndemonstrate that the full hybrid system achieves 98.66% accuracy, while the\nanomaly-only and reputation-only variants drop to 84.77% and 78.52%,\nrespectively, validating the synergistic value of our dual-mechanism approach.\nThis demonstrates HRA's enhanced resilience and robustness in 5G/edge federated\nlearning deployments, even under significant adversarial conditions.", "AI": {"tldr": "HRA is a novel FL aggregation method for 5G/edge networks that combines anomaly detection and reputation tracking to defend against unknown attacks, achieving state-of-the-art 98.66% model accuracy.", "motivation": "FL in 5G/edge networks faces severe security risks from adversarial clients (label flipping, backdoors, Sybil attacks). Existing aggregation methods lack robustness against diverse unknown attacks, necessitating a defense mechanism without prior attack knowledge.", "method": "Hybrid Reputation Aggregation (HRA) integrates geometric anomaly detection (distance-based outlier identification) with momentum-based reputation tracking (historical trust scoring). This combines real-time anomaly filtering with long-term client reliability assessment to counter attacks adaptively.", "result": "HRA achieved 98.66% accuracy on a 3M+ 5G dataset (outperforming Krum by +10.3%) and 96.60% on NF-CSE-CIC-IDS2018. Ablation studies showed 84.77% accuracy for anomaly-only and 78.52% for reputation-only, confirming the synergy of the hybrid design.", "conclusion": "HRA demonstrates enhanced resilience in 5G/edge FL against adversarial attacks, achieving robust model accuracy even under significant threats. The hybrid approach outperforms existing aggregation methods, validating its dual-mechanism design."}}
