<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 29]
- [cs.SE](#cs.SE) [Total: 18]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Lightweight MobileNetV1+GRU for ECG Biometric Authentication: Federated and Adversarial Evaluation](https://arxiv.org/abs/2509.20382)
*Dilli Hang Rai,Sabin Kafley*

Main category: cs.CR

TL;DR: The paper proposes a lightweight ECG-based authentication model (MobileNetV1 + GRU) for wearables, addressing real-time processing, privacy, and spoofing risks. It achieves high accuracy but shows vulnerabilities under adversarial attacks, emphasizing federated learning and diverse datasets for reliable deployment.


<details>
  <summary>Details</summary>
Motivation: ECG biometrics on wearables face challenges in real-time performance, privacy leakage, and spoofing. Lightweight models are needed to balance efficiency and security.

Method: Combined MobileNetV1 and GRU with 20dB Gaussian noise injection and custom preprocessing. Evaluated on ECGID, MIT-BIH, CYBHi, and PTB datasets under wearable-edge conditions, federated learning, and FGSM adversarial attacks.

Result: Achieved 98.7%-99.34%-level accuracy across datasets, with EER rates <1%. Under FGSM attacks, accuracy dropped from 96.82% to 0.80%, revealing security vulnerabilities.

Conclusion: The model demonstrates strong baseline performance but highlights critical vulnerabilities under adversarial attacks. Federated learning and diverse physiological datasets are essential for secure, scalable ECG biometrics in wearables.

Abstract: ECG biometrics offer a unique, secure authentication method, yet their
deployment on wearable devices faces real-time processing, privacy, and
spoofing vulnerability challenges. This paper proposes a lightweight deep
learning model (MobileNetV1+GRU) for ECG-based authentication, injection of
20dB Gaussian noise & custom preprocessing. We simulate wearable conditions and
edge deployment using the ECGID, MIT-BIH, CYBHi, and PTB datasets, achieving
accuracies of 99.34%, 99.31%, 91.74%, and 98.49%, F1-scores of 0.9869, 0.9923,
0.9125, and 0.9771, Precision of 0.9866, 0.9924, 0.9180 and 0.9845, Recall of
0.9878, 0.9923, 0.9129, and 0.9756, equal error rates (EER) of 0.0009, 0.00013,
0.0091, and 0.0009, and ROC-AUC values of 0.9999, 0.9999, 0.9985, and 0.9998,
while under FGSM adversarial attacks, accuracy drops from 96.82% to as low as
0.80%. This paper highlights federated learning, adversarial testing, and the
need for diverse wearable physiological datasets to ensure secure and scalable
biometrics.

</details>


### [2] [MARS: A Malignity-Aware Backdoor Defense in Federated Learning](https://arxiv.org/abs/2509.20383)
*Wei Wan,Yuxuan Ning,Zhicong Huang,Cheng Hong,Shengshan Hu,Ziqi Zhou,Yechao Zhang,Tianqing Zhu,Wanlei Zhou,Leo Yu Zhang*

Main category: cs.CR

TL;DR: This paper proposes MARS, a malignity-aware backdoor detection method for federated learning that addresses limitations of existing defenses by using backdoor energy analysis and Wasserstein-based clustering.


<details>
  <summary>Details</summary>
Motivation: Existing FL defenses fail due to loose coupling with backdoor attack characteristics, as shown by the adaptive 3DFed attack (SP2023) that bypasses statistical measures. The paper identifies weaknesses in empirical defense approaches.

Method: MARS operates in three stages: (1): Calculates neuron-level backdoor energy (BE) to quantify malignity, (2): Concentrates prominent BE values into CBE to amplify malicious patterns, (3): Uses Wasserstein distance-based clustering to isolate backdoor models.

Result: Extensive experiments show MARS achieves superior defense performance against state-of-the-art backdoor attacks like 3DFed, outperforming existing methods with significant improvements in detection and mitigation.

Conclusion: The paper presents MARS as a fundamentally novel defense framework that overcomes existing limitations through focused energy analysis and optimized clustering, setting a new benchmark for FL backdoor defenses.

Abstract: Federated Learning (FL) is a distributed paradigm aimed at protecting
participant data privacy by exchanging model parameters to achieve high-quality
model training. However, this distributed nature also makes FL highly
vulnerable to backdoor attacks. Notably, the recently proposed state-of-the-art
(SOTA) attack, 3DFed (SP2023), uses an indicator mechanism to determine whether
the backdoor models have been accepted by the defender and adaptively optimizes
backdoor models, rendering existing defenses ineffective. In this paper, we
first reveal that the failure of existing defenses lies in the employment of
empirical statistical measures that are loosely coupled with backdoor attacks.
Motivated by this, we propose a Malignity-Aware backdooR defenSe (MARS) that
leverages backdoor energy (BE) to indicate the malicious extent of each neuron.
To amplify malignity, we further extract the most prominent BE values from each
model to form a concentrated backdoor energy (CBE). Finally, a novel
Wasserstein distance-based clustering method is introduced to effectively
identify backdoor models. Extensive experiments demonstrate that MARS can
defend against SOTA backdoor attacks and significantly outperforms existing
defenses.

</details>


### [3] [R1-Fuzz: Specializing Language Models for Textual Fuzzing via Reinforcement Learning](https://arxiv.org/abs/2509.20384)
*Jiayi Lin,Liangcai Su,Junzhe Li,Chenxiong Qian*

Main category: cs.CR

TL;DR: R1-Fuzz is a cost-efficient framework using reinforcement learning to enhance language models for complex fuzzing tasks, achieving performance comparable to larger models while discovering 29 new vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Fuzzing struggles with complex targets due to syntactic/semantic constraints, and language models (LMs), despite potential, face limitations in exploration and cost-of-large-models.

Method: R1-Fuzz leverages RL-based post-training of a small model (R1-Fuzz-7B) using coverage-slicing question construction and distance-based rewards to integrate LMs with deep program logic reasoning during fuzzing.

Result: R1-Fuzz outperforms state-of-the-art fuzzers by 75\% in coverage and discovers 29 new vulnerabilities, while a 7B model matches performance of larger models.

Conclusion: By combining RL and specialized LM training, R1-Fuzz addresses real-world fuzzing limitations with cost-effective, effective input generation, proving practical for complex software targets.

Abstract: Fuzzing is effective for vulnerability discovery but struggles with complex
targets such as compilers, interpreters, and database engines, which accept
textual input that must satisfy intricate syntactic and semantic constraints.
Although language models (LMs) have attracted interest for this task due to
their vast latent knowledge and reasoning potential, their practical adoption
has been limited. The major challenges stem from insufficient exploration of
deep program logic among real-world codebases, and the high cost of leveraging
larger models. To overcome these challenges, we propose R1-Fuzz, the first
framework that leverages reinforcement learning (RL) to specialize
cost-efficient LMs and integrate them for complex textual fuzzing input
generation. R1-Fuzz introduces two key designs: coverage-slicing-based question
construction and a distance-based reward calculation. Through RL-based
post-training of a model with our constructed dataset, R1-Fuzz designs a
fuzzing workflow that tightly integrates LMs to reason deep program semantics
during fuzzing. Evaluations on diverse real-world targets show that our design
enables a small model, named R1-Fuzz-7B, to rival or even outperform much
larger models in real-world fuzzing. Notably, R1-Fuzz achieves up to 75\%
higher coverage than state-of-the-art fuzzers and discovers 29 previously
unknown vulnerabilities, demonstrating its practicality.

</details>


### [4] [Can You Trust Your Copilot? A Privacy Scorecard for AI Coding Assistants](https://arxiv.org/abs/2509.20388)
*Amir AL-Maamari*

Main category: cs.CR

TL;DR: This paper evaluates five leading AI coding assistants using a novel privacy scorecard, identifying significant privacy concerns and gaps in their data practices.


<details>
  <summary>Details</summary>
Motivation: The integration of AI coding assistants poses privacy and trust risks due to opaque data handling practices, necessitating a standardized evaluation method.

Method: An expert-validated privacy scorecard was designed through collaboration between a legal expert and a data protection officer. It involved analysis of four document types (legal policies, external audits, etc.) against 14 weighted criteria.

Result: A 20-point gap in privacy protections was found among the five tools, with common issues such as opt-out consent for training and failure to filter secrets from prompts.

Conclusion: The scorecard offers actionable guidance for developers, establishes a new transparency benchmark, and advocates for user-centric privacy improvements in the AI industry.

Abstract: The rapid integration of AI-powered coding assistants into developer
workflows has raised significant privacy and trust concerns. As developers
entrust proprietary code to services like OpenAI's GPT, Google's Gemini, and
GitHub Copilot, the unclear data handling practices of these tools create
security and compliance risks. This paper addresses this challenge by
introducing and applying a novel, expert-validated privacy scorecard. The
methodology involves a detailed analysis of four document types; from legal
policies to external audits; to score five leading assistants against 14
weighted criteria. A legal expert and a data protection officer refined these
criteria and their weighting. The results reveal a distinct hierarchy of
privacy protections, with a 20-point gap between the highest- and lowest-ranked
tools. The analysis uncovers common industry weaknesses, including the
pervasive use of opt-out consent for model training and a near-universal
failure to filter secrets from user prompts proactively. The resulting
scorecard provides actionable guidance for developers and organizations,
enabling evidence-based tool selection. This work establishes a new benchmark
for transparency and advocates for a shift towards more user-centric privacy
standards in the AI industry.

</details>


### [5] [A Comparative Analysis of Ensemble-Based Machine Learning Approaches with Explainable AI for Multi-Class Intrusion Detection in Drone Networks](https://arxiv.org/abs/2509.20391)
*Md. Alamgir Hossain,Waqas Ishtiaq,Md. Samiul Islam*

Main category: cs.CR

TL;DR: The paper introduces a robust, interpretable intrusion detection framework for drones, evaluating ensemble machine learning models and incorporating explainable AI methods.


<details>
  <summary>Details</summary>
Motivation: The integration of drones into various sectors raises cybersecurity concerns due to dynamic traffic and sophisticated attack vectors like spoofing and MITM attacks.

Method: The authors compared ensemble-based machine learning models (Random Forest, Extra Trees, AdaBoost, CatBoost, XGBoost) on a labeled dataset with benign and intrusive drone traffic, using metrics like F1-score and ROC AUC, and incorporated SHAP and LIME for model explainability.

Result: Random Forest achieved the highest performance with a macro F1-score of 0.9998 and ROC AUC of 1.0000, and statistical tests validated the models superiority.

Conclusion: The proposed framework ensures both near-perfect accuracy and interpretability, making it suitable for real-time, safety-critical drone operations.

Abstract: The growing integration of drones into civilian, commercial, and defense
sectors introduces significant cybersecurity concerns, particularly with the
increased risk of network-based intrusions targeting drone communication
protocols. Detecting and classifying these intrusions is inherently challenging
due to the dynamic nature of drone traffic and the presence of multiple
sophisticated attack vectors such as spoofing, injection, replay, and
man-in-the-middle (MITM) attacks. This research aims to develop a robust and
interpretable intrusion detection framework tailored for drone networks, with a
focus on handling multi-class classification and model explainability. We
present a comparative analysis of ensemble-based machine learning models,
namely Random Forest, Extra Trees, AdaBoost, CatBoost, and XGBoost, trained on
a labeled dataset comprising benign traffic and nine distinct intrusion types.
Comprehensive data preprocessing was performed, including missing value
imputation, scaling, and categorical encoding, followed by model training and
extensive evaluation using metrics such as macro F1-score, ROC AUC, Matthews
Correlation Coefficient, and Log Loss. Random Forest achieved the highest
performance with a macro F1-score of 0.9998 and ROC AUC of 1.0000. To validate
the superiority of the models, statistical tests, including Friedmans test, the
Wilcoxon signed-rank test with Holm correction, and bootstrapped confidence
intervals, were applied. Furthermore, explainable AI methods, SHAP and LIME,
were integrated to interpret both global and local feature importance,
enhancing model transparency and decision trustworthiness. The proposed
approach not only delivers near-perfect accuracy but also ensures
interpretability, making it highly suitable for real-time and safety-critical
drone operations.

</details>


### [6] [Centralized vs. Decentralized Security for Space AI Systems? A New Look](https://arxiv.org/abs/2509.20395)
*Noam Schmitt,Marc Antoine Lacoste*

Main category: cs.CR

TL;DR: The paper investigates the trade-off between centralized and decentralized security management in satellite constellations, focusing on balancing security and performance. It highlights three AI architectures for automated security: centralized, distributed, and federated.


<details>
  <summary>Details</summary>
Motivation: The operational constraints of space, such as communication latency, require a careful study of security management strategies to ensure both security and performance. The paper is motivated by the need to compare and evaluate centralized and decentralized approaches in terms of their suitability for space-based AI systems.

Method: The paper likely uses a comparative analysis approach to evaluate the three AI architectures (centralized, distributed, and federated) considering the unique challenges of space communication, such as latency and scalability. The analysis probably involves theoretical evaluation and maybe some simulations or use-case studies.

Result: The standardized evaluation of AI architectures for satellite security by this paper reveals that the centralized architecture is optimal in the short term due to fast training despite communication latency challenges. Decentralized systems, particularly federated learning, are suggested as viable long-term options for handling the growing number of satellites and improving security.

Conclusion: The paper concludes that while centralized security management is suitable for the near future, decentralized approaches, especially federated learning, will become more advantageous as the size and scale of satellite constellations expand, offering better scalability and security.

Abstract: This paper investigates the trade-off between centralized and decentralized
security management in constellations of satellites to balance security and
performance. We highlight three key AI architectures for automated security
management: (a) centralized, (b) distributed and (c) federated. The centralized
architecture is the best option short term, providing fast training, despite
the hard challenge of the communication latency overhead across space.
Decentralized architectures are better alternatives in the longer term,
providing enhanced scalability and security.

</details>


### [7] [Defending against Stegomalware in Deep Neural Networks with Permutation Symmetry](https://arxiv.org/abs/2509.20399)
*Birk Torpmann-Hagen,Michael A. Riegler,Pål Halvorsen,Dag Johansen*

Main category: cs.CR

TL;DR: This paper introduces the first effective defense against neural network stegomalware by shuffling weight/bias matrix columns or convolutional channel orders, neutralizing malware without affecting model accuracy.


<details>
  <summary>Details</summary>
Motivation: Shared deep learning checkpoints pose security risks as malware can be stealthily embedded with minor accuracy costs. Both practitioners and security experts overlook this threat.

Method: The proposed method disrupts stegomalware by shuffling neural network weight/bias matrices or convolutional channel orders, corrupting embedded payloads while preserving model accuracy.

Result: The approach outperforms existing methods by completely neutralizing state-of-the-art steganography payloads with zero accuracy degradation.

Conclusion: The work demonstrates a robust mitigation strategy while advocating for ongoing research into ML security to address potential defense bypasses and broader system robustness.

Abstract: Deep neural networks are being utilized in a growing number of applications,
both in production systems and for personal use. Network checkpoints are as a
consequence often shared and distributed on various platforms to ease the
development process. This work considers the threat of neural network
stegomalware, where malware is embedded in neural network checkpoints at a
negligible cost to network accuracy. This constitutes a significant security
concern, but is nevertheless largely neglected by the deep learning
practitioners and security specialists alike. We propose the first effective
countermeasure to these attacks. In particular, we show that state-of-the-art
neural network stegomalware can be efficiently and effectively neutralized
through shuffling the column order of the weight- and bias-matrices, or
equivalently the channel-order of convolutional layers. We show that this
effectively corrupts payloads that have been embedded by state-of-the-art
methods in neural network steganography at no cost to network accuracy,
outperforming competing methods by a significant margin. We then discuss
possible means by which to bypass this defense, additional defense methods, and
advocate for continued research into the security of machine learning systems.

</details>


### [8] [Why Speech Deepfake Detectors Won't Generalize: The Limits of Detection in an Open World](https://arxiv.org/abs/2509.20405)
*Visar Berisha,Prad Kadambi,Isabella Lenz*

Main category: cs.CR

TL;DR: The paper discusses how speech deepfake detectors face performance issues under changing real-world conditions, creating a 'coverage debt.' It highlights vulnerabilities with newer synthesizers and in conversational domains, suggesting that detection should be part of a multi-layered defense strategy rather than a sole decision-making factor.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of speech deepfake detectors often use controlled benchmarks, but real-word conditions in deployment cause these models to miss data blind spots when new factors like synthesizers or environments are introduced.

Method: The authors analyzed cross-testing framework results by categorizing performance in terms of bonafide domains and spoof release years, identifying patterns in failure rates.

Result: Newer deepfake synthesizers produce outputs without legacy artifacts, evading detectors, and conversational speech domains are most vulnerable to attacks.

Conclusion: Speech detectors alone are insufficient for security in high-stakes scenarios; they should be integrated with other methods like provenance checks and policy measures.

Abstract: Speech deepfake detectors are often evaluated on clean, benchmark-style
conditions, but deployment occurs in an open world of shifting devices,
sampling rates, codecs, environments, and attack families. This creates a
``coverage debt" for AI-based detectors: every new condition multiplies with
existing ones, producing data blind spots that grow faster than data can be
collected. Because attackers can target these uncovered regions, worst-case
performance (not average benchmark scores) determines security. To demonstrate
the impact of the coverage debt problem, we analyze results from a recent
cross-testing framework. Grouping performance by bona fide domain and spoof
release year, two patterns emerge: newer synthesizers erase the legacy
artifacts detectors rely on, and conversational speech domains
(teleconferencing, interviews, social media) are consistently the hardest to
secure. These findings show that detection alone should not be relied upon for
high-stakes decisions. Detectors should be treated as auxiliary signals within
layered defenses that include provenance, personhood credentials, and policy
safeguards.

</details>


### [9] [Adversarial Defense in Cybersecurity: A Systematic Review of GANs for Threat Detection and Mitigation](https://arxiv.org/abs/2509.20411)
*Tharcisse Ndayipfukamiye,Jianguo Ding,Doreen Sebastian Sarwatt,Adamu Gaston Philipo,Huansheng Ning*

Main category: cs.CR

TL;DR: This paper reviews GAN-based adversarial defenses in cybersecurity from 2021-2025, analyzing 185 studies to identify advancements (e.g., WGAN-GP, CGANs) and persistent challenges (training instability, computational costs). A four-dimensional taxonomy is proposed, along with a roadmap for improving scalability and adaptability against threats like AI-driven attacks.


<details>
  <summary>Details</summary>
Motivation: ML cybersecurity systems face vulnerabilities from adversarial attacks, while GANs simultaneously enable new threats and promising defenses. This survey addresses the need for systematic evaluation of GAN-based solutions to identify gaps and guide future research.

Method: PRISMA-compliant review of 185 peer-reviewed studies from five digital libraries using (1) quantitative trend analysis and (2)

Result: GANs improve detection accuracy in network intrusion, malware analysis, and IoT with specialized architectures (CGANs, hybrid GANs). Challenges include: 63% of studies report training instability; 78\% lack standardized benchmarks; computational costs increase by 40-150\% compared to traditional methods.

Conclusion: While GAN-based defenses show potential, deployment requires advances in stable architectures, unified benchmarking frameworks, and explainability. The proposed roadmap prioritizes hybrid models and real-world integration to prepare against emerging threats like LLM-driven attacks.

Abstract: Machine learning-based cybersecurity systems are highly vulnerable to
adversarial attacks, while Generative Adversarial Networks (GANs) act as both
powerful attack enablers and promising defenses. This survey systematically
reviews GAN-based adversarial defenses in cybersecurity (2021--August 31,
2025), consolidating recent progress, identifying gaps, and outlining future
directions. Using a PRISMA-compliant systematic literature review protocol, we
searched five major digital libraries. From 829 initial records, 185
peer-reviewed studies were retained and synthesized through quantitative trend
analysis and thematic taxonomy development. We introduce a four-dimensional
taxonomy spanning defensive function, GAN architecture, cybersecurity domain,
and adversarial threat model. GANs improve detection accuracy, robustness, and
data utility across network intrusion detection, malware analysis, and IoT
security. Notable advances include WGAN-GP for stable training, CGANs for
targeted synthesis, and hybrid GAN models for improved resilience. Yet,
persistent challenges remain such as instability in training, lack of
standardized benchmarks, high computational cost, and limited explainability.
GAN-based defenses demonstrate strong potential but require advances in stable
architectures, benchmarking, transparency, and deployment. We propose a roadmap
emphasizing hybrid models, unified evaluation, real-world integration, and
defenses against emerging threats such as LLM-driven cyberattacks. This survey
establishes the foundation for scalable, trustworthy, and adaptive GAN-powered
defenses.

</details>


### [10] [A Taxonomy of Data Risks in AI and Quantum Computing (QAI) - A Systematic Review](https://arxiv.org/abs/2509.20418)
*Grace Billiris,Asif Gill,Madhushi Bandara*

Main category: cs.CR

TL;DR: The paper investigates data risks in Quantum Artificial Intelligence (QAI), creating a taxonomy of 22 risks across five categories (governance, risk assessment, control implementation, user considerations, and continuous monitoring). It highlights QAI-specific vulnerabilities and gaps in holistic risk evaluation, contributing to trustworthy AI/qAI research.


<details>
  <summary>Details</summary>
Motivation: QAI combines AI and quantum computing risks, but systemic analysis of privacy/security vulnerabilities is lacking. Understanding these risks is critical for ensuring the trustworthiness and reliability of QAI systems.

Method: A systematic review of 67 privacy- and security-related studies to identify and categorize QAI data risks, resulting in a structured taxonomy.

Result: Identified 22 key QAI data risks in five categories, revealed unique QAI vulnerabilities, and uncovered gaps in comprehensive risk assessment approaches.

Conclusion: Establishes a foundational framework for addressing QAI risks, advancing trustworthy AI/qAI research, and guiding future risk assessment tool development.

Abstract: Quantum Artificial Intelligence (QAI), the integration of Artificial
Intelligence (AI) and Quantum Computing (QC), promises transformative advances,
including AI-enabled quantum cryptography and quantum-resistant encryption
protocols. However, QAI inherits data risks from both AI and QC, creating
complex privacy and security vulnerabilities that are not systematically
studied. These risks affect the trustworthiness and reliability of AI and QAI
systems, making their understanding critical. This study systematically reviews
67 privacy- and security-related studies to expand understanding of QAI data
risks. We propose a taxonomy of 22 key data risks, organised into five
categories: governance, risk assessment, control implementation, user
considerations, and continuous monitoring. Our findings reveal vulnerabilities
unique to QAI and identify gaps in holistic risk assessment. This work
contributes to trustworthy AI and QAI research and provides a foundation for
developing future risk assessment tools.

</details>


### [11] [Differential Privacy of Network Parameters from a System Identification Perspective](https://arxiv.org/abs/2509.20460)
*Andrew Campbell,Anna Scaglione,Hang Liu,Victor Elvira,Sean Peisert,Daniel Arnold*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper addresses the problem of protecting network information from
privacy system identification (SI) attacks when sharing cyber-physical system
simulations. We model analyst observations of networked states as time-series
outputs of a graph filter driven by differentially private (DP) nodal
excitations, with the analyst aiming to infer the underlying graph shift
operator (GSO). Unlike traditional SI, which estimates system parameters, we
study the inverse problem: what assumptions prevent adversaries from
identifying the GSO while preserving utility for legitimate analysis. We show
that applying DP mechanisms to inputs provides formal privacy guarantees for
the GSO, linking the $(\epsilon,\delta)$-DP bound to the spectral properties of
the graph filter and noise covariance. More precisely, for DP Gaussian signals,
the spectral characteristics of both the filter and noise covariance determine
the privacy bound, with smooth filters and low-condition-number covariance
yielding greater privacy.

</details>


### [12] [Advancing Practical Homomorphic Encryption for Federated Learning: Theoretical Guarantees and Efficiency Optimizations](https://arxiv.org/abs/2509.20476)
*Ren-Yi Huang,Dumindu Samaraweera,Prashant Shekhar,J. Morris Chang*

Main category: cs.CR

TL;DR: This paper introduces a theoretical framework for analyzing the principles of selective encryption as a defense against model inversion attacks in Federated Learning, and an empirical study to identify factors affecting its effectiveness.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the vulnerability of gradient sharing to model inversion attacks like DLG, and the computational expense of full encryption. The paper seeks to provide theoretical insights to improve selective encryption strategies for privacy with efficiency.

Method: The authors developed a theoretical framework to analyze selective encryption and conducted empirical experiments under various conditions to assess the defense effectiveness in relation to factors such as model complexity, encryption ratios, and exposed gradients.

Result: The study results reveal the impact of model complexity, encryption ratios, and exposed gradients on the effectiveness of selective encryption defenses against model inversion attacks, supported by empirical validations.

Conclusion: The paper concludes that the presented theoretical and empirical analyses enhance understanding of selective encryption, offering guidance to build more efficient and scalable privacy-preserving Federated Learning systems.

Abstract: Federated Learning (FL) enables collaborative model training while preserving
data privacy by keeping raw data locally stored on client devices, preventing
access from other clients or the central server. However, recent studies reveal
that sharing model gradients creates vulnerability to Model Inversion Attacks,
particularly Deep Leakage from Gradients (DLG), which reconstructs private
training data from shared gradients. While Homomorphic Encryption has been
proposed as a promising defense mechanism to protect gradient privacy, fully
encrypting all model gradients incurs high computational overhead. Selective
encryption approaches aim to balance privacy protection with computational
efficiency by encrypting only specific gradient components. However, the
existing literature largely overlooks a theoretical exploration of the spectral
behavior of encrypted versus unencrypted parameters, relying instead primarily
on empirical evaluations. To address this gap, this paper presents a framework
for theoretical analysis of the underlying principles of selective encryption
as a defense against model inversion attacks. We then provide a comprehensive
empirical study that identifies and quantifies the critical factors, such as
model complexity, encryption ratios, and exposed gradients, that influence
defense effectiveness. Our theoretical framework clarifies the relationship
between gradient selection and privacy preservation, while our experimental
evaluation demonstrates how these factors shape the robustness of defenses
against model inversion attacks. Collectively, these contributions advance the
understanding of selective encryption mechanisms and offer principled guidance
for designing efficient, scalable, privacy-preserving federated learning
systems.

</details>


### [13] [Every Character Counts: From Vulnerability to Defense in Phishing Detection](https://arxiv.org/abs/2509.20589)
*Maria Chiper,Radu Tudor Ionescu*

Main category: cs.CR

TL;DR: This paper evaluates character-level deep learning models (CharCNN, CharGRU, CharBiLSTM) for phishing detection, emphasizing robustness, interpretability, and performance under adversarial scenarios. CharGRU performs best in constrained environments, and adversarial training improves model robustness. Grad-CAM visualization provides interpretability.


<details>
  <summary>Details</summary>
Motivation: Current phishing detection methods lack explainability and robustness against new attacks. Character-level models offer potential advantages in capturing subtle attack patterns while enabling interpretability for security experts.

Method: Compared three character-level architectures (CharCNN, CharGRU, CharBiLSTM) on a multi-source email dataset under three scenarios: standard detection, adversarial attacks, and adversarial training. Evaluated model performance with Grad-CAM visualization for interpretability under limited computational resources.

Result: CharGRU achieved best performance across all scenarios. All models showed adversarial vulnerability but improved with adversarial training. Grad-CAM successfully identified character-level decision patterns. Code and data were open-sourced.

Conclusion: Character-level models, particularly CharGRU, provide effective phishing detection with interpretability. Adversarial training enhances robustness, and Grad-CAM visualization offers valuable insights for security analysis. The approach addresses limitations in current detection methods.

Abstract: Phishing attacks targeting both organizations and individuals are becoming an
increasingly significant threat as technology advances. Current automatic
detection methods often lack explainability and robustness in detecting new
phishing attacks. In this work, we investigate the effectiveness of
character-level deep learning models for phishing detection, which can provide
both robustness and interpretability. We evaluate three neural architectures
adapted to operate at the character level, namely CharCNN, CharGRU, and
CharBiLSTM, on a custom-built email dataset, which combines data from multiple
sources. Their performance is analyzed under three scenarios: (i) standard
training and testing, (ii) standard training and testing under adversarial
attacks, and (iii) training and testing with adversarial examples. Aiming to
develop a tool that operates as a browser extension, we test all models under
limited computational resources. In this constrained setup, CharGRU proves to
be the best-performing model across all scenarios. All models show
vulnerability to adversarial attacks, but adversarial training substantially
improves their robustness. In addition, by adapting the Gradient-weighted Class
Activation Mapping (Grad-CAM) technique to character-level inputs, we are able
to visualize which parts of each email influence the decision of each model.
Our open-source code and data is released at
https://github.com/chipermaria/every-character-counts.

</details>


### [14] [Beyond SSO: Mobile Money Authentication for Inclusive e-Government in Sub-Saharan Africa](https://arxiv.org/abs/2509.20592)
*Oluwole Adewusi,Wallace S. Msagusa,Jean Pierre Imanirumva,Okemawo Obadofin,Jema D. Ndibwile*

Main category: cs.CR

TL;DR: This paper proposes a hybrid Mobile Money Authentication (MMA) framework for Sub-Saharan Africa, combining USSD-based multi-factor authentication with cryptographically bound JSON Web Tokens (JWT). The solution addresses vulnerabilities like SIM swapping, improves scalability, and performs significantly better than OAuth-based SSO in low-network conditions.


<details>
  <summary>Details</summary>
Motivation: Mobile Money Services (MMS) are widespread in Sub-Saharan Africa (SSA), but existing MMA methods face critical issues: SIM swapping susceptibility, weak session protection, and poor scalability during high demand. Internet penetration remains low, necessitating authentication solutions tailored for resource-constrained environments.

Method: The authors designed a three-factor authentication model: SIM verification (leveraging USSD’s offline capabilities), user PIN entry, and cryptographically bound JWTs for session security. The framework prioritizes offline accessibility, secure session continuity, and resistance to phishing/brute-force attacks through hybrid security mechanisms.

Result: Compared to traditional OAuth-based Single Sign-On (SSO), the framework achieves 45s faster authentication time (8s vs. 12-15s), 15s higher success rate under poor network conditions (95 vs. 80%), and demonstrates resilience against phishing and brute-force attacks. Threat modeling and penetration testing validate reduced vulnerability exposure.

Conclusion: The framework advances secure digital inclusion in SSA by providing a scalable, low-infrastructure MMA solution. Key contributions include: (1)a hybrid authentication protocol for offline/low-bandwidth environments; (2)a threat-specific security framework for SIM swapping and social engineering; and (3)demonstrated scalability for large user bases with minimal overhead.

Abstract: The rapid adoption of Mobile Money Services (MMS) in Sub-Saharan Africa (SSA)
offers a viable path to improve e-Government service accessibility in the face
of persistent low internet penetration. However, existing Mobile Money
Authentication (MMA) methods face critical limitations, including
susceptibility to SIM swapping, weak session protection, and poor scalability
during peak demand. This study introduces a hybrid MMA framework that combines
Unstructured Supplementary Service Data (USSD)-based multi-factor
authentication with secure session management via cryptographically bound JSON
Web Tokens (JWT). Unlike traditional MMA systems that rely solely on SIM-PIN
verification or smartphone-dependent biometrics, our design implements a
three-factor authentication model; SIM verification, PIN entry, and session
token binding, tailored for resource-constrained environments. Simulations and
comparative analysis against OAuth-based Single Sign-On (SSO) methods reveal a
45% faster authentication time (8 seconds vs. 12 to 15 seconds), 15% higher
success under poor network conditions (95% vs. 80%), and increased resistance
to phishing and brute-force attacks. Penetration testing and threat modeling
further demonstrate a substantial reduction in vulnerability exposure compared
to conventional approaches. The primary contributions of this work are: (1) a
hybrid authentication protocol that ensures offline accessibility and secure
session continuity; (2) a tailored security framework addressing threats like
SIM swapping and social engineering in SSA; and (3) demonstrated scalability
for thousands of users with reduced infrastructure overhead. The proposed
approach advances secure digital inclusion in SSA and other regions with
similar constraints.

</details>


### [15] [A Framework for Rapidly Developing and Deploying Protection Against Large Language Model Attacks](https://arxiv.org/abs/2509.20639)
*Adam Swanda,Amy Chang,Alexander Chen,Fraser Burch,Paul Kassianik,Konstantin Berlin*

Main category: cs.CR

TL;DR: This paper proposes a production-grade defense system for Large Language Models (LLMs), integrating threat intelligence, data platforms, and rapid deployment to address evolving security threats. The system enables continuous adaptation to emerging risks while maintaining operational stability through layered protections and ML-driven improvement.


<details>
  <summary>Details</summary>
Motivation: Widespread LLM adoption creates new security vulnerabilities as attackers target autonomous systems. Existing approaches focus on isolated detection models while neglecting end-to-end systems for dynamic threat landscapes. Zero-day attacks require a robust defense framework with continuous adaptation capabilities.

Method: The system comprises three components: (1)a threat intelligence module converting emerging threats into actionable protections, (2)a data platform for observability, monitoring, and ML operations with aggregated information, (3)a deployment platform enabling rapid detection updates without workflow disruption. The architecture emphasizes layered defense, continuous model training from threat data, and seamless production updates.

Result: The integrated platform demonstrates effective layered protection against evolving LLM attacks while generating training data for model improvement. Defense updates are deployed without production interruptions, validating the system's capacity for continuous adaptation to new threats through its threat intelligence and deployment mechanisms.

Conclusion: LLM security requires a paradigm shift from isolated detection models to comprehensive, production-grade systems that combine malware defense principles with continuous threat intelligence. The proposed architecture provides scalable protection through layered defenses, real-time observability, and automated model improvement, setting a foundation for future AI security frameworks.

Abstract: The widespread adoption of Large Language Models (LLMs) has revolutionized AI
deployment, enabling autonomous and semi-autonomous applications across
industries through intuitive language interfaces and continuous improvements in
model development. However, the attendant increase in autonomy and expansion of
access permissions among AI applications also make these systems compelling
targets for malicious attacks. Their inherent susceptibility to security flaws
necessitates robust defenses, yet no known approaches can prevent zero-day or
novel attacks against LLMs. This places AI protection systems in a category
similar to established malware protection systems: rather than providing
guaranteed immunity, they minimize risk through enhanced observability,
multi-layered defense, and rapid threat response, supported by a threat
intelligence function designed specifically for AI-related threats.
  Prior work on LLM protection has largely evaluated individual detection
models rather than end-to-end systems designed for continuous, rapid adaptation
to a changing threat landscape. We present a production-grade defense system
rooted in established malware detection and threat intelligence practices. Our
platform integrates three components: a threat intelligence system that turns
emerging threats into protections; a data platform that aggregates and enriches
information while providing observability, monitoring, and ML operations; and a
release platform enabling safe, rapid detection updates without disrupting
customer workflows. Together, these components deliver layered protection
against evolving LLM threats while generating training data for continuous
model improvement and deploying updates without interrupting production.

</details>


### [16] [Reliability Analysis of Fully Homomorphic Encryption Systems Under Memory Faults](https://arxiv.org/abs/2509.20686)
*Rian Adam Rajagede,Yan Solihin*

Main category: cs.CR

TL;DR: The paper explores how FHE systems behave under memory faults and evaluates the effectiveness of fault mitigation techniques.


<details>
  <summary>Details</summary>
Motivation: FHE allows computations on encrypted data, which is crucial for privacy, but the reliability, particularly fault tolerance, has not been thoroughly studied.

Method: The study analyzes the impact of memory faults on FHE computation at both the operation level and application level, comparing different FHE schemes.

Result: The paper presents findings on the behavior of FHE under faults and assesses mitigation techniques, though specific results are not detailed in the abstract.

Conclusion: Understanding fault tolerance in FHE systems is essential, and the paper highlights the need for reliable fault mitigation strategies.

Abstract: Fully Homomorphic Encryption (FHE) represents a paradigm shift in
cryptography, enabling computation directly on encrypted data and unlocking
privacy-critical computation. Despite being increasingly deployed in real
platforms, the reliability aspects of FHE systems, especially how they respond
to faults, have been mostly neglected. This paper aims to better understand of
how FHE computation behaves in the presence of memory faults, both in terms of
individual operations as well as at the level of applications, for different
FHE schemes. Finally, we investigate how effective traditional and FHE-specific
fault mitigation techniques are.

</details>


### [17] [Cryptographic Backdoor for Neural Networks: Boon and Bane](https://arxiv.org/abs/2509.20714)
*Anh Tu Ngo,Anupam Chattopadhyay,Subhamoy Maitra*

Main category: cs.CR

TL;DR: This paper explores the use of cryptographic backdoors in neural networks (NNs) for both attacking and defending against adversaries. It presents three practical protocols for watermarking, user authentication, and IP tracking, all with provable robustness under black-box assumptions. Empirical results are provided, and post-quantum applications are suggested.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to address vulnerabilities in NNs through cryptographic backdoors that can be used for attacks and defenses, leading to robust and secure ML systems, particularly for future quantum-era applications.

Method: The authors utilize cryptographic principles, inspired by Goldwasser et. al. [FOCS 2022], to design a backdoor that is integrated within a NN. These backdoors enable adversarial attacks. For defense, three protocols are proposed (watermarking, authentication, and tracking of unauthorized IP sharing), and the paper contributes theoretical analysis of their robustness against black-box adversaries.

Result: The paper successfully demonstrates the implementation of backdoor-based adversarial attack and defense protocols on state-of-the-art NNs. Theoretical robustness is shown for all three protocols, and practical results confirm these claims.

Conclusion: The study concludes that cryptographic backdoors in NNs are versatile and effective for conducting attacks and enhancing security through watermarking authentication and IP tracking, all with theoretical guarantees. It opens up potential for applying post-quantum cryptography in ML going forward.

Abstract: In this paper we show that cryptographic backdoors in a neural network (NN)
can be highly effective in two directions, namely mounting the attacks as well
as in presenting the defenses as well. On the attack side, a carefully planted
cryptographic backdoor enables powerful and invisible attack on the NN.
Considering the defense, we present applications: first, a provably robust NN
watermarking scheme; second, a protocol for guaranteeing user authentication;
and third, a protocol for tracking unauthorized sharing of the NN intellectual
property (IP). From a broader theoretical perspective, borrowing the ideas from
Goldwasser et. al. [FOCS 2022], our main contribution is to show that all these
instantiated practical protocol implementations are provably robust. The
protocols for watermarking, authentication and IP tracking resist an adversary
with black-box access to the NN, whereas the backdoor-enabled adversarial
attack is impossible to prevent under the standard assumptions. While the
theoretical tools used for our attack is mostly in line with the Goldwasser et.
al. ideas, the proofs related to the defense need further studies. Finally, all
these protocols are implemented on state-of-the-art NN architectures with
empirical results corroborating the theoretical claims. Further, one can
utilize post-quantum primitives for implementing the cryptographic backdoors,
laying out foundations for quantum-era applications in machine learning (ML).

</details>


### [18] [ExpIDS: A Drift-adaptable Network Intrusion Detection System With Improved Explainability](https://arxiv.org/abs/2509.20767)
*Ayush Kumar,Kar Wai Fok,Vrizlynn L. L. Thing*

Main category: cs.CR

TL;DR: ExpIDS is a deep learning-based NIDS designed for high decision tree explanation fidelity and adaptability to network traffic drift, achieving performance comparable to state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Cyber security experts are hesitant to adopt ML-driven NIDS due to their 'black box' nature; explaining model decisions is critical for real-world deployment.

Method: ExpIDS uses deep learning to optimize decision tree explanation fidelity (aligning predictions with interpretable decision trees), incorporates mechanisms to adapt to traffic distribution drift, and employs extensive experiments for validation.

Result: ExpIDS achieves higher decision tree explanation fidelity than alternatives and matches state-of-the-art NIDS in detecting common attacks, even under realistic network traffic shifts.

Conclusion: ExpIDS bridges the gap between ML effectiveness and explainability in NIDS, demonstrating that high-performance, transparent systems can be both interpretable and robust to real-world conditions.

Abstract: Despite all the advantages associated with Network Intrusion Detection
Systems (NIDSs) that utilize machine learning (ML) models, there is a
significant reluctance among cyber security experts to implement these models
in real-world production settings. This is primarily because of their opaque
nature, meaning it is unclear how and why the models make their decisions. In
this work, we design a deep learning-based NIDS, ExpIDS to have high decision
tree explanation fidelity, i.e., the predictions of decision tree explanation
corresponding to ExpIDS should be as close to ExpIDS's predictions as possible.
ExpIDS can also adapt to changes in network traffic distribution (drift). With
the help of extensive experiments, we verify that ExpIDS achieves higher
decision tree explanation fidelity and a malicious traffic detection
performance comparable to state-of-the-art NIDSs for common attacks with
varying levels of real-world drift.

</details>


### [19] [Fast Revocable Attribute-Based Encryption with Data Integrity for Internet of Things](https://arxiv.org/abs/2509.20796)
*Yongjiao Li,Liang Zhu,Yalin Deng,Qikun Zhang,Zhenlei Wang,Zhu Cao*

Main category: cs.CR

TL;DR: This paper proposes an efficient and secure revocable attribute-based encryption (RABE), designed for IoT environments, emphasizing adaptive security with multiple challenge ciphertexts, supports user revocation, and ensures data integrity by offloading heavy computations to the cloud.


<details>
  <summary>Details</summary>
Motivation: Current RABE schemes struggle with balancing efficiency, security, scalability, and data integrity, limiting their practical use in data-sharing scenarios in IoT.', 'The computational constraints of IoT devices further exacerbate this challenge.

Method: The scheme transfers computationally intensive revocation processes to the cloud, supports user revocation, and integrates data integrity assurance. Adaptive security is achieved under a defined security model involving multiple challenge ciphertexts.

Result: The proposed scheme demonstrates 7–9× reduction in computational consumption compared to existing solutions, while maintaining adaptive security and data integrity. Experimental validation confirms its superior performance and optimization.

Conclusion: The proposed RABE scheme effectively addresses efficiency, scalability, and security limitations in IoT environments, offering a practical solution for secure data sharing with strong revocation mechanisms and reduced device computational load.

Abstract: Efficient and secure revocable attribute-based encryption (RABE) is vital for
ensuring flexible and fine-grained access control and data sharing in cloud
storage and outsourced data environments within the Internet of Things (IoT).
However, current RABE schemes often struggle to achieve an optimal balance
between efficiency, security, dynamic scalability, and other important
features, which hampers their practical application. To overcome these
limitations, we propose a fast RABE scheme with data integrity for IoT that
achieves adaptive security with multiple challenge ciphertexts. Our scheme
supports the revocation of authorized users and transfers the computationally
heavy revocation processes to the cloud, thereby easing the computational
burden on IoT devices. Moreover, it consistently guarantees the integrity and
correctness of data. We have demonstrated its adaptive security within the
defined security model with multiple challenge ciphertexts and optimized its
performance. Experimental results indicate that our scheme provides better
performance than existing solutions. Under the same access policy, our scheme
reduces computational consumption by 7 to 9 times compared to previous schemes.

</details>


### [20] [Intelligent Graybox Fuzzing via ATPG-Guided Seed Generation and Submodule Analysis](https://arxiv.org/abs/2509.20808)
*Raghul Saravanan,Sudipta Paria,Aritra Dasgupta,Swarup Bhunia,Sai Manoj P D*

Main category: cs.CR

TL;DR: This paper introduces PROFUZZ, a scalable directed fuzzing framework that combines ATPG and fuzzing to overcome limitations in existing hardware fuzzing techniques.


<details>
  <summary>Details</summary>
Motivation: Current Directed Gray-box Fuzzing tools like DirectFuzz suffer from limited language support, poor scalability, abstraction mismatches, and inefficiencies in targeted test generation for hardware designs.

Method: PROFUZZ integrates Automatic Test Pattern Generation (ATPG) with Directed Fuzzing to structurally analyze hardware designs and generate precise input seeds that target specific regions while maintaining high throughput.

Result: PROFUZZ achieves 30× better scalability for multiple targets, 11.66% coverage improvement, and 2.76× faster execution compared to DirectFuzz on complex hardware systems.

Conclusion: PROFUZZ effectively addresses existing DGF limitations through ATPG-based structural analysis, demonstrating superior scalability and efficiency in directed hardware fuzzing.

Abstract: Hardware Fuzzing emerged as one of the crucial techniques for finding
security flaws in modern hardware designs by testing a wide range of input
scenarios. One of the main challenges is creating high-quality input seeds that
maximize coverage and speed up verification. Coverage-Guided Fuzzing (CGF)
methods help explore designs more effectively, but they struggle to focus on
specific parts of the hardware. Existing Directed Gray-box Fuzzing (DGF)
techniques like DirectFuzz try to solve this by generating targeted tests, but
it has major drawbacks, such as supporting only limited hardware description
languages, not scaling well to large circuits, and having issues with
abstraction mismatches. To address these problems, we introduce a novel
framework, PROFUZZ, that follows the DGF approach and combines fuzzing with
Automatic Test Pattern Generation (ATPG) for more efficient fuzzing. By
leveraging ATPG's structural analysis capabilities, PROFUZZ can generate
precise input seeds that target specific design regions more effectively while
maintaining high fuzzing throughput. Our experiments show that PROFUZZ scales
30x better than DirectFuzz when handling multiple target sites, improves
coverage by 11.66%, and runs 2.76x faster, highlighting its scalability and
effectiveness for directed fuzzing in complex hardware systems.

</details>


### [21] [Security-aware Semantic-driven ISAC via Paired Adversarial Residual Networks](https://arxiv.org/abs/2509.20835)
*Yu Liu,Boxiang He,Fanggang Wang*

Main category: cs.CR

TL;DR: The paper introduces SS-ISAC, a security-aware semantic-driven ISAC framework that integrates adversarial residual networks (ARNs) for encryption/decryption to balance SAC performance and eavesdropping prevention without hardware overhauls.


<details>
  <summary>Details</summary>
Motivation: Existing ISAC systems lack robust security against eavesdropping and adversarial attacks. The authors aim to address both communication quality and security demands by leveraging adversarial mechanisms from machine learning.

Method: 1. Implements a dual-ARN architecture with encryption (post-semantic transmitter) and decryption (pre-semantic receiver) modules. 2. Optimizes ARNs using a custom loss function that jointly minimizes adversarial attack power, SAC performance degradation, and privacy leakage risks.

Result: Simulation results demonstrate SS-ISAC achieves superior SAC performance and eavesdropping suppression compared to baseline methods, validating the effectiveness of the adversarial network design and joint optimization strategy.

Conclusion: SS-ISAC provides a flexible, hardware-efficient solution for secure ISAC systems by transforming adversarial attacks into features for security enhancement while maintaining communication/sensing performance.

Abstract: This paper proposes a novel and flexible security-aware semantic-driven
integrated sensing and communication (ISAC) framework, namely security semantic
ISAC (SS-ISAC). Inspired by the positive impact of the adversarial attack, a
pair of pluggable encryption and decryption modules is designed in the proposed
SS-ISAC framework. The encryption module is installed after the semantic
transmitter, adopting a trainable adversarial residual network (ARN) to create
the adversarial attack. Correspondingly, the decryption module before the
semantic receiver utilizes another trainable ARN to mitigate the adversarial
attack and noise. These two modules can be flexibly assembled considering the
system security demands, without drastically modifying the hardware
infrastructure. To ensure the sensing and communication (SAC) performance while
preventing the eavesdropping threat, the above ARNs are jointly optimized by
minimizing a carefully designed loss function that relates to the adversarial
attack power, SAC performance, as well as the privacy leakage risk. Simulation
results validate the effectiveness of the proposed SS-ISAC framework in terms
of both SAC and eavesdropping prevention performance.

</details>


### [22] [FlowXpert: Context-Aware Flow Embedding for Enhanced Traffic Detection in IoT Network](https://arxiv.org/abs/2509.20861)
*Chao Zha,Haolin Pan,Bing Bai,Jiangxing Wu,Ruyun Zhang*

Main category: cs.CR

TL;DR: This paper addresses the limitations of ML-based network traffic detection in IoT by proposing a feature extraction tool that eliminates time and length features for semantic, context-aware features. It introduces an embedding framework combining DBSCAN clustering and contrastive learning to enhance model performance, validated on the Mawi dataset with promising results.


<details>
  <summary>Details</summary>
Motivation: Continuous interaction among IoT devices generates complex and dynamic network traffic, making rule-based detection difficult. Current ML-based methods rely on tools like CICMeterFlow, which introduce high sparsity due to time- and length-related features and lack mechanisms to fully capture traffic semantics.

Method: The paper presents a novel feature extraction method that replaces time and length features with semantic features related to the source host. It also develops an embedding training framework combining unsupervised DBSCAN clustering with a contrastive learning strategy to capture fine-grained semantic representations of traffic.

Result: Extensive experiments on the Mawi dataset show improved detection accuracy, robustness, and generalization compared to state-of-the-art models. The approach also demonstrates applicability in real-time scenarios.

Conclusion: The paper concludes that its proposed method effectively addresses feature sparsity and semantic representation issues in IoT traffic detection, offering significant improvements in model performance and practical deployment.

Abstract: In the Internet of Things (IoT) environment, continuous interaction among a
large number of devices generates complex and dynamic network traffic, which
poses significant challenges to rule-based detection approaches. Machine
learning (ML)-based traffic detection technology, capable of identifying
anomalous patterns and potential threats within this traffic, serves as a
critical component in ensuring network security. This study first identifies a
significant issue with widely adopted feature extraction tools (e.g.,
CICMeterFlow): the extensive use of time- and length-related features leads to
high sparsity, which adversely affects model convergence. Furthermore, existing
traffic detection methods generally lack an embedding mechanism capable of
efficiently and comprehensively capturing the semantic characteristics of
network traffic. To address these challenges, we propose a novel feature
extraction tool that eliminates traditional time and length features in favor
of context-aware semantic features related to the source host, thus improving
the generalizability of the model. In addition, we design an embedding training
framework that integrates the unsupervised DBSCAN clustering algorithm with a
contrastive learning strategy to effectively capture fine-grained semantic
representations of traffic. Extensive empirical evaluations are conducted on
the real-world Mawi data set to validate the proposed method in terms of
detection accuracy, robustness, and generalization. Comparative experiments
against several state-of-the-art (SOTA) models demonstrate the superior
performance of our approach. Furthermore, we confirm its applicability and
deployability in real-time scenarios.

</details>


### [23] [A Generalized $χ_n$-Function](https://arxiv.org/abs/2509.20880)
*Cheng Lyu,Mu Yuan,Dabin Zheng,Siwei Sun,Shun Li*

Main category: cs.CR

TL;DR: Generalizes χ_n mapping for even n via χ_{n,m} and θ_{m,k}, establishes algebraic structures, proves bijections, analyzes security properties, and resolves prior conjectures.


<details>
  <summary>Details</summary>
Motivation: Original χ_n is non-bijective for even n, limiting cryptography use; prior work lacks structure-based permutation constructions for even dimensions.

Method: Introduces generalized mappings χ_{n,m} and θ_{m,k} with modular indices, proves abelian group isomorphism to F₂[z]/(z^⌊n/m⌋+1) units, analyzes iterations/fixed points, and provides cryptographic benchmarks.

Result: Constructs permutations for all n ≥ 1 with explicit inverse mappings, proves group-theoretic properties, confirms security advantages over χ_n and χχ_n via implementation analysis.

Conclusion: Solves the parity constraint of χ_n, provides a comprehensive framework for lightweight cryptographic permutations with verified algebraic and security properties.

Abstract: The mapping $\chi_n$ from $\F_{2}^{n}$ to itself defined by $y=\chi_n(x)$
with $y_i=x_i+x_{i+2}(1+x_{i+1})$, where the indices are computed modulo $n$,
has been widely studied for its applications in lightweight cryptography.
However, $\chi_n $ is bijective on $\F_2^n$ only when $n$ is odd, restricting
its use to odd-dimensional vector spaces over $\F_2$. To address this
limitation, we introduce and analyze the generalized mapping $\chi_{n, m}$
defined by $y=\chi_{n,m}(x)$ with $y_i=x_i+x_{i+m} (x_{i+m-1}+1)(x_{i+m-2}+1)
\cdots (x_{i+1}+1)$, where $m$ is a fixed integer with $m\nmid n$. To
investigate such mappings, we further generalize $\chi_{n,m}$ to $\theta_{m,
k}$, where $\theta_{m, k}$ is given by $y_i=x_{i+mk} \prod_{\substack{j=1,\,\,
m \nmid j}}^{mk-1} \left(x_{i+j}+1\right), \,\,{\rm for }\,\, i\in
\{0,1,\ldots,n-1\}$. We prove that these mappings generate an abelian group
isomorphic to the group of units in $\F_2[z]/(z^{\lfloor n/m\rfloor +1})$. This
structural insight enables us to construct a broad class of permutations over
$\F_2^n$ for any positive integer $n$, along with their inverses. We rigorously
analyze algebraic properties of these mappings, including their iterations,
fixed points, and cycle structures. Additionally, we provide a comprehensive
database of the cryptographic properties for iterates of $\chi_{n,m}$ for small
values of $n$ and $m$. Finally, we conduct a comparative security and
implementation cost analysis among $\chi_{n,m}$, $\chi_n$, $\chi\chi_n$
(EUROCRYPT 2025 \cite{belkheyar2025chi}) and their variants, and prove
Conjecture~1 proposed in~\cite{belkheyar2025chi} as a by-product of our study.
Our results lead to generalizations of $\chi_n$, providing alternatives to
$\chi_n$ and $\chi\chi_n$.

</details>


### [24] [Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools](https://arxiv.org/abs/2509.21011)
*Ping He,Changjiang Li,Binbin Zhao,Tianyu Du,Shouling Ji*

Main category: cs.CR

TL;DR: The paper introduces AutoMalTool, an automated red teaming framework that generates malicious MCP tools to exploit vulnerabilities in LLM-based agents, demonstrating their susceptibility to tool poisoning attacks while evading detection.


<details>
  <summary>Details</summary>
Motivation: Existing red teaming approaches for LLM agent vulnerabilities remain at the proof-of-concept stage, lacking systematic methods to identify security risks under MCP tool poisoning paradigms.

Method: Proposes AutoMalTool, an automated framework that generates malicious MCP tools through systematic attack pattern synthesis to test and manipulate mainstream LLM-based agents.

Result: AutoMalTool successfully generates undetectable malicious MCP tools that manipulate agent behavior, uncovering new security risks in widely adopted LLM-based agent systems.

Conclusion: Highlights critical security gaps in LLM-based agent systems under MCP tool integration, necessitating advanced defense mechanisms against automated tool poisoning attacks.

Abstract: The remarkable capability of large language models (LLMs) has led to the wide
application of LLM-based agents in various domains. To standardize interactions
between LLM-based agents and their environments, model context protocol (MCP)
tools have become the de facto standard and are now widely integrated into
these agents. However, the incorporation of MCP tools introduces the risk of
tool poisoning attacks, which can manipulate the behavior of LLM-based agents.
Although previous studies have identified such vulnerabilities, their red
teaming approaches have largely remained at the proof-of-concept stage, leaving
the automatic and systematic red teaming of LLM-based agents under the MCP tool
poisoning paradigm an open question. To bridge this gap, we propose
AutoMalTool, an automated red teaming framework for LLM-based agents by
generating malicious MCP tools. Our extensive evaluation shows that AutoMalTool
effectively generates malicious MCP tools capable of manipulating the behavior
of mainstream LLM-based agents while evading current detection mechanisms,
thereby revealing new security risks in these agents.

</details>


### [25] [RLCracker: Exposing the Vulnerability of LLM Watermarks with Adaptive RL Attacks](https://arxiv.org/abs/2509.20924)
*Hanbo Huang,Yiran Zhang,Hao Zheng,Xuan Gong,Yihan Li,Lin Liu,Shiyu Liang*

Main category: cs.CR

TL;DR: This paper reveals critical vulnerabilities in LLM watermarking defenses by introducing adaptive robustness radius as a formal metric. It proposes RLCracker, an RL-based paraphrase attack achieving 98.58% watermark removal success with minimal samples, demonstrating severe threats to existing AI watermarking security.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking security claims are overstated due to insufficiently adversarial evaluations, creating a false sense of security while critical vulnerabilities persist.

Method: 1) Theoretical proof showing optimizing attack context/model parameters reduces adaptive robustness radius 2Development of RLCracker: a reinforcement learning attack that erases watermarks while preserving semantic content using limited watermarked examples and no detector access.

Result: RLCracker achieves 98.58% removal success rate and 0.92 P-SP semantic preservation score on 1500-token texts with just 100 short training samples. Outperforms GPT-4o by 1400% and generalizes across 10 watermarking schemes and 5 model sizes.

Conclusion: Current LLM watermarking defenses are fundamentally vulnerable to adaptive attacks, necessitating reevaluation of security claims and development of more robust watermarking techniques.

Abstract: Large Language Models (LLMs) watermarking has shown promise in detecting
AI-generated content and mitigating misuse, with prior work claiming robustness
against paraphrasing and text editing. In this paper, we argue that existing
evaluations are not sufficiently adversarial, obscuring critical
vulnerabilities and overstating the security. To address this, we introduce
adaptive robustness radius, a formal metric that quantifies watermark
resilience against adaptive adversaries. We theoretically prove that optimizing
the attack context and model parameters can substantially reduce this radius,
making watermarks highly susceptible to paraphrase attacks. Leveraging this
insight, we propose RLCracker, a reinforcement learning (RL)-based adaptive
attack that erases watermarks while preserving semantic fidelity. RLCracker
requires only limited watermarked examples and zero access to the detector.
Despite weak supervision, it empowers a 3B model to achieve 98.5% removal
success and an average 0.92 P-SP score on 1,500-token Unigram-marked texts
after training on only 100 short samples. This performance dramatically exceeds
6.75% by GPT-4o and generalizes across five model sizes over ten watermarking
schemes. Our results confirm that adaptive attacks are broadly effective and
pose a fundamental threat to current watermarking defenses.

</details>


### [26] [CTI Dataset Construction from Telegram](https://arxiv.org/abs/2509.20943)
*Dincy R. Arikkat,Sneha B. T.,Serena Nicolazzo,Antonino Nocera,Vinod P.,Rafidha Rehiman K. A.,Karthika R*

Main category: cs.CR

TL;DR: This paper presents an automated pipeline to extract a large-scale Cyber Threat Intelligence (CTI) dataset from Telegram, achieving 96.64% classification accuracy and compiling 86,509 malicious indicators.


<details>
  <summary>Details</summary>
Motivation: High-quality CTI datasets are essential for combating evolving cyber threats, but building such datasets is challenging due to the dynamic nature of attack vectors and the need for timely threat sources like Telegram.

Method: An end-to-end pipeline identifies relevant Telegram channels, scrapes 145,349 messages from curated channels, and applies a BERT-based classifier to filter threat intelligence messages, extracting malicious indicators (domains, IPs, URLs, hashes, CVEs).

Result: A dataset of 86,509 high-fidelity malicious indicators with 96.64% classification accuracy, demonstrating scalability and reliability for CTI collection.

Conclusion: The pipeline establishes a durable framework for generating and maintaining large-scale CTI datasets from Telegram, enabling improved threat detection and serving as a foundation for future research.

Abstract: Cyber Threat Intelligence (CTI) enables organizations to anticipate, detect,
and mitigate evolving cyber threats. Its effectiveness depends on high-quality
datasets, which support model development, training, evaluation, and
benchmarking. Building such datasets is crucial, as attack vectors and
adversary tactics continually evolve. Recently, Telegram has gained prominence
as a valuable CTI source, offering timely and diverse threat-related
information that can help address these challenges. In this work, we address
these challenges by presenting an end-to-end automated pipeline that
systematically collects and filters threat-related content from Telegram. The
pipeline identifies relevant Telegram channels and scrapes 145,349 messages
from 12 curated channels out of 150 identified sources. To accurately filter
threat intelligence messages from generic content, we employ a BERT-based
classifier, achieving an accuracy of 96.64%. From the filtered messages, we
compile a dataset of 86,509 malicious Indicators of Compromise, including
domains, IPs, URLs, hashes, and CVEs. This approach not only produces a
large-scale, high-fidelity CTI dataset but also establishes a foundation for
future research and operational applications in cyber threat detection.

</details>


### [27] [Dual-Path Phishing Detection: Integrating Transformer-Based NLP with Structural URL Analysis](https://arxiv.org/abs/2509.20972)
*Ibrahim Altan,Abdulla Bachir,Yousuf Parbhulkar,Abdul Muksith Rizvi,Moshiur Farazi*

Main category: cs.CR

TL;DR: This paper proposes a dual-path phishing detection framework combining transformer-based NLP (e.g., DistilBERT) for email text analysis and classical machine learning (e.g., Random Forests) for URL analysis, achieving improved accuracy and flexibility.


<details>
  <summary>Details</summary>
Motivation: Traditional phishing detection methods inadequately address sophisticated attacks by analyzing email content or URLs in isolation, failing to leverage complementary semantic and structural features effectively.

Method: A dual-path framework integrating fine-tuned transformers (for semantic email text analysis) and character-level TF-IDF vectorization with classical classifiers (for structural URL analysis), enabling joint analysis of phishing components.

Result: The DistilBERT model achieved optimal accuracy-efficiency tradeoffs for text analysis, while Random Forest classifiers outperformed peers in URL-based detection. Combined, the framework demonstrated significantly improved detection accuracy on representative datasets.

Conclusion: The dual-path approach establishes a scalable, interpretable phishing detection solution that outperforms single-modality methods, offering practical deployment flexibility and enhanced defenses against evolving phishing threats.

Abstract: Phishing emails pose a persistent and increasingly sophisticated threat,
undermining email security through deceptive tactics designed to exploit both
semantic and structural vulnerabilities. Traditional detection methods, often
based on isolated analysis of email content or embedded URLs, fail to
comprehensively address these evolving attacks. In this paper, we propose a
dual-path phishing detection framework that integrates transformer-based
natural language processing (NLP) with classical machine learning to jointly
analyze email text and embedded URLs. Our approach leverages the complementary
strengths of semantic analysis using fine-tuned transformer architectures
(e.g., DistilBERT) and structural link analysis via character-level TF-IDF
vectorization paired with classical classifiers (e.g., Random Forest).
Empirical evaluation on representative email and URL datasets demonstrates that
this combined approach significantly improves detection accuracy. Specifically,
the DistilBERT model achieves a near-optimal balance between accuracy and
computational efficiency for textual phishing detection, while Random Forest
notably outperforms other classical classifiers in identifying malicious URLs.
The modular design allows flexibility for standalone deployment or ensemble
integration, facilitating real-world adoption. Collectively, our results
highlight the efficacy and practical value of this dual-path approach,
establishing a scalable, accurate, and interpretable solution capable of
enhancing email security against contemporary phishing threats.

</details>


### [28] [PMark: Towards Robust and Distortion-free Semantic-level Watermarking with Channel Constraints](https://arxiv.org/abs/2509.21057)
*Jiahao Huo,Shuliang Liu,Bin Wang,Junyan Zhang,Yibo Yan,Aiwei Liu,Xuming Hu,Mingxun Zhou*

Main category: cs.CR

TL;DR: The paper introduces PMark, a new semantic-level watermarking method for large language models using proxy functions, which improves robustness and text quality over existing methods.


<details>
  <summary>Details</summary>
Motivation: Important for watermarking models to remain robust against modifications and maintain natural text output. There is a problem with distortion when using reject sampling's-based generation and inadequate theoretical robustness.

Method: The method consists of leveraging proxy functions (PFs) to map sentences to scalar values, then dynamically estimating the PF median while enforcing multiple PF constraints (channels) to strengthen watermark evidence.

Result: PMark offers improved robustness against paraphrasing attacks and maintains text quality with minimal distortion. The optimized version is efficient for implementation as it removes the need for dynamic median estimation.

Conclusion: The proposed PMark method provides a more effective paradigms for watermarking LLMs, which is demonstrated through experimental results improving  current SWM baselines in effectiveness and efficiency.

Abstract: Semantic-level watermarking (SWM) for large language models (LLMs) enhances
watermarking robustness against text modifications and paraphrasing attacks by
treating the sentence as the fundamental unit. However, existing methods still
lack strong theoretical guarantees of robustness, and reject-sampling-based
generation often introduces significant distribution distortions compared with
unwatermarked outputs. In this work, we introduce a new theoretical framework
on SWM through the concept of proxy functions (PFs) $\unicode{x2013}$ functions
that map sentences to scalar values. Building on this framework, we propose
PMark, a simple yet powerful SWM method that estimates the PF median for the
next sentence dynamically through sampling while enforcing multiple PF
constraints (which we call channels) to strengthen watermark evidence. Equipped
with solid theoretical guarantees, PMark achieves the desired distortion-free
property and improves the robustness against paraphrasing-style attacks. We
also provide an empirically optimized version that further removes the
requirement for dynamical median estimation for better sampling efficiency.
Experimental results show that PMark consistently outperforms existing SWM
baselines in both text quality and robustness, offering a more effective
paradigm for detecting machine-generated text. Our code will be released at
[this URL](https://github.com/PMark-repo/PMark).

</details>


### [29] [Emerging Paradigms for Securing Federated Learning Systems](https://arxiv.org/abs/2509.21147)
*Amr Akmal Abouelmagd,Amr Hilal*

Main category: cs.CR

TL;DR: This survey explores novel techniques to improve privacy and efficiency in Federated Learning (FL), addressing the limitations of traditional methods like MPC, HE, and DP, and highlights open challenges for future research.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the limitations of existing privacy-preserving techniques in FL, such as high computational costs and scalability issues, necessitating the exploration of emerging solutions for secure and efficient decentralized learning.

Method: The method involves a comprehensive survey and analysis of emerging approaches such as Trusted Execution Environments (TEEs), Physical Unclonable Functions (PUFs), Quantum Computing (QC), Chaos-Based Encryption (CBE), Neuromorphic Computing (NC), and Swarm Intelligence (SI), evaluating their relevance to FL by examining strengths, limitations, and practical deployment factors.

Result: The result is an in-depth assessment of how these emerging techniques can enhance privacy and efficiency in FL, providing a detailed taxonomy of their applications, advantages, and drawbacks within FL frameworks.

Conclusion: The conclusion emphasizes that while these emerging methods present new opportunities to overcome FL's privacy-efficiency trade-offs, significant challenges remain in their integration and scalability, and the paper outlines a roadmap for future advancements in secure and scalable FL systems.

Abstract: Federated Learning (FL) facilitates collaborative model training while
keeping raw data decentralized, making it a conduit for leveraging the power of
IoT devices while maintaining privacy of the locally collected data. However,
existing privacy- preserving techniques present notable hurdles. Methods such
as Multi-Party Computation (MPC), Homomorphic Encryption (HE), and Differential
Privacy (DP) often incur high compu- tational costs and suffer from limited
scalability. This survey examines emerging approaches that hold promise for
enhancing both privacy and efficiency in FL, including Trusted Execution
Environments (TEEs), Physical Unclonable Functions (PUFs), Quantum Computing
(QC), Chaos-Based Encryption (CBE), Neuromorphic Computing (NC), and Swarm
Intelligence (SI). For each paradigm, we assess its relevance to the FL
pipeline, outlining its strengths, limitations, and practical considerations.
We conclude by highlighting open challenges and prospective research avenues,
offering a detailed roadmap for advancing secure and scalable FL systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [30] [ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation](https://arxiv.org/abs/2509.20380)
*Samyak Jhaveri,Vanessa Klotzmann,Crista Lopes*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The increasing ubiquity of GPUs is accompanied by the increasing complexity
of their hardware and parallel programming frameworks. Directive-based parallel
programming standards like OpenACC simplify GPU programming to some extent by
abstracting away low-level complexities, but a fair amount of expertise is
still required in order to use those directives effectively.
  We introduce ACCeLLiuM, two open weights Large Language Models specifically
fine-tuned for generating expert OpenACC directives for data-parallel loops,
along with the supervised fine-tuning dataset that was used to train them. The
ACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from
public GitHub C/C++ repositories, with 3,223 pairs for training and 810 for
testing. Experimental evaluations show a pronounced performance gap in
generating correct OpenACC pragmas between base LLMs and our fine-tuned
versions. On the held-out test set, base LLMs fail to consistently generate
valid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid
pragmas with the correct directive type for $87\%$ of the data-parallel loops,
and exact pragmas--including directives, clauses, clause order, and clause
variables--for $50\%$ of the cases. Even when not exact, generated pragmas
frequently incorporate the correct clauses in a different order than the
ground-truth label, or include additional clauses that enable finer control
over parallel execution, data movement, and concurrency, offering practical
value beyond strict string-matching. By publicly releasing the code, models,
and dataset as ACCeLLiuM we hope to establish a reproducible benchmark for
LLM-powered OpenACC pragma generation, and lower the barrier to automated GPU
offloading of serially written programs.

</details>


### [31] [State-of-the-Art in Software Security Visualization: A Systematic Review](https://arxiv.org/abs/2509.20385)
*Ishara Devendra,Chaman Wijesiriwardana,Prasad Wimalaratne*

Main category: cs.SE

TL;DR: The paper systematically reviews software security visualization techniques, categorizing them into four types and highlighting two main areas—software development and operational security visualization—with a focus on adapting to evolving threats.


<details>
  <summary>Details</summary>
Motivation: Traditional text-based and numerical methods are becoming ineffective as software systems and the threat landscape grow more complex. There is a need for better visualization techniques to enhance threat detection and security response strategies.

Method: The authors conducted a systematic review of the literature, analyzing over 60 recent key research papers to identify and categorize software security visualization techniques into a comprehensive taxonomy.

Result: The review resulted in a taxonomy of four visualization types: graph-based, notation-based, matrix-based, and metaphor-based. It identified two main areas—extensive software development visualization and operational/cybersecurity visualization—shaping the current research and practices.

Conclusion: Innovative and adaptable visualization techniques are essential for addressing current and future software security challenges. They offer practical benefits for threat detection and response, and guide future research.

Abstract: Software security visualization is an interdisciplinary field that combines
the technical complexity of cybersecurity, including threat intelligence and
compliance monitoring, with visual analytics, transforming complex security
data into easily digestible visual formats. As software systems get more
complex and the threat landscape evolves, traditional text-based and numerical
methods for analyzing and interpreting security concerns become increasingly
ineffective. The purpose of this paper is to systematically review existing
research and create a comprehensive taxonomy of software security visualization
techniques through literature, categorizing these techniques into four types:
graph-based, notation-based, matrix-based, and metaphor-based visualization.
This systematic review explores over 60 recent key research papers in software
security visualization, highlighting its key issues, recent advancements, and
prospective future research directions. From the comprehensive analysis, the
two main areas were distinctly highlighted as extensive software development
visualization, focusing on advanced methods for depicting software
architecture: operational security visualization and cybersecurity
visualization. The findings highlight the necessity for innovative
visualization techniques that adapt to the evolving security landscape, with
practical implications for enhancing threat detection, improving security
response strategies, and guiding future research.

</details>


### [32] [Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments](https://arxiv.org/abs/2509.20386)
*Nishant Gaurav,Adit Akarsh,Ankit Ranjan,Manoj Bajaj*

Main category: cs.SE

TL;DR: Dynamic ReAct is a new approach that allows ReAct agents to handle large tool sets efficiently without overloading memory. The method uses a search-and-load mechanism to select the most relevant tools dynamically, reducing the number of tools loaded by up to 50% and maintaining task accuracy. This advancement supports the development of more general AI agents.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper arises from the challenge of enabling ReAct agents to operate effectively in environments with a vast number of available Model Control Protocol (MCP) tools. Loading all these tools simultaneously is computationally infeasible due to the contextual memory limitations of large language models. This issue limits the scalability and real-world applicability of ReAct agents when dealing with extensive tool sets.

Method: The method proposed in the paper involves creating and evaluating five distinct architectures that progressively refine the process of selecting relevant tools from a large set of Model Control Protocol (MCP) tools. These architectures focus on optimizing the tool selection to reduce the number of tools loaded, with the ultimate solution being a search-and-load mechanism that allows for intelligent and efficient tool selection without overloading the model's memory. The search-and-load approach prioritizes the most suitable tools for the current task based on dynamic consideration, ensuring adaptive and intelligent selection.

Result: The results show that the proposed approach achieves a reduction in the number of tools loaded by up to 50% compared to traditional methods, without compromising the accuracy of task completion. This significant overall efficiency makes it possible for ReAct agents to manage larger and more diverse Model Control Protocol (MCP) tool sets effectively.

Conclusion: The conclusion emphasizes that Dynamic ReAct addresses the critical challenge of inefficiency in large-scale tool usage for ReAct agents by dynamically selecting the most relevant tools. By achieving high task completion accuracy with reduced computational overhead, the approach marks a significant step forward in the development of general AI agents capable of adaption to a variety of task environments.

Abstract: We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef-
ficiently operate with extensive Model Control Protocol (MCP) tool sets that
exceed the contextual memory limitations of large language models. Our approach
addresses the fundamental challenge of tool selection in environments
containing hundreds or thousands of available tools, where loading all tools
simultaneously is computationally infeasible. We propose and evaluate five
distinct architectures that progressively refine the tool selection process,
culminating in a search-and-load mechanism that achieves intelligent tool
selection with minimal computational overhead. Our experimental results
demonstrate that the proposed approach reduces tool loading by up to 50% while
maintaining task completion accuracy, advancing the path towards truly
general-purpose AI agents capable of dynamically adapting to diverse task
environments.

</details>


### [33] [Towards Systematic Specification and Verification of Fairness Requirements: A Position Paper](https://arxiv.org/abs/2509.20387)
*Qusai Ramadan,Jukka Ruohonen,Abhishek Tiwari,Adam Alami,Zeyd Boukhers*

Main category: cs.SE

TL;DR: The paper proposes a knowledge graph-based framework to specify and verify fairness requirements in software systems.


<details>
  <summary>Details</summary>
Motivation: Improperly designed software systems can discriminate against people based on protected characteristics like gender and ethnicity. Existing studies focus on algorithmic flaws and biased data, but overlook the role of ambiguous or missing fairness requirements. This paper addresses that gap.

Method: We draw on lessons from the use of knowledge graphs in security engineering domains and propose a framework specifically for formalizing fairness requirements. The methodology includes the discussion of challenges, formulation of research questions, and a strategic roadmap for research progress in this area.

Result: The paper does not present experimental results but lays the theoretical and conceptual groundwork for a new approach to fairness in software development.

Conclusion: This study fills a critical gap by formalizing the need for a knowledge graph-based framework to articulate and verify fairness requirements, thereby offering potential to mitigate discrimination in software systems.

Abstract: Decisions suggested by improperly designed software systems might be prone to
discriminate against people based on protected characteristics, such as gender
and ethnicity. Previous studies attribute such undesired behavior to flaws in
algorithmic design or biased data. However, these studies ignore that
discrimination is often the result of a lack of well-specified fairness
requirements and their verification. The fact that experts' knowledge about
fairness is often implicit makes the task of specifying precise and verifiable
fairness requirements difficult. In related domains, such as security
engineering, knowledge graphs have been proven to be effective in formalizing
knowledge to assist requirements specification and verification. To address the
lack of formal mechanisms for specifying and verifying fairness requirements,
we propose the development of a knowledge graph-based framework for fairness.
In this paper, we discuss the challenges, research questions, and a road map
towards addressing the research questions.

</details>


### [34] [Online-Optimized RAG for Tool Use and Function Calling](https://arxiv.org/abs/2509.20415)
*Yu Pan,Xiaocheng Li,Hanzhao Wang*

Main category: cs.SE

TL;DR: The paper introduces Online-Optimized RAG, a framework that continuously adjusts retrieval embeddings using minimal feedback during deployment to enhance tool selection accuracy and task success in RAG systems.


<details>
  <summary>Details</summary>
Motivation: Current RAG systems often encounter task failures due to misalignment in retrieval embeddings caused by imperfect models or noisy descriptions, which is crucial to address for real-world applications.

Method: Online-Optimized RAG employs lightweight online gradient updates to refine retrieval embeddings with minimal deployment-time latency, allowing for adaptation to live interactions without modifying the underlying LLM.

Result: Across various tool-use and document-retrieval scenarios, the proposed method consistently improves tool selection accuracy and end-task success rates.

Conclusion: Online-Optimized RAG provides a robust, self-improving solution for RAG systems, making them more practical and reliable for diverse applications through adaptive embeddings.

Abstract: In many applications, retrieval-augmented generation (RAG) drives tool use
and function calling by embedding the (user) queries and matching them to
pre-specified tool/function descriptions. In this paper, we address an
embedding misalignment issue that often arises in practical applications due to
imperfect embedding models or noisy descriptions; such misalignment may lead to
incorrect retrieval and task failure. We introduce Online-Optimized RAG, a
deployment-time framework that continually adapts retrieval embeddings from
live interactions using minimal feedback (e.g., task success). Online-Optimized
RAG applies lightweight online gradient updates with negligible per-query
latency and requires no changes to the underlying LLM. The method is
plug-and-play: it supports both single- and multi-hop tool use, dynamic tool
inventories, and $K$-retrieval with re-ranking. We provide a problem-dependent
theoretical analysis that quantifies how the method's performance depends on
the initialization quality of the embeddings and other related quantities.
Across diverse tool-use and document-retrieval scenarios, our Online-Optimized
RAG consistently improves tool selection accuracy and end-task success, thus
providing a simple, practical path to robust, self-improving RAG systems.

</details>


### [35] [Formal Verification of Legal Contracts: A Translation-based Approach](https://arxiv.org/abs/2509.20421)
*Reiner Hähnle,Cosimo Laneve,Adele Veschetti*

Main category: cs.SE

TL;DR: Stipula is a domain-specific language for legal contracts. This paper presents a method to verify their correctness by translating them into Java with JML annotations, using KeY for fully automatic deductive verification.


<details>
  <summary>Details</summary>
Motivation: Ensuring correctness of legal contracts with enforceable properties (e.g., asset transfers and obligations) is critical to avoid errors and disputes.

Method: Contracts are translated to Java code annotated with JML specifications. The deductive verification tool KeY is then used to verify partial and total correctness.

Result: The approach achieves fully automatic verification for a subset of Stipula contracts (those with disjoint cycles), demonstrating seamless integration with a general-purpose tool.

Conclusion: The study shows that general-purpose deductive verification tools like KeY can succeed in specialized domains like legal contract verification through translation-based approaches.

Abstract: Stipula is a domain-specific programming language designed to model legal
contracts with enforceable properties, especially those involving asset
transfers and obligations. This paper presents a methodology to formally verify
the correctness of Stipula contracts through translation into Java code
annotated with Java Modeling Language specifications. As a verification
backend, the deductive verification tool KeY is used. Both, the translation and
the verification of partial and total correctness for a large subset of Stipula
contracts, those with disjoint cycles, is fully automatic. Our work
demonstrates that a general-purpose deductive verification tool can be used
successfully in a translation approach.

</details>


### [36] [AI-Specific Code Smells: From Specification to Detection](https://arxiv.org/abs/2509.20491)
*Brahim Mahmoudi,Naouel Moha,Quentin Stievenert,Florent Avellaneda*

Main category: cs.SE

TL;DR: SpecDetect4AI addresses undetected AI-specific code smells by introducing a DSL-driven static analysis tool, detecting 22 smells with 88.66% precision and 88.89 recall across 20M lines of code.


<details>
  <summary>Details</summary>
Motivation: Existing tools fail to detect AI-specific code smells causing issues like unreproducibility and poor model generalization. This paper improves detection for AI-based systems.

Method: Combines a Domain-Specific Language (DSL) for declarative rule specification with an extensible static analysis tool. Defined 22 AI-specific code smells.

Result: Achieved 88.66% precision and 88.89 recall on 826 AI-based systems (20MLOC), outperforming existing tools. SUS score of 81.7/100 validates usability.

Conclusion: SpecDetect4AI effectively identifies AI-specific code smells at scale, demonstrating superior efficiency and extensibility for maintaining AI system quality.

Abstract: The rise of Artificial Intelligence (AI) is reshaping how software systems
are developed and maintained. However, AI-based systems give rise to new
software issues that existing detection tools often miss. Among these, we focus
on AI-specific code smells, recurring patterns in the code that may indicate
deeper problems such as unreproducibility, silent failures, or poor model
generalization. We introduce SpecDetect4AI, a tool-based approach for the
specification and detection of these code smells at scale. This approach
combines a high-level declarative Domain-Specific Language (DSL) for rule
specification with an extensible static analysis tool that interprets and
detects these rules for AI-based systems. We specified 22 AI-specific code
smells and evaluated SpecDetect4AI on 826 AI-based systems (20M lines of code),
achieving a precision of 88.66% and a recall of 88.89%, outperforming other
existing detection tools. Our results show that SpecDetect4AI supports the
specification and detection of AI-specific code smells through dedicated rules
and can effectively analyze large AI-based systems, demonstrating both
efficiency and extensibility (SUS 81.7/100).

</details>


### [37] [PromptDebt: A Comprehensive Study of Technical Debt Across LLM Projects](https://arxiv.org/abs/2509.20497)
*Ahmed Aljohani,Hyunsook Do*

Main category: cs.SE

TL;DR: The paper analyzes LLM-specific SATD by examining 93,142 Python files, finding that prompt design is the main source of debt and releasing a dataset for reproducibility.


<details>
  <summary>Details</summary>
Motivation: Integrating LLMs via APIs introduces new forms of technical debt, but systematic analysis of how it arises and what strategies mitigate it is missing.

Method: They perform a large-scale empirical study on 93,142 Python files across major LLM APIs and identify sources and frequency of LLM-specific SATD.

Result: 54.49% of debt from OpenAI; 12.35% from LangChain; prompt design is the primary cause (6.61%), especially instruction-based and few-shot prompts (38.60% and 18.13%), and a dataset is released.

Conclusion: Prompt design strategies and optimizations require further attention in managing technical debt for LLM integration, and the dataset provides support for future improvements.

Abstract: Large Language Models (LLMs) are increasingly embedded in software via APIs
like OpenAI, offering powerful AI features without heavy infrastructure. Yet
these integrations bring their own form of self-admitted technical debt (SATD).
In this paper, we present the first large-scale empirical study of LLM-specific
SATD: its origins, prevalence, and mitigation strategies. By analyzing 93,142
Python files across major LLM APIs, we found that 54.49% of SATD instances stem
from OpenAI integrations and 12.35% from LangChain use. Prompt design emerged
as the primary source of LLM-specific SATD, with 6.61% of debt related to
prompt configuration and optimization issues, followed by hyperparameter tuning
and LLM-framework integration. We further explored which prompt techniques
attract the most debt, revealing that instruction-based prompts (38.60%) and
few-shot prompts (18.13%) are particularly vulnerable due to their dependence
on instruction clarity and example quality. Finally, we release a comprehensive
SATD dataset to support reproducibility and offer practical guidance for
managing technical debt in LLM-powered systems.

</details>


### [38] [Enhancing Python Programming Education with an AI-Powered Code Helper: Design, Implementation, and Impact](https://arxiv.org/abs/2509.20518)
*Sayed Mahbub Hasan Amiri,Md Mainul Islam*

Main category: cs.SE

TL;DR: This paper introduces an AI-Python chatbot that enhances programming education by combining static analysis, dynamic execution, and LLMs like CodeLlama and GPT-4. It addresses gaps in traditional tools and AI code assistants, demonstrating high error-resolution success and improved student learning.


<details>
  <summary>Details</summary>
Motivation: Traditional coding tools lack interactive guidance while AI assistants prioritize code completion over learning. This chatbot bridges the gap by providing pedagogically focused, practical assistance for deeper conceptual understanding.

Method: The hybrid chatbot uses CodeLlama for code embeddings, GPT-4 for natural language interaction, and Docker sandboxing for safety. It integrates static/dynamic analysis and LLMs, tested via 1,500 student submissions with mixed-methods evaluation.

Result: Achieved 85% error resolution success (vs. 62% for pylint, 73% for GPT-4 alone), reduced debugging time by 59.3%, and 34% coding proficiency improvement. Student feedback praised clarity and confidence-building but noted latency and code sanitization limitations.

Conclusion: This chatbot demonstrates AI's potential to complement education by prioritizing skill retention and equity over code completion. Its architecture provides a framework for future AI tools that enhance conceptual understanding in programming instruction.

Abstract: This is the study that presents an AI-Python-based chatbot that helps
students to learn programming by demonstrating solutions to such problems as
debugging errors, solving syntax problems or converting abstract theoretical
concepts to practical implementations. Traditional coding tools like Integrated
Development Environments (IDEs) and static analyzers do not give robotic help
while AI-driven code assistants such as GitHub Copilot focus on getting things
done. To close this gap, our chatbot combines static code analysis, dynamic
execution tracing, and large language models (LLMs) to provide the students
with relevant and practical advice, hence promoting the learning process. The
chatbots hybrid architecture employs CodeLlama for code embedding, GPT-4 for
natural language interactions, and Docker-based sandboxing for secure
execution. Evaluated through a mixed-methods approach involving 1,500 student
submissions, the system demonstrated an 85% error resolution success rate,
outperforming standalone tools like pylint (62%) and GPT-4 (73%). Quantitative
results revealed a 59.3% reduction in debugging time among users, with pre- and
post-test assessments showing a 34% improvement in coding proficiency,
particularly in recursion and exception handling. Qualitative feedback from 120
students highlighted the chatbots clarity, accessibility, and
confidence-building impact, though critiques included occasional latency and
restrictive code sanitization. By balancing technical innovation with
pedagogical empathy, this research provides a blueprint for AI tools that
prioritize educational equity and long-term skill retention over mere code
completion. The chatbot exemplifies how AI can augment human instruction,
fostering deeper conceptual understanding in programming education.

</details>


### [39] [Enhancing LLM-based Fault Localization with a Functionality-Aware Retrieval-Augmented Generation Framework](https://arxiv.org/abs/2509.20552)
*Xinyu Shi,Zhenhao Li,An Ran Chen*

Main category: cs.SE

TL;DR: FaR-Loc is a new FL framework integrating LLMs and RAG to improve method-level fault localization.


<details>
  <summary>Details</summary>
Motivation: Despite the potential of LLMs in FL, they struggle in complex systems due to the lack of project-specific knowledge and difficulty in navigating large codebases.

Method: FaR-Loc employs three components: LLM Functionality Extraction for behavior description generation, Semantic Dense Retrieval for embedding both code and NL descriptions in a shared space, and LLM Re-ranking for contextual relevance-based ordering of retrieved methods.

Result: FaR-Loc outperforms existing LLM-based FL approaches by significant margins in both Top-1 and Top-5 accuracies on Defects4J, and also exhibits superiority over learning-based and spectrum-based baselines.

Conclusion: The integration of LLMs with retrieval-augmented generation via FaR-Loc can enhance FL effectiveness. The use of code-structure-aware pre-trained code embedding models further significantly boosts FL performance,.

Abstract: Fault localization (FL) is a critical but time-consuming task in software
debugging, aiming to identify faulty code elements. While recent advances in
large language models (LLMs) have shown promise for FL, they often struggle
with complex systems due to the lack of project-specific knowledge and the
difficulty of navigating large projects. To address these limitations, we
propose FaR-Loc, a novel framework that enhances method-level FL by integrating
LLMs with retrieval-augmented generation (RAG). FaR-Loc consists of three key
components: LLM Functionality Extraction, Semantic Dense Retrieval, and LLM
Re-ranking. First, given a failed test and its associated stack trace, the LLM
Functionality Extraction module generates a concise natural language
description that captures the failing behavior. Next, the Semantic Dense
Retrieval component leverages a pre-trained code-understanding encoder to embed
both the functionality description (natural language) and the covered methods
(code) into a shared semantic space, enabling the retrieval of methods with
similar functional behavior. Finally, the LLM Re-ranking module reorders the
retrieved methods based on their contextual relevance. Our experiments on the
widely used Defects4J benchmark show that FaR-Loc outperforms state-of-the-art
LLM-based baselines SoapFL and AutoFL, by 14.6% and 9.1% in Top-1 accuracy, by
19.2% and 22.1% in Top-5 accuracy, respectively. It also surpasses all
learning-based and spectrum-based baselines across all Top-N metrics without
requiring re-training. Furthermore, we find that pre-trained code embedding
models that incorporate code structure, such as UniXcoder, can significantly
improve fault localization performance by up to 49.0% in Top-1 accuracy.
Finally, we conduct a case study to illustrate the effectiveness of FaR-Loc and
to provide insights for its practical application.

</details>


### [40] [Design, Implementation and Evaluation of a Novel Programming Language Topic Classification Workflow](https://arxiv.org/abs/2509.20631)
*Michael Zhang,Yuan Tian,Mariam Guizani*

Main category: cs.SE

TL;DR: The paper introduces a new classification workflow for identifying core programming language topics in code, achieving high F1 scores using an SVM model on a large code dataset.


<details>
  <summary>Details</summary>
Motivation: Understanding the distribution of programming language topics in source code is crucial for technical decisions, onboarding, tooling, and education, especially as systems become more complex.

Method: The approach uses a multi-label Support Vector Machine (SVM) with a sliding window and voting strategy to detect and localize key language concepts in code. The model is trained on the IBM Project CodeNet dataset.

Result: The SVM model achieves an average F1 score of 0.90 across code language topics and 0.75 in code-topic highlight performance.

Conclusion: The paper provides a reusable pipeline for code analysis and data-driven software engineering, offering empirical evidence of its classification method's effectiveness.

Abstract: As software systems grow in scale and complexity, understanding the
distribution of programming language topics within source code becomes
increasingly important for guiding technical decisions, improving onboarding,
and informing tooling and education. This paper presents the design,
implementation, and evaluation of a novel programming language topic
classification workflow. Our approach combines a multi-label Support Vector
Machine (SVM) with a sliding window and voting strategy to enable fine-grained
localization of core language concepts such as operator overloading, virtual
functions, inheritance, and templates. Trained on the IBM Project CodeNet
dataset, our model achieves an average F1 score of 0.90 across topics and 0.75
in code-topic highlight. Our findings contribute empirical insights and a
reusable pipeline for researchers and practitioners interested in code analysis
and data-driven software engineering.

</details>


### [41] [Exploring Engagement in Hybrid Meetings](https://arxiv.org/abs/2509.20780)
*Daniela Grassi,Fabio Calefato,Darja Smite,Nicole Novielli,Filippo Lanubile*

Main category: cs.SE

TL;DR: This study examines engagement patterns in hybrid software development meetings, finding remote and onsite participants have similar engagement levels with some variations.


<details>
  <summary>Details</summary>
Motivation: The shift to hybrid work post-COVID-19 has introduced collaboration challenges, potentially causing remote isolation despite common remote meeting practices.

Method: The study analyzed engagement using self-reported questionnaires and biometric physiological data collected over weeks from professionals in three companies.

Result: Regression analysis showed similar engagement between onsite and remote workers. Remote experience lower engagement in long meetings regardless of participation mode. Engagement correlates with active roles but is reduced in large teams and afternoon meetings.

Conclusion: The study provides actionable insights for improving hybrid meetings in software teams and knowledge-sector organizations facing similar challenges.

Abstract: Background. The widespread adoption of hybrid work following the COVID-19
pandemic has fundamentally transformed software development practices,
introducing new challenges in communication and collaboration as organizations
transition from traditional office-based structures to flexible working
arrangements. This shift has established a new organizational norm where even
traditionally office-first companies now embrace hybrid team structures. While
remote participation in meetings has become commonplace in this new
environment, it may lead to isolation, alienation, and decreased engagement
among remote team members. Aims. This study aims to identify and characterize
engagement patterns in hybrid meetings through objective measurements, focusing
on the differences between co-located and remote participants. Method. We
studied professionals from three software companies over several weeks,
employing a multimodal approach to measure engagement. Data were collected
through self-reported questionnaires and physiological measurements using
biometric devices during hybrid meetings to understand engagement dynamics.
Results. The regression analyses revealed comparable engagement levels between
onsite and remote participants, though remote participants show lower
engagement in long meetings regardless of participation mode. Active roles
positively correlate with higher engagement, while larger meetings and
afternoon sessions are associated with lower engagement. Conclusions. Our
results offer insights into factors associated with engagement and
disengagement in hybrid meetings, as well as potential meeting improvement
recommendations. These insights are potentially relevant not only for software
teams but also for knowledge-intensive organizations across various sectors
facing similar hybrid collaboration challenges.

</details>


### [42] [Verification Limits Code LLM Training](https://arxiv.org/abs/2509.20837)
*Srishti Gureja,Elena Tommasone,Jingyi He,Sara Hooker,Matthias Gallé,Marzieh Fadaee*

Main category: cs.SE

TL;DR: This paper examines the 'verification ceiling' limitation in code generation models using synthetic data, proposing that relaxing verification thresholds and enhancing test diversity can improve model performance while retaining verification's necessity.


<details>
  <summary>Details</summary>
Motivation: Synthetic data for code generation is constrained by verification ceiling issues where rigid verification standards limit training data quality and diversity, hindering model performance.

Method: The study analyzes three verification aspects: test suite complexity/quantity, relaxed pass thresholds (e.g., LLM-based soft verification), and the importance of retaining correct solutions through formal correctness comparisons and human evaluation.

Result: Richer test suites improved pass@1 by 3 points, relaxed thresholds with high-quality tests yielded 2-4 point gains, and problem-solution diversity enhanced generalization. Overly strict verification risks discarding valuable diverse data.

Conclusion: Verification must be recalibrated—not eliminated—to balance quality and diversity. Combining calibrated verification with challenging problem-solution pairs is proposed to break the verification ceiling and advance code generation models.

Abstract: Large language models for code generation increasingly rely on synthetic
data, where both problem solutions and verification tests are generated by
models. While this enables scalable data creation, it introduces a previously
unexplored bottleneck: the verification ceiling, in which the quality and
diversity of training data are fundamentally constrained by the capabilities of
synthetic verifiers. In this work, we systematically study how verification
design and strategies influence model performance. We investigate (i) what we
verify by analyzing the impact of test complexity and quantity: richer test
suites improve code generation capabilities (on average +3 pass@1), while
quantity alone yields diminishing returns, (ii) how we verify by exploring
relaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By
allowing for relaxed thresholds or incorporating LLM-based soft verification,
we can recover valuable training data, leading to a 2-4 point improvement in
pass@1 performance. However, this benefit is contingent upon the strength and
diversity of the test cases used, and (iii) why verification remains necessary
through controlled comparisons of formally correct versus incorrect solutions
and human evaluation: retaining diverse correct solutions per problem yields
consistent generalization gains. Our results show that Verification as
currently practiced is too rigid, filtering out valuable diversity. But it
cannot be discarded, only recalibrated. By combining calibrated verification
with diverse, challenging problem-solution pairs, we outline a path to break
the verification ceiling and unlock stronger code generation models.

</details>


### [43] [PseudoBridge: Pseudo Code as the Bridge for Better Semantic and Logic Alignment in Code Retrieval](https://arxiv.org/abs/2509.20881)
*Yixuan Li,Xinyi Liu,Weidong Yang,Ben Fei,Shuhao Li,Mingjie Zhou,Lipeng Ma*

Main category: cs.SE

TL;DR: This paper introduces PseudoBridge, a code retrieval framework utilizing pseudo-code as an intermediate modality to bridge the semantic gap between natural language and programming languages, enhancing PLM-based methods through explicit alignment and code style augmentation.


<details>
  <summary>Details</summary>
Motivation: Existing PLM-based code search methods face a fundamental semantic gap between human intent and machine logic, as well as poor robustness to diverse coding styles, motivating the need for improved alignment and generalization strategies.

Method: PseudoBridge employs a two-stage approach: (1) uses an LLM to synthesize pseudo-code for explicit NL-PL alignment, and (2) applies a logic-invariant code style augmentation technique to generate stylistically diverse yet equivalent code implementations aligned with pseudo-code, enhancing style robustness.

Result: PseudoBridge outperforms baseline methods across 6 programming languages, achieving significant improvements in retrieval accuracy and zero-shot domain transfer scenarios (e.g., Solidity, XLCoST), demonstrating robustness and generalization.

Conclusion: The pseudo-code-mediated semantic alignment and style augmentation strategy in PseudoBridge effectively addresses key challenges in code search, offering a generalizable solution with potential for scalable, domain-agnostic code retrieval.

Abstract: Code search aims to precisely find relevant code snippets that match natural
language queries within massive codebases, playing a vital role in software
development. Recent advances leverage pre-trained language models (PLMs) to
bridge the semantic gap between unstructured natural language (NL) and
structured programming languages (PL), yielding significant improvements over
traditional information retrieval and early deep learning approaches. However,
existing PLM-based methods still encounter key challenges, including a
fundamental semantic gap between human intent and machine execution logic, as
well as limited robustness to diverse code styles. To address these issues, we
propose PseudoBridge, a novel code retrieval framework that introduces
pseudo-code as an intermediate, semi-structured modality to better align NL
semantics with PL logic. Specifically, PseudoBridge consists of two stages.
First, we employ an advanced large language model (LLM) to synthesize
pseudo-code, enabling explicit alignment between NL queries and pseudo-code.
Second, we introduce a logic-invariant code style augmentation strategy and
employ the LLM to generate stylistically diverse yet logically equivalent code
implementations with pseudo-code, then align the code snippets of different
styles with pseudo-code, enhancing model robustness to code style variation. We
build PseudoBridge across 10 different PLMs and evaluate it on 6 mainstream
programming languages. Extensive experiments demonstrate that PseudoBridge
consistently outperforms baselines, achieving significant gains in retrieval
accuracy and generalization, particularly under zero-shot domain transfer
scenarios such as Solidity and XLCoST datasets. These results demonstrate the
effectiveness of explicit logical alignment via pseudo-code and highlight
PseudoBridge's potential as a robust, generalizable solution for code
retrieval.

</details>


### [44] [Designing for Novice Debuggers: A Pilot Study on an AI-Assisted Debugging Tool](https://arxiv.org/abs/2509.21067)
*Oka Kurniawan,Erick Chandra,Christopher M. Poskitt,Yannic Noller,Kenny Tsu Wei Choo,Cyrille Jegourel*

Main category: cs.SE

TL;DR: This paper introduces CodeHinter, an AI debugging assistant designed to help novice programmers fix semantic errors while promoting active engagement. Through a user study, the authors demonstrate improved usability and effectiveness compared to previous versions, emphasizing the value of error localization and personalization in AI-assisted debugging tools.


<details>
  <summary>Details</summary>
Motivation: Existing AI debugging tools often lead to over-reliance on automation, lacking engagement. Novice programmers need tools that balance guidance with active participation to develop debugging skills.

Method: The authors designed CodeHinter by integrating traditional debugging tools with LLM-based techniques. They conducted a second design iteration and tested it with undergraduate students through a user study to evaluate usability and effectiveness in resolving semantic errors.

Result: Students found CodeHinter highly effective for semantic error resolution and significantly easier to use than the first version. Error localization was identified as the most valuable feature, consistent with prior findings.

Conclusion: AI-assisted debugging tools must be personalized based on user profiles to optimize interactions and promote active learning. CodeHinter’s success highlights the importance of balancing automation with student engagement.

Abstract: Debugging is a fundamental skill that novice programmers must develop.
Numerous tools have been created to assist novice programmers in this process.
Recently, large language models (LLMs) have been integrated with automated
program repair techniques to generate fixes for students' buggy code. However,
many of these tools foster an over-reliance on AI and do not actively engage
students in the debugging process. In this work, we aim to design an intuitive
debugging assistant, CodeHinter, that combines traditional debugging tools with
LLM-based techniques to help novice debuggers fix semantic errors while
promoting active engagement in the debugging process. We present findings from
our second design iteration, which we tested with a group of undergraduate
students. Our results indicate that the students found the tool highly
effective in resolving semantic errors and significantly easier to use than the
first version. Consistent with our previous study, error localization was the
most valuable feature. Finally, we conclude that any AI-assisted debugging tool
should be personalized based on user profiles to optimize their interactions
with students.

</details>


### [45] [An Improved Quantum Software Challenges Classification Approach using Transfer Learning and Explainable AI](https://arxiv.org/abs/2509.21068)
*Nek Dil Khan,Javed Ali Khan,Mobashir Husain,Muhammad Sohail Khan,Arif Ali Khan,Muhammad Azeem Akbar,Shahid Hussain*

Main category: cs.SE

TL;DR: This paper studies quantum software engineering (QSE)-related questions on Stack Overflow, categorizing them into challenges (Tooling, Theoretical, etc.) and comparing transformer-based (BERT, RoBERTa, 95% accuracy) vs. traditional ML (FNN, LSTM, 84-89%) classification methods. SHAP analysis enhances model transparency.


<details>
  <summary>Details</summary>
Motivation: Quantum developers face optimization challenges, often documented via SO posts with quantum tags. Existing tags lack specificity for QSE issues, necessitating systematic categorization to improve forums and vendor tools.

Method: Collected 2829 quantum-tagged questions from Q&As. Applied grounded theory/content analysis for dataset creation. Used ChatGPT to resolve annotation disagreements. Trained/trained bert_distilbert, Feedforward Neural Networks (FNN), CNN, LSTM models and evaluated SHAP-based interpretability.

Result: Transformer models achieved 95% accuracy, surpassing D/ML models by 6-9%. SHAP analysis clarified linguistic feature impacts on classification. No data augmentation required for transformers, unlike D&ML methods.

Conclusion: Transfomer algorithms offer superior QSE challenge classification with better accuracy and interpretability. Results support forum organization but require validation via empirical studies with quantum developers/vendors.

Abstract: Quantum Software Engineering (QSE) is a research area practiced by tech
firms. Quantum developers face challenges in optimizing quantum computing and
QSE concepts. They use Stack Overflow (SO) to discuss challenges and label
posts with specialized quantum tags, which often refer to technical aspects
rather than developer posts. Categorizing questions based on quantum concepts
can help identify frequent QSE challenges. We conducted studies to classify
questions into various challenges. We extracted 2829 questions from Q&A
platforms using quantum-related tags. Posts were analyzed to identify frequent
challenges and develop a novel grounded theory. Challenges include Tooling,
Theoretical, Learning, Conceptual, Errors, and API Usage. Through content
analysis and grounded theory, discussions were annotated with common challenges
to develop a ground truth dataset. ChatGPT validated human annotations and
resolved disagreements. Fine-tuned transformer algorithms, including BERT,
DistilBERT, and RoBERTa, classified discussions into common challenges. We
achieved an average accuracy of 95% with BERT DistilBERT, compared to
fine-tuned Deep and Machine Learning (D&ML) classifiers, including Feedforward
Neural Networks (FNN), Convolutional Neural Networks (CNN), and Long Short-Term
Memory networks (LSTM), which achieved accuracies of 89%, 86%, and 84%,
respectively. The Transformer-based approach outperforms the D&ML-based
approach with a 6\% increase in accuracy by processing actual discussions,
i.e., without data augmentation. We applied SHAP (SHapley Additive
exPlanations) for model interpretability, revealing how linguistic features
drive predictions and enhancing transparency in classification. These findings
can help quantum vendors and forums better organize discussions for improved
access and readability. However,empirical evaluation studies with actual
developers and vendors are needed.

</details>


### [46] [Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach](https://arxiv.org/abs/2509.21170)
*Yongda Yu,Guohao Shi,Xianwei Wu,Haochuan He,XueMing Gu,Qianqian Zhao,Kui Liu,Qiushi Wang,Zhao Tian,Haifeng Shen,Guoping Rong*

Main category: cs.SE

TL;DR: MelcotCR is a novel fine-tuning approach for Large Language Models (LLMs) that enhances their reasoning capabilities for multi-dimensional code review analysis. By combining the Maximum Entropy (ME) modeling principle with chain-of-thought (COT) techniques, it mitigates context and reasoning logic loss in long COT prompts, allowing a 14B base model to outperform state-of-the-art methods and rival a 671B model in accuracy for detecting and describing code issues.


<details>
  <summary>Details</summary>
Motivation: Despite the potential of LLMs in code review, their performance is constrained by the limited or vague information used during fine-tuning, which doesn't match the multi-dimensional analysis of human reviewers. Additionally, LLMs suffer from context loss and reasoning logic loss when handling long COT prompts, limiting their effectiveness in code issue detection and description.

Method: MelcotCR introduces a fine-tuning method that uses chain-of-thought (COT) techniques to train LLMs with rich structured information, considering multiple dimensions of code review. It combines the Maximum Entropy (ME) principle with pre-defined reasoning pathways to enhance in-context knowledge utilization and strengthen the logical coherence in the reasoning process. This addresses the limitations of long COT prompts in LLMs, improving their capacity for comprehensive code analysis.

Result: Empirical results show that MelcotCR allows a low-parameter base model (14B Qwen2.5) to surpass current leading methods in code issue detection and description accuracy, performing comparably to a high-capacity model (671B DeepSeek-R1) on both the MelcotCR dataset and the public CodeReviewer dataset.

Conclusion: This work demonstrates that MelcotCR significantly improves the performance of LLMs in code review by addressing context and reasoning loss challenges in long COT prompts, enabling a 14B model to outperform state-of-the-art methods and match the performance of a 671B model. This offers a promising approach for enhancing code review with less resource-intensive models while maintaining high accuracy levels.

Abstract: Large Language Models (LLMs) have shown great potential in supporting
automated code review due to their impressive capabilities in context
understanding and reasoning. However, these capabilities are still limited
compared to human-level cognition because they are heavily influenced by the
training data. Recent research has demonstrated significantly improved
performance through fine-tuning LLMs with code review data. However, compared
to human reviewers who often simultaneously analyze multiple dimensions of code
review to better identify issues, the full potential of these methods is
hampered by the limited or vague information used to fine-tune the models. This
paper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that
trains LLMs with an impressive reasoning ability to analyze multiple dimensions
of code review by harnessing long COT techniques to provide rich structured
information. To address context loss and reasoning logic loss issues that
frequently occur when LLMs process long COT prompts, we propose a solution that
combines the Maximum Entropy (ME) modeling principle with pre-defined reasoning
pathways in MelcotCR to enable more effective utilization of in-context
knowledge within long COT prompts while strengthening the logical tightness of
the reasoning process. Empirical evaluations on our curated MelcotCR dataset
and the public CodeReviewer dataset reveal that a low-parameter base model,
such as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art
methods in terms of the accuracy of detecting and describing code issues, with
its performance remarkably on par with that of the 671B DeepSeek-R1 model.

</details>


### [47] [Semantic Clustering of Civic Proposals: A Case Study on Brazil's National Participation Platform](https://arxiv.org/abs/2509.21292)
*Ronivaldo Ferreira,Guilherme da Silva,Carla Rocha,Gustavo Pinto*

Main category: cs.SE

TL;DR: This paper presents a scalable method combining BERTopic, seed words, and LLM validation to organize large volumes of citizen input on digital governance platforms with minimal human effort.


<details>
  <summary>Details</summary>
Motivation: Governments face challenges organizing massive citizen contributions on platforms like Brasil Participativo due to scale, reliance on expert curation, and alignment with official taxonomies.

Method: The approach integrates BERTopic for topic modeling, leverages seed words to guide topic coherence, and employs large language models for automatic validation against institutional requirements.

Result: Initial results demonstrate the methodology produces coherent, institutionally aligned topics that significantly reduce manual effort required for classifying citizen input.

Conclusion: This approach enables governments to efficiently transform unstructured citizen engagement into actionable policy data at scale through automated taxonomy alignment.

Abstract: Promoting participation on digital platforms such as Brasil Participativo has
emerged as a top priority for governments worldwide. However, due to the sheer
volume of contributions, much of this engagement goes underutilized, as
organizing it presents significant challenges: (1) manual classification is
unfeasible at scale; (2) expert involvement is required; and (3) alignment with
official taxonomies is necessary. In this paper, we introduce an approach that
combines BERTopic with seed words and automatic validation by large language
models. Initial results indicate that the generated topics are coherent and
institutionally aligned, with minimal human effort. This methodology enables
governments to transform large volumes of citizen input into actionable data
for public policy.

</details>
