<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 40]
- [cs.SE](#cs.SE) [Total: 46]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [SVDefense: Effective Defense against Gradient Inversion Attacks via Singular Value Decomposition](https://arxiv.org/abs/2510.03319)
*Chenxiang Luo,David K. Y. Yau,Qun Song*

Main category: cs.CR

TL;DR: Federated learning (FL) is susceptible to gradient inversion attacks (GIAs), from which adversaries reconstruct private training data from model gradients. In response, the authors propose a novel FL defense framework known as SVDefense that effectively addresses these privacy vulnerabilities. It is founded on the use of truncated SVD for obfuscating gradients and introduces three key innovations: a Self-Adaptive Energy Threshold, a Channel-Wise Weighted Approximation, and a Layer-Wise Weighted Aggregation. These innovations allow SVDefense to offer robust privacy protection without compromising model utility across a variety of embedded applications.


<details>
  <summary>Details</summary>
Motivation: This paper is driven by the challenge to effectively defeat gradient inversion attacks and enable FL to be more secure and privacy-compliant, particularly in resource-constrained embedded platforms. It aims for a solution that provides strong privacy guarantees without diminishing the model's performance. The motivation stems from the limitations of existing FL defense mechanisms that either lack scalability for embedded systems or suffer from inefficiencies when potential adversaries adapt by learning the defense mechanisms themselves.

Method: SVDefense employs truncated SVD to obscure gradient updates to prevent gradient inversion attacks. The framework presents three groundbreaking components: 1) a Self-Adaptive Energy Threshold that adapts dynamically to a client's vulnerability profile. 2) a Channel-Wise Weighted Approximation that enhances privacy while keeping the most pertinent gradient data for training. 3) a Layer-Wise Weighted Aggregation, that handles such issues as class imbalance, ensuring effective model training while maintaining robust privacy protection through these layered weighting strategies.

Result: Comprehensive testing of SVDefense shows its superiority over existing defenses in protecting against gradient inversion attacks across multiple real-world applications such as image classification, human activity recognition, and keyword spotting. It offers a more robust defense while preserving as much as possible the accuracy of the model. In addition, the paper highlights the practicality of SVDefense by showing its suitability for deployment in various resource-constrained embedded platforms.

Conclusion: SVDefense presents a strong framework for enhancing the privacy of federated learning systems. By introducing novel innovations to gradient obfuscation methods, it effectively balances both privacy and model performance. This paper concludes that these improvements render FL techniques more secure and deployable for real-world settings where computational resources are limited. The authors plan on making their code public upon paper acceptance to foster further research and development in this space.

Abstract: Federated learning (FL) enables collaborative model training without sharing
raw data but is vulnerable to gradient inversion attacks (GIAs), where
adversaries reconstruct private data from shared gradients. Existing defenses
either incur impractical computational overhead for embedded platforms or fail
to achieve privacy protection and good model utility at the same time.
Moreover, many defenses can be easily bypassed by adaptive adversaries who have
obtained the defense details. To address these limitations, we propose
SVDefense, a novel defense framework against GIAs that leverages the truncated
Singular Value Decomposition (SVD) to obfuscate gradient updates. SVDefense
introduces three key innovations, a Self-Adaptive Energy Threshold that adapts
to client vulnerability, a Channel-Wise Weighted Approximation that selectively
preserves essential gradient information for effective model training while
enhancing privacy protection, and a Layer-Wise Weighted Aggregation for
effective model aggregation under class imbalance. Our extensive evaluation
shows that SVDefense outperforms existing defenses across multiple
applications, including image classification, human activity recognition, and
keyword spotting, by offering robust privacy protection with minimal impact on
model accuracy. Furthermore, SVDefense is practical for deployment on various
resource-constrained embedded platforms. We will make our code publicly
available upon paper acceptance.

</details>


### [2] [Attack logics, not outputs: Towards efficient robustification of deep neural networks by falsifying concept-based properties](https://arxiv.org/abs/2510.03320)
*Raik Dankworth,Gesina Schwalbe*

Main category: cs.CR

TL;DR: This paper proposes extending adversarial attacks from output class flips to falsifying logical constraints based on human-interpretable concepts (e.g., red octagons must be stop signs) in vision neural networks, aiming to enhance robustness through reduced search spaces and better alignment with intuitive security goals.


<details>
  <summary>Details</summary>
Motivation: Current adversarial attacks only target final output classes (e.g., stop_sign → ¬stop_sign), neglecting deeper logical relationships within input concepts, leading to robustness vulnerabilities that don't reflect real-world failings.

Method: Introduces concept-based property falsification using explainable AI techniques (like concept activation analysis) to implement and attack logical constraints on pre-trained networks. Theoretical proof outlines how concept-based attacks have smaller search spaces than class-based attacks.

Result: Framework for testing logical compliance with human-readable constraints, preliminary theoretical foundation showing efficiency advantages, and a hypothesis that this method simultaneously improves logical consistency and robustness through targeted adversarial training.

Conclusion: Concept-based adversarial attacks provide a promising direction for neural network verification by aligning robustness goals with semantic properties; further work is needed to implement and test the approach on real-world vision systems.

Abstract: Deep neural networks (NNs) for computer vision are vulnerable to adversarial
attacks, i.e., miniscule malicious changes to inputs may induce unintuitive
outputs. One key approach to verify and mitigate such robustness issues is to
falsify expected output behavior. This allows, e.g., to locally proof security,
or to (re)train NNs on obtained adversarial input examples. Due to the
black-box nature of NNs, current attacks only falsify a class of the final
output, such as flipping from $\texttt{stop_sign}$ to $\neg\texttt{stop_sign}$.
In this short position paper we generalize this to search for generally
illogical behavior, as considered in NN verification: falsify constraints
(concept-based properties) involving further human-interpretable concepts, like
$\texttt{red}\wedge\texttt{octogonal}\rightarrow\texttt{stop_sign}$. For this,
an easy implementation of concept-based properties on already trained NNs is
proposed using techniques from explainable artificial intelligence. Further, we
sketch the theoretical proof that attacks on concept-based properties are
expected to have a reduced search space compared to simple class falsification,
whilst arguably be more aligned with intuitive robustness targets. As an
outlook to this work in progress we hypothesize that this approach has
potential to efficiently and simultaneously improve logical compliance and
robustness.

</details>


### [3] [Security Analysis and Threat Modeling of Research Management Applications [Extended Version]](https://arxiv.org/abs/2510.03407)
*Boniface M. Sindala,Ragib Hasan*

Main category: cs.CR

TL;DR: Research management applications, such as REDCap, face significant security challenges. This study assesses their vulnerabilities and defenses using frameworks like MITRE ATT&CK and STRIDE, and offers recommendations to enhance security while maintaining usability.


<details>
  <summary>Details</summary>
Motivation: The increasing use of RMAs in clinical research to handle sensitive data necessitates a thorough understanding of their security vulnerabilities to ensure data protection without compromising usability.

Method: The analysis evaluates the architecture, data flow, and security features of RMAs, focusing on REDCap. Potential security risks are identified and assessed using the MITRE ATT&CK framework and STRIDE model.

Result: The study identifies vulnerabilities in RMAs, examines their defenses against common attack vectors, and highlights the critical aspects of security such as confidentiality, integrity, availability, non-repudiation, and authentication.

Conclusion: The research proposes recommendations to enhance the security of RMAs, contributing to a more secure framework for managing sensitive research data in high-risk environments.

Abstract: Research management applications (RMA) are widely used in clinical research
environments to collect, transmit, analyze, and store sensitive data. This data
is so valuable making RMAs susceptible to security threats. This analysis,
analyzes RMAs' security, focusing on Research Electronic Data Capture (REDCap)
as an example. We explore the strengths and vulnerabilities within RMAs by
evaluating the architecture, data flow, and security features. We identify and
assess potential risks using the MITRE ATT\&CK framework and STRIDE model. We
assess REDCap's defenses against common attack vectors focusing on security to
provide confidentiality, integrity, availability, non-repudiation, and
authentication. We conclude by proposing recommendations for enhancing the
security of RMAs, ensuring that critical research data remains protected
without compromising usability. This research aims to contribute towards a more
secure framework for managing sensitive information in research-intensive
environments.

</details>


### [4] [NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn LLM Jailbreaks](https://arxiv.org/abs/2510.03417)
*Javad Rafiei Asl,Sidhant Narula,Mohammad Ghasemigol,Eduardo Blanco,Daniel Takabi*

Main category: cs.CR

TL;DR: NEXUS is a modular framework for LLM multi-turn jailbreak attacks, improving effectiveness via structured semantic networks, active LLM-based refinement, and adaptive query execution.


<details>
  <summary>Details</summary>
Motivation: Existing methods for multi-turn LLM jailbreaks lack systematic exploration, rely on heuristics, and fail to optimize query sequences effectively.

Method: The framework integrates 1) ThoughtNet (semantic network expansion of harmful intent), 2) Simulator (feedback-driven refinement using aligned LLMs), and 3) Network Traverser (adaptive path selection during attacks).

Result: Achieved 2.1-19.4% higher success rates across major LLMs compared to prior art, with scalable, stealthy, and adaptive attack execution.

Conclusion: NEXUS demonstrates a robust, modular approach to multi-turn jailbreaks through semantic modeling, collaborative refinement, and adaptive traversal, outperforming existing methodologies.

Abstract: Large Language Models (LLMs) have revolutionized natural language processing
but remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks
that distribute malicious intent across benign exchanges and bypass alignment
mechanisms. Existing approaches often explore the adversarial space poorly,
rely on hand-crafted heuristics, or lack systematic query refinement. We
present NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular
framework for constructing, refining, and executing optimized multi-turn
attacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a
harmful intent into a structured semantic network of topics, entities, and
query chains; (2) a feedback-driven Simulator that iteratively refines and
prunes these chains through attacker-victim-judge LLM collaboration using
harmfulness and semantic-similarity benchmarks; and (3) a Network Traverser
that adaptively navigates the refined query space for real-time attacks. This
pipeline uncovers stealthy, high-success adversarial paths across LLMs. On
several closed-source and open-source LLMs, NEXUS increases attack success rate
by 2.1% to 19.4% over prior methods. Code: https://github.com/inspire-lab/NEXUS

</details>


### [5] [A Multi-Layer Electronic and Cyber Interference Model for AI-Driven Cruise Missiles: The Case of Khuzestan Province](https://arxiv.org/abs/2510.03542)
*Pouriya Alimoradi,Ali Barati,Hamid Barati*

Main category: cs.CR

TL;DR: This paper proposes a multi-layer interference framework combining electronic warfare, cyberattacks, and deception tactics using deep reinforcement learning to counter AI-guided cruise missiles. Simulations show significant improvements in disrupting missile accuracy compared to baseline methods.


<details>
  <summary>Details</summary>
Motivation: AI-driven cruise missiles with high autonomy and precision pose critical threats to strategic infrastructure, particularly in complex environments like Iran's Khuzestan Province. Current single-layer defense systems are insufficient to counter these advanced threats.

Method: The approach integrates three layers: (1) electronic warfare jamming, (2) cyberattacks on missile navigation systems, and (3) deception tactics using active decoys. The framework employs a deep reinforcement learning coordinator to dynamically select optimal defense strategies in real-time across four simulation scenarios.

Result: Multi-layer interference increased average missile deviation from 0.25 to 8.65 angular units (3300% increase) and reduced target acquisition rate from 92.7% to 31.5% (66% reduction). While requiring 25% more resources than single-layer methods, the system reduces successful strikes by two-thirds.

Conclusion: The multi-layer framework demonstrates superior effectiveness in degrading AI-guided cruise missile performance despite higher resource costs. The adaptive defense coordinator enables real-time optimization of countermeasures, making it a justified solution for protecting strategic infrastructure against advanced autonomous threats.

Abstract: The rapid advancement of Artificial Intelligence has enabled the development
of cruise missiles endowed with high levels of autonomy, adaptability, and
precision. These AI driven missiles integrating deep learning algorithms, real
time data processing, and advanced guidance systems pose critical threats to
strategic infrastructures, especially under complex geographic and climatic
conditions such as those found in Irans Khuzestan Province. In this paper, we
propose a multi layer interference model, encompassing electronic warfare,
cyberattacks, and deception strategies, to degrade the performance of AI guided
cruise missiles significantly. Our experimental results, derived from 400
simulation runs across four distinct scenarios, demonstrate notable
improvements when employing the integrated multi layer approach compared to
single layer or no interference baselines. Specifically, the average missile
deviation from its intended target increases from 0.25 to 8.65 under multi
layer interference a more than 3300 increase in angular deviation. Furthermore,
the target acquisition success rate is reduced from 92.7 in the baseline
scenario to 31.5, indicating a 66 decrease in successful strikes. While
resource consumption for multi layer strategies rises by approximately 25
compared to single layer methods, the significant drop in missile accuracy and
reliability justifies the more intensive deployment of jamming power, cyber
resources, and decoy measures. Beyond these quantitative improvements, the
proposed framework uses a deep reinforcement learning based defense coordinator
to adaptively select the optimal configuration of EW, cyber, and deception
tactics in real time.

</details>


### [6] [PrivacyMotiv: Speculative Persona Journeys for Empathic and Motivating Privacy Reviews in UX Design](https://arxiv.org/abs/2510.03559)
*Zeya Chen,Jianing Wen,Ruth Schmidt,Yaxing Yao,Toby Jia-Jun Li,Tianshi Li*

Main category: cs.CR

TL;DR: PrivacyMotiv is an LLM-powered system that helps UX professionals address privacy concerns through speculative personas and relatable scenarios, which increases empathy, motivation, and perceived usefulness in privacy reviews.


<details>
  <summary>Details</summary>
Motivation: UX professionals often overlook privacy due to limited tools and low motivation. Lack of privacy knowledge, empathy for affected users, and confidence in identifying harms hinders privacy consideration.

Method: PrivacyMotiv generates speculative personas and UX user journeys focused on individuals vulnerable to privacy risks. It uses narrative strategies to create relatable scenarios demonstrating how common design choices can lead to unintended privacy harms.

Result: A within-subjects study (N=16) showed that PrivacyMotiv significantly improved UX practitioners' empathy, intrinsic motivation, and perceived usefulness of privacy reviews compared to their self-proposed methods.

Conclusion: PrivacyMotiv provides a promising approach to privacy review by overcoming motivational barriers in privacy-aware UX design through narrative-based personas and scenarios.

Abstract: UX professionals routinely conduct design reviews, yet privacy concerns are
often overlooked -- not only due to limited tools, but more critically because
of low intrinsic motivation. Limited privacy knowledge, weak empathy for
unexpectedly affected users, and low confidence in identifying harms make it
difficult to address risks. We present PrivacyMotiv, an LLM-powered system that
supports privacy-oriented design diagnosis by generating speculative personas
with UX user journeys centered on individuals vulnerable to privacy risks.
Drawing on narrative strategies, the system constructs relatable and
attention-drawing scenarios that show how ordinary design choices may cause
unintended harms, expanding the scope of privacy reflection in UX. In a
within-subjects study with professional UX practitioners (N=16), we compared
participants' self-proposed methods with PrivacyMotiv across two privacy review
tasks. Results show significant improvements in empathy, intrinsic motivation,
and perceived usefulness. This work contributes a promising privacy review
approach which addresses the motivational barriers in privacy-aware UX.

</details>


### [7] [CryptOracle: A Modular Framework to Characterize Fully Homomorphic Encryption](https://arxiv.org/abs/2510.03565)
*Cory Brynds,Parker McLeod,Lauren Caccamise,Asmita Pal,Dewan Saiham,Sazadur Rahman,Joshua San Miguel,Di Wu*

Main category: cs.CR

TL;DR: This paper analyzes FHE's computational overhead through the OpenFHE library, introducing CryptOracle—a modular evaluation framework to benchmark, profile, and predict performance for CKKS-based AI/ML applications.


<details>
  <summary>Details</summary>
Motivation: FHE's adoption is hindered by its high computational cost (~6x slower than plaintext), necessitating tools to reduce overhead as algorithms evolve rapidly.

Method: CryptOracle integrates (1)a benchmark suite with OpenFHE kernels at multiple abstraction levels, (2)a hardware/security parameter profiler, and (3)a predictive model for runtime/energy estimation.

Result: Achieved runtime prediction error between -7.02%-8.40%, energy error -9.74%-15.67%; framework supports AMD/Intel system metrics and open-source sharing.

Conclusion: CryptOracle provides a collaborative platform to advance FHE applications/optimizations by systematically characterizing performance bottlenecks.

Abstract: Privacy-preserving machine learning has become an important long-term pursuit
in this era of artificial intelligence (AI). Fully Homomorphic Encryption (FHE)
is a uniquely promising solution, offering provable privacy and security
guarantees. Unfortunately, computational cost is impeding its mass adoption.
Modern solutions are up to six orders of magnitude slower than plaintext
execution. Understanding and reducing this overhead is essential to the
advancement of FHE, particularly as the underlying algorithms evolve rapidly.
This paper presents a detailed characterization of OpenFHE, a comprehensive
open-source library for FHE, with a particular focus on the CKKS scheme due to
its significant potential for AI and machine learning applications. We
introduce CryptOracle, a modular evaluation framework comprising (1) a
benchmark suite, (2) a hardware profiler, and (3) a predictive performance
model. The benchmark suite encompasses OpenFHE kernels at three abstraction
levels: workloads, microbenchmarks, and primitives. The profiler is compatible
with standard and user-specified security parameters. CryptOracle monitors
application performance, captures microarchitectural events, and logs power and
energy usage for AMD and Intel systems. These metrics are consumed by a
modeling engine to estimate runtime and energy efficiency across different
configuration scenarios, with error geomean of $-7.02\%\sim8.40\%$ for runtime
and $-9.74\%\sim15.67\%$ for energy. CryptOracle is open source, fully modular,
and serves as a shared platform to facilitate the collaborative advancements of
applications, algorithms, software, and hardware in FHE. The CryptOracle code
can be accessed at https://github.com/UnaryLab/CryptOracle.

</details>


### [8] [PentestMCP: A Toolkit for Agentic Penetration Testing](https://arxiv.org/abs/2510.03610)
*Zachary Ezetta,Wu-chang Feng*

Main category: cs.CR

TL;DR: This paper introduces PentestMCP, which allows agentic applications to reliably interact with common pen testing tools in a server environment. Framework can be modified for new attack surfaces and multiple agents can interact for a coordinated pen test.


<details>
  <summary>Details</summary>
Motivation: This paper introduces a library of Model-Context-Protocol (MCP) server implementations that support agentic penetration testing by enabling developers to create custom multi-agent workflows while automating common penetration testing tasks.

Method: The researchers developed a library based on the Model-Context-Protocol to enable automation and flexible composition of agentic multi-function agents for penetration testing.

Result: PentestMCP supports common tasks like network scanning, resource enumeration, service fingerprinting, vulnerability scanning, exploitation, and post-exploitation, allowing customized penetration testing workflows.

Conclusion: PentestMCP provides a new approach to performing penetration testing via agentic applications supported by the Model-Context-Protocol, enabling more adaptable and scalable testing workflows.

Abstract: Agentic AI is transforming security by automating many tasks being performed
manually. While initial agentic approaches employed a monolithic architecture,
the Model-Context-Protocol has now enabled a remote-procedure call (RPC)
paradigm to agentic applications, allowing for the flexible construction and
composition of multi-function agents. This paper describes PentestMCP, a
library of MCP server implementations that support agentic penetration testing.
By supporting common penetration testing tasks such as network scanning,
resource enumeration, service fingerprinting, vulnerability scanning,
exploitation, and post-exploitation, PentestMCP allows a developer to customize
multi-agent workflows for performing penetration tests.

</details>


### [9] [Explainable but Vulnerable: Adversarial Attacks on XAI Explanation in Cybersecurity Applications](https://arxiv.org/abs/2510.03623)
*Maraz Mia,Mir Mehedi A. Pritom*

Main category: cs.CR

TL;DR: This paper analyzes adversarial attacks on XAI methods, highlighting real-world impacts in cybersecurity and the need for improved resiliency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the vulnerability of XAI methods to post-adversarial attacks that manipulate model decisions by altering explanations.

Method: The authors explored six attack procedures on post-hoc XAI methods like SHAP, LIME, and IG, testing them in cybersecurity scenarios including phishing, malware, intrusion, and fraudulent website detection.

Result: Experiments demonstrated that these attacks are effective in real-world cybersecurity contexts, exposing weaknesses in current XAI techniques.

Conclusion: The conclusion emphasizes the urgency of enhancing XAI resilience to prevent exploitation by adversarial TTPs in critical domains like cybersecurity.

Abstract: Explainable Artificial Intelligence (XAI) has aided machine learning (ML)
researchers with the power of scrutinizing the decisions of the black-box
models. XAI methods enable looking deep inside the models' behavior, eventually
generating explanations along with a perceived trust and transparency. However,
depending on any specific XAI method, the level of trust can vary. It is
evident that XAI methods can themselves be a victim of post-adversarial attacks
that manipulate the expected outcome from the explanation module. Among such
attack tactics, fairwashing explanation (FE), manipulation explanation (ME),
and backdoor-enabled manipulation attacks (BD) are the notable ones. In this
paper, we try to understand these adversarial attack techniques, tactics, and
procedures (TTPs) on explanation alteration and thus the effect on the model's
decisions. We have explored a total of six different individual attack
procedures on post-hoc explanation methods such as SHAP (SHapley Additive
exPlanations), LIME (Local Interpretable Model-agnostic Explanation), and IG
(Integrated Gradients), and investigated those adversarial attacks in
cybersecurity applications scenarios such as phishing, malware, intrusion, and
fraudulent website detection. Our experimental study reveals the actual
effectiveness of these attacks, thus providing an urgency for immediate
attention to enhance the resiliency of XAI methods and their applications.

</details>


### [10] [On the Limits of Consensus under Dynamic Availability and Reconfiguration](https://arxiv.org/abs/2510.03625)
*Joachim Neu,Javier Nieto,Ling Ren*

Main category: cs.CR

TL;DR: This paper addresses consensus challenges in proof-of-stake blockchains under Dynamic Availability and Reconfiguration (DAR) settings, proposing a realistic adversarial condition and a simple bootstrap gadget to enable secure reconfiguration without prior assumptions.


<details>
  <summary>Details</summary>
Motivation: Existing DAR protocols (e.g., Ethereum, Ouroboros, Snow White) rely on unrealistic assumptions like social consensus or continuous key evolution during node inactivity. Current solutions fail to provide practical security guarantees for dynamic node participation.

Method: 1) Derive necessary/sufficient adversarial conditions for DAR consensus security without additional assumptions
2 ) Introduce a realistic assumption: Honest nodes discard keys immediately when exiting
3) Design a lightweight bootstrap gadget for reconfiguration in optimistic cases

Result: Established formal security boundaries for DAR consensus, demonstrated gadget efficiency in common cases with low reconfiguration frequency and without double-spending attempts.

Conclusion: The proposed adversarial model and bootstrap mechanism provide a practical foundation for secure, dynamically available blockchain protocols without requiring unrealistic coordination or assumptions beyond standard cryptography.

Abstract: Proof-of-stake blockchains require consensus protocols that support Dynamic
Availability and Reconfiguration (so-called DAR setting), where the former
means that the consensus protocol should remain live even if a large number of
nodes temporarily crash, and the latter means it should be possible to change
the set of operating nodes over time. State-of-the-art protocols for the DAR
setting, such as Ethereum, Cardano's Ouroboros, or Snow White, require
unrealistic additional assumptions, such as social consensus, or that key
evolution is performed even while nodes are not participating. In this paper,
we identify the necessary and sufficient adversarial condition under which
consensus can be achieved in the DAR setting without additional assumptions. We
then introduce a new and realistic additional assumption: honest nodes dispose
of their cryptographic keys the moment they express intent to exit from the set
of operating nodes. To add reconfiguration to any dynamically available
consensus protocol, we provide a bootstrapping gadget that is particularly
simple and efficient in the common optimistic case of few reconfigurations and
no double-spending attempts.

</details>


### [11] [QPADL: Post-Quantum Private Spectrum Access with Verified Location and DoS Resilience](https://arxiv.org/abs/2510.03631)
*Saleh Darzi,Saif Eddine Nouma,Kiarash Sedghighadikolaei,Attila Altay*

Main category: cs.CR

TL;DR: This paper proposes QPADL, a post-quantum secure framework for Spectrum Access Systems (SASs) addressing privacy, anonymity, DoS resilience, and scalability challenges through tailored cryptographic methods.


<details>
  <summary>Details</summary>
Motivation: Current SASs face severe security risks from privacy exposure (location/transmission data disclosure), DoS attacks, spoofing vulnerabilities, and emerging quantum threats. Existing solutions lack comprehensive post-quantum security guarantees.

Method: QPADL combines SAS-specific private information retrieval for location privacy, a quantum-resistant Tor-like anonymity network, cryptographic signatures for location verification, and client puzzles/rate-limiting for DoS defense. GPU optimization enhances scalability.

Result: Security analysis confirms robustness against quantum threats and adversarial scenarios. Performance evaluations with GPU acceleration demonstrate practicality for large-scale deployments.

Conclusion: QPADL is the first post-quantum framework to simultaneously achieve privacy, anonymity, location verification, and DoS resilience in SASs while maintaining efficiency, addressing critical gaps in 5G/6G spectrum management security.

Abstract: With advances in wireless communication and growing spectrum scarcity,
Spectrum Access Systems (SASs) offer an opportunistic solution but face
significant security challenges. Regulations require disclosure of location
coordinates and transmission details, exposing user privacy and anonymity
during spectrum queries, while the database operations themselves permit
Denial-of-Service (DoS) attacks. As location-based services, SAS is also
vulnerable to compromised or malicious users conducting spoofing attacks. These
threats are further amplified given the quantum computing advancements. Thus,
we propose QPADL, the first post-quantum (PQ) secure framework that
simultaneously ensures privacy, anonymity, location verification, and DoS
resilience while maintaining efficiency for large-scale spectrum access
systems. QPADL introduces SAS-tailored private information retrieval for
location privacy, a PQ-variant of Tor for anonymity, and employs advanced
signature constructions for location verification alongside client puzzle
protocols and rate-limiting technique for DoS defense. We formally assess its
security and conduct a comprehensive performance evaluation, incorporating GPU
parallelization and optimization strategies to demonstrate practicality and
scalability.

</details>


### [12] [A Time-Bound Signature Scheme for Blockchains](https://arxiv.org/abs/2510.03697)
*Benjamin Marsh,Paolo Serafino*

Main category: cs.CR

TL;DR: The paper presents a modified Schnorr signature scheme to enable time-bound signatures for blockchain transaction fee auction and smart contracts, using the immutable blockchain as universal time. It shows that such signatures can reduce MEV (Maximal Extractable Value) revenue for builders when applied to EIP-1559.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the issues of transaction fee auctions and MEV (Maximal Extractable Value) in blockchain systems, especially in the context of Ethereum's EIP-1559. By introducing a time-bound element to Schnorr signatures, the authors seek to limit the time during which a signature is valid, thereby ensuring that honest producers can only validate it before a specific block height. This is intended to reduce the potential for MEV by preventing builders from profiting from early access to transactions.

Method: The authors propose a modified Schnorr signature scheme that incorporates a block height as a time parameter. In this scheme, a signature is only valid up to a certain block height, after which it can no longer be used. This is achieved by integrating the block height directly into the signature generation and verification process. The paper demonstrates how this modification can be applied to EIP-1559, ensuring the signature is time-bound and honest block producers are incentivized to validate transactions before a specified height.

Result: Through one application example, the paper shows that the modified Schnorr signature scheme can lower the MEV revenue for block builders in the EIP-1559 context. The time-bound nature of the signatures limits the ability of builders to profit from transaction reordering or inclusion, thus mitigating the impact of MEV on the predicted equilibrium strategies of participants.

Conclusion: The modified Schnorr signature scheme using time-bound functionality is effective in reducing MEV revenues in blockchain-based transaction fee auctions and may have useful applications in complex smart contracts to prevent time-based manipulation. It is suggested that the approach could be applied to improve fairness and economic incentives in various blockchain protocols.

Abstract: We introduce a modified Schnorr signature scheme to allow for time-bound
signatures for transaction fee auction bidding and smart contract purposes in a
blockchain context, ensuring an honest producer can only validate a signature
before a given block height. The immutable blockchain is used as a source of
universal time for the signature scheme. We show the use of such a signature
scheme leads to lower MEV revenue for builders. We then apply our time-bound
signatures to Ethereum's EIP-1559 and show how it can be used to mitigate the
effect of MEV on predicted equilibrium strategies.

</details>


### [13] [Backdoor-Powered Prompt Injection Attacks Nullify Defense Methods](https://arxiv.org/abs/2510.03705)
*Yulin Chen,Haoran Li,Yuan Sui,Yangqiu Song,Bryan Hooi*

Main category: cs.CR

TL;DR: This paper introduces backdoor-powered prompt injection attacks, which exploit poisoned training data to bypass existing defense methods, including instruction hierarchy techniques, making them more harmful than previous attacks.


<details>
  <summary>Details</summary>
Motivation: Existing prompt injection defenses using instruction hierarchies are vulnerable to advanced attacks that nullify their effectiveness, necessitating exploration of more sophisticated threats.

Method: Attackers poison supervised fine-tuning samples by inserting backdoors. When activated, the backdoored model executes attacker-injected instructions surrounded by triggers.

Result: Experiments demonstrate that these attacks outperform previous methods in circumventing defenses, including instruction hierarchy techniques, with a constructed benchmark for evaluation.

Conclusion: Backdoor-powered attacks pose a critical threat by defeating current defenses, highlighting the need for robust mitigation strategies beyond instruction hierarchy approaches.

Abstract: With the development of technology, large language models (LLMs) have
dominated the downstream natural language processing (NLP) tasks. However,
because of the LLMs' instruction-following abilities and inability to
distinguish the instructions in the data content, such as web pages from search
engines, the LLMs are vulnerable to prompt injection attacks. These attacks
trick the LLMs into deviating from the original input instruction and executing
the attackers' target instruction. Recently, various instruction hierarchy
defense strategies are proposed to effectively defend against prompt injection
attacks via fine-tuning. In this paper, we explore more vicious attacks that
nullify the prompt injection defense methods, even the instruction hierarchy:
backdoor-powered prompt injection attacks, where the attackers utilize the
backdoor attack for prompt injection attack purposes. Specifically, the
attackers poison the supervised fine-tuning samples and insert the backdoor
into the model. Once the trigger is activated, the backdoored model executes
the injected instruction surrounded by the trigger. We construct a benchmark
for comprehensive evaluation. Our experiments demonstrate that backdoor-powered
prompt injection attacks are more harmful than previous prompt injection
attacks, nullifying existing prompt injection defense methods, even the
instruction hierarchy techniques.

</details>


### [14] [Shrinking the Kernel Attack Surface Through Static and Dynamic Syscall Limitation](https://arxiv.org/abs/2510.03720)
*Dongyang Zhan,Zhaofeng Yu,Xiangzhan Yu,Hongli Zhang,Lin Ye*

Main category: cs.CR

TL;DR: The paper introduces sysverify, a systematic approach combining static analysis and dynamic verification to accurately identify and block unused syscalls in Linux containers, significantly shrinking the kernel attack surface beyond current tools like Docker's default 50 syscall limit.


<details>
  <summary>Details</summary>
Motivation: Modern container security is hindered by incomplete syscall whitelisting - dynamic tracking misses syscalls and static analysis generates excessive false positives, leaving systems vulnerable to attacks through unblocked syscalls.

Method: Sysverify bridges the binary-syscall semantic gap through joint analysis of binary executables and source code to create precise API-syscall mappings, complemented by dynamic verification of potentially dangerous syscalls with low overhead analysis of indirect/rare syscalls.

Result: Sysverify achieves comprehensive syscall analysis with reduced false positives, enabling effective blocking of non-essential syscalls and demonstrating significant attack surface reduction compared to default container security implementations.

Conclusion: The integrated static-dynamic analysis framework provides a scalable solution for developer-free syscall whitelisting, setting a new standard for production system hardening through systematic dependency verification.

Abstract: Linux Seccomp is widely used by the program developers and the system
maintainers to secure the operating systems, which can block unused syscalls
for different applications and containers to shrink the attack surface of the
operating systems. However, it is difficult to configure the whitelist of a
container or application without the help of program developers. Docker
containers block about only 50 syscalls by default, and lots of unblocked
useless syscalls introduce a big kernel attack surface. To obtain the dependent
syscalls, dynamic tracking is a straight-forward approach but it cannot get the
full syscall list. Static analysis can construct an over-approximated syscall
list, but the list contains many false positives. In this paper, a systematic
dependent syscall analysis approach, sysverify, is proposed by combining static
analysis and dynamic verification together to shrink the kernel attack surface.
The semantic gap between the binary executables and syscalls is bridged by
analyzing the binary and the source code, which builds the mapping between the
library APIs and syscalls systematically. To further reduce the attack surface
at best effort, we propose a dynamic verification approach to intercept and
analyze the security of the invocations of indirect-call-related or rarely
invoked syscalls with low overhead.

</details>


### [15] [Securing Operating Systems Through Fine-grained Kernel Access Limitation for IoT Systems](https://arxiv.org/abs/2510.03737)
*Dongyang Zhan,Zhaofeng Yu,Xiangzhan Yu,Hongli Zhang,Lin Ye,Likun Liu*

Main category: cs.CR

TL;DR: This paper proposes a new static analysis approach to generate fine-grained Seccomp profiles for embedded IoT applications, improving security by analyzing dependent syscalls and their arguments without developer intervention.


<details>
  <summary>Details</summary>
Motivation: Securing low-overhead embedded systems is critical in the growing IoT landscape. Linux Seccomp is often used but lacks systematic, fine-grained configuration methods not requiring developer input. Existing methods are too coarse to limit syscall arguments effectively.

Method: The authors analyze the control flow graphs and data dependency relationships within dynamic libraries to map their APIs to corresponding syscalls and their arguments, enabling a novel static dependent syscall analysis approach.

Result: A fine-grained Seccomp configuration methodology for embedded applications was developed, capturing all possible dependent syscalls and their arguments, which allows for more precise kernel access restrictions in IoT systems.

Conclusion: This work introduces the first approach for automatically generating fine-grained Seccomp profiles for embedded applications, offering improved security and reducing reliance on developers for configuration.

Abstract: With the development of Internet of Things (IoT), it is gaining a lot of
attention. It is important to secure the embedded systems with low overhead.
The Linux Seccomp is widely used by developers to secure the kernels by
blocking the access of unused syscalls, which introduces less overhead.
However, there are no systematic Seccomp configuration approaches for IoT
applications without the help of developers. In addition, the existing Seccomp
configuration approaches are coarse-grained, which cannot analyze and limit the
syscall arguments. In this paper, a novel static dependent syscall analysis
approach for embedded applications is proposed, which can obtain all of the
possible dependent syscalls and the corresponding arguments of the target
applications. So, a fine-grained kernel access limitation can be performed for
the IoT applications. To this end, the mappings between dynamic library APIs
and syscalls according with their arguments are built, by analyzing the control
flow graphs and the data dependency relationships of the dynamic libraries. To
the best of our knowledge, this is the first work to generate the fine-grained
Seccomp profile for embedded applications.

</details>


### [16] [Public-Key Encryption from the MinRank Problem](https://arxiv.org/abs/2510.03752)
*Rohit Chatterjee,Changrui Mu,Prashant Nalini Vasudevan*

Main category: cs.CR

TL;DR: The paper presents a public-key encryption scheme based on the MinRank problem over uniformly random instances using a new rank-metric code duality concept.


<details>
  <summary>Details</summary>
Motivation: There is a need for public-key encryption based on the hardness of decoding random linear rank-metric codes.

Method: Development of a new notion of duality for rank-metric codes and constructing a public-key encryption scheme using the hardness of the (planted) MinRank problem over uniformly random instances.

Result: A public-key encryption scheme constructed without relying on structured instances, which is a novel approach compared to existing methods.

Conclusion: The paper advances the field by providing a new PKC construction from the MinRank problem, using duality concepts to avoid traditional structured instance requirements.

Abstract: We construct a public-key encryption scheme from the hardness of the
(planted) MinRank problem over uniformly random instances. This corresponds to
the hardness of decoding random linear rank-metric codes. Existing
constructions of public-key encryption from such problems require hardness for
structured instances arising from the masking of efficiently decodable codes.
Central to our construction is the development of a new notion of duality for
rank-metric codes.

</details>


### [17] [You Have Been LaTeXpOsEd: A Systematic Analysis of Information Leakage in Preprint Archives Using Large Language Models](https://arxiv.org/abs/2510.03761)
*Richard A. Dubniczky,Bertalan Borsos,Tihanyi Norbert*

Main category: cs.CR

TL;DR: This paper reveals significant security risks in arXiv preprint submissions by analyzing 100,000 source materials. It introduces LaTeXpOsEd, a framework combining pattern matching and LLMs to detect 1.2TB of sensitive data leaks including PII, API keys, and confidential communications. A novel benchmark (LLMSec-DB), tools, and scripts are released to address repository security gaps.


<details>
  <summary>Details</summary>
Motivation: Public access to preprint source materials creates critical security vulnerabilities by exposing unfiltered sensitive information. Existing systems lack systematic auditing of non-referenced files and comments in scientific submissions.

Method: LaTeXpOsEd employs a four-stage pipeline integrating regular expression pattern matching, logical filtering, traditional file harvesting techniques, and 25 state-of-the-art LLMs for anomaly detection. A dedicated benchmark (LLMSec-DB) evaluates model performance across secret-detection tasks.

Result: Thousands of security incidents detected, including GPS-tagged EXIF files, exposed API credentials, editable SharePoint links, and private author correspondences. The methodology successfully uncovered vulnerabilities that could be exploited for open-source intelligence harvesting.

Conclusion: The study demonstrates urgent need for repository-level sanitization protocols. Open-science contributions include the full research toolkit while sensitive findings are withheld following ethical guidelines. The authors advocate combined technical countermeasures and policy changes to secure academic publishing workflows.

Abstract: The widespread use of preprint repositories such as arXiv has accelerated the
communication of scientific results but also introduced overlooked security
risks. Beyond PDFs, these platforms provide unrestricted access to original
source materials, including LaTeX sources, auxiliary code, figures, and
embedded comments. In the absence of sanitization, submissions may disclose
sensitive information that adversaries can harvest using open-source
intelligence. In this work, we present the first large-scale security audit of
preprint archives, analyzing more than 1.2 TB of source data from 100,000 arXiv
submissions. We introduce LaTeXpOsEd, a four-stage framework that integrates
pattern matching, logical filtering, traditional harvesting techniques, and
large language models (LLMs) to uncover hidden disclosures within
non-referenced files and LaTeX comments. To evaluate LLMs' secret-detection
capabilities, we introduce LLMSec-DB, a benchmark on which we tested 25
state-of-the-art models. Our analysis uncovered thousands of PII leaks,
GPS-tagged EXIF files, publicly available Google Drive and Dropbox folders,
editable private SharePoint links, exposed GitHub and Google credentials, and
cloud API keys. We also uncovered confidential author communications, internal
disagreements, and conference submission credentials, exposing information that
poses serious reputational risks to both researchers and institutions. We urge
the research community and repository operators to take immediate action to
close these hidden security gaps. To support open science, we release all
scripts and methods from this study but withhold sensitive findings that could
be misused, in line with ethical principles. The source code and related
material are available at the project website https://github.com/LaTeXpOsEd

</details>


### [18] [Complex Domain Approach for Reversible Data Hiding and Homomorphic Encryption: General Framework and Application to Dispersed Data](https://arxiv.org/abs/2510.03770)
*David Megias*

Main category: cs.CR

TL;DR: This paper proposes H[i]dden, a complex-number-based framework for reversible data hiding and encryption that enables perfect reversibility, unlimited watermark size, and intrinsic data-watermark mixing, along with two protocols (H[i]dden-EG and H[i]dden-AggP) for secure data handling in distributed environments.


<details>
  <summary>Details</summary>
Motivation: Existing Reversible Data Hiding (RDH) methods for scalar data face limitations in embedding capacity and data-watermark mixing, creating vulnerabilities in resource-constrained environments like IoT and Wireless Sensor Networks.

Method: The framework leverages complex number arithmetic for simultaneous information embedding and encryption. It introduces: (1)...

Result: The framework achieves perfect reversibility, supports in-principle unlimited watermark sizes, and enables privacy-preserving aggregation of watermarked data. Protocols are validated as efficient and resilient solutions for ensuring data integrity, provenance, and confidentiality.

Conclusion: H[i]dden establishes a novel foundation for secure data handling in distributed systems by exploiting complex domain algebraic properties, with the potential to inspire future cryptographic and data protection schemes.

Abstract: Ensuring the trustworthiness of data from distributed and
resource-constrained environments, such as Wireless Sensor Networks or IoT
devices, is critical. Existing Reversible Data Hiding (RDH) methods for scalar
data suffer from low embedding capacity and poor intrinsic mixing between host
data and watermark. This paper introduces Hiding in the Imaginary Domain with
Data Encryption (H[i]dden), a novel framework based on complex number
arithmetic for simultaneous information embedding and encryption. The H[i]dden
framework offers perfect reversibility, in-principle unlimited watermark size,
and intrinsic data-watermark mixing. The paper further introduces two
protocols: H[i]dden-EG, for joint reversible data hiding and encryption, and
H[i]dden-AggP, for privacy-preserving aggregation of watermarked data, based on
partially homomorphic encryption. These protocols provide efficient and
resilient solutions for data integrity, provenance and confidentiality, serving
as a foundation for new schemes based on the algebraic properties of the
complex domain.

</details>


### [19] [Security Analysis of Ponzi Schemes in Ethereum Smart Contracts](https://arxiv.org/abs/2510.03819)
*Chunyi Zhang,Qinghong Wei,Xiaoqi Li*

Main category: cs.CR

TL;DR: This paper addresses Ethereum Ponzi scheme detection by categorizing scams into four structural types, analyzing contract source code via Mythril tool, and using shell scripts for batch detection of vulnerabilities in open-source smart contracts.


<details>
  <summary>Details</summary>
Motivation: The proliferation of blockchain fraud through Ethereum smart contracts has caused significant investor losses, yet effective detection methods for embedded Ponzi schemes remain lacking.

Method: The paper 1. Classifies Ponzi contracts into four structural categories, 2. Performs static/dynamic analysis using Mythril on representative cases, 3. Employs shell scripts and command patterns for batch analysis of open-source code to identify common characteristics.

Result: Revealed intrinsic code characteristics of Ponzi schemes, exposed vulnerabilities and operational mechanisms through case analysis, and developed a batch detection framework that identifies common patterns across scams.

Conclusion: The study establishes a systematic analysis approach for blockchain fraud by combining code categorization, program analysis, and automated detection techniques, enabling proactive identification of Ponzi scheme smart contracts.

Abstract: The rapid advancement of blockchain technology has precipitated the
widespread adoption of Ethereum and smart contracts across a variety of
sectors. However, this has also given rise to numerous fraudulent activities,
with many speculators embedding Ponzi schemes within smart contracts, resulting
in significant financial losses for investors. Currently, there is a lack of
effective methods for identifying and analyzing such new types of fraudulent
activities. This paper categorizes these scams into four structural types and
explores the intrinsic characteristics of Ponzi scheme contract source code
from a program analysis perspective. The Mythril tool is employed to conduct
static and dynamic analyses of representative cases, thereby revealing their
vulnerabilities and operational mechanisms. Furthermore, this paper employs
shell scripts and command patterns to conduct batch detection of open-source
smart contract code, thereby unveiling the common characteristics of Ponzi
scheme smart contracts.

</details>


### [20] [Pilot Contamination Attacks Detection with Machine Learning for Multi-User Massive MIMO](https://arxiv.org/abs/2510.03831)
*Pedro Ivo da Cruz,Dimitri Silva,Tito Spadini,Ricardo Suyama,Murilo Bellezoni Loiola*

Main category: cs.CR

TL;DR: This paper proposes using Decision Trees (DT) at the base station to detect pilot contamination attacks (PCA) in massive MIMO systems, demonstrating that a simple DT with one level of depth outperforms traditional likelihood ratio testing (LRT), particularly in noisy or low-power attack scenarios, without requiring prior knowledge of noise or malicious signal power.


<details>
  <summary>Details</summary>
Motivation: Massive MIMO systems are critical for 5G/6G but are vulnerable to PCA, where malicious users mimic pilot signals, degrading channel estimation. Current methods like LRT have limitations, including dependence on prior knowledge of noise and signal parameters, necessitating a more robust solution.

Method: The authors develop a methodology to generate training data for a DT classifier and select the optimal tree depth. They simulate various practical scenarios and benchmark the DT against LRT, analyzing detection probabilities under noise and low-power attacks.

Result: A single-level DT outperforms LRT in PCA detection, particularly in noisy or low-power malicious transmission conditions where LRT fails. The DT’s superior performance is attributed to its ability to learn an effective decision threshold without requiring assumptions about noise or signal power.

Conclusion: Decision Trees offer a practical, parameter-free alternative to traditional hypothesis testing methods like LRT for PCA detection. Their robustness in adverse conditions and independence from prior knowledge make them suitable for real-world massive MIMO systems.

Abstract: Massive multiple-input multiple-output (MMIMO) is essential to modern
wireless communication systems, like 5G and 6G, but it is vulnerable to active
eavesdropping attacks. One type of such attack is the pilot contamination
attack (PCA), where a malicious user copies pilot signals from an authentic
user during uplink, intentionally interfering with the base station's (BS)
channel estimation accuracy. In this work, we propose to use a Decision Tree
(DT) algorithm for PCA detection at the BS in a multi-user system. We present a
methodology to generate training data for the DT classifier and select the best
DT according to their depth. Then, we simulate different scenarios that could
be encountered in practice and compare the DT to a classical technique based on
likelihood ratio testing (LRT) submitted to the same scenarios. The results
revealed that a DT with only one level of depth is sufficient to outperform the
LRT. The DT shows a good performance regarding the probability of detection in
noisy scenarios and when the malicious user transmits with low power, in which
case the LRT fails to detect the PCA. We also show that the reason for the good
performance of the DT is its ability to compute a threshold that separates PCA
data from non-PCA data better than the LRT's threshold. Moreover, the DT does
not necessitate prior knowledge of noise power or assumptions regarding the
signal power of malicious users, prerequisites typically essential for LRT and
other hypothesis testing methodologies.

</details>


### [21] [Quantifying Distributional Robustness of Agentic Tool-Selection](https://arxiv.org/abs/2510.03992)
*Jehyeok Yeon,Isha Chaudhary,Gagandeep Singh*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) are increasingly deployed in agentic systems
where they map user intents to relevant external tools to fulfill a task. A
critical step in this process is tool selection, where a retriever first
surfaces candidate tools from a larger pool, after which the LLM selects the
most appropriate one. This pipeline presents an underexplored attack surface
where errors in selection can lead to severe outcomes like unauthorized data
access or denial of service, all without modifying the agent's model or code.
While existing evaluations measure task performance in benign settings, they
overlook the specific vulnerabilities of the tool selection mechanism under
adversarial conditions. To address this gap, we introduce ToolCert, the first
statistical framework that formally certifies tool selection robustness.
ToolCert models tool selection as a Bernoulli success process and evaluates it
against a strong, adaptive attacker who introduces adversarial tools with
misleading metadata, and are iteratively refined based on the agent's previous
choices. By sampling these adversarial interactions, ToolCert produces a
high-confidence lower bound on accuracy, formally quantifying the agent's
worst-case performance. Our evaluation with ToolCert uncovers the severe
fragility: under attacks injecting deceptive tools or saturating retrieval, the
certified accuracy bound drops near zero, an average performance drop of over
60% compared to non-adversarial settings. For attacks targeting the retrieval
and selection stages, the certified accuracy bound plummets to less than 20%
after just a single round of adversarial adaptation. ToolCert thus reveals
previously unexamined security threats inherent to tool selection and provides
a principled method to quantify an agent's robustness to such threats, a
necessary step for the safe deployment of agentic systems.

</details>


### [22] [PrivSpike: Employing Homomorphic Encryption for Private Inference of Deep Spiking Neural Networks](https://arxiv.org/abs/2510.03995)
*Nges Brian Njungle,Eric Jahns,Milan Stojkov,Michel A. Kinsy*

Main category: cs.CR

TL;DR: PRIVSPIKE is a privacy-preserving inference framework for Spiking Neural Networks (SNNs), using CKKS homomorphic encryption to enable encrypted data processing. It introduces two algorithms for the Leaky Integrate-and-Fire activation function, achieving high accuracy and efficiency on datasets like MNIST and CIFAR-10.


<details>
  <summary>Details</summary>
Motivation: Spiking Neural Networks (SNNs), despite their energy efficiency, inherit privacy risks due to data sensitivity. Traditional solutions fail to balance cryptographic security and performance, necessitating a framework that preserves privacy without sacrificing SNN efficiency.

Method: PRIVSPIKE employs CKKS homomorphic encryption for secure SNN inference. It proposes (1)a polynomial approximation algorithm for high-performance inference and (2)a scheme-switching algorithm for precision-optimized computations. These algorithms are validated on LeNet-5 and ResNet-19 architectures.

Result: PRIVSPIKE achieves 98.10% accuracy on MNIST and 66.0% on CIFAR-10 DVS with ResNet-19, with encrypted inference times of 28s (MNIST LeNet-5) to 1846s (CIFAR-10 DVS ResNet-19). These results outperform prior encrypted SNN solutions.

Conclusion: PRIVSPIKE bridges the gap between energy-efficient SNNs and strong privacy guarantees via homomorphic encryption. It demonstrates practical secure inference for SNNs, setting a new benchmark for encrypted deep learning.

Abstract: Deep learning has become a cornerstone of modern machine learning. It relies
heavily on vast datasets and significant computational resources for high
performance. This data often contains sensitive information, making privacy a
major concern in deep learning. Spiking Neural Networks (SNNs) have emerged as
an energy-efficient alternative to conventional deep learning approaches.
Nevertheless, SNNs still depend on large volumes of data, inheriting all the
privacy challenges of deep learning. Homomorphic encryption addresses this
challenge by allowing computations to be performed on encrypted data, ensuring
data confidentiality throughout the entire processing pipeline. In this paper,
we introduce PRIVSPIKE, a privacy-preserving inference framework for SNNs using
the CKKS homomorphic encryption scheme. PRIVSPIKE supports arbitrary depth SNNs
and introduces two key algorithms for evaluating the Leaky Integrate-and-Fire
activation function: (1) a polynomial approximation algorithm designed for
high-performance SNN inference, and (2) a novel scheme-switching algorithm that
optimizes precision at a higher computational cost. We evaluate PRIVSPIKE on
MNIST, CIFAR-10, Neuromorphic MNIST, and CIFAR-10 DVS using models from LeNet-5
and ResNet-19 architectures, achieving encrypted inference accuracies of
98.10%, 79.3%, 98.1%, and 66.0%, respectively. On a consumer-grade CPU, SNN
LeNet-5 models achieved inference times of 28 seconds on MNIST and 212 seconds
on Neuromorphic MNIST. For SNN ResNet-19 models, inference took 784 seconds on
CIFAR-10 and 1846 seconds on CIFAR-10 DVS. These results establish PRIVSPIKE as
a viable and efficient solution for secure SNN inference, bridging the gap
between energy-efficient deep neural networks and strong cryptographic privacy
guarantees while outperforming prior encrypted SNN solutions.

</details>


### [23] [FHEON: A Configurable Framework for Developing Privacy-Preserving Neural Networks Using Homomorphic Encryption](https://arxiv.org/abs/2510.03996)
*Nges Brian Njungle,Eric Jahns,Michel A. Kinsy*

Main category: cs.CR

TL;DR: FHEON is a configurable framework for developing privacy-preserving CNN models using Homomorphic Encryption (HE). It supports standard CNN architectures and achieves near-plaintext accuracy with 13-403s latency on consumer-grade CPUs.


<details>
  <summary>Details</summary>
Motivation: ML-as-a-Service faces security risks from data confidentiality breaches. Existing HE solutions for neural networks lack flexibility and generality, necessitating a framework akin to standard ML development for HE-friendly models.

Method: FHEON implements HE-optimized CNN layers (convolutional, pooling, ReLU, fully connected). Architecture is configured via parameters like channel counts, kernel sizes, and padding. Evaluations use LeNet-5, VGG, ResNet benchmarks.

Result: LeNet-5 achieves 98.5%M318F accuracy (13s latency) on MNIST; ResNet-20 reaches 92.2%M30; encfigssemble Conversely 403s on CIFAR-10. All models maintain <1%% accuracy loss. Memory usage stays under 42.3GB for VGG-16.

Conclusion: FHEON closes the gap in HE-based CNN development by offering both flexibility and practical performance metrics, proving privacy-preserving deep learning is achievable without significant accuracy loss.

Abstract: The widespread adoption of Machine Learning as a Service raises critical
privacy and security concerns, particularly about data confidentiality and
trust in both cloud providers and the machine learning models. Homomorphic
Encryption (HE) has emerged as a promising solution to this problems, allowing
computations on encrypted data without decryption. Despite its potential,
existing approaches to integrate HE into neural networks are often limited to
specific architectures, leaving a wide gap in providing a framework for easy
development of HE-friendly privacy-preserving neural network models similar to
what we have in the broader field of machine learning. In this paper, we
present FHEON, a configurable framework for developing privacy-preserving
convolutional neural network (CNN) models for inference using HE. FHEON
introduces optimized and configurable implementations of privacy-preserving CNN
layers including convolutional layers, average pooling layers, ReLU activation
functions, and fully connected layers. These layers are configured using
parameters like input channels, output channels, kernel size, stride, and
padding to support arbitrary CNN architectures. We assess the performance of
FHEON using several CNN architectures, including LeNet-5, VGG-11, VGG- 16,
ResNet-20, and ResNet-34. FHEON maintains encrypted-domain accuracies within
+/- 1% of their plaintext counterparts for ResNet-20 and LeNet-5 models.
Notably, on a consumer-grade CPU, the models build on FHEON achieved 98.5%
accuracy with a latency of 13 seconds on MNIST using LeNet-5, and 92.2%
accuracy with a latency of 403 seconds on CIFAR-10 using ResNet-20.
Additionally, FHEON operates within a practical memory budget requiring not
more than 42.3 GB for VGG-16.

</details>


### [24] [Real-VulLLM: An LLM Based Assessment Framework in the Wild](https://arxiv.org/abs/2510.04056)
*Rijha Safdar,Danyail Mateen,Syed Taha Ali,Wajahat Hussain*

Main category: cs.CR

TL;DR: This paper is about improving the ability of large language models to detect vulnerabilities in software in real-world scenarios by using various prompt designs, a real-world vector data store for context, and a scoring measure that combines accuracy and reasoning quality.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs have shown progress in software engineering, but there is a need to explore their effectiveness and efficiency for vulnerability detection scenarios, particularly in real-world conditions.

Method: The paper introduces three main contributions: (i) prompt designs for vulnerability detection and reasoning, (ii) a real-world vector data store built from the National Vulnerability Database to provide real-time context, and (iii) a scoring method that measures both accuracy and reasoning quality in detecting vulnerabilities.

Result: The results of applying the proposed methods for vulnerability detection using LLMs are not detailed in the abstract but are expected to demonstrate improved performance through effective prompting, contextual data, and combined scoring of accuracy and reasoning.

Conclusion: The paper's conclusion emphasizes the examination of the readiness of LLMs for deployment in real-world vulnerability detection settings, providing a framework to reliably leverage LLMs for secure software development.

Abstract: Artificial Intelligence (AI) and more specifically Large Language Models
(LLMs) have demonstrated exceptional progress in multiple areas including
software engineering, however, their capability for vulnerability detection in
the wild scenario and its corresponding reasoning remains underexplored.
Prompting pre-trained LLMs in an effective way offers a computationally
effective and scalable solution. Our contributions are (i)varied prompt designs
for vulnerability detection and its corresponding reasoning in the wild. (ii)a
real-world vector data store constructed from the National Vulnerability
Database, that will provide real time context to vulnerability detection
framework, and (iii)a scoring measure for combined measurement of accuracy and
reasoning quality. Our contribution aims to examine whether LLMs are ready for
wild deployment, thus enabling the reliable use of LLMs stronger for the
development of secure software's.

</details>


### [25] [Gluing Random Unitaries with Inverses and Applications to Strong Pseudorandom Unitaries](https://arxiv.org/abs/2510.04085)
*Prabhanjan Ananth,John Bostanci,Aditya Gulati,Yao-Ting Lin*

Main category: cs.CR

TL;DR: This paper presents a secure gluing theorem for random unitaries, enabling extended key length and reduced randomness requirements for strong pseudorandom unitaries.


<details>
  <summary>Details</summary>
Motivation: Existing gluing methods lack security against inverse-query adversaries, and efficient pseudorandom unitary constructions with sublinear randomness were missing.

Method: An alternate gluing approach using the Haar random unitary lemma from [Schuster et al., QIP 2025], ensuring security against adversaries with inverse query access to the combined unitary.

Result: First generic key-length extension for strong pseudorandom unitaries, and constructions using O(n^{1/c}) bits of randomness for any constant c.

Conclusion: The method advances quantum pseudorandomness by addressing security and efficiency gaps, with implications for quantum cryptography and circuit design.

Abstract: Gluing theorem for random unitaries [Schuster, Haferkamp, Huang, QIP 2025]
have found numerous applications, including designing low depth random
unitaries [Schuster, Haferkamp, Huang, QIP 2025], random unitaries in ${\sf
QAC0}$ [Foxman, Parham, Vasconcelos, Yuen'25] and generically shortening the
key length of pseudorandom unitaries [Ananth, Bostanci, Gulati, Lin
EUROCRYPT'25]. We present an alternate method of combining Haar random
unitaries from the gluing lemma from [Schuster, Haferkamp, Huang, QIP 2025]
that is secure against adversaries with inverse query access to the joined
unitary. As a consequence, we show for the first time that strong pseudorandom
unitaries can generically have their length extended, and can be constructed
using only $O(n^{1/c})$ bits of randomness, for any constant $c$, if any family
of strong pseudorandom unitaries exists.

</details>


### [26] [Cyber Warfare During Operation Sindoor: Malware Campaign Analysis and Detection Framework](https://arxiv.org/abs/2510.04118)
*Prakhar Paliwal,Atul Kabra,Manjesh Kumar Hanawal*

Main category: cs.CR

TL;DR: This paper analyzes cyberwarfare tactics in Operation Sindoor, focusing on Pakistan's use of Remote Access Trojans (RATs), techniques for deployment, and a telemetry framework for detection.


<details>
  <summary>Details</summary>
Motivation: Digitization of critical infrastructure enables remote cyberattacks to disrupt opponents' capabilities and morale through disinformation, as seen in recent global conflicts.

Method: Study of Pakistan APT groups' malware in Operation Sindoor, including RAT deployment tactics, Osquery-based telemetry framework for log collection, and development of a detection rule.

Result: Detailed insights into RAT deployment techniques and a deployable detection rule to identify the malware or its exploitation.

Conclusion: The research emphasizes the necessity of monitoring critical infrastructure and implementing proactive detection systems against sophisticated cyberwarfare strategies.

Abstract: Rapid digitization of critical infrastructure has made cyberwarfare one of
the important dimensions of modern conflicts. Attacking the critical
infrastructure is an attractive pre-emptive proposition for adversaries as it
can be done remotely without crossing borders. Such attacks disturb the support
systems of the opponents to launch any offensive activities, crippling their
fighting capabilities. Cyberattacks during cyberwarfare can not only be used to
steal information, but also to spread disinformation to bring down the morale
of the opponents. Recent wars in Europe, Africa, and Asia have demonstrated the
scale and sophistication that the warring nations have deployed to take the
early upper hand. In this work, we focus on the military action launched by
India, code-named Operation Sindoor, to dismantle terror infrastructure
emanating from Pakistan and the cyberattacks launched by Pakistan. In
particular, we study the malware used by Pakistan APT groups to deploy Remote
Access Trojans in Indian systems. We provide details of the tactics and
techniques used in the RAT deployment and develop a telemetry framework to
collect necessary event logs using Osquery with a custom extension. Finally, we
develop a detection rule that can be readily deployed to detect the presence of
the RAT or any exploitation performed by the malware.

</details>


### [27] [ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy Preservation](https://arxiv.org/abs/2510.04153)
*Haoqi Wu,Wei Dai,Ming Xu,Li Wang,Qiang Yan*

Main category: cs.CR

TL;DR: This paper proposes ObCLIP, a privacy-preserving method for diffusion models that hides sensitive attributes in user prompts by generating semantically similar candidates, preventing information leakage on cloud servers, while minimizing computational costs.


<details>
  <summary>Details</summary>
Motivation: Diffusion Models are computationally intensive and lack strong privacy guarantees for user prompts in cloud inference services, leading to potential sensitive information leakage.

Method: ObCLIP transforms input prompts into semantically similar candidates that vary only by sensitive attributes. The cloud processes all candidates without knowing the real one, and a device model completes the generation using intermediate latents with cache-based optimizations for efficiency.

Result: Experiments show ObCLIP achieves rigorous privacy with utility comparable to cloud models, at slightly increased cost.

Conclusion: ObClip is an effective, efficient, and practical solution for privacy in cloud-based diffusion model inference by combining prompt obfuscation, partial generation on the cloud, and local completion with cache optimizations.

Abstract: Diffusion Models have gained significant popularity due to their remarkable
capabilities in image generation, albeit at the cost of intensive computation
requirement. Meanwhile, despite their widespread deployment in inference
services such as Midjourney, concerns about the potential leakage of sensitive
information in uploaded user prompts have arisen. Existing solutions either
lack rigorous privacy guarantees or fail to strike an effective balance between
utility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play
safeguard that enables oblivious cloud-device hybrid generation. By oblivious,
each input prompt is transformed into a set of semantically similar candidate
prompts that differ only in sensitive attributes (e.g., gender, ethnicity). The
cloud server processes all candidate prompts without knowing which one is the
real one, thus preventing any prompt leakage. To mitigate server cost, only a
small portion of denoising steps is performed upon the large cloud model. The
intermediate latents are then sent back to the client, which selects the
targeted latent and completes the remaining denoising using a small device
model. Additionally, we analyze and incorporate several cache-based
accelerations that leverage temporal and batch redundancy, effectively reducing
computation cost with minimal utility degradation. Extensive experiments across
multiple datasets demonstrate that ObCLIP provides rigorous privacy and
comparable utility to cloud models with slightly increased server cost.

</details>


### [28] [AgentTypo: Adaptive Typographic Prompt Injection Attacks against Black-box Multimodal Agents](https://arxiv.org/abs/2510.04257)
*Yanjie Li,Yiming Cao,Dong Wang,Bin Xiao*

Main category: cs.CR

TL;DR: AgentTypo introduces a black-box typographic prompt injection framework that outperforms existing methods by optimizing stealthy text embedding in webpage images for attacking multimodal agents.


<details>
  <summary>Details</summary>
Motivation: Multimodal agents face significant vulnerabilities to visual prompt injections, with current attacks lacking adaptability and stealth. The paper addresses this by developing a practical framework for scalable, hard-to-detect attacks.

Method: The ATPI algorithm combines prompt reconstruction via captioner substitution, stealth loss for human detection resistance, and Tree-structured Parzen Estimator for optimization. AgentTypo-pro adds multi-LLM iterative refinement, feedback-driven learning, and a strategy repository for knowledge reuse.

Result: Achieves 0.45 success rate on GPT-4o (up from 0.23), 0.68 attack success rate (ASR), and outperforms baselines across GPT-4V, Gemini 1.5 Pro, and Claude 3 Opus in VWA-Adv benchmark scenarios.

Conclusion: AgentTypo demonstrates a critical threat vector for multimodal agents, exposing urgent defense needs through its superior practical attack performance and adaptability across models.

Abstract: Multimodal agents built on large vision-language models (LVLMs) are
increasingly deployed in open-world settings but remain highly vulnerable to
prompt injection, especially through visual inputs. We introduce AgentTypo, a
black-box red-teaming framework that mounts adaptive typographic prompt
injection by embedding optimized text into webpage images. Our automatic
typographic prompt injection (ATPI) algorithm maximizes prompt reconstruction
by substituting captioners while minimizing human detectability via a stealth
loss, with a Tree-structured Parzen Estimator guiding black-box optimization
over text placement, size, and color. To further enhance attack strength, we
develop AgentTypo-pro, a multi-LLM system that iteratively refines injection
prompts using evaluation feedback and retrieves successful past examples for
continual learning. Effective prompts are abstracted into generalizable
strategies and stored in a strategy repository, enabling progressive knowledge
accumulation and reuse in future attacks. Experiments on the VWA-Adv benchmark
across Classifieds, Shopping, and Reddit scenarios show that AgentTypo
significantly outperforms the latest image-based attacks such as AgentAttack.
On GPT-4o agents, our image-only attack raises the success rate from 0.23 to
0.45, with consistent results across GPT-4V, GPT-4o-mini, Gemini 1.5 Pro, and
Claude 3 Opus. In image+text settings, AgentTypo achieves 0.68 ASR, also
outperforming the latest baselines. Our findings reveal that AgentTypo poses a
practical and potent threat to multimodal agents and highlight the urgent need
for effective defense.

</details>


### [29] [VortexPIA: Indirect Prompt Injection Attack against LLMs for Efficient Extraction of User Privacy](https://arxiv.org/abs/2510.04261)
*Yu Cui,Sicheng Pan,Yifei Liu,Haibin Zhang,Cong Zuo*

Main category: cs.CR

TL;DR: Can we extract private data from LLM-powered CIs with the black-box setting?


<details>
  <summary>Details</summary>
Motivation: Traditional methods rely on white-box, but in real application, attackers can't access system prompt. This study aim to answer the question by proposing VortexPIA to test the possibility in black-box scenario.

Method: VortexPIA crafts a token-efficient dataset with false memory and uses it to train a LLM that request user to disclose their private info in multiple privleges categories while being applied to several applications in black-box scenario.

Result: VortexPIA outperforms prior approaches in extracting user's private info in a black-box setting with less token usage and better robustness against known defense techniques.

Conclusion: VortexPIA shows that even black-box attackers can successfully extract private info from LLM-powered CAIs, highlighting the privacy risks. Future research can focus towards more robust defense techniques.

Abstract: Large language models (LLMs) have been widely deployed in Conversational AIs
(CAIs), while exposing privacy and security threats. Recent research shows that
LLM-based CAIs can be manipulated to extract private information from human
users, posing serious security threats. However, the methods proposed in that
study rely on a white-box setting that adversaries can directly modify the
system prompt. This condition is unlikely to hold in real-world deployments.
The limitation raises a critical question: can unprivileged attackers still
induce such privacy risks in practical LLM-integrated applications? To address
this question, we propose \textsc{VortexPIA}, a novel indirect prompt injection
attack that induces privacy extraction in LLM-integrated applications under
black-box settings. By injecting token-efficient data containing false
memories, \textsc{VortexPIA} misleads LLMs to actively request private
information in batches. Unlike prior methods, \textsc{VortexPIA} allows
attackers to flexibly define multiple categories of sensitive data. We evaluate
\textsc{VortexPIA} on six LLMs, covering both traditional and reasoning LLMs,
across four benchmark datasets. The results show that \textsc{VortexPIA}
significantly outperforms baselines and achieves state-of-the-art (SOTA)
performance. It also demonstrates efficient privacy requests, reduced token
consumption, and enhanced robustness against defense mechanisms. We further
validate \textsc{VortexPIA} on multiple realistic open-source LLM-integrated
applications, demonstrating its practical effectiveness.

</details>


### [30] [MulVuln: Enhancing Pre-trained LMs with Shared and Language-Specific Knowledge for Multilingual Vulnerability Detection](https://arxiv.org/abs/2510.04397)
*Van Nguyen,Surya Nepal,Xingliang Yuan,Tingmin Wu,Fengchao Chen,Carsten Rudolph*

Main category: cs.CR

TL;DR: Multilingual vulnerability detection is difficult because existing approaches are language specific and cannot capture multi language domain code. MULVULN is a new approach that address this by learn re common and language specific knowledge.


<details>
  <summary>Details</summary>
Motivation: Current vulnerability detection methods are limited to a single language, making it hard to generalize for multilingual software.', 

Method: MULVULN leverages both shared knowledge across programming languages and language-specific knowledge to enhance vulnerability detection in multilingual codebases.

Result: MULVULN achieves higher F1-scores on the REEF dataset, outperforming existing methods by 1.45% to 23.59%.

Conclusion: MULVULN is an effective multilingual vulnerability detection approach that addresses the challenge of multilingual code by learning common and language-specific patterns.

Abstract: Software vulnerabilities (SVs) pose a critical threat to safety-critical
systems, driving the adoption of AI-based approaches such as machine learning
and deep learning for software vulnerability detection. Despite promising
results, most existing methods are limited to a single programming language.
This is problematic given the multilingual nature of modern software, which is
often complex and written in multiple languages. Current approaches often face
challenges in capturing both shared and language-specific knowledge of source
code, which can limit their performance on diverse programming languages and
real-world codebases. To address this gap, we propose MULVULN, a novel
multilingual vulnerability detection approach that learns from source code
across multiple languages. MULVULN captures both the shared knowledge that
generalizes across languages and the language-specific knowledge that reflects
unique coding conventions. By integrating these aspects, it achieves more
robust and effective detection of vulnerabilities in real-world multilingual
software systems. The rigorous and extensive experiments on the real-world and
diverse REEF dataset, consisting of 4,466 CVEs with 30,987 patches across seven
programming languages, demonstrate the superiority of MULVULN over thirteen
effective and state-of-the-art baselines. Notably, MULVULN achieves
substantially higher F1-score, with improvements ranging from 1.45% to 23.59%
compared to the baseline methods.

</details>


### [31] [P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs](https://arxiv.org/abs/2510.04503)
*Shuai Zhao,Xinyi Wu,Shiqian Zhao,Xiaobao Wu,Zhongliang Guo,Yanhao Jia,Anh Tuan Luu*

Main category: cs.CR

TL;DR: The paper proposes P2P, a general backdoor defense algorithm for LLMs during fine-tuning by injecting benign triggers with safe labels into datasets using prompt-based learning.


<details>
  <summary>Details</summary>
Motivation: Existing defense strategies lack generalization against diverse backdoor attack types and task settings, creating reliability risks for LLMs undergoing fine-tuning.

Method: P2P introduces benign triggers with safe labels into training samples via re-poisoning, leveraging prompt-based learning to force models to associate triggers with non-malicious outputs.

Result: Experiments show P2P significantly reduces attack success rates across classification, reasoning, and summarization tasks while maintaining performance on multiple SOTA LLMs.

Conclusion: P2P provides a robust, generalizable solution for backdoor mitigation, offering a framework for developing secure LLM applications through trigger-based finetuning.

Abstract: During fine-tuning, large language models (LLMs) are increasingly vulnerable
to data-poisoning backdoor attacks, which compromise their reliability and
trustworthiness. However, existing defense strategies suffer from limited
generalization: they only work on specific attack types or task settings. In
this study, we propose Poison-to-Poison (P2P), a general and effective backdoor
defense algorithm. P2P injects benign triggers with safe alternative labels
into a subset of training samples and fine-tunes the model on this re-poisoned
dataset by leveraging prompt-based learning. This enforces the model to
associate trigger-induced representations with safe outputs, thereby overriding
the effects of original malicious triggers. Thanks to this robust and
generalizable trigger-based fine-tuning, P2P is effective across task settings
and attack types. Theoretically and empirically, we show that P2P can
neutralize malicious backdoors while preserving task performance. We conduct
extensive experiments on classification, mathematical reasoning, and summary
generation tasks, involving multiple state-of-the-art LLMs. The results
demonstrate that our P2P algorithm significantly reduces the attack success
rate compared with baseline models. We hope that the P2P can serve as a
guideline for defending against backdoor attacks and foster the development of
a secure and trustworthy LLM community.

</details>


### [32] [Unified Threat Detection and Mitigation Framework (UTDMF): Combating Prompt Injection, Deception, and Bias in Enterprise-Scale Transformers](https://arxiv.org/abs/2510.04528)
*Santhosh KumarRavindran*

Main category: cs.CR

TL;DR: The paper introduces UTDMF, a real-time threat detection and mitigation framework for enterprise LLMs, achieving high detection accuracy for prompt injections, reduced deceptive outputs, and improved fairness metrics. It builds on adversarial activation patching with novel contributions for multi-threat detection and deployment.


<details>
  <summary>Details</summary>
Motivation: Enterprise adoption of LLMs creates critical vulnerabilities to prompt injections, deceptive outputs, and fairness issues, requiring scalable real-time solutions for production models like Llama-3.1 and GPT-4o.

Method: Extends adversarial activation patching to create UTDMF, featuring a generalized patching algorithm for multi-threat detection, experiments across 700+$ cases per model, and three hypotheses on enterprise threat interactions including 'threat chaining'.

Result: 92% prompt injection detection accuracy, 65% reduction in deceptive outputs via optimized patching, 78% improvement in demographic fairness metrics across enterprise-grade models.

Conclusion: UTDMF establishes a robust solution for enterprise LLM security with novel threat interaction hypotheses, a production-ready API toolkit, and quantifiable improvements in three critical risk areas.

Abstract: The rapid adoption of large language models (LLMs) in enterprise systems
exposes vulnerabilities to prompt injection attacks, strategic deception, and
biased outputs, threatening security, trust, and fairness. Extending our
adversarial activation patching framework (arXiv:2507.09406), which induced
deception in toy networks at a 23.9% rate, we introduce the Unified Threat
Detection and Mitigation Framework (UTDMF), a scalable, real-time pipeline for
enterprise-grade models like Llama-3.1 (405B), GPT-4o, and Claude-3.5. Through
700+ experiments per model, UTDMF achieves: (1) 92% detection accuracy for
prompt injection (e.g., jailbreaking); (2) 65% reduction in deceptive outputs
via enhanced patching; and (3) 78% improvement in fairness metrics (e.g.,
demographic bias). Novel contributions include a generalized patching algorithm
for multi-threat detection, three groundbreaking hypotheses on threat
interactions (e.g., threat chaining in enterprise workflows), and a
deployment-ready toolkit with APIs for enterprise integration.

</details>


### [33] [Computational Certified Deletion Property of Magic Square Game and its Application to Classical Secure Key Leasing](https://arxiv.org/abs/2510.04529)
*Yuki Takeuchi,Duo Xu*

Main category: cs.CR

TL;DR: The paper constructs the first computational Certified Deletion Property (CDP) using classical communication by compiling the Magic Square Game (MSG) into a 2-round interactive protocol. It applies this CDP to realize Secure Key Leasing (cSKL) for PRF and digital signatures with weakened assumptions.


<details>
  <summary>Details</summary>
Motivation: Existing CDP solutions required quantum communication or were limited to specific cryptographic primitives. The work addresses the need for classical-communication CDP and expands cSKL capabilities.

Method: 1) Compile the non-local MSG into a 2-round protocol via KLVY compiler. 2) Prove the compiled protocol preserves CDP. 3) Integrate with the [Kitagawa et al.] framework to construct cSKL for PKE, PRF, and digital signatures.

Result: First classical CDP construction. First cSKL implementation for PRF/digital signatures. Reduced assumptions compared to prior cSKL work.

Conclusion: Demonstrates CDP can be achieved via classical communication, enabling broader cryptographic applications like efficient key leasing with verified deletion.

Abstract: We present the first construction of a computational Certified Deletion
Property (CDP) achievable with classical communication, derived from the
compilation of the non-local Magic Square Game (MSG). We leverage the KLVY
compiler to transform the non-local MSG into a 2-round interactive protocol,
rigorously demonstrating that this compilation preserves the game-specific CDP.
Previously, the quantum value and rigidity of the compiled game were
investigated. We emphasize that we are the first to investigate CDP (local
randomness in [Fu and Miller, Phys. Rev. A 97, 032324 (2018)]) for the compiled
game. Then, we combine this CDP with the framework [Kitagawa, Morimae, and
Yamakawa, Eurocrypt 2025] to construct Secure Key Leasing with classical Lessor
(cSKL). SKL enables the Lessor to lease the secret key to the Lessee and verify
that a quantum Lessee has indeed deleted the key. In this paper, we realize
cSKL for PKE, PRF, and digital signature. Compared to prior works for cSKL, we
realize cSKL for PRF and digital signature for the first time. In addition, we
succeed in weakening the assumption needed to construct cSKL.

</details>


### [34] [PoS-CoPOR: Proof-of-Stake Consensus Protocol with Native Onion Routing Providing Scalability and DoS-Resistance](https://arxiv.org/abs/2510.04619)
*Ivan Homoliak,Martin Perešíni,Marek Tamaškovič,Timotej Ponek,Lukáš Hellebrandt,Kamil Malinka*

Main category: cs.CR

TL;DR: Proof-of-Stake (PoS) consensus protocols often face a trade-off between performance and security. Existing protocols pre-elect leaders, making them vulnerable to DoS attacks. This paper introduces PoS-CoPOR, which integrates onion routing into PoS to anonymize the next block proposer and resist DoS attacks. Evaluations show it maintains reasonable performance with enhanced security.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address the inherent vulnerability of pre-elect leaders in Proof-of-Stake (PoS) consensus protocols to Denial-of-Service (DoS) attacks, which can compromise the liveness of the network.

Method: The method proposed in this paper is the introduction of PoS-CoPOR, a single-chain PoS consensus protocol that integrates an onion routing mechanism to anonymize the identity of the next block proposer. It uses stake-weighted probabilistic leader election combined with an anonymization layer.

Result: The results indicate that PoS-CoPOR can achieve an impressive throughput of up to 110 tx/s with 6 nodes, even considering the overhead introduced by the onion routing anonymization mechanism. This demonstrates its capacity to coincide high performance with robust DoS resistance.

Conclusion: The conclusion drawn is that native anonymization through PoS-CoPOR can effectively deliver robust resistance to Denial-of-Service (DoS) attacks with a minimal impact on performance, providing a feasible solution for constructing secure and scalable PoS blockchain networks.

Abstract: Proof-of-Stake (PoS) consensus protocols often face a trade-off between
performance and security. Protocols that pre-elect leaders for subsequent
rounds are vulnerable to Denial-of-Service (DoS) attacks, which can disrupt the
network and compromise liveness. In this work, we present PoS-CoPOR, a
single-chain PoS consensus protocol that mitigates this vulnerability by
integrating a native onion routing mechanism into the consensus protocol
itself. PoS-CoPOR combines stake-weighted probabilistic leader election with an
anonymization layer that conceals the network identity of the next block
proposer. This approach prevents targeted DoS attacks on leaders before they
produce a block, thus enhancing network resilience. We implemented and
evaluated PoS-CoPOR, demonstrating its ability to achieve a throughput of up to
110 tx/s with 6 nodes, even with the overhead of the anonymization layer. The
results show that native anonymization can provide robust DoS resistance with
only a modest impact on performance, offering a solution to build secure and
scalable PoS blockchains.

</details>


### [35] [Backing the Wrong Horse: How Bit-Level Netlist Augmentation can Counter Power Side Channel Attacks](https://arxiv.org/abs/2510.04640)
*Ali Asghar,Andreas Becher,Daniel Ziener*

Main category: cs.CR

TL;DR: The paper discusses how CMOS circuits' power consumption can be exploited in side channel attacks and proposes a bit-level countermeasure that effectively thwarts conventional attack models.


<details>
  <summary>Details</summary>
Motivation: CMOS circuits are vulnerable to side channel attacks since their power consumption varies with processed data. Existing countermeasures targeting byte-level analysis may insufficiently address vulnerabilities at the individual bit level.

Method: The study introduces a countermeasure based on analyzing and mitigating single-bit leakage rather than byte-level leakage in CMOS circuits.

Result: The proposed countermeasure can resist side channel attacks that rely on conventional leakage models, indicating improved security against these threats.

Conclusion: The bit-level countermeasure minimizes side-channel leakage by addressing individual bit impacts, offering a robust solution against current power-based side channel attacks.

Abstract: The dependence of power-consumption on the processed data is a known
vulnerability of CMOS circuits, resulting in side channels which can be
exploited by power-based side channel attacks (SCAs). These attacks can extract
sensitive information, such as secret keys, from the implementation of
cryptographic algorithms. Existing countermeasures against power-based side
channel attacks focus on analyzing information leakage at the byte level.
However, this approach neglects the impact of individual bits on the overall
resistance of a cryptographic implementation. In this work, we present a
countermeasure based on single-bit leakage. The results suggest that the
proposed countermeasure cannot be broken by attacks using conventional SCA
leakage models.

</details>


### [36] [Modeling and Managing Temporal Obligations in GUCON Using SPARQL-star and RDF-star](https://arxiv.org/abs/2510.04652)
*Ines Akaichi,Giorgos Flouris,Irini Fundulaki,Sabrina Kirrane*

Main category: cs.CR

TL;DR: Extends GUCON to model temporal obligations using SPAQRL semantics and temporal knowledge graphs, enabling continuous obligation state monitoring and compliance assessment.


<details>
  <summary>Details</summary>
Motivation: Existing obligation monitoring solutions lack formal semantics and robust temporal reasoning capabilities for evolving obligation states (fulfilled/violated/expired), creating governance gaps in dynamic data sharing environments.

Method: Formally integrates temporal aspects into GUCON policy framework via SPAQRL-based semantics; uses temporal knowledge graphs to store usage traces; introduces RDF-star/SPARQL-star for temporal obligation representation; develops Obligation State Manager for continuous state monitoring.

Result: Proposed temporal extension to GUCON, implementation of Obligation State Manager, and evaluation showing effective support for temporal obligation monitoring across evolving data usage scenarios.

Conclusion: Temporal-enhanced GUCON provides formal foundations for monitoring obligation state evolution, enabling systematic compliance assessment in cross-organizational data governance workflows.

Abstract: In the digital age, data frequently crosses organizational and jurisdictional
boundaries, making effective governance essential. Usage control policies have
emerged as a key paradigm for regulating data usage, safeguarding privacy,
protecting intellectual property, and ensuring compliance with regulations. A
central mechanism for usage control is the handling of obligations, which arise
as a side effect of using and sharing data. Effective monitoring of obligations
requires capturing usage traces and accounting for temporal aspects such as
start times and deadlines, as obligations may evolve over times into different
states, such as fulfilled, violated, or expired. While several solutions have
been proposed for obligation monitoring, they often lack formal semantics or
provide limited support for reasoning over obligation states. To address these
limitations, we extend GUCON, a policy framework grounded in the formal
semantics of SPAQRL graph patterns, to explicitly model the temporal aspects of
an obligation. This extension enables the expressing of temporal obligations
and supports continuous monitoring of their evolving states based on usage
traces stored in temporal knowledge graphs. We demonstrate how this extended
model can be represented using RDF-star and SPARQL-star and propose an
Obligation State Manager that monitors obligation states and assess their
compliance with respect to usage traces. Finally, we evaluate both the extended
model and its prototype implementation.

</details>


### [37] [Enhancing TreePIR for a Single-Server Setting via Resampling](https://arxiv.org/abs/2510.04882)
*Elian Morel*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Private Information Retrieval (PIR) allows a client to retrieve an entry
$\text{DB}[i]$ from a public database $\text{DB}$ held by one or more servers,
without revealing the queried index $i$. Traditional PIR schemes achieve
sublinear server computation only under strong assumptions, such as the
presence of multiple non-colluding servers or the use of public-key
cryptography. To overcome these limitations, \textit{preprocessing PIR} schemes
introduce a query-independent offline phase where the client collects
\textit{hints} that enable efficient private queries during the online phase.
  In this work, we focus on preprocessing PIR schemes relying solely on
\textit{One-Way Functions} (OWFs), which provide minimal cryptographic
assumptions and practical implementability. We study three main constructions
-- TreePIR, PIANO, and PPPS -- that explore different trade-offs between
communication, storage, and server trust assumptions. Building upon the
mechanisms introduced in PIANO and PPPS, we propose an adaptation of TreePIR to
the single-server setting by introducing a dual-table hint structure (primary
and backup tables) and a \textit{resampling} technique to refresh hints
efficiently.
  Our proposed scheme achieves logarithmic upload bandwidth and $O(\sqrt{n}\log
n)$ download complexity while requiring $O(\sqrt{n}\log n)$ client storage.
This represents a significant improvement over prior single-server
preprocessing PIR schemes such as PIANO ($O(\sqrt{n})$ bandwidth) and PPPS
($O(n^{1/4})$ bandwidth), while maintaining the simplicity and minimal
assumptions of the OWF-based setting.

</details>


### [38] [RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning Recipe for Strong Prompt Injection](https://arxiv.org/abs/2510.04885)
*Yuxin Wen,Arman Zharmagambetov,Ivan Evtimov,Narine Kokhlikyan,Tom Goldstein,Kamalika Chaudhuri,Chuan Guo*

Main category: cs.CR

TL;DR: The paper introduces RL-Hammer, a reinforcement learning-based method for automated prompt injection attacks against LLMs with advanced defenses, achieving high success rates (e.g., 98% ASR against GPT-4o) and evading detectors. The work emphasizes improving red-teaming and defense robustness.


<details>
  <summary>Details</summary>
Motivation: Existing defenses like Instruction Hierarchy and SecAlign remain poorly tested against strong automated attacks, necessitating rigorous evaluation via red-teaming to drive safer LLM development.

Method: RL-Hammer trains attacker models from scratch using reinforcement learning, employing techniques to maximize attack success rates and diversity without requiring pre-labeled data, enabling universal attacks against industrial models with defenses.

Result: Achieved 98.0% ASR on GPT-4o and 72.0±4.5% ASR on GPT-5 with Instruction Hierarchy defense, while evading multiple prompt injection detectors, demonstrating strong effectiveness and adaptability.

Conclusion: Highlights RL-Hammer's role in stress-testing defenses through automated red-teaming, exposing limitations in current detector diversity, and advocating for principled defense frameworks against dynamic attacks.

Abstract: Prompt injection poses a serious threat to the reliability and safety of LLM
agents. Recent defenses against prompt injection, such as Instruction Hierarchy
and SecAlign, have shown notable robustness against static attacks. However, to
more thoroughly evaluate the robustness of these defenses, it is arguably
necessary to employ strong attacks such as automated red-teaming. To this end,
we introduce RL-Hammer, a simple recipe for training attacker models that
automatically learn to perform strong prompt injections and jailbreaks via
reinforcement learning. RL-Hammer requires no warm-up data and can be trained
entirely from scratch. To achieve high ASRs against industrial-level models
with defenses, we propose a set of practical techniques that enable highly
effective, universal attacks. Using this pipeline, RL-Hammer reaches a 98% ASR
against GPT-4o and a $72\%$ ASR against GPT-5 with the Instruction Hierarchy
defense. We further discuss the challenge of achieving high diversity in
attacks, highlighting how attacker models tend to reward-hack diversity
objectives. Finally, we show that RL-Hammer can evade multiple prompt injection
detectors. We hope our work advances automatic red-teaming and motivates the
development of stronger, more principled defenses. Code is available at
https://github.com/facebookresearch/rl-injector.

</details>


### [39] [NatGVD: Natural Adversarial Example Attack towards Graph-based Vulnerability Detection](https://arxiv.org/abs/2510.04987)
*Avilash Rath,Weiliang Qi,Youpeng Li,Xinda Wang*

Main category: cs.CR

TL;DR: The paper introduces NatGVD, an attack method for vulnerability detectors that uses natural adversarial vulnerable code, showing significant evasion rates and proposing potential defenses.


<details>
  <summary>Details</summary>
Motivation: Current robustness of graph-based models to adversarial attacks in vulnerability detection is unverified, raising concerns about their dependability.

Method: NatGVD leverages code transformations to alter graph structure without affecting semantics, emphasizing naturalness to avoid human and tool detection.

Result: NatGVD achieves an evasion rate of up to 53.04% against advanced vulnerability detectors, highlighting their susceptibility to natural-like attacks.

Conclusion: NatGVD's effectiveness against vulnerability detectors necessitates implementing defense mechanisms, which the study also presents in the form of preliminary strategies.

Abstract: Graph-based models learn rich code graph structural information and present
superior performance on various code analysis tasks. However, the robustness of
these models against adversarial example attacks in the context of
vulnerability detection remains an open question. This paper proposes NatGVD, a
novel attack methodology that generates natural adversarial vulnerable code to
circumvent GNN-based and graph-aware transformer-based vulnerability detectors.
NatGVD employs a set of code transformations that modify graph structure while
preserving code semantics. Instead of injecting dead or unrelated code like
previous works, NatGVD considers naturalness requirements: generated examples
should not be easily recognized by humans or program analysis tools. With
extensive evaluation of NatGVD on state-of-the-art vulnerability detection
systems, the results reveal up to 53.04% evasion rate across GNN-based
detectors and graph-aware transformer-based detectors. We also explore
potential defense strategies to enhance the robustness of these systems against
NatGVD.

</details>


### [40] [Proactive defense against LLM Jailbreak](https://arxiv.org/abs/2510.05052)
*Weiliang Zhao,Jinjun Peng,Daniel Ben-Levi,Zhou Yu,Junfeng Yang*

Main category: cs.CR

TL;DR: The paper introduces ProAct, a proactive defense framework against adversarial multi-turn jailbreak attacks on large language models (LLMs). By generating 'spurious responses' that mimic successful jailbreak outputs but lack real harmful content, ProAct misleads attackers into false estimation of success, reducing attack success rates by up to 92%.


<details>
  <summary>Details</summary>
Motivation: LLMs are vulnerable to evolving search-based jailbreak attacks that existing reactive/static defenses cannot counter. This paper addresses the need for proactive strategies to disrupt adversarial attack processes in real-time.

Method: ProAct deploys a 'decoy-based defense strategy' that generates realistic but harmless responses during autonomous jailbreaking. These spurious responses exploit the heuristic feedback loops in attack optimization, causing early termination of adversarial search without relying on explicit content filtering.

Result: Experiments show ProAct reduces attack success rates by 92% across SOTA LLMs and frameworks. When combined with existing defense systems, it achieves 100%, non-intrusive mitigation of state-of-the-art jailbreaking techniques.

Conclusion: ProAct provides an orthogonal defense mechanism that can complement existing safety measures while maintaining LLM capabilities. This strategy shifts the paradigm from reactive filtering to proactive deception in adversarial robustness.

Abstract: The proliferation of powerful large language models (LLMs) has necessitated
robust safety alignment, yet these models remain vulnerable to evolving
adversarial attacks, including multi-turn jailbreaks that iteratively search
for successful queries. Current defenses, primarily reactive and static, often
fail to counter these search-based attacks. In this paper, we introduce ProAct,
a novel proactive defense framework designed to disrupt and mislead autonomous
jailbreaking processes. Our core idea is to intentionally provide adversaries
with "spurious responses" that appear to be results of successful jailbreak
attacks but contain no actual harmful content. These misleading responses
provide false signals to the attacker's internal optimization loop, causing the
adversarial search to terminate prematurely and effectively jailbreaking the
jailbreak. By conducting extensive experiments across state-of-the-art LLMs,
jailbreaking frameworks, and safety benchmarks, our method consistently and
significantly reduces attack success rates by up to 92\%. When combined with
other defense frameworks, it further reduces the success rate of the latest
attack strategies to 0\%. ProAct represents an orthogonal defense strategy that
can serve as an additional guardrail to enhance LLM safety against the most
effective jailbreaking attacks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [41] [Repairing Leaks in Resource Wrappers](https://arxiv.org/abs/2510.03461)
*Sanjay Malakar,Michael D. Ernst,Martin Kellogg,Manu Sridharan*

Main category: cs.SE

TL;DR: The paper presents a novel approach to automatically repair resource leaks in programs by extending existing leak detection and repair techniques to handle resource wrappers. It introduces four key contributions, including specification inference, program transformation for better analysis, field lifetime analysis, and a new repair pattern, achieving a 68% success rate in the NJR benchmark suite.


<details>
  <summary>Details</summary>
Motivation: Resource leaks are a common issue in software where programs fail to release finite resources. While static analysis can detect these leaks, automatically repairing them is still a challenge, especially when resources are wrapped and stored in fields, which previous methods couldn't handle effectively.

Method: The authors propose four improvements to resource leak repair: (1) integrating specification inference into the repair pipeline to reason about wrappers, (2) transforming programs for easier analysis, (3) implementing a field containment analysis for resource lifetime handling, and (4) introducing a new repair pattern for non-final fields.

Result: Their implementation, Arodnap, fixes 68% of resource leak warnings in the NJR benchmark, improving upon previous work that achieved 41%.

Conclusion: The approach significantly enhances the effectiveness of resource leak repair by addressing wrapper-based leaks more comprehensively. The success rate of automated repair jumps to 68%, showing the impact of reasoning about field-contained resources and non-final fields.

Abstract: A resource leak occurs when a program fails to release a finite resource like
a socket, file descriptor or database connection. While sound static analysis
tools can detect all leaks, automatically repairing them remains challenging.
Prior work took the output of a detection tool and attempted to repair only
leaks from a hard-coded list of library resource types. That approach limits
the scope of repairable leaks: real-world code uses resource wrappers that
store a resource in a field and must themselves be closed. This paper makes
four key contributions to improve resource leak repair in the presence of
wrappers. (1) It integrates inference of resource management specifications
into the repair pipeline, enabling extant fixing approaches to reason about
wrappers. (2) It transforms programs into variants that are easier to analyze,
making inference, detection, and fixing tools more effective; for instance, it
makes detection tools report problems closer to the root cause, often in a
client of a resource wrapper rather than within the wrapper class itself. (3) A
novel field containment analysis reasons about resource lifetimes, enabling
repair of more leaks involving resources stored in fields. (4) It introduces a
new repair pattern and more precise reasoning to better handle resources stored
in non-final fields. Prior work fixed 41% of resource leak warnings in the NJR
benchmark suite; our implementation Arodnap fixes 68%.

</details>


### [42] [ALMAS: an Autonomous LLM-based Multi-Agent Software Engineering Framework](https://arxiv.org/abs/2510.03463)
*Vali Tawosi,Keshav Ramani,Salwa Alamir,Xiaomo Liu*

Main category: cs.SE

TL;DR: TL;DR: This paper proposes ALMAS, an autonomous multi-agent LLM-based framework for software engineering that aligns with agile methodologies. ALMAS aims to perform multiple stages of the software development life cycle end-to-end and integrate with human teams.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is driven by the complexity and multifaceted nature of software development, which suggests the need for a more comprehensive approach for LLM systems. Current LLM agents primarily focus on code implementation, testing, and maintenance, but agile software development requires a more integrated and end-to-end system to handle the full spectrum of SDLC activities.

Method: To achieve the proposed vision, the authors focus on developing a framework that aligns LLM agent functionalities with agile software development roles and multiple stages of the software development life cycle (SDLC). The framework is designed to be modular, allowing it to naturally integrate with human developers and existing development environments. The approach is validated using use cases and results from related published works that have contributed to the foundation of ALMAS.

Result: Results presented in the paper include the successful demonstration of ALMAS, where the framework was able to autonomously generate an application and implement a new feature through its agent architecture. These findings are supported by the authors' published works and a specific use case illustrating ALMAS's integration and functionality within the SDLC.

Conclusion: In conclusion, the authors presented a vision for ALMAS as a comprehensive autonomous multi-agent framework for LLM-based software engineering. They emphasize the potential of ALMAS to be used in agile development workflows, where its modular design allows for flexible use and seamless integration with human teams and environments.

Abstract: Multi-agent Large Language Model (LLM) systems have been leading the way in
applied LLM research across a number of fields. One notable area is software
development, where researchers have advanced the automation of code
implementation, code testing, code maintenance, inter alia, using LLM agents.
However, software development is a multifaceted environment that extends beyond
just code. As such, a successful LLM system must factor in multiple stages of
the software development life-cycle (SDLC). In this paper, we propose a vision
for ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework,
which follows the above SDLC philosophy such that it may work within an agile
software development team to perform several tasks end-to-end. ALMAS aligns its
agents with agile roles, and can be used in a modular fashion to seamlessly
integrate with human developers and their development environment. We showcase
the progress towards ALMAS through our published works and a use case
demonstrating the framework, where ALMAS is able to seamlessly generate an
application and add a new feature.

</details>


### [43] [Relative Code Comprehensibility Prediction](https://arxiv.org/abs/2510.03474)
*Nadeeshan De Silva,Martin Kellogg,Oscar Chaparro*

Main category: cs.SE

TL;DR: This paper introduces a more accurate approach to predicting code comprehensibility by focusing on relative comparisons rather than absolute measurements, finding that this method improves model performance significantly over traditional methods and naive baselines.


<details>
  <summary>Details</summary>
Motivation: Current code comprehensibility metrics and machine learning models struggle with accuracy due to noise in human understanding measurements. Developers need tools to decide when to refactor code, and the paper addresses this by proposing a better model approach.

Method: The authors propose models that predict which of two code snippets is more comprehensible to a human, instead of evaluating each snippet's absolute comprehensibility. They compare the effectiveness of absolute and relative prediction models using 150 Java snippets and existing 12.5k human measurements in a study.

Result: Absolute models only improved over baselines by up to 33.4% and often underperformed. Relative models improved performance by 137.8% and 74.7% for snippet-wise and developer-wise predictions, respectively, outperforming naive baselines and absolute models.

Conclusion: Relative comprehensibility models are more effective and robust against data noise, making them practical for software engineering tasks. This approach outperforms current methods, allowing better assessment of factors like refactoring impact without requiring absolute ease metrics.

Abstract: Automatically predicting how difficult it is for humans to understand a code
snippet can assist developers in tasks like deciding when and where to
refactor. Despite many proposed code comprehensibility metrics, studies have
shown they often correlate poorly with actual measurements of human
comprehensibility. This has motivated the use of machine learning models to
predict human comprehensibility directly from code, but these models have also
shown limited accuracy.
  We argue that model inaccuracy stems from inherent noise in human
comprehensibility data, which confuses models trained to predict it directly.
To address this, we propose training models to predict the relative
comprehensibility of two code snippets - that is, predicting which snippet a
human would find easier to understand without predicting each snippet's
comprehensibility in isolation. This mitigates noise in predicting 'absolute'
comprehensibility measurements, but is still useful for downstream
software-engineering tasks like assessing whether refactoring improves or
hinders comprehensibility.
  We conducted a study to assess and compare the effectiveness of absolute and
relative code comprehensibility prediction via machine learning. We used a
dataset of 150 Java code snippets and 12.5k human comprehensibility
measurements from prior user studies, comparing the models' performance with
naive baselines (eg 'always predict the majority class'). Our findings indicate
that absolute comprehensibility models improve over the baselines by at most
33.4% and frequently underperform. In contrast, relative comprehensibility
models are substantially better, with average improvements of 137.8% and 74.7%
for snippet-wise and developer-wise prediction, respectively. These results
suggest that relative comprehensibility models learn more effectively from the
data, supporting their practical applicability for downstream SE tasks.

</details>


### [44] [LLM Agents for Automated Dependency Upgrades](https://arxiv.org/abs/2510.03480)
*Vali Tawosi,Salwa Alamir,Xiaomo Liu,Manuela Veloso*

Main category: cs.SE

TL;DR: The paper proposes an LLM agent framework to automatically update and maintain Java codebases by leveraging migration documentation, achieving high precision and efficiency.


<details>
  <summary>Details</summary>
Motivation: Outdated library dependencies in expanding codebases demand updates for innovation and security, which often require substantial developer time due to breaking changes.

Method: A system architecture comprising Summary, Control, and Code Agents is introduced, integrating LLM capabilities with migration documentation to recommend and apply compatible code updates.

Result: The framework achieves 71.4% precision and uses fewer tokens than state-of-the-art methods when benchmarked across three synthetic repositories with major upgrades.

Conclusion: The LLM agent-based approach provides an efficient and effective automated solution for library upgrades, significantly reducing token usage and maintenance effort.

Abstract: As a codebase expands over time, its library dependencies can become outdated
and require updates to maintain innovation and security. However, updating a
library can introduce breaking changes in the code, necessitating significant
developer time for maintenance. To address this, we introduce a framework of
LLM agents to be used in combination with migration documentation to
automatically recommend and apply code updates and ensure compatibility with
new versions. Our solution can automatically localize updated library usages in
live Java codebases and implement recommended fixes in a user-friendly manner.
The system architecture consists of multiple key components: a Summary Agent,
Control Agent, and Code Agent. To validate our approach, we apply the framework
on an industrial use case by which we create three synthetic code repositories
with major Upgrade changes and benchmark our approach against state-of-the-art
methods. Results show that our approach not only performs upgrades using fewer
tokens across all cases but also achieves a precision of 71.4%, highlighting
its efficiency and effectiveness compared to state-of-the-art methods.

</details>


### [45] [AgentHub: A Research Agenda for Agent Sharing Infrastructure](https://arxiv.org/abs/2510.03495)
*Erik Pautsch,Tanmay Singla,Wenxin Jiang,Huiyun Peng,Behnaz Hassanshahi,Konstantin Läufer,George K. Thiruvathukal,James C. Davis*

Main category: cs.SE

TL;DR: The paper highlights the need for a standardized infrastructure for sharing and managing LLM-based agents and introduces AgentHub, a research agenda to address this gap, with the goal of enabling seamless sharing and reuse of agents like modern software libraries.


<details>
  <summary>Details</summary>
Motivation: Currently, the ecosystem for LLM-based agents is fragmented, unlike mature platforms like npm or Hugging Face. Existing research has focused on limited aspects like distribution and naming, but broad software engineering requirements for infrastructure have not been fully considered, hindering open-source distribution and reuse.

Method: The authors outline AgentHub, which systematically identifies critical infrastructure challenges for LLM agent sharing. They analyze key requirements like capability clarity, lifecycle transparency, interoperability, governance, security, and workflow integration.

Result: AgentHub provides a structured research agenda and sets a foundation for community collaboration in building robust and scalable ecosystems for agent sharing.

Conclusion: Addressing infrastructure requirements will enable LLM agents to be shared efficiently and reused across various applications, similar to software libraries. The proposed research agenda sets a path toward a standardized and reliable agent ecosystem.

Abstract: LLM-based agents are rapidly proliferating, yet the infrastructure for
discovering, evaluating, and governing them remains fragmented compared to
mature ecosystems like software package registries (e.g., npm) and model hubs
(e.g., Hugging Face). Recent research and engineering works have begun to
consider the requisite infrastructure, but so far they focus narrowly -- on
distribution, naming, or protocol negotiation. However, considering broader
software engineering requirements would improve open-source distribution and
ease reuse. We therefore propose AgentHub, a research agenda for agent sharing.
By framing the key challenges of capability clarity, lifecycle transparency,
interoperability, governance, security, and workflow integration, AgentHub
charts a community-wide agenda for building reliable and scalable agent
ecosystems. Our vision is a future where agents can be shared, trusted, and
composed as seamlessly as today's software libraries.

</details>


### [46] [REFINE: Enhancing Program Repair Agents through Context-Aware Patch Refinement](https://arxiv.org/abs/2510.03588)
*Anvith Pabba,Simin Chen,Alex Mathai,Anindya Chakraborty,Baishakhi Ray*

Main category: cs.SE

TL;DR: This paper introduces Refine, a patch refinement framework for LLM-based automatic program repair (APR). It addresses limitations in current APR methods by transforming draft patches into correct ones through disambiguating context, diversifying patch candidates, and aggregating partial fixes via code review. Evaluated on SWE-Bench benchmarks, Refine achieves state-of-the-art results with significant performance boosts.


<details>
  <summary>Details</summary>
Motivation: Current LLM-AVR techniques struggle with incomplete test suite coverage and limited code context understanding, producing partially correct 'Draft Patches'. This limits their practical effectiveness in repository-level APR.

Method: Refine systematically refines Draft Patches via three mechanisms: (1): disambiguating issue/code context through context-enrichment, (2) diversifying patch candidates using test-time scaling, and (3): aggregating partial fixes using an LLM-powered code review process. It is designed as a modular component compatible with both open-agent-based and workflow-based APR systems.

Result: On SWE-Bench Lite, Refine improves AutoCodeRover by 14.67%, achieving 51.67%. On SWE-Bench Verified, it increases resolution by 12.2%. Across multiple APR systems, it yields average 14% improvement while achieving state-of-the-art workflow-based performance.

Conclusion: Refine demonstrates the critical value of refinement stages in APR pipelines as a missing component for transforming near-correct to correct patches. It establishes a new paradigm for agentic collaboration in APR and significantly advances baseline performance while showing strong generalizability across systems.

Abstract: Large Language Models (LLMs) have recently shown strong potential in
automatic program repair (APR), especially in repository-level settings where
the goal is to generate patches based on natural language issue descriptions,
large codebases, and regression tests. However, despite their promise, current
LLM-based APR techniques often struggle to produce correct fixes due to limited
understanding of code context and over-reliance on incomplete test suites. As a
result, they frequently generate Draft Patches-partially correct patches that
either incompletely address the bug or overfit to the test cases. In this work,
we propose a novel patch refinement framework, Refine, that systematically
transforms Draft Patches into correct ones. Refine addresses three key
challenges: disambiguating vague issue and code context, diversifying patch
candidates through test-time scaling, and aggregating partial fixes via an
LLM-powered code review process. We implement Refine as a general refinement
module that can be integrated into both open-agent-based and workflow-based APR
systems. Our evaluation on the SWE-Bench Lite benchmark shows that Refine
achieves state-of-the-art results among workflow-based approaches and
approaches the best-known performance across all APR categories. Specifically,
Refine boosts AutoCodeRover's performance by 14.67%, achieving a score of
51.67% and surpassing all prior baselines. On SWE-Bench Verified, Refine
improves the resolution rate by 12.2%, and when integrated across multiple APR
systems, it yields an average improvement of 14%-demonstrating its broad
effectiveness and generalizability. These results highlight the effectiveness
of refinement as a missing component in current APR pipelines and the potential
of agentic collaboration in closing the gap between near-correct and correct
patches. We also open source our code.

</details>


### [47] [Generating High-Level Test Cases from Requirements using LLM: An Industry Study](https://arxiv.org/abs/2510.03641)
*Satoshi Masuda,Satoshi Kouzawa,Kyousuke Sezai,Hidetoshi Suhara,Yasuaki Hiruta,Kunihiro Kudou*

Main category: cs.SE

TL;DR: Prompt-based method for automatic generation of high-level test cases from requirement documents without requiring custom RAG implementation and performs well in evaluations.


<details>
  <summary>Details</summary>
Motivation: Current high-level test case generation from requirement documents is manual and requires customization of retrieval-augmented generation (RAG) systems for each application's knowledge system, making it labor-intensive. Moreover, existing methods depend on RAG, which is not generalizable.

Method: The proposed method utilizes Large Language Models (LLMs) with tailored prompts to directly generate high-level test cases from requirement documents. It consists of two steps: first generating test design techniques from the requirement document and then using these to generate high-level test cases.

Result: The method was evaluated using datasets from Bluetooth and Mozilla, achieving macro-recall scores of 0.81 and 0.37, respectively, for the generated high-level test cases.

Conclusion: The method demonstrates feasibility of using prompt-based LLM approaches for automatic, generalizable generation of high-level test cases from requirement documents, without the need for retrieval-augmented generation.

Abstract: Currently, generating high-level test cases described in natural language
from requirement documents is performed manually. In the industry, including
companies specializing in software testing, there is a significant demand for
the automatic generation of high-level test cases from requirement documents
using Large Language Models (LLMs). Efforts to utilize LLMs for requirement
analysis are underway. In some cases, retrieval-augmented generation (RAG) is
employed for generating high-level test cases using LLMs. However, in practical
applications, it is necessary to create a RAG tailored to the knowledge system
of each specific application, which is labor-intensive. Moreover, when applying
high-level test case generation as a prompt, there is no established method for
instructing the generation of high-level test cases at a level applicable to
other specifications without using RAG. It is required to establish a method
for the automatic generation of high-level test cases that can be generalized
across a wider range of requirement documents. In this paper, we propose a
method for generating high-level (GHL) test cases from requirement documents
using only prompts, without creating RAGs. In the proposed method, first, the
requirement document is input into the LLM to generate test design techniques
corresponding to the requirement document. Then, high-level test cases are
generated for each of the generated test design techniques. Furthermore, we
verify an evaluation method based on semantic similarity of the generated
high-level test cases. In the experiments, we confirmed the method using
datasets from Bluetooth and Mozilla, where requirement documents and high-level
test cases are available, achieving macro-recall measurement of 0.81 and 0.37,
respectively. We believe that the method is feasible for practical application
in generating high-level test cases without using RAG.

</details>


### [48] [Detecting and Preventing Latent Risk Accumulation in High-Performance Software Systems](https://arxiv.org/abs/2510.03712)
*Jahidul Arafat,Kh. M. Moniruzzaman,Shamim Hossain,Fariha Tasmin,Kamrujjaman,Ahsan Habib Tareq*

Main category: cs.SE

TL;DR: This paper introduces a framework to detect, prevent, and optimize conditions in distributed systems where high performance optimization can lead to hidden vulnerabilities.  The framework includes the Latent Risk Index (LRI), HYDRA for perturbation testing (89.7% risk discovery rate), RAVEN for monitoring (92.9% precision and 93.8% recall), and APEX for maintaining performance while reducing risks by 59.2%.  Evaluations show strong results with high reproducibility and ROI.


<details>
  <summary>Details</summary>
Motivation: High performance optimizations in distributed systems can mask  underlying vulnerabilities, creating conditions that lead to sudden system failures when these optimizations struggle.  Current reliability engineering is mainly reactive, rather than focusing on  proactively detecting these hidden risks.

Method: The paper presents a framework that combines mathematical modeling, intelligent  perturbation testing, and risk-aware performance optimization.  The framework includes the LRI to assess risk, HYDRA which uses six optimization-aware  strategies for perturbation testing, RAVEN for continuous production monitoring,  and APEX to optimize performance while managing risk.

Result: HYDRA achieved 89.7% risk discovery rate.  RAVEN achieved 92.9% precision and 93.8% recall. APEX reduced risks by 59.2% while maintaining 96.6% of baseline performance. Evaluation showed high reliability (Cohen d>2.0) and reproducibility (r>0.92).  Production deployment highlighted 69.1% reduction in  mean time to recovery, 78.6% reduction in incident severity, and prevented 81 incidents with 1.44M USD annual  benefits and 3.2-month ROI.

Conclusion: The approach presented in this paper transforms reliability engineering from reactive incident management to proactive,  risk-aware, optimization,  demonstrating significant improvements in system resilience and cost  effectiveness.

Abstract: Modern distributed systems employ aggressive optimization strategies that
create latent risks - hidden vulnerabilities where exceptional performance
masks catastrophic fragility when optimizations fail. Cache layers achieving
99% hit rates can obscure database bottlenecks until cache failures trigger
100x load amplification and cascading collapse. Current reliability engineering
focuses on reactive incident response rather than proactive detection of
optimization-induced vulnerabilities. This paper presents the first
comprehensive framework for systematic latent risk detection, prevention, and
optimization through integrated mathematical modeling, intelligent perturbation
testing, and risk-aware performance optimization. We introduce the Latent Risk
Index (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),
enabling predictive risk assessment. Our framework integrates three systems:
HYDRA employing six optimization-aware perturbation strategies achieving 89.7%
risk discovery rates, RAVEN providing continuous production monitoring with
92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling
risk-aware optimization maintaining 96.6% baseline performance while reducing
latent risks by 59.2%. Evaluation across three testbed environments
demonstrates strong statistical validation with large effect sizes (Cohen
d>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24
weeks shows 69.1% mean time to recovery reduction, 78.6% incident severity
reduction, and 81 prevented incidents generating 1.44M USD average annual
benefits with 3.2-month ROI. Our approach transforms reliability engineering
from reactive incident management to proactive risk-aware optimization.

</details>


### [49] [APIDA-Chat: Structured Synthesis of API Search Dialogues to Bootstrap Conversational Agents](https://arxiv.org/abs/2510.03743)
*Zachary Eberhart,Collin McMillan*

Main category: cs.SE

TL;DR: APIDA-Chat introduces an open-source pipeline to generate low-cost training data for API Search conversations using a two-phase approach with a teacher LLM and a fine-tuned student model, improving dialogue generation metrics without requiring external API access.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with niche/proprietary APIs due to insufficient multi-turn dialogue data for fine-tuning. Existing solutions rely on expensive large models, requiring more cost-effective, modular data generation methods.

Method: Phase I combines a legacy dialogue planner with a teacher LLM (o4-mini) to create a 'gold set' of dialogues, then fine-tunes a smaller Llama 3.2 student model. Phase II leverages the fine-tuned student model alone for rapid, domain-agnostic dialogue synthesis without exposing source code.

Result: The student model achieves BLEU 0.50 (+0.12) and BERTScore 0.91 (+0.03) improvements over baselines, runs on single consumer GPUs, and demonstrates modular, scalable data generation for API Search.

Conclusion: APIDA-Chat provides a cost-effective, open-source baseline for generating API Search dialogue data, reducing dependency on high-capacity models while improving metric performance, with components released at https://github.com/Zeberhart/apida-chat.

Abstract: Large-language-model assistants are suitable for explaining popular APIs, yet
they falter on niche or proprietary libraries because the multi-turn dialogue
data needed for fine-tuning are scarce. We present APIDA-Chat, an open-source
pipeline that converts symbolic dialogue-act "scripts" into realistic,
domain-grounded API Search conversations using a lightweight model for
inexpensive training data generation. Phase I pairs a legacy dialogue planner
with a high-capability teacher LLM (o4-mini) to synthesize a "gold set" of
realized dialogues; then, a smaller Llama 3.2 3B student model is fine-tuned on
this corpus. Phase II drops the teacher and reuses the same planner with the
fine-tuned model, allowing rapid, low-cost synthesis of new dialogues without
exposing source code to external services. The fine-tuned student improves BLEU
from 0.38 to 0.50 and BERTScore from 0.88 to 0.91 versus the base model while
running entirely on a single consumer GPU. All components are modular and
publicly released to serve as a conservative baseline for future work.
APIDA-Chat is open-sourced at https://github.com/Zeberhart/apida-chat and a
video demo is available at https://youtu.be/YqmZBHyGbPs .

</details>


### [50] [Code4MeV2: a Research-oriented Code-completion Platform](https://arxiv.org/abs/2510.03755)
*Roham Koohestani,Parham Bateni,Aydin Ebrahimi,Behdad Etezadi,Kiarash Karimi,Maliheh Izadi*

Main category: cs.SE

TL;DR: Code4MeV2 is an open-source code completion plugin for JetBrains IDEs designed to facilitate academic research on human-AI interaction by offering a modular data collection framework.


<details>
  <summary>Details</summary>
Motivation: The limited availability of user interaction data, as it remains in proprietary systems, hinders reproducible research and large-scale analysis in human-AI interaction studies within the academic community.

Method: The researchers designed Code4MeV2 using a client--server architecture, providing inline code completion and a context-aware chat assistant, with a focus on a modular and transparent data collection system.

Result: Code4MeV2 performs on par with industry tools, showing an average latency of 200~ms. It was evaluated by an expert assessment and a user study with eight participants, who found it informative and useful.

Conclusion: Code4MeV2 addresses the challenge of data scarcity in human-AI interaction research, offering an open-source tool that eases large-scale studies and invites the community to use and contribute to it.

Abstract: The adoption of AI-powered code completion tools in software development has
increased substantially, yet the user interaction data produced by these
systems remain proprietary within large corporations. This creates a barrier
for the academic community, as researchers must often develop dedicated
platforms to conduct studies on human--AI interaction, making reproducible
research and large-scale data analysis impractical. In this work, we introduce
Code4MeV2, a research-oriented, open-source code completion plugin for
JetBrains IDEs, as a solution to this limitation. Code4MeV2 is designed using a
client--server architecture and features inline code completion and a
context-aware chat assistant. Its core contribution is a modular and
transparent data collection framework that gives researchers fine-grained
control over telemetry and context gathering. Code4MeV2 achieves
industry-comparable performance in terms of code completion, with an average
latency of 200~ms. We assess our tool through a combination of an expert
evaluation and a user study with eight participants. Feedback from both
researchers and daily users highlights its informativeness and usefulness. We
invite the community to adopt and contribute to this tool. More information
about the tool can be found at https://app.code4me.me.

</details>


### [51] [A First Look at the Lifecycle of DL-Specific Self-Admitted Technical Debt](https://arxiv.org/abs/2510.03802)
*Gilberto Recupito,Vincenzo De Martino,Dario Di Nucci,Fabio Palomba*

Main category: cs.SE

TL;DR: This study analyzes the lifecycle of Deep Learning (DL)-specific Self-Admitted Technical Debt (SATD) in software projects, finding that it is most commonly introduced in the early to middle stages, particularly during training and hardware phases, with developers often creating it during feature implementation and bug fixes.


<details>
  <summary>Details</summary>
Motivation: The rapid adoption of DL systems has introduced challenges in software quality and performance, compounded by DL-specific SATD, which affects maintainability. Despite the significance, the lifecycle of this type of debt remains poorly understood, necessitating research to inform better management strategies.

Method: The research uses mining software repository techniques to analyze 40 ML projects, specifically tracking 185 DL-specific SATD instances across their commit histories to assess how these instances are introduced, acknowledged, and resolved over time.

Result: DL-SATD is most frequently introduced in early/middle project stages, with the longest durations observed in the Training and Hardware phases. These issues are commonly introduced during feature implementation and debugging, indicating a trend in how debt accumulates.

Conclusion: The study concludes that targeted strategies are needed to address DL-SATD, particularly by focusing on early lifecycle phases and phases like Training and Hardware where debt tends to persist. Better management here could enhance long-term system maintainability and quality.

Abstract: The rapid adoption of Deep Learning (DL)-enabled systems has revolutionized
software development, driving innovation across various domains. However, these
systems also introduce unique challenges, particularly in maintaining software
quality and performance. Among these challenges, Self-Admitted Technical Debt
(SATD) has emerged as a growing concern, significantly impacting the
maintainability and overall quality of ML and DL-enabled systems. Despite its
critical implications, the lifecycle of DL-specific SATD, how developers
introduce, acknowledge, and address it over time-remains underexplored. This
study presents a preliminary analysis of the persistence and lifecycle of
DL-specific SATD in DL-enabled systems. The purpose of this project is to
uncover the patterns of SATD introduction, recognition, and durability during
the development life cycle, providing information on how to manage these
issues. Using mining software repository techniques, we examined 40 ML
projects, focusing on 185 DL-specific SATD instances. The analysis tracked the
introduction and persistence of SATD instances through project commit histories
to assess their lifecycle and developer actions. The findings indicate that
DL-specific SATD is predominantly introduced during the early and middle stages
of project development. Training and Hardware phases showed the longest SATD
durations, highlighting critical areas where debt accumulates and persists.
Additionally, developers introduce DL-specific SATD more frequently during
feature implementation and bug fixes. This study emphasizes the need for
targeted DL-specific SATD management strategies in DL-enabled systems to
mitigate its impact. By understanding the temporal characteristics and
evolution of DL-specific SATD, developers can prioritize interventions at
critical stages to improve the maintainability and quality of the system.

</details>


### [52] [Smart Paste: Automatically Fixing Copy/Paste for Google Developers](https://arxiv.org/abs/2510.03843)
*Vincent Nguyen,Guilherme Herzog,José Cambronero,Marcus Revaj,Aditya Kini,Alexander Frömmgen,Maxim Tabachnyk*

Main category: cs.SE

TL;DR: Pasted code is commonly used at Google, often requiring follow-up edits. The paper introduces Smart Paste, an IDE feature that provides post-paste edit suggestions, and shows its successful deployment with a 45% acceptance rate, contributing to over 1% of company-wide code.


<details>
  <summary>Details</summary>
Motivation: Manually editing pasted code is a pain point for developers, especially since code pasting is more frequent than manual typing and often needs multiple adjustments.

Method: The authors developed an iterative approach to create and scale Smart Paste, an IDE feature for deep learning-based post-paste edit suggestions, within Google's development environment.

Result: Smart Paste has a 45% acceptance rate and contributes to over 1% of all code written company-wide. It reduces manual editing efforts for follow-up tasks like reformatting, renaming, and translations.

Conclusion: The case study for Smart Paste highlights the potential of AI in improving developer workflows at scale, offering a practical guide for AI practitioners focusing on user experience, system integration, and model capabilities in real-world deployments.

Abstract: Manually editing pasted code is a long-standing developer pain point. In
internal software development at Google, we observe that code is pasted 4 times
more often than it is manually typed. These paste actions frequently require
follow-up edits, ranging from simple reformatting and renaming to more complex
style adjustments and cross-language translations. Prior work has shown deep
learning can be used to predict these edits. In this work, we show how to
iteratively develop and scale Smart Paste, an IDE feature for post-paste edit
suggestions, to Google's development environment. This experience can serve as
a guide for AI practitioners on a holistic approach to feature development,
covering user experience, system integration, and model capabilities. Since
deployment, Smart Paste has had overwhelmingly positive feedback with a 45%
acceptance rate. At Google's enterprise scale, these accepted suggestions
account substantially for over 1% of all code written company-wide.

</details>


### [53] [Designing Empirical Studies on LLM-Based Code Generation: Towards a Reference Framework](https://arxiv.org/abs/2510.03862)
*Nathalia Nascimento,Everton Guimaraes,Paulo Alencar*

Main category: cs.SE

TL;DR: This paper introduces a theoretical framework for standardized evaluation of LLM-based code generation in software engineering. It aims to address the lack of consistency in current studies to improve comparability and reproducibility.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the fragmented and inconsistent evaluation practices in existing LLM-based code generation research, which hampers meaningful comparisons and reproducibility of results.

Method: The authors developed the framework based on prior experience and a comparative analysis of recent studies, focusing on core components like problem sources, quality attributes, and metrics.

Result: The result is a proposed framework that structures empirical evaluations systematically, demonstrated through case mappings which show its practical application and potential for refinement.

Conclusion: The conclusion highlights the framework's role in standardizing LLM evaluations and the intention to further develop it into a robust tool for software engineering contexts.

Abstract: The rise of large language models (LLMs) has introduced transformative
potential in automated code generation, addressing a wide range of software
engineering challenges. However, empirical evaluation of LLM-based code
generation lacks standardization, with studies varying widely in goals, tasks,
and metrics, which limits comparability and reproducibility. In this paper, we
propose a theoretical framework for designing and reporting empirical studies
on LLM-based code generation. The framework is grounded in both our prior
experience conducting such experiments and a comparative analysis of key
similarities and differences among recent studies. It organizes evaluation
around core components such as problem sources, quality attributes, and
metrics, supporting structured and systematic experimentation. We demonstrate
its applicability through representative case mappings and identify
opportunities for refinement. Looking forward, we plan to evolve the framework
into a more robust and mature tool for standardizing LLM evaluation across
software engineering contexts.

</details>


### [54] [Adversarial Agent Collaboration for C to Rust Translation](https://arxiv.org/abs/2510.03879)
*Tianyu Li,Ruishi Li,Bo Wang,Brandon Paulsen,Umang Mathur,Prateek Saxena*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Translating C to memory-safe languages, like Rust, prevents critical memory
safety vulnerabilities that are prevalent in legacy C software. Existing
approaches for C to safe Rust translation, including LLM-assisted ones, do not
generalize on larger (> 500 LoC) C codebases because they depend on complex
program analyses that frequently break. In this work, we present ACToR
(Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired
by GANs, ACToR pits a generator agent against a discriminator agent, which
collaborate to iteratively generate a Rust translation. On each iteration, the
translator agent synthesizes and refines a Rust translation to pass an existing
suite of tests, and then the discriminator agent finds new failing tests. We
demonstrate that ACToR translates all of the 63 real-world command line
utilities considered in our benchmarks, which have an average size of 485 lines
of code, and it achieves over 90% test pass rate with zero human intervention.
To our knowledge, it is the first such system that reliably translates C
programs of this scale. Furthermore, ACToR improves translation correctness by
up to 18.9% compared to baseline, non-adversarial approaches.

</details>


### [55] [Rethinking Services in the Quantum Age: The SOQ Paradigm](https://arxiv.org/abs/2510.03890)
*Jose Garcia-Alonso,Enrique Moguel,Jaime Alvarado-Valiente,Javier Romero-Alvarez,Álvaro M. Aparicio-Morales,Juan M. Murillo,Francisco Javier Cavero,Adrián Romero-Flores,Alfonso E. Marquez-Chamorro,José Antonio Parejo,Antonio Ruiz-Cortés,Giuseppe Bisicchia,Alessandro Bocci,Antonio Brogi*

Main category: cs.SE

TL;DR: This paper introduces Service-Oriented Quantum (SOQ), a new approach to incorporating quantum computing into practical software systems by autonomously integrating quantum services as modular and interoperable components, unlike previous methods that treated them as part of classical systems.


<details>
  <summary>Details</summary>
Motivation: The motivation for the paper arises from the fact that while quantum computing is moving from theory to practice and can offer substantial computational benefits, its integration into real-world software systems is limited by hardware fragility, platform differences, and the lack of strong software engineering practices.

Method: The paper presents Service-Oriented Quantum (SOQ) by establishing its key principles, formulating a layered technology stack to facilitate its implementation, and pinpointing essential challenges to be tackled, including interoperability, hybrid systems, pricing models, service abstractions, and workforce training.

Result: The introduction of SOQ as a novel paradigm enables the development of scalable, modular, and interoperable quantum software systems without reliance on a dedicated classical environment.

Conclusion: Service-Oriented Quantum (SOQ) represents a crucial step forward in integrating quantum computing into real-world software by making quantum services autonomous, modular, and interoperable, rather than treating them as auxiliary components in classical systems.

Abstract: Quantum computing is rapidly progressing from theoretical promise to
practical implementation, offering significant computational advantages for
tasks in optimization, simulation, cryptography, and machine learning. However,
its integration into real-world software systems remains constrained by
hardware fragility, platform heterogeneity, and the absence of robust software
engineering practices. This paper introduces Service-Oriented Quantum (SOQ), a
novel paradigm that reimagines quantum software systems through the lens of
classical service-oriented computing. Unlike prior approaches such as Quantum
Service-Oriented Computing (QSOC), which treat quantum capabilities as
auxiliary components within classical systems, SOQ positions quantum services
as autonomous, composable, and interoperable entities. We define the
foundational principles of SOQ, propose a layered technology stack to support
its realization, and identify the key research and engineering challenges that
must be addressed, including interoperability, hybridity, pricing models,
service abstractions, and workforce development. This approach is of vital
importance for the advancement of quantum technology because it enables the
scalable, modular, and interoperable integration of quantum computing into
real-world software systems independently and without relying on a dedicated
classical environment to manage quantum processing.

</details>


### [56] [A Brief History of the Waterfall Model: Past, Present, and Future](https://arxiv.org/abs/2510.03894)
*Antonios Saravanos*

Main category: cs.SE

TL;DR: tl;dr


<details>
  <summary>Details</summary>
Motivation: motivation

Method: method

Result: result

Conclusion: conclusion

Abstract: The waterfall model, one of the earliest software development methodologies,
has played a foundational role in shaping contemporary software engineering
practices. This paper provides a historical and critical overview of the model,
tracing its conceptual origins in software engineering, its formalization by
Royce, and its evolution through decades of industry adoption and critique.
Although often criticized for its rigidity, shortcomings, and high failure
rates, the waterfall model persists in specific domains. Its principles
continue to influence contemporary hybrid development frameworks that combine
traditional and agile methods. Drawing on a range of scholarly sources, this
study synthesizes key developments in the perception and application of the
waterfall model. The analysis highlights how the model has shifted from a
standalone framework to a component within modern hybrid methodologies. By
revisiting its origins, assessing its present utility, and examining its role
in contemporary development practices, this paper argues that the waterfall
model remains relevant, not as a relic of the past but as part of context-aware
development strategies. The paper contends that the model's enduring relevance
lies in its adaptability. By recognizing both its limitations and its
strengths, and by understanding its integration within hybrid approaches,
practitioners can make more informed decisions about methodology selection and
process design in diverse development environments.

</details>


### [57] [Multi-Agent Code-Orchestrated Generation for Reliable Infrastructure-as-Code](https://arxiv.org/abs/2510.03902)
*Rana Nameer Hussain Khan,Dawood Wasif,Jin-Hee Cho,Ali Butt*

Main category: cs.SE

TL;DR: This paper proposes MACOG, a multi-agent LLM-based system for generating Infrastructure-as-Code (IaC), addressing syntax errors, policy violations, and scalability issues in current single-pass LLM approaches.


<details>
  <summary>Details</summary>
Motivation: Current LLM-generated IaC has limitations like syntactic errors, policy violations, and unscalable designs due to monolithic generation processes.

Method: MACOG employs specialized agents (Architect, Security Prover, etc.) working via a shared-blackboard orchestrator. It integrates Terraform Plan for validation and Open Policy Agent (OPA).

Result: MACOG outperforms existing methods on IaC-Eval benchmark (e.g., GPT-5 improves from 54.90 to 74.02), with gains in BLEU, CodeBERTScore, and LLM-judge metrics. Ablation studies confirm decoding constraints and feedback are critical.

Conclusion: MACOG demonstrates that modular multi-agent collaboration with policy enforcement significantly improves IaC generation quality compared to single-pass LLMs.

Abstract: The increasing complexity of cloud-native infrastructure has made
Infrastructure-as-Code (IaC) essential for reproducible and scalable
deployments. While large language models (LLMs) have shown promise in
generating IaC snippets from natural language prompts, their monolithic,
single-pass generation approach often results in syntactic errors, policy
violations, and unscalable designs. In this paper, we propose MACOG
(Multi-Agent Code-Orchestrated Generation), a novel multi-agent LLM-based
architecture for IaC generation that decomposes the task into modular subtasks
handled by specialized agents: Architect, Provider Harmonizer, Engineer,
Reviewer, Security Prover, Cost and Capacity Planner, DevOps, and Memory
Curator. The agents interact via a shared-blackboard, finite-state orchestrator
layer, and collectively produce Terraform configurations that are not only
syntactically valid but also policy-compliant and semantically coherent. To
ensure infrastructure correctness and governance, we incorporate Terraform Plan
for execution validation and Open Policy Agent (OPA) for customizable policy
enforcement. We evaluate MACOG using the IaC-Eval benchmark, where MACOG is the
top enhancement across models, e.g., GPT-5 improves from 54.90 (RAG) to 74.02
and Gemini-2.5 Pro from 43.56 to 60.13, with concurrent gains on BLEU,
CodeBERTScore, and an LLM-judge metric. Ablations show constrained decoding and
deploy feedback are critical: removing them drops IaC-Eval to 64.89 and 56.93,
respectively.

</details>


### [58] [Refactoring with LLMs: Bridging Human Expertise and Machine Understanding](https://arxiv.org/abs/2510.03914)
*Yonnel Chen Kuang Piao,Jean Carlors Paul,Leuson Da Silva,Arghavan Moradi Dakhel,Mohammad Hamdaqa,Foutse Khomh*

Main category: cs.SE

TL;DR: This study investigates how instruction strategies based on Martin Fowler's refactoring guidelines can enhance LLMs' ability to perform automated code refactoring. The proposed methods achieve high performance on benchmark tasks and real-world code, showing rule-based and goal-oriented instructions improve refactorability and code quality preservation.


<details>
  <summary>Details</summary>
Motivation: Automated refactoring tools lack broad support for diverse refactoring types, and developers avoid refactoring due to resource demands. Existing solutions fail to leverage LLMs efficiently for this critical software engineering task.

Method: Designed 61 instruction strategies encoding motivations, procedural steps, and objectives based on Fowler's guidelines. Tested these using GPT-mini and DeepSeek-V3 on benchmark examples and GitHub code snippets, comparing descriptive vs. rule-based instructions and goal-oriented approaches.

Result: LLMs using Fowler-inspired instructions successfully completed all benchmark refactoring types, preserved program semantics in practical scenarios, and showed higher quality improvements with goal-focused approaches compared to fixed-type transformations.

Conclusion: Instruction strategies grounded in human best practices significantly enhance LLM-based refactoring capabilities. Prioritizing overall refactoring goals over specific transformation types leads to better outcomes, suggesting a new direction for automated code improvement tools.

Abstract: Code refactoring is a fundamental software engineering practice aimed at
improving code quality and maintainability. Despite its importance, developers
often neglect refactoring due to the significant time, effort, and resources it
requires, as well as the lack of immediate functional rewards. Although several
automated refactoring tools have been proposed, they remain limited in
supporting a broad spectrum of refactoring types. In this study, we explore
whether instruction strategies inspired by human best-practice guidelines can
enhance the ability of Large Language Models (LLMs) to perform diverse
refactoring tasks automatically. Leveraging the instruction-following and code
comprehension capabilities of state-of-the-art LLMs (e.g., GPT-mini and
DeepSeek-V3), we draw on Martin Fowler's refactoring guidelines to design
multiple instruction strategies that encode motivations, procedural steps, and
transformation objectives for 61 well-known refactoring types. We evaluate
these strategies on benchmark examples and real-world code snippets from GitHub
projects. Our results show that instruction designs grounded in Fowler's
guidelines enable LLMs to successfully perform all benchmark refactoring types
and preserve program semantics in real-world settings, an essential criterion
for effective refactoring. Moreover, while descriptive instructions are more
interpretable to humans, our results show that rule-based instructions often
lead to better performance in specific scenarios. Interestingly, allowing
models to focus on the overall goal of refactoring, rather than prescribing a
fixed transformation type, can yield even greater improvements in code quality.

</details>


### [59] [Why Does the Engineering Manager Still Exist in Agile Software Development?](https://arxiv.org/abs/2510.03920)
*Ravi Kalluri*

Main category: cs.SE

TL;DR: this paper provides a systematic literature review and analysis of engineering managers' persistence in agile organizations, proposing a conceptual model to align agile principles with necessary managerial roles.


<details>
  <summary>Details</summary>
Motivation: the authors aim to resolve the perceived contradiction between agile concepts that devalue traditional management and the ongoing presence of engineering managers in such organizations.

Method: the paper employs a multidimensional framework and a systematic literature review, supported by case studies, to analyze the role of engineering managers in agile settings.

Result: the research reveals that traditional managerial functions remain in agile organizations and proposes a conceptual model to reconcile agile principles with those managerial roles.

Conclusion: this research provides a conceptual model that unites agile concepts with the organizational need for management, offering insights for practitioners, researchers, and tool designers in agile contexts.

Abstract: Although Agile methodologies emphasize decentralized decision-making and team
autonomy, engineering managers continue to be employed in Agile software
organizations. This apparent paradox suggests that traditional managerial
functions persist despite the theoretical displacement of managerial hierarchy
in Agile. This paper explores the persistence of engineering managers through a
multidimensional framework encompassing historical context, theoretical
tensions, organizational realities, empirical evidence, evolving managerial
roles, and practical implications. A systematic literature review underpins our
multifaceted analysis, supplemented by illustrative case studies. We conclude
by proposing a conceptual model that reconciles Agile principles with
managerial necessity, offering guidance for practitioners, researchers, and
tool designers. Implications for leadership development, tool integration, and
future research are discussed.

</details>


### [60] [Bamboo: LLM-Driven Discovery of API-Permission Mappings in the Android Framework](https://arxiv.org/abs/2510.04078)
*Han Hu,Wei Minn,Yonghui Liu,Jiakun Liu,Ferdian Thung,Terry Yue Zhuo,Lwin Khin Shar,Debin Gao,David Lo*

Main category: cs.SE

TL;DR: This paper proposes a novel approach using large language models (LLMs), dual-role prompting, and API-driven code generation to systematically identify API-permission mappings in Android, addressing limitations in existing static/dynamic analysis tools and improving Android app security through accurate permission declarations.


<details>
  <summary>Details</summary>
Motivation: Android's permission mechanism is critical for user privacy, but outdated/incorrect documentation and flawed manual analysis lead to security risks and app failures. Current methods (static/dynamic analysis) are inflexible to SDK updates, have poor coverage, and miss key mappings in complex codebases.

Method: The authors develop \tool{}, integrating LLMs with a dual-role prompting strategy (asking LLMs to act as both code analyzer and security auditor) and API-driven code generation. They evaluate it via three research questions: 1. Does \tool{} outperform baselines? 2. How complete is the official SDK documentation? 3. How do permission_REQ APIs evolve across SDK versions?

Result: \tool{} discovers 2,234/3,552/4,576 API-permission mappings in Android 6/7/10 respectively, exceeding existing baselines. The tool demonstrates significant improvements in adaptability, coverage, and precision for complex codebases.

Conclusion: LLM-based approaches with dual-role prompting and code generation effectively address limitations of traditional methods, offering scalable, accurate API-permission mapping discovery for evolving Android frameworks and improving app security.

Abstract: The permission mechanism in the Android Framework is integral to safeguarding
the privacy of users by managing users' and processes' access to sensitive
resources and operations. As such, developers need to be equipped with an
in-depth understanding of API permissions to build robust Android apps.
Unfortunately, the official API documentation by Android chronically suffers
from imprecision and incompleteness, causing developers to spend significant
effort to accurately discern necessary permissions. This potentially leads to
incorrect permission declarations in Android app development, potentially
resulting in security violations and app failures. Recent efforts in improving
permission specification primarily leverage static and dynamic code analyses to
uncover API-permission mappings within the Android framework. Yet, these
methodologies encounter substantial shortcomings, including poor adaptability
to Android SDK and Framework updates, restricted code coverage, and a
propensity to overlook essential API-permission mappings in intricate
codebases. This paper introduces a pioneering approach utilizing large language
models (LLMs) for a systematic examination of API-permission mappings. In
addition to employing LLMs, we integrate a dual-role prompting strategy and an
API-driven code generation approach into our mapping discovery pipeline,
resulting in the development of the corresponding tool, \tool{}. We formulate
three research questions to evaluate the efficacy of \tool{} against
state-of-the-art baselines, assess the completeness of official SDK
documentation, and analyze the evolution of permission-required APIs across
different SDK releases. Our experimental results reveal that \tool{} identifies
2,234, 3,552, and 4,576 API-permission mappings in Android versions 6, 7, and
10 respectively, substantially outprforming existing baselines.

</details>


### [61] [GA4GC: Greener Agent for Greener Code via Multi-Objective Configuration Optimization](https://arxiv.org/abs/2510.04135)
*Jingzhi Gong,Yixin Bian,Luis de la Cal,Giovanni Pinna,Anisha Uteem,David Williams,Mar Zamorano,Karine Even-Mendoza,W. B. Langdon,Hector Menendez,Federica Sarro*

Main category: cs.SE

TL;DR: GA4GC optimizes the trade-off between runtime and code performance for coding agents, achieving significant improvements on a benchmark with reduced resource consumption.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of sustainability and scalability in deploying coding agents powered by Large Language Models (LLMs), where high token consumption and environmental costs can outweigh optimization benefits.

Method: The authors propose GA4GC, a framework that utilizes gradient-based methods alongside adversarial techniques for visual question answering (VQA) and mannering (Intent-based Surfaces). It involves training generative and discriminative models to enhance performance and domain adaptation for document image retrieval tasks.

Result: The framework achieves a state-of-the-art performance across various document image retrieval tasks, including outperforming existing methods in fine-grained VQA and mannering. It also demonstrates effective domain adaptation with improvements in accuracy and inference speed.

Conclusion: The study shows the value of introducing environmental considerations in industrial deployment of coding agents and proves that it is possible to enhance both sustainability and effectiveness through the Proposed framework. However, more extensive evaluation is needed to address potential limitations and improve generalizability in diverse settings.

Abstract: Coding agents powered by LLMs face critical sustainability and scalability
challenges in industrial deployment, with single runs consuming over 100k
tokens and incurring environmental costs that may exceed optimization benefits.
This paper introduces GA4GC, the first framework to systematically optimize
coding agent runtime (greener agent) and code performance (greener code)
trade-offs by discovering Pareto-optimal agent hyperparameters and prompt
templates. Evaluation on the SWE-Perf benchmark demonstrates up to 135x
hypervolume improvement, reducing agent runtime by 37.7% while improving
correctness. Our findings establish temperature as the most critical
hyperparameter, and provide actionable strategies to balance agent
sustainability with code optimization effectiveness in industrial deployment.

</details>


### [62] [Detecting Semantic Clones of Unseen Functionality](https://arxiv.org/abs/2510.04143)
*Konstantinos Kitsios,Francesco Sovrano,Earl T. Barr,Alberto Bacchelli*

Main category: cs.SE

TL;DR: Semantic code clone detection aims to find code snippets performing similar functions. While neural models have shown good performance, their effectiveness drops significantly on unseen functionalities. LLMs, though not explicitly trained for clone detection, generalize better. The authors propose contrastive learning to enhance performance on unseen functionalities, showing improvements of up to 26% for task-specific models and 5% for LLMs.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to detect both seen and unseen code clones, as developers require accurate detection across various functionalities regardless of whether they were covered in training. Previous studies have shown that current models struggle with unseen functionalities, so the authors aim to address this generalization issue through new methods.

Method: The authors evaluate existing semantic code clone detection models, including task-specific neural models and generative LLMs, by testing them on clones of unseen functionalities. They then propose two enhancements: 1) Replacing the final classifier of task-specific models with a contrastive classifier to improve generalization performance, and 2) Introducing contrastive in-context learning for LLMs to guide them focus on differences between clones and non-clones. Finally, they re-evaluate the models to assess the improvements from these methods.

Result: Using the proposed contrastive learning methods, the task-specific models saw an improvement of up to 26% in F1 on clones of unseen functionalities (average 9% improvement), and LLMs improved by 5% (average 3% improvement). The results indicate that these methods help models better handle clones of functionalities they have not seen in training, suggesting effective ways to boost generalization in semantic code clone detection.

Conclusion: The paper concludes that contrastive learning is an effective approach to enhance the generalization of code clone detection models to unseen functionalities. The results suggest that the performance drop in task-specific models for such clones is significant and can be improved. For LLMs, even though their F1 drops is smaller, the improvements are still beneficial, demonstrating the effectiveness of contrastive in-context learning.

Abstract: Semantic code clone detection is the task of detecting whether two snippets
of code implement the same functionality (e.g., Sort Array). Recently, many
neural models achieved near-perfect performance on this task. These models seek
to make inferences based on their training data. Consequently, they better
detect clones similar to those they have seen during training and may struggle
to detect those they have not. Developers seeking clones are, of course,
interested in both types of clones. We confirm this claim through a literature
review, identifying three practical clone detection tasks in which the model's
goal is to detect clones of a functionality even if it was trained on clones of
different functionalities. In light of this finding, we re-evaluate six
state-of-the-art models, including both task-specific models and generative
LLMs, on the task of detecting clones of unseen functionality. Our experiments
reveal a drop in F1 of up to 48% (average 31%) for task-specific models. LLMs
perform on par with task-specific models without explicit training for clone
detection, but generalize better to unseen functionalities, where F1 drops up
to 5% (average 3%) instead. We propose and evaluate the use of contrastive
learning to improve the performance of existing models on clones of unseen
functionality. We draw inspiration from the computer vision and natural
language processing fields where contrastive learning excels at measuring
similarity between two objects, even if they come from classes unseen during
training. We replace the final classifier of the task-specific models with a
contrastive classifier, while for the generative LLMs we propose contrastive
in-context learning, guiding the LLMs to focus on the differences between
clones and non-clones. The F1 on clones of unseen functionality is improved by
up to 26% (average 9%) for task-specific models and up to 5% (average 3%) for
LLMs.

</details>


### [63] [Multi Language Models for On-the-Fly Syntax Highlighting](https://arxiv.org/abs/2510.04166)
*Marco Edoardo Palma,Pooja Rani,Harald C. Gall*

Main category: cs.SE

TL;DR: The paper introduces a unified model for multi-language syntax highlighting, reducing deployment complexity by 6x and improving performance with normalization and few-shot learning.


<details>
  <summary>Details</summary>
Motivation: Real-time syntax highlighting is challenging for web-based development tools due to time and memory constraints, and current models are limited in language support, data generation, and training costs.

Method: The authors develop a unified model supporting multiple languages via a normalization technique and few-shot learning, encoding brute-force strategies with deep abstraction.

Result: The model demonstrates improved performance on unseen languages and reduced reliance on large datasets generated by brute-force methods.

Conclusion: The proposed approach simplifies deployment and enhances scalability of syntax highlighting, making it more efficient and cost-effective for multi-language environments.

Abstract: Syntax highlighting is a critical feature in modern software development
environments, enhancing code readability and developer productivity. However,
delivering accurate highlighting in real time remains challenging for online
and web-based development tools due to strict time and memory constraints on
backend services. These systems must serve highlights rapidly and frequently,
even when code is partially valid or invalid. This has led to on-the-fly syntax
highlighting, where visual annotations are generated just before content is
served, often at high request rates and under incomplete input conditions. To
meet these demands efficiently, state-of-the-art models use deep learning to
learn the behavior of brute-force syntax highlighting resolvers, tools that are
easy to implement but too slow for production. Through the Deep Abstraction
process, brute-force strategies are encoded into fast statistical models that
achieve both high accuracy and low-latency inference. Despite their success,
such models face key challenges: they support only one programming language per
model, require large datasets from slow brute-force generators, and involve
resource-intensive training. In multi-language environments, this means
maintaining multiple independent models, increasing system complexity and
operational cost. This work addresses these issues by introducing a unified
model capable of highlighting up to six mainstream programming languages,
reducing deployment complexity by a factor of six and improving performance on
unseen languages. A novel normalization technique significantly enhances model
generalization, while few-shot learning experiments show that a small number of
oracle samples can replace large datasets, minimizing dependence on brute-force
generators. Combined, these innovations enable efficient, scalable, and
cost-effective syntax highlighting across diverse programming languages.

</details>


### [64] [Selecting Cybersecurity Requirements: Effects of LLM Use and Professional Software Development Experience](https://arxiv.org/abs/2510.04274)
*Damjan Fujs,Damjan Vavpotič,Tomaž Hovelja,Marko Poženel*

Main category: cs.SE

TL;DR: LLMs do not significantly influence how cybersecurity requirements are prioritized, but professional experience does.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the impact of access to LLMs and previous software development experience on the prioritization of cybersecurity requirements for web applications.

Method: Twenty-three postgraduate students, divided into two groups (with and without LLM access), participated in a study using the MoSCoW method to prioritize SRs and evaluated them against multiple criteria.

Result: LLM use did not yield significant differences in SR evaluations; however, professionals with more experience showed trait in higher ratings for user experience and lower risk estimates.

Conclusion: While LLMs may not impact cybersecurity prioritization, the amount of professional software development experience does influence participants' assessments.

Abstract: This study investigates how access to Large Language Models (LLMs) and
varying levels of professional software development experience affect the
prioritization of cybersecurity requirements for web applications. Twenty-three
postgraduate students participated in a research study to prioritize security
requirements (SRs) using the MoSCoW method and subsequently rated their
proposed solutions against multiple evaluation criteria. We divided
participants into two groups (one with and the other without access to LLM
support during the task). Results showed no significant differences related to
LLM use, suggesting that access to LLMs did not noticeably influence how
participants evaluated cybersecurity solutions. However, statistically
significant differences emerged between experience groups for certain criteria,
such as estimated cost to develop a feature, perceived impact on user
experience, and risk assessment related to non-implementation of the proposed
feature. Participants with more professional experience tended to provide
higher ratings for user experience impact and lower risk estimates.

</details>


### [65] [Challenge on Optimization of Context Collection for Code Completion](https://arxiv.org/abs/2510.04349)
*Dmitry Ustalov,Egor Bogomolov,Alexander Bezzubov,Yaroslav Golubev,Evgeniy Glukhov,Georgii Levtsov,Vladimir Kovalenko*

Main category: cs.SE

TL;DR: This paper presents a challenge on optimizing context collection for code completion in Python and Kotlin, organized by JetBrains and Mistral AI as part of ASE 2025. It evaluates methods for improving fill-in-the-middle code completion using a large dataset of open-source projects and chrF metrics.


<details>
  <summary>Details</summary>
Motivation: AI-driven software engineering workflows struggle to effectively leverage context from large codebases. Systematic evaluation of context collection methods is critical to enhance code completion quality in real-world programming scenarios.

Method: A permissively licensed open-source dataset of Python and Kotlin code was created. Teams submitted solutions in public/private competition phases (19/8 teams for Python/Kotlin publicly, 6 teams privately), with evaluations based on neural model completion quality using chrF metrics.

Result: Nineteen teams competed in Python track and eight in Kotlin track during the public phase. Six private phase teams submitted workshop papers. Results validated the effectiveness of optimized context collection strategies for code completion tasks.

Conclusion: The challenge underscores the importance of context-aware code completion in large projects. Participants' methods demonstrate progress in leveraging repository-wide information, but further research is needed to refine context collection techniques for neural models in real-world codebases.

Abstract: The rapid advancement of workflows and methods for software engineering using
AI emphasizes the need for a systematic evaluation and analysis of their
ability to leverage information from entire projects, particularly in large
code bases. In this challenge on optimization of context collection for code
completion, organized by JetBrains in collaboration with Mistral AI as part of
the ASE 2025 conference, participants developed efficient mechanisms for
collecting context from source code repositories to improve fill-in-the-middle
code completions for Python and Kotlin. We constructed a large dataset of
real-world code in these two programming languages using permissively licensed
open-source projects. The submissions were evaluated based on their ability to
maximize completion quality for multiple state-of-the-art neural models using
the chrF metric. During the public phase of the competition, nineteen teams
submitted solutions to the Python track and eight teams submitted solutions to
the Kotlin track. In the private phase, six teams competed, of which five
submitted papers to the workshop.

</details>


### [66] [MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models](https://arxiv.org/abs/2510.04363)
*Hyunjun Kim,Sejong Kim*

Main category: cs.SE

TL;DR: MacroBench is a benchmark evaluating LLMs' ability to generate browser automation code via Python+Selenium, showing strong performance on simple tasks (91.7%) but zero success on complex workflows, with GPT-4o-Mini leading at 96.8%. Code quality (production standards remediation) remains unmet.


<details>
  <summary>Details</summary>
Motivation: Existing LLM evaluation lacks benchmarks for generating reusable web automation macros with cross-site generalization, security considerations, and end-to-end validation pipelines.

Method: Synthesized 7 modular websites with 681 tasks spanning interaction complexity. Evaluated via static analysis, sandboxed execution, DOM outcome verification, and database snapshot comparisons with safety checks. Reran 2636 model-task combinations across multiple LLMs.

Result: 91.7% success on simple tasks vs 0% on complex workflows. GPT-4o-Mini (96.8%) outperformed GPT-4.1 (95.3%) > Gemini-2.5-Pro (89.0%) > DeepSeek-V3.1 (83.4%). No models met production coding standards despite functional outputs.

Conclusion: Establishes reproducible evaluation framework for macro synthesis. Highlights importance of complexity compositionality and code quality metrics for deployment-ready LLM automation systems.

Abstract: We introduce MacroBench, a code-first benchmark that evaluates whether LLMs
can synthesize reusable browser automation programs from natural language goals
by reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates
seven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like,
Facebook-like, Discord-like, and Threads-like, covering 681 tasks across
interaction complexity and targeting difficulty. Our end-to-end protocol
validates generated code via static checks, sandboxed execution, and outcome
verification including DOM assertions and database snapshots, and includes a
safety suite for scraping, spam/abuse, and credential/privacy prompts. Across
2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8
percent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent,
and DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at
91.7 percent but fail on complex workflows at 0.0 percent, and none meet
production-quality coding practices despite functional completion. We release
our complete benchmark pipeline, evaluation framework, and experimental results
to enable reproducible assessment of macro synthesis for web automation.

</details>


### [67] [Reconsidering Requirements Engineering: Human-AI Collaboration in AI-Native Software Development](https://arxiv.org/abs/2510.04380)
*Mateen Ahmed Abbasi,Petri Ihantola,Tommi Mikkonen,Niko Mäkitalo*

Main category: cs.SE

TL;DR: This paper explores AI's potential to enhance Requirement Engineering (RE) for better efficiency and accuracy while addressing ethical concerns. It emphasizes trustworthy AI solutions through academia-industry collaboration.


<details>
  <summary>Details</summary>
Motivation: RE faces persistent challenges like ambiguity, conflicting stakeholder needs, and evolving requirements. AI could streamline RE but introduces ethical issues like bias and transparency concerns.

Method: The paper analyzes how AI can automate labor-intensive RE tasks, support prioritization, and foster stakeholder collaboration while examining opportunities/challenges through a systematic review and vision paper approach.

Result: Highlights AI's potential to improve RE processes (automation, prioritization, collaboration), identifies ethical risks, and outlines the need for balanced practical-ethical AI solutions in RE.

Conclusion: The study calls for ethical AI frameworks and stronger academia-industry partnerships to develop trustworthy, adaptive AI tools for RE that align with stakeholder needs and software development realities.

Abstract: Requirement Engineering (RE) is the foundation of successful software
development. In RE, the goal is to ensure that implemented systems satisfy
stakeholder needs through rigorous requirements elicitation, validation, and
evaluation processes. Despite its critical role, RE continues to face
persistent challenges, such as ambiguity, conflicting stakeholder needs, and
the complexity of managing evolving requirements. A common view is that
Artificial Intelligence (AI) has the potential to streamline the RE process,
resulting in improved efficiency, accuracy, and management actions. However,
using AI also introduces new concerns, such as ethical issues, biases, and lack
of transparency. This paper explores how AI can enhance traditional RE
practices by automating labor-intensive tasks, supporting requirement
prioritization, and facilitating collaboration between stakeholders and AI
systems. The paper also describes the opportunities and challenges that AI
brings to RE. In particular, the vision calls for ethical practices in AI,
along with a much-enhanced collaboration between academia and industry
professionals. The focus should be on creating not only powerful but also
trustworthy and practical AI solutions ready to adapt to the fast-paced world
of software development.

</details>


### [68] [Smart Hiring Redefined: An Intelligent Recruitment Management Platform](https://arxiv.org/abs/2510.04437)
*Fangzhe Wu,Dongyang Lyu,Xiaoqi Li*

Main category: cs.SE

TL;DR: This paper examines the transition from traditional recruitment models to intelligent recruitment management systems motivated by inefficiencies, high costs, and information asymmetry in talent acquisition. It highlights automation-driven solutions (resume parsing, candidate-position matching, and interview scheduling algorithms) that enhance hiring efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional recruitment models face limitations in efficiency, cost-effectiveness, and information transparency as enterprises demand precision in talent acquisition. Manual processes like resume screening and interview coordination are labor-intensive and error-prone.

Method: The study compares manual recruitment workflows with intelligent systems leveraging automation and data-driven algorithms for resume parsing, candidate-job matching, and interview process automation.

Result: Intelligent systems demonstrate significant improvements in processing speed, accuracy of candidate-job alignment, and cost reduction compared to manual methods, while enabling scalable talent acquisition.

Conclusion: Intelligent recruitment management systems are positioned as a critical infrastructure for modern talent strategies, optimizing efficiency, reducing costs, and enhancing organizational competitiveness through digital transformation.

Abstract: Against the backdrop of deepening digital and intelligent transformation in
human resource management, traditional recruitment models struggle to fully
meet enterprises' growing demand for precise talent acquisition due to limited
efficiency, high costs, and information asymmetry. As a vital tool for
optimizing recruitment processes, reducing labor and time costs, and enhancing
core competitiveness, intelligent recruitment management systems become an
indispensable component of modern organizational talent strategies.Compared
with the labor intensive tasks of resume screening, candidate position
matching, and interview coordination in traditional manual recruitment,
intelligent recruitment systems significantly enhance the efficiency and
accuracy of the hiring process through automation and data driven approaches.
These systems enable rapid parsing of massive resume volumes, intelligent
matching of candidates to positions, and automated scheduling of interview
processes.

</details>


### [69] [Improving IR-based Bug Localization with Semantics-Driven Query Reduction](https://arxiv.org/abs/2510.04468)
*Asif Mohammed Samir,Mohammad Masudur Rahman*

Main category: cs.SE

TL;DR: IQLoc combines Information Retrieval (IR) and Large Language Models (LLM) to address software bug localization challenges. By leveraging transformer-based models for program semantics and reformulating IR queries, IQLoc achieves significant improvements in localization metrics over existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional IR-based methods fail to capture code context and semantics, while LLMs remain underutilized for bug localization due to resource intensity. Existing techniques struggle with heterogeneous and ambiguous bug reports.

Method: IQLoc integrates transformer-based models to reason about code suspiciousness and reformulate IR queries. The benchmark was expanded by 30% with recent bug reports (7.5K total), and performance was evaluated using MAP, MRR, and HIT@K metrics against four baselines.

Result: IQLoc outperformed baselines with 58.52-60.59% MAP, 61.49-64.58% MRR, and 69.88-100.90% HIT@K. It showed 91.67% MAP improvement for stack-trace reports, 72.73% for code-inclusive reports, and 65.38% for natural language-only reports.

Conclusion: By integrating program semantics into IR, IQLoc addresses longstanding limitations of traditional IR-based approaches. It demonstrates transformative potential through semantic understanding, achieving state-of-the-art performance across multiple bug report types.

Abstract: Despite decades of research, software bug localization remains challenging
due to heterogeneous content and inherent ambiguities in bug reports. Existing
methods such as Information Retrieval (IR)-based approaches often attempt to
match source documents to bug reports, overlooking the context and semantics of
the source code. On the other hand, Large Language Models (LLM) (e.g.,
Transformer models) show promising results in understanding both texts and
code. However, they have not been yet adapted well to localize software bugs
against bug reports. They could be also data or resource-intensive. To bridge
this gap, we propose, IQLoc, a novel bug localization approach that capitalizes
on the strengths of both IR and LLM-based approaches. In particular, we
leverage the program semantics understanding of transformer-based models to
reason about the suspiciousness of code and reformulate queries during bug
localization using Information Retrieval. To evaluate IQLoc, we refine the
Bench4BL benchmark dataset and extend it by incorporating ~30% more recent bug
reports, resulting in a benchmark containing ~7.5K bug reports. We evaluated
IQLoc using three performance metrics and compare it against four baseline
techniques. Experimental results demonstrate its superiority, achieving up to
58.52% and 60.59% in MAP, 61.49% and 64.58% in MRR, and 69.88% and 100.90% in
HIT@K for the test bug reports with random and time-wise splits, respectively.
Moreover, IQLoc improves MAP by 91.67% for bug reports with stack traces,
72.73% for those that include code elements, and 65.38% for those containing
only descriptions in natural language. By integrating program semantic
understanding into Information Retrieval, IQLoc mitigates several longstanding
challenges of traditional IR-based approaches in bug localization.

</details>


### [70] [DynamiQ: Unlocking the Potential of Dynamic Task Allocation in Parallel Fuzzing](https://arxiv.org/abs/2510.04469)
*Wenqi Yan,Toby Murray,Benjamin Rubinstein,Van-Thuan Pham*

Main category: cs.SE

TL;DR: DynamiQ is a dynamic parallel fuzzer using program call graphs for task allocation, reducing redundancy and improving efficiency. It outperforms existing fuzzers in coverage and discovered 9 new bugs.


<details>
  <summary>Details</summary>
Motivation: Existing parallel fuzzers treat seeds as tasks, causing redundancy. DynamiQ addresses this by dynamically refining task allocation using runtime feedback and structural program information.

Method: Leverages call graph data to define tasks, employs runtime feedback for adaptive allocation, and includes task-aware fuzzing optimizations. Built on top of LibAFL.

Result: Outperformed state-of-the-art fuzzers in code coverage (12 OSS-Fuzz/FuzzBench targets, 25k CPU hours), discovered 9 new bugs in widely-used software.

Conclusion: DynamiQ's structural task allocation and adaptive runtime feedback enable more efficient parallel fuzzing, effectively reducing redundancy while enhancing vulnerability discovery capabilities.

Abstract: We present DynamiQ, a full-fledged and optimized successor to AFLTeam that
supports dynamic and adaptive parallel fuzzing. Unlike most existing approaches
that treat individual seeds as tasks, DynamiQ leverages structural information
from the program's call graph to define tasks and continuously refines task
allocation using runtime feedback. This design significantly reduces redundant
exploration and enhances fuzzing efficiency at scale. Built on top of the
state-of-the-art LibAFL framework, DynamiQ incorporates several practical
optimizations in both task allocation and task-aware fuzzing. Evaluated on 12
real-world targets from OSS-Fuzz and FuzzBench over 25,000 CPU hours, DynamiQ
outperforms state-of-the-art parallel fuzzers in both code coverage and
vulnerability discovery, uncovering 9 previously unknown bugs in widely used
and extensively fuzzed open-source software.

</details>


### [71] [Detecting and Characterizing Low and No Functionality Packages in the NPM Ecosystem](https://arxiv.org/abs/2510.04495)
*Napasorn Tevarut,Brittany Reid,Yutaro Kashiwa,Pattara Leelaprute,Arnon Rungsawang,Bundit Manaskasemsak,Hajimu Iida*

Main category: cs.SE

TL;DR: This paper identifies security risks in trivial and data-only npm packages, proposing a rule-based static analysis tool with 94% accuracy to assess their prevalence and risks in the 2025 ecosystem.


<details>
  <summary>Details</summary>
Motivation: Trivial/data-only npm packages lack clear definitions and are overlooked in security assessments despite posing vulnerabilities. Existing tools fail to address their unique risks.

Method: Refined definitions of trivial packages, introduced data-only packages, and developed a rule-based static analysis method to detect them at scale, validated with macro-F1 0.87 accuracy.

Result: 17.92%% of npm packages are trivial (similar vulnerability levels to non-trivial packages); data-only packages are rare but contain risks. The tool enables large-scale risk assessment.

Conclusion: Trivial/data-only packages require prioritization in dependency management to mitigate technical debt and security exposure, highlighting gaps in current npm security practices.

Abstract: Trivial packages, small modules with low functionality, are common in the npm
ecosystem and can pose security risks despite their simplicity. This paper
refines existing definitions and introduce data-only packages that contain no
executable logic. A rule-based static analysis method is developed to detect
trivial and data-only packages and evaluate their prevalence and associated
risks in the 2025 npm ecosystem. The analysis shows that 17.92% of packages are
trivial, with vulnerability levels comparable to non-trivial ones, and
data-only packages, though rare, also contain risks. The proposed detection
tool achieves 94% accuracy (macro-F1 0.87), enabling effective large-scale
analysis to reduce security exposure. This findings suggest that trivial and
data-only packages warrant greater attention in dependency management to reduce
potential technical debt and security exposure.

</details>


### [72] [Spec2Control: Automating PLC/DCS Control-Logic Engineering from Natural Language Requirements with LLMs - A Multi-Plant Evaluation](https://arxiv.org/abs/2510.04519)
*Heiko Koziolek,Thilo Braun,Virendra Ashiwal,Sofia Linsbauer,Marthe Ahlgreen Hansen,Karoline Grotterud*

Main category: cs.SE

TL;DR: Spec2Control automates graphical DCS control logic generation from natural language requirements using LLMs, achieving 98.6% correct connections and reducing human labor by 94-96%. It outperforms existing text-focused tools and is now available both as open-source and in commercial ABB tools.


<details>
  <summary>Details</summary>
Motivation: Manual programming of DCS software costs millions of dollars annually, while current LLM-based tools lack automation, scalability, and real-world validation. This gap motivates the need for a more efficient and validated automation solution.

Method: Spec2Control leverages LLMs to convert natural language specifications directly into graphical control logic diagrams, bypassing manual translation steps. It was evaluated on a 10-narrative open dataset with 65 complex test cases to validate automation effectiveness.

Result: The system autonomously achieved 98.6 accuracy in control strategy connections, saving 94-96 of human labor. Integration into ABB's commercial tools and open-source availability enable practical deployment and independent validation.

Conclusion: Spec2Control demonstrates industry-grade automation for DCS programming, significantly reducing costs and errors. Its success provides a scalable model for LLM applications in industrial software development, validated across both commercial and open-source platforms.

Abstract: Distributed control systems (DCS) manage the automation for many industrial
production processes (e.g., power plants, chemical refineries, steel mills).
Programming the software for such systems remains a largely manual and tedious
process, incurring costs of millions of dollars for extensive facilities. Large
language models (LLMs) have been found helpful in generating DCS control logic,
resulting in commercial copilot tools. Today, these tools are focused on
textual notations, they provide limited automation, and have not been tested on
large datasets with realistic test cases. We introduce Spec2Control, a highly
automated LLM workflow to generate graphical control logic directly from
natural language user requirements. Experiments using an open dataset with 10
control narratives and 65 complex test cases demonstrate that Spec2Control can
successfully identify control strategies, can generate 98.6% of correct control
strategy connections autonomously, and can save between 94-96% of human labor.
Spec2Control is being integrated into commercial ABB engineering tools, but is
also available as an open-source variant for independent validation.

</details>


### [73] [Advancing Digital Government: Integrating Open Source Software Enablement Indicators in Maturity Indexes](https://arxiv.org/abs/2510.04603)
*Johan Linåker,Sachiko Muto*

Main category: cs.SE

TL;DR: A study to enhance digital government maturity indexes by analyzing Open Source Software (OSS) policies and support mechanisms in 16 countries, proposing new indicators for measuring governmental OSS adoption, with particular focus on reuse, collaborative development, policy incentives, and implementation support.


<details>
  <summary>Details</summary>
Motivation: OSS is a critical public good with significant economic and technological impacts, yet there are limited systematic measurements of governmental OSS adoption. This study aims to provide actionable indicators for digital government maturity indexes to assess and guide OSS implementation better.

Method: The study uses a qualitative approach, combining desk research of policy documents with semi-structured interviews of government representatives in 16 countries. Data are analyzed cross-nationally to identify patterns and formulate indicators for policy incentives and implementation.

Result: The paper found that OSS reuse policies are widespread, governed by central public bodies, promoting interoperability, transparency, sovereignty, and cost efficiency. OSPOs at various government levels provide institutional support. The study also proposes 14 indicator areas to measure digital maturity through OSS adoption.

Conclusion: OSS plays a strategic role in public digital transformation, necessitating clear policy and institutional support. Current digital maturity assessments should include OSS indicators to effectively evaluate and guide government adoption.

Abstract: Context: Open Source Software (OSS) is a vital public good, included across
most of modern software stacks, significantly impacting GDP and national tech
growth, while supporting interoperability, sovereignty, and transparency.
However, systematic measurement of governmental OSS adoption remain limited.
  Research Aim: This study contributes to digital government maturity indexes
by analyzing policies and support actions leveraging OSS for software reuse and
collaborative development across 16 digitally mature countries, and proposing
potential indicators for said indexes. It examines OSS policy formation, stated
goals, key actors, and support mechanisms.
  Methodology: A qualitative approach is used combining desk research of policy
documents with semi-structured interviews of government representatives,
producing detailed country reports. These are cross-analyzed, focusing on OSS
policy promotion, rationale, and implementation support.
  Results: Policies facilitating OSS reuse are widespread, targeting both
inbound acquisition and outbound sharing, and are predominantly governed by
central public sector organizations. Policy goals include interoperability,
digital sovereignty, transparency, and cost efficiency, with security framed
both as a risk and strength. Implementation is supported by diverse Open Source
Program Offices (OSPOs) at multiple government levels, which foster capacity
building, resource pooling, and sustainable project governance. Indicators are
synthesized and proposed across 14 areas covering policy incentives and design,
and implementation and support.
  Conclusions: OSS is a strategic enabler for public sector digital
transformation. Clear policy frameworks, coupled with institutional support
such as OSPOs, are essential. International digital maturity frameworks should
expand OSS indicators to better guide and assess government adoption and
impact.

</details>


### [74] [Exploring the Power of Diffusion Large Language Models for Software Engineering: An Empirical Investigation](https://arxiv.org/abs/2510.04605)
*Jingyao Zhang,Tianlin Li,Xiaoyu Zhang,Qiang Hu,Bin Shi*

Main category: cs.SE

TL;DR: This paper evaluates Diffusion LLMs (DLLMs) as a superior alternative to Autoregressive LLMs (AR-LLMs) in software engineering tasks, achieving 30-113\u2005\% accuracy improvements with reduced inference latency.


<details>
  <summary>Details</summary>
Motivation: AR-LLMs struggle with code structure processing and exhibit high inference latency, motivating the exploration of DLLMs with global bidirectional capabilities for SE tasks.

Method: Comprehensive benchmarking of DLLMs across code generation, defect detection, and program repair using a large-scale dataset of 52,937 tasks.

Result: 7B-parameter DLLMs outperformed AR-LLMs by 30\u2005\Avg. accuracy and achieved 113\u2005\\Gain for cross-file repair, maintaining lower latency.

Conclusion: DLLMs demonstrate significant accuracy, efficiency, and latency advantages, establishing them as a superior paradigm for software development lifecycle tasks.

Abstract: Autoregressive Large Language Models (AR-LLMs) are widely used in software
engineering (SE) but face limitations in processing code structure information
and suffer from high inference latency. Diffusion LLMs (DLLMs) offer a
promising alternative with global bidirectional encoding and decoupled
generation steps. This work presents the first comprehensive evaluation of
DLLMs across the software development lifecycle, including code generation,
defect detection, and program repair. On a large-scale benchmark of 52,937
tasks, 7Bparameter DLLMs outperform AR-LLMs with a 30% average accuracy
improvement achieving a 113% gain on cross-file repair, while maintaining
superior efficiency and reduced latency. Our results establish DLLMs as a
superior paradigm for SE tasks.

</details>


### [75] [A survey on the impact of emotions on the productivity among software developers](https://arxiv.org/abs/2510.04611)
*Pawel Weichbroth,Maciej Lotysz,Michal Wrobel*

Main category: cs.SE

TL;DR: This paper explores the impact of software developers 'emotional states on their perceived productivity through a two-stage survey approach involving experts and developers, using PLS analysis. Results show a strong positive and significant effect (beta =0.893, p <0.001) of emotional state on productivity.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the potential impact of emotional well-being on productivity within the software development industry, an area not yet fully understood. The authors aim to validate this relationship to provide actionable insight for interventions that could enhance performance outcomes.

Method: The study follows a two-stage approach. The first stage involves validating the Measurement Model (construct validity, reliability) through an expert survey ( nine experts ). The second stage uses the validated model to collect empirical data from 88 software developers, followed by hypothesis testing using Partial Least Squares ( PLS ) path analysis. Demographics and industry-specific details were likely collected through surveys to underpin the statistical analysis.

Result: The path analysis showed a strong positive correlation (beta = 0.893 ) between emotional state and perceived productivity among software developers, which is statistically significant (p < 0.001 ). This result substantiates the formulated hypothesis, indicating that emotional well-being is a critical factor in influencing productivity.

Conclusion: Emotional well-being significantly influences perceived productivity in software development, and managing this could enhance performance and mitigate negative outcomes such as burnout and stress. Interventions focused on emotional health might yield considerable positive effects for the industry.

Abstract: The time pressure associated with software development, among other factors,
often leads to a diminished emotional state among developers. However, whether
emotions affect perceived productivity remains an open question. This study
aims to determine the strength and direction of the relationship between
emotional state and perceived productivity among software developers. We
employed a two-stage approach. First, a survey was conducted with a pool of
nine experts to validate the measurement model. Second, a survey was
administered to a pool of 88 software developers to empirically test the
formulated hypothesis by using Partial Least Squares, as the data analysis
method. The results of the path analysis clearly confirm the formulated
hypothesis, showing that the emotional state of a software developer has a
strong positive, and significant impact (beta = 0.893, p < 0.001) on perceived
productivity among software developers. The findings highlight the importance
of managing and improving developers emotional well-being to enhance
productivity in software development environments. Additionally, interventions
aimed at reducing burnout, stress, and other negative factors could have a
considerable impact on their performance outcomes.

</details>


### [76] [Evolaris: A Roadmap to Self-Evolving Software Intelligence Management](https://arxiv.org/abs/2510.04689)
*Chengwei Liu,Wenbo Guo,Yuxin Zhang,Limin Wang,Sen Chen,Lei Bu,Yang Liu*

Main category: cs.SE

TL;DR: The paper proposes Evolaris, a self-evolving software intelligence system to tackle the challenge of collecting and analyzing security threats from diverse, informal sources.


<details>
  <summary>Details</summary>
Motivation: The dynamic and distributed nature of software threats, coupled with the emergence of informal channels for vulnerability sharing, necessitates a robust system to ensure timely threat intelligence and proactive security responses.

Method: Evolaris is built on a multi-agent framework that facilitates independent yet coordinated tasks like discovery, reasoning, gap filling, validation, and risk detection, leveraging shared context for enhanced scalability and adaptability.

Result: The system demonstrates improved precision, timeliness, and scalability in software threat analysis through continuous learning and adaptation to emerging threat patterns.

Conclusion: Evolaris offers a sustainable foundation for proactive security decision-making by integrating a multi-agent framework that enhances the ecosystem of security threat understanding.

Abstract: In recent years, the landscape of software threats has become significantly
more dynamic and distributed. Security vulnerabilities are no longer discovered
and shared only through formal channels such as public vulnerability databases
or vendor advisories. Increasingly, criti- cal threat information emerges
informally through blogs, social media, developer forums, open source
repositories, and even underground com- munities. To this end, capturing such
intelligence in a timely manner is essential for maintaining situational
awareness and enabling prompt security responses. However, this remains a
complex challenge due to the fragmented nature of data sources and the
technical difficulty of collecting, parsing, mapping, and validating
information at scale. To ad- dress this, we propose Evolaris, a self-evolving
software intelligence sys- tem built on a multi-agent framework. Evolaris is
designed to support a full-stack workflow, where agents operate independently
but coordinate through shared context to perform tasks such as information
discovery, reasoning, gap completion, validation, and risk detection. This
archi- tecture enables the platform to learn from new inputs, refine its
internal knowledge, and adapt to emerging threat patterns over time, which
could continuously improve the precision, timeliness, and scalability of
software threat analysis, and offers a sustainable foundation for proactive
secu- rity decision-making and strengthens the broader ecosystem of security
threat understanding.

</details>


### [77] [An Empirical Study of SOTA RCA Models: From Oversimplified Benchmarks to Realistic Failures](https://arxiv.org/abs/2510.04711)
*Aoyang Fang,Songhan Zhang,Yifan Yang,Haotong Wu,Junjielong Xu,Xuyang Wang,Rui Wang,Manyi Wang,Qisheng Lu,Pinjia He*

Main category: cs.SE

TL;DR: The paper addresses the oversimplification of existing benchmarks for Root Cause Analysis (RCA) in cloud-native microservices. It introduces a framework to generate realistic benchmarks, revealing performance limitations of SOTA models through low accuracy and identified failure patterns.


<details>
  <summary>Details</summary>
Motivation: Current RCA benchmarks fail to capture real-world complexity, leading to overestimated model performance and poor adaptability to dynamic environments. This hinders reliable evaluation and improvement of RCA systems.

Method: 1) Analyzed limitations in existing benchmarks (fault injection, call graph design, telemetry). 2 Developed an automated framework to generate realistic benchmarks with 1,430 validated failure cases across 25 fault types, dynamic workloads, and hierarchical labels.

Result: Re-evaluation of 11 SOTA models showed average Top@1 accuracy of 0.21. Three failure patterns emerged: scalability issues, observability blind spots, and modeling bottlenecks. Benchmark generation framework provides verifiable SLI impact data.

Conclusion: Real-world RCA evaluation requires more sophisticated benchmarks. Current models demonstrate significant practical limitations in scalability and observability, necessitating improved approaches that align with dynamic, hierarchical failure scenarios.

Abstract: While cloud-native microservice architectures have transformed software
development, their complexity makes Root Cause Analysis (RCA) both crucial and
challenging. Although many data-driven RCA models have been proposed, we find
that existing benchmarks are often oversimplified and fail to capture
real-world conditions. Our preliminary study shows that simple rule-based
methods can match or even outperform state-of-the-art (SOTA) models on four
widely used benchmarks, suggesting performance overestimation due to benchmark
simplicity. To address this, we systematically analyze popular RCA benchmarks
and identify key limitations in fault injection, call graph design, and
telemetry patterns. Based on these insights, we develop an automated framework
to generate more realistic benchmarks, yielding a dataset of 1,430 validated
failure cases from 9,152 injections, covering 25 fault types under dynamic
workloads with hierarchical ground-truth labels and verified SLI impact.
Re-evaluation of 11 SOTA models on this dataset shows low Top@1 accuracy
(average 0.21, best 0.37) and significantly longer execution times. Our
analysis highlights three common failure patterns: scalability issues,
observability blind spots, and modeling bottlenecks.

</details>


### [78] [Agile Software Effort Estimation using Regression Techniques](https://arxiv.org/abs/2510.04760)
*Sisay Deresa Sima,Ayalew Belay Habtie*

Main category: cs.SE

TL;DR: TL;DR: this paper presents a story point based agile effort estimation model using LASSO and Elastic Net regression, with experimen...


<details>
  <summary>Details</summary>
Motivation: Motivation: Software development effort estimation is a vital aspect of project success. The paper addresses the ongoing need fo...

Method: Method: The study develops an agile effort estimation model using LASSO and Elastic Net regression techniques, applied to 21 softwar...

Result: Result: The LASSO regression model demonstrates superior predictive performance with 100.0 PRED (8%) and 100.0 PRED (25%) results, a...

Conclusion: Conclusion: The research successfully created an effective agile effort estimation model using LASSO regression, showing high pred...

Abstract: Software development effort estimation is one of the most critical aspect in
software development process, as the success or failure of the entire project
depends on the accuracy of estimations. Researchers are still conducting
studies on agile effort estimation. The aim of this research is to develop a
story point based agile effort estimation model using LASSO and Elastic Net
regression techniques. The experimental work is applied to the agile story
point approach using 21 software projects collected from six firms. The two
algorithms are trained using their default parameters and tuned grid search
with 5-fold cross-validation to get an enhanced model. The experiment result
shows LASSO regression achieved better predictive performance PRED (8%) and
PRED (25%) results of 100.0, MMRE of 0.0491, MMER of 0.0551, MdMRE of 0.0593,
MdMER of 0.063, and MSE of 0.0007. The results are also compared with other
related literature.

</details>


### [79] [GUISpector: An MLLM Agent Framework for Automated Verification of Natural Language Requirements in GUI Prototypes](https://arxiv.org/abs/2510.04791)
*Kristian Kolthoff,Felix Kretzer,Simone Paolo Ponzetto,Alexander Maedche,Christian Bartelt*

Main category: cs.SE

TL;DR: GUISpector is a multi-modal LLM-based framework that automates verification of natural language (NL) requirements in GUI prototypes, offering actionable feedback and integration with LLM-driven development workflows.


<details>
  <summary>Details</summary>
Motivation: Traditional and LLM-driven GUI testing approaches struggle with modern interface complexity and lack actionable feedback, hindering requirements verification in LLM-driven development workflows.

Method: 1) Adapts a multi-modal LLM agent to interpret NL requirements and execute verification trajectories. 2)...extracts detailed NL feedback for iterative refinement. 3)...provides an integrated tool for supervising verification processes and managing workflows.

Result: Evaluated on 150 requirements (900 criteria), demonstrating effective detection of requirement satisfaction/violations and seamless feedback integration into LLM-driven workflows.

Conclusion: GUISpector advances automated GUI verification with actionable feedback mechanisms, enabling closed-loop improvements in GUI prototypes and LLM-based code generation within development pipelines.

Abstract: GUIs are foundational to interactive systems and play a pivotal role in early
requirements elicitation through prototyping. Ensuring that GUI implementations
fulfill NL requirements is essential for robust software engineering,
especially as LLM-driven programming agents become increasingly integrated into
development workflows. Existing GUI testing approaches, whether traditional or
LLM-driven, often fall short in handling the complexity of modern interfaces,
and typically lack actionable feedback and effective integration with automated
development agents. In this paper, we introduce GUISpector, a novel framework
that leverages a multi-modal (M)LLM-based agent for the automated verification
of NL requirements in GUI prototypes. First, GUISpector adapts a MLLM agent to
interpret and operationalize NL requirements, enabling to autonomously plan and
execute verification trajectories across GUI applications. Second, GUISpector
systematically extracts detailed NL feedback from the agent's verification
process, providing developers with actionable insights that can be used to
iteratively refine the GUI artifact or directly inform LLM-based code
generation in a closed feedback loop. Third, we present an integrated tool that
unifies these capabilities, offering practitioners an accessible interface for
supervising verification runs, inspecting agent rationales and managing the
end-to-end requirements verification process. We evaluated GUISpector on a
comprehensive set of 150 requirements based on 900 acceptance criteria
annotations across diverse GUI applications, demonstrating effective detection
of requirement satisfaction and violations and highlighting its potential for
seamless integration of actionable feedback into automated LLM-driven
development workflows. The video presentation of GUISpector is available at:
https://youtu.be/JByYF6BNQeE, showcasing its main capabilities.

</details>


### [80] [RevMine: An LLM-Assisted Tool for Code Review Mining and Analysis Across Git Platforms](https://arxiv.org/abs/2510.04796)
*Samah Kansab,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: RevMine is a conceptual tool that uses large language models to automate and streamline code review data mining, reducing manual scripting and democratizing access to software engineering research.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the growing importance of code review analysis for software quality understanding and the current technical challenges in data collection and analysis.

Method: RevMine employs LLMs to automate data collection from review platforms through natural language interfaces and streamline analysis workflows with both user-defined and model-inferred processing.

Result: The tool demonstrates the ability to simplify code review mining pipelines by automating authentication, endpoint discovery, data filtering, and analysis while supporting both quantitative and qualitative approaches.

Conclusion: RevMine represents a new approach to code review mining by leveraging LLMs for intelligent tooling, potentially expanding the scope and accessibility of empirical software engineering research.

Abstract: Empirical research on code review processes is increasingly central to
understanding software quality and collaboration. However, collecting and
analyzing review data remains a time-consuming and technically intensive task.
Most researchers follow similar workflows - writing ad hoc scripts to extract,
filter, and analyze review data from platforms like GitHub and GitLab. This
paper introduces RevMine, a conceptual tool that streamlines the entire code
review mining pipeline using large language models (LLMs). RevMine guides users
through authentication, endpoint discovery, and natural language-driven data
collection, significantly reducing the need for manual scripting. After
retrieving review data, it supports both quantitative and qualitative analysis
based on user-defined filters or LLM-inferred patterns. This poster outlines
the tool's architecture, use cases, and research potential. By lowering the
barrier to entry, RevMine aims to democratize code review mining and enable a
broader range of empirical software engineering studies.

</details>


### [81] [InsightQL: Advancing Human-Assisted Fuzzing with a Unified Code Database and Parameterized Query Interface](https://arxiv.org/abs/2510.04835)
*Wentao Gao,Renata Borovica-Gajic,Sang Kil Cha,Tian Qiu,Van-Thuan Pham*

Main category: cs.SE

TL;DR: InsightQL is a human-assisting framework for fuzz blocker analysis that improves code coverage by up to 13.90%, addressing limitations of existing fuzzers through a unified database and parameterized query interface.


<details>
  <summary>Details</summary>
Motivation: Coverage-guided greybox fuzzers struggle with coverage plateaus caused by fuzz blockers; manual analysis to resolve these is labor-intensive and inefficient.

Method: InsightQL introduces a unified database and intuitive parameterized query interface to systematically extract insights and unblock fuzz blockers with human assistance.

Result: Experiments on 14 real-world libraries in FuzzBench unblocked numerous fuzz blockers, achieving up to 13.90% code coverage improvements.

Conclusion: InsightQL effectively enhances fuzzing by addressing blockers, demonstrating significant coverage gains and validating its utility as a human-assisting framework.

Abstract: Fuzzing is a highly effective automated testing method for uncovering
software vulnerabilities. Despite advances in fuzzing techniques, such as
coverage-guided greybox fuzzing, many fuzzers struggle with coverage plateaus
caused by fuzz blockers, limiting their ability to find deeper vulnerabilities.
Human expertise can address these challenges, but analyzing fuzzing results to
guide this support remains labor-intensive. To tackle this, we introduce
InsightQL, the first human-assisting framework for fuzz blocker analysis.
Powered by a unified database and an intuitive parameterized query interface,
InsightQL aids developers in systematically extracting insights and efficiently
unblocking fuzz blockers. Our experiments on 14 popular real-world libraries
from the FuzzBench benchmark demonstrate the effectiveness of InsightQL,
leading to the unblocking of many fuzz blockers and considerable improvements
in code coverage (up to 13.90%).

</details>


### [82] [FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration](https://arxiv.org/abs/2510.04852)
*Victor May,Diganta Misra,Yanqi Luo,Anjali Sridhar,Justine Gehring,Silvio Soares Ribeiro Junior*

Main category: cs.SE

TL;DR: The paper introduces FreshBrew, a benchmark for evaluating AI agents in Java codebase modernization, revealing strengths and limitations of current agentic approaches in this domain.


<details>
  <summary>Details</summary>
Motivation: Traditional code migration relies on rule-based systems and human effort, but the effectiveness of AI-driven agentic frameworks remains unexamined with rigorous project-level evaluations for Java modernization.

Method: The authors developed FreshBrew, a benchmark requiring semantics-preserving migrations across 228 repositories with high test coverage to detect reward hacking, comparing state-of-the-art LLMs to established rule-based tools.

Result: Gemini 2.5 Flash achieved 52.3% successful migrations to JDK 17, while the study uncovered novel insights into AI agent effectiveness, critical limitations, and failure modes in realistic modernization scenarios.

Conclusion: FreshBrew establishes a foundation for trustworthy code-migration evaluation, enabling reproducible research and progress in AI-driven codebase modernization while highlighting the need for improved agent reliability.

Abstract: AI coding assistants are rapidly becoming integral to modern software
development. A key challenge in this space is the continual need to migrate and
modernize codebases in response to evolving software ecosystems. Traditionally,
such migrations have relied on rule-based systems and human intervention. With
the advent of powerful large language models (LLMs), AI-driven agentic
frameworks offer a promising alternative-but their effectiveness has not been
systematically evaluated. In this paper, we introduce FreshBrew, a novel
benchmark for evaluating AI agents on project-level Java migrations, with a
specific focus on measuring an agent's ability to preserve program semantics
and avoid reward hacking, which we argue requires projects with high test
coverage for a rigorous and reliable evaluation. We benchmark several
state-of-the-art LLMs, and compare their performance against established
rule-based tools. Our evaluation of AI agents on this benchmark of 228
repositories shows that the top-performing model, Gemini 2.5 Flash, can
successfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis
reveals novel insights into the critical strengths and limitations of current
agentic approaches, offering actionable insights into their real-world
applicability. Our empirical study reveals failure modes of current AI agents
in realistic Java modernization tasks, providing a foundation for evaluating
trustworthy code-migration systems. By releasing FreshBrew, we aim to
facilitate rigorous, reproducible evaluation and catalyze progress in AI-driven
codebase modernization.

</details>


### [83] [Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches](https://arxiv.org/abs/2510.04905)
*Yicheng Tao,Yao Qin,Yepang Liu*

Main category: cs.SE

TL;DR: This paper surveys Retrieval-Augmented Code Generation (RACG), focusing on repository-level approaches, categorizing research dimensions, summarizing datasets, and identifying future challenges.


<details>
  <summary>Details</summary>
Motivation: Current code generation models struggle with repository-level tasks requiring global consistency and long-range dependencies, necessitating scalable solutions like retrieval-augmented generation.

Method: The authors perform a systematic review, categorizing RACG works along generation strategies, retrieval modalities, model architectures, training paradigms, and evaluation protocols, while analyzing existing datasets.

Result: Highlights progress in RACG but identifies limitations in scalability, evaluation protocols, and cross-repository generalization, providing a structured overview of current methods and benchmarks.

Conclusion: Establishes a unified analytical framework for RACG, outlining key challenges and opportunities to guide future research in AI-driven software engineering at the repository level.

Abstract: Recent advancements in large language models (LLMs) have substantially
improved automated code generation. While function-level and file-level
generation have achieved promising results, real-world software development
typically requires reasoning across entire repositories. This gives rise to the
challenging task of Repository-Level Code Generation (RLCG), where models must
capture long-range dependencies, ensure global semantic consistency, and
generate coherent code spanning multiple files or modules. To address these
challenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful
paradigm that integrates external retrieval mechanisms with LLMs, enhancing
context-awareness and scalability. In this survey, we provide a comprehensive
review of research on Retrieval-Augmented Code Generation (RACG), with an
emphasis on repository-level approaches. We categorize existing work along
several dimensions, including generation strategies, retrieval modalities,
model architectures, training paradigms, and evaluation protocols. Furthermore,
we summarize widely used datasets and benchmarks, analyze current limitations,
and outline key challenges and opportunities for future research. Our goal is
to establish a unified analytical framework for understanding this rapidly
evolving field and to inspire continued progress in AI-powered software
engineering.

</details>


### [84] [Why Software Signing (Still) Matters: Trust Boundaries in the Software Supply Chain](https://arxiv.org/abs/2510.04964)
*Kelechi G. Kalu,James C. Davis*

Main category: cs.SE

TL;DR: This paper argues that software artifact signing remains critical for supply chain security even with hardened registry controls, as trust boundaries beyond registries (mirrors, proxies, etc.) require end-to-end provenance guarantees.


<details>
  <summary>Details</summary>
Motivation: Centralized registries have security controls, but the paper challenges whether these alone suffice to replace artifact signing given distribution boundary limitations.

Method: Analyzes historical practices and proposes a trust model to determine when signing is required to maintain integrity across distribution channels beyond registry control.

Result: Demonstrates that registry security controls cannot ensure provenance across mirrors, re-hosting, and air-gapped transfers, necessitating signing as a baseline defense mechanism.

Conclusion: Software signing provides essential accountability and integrity guarantees that complement registry security, ensuring trust extends beyond registry-controlled boundaries.

Abstract: Software signing provides a formal mechanism for provenance by ensuring
artifact integrity and verifying producer identity. It also imposes tooling and
operational costs to implement in practice. In an era of centralized registries
such as PyPI, npm, Maven Central, and Hugging Face, it is reasonable to ask
whether hardening registry security controls obviates the need for end-to-end
artifact signing. In this work, we posit that the core guarantees of signing,
provenance, integrity, and accountability are not automatically carried across
different software distribution boundaries. These boundaries include mirrors,
corporate proxies, re-hosting, and air-gapped transfers, where registry
security controls alone cannot provide sufficient assurance. We synthesize
historical practice and present a trust model for modern distribution modes to
identify when signing is necessary to extend trust beyond registry control.
Treating signing as a baseline layer of defense strengthens software supply
chain assurance even when registries are secure.

</details>


### [85] [Quantum Computing as a Service - a Software Engineering Perspective](https://arxiv.org/abs/2510.04982)
*Aakash Ahmad,Muhammad Waseem,Bakheet Aljedaani,Mahdi Fahmideh,Peng Liang,Feras Awaysheh*

Main category: cs.SE

TL;DR: This paper proposes a software engineering approach for enabling Quantum Computing as a Service (QCaaS) via process-centric architecture development, integrating systematic reviews and reference architectures for quantum service lifecycle management.


<details>
  <summary>Details</summary>
Motivation: As quantum systems advance towards practical deployment, there is a growing need to address service-oriented software engineering challenges for QCaaS to make quantum resources accessible as utility computing. Existing research lacks structured frameworks for quantum service development.

Method: A two-phase approach combining (a) systematic mapping study (SMS) to analyze 41 peer-reviewed studies through multi-step selection and (b) architecture-driven development to create a reference architecture. The study addresses three research questions spanning research trends, service lifecycle phases, and technical requirements.

Result: Identified a four-phased quantum service lifecycle (conception, modeling, assembly, deployment), quantum significant requirements (QSRs), modeling notations, design patterns, programming languages, and deployment platforms. These components were integrated into a layered reference architecture for QCaaS.

Conclusion: This research provides the first structured software engineering perspective for QCaaS engineering, offering a reference architecture and standardized lifecycle model to support organizations in developing quantum services. The outcomes contribute to both academic research and industry adoption of quantum technologies.

Abstract: Quantum systems have started to emerge as a disruptive technology and
enabling platforms - exploiting the principles of quantum mechanics via
programmable quantum bits (QuBits) - to achieve quantum supremacy in computing.
Academic research, industrial projects (e.g., Amazon Braket, IBM Qiskit), and
consortiums like 'Quantum Flagship' are striving to develop practically capable
and commercially viable quantum computing (QC) systems and technologies.
Quantum Computing as a Service (QCaaS) is viewed as a solution attuned to the
philosophy of service-orientation that can offer QC resources and platforms, as
utility computing, to individuals and organisations who do not own quantum
computers. This research investigates a process-centric and architecture-driven
approach to offer a software engineering perspective on enabling QCaaS - a.k.a
quantum service-orientation. We employed a two-phase research method comprising
(a) a systematic mapping study and (b) an architecture-based development, first
to identify the phases of the quantum service development life cycle and
subsequently to integrate these phases into a reference architecture that
supports QCaaS. The SMS process retrieved a collection of potentially relevant
research literature and based on a multi-step selection and qualitative
assessment, we selected 41 peer-reviewed studies to answer three RQs. The RQs
investigate (i) demographic details in terms of frequency, types, and trends of
research, (ii) phases of quantum service development lifecycle to derive a
reference architecture for conception, modeling, assembly, and deployment of
services, and (iii) The results identify a 4-phased development lifecycle along
with quantum significant requirements (QSRs), various modeling notations,
catalogue of patterns, programming languages, and deployment platforms that can
be integrated in a layered reference architecture to engineer QCaaS.

</details>


### [86] [AutoEmpirical: LLM-Based Automated Research for Empirical Software Fault Analysis](https://arxiv.org/abs/2510.04997)
*Jiongchi Yu,Weipeng Jiang,Xiaoyu Zhang,Qiang Hu,Xiaofei Xie,Chao Shen*

Main category: cs.SE

TL;DR: This paper explores using Large Language Models (LLMs) to automate software fault analysis, demonstrating significant efficiency improvements over traditional manual methods. By decomposing the empirical study process into three phases and evaluating 3,829 faults, the authors show LLMs can reduce analysis time from weeks to two hours while highlighting challenges for achieving full automation.


<details>
  <summary>Details</summary>
Motivation: Traditional software fault analysis relies on manual, labor-intensive processes (collection, filtering, investigation) that create bottlenecks in large-scale or iterative empirical research. Automated solutions are needed to enable more scalable and efficient fault studies.

Method: The authors decompose the fault study process into 1) research objective definition, 2)data preparation, and 3)fault analysis phases. They conduct an initial exploration using LLMs for fault analysis, evaluating 3,829 faults from a high-quality empirical study dataset.

Result: LLMs achieved an average fault analysis time of ~2 hours compared to weeks of manual effort. The evaluation demonstrated improved efficiency while maintaining accuracy in fault classification/localization tasks.

Conclusion: LLMs show promise for accelerating empirical software fault studies but require addressing open challenges (unknowns) to enable fully automated, end-to-end analysis. The authors outline a research plan to advance LLMs in this domain.

Abstract: Understanding software faults is essential for empirical research in software
development and maintenance. However, traditional fault analysis, while
valuable, typically involves multiple expert-driven steps such as collecting
potential faults, filtering, and manual investigation. These processes are both
labor-intensive and time-consuming, creating bottlenecks that hinder
large-scale fault studies in complex yet critical software systems and slow the
pace of iterative empirical research.
  In this paper, we decompose the process of empirical software fault study
into three key phases: (1) research objective definition, (2) data preparation,
and (3) fault analysis, and we conduct an initial exploration study of applying
Large Language Models (LLMs) for fault analysis of open-source software.
Specifically, we perform the evaluation on 3,829 software faults drawn from a
high-quality empirical study. Our results show that LLMs can substantially
improve efficiency in fault analysis, with an average processing time of about
two hours, compared to the weeks of manual effort typically required. We
conclude by outlining a detailed research plan that highlights both the
potential of LLMs for advancing empirical fault studies and the open challenges
that required be addressed to achieve fully automated, end-to-end software
fault analysis.

</details>
