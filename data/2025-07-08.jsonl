{"id": "2507.03156", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.03156", "abs": "https://arxiv.org/abs/2507.03156", "authors": ["Amr Mohamed", "Maram Assi", "Mariam Guizani"], "title": "The Impact of LLM-Assistants on Software Developer Productivity: A Systematic Literature Review", "comment": "37 pages", "summary": "Large language model assistants (LLM-assistants) present new opportunities to\ntransform software development. Developers are increasingly adopting these\ntools across tasks, including coding, testing, debugging, documentation, and\ndesign. Yet, despite growing interest, there is no synthesis of how\nLLM-assistants affect software developer productivity. In this paper, we\npresent a systematic literature review of 37 peer-reviewed studies published\nbetween January 2014 and December 2024 that examine this impact. Our analysis\nreveals that LLM-assistants offer both considerable benefits and critical\nrisks. Commonly reported gains include minimized code search, accelerated\ndevelopment, and the automation of trivial and repetitive tasks. However,\nstudies also highlight concerns around cognitive offloading, reduced team\ncollaboration, and inconsistent effects on code quality. While the majority of\nstudies (92%) adopt a multi-dimensional perspective by examining at least two\nSPACE dimensions, reflecting increased awareness of the complexity of developer\nproductivity, only 14% extend beyond three dimensions, indicating substantial\nroom for more integrated evaluations. Satisfaction, Performance, and Efficiency\nare the most frequently investigated dimensions, whereas Communication and\nActivity remain underexplored. Most studies are exploratory (64%) and\nmethodologically diverse, but lack longitudinal and team-based evaluations.\nThis review surfaces key research gaps and provides recommendations for future\nresearch and practice. All artifacts associated with this study are publicly\navailable at https://zenodo.org/records/15788502."}
{"id": "2507.03160", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03160", "abs": "https://arxiv.org/abs/2507.03160", "authors": ["Md Mahade Hasan", "Muhammad Waseem", "Kai-Kristian Kemell", "Jussi Raskua", "Juha Ala-Rantalaa", "Pekka Abrahamsson"], "title": "Assessing Small Language Models for Code Generation: An Empirical Study with Benchmarks", "comment": null, "summary": "The recent advancements of Small Language Models (SLMs) have opened new\npossibilities for efficient code generation. SLMs offer lightweight and\ncost-effective alternatives to Large Language Models (LLMs), making them\nattractive for use in resource-constrained environments. However, empirical\nunderstanding of SLMs, particularly their capabilities, limitations, and\nperformance trade-offs in code generation remains limited. This study presents\na comprehensive empirical evaluation of 20 open-source SLMs ranging from 0.4B\nto 10B parameters on five diverse code-related benchmarks (HumanEval, MBPP,\nMercury, HumanEvalPack, and CodeXGLUE). The models are assessed along three\ndimensions: i) functional correctness of generated code, ii) computational\nefficiency and iii) performance across multiple programming languages. The\nfindings of this study reveal that several compact SLMs achieve competitive\nresults while maintaining a balance between performance and efficiency, making\nthem viable for deployment in resource-constrained environments. However,\nachieving further improvements in accuracy requires switching to larger models.\nThese models generally outperform their smaller counterparts, but they require\nmuch more computational power. We observe that for 10% performance\nimprovements, models can require nearly a 4x increase in VRAM consumption,\nhighlighting a trade-off between effectiveness and scalability. Besides, the\nmultilingual performance analysis reveals that SLMs tend to perform better in\nlanguages such as Python, Java, and PHP, while exhibiting relatively weaker\nperformance in Go, C++, and Ruby. However, statistical analysis suggests these\ndifferences are not significant, indicating a generalizability of SLMs across\nprogramming languages. Based on the findings, this work provides insights into\nthe design and selection of SLMs for real-world code generation tasks."}
{"id": "2507.03263", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03263", "abs": "https://arxiv.org/abs/2507.03263", "authors": ["Haiqiao Gu", "Yiliang Zhao", "Kai Gao", "Minghui Zhou"], "title": "Analyzing C/C++ Library Migrations at the Package-level: Prevalence, Domains, Targets and Rationals across Seven Package Management Tools", "comment": null, "summary": "Library migration happens when a library can not meet the project's\nrequirements and is non-trivial to accomplish. To mitigate the problem,\nsubstantial efforts have been devoted to understanding its characteristics and\nrecommending alternative libraries, especially for programming language (PL)\necosystems with a central package hosting platform, such as Python (PyPI).\nHowever, to the best of our knowledge, understanding of C/C++ library\nmigrations is still lacking, possibly due to challenges resulting from the\nfragmented and complicated dependency management practices in the C/C++\necosystem. To bridge this knowledge gap, this paper analyzes 19,943 C/C++\nprojects that utilize different package management tools and establishes the\nfirst C/C++ library migration dataset. Based on the dataset, we investigate the\nprevalence, domains, target library, and rationale of C/C++ library migrations\nand compare the results with three widely investigated PLs: Python, JavaScript,\nand Java. We find that the overall trend in the number of C/C++ library\nmigrations is similar to Java. Migrations across different package management\ntools are also observed. In C/C++, library migrations mainly occur in GUI,\nBuild, and OS development, but are rare in domains (e.g., Testing and Logging)\nthat dominate library migrations in the three compared PLs. 83.46\\% of C/C++\nsource libraries only have one migration target, suggesting that our library\nmigration dataset could be used directly to recommend migration targets. We\nfind four C/C++-specific migration reasons, such as less compile time and\nunification of dependency management, revealing the unique dependency\nmanagement requirements in C/C++ projects. We believe our findings can help\nC/C++ developers make more informed library migration decisions and shed light\non the design of C/C++ library migration tools."}
{"id": "2507.03328", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03328", "abs": "https://arxiv.org/abs/2507.03328", "authors": ["S. Lee", "C. Myers", "A. Yang", "T. Zhang", "S. J. L. Billinge"], "title": "scikit-package -- software packaging standards and roadmap for sharing reproducible scientific software", "comment": "GitHub: https://github.com/scikit-package/scikit-package Doc:\n  https://scikit-package.github.io/scikit-package/", "summary": "Scientific advancement relies on the ability to share and reproduce results.\nWhen data analysis or calculations are carried out using software written by\nscientists there are special challenges around code versions, quality and code\nsharing. scikit-package provides a roadmap to facilitate code reuse and sharing\nwith minimal effort through tutorials coupled with automated and centralized\nreusable workflows. The goal of the project is to provide pedagogical and\npractical tools for scientists who are not professionally trained software\nengineers to write more reusable and maintainable software code. Code reuse can\noccur at multiple levels of complexity-from turning a code block into a\nfunction within a single script, to publishing a publicly installable, fully\ntested, and documented software package scikit-package provides a community\nmaintained set of tools, and a roadmap, to help scientists bring their software\nhigher levels of reproducibility and shareability."}
{"id": "2507.02951", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02951", "abs": "https://arxiv.org/abs/2507.02951", "authors": ["Elizabeth Lui", "Jiahao Sun"], "title": "Bittensor Protocol: The Bitcoin in Decentralized Artificial Intelligence? A Critical and Empirical Analysis", "comment": "MARBLE 2025", "summary": "This paper investigates whether Bittensor can be considered the Bitcoin of\ndecentralized Artificial Intelligence by directly comparing its tokenomics,\ndecentralization properties, consensus mechanism, and incentive structure\nagainst those of Bitcoin. Leveraging on-chain data from all 64 active Bittensor\nsubnets, we first document considerable concentration in both stake and\nrewards. We further show that rewards are overwhelmingly driven by stake,\nhighlighting a clear misalignment between quality and compensation. As a\nremedy, we put forward a series of two-pronged protocol-level interventions.\nFor incentive realignment, our proposed solutions include performance-weighted\nemission split, composite scoring, and a trust-bonus multiplier. As for\nmitigating security vulnerability due to stake concentration, we propose and\nempirically validate stake cap at the 88th percentile, which elevates the\nmedian coalition size required for a 51-percent attack and remains robust\nacross daily, weekly, and monthly snapshots."}
{"id": "2507.03405", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03405", "abs": "https://arxiv.org/abs/2507.03405", "authors": ["Krishna Ronanki", "Simon Arvidsson", "Johan Axell"], "title": "Prompt Engineering Guidelines for Using Large Language Models in Requirements Engineering", "comment": "Accepted for publication at the 51st Euromicro Conference Series on\n  Software Engineering and Advanced Applications (SEAA) 2025", "summary": "The rapid emergence of generative AI models like Large Language Models (LLMs)\nhas demonstrated its utility across various activities, including within\nRequirements Engineering (RE). Ensuring the quality and accuracy of\nLLM-generated output is critical, with prompt engineering serving as a key\ntechnique to guide model responses. However, existing literature provides\nlimited guidance on how prompt engineering can be leveraged, specifically for\nRE activities. The objective of this study is to explore the applicability of\nexisting prompt engineering guidelines for the effective usage of LLMs within\nRE. To achieve this goal, we began by conducting a systematic review of primary\nliterature to compile a non-exhaustive list of prompt engineering guidelines.\nThen, we conducted interviews with RE experts to present the extracted\nguidelines and gain insights on the advantages and limitations of their\napplication within RE. Our literature review indicates a shortage of prompt\nengineering guidelines for domain-specific activities, specifically for RE. Our\nproposed mapping contributes to addressing this shortage. We conclude our study\nby identifying an important future line of research within this field."}
{"id": "2507.02956", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02956", "abs": "https://arxiv.org/abs/2507.02956", "authors": ["Blake Bullwinkel", "Mark Russinovich", "Ahmed Salem", "Santiago Zanella-Beguelin", "Daniel Jones", "Giorgio Severi", "Eugenia Kim", "Keegan Hines", "Amanda Minnich", "Yonatan Zunger", "Ram Shankar Siva Kumar"], "title": "A Representation Engineering Perspective on the Effectiveness of Multi-Turn Jailbreaks", "comment": null, "summary": "Recent research has demonstrated that state-of-the-art LLMs and defenses\nremain susceptible to multi-turn jailbreak attacks. These attacks require only\nclosed-box model access and are often easy to perform manually, posing a\nsignificant threat to the safe and secure deployment of LLM-based systems. We\nstudy the effectiveness of the Crescendo multi-turn jailbreak at the level of\nintermediate model representations and find that safety-aligned LMs often\nrepresent Crescendo responses as more benign than harmful, especially as the\nnumber of conversation turns increases. Our analysis indicates that at each\nturn, Crescendo prompts tend to keep model outputs in a \"benign\" region of\nrepresentation space, effectively tricking the model into fulfilling harmful\nrequests. Further, our results help explain why single-turn jailbreak defenses\nlike circuit breakers are generally ineffective against multi-turn attacks,\nmotivating the development of mitigations that address this generalization gap."}
{"id": "2507.03515", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03515", "abs": "https://arxiv.org/abs/2507.03515", "authors": ["Radouane Bouchekir", "Michell Guzman Cancimance"], "title": "Enhancing Uncertainty Quantification for Runtime Safety Assurance Using Causal Risk Analysis and Operational Design Domain", "comment": null, "summary": "Ensuring the runtime safety of autonomous systems remains challenging due to\ndeep learning components' inherent uncertainty and their sensitivity to\nenvironmental changes. In this paper, we propose an enhancement of traditional\nuncertainty quantification by explicitly incorporating environmental conditions\nusing risk-based causal analysis. We leverage Hazard Analysis and Risk\nAssessment (HARA) and fault tree modeling to identify critical operational\nconditions affecting system functionality. These conditions, together with\nuncertainties from the data and model, are integrated into a unified Bayesian\nNetwork (BN). At runtime, this BN is instantiated using real-time environmental\nobservations to infer a probabilistic distribution over the safety estimation.\nThis distribution enables the computation of both expected performance and its\nassociated variance, providing a dynamic and context-aware measure of\nuncertainty. We demonstrate our approach through a case study of the Object\nDetection (OD) component in an Automated Valet Parking (AVP)."}
{"id": "2507.02959", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02959", "abs": "https://arxiv.org/abs/2507.02959", "authors": ["Ahmed Bensaoud", "Jugal Kalita"], "title": "A Novel Active Learning Approach to Label One Million Unknown Malware Variants", "comment": null, "summary": "Active learning for classification seeks to reduce the cost of labeling\nsamples by finding unlabeled examples about which the current model is least\ncertain and sending them to an annotator/expert to label. Bayesian theory can\nprovide a probabilistic view of deep neural network models by asserting a prior\ndistribution over model parameters and estimating the uncertainties by\nposterior distribution over these parameters. This paper proposes two novel\nactive learning approaches to label one million malware examples belonging to\ndifferent unknown modern malware families. The first model is Inception-V4+PCA\ncombined with several support vector machine (SVM) algorithms (UTSVM, PSVM,\nSVM-GSU, TBSVM). The second model is Vision Transformer based Bayesian Neural\nNetworks ViT-BNN. Our proposed ViT-BNN is a state-of-the-art active learning\napproach that differs from current methods and can apply to any particular\ntask. The experiments demonstrate that the ViT-BNN is more stable and robust in\nhandling uncertainty."}
{"id": "2507.03527", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03527", "abs": "https://arxiv.org/abs/2507.03527", "authors": ["Dulaji Hidellaarachchi", "John Grundy", "Rashina Hoda"], "title": "The Role of Humour in Software Engineering -- A Literature Review and Preliminary Taxonomy", "comment": "Accepted to publish in Journal of Software Systems (JSS) New Idea\n  Track 2025 (23 pages, 1 figure)", "summary": "Humour has long been recognized as a key factor in enhancing creativity,\ngroup effectiveness, and employee well-being across various domains. However,\nits occurrence and impact within software engineering (SE) teams remains\nunder-explored. This paper introduces a comprehensive, literature review-based\ntaxonomy exploring the characterisation and use of humour in SE teams, with the\ngoal of boosting productivity, improving communication, and fostering a\npositive work environment while emphasising the responsible use of humour to\nmitigate its potential negative impacts. Drawing from a wide array of studies\nin psychology, sociology, and organizational behaviour, our proposed framework\ncategorizes humour into distinct theories, styles, models, and scales, offering\nSE professionals and researchers a structured approach to understanding humour\nin their work. This study also addresses the unique challenges of applying\nhumour in SE, highlighting its potential benefits while acknowledging the need\nfor further empirical validation in this context. Ultimately, our study aims to\npave the way for more cohesive, creative, and psychologically supportive SE\nenvironments through the strategic use of humour."}
{"id": "2507.02968", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02968", "abs": "https://arxiv.org/abs/2507.02968", "authors": ["Vijayalakshmi Ramasamy", "Seth Barrett", "Gokila Dorai", "Jessica Zumbach"], "title": "Unveiling Privacy Policy Complexity: An Exploratory Study Using Graph Mining, Machine Learning, and Natural Language Processing", "comment": "7 Pages; 1 Algorithm; 1 Table; 2 Figures; Accepted by AIRC 2025", "summary": "Privacy policy documents are often lengthy, complex, and difficult for\nnon-expert users to interpret, leading to a lack of transparency regarding the\ncollection, processing, and sharing of personal data. As concerns over online\nprivacy grow, it is essential to develop automated tools capable of analyzing\nprivacy policies and identifying potential risks. In this study, we explore the\npotential of interactive graph visualizations to enhance user understanding of\nprivacy policies by representing policy terms as structured graph models. This\napproach makes complex relationships more accessible and enables users to make\ninformed decisions about their personal data (RQ1). We also employ graph mining\nalgorithms to identify key themes, such as User Activity and Device\nInformation, using dimensionality reduction techniques like t-SNE and PCA to\nassess clustering effectiveness. Our findings reveal that graph-based\nclustering improves policy content interpretability. It highlights patterns in\nuser tracking and data sharing, which supports forensic investigations and\nidentifies regulatory non-compliance. This research advances AI-driven tools\nfor auditing privacy policies by integrating interactive visualizations with\ngraph mining. Enhanced transparency fosters accountability and trust."}
{"id": "2507.03536", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03536", "abs": "https://arxiv.org/abs/2507.03536", "authors": ["Adam Tornhill", "Markus Borg", "Nadim Hagatulah", "Emma Söderberg"], "title": "ACE: Automated Technical Debt Remediation with Validated Large Language Model Refactorings", "comment": "Published in proceedings of the 1st International Workshop on\n  Artificial Intelligence for Integrated Development Environments (AI-IDE)\n  (2025)", "summary": "The remarkable advances in AI and Large Language Models (LLMs) have enabled\nmachines to write code, accelerating the growth of software systems. However,\nthe bottleneck in software development is not writing code but understanding\nit; program understanding is the dominant activity, consuming approximately 70%\nof developers' time. This implies that improving existing code to make it\neasier to understand has a high payoff and - in the age of AI-assisted coding -\nis an essential activity to ensure that a limited pool of developers can keep\nup with ever-growing codebases. This paper introduces Augmented Code\nEngineering (ACE), a tool that automates code improvements using validated LLM\noutput. Developed through a data-driven approach, ACE provides reliable\nrefactoring suggestions by considering both objective code quality improvements\nand program correctness. Early feedback from users suggests that AI-enabled\nrefactoring helps mitigate code-level technical debt that otherwise rarely gets\nacted upon."}
{"id": "2507.02969", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02969", "abs": "https://arxiv.org/abs/2507.02969", "authors": ["Daniel López-Montero", "José L. Álvarez-Aldana", "Alicia Morales-Martínez", "Marta Gil-López", "Juan M. Auñón García"], "title": "Reinforcement Learning for Automated Cybersecurity Penetration Testing", "comment": null, "summary": "This paper aims to provide an innovative machine learning-based solution to\nautomate security testing tasks for web applications, ensuring the correct\nfunctioning of all components while reducing project maintenance costs.\nReinforcement Learning is proposed to select and prioritize tools and optimize\nthe testing path. The presented approach utilizes a simulated webpage along\nwith its network topology to train the agent. Additionally, the model leverages\nGeometric Deep Learning to create priors that reduce the search space and\nimprove learning convergence. The validation and testing process was conducted\non real-world vulnerable web pages commonly used by human hackers for learning.\nAs a result of this study, a reinforcement learning algorithm was developed\nthat maximizes the number of vulnerabilities found while minimizing the number\nof steps required"}
{"id": "2507.03620", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG", "68T50", "I.2.7; D.2.3"], "pdf": "https://arxiv.org/pdf/2507.03620", "abs": "https://arxiv.org/abs/2507.03620", "authors": ["Francisca Lemos", "Victor Alves", "Filipa Ferraz"], "title": "Is It Time To Treat Prompts As Code? A Multi-Use Case Study For Prompt Optimization Using DSPy", "comment": "20 pages with 1 figure", "summary": "Although prompt engineering is central to unlocking the full potential of\nLarge Language Models (LLMs), crafting effective prompts remains a\ntime-consuming trial-and-error process that relies on human intuition. This\nstudy investigates Declarative Self-improving Python (DSPy), an optimization\nframework that programmatically creates and refines prompts, applied to five\nuse cases: guardrail enforcement, hallucination detection in code, code\ngeneration, routing agents, and prompt evaluation. Each use case explores how\nprompt optimization via DSPy influences performance. While some cases\ndemonstrated modest improvements - such as minor gains in the guardrails use\ncase and selective enhancements in hallucination detection - others showed\nnotable benefits. The prompt evaluation criterion task demonstrated a\nsubstantial performance increase, rising accuracy from 46.2% to 64.0%. In the\nrouter agent case, the possibility of improving a poorly performing prompt and\nof a smaller model matching a stronger one through optimized prompting was\nexplored. Although prompt refinement increased accuracy from 85.0% to 90.0%,\nusing the optimized prompt with a cheaper model did not improve performance.\nOverall, this study's findings suggest that DSPy's systematic prompt\noptimization can enhance LLM performance, particularly when instruction tuning\nand example selection are optimized together. However, the impact varies by\ntask, highlighting the importance of evaluating specific use cases in prompt\noptimization research."}
{"id": "2507.02971", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.02971", "abs": "https://arxiv.org/abs/2507.02971", "authors": ["Mohsen Ghasemizade", "Juniper Lovato", "Christopher M. Danforth", "Peter Sheridan Dodds", "Laura S. P. Bloomfield", "Matthew Price", "Team LEMURS", "Joseph P. Near"], "title": "Aim High, Stay Private: Differentially Private Synthetic Data Enables Public Release of Behavioral Health Information with High Utility", "comment": "14 pages, 8 figures, 2 tables", "summary": "Sharing health and behavioral data raises significant privacy concerns, as\nconventional de-identification methods are susceptible to privacy attacks.\nDifferential Privacy (DP) provides formal guarantees against re-identification\nrisks, but practical implementation necessitates balancing privacy protection\nand the utility of data.\n  We demonstrate the use of DP to protect individuals in a real behavioral\nhealth study, while making the data publicly available and retaining high\nutility for downstream users of the data. We use the Adaptive Iterative\nMechanism (AIM) to generate DP synthetic data for Phase 1 of the Lived\nExperiences Measured Using Rings Study (LEMURS). The LEMURS dataset comprises\nphysiological measurements from wearable devices (Oura rings) and self-reported\nsurvey data from first-year college students. We evaluate the synthetic\ndatasets across a range of privacy budgets, epsilon = 1 to 100, focusing on the\ntrade-off between privacy and utility.\n  We evaluate the utility of the synthetic data using a framework informed by\nactual uses of the LEMURS dataset. Our evaluation identifies the trade-off\nbetween privacy and utility across synthetic datasets generated with different\nprivacy budgets. We find that synthetic data sets with epsilon = 5 preserve\nadequate predictive utility while significantly mitigating privacy risks. Our\nmethodology establishes a reproducible framework for evaluating the practical\nimpacts of epsilon on generating private synthetic datasets with numerous\nattributes and records, contributing to informed decision-making in data\nsharing practices."}
{"id": "2507.03659", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.03659", "abs": "https://arxiv.org/abs/2507.03659", "authors": ["Valentina Wu", "Alexandra Mendes", "Alexandre Abreu"], "title": "Specification-Guided Repair of Arithmetic Errors in Dafny Programs using LLMs", "comment": null, "summary": "Formal verification offers strong assurances of software correctness.\nHowever, debugging and repairing the underlying faults can be complex and\ntime-consuming when verification fails. Automated Program Repair (APR) aims to\nease this by automatically identifying and fixing faults. Traditional APR\ntechniques often depend on test suites for validation, but these may fail to\ncapture all scenarios. In contrast, formal specifications provide stronger\ncorrectness criteria for effective repairs.\n  We present an innovative APR tool for Dafny, a verification-aware programming\nlanguage that uses formal specifications - including pre-conditions,\npost-conditions, and invariants - as oracles for fault localization and repair.\nAssuming the correctness of the specifications and focusing on arithmetic bugs,\nwe localize faults through a series of steps, which include using Hoare Logic\nto determine the state of each statement within the program and\nstate-of-the-art Large Language Models (LLMs) to synthesize candidate fixes.\nThe chosen models were GPT-4o mini, Llama 3, Mistral 7B, and Llemma 7B.\n  We evaluate our approach using DafnyBench, a benchmark of real-world Dafny\nprograms. Our tool achieves 89.6% accuracy in fault localization, with GPT-4o\nmini yielding the highest repair success rate (74.18%). These results highlight\nthe potential of combining formal reasoning with LLM-driven program synthesis\nfor automated program repair."}
{"id": "2507.02976", "categories": ["cs.CR", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.02976", "abs": "https://arxiv.org/abs/2507.02976", "authors": ["Amirali Sajadi", "Kostadin Damevski", "Preetha Chatterjee"], "title": "Are AI-Generated Fixes Secure? Analyzing LLM and Agent Patches on SWE-bench", "comment": null, "summary": "Large Language Models (LLMs) and their agentic frameworks are increasingly\nadopted to automate software development tasks such as issue resolution and\nprogram repair. While prior work has identified security risks in LLM-generated\ncode, most evaluations have focused on synthetic or isolated settings, leaving\nopen questions about the security of these systems in real-world development\ncontexts. In this study, we present the first large-scale security analysis of\nLLM-generated patches using 20,000+ issues from the SWE-bench dataset. We\nevaluate patches produced by a standalone LLM (Llama 3.3) and compare them to\ndeveloper-written patches. We also assess the security of patches generated by\nthree top-performing agentic frameworks (OpenHands, AutoCodeRover, HoneyComb)\non a subset of our data. Finally, we analyze a wide range of code, issue, and\nproject-level factors to understand the conditions under which LLMs and agents\nare most likely to generate insecure code. Our findings reveal that the\nstandalone LLM introduces nearly 9x more new vulnerabilities than developers,\nwith many of these exhibiting unique patterns not found in developers' code.\nAgentic workflows also generate a significant number of vulnerabilities,\nparticularly when granting LLMs more autonomy, potentially increasing the\nlikelihood of misinterpreting project context or task requirements. We find\nthat vulnerabilities are more likely to occur in LLM patches associated with a\nhigher number of files, more lines of generated code, and GitHub issues that\nlack specific code snippets or information about the expected code behavior and\nsteps to reproduce. These results suggest that contextual factors play a\ncritical role in the security of the generated code and point toward the need\nfor proactive risk assessment methods that account for both code and\nissue-level information to complement existing vulnerability detection tools."}
{"id": "2507.04173", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.04173", "abs": "https://arxiv.org/abs/2507.04173", "authors": ["Henri Aïdasso", "Francis Bordeleau", "Ali Tizghadam"], "title": "Efficient Detection of Intermittent Job Failures Using Few-Shot Learning", "comment": "Accepted at the 41st International Conference on Software Maintenance\n  and Evolution - ICSME 2025, Industry Track", "summary": "One of the main challenges developers face in the use of continuous\nintegration (CI) and deployment pipelines is the occurrence of intermittent job\nfailures, which result from unexpected non-deterministic issues (e.g., flaky\ntests or infrastructure problems) rather than regular code-related errors such\nas bugs. Prior studies developed machine-learning (ML) models trained on large\ndatasets of job logs to classify job failures as either intermittent or\nregular. As an alternative to costly manual labeling of large datasets, the\nstate-of-the-art (SOTA) approach leveraged a heuristic based on\nnon-deterministic job reruns. However, this method mislabels intermittent job\nfailures as regular in contexts where rerunning suspicious job failures is not\nan explicit policy, and therefore limits the SOTA's performance in practice. In\nfact, our manual analysis of 2,125 job failures from 5 industrial and 1\nopen-source projects reveals that, on average, 32\\% of intermittent job\nfailures are mislabeled as regular. To address these limitations, this paper\nintroduces a novel approach to intermittent job failure detection using\nfew-shot learning (FSL). Specifically, we fine-tune a small language model\nusing a few number of manually labeled log examples to generate rich\nembeddings, which are then used to train an ML classifier. Our FSL-based\napproach achieves 70-88\\% F1-score with only 12 shots in all projects,\noutperforming the SOTA, which proved ineffective (34-52\\% F1-score) in 4\nprojects. Overall, this study underlines the importance of data quality over\nquantity and provides a more efficient and practical framework for the\ndetection of intermittent job failures in organizations."}
{"id": "2507.03000", "categories": ["cs.CR", "cs.IT", "math.IT", "Primary 05A17, Secondary 11D45, 11Y60, 94A60", "F.2.1"], "pdf": "https://arxiv.org/pdf/2507.03000", "abs": "https://arxiv.org/abs/2507.03000", "authors": ["Michael A. Idowu"], "title": "Deterministic Cryptographic Seed Generation via Cyclic Modular Inversion over $\\mathbb{Z}/3^p\\mathbb{Z}$", "comment": "29 pages, 13 figures, 13 tables. Includes entropy analysis, symbolic\n  residue formulation, empirical validation, and benchmarking against\n  NIST-recommended DRBG frameworks", "summary": "We present a deterministic framework for cryptographic seed generation based\non cyclic modular inversion over $\\mathbb{Z}/3^p\\mathbb{Z}$. The method\nenforces algebraic admissibility on seed inputs via the identity $d_k \\equiv\n-\\left(2^{k-1}\\right)^{-1} \\bmod 3^p$, thereby producing structured and\ninvertible residue sequences. This mapping yields entropy-rich, cycle-complete\nseeds well-suited for cryptographic primitives such as DRBGs, KDFs, and\npost-quantum schemes. To assess the quality of randomness, we introduce the\nEntropy Confidence Score (ECS), a composite metric reflecting coverage,\nuniformity, and modular bias. Although not a cryptographic PRNG in itself, the\nframework serves as a deterministic entropy filter that conditions and\nvalidates seed inputs prior to their use by conventional generators. Empirical\nand hardware-based results confirm constant-time execution, minimal\nside-channel leakage, and lightweight feasibility for embedded applications.\nThe framework complements existing cryptographic stacks by acting as an\nalgebraically verifiable entropy filter, thereby enhancing structural soundness\nand auditability."}
{"id": "2507.04185", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04185", "abs": "https://arxiv.org/abs/2507.04185", "authors": ["Aniket Kesari", "Travis Breaux", "Tom Norton", "Sarah Santos", "Anmol Singhal"], "title": "From Legal Text to Tech Specs: Generative AI's Interpretation of Consent in Privacy Law", "comment": "10 pages, 1 figure, 20th International Conference on Artificial\n  Intelligence and Law (ICAIL 2025)", "summary": "Privacy law and regulation have turned to \"consent\" as the legitimate basis\nfor collecting and processing individuals' data. As governments have rushed to\nenshrine consent requirements in their privacy laws, such as the California\nConsumer Privacy Act (CCPA), significant challenges remain in understanding how\nthese legal mandates are operationalized in software. The opaque nature of\nsoftware development processes further complicates this translation. To address\nthis, we explore the use of Large Language Models (LLMs) in requirements\nengineering to bridge the gap between legal requirements and technical\nimplementation. This study employs a three-step pipeline that involves using an\nLLM to classify software use cases for compliance, generating LLM modifications\nfor non-compliant cases, and manually validating these changes against legal\nstandards. Our preliminary findings highlight the potential of LLMs in\nautomating compliance tasks, while also revealing limitations in their\nreasoning capabilities. By benchmarking LLMs against real-world use cases, this\nresearch provides insights into leveraging AI-driven solutions to enhance legal\ncompliance of software."}
{"id": "2507.03014", "categories": ["cs.CR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.03014", "abs": "https://arxiv.org/abs/2507.03014", "authors": ["Do-hyeon Yoon", "Minsoo Chun", "Thomas Allen", "Hans Müller", "Min Wang", "Rajesh Sharma"], "title": "Intrinsic Fingerprint of LLMs: Continue Training is NOT All You Need to Steal A Model!", "comment": "This paper flags a potential case of model plagiarism, copyright\n  violation, and information fabrication in arXiv:2505.21411", "summary": "Large language models (LLMs) face significant copyright and intellectual\nproperty challenges as the cost of training increases and model reuse becomes\nprevalent. While watermarking techniques have been proposed to protect model\nownership, they may not be robust to continue training and development, posing\nserious threats to model attribution and copyright protection. This work\nintroduces a simple yet effective approach for robust LLM fingerprinting based\non intrinsic model characteristics. We discover that the standard deviation\ndistributions of attention parameter matrices across different layers exhibit\ndistinctive patterns that remain stable even after extensive continued\ntraining. These parameter distribution signatures serve as robust fingerprints\nthat can reliably identify model lineage and detect potential copyright\ninfringement. Our experimental validation across multiple model families\ndemonstrates the effectiveness of our method for model authentication. Notably,\nour investigation uncovers evidence that a recently Pangu Pro MoE model\nreleased by Huawei is derived from Qwen-2.5 14B model through upcycling\ntechniques rather than training from scratch, highlighting potential cases of\nmodel plagiarism, copyright violation, and information fabrication. These\nfindings underscore the critical importance of developing robust fingerprinting\nmethods for protecting intellectual property in large-scale model development\nand emphasize that deliberate continued training alone is insufficient to\ncompletely obscure model origins."}
{"id": "2507.04354", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04354", "abs": "https://arxiv.org/abs/2507.04354", "authors": ["Yanzhou Mu", "Juan Zhai", "Chunrong Fang", "Xiang Chen", "Zhixiang Cao", "Peiran Yang", "Kexin Zhao", "An Guo", "Zhenyu Chen"], "title": "Improving Deep Learning Framework Testing with Model-Level Metamorphic Testing", "comment": "23 pages, 5 figures", "summary": "Deep learning (DL) frameworks are essential to DL-based software systems, and\nframework bugs may lead to substantial disasters, thus requiring effective\ntesting. Researchers adopt DL models or single interfaces as test inputs and\nanalyze their execution results to detect bugs. However, floating-point errors,\ninherent randomness, and the complexity of test inputs make it challenging to\nanalyze execution results effectively, leading to existing methods suffering\nfrom a lack of suitable test oracles. Some researchers utilize metamorphic\ntesting to tackle this challenge. They design Metamorphic Relations (MRs) based\non input data and parameter settings of a single framework interface to\ngenerate equivalent test inputs, ensuring consistent execution results between\noriginal and generated test inputs. Despite their promising effectiveness, they\nstill face certain limitations. (1) Existing MRs overlook structural\ncomplexity, limiting test input diversity. (2) Existing MRs focus on limited\ninterfaces, which limits generalization and necessitates additional\nadaptations. (3) Their detected bugs are related to the result consistency of\nsingle interfaces and far from those exposed in multi-interface combinations\nand runtime metrics (e.g., resource usage). To address these limitations, we\npropose ModelMeta, a model-level metamorphic testing method for DL frameworks\nwith four MRs focused on the structure characteristics of DL models. ModelMeta\naugments seed models with diverse interface combinations to generate test\ninputs with consistent outputs, guided by the QR-DQN strategy. It then detects\nbugs through fine-grained analysis of training loss/gradients, memory/GPU\nusage, and execution time."}
{"id": "2507.03021", "categories": ["cs.CR", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.03021", "abs": "https://arxiv.org/abs/2507.03021", "authors": ["Ya-Ting Yang", "Quanyan Zhu"], "title": "A Multi-Resolution Dynamic Game Framework for Cross-Echelon Decision-Making in Cyber Warfare", "comment": null, "summary": "Cyber warfare has become a critical dimension of modern conflict, driven by\nsociety's increasing dependence on interconnected digital and physical\ninfrastructure. Effective cyber defense often requires decision-making at\ndifferent echelons, where the tactical layer focuses on detailed actions such\nas techniques, tactics, and procedures, while the strategic layer addresses\nlong-term objectives and coordinated planning. Modeling these interactions at\ndifferent echelons remains challenging due to the dynamic, large-scale, and\ninterdependent nature of cyber environments. To address this, we propose a\nmulti-resolution dynamic game framework in which the tactical layer captures\nfine-grained interactions using high-resolution extensive-form game trees,\nwhile the strategic layer is modeled as a Markov game defined over\nlower-resolution states abstracted from those game trees. This framework\nsupports scalable reasoning and planning across different levels of abstraction\nthrough zoom-in and zoom-out operations that adjust the granularity of the\nmodeling based on operational needs. A case study demonstrates how the\nframework works and its effectiveness in improving the defender's strategic\nadvantage."}
{"id": "2507.04360", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04360", "abs": "https://arxiv.org/abs/2507.04360", "authors": ["Yanzhou Mu", "Juan Zhai", "Chunrong Fang", "Xiang Chen", "Zhixiang Cao", "Peiran Yang", "Yinglong Zou", "Tao Zheng", "Zhenyu Chen"], "title": "DevMuT: Testing Deep Learning Framework via Developer Expertise-Based Mutation", "comment": "12 pages, 8 figures", "summary": "Deep learning (DL) frameworks are the fundamental infrastructure for various\nDL applications. Framework defects can profoundly cause disastrous accidents,\nthus requiring sufficient detection. In previous studies, researchers adopt DL\nmodels as test inputs combined with mutation to generate more diverse models.\nThough these studies demonstrate promising results, most detected defects are\nconsidered trivial (i.e., either treated as edge cases or ignored by the\ndevelopers). To identify important bugs that matter to developers, we propose a\nnovel DL framework testing method DevMuT, which generates models by adopting\nmutation operators and constraints derived from developer expertise. DevMuT\nsimulates developers'common operations in development and detects more diverse\ndefects within more stages of the DL model lifecycle (e.g., model training and\ninference). We evaluate the performance of DevMuT on three widely used DL\nframeworks (i.e., PyTorch, JAX, and Mind- Spore) with 29 DL models from nine\ntypes of industry tasks. The experiment results show that DevMuT outperforms\nstate-of-the-art baselines: it can achieve at least 71.68% improvement on\naverage in the diversity of generated models and 28.20% improvement on average\nin the legal rates of generated models. Moreover, DevMuT detects 117 defects,\n63 of which are confirmed, 24 are fixed, and eight are of high value confirmed\nby developers. Finally, DevMuT has been deployed in the MindSpore community\nsince December 2023. These demonstrate the effectiveness of DevMuT in detecting\ndefects that are close to the real scenes and are of concern to developers."}
{"id": "2507.03051", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.03051", "abs": "https://arxiv.org/abs/2507.03051", "authors": ["Marco Simoni", "Aleksandar Fontana", "Giulio Rossolini", "Andrea Saracino"], "title": "Improving LLM Reasoning for Vulnerability Detection via Group Relative Policy Optimization", "comment": "Under Review", "summary": "Improving and understanding the training dynamics and reasoning of Large\nLanguage Models (LLMs) has become essential for their deployment in AI-based\nsecurity tools, such as software vulnerability detection. In this work, we\npresent an extensive study aimed at advancing recent RL-based finetuning\ntechniques for LLMs in the context of vulnerability detection.\n  We start by highlighting key limitations of commonly adopted LLMs, such as\ntheir tendency to over-predict certain types of vulnerabilities while failing\nto detect others. To address this challenge, we explore the use of Group\nRelative Policy Optimization (GRPO), a recent policy-gradient method, for\nguiding LLM behavior through structured, rule-based rewards. We enable its\napplication to the vulnerability detection task by redefining its advantage\nfunctions and reward signals using annotations from widely used datasets in the\nfield, including BigVul, DiverseVul, and CleanVul.\n  The proposed methodology enables an extensive set of experiments, addressing\nmultiple research questions regarding the impact of GRPO on generalization,\nreasoning capabilities, and performance improvements over standard supervised\nfinetuning (SFT). Our findings offer valuable insights into the potential of\nRL-based training to enhance both the performance and reasoning abilities of\nLLMs in the context of software vulnerability detection."}
{"id": "2507.04390", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04390", "abs": "https://arxiv.org/abs/2507.04390", "authors": ["Vanesya Aura Ardity", "Yusuf Sulistyo Nugroho", "Syful Islam"], "title": "Exploring React Library Related Questions on Stack Overflow: Answered vs. Unanswered", "comment": "6 pages, 9 figures, 7 tables, conference paper", "summary": "React is a popular JavaScript framework in modern web application\ndevelopment. Due to its high performance and efficiency, many developers use\nthis framework. Although React library offers many advantages, it is not\nwithout its challenges. When using React library, developers often face\nproblems where they often seek solutions through question-and-answer forums,\nsuch as Stack Overflow (SO). However, despite its high popularity, many\nReact-related questions on SO remain unanswered. Thus, this study aims to\nanalyze the factors associated with question answerability and difficulty\nlevels of React-related questions on SO. To facilitate our study, Exploratory\nData Analysis was applied to 534,820 questions, where they are filtered based\non 23 React-related tags. We implemented a quantitative approach through text\nmining and statistical analysis. A logistic regression model was used to\nidentify attributes associated with question answerability, while a simple\nlinear regression model was employed to examine the correlation between user\nreputations and performance difficulty scores (PD Score). The results show that\nsome attributes, such as number of views, code snippet inclusion, number of\nlines of code, and user reputation, positively affect the likelihood of\nquestion answerability. In contrast, the number of comments, question lengths,\nand presence of images in React-related questions reduce the probability of a\nquestion receiving responses from users. Further investigation indicates a\nnegative correlation between user reputations and PD Score, where reputation\nincrease corresponds to -0.092 reduction in PD score, signaling experienced\nusers tend to propose more complex technical inquiries. This study provides\ninsights into the characteristics of technical question-and-answer platforms,\nsuch as SO, that users need to consider the answerability factors when posting\nquestions related to React."}
{"id": "2507.03064", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03064", "abs": "https://arxiv.org/abs/2507.03064", "authors": ["Hetvi Shastri", "Walid A. Hanafy", "Li Wu", "David Irwin", "Mani Srivastava", "Prashant Shenoy"], "title": "LLM-Driven Auto Configuration for Transient IoT Device Collaboration", "comment": null, "summary": "Today's Internet of Things (IoT) has evolved from simple sensing and\nactuation devices to those with embedded processing and intelligent services,\nenabling rich collaborations between users and their devices. However, enabling\nsuch collaboration becomes challenging when transient devices need to interact\nwith host devices in temporarily visited environments. In such cases,\nfine-grained access control policies are necessary to ensure secure\ninteractions; however, manually implementing them is often impractical for\nnon-expert users. Moreover, at run-time, the system must automatically\nconfigure the devices and enforce such fine-grained access control rules.\nAdditionally, the system must address the heterogeneity of devices.\n  In this paper, we present CollabIoT, a system that enables secure and\nseamless device collaboration in transient IoT environments. CollabIoT employs\na Large language Model (LLM)-driven approach to convert users' high-level\nintents to fine-grained access control policies. To support secure and seamless\ndevice collaboration, CollabIoT adopts capability-based access control for\nauthorization and uses lightweight proxies for policy enforcement, providing\nhardware-independent abstractions.\n  We implement a prototype of CollabIoT's policy generation and auto\nconfiguration pipelines and evaluate its efficacy on an IoT testbed and in\nlarge-scale emulated environments. We show that our LLM-based policy generation\npipeline is able to generate functional and correct policies with 100%\naccuracy. At runtime, our evaluation shows that our system configures new\ndevices in ~150 ms, and our proxy-based data plane incurs network overheads of\nup to 2 ms and access control overheads up to 0.3 ms."}
{"id": "2507.04422", "categories": ["cs.SE", "cs.AI", "D.2.7; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.04422", "abs": "https://arxiv.org/abs/2507.04422", "authors": ["Guoming Long", "Jingzhi Gong", "Hui Fang", "Tao Chen"], "title": "Learning Software Bug Reports: A Systematic Literature Review", "comment": "Accepted by TOSEM", "summary": "The recent advancement of artificial intelligence, especially machine\nlearning (ML), has significantly impacted software engineering research,\nincluding bug report analysis. ML aims to automate the understanding,\nextraction, and correlation of information from bug reports. Despite its\ngrowing importance, there has been no comprehensive review in this area. In\nthis paper, we present a systematic literature review covering 1,825 papers,\nselecting 204 for detailed analysis. We derive seven key findings: 1) Extensive\nuse of CNN, LSTM, and $k$NN for bug report analysis, with advanced models like\nBERT underutilized due to their complexity. 2) Word2Vec and TF-IDF are popular\nfor feature representation, with a rise in deep learning approaches. 3) Stop\nword removal is the most common preprocessing, with structural methods rising\nafter 2020. 4) Eclipse and Mozilla are the most frequently evaluated software\nprojects. 5) Bug categorization is the most common task, followed by bug\nlocalization and severity prediction. 6) There is increasing attention on\nspecific bugs like non-functional and performance bugs. 7) Common evaluation\nmetrics are F1-score, Recall, Precision, and Accuracy, with $k$-fold\ncross-validation preferred for model evaluation. 8) Many studies lack robust\nstatistical tests. We also identify six promising future research directions to\nprovide useful insights for practitioners."}
{"id": "2507.03136", "categories": ["cs.CR", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.03136", "abs": "https://arxiv.org/abs/2507.03136", "authors": ["Ricardo Queiroz de Araujo Fernandes", "Anderson Santos", "Daniel Maier de Carvalho", "André Luiz Bandeira Molina"], "title": "Holographic Projection and Cyber Attack Surface: A Physical Analogy for Digital Security", "comment": "The paper was produced to base a presentation in the V Jornadas STIC\n  capitulo Panam\\'a", "summary": "This article presents an in-depth exploration of the analogy between the\nHolographic Principle in theoretical physics and cyber attack surfaces in\ndigital security. Building on concepts such as black hole entropy and AdS/CFT\nduality, it highlights how complex infrastructures project their\nvulnerabilities onto their external interfaces. The paper draws a parallel\nbetween a black hole's event horizon, which encodes all internal information,\nand the attack surface, which reflects the internal architecture's security\nposture. Additionally, the article outlines how this conceptual framework can\nguide cybersecurity practices, emphasizing strategies such as attack surface\nreduction, continuous scanning with tools like OWASP ZAP and Greenbone OpenVAS,\nand the implementation of Zero Trust Architecture. This analogy not only\nprovides a unique perspective on digital security but also underscores the\ncritical importance of boundary-level defenses in protecting vast internal\ninfrastructures."}
{"id": "2507.04548", "categories": ["cs.SE", "cs.AI", "cs.LG", "D.2.11; D.2.7; I.2.7; I.5.4"], "pdf": "https://arxiv.org/pdf/2507.04548", "abs": "https://arxiv.org/abs/2507.04548", "authors": ["Renato Cordeiro Ferreira", "Dayanne Gomes", "Vitor Tamae", "Francisco Wernke", "Alfredo Goldman"], "title": "SPIRA: Building an Intelligent System for Respiratory Insufficiency Detection", "comment": "4 pages, 1 figure (1 diagram), published at ISE 2022", "summary": "Respiratory insufficiency is a medic symptom in which a person gets a reduced\namount of oxygen in the blood. This paper reports the experience of building\nSPIRA: an intelligent system for detecting respiratory insufficiency from\nvoice. It compiles challenges faced in two succeeding implementations of the\nsame architecture, summarizing lessons learned on data collection, training,\nand inference for future projects in similar systems."}
{"id": "2507.03236", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03236", "abs": "https://arxiv.org/abs/2507.03236", "authors": ["Noureldin Zahran", "Ahmad Tahmasivand", "Ihsen Alouani", "Khaled Khasawneh", "Mohammed E. Fouda"], "title": "On Jailbreaking Quantized Language Models Through Fault Injection Attacks", "comment": "This work has been published in GLSVLSI 2025", "summary": "The safety alignment of Language Models (LMs) is a critical concern, yet\ntheir integrity can be challenged by direct parameter manipulation attacks,\nsuch as those potentially induced by fault injection. As LMs are increasingly\ndeployed using low-precision quantization for efficiency, this paper\ninvestigates the efficacy of such attacks for jailbreaking aligned LMs across\ndifferent quantization schemes. We propose gradient-guided attacks, including a\ntailored progressive bit-level search algorithm introduced herein and a\ncomparative word-level (single weight update) attack. Our evaluation on\nLlama-3.2-3B, Phi-4-mini, and Llama-3-8B across FP16 (baseline), and\nweight-only quantization (FP8, INT8, INT4) reveals that quantization\nsignificantly influences attack success. While attacks readily achieve high\nsuccess (>80\\% Attack Success Rate, ASR) on FP16 models, within an attack\nbudget of 25 perturbations, FP8 and INT8 models exhibit ASRs below 20\\% and\n50\\%, respectively. Increasing the perturbation budget up to 150 bit-flips, FP8\nmodels maintained ASR below 65\\%, demonstrating some resilience compared to\nINT8 and INT4 models that have high ASR. In addition, analysis of perturbation\nlocations revealed differing architectural targets across quantization schemes,\nwith (FP16, INT4) and (INT8, FP8) showing similar characteristics. Besides,\njailbreaks induced in FP16 models were highly transferable to subsequent\nFP8/INT8 quantization (<5\\% ASR difference), though INT4 significantly reduced\ntransferred ASR (avg. 35\\% drop). These findings highlight that while common\nquantization schemes, particularly FP8, increase the difficulty of direct\nparameter manipulation jailbreaks, vulnerabilities can still persist,\nespecially through post-attack quantization."}
{"id": "2507.04555", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04555", "abs": "https://arxiv.org/abs/2507.04555", "authors": ["Gabriella Waters"], "title": "Testing, Evaluation, Verification and Validation (TEVV) of Digital Twins: A Comprehensive Framework", "comment": "1 figure, 41 pages, 3 tables", "summary": "Digital twins have emerged as a powerful technology for modeling and\nsimulating complex systems across various domains (Fuller et al., 2020; Tao et\nal., 2019). As virtual representations of physical assets, processes, or\nsystems, digital twins enable real-time monitoring, predictive analysis, and\noptimization. However, as digital twins become more sophisticated and integral\nto decision-making processes, ensuring their accuracy, reliability, and ethical\nimplementation is essential. This paper presents a comprehensive framework for\nthe Testing, Evaluation, Verification and Validation (TEVV) of digital twins to\naddress the unique challenges posed by these dynamic and complex virtual\nmodels."}
{"id": "2507.03258", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.03258", "abs": "https://arxiv.org/abs/2507.03258", "authors": ["Zhaorun Lin"], "title": "Novel Blockchain-based Protocols for Electronic Voting and Auctions", "comment": "My thesis for MPhil at HKUST", "summary": "Programmable blockchains have long been a hot research topic given their\ntremendous use in decentralized applications. Smart contracts, using\nblockchains as their underlying technology, inherit the desired properties such\nas verifiability, immutability, and transparency, which make it a great suit in\ntrustless environments.\n  In this thesis, we consider several decentralized protocols to be built on\nblockchains, specifically using smart contracts on Ethereum. We used\nalgorithmic and cryptographic tools in our implementations to further improve\nthe level of security and efficiency beyond the state-of-the-art works. We\nproposed a new approach called Blind Vote, which is an untraceable, secure,\nefficient, secrecy-preserving, and fully on-chain electronic voting protocol\nbased on the well-known concept of Chaum's blind signatures. We illustrate that\nour approach achieves the same security guarantees as previous methods such as\nTornado Vote [1], while consuming significantly less gas. Thus, we provide a\ncheaper and considerably more gas-efficient alternative for anonymous\nblockchain-based voting. On the other hand, we propose a new family of\nalgorithms for private, trustless auctions that protect bidder identities and\nbid values while remaining practical for smart contract execution. We ensure\ntrustlessness by running the auction logic in a smart contract, thereby\neliminating reliance on any single trusted party. This approach prevents bid\ntampering, front-running, and collusion by enforcing immutability and\ndecentralized verification of bids. The resulting protocol uniquely combines\nefficiency, trustlessness, and enduring bid privacy, offering a scalable and\nsecure solution for blockchain-based marketplaces and other decentralized\napplications."}
{"id": "2507.04857", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04857", "abs": "https://arxiv.org/abs/2507.04857", "authors": ["Weiqi Wang", "Marie Farrell", "Lucas C. Cordeiro", "Liping Zhao"], "title": "Supporting Software Formal Verification with Large Language Models: An Experimental Study", "comment": "Accepted for publication in 2025 IEEE 33rd International Requirements\n  Engineering Conference (RE)", "summary": "Formal methods have been employed for requirements verification for a long\ntime. However, it is difficult to automatically derive properties from natural\nlanguage requirements. SpecVerify addresses this challenge by integrating large\nlanguage models (LLMs) with formal verification tools, providing a more\nflexible mechanism for expressing requirements. This framework combines Claude\n3.5 Sonnet with the ESBMC verifier to form an automated workflow. Evaluated on\nnine cyber-physical systems from Lockheed Martin, SpecVerify achieves 46.5%\nverification accuracy, comparable to NASA's CoCoSim, but with lower false\npositives. Our framework formulates assertions that extend beyond the\nexpressive power of LTL and identifies falsifiable cases that are missed by\nmore traditional methods. Counterexample analysis reveals CoCoSim's limitations\nstemming from model connection errors and numerical approximation issues. While\nSpecVerify advances verification automation, our comparative study of Claude,\nChatGPT, and Llama shows that high-quality requirements documentation and human\nmonitoring remain critical, as models occasionally misinterpret specifications.\nOur results demonstrate that LLMs can significantly reduce the barriers to\nformal verification, while highlighting the continued importance of\nhuman-machine collaboration in achieving optimal results."}
{"id": "2507.03278", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.03278", "abs": "https://arxiv.org/abs/2507.03278", "authors": ["Jiaqi Xue", "Yifei Zhao", "Mengxin Zheng", "Xun Chen", "Fan Yao", "Yan Solihin", "Qian Lou"], "title": "Securing Transformer-based AI Execution via Unified TEE and Crypto-protected Accelerators", "comment": "15 pages", "summary": "Recent advances in Transformer models, e.g., large language models (LLMs),\nhave brought tremendous breakthroughs in various artificial intelligence (AI)\ntasks, leading to their wide applications in many security-critical domains.\nDue to their unprecedented scale and prohibitively high development cost, these\nmodels have become highly valuable intellectual property for AI stakeholders\nand are increasingly deployed via machine learning as a service (MLaaS).\nHowever, MLaaS often runs on untrusted cloud infrastructure, exposing data and\nmodels to potential breaches. Mainstream protection mechanisms leverage trusted\nexecution environments (TEEs) where confidentiality and integrity for secretive\ndata are shielded using hardware-based encryption and integrity checking.\nUnfortunately, running model inference entirely within TEEs is subject to\nnon-trivial slowdown, which is further exacerbated in LLMs due to the\nsubstantial computation and memory footprint involved. Recent studies reveal\nthat the hybrid TEE-based scheme offloading partial model inference operations\nto the untrusted accelerators (e.g., GPU) is a promising solution. However,\nprior offloading schemes fail to ensure dual protection of data and model in\nTransformer inference, as they cannot securely offload critical operations,\ni.e., Attention and SoftMax, forcing these computations to remain confined\nwithin TEEs. To address these challenges, we propose TwinShield, a framework\nenabling secure Transformer inference in heterogeneous TEE and accelerator\nsystems with dual protection for both model and data. TwinShield offloads ~87%\nof computation to GPUs and delivers 4.0x - 6.1x speedups over previous\napproaches across various Transformer models."}
{"id": "2507.04871", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04871", "abs": "https://arxiv.org/abs/2507.04871", "authors": ["Jerome Pfeiffer", "Jingxi Zhang", "Benoit Combemale", "Judith Michael", "Bernhard Rumpe", "Manuel Wimmer", "Andreas Wortmann"], "title": "Towards a Unifying Reference Model for Digital Twins of Cyber-Physical Systems", "comment": null, "summary": "Digital twins are sophisticated software systems for the representation,\nmonitoring, and control of cyber-physical systems, including automotive,\navionics, smart manufacturing, and many more. Existing definitions and\nreference models of digital twins are overly abstract, impeding their\ncomprehensive understanding and implementation guidance. Consequently, a\nsignificant gap emerges between abstract concepts and their industrial\nimplementations. We analyze popular reference models for digital twins and\ncombine these into a significantly detailed unifying reference model for\ndigital twins that reduces the concept-implementation gap to facilitate their\nengineering in industrial practice. This enhances the understanding of the\nconcepts of digital twins and their relationships and guides developers to\nimplement digital twins effectively."}
{"id": "2507.03323", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.03323", "abs": "https://arxiv.org/abs/2507.03323", "authors": ["Kazumasa Shinagawa", "Koji Nuida"], "title": "A Note on Single-Cut Full-Open Protocols", "comment": null, "summary": "Card-based cryptography is a research area that realizes cryptographic\nprotocols such as secure computation by applying shuffles to sequences of cards\nthat encode input values. A single-cut full-open protocol is one that obtains\nan output value by applying a random cut to an input sequence of cards, after\nwhich all cards are opened. In this paper, we propose three single-cut\nfull-open protocols: two protocols for three-variable functions and one\nprotocol for a four-variable function."}
{"id": "2507.05100", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05100", "abs": "https://arxiv.org/abs/2507.05100", "authors": ["Haoran Wei", "Nazim Madhavji", "John Steinbacher"], "title": "Understanding Everything as Code: A Taxonomy and Conceptual Model", "comment": "Accepted by the 19th ACM/IEEE International Symposium on Empirical\n  Software Engineering and Measurement (ESEM 2025), Technical Papers track", "summary": "Background: Everything as Code (EaC) is an emerging paradigm aiming to codify\nall aspects of modern software systems. Despite its growing popularity,\ncomprehensive industry standards and peer-reviewed research clarifying its\nscope and guiding its adoption remain scarce. Aims: This study systematically\nanalyzes existing knowledge and perceptions of EaC, clarifies its scope and\nboundaries, and provides structured guidance for researchers and practitioners.\nMethod: We conducted a large-scale multivocal literature review (MLR),\nsynthesizing academic and grey literature sources. Findings were analyzed\nquantitatively and thematically. Based on this analysis, we developed a\ntaxonomy and conceptual model of EaC, validated through collaboration with\nindustry experts. Results: The resulting taxonomy comprises 25 distinct EaC\npractices organized into six layers based on industry awareness and functional\nroles. The conceptual model illustrates focus areas, overlaps, and interactions\namong these EaC practices within the software delivery lifecycle. Additionally,\npractical code examples demonstrating the implementation of these practices\nwere developed in collaboration with industry experts. Conclusions: This work\naddresses the current scarcity of academic discourse on EaC by providing the\nfirst comprehensive taxonomy and conceptual model. These contributions enhance\nconceptual clarity, offer actionable guidance to practitioners, and lay the\ngroundwork for future research in this emerging domain."}
{"id": "2507.03344", "categories": ["cs.CR", "cs.SE", "C.1.3; D.2.5"], "pdf": "https://arxiv.org/pdf/2507.03344", "abs": "https://arxiv.org/abs/2507.03344", "authors": ["Jason Zhijingcheng Yu", "Fangqi Han", "Kaustab Choudhury", "Trevor E. Carlson", "Prateek Saxena"], "title": "Securing Mixed Rust with Hardware Capabilities", "comment": "To appear at CCS '25", "summary": "The Rust programming language enforces three basic Rust principles, namely\nownership, borrowing, and AXM (Aliasing Xor Mutability) to prevent security\nbugs such as memory safety violations and data races. However, Rust projects\noften have mixed code, i.e., code that also uses unsafe Rust, FFI (Foreign\nFunction Interfaces), and inline assembly for low-level control. The Rust\ncompiler is unable to statically enforce Rust principles in mixed Rust code\nwhich can lead to many security vulnerabilities. In this paper, we propose\nCapsLock, a security enforcement mechanism that can run at the level of machine\ncode and detect Rust principle violations at run-time in mixed code. CapsLock\nis kept simple enough to be implemented into recent capability-based hardware\nabstractions that provide low-cost spatial memory safety. CapsLock introduces a\nnovel revoke-on-use abstraction for capability-based designs, wherein accessing\na memory object via a capability implicitly invalidates certain other\ncapabilities pointing to it, thereby also providing temporal memory safety\nautomatically, without requiring software to explicitly specify such\ninvalidation. Thus, CapsLock is the first mechanism capable of providing\ncross-language enforcement of Rust principles. We implemented a prototype of\nCapsLock on QEMU. Evaluation results show that CapsLock is highly compatible\nwith existing Rust code (passing 99.7% of the built-in test cases of the 100\nmost popular crates) and flags Rust principle violations in real-world Rust\nprojects that use FFI or inline assembly. We discovered 8 previously unknown\nbugs in such crates in our experiments."}
{"id": "2507.05200", "categories": ["cs.SE", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.05200", "abs": "https://arxiv.org/abs/2507.05200", "authors": ["Susmita Das", "Madhusudan Ghosh", "Priyanka Swami", "Debasis Ganguly", "Gul Calikli"], "title": "In-Context Learning as an Effective Estimator of Functional Correctness of LLM-Generated Code", "comment": null, "summary": "When applying LLM-based code generation to software development projects that\nfollow a feature-driven or rapid application development approach, it becomes\nnecessary to estimate the functional correctness of the generated code in the\nabsence of test cases. Just as a user selects a relevant document from a ranked\nlist of retrieved ones, a software generation workflow requires a developer to\nchoose (and potentially refine) a generated solution from a ranked list of\nalternative solutions, ordered by their posterior likelihoods. This implies\nthat estimating the quality of a ranked list -- akin to estimating \"relevance\"\nfor query performance prediction (QPP) in IR -- is also crucial for generative\nsoftware development, where quality is defined in terms of \"functional\ncorrectness\". In this paper, we propose an in-context learning (ICL) based\napproach for code quality estimation. Our findings demonstrate that providing\nfew-shot examples of functionally correct code from a training set enhances the\nperformance of existing QPP approaches as well as a zero-shot-based approach\nfor code quality estimation."}
{"id": "2507.03361", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.03361", "abs": "https://arxiv.org/abs/2507.03361", "authors": ["Rayne Holland"], "title": "Accelerating Private Heavy Hitter Detection on Continual Observation Streams", "comment": "24 pages, 8 figures", "summary": "Differentially private frequency estimation and heavy hitter detection are\ncore problems in the private analysis of data streams. Two models are typically\nconsidered: the one-pass model, which outputs results only at the end of the\nstream, and the continual observation model, which requires releasing private\nsummaries at every time step. While the one-pass model allows more efficient\nsolutions, continual observation better reflects scenarios where timely and\nongoing insights are critical.\n  In the one-pass setting, sketches have proven to be an effective tool for\ndifferentially private frequency analysis, as they can be privatized by a\nsingle injection of calibrated noise. In contrast, existing methods in the\ncontinual observation model add fresh noise to the entire sketch at every step,\nincurring high computational costs. This challenge is particularly acute for\nheavy hitter detection, where current approaches often require querying every\nitem in the universe at each step, resulting in untenable per-update costs for\nlarge domains.\n  To overcome these limitations, we introduce a new differentially private\nsketching technique based on lazy updates, which perturbs and updates only a\nsmall, rotating part of the output sketch at each time step. This significantly\nreduces computational overhead while maintaining strong privacy and utility\nguarantees. In comparison to prior art, for frequency estimation, our method\nimproves the update time by a factor of $O(w)$ for sketches of dimension $d\n\\times w$; for heavy hitter detection, it reduces per-update complexity from\n$\\Omega(|U|)$ to $O(d \\log w)$, where $U$ is the input domain. Experiments show\na increase in throughput by a factor of~$250$, making differential privacy more\npractical for real-time, continual observation, applications."}
{"id": "2507.05245", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05245", "abs": "https://arxiv.org/abs/2507.05245", "authors": ["Fatema Tuz Zohra", "Brittany Johnson"], "title": "An Investigation into Maintenance Support for Neural Networks", "comment": "Revised version accepted at the HumanAISE Workshop, co-located with\n  FSE 2025", "summary": "As the potential for neural networks to augment our daily lives grows,\nensuring their quality through effective testing, debugging, and maintenance is\nessential. This is especially the case as we acknowledge the prospects of\nnegative impacts from these technologies. Traditional software engineering\nmethods, such as testing and debugging, have proven effective in maintaining\nsoftware quality; however, they reveal significant research and practice gaps\nin maintaining neural networks. In particular, there is a limited understanding\nof how practitioners currently address challenges related to understanding and\nmitigating undesirable behaviors in neural networks. In our ongoing research,\nwe explore the current state of research and practice in maintaining neural\nnetworks by curating insights from practitioners through a preliminary study\ninvolving interviews and supporting survey responses. Our findings thus far\nindicate that existing tools primarily concentrate on building and training\nmodels. While these tools can be beneficial, they often fall short of\nsupporting practitioners' understanding and addressing the underlying causes of\nunexpected model behavior. By evaluating current procedures and identifying the\nlimitations of traditional methodologies, our study aims to offer a\ndeveloper-centric perspective on where current practices fall short and\nhighlight opportunities for improving maintenance support in neural networks."}
{"id": "2507.03387", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.03387", "abs": "https://arxiv.org/abs/2507.03387", "authors": ["Andong Chen", "Zhaoxuan Jin", "Ziyi Guo", "Yan Chen"], "title": "Breaking the Bulkhead: Demystifying Cross-Namespace Reference Vulnerabilities in Kubernetes Operators", "comment": "12 pages", "summary": "Kubernetes Operators, automated tools designed to manage application\nlifecycles within Kubernetes clusters, extend the functionalities of\nKubernetes, and reduce the operational burden on human engineers. While\nOperators significantly simplify DevOps workflows, they introduce new security\nrisks. In particular, Kubernetes enforces namespace isolation to separate\nworkloads and limit user access, ensuring that users can only interact with\nresources within their authorized namespaces. However, Kubernetes Operators\noften demand elevated privileges and may interact with resources across\nmultiple namespaces. This introduces a new class of vulnerabilities, the\nCross-Namespace Reference Vulnerability. The root cause lies in the mismatch\nbetween the declared scope of resources and the implemented scope of the\nOperator logic, resulting in Kubernetes being unable to properly isolate the\nnamespace. Leveraging such vulnerability, an adversary with limited access to a\nsingle authorized namespace may exploit the Operator to perform operations\naffecting other unauthorized namespaces, causing Privilege Escalation and\nfurther impacts. To the best of our knowledge, this paper is the first to\nsystematically investigate the security vulnerability of Kubernetes Operators.\nWe present Cross-Namespace Reference Vulnerability with two strategies,\ndemonstrating how an attacker can bypass namespace isolation. Through\nlarge-scale measurements, we found that over 14% of Operators in the wild are\npotentially vulnerable. Our findings have been reported to the relevant\ndevelopers, resulting in 7 confirmations and 6 CVEs by the time of submission,\naffecting vendors including ****** and ******, highlighting the critical need\nfor enhanced security practices in Kubernetes Operators. To mitigate it, we\nalso open-source the static analysis suite to benefit the ecosystem."}
{"id": "2507.02976", "categories": ["cs.CR", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.02976", "abs": "https://arxiv.org/abs/2507.02976", "authors": ["Amirali Sajadi", "Kostadin Damevski", "Preetha Chatterjee"], "title": "Are AI-Generated Fixes Secure? Analyzing LLM and Agent Patches on SWE-bench", "comment": null, "summary": "Large Language Models (LLMs) and their agentic frameworks are increasingly\nadopted to automate software development tasks such as issue resolution and\nprogram repair. While prior work has identified security risks in LLM-generated\ncode, most evaluations have focused on synthetic or isolated settings, leaving\nopen questions about the security of these systems in real-world development\ncontexts. In this study, we present the first large-scale security analysis of\nLLM-generated patches using 20,000+ issues from the SWE-bench dataset. We\nevaluate patches produced by a standalone LLM (Llama 3.3) and compare them to\ndeveloper-written patches. We also assess the security of patches generated by\nthree top-performing agentic frameworks (OpenHands, AutoCodeRover, HoneyComb)\non a subset of our data. Finally, we analyze a wide range of code, issue, and\nproject-level factors to understand the conditions under which LLMs and agents\nare most likely to generate insecure code. Our findings reveal that the\nstandalone LLM introduces nearly 9x more new vulnerabilities than developers,\nwith many of these exhibiting unique patterns not found in developers' code.\nAgentic workflows also generate a significant number of vulnerabilities,\nparticularly when granting LLMs more autonomy, potentially increasing the\nlikelihood of misinterpreting project context or task requirements. We find\nthat vulnerabilities are more likely to occur in LLM patches associated with a\nhigher number of files, more lines of generated code, and GitHub issues that\nlack specific code snippets or information about the expected code behavior and\nsteps to reproduce. These results suggest that contextual factors play a\ncritical role in the security of the generated code and point toward the need\nfor proactive risk assessment methods that account for both code and\nissue-level information to complement existing vulnerability detection tools."}
{"id": "2507.03450", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.03450", "abs": "https://arxiv.org/abs/2507.03450", "authors": ["Antonio Emanuele Cinà", "Maura Pintor", "Luca Demetrio", "Ambra Demontis", "Battista Biggio", "Fabio Roli"], "title": "Evaluating the Evaluators: Trust in Adversarial Robustness Tests", "comment": null, "summary": "Despite significant progress in designing powerful adversarial evasion\nattacks for robustness verification, the evaluation of these methods often\nremains inconsistent and unreliable. Many assessments rely on mismatched\nmodels, unverified implementations, and uneven computational budgets, which can\nlead to biased results and a false sense of security. Consequently, robustness\nclaims built on such flawed testing protocols may be misleading and give a\nfalse sense of security. As a concrete step toward improving evaluation\nreliability, we present AttackBench, a benchmark framework developed to assess\nthe effectiveness of gradient-based attacks under standardized and reproducible\nconditions. AttackBench serves as an evaluation tool that ranks existing attack\nimplementations based on a novel optimality metric, which enables researchers\nand practitioners to identify the most reliable and effective attack for use in\nsubsequent robustness evaluations. The framework enforces consistent testing\nconditions and enables continuous updates, making it a reliable foundation for\nrobustness verification."}
{"id": "2507.03344", "categories": ["cs.CR", "cs.SE", "C.1.3; D.2.5"], "pdf": "https://arxiv.org/pdf/2507.03344", "abs": "https://arxiv.org/abs/2507.03344", "authors": ["Jason Zhijingcheng Yu", "Fangqi Han", "Kaustab Choudhury", "Trevor E. Carlson", "Prateek Saxena"], "title": "Securing Mixed Rust with Hardware Capabilities", "comment": "To appear at CCS '25", "summary": "The Rust programming language enforces three basic Rust principles, namely\nownership, borrowing, and AXM (Aliasing Xor Mutability) to prevent security\nbugs such as memory safety violations and data races. However, Rust projects\noften have mixed code, i.e., code that also uses unsafe Rust, FFI (Foreign\nFunction Interfaces), and inline assembly for low-level control. The Rust\ncompiler is unable to statically enforce Rust principles in mixed Rust code\nwhich can lead to many security vulnerabilities. In this paper, we propose\nCapsLock, a security enforcement mechanism that can run at the level of machine\ncode and detect Rust principle violations at run-time in mixed code. CapsLock\nis kept simple enough to be implemented into recent capability-based hardware\nabstractions that provide low-cost spatial memory safety. CapsLock introduces a\nnovel revoke-on-use abstraction for capability-based designs, wherein accessing\na memory object via a capability implicitly invalidates certain other\ncapabilities pointing to it, thereby also providing temporal memory safety\nautomatically, without requiring software to explicitly specify such\ninvalidation. Thus, CapsLock is the first mechanism capable of providing\ncross-language enforcement of Rust principles. We implemented a prototype of\nCapsLock on QEMU. Evaluation results show that CapsLock is highly compatible\nwith existing Rust code (passing 99.7% of the built-in test cases of the 100\nmost popular crates) and flags Rust principle violations in real-world Rust\nprojects that use FFI or inline assembly. We discovered 8 previously unknown\nbugs in such crates in our experiments."}
{"id": "2507.03607", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.03607", "abs": "https://arxiv.org/abs/2507.03607", "authors": ["Cédric Bonhomme", "Alexandre Dulaunoy"], "title": "VLAI: A RoBERTa-Based Model for Automated Vulnerability Severity Classification", "comment": "This paper is a preprint for the 25V4C-TC: 2025 Vulnerability\n  Forecasting Technical Colloquia. Darwin College Cambridge, UK, September\n  25-26, 2025", "summary": "This paper presents VLAI, a transformer-based model that predicts software\nvulnerability severity levels directly from text descriptions. Built on\nRoBERTa, VLAI is fine-tuned on over 600,000 real-world vulnerabilities and\nachieves over 82% accuracy in predicting severity categories, enabling faster\nand more consistent triage ahead of manual CVSS scoring. The model and dataset\nare open-source and integrated into the Vulnerability-Lookup service."}
{"id": "2507.03773", "categories": ["cs.CR", "cs.DC", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03773", "abs": "https://arxiv.org/abs/2507.03773", "authors": ["Yibo He", "Cunjian Huang", "Xianmiao Qu", "Hongdeng Chen", "Wei Yang", "Tao Xie"], "title": "RVISmith: Fuzzing Compilers for RVV Intrinsics", "comment": "To appear in ACM CCS 2025", "summary": "Modern processors are equipped with single instruction multiple data (SIMD)\ninstructions for fine-grained data parallelism. Compiler auto-vectorization\ntechniques that target SIMD instructions face performance limitations due to\ninsufficient information available at compile time, requiring programmers to\nmanually manipulate SIMD instructions. SIMD intrinsics, a type of built-in\nfunction provided by modern compilers, enable programmers to manipulate SIMD\ninstructions within high-level programming languages. Bugs in compilers for\nSIMD intrinsics can introduce potential threats to software security, producing\nunintended calculation results, data loss, program crashes, etc.\n  To detect bugs in compilers for SIMD intrinsics, we propose RVISmith, a\nrandomized fuzzer that generates well-defined C programs that include various\ninvocation sequences of RVV (RISC-V Vector Extension) intrinsics. We design\nRVISmith to achieve the following objectives: (i) achieving high intrinsic\ncoverage, (ii) improving sequence variety, and (iii) without known undefined\nbehaviors. We implement RVISmith based on the ratified RVV intrinsic\nspecification and evaluate our approach with three modern compilers: GCC, LLVM,\nand XuanTie. Experimental results show that RVISmith achieves 11.5 times higher\nintrinsic coverage than the state-of-the-art fuzzer for RVV intrinsics. By\ndifferential testing that compares results across different compilers,\noptimizations, and equivalent programs, we detect and report 13 previously\nunknown bugs of the three compilers under test to date. Of these bugs, 10 are\nconfirmed and another 3 are fixed by the compiler developers."}
{"id": "2507.03619", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.03619", "abs": "https://arxiv.org/abs/2507.03619", "authors": ["Ruikai Zhou", "Kang Yang", "Xun Chen", "Wendy Hui Wang", "Guanhong Tao", "Jun Xu"], "title": "Blackbox Dataset Inference for LLM", "comment": null, "summary": "Today, the training of large language models (LLMs) can involve personally\nidentifiable information and copyrighted material, incurring dataset misuse. To\nmitigate the problem of dataset misuse, this paper explores \\textit{dataset\ninference}, which aims to detect if a suspect model $\\mathcal{M}$ used a victim\ndataset $\\mathcal{D}$ in training. Previous research tackles dataset inference\nby aggregating results of membership inference attacks (MIAs) -- methods to\ndetermine whether individual samples are a part of the training dataset.\nHowever, restricted by the low accuracy of MIAs, previous research mandates\ngrey-box access to $\\mathcal{M}$ to get intermediate outputs (probabilities,\nloss, perplexity, etc.) for obtaining satisfactory results. This leads to\nreduced practicality, as LLMs, especially those deployed for profits, have\nlimited incentives to return the intermediate outputs.\n  In this paper, we propose a new method of dataset inference with only\nblack-box access to the target model (i.e., assuming only the text-based\nresponses of the target model are available). Our method is enabled by two sets\nof locally built reference models, one set involving $\\mathcal{D}$ in training\nand the other not. By measuring which set of reference model $\\mathcal{M}$ is\ncloser to, we determine if $\\mathcal{M}$ used $\\mathcal{D}$ for training.\nEvaluations of real-world LLMs in the wild show that our method offers high\naccuracy in all settings and presents robustness against bypassing attempts."}
{"id": "2507.04055", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04055", "abs": "https://arxiv.org/abs/2507.04055", "authors": ["Yufan Chen", "Daoyuan Wu", "Juantao Zhong", "Zicheng Zhang", "Debin Gao", "Shuai Wang", "Yingjiu Li", "Ning Liu"], "title": "Rethinking and Exploring String-Based Malware Family Classification in the Era of LLMs and RAG", "comment": null, "summary": "Malware Family Classification (MFC) aims to identify the fine-grained family\n(e.g., GuLoader or BitRAT) to which a potential malware sample belongs, in\ncontrast to malware detection or sample classification that predicts only an\nYes/No. Accurate family identification can greatly facilitate automated sample\nlabeling and understanding on crowdsourced malware analysis platforms such as\nVirusTotal and MalwareBazaar, which generate vast amounts of data daily. In\nthis paper, we explore and assess the feasibility of using traditional binary\nstring features for MFC in the new era of large language models (LLMs) and\nRetrieval-Augmented Generation (RAG). Specifically, we investigate how\nFamily-Specific String (FSS) features could be utilized in a manner similar to\nRAG to facilitate MFC. To this end, we develop a curated evaluation framework\ncovering 4,347 samples from 67 malware families, extract and analyze over 25\nmillion strings, and conduct detailed ablation studies to assess the impact of\ndifferent design choices in four major modules."}
{"id": "2507.03636", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.03636", "abs": "https://arxiv.org/abs/2507.03636", "authors": ["Xiaodong Wu", "Xiangman Li", "Qi Li", "Jianbing Ni", "Rongxing Lu"], "title": "SecureT2I: No More Unauthorized Manipulation on AI Generated Images from Prompts", "comment": null, "summary": "Text-guided image manipulation with diffusion models enables flexible and\nprecise editing based on prompts, but raises ethical and copyright concerns due\nto potential unauthorized modifications. To address this, we propose SecureT2I,\na secure framework designed to prevent unauthorized editing in diffusion-based\ngenerative models. SecureT2I is compatible with both general-purpose and\ndomain-specific models and can be integrated via lightweight fine-tuning\nwithout architectural changes. We categorize images into a permit set and a\nforbid set based on editing permissions. For the permit set, the model learns\nto perform high-quality manipulations as usual. For the forbid set, we\nintroduce training objectives that encourage vague or semantically ambiguous\noutputs (e.g., blurred images), thereby suppressing meaningful edits. The core\nchallenge is to block unauthorized editing while preserving editing quality for\npermitted inputs. To this end, we design separate loss functions that guide\nselective editing behavior. Extensive experiments across multiple datasets and\nmodels show that SecureT2I effectively degrades manipulation quality on\nforbidden images while maintaining performance on permitted ones. We also\nevaluate generalization to unseen inputs and find that SecureT2I consistently\noutperforms baselines. Additionally, we analyze different vagueness strategies\nand find that resize-based degradation offers the best trade-off for secure\nmanipulation control."}
{"id": "2507.03646", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.03646", "abs": "https://arxiv.org/abs/2507.03646", "authors": ["Xiaodong Wu", "Tianyi Tang", "Xiangman Li", "Jianbing Ni", "Yong Yu"], "title": "When There Is No Decoder: Removing Watermarks from Stable Diffusion Models in a No-box Setting", "comment": "arXiv admin note: text overlap with arXiv:2408.02035", "summary": "Watermarking has emerged as a promising solution to counter harmful or\ndeceptive AI-generated content by embedding hidden identifiers that trace\ncontent origins. However, the robustness of current watermarking techniques is\nstill largely unexplored, raising critical questions about their effectiveness\nagainst adversarial attacks. To address this gap, we examine the robustness of\nmodel-specific watermarking, where watermark embedding is integrated with\ntext-to-image generation in models like latent diffusion models. We introduce\nthree attack strategies: edge prediction-based, box blurring, and\nfine-tuning-based attacks in a no-box setting, where an attacker does not\nrequire access to the ground-truth watermark decoder. Our findings reveal that\nwhile model-specific watermarking is resilient against basic evasion attempts,\nsuch as edge prediction, it is notably vulnerable to blurring and\nfine-tuning-based attacks. Our best-performing attack achieves a reduction in\nwatermark detection accuracy to approximately 47.92\\%. Additionally, we perform\nan ablation study on factors like message length, kernel size and decoder\ndepth, identifying critical parameters influencing the fine-tuning attack's\nsuccess. Finally, we assess several advanced watermarking defenses, finding\nthat even the most robust methods, such as multi-label smoothing, result in\nwatermark extraction accuracy that falls below an acceptable level when\nsubjected to our no-box attacks."}
{"id": "2507.03694", "categories": ["cs.CR", "cs.CE", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.03694", "abs": "https://arxiv.org/abs/2507.03694", "authors": ["Jovonni L. PHarr"], "title": "Willchain: Decentralized, Privacy-Preserving, Self-Executing, Digital Wills", "comment": null, "summary": "This work presents a novel decentralized protocol for digital estate planning\nthat integrates advances distributed computing, and cryptography. The original\nproof-of-concept was constructed using purely solidity contracts. Since then,\nwe have enhanced the implementation into a layer-1 protocol that uses modern\ninterchain communication to connect several heterogeneous chain types. A key\ncontribution of this research is the implementation of several modern\ncryptographic primitives to support various forms of claims for information\nvalidation. These primitives introduce an unmatched level of privacy to the\nprocess of digital inheritance. We also demonstrate on a set of heterogeneous\nsmart contracts, following the same spec, on each chain to serve as entry\npoints, gateways, or bridge contracts that are invoked via a path from the will\nmodule on our protocol, to the contract. This ensures a fair and secure\ndistribution of digital assets in accordance with the wishes of the decedent\nwithout the requirement of moving their funds. This research further extends\nits innovations with a user interaction model, featuring a check-in system and\naccount abstraction process, which enhances flexibility and user-friendliness\nwithout compromising on security. By developing a dedicated permissionless\nblockchain that is secured by a network of validators, and interchain relayers,\nthe proposed protocol signifies a transformation in the digital estate planning\nindustry and illustrates the potential of blockchain technology in\nrevolutionizing traditional legal and personal spheres. Implementing a\ncryptoeconomic network at the core of inheritance planning allows for unique\nincentive compatible economic mechanisms to be constructed."}
{"id": "2507.03773", "categories": ["cs.CR", "cs.DC", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03773", "abs": "https://arxiv.org/abs/2507.03773", "authors": ["Yibo He", "Cunjian Huang", "Xianmiao Qu", "Hongdeng Chen", "Wei Yang", "Tao Xie"], "title": "RVISmith: Fuzzing Compilers for RVV Intrinsics", "comment": "To appear in ACM CCS 2025", "summary": "Modern processors are equipped with single instruction multiple data (SIMD)\ninstructions for fine-grained data parallelism. Compiler auto-vectorization\ntechniques that target SIMD instructions face performance limitations due to\ninsufficient information available at compile time, requiring programmers to\nmanually manipulate SIMD instructions. SIMD intrinsics, a type of built-in\nfunction provided by modern compilers, enable programmers to manipulate SIMD\ninstructions within high-level programming languages. Bugs in compilers for\nSIMD intrinsics can introduce potential threats to software security, producing\nunintended calculation results, data loss, program crashes, etc.\n  To detect bugs in compilers for SIMD intrinsics, we propose RVISmith, a\nrandomized fuzzer that generates well-defined C programs that include various\ninvocation sequences of RVV (RISC-V Vector Extension) intrinsics. We design\nRVISmith to achieve the following objectives: (i) achieving high intrinsic\ncoverage, (ii) improving sequence variety, and (iii) without known undefined\nbehaviors. We implement RVISmith based on the ratified RVV intrinsic\nspecification and evaluate our approach with three modern compilers: GCC, LLVM,\nand XuanTie. Experimental results show that RVISmith achieves 11.5 times higher\nintrinsic coverage than the state-of-the-art fuzzer for RVV intrinsics. By\ndifferential testing that compares results across different compilers,\noptimizations, and equivalent programs, we detect and report 13 previously\nunknown bugs of the three compilers under test to date. Of these bugs, 10 are\nconfirmed and another 3 are fixed by the compiler developers."}
{"id": "2507.03993", "categories": ["cs.CR", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.03993", "abs": "https://arxiv.org/abs/2507.03993", "authors": ["Dipo Dunsin", "Mohamed Chahine Ghanem", "Eduardo Almeida Palmieri"], "title": "MalVol-25: A Diverse, Labelled and Detailed Volatile Memory Dataset for Malware Detection and Response Testing and Validation", "comment": "6 pages", "summary": "This paper addresses the critical need for high-quality malware datasets that\nsupport advanced analysis techniques, particularly machine learning and agentic\nAI frameworks. Existing datasets often lack diversity, comprehensive labelling,\nand the complexity necessary for effective machine learning and agent-based AI\ntraining. To fill this gap, we developed a systematic approach for generating a\ndataset that combines automated malware execution in controlled virtual\nenvironments with dynamic monitoring tools. The resulting dataset comprises\nclean and infected memory snapshots across multiple malware families and\noperating systems, capturing detailed behavioural and environmental features.\nKey design decisions include applying ethical and legal compliance, thorough\nvalidation using both automated and manual methods, and comprehensive\ndocumentation to ensure replicability and integrity. The dataset's distinctive\nfeatures enable modelling system states and transitions, facilitating RL-based\nmalware detection and response strategies. This resource is significant for\nadvancing adaptive cybersecurity defences and digital forensic research. Its\nscope supports diverse malware scenarios and offers potential for broader\napplications in incident response and automated threat mitigation."}
{"id": "2507.04055", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04055", "abs": "https://arxiv.org/abs/2507.04055", "authors": ["Yufan Chen", "Daoyuan Wu", "Juantao Zhong", "Zicheng Zhang", "Debin Gao", "Shuai Wang", "Yingjiu Li", "Ning Liu"], "title": "Rethinking and Exploring String-Based Malware Family Classification in the Era of LLMs and RAG", "comment": null, "summary": "Malware Family Classification (MFC) aims to identify the fine-grained family\n(e.g., GuLoader or BitRAT) to which a potential malware sample belongs, in\ncontrast to malware detection or sample classification that predicts only an\nYes/No. Accurate family identification can greatly facilitate automated sample\nlabeling and understanding on crowdsourced malware analysis platforms such as\nVirusTotal and MalwareBazaar, which generate vast amounts of data daily. In\nthis paper, we explore and assess the feasibility of using traditional binary\nstring features for MFC in the new era of large language models (LLMs) and\nRetrieval-Augmented Generation (RAG). Specifically, we investigate how\nFamily-Specific String (FSS) features could be utilized in a manner similar to\nRAG to facilitate MFC. To this end, we develop a curated evaluation framework\ncovering 4,347 samples from 67 malware families, extract and analyze over 25\nmillion strings, and conduct detailed ablation studies to assess the impact of\ndifferent design choices in four major modules."}
{"id": "2507.04077", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.04077", "abs": "https://arxiv.org/abs/2507.04077", "authors": ["Yue Su", "Meng Shen", "Cong Zuo", "Yuzhi Liu", "Liehuang Zhu"], "title": "S-Leak: Leakage-Abuse Attack Against Efficient Conjunctive SSE via s-term Leakage", "comment": "16 pages, 12 figures. Preliminary version. Future journal/conference\n  submission intended", "summary": "Conjunctive Searchable Symmetric Encryption (CSSE) enables secure conjunctive\nsearches over encrypted data. While leakage-abuse attacks (LAAs) against\nsingle-keyword SSE have been extensively studied, their extension to\nconjunctive queries faces a critical challenge: the combinatorial explosion of\ncandidate keyword combinations, leading to enormous time and space overhead for\nattacks. In this paper, we reveal a fundamental vulnerability in\nstate-of-the-art CSSE schemes: s-term leakage, where the keyword with the\nminimal document frequency in a query leaks distinct patterns. We propose\nS-Leak, the first passive attack framework that progressively recovers\nconjunctive queries by exploiting s-term leakage and global leakage. Our key\ninnovation lies in a three-stage approach: identifying the s-term of queries,\npruning low-probability keyword conjunctions, and reconstructing full queries.\nWe propose novel metrics to better assess attacks in conjunctive query\nscenarios. Empirical evaluations on real-world datasets demonstrate that our\nattack is effective in diverse CSSE configurations. When considering 161,700\nconjunctive keyword queries, our attack achieves a 95.15% accuracy in\nrecovering at least one keyword, 82.57% for at least two, 58% for all three\nkeywords, and maintains efficacy against defenses such as SEAL padding and CLRZ\nobfuscation. Our work exposes the underestimated risks of s-term leakage in\npractical SSE deployments and calls for a redesign of leakage models for\nmulti-keyword search scenarios."}
{"id": "2507.04104", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.04104", "abs": "https://arxiv.org/abs/2507.04104", "authors": ["Sri Harsha Gajavalli"], "title": "Human-Centered Interactive Anonymization for Privacy-Preserving Machine Learning: A Case for Human-Guided k-Anonymity", "comment": null, "summary": "Privacy-preserving machine learning (ML) seeks to balance data utility and\nprivacy, especially as regulations like the GDPR mandate the anonymization of\npersonal data for ML applications. Conventional anonymization approaches often\nreduce data utility due to indiscriminate generalization or suppression of data\nattributes. In this study, we propose an interactive approach that incorporates\nhuman input into the k-anonymization process, enabling domain experts to guide\nattribute preservation based on contextual importance. Using the UCI Adult\ndataset, we compare classification outcomes of interactive human-influenced\nanonymization with traditional, fully automated methods. Our results show that\nhuman input can enhance data utility in some cases, although results vary\nacross tasks and settings. We discuss limitations of our approach and suggest\npotential areas for improved interactive frameworks in privacy-aware ML."}
{"id": "2507.04106", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04106", "abs": "https://arxiv.org/abs/2507.04106", "authors": ["Stanisław Pawlak", "Bartłomiej Twardowski", "Tomasz Trzciński", "Joost van de Weijer"], "title": "Addressing The Devastating Effects Of Single-Task Data Poisoning In Exemplar-Free Continual Learning", "comment": "Accepted at CoLLAs 2025", "summary": "Our research addresses the overlooked security concerns related to data\npoisoning in continual learning (CL). Data poisoning - the intentional\nmanipulation of training data to affect the predictions of machine learning\nmodels - was recently shown to be a threat to CL training stability. While\nexisting literature predominantly addresses scenario-dependent attacks, we\npropose to focus on a more simple and realistic single-task poison (STP)\nthreats. In contrast to previously proposed poisoning settings, in STP\nadversaries lack knowledge and access to the model, as well as to both previous\nand future tasks. During an attack, they only have access to the current task\nwithin the data stream. Our study demonstrates that even within these stringent\nconditions, adversaries can compromise model performance using standard image\ncorruptions. We show that STP attacks are able to strongly disrupt the whole\ncontinual training process: decreasing both the stability (its performance on\npast tasks) and plasticity (capacity to adapt to new tasks) of the algorithm.\nFinally, we propose a high-level defense framework for CL along with a poison\ntask detection method based on task vectors. The code is available at\nhttps://github.com/stapaw/STP.git ."}
{"id": "2507.04126", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.04126", "abs": "https://arxiv.org/abs/2507.04126", "authors": ["Howard Halim", "Eyasu Getahun Chekole", "Daniël Reijsbergen", "Jianying Zhou"], "title": "BlowPrint: Blow-Based Multi-Factor Biometrics for Smartphone User Authentication", "comment": null, "summary": "Biometric authentication is a widely used security mechanism that leverages\nunique physiological or behavioral characteristics to authenticate users. In\nmulti-factor biometrics (MFB), multiple biometric modalities, e.g.,\nphysiological and behavioral, are integrated to mitigate the limitations\ninherent in single-factor biometrics. The main challenge in MFB lies in\nidentifying novel behavioral techniques capable of meeting critical criteria,\nincluding high accuracy, high usability, non-invasiveness, resilience against\nspoofing attacks, and low use of computational resources. Despite ongoing\nadvancements, current behavioral biometric techniques often fall short of\nfulfilling one or more of these requirements. In this work, we propose\nBlowPrint, a novel behavioral biometric technique that allows us to\nauthenticate users based on their phone blowing behaviors. In brief, we assume\nthat the way users blow on a phone screen can produce distinctive acoustic\npatterns, which can serve as a unique biometric identifier for effective user\nauthentication. It can also be seamlessly integrated with physiological\ntechniques, such as facial recognition, to enhance its robustness and security.\nTo assess BlowPrint's effectiveness, we conduct an empirical study involving 50\nparticipants from whom we collect blow-acoustic and facial feature data.\nSubsequently, we compute the similarity scores of the two modalities using\nvarious similarity algorithms and combine them through score-level fusion.\nFinally, we compute the accuracy using a machine learning-based classifier. As\na result, the proposed method demonstrates an accuracy of 99.35% for blow\nacoustics, 99.96% for facial recognition, and 99.82% for the combined approach.\nThe experimental results demonstrate BlowPrint's high effectiveness in terms of\nauthentication accuracy, spoofing attack resilience, usability,\nnon-invasiveness, and other aspects."}
{"id": "2507.04174", "categories": ["cs.CR", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.04174", "abs": "https://arxiv.org/abs/2507.04174", "authors": ["Abdellah Akilal", "M-Tahar Kechadi"], "title": "Cloud Digital Forensic Readiness: An Open Source Approach to Law Enforcement Request Management", "comment": null, "summary": "Cloud Forensics presents a multi-jurisdictional challenge that may undermines\nthe success of digital forensic investigations (DFIs). The growing volumes of\ndomiciled and foreign law enforcement (LE) requests, the latency and complexity\nof formal channels for crossborder data access are challenging issues. In this\npaper, we first discuss major Cloud Service Providers (CSPs) transparency\nreports and law enforcement guidelines, then propose an abstract architecture\nfor a Cloud Law Enforcement Requests Management System (CLERMS). A proof of\nconcept of the proposed solution is developed, deployed and validated by two\nrealistic scenarios, in addition to an economic estimation of its associated\ncosts. Based on available open source components, our solution is for the\nbenefit of both CSPs and Cloud Service Consumers (CSCs), and aims to enhance\nthe due Cloud Digital Forensic Readiness (CDFR)."}
{"id": "2507.04197", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.04197", "abs": "https://arxiv.org/abs/2507.04197", "authors": ["Nishant Chinnasami", "Rye Stahle-Smith", "Rasha Karakchi"], "title": "ML-Enhanced AES Anomaly Detection for Real-Time Embedded Security", "comment": null, "summary": "Advanced Encryption Standard (AES) is a widely adopted cryptographic\nalgorithm, yet its practical implementations remain susceptible to side-channel\nand fault injection attacks. In this work, we propose a comprehensive framework\nthat enhances AES-128 encryption security through controlled anomaly injection\nand real-time anomaly detection using both statistical and machine learning\n(ML) methods. We simulate timing and fault-based anomalies by injecting\nexecution delays and ciphertext perturbations during encryption, generating\nlabeled datasets for detection model training. Two complementary detection\nmechanisms are developed: a threshold-based timing anomaly detector and a\nsupervised Random Forest classifier trained on combined timing and ciphertext\nfeatures. We implement and evaluate the framework on both CPU and FPGA-based\nSoC hardware (PYNQ-Z1), measuring performance across varying block sizes,\ninjection rates, and core counts. Our results show that ML-based detection\nsignificantly outperforms threshold-based methods in precision and recall while\nmaintaining real-time performance on embedded hardware. Compared to existing\nAES anomaly detection methods, our solution offers a low-cost, real-time, and\naccurate detection approach deployable on lightweight FPGA platforms."}
{"id": "2507.04214", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.04214", "abs": "https://arxiv.org/abs/2507.04214", "authors": ["Jianshuo Dong", "Tianyi Zhang", "Feng Yan", "Yuanjie Li", "Hewu Li", "Han Qiu"], "title": "Can Large Language Models Automate the Refinement of Cellular Network Specifications?", "comment": null, "summary": "Cellular networks serve billions of users globally, yet concerns about\nreliability and security persist due to weaknesses in 3GPP standards. However,\ntraditional analysis methods, including manual inspection and automated tools,\nstruggle with increasingly expanding cellular network specifications. This\npaper investigates the feasibility of Large Language Models (LLMs) for\nautomated cellular network specification refinement. To advance it, we leverage\n200,000+ approved 3GPP Change Requests (CRs) that document specification\nrevisions, constructing a valuable dataset for domain tasks. We introduce\nCR-eval, a principled evaluation framework, and benchmark 16 state-of-the-art\nLLMs, demonstrating that top models can discover security-related weaknesses in\nover 127 out of 200 test cases within five trials. To bridge potential gaps, we\nexplore LLM specialization techniques, including fine-tuning an 8B model to\nmatch or surpass advanced LLMs like GPT-4o and DeepSeek-R1. Evaluations on 30\ncellular attacks identify open challenges for achieving full automation. These\nfindings confirm that LLMs can automate the refinement of cellular network\nspecifications and provide valuable insights to guide future research in this\ndirection."}
{"id": "2507.04227", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04227", "abs": "https://arxiv.org/abs/2507.04227", "authors": ["Guohong Liu", "Jialei Ye", "Jiacheng Liu", "Yuanchun Li", "Wei Liu", "Pengzhi Gao", "Jian Luan", "Yunxin Liu"], "title": "Hijacking JARVIS: Benchmarking Mobile GUI Agents against Unprivileged Third Parties", "comment": null, "summary": "Mobile GUI agents are designed to autonomously execute diverse device-control\ntasks by interpreting and interacting with mobile screens. Despite notable\nadvancements, their resilience in real-world scenarios where screen content may\nbe partially manipulated by untrustworthy third parties remains largely\nunexplored. Owing to their black-box and autonomous nature, these agents are\nvulnerable to manipulations that could compromise user devices. In this work,\nwe present the first systematic investigation into the vulnerabilities of\nmobile GUI agents. We introduce a scalable attack simulation framework\nAgentHazard, which enables flexible and targeted modifications of screen\ncontent within existing applications. Leveraging this framework, we develop a\ncomprehensive benchmark suite comprising both a dynamic task execution\nenvironment and a static dataset of vision-language-action tuples, totaling\nover 3,000 attack scenarios. The dynamic environment encompasses 58\nreproducible tasks in an emulator with various types of hazardous UI content,\nwhile the static dataset is constructed from 210 screenshots collected from 14\npopular commercial apps. Importantly, our content modifications are designed to\nbe feasible for unprivileged third parties. We evaluate 7 widely-used mobile\nGUI agents and 5 common backbone models using our benchmark. Our findings\nreveal that all examined agents are significantly influenced by misleading\nthird-party content (with an average misleading rate of 28.8% in human-crafted\nattack scenarios) and that their vulnerabilities are closely linked to the\nemployed perception modalities and backbone LLMs. Furthermore, we assess\ntraining-based mitigation strategies, highlighting both the challenges and\nopportunities for enhancing the robustness of mobile GUI agents. Our code and\ndata will be released at https://agenthazard.github.io."}
{"id": "2507.04275", "categories": ["cs.CR", "cs.AI", "cs.LG", "68T05, 68M25", "D.4.6; I.2.6; K.6.5"], "pdf": "https://arxiv.org/pdf/2507.04275", "abs": "https://arxiv.org/abs/2507.04275", "authors": ["M. Tahir Akdeniz", "Zeynep Yeşilkaya", "İ. Enes Köse", "İ. Ulaş Ünal", "Sevil Şen"], "title": "VOLTRON: Detecting Unknown Malware Using Graph-Based Zero-Shot Learning", "comment": "17 pages, 6 figures, Submitted as a preprint", "summary": "The persistent threat of Android malware presents a serious challenge to the\nsecurity of millions of users globally. While many machine learning-based\nmethods have been developed to detect these threats, their reliance on large\nlabeled datasets limits their effectiveness against emerging, previously unseen\nmalware families, for which labeled data is scarce or nonexistent.\n  To address this challenge, we introduce a novel zero-shot learning framework\nthat combines Variational Graph Auto-Encoders (VGAE) with Siamese Neural\nNetworks (SNN) to identify malware without needing prior examples of specific\nmalware families. Our approach leverages graph-based representations of Android\napplications, enabling the model to detect subtle structural differences\nbetween benign and malicious software, even in the absence of labeled data for\nnew threats.\n  Experimental results show that our method outperforms the state-of-the-art\nMaMaDroid, especially in zero-day malware detection. Our model achieves 96.24%\naccuracy and 95.20% recall for unknown malware families, highlighting its\nrobustness against evolving Android threats."}
{"id": "2507.04365", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.04365", "abs": "https://arxiv.org/abs/2507.04365", "authors": ["Xiaomeng Hu", "Pin-Yu Chen", "Tsung-Yi Ho"], "title": "Attention Slipping: A Mechanistic Understanding of Jailbreak Attacks and Defenses in LLMs", "comment": null, "summary": "As large language models (LLMs) become more integral to society and\ntechnology, ensuring their safety becomes essential. Jailbreak attacks exploit\nvulnerabilities to bypass safety guardrails, posing a significant threat.\nHowever, the mechanisms enabling these attacks are not well understood. In this\npaper, we reveal a universal phenomenon that occurs during jailbreak attacks:\nAttention Slipping. During this phenomenon, the model gradually reduces the\nattention it allocates to unsafe requests in a user query during the attack\nprocess, ultimately causing a jailbreak. We show Attention Slipping is\nconsistent across various jailbreak methods, including gradient-based token\nreplacement, prompt-level template refinement, and in-context learning.\nAdditionally, we evaluate two defenses based on query perturbation, Token\nHighlighter and SmoothLLM, and find they indirectly mitigate Attention\nSlipping, with their effectiveness positively correlated with the degree of\nmitigation achieved. Inspired by this finding, we propose Attention Sharpening,\na new defense that directly counters Attention Slipping by sharpening the\nattention score distribution using temperature scaling. Experiments on four\nleading LLMs (Gemma2-9B-It, Llama3.1-8B-It, Qwen2.5-7B-It, Mistral-7B-It v0.2)\nshow that our method effectively resists various jailbreak attacks while\nmaintaining performance on benign tasks on AlpacaEval. Importantly, Attention\nSharpening introduces no additional computational or memory overhead, making it\nan efficient and practical solution for real-world deployment."}
{"id": "2507.04426", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.04426", "abs": "https://arxiv.org/abs/2507.04426", "authors": ["Novruz Amirov", "Leminur Celik", "Egemen Ali Caner", "Emre Yurdakul", "Fahri Anil Yerlikaya", "Serif Bahtiyar"], "title": "Enhancing Phishing Detection in Financial Systems through NLP", "comment": null, "summary": "The threat of phishing attacks in financial systems is continuously growing.\nTherefore, protecting sensitive information from unauthorized access is\nparamount. This paper discusses the critical need for robust email phishing\ndetection. Several existing methods, including blacklists and whitelists, play\na crucial role in detecting phishing attempts. Nevertheless, these methods\npossess inherent limitations, emphasizing the need for the development of a\nmore advanced solution. Our proposed solution presents a pioneering Natural\nLanguage Processing (NLP) approach for phishing email detection. Leveraging\nsemantic similarity and TFIDF (Term Frequency-Inverse Document Frequency)\nanalysis, our solution identifies keywords in phishing emails, subsequently\nevaluating the semantic similarities with a dedicated phishing dataset,\nultimately contributing to the enhancement of cybersecurity and NLP domains\nthrough a robust solution for detecting phishing threats in financial systems.\nExperimental results show the accuracy of our phishing detection method can\nreach 79.8 percent according to TF-IDF analysis, while it can reach 67.2\npercent according to semantic analysis."}
{"id": "2507.04457", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.04457", "abs": "https://arxiv.org/abs/2507.04457", "authors": ["Ruixuan Liu", "Li Xiong"], "title": "UniAud: A Unified Auditing Framework for High Auditing Power and Utility with One Training Run", "comment": "14 pages", "summary": "Differentially private (DP) optimization has been widely adopted as a\nstandard approach to provide rigorous privacy guarantees for training datasets.\nDP auditing verifies whether a model trained with DP optimization satisfies its\nclaimed privacy level by estimating empirical privacy lower bounds through\nhypothesis testing. Recent O(1) frameworks improve auditing efficiency by\nchecking the membership status of multiple audit samples in a single run,\nrather than checking individual samples across multiple runs. However, we\nreveal that there is no free lunch for this improved efficiency: data\ndependency and an implicit conflict between auditing and utility impair the\ntightness of the auditing results. Addressing these challenges, our key\ninsights include reducing data dependency through uncorrelated data and\nresolving the auditing-utility conflict by decoupling the criteria for\neffective auditing and separating objectives for utility and auditing. We first\npropose a unified framework, UniAud, for data-independent auditing that\nmaximizes auditing power through a novel uncorrelated canary construction and a\nself-comparison framework. We then extend this framework as UniAud++ for\ndata-dependent auditing, optimizing the auditing and utility trade-off through\nmulti-task learning with separate objectives for auditing and training.\nExperimental results validate that our black-box O(1) framework matches the\nstate-of-the-art auditing results of O(T) auditing with thousands of runs,\ndemonstrating the best efficiency-auditing trade-off across vision and language\ntasks. Additionally, our framework provides meaningful auditing with only\nslight utility degradation compared to standard DP training, showing the\noptimal utility-auditing trade-off and the benefit of requiring no extra\ntraining for auditing."}
{"id": "2507.04461", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.04461", "abs": "https://arxiv.org/abs/2507.04461", "authors": ["Tanvir Rahman", "A. B. M. Harun-ur Rashid"], "title": "Arbiter PUF: Uniqueness and Reliability Analysis Using Hybrid CMOS-Stanford Memristor Model", "comment": null, "summary": "In an increasingly interconnected world, protecting electronic devices has\ngrown more crucial because of the dangers of data extraction, reverse\nengineering, and hardware tampering. Producing chips in a third-party\nmanufacturing company can let hackers change the design. As the Internet of\nThings (IoT) proliferates, physical attacks happen more, and conventional\ncryptography techniques do not function well. In this paper, we investigate the\ndesign and assessment of PUFs using the Stanford Memristor Model, utilizing its\nrandom filament evolution to improve security. The system was built using 45nm\nCMOS technology. A comparison is made between CMOS-based and memristor-based\nArbiter PUFs, evaluating their performance under temperature, voltage, and\nprocess variations. Intra- and inter-hamming distances are employed by Monte\nCarlo simulations to estimate uniqueness and reliability. The results show that\nmemristor-based PUFs offer better reliability than CMOS-based designs, though\nuniqueness needs further improvement. Furthermore, this study sheds light on\nthe reasonableness of memristor-based PUFs for secure applications in hardware\nsecurity."}
{"id": "2507.04495", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.04495", "abs": "https://arxiv.org/abs/2507.04495", "authors": ["Hyunwook Choi", "Sangyun Won", "Daeyeon Hwang", "Junhyeok Choi"], "title": "README: Robust Error-Aware Digital Signature Framework via Deep Watermarking Model", "comment": null, "summary": "Deep learning-based watermarking has emerged as a promising solution for\nrobust image authentication and protection. However, existing models are\nlimited by low embedding capacity and vulnerability to bit-level errors, making\nthem unsuitable for cryptographic applications such as digital signatures,\nwhich require over 2048 bits of error-free data. In this paper, we propose\nREADME (Robust Error-Aware Digital Signature via Deep WaterMarking ModEl), a\nnovel framework that enables robust, verifiable, and error-tolerant digital\nsignatures within images. Our method combines a simple yet effective\ncropping-based capacity scaling mechanism with ERPA (ERror PAinting Module), a\nlightweight error correction module designed to localize and correct bit errors\nusing Distinct Circular Subsum Sequences (DCSS). Without requiring any\nfine-tuning of existing pretrained watermarking models, README significantly\nboosts the zero-bit-error image rate (Z.B.I.R) from 1.2% to 86.3% when\nembedding 2048-bit digital signatures into a single image, even under\nreal-world distortions. Moreover, our use of perceptual hash-based signature\nverification ensures public verifiability and robustness against tampering. The\nproposed framework unlocks a new class of high-assurance applications for deep\nwatermarking, bridging the gap between signal-level watermarking and\ncryptographic security."}
{"id": "2507.04501", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.04501", "abs": "https://arxiv.org/abs/2507.04501", "authors": ["Gennady Khalimov", "Yevgen Kotukh"], "title": "LINE: Public-key encryption", "comment": null, "summary": "We propose a public key encryption cryptosystem based on solutions of linear\nequation systems with predefinition of input parameters through shared secret\ncomputation for factorizable substitutions. The existence of multiple\nequivalent solutions for an underdetermined system of linear equations\ndetermines the impossibility of its resolution by a cryptanalyst in polynomial\ntime. The completion of input parameters of the equation system is implemented\nthrough secret homomorphic matrix transformation for substitutions factorized\nover the basis of a vector space of dimension m over the field F2. Encryption\nis implemented through computation of substitutions that are one-way functions\non an elementary abelian 2-group of order 2\"m. Decryption is implemented\nthrough completion of input parameters of the equation system. Homomorphic\ntransformations are constructed based on matrix computations. Matrix\ncomputations enable the implementation of high security and low computational\noverhead for homomorphic transformations."}
{"id": "2507.04752", "categories": ["cs.CR", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.04752", "abs": "https://arxiv.org/abs/2507.04752", "authors": ["Shuo Yang", "Xinran Zheng", "Xinchen Zhang", "Jinfeng Xu", "Jinze Li", "Donglin Xie", "Weicai Long", "Edith C. H. Ngai"], "title": "Large Language Models for Network Intrusion Detection Systems: Foundations, Implementations, and Future Directions", "comment": null, "summary": "Large Language Models (LLMs) have revolutionized various fields with their\nexceptional capabilities in understanding, processing, and generating\nhuman-like text. This paper investigates the potential of LLMs in advancing\nNetwork Intrusion Detection Systems (NIDS), analyzing current challenges,\nmethodologies, and future opportunities. It begins by establishing a\nfoundational understanding of NIDS and LLMs, exploring the enabling\ntechnologies that bridge the gap between intelligent and cognitive systems in\nAI-driven NIDS. While Intelligent NIDS leverage machine learning and deep\nlearning to detect threats based on learned patterns, they often lack\ncontextual awareness and explainability. In contrast, Cognitive NIDS integrate\nLLMs to process both structured and unstructured security data, enabling deeper\ncontextual reasoning, explainable decision-making, and automated response for\nintrusion behaviors. Practical implementations are then detailed, highlighting\nLLMs as processors, detectors, and explainers within a comprehensive AI-driven\nNIDS pipeline. Furthermore, the concept of an LLM-centered Controller is\nproposed, emphasizing its potential to coordinate intrusion detection\nworkflows, optimizing tool collaboration and system performance. Finally, this\npaper identifies critical challenges and opportunities, aiming to foster\ninnovation in developing reliable, adaptive, and explainable NIDS. By\npresenting the transformative potential of LLMs, this paper seeks to inspire\nadvancement in next-generation network security systems."}
{"id": "2507.04771", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.04771", "abs": "https://arxiv.org/abs/2507.04771", "authors": ["Josep Domingo-Ferrer", "Najeeb Jebreel", "David Sánchez"], "title": "Efficient Unlearning with Privacy Guarantees", "comment": null, "summary": "Privacy protection laws, such as the GDPR, grant individuals the right to\nrequest the forgetting of their personal data not only from databases but also\nfrom machine learning (ML) models trained on them. Machine unlearning has\nemerged as a practical means to facilitate model forgetting of data instances\nseen during training. Although some existing machine unlearning methods\nguarantee exact forgetting, they are typically costly in computational terms.\nOn the other hand, more affordable methods do not offer forgetting guarantees\nand are applicable only to specific ML models. In this paper, we present\n\\emph{efficient unlearning with privacy guarantees} (EUPG), a novel machine\nunlearning framework that offers formal privacy guarantees to individuals whose\ndata are being unlearned. EUPG involves pre-training ML models on data\nprotected using privacy models, and it enables {\\em efficient unlearning with\nthe privacy guarantees offered by the privacy models in use}. Through empirical\nevaluation on four heterogeneous data sets protected with $k$-anonymity and\n$\\epsilon$-differential privacy as privacy models, our approach demonstrates\nutility and forgetting effectiveness comparable to those of exact unlearning\nmethods, while significantly reducing computational and storage costs. Our code\nis available at https://github.com/najeebjebreel/EUPG."}
{"id": "2507.04775", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.04775", "abs": "https://arxiv.org/abs/2507.04775", "authors": ["Carlos Agulló-Domingo", "Óscar Vera-López", "Seyda Guzelhan", "Lohit Daksha", "Aymane El Jerari", "Kaustubh Shivdikar", "Rashmi Agrawal", "David Kaeli", "Ajay Joshi", "José L. Abellán"], "title": "FIDESlib: A Fully-Fledged Open-Source FHE Library for Efficient CKKS on GPUs", "comment": "Presented as poster paper at 2025 IEEE International Symposium on\n  Performance Analysis of Systems and Software (ISPASS)", "summary": "Word-wise Fully Homomorphic Encryption (FHE) schemes, such as CKKS, are\ngaining significant traction due to their ability to provide\npost-quantum-resistant, privacy-preserving approximate computing; an especially\ndesirable feature in Machine-Learning-as-a-Service (MLaaS) cloud-computing\nparadigms. OpenFHE is a leading CPU-based FHE library with robust CKKS\noperations, but its server-side performance is not yet sufficient for practical\ncloud deployment. As GPU computing becomes more common in data centers, many\nFHE libraries are adding GPU support. However, integrating an efficient GPU\nbackend into OpenFHE is challenging. While OpenFHE uses a Hardware Abstraction\nLayer (HAL), its flexible architecture sacrifices performance due to the\nabstraction layers required for multi-scheme and multi-backend compatibility.\nIn this work, we introduce FIDESlib, the first open-source server-side CKKS GPU\nlibrary that is fully interoperable with well-established client-side OpenFHE\noperations. Unlike other existing open-source GPU libraries, FIDESlib provides\nthe first implementation featuring heavily optimized GPU kernels for all CKKS\nprimitives, including bootstrapping. Our library also integrates robust\nbenchmarking and testing, ensuring it remains adaptable to further\noptimization. Furthermore, its software architecture is designed to support\nextensions to a multi-GPU backend for enhanced acceleration. Our experiments\nacross various GPU systems and the leading open-source CKKS library to date,\nPhantom, show that FIDESlib offers superior performance and scalability. For\nbootstrapping, FIDESlib achieves no less than 70x speedup over the\nAVX-optimized OpenFHE implementation."}
{"id": "2507.04855", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.04855", "abs": "https://arxiv.org/abs/2507.04855", "authors": ["Darya Parygina", "Timofey Mezhuev", "Daniil Kuts"], "title": "Hybrid Approach to Directed Fuzzing", "comment": null, "summary": "Program analysis and automated testing have recently become an essential part\nof SSDLC. Directed greybox fuzzing is one of the most popular automated testing\nmethods that focuses on error detection in predefined code regions. However, it\nstill lacks ability to overcome difficult program constraints. This problem can\nbe well addressed by symbolic execution, but at the cost of lower performance.\nThus, combining directed fuzzing and symbolic execution techniques can lead to\nmore efficient error detection.\n  In this paper, we propose a hybrid approach to directed fuzzing with novel\nseed scheduling algorithm, based on target-related interestingness and\ncoverage. The approach also performs minimization and sorting of objective\nseeds according to a target-related information. We implement our approach in\nSydr-Fuzz tool using LibAFL-DiFuzz as directed fuzzer and Sydr as dynamic\nsymbolic executor. We evaluate our approach with Time to Exposure metric and\ncompare it with pure LibAFL-DiFuzz, AFLGo, BEACON, WAFLGo, WindRanger,\nFishFuzz, and Prospector. The results show an improvement for 3 out of 7\nexamples with speedup up to 1.86 times over the second best result, as well as\na significant improvement for 3 out of 7 examples over the pure LibAFL-DiFuzz\nfuzzer. Sydr-Fuzz hybrid approach to directed fuzzing shows high performance\nand helps to improve directed fuzzing efficiency."}
{"id": "2507.04903", "categories": ["cs.CR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.04903", "abs": "https://arxiv.org/abs/2507.04903", "authors": ["Thinh Dao", "Dung Thuy Nguyen", "Khoa D Doan", "Kok-Seng Wong"], "title": "BackFed: An Efficient & Standardized Benchmark Suite for Backdoor Attacks in Federated Learning", "comment": "Under review at NeurIPS'25", "summary": "Federated Learning (FL) systems are vulnerable to backdoor attacks, where\nadversaries train their local models on poisoned data and submit poisoned model\nupdates to compromise the global model. Despite numerous proposed attacks and\ndefenses, divergent experimental settings, implementation errors, and\nunrealistic assumptions hinder fair comparisons and valid conclusions about\ntheir effectiveness in real-world scenarios. To address this, we introduce\nBackFed - a comprehensive benchmark suite designed to standardize, streamline,\nand reliably evaluate backdoor attacks and defenses in FL, with a focus on\npractical constraints. Our benchmark offers key advantages through its\nmulti-processing implementation that significantly accelerates experimentation\nand the modular design that enables seamless integration of new methods via\nwell-defined APIs. With a standardized evaluation pipeline, we envision BackFed\nas a plug-and-play environment for researchers to comprehensively and reliably\nevaluate new attacks and defenses. Using BackFed, we conduct large-scale\nstudies of representative backdoor attacks and defenses across both Computer\nVision and Natural Language Processing tasks with diverse model architectures\nand experimental settings. Our experiments critically assess the performance of\nproposed attacks and defenses, revealing unknown limitations and modes of\nfailures under practical conditions. These empirical insights provide valuable\nguidance for the development of new methods and for enhancing the security of\nFL systems. Our framework is openly available at\nhttps://github.com/thinh-dao/BackFed."}
{"id": "2507.04916", "categories": ["cs.CR", "math.CO"], "pdf": "https://arxiv.org/pdf/2507.04916", "abs": "https://arxiv.org/abs/2507.04916", "authors": ["Kazumasa Shinagawa", "Koji Nuida"], "title": "Cyclic Equalizability of Words and Its Application to Card-Based Cryptography", "comment": "11 pages, to appear in 25th International Symposium on Fundamentals\n  of Computation Theory (FCT 2025)", "summary": "Card-based cryptography is a research area to implement cryptographic\nprocedures using a deck of physical cards. In recent years, it has been found\nto be related to finite group theory and algebraic combinatorics, and is\nbecoming more and more closely connected to the field of mathematics. In this\npaper, we discuss the relationship between card-based cryptography and\ncombinatorics on words for the first time. In particular, we focus on cyclic\nequality of words. We say that a set of words are cyclically equalizable if\nthey can be transformed to be cyclically equal by repeated simultaneous\ninsertion of letters. The main result of this paper is to show that two binary\nwords of equal length and equal Hamming weight are cyclically equalizable. As\napplications of cyclic equalizability to card-based cryptography, we describe\nits applications to the information erasure problem and to single-cut full-open\nprotocols."}
{"id": "2507.04931", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.04931", "abs": "https://arxiv.org/abs/2507.04931", "authors": ["Ruoxi Wang", "Kun Li", "Minghui Xu", "Yue Zhang", "Kaidi Xu", "Chunchi Liu", "Yinhao Xiao", "Xiuzhen Cheng"], "title": "LIFT: Automating Symbolic Execution Optimization with Large Language Models for AI Networks", "comment": "Accepted by ACM SIGCOMM 2025 - 2nd Workshop on Networks for AI\n  Computing (NAIC). 7 pages, 2 figures, 2 tables", "summary": "Dynamic Symbolic Execution (DSE) is a key technique in program analysis,\nwidely used in software testing, vulnerability discovery, and formal\nverification. In distributed AI systems, DSE plays a crucial role in\nidentifying hard-to-detect bugs, especially those arising from complex network\ncommunication patterns. However, traditional approaches to symbolic execution\nare often hindered by scalability issues and inefficiencies, particularly in\nlarge-scale systems. This paper introduces LIFT (Large-language-model\nIntegrated Functional-equivalent-IR Transformation), a novel framework that\nleverages Large Language Models (LLMs) to automate the optimization of\nIntermediate Representations (IRs) in symbolic execution. LIFT addresses the\nchallenges of symbolic execution by providing a scalable, context-sensitive\nsolution for IR transformation. The framework consists of two phases: IR\nAnalysis and Optimization, where LLMs optimize time-intensive IR blocks, and\nSymbolic Execution and Validation, which includes benchmarking and semantic\nverification to ensure correctness and generalizability. Experiments on\nreal-world binaries demonstrated significant performance improvements,\nincluding a 53.5\\% reduction in execution time for bigtest and a 10.24\\%\nreduction for random, along with reductions in IR statements, PUT instructions,\nand temporary variables. These results demonstrate that LLMs simplify IRs while\nmaintaining functional correctness, enhancing symbolic execution in distributed\nAI systems."}
{"id": "2507.04956", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.04956", "abs": "https://arxiv.org/abs/2507.04956", "authors": ["Yusei Tanaka"], "title": "Bullshark on Narwhal: Implementation-level Workflow Analysis of Round-based DAG Consensus in Theory and Practice", "comment": "17 pages, in Japanese language, 11 figures", "summary": "Round-based DAGs enable high-performance Byzantine fault-tolerant consensus,\nyet their technical advantages remain underutilized due to their short history.\nWhile research on consensus protocols is active in both academia and industry,\nmany studies overlook implementation-level algorithms, leaving actual\nperformance unclear - particularly for theoretical protocols whose practical\nperformance cannot often be evaluated. Bullshark, a Round-based DAG BFT\nprotocol on Narwhal mempool, achieves optimal performance: 297,000 transactions\nper second with 2-second latency. We analyze the algorithm's workflow, from\ntransaction submission to blockchain commitment, breaking it down layer by\nlayer at the functional level and delineating the key features and interactions\nof the Bullshark and Narwhal components. Future work aims to improve\nperformance in Byzantine fault environments and optimize trade-offs in the CAP\ntheorem."}
{"id": "2507.05093", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05093", "abs": "https://arxiv.org/abs/2507.05093", "authors": ["Alberto Castagnaro", "Umberto Salviati", "Mauro Conti", "Luca Pajola", "Simeone Pizzi"], "title": "The Hidden Threat in Plain Text: Attacking RAG Data Loaders", "comment": "currently under submission", "summary": "Large Language Models (LLMs) have transformed human-machine interaction since\nChatGPT's 2022 debut, with Retrieval-Augmented Generation (RAG) emerging as a\nkey framework that enhances LLM outputs by integrating external knowledge.\nHowever, RAG's reliance on ingesting external documents introduces new\nvulnerabilities. This paper exposes a critical security gap at the data loading\nstage, where malicious actors can stealthily corrupt RAG pipelines by\nexploiting document ingestion.\n  We propose a taxonomy of 9 knowledge-based poisoning attacks and introduce\ntwo novel threat vectors -- Content Obfuscation and Content Injection --\ntargeting common formats (DOCX, HTML, PDF). Using an automated toolkit\nimplementing 19 stealthy injection techniques, we test five popular data\nloaders, finding a 74.4% attack success rate across 357 scenarios. We further\nvalidate these threats on six end-to-end RAG systems -- including white-box\npipelines and black-box services like NotebookLM and OpenAI Assistants --\ndemonstrating high success rates and critical vulnerabilities that bypass\nfilters and silently compromise output integrity. Our results emphasize the\nurgent need to secure the document ingestion process in RAG systems against\ncovert content manipulations."}
{"id": "2507.05132", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.05132", "abs": "https://arxiv.org/abs/2507.05132", "authors": ["Nelly Elsayed", "Lily Dzamesi", "Zag ElSayed", "Murat Ozer"], "title": "Extreme Learning Machine Based System for DDoS Attacks Detections on IoMT Devices", "comment": "8 pages, under review", "summary": "The Internet of Medical Things (IoMT) represents a paradigm shift in the\nhealthcare sector, enabling the interconnection of medical devices, sensors,\nand systems to enhance patient monitoring, diagnosis, and management. The rapid\nevolution of IoMT presents significant benefits to the healthcare domains.\nHowever, there is a rapid increase in distributed denial of service (DDoS)\nattacks on the IoMT networks due to several vulnerabilities in the\nIoMT-connected devices, which negatively impact patients' health and can even\nlead to deaths. Thus, in this paper, we aim to save lives via investigating an\nextreme learning machine for detecting DDoS attacks on IoMT devices. The\nproposed approach achieves a high accuracy at a low implementation budget.\nThus, it can reduce the implementation cost of the DDoS detection system,\nmaking the model capable of executing on the fog level."}
{"id": "2507.05213", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.05213", "abs": "https://arxiv.org/abs/2507.05213", "authors": ["Max Gao", "Michael Collins", "Ricky Mok", "kc Claffy"], "title": "Hunting in the Dark: Metrics for Early Stage Traffic Discovery", "comment": "12 pages, 8 figures", "summary": "Threat hunting is an operational security process where an expert analyzes\ntraffic, applying knowledge and lightweight tools on unlabeled data in order to\nidentify and classify previously unknown phenomena. In this paper, we examine\nthreat hunting metrics and practice by studying the detection of Crackonosh, a\ncryptojacking malware package, has on various metrics for identifying its\nbehavior. Using a metric for discoverability, we model the ability of defenders\nto measure Crackonosh traffic as the malware population decreases, evaluate the\nstrength of various detection methods, and demonstrate how different darkspace\nsizes affect both the ability to track the malware, but enable emergent\nbehaviors by exploiting attacker mistakes."}
