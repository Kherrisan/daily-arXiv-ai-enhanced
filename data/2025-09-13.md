<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 17]
- [cs.SE](#cs.SE) [Total: 13]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Cross-Service Token: Finding Attacks in 5G Core Networks](https://arxiv.org/abs/2509.08992)
*Anqi Chen,Riccardo Preatoni,Alessandro Brighente,Mauro Conti,Cristina Nita-Rotaru*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: 5G marks a major departure from previous cellular architectures, by
transitioning from a monolithic design of the core network to a Service-Based
Architecture (SBA) where services are modularized as Network Functions (NFs)
which communicate with each other via standard-defined HTTP-based APIs called
Service-Based Interfaces (SBIs). These NFs are deployed in private and public
cloud infrastructure, and an access control framework based on OAuth restricts
how they communicate with each other and obtain access to resources. Given the
increased vulnerabilities of clouds to insiders, it is important to study the
security of the 5G Core services for vulnerabilities that allow attackers to
use compromised NFs to obtain unauthorized access to resources.
  We present FivGeeFuzz, a grammar-based fuzzing framework designed to uncover
security flaws in 5G core SBIs. FivGeeFuzz automatically derives grammars from
3GPP API specifications to generate malformed, unexpected, or semantically
inconsistent inputs, and it integrates automated bug detection with manual
validation and root-cause analysis. We evaluate our approach on free5GC, the
only open-source 5G core implementing Release 17-compliant SBIs with an access
control mechanism. Using FivGeeFuzz, we discovered 8 previously unknown
vulnerabilities in free5GC, leading to runtime crashes, improper error
handling, and unauthorized access to resources, including a very severe attack
we call Cross-Service Token Attack. All bugs were confirmed by the free5GC
team, 7 have already been patched, and the remaining one has a patch under
development.

</details>


### [2] [When FinTech Meets Privacy: Securing Financial LLMs with Differential Private Fine-Tuning](https://arxiv.org/abs/2509.08995)
*Sichen Zhu,Hoyeung Leung,Xiaoyi Wang,Jia Wei,Honghui Xu*

Main category: cs.CR

TL;DR: DPFinLLM is a privacy-preserving, lightweight LLM for on-device financial tasks that matches full models' performance while safeguarding data privacy.


<details>
  <summary>Details</summary>
Motivation: The growing deployment of AI models on edge devices in financial applications highlights the critical need to protect sensitive financial data from privacy breaches, while maintaining computational efficiency at the edge.

Method: The method proposes a streamlined architecture derived from state-of-the-art models combined with a robust differential privacy mechanism to enable secure on-device financial data processing while maintaining lightweight efficiency.

Result: Experiments on multiple financial sentiment datasets show DPFinLLM achieves performance comparable to fully fine-tuned models under strict privacy constraints, validating its effectiveness and efficiency.

Conclusion: DPFinLLM demonstrates that it is possible to balance strong privacy guarantees with high performance for on-device financial applications, offering a viable solution for secure edge AI in FinTech.

Abstract: The integration of Large Language Models (LLMs) into financial technology
(FinTech) has revolutionized the analysis and processing of complex financial
data, driving advancements in real-time decision-making and analytics. With the
growing trend of deploying AI models on edge devices for financial
applications, ensuring the privacy of sensitive financial data has become a
significant challenge. To address this, we propose DPFinLLM, a
privacy-enhanced, lightweight LLM specifically designed for on-device financial
applications. DPFinLLM combines a robust differential privacy mechanism with a
streamlined architecture inspired by state-of-the-art models, enabling secure
and efficient processing of financial data. This proposed DPFinLLM can not only
safeguard user data from privacy breaches but also ensure high performance
across diverse financial tasks. Extensive experiments on multiple financial
sentiment datasets validate the effectiveness of DPFinLLM, demonstrating its
ability to achieve performance comparable to fully fine-tuned models, even
under strict privacy constraints.

</details>


### [3] [Beyond Tag Collision: Cluster-based Memory Management for Tag-based Sanitizers](https://arxiv.org/abs/2509.09089)
*Mengfei Xie,Yan Lin,Hongtao Wu,Jianming Fu,Chenke Luo,Guojun Peng*

Main category: cs.CR

TL;DR: ClusterTag is a novel cluster-based memory allocator addressing tag collisions in tag-based sanitizers effectively via heap randomization and balanced tag assignment improvement.


<details>
  <summary>Details</summary>
Motivation: Tag collisions in current tag-based sanitizers result from limited tag encoding space, leading to missed memory violations and probabilistic false negatives in detecting bugs.

Method: ClusterTag divides memory objects into independent clusters, using a cluster-grained randomization to prevent inter-cluster tag collisions by randomizing intervals between clusters.

Result: ClusterTag outperformed existing tag assignment strategies in security evaluations, with deterministic results across 500 repeated test runs, reporting 5,652 issues versus 1,530 missed, and showed balanced improvements across three collision metrics.

Conclusion: ClusterTag offers a deterministic way to detect pointer-object inconsistencies without significant performance overhead (1%), making it a robust integration for tag-based sanitizers.

Abstract: Tag-based sanitizers attach a small "key" to each pointer and a matching
"lock" tag to its target memory object, enabling runtime verification of
pointer-object consistency and helping developers to detect potential memory
violations. However, the limited tag encoding space challenges existing studies
in assigning distinct tags to memory objects across temporal and spatial
dimensions, leading to potential tag collisions. In this paper, we present
ClusterTag, a novel cluster-based memory allocator aimed at simultaneously
mitigating tag collisions in both temporal and spatial dimensions. The core
design of ClusterTag effectively balances the significant mismatch between tag
encoding space and memory objects: it divides memory objects into multiple
independent clusters, thereby limiting tag collisions to finite chunks within
each cluster. To mitigate tag collisions across clusters, we design a
cluster-grained heap randomization scheme. This approach introduces random
address intervals between clusters and further breaks the entropy limitation of
the tag space. ClusterTag has been implemented as an independent memory
allocator that seamlessly integrates with tag-based sanitizers such as HWASan,
and maintains comparable performance overhead (within 1%) at various
randomization densities. Security evaluations on the Juliet dataset indicate
that ClusterTag exhibits deterministic results across 500 repeated tests (5,652
reported and 1,530 missed), while the existing three types of tag assignment
strategies all exhibit probabilistic false negatives due to tag collisions.
Quantitative analysis across three tag collision distance metrics-minimum,
average, and unpredictability-demonstrates that ClusterTag achieves balanced
improvements across all three, whereas prior tag assignment schemes (random,
staggered, fixed) show significant trade-offs in at least one metric.

</details>


### [4] [Towards Confidential and Efficient LLM Inference with Dual Privacy Protection](https://arxiv.org/abs/2509.09091)
*Honglan Yu,Yibin Wang,Feifei Dai,Dong Liu,Haihui Fan,Xiaoyan Gu*

Main category: cs.CR

TL;DR: CMIF is a confidential, efficient framework for LLM inference that mitigates TEE latency and communication costs while improving privacy-utility tradeoffs through embedding-layer isolation and optimized differential privacy mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing TEE-based methods suffer from high latency and communication overhead, while DP-based approaches degrade model performance. A holistic solution is needed for private and efficient LLM inference.

Method: The framework confidentially deploys the embedding layer on client-side TEEs and subsequent layers on GPU servers, while optimizing the Report-Noisy-Max mechanism for privacy preservation.

Result: Experiments on Llama-series models reveal reduced TEE inference overhead compared to prior work, with preserved data privacy and acceptable performance tradeoffs.

Conclusion: CMIF effectively addresses the high latency and privacy issues in TEEs, offering a balanced solution for efficient and secure model inference.

Abstract: CPU-based trusted execution environments (TEEs) and differential privacy (DP)
have gained wide applications for private inference. Due to high inference
latency in TEEs, researchers use partition-based approaches that offload linear
model components to GPUs. However, dense nonlinear layers of large language
models (LLMs) result in significant communication overhead between TEEs and
GPUs. DP-based approaches apply random noise to protect data privacy, but this
compromises LLM performance and semantic understanding. To overcome the above
drawbacks, this paper proposes CMIF, a Confidential and efficient Model
Inference Framework. CMIF confidentially deploys the embedding layer in the
client-side TEE and subsequent layers on GPU servers. Meanwhile, it optimizes
the Report-Noisy-Max mechanism to protect sensitive inputs with a slight
decrease in model performance. Extensive experiments on Llama-series models
demonstrate that CMIF reduces additional inference overhead in TEEs while
preserving user data privacy.

</details>


### [5] [DP-FedLoRA: Privacy-Enhanced Federated Fine-Tuning for On-Device Large Language Models](https://arxiv.org/abs/2509.09097)
*Honghui Xu,Shiva Shrestha,Wei Chen,Zhiyuan Li,Zhipeng Cai*

Main category: cs.CR

TL;DR: DP-FedLoRA (2023) combines LoRA compression with differential privacy to enable secure on-device LLM training, balancing privacy guarantees and model performance through noise-clip perturbation with proven theoretical bounds.


<details>
  <summary>Details</summary>
Motivation: On-device LLM systems using federated fine-tuning risk exposing sensitive user data, necessitating privacy-preserving frameworks for secure large-scale deployment.

Method: The framework combines LoRA-based parameter adaptation with differential privacy by clipping and Gaussian-noise perturbation of client-side LoRA matrices, supported by theoretical analysis of update unbiasedness and variance bounds.

Result: Experiments across mainstream benchmarks demonstrate DP-FedLoRA maintains strong model performance while satisfying (ε, δ)-differential privacy, outperforming existing privacy-preserving approaches in accuracy-noise tradeoff.

Conclusion: DP-FedLoRA provides a communication-efficient, privacy-preserving framework for federated on-device LLM training, proving that strong privacy guarantees can coexist with competitive performance in real-world deployments.

Abstract: As on-device large language model (LLM) systems become increasingly
prevalent, federated fine-tuning enables advanced language understanding and
generation directly on edge devices; however, it also involves processing
sensitive, user-specific data, raising significant privacy concerns within the
federated learning framework. To address these challenges, we propose
DP-FedLoRA, a privacy-enhanced federated fine-tuning framework that integrates
LoRA-based adaptation with differential privacy in a communication-efficient
setting. Each client locally clips and perturbs its LoRA matrices using
Gaussian noise to satisfy ($\epsilon$, $\delta$)-differential privacy. We
further provide a theoretical analysis demonstrating the unbiased nature of the
updates and deriving bounds on the variance introduced by noise, offering
practical guidance for privacy-budget calibration. Experimental results across
mainstream benchmarks show that DP-FedLoRA delivers competitive performance
while offering strong privacy guarantees, paving the way for scalable and
privacy-preserving LLM deployment in on-device environments.

</details>


### [6] [AgriSentinel: Privacy-Enhanced Embedded-LLM Crop Disease Alerting System](https://arxiv.org/abs/2509.09103)
*Chanti Raju Mylay,Bobin Deng,Zhipeng Cai,Honghui Xu*

Main category: cs.CR

TL;DR: AgriSentinel combines privacy, lightweight AI, and on-device LLMs to create a farmer-friendly crop disease alerting system, addressing gaps in data protection and actionable insights.


<details>
  <summary>Details</summary>
Motivation: Existing AI-based crop disease systems neglect data privacy, market pricing, and usability, leaving farmers vulnerable to exploitation and privacy breaches while failing to provide practical disease management strategies.

Method: The system employs a differential privacy mechanism for data protection, a lightweight deep learning model optimized for mobile devices, and a fine-tuned on-device LLM that generates actionable disease management suggestions using a curated knowledge pool.

Result: Experiments confirm AgriSentinel's effectiveness in maintaining classification accuracy while safeguarding privacy, delivering actionable disease strategies, and ensuring usability on mobile devices for farmers.

Conclusion: AgriSentinel presents a robust, farmer-centric solution for crop disease management by integrating privacy-preserving techniques, lightweight deep learning, and on-device LLMs, enhancing agricultural decision-making and productivity.

Abstract: Crop diseases pose significant threats to global food security, agricultural
productivity, and sustainable farming practices, directly affecting farmers'
livelihoods and economic stability. To address the growing need for effective
crop disease management, AI-based disease alerting systems have emerged as
promising tools by providing early detection and actionable insights for timely
intervention. However, existing systems often overlook critical aspects such as
data privacy, market pricing power, and farmer-friendly usability, leaving
farmers vulnerable to privacy breaches and economic exploitation. To bridge
these gaps, we propose AgriSentinel, the first Privacy-Enhanced Embedded-LLM
Crop Disease Alerting System. AgriSentinel incorporates a differential privacy
mechanism to protect sensitive crop image data while maintaining classification
accuracy. Its lightweight deep learning-based crop disease classification model
is optimized for mobile devices, ensuring accessibility and usability for
farmers. Additionally, the system includes a fine-tuned, on-device large
language model (LLM) that leverages a curated knowledge pool to provide farmers
with specific, actionable suggestions for managing crop diseases, going beyond
simple alerting. Comprehensive experiments validate the effectiveness of
AgriSentinel, demonstrating its ability to safeguard data privacy, maintain
high classification performance, and deliver practical, actionable disease
management strategies. AgriSentinel offers a robust, farmer-friendly solution
for automating crop disease alerting and management, ultimately contributing to
improved agricultural decision-making and enhanced crop productivity.

</details>


### [7] [CryptGNN: Enabling Secure Inference for Graph Neural Networks](https://arxiv.org/abs/2509.09107)
*Pritam Sen,Yao Ma,Cristian Borcea*

Main category: cs.CR

TL;DR: CryptGNN provides a secure inference framework for using graph neural networks (GNNs) on sensitive data in the cloud, ensuring data and model privacy.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the growing need for privacy-preserving machine learning in cloud-based services where clients delegate both data and computation to untrusted third parties.

Method: CryptGNN introduces secure message passing and feature transformation layers based on SMPC. Model and data privacy are preserved through cryptographic protocols, with no need for a trusted server and resilience to collusion.

Result: The authors confirm that CryptGNN achieves security against adversarial cloud providers and effective performance in real-world scenarios.

Conclusion: CryptGNN is a breakthrough in secure GNN inference, suitable for cloud and third-party MLaaS applications, and sets a new standard in privacy-preserving graph machine learning.

Abstract: We present CryptGNN, a secure and effective inference solution for
third-party graph neural network (GNN) models in the cloud, which are accessed
by clients as ML as a service (MLaaS). The main novelty of CryptGNN is its
secure message passing and feature transformation layers using distributed
secure multi-party computation (SMPC) techniques. CryptGNN protects the
client's input data and graph structure from the cloud provider and the
third-party model owner, and it protects the model parameters from the cloud
provider and the clients. CryptGNN works with any number of SMPC parties, does
not require a trusted server, and is provably secure even if P-1 out of P
parties in the cloud collude. Theoretical analysis and empirical experiments
demonstrate the security and efficiency of CryptGNN.

</details>


### [8] [Character-Level Perturbations Disrupt LLM Watermarks](https://arxiv.org/abs/2509.09112)
*Zhaoxi Zhang,Xiaomei Zhang,Yanjun Zhang,He Zhang,Shirui Pan,Bo Liu,Asif Qumer Gill,Leo Yu Zhang*

Main category: cs.CR

TL;DR: This paper reveals vulnerabilities in LLM watermarking by demonstrating effective removal attacks using character-level perturbations and genetic algorithms (GA). The authors highlight the adversarial dilemma in watermark defense design and propose adaptive attacks that bypass existing mechanisms, emphasizing the need for robust watermarking schemes.


<details>
  <summary>Details</summary>
Motivation: Prior watermark removal attacks are perceived as requiring large perturbations or strong adversaries, creating unrealistic expectations about robustness. The paper aims to address this gap by evaluating watermarking under realistic threat models where attackers have limited access to detectors and their query budgets are constrained.

Method: 1) Formalized system and threat models for LLM watermarks. 2} Analyzed attack range of perturbations at different levels (character/token). 3} Proposed GA-based guided removal attacks using a reference detector for optimization under black-box query limits. 4} Developed adaptive compound character-level attacks to exploit adversarial dilemma in fixed defenses.

Result: 1) Character-level perturbations (typos, swaps, homoglyphs, etc.) significantly outperformed token-level attacks in watermark removal under strict threat models. 2} GA-based attacks achieved strong performance with limited detector queries. 3} Adaptive compound attacks effectively defeated shielding defenses, confirming critical vulnerabilities in current watermarking schemes across multiple realistic scenarios.

Conclusion: The study demonstrates existing LLM watermarking systems are vulnerable to practical removal attacks under realistic adversary constraints. The inherent adversarial dilemma (fixed defenses vs. adaptive attacks) necessitates immediate development of next-generation watermarking methods that can resist character-level perturbations and adaptive compound attacks.

Abstract: Large Language Model (LLM) watermarking embeds detectable signals into
generated text for copyright protection, misuse prevention, and content
detection. While prior studies evaluate robustness using watermark removal
attacks, these methods are often suboptimal, creating the misconception that
effective removal requires large perturbations or powerful adversaries.
  To bridge the gap, we first formalize the system model for LLM watermark, and
characterize two realistic threat models constrained on limited access to the
watermark detector. We then analyze how different types of perturbation vary in
their attack range, i.e., the number of tokens they can affect with a single
edit. We observe that character-level perturbations (e.g., typos, swaps,
deletions, homoglyphs) can influence multiple tokens simultaneously by
disrupting the tokenization process. We demonstrate that character-level
perturbations are significantly more effective for watermark removal under the
most restrictive threat model. We further propose guided removal attacks based
on the Genetic Algorithm (GA) that uses a reference detector for optimization.
Under a practical threat model with limited black-box queries to the watermark
detector, our method demonstrates strong removal performance. Experiments
confirm the superiority of character-level perturbations and the effectiveness
of the GA in removing watermarks under realistic constraints. Additionally, we
argue there is an adversarial dilemma when considering potential defenses: any
fixed defense can be bypassed by a suitable perturbation strategy. Motivated by
this principle, we propose an adaptive compound character-level attack.
Experimental results show that this approach can effectively defeat the
defenses. Our findings highlight significant vulnerabilities in existing LLM
watermark schemes and underline the urgency for the development of new robust
mechanisms.

</details>


### [9] [IoTFuzzSentry: A Protocol Guided Mutation Based Fuzzer for Automatic Vulnerability Testing in Commercial IoT Devices](https://arxiv.org/abs/2509.09158)
*Priyanka Rushikesh Chaudhary,Rajib Ranjan Maiti*

Main category: cs.CR

TL;DR: The paper introduces IoTFuzzSentry, a mutation-based fuzzing tool for uncovering security vulnerabilities in commercial IoT devices, particularly emphasizing unauthorized access, data leakage, and other transport/application-layer issues, and evaluates its effectiveness through integration with Cotopaxi, finding multiple vulnerabilities and contributing to security improvement.


<details>
  <summary>Details</summary>
Motivation: During operational phase, IoT run lightweight servers for user interation, and implementation flaws in network-layer security can expose them to various vulnerabilities. The paper's motivation is to address this challenge by using protocol fuzzing to discover these flaws effectively.

Method: Authors present IoTFuzzSentry, a mutation-based fuzzing tool designed to detect transport and application-layer security vulnerabilities in IoT devices. The approach involves injecting crafted packets into IoT communications and integrates the tool into Cotopaxi for broader applicability.

Result: The evaluation of IoTFuzzSentry with commercial IoT devices leads to the discovery of 4 vulnerability categories (IoT Access Credential Leakage, Sneak IoT Live Video Stream, Creep IoT Live Image, IoT Command Injection). The researchers responsibly disclose all these issues and have filed 3 CVEs. Traffic analysis of six more IoT devices suggests potential for similar vulnerabilities.

Conclusion: The paper concludes that IoTFuzzSentry can automatically uncover unconventional security threats in IoT devices with minimal overhead, providing valuable insights for strengthening network-layer security and potentially preventing widespread exploitation.

Abstract: Protocol fuzzing is a scalable and cost-effective technique for identifying
security vulnerabilities in deployed Internet of Things devices. During their
operational phase, IoT devices often run lightweight servers to handle user
interactions, such as video streaming or image capture in smart cameras.
Implementation flaws in transport or application-layer security mechanisms can
expose IoT devices to a range of threats, including unauthorized access and
data leakage. This paper addresses the challenge of uncovering such
vulnerabilities by leveraging protocol fuzzing techniques that inject crafted
transport and application-layer packets into IoT communications. We present a
mutation-based fuzzing tool, named IoTFuzzSentry, to identify specific
non-trivial vulnerabilities in commercial IoT devices. We further demonstrate
how these vulnerabilities can be exploited in real-world scenarios. We
integrated our fuzzing tool into a well-known testing tool Cotopaxi and
evaluated it with commercial-off-the-shelf IoT devices such as IP cameras and
Smart Plug. Our evaluation revealed vulnerabilities categorized into 4 types
(IoT Access Credential Leakage, Sneak IoT Live Video Stream, Creep IoT Live
Image, IoT Command Injection) and we show their exploits using three IoT
devices. We have responsibly disclosed all these vulnerabilities to the
respective vendors. So far, we have published two CVEs, CVE-2024-41623 and
CVE-2024-42531, and one is awaiting. To extend the applicability, we have
investigated the traffic of six additional IoT devices and our analysis shows
that these devices can have similar vulnerabilities, due to the presence of a
similar set of application protocols. We believe that IoTFuzzSentry has the
potential to discover unconventional security threats and allow IoT vendors to
strengthen the security of their commercialized IoT devices automatically with
negligible overhead.

</details>


### [10] [Enhancing Cyber Threat Hunting -- A Visual Approach with the Forensic Visualization Toolkit](https://arxiv.org/abs/2509.09185)
*Jihane Najar,Marinos Tsantekidis,Aris Sotiropoulos,Vassilis Prevelakis*

Main category: cs.CR

TL;DR: This paper introduces the Forensic Visualization Toolkit (FVT), a proactive cyber threat hunting tool that combines digital forensics, advanced visualizations, and real-world validation. FVT addresses gaps in traditional security systems by enhancing situational awareness, enabling analysts to detect and mitigate advanced threats more effectively.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for proactive cybersecurity measures to combat advanced threats that evade traditional security systems. It highlights the limitations of passive defense mechanisms and the growing importance of tools like FVT to empower security analysts in dynamic threat landscapes.

Method: The authors developed the Forensic Visualization Toolkit (FVT), which integrates digital forensics investigations, analysis of digital evidence, and interactive visualizations to enhance situational awareness and risk management. The method emphasizes practical real-world scenarios and integration into EU-funded research projects for continuous improvement.

Result: FVT demonstrates improved effectiveness in identifying and responding to cyber threats through real-world applications and integration into EU research projects. It empowers analysts with intuitive, interactive tools for enhanced threat analysis and decision-making.

Conclusion: The paper concludes that the Forensic Visualization Toolkit (FVT) is a valuable tool for enhancing cybersecurity through proactive threat hunting, digital forensics, and advanced visualizations, significantly improving threat detection and response capabilities.

Abstract: In today's dynamic cyber threat landscape, organizations must take proactive
steps to bolster their cybersecurity defenses. Cyber threat hunting is a
proactive and iterative process aimed at identifying and mitigating advanced
threats that may go undetected by traditional security measures. Rather than
waiting for automated security systems to flag potential threats, threat
hunting involves actively searching for signs of malicious activity within an
organization's network. In this paper, we present the Forensic Visualization
Toolkit, a powerful tool designed for digital forensics investigations,
analysis of digital evidence, and advanced visualizations to enhance
cybersecurity situational awareness and risk management and empower security
analysts with an intuitive and interactive tool. Through practical, real-world
scenarios, we demonstrate how FVT significantly amplifies the capabilities of
cybersecurity professionals, enabling them to effectively identify, analyze,
and respond to threats. Furthermore, it is important to highlight that FVT has
been integrated into, utilized, and continually enhanced within various
EU-funded research projects over recent years.

</details>


### [11] [Shell or Nothing: Real-World Benchmarks and Memory-Activated Agents for Automated Penetration Testing](https://arxiv.org/abs/2509.09207)
*Wuyuao Mai,Geng Hong,Qi Liu,Jinsong Chen,Jiarun Dai,Xudong Pan,Yuan Zhang,Min Yang*

Main category: cs.CR

TL;DR: Researchers introduce TermiBench and TermiAgent, the first real-world benchmark and multi-agent framework for autonomous pentesting, closing the gap between simplified CTF-based evaluations and practical deployment while reducing costs and improving scalability.


<details>
  <summary>Details</summary>
Motivation: Traditional pentesting is costly, time-intensive, and human-dependent. Existing AI-based agents are over-optimized for simplified CTF environments that lack real-world complexity in tasks like host discrimination, autonomous reconnaissance, and robust exploit execution. This creates a gap between research performance and practical deployment.

Method: The authors (1) develop TermiBench, a benchmark with 510 hosts spanning 25 services and 30 CVEs, focusing on full system control rather than flag-hunting. (2) Propose TermiAgent, a multi-agent framework with Located Memory Activation to overcome long-context forgetting and structured code understanding for exploit generation. (3) Evaluate these techniques in realistic scenarios.

Result: TermiAgent significantly outperforms state-of-the-art agents in penetration testing capability under realistic conditions, reducing execution time and financial costs by 50% and demonstrating practical deployment on laptop-scale hardware. Existing pentesting systems struggle to achieve system shells in the proposed benchmark.

Conclusion: The paper introduces TermiBench, the first real-world autonomous pentesting benchmark, and TermiAgent, a novel multi-agent framework that addresses limitations of existing AI-driven penetration testing methods. These contributions establish a milestone for practical, scalable, and cost-effective AI-based cybersecurity solutions.

Abstract: Penetration testing is critical for identifying and mitigating security
vulnerabilities, yet traditional approaches remain expensive, time-consuming,
and dependent on expert human labor. Recent work has explored AI-driven
pentesting agents, but their evaluation relies on oversimplified
capture-the-flag (CTF) settings that embed prior knowledge and reduce
complexity, leading to performance estimates far from real-world practice. We
close this gap by introducing the first real-world, agent-oriented pentesting
benchmark, TermiBench, which shifts the goal from 'flag finding' to achieving
full system control. The benchmark spans 510 hosts across 25 services and 30
CVEs, with realistic environments that require autonomous reconnaissance,
discrimination between benign and exploitable services, and robust exploit
execution. Using this benchmark, we find that existing systems can hardly
obtain system shells under realistic conditions.
  To address these challenges, we propose TermiAgent, a multi-agent penetration
testing framework. TermiAgent mitigates long-context forgetting with a Located
Memory Activation mechanism and builds a reliable exploit arsenal via
structured code understanding rather than naive retrieval. In evaluations, our
work outperforms state-of-the-art agents, exhibiting stronger penetration
testing capability, reducing execution time and financial cost, and
demonstrating practicality even on laptop-scale deployments. Our work delivers
both the first open-source benchmark for real-world autonomous pentesting and a
novel agent framework that establishes a milestone for AI-driven penetration
testing.

</details>


### [12] [A Cyber-Twin Based Honeypot for Gathering Threat Intelligence](https://arxiv.org/abs/2509.09222)
*Muhammad Azmi Umer,Zhan Xuna,Yan Lin Aung,Aditya P. Mathur,Jianying Zhou*

Main category: cs.CR

TL;DR: This paper proposes a cyber twin-based honeypot for water treatment plants to collect threat intelligence by attracting and analyzing attacks, enhancing critical infrastructure protection.


<details>
  <summary>Details</summary>
Motivation: Critical infrastructure (CI) systems like water treatment plants face cyber threats. Existing protection methods need improvement, necessitating realistic threat intelligence collection for proactive defense.

Method: A cyber twin honeypot replicating a water treatment plant was developed. It records attacks, analyzes threats, and shares intelligence with plant management to strengthen defenses. The system has been operationally tested.

Result: The honeypot experienced multiple attacks, including ransomware, demonstrating its effectiveness in attracting real-world threats and generating actionable threat intelligence for security improvement.

Conclusion: The honeypot approach successfully collects cyber threat intelligence for critical infrastructure protection. Operational results validate its potential to enhance CI defenses through real-time attack analysis and intelligence sharing.

Abstract: Critical Infrastructure (CI) is prone to cyberattacks. Several techniques
have been developed to protect CI against such attacks. In this work, we
describe a honeypot based on a cyber twin for a water treatment plant. The
honeypot is intended to serve as a realistic replica of a water treatment plant
that attracts potential attackers. The attacks launched on the honeypot are
recorded and analyzed for threat intelligence. The intelligence so obtained is
shared with the management of water treatment plants, who in turn may use it to
improve plant protection systems. The honeypot used here is operational and has
been attacked on several occasions using, for example, a ransomware attack that
is described in detail.

</details>


### [13] [What You Code Is What We Prove: Translating BLE App Logic into Formal Models with LLMs for Vulnerability Detection](https://arxiv.org/abs/2509.09291)
*Biwei Yan,Yue Zhang,Minghui Xu,Runyu Pan,Jinku Li,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: VerifiaBLE uses LLMs to automate formal verification of BLE applications, exposing that most apps lack basic security protections and demonstrating scalable application of formal methods.


<details>
  <summary>Details</summary>
Motivation: BLE applications frequently lack fundamental security protections due to developers omitting encryption, authentication, and freshness mechanisms. Manual formal verification is impractical for large-scale analysis, necessitating automated solutions.

Method: The paper proposes VerifiaBLE, a system combining static analysis, prompt-guided LLM translation of BLE code into process models, and symbolic verification (e.g., ProVerif) to validate encryption, randomness, and authentication properties.

Result: Analysis of 1,050 Android BLE apps revealed that only 10.2% implemented all three core security features, and 53.9% omitted them entirely, highlighting systemic weaknesses in BLE security practices.

Conclusion: The study demonstrates that using large language models (LLMs) as translators to convert BLE code into formal models significantly reduces the barrier to formal verification, enabling scalable security analysis across critical domains like BLE applications.

Abstract: The application layer of Bluetooth Low Energy (BLE) is a growing source of
security vulnerabilities, as developers often neglect to implement critical
protections such as encryption, authentication, and freshness. While formal
verification offers a principled way to check these properties, the manual
effort of constructing formal models makes it impractical for large-scale
analysis. This paper introduces a key insight: BLE application security
analysis can be reframed as a semantic translation problem, i.e., from
real-world code to formal models. We leverage large language models (LLMs) not
to directly detect vulnerabilities, but to serve as translators that convert
BLE-specific code into process models verifiable by tools like ProVerif. We
implement this idea in VerifiaBLE, a system that combines static analysis,
prompt-guided LLM translation, and symbolic verification to check three core
security features: encryption, randomness, and authentication. Applied to 1,050
Android BLE apps, VerifiaBLE uncovers systemic weaknesses: only 10.2\% of apps
implement all three protections, while 53.9\% omit them entirely. Our work
demonstrates that using LLMs as structured translators can lower the barrier to
formal methods, unlocking scalable verification across security-critical
domains.

</details>


### [14] [On the Security of SSH Client Signatures](https://arxiv.org/abs/2509.09331)
*Fabian Bäumer,Marcus Brinkmann,Maximilian Radoy,Jörg Schwenk,Juraj Somorovsky*

Main category: cs.CR

TL;DR: Analysis of 31M SSH client keys reveals declining RSA use but persistent vulnerabilities, including a PuTTY ECDSA nonce exploit requiring just 58 signatures. The work uncovers critical client-side SSH security issues.


<details>
  <summary>Details</summary>
Motivation: SSH client keys are critical for authentication but lack systematic security analysis compared to server keys. This work addresses this gap by empirically assessing client key security and implementation flaws.

Method: The research collected 31 million SSH client keys from public repositories and conducted longitudinal security tests, followed by black-box experiments analyzing 24 SSH clients' signature algorithms.

Result: 98 weak keys, 139 low-entropy keys, and 149 shared-factor keys were identified. Key findings include PuTTY's deterministic ECDSA nonce vulnerability exploitable with 58 signatures, confirmed in CVE-2024-31497.

Conclusion: The study highlights the importance of SSH client key security, identifies vulnerabilities in key implementations, and demonstrates potential risks even with deterministic nonces in ECDSA, leading to a critical CVE fix in PuTTY.

Abstract: Administrators and developers use SSH client keys and signatures for
authentication, for example, to access internet backbone servers or to commit
new code on platforms like GitHub. However, unlike servers, SSH clients cannot
be measured through internet scans. We close this gap in two steps. First, we
collect SSH client public keys. Such keys are regularly published by their
owners on open development platforms like GitHub and GitLab. We systematize
previous non-academic work by subjecting these keys to various security tests
in a longitudinal study. Second, in a series of black-box lab experiments, we
analyze the implementations of algorithms for SSH client signatures in 24
popular SSH clients for Linux, Windows, and macOS.
  We extracted 31,622,338 keys from three public sources in two scans. Compared
to previous work, we see a clear tendency to abandon RSA signatures in favor of
EdDSA signatures. Still, in January 2025, we found 98 broken short keys, 139
keys generated from weak randomness, and 149 keys with common or small
factors-the large majority of the retrieved keys exposed no weakness.
  Weak randomness can not only compromise a secret key through its public key,
but also through signatures. It is well-known that a bias in random nonces in
ECDSA can reveal the secret key through public signatures. For the first time,
we show that the use of deterministic nonces in ECDSA can also be dangerous:
The private signing key of a PuTTY client can be recovered from just 58 valid
signatures if ECDSA with NIST curve P-521 is used. PuTTY acknowledged our
finding in CVE-2024-31497, and they subsequently replaced the nonce generation
algorithm.

</details>


### [15] [[Extended] Ethics in Computer Security Research: A Data-Driven Assessment of the Past, the Present, and the Possible Future](https://arxiv.org/abs/2509.09351)
*Harshini Sri Ramulu,Helen Schmitt,Bogdan Rerich,Rachel Gonzalez Rodriguez,Tadayoshi Kohno,Yasemin Acar*

Main category: cs.CR

TL;DR: This paper analyzes ethical reporting gaps in 2024 top-tier computer security research via a literature review and interviews with 24 researchers, revealing inconsistent ethics practices and offering suggestions to strengthen ethical decision-making in the field.


<details>
  <summary>Details</summary>
Motivation: The lack of clear guidance for computer security researchers on making, documenting, and assessing ethical decisions when moral clarity is absent motivates this work, as ethical implications in the field are currently inconsistently addressed.

Method: The authors reviewed all 1154 top-tier computer security papers published in 2024 to assess ethics reporting practices and conducted a semi-structured interview study with 24 researchers (including reviewers, ethics committee members, and program chairs) to analyze ethical decision-making in research and peer review.

Result: Results show inconsistent ethics reporting in papers, with overemphasis on institutional approval, human subjects protection, and responsible disclosure, but underemphasis on harm-benefit balancing. Interviews revealed a strong ethical intent among researchers but inconsistent application of ethical frameworks, values, and decision-making outcomes.

Conclusion: The paper concludes by contributing suggestions to improve the state of ethics in computer security research, emphasizing the need for more consistent ethical frameworks and discussion of balancing harms and benefits.

Abstract: Ethical questions are discussed regularly in computer security. Still,
researchers in computer security lack clear guidance on how to make, document,
and assess ethical decisions in research when what is morally right or
acceptable is not clear-cut. In this work, we give an overview of the
discussion of ethical implications in current published work in computer
security by reviewing all 1154 top-tier security papers published in 2024,
finding inconsistent levels of ethics reporting with a strong focus of
reporting institutional or ethics board approval, human subjects protection,
and responsible disclosure, and a lack of discussion of balancing harms and
benefits. We further report on the results of a semi-structured interview study
with 24 computer security and privacy researchers (among whom were also:
reviewers, ethics committee members, and/or program chairs) and their ethical
decision-making both as authors and during peer review, finding a strong desire
for ethical research, but a lack of consistency in considered values, ethical
frameworks (if articulated), decision-making, and outcomes. We present an
overview of the current state of the discussion of ethics and current de-facto
standards in computer security research, and contribute suggestions to improve
the state of ethics in computer security research.

</details>


### [16] [ENSI: Efficient Non-Interactive Secure Inference for Large Language Models](https://arxiv.org/abs/2509.09424)
*Zhiyu He,Maojiang Wang,Xinwen Gao,Yuchuan Luo,Lin Liu,Shaojing Fu*

Main category: cs.CR

TL;DR: ENSI is a non-interactive secure inference framework for LLMs that co-designs cryptographic protocols and architecture to achieve 8x matrix multiplication and 2.6x softmax speedups with minimal bootstrapping.


<details>
  <summary>Details</summary>
Motivation: Traditional cryptographic methods struggle with LLMs due to their massive scale and complexity, limiting practical privacy-preserving inference.

Method: 1) Co-designs CKKS encryption with BitNet's efficient architecture 232) Introduces HE-compatible sigmoid attention to avoid softmax computations 3) Integrates bootstrapping into RMSNorm for reduced overhead.

Result: 8x faster matrix multiplications, 2.6x faster HE softmax inference on CPU, and bootstrapping frequency reduced to 1%. Achieved through BitNet-CKKS synergy, hardware-optimized attention, and adaptive bootstrapping.

Conclusion: ENSI demonstrates a practical solution for secure LLM inference by synergizing cryptographic optimizations with architecture design, achieving state-of-the-art performance while maintaining strong privacy guarantees.

Abstract: Secure inference enables privacy-preserving machine learning by leveraging
cryptographic protocols that support computations on sensitive user data
without exposing it. However, integrating cryptographic protocols with large
language models (LLMs) presents significant challenges, as the inherent
complexity of these protocols, together with LLMs' massive parameter scale and
sophisticated architectures, severely limits practical usability. In this work,
we propose ENSI, a novel non-interactive secure inference framework for LLMs,
based on the principle of co-designing the cryptographic protocols and LLM
architecture. ENSI employs an optimized encoding strategy that seamlessly
integrates CKKS scheme with a lightweight LLM variant, BitNet, significantly
reducing the computational complexity of encrypted matrix multiplications. In
response to the prohibitive computational demands of softmax under homomorphic
encryption (HE), we pioneer the integration of the sigmoid attention mechanism
with HE as a seamless, retraining-free alternative. Furthermore, by embedding
the Bootstrapping operation within the RMSNorm process, we efficiently refresh
ciphertexts while markedly decreasing the frequency of costly bootstrapping
invocations. Experimental evaluations demonstrate that ENSI achieves
approximately an 8x acceleration in matrix multiplications and a 2.6x speedup
in softmax inference on CPU compared to state-of-the-art method, with the
proportion of bootstrapping is reduced to just 1%.

</details>


### [17] [Prompt Pirates Need a Map: Stealing Seeds helps Stealing Prompts](https://arxiv.org/abs/2509.09488)
*Felix Mächtle,Ashwath Shetty,Jonas Sander,Nils Loose,Sören Pirk,Thomas Eisenbarth*

Main category: cs.CR

TL;DR: This paper presents SeedSnitch and PromptPirate as a two-stage prompt-stealing attack targeting the seed-recovery vulnerability in diffusion models on major image generation frameworks. They demonstrated 95% seed recovery success with 8-11% improvement over state-of-art methods.


<details>
  <summary>Details</summary>
Motivation: The research was motivated by the critical need to address the issue of prompt theft in diffusion models, where the initial random seeds might be exploited by attackers. Previous prompt recovery methods were not considering this aspect, which the authors identified as a limitation for comprehensively secure text-to-image systems.

Method: The paper reveals a vulnerability in the randomness generation (seed range restriction in PyTorch on CPUs) and formulates the prompt-stealing challenge via seed recovery (SeedSnitch tool) followed by prompt optimization using the recovered seeds with PromptPirate method. It leverages the incomplete consideration of initial randomness in prior work to attack the system.

Result: The analysis showed that 95% of seed values in CivitAI can be brute-forced in 140 minutes per seed by SeedSnitch. PromptPirate demonstrated an 8-11% improvement in LPIPS similarity compared to existing prompt recovery methods like PromptStealer, P2HP, and CLIP-Interrogator.

Conclusion: The research highlights the critical seed generation vulnerability in diffusion models (CWE-339), effectively demonstrates the practical threat using SeedSnitch and PromptPirate, and proposes countermeasures that prevent prompt-stuffing. The improved seed and prompt recovery metrics and collaboration with developers present a proactive approach to securing text-to-image generation.

Abstract: Diffusion models have significantly advanced text-to-image generation,
enabling the creation of highly realistic images conditioned on textual prompts
and seeds. Given the considerable intellectual and economic value embedded in
such prompts, prompt theft poses a critical security and privacy concern. In
this paper, we investigate prompt-stealing attacks targeting diffusion models.
We reveal that numerical optimization-based prompt recovery methods are
fundamentally limited as they do not account for the initial random noise used
during image generation. We identify and exploit a noise-generation
vulnerability (CWE-339), prevalent in major image-generation frameworks,
originating from PyTorch's restriction of seed values to a range of $2^{32}$
when generating the initial random noise on CPUs. Through a large-scale
empirical analysis conducted on images shared via the popular platform CivitAI,
we demonstrate that approximately 95% of these images' seed values can be
effectively brute-forced in 140 minutes per seed using our seed-recovery tool,
SeedSnitch. Leveraging the recovered seed, we propose PromptPirate, a genetic
algorithm-based optimization method explicitly designed for prompt stealing.
PromptPirate surpasses state-of-the-art methods, i.e., PromptStealer, P2HP, and
CLIP-Interrogator, achieving an 8-11% improvement in LPIPS similarity.
Furthermore, we introduce straightforward and effective countermeasures that
render seed stealing, and thus optimization-based prompt stealing, ineffective.
We have disclosed our findings responsibly and initiated coordinated mitigation
efforts with the developers to address this critical vulnerability.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [18] [Pattern-Based File and Data Access with Python Glob: A Comprehensive Guide for Computational Research](https://arxiv.org/abs/2509.08843)
*Sidney Shapiro*

Main category: cs.SE

TL;DR: This paper promotes Python's glob module as an essential, underappreciated tool for reproducible research and data engineering, offering practical examples for data science, analytics, and AI workflows.


<details>
  <summary>Details</summary>
Motivation: Pattern-based file access is identified as a critical but under-documented aspect of computational research, requiring better tools for reproducibility and cross-disciplinary workflow scalability.

Method: The authors demonstrate practical use cases through Python examples integrated with pandas, scikit-learn, and matplotlib, showing how glob enables scalable data ingestion and analytical pipeline integration.

Result: Concrete implementations of glob in large-scale data ingestion, organizational analysis, AI dataset construction, and reproducible research practices are presented, validating its versatility across domains.

Conclusion: The paper positions glob as a foundational, methodological building block for Python-based research workflows, advocating for its adoption as a default citation for file pattern matching in computational research.

Abstract: Pattern-based file access is a fundamental but often under-documented aspect
of computational research. The Python glob module provides a simple yet
powerful way to search, filter, and ingest files using wildcard patterns,
enabling scalable workflows across disciplines. This paper introduces glob as a
versatile tool for data science, business analytics, and artificial
intelligence applications. We demonstrate use cases including large-scale data
ingestion, organizational data analysis, AI dataset construction, and
reproducible research practices. Through concrete Python examples with widely
used libraries such as pandas,scikit-learn, and matplotlib, we show how glob
facilitates efficient file traversal and integration with analytical pipelines.
By situating glob within the broader context of reproducible research and data
engineering, we highlight its role as a methodological building block. Our goal
is to provide researchers and practitioners with a concise reference that
bridges foundational concepts and applied practice, making glob a default
citation for file pattern matching in Python-based research workflows.

</details>


### [19] [A Systematic Mapping Study on Chatbots in Programming Education](https://arxiv.org/abs/2509.08857)
*Marcelino Garcia,Renato Garcia,Arthur Parizotto,Andre Mendes,Pedro Valle,Ricardo Vilela,Renato Balancieri,Williamson Silva*

Main category: cs.SE

TL;DR: This SMS of 54 studies reveals Python-centric chatbots for programming education employ varied pedagogical methods but highlight research gaps.


<details>
  <summary>Details</summary>
Motivation: Chatbots in programming education require systematic analysis to understand their design, applications, and pedagogical relevance for effective learning.

Method: A Systematic Mapping Study (SMS) analyzed 54 selected publications from an initial 3,216, addressing chatbot characteristics through five research subquestions.

Result: Python-focused chatbots dominate the literature, emphasizing fundamental programming concepts with diverse pedagogical and technological approaches.

Conclusion: The study provides insights to guide the development of educational chatbots for programming instruction and highlights trends and gaps in existing research.

Abstract: Educational chatbots have gained prominence as support tools for teaching
programming, particularly in introductory learning contexts. This paper
presents a Systematic Mapping Study (SMS) that investigated how such agents
have been developed and applied in programming education. From an initial set
of 3,216 publications, 54 studies were selected and analyzed based on five
research subquestions, addressing chatbot types, programming languages used,
educational content covered, interaction models, and application contexts. The
results reveal a predominance of chatbots designed for Python instruction,
focusing on fundamental programming concepts, and employing a wide variety of
pedagogical approaches and technological architectures. In addition to
identifying trends and gaps in the literature, this study provides insights to
inform the development of new educational tools for programming instruction.

</details>


### [20] [GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation](https://arxiv.org/abs/2509.08863)
*Qianqian Luo,Liuchang Xu,Qingming Lin,Sensen Wu,Ruichen Mao,Chao Wang,Hailin Feng,Bo Huang,Zhenhong Du*

Main category: cs.SE

TL;DR: GeoJSON Agents bridge LLMs and GIS via a multi-agent framework with Function Calling and Code Generation. Code-based agents outperform (97.14%) while both surpass general models, offering insights for GeoAI systems.


<details>
  <summary>Details</summary>
Motivation: LLMs lack GIS expertise, limiting their spatial task capabilities. Existing methods lack scalability and performance for complex GIS operations.

Method: GeoJSON Agents utilize a three-component architecture (task parsing, agent collaboration, result integration) combining Function Calling and Code Generation. Specialized Worker agents process spatial data by invoking APIs or generating Python code.

Result: Code Generation-based agents achieved 97.14% accuracy (vs. 48.57% for general models), while Function Calling reached 85.71%. Code Generation offers flexibility; Function Calling provides stability.

Conclusion: The study introduces GeoJSON Agents, a novel LLM multi-agent framework for GIS automation, demonstrating that Code Generation outperforms Function Calling in flexibility while both surpass general-purpose models. It provides a benchmark for future GeoAI research.

Abstract: LLMs have made substantial progress in task automation and natural language
understanding.However,without expertise in GIS,they continue to encounter
limitations.To address these issues, we propose GeoJSON Agents-a multi-agent
LLM architecture.This framework transforms natural language tasks into
structured GeoJSON operation commands and processes spatial data using two
widely adopted LLM enhancement techniques:Function Calling and Code
Generation.The architecture consists of three components-task parsing,agent
collaboration,and result integration-aimed at enhancing both the performance
and scalability of GIS automation.The Planner agent interprets natural language
tasks into structured GeoJSON commands.Then,specialized Worker agents
collaborate according to assigned roles to perform spatial data processing and
analysis,either by invoking predefined function APIs or by dynamically
generating and executing Python-based spatial analysis code.Finally,the system
integrates the outputs from multiple execution rounds into
reusable,standards-compliant GeoJSON files.To systematically evaluate the
performance of the two approaches,we constructed a benchmark dataset of 70
tasks with varying complexity and conducted experiments using OpenAI's GPT-4o
as the core model.Results indicate that the Function Calling-based GeoJSON
Agent achieved an accuracy of 85.71%,while the Code Generation-based agent
reached 97.14%,both significantly outperforming the best-performing
general-purpose model (48.57%).Further analysis reveals that the Code
Generation provides greater flexibility,whereas the Function Calling approach
offers more stable execution.This study is the first to introduce an LLM
multi-agent framework for GeoJSON data and to compare the strengths and
limitations of two mainstream LLM enhancement methods,offering new perspectives
for improving GeoAI system performance.

</details>


### [21] [TraceRAG: A LLM-Based Framework for Explainable Android Malware Detection and Behavior Analysis](https://arxiv.org/abs/2509.08865)
*Guangyu Zhang,Xixuan Wang,Shiyu Sun,Peiyan Xiao,Kun Sun,Yanhai Xiong*

Main category: cs.SE

TL;DR: TraceRAG is a LLM-powered RAG framework for explainable Android malware analysis, achieving high detection accuracy with human-readable behavior reports.


<details>
  <summary>Details</summary>
Motivation: Traditional analysis techniques struggle to detect deeply hidden malware behaviors and provide explainable justifications. The paper addresses the critical need for robust frameworks that reveal evasion tactics while maintaining human-interpretable analysis.

Method: TraceRAG employs a retrieval-augmented generation framework using LLMs to generate method-level code summaries in a vector database. Behavior-focused queries retrieve relevant code snippets for multi-turn analysis, producing human-readable reports linking malicious behaviors to their code implementations.

Result: 96% malware detection accuracy and 83.81% behavior identification accuracy via VirusTotal validation and manual checks. Expert assessments confirm the practical utility of the generated reports.

Conclusion: TraceRAG demonstrates effective malware detection and analysis with high accuracy, providing human-readable explanations for malicious behavior identification, validated by expert evaluation and updated VirusTotal metrics.

Abstract: Sophisticated evasion tactics in malicious Android applications, combined
with their intricate behavioral semantics, enable attackers to conceal
malicious logic within legitimate functions, underscoring the critical need for
robust and in-depth analysis frameworks. However, traditional analysis
techniques often fail to recover deeply hidden behaviors or provide
human-readable justifications for their decisions. Inspired by advances in
large language models (LLMs), we introduce TraceRAG, a retrieval-augmented
generation (RAG) framework that bridges natural language queries and Java code
to deliver explainable malware detection and analysis. First, TraceRAG
generates summaries of method-level code snippets, which are indexed in a
vector database. At query time, behavior-focused questions retrieve the most
semantically relevant snippets for deeper inspection. Finally, based on the
multi-turn analysis results, TraceRAG produces human-readable reports that
present the identified malicious behaviors and their corresponding code
implementations. Experimental results demonstrate that our method achieves 96\%
malware detection accuracy and 83.81\% behavior identification accuracy based
on updated VirusTotal (VT) scans and manual verification. Furthermore, expert
evaluation confirms the practical utility of the reports generated by TraceRAG.

</details>


### [22] [Benchmarking Energy Efficiency of Large Language Models Using vLLM](https://arxiv.org/abs/2509.08867)
*K. Pronk,Q. Zhao*

Main category: cs.SE

TL;DR: This paper introduces the LLM Efficiency Benchmark to assess energy consumption of LLMs in real-world production scenarios, using vLLM for accurate evaluation.


<details>
  <summary>Details</summary>
Motivation: Growing use of LLMs is increasing energy consumption and environmental impact; benchmarks are needed that reflect realistic deployment scenarios for sustainable development.

Method: Introduced the LLM Efficiency Benchmark that simulates real-world conditions and uses vLLM, a production backend, to evaluate energy efficiency under varying model size, architecture, and concurrent request volumes.

Result: Successfully developed a benchmark showing the impact of model size, architecture, and request volume on energy consumption, offering practical insights for sustainable AI deployment.

Conclusion: The LLM Efficiency Benchmark provides a realistic method for developers to measure energy efficiency in practical use, aiding in the creation of more sustainable AI systems.

Abstract: The prevalence of Large Language Models (LLMs) is having an growing impact on
the climate due to the substantial energy required for their deployment and
use. To create awareness for developers who are implementing LLMs in their
products, there is a strong need to collect more information about the energy
efficiency of LLMs. While existing research has evaluated the energy efficiency
of various models, these benchmarks often fall short of representing realistic
production scenarios. In this paper, we introduce the LLM Efficiency Benchmark,
designed to simulate real-world usage conditions. Our benchmark utilizes vLLM,
a high-throughput, production-ready LLM serving backend that optimizes model
performance and efficiency. We examine how factors such as model size,
architecture, and concurrent request volume affect inference energy efficiency.
Our findings demonstrate that it is possible to create energy efficiency
benchmarks that better reflect practical deployment conditions, providing
valuable insights for developers aiming to build more sustainable AI systems.

</details>


### [23] [CLARA: A Developer's Companion for Code Comprehension and Analysis](https://arxiv.org/abs/2509.09072)
*Ahmed Adnan,Mushfiqur Rahman,Saad Sakib Noor,Kazi Sakib*

Main category: cs.SE

TL;DR: CLARA is an open-source browser extension using AI to aid code comprehension and refactoring, shown via datasets and user studies to be accurate, useful, and practical.


<details>
  <summary>Details</summary>
Motivation: Prior tools require significant manual effort, lack context-awareness, and demand project setup, limiting their practicality for code comprehension and analysis tasks.

Method: CLARA employs a state-of-the-art inference model integrated into a browser extension, evaluated via existing datasets and a user study with 10 participants to assess usability and accuracy.

Result: Results demonstrate CLARA's accuracy (validated via datasets) and usability (affirmed by user study), showing it is practical for real-world code analysis tasks.

Conclusion: CLARA effectively addresses the limitations of existing code analysis tools by providing a context-aware, user-friendly solution with strong empirical validation through datasets and user studies.

Abstract: Code comprehension and analysis of open-source project codebases is a task
frequently performed by developers and researchers. However, existing tools
that practitioners use for assistance with such tasks often require prior
project setup, lack context-awareness, and involve significant manual effort.
To address this, we present CLARA, a browser extension that utilizes a
state-of-the-art inference model to assist developers and researchers in: (i)
comprehending code files and code fragments, (ii) code refactoring, and (iii)
code quality attribute detection. We qualitatively evaluated CLARA's inference
model using existing datasets and methodology, and performed a comprehensive
user study with 10 developers and academic researchers to assess its usability
and usefulness. The results show that CLARA is useful, accurate, and practical
in code comprehension and analysis tasks. CLARA is an open-source tool
available at https://github.com/SaadNoor555/CLARA_tool_demo. A video showing
the full capabilities of CLARA can be found at
https://youtu.be/VDKVXvIH41Q?si=qBFsmS_Y4m_9x3YH.

</details>


### [24] [Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset](https://arxiv.org/abs/2509.09192)
*Doha Nam,Taehyoun Kim,Duksan Ryu,Jongmoon Baik*

Main category: cs.SE

TL;DR: The paper introduces ReDef, a high-confidence function-level JIT-SDP dataset curated via revert commit analysis and GPT-driven triage, and evaluates pre-trained language models (PLMs) by comparing diff-style vs. whole-function encodings, revealing limitations in semantic understanding of code changes.


<details>
  <summary>Details</summary>
Motivation: Existing JIT-SDP datasets suffer from noisy labels and poor precision in identifying bug-inducing commits, hindering reliable model evaluation and insights into code modification comprehension by PLMs.

Method: 1) Construct ReDef: uses revert commits for defective cases and post-hoc validation for clean cases, filtering out >95%,750 ambiguous instances via GPT-assisted triage. 2. Evaluate PLMs (CodeBERT, CodeT5+, UniXcoder) via 5 encoding strategies, then test robustness using counterfactual perturbations (e.g., inverting diff polarity, spurious markers).

Result: ReDef shows 35% fewer noisy labels vs. prior datasets. Diff-style encodings consistently outperform whole-function formats (p < 0.01), but counterfactual tests reveal most models' performance drops <15%, indicating reliance on superficial cues rather than semantic understanding.

Conclusion: While compact diff encodings improve JIT-SDP accuracy, current PLMs lack true comprehension of code modifications, as performance remains tied to surface-level patterns rather than meaningful edit semantics.

Abstract: Just-in-Time software defect prediction (JIT-SDP) plays a critical role in
prioritizing risky code changes during code review and continuous integration.
However, existing datasets often suffer from noisy labels and low precision in
identifying bug-inducing commits. To address this, we present ReDef
(Revert-based Defect dataset), a high-confidence benchmark of function-level
modifications curated from 22 large-scale C/C++ projects. Defective cases are
anchored by revert commits, while clean cases are validated through post-hoc
history checks. Ambiguous instances are conservatively filtered out via a
GPT-assisted triage process involving multiple votes and audits. This pipeline
yields 3,164 defective and 10,268 clean modifications, offering substantially
more reliable labels than prior existing resources. Beyond dataset
construction, we provide the first systematic evaluation of how pre-trained
language models (PLMs) reason about code modifications -- specifically, which
input encodings most effectively expose change information, and whether models
genuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder
under five encoding strategies, and further probe their sensitivity through
counterfactual perturbations that swap added/deleted blocks, invert diff
polarity, or inject spurious markers. Our results show that compact diff-style
encodings consistently outperform whole-function formats across all PLMs, with
statistical tests confirming large, model-independent effects. However, under
counterfactual tests, performance degrades little or not at all -- revealing
that what appears to be robustness in fact reflects reliance on superficial
cues rather than true semantic understanding. These findings indicate that,
unlike in snapshot-based tasks, current PLMs remain limited in their ability to
genuinely comprehend code modifications.

</details>


### [25] [On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability](https://arxiv.org/abs/2509.09194)
*Ayelet Berzack,Guy Katz*

Main category: cs.SE

TL;DR: This paper proposes combining LLMs with traditional techniques via Scenario-Based Programming (SBP) to improve software development reliability. A case study on Connect4 shows improved agent performance and successful verification. The approach balances LLM strengths with structured validation.


<details>
  <summary>Details</summary>
Motivation: LLMs offer significant benefits in software development but risk introducing undetected errors. The motivation is to address this reliability issue by structuring the integration of LLMs with traditional methods, ensuring critical properties are verifiable and errors are minimized.

Method: The methodology integrates LLMs with traditional techniques via Scenario-Based Programming (SBP), an event-driven, scenario-based paradigm that facilitates structured collaboration between human expertise and LLM capabilities. This approach allows developers to inject domain knowledge and validate generated outputs systematically.

Result: A Connect4 agent implemented using LLM-SBP integration defeated existing strong agents, achieved formal verification in some cases, and demonstrated practical usability. The approach proved effective in balancing LLMs' creativity with rigorous validation, supported by public availability of the case-study code.

Conclusion: The paper concludes that combining LLMs with traditional software engineering techniques through the SBP paradigm enhances reliability, reduces errors, and enables verification of program properties, demonstrating a viable path for integrating LLMs into software development.

Abstract: Large Language Models (LLMs) are fast becoming indispensable tools for
software developers, assisting or even partnering with them in crafting complex
programs. The advantages are evident -- LLMs can significantly reduce
development time, generate well-organized and comprehensible code, and
occasionally suggest innovative ideas that developers might not conceive on
their own. However, despite their strengths, LLMs will often introduce
significant errors and present incorrect code with persuasive confidence,
potentially misleading developers into accepting flawed solutions.
  In order to bring LLMs into the software development cycle in a more reliable
manner, we propose a methodology for combining them with ``traditional''
software engineering techniques in a structured way, with the goal of
streamlining the development process, reducing errors, and enabling users to
verify crucial program properties with increased confidence. Specifically, we
focus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,
scenario-based approach for software engineering -- to allow human developers
to pour their expert knowledge into the LLM, as well as to inspect and verify
its outputs.
  To evaluate our methodology, we conducted a significant case study, and used
it to design and implement the Connect4 game. By combining LLMs and SBP we were
able to create a highly-capable agent, which could defeat various strong
existing agents. Further, in some cases, we were able to formally verify the
correctness of our agent. Finally, our experience reveals interesting insights
regarding the ease-of-use of our proposed approach. The full code of our
case-study will be made publicly available with the final version of this
paper.

</details>


### [26] [Altered Histories in Version Control System Repositories: Evidence from the Trenches](https://arxiv.org/abs/2509.09294)
*Solal Rapaport,Laurent Pautet,Samuel Tardieu,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: This paper explores Git history rewriting's risks in public repositories, analyzes 111M projects to identify 8.7M alterations (1.22M repos), demonstrates misuse via license changes/secret removal, and introduces GitHistorian to automate detection.


<details>
  <summary>Details</summary>
Motivation: The study addresses the risks posed by Git's history-rewriting capabilities in public repositories, which compromise workflow consistency, reproducibility, and security, enabling potential supply chain attacks.

Method: The authors conducted the first large-scale investigation by analyzing 111 million public repositories from Software Heritage, identifying 8.7 million rewritten histories. They categorized alterations by context (repositories/branches) and content (files/metadata) and validated findings through case studies on license retroactivity and secret removal.

Result: 1.22 million repositories (1.1%) exhibited altered histories, with case studies revealing prevalent misuse via retroactive license changes and accidental secret removal. These behaviors highlight governance and security vulnerabilities in software ecosystems.

Conclusion: The paper concludes by introducing GitHistorian, an automated tool to detect and describe history alterations in public Git repositories, addressing governance and security concerns arising from such changes.

Abstract: Version Control Systems (VCS) like Git allow developers to locally rewrite
recorded history, e.g., to reorder and suppress commits or specific data in
them. These alterations have legitimate use cases, but become problematic when
performed on public branches that have downstream users: they break push/pull
workflows, challenge the integrity and reproducibility of repositories, and
create opportunities for supply chain attackers to sneak into them nefarious
changes. We conduct the first large-scale investigation of Git history
alterations in public code repositories. We analyze 111 M (millions)
repositories archived by Software Heritage, which preserves VCS histories even
across alterations. We find history alterations in 1.22 M repositories, for a
total of 8.7 M rewritten histories. We categorize changes by where they happen
(which repositories, which branches) and what is changed in them (files or
commit metadata). Conducting two targeted case studies we show that altered
histories recurrently change licenses retroactively, or are used to remove
''secrets'' (e.g., private keys) committed by mistake. As these behaviors
correspond to bad practices-in terms of project governance or security
management, respectively-that software recipients might want to avoid, we
introduce GitHistorian, an automated tool, that developers can use to spot and
describe history alterations in public Git repositories.

</details>


### [27] [Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data](https://arxiv.org/abs/2509.09313)
*Moritz Mock,Thomas Forrer,Barbara Russo*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Deep learning solutions for vulnerability detection proposed in academic
research are not always accessible to developers, and their applicability in
industrial settings is rarely addressed. Transferring such technologies from
academia to industry presents challenges related to trustworthiness, legacy
systems, limited digital literacy, and the gap between academic and industrial
expertise. For deep learning in particular, performance and integration into
existing workflows are additional concerns. In this work, we first evaluate the
performance of CodeBERT for detecting vulnerable functions in industrial and
open-source software. We analyse its cross-domain generalisation when
fine-tuned on open-source data and tested on industrial data, and vice versa,
also exploring strategies for handling class imbalance. Based on these results,
we develop AI-DO(Automating vulnerability detection Integration for Developers'
Operations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated
recommender system that uses fine-tuned CodeBERT to detect and localise
vulnerabilities during code review without disrupting workflows. Finally, we
assess the tool's perceived usefulness through a survey with the company's IT
professionals. Our results show that models trained on industrial data detect
vulnerabilities accurately within the same domain but lose performance on
open-source code, while a deep learner fine-tuned on open data, with
appropriate undersampling techniques, improves the detection of
vulnerabilities.

</details>


### [28] [ORCA: Unveiling Obscure Containers In The Wild](https://arxiv.org/abs/2509.09322)
*Jacopo Bufalino,Agathe Blaise,Stefano Secci*

Main category: cs.SE

TL;DR: This paper addresses the limitations of Software Composition Analysis (SCA) tools in detecting vulnerabilities in obscured container images due to incomplete filesystems. It evaluates current SCA tools, identifies coverage gaps, and introduces ORCA—a resilient methodology achieving a 40\% median improvement in file coverage over tools like Docker Scout and Syft.


<details>
  <summary>Details</summary>
Motivation: Modern applications rely on containerized environments with third-party components, introducing security risks when SCA tools fail to analyze incomplete or modified containers. Obscured containers in trusted registries compromise the reliability of vulnerability detection, necessitating improved SCA methodologies.

Method: The authors analyzed 600 popular containers to study SCA tool limitations in handling obscure images. They identified filesystem inconsistencies and proposed ORCA, an obscuration-resilient container analysis method. ORCA was evaluated against tools like Docker Scout and Syft to measure file coverage improvements.

Result: Existing SCA tools failed to analyze many containers with obscured filesystems. ORCA demonstrated a median 40\% improvement in file coverage, effectively detecting content in obscure containers where traditional tools fell short.

Conclusion: The paper highlights SCA tool limitations in real-world container environments and presents ORCA as a robust solution. By mitigating obscuration challenges, ORCA enhances the reliability of vulnerability detection, improving software supply chain security.

Abstract: Modern software development increasingly depends on open-source libraries and
third-party components, which are often encapsulated into containerized
environments. While improving the development and deployment of applications,
this approach introduces security risks, particularly when outdated or
vulnerable components are inadvertently included in production environments.
Software Composition Analysis (SCA) is a critical process that helps identify
and manage packages and dependencies inside a container. However, unintentional
modifications to the container filesystem can lead to incomplete container
images, which compromise the reliability of SCA tools. In this paper, we
examine the limitations of both cloud-based and open-source SCA tools when
faced with such obscure images. An analysis of 600 popular containers revealed
that obscure containers exist in well-known registries and trusted images and
that many tools fail to analyze such containers. To mitigate these issues, we
propose an obscuration-resilient methodology for container analysis and
introduce ORCA (Obscuration-Resilient Container Analyzer), its open-source
implementation. We reported our findings to all vendors using their appropriate
channels. Our results demonstrate that ORCA effectively detects the content of
obscure containers and achieves a median 40% improvement in file coverage
compared to Docker Scout and Syft.

</details>


### [29] [LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering](https://arxiv.org/abs/2509.09614)
*Jielin Qiu,Zuxin Liu,Zhiwei Liu,Rithesh Murthy,Jianguo Zhang,Haolin Chen,Shiyu Wang,Ming Zhu,Liangwei Yang,Juntao Tan,Zhepeng Cen,Cheng Qian,Shelby Heinecke,Weiran Yao,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.SE

TL;DR: LoCoBench benchmark evaluates long-context LLMs in realistic software tasks (10K-1M tokens, 10 languages) via 8 task categories and 17 metrics. Despite state-of-the-art models, performance gaps persist in complex codebase reasoning, highlighting a major unsolved challenge in AI-assisted software development.


<details>
  <summary>Details</summary>
Motivation: Existing code evaluation benchmarks focus on single-function completion and short-context tasks, leaving a critical gap in evaluating long-context capabilities required for understanding entire codebases, cross-file reasoning, and architectural consistency in large-scale systems.

Method: LoCoBench features 8,000 scenarios across 10 programming languages, 10K-1M token context lengths, 8 task categories (e.g., architectural understanding, cross-file refactoring), and a 5-phase pipeline for diverse scenario generation. It employs 17 metrics across 4 dimensions (including 8 novel metrics) to compute the LoCoBench Score (LCBS).

Result: Evaluation of state-of-the-art long-context models reveals substantial performance degradation in complex real-world software scenarios, confirming the significant unsolved challenge of long-context understanding in software development.

Conclusion: The paper introduces LoCoBench, a benchmark for evaluating long-context LLMs in complex software development scenarios. It highlights significant performance gaps in state-of-the-art models, demonstrating the challenge of long-context understanding in real-world software systems. LoCoBench is released to address this unmet need.

Abstract: The emergence of long-context language models with context windows extending
to millions of tokens has created new opportunities for sophisticated code
understanding and software development evaluation. We propose LoCoBench, a
comprehensive benchmark specifically designed to evaluate long-context LLMs in
realistic, complex software development scenarios. Unlike existing code
evaluation benchmarks that focus on single-function completion or short-context
tasks, LoCoBench addresses the critical evaluation gap for long-context
capabilities that require understanding entire codebases, reasoning across
multiple files, and maintaining architectural consistency across large-scale
software systems. Our benchmark provides 8,000 evaluation scenarios
systematically generated across 10 programming languages, with context lengths
spanning 10K to 1M tokens, a 100x variation that enables precise assessment of
long-context performance degradation in realistic software development
settings. LoCoBench introduces 8 task categories that capture essential
long-context capabilities: architectural understanding, cross-file refactoring,
multi-session development, bug investigation, feature implementation, code
comprehension, integration testing, and security analysis. Through a 5-phase
pipeline, we create diverse, high-quality scenarios that challenge LLMs to
reason about complex codebases at unprecedented scale. We introduce a
comprehensive evaluation framework with 17 metrics across 4 dimensions,
including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our
evaluation of state-of-the-art long-context models reveals substantial
performance gaps, demonstrating that long-context understanding in complex
software development represents a significant unsolved challenge that demands
more attention. LoCoBench is released at:
https://github.com/SalesforceAIResearch/LoCoBench.

</details>


### [30] [I Know Who Clones Your Code: Interpretable Smart Contract Similarity Detection](https://arxiv.org/abs/2509.09630)
*Zhenguang Liu,Lixun Ma,Zhongzheng Mu,Chengkun Wei,Xiaojun Xu,Yingying Jiao,Kui Ren*

Main category: cs.SE

TL;DR: SmartDetector is a novel framework for smart contract function similarity detection, achieving 95.88% F1-score through AST decomposition and hyperparameter optimization.


<details>
  <summary>Details</summary>
Motivation: Widespread code reuse in smart contracts propagates bugs, but existing methods (AST-based and deep-learning approaches) fail to handle complex tree structures, overlook syntax, or lack interpretability.

Method: SmartDetector decomposes contract function ASTs into statement-level trees, compares them with a classifier, and optimizes hyperparameters via a cosine-wise diffusion process for efficient search.

Result: Outperforms state-of-the-art methods by 14.01% in F1-score on three real-world datasets, achieving 95.88% overall performance.

Conclusion: The framework addresses critical gaps in smart contract similarity analysis by combining structural decomposition, interpretable comparison, and mathematically optimized hyperparameters.

Abstract: Widespread reuse of open-source code in smart contract development boosts
programming efficiency but significantly amplifies bug propagation across
contracts, while dedicated methods for detecting similar smart contract
functions remain very limited. Conventional abstract-syntax-tree (AST) based
methods for smart contract similarity detection face challenges in handling
intricate tree structures, which impedes detailed semantic comparison of code.
Recent deep-learning based approaches tend to overlook code syntax and
detection interpretability, resulting in suboptimal performance.
  To fill this research gap, we introduce SmartDetector, a novel approach for
computing similarity between smart contract functions, explainable at the
fine-grained statement level. Technically, SmartDetector decomposes the AST of
a smart contract function into a series of smaller statement trees, each
reflecting a structural element of the source code. Then, SmartDetector uses a
classifier to compute the similarity score of two functions by comparing each
pair of their statement trees. To address the infinite hyperparameter space of
the classifier, we mathematically derive a cosine-wise diffusion process to
efficiently search optimal hyperparameters. Extensive experiments conducted on
three large real-world datasets demonstrate that SmartDetector outperforms
current state-of-the-art methods by an average improvement of 14.01% in
F1-score, achieving an overall average F1-score of 95.88%.

</details>
