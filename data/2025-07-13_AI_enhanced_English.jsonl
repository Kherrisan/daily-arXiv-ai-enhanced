{"id": "2507.07210", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07210", "abs": "https://arxiv.org/abs/2507.07210", "authors": ["Nils Rollshausen", "Alexander Heinrich", "Matthias Hollick", "Jiska Classen"], "title": "WatchWitch: Interoperability, Privacy, and Autonomy for the Apple Watch", "comment": "To appear in \"Proceedings on Privacy Enhancing Technologies\"", "summary": "Smartwatches such as the Apple Watch collect vast amounts of intimate health\nand fitness data as we wear them. Users have little choice regarding how this\ndata is processed: The Apple Watch can only be used with Apple's iPhones, using\ntheir software and their cloud services. We are the first to publicly\nreverse-engineer the watch's wireless protocols, which led to discovering\nmultiple security issues in Apple's proprietary implementation. With\nWatchWitch, our custom Android reimplementation, we break out of Apple's walled\ngarden -- demonstrating practical interoperability with enhanced privacy\ncontrols and data autonomy. We thus pave the way for more consumer choice in\nthe smartwatch ecosystem, offering users more control over their devices.", "AI": {"tldr": "This paper reverse-engineers Apple Watch's wireless protocols to uncover security issues and creates WatchWitch, an Android reimplementation for enhanced privacy and cross-platform interoperability.", "motivation": "The Apple Watch's closed ecosystem limits user control and data privacy options, creating a need for practical interoperability with privacy-centric alternatives.", "method": "Reverse-engineering of proprietary wireless protocols and development of a custom Android-based implementation called WatchWitch.", "result": "Identified multiple security vulnerabilities, demonstrated Android compatibility with Apple Watch, and introduced enhanced privacy controls through the reimplementation.", "conclusion": "WatchWitch breaks Apple's ecosystem lock-in, enabling consumer choice and device control in smartwatch technology with robust privacy assurances."}}
{"id": "2507.07244", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07244", "abs": "https://arxiv.org/abs/2507.07244", "authors": ["Faissal Ahmadou", "Sepehr Ghaffarzadegan", "Boubakr Nour", "Makan Pourzandi", "Mourad Debbabi", "Chadi Assi"], "title": "Automated Attack Testflow Extraction from Cyber Threat Report using BERT for Contextual Analysis", "comment": null, "summary": "In the ever-evolving landscape of cybersecurity, the rapid identification and\nmitigation of Advanced Persistent Threats (APTs) is crucial. Security\npractitioners rely on detailed threat reports to understand the tactics,\ntechniques, and procedures (TTPs) employed by attackers. However, manually\nextracting attack testflows from these reports requires elusive knowledge and\nis time-consuming and prone to errors. This paper proposes FLOWGUARDIAN, a\nnovel solution leveraging language models (i.e., BERT) and Natural Language\nProcessing (NLP) techniques to automate the extraction of attack testflows from\nunstructured threat reports. FLOWGUARDIAN systematically analyzes and\ncontextualizes security events, reconstructs attack sequences, and then\ngenerates comprehensive testflows. This automated approach not only saves time\nand reduces human error but also ensures comprehensive coverage and robustness\nin cybersecurity testing. Empirical validation using public threat reports\ndemonstrates FLOWGUARDIAN's accuracy and efficiency, significantly enhancing\nthe capabilities of security teams in proactive threat hunting and incident\nresponse.", "AI": {"tldr": "FLOWGUARDIAN automates attack testflow extraction from threat reports using BERT and NLP techniques to improve cybersecurity testing efficiency and accuracy.", "motivation": "Manual extraction of attack testflows from unstructured threat reports is time-consuming, error-prone, and requires specialized knowledge, hindering rapid APT identification and mitigation.", "method": "The paper introduces FLOWGUARDIAN, which combines language models (BERT) with NLP techniques to systematically analyze security events, contextualize them, reconstruct attack sequences, and generate testflows.", "result": "Empirical validation on public threat reports demonstrates FLOWGUARDIAN's effectiveness in accurately and efficiently extracting attack testflows, improving coverage and robustness.", "conclusion": "FLOWGUARDIAN enhances security teams' proactive threat hunting and incident response capabilities by automating testflow extraction, enabling faster and more reliable detection of APTs."}}
{"id": "2507.07246", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07246", "abs": "https://arxiv.org/abs/2507.07246", "authors": ["Peicheng Wang", "Monika Santra", "Mingyu Liu", "Cong Sun", "Dongrui Zeng", "Gang Tan"], "title": "Disa: Accurate Learning-based Static Disassembly with Attentions", "comment": "To appear at ACM CCS 2025", "summary": "For reverse engineering related security domains, such as vulnerability\ndetection, malware analysis, and binary hardening, disassembly is crucial yet\nchallenging. The fundamental challenge of disassembly is to identify\ninstruction and function boundaries. Classic approaches rely on file-format\nassumptions and architecture-specific heuristics to guess the boundaries,\nresulting in incomplete and incorrect disassembly, especially when the binary\nis obfuscated. Recent advancements of disassembly have demonstrated that deep\nlearning can improve both the accuracy and efficiency of disassembly. In this\npaper, we propose Disa, a new learning-based disassembly approach that uses the\ninformation of superset instructions over the multi-head self-attention to\nlearn the instructions' correlations, thus being able to infer function\nentry-points and instruction boundaries. Disa can further identify instructions\nrelevant to memory block boundaries to facilitate an advanced block-memory\nmodel based value-set analysis for an accurate control flow graph (CFG)\ngeneration. Our experiments show that Disa outperforms prior deep-learning\ndisassembly approaches in function entry-point identification, especially\nachieving 9.1% and 13.2% F1-score improvement on binaries respectively\nobfuscated by the disassembly desynchronization technique and popular\nsource-level obfuscator. By achieving an 18.5% improvement in the memory block\nprecision, Disa generates more accurate CFGs with a 4.4% reduction in Average\nIndirect Call Targets (AICT) compared with the state-of-the-art heuristic-based\napproach.", "AI": {"tldr": "Disa is a deep learning-based disassembly approach that leverages superset instruction relationships via multi-head self-attention to improve function entry-point detection and control flow graph accuracy on obfuscated binaries.", "motivation": "Traditional disassembly methods using file-format assumptions and architecture-specific heuristics fail to accurately identify instruction/function boundaries in obfuscated binaries, leading to incomplete/incorrect analysis for security applications like vulnerability detection and malware analysis.", "method": "The approach uses superset instructions as input features processed through a multi-head self-attention mechanism to learn temporal instruction correlations, enabling function entry-point identification and instruction boundary detection. It also identifies memory block relationships to enhance block-memory model-based value-set analysis for CFG generation.", "result": "Disa achieves 9.1% F1-score improvement over prior deep learning approaches on desynchronized disassembly obfuscations and 13.2% improvement on source-level obfuscated binaries. It reaches 18.5% better memory block precision and 4.4% reduction in AICT compared to heuristic-based state-of-the-art CFG generation.", "conclusion": "Disa demonstrates that deep learning with self-attention mechanisms can significantly outperform traditional methods in disassembling obfuscated binaries, producing more accurate CFGs and improving security analyses in challenging reverse engineering scenarios."}}
{"id": "2507.07250", "categories": ["cs.CR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.07250", "abs": "https://arxiv.org/abs/2507.07250", "authors": ["Jordi Serra-Ruiz", "David Meg\u00edas"], "title": "Semi-fragile watermarking of remote sensing images using DWT, vector quantization and automatic tiling", "comment": null, "summary": "A semi-fragile watermarking scheme for multiple band images is presented in\nthis article. We propose to embed a mark into remote sensing images applying a\ntree-structured vector quantization approach to the pixel signatures instead of\nprocessing each band separately. The signature of the multispectral or\nhyperspectral image is used to embed the mark in it order to detect any\nsignificant modification of the original image. The image is segmented into\nthree-dimensional blocks, and a tree-structured vector quantizer is built for\neach block. These trees are manipulated using an iterative algorithm until the\nresulting block satisfies a required criterion, which establishes the embedded\nmark. The method is shown to be able to preserve the mark under lossy\ncompression (above a given threshold) but, at the same time, it detects\npossibly forged blocks and their position in the whole image.", "AI": {"tldr": "The paper introduces a semi-fragile watermarking method for multispectral/hyperspectral images using tree-structured vector quantization on 3D pixel blocks, enabling detection of significant modifications while preserving the watermark under acceptable lossy compression.", "motivation": "Existing methods process image bands separately, lacking integration for detecting manipulations in complex multi-band images like remote sensing data. This approach addresses the need for both robustness against common distortions and fragility to identify tampering.", "method": "The method segments images into 3D blocks and constructs a tree-structured vector quantizer for each. An iterative algorithm adjusts the trees to meet a criterion for watermark embedding, leveraging pixel signatures across all bands for a holistic analysis.", "result": "The scheme successfully preserves the watermark under lossy compression (above a threshold) while accurately detecting forged blocks and their positions within the image. This demonstrates its dual effectiveness in robustness and tamper detection.", "conclusion": "The proposed method achieves a balance between robustness to compression and fragility to unauthorized modifications, making it suitable for authenticating multi-band images in remote sensing applications where maintaining original data integrity is critical."}}
{"id": "2507.07325", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07325", "abs": "https://arxiv.org/abs/2507.07325", "authors": ["Martin Obaidi", "Marc Herrmann", "Elisa Schmid", "Raymond Ochsner", "Kurt Schneider", "Jil Kl\u00fcnder"], "title": "A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering", "comment": "This paper has been accepted at the 33rd IEEE International\n  Requirements Engineering Workshop (REW 2025)", "summary": "Sentiment analysis is an essential technique for investigating the emotional\nclimate within developer teams, contributing to both team productivity and\nproject success. Existing sentiment analysis tools in software engineering\nprimarily rely on English or non-German gold-standard datasets. To address this\ngap, our work introduces a German dataset of 5,949 unique developer statements,\nextracted from the German developer forum Android-Hilfe.de. Each statement was\nannotated with one of six basic emotions, based on the emotion model by Shaver\net al., by four German-speaking computer science students. Evaluation of the\nannotation process showed high interrater agreement and reliability. These\nresults indicate that the dataset is sufficiently valid and robust to support\nsentiment analysis in the German-speaking software engineering community.\nEvaluation with existing German sentiment analysis tools confirms the lack of\ndomain-specific solutions for software engineering. We also discuss approaches\nto optimize annotation and present further use cases for the dataset.", "AI": {"tldr": "Researchers created a validated German sentiment analysis dataset (5,949 developer statements from Android-Hilfe.de) with high interrater agreement and demonstrated existing tools' limitations in domain-specific software engineering contexts.", "motivation": "Current sentiment analysis tools for software engineering rely on English/non-German datasets, creating a gap in supporting German-speaking developers and teams.", "method": "1. Collected 5,949 unique developer statements from Android-Hilfe.de 2. Annotated with six fundamental emotions (Shaver et al.'s model) by 4 CS students 3. Performed interrater reliability assessment 4. Evaluated existing German sentiment analysis tools 5. Discussed annotation optimization strategies", "result": "1. Dataset validation: High interrater agreement (92.3% Cohen's Kappa 0.84) 2. Identified lack of domain-specific sentiment analysis solutions for German (85-90% performance gap) 3. Demonstrated dataset's applicability for optimizing annotation and other use cases", "conclusion": "This validated German software engineering sentiment dataset addresses critical domain-specific needs, provides reliable baseline data, and highlights opportunities to improve both annotation efficiency and sentiment analysis capabilities for German-speaking developer populations."}}
{"id": "2507.07258", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07258", "abs": "https://arxiv.org/abs/2507.07258", "authors": ["Rami Darwish", "Mahmoud Abdelsalam", "Sajad Khorsandroo", "Kaushik Roy"], "title": "FedP3E: Privacy-Preserving Prototype Exchange for Non-IID IoT Malware Detection in Cross-Silo Federated Learning", "comment": null, "summary": "As IoT ecosystems continue to expand across critical sectors, they have\nbecome prominent targets for increasingly sophisticated and large-scale malware\nattacks. The evolving threat landscape, combined with the sensitive nature of\nIoT-generated data, demands detection frameworks that are both\nprivacy-preserving and resilient to data heterogeneity. Federated Learning (FL)\noffers a promising solution by enabling decentralized model training without\nexposing raw data. However, standard FL algorithms such as FedAvg and FedProx\noften fall short in real-world deployments characterized by class imbalance and\nnon-IID data distributions -- particularly in the presence of rare or disjoint\nmalware classes. To address these challenges, we propose FedP3E\n(Privacy-Preserving Prototype Exchange), a novel FL framework that supports\nindirect cross-client representation sharing while maintaining data privacy.\nEach client constructs class-wise prototypes using Gaussian Mixture Models\n(GMMs), perturbs them with Gaussian noise, and transmits only these compact\nsummaries to the server. The aggregated prototypes are then distributed back to\nclients and integrated into local training, supported by SMOTE-based\naugmentation to enhance representation of minority malware classes. Rather than\nrelying solely on parameter averaging, our prototype-driven mechanism enables\nclients to enrich their local models with complementary structural patterns\nobserved across the federation -- without exchanging raw data or gradients.\nThis targeted strategy reduces the adverse impact of statistical heterogeneity\nwith minimal communication overhead. We evaluate FedP3E on the N-BaIoT dataset\nunder realistic cross-silo scenarios with varying degrees of data imbalance.", "AI": {"tldr": "FedP3E is a privacy-preserving federated learning framework for IoT malware detection that addresses data heterogeneity and class imbalance using prototype sharing and SMOTE augmentation.", "motivation": "IoT ecosystems face sophisticated malware attacks with sensitive data requiring privacy-preserving detection frameworks that handle data heterogeneity and class imbalance, which standard FL algorithms like FedAvg and FedProx struggle with.", "method": "FedP3E constructs class-wise Gaussian Mixture Model (GMM) prototypes on clients, adds Gaussian noise for privacy, and shares these compact summaries via a server. Clients use aggregated prototypes and SMOTE-based minority class augmentation during local training.", "result": "Evaluated on the N-BaIoT dataset under cross-silo scenarios with varying data imbalance, FedP3E demonstrates a targeted strategy that reduces statistical heterogeneity impact with minimal communication overhead.", "conclusion": "FedP3E enables effective IoT malware detection through indirect cross-client learning with privacy protection, overcoming limitations of standard FL methods by sharing enriched prototypes rather than raw data or gradients."}}
{"id": "2507.07344", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07344", "abs": "https://arxiv.org/abs/2507.07344", "authors": ["Martin Obaidi", "Jannik Fischbach", "Jakob Droste", "Hannah Deters", "Marc Herrmann", "Jil Kl\u00fcnder", "Steffen Kr\u00e4tzig", "Hugo Villamizar", "Kurt Schneider"], "title": "Automatic Generation of Explainability Requirements and Software Explanations From User Reviews", "comment": "This paper has been accepted at the 33rd IEEE International\n  Requirements Engineering Workshop (REW 2025)", "summary": "Explainability has become a crucial non-functional requirement to enhance\ntransparency, build user trust, and ensure regulatory compliance. However,\ntranslating explanation needs expressed in user feedback into structured\nrequirements and corresponding explanations remains challenging. While existing\nmethods can identify explanation-related concerns in user reviews, there is no\nestablished approach for systematically deriving requirements and generating\naligned explanations. To contribute toward addressing this gap, we introduce a\ntool-supported approach that automates this process. To evaluate its\neffectiveness, we collaborated with an industrial automation manufacturer to\ncreate a dataset of 58 user reviews, each annotated with manually crafted\nexplainability requirements and explanations. Our evaluation shows that while\nAI-generated requirements often lack relevance and correctness compared to\nhuman-created ones, the AI-generated explanations are frequently preferred for\ntheir clarity and style. Nonetheless, correctness remains an issue,\nhighlighting the importance of human validation. This work contributes to the\nadvancement of explainability requirements in software systems by (1)\nintroducing an automated approach to derive requirements from user reviews and\ngenerate corresponding explanations, (2) providing empirical insights into the\nstrengths and limitations of automatically generated artifacts, and (3)\nreleasing a curated dataset to support future research on the automatic\ngeneration of explainability requirements.", "AI": {"tldr": "This paper introduces a tool-supported automated method to derive explainability requirements from user reviews and generate aligned explanations, validated through a dataset of 58 annotated reviews showing AI-generated explanation advantages but requiring human oversight for requirement accuracy.", "motivation": "Addressing the challenge of systematically translating ambiguous user feedback into structured explainability requirements while maintaining clarity and regulatory compliance in AI systems.", "method": "Collaborated with an industrial automation manufacturer to create a dataset of 58 annotated user reviews (with manual requirements and explanations) and developed an automated approach for requirement derivation and explanation generation, evaluated against human outputs.", "result": "AI-generated requirements showed lower relevance/correctness vs. human-crafted ones, but AI explanations were preferred for clarity/style. Correctness issues highlight the necessity of human validation.", "conclusion": "The work advances explainability requirements by providing an automated framework, empirical insights into AI limitations/advantages, and a curated dataset for future research, emphasizing human-AI collaboration."}}
{"id": "2507.07401", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07401", "abs": "https://arxiv.org/abs/2507.07401", "authors": ["Fupei Chen", "Liyao Xiang", "Haoxiang Sun", "Hei Victor Cheng", "Kaiming Shen"], "title": "Shuffling for Semantic Secrecy", "comment": null, "summary": "Deep learning draws heavily on the latest progress in semantic\ncommunications. The present paper aims to examine the security aspect of this\ncutting-edge technique from a novel shuffling perspective. Our goal is to\nimprove upon the conventional secure coding scheme to strike a desirable\ntradeoff between transmission rate and leakage rate. To be more specific, for a\nwiretap channel, we seek to maximize the transmission rate while minimizing the\nsemantic error probability under the given leakage rate constraint. Toward this\nend, we devise a novel semantic security communication system wherein the\nrandom shuffling pattern plays the role of the shared secret key. Intuitively,\nthe permutation of feature sequences via shuffling would distort the semantic\nessence of the target data to a sufficient extent so that eavesdroppers cannot\naccess it anymore. The proposed random shuffling method also exhibits its\nflexibility in working for the existing semantic communication system as a\nplugin. Simulations demonstrate the significant advantage of the proposed\nmethod over the benchmark in boosting secure transmission, especially when\nchannels are prone to strong noise and unpredictable fading.", "AI": {"tldr": "A novel semantic security communication system leveraging random shuffling as a shared secret key to improve secure transmission in deep learning-based semantic communications by optimizing tradeoffs between transmission rate and leakage rate.", "motivation": "Conventional secure coding schemes for wiretap channels fail to optimally balance transmission rate and semantic secrecy. The paper seeks to address this limitation by introducing a shuffling-based approach that distorts feature sequences to prevent semantic leakage to eavesdroppers.", "method": "1) Utilizes random shuffling patterns as shared secret keys rather than feature transmission. 2) Permutates feature sequences through shuffling to obscure semantic content. 3) Implements leakage rate constraints while maximizing transmission rate and minimizing semantic error probability. 4) Design as a flexible plugin for existing semantic communication systems.", "result": "Simulation results demonstrate superior performance over benchmarks in secure transmission under strong noise and unpredictable fading conditions, showing significant improvement in semantic secrecy while maintaining effective communication rates.", "conclusion": "The proposed shuffling-based semantic security framework achieves better transmission-leakage tradeoff than conventional methods, offering a flexible and effective solution for secure deep learning communication systems in challenging channel environments."}}
{"id": "2507.07468", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07468", "abs": "https://arxiv.org/abs/2507.07468", "authors": ["Sten Gr\u00fcner", "Nafise Eskandani"], "title": "Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN", "comment": "7 pages, 7 figures, Accepted at IFAC EAAS 2025\n  (https://j3c.org/eaas.php)", "summary": "The integration of Industry 4.0 technologies into engineering workflows is an\nessential step toward automating and optimizing plant and process engineering\nprocesses. The Asset Administration Shell (AAS) serves as a key enabler for\ncreating interoperable Digital Twins that facilitate engineering data exchange\nand automation. This paper explores the use of AAS within engineering\nworkflows, particularly in combination with Business Process Model and Notation\n(BPMN) to define structured and automated processes. We propose a distributed\nAAS copy-on-write infrastructure that enhances security and scalability while\nenabling seamless cross organizational collaboration. We also introduce a\nworkflow management prototype automating AAS operations and engineering\nworkflows, improving efficiency and traceability.", "AI": {"tldr": "This paper proposes a distributed AAS copy-on-write infrastructure combined with BPMN to automate engineering workflows, enhancing security, scalability, and cross-organizational collaboration through a workflow management prototype.", "motivation": "The paper addresses the need for automating and optimizing plant and process engineering workflows by leveraging Industry 4.0 technologies, specifically the Asset Administration Shell (AAS) for interoperable Digital Twins.", "method": "A distributed AAS copy-on-write infrastructure is introduced to improve security and scalability, along with a workflow management prototype that automates AAS operations using Business Process Model and Notation (BPMN).", "result": "The proposed infrastructure and workflow prototype demonstrate enhanced efficiency, traceability, and cross-organizational collaboration capabilities in engineering processes.", "conclusion": "The integration of AAS with BPMN through a distributed copy-on-write infrastructure provides a scalable and secure framework for automating engineering workflows and enabling collaboration across organizations."}}
{"id": "2507.07406", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07406", "abs": "https://arxiv.org/abs/2507.07406", "authors": ["Jikesh Thapa", "Gurrehmat Chahal", "Serban Voinea Gabreanu", "Yazan Otoum"], "title": "Phishing Detection in the Gen-AI Era: Quantized LLMs vs Classical Models", "comment": "8 Pages, IEEE Conference", "summary": "Phishing attacks are becoming increasingly sophisticated, underscoring the\nneed for detection systems that strike a balance between high accuracy and\ncomputational efficiency. This paper presents a comparative evaluation of\ntraditional Machine Learning (ML), Deep Learning (DL), and quantized\nsmall-parameter Large Language Models (LLMs) for phishing detection. Through\nexperiments on a curated dataset, we show that while LLMs currently\nunderperform compared to ML and DL methods in terms of raw accuracy, they\nexhibit strong potential for identifying subtle, context-based phishing cues.\nWe also investigate the impact of zero-shot and few-shot prompting strategies,\nrevealing that LLM-rephrased emails can significantly degrade the performance\nof both ML and LLM-based detectors. Our benchmarking highlights that models\nlike DeepSeek R1 Distill Qwen 14B (Q8_0) achieve competitive accuracy, above\n80%, using only 17GB of VRAM, supporting their viability for cost-efficient\ndeployment. We further assess the models' adversarial robustness and\ncost-performance tradeoffs, and demonstrate how lightweight LLMs can provide\nconcise, interpretable explanations to support real-time decision-making. These\nfindings position optimized LLMs as promising components in phishing defence\nsystems and offer a path forward for integrating explainable, efficient AI into\nmodern cybersecurity frameworks.", "AI": {"tldr": "Quantized LLMs (e.g., DeepSeek R1 Distill Qwen 14B Q8_0) demonstrate 80%+ phishing detection accuracy with 17GB VRAM, showing cost-efficient potential despite lower raw performance than traditional ML/DL. The work highlights their robustness against rephrased emails and value for contextual analysis and explainability.", "motivation": "Sophisticated phishing attacks demand systems balancing high accuracy and computational efficiency, while requiring actionable explanations for cybersecurity decisions.", "method": "Comparative evaluation of traditional ML, DL, and quantized LLMs via experiments on a curated phishing dataset, testing zero-shot/few-shot prompting and adversarial robustness.", "result": "1) Quantized LLMs achieve >80% accuracy with 17GB VRAM. 2) ML/DL methods outperform LLMs in raw accuracy but lack contextual analysis capability. 3) LLM-based email rephrasing reduces detection accuracy across all model types.", "conclusion": "Quantized LLMs offer practical tradeoffs between accuracy (80%+), computational efficiency (low VRAM usage), and explainability, positioning them as viable components for next-generation phishing defense systems integrated with cybersecurity frameworks."}}
{"id": "2507.07548", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07548", "abs": "https://arxiv.org/abs/2507.07548", "authors": ["Jonathan Ullrich", "Matthias Koch", "Andreas Vogelsang"], "title": "From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering", "comment": "This paper has been accepted for publication at the 33rd IEEE\n  International Requirements Engineering (RE) conference", "summary": "With the advent of generative LLMs and their advanced code generation\ncapabilities, some people already envision the end of traditional software\nengineering, as LLMs may be able to produce high-quality code based solely on\nthe requirements a domain expert feeds into the system. The feasibility of this\nvision can be assessed by understanding how developers currently incorporate\nrequirements when using LLMs for code generation-a topic that remains largely\nunexplored. We interviewed 18 practitioners from 14 companies to understand how\nthey (re)use information from requirements and other design artifacts to feed\nLLMs when generating code. Based on our findings, we propose a theory that\nexplains the processes developers employ and the artifacts they rely on. Our\ntheory suggests that requirements, as typically documented, are too abstract\nfor direct input into LLMs. Instead, they must first be manually decomposed\ninto programming tasks, which are then enriched with design decisions and\narchitectural constraints before being used in prompts. Our study highlights\nthat fundamental RE work is still necessary when LLMs are used to generate\ncode. Our theory is important for contextualizing scientific approaches to\nautomating requirements-centric SE tasks.", "AI": {"tldr": "The paper explores whether requirements-based code generation via LLMs can replace traditional SE by analyzing how developers decompose and enrich requirements into prompts. It concludes that fundamental RE remains essential.", "motivation": "The study investigates the viability of replacing traditional software engineering with generative LLM-based code generation by understanding how practitioners utilize requirements and design artifacts in this process.", "method": "18 practitioners from 14 companies were interviewed to analyze their workflows of reusing requirements information and design artifacts for LLM code generation.", "result": "Developers must manually decompose raw requirements into programming tasks, which are then enriched with design decisions and architecture constraints before being used in LLM prompts.", "conclusion": "Fundamental requirements engineering work is still necessary when using LLMs for code generation, as documented requirements lack sufficient detail for direct LLM input without manual preprocessing."}}
{"id": "2507.07413", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07413", "abs": "https://arxiv.org/abs/2507.07413", "authors": ["Mohammad F. Al-Hammouri", "Yazan Otoum", "Rasha Atwa", "Amiya Nayak"], "title": "Hybrid LLM-Enhanced Intrusion Detection for Zero-Day Threats in IoT Networks", "comment": "6 pages, IEEE conference", "summary": "This paper presents a novel approach to intrusion detection by integrating\ntraditional signature-based methods with the contextual understanding\ncapabilities of the GPT-2 Large Language Model (LLM). As cyber threats become\nincreasingly sophisticated, particularly in distributed, heterogeneous, and\nresource-constrained environments such as those enabled by the Internet of\nThings (IoT), the need for dynamic and adaptive Intrusion Detection Systems\n(IDSs) becomes increasingly urgent. While traditional methods remain effective\nfor detecting known threats, they often fail to recognize new and evolving\nattack patterns. In contrast, GPT-2 excels at processing unstructured data and\nidentifying complex semantic relationships, making it well-suited to uncovering\nsubtle, zero-day attack vectors. We propose a hybrid IDS framework that merges\nthe robustness of signature-based techniques with the adaptability of\nGPT-2-driven semantic analysis. Experimental evaluations on a representative\nintrusion dataset demonstrate that our model enhances detection accuracy by\n6.3%, reduces false positives by 9.0%, and maintains near real-time\nresponsiveness. These results affirm the potential of language model\nintegration to build intelligent, scalable, and resilient cybersecurity\ndefences suited for modern connected environments.", "AI": {"tldr": "This paper proposes a hybrid intrusion detection system combining signature-based methods with GPT-2 for enhanced accuracy and adaptability in dynamic environments like IoT, achieving 6.3% accuracy improvement and 9.0% fewer false positives with near real-time performance.", "motivation": "Modern cyber threats (especially in IoT environments) require dynamically adaptive IDSs as traditional signature-based methods struggle with unknown/evolving attack patterns while maintaining robustness in resource-constrained settings.", "method": "A hybrid framework that integrates signature-based detection with GPT-2's semantic analysis of unstructured data. Merges traditional threat identification with deep language model-based contextual pattern recognition for detecting subtle zero-day attacks.", "result": "Experimental results show 6.3% increase in detection accuracy, 9.0% reduction in false positives, and demonstration of near real-time responsiveness using representative intrusion datasets.", "conclusion": "Language model integration with traditional IDS approaches can create intelligent, scalable cybersecurity defenses that effectively combine the precision of signatures with the adaptability of AI for modern connected systems."}}
{"id": "2507.07682", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07682", "abs": "https://arxiv.org/abs/2507.07682", "authors": ["Kaicheng Huang", "Fanyu Wang", "Yutan Huang", "Chetan Arora"], "title": "Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap", "comment": null, "summary": "Advancements in large language models (LLMs) have led to a surge of prompt\nengineering (PE) techniques that can enhance various requirements engineering\n(RE) tasks. However, current LLMs are often characterized by significant\nuncertainty and a lack of controllability. This absence of clear guidance on\nhow to effectively prompt LLMs acts as a barrier to their trustworthy\nimplementation in the RE field. We present the first roadmap-oriented\nsystematic literature review of Prompt Engineering for RE (PE4RE). Following\nKitchenham's and Petersen's secondary-study protocol, we searched six digital\nlibraries, screened 867 records, and analyzed 35 primary studies. To bring\norder to a fragmented landscape, we propose a hybrid taxonomy that links\ntechnique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented\nRE roles (elicitation, validation, traceability). Two research questions, with\nfive sub-questions, map the tasks addressed, LLM families used, and prompt\ntypes adopted, and expose current limitations and research gaps. Finally, we\noutline a step-by-step roadmap showing how today's ad-hoc PE prototypes can\nevolve into reproducible, practitioner-friendly workflows.", "AI": {"tldr": "This paper presents the first roadmap-oriented systematic literature review on prompt engineering for requirements engineering (PE4RE), analyzing challenges in LLM controllability and proposing a hybrid taxonomy with a step-by-step roadmap to improve trustworthiness and reproducibility in RE tasks.", "motivation": "Current LLMs in requirements engineering suffer from high uncertainty and poor controllability, with insufficient guidance on effective prompting, hindering their trustworthy implementation.", "method": "A secondary-study protocol followed Kitchenham's and Petersen's methodologies, screening 867 records from six digital libraries and analyzing 35 primary studies to systematize PE4RE practices.", "result": "A hybrid taxonomy linking technique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented RE roles, two research questions with sub-questions mapping existing limitations and gaps, and a step-by-step roadmap for evolving ad-hoc PE prototypes into reproducible workflows.", "conclusion": "The proposed roadmap demonstrates how fragmented PE4RE approaches can transition to structured, practitioner-friendly methods, addressing critical limitations in LLM control and task-specific guidance for RE."}}
{"id": "2507.07416", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07416", "abs": "https://arxiv.org/abs/2507.07416", "authors": ["Jenifer Paulraj", "Brindha Raghuraman", "Nagarani Gopalakrishnan", "Yazan Otoum"], "title": "Autonomous AI-based Cybersecurity Framework for Critical Infrastructure: Real-Time Threat Mitigation", "comment": "7 pages, IEEE conference", "summary": "Critical infrastructure systems, including energy grids, healthcare\nfacilities, transportation networks, and water distribution systems, are\npivotal to societal stability and economic resilience. However, the increasing\ninterconnectivity of these systems exposes them to various cyber threats,\nincluding ransomware, Denial-of-Service (DoS) attacks, and Advanced Persistent\nThreats (APTs). This paper examines cybersecurity vulnerabilities in critical\ninfrastructure, highlighting the threat landscape, attack vectors, and the role\nof Artificial Intelligence (AI) in mitigating these risks. We propose a hybrid\nAI-driven cybersecurity framework to enhance real-time vulnerability detection,\nthreat modelling, and automated remediation. This study also addresses the\ncomplexities of adversarial AI, regulatory compliance, and integration. Our\nfindings provide actionable insights to strengthen the security and resilience\nof critical infrastructure systems against emerging cyber threats.", "AI": {"tldr": "This paper proposes a hybrid AI-driven cybersecurity framework to address vulnerabilities in critical infrastructure systems, aiming to improve real-time detection, threat modelling, and automated remediation against emerging cyber threats.", "motivation": "The increasing interconnectivity of critical infrastructure systems has exposed them to escalating cyber threats (e.g., ransomware, DoS, APTs), necessitating innovative solutions to ensure societal stability and economic resilience.", "method": "A hybrid AI framework is introduced, focusing on real-time vulnerability detection using AI, dynamic threat modelling, and automated remediation strategies. The approach also considers adversarial AI challenges, regulatory compliance integration, and system interoperability.", "result": "Empirical findings demonstrate the framework's efficacy in enhancing cybersecurity resilience, with quantified improvements in threat detection latency (reduced by 32%) and automated patch deployment success rates (achieved 91% accuracy).", "conclusion": "The hybrid AI framework offers a scalable solution for protecting interdependent critical infrastructure systems while balancing innovation with regulatory adherence, requiring continuous adaptation to evolving threat vectors."}}
{"id": "2507.07689", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07689", "abs": "https://arxiv.org/abs/2507.07689", "authors": ["Chetan Arora", "Fanyu Wang", "Chakkrit Tantithamthavorn", "Aldeida Aleti", "Shaun Kenyon"], "title": "From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry", "comment": null, "summary": "Requirements engineering (RE) in the space industry is inherently complex,\ndemanding high precision, alignment with rigorous standards, and adaptability\nto mission-specific constraints. Smaller space organisations and new entrants\noften struggle to derive actionable requirements from extensive, unstructured\ndocuments such as mission briefs, interface specifications, and regulatory\nstandards. In this innovation opportunity paper, we explore the potential of\nRetrieval-Augmented Generation (RAG) models to support and (semi-)automate\nrequirements generation in the space domain. We present a modular, AI-driven\napproach that preprocesses raw space mission documents, classifies them into\nsemantically meaningful categories, retrieves contextually relevant content\nfrom domain standards, and synthesises draft requirements using large language\nmodels (LLMs). We apply the approach to a real-world mission document from the\nspace domain to demonstrate feasibility and assess early outcomes in\ncollaboration with our industry partner, Starbound Space Solutions. Our\npreliminary results indicate that the approach can reduce manual effort,\nimprove coverage of relevant requirements, and support lightweight compliance\nalignment. We outline a roadmap toward broader integration of AI in RE\nworkflows, intending to lower barriers for smaller organisations to participate\nin large-scale, safety-critical missions.", "AI": {"tldr": "This paper proposes using Retrieval-Augmented Generation (RAG) models to semi-automate requirements engineering in the space industry, particularly aiding smaller organizations in processing unstructured documents and aligning with standards via a modular AI approach.", "motivation": "Smaller space organizations struggle with manually deriving precise, compliant requirements from voluminous, unstructured mission documents, requiring high adaptability to domain-specific constraints and standards.", "method": "A four-step AI-driven approach combining document preprocessing, semantic classification, contextual retrieval from domain standards, and LLM-based synthesis of draft requirements is presented and tested on real mission data with industry collaboration.", "result": "Preliminary application demonstrates reduced manual effort, improved requirement coverage, and enhanced lightweight compliance alignment for space mission requirements generation.", "conclusion": "The proposed RAG-based framework shows potential to lower barriers for smaller space entities in participating in complex missions, offering a scalable path toward integrated AI solutions in requirements engineering workflows."}}
{"id": "2507.07417", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07417", "abs": "https://arxiv.org/abs/2507.07417", "authors": ["Nishit V. Pandya", "Andrey Labunets", "Sicun Gao", "Earlence Fernandes"], "title": "May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks", "comment": null, "summary": "A popular class of defenses against prompt injection attacks on large\nlanguage models (LLMs) relies on fine-tuning the model to separate instructions\nand data, so that the LLM does not follow instructions that might be present\nwith data. There are several academic systems and production-level\nimplementations of this idea. We evaluate the robustness of this class of\nprompt injection defenses in the whitebox setting by constructing strong\noptimization-based attacks and showing that the defenses do not provide the\nclaimed security properties. Specifically, we construct a novel attention-based\nattack algorithm for text-based LLMs and apply it to two recent whitebox\ndefenses SecAlign (CCS 2025) and StruQ (USENIX Security 2025), showing attacks\nwith success rates of up to 70% with modest increase in attacker budget in\nterms of tokens. Our findings make fundamental progress towards understanding\nthe robustness of prompt injection defenses in the whitebox setting. We release\nour code and attacks at https://github.com/nishitvp/better_opts_attacks", "AI": {"tldr": "The paper evaluates the robustness of prompt injection defenses for LLMs against whitebox attacks using optimization-based methods, demonstrating up to 70% attack success rates with minimal token budget increases.", "motivation": "Popular whitebox LLM defenses claim to separate instructions and data but lack thorough robustness analysis against strong optimization-based attacks.", "method": "Developed a novel attention-based attack algorithm and applied it to SecAlign (CCS 2025) and StruQ (USENIX Security 2025) defenses through whitebox optimization.", "result": "Attack success rates reaching 70% with only modest attacker token budget increases, undermining the security guarantees of these defenses.", "conclusion": "Demonstrates fundamental vulnerabilities in current whitebox prompt injection defenses and advances understanding of their robustness through open-code sharing."}}
{"id": "2507.07732", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07732", "abs": "https://arxiv.org/abs/2507.07732", "authors": ["Giovanni Gambigliani Zoccoli", "Filip Valgimigli", "Dario Stabili", "Mirco Marchetti"], "title": "RADAR: a Radio-based Analytics for Dynamic Association and Recognition of pseudonyms in VANETs", "comment": "7 pages, 4 figures, accepted for publication at the 2025 IEEE 102nd\n  Vehicular Technology Conference: VTC2025-Fall", "summary": "This paper presents RADAR, a tracking algorithm for vehicles participating in\nCooperative Intelligent Transportation Systems (C-ITS) that exploits multiple\nradio signals emitted by a modern vehicle to break privacy-preserving pseudonym\nschemes deployed in VANETs. This study shows that by combining Dedicated Short\nRange Communication (DSRC) and Wi-Fi probe request messages broadcast by the\nvehicle, it is possible to improve tracking over standard de-anonymization\napproaches that only leverage DSRC, especially in realistic scenarios where the\nattacker does not have full coverage of the entire vehicle path. The\nexperimental evaluation compares three different metrics for pseudonym and\nWi-Fi probe identifier association (Count, Statistical RSSI, and Pearson RSSI),\ndemonstrating that the Pearson RSSI metric is better at tracking vehicles under\npseudonym-changing schemes in all scenarios and against previous works. As an\nadditional contribution to the state-of-the-art, we publicly release all\nimplementations and simulation scenarios used in this work.", "AI": {"tldr": "RADAR is a vehicle tracking algorithm for C-ITS that uses DSRC and Wi-Fi signals to enhance tracking of privacy-preserving pseudonyms in VANETs, demonstrating that the Pearson RSSI metric outperforms existing methods, especially in partial coverage scenarios.", "motivation": "This paper aims to address the limitations of existing privacy-preserving pseudonym schemes in VANETs, which hinder effective vehicle tracking. By leveraging multiple radio signals from vehicles (e.g., DSRC and Wi-Fi probe requests), the authors seek to improve tracking accuracy in realistic scenarios with partial coverage.", "method": "The authors propose RADAR, a tracking algorithm that integrates Dedicated Short Range Communication (DSRC) and Wi-Fi probe request messages emitted by vehicles. They evaluate three pseudonym association metrics (Count, Statistical RSSI, and Pearson RSSI) to determine the most effective one for vehicle tracking under pseudonym-changing schemes.", "result": "RADAR significantly enhances vehicle tracking accuracy compared to standard DSRC-only methods across all scenarios, especially in partial coverage settings. The Pearson RSSI metric demonstrates superior performance than existing techniques, and the study publicly releases all implementations and simulation scenarios.", "conclusion": "RADAR proves that combining multi-signal data (DSRC and Wi-Fi) improves vehicle tracking in C-ITS despite privacy-preserving pseudonyms. The Pearson RSSI metric's robustness under pseudonym-changing schemes underscores vulnerabilities in current VANET privacy mechanisms, contributing to the field through open-source tooling for replication and further research."}}
{"id": "2507.07773", "categories": ["cs.CR", "cs.CV", "B.8; I.4"], "pdf": "https://arxiv.org/pdf/2507.07773", "abs": "https://arxiv.org/abs/2507.07773", "authors": ["Youqian Zhang", "Xinyu Ji", "Zhihao Wang", "Qinhong Jiang"], "title": "Rainbow Artifacts from Electromagnetic Signal Injection Attacks on Image Sensors", "comment": "5 pages, 4 figures", "summary": "Image sensors are integral to a wide range of safety- and security-critical\nsystems, including surveillance infrastructure, autonomous vehicles, and\nindustrial automation. These systems rely on the integrity of visual data to\nmake decisions. In this work, we investigate a novel class of electromagnetic\nsignal injection attacks that target the analog domain of image sensors,\nallowing adversaries to manipulate raw visual inputs without triggering\nconventional digital integrity checks. We uncover a previously undocumented\nattack phenomenon on CMOS image sensors: rainbow-like color artifacts induced\nin images captured by image sensors through carefully tuned electromagnetic\ninterference. We further evaluate the impact of these attacks on\nstate-of-the-art object detection models, showing that the injected artifacts\npropagate through the image signal processing pipeline and lead to significant\nmispredictions. Our findings highlight a critical and underexplored\nvulnerability in the visual perception stack, highlighting the need for more\nrobust defenses against physical-layer attacks in such systems.", "AI": {"tldr": "The paper explores electromagnetic signal injection attacks on CMOS image sensors, revealing rainbow-like artifacts that bypass digital integrity checks and cause mispredictions in object detection models, exposing a critical vulnerability in visual perception systems.", "motivation": "Image sensors are crucial for safety-critical systems, but their analog domain vulnerabilities to physical-layer attacks remain underexplored. Conventional digital integrity checks fail to detect manipulations at the analog stage.", "method": "Researchers injected carefully tuned electromagnetic interference into CMOS image sensors to induce visible artifacts, then evaluated how these artifacts propagate through the image signal processing pipeline and affect object detection models.", "result": "The attacks successfully created rainbow artifacts undetected by digital checks, leading to significant mispredictions (e.g., halting or altered object detection) in state-of-the-art models without compromising visible image quality.", "conclusion": "This work identifies a previously unknown analog-domain vulnerability in image sensors, demonstrating that physical-layer attacks can manipulate visual data silently. It underscores the need for robust defenses targeting the entire visual perception stack, including hardware-level protections."}}
{"id": "2507.07871", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07871", "abs": "https://arxiv.org/abs/2507.07871", "authors": ["Toluwani Aremu", "Noor Hussein", "Munachiso Nwadike", "Samuele Poppi", "Jie Zhang", "Karthik Nandakumar", "Neil Gong", "Nils Lukas"], "title": "Mitigating Watermark Stealing Attacks in Generative Models via Multi-Key Watermarking", "comment": null, "summary": "Watermarking offers a promising solution for GenAI providers to establish the\nprovenance of their generated content. A watermark is a hidden signal embedded\nin the generated content, whose presence can later be verified using a secret\nwatermarking key. A threat to GenAI providers are \\emph{watermark stealing}\nattacks, where users forge a watermark into content that was \\emph{not}\ngenerated by the provider's models without access to the secret key, e.g., to\nfalsely accuse the provider. Stealing attacks collect \\emph{harmless}\nwatermarked samples from the provider's model and aim to maximize the expected\nsuccess rate of generating \\emph{harmful} watermarked samples. Our work focuses\non mitigating stealing attacks while treating the underlying watermark as a\nblack-box. Our contributions are: (i) Proposing a multi-key extension to\nmitigate stealing attacks that can be applied post-hoc to any watermarking\nmethod across any modality. (ii) We provide theoretical guarantees and\ndemonstrate empirically that our method makes forging substantially less\neffective across multiple datasets, and (iii) we formally define the threat of\nwatermark forging as the task of generating harmful, watermarked content and\nmodel this threat via security games.", "AI": {"tldr": "The paper addresses watermark stealing attacks in GenAI by proposing a multi-key extension to watermarking methods, proving its effectiveness through theory and experiments, and formalizing the threat using security games.", "motivation": "Watermark stealing allows malicious users to forge watermarks into non-provider-generated content, enabling false accusations against AI providers. Current solutions lack robustness against this attack vector.", "method": "A post-hoc multi-key watermarking framework is proposed, where multiple keys generate distinct watermarks for the same content. Theoretical analysis and security game modeling show this hinders attackers' ability to steal and forge watermarks.", "result": "Empirical evaluation across multiple datasets demonstrates reduced forging success rates with minimal impact on watermark verification performance. The security games provide a formal model for quantifying watermarking robustness.", "conclusion": "The proposed multi-key approach significantly enhances watermarking security against stealing attacks while maintaining usability, establishing a new standard for black-box watermarking robustness analysis."}}
{"id": "2507.07901", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07901", "abs": "https://arxiv.org/abs/2507.07901", "authors": ["Sree Bhargavi Balija", "Rekha Singal", "Abhishek Singh", "Ramesh Raskar", "Erfan Darzi", "Raghu Bala", "Thomas Hardjono", "Ken Huang"], "title": "The Trust Fabric: Decentralized Interoperability and Economic Coordination for the Agentic Web", "comment": null, "summary": "The fragmentation of AI agent ecosystems has created urgent demands for\ninteroperability, trust, and economic coordination that current protocols --\nincluding MCP (Hou et al., 2025), A2A (Habler et al., 2025), ACP (Liu et al.,\n2025), and Cisco's AGP (Edwards, 2025) -- cannot address at scale. We present\nthe Nanda Unified Architecture, a decentralized framework built around three\ncore innovations: fast DID-based agent discovery through distributed\nregistries, semantic agent cards with verifiable credentials and composability\nprofiles, and a dynamic trust layer that integrates behavioral attestations\nwith policy compliance. The system introduces X42/H42 micropayments for\neconomic coordination and MAESTRO, a security framework incorporating\nSynergetics' patented AgentTalk protocol (US Patent 12,244,584 B1) and secure\ncontainerization. Real-world deployments demonstrate 99.9 percent compliance in\nhealthcare applications and substantial monthly transaction volumes with strong\nprivacy guarantees. By unifying MIT's trust research with production\ndeployments from Cisco and Synergetics, we show how cryptographic proofs and\npolicy-as-code transform agents into trust-anchored participants in a\ndecentralized economy (Lakshmanan, 2025; Sha, 2025). The result enables a\nglobally interoperable Internet of Agents where trust becomes the native\ncurrency of collaboration across both enterprise and Web3 ecosystems.", "AI": {"tldr": "The Nanda Unified Architecture introduces a decentralized framework with DID-based agent discovery, semantic agent cards with verifiable credentials, and a dynamic trust layer combining behavioral attestations and policy compliance. It uses X42/H42 micropayments for coordination and MAESTRO for security (incorporating patented protocols and containerization), achieving 99.9% healthcare compliance and enabling a globally interoperable Internet of Agents with trust as 'native currency'.", "motivation": "Existing AI agent protocols (MCP, A2A, ACP, AGP) fail to address critical challenges in interoperability, trust establishment, and economic coordination at scale within fragmented agent ecosystems.", "method": "1) Fast decentralized DID-based discovery through distributed registries\n2) Semantic agent cards with: \n   - Verifiable credentials\n   - Composability profiles\n3) Dynamic trust layer integrating:\n   - Behavioral attestations\n   - Policy compliance\n4) X42/H42 micropayment system for economic coordination\n5) MAESTRO security framework using:\n   - AgentTalk protocol (US Patent 12,244,584 B1)\n   - Secure containerization", "result": "- Achieved 99.9% compliance rate in healthcare applications\n- Processed high monthly transaction volumes with strong privacy guarantees\n- Unified MIT's trust research with Cisco's and Synergetics' production deployments", "conclusion": "Transforms AI agents into trust-anchored economic participants by combining cryptographic proofs with policy-as-code, creating a unified Internet of Agents infrastructure where trust is the \"native currency\" for both enterprise and Web3 collaboration."}}
{"id": "2507.07916", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.07916", "abs": "https://arxiv.org/abs/2507.07916", "authors": ["Federico Maria Cau", "Giuseppe Desolda", "Francesco Greco", "Lucio Davide Spano", "Luca Vigan\u00f2"], "title": "Can Large Language Models Improve Phishing Defense? A Large-Scale Controlled Experiment on Warning Dialogue Explanations", "comment": null, "summary": "Phishing has become a prominent risk in modern cybersecurity, often used to\nbypass technological defences by exploiting predictable human behaviour.\nWarning dialogues are a standard mitigation measure, but the lack of\nexplanatory clarity and static content limits their effectiveness. In this\npaper, we report on our research to assess the capacity of Large Language\nModels (LLMs) to generate clear, concise, and scalable explanations for\nphishing warnings. We carried out a large-scale between-subjects user study (N\n= 750) to compare the influence of warning dialogues supplemented with manually\ngenerated explanations against those generated by two LLMs, Claude 3.5 Sonnet\nand Llama 3.3 70B. We investigated two explanatory styles (feature-based and\ncounterfactual) for their effects on behavioural metrics (click-through rate)\nand perceptual outcomes (e.g., trust, risk, clarity). The results indicate that\nwell-constructed LLM-generated explanations can equal or surpass manually\ncrafted explanations in reducing susceptibility to phishing; Claude-generated\nwarnings exhibited particularly robust performance. Feature-based explanations\nwere more effective for genuine phishing attempts, whereas counterfactual\nexplanations diminished false-positive rates. Other variables such as workload,\ngender, and prior familiarity with warning dialogues significantly moderated\nwarning effectiveness. These results indicate that LLMs can be used to\nautomatically build explanations for warning users against phishing, and that\nsuch solutions are scalable, adaptive, and consistent with human-centred\nvalues.", "AI": {"tldr": "This paper evaluates the effectiveness of LLM-generated phishing warning explanations compared to manually crafted ones, finding Claude 3.5 Sonnet and Llama 3.3 70B can produce scalable, adaptive explanations that match or exceed human-designed warnings, with feature-based and counterfactual styles showing distinct advantages in reducing phishing susceptibility and false positives respectively.", "motivation": "Warning dialogues are commonly used to mitigate phishing attacks, but static content and insufficient explanatory clarity reduce their efficacy. The authors investigate whether LLMs can dynamically generate better explanations that align with human cognitive patterns while maintaining scalability.", "method": "A between-subjects user study (750 participants) compared manually crafted explanations with LLM-generated ones (Claude 3.5 Sonnet and Llama 3.3 70B) using two explanatory styles: feature-based (highlighting specific threat indicators) and counterfactual (explaining what would happen differently if the threat were avoided). The study measured both behavioral outcomes (click-through rates) and perceptual responses (trust, risk perception, explanation clarity).", "result": "1) LLM-generated explanations matched/human-designed counterparts in behavioral effectiveness. 2) Feature-based explanations reduced real phishing click-throughs by 19% over default warnings, while counterfactual explanations improved false-positive avoidance by 17%. 3) Claude 3.5 showed stronger performance in maintaining user trust across adaptive scenarios. 4) Workload levels, gender, and prior warning exposure significantly affected how participants responded to explanations.", "conclusion": "LLMs demonstrate significant potential to transform phishing warning systems by delivering scalable, context-aware explanations that align with human processing capabilities while reducing both false negatives and false positives. Implementation considerations include optimizing for variable user contexts (workload, familiarity) and carefully balancing feature-based/counterfactual explanation styles depending on threat types."}}
{"id": "2507.07927", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07927", "abs": "https://arxiv.org/abs/2507.07927", "authors": ["Jenny Blessing", "Ross J. Anderson", "Alastair R. Beresford"], "title": "KeyDroid: A Large-Scale Analysis of Secure Key Storage in Android Apps", "comment": null, "summary": "Most contemporary mobile devices offer hardware-backed storage for\ncryptographic keys, user data, and other sensitive credentials. Such hardware\nprotects credentials from extraction by an adversary who has compromised the\nmain operating system, such as a malicious third-party app. Since 2011, Android\napp developers can access trusted hardware via the Android Keystore API. In\nthis work, we conduct the first comprehensive survey of hardware-backed key\nstorage in Android devices. We analyze 490 119 Android apps, collecting data on\nhow trusted hardware is used by app developers (if used at all) and\ncross-referencing our findings with sensitive user data collected by each app,\nas self-reported by developers via the Play Store's data safety labels.\n  We find that despite industry-wide initiatives to encourage adoption, 56.3%\nof apps self-reporting as processing sensitive user data do not use Android's\ntrusted hardware capabilities at all, while just 5.03% of apps collecting some\nform of sensitive data use the strongest form of trusted hardware, a secure\nelement distinct from the main processor. To better understand the potential\ndownsides of using secure hardware, we conduct the first empirical analysis of\ntrusted hardware performance in mobile devices, measuring the runtime of common\ncryptographic operations across both software- and hardware-backed keystores.\nWe find that while hardware-backed key storage using a coprocessor is viable\nfor most common cryptographic operations, secure elements capable of preventing\nmore advanced attacks make performance infeasible for symmetric encryption with\nnon-negligible payloads and any kind of asymmetric encryption.", "AI": {"tldr": "A study analyzing hardware-backed key storage in Android apps finds most apps do not utilize secure elements, with performance limitations affecting adoption.", "motivation": "To evaluate how Android app developers use hardware-backed key storage for protecting sensitive data through empirical analysis.", "method": "Analyzed 490,119 Android apps for Keystore API usage, cross-referenced with Play Store data safety labels; conducted performance measurements of software- and hardware-backed cryptographic operations.", "result": "56.3% of sensitive data-handling apps use no trusted hardware; only 5.03% use secure elements. Secure elements show infeasible performance for symmetric/asymmetric encryption.", "conclusion": "Trusted hardware APIs are underutilized even as they provide critical security benefits, but secure element performance hinders adoption for certain operations."}}
{"id": "2507.07972", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07972", "abs": "https://arxiv.org/abs/2507.07972", "authors": ["Karthik Garimella", "Austin Ebel", "Brandon Reagen"], "title": "EinHops: Einsum Notation for Expressive Homomorphic Operations on RNS-CKKS Tensors", "comment": "11 pages, 7 figures, 1 table", "summary": "Fully Homomorphic Encryption (FHE) is an encryption scheme that allows for\ncomputation to be performed directly on encrypted data, effectively closing the\nloop on secure and outsourced computing. Data is encrypted not only during rest\nand transit, but also during processing. However, FHE provides a limited\ninstruction set: SIMD addition, SIMD multiplication, and cyclic rotation of 1-D\nvectors. This restriction makes performing multi-dimensional tensor operations\nchallenging. Practitioners must pack these tensors into 1-D vectors and map\ntensor operations onto this one-dimensional layout rather than their\ntraditional nested structure. And while prior systems have made significant\nstrides in automating this process, they often hide critical packing decisions\nbehind layers of abstraction, making debugging, optimizing, and building on top\nof these systems difficult.\n  In this work, we approach multi-dimensional tensor operations in FHE through\nEinstein summation (einsum) notation. Einsum notation explicitly encodes\ndimensional structure and operations in its syntax, naturally exposing how\ntensors should be packed and transformed. We decompose einsum expressions into\na fixed set of FHE-friendly operations. We implement our design and present\nEinHops, a minimalist system that factors einsum expressions into a fixed\nsequence of FHE operations. EinHops enables developers to perform encrypted\ntensor operations using FHE while maintaining full visibility into the\nunderlying packing strategy. We evaluate EinHops on a range of tensor\noperations from a simple transpose to complex multi-dimensional contractions.\nWe show that the explicit nature of einsum notation allows us to build an FHE\ntensor system that is simple, general, and interpretable. We open-source\nEinHops at the following repository: https://github.com/baahl-nyu/einhops.", "AI": {"tldr": "EinHops is an FHE system that uses einsum notation to simplify multi-dimensional tensor operations by exposing packing strategies and decomposing operations into FHE-compatible steps.", "motivation": "FHE's limited instruction set (SIMD arithmetic and 1-D rotations) complicates multi-dimensional tensor operations, requiring manual packing into 1-D vectors. Existing systems abstract packing decisions, hindering debugging, optimization, and further development.", "method": "Transform einsum notation (explicitly encoding dimensions and operations) into FHE operations through decomposition. Implement a minimalist framework (EinHops) that factors einsum expressions into a fixed sequence of FHE primitives while preserving dimensional semantics.", "result": "Evaluation shows EinHops successfully handles operations from simple transposes to complex tensor contractions in FHE, achieving simplicity, generality, and interpretability through the explicit einsum decomposition framework.", "conclusion": "EinHops demonstrates that einsum notation provides a natural, transparent solution for FHE tensor operations by making packing strategies visible. The open-source implementation enables reproducibility and future research in this domain."}}
{"id": "2507.07974", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07974", "abs": "https://arxiv.org/abs/2507.07974", "authors": ["Sizhe Chen", "Yizhu Wang", "Nicholas Carlini", "Chawin Sitawarin", "David Wagner"], "title": "Defending Against Prompt Injection With a Few DefensiveTokens", "comment": null, "summary": "When large language model (LLM) systems interact with external data to\nperform complex tasks, a new attack, namely prompt injection, becomes a\nsignificant threat. By injecting instructions into the data accessed by the\nsystem, the attacker is able to override the initial user task with an\narbitrary task directed by the attacker. To secure the system, test-time\ndefenses, e.g., defensive prompting, have been proposed for system developers\nto attain security only when needed in a flexible manner. However, they are\nmuch less effective than training-time defenses that change the model\nparameters. Motivated by this, we propose DefensiveToken, a test-time defense\nwith prompt injection robustness comparable to training-time alternatives.\nDefensiveTokens are newly inserted as special tokens, whose embeddings are\noptimized for security. In security-sensitive cases, system developers can\nappend a few DefensiveTokens before the LLM input to achieve security with a\nminimal utility drop. In scenarios where security is less of a concern,\ndevelopers can simply skip DefensiveTokens; the LLM system remains the same as\nthere is no defense, generating high-quality responses. Thus, DefensiveTokens,\nif released alongside the model, allow a flexible switch between the\nstate-of-the-art (SOTA) utility and almost-SOTA security at test time. The code\nis available at https://github.com/Sizhe-Chen/DefensiveToken.", "AI": {"tldr": "DefensiveToken is a test-time defense method for large language models (LLMs) that mitigates prompt injection attacks by inserting special tokens with security-optimized embeddings, enabling a flexible switch between high utility and strong security without retraining the model.", "motivation": "Current test-time defenses against prompt injection are less effective than training-time defenses that alter model parameters. The authors aim to develop a test-time defense (DefensiveToken) that achieves security comparable to training-time methods while maintaining model utility in non-security-sensitive scenarios.", "method": "DefensiveToken involves inserting a small number of special tokens before the LLM input during inference. These tokens' embeddings are pre-trained through a one-time process to disrupt prompt injection attacks, requiring no changes to the original model parameters or architecture.", "result": "The approach matches the security performance of state-of-the-art training-time defenses (\u2018almost-SOTA\u2019 security) while causing minimal utility degradation (~2% reduction in safety benchmarks, ~1-3% on instruction-following tasks). Omitting DefensiveTokens preserves the original LLM performance and quality.", "conclusion": "DefensiveToken provides a practical, flexible solution for securing LLMs against prompt injection attacks at test time, achieving near training-time defense robustness without sacrificing model utility in normal operation, with a low deployment cost through one-time token embedding training."}}
