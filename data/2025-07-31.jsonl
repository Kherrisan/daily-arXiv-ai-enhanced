{"id": "2507.22133", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22133", "abs": "https://arxiv.org/abs/2507.22133", "authors": ["Michael Freenor", "Lauren Alvarez", "Milton Leal", "Lily Smith", "Joel Garrett", "Yelyzaveta Husieva", "Madeline Woodruff", "Ryan Miller", "Erich Kummerfeld", "Rafael Medeiros", "Sander Schulhoff"], "title": "Prompt Optimization and Evaluation for LLM Automated Red Teaming", "comment": "9 pages, 5 Figures, and 1 Appendix item", "summary": "Applications that use Large Language Models (LLMs) are becoming widespread,\nmaking the identification of system vulnerabilities increasingly important.\nAutomated Red Teaming accelerates this effort by using an LLM to generate and\nexecute attacks against target systems. Attack generators are evaluated using\nthe Attack Success Rate (ASR) the sample mean calculated over the judgment of\nsuccess for each attack. In this paper, we introduce a method for optimizing\nattack generator prompts that applies ASR to individual attacks. By repeating\neach attack multiple times against a randomly seeded target, we measure an\nattack's discoverability the expectation of the individual attack success. This\napproach reveals exploitable patterns that inform prompt optimization,\nultimately enabling more robust evaluation and refinement of generators."}
{"id": "2507.22160", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22160", "abs": "https://arxiv.org/abs/2507.22160", "authors": ["Yassine Rachidy", "Jihad Rbaiti", "Youssef Hmamouche", "Faissal Sehbaoui", "Amal El Fallah Seghrouchni"], "title": "Strategic Deflection: Defending LLMs from Logit Manipulation", "comment": "20 pages", "summary": "With the growing adoption of Large Language Models (LLMs) in critical areas,\nensuring their security against jailbreaking attacks is paramount. While\ntraditional defenses primarily rely on refusing malicious prompts, recent\nlogit-level attacks have demonstrated the ability to bypass these safeguards by\ndirectly manipulating the token-selection process during generation. We\nintroduce Strategic Deflection (SDeflection), a defense that redefines the\nLLM's response to such advanced attacks. Instead of outright refusal, the model\nproduces an answer that is semantically adjacent to the user's request yet\nstrips away the harmful intent, thereby neutralizing the attacker's harmful\nintent. Our experiments demonstrate that SDeflection significantly lowers\nAttack Success Rate (ASR) while maintaining model performance on benign\nqueries. This work presents a critical shift in defensive strategies, moving\nfrom simple refusal to strategic content redirection to neutralize advanced\nthreats."}
{"id": "2507.22165", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.22165", "abs": "https://arxiv.org/abs/2507.22165", "authors": ["Gursimran Singh", "H. B. Acharya", "Minseok Kwon"], "title": "Programmable Data Planes for Network Security", "comment": "17th International Conference on Networks & Communications (NeTCoM\n  2025)", "summary": "The emergence of programmable data planes, and particularly switches\nsupporting the P4 language, has transformed network security by enabling\ncustomized, line-rate packet processing. These switches, originally intended\nfor flexible forwarding, now play a broader role: detecting and mitigating\nattacks such as DDoS and spoofing, enforcing next-generation firewall policies,\nand even supporting in-network cryptography and machine learning. These\ncapabilities are made possible by techniques such as recirculate-and-truncate\nand lookup-table precomputation, which work around architectural constraints\nlike limited memory and restricted instruction sets. In this paper, we\nsystematize recent advances in security applications built on programmable\nswitches, with an emphasis on the capabilities, challenges, and architectural\nworkarounds. We highlight the non-obvious design techniques that make complex\nin-network security functions feasible despite the constraints of the hardware\nplatform, and also comment on remaining issues and emerging research\ndirections."}
{"id": "2507.22171", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22171", "abs": "https://arxiv.org/abs/2507.22171", "authors": ["Zheng Zhang", "Peilin Zhao", "Deheng Ye", "Hao Wang"], "title": "Enhancing Jailbreak Attacks on LLMs via Persona Prompts", "comment": null, "summary": "Jailbreak attacks aim to exploit large language models (LLMs) by inducing\nthem to generate harmful content, thereby revealing their vulnerabilities.\nUnderstanding and addressing these attacks is crucial for advancing the field\nof LLM safety. Previous jailbreak approaches have mainly focused on direct\nmanipulations of harmful intent, with limited attention to the impact of\npersona prompts. In this study, we systematically explore the efficacy of\npersona prompts in compromising LLM defenses. We propose a genetic\nalgorithm-based method that automatically crafts persona prompts to bypass\nLLM's safety mechanisms. Our experiments reveal that: (1) our evolved persona\nprompts reduce refusal rates by 50-70% across multiple LLMs, and (2) these\nprompts demonstrate synergistic effects when combined with existing attack\nmethods, increasing success rates by 10-20%. Our code and data are available at\nhttps://github.com/CjangCjengh/Generic_Persona."}
{"id": "2507.22063", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22063", "abs": "https://arxiv.org/abs/2507.22063", "authors": ["Wenjie Jacky Mo", "Qin Liu", "Xiaofei Wen", "Dongwon Jung", "Hadi Askari", "Wenxuan Zhou", "Zhe Zhao", "Muhao Chen"], "title": "RedCoder: Automated Multi-Turn Red Teaming for Code LLMs", "comment": null, "summary": "Large Language Models (LLMs) for code generation (i.e., Code LLMs) have\ndemonstrated impressive capabilities in AI-assisted software development and\ntesting. However, recent studies have shown that these models are prone to\ngenerating vulnerable or even malicious code under adversarial settings.\nExisting red-teaming approaches rely on extensive human effort, limiting their\nscalability and practicality, and generally overlook the interactive nature of\nreal-world AI-assisted programming, which often unfolds over multiple turns. To\nbridge these gaps, we present RedCoder, a red-teaming agent that engages victim\nmodels in multi-turn conversation to elicit vulnerable code. The pipeline to\nconstruct RedCoder begins with a multi-agent gaming process that simulates\nadversarial interactions, yielding a set of prototype conversations and an\narsenal of reusable attack strategies. We then fine-tune an LLM on these\nprototype conversations to serve as the backbone of RedCoder. Once deployed,\nRedCoder autonomously engages Code LLMs in multi-turn conversations,\ndynamically retrieving relevant strategies from the arsenal to steer the\ndialogue toward vulnerability-inducing outputs. Experiments across multiple\nCode LLMs show that our approach outperforms prior single-turn and multi-turn\nred-team methods in inducing vulnerabilities in code generation, offering a\nscalable and effective tool for evaluating the security boundaries of modern\ncode-generation systems."}
{"id": "2507.22177", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.22177", "abs": "https://arxiv.org/abs/2507.22177", "authors": ["Tanzim Mahfuz", "Sudipta Paria", "Tasneem Suha", "Swarup Bhunia", "Prabuddha Chakraborty"], "title": "POLARIS: Explainable Artificial Intelligence for Mitigating Power Side-Channel Leakage", "comment": null, "summary": "Microelectronic systems are widely used in many sensitive applications (e.g.,\nmanufacturing, energy, defense). These systems increasingly handle sensitive\ndata (e.g., encryption key) and are vulnerable to diverse threats, such as,\npower side-channel attacks, which infer sensitive data through dynamic power\nprofile. In this paper, we present a novel framework, POLARIS for mitigating\npower side channel leakage using an Explainable Artificial Intelligence (XAI)\nguided masking approach. POLARIS uses an unsupervised process to automatically\nbuild a tailored training dataset and utilize it to train a masking model.The\nPOLARIS framework outperforms state-of-the-art mitigation solutions (e.g.,\nVALIANT) in terms of leakage reduction, execution time, and overhead across\nlarge designs."}
{"id": "2507.22064", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22064", "abs": "https://arxiv.org/abs/2507.22064", "authors": ["Michael Cohoon", "Debbie Furman"], "title": "Machine Learning Experiences: A story of learning AI for use in enterprise software testing that can be used by anyone", "comment": null, "summary": "This paper details the machine learning (ML) journey of a group of people\nfocused on software testing. It tells the story of how this group progressed\nthrough a ML workflow (similar to the CRISP-DM process). This workflow consists\nof the following steps and can be used by anyone applying ML techniques to a\nproject: gather the data; clean the data; perform feature engineering on the\ndata; splitting the data into two sets, one for training and one for testing;\nchoosing a machine learning model; training the model; testing the model and\nevaluating the model performance. By following this workflow, anyone can\neffectively apply ML to any project that they are doing."}
{"id": "2507.22231", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22231", "abs": "https://arxiv.org/abs/2507.22231", "authors": ["Ahmed Sabbah", "Radi Jarrar", "Samer Zein", "David Mohaisen"], "title": "Understanding Concept Drift with Deprecated Permissions in Android Malware Detection", "comment": "13 pages, 9 figures, 5 tables, under review", "summary": "Permission analysis is a widely used method for Android malware detection. It\ninvolves examining the permissions requested by an application to access\nsensitive data or perform potentially malicious actions. In recent years,\nvarious machine learning (ML) algorithms have been applied to Android malware\ndetection using permission-based features and feature selection techniques,\noften achieving high accuracy. However, these studies have largely overlooked\nimportant factors such as protection levels and the deprecation or restriction\nof permissions due to updates in the Android OS -- factors that can contribute\nto concept drift.\n  In this study, we investigate the impact of deprecated and restricted\npermissions on the performance of machine learning models. A large dataset\ncontaining 166 permissions was used, encompassing more than 70,000 malware and\nbenign applications. Various machine learning and deep learning algorithms were\nemployed as classifiers, along with different concept drift detection\nstrategies. The results suggest that Android permissions are highly effective\nfeatures for malware detection, with the exclusion of deprecated and restricted\npermissions having only a marginal impact on model performance. In some cases,\nsuch as with CNN, accuracy improved. Excluding these permissions also enhanced\nthe detection of concept drift using a year-to-year analysis strategy. Dataset\nbalancing further improved model performance, reduced low-accuracy instances,\nand enhanced concept drift detection via the Kolmogorov-Smirnov test."}
{"id": "2507.22065", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.22065", "abs": "https://arxiv.org/abs/2507.22065", "authors": ["Xiaotao Feng", "Xiaogang Zhu", "Kun Hu", "Jincheng Wang", "Yingjie Cao", "Guang Gong", "Jianfeng Pan"], "title": "Fuzzing: Randomness? Reasoning! Efficient Directed Fuzzing via Large Language Models", "comment": null, "summary": "Fuzzing is highly effective in detecting bugs due to the key contribution of\nrandomness. However, randomness significantly reduces the efficiency of\nfuzzing, causing it to cost days or weeks to expose bugs. Even though directed\nfuzzing reduces randomness by guiding fuzzing towards target buggy locations,\nthe dilemma of randomness still challenges directed fuzzers. Two critical\ncomponents, which are seeds and mutators, contain randomness and are closely\ntied to the conditions required for triggering bugs. Therefore, to address the\nchallenge of randomness, we propose to use large language models (LLMs) to\nremove the randomness in seeds and reduce the randomness in mutators. With\ntheir strong reasoning and code generation capabilities, LLMs can be used to\ngenerate reachable seeds that target pre-determined locations and to construct\nbug-specific mutators tailored for specific bugs. We propose RandLuzz, which\nintegrates LLMs and directed fuzzing, to improve the quality of seeds and\nmutators, resulting in efficient bug exposure. RandLuzz analyzes function call\nchain or functionality to guide LLMs in generating reachable seeds. To\nconstruct bug-specific mutators, RandLuzz uses LLMs to perform bug analysis,\nobtaining information such as bug causes and mutation suggestions, which\nfurther help generate code that performs bug-specific mutations. We evaluate\nRandLuzz by comparing it with four state-of-the-art directed fuzzers, AFLGo,\nBeacon, WindRanger, and SelectFuzz. With RandLuzz-generated seeds, the fuzzers\nachieve an average speedup ranging from 2.1$\\times$ to 4.8$\\times$ compared to\nusing widely-used initial seeds. Additionally, when evaluated on individual\nbugs, RandLuzz achieves up to a 2.7$\\times$ speedup compared to the\nsecond-fastest exposure. On 8 bugs, RandLuzz can even expose them within 60\nseconds."}
{"id": "2507.22239", "categories": ["cs.CR", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.22239", "abs": "https://arxiv.org/abs/2507.22239", "authors": ["Muhammad Sharshar", "Ahmad Mohammad Saber", "Davor Svetinovic", "Amr M. Youssef", "Deepa Kundur", "Ehab F. El-Saadany"], "title": "Large Language Model-Based Framework for Explainable Cyberattack Detection in Automatic Generation Control Systems", "comment": "Accepted Publication", "summary": "The increasing digitization of smart grids has improved operational\nefficiency but also introduced new cybersecurity vulnerabilities, such as False\nData Injection Attacks (FDIAs) targeting Automatic Generation Control (AGC)\nsystems. While machine learning (ML) and deep learning (DL) models have shown\npromise in detecting such attacks, their opaque decision-making limits operator\ntrust and real-world applicability. This paper proposes a hybrid framework that\nintegrates lightweight ML-based attack detection with natural language\nexplanations generated by Large Language Models (LLMs). Classifiers such as\nLightGBM achieve up to 95.13% attack detection accuracy with only 0.004 s\ninference latency. Upon detecting a cyberattack, the system invokes LLMs,\nincluding GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o mini, to generate\nhuman-readable explanation of the event. Evaluated on 100 test samples, GPT-4o\nmini with 20-shot prompting achieved 93% accuracy in identifying the attack\ntarget, a mean absolute error of 0.075 pu in estimating attack magnitude, and\n2.19 seconds mean absolute error (MAE) in estimating attack onset. These\nresults demonstrate that the proposed framework effectively balances real-time\ndetection with interpretable, high-fidelity explanations, addressing a critical\nneed for actionable AI in smart grid cybersecurity."}
{"id": "2507.22066", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.22066", "abs": "https://arxiv.org/abs/2507.22066", "authors": ["Dylan Manuel", "Paul Rad"], "title": "CodableLLM: Automating Decompiled and Source Code Mapping for LLM Dataset Generation", "comment": null, "summary": "The generation of large, high-quality datasets for code understanding and\ngeneration remains a significant challenge, particularly when aligning\ndecompiled binaries with their original source code. To address this, we\npresent CodableLLM, a Python framework designed to automate the creation and\ncuration of datasets by mapping decompiled functions to their corresponding\nsource functions. This process enhances the alignment between decompiled and\nsource code representations, facilitating the development of large language\nmodels (LLMs) capable of understanding and generating code across multiple\nabstraction levels. CodableLLM supports multiple programming languages and\nintegrates with existing decompilers and parsers to streamline dataset\ngeneration. This paper presents the design and implementation of CodableLLM,\nevaluates its performance in dataset creation, and compares it to existing\ntools in the field. The results demonstrate that CodableLLM offers a robust and\nefficient solution for generating datasets tailored for code-focused LLMS."}
{"id": "2507.22304", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.22304", "abs": "https://arxiv.org/abs/2507.22304", "authors": ["Chetan Pathade"], "title": "Invisible Injections: Exploiting Vision-Language Models Through Steganographic Prompt Embedding", "comment": "14 Pages", "summary": "Vision-language models (VLMs) have revolutionized multimodal AI applications\nbut introduce novel security vulnerabilities that remain largely unexplored. We\npresent the first comprehensive study of steganographic prompt injection\nattacks against VLMs, where malicious instructions are invisibly embedded\nwithin images using advanced steganographic techniques. Our approach\ndemonstrates that current VLM architectures can inadvertently extract and\nexecute hidden prompts during normal image processing, leading to covert\nbehavioral manipulation. We develop a multi-domain embedding framework\ncombining spatial, frequency, and neural steganographic methods, achieving an\noverall attack success rate of 24.3% (plus or minus 3.2%, 95% CI) across\nleading VLMs including GPT-4V, Claude, and LLaVA, with neural steganography\nmethods reaching up to 31.8%, while maintaining reasonable visual\nimperceptibility (PSNR greater than 38 dB, SSIM greater than 0.94). Through\nsystematic evaluation on 12 diverse datasets and 8 state-of-the-art models, we\nreveal moderate but meaningful vulnerabilities in current VLM architectures and\npropose effective countermeasures. Our findings have significant implications\nfor VLM deployment in security-critical applications and highlight the need for\nproportionate multimodal AI security frameworks."}
{"id": "2507.22070", "categories": ["cs.SE", "cs.CE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.22070", "abs": "https://arxiv.org/abs/2507.22070", "authors": ["Y. Du"], "title": "Automated Test Data Generation for Enterprise Protobuf Systems: A Metaclass-Enhanced Statistical Approach", "comment": "7 pages", "summary": "Large-scale enterprise systems utilizing Protocol Buffers (protobuf) present\nsignificant challenges for performance testing, particularly when targeting\nintermediate business interfaces with complex nested data structures.\nTraditional test data generation approaches are inadequate for handling the\nintricate hierarchical and graph-like structures inherent in enterprise\nprotobuf schemas. This paper presents a novel test data generation framework\nthat leverages Python's metaclass system for dynamic type enhancement and\nstatistical analysis of production logs for realistic value domain extraction.\nOur approach combines automatic schema introspection, statistical value\ndistribution analysis, and recursive descent algorithms for handling deeply\nnested structures. Experimental evaluation on three real-world enterprise\nsystems demonstrates up to 95\\% reduction in test data preparation time and\n80\\% improvement in test coverage compared to existing approaches. The\nframework successfully handles protobuf structures with up to 15 levels of\nnesting and generates comprehensive test suites containing over 100,000 test\ncases within seconds."}
{"id": "2507.22306", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.22306", "abs": "https://arxiv.org/abs/2507.22306", "authors": ["Sahan Sanjaya", "Aruna Jayasena", "Prabhat Mishra"], "title": "SleepWalk: Exploiting Context Switching and Residual Power for Physical Side-Channel Attacks", "comment": null, "summary": "Context switching is utilized by operating systems to change the execution\ncontext between application programs. It involves saving and restoring the\nstates of multiple registers and performing a pipeline flush to remove any\npre-fetched instructions, leading to a higher instantaneous power consumption\ncompared to typical program execution. In this paper, we introduce a physical\npower side-channel leakage source that exploits the power spike observed during\na context switch, triggered by the inbuilt sleep function of the system kernel.\nWe observed that this power spike directly correlates with both the power\nconsumption during context switching and the residual power consumption of the\npreviously executed program. Notably, the persistence of residual power\nsignatures from previous workloads extends the scope of this side-channel\nbeyond extracting the data in registers during the context switch. Unlike\ntraditional approaches that require analyzing full power traces, applying\ncomplex preprocessing, or relying on external synchronization triggers, this\nnovel technique leverages only the amplitude of a single power spike,\nsignificantly simplifying the attack. We developed a power model to illustrate\nthe feasibility of mounting end-to-end side-channel attacks using the\nsleep-induced power spikes. Experimental evaluation demonstrates that our\nframework can successfully perform cryptographic key recovery for both AES and\nSIKE implementations on Broadcom BCM2711."}
{"id": "2507.22071", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22071", "abs": "https://arxiv.org/abs/2507.22071", "authors": ["Niels Glodny"], "title": "Analyzing and Evaluating the Behavior of Git Diff and Merge", "comment": "Bachelor's thesis", "summary": "Despite being widely used, the algorithms that enable collaboration with Git\nare not well understood. The diff and merge algorithms are particularly\ninteresting, as they could be applied in other contexts. In this thesis, I\ndocument the main functionalities of Git: how diffs are computed, how they are\nused to run merges, and how merges enable more complex operations. In the\nprocess, I show multiple unexpected behaviors in Git, including the following:\nThe histogram diff algorithm has pathological cases where a single-line change\ncan cause the entire rest of the file to be marked as changed. The default\nmerge strategy (ort) can result in merges requiring exponential time in the\nnumber of commits in the history. Merges and rebases are not commutative, and\neven when merges do not result in a conflict, the result is not specified but\ndepends on the diff algorithm used. And finally, sometimes when two sides of a\nmerge add different lines at the same position, the result is not a conflict,\nbut a merge containing both changes after each other, in arbitrary order."}
{"id": "2507.22347", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.22347", "abs": "https://arxiv.org/abs/2507.22347", "authors": ["Alexander Goldberg", "Giulia Fanti", "Nihar Shah", "Zhiwei Steven Wu"], "title": "Benchmarking Fraud Detectors on Private Graph Data", "comment": null, "summary": "We introduce the novel problem of benchmarking fraud detectors on private\ngraph-structured data. Currently, many types of fraud are managed in part by\nautomated detection algorithms that operate over graphs. We consider the\nscenario where a data holder wishes to outsource development of fraud detectors\nto third parties (e.g., vendors or researchers). The third parties submit their\nfraud detectors to the data holder, who evaluates these algorithms on a private\ndataset and then publicly communicates the results. We propose a realistic\nprivacy attack on this system that allows an adversary to de-anonymize\nindividuals' data based only on the evaluation results. In simulations of a\nprivacy-sensitive benchmark for facial recognition algorithms by the National\nInstitute of Standards and Technology (NIST), our attack achieves near perfect\naccuracy in identifying whether individuals' data is present in a private\ndataset, with a True Positive Rate of 0.98 at a False Positive Rate of 0.00. We\nthen study how to benchmark algorithms while satisfying a formal differential\nprivacy (DP) guarantee. We empirically evaluate two classes of solutions:\nsubsample-and-aggregate and DP synthetic graph data. We demonstrate through\nextensive experiments that current approaches do not provide utility when\nguaranteeing DP. Our results indicate that the error arising from DP trades off\nbetween bias from distorting graph structure and variance from adding random\nnoise. Current methods lie on different points along this bias-variance\ntrade-off, but more complex methods tend to require high-variance noise\naddition, undermining utility."}
{"id": "2507.22080", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22080", "abs": "https://arxiv.org/abs/2507.22080", "authors": ["Qiushi Sun", "Jinyang Gong", "Lei Li", "Qipeng Guo", "Fei Yuan"], "title": "CodeEvo: Interaction-Driven Synthesis of Code-centric Data through Hybrid and Iterative Feedback", "comment": "Work in progress", "summary": "Acquiring high-quality instruction-code pairs is essential for training Large\nLanguage Models (LLMs) for code generation. Manually curated data is expensive\nand inherently limited in scale, motivating the development of code-centric\nsynthesis methods. Yet, current approaches either focus on augmenting existing\ncode or rely on predefined heuristics, both lacking rigorous data validation,\nwhich results in synthetic data that is ungrounded, repetitive, or overly\nsimplistic. Inspired by collaborative programming practices, we propose\nCodeEvo, a framework that synthesizes code data through iterative interactions\nbetween two LLM agents: a Coder, which generates candidate code and test cases\nbased on given instructions, and a Reviewer, which guides the synthesis process\nby producing new instructions and feedback. We further introduce a hybrid\nfeedback mechanism that combines compiler determinism with the generative\nflexibility of agents, enabling automatic quality control throughout synthesis.\nExtensive experiments demonstrate that models fine-tuned on CodeEvo data\nsignificantly outperform established baselines across code generation\nbenchmarks with various difficulties. In-depth analyses further provide\ninsights from multiple perspectives into effective code-centric data synthesis."}
{"id": "2507.22371", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22371", "abs": "https://arxiv.org/abs/2507.22371", "authors": ["Lei Yu", "Shiqi Cheng", "Zhirong Huang", "Jingyuan Zhang", "Chenjie Shen", "Junyi Lu", "Li Yang", "Fengjun Zhang", "Jiajia Ma"], "title": "SAEL: Leveraging Large Language Models with Adaptive Mixture-of-Experts for Smart Contract Vulnerability Detection", "comment": "Accepted to ICSME 2025", "summary": "With the increasing security issues in blockchain, smart contract\nvulnerability detection has become a research focus. Existing vulnerability\ndetection methods have their limitations: 1) Static analysis methods struggle\nwith complex scenarios. 2) Methods based on specialized pre-trained models\nperform well on specific datasets but have limited generalization capabilities.\nIn contrast, general-purpose Large Language Models (LLMs) demonstrate\nimpressive ability in adapting to new vulnerability patterns. However, they\noften underperform on specific vulnerability types compared to methods based on\nspecialized pre-trained models. We also observe that explanations generated by\ngeneral-purpose LLMs can provide fine-grained code understanding information,\ncontributing to improved detection performance.\n  Inspired by these observations, we propose SAEL, an LLM-based framework for\nsmart contract vulnerability detection. We first design targeted prompts to\nguide LLMs in identifying vulnerabilities and generating explanations, which\nserve as prediction features. Next, we apply prompt-tuning on CodeT5 and T5 to\nprocess contract code and explanations, enhancing task-specific performance. To\ncombine the strengths of each approach, we introduce an Adaptive\nMixture-of-Experts architecture. This dynamically adjusts feature weights via a\nGating Network, which selects relevant features using TopK filtering and\nSoftmax normalization, and incorporates a Multi-Head Self-Attention mechanism\nto enhance cross-feature relationships. This design enables effective\nintegration of LLM predictions, explanation features, and code features through\ngradient optimization. The loss function jointly considers both independent\nfeature performance and overall weighted predictions. Experiments show that\nSAEL outperforms existing methods across various vulnerabilities."}
{"id": "2507.22085", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22085", "abs": "https://arxiv.org/abs/2507.22085", "authors": ["Vaani Goenka", "Aalok D. Thakkar"], "title": "BOOP: Write Right Code", "comment": null, "summary": "Novice programmers frequently adopt a syntax-specific and test-case-driven\napproach, writing code first and adjusting until programs compile and test\ncases pass, rather than developing correct solutions through systematic\nreasoning. AI coding tools exacerbate this challenge by providing syntactically\ncorrect but conceptually flawed solutions. In this paper, we introduce BOOP\n(Blueprint, Operations, OCaml, Proof), a structured framework requiring four\nmandatory phases: formal specification, language-agnostic algorithm\ndevelopment, implementation, and correctness proof. This shifts focus from\n``making code work'' to understanding why code is correct.\n  BOOP was implemented at our institution using a VS Code extension and\npreprocessor that enforces constraints and identifies counterproductive\npatterns. Initial evaluation shows improved algorithmic reasoning and reduced\ntrial-and-error debugging. Students reported better edge case understanding and\nproblem decomposition, though some initially found the format verbose.\nInstructors observed stronger foundational skills compared to traditional\napproaches."}
{"id": "2507.22447", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22447", "abs": "https://arxiv.org/abs/2507.22447", "authors": ["Zhihong Liang", "Xin Wang", "Zhenhuang Hu", "Liangliang Song", "Lin Chen", "Jingjing Guo", "Yanbin Wang", "Ye Tian"], "title": "Breaking Obfuscation: Cluster-Aware Graph with LLM-Aided Recovery for Malicious JavaScript Detection", "comment": null, "summary": "With the rapid expansion of web-based applications and cloud services,\nmalicious JavaScript code continues to pose significant threats to user\nprivacy, system integrity, and enterprise security. But, detecting such threats\nremains challenging due to sophisticated code obfuscation techniques and\nJavaScript's inherent language characteristics, particularly its nested closure\nstructures and syntactic flexibility. In this work, we propose DeCoda, a hybrid\ndefense framework that combines large language model (LLM)-based deobfuscation\nwith code graph learning: (1) We first construct a sophisticated\nprompt-learning pipeline with multi-stage refinement, where the LLM\nprogressively reconstructs the original code structure from obfuscated inputs\nand then generates normalized Abstract Syntax Tree (AST) representations; (2)\nIn JavaScript ASTs, dynamic typing scatters semantically similar nodes while\ndeeply nested functions fracture scope capturing, introducing structural noise\nand semantic ambiguity. To address these challenges, we then propose to learn\nhierarchical code graph representations via a Cluster-wise Graph that\nsynergistically integrates graph transformer network, node clustering, and\nnode-to-cluster attention to simultaneously capture both local node-level\nsemantics and global cluster-induced structural relationships from AST graph.\nExperimental results demonstrate that our method achieves F1-scores of 94.64%\nand 97.71% on two benchmark datasets, demonstrating absolute improvements of\n10.74% and 13.85% over state-of-the-art baselines. In false-positive control\nevaluation at fixed FPR levels (0.0001, 0.001, 0.01), our approach delivers\n4.82, 5.91, and 2.53 higher TPR respectively compared to the best-performing\nbaseline. These results highlight the effectiveness of LLM-based deobfuscation\nand underscore the importance of modeling cluster-level relationships in\ndetecting malicious code."}
{"id": "2507.22086", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.22086", "abs": "https://arxiv.org/abs/2507.22086", "authors": ["Honghua Dong", "Jiacheng Yang", "Xun Deng", "Yuhe Jiang", "Gennady Pekhimenko", "Fan Long", "Xujie Si"], "title": "TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories", "comment": null, "summary": "Type inference for dynamic languages like Python is a persistent challenge in\nsoftware engineering. While large language models (LLMs) have shown promise in\ncode understanding, their type inference capabilities remain underexplored. We\nintroduce TypyBench, a benchmark designed to evaluate LLMs' type inference\nacross entire Python repositories. TypyBench features two novel metrics:\nTypeSim, which captures nuanced semantic relationships between predicted and\nground truth types, and TypeCheck, which assesses type consistency across\ncodebases. Our evaluation of various LLMs on a curated dataset of 50\nhigh-quality Python repositories reveals that, although LLMs achieve decent\nTypeSim scores, they struggle with complex nested types and exhibit significant\ntype consistency errors. These findings suggest that future research should\nshift focus from improving type similarity to addressing repository-level\nconsistency. TypyBench provides a foundation for this new direction, offering\ninsights into model performance across different type complexities and usage\ncontexts. Our code and data are available at\nhttps://github.com/typybench/typybench."}
{"id": "2507.22611", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.22611", "abs": "https://arxiv.org/abs/2507.22611", "authors": ["Chunyi Zhang", "Fengjiao Dou", "Xiaoqi Li"], "title": "DoS Attacks and Defense Technologies in Blockchain Systems: A Hierarchical Analysis", "comment": null, "summary": "Blockchain technology is widely used in various fields due to its ability to\nprovide decentralization and trustless security. This is a fundamental\nunderstanding held by many advocates, but it is misunderstood, leading\nparticipants to fail to recognize the limitations of the security that\nblockchain can provide. Among all current network attacks, Denial of Service\n(DoS) attacks pose significant threats due to their ease of execution and\ndestructive potential. This paper, based on the blockchain architecture\nhierarchy, categorizes and organizes existing DoS attacks, with a focus on\nexplaining the principles and methods of contract layer and consensus layer DoS\nattacks. Furthermore, this paper comprehensively analyzes and compares commonly\nused detection methods and defense technologies, which will contribute to\nstrengthening the security and stability of blockchain systems and promoting\nfurther innovation and application of blockchain systems."}
{"id": "2507.22223", "categories": ["cs.SE", "D.2; D.2.4; D.4.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.22223", "abs": "https://arxiv.org/abs/2507.22223", "authors": ["Kiana Kiashemshaki", "Mohammad Jalili Torkamani", "Negin Mahmoudi"], "title": "Secure coding for web applications: Frameworks, challenges, and the role of LLMs", "comment": "11 pages, 5 figures, 3 tables, 6 listings", "summary": "Secure coding is a critical yet often overlooked practice in software\ndevelopment. Despite extensive awareness efforts, real-world adoption remains\ninconsistent due to organizational, educational, and technical barriers. This\npaper provides a comprehensive review of secure coding practices across major\nframeworks and domains, including web development, DevSecOps, and cloud\nsecurity. It introduces a structured framework comparison and categorizes\nthreats aligned with the OWASP Top 10. Additionally, we explore the rising role\nof Large Language Models (LLMs) in evaluating and recommending secure code,\npresenting a reproducible case study across four major vulnerability types.\nThis paper offers practical insights for researchers, developers, and educators\non integrating secure coding into real-world development processes."}
{"id": "2507.22617", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22617", "abs": "https://arxiv.org/abs/2507.22617", "authors": ["Yiting Qu", "Ziqing Yang", "Yihan Ma", "Michael Backes", "Savvas Zannettou", "Yang Zhang"], "title": "Hate in Plain Sight: On the Risks of Moderating AI-Generated Hateful Illusions", "comment": "Accepted at ICCV 2025", "summary": "Recent advances in text-to-image diffusion models have enabled the creation\nof a new form of digital art: optical illusions--visual tricks that create\ndifferent perceptions of reality. However, adversaries may misuse such\ntechniques to generate hateful illusions, which embed specific hate messages\ninto harmless scenes and disseminate them across web communities. In this work,\nwe take the first step toward investigating the risks of scalable hateful\nillusion generation and the potential for bypassing current content moderation\nmodels. Specifically, we generate 1,860 optical illusions using Stable\nDiffusion and ControlNet, conditioned on 62 hate messages. Of these, 1,571 are\nhateful illusions that successfully embed hate messages, either overtly or\nsubtly, forming the Hateful Illusion dataset. Using this dataset, we evaluate\nthe performance of six moderation classifiers and nine vision language models\n(VLMs) in identifying hateful illusions. Experimental results reveal\nsignificant vulnerabilities in existing moderation models: the detection\naccuracy falls below 0.245 for moderation classifiers and below 0.102 for VLMs.\nWe further identify a critical limitation in their vision encoders, which\nmainly focus on surface-level image details while overlooking the secondary\nlayer of information, i.e., hidden messages. To address this risk, we explore\npreliminary mitigation measures and identify the most effective approaches from\nthe perspectives of image transformations and training-level strategies."}
{"id": "2507.22324", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22324", "abs": "https://arxiv.org/abs/2507.22324", "authors": ["Cameron S. Movassaghi", "Amanda Momenzadeh", "Jesse G. Meyer"], "title": "From Articles to Code: On-Demand Generation of Core Algorithms from Scientific Publications", "comment": null, "summary": "Maintaining software packages imposes significant costs due to dependency\nmanagement, bug fixes, and versioning. We show that rich method descriptions in\nscientific publications can serve as standalone specifications for modern large\nlanguage models (LLMs), enabling on-demand code generation that could supplant\nhuman-maintained libraries. We benchmark state-of-the-art models\n(GPT-o4-mini-high, Gemini Pro 2.5, Claude Sonnet 4) by tasking them with\nimplementing a diverse set of core algorithms drawn from original publications.\nOur results demonstrate that current LLMs can reliably reproduce package\nfunctionality with performance indistinguishable from conventional libraries.\nThese findings foreshadow a paradigm shift toward flexible, on-demand code\ngeneration and away from static, human-maintained packages, which will result\nin reduced maintenance overhead by leveraging published articles as sufficient\ncontext for the automated implementation of analytical workflows."}
{"id": "2507.22674", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.22674", "abs": "https://arxiv.org/abs/2507.22674", "authors": ["Ramprasad Sarkar"], "title": "Cryptanalysis of LC-MUME: A Lightweight Certificateless Multi-User Matchmaking Encryption for Mobile Devices", "comment": null, "summary": "Yang et al. proposed a lightweight certificateless multiuser matchmaking\nencryption (LC-MUME) scheme for mobile devices, published in IEEE Transactions\non Information Forensics and Security (TIFS) (DOI: 10.1109/TIFS.2023.3321961).\nTheir construction aims to reduce computational and communication overhead\nwithin a one-to-many certificateless cryptographic framework. The authors claim\nthat their scheme satisfies existential unforgeability under chosen-message\nattacks (EUF-CMA) in the random oracle model. However, our cryptanalytic study\ndemonstrates that the scheme fails to meet this critical security requirement.\nIn particular, we show that a Type-I adversary can successfully forge a valid\nciphertext without possessing the complete private key of the sender. Both\ntheoretical analysis and practical implementation confirm that this attack can\nbe mounted with minimal computational cost. To address these weaknesses, we\npropose a modification strategy to strengthen the security of matchmaking\nencryption schemes in mobile computing environments."}
{"id": "2507.22414", "categories": ["cs.SE", "D.2; I.2"], "pdf": "https://arxiv.org/pdf/2507.22414", "abs": "https://arxiv.org/abs/2507.22414", "authors": ["Sungmin Kang", "Haifeng Ruan", "Abhik Roychoudhury"], "title": "AutoCodeSherpa: Symbolic Explanations in AI Coding Agents", "comment": null, "summary": "Large Language Model (LLM) agents autonomously use external tools on top of\none or more LLMs to accomplish specific tasks. Lately LLM agents for software\nengineering tasks have become popular. These agents can benefit from the use of\nprogram analysis tools working on program representations. This is demonstrated\nby existing agentic AI solutions such as AutoCodeRover or SpecRover which\nperform automated program repair. Specifically the goal of these works is to\nuse program analysis to improve the patch quality. These agents are currently\nbeing used to automatically fix static analysis issues from the widely used\nSonarQube static analyzer.\n  Nevertheless, for the agents to be deployed in a production environment,\nagents need to suggest software artifacts, such as patches, with evidence and\nwith high confidence. In this work, we provide a workflow where an agent\nprovides explanations of the bug in the form of symbolic formulae. The\nexplanations are in the form of input conditions, infection conditions and\noutput conditions, implemented as property based tests (PBT) and\nprogram-internal symbolic expressions. These can help in human developer\ncognition of the agent outputs as well as in achieving completely automated\nagentic workflows for software. The human developer can benefit from the input\ncondition, represented as a PBT, to generate various concrete inputs showing a\ngiven issue. Furthermore, since the PBTs are executable, our explanations are\nexecutable as well. We can thus also use the explanations in a completely\nautomated issue resolution environment for accepting or rejecting the patches\nthat are suggested by patching agents such as AutoCodeRover. Finally, as\nagentic AI approaches continue to develop, the program analysis driven\nexplanations can be provided to other LLM-based repair techniques such as\nAgentless to improve their output."}
{"id": "2507.22772", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22772", "abs": "https://arxiv.org/abs/2507.22772", "authors": ["Ahmed Sabbah", "Radi Jarrar", "Samer Zein", "David Mohaisen"], "title": "Empirical Evaluation of Concept Drift in ML-Based Android Malware Detection", "comment": "18 pages, 12 tables, 14 figures, paper under review", "summary": "Despite outstanding results, machine learning-based Android malware detection\nmodels struggle with concept drift, where rapidly evolving malware\ncharacteristics degrade model effectiveness. This study examines the impact of\nconcept drift on Android malware detection, evaluating two datasets and nine\nmachine learning and deep learning algorithms, as well as Large Language Models\n(LLMs). Various feature types--static, dynamic, hybrid, semantic, and\nimage-based--were considered. The results showed that concept drift is\nwidespread and significantly affects model performance. Factors influencing the\ndrift include feature types, data environments, and detection methods.\nBalancing algorithms helped with class imbalance but did not fully address\nconcept drift, which primarily stems from the dynamic nature of the malware\nlandscape. No strong link was found between the type of algorithm used and\nconcept drift, the impact was relatively minor compared to other variables\nsince hyperparameters were not fine-tuned, and the default algorithm\nconfigurations were used. While LLMs using few-shot learning demonstrated\npromising detection performance, they did not fully mitigate concept drift,\nhighlighting the need for further investigation."}
{"id": "2507.22442", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22442", "abs": "https://arxiv.org/abs/2507.22442", "authors": ["Yukai Zhao", "Shaohua Wang", "Jue Wang", "Xing Hu", "Xin Xia"], "title": "Ensemble Fuzzing with Dynamic Resource Scheduling and Multidimensional Seed Evaluation", "comment": "first submit", "summary": "Fuzzing is widely used for detecting bugs and vulnerabilities, with various\ntechniques proposed to enhance its effectiveness. To combine the advantages of\nmultiple technologies, researchers proposed ensemble fuzzing, which integrates\nmultiple base fuzzers. Despite promising results, state-of-the-art ensemble\nfuzzing techniques face limitations in resource scheduling and performance\nevaluation, leading to unnecessary resource waste. In this paper, we propose\nLegion, a novel ensemble fuzzing framework that dynamically schedules resources\nduring the ensemble fuzzing campaign. We designed a novel resource scheduling\nalgorithm based on the upper confidence bound algorithm to reduce the resource\nconsumption of ineffective base fuzzers. Additionally, we introduce a\nmultidimensional seed evaluation strategy, which considers multiple metrics to\nachieve more comprehensive fine-grained performance evaluation. We implemented\nLegion as a prototype tool and evaluated its effectiveness on Google's\nfuzzer-test-suite as well as real-world open-source projects. Results show that\nLegion outperforms existing state-of-the-art base fuzzers and ensemble fuzzing\ntechniques, detecting 20 vulnerabilities in real-world open-source\nprojects-five previously unknown and three classified as CVEs."}
{"id": "2507.22065", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.22065", "abs": "https://arxiv.org/abs/2507.22065", "authors": ["Xiaotao Feng", "Xiaogang Zhu", "Kun Hu", "Jincheng Wang", "Yingjie Cao", "Guang Gong", "Jianfeng Pan"], "title": "Fuzzing: Randomness? Reasoning! Efficient Directed Fuzzing via Large Language Models", "comment": null, "summary": "Fuzzing is highly effective in detecting bugs due to the key contribution of\nrandomness. However, randomness significantly reduces the efficiency of\nfuzzing, causing it to cost days or weeks to expose bugs. Even though directed\nfuzzing reduces randomness by guiding fuzzing towards target buggy locations,\nthe dilemma of randomness still challenges directed fuzzers. Two critical\ncomponents, which are seeds and mutators, contain randomness and are closely\ntied to the conditions required for triggering bugs. Therefore, to address the\nchallenge of randomness, we propose to use large language models (LLMs) to\nremove the randomness in seeds and reduce the randomness in mutators. With\ntheir strong reasoning and code generation capabilities, LLMs can be used to\ngenerate reachable seeds that target pre-determined locations and to construct\nbug-specific mutators tailored for specific bugs. We propose RandLuzz, which\nintegrates LLMs and directed fuzzing, to improve the quality of seeds and\nmutators, resulting in efficient bug exposure. RandLuzz analyzes function call\nchain or functionality to guide LLMs in generating reachable seeds. To\nconstruct bug-specific mutators, RandLuzz uses LLMs to perform bug analysis,\nobtaining information such as bug causes and mutation suggestions, which\nfurther help generate code that performs bug-specific mutations. We evaluate\nRandLuzz by comparing it with four state-of-the-art directed fuzzers, AFLGo,\nBeacon, WindRanger, and SelectFuzz. With RandLuzz-generated seeds, the fuzzers\nachieve an average speedup ranging from 2.1$\\times$ to 4.8$\\times$ compared to\nusing widely-used initial seeds. Additionally, when evaluated on individual\nbugs, RandLuzz achieves up to a 2.7$\\times$ speedup compared to the\nsecond-fastest exposure. On 8 bugs, RandLuzz can even expose them within 60\nseconds."}
{"id": "2507.22538", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22538", "abs": "https://arxiv.org/abs/2507.22538", "authors": ["Matilde Gargiani", "Robin Sieber", "Philip Pawlowsky", "John Lygeros"], "title": "Inside madupite: Technical Design and Performance", "comment": null, "summary": "In this work, we introduce and benchmark madupite, a newly proposed\nhigh-performance solver designed for large-scale discounted infinite-horizon\nMarkov decision processes with finite state and action spaces. After a brief\noverview of the class of mathematical optimization methods on which madupite\nrelies, we provide details on implementation choices, technical design and\ndeployment. We then demonstrate its scalability and efficiency by showcasing\nits performance on the solution of Markov decision processes arising from\ndifferent application areas, including epidemiology and classical control.\nMadupite sets a new standard as, to the best of our knowledge, it is the only\nsolver capable of efficiently computing exact solutions for large-scale Markov\ndecision processes, even when these exceed the memory capacity of modern\nlaptops and operate in near-undiscounted settings. This is possible as madupite\ncan work in a fully distributed manner and therefore leverage the memory\nstorage and computation capabilities of modern high-performance computing\nclusters. This key feature enables the solver to efficiently handle problems of\nmedium to large size in an exact manner instead of necessarily resorting to\nfunction approximations. Moreover, madupite is unique in allowing users to\ncustomize the solution algorithm to better exploit the specific structure of\ntheir problem, significantly accelerating convergence especially in\nlarge-discount factor settings. Overall, madupite represents a significant\nadvancement, offering unmatched scalability and flexibility in solving\nlarge-scale Markov decision processes."}
{"id": "2507.22066", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.22066", "abs": "https://arxiv.org/abs/2507.22066", "authors": ["Dylan Manuel", "Paul Rad"], "title": "CodableLLM: Automating Decompiled and Source Code Mapping for LLM Dataset Generation", "comment": null, "summary": "The generation of large, high-quality datasets for code understanding and\ngeneration remains a significant challenge, particularly when aligning\ndecompiled binaries with their original source code. To address this, we\npresent CodableLLM, a Python framework designed to automate the creation and\ncuration of datasets by mapping decompiled functions to their corresponding\nsource functions. This process enhances the alignment between decompiled and\nsource code representations, facilitating the development of large language\nmodels (LLMs) capable of understanding and generating code across multiple\nabstraction levels. CodableLLM supports multiple programming languages and\nintegrates with existing decompilers and parsers to streamline dataset\ngeneration. This paper presents the design and implementation of CodableLLM,\nevaluates its performance in dataset creation, and compares it to existing\ntools in the field. The results demonstrate that CodableLLM offers a robust and\nefficient solution for generating datasets tailored for code-focused LLMS."}
{"id": "2507.22580", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22580", "abs": "https://arxiv.org/abs/2507.22580", "authors": ["Marcos Fuster-Pena", "David de-Fitero-Dominguez", "Antonio Garcia-Cabot", "Eva Garcia-Lopez"], "title": "RePaCA: Leveraging Reasoning Large Language Models for Static Automated Patch Correctness Assessment", "comment": null, "summary": "Automated Program Repair (APR) seeks to automatically correct software bugs\nwithout requiring human intervention. However, existing tools tend to generate\npatches that satisfy test cases without fixing the underlying bug, those are\nknown as overfitting patches. To address this issue, Automated Patch\nCorrectness Assessment (APCA) attempts to identify overfitting patches\ngenerated by APR tools. It can be solved as a static approach, meaning that no\nadditional information is needed beyond the original and fixed code snippets.\nCurrent static techniques often struggle with reliability, flexibility and\ntransparency. To address these issues, we introduce RePaCA, a novel static APCA\ntechnique that leverages Large Language Models (LLMs) specialized in thinking\ntasks. Our model is prompted with both buggy and fixed code snippets and guided\nto generate a Chain of Thought that analyses code differences, reasons about\nhow the patch addresses the root cause, and ultimately provides a binary\nclassification: correct or overfitting. To enhance these reasoning capabilities\nfor the APCA task specifically, the LLM is finetuned using Reinforcement\nLearning with the Group Relative Policy Optimization algorithm. When evaluated\non a standard Defects4J-derived test, our approach achieves state-of-the-art\nperformance, with 83.1% accuracy and an 84.8% F1-score. Furthermore, our model\ndemonstrates superior generalization capabilities when trained on different\ndatasets, outperforming the leading technique. This reasoning capability also\nprovides enhanced explainability for the patch assessment. These findings\nunderscore the considerable promise of finetuned, reasoning LLMs to advance\nstatic APCA by enhancing accuracy, generalization, and explainability."}
{"id": "2507.22610", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22610", "abs": "https://arxiv.org/abs/2507.22610", "authors": ["Ali Asgari", "Milan de Koning", "Pouria Derakhshanfar", "Annibale Panichella"], "title": "Metamorphic Testing of Deep Code Models: A Systematic Literature Review", "comment": null, "summary": "Large language models and deep learning models designed for code intelligence\nhave revolutionized the software engineering field due to their ability to\nperform various code-related tasks. These models can process source code and\nsoftware artifacts with high accuracy in tasks such as code completion, defect\ndetection, and code summarization; therefore, they can potentially become an\nintegral part of modern software engineering practices. Despite these\ncapabilities, robustness remains a critical quality attribute for deep-code\nmodels as they may produce different results under varied and adversarial\nconditions (e.g., variable renaming). Metamorphic testing has become a widely\nused approach to evaluate models' robustness by applying semantic-preserving\ntransformations to input programs and analyzing the stability of model outputs.\nWhile prior research has explored testing deep learning models, this systematic\nliterature review focuses specifically on metamorphic testing for deep code\nmodels. By studying 45 primary papers, we analyze the transformations,\ntechniques, and evaluation methods used to assess robustness. Our review\nsummarizes the current landscape, identifying frequently evaluated models,\nprogramming tasks, datasets, target languages, and evaluation metrics, and\nhighlights key challenges and future directions for advancing the field."}
{"id": "2507.22659", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22659", "abs": "https://arxiv.org/abs/2507.22659", "authors": ["Sabrina Kaniewski", "Fabian Schmidt", "Markus Enzweiler", "Michael Menth", "Tobias Heer"], "title": "A Systematic Literature Review on Detecting Software Vulnerabilities with Large Language Models", "comment": "36 pages + 17 pages references, 6 tables, 10 figures", "summary": "The increasing adoption of Large Language Models (LLMs) in software\nengineering has sparked interest in their use for software vulnerability\ndetection. However, the rapid development of this field has resulted in a\nfragmented research landscape, with diverse studies that are difficult to\ncompare due to differences in, e.g., system designs and dataset usage. This\nfragmentation makes it difficult to obtain a clear overview of the\nstate-of-the-art or compare and categorize studies meaningfully. In this work,\nwe present a comprehensive systematic literature review (SLR) of LLM-based\nsoftware vulnerability detection. We analyze 227 studies published between\nJanuary 2020 and June 2025, categorizing them by task formulation, input\nrepresentation, system architecture, and adaptation techniques. Further, we\nanalyze the datasets used, including their characteristics, vulnerability\ncoverage, and diversity. We present a fine-grained taxonomy of vulnerability\ndetection approaches, identify key limitations, and outline actionable future\nresearch opportunities. By providing a structured overview of the field, this\nreview improves transparency and serves as a practical guide for researchers\nand practitioners aiming to conduct more comparable and reproducible research.\nWe publicly release all artifacts and maintain a living repository of LLM-based\nsoftware vulnerability detection studies."}
{"id": "2507.22664", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22664", "abs": "https://arxiv.org/abs/2507.22664", "authors": ["Mashal Afzal Memon", "Gianluca Filippone", "Gian Luca Scoccia", "Marco Autili", "Paola Inverardi"], "title": "RobEthiChor: Automated Context-aware Ethics-based Negotiation for Autonomous Robots", "comment": null, "summary": "The presence of autonomous systems is growing at a fast pace and it is\nimpacting many aspects of our lives. Designed to learn and act independently,\nthese systems operate and perform decision-making without human intervention.\nHowever, they lack the ability to incorporate users' ethical preferences, which\nare unique for each individual in society and are required to personalize the\ndecision-making processes. This reduces user trust and prevents autonomous\nsystems from behaving according to the moral beliefs of their end-users. When\nmultiple systems interact with differing ethical preferences, they must\nnegotiate to reach an agreement that satisfies the ethical beliefs of all the\nparties involved and adjust their behavior consequently. To address this\nchallenge, this paper proposes RobEthiChor, an approach that enables autonomous\nsystems to incorporate user ethical preferences and contextual factors into\ntheir decision-making through ethics-based negotiation. RobEthiChor features a\ndomain-agnostic reference architecture for designing autonomous systems capable\nof ethic-based negotiating. The paper also presents RobEthiChor-Ros, an\nimplementation of RobEthiChor within the Robot Operating System (ROS), which\ncan be deployed on robots to provide them with ethics-based negotiation\ncapabilities. To evaluate our approach, we deployed RobEthiChor-Ros on real\nrobots and ran scenarios where a pair of robots negotiate upon resource\ncontention. Experimental results demonstrate the feasibility and effectiveness\nof the system in realizing ethics-based negotiation. RobEthiChor allowed robots\nto reach an agreement in more than 73\\% of the scenarios with an acceptable\nnegotiation time (0.67s on average). Experiments also demonstrate that the\nnegotiation approach implemented in RobEthiChor is scalable."}
{"id": "2507.22800", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22800", "abs": "https://arxiv.org/abs/2507.22800", "authors": ["Rui Ren"], "title": "The Multi-Agent Fault Localization System Based on Monte Carlo Tree Search Approach", "comment": null, "summary": "In real-world scenarios, due to the highly decoupled and flexible nature of\nmicroservices, it poses greater challenges to system reliability. The more\nfrequent occurrence of incidents has created a demand for Root Cause\nAnalysis(RCA) methods that enable rapid identification and recovery of\nincidents. Large language model (LLM) provides a new path for quickly locating\nand recovering from incidents by leveraging their powerful generalization\nability combined with expert experience. Current LLM for RCA frameworks are\nbased on ideas like ReAct and Chain-of-Thought, but the hallucination of LLM\nand the propagation nature of anomalies often lead to incorrect localization\nresults. Moreover, the massive amount of anomalous information generated in\nlarge, complex systems presents a huge challenge for the context window length\nof LLMs. To address these challenges, we propose KnowledgeMind, an innovative\nLLM multi-agent system based on Monte Carlo Tree Search and a knowledge base\nreward mechanism for standardized service-by-service reasoning. Compared to\nState-Of-The-Art(SOTA) LLM for RCA methods, our service-by-service exploration\napproach significantly reduces the burden on the maximum context window length,\nrequiring only one-tenth of its size. Additionally, by incorporating a\nrule-based real-time reward mechanism, our method effectively mitigates\nhallucinations during the inference process. Compared to the SOTA LLM for RCA\nframework, our method achieves a 49.29% to 128.35% improvement in root cause\nlocalization accuracy."}
{"id": "2507.22853", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22853", "abs": "https://arxiv.org/abs/2507.22853", "authors": ["Haichuan Hu", "Xiaochen Xie", "Quanjun Zhang"], "title": "Repair-R1: Better Test Before Repair", "comment": null, "summary": "APR (Automated Program Repair) aims to automatically locate program defects,\ngenerate patches and validate the repairs. Existing techniques for APR are\noften combined with LLMs (Large Language Models), which leverages the\ncode-related knowledge of LLMs to improve repair effectiveness. Current\nLLM-based APR methods typically utilize test cases only during the inference\nstage, adopting an iterative approach that performs repair first and validates\nit through test execution afterward. This conventional paradigm neglects two\nimportant aspects: the potential contribution of test cases in the training\nphase, and the possibility of leveraging testing prior to repair. To address\nthis, we propose Repair-R1, which introduces test cases into the model's\ntraining phase and shifts test generation to precede repair. The model is\nrequired to first generate discriminative test cases that can distinguish\ndefective behaviors, and then perform repair based on these tests. This enables\nthe model to better locate defects and understand the underlying causes of\ndefects, thereby improving repair effectiveness. We implement Repair-R1 with\nthree different backbone models, using RL (reinforcement learning) to\nco-optimize test generation and bug repair. Experimental results on four widely\nadopted benchmarks demonstrate the superiority of Repair-R1. Specially,\ncompared to vanilla models, Repair-R1 improves repair success rate by 2.68\\% to\n48.29\\%, test generation success rate by 16.38\\% to 53.28\\%, and test coverage\nby 0.78\\% to 53.96\\%. We publish the code and weights at\nhttps://github.com/Tomsawyerhu/APR-RL and\nhttps://huggingface.co/tomhu/Qwen3-4B-RL-5000-step."}
{"id": "2507.22871", "categories": ["cs.SE", "cs.DL", "D.2.13"], "pdf": "https://arxiv.org/pdf/2507.22871", "abs": "https://arxiv.org/abs/2507.22871", "authors": ["Domhnall Carlin", "Austen Rainer"], "title": "Tracking research software outputs in the UK", "comment": null, "summary": "Research software is crucial in the research process and the growth of Open\nScience underscores the importance of accessing research artifacts, like data\nand code, raising traceability challenges among outputs. While it is a clear\nprinciple that research code, along with other essential outputs, should be\nrecognised as artifacts of the research process, the how of this principle\nremains variable. This study examines where UK academic institutions store and\nregister software as a unique research output, searching the UKRI's Gateway to\nResearch (GtR) metadata for publicly funded research software in the UK. The\nquantity of software reported as research outcomes remains low in proportion to\nother categories. Artifact sharing appears low, with one-quarter of the\nreported software having no links and 45% having either a missing or erroneous\nURL. Of the valid URLs, we find the single largest category is Public\nCommercial Code Repository, with GitHub being the host of 18% of all publicly\nfunded research software listed. These observations are contrasted with past\nfindings from 2023 and finally, we discuss the lack of artifact sharing in UK\nresearch, with resulting implications for the maintenance and evolution of\nresearch software. Without dissemination, research software risks demotion to a\ntransient artifact, useful only to meet short term research demands but\nultimately lost to the broader enterprise of science."}
{"id": "2507.22371", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22371", "abs": "https://arxiv.org/abs/2507.22371", "authors": ["Lei Yu", "Shiqi Cheng", "Zhirong Huang", "Jingyuan Zhang", "Chenjie Shen", "Junyi Lu", "Li Yang", "Fengjun Zhang", "Jiajia Ma"], "title": "SAEL: Leveraging Large Language Models with Adaptive Mixture-of-Experts for Smart Contract Vulnerability Detection", "comment": "Accepted to ICSME 2025", "summary": "With the increasing security issues in blockchain, smart contract\nvulnerability detection has become a research focus. Existing vulnerability\ndetection methods have their limitations: 1) Static analysis methods struggle\nwith complex scenarios. 2) Methods based on specialized pre-trained models\nperform well on specific datasets but have limited generalization capabilities.\nIn contrast, general-purpose Large Language Models (LLMs) demonstrate\nimpressive ability in adapting to new vulnerability patterns. However, they\noften underperform on specific vulnerability types compared to methods based on\nspecialized pre-trained models. We also observe that explanations generated by\ngeneral-purpose LLMs can provide fine-grained code understanding information,\ncontributing to improved detection performance.\n  Inspired by these observations, we propose SAEL, an LLM-based framework for\nsmart contract vulnerability detection. We first design targeted prompts to\nguide LLMs in identifying vulnerabilities and generating explanations, which\nserve as prediction features. Next, we apply prompt-tuning on CodeT5 and T5 to\nprocess contract code and explanations, enhancing task-specific performance. To\ncombine the strengths of each approach, we introduce an Adaptive\nMixture-of-Experts architecture. This dynamically adjusts feature weights via a\nGating Network, which selects relevant features using TopK filtering and\nSoftmax normalization, and incorporates a Multi-Head Self-Attention mechanism\nto enhance cross-feature relationships. This design enables effective\nintegration of LLM predictions, explanation features, and code features through\ngradient optimization. The loss function jointly considers both independent\nfeature performance and overall weighted predictions. Experiments show that\nSAEL outperforms existing methods across various vulnerabilities."}
