{"id": "2508.10017", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10017", "abs": "https://arxiv.org/abs/2508.10017", "authors": ["Rodrigo Tertulino"], "title": "A Robust Pipeline for Differentially Private Federated Learning on Imbalanced Clinical Data using SMOTETomek and FedProx", "comment": "This is being prepared to be submitted to the Journal of the\n  Brazilian Computer Society (JBCS), which is still under construction", "summary": "Federated Learning (FL) presents a groundbreaking approach for collaborative\nhealth research, allowing model training on decentralized data while\nsafeguarding patient privacy. FL offers formal security guarantees when\ncombined with Differential Privacy (DP). The integration of these technologies,\nhowever, introduces a significant trade-off between privacy and clinical\nutility, a challenge further complicated by the severe class imbalance often\npresent in medical datasets. The research presented herein addresses these\ninterconnected issues through a systematic, multi-stage analysis. An FL\nframework was implemented for cardiovascular risk prediction, where initial\nexperiments showed that standard methods struggled with imbalanced data,\nresulting in a recall of zero. To overcome such a limitation, we first\nintegrated the hybrid Synthetic Minority Over-sampling Technique with Tomek\nLinks (SMOTETomek) at the client level, successfully developing a clinically\nuseful model. Subsequently, the framework was optimized for non-IID data using\na tuned FedProx algorithm. Our final results reveal a clear, non-linear\ntrade-off between the privacy budget (epsilon) and model recall, with the\noptimized FedProx consistently out-performing standard FedAvg. An optimal\noperational region was identified on the privacy-utility frontier, where strong\nprivacy guarantees (with epsilon 9.0) can be achieved while maintaining high\nclinical utility (recall greater than 77%). Ultimately, our study provides a\npractical methodological blueprint for creating effective, secure, and accurate\ndiagnostic tools that can be applied to real-world, heterogeneous healthcare\ndata.", "AI": {"tldr": "This paper proposes an optimized Federated Learning (FL) framework with Differential Privacy (DP) for cardiovascular risk prediction, addressing privacy-utility trade-offs and class imbalance in medical datasets. Key results show FedProx optimization achieves 77% recall while maintaining strong privacy (epsilon=9.0).", "motivation": "Existing FL methods struggle with class imbalance in medical datasets and the inherent privacy-utility trade-off when combining FL with DP. These challenges threaten both diagnostic accuracy and patient privacy in decentralized healthcare research.", "method": "1. Implemented FL for cardiovascular risk prediction\n2. Applied SMOTETomek at client level to handle class imbalance\n3. Enhanced FedProx algorithm for non-IID medical data\n4. Quantified privacy-utility trade-off through epsilon-recall analysis", "result": "Non-linear privacy-utility trade-off confirmed with\n- FedProx outperforming FedAvg significantly\n- Recall >77% achieved with DP privacy budget \u03b5=9.0\n- SMOTE+Tomek links resolved zero-recall issue\n- Operational region identified for strong privacy and clinical utility", "conclusion": "The study establishes a methodological blueprint for FL+DP in healthcare, demonstrating that optimized algorithms (FedProx with SMOTE) can achieve high diagnostic accuracy (77% recall) with rigorous privacy guarantees (\u03b5=9.0), making it feasible for real-world heterogeneous medical applications."}}
{"id": "2508.10059", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10059", "abs": "https://arxiv.org/abs/2508.10059", "authors": ["Yueke Zhang", "Yifan Zhang", "Kevin Leach", "Yu Huang"], "title": "FormalGrad: Integrating Formal Methods with Gradient-Based LLM Refinement", "comment": "6 Pages", "summary": "While Large Language Models (LLMs) have demonstrated remarkable capabilities\nin code generation, they often produce solutions that lack guarantees of\ncorrectness, robustness, and efficiency. The limitation is acute in domains\nrequiring strict constraints. FormalGrad introduces a principled framework that\nintegrates formal methods directly into an iterative LLM-based generation loop.\nIt uniquely treats code as a differentiable variable, converting structured\nfeedback and formal constraints into a textual pseudo-gradient. This gradient\nguides the model to iteratively refine solutions, ensuring they are not only\nfunctional but also robust and formally justified. We evaluate FormalGrad on\nthe HumanEval, HumanEval+, and LiveCodeBench benchmarks. Our implementation\noutperforms strong baselines, achieving an absolute improvement of up to 27% on\nHumanEval and a 41% relative improvement on the challenging LiveCodeBench V6.\nFormalGrad generates formally justified code that is robust and efficient,\npaving the way for reliable AI-assisted software development in high-stakes\napplications.", "AI": {"tldr": "FormalGrad integrates formal methods into an iterative LLM-based code generation loop, using structured feedback and formal constraints to produce robust, correct, and efficient code through pseudo-gradients. It outperforms baselines by up to 27% on HumanEval and 41% on LiveCodeBench V6.", "motivation": "LLMs generate code that lacks correctness, robustness, and efficiency, especially in domains requiring strict constraints. Traditional LLM-based solutions cannot guarantee compliance with formal requirements.", "method": "FormalGrad treats code as a differentiable variable, converting structured feedback and formal constraints into a textual pseudo-gradient that iteratively refines code generation. This framework enables direct integration of formal methods (e.g., constraints, verification logs) into the optimization loop via LLM feedback.", "result": "Achieved up to 27% absolute improvement on HumanEval and 41% relative improvement on LiveCodeBench V6. Generated code is formally justified, robust, and efficient across multiple benchmarks.", "conclusion": "FormalGrad demonstrates reliable AI-assisted software development by ensuring correctness and robustness through formal integration. It establishes a framework for handling strict constraints in code generation tasks, advancing safe LLM applications in critical domains."}}
{"id": "2508.10023", "categories": ["cs.CR", "quant-ph", "94A60 (Cryptography)"], "pdf": "https://arxiv.org/pdf/2508.10023", "abs": "https://arxiv.org/abs/2508.10023", "authors": ["Samet \u00dcnsal"], "title": "A Comparative Performance Evaluation of Kyber, sntrup761, and FrodoKEM for Post-Quantum Cryptography", "comment": "12 pages, 3 tables, IEEE conference format", "summary": "Post-quantum cryptography (PQC) aims to develop cryptographic algorithms that\nare secure against attacks from quantum computers. This paper compares the\nleading postquantum cryptographic algorithms, such as Kyber, sntrup761, and\nFrodoKEM, in terms of their security, performance, and real-world\napplicability. The review highlights the strengths and weaknesses of each\nalgorithm and provides insights into future research directions. We also\ndiscuss the challenges of transitioning from classical to post-quantum systems\nand the potential impacts on various industries. This paper serves as a\nfoundation for understanding the current state of post-quantum cryptography and\nits future prospects in the quantum computing era.", "AI": {"tldr": "This paper compares leading post-quantum cryptographic algorithms (Kyber, sntrup761, FrodoKEM) in terms of security, performance, and real-world applicability, while discussing challenges in transitioning to post-quantum systems.", "motivation": "Post-quantum cryptography is needed to ensure security against quantum computing attacks, prompting a need to evaluate and compare emerging algorithms for practical adoption.", "method": "The authors conduct a comparative review of post-quantum cryptographic algorithms, analyzing their security, performance, and real-world applicability through theoretical insights and practical considerations.", "result": "The paper identifies strengths and weaknesses of Kyber, sntrup761, and FrodoKEM, providing actionable insights for their implementation and highlighting industry-specific transition challenges.", "conclusion": "The review establishes a foundation for understanding the current state of post-quantum cryptography, emphasizes research directions for improving algorithms, and underscores the urgency of addressing transition challenges to prepare for the quantum computing era."}}
{"id": "2508.10068", "categories": ["cs.SE", "cs.CL", "cs.IR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.10068", "abs": "https://arxiv.org/abs/2508.10068", "authors": ["Xiaohan Chen", "Zhongying Pan", "Quan Feng", "Yu Tian", "Shuqun Yang", "Mengru Wang", "Lina Gong", "Yuxia Geng", "Piji Li", "Xiang Chen"], "title": "SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion", "comment": null, "summary": "Retrieval-augmented generation (RAG) for repository-level code completion\ncommonly relies on superficial text similarity, leading to results plagued by\nsemantic misguidance, redundancy, and homogeneity, while also failing to\nresolve external symbol ambiguity. To address these challenges, we introduce\nSaracoder, a Hierarchical Feature-Optimized retrieval framework. Its core\nHierarchical Feature Optimization module systematically refines candidates by\ndistilling deep semantic relationships, pruning exact duplicates, assessing\nstructural similarity with a novel graph-based metric that weighs edits by\ntheir topological importance, and reranking results to maximize both relevance\nand diversity. Furthermore, an External-Aware Identifier Disambiguator module\naccurately resolves cross-file symbol ambiguity via dependency analysis.\nExtensive experiments on the challenging CrossCodeEval and RepoEval-Updated\nbenchmarks demonstrate that Saracoder significantly outperforms existing\nbaselines across multiple programming languages and models. Our work proves\nthat systematically refining retrieval results across multiple dimensions\nprovides a new paradigm for building more accurate and robust repository-level\ncode completion systems.", "AI": {"tldr": "Saracoder, a Hierarchical Feature-Optimized retrieval framework, improves repository-level code completion by refining candidates through semantic distillation, duplicate pruning, structural similarity assessment via a graph-based metric, reranking for relevance/diversity, and resolving external symbol ambiguity with dependency analysis, significantly outperforming existing baselines in multiple languages/models.", "motivation": "Existing RAG-based repository-level code completion systems rely on superficial text similarity, causing semantic misguidance, redundancy, homogeneity, and failing to resolve external symbol ambiguity.", "method": "Saracoder employs two modules: (1) Hierarchical Feature Optimization which distills semantics, prunes duplicates, calculates topological edit-weighted structural similarity, and reranks results; (2) External-Aware Identifier Disambiguator using dependency analysis for cross-file symbol ambiguity resolution.", "result": "Extensive experiments on CrossCodeEval and RepoEval-Updated benchmarks show Saracoder outperforms baselines across multiple programming languages and models.", "conclusion": "Systematically refining retrieval through semantic, structural, and diversity dimensions establishes a new paradigm for accurate, robust repository-level code completion systems."}}
{"id": "2508.10031", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10031", "abs": "https://arxiv.org/abs/2508.10031", "authors": ["Jinhwa Kim", "Ian G. Harris"], "title": "Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs", "comment": "13 pages, 2 figures", "summary": "While Large Language Models (LLMs) have shown significant advancements in\nperformance, various jailbreak attacks have posed growing safety and ethical\nrisks. Malicious users often exploit adversarial context to deceive LLMs,\nprompting them to generate responses to harmful queries. In this study, we\npropose a new defense mechanism called Context Filtering model, an input\npre-processing method designed to filter out untrustworthy and unreliable\ncontext while identifying the primary prompts containing the real user intent\nto uncover concealed malicious intent. Given that enhancing the safety of LLMs\noften compromises their helpfulness, potentially affecting the experience of\nbenign users, our method aims to improve the safety of the LLMs while\npreserving their original performance. We evaluate the effectiveness of our\nmodel in defending against jailbreak attacks through comparative analysis,\ncomparing our approach with state-of-the-art defense mechanisms against six\ndifferent attacks and assessing the helpfulness of LLMs under these defenses.\nOur model demonstrates its ability to reduce the Attack Success Rates of\njailbreak attacks by up to 88% while maintaining the original LLMs'\nperformance, achieving state-of-the-art Safety and Helpfulness Product results.\nNotably, our model is a plug-and-play method that can be applied to all LLMs,\nincluding both white-box and black-box models, to enhance their safety without\nrequiring any fine-tuning of the models themselves. We will make our model\npublicly available for research purposes.", "AI": {"tldr": "This paper introduces a Context Filtering model as an input pre-processing method to defend against jailbreak attacks on Large Language Models (LLMs) by filtering harmful context while preserving LLMs' helpfulness, achieving up to 88% attack success rate reduction without performance trade-offs.", "motivation": "LLMs face increasing safety and ethical risks due to jailbreak attacks that exploit adversarial contexts to elicit harmful responses. Current mitigation strategies often compromise the helpfulness of LLMs for legitimate users, necessitating a defense mechanism that maintains effectiveness without sacrificing model utility.", "method": "The Context Filtering model employs input pre-processing to: 1) identify trustworthy primary prompts containing true user intent, and 2) remove untrustworthy/reliable context. It is model-agnostic (works with both white-box and black-box LLMs) and requires no fine-tuning of the LLM itself.", "result": "Evaluations across six jailbreak attack types show the method reduces attack success rates by 88% while maintaining baseline model helpfulness, setting new state-of-the-art results in Safety and Helpfulness Product metrics.", "conclusion": "The proposed Context Filtering model demonstrates universal applicability (plug-and-play) and strong performance in defending against jailbreak attacks without performance degradation, with plans for open-source release to enable further research and adoption."}}
{"id": "2508.10074", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10074", "abs": "https://arxiv.org/abs/2508.10074", "authors": ["Ruofan Lu", "Yintong Huo", "Meng Zhang", "Yichen Li", "Michael R. Lyu"], "title": "Next Edit Prediction: Learning to Predict Code Edits from Context and Interaction History", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has led to the\nwidespread adoption of AI-powered coding assistants integrated into a\ndevelopment environment. On one hand, low-latency code completion offers\ncompletion suggestions but is fundamentally constrained to the cursor's current\nposition. On the other hand, chat-based editing can perform complex\nmodifications, yet forces developers to stop their work, describe the intent in\nnatural language, which causes a context-switch away from the code. This\ncreates a suboptimal user experience, as neither paradigm proactively predicts\nthe developer's next edit in a sequence of related edits. To bridge this gap\nand provide the seamless code edit suggestion, we introduce the task of Next\nEdit Prediction, a novel task designed to infer developer intent from recent\ninteraction history to predict both the location and content of the subsequent\nedit. Specifically, we curate a high-quality supervised fine-tuning dataset and\nan evaluation benchmark for the Next Edit Prediction task. Then, we conduct\nsupervised fine-tuning on a series of models and performed a comprehensive\nevaluation of both the fine-tuned models and other baseline models, yielding\nseveral novel findings. This work lays the foundation for a new interaction\nparadigm that proactively collaborate with developers by anticipating their\nfollowing action, rather than merely reacting to explicit instructions.", "AI": {"tldr": "This paper proposes Next Edit Prediction, a novel task to proactively anticipate developers' next code edits using historical interaction data, bridging the gap between cursor-position constrained code completion and context-switching chat-based editing. The contribution includes a dataset/benchmark and evaluations showing this paradigm enables anticipatory code collaboration.", "motivation": "Existing AI coding assistants face limitations: code completion is constrained to cursor position while chat-based editing requires explicit intent description, causing context-switching and suboptimal user experiences in sequential editing tasks.", "method": "The paper introduces the Next Edit Prediction task, curates a high-quality supervised fine-tuning dataset with evaluation benchmark, and conducts comprehensive model evaluations by fine-tuning architectures and comparing against baselines.", "result": "The proposed framework yields novel findings through experiments, demonstrating the feasibility of predicting both edit location and content from interaction history to enable proactive code editing suggestions.", "conclusion": "Next Edit Prediction establishes a foundation for a next-generation coding assistant paradigm that anticipates developer actions through sequential interaction modeling rather than merely reacting to static code or explicit natural language instructions."}}
{"id": "2508.10033", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10033", "abs": "https://arxiv.org/abs/2508.10033", "authors": ["Yuksel Aydin"], "title": "Cognitive Cybersecurity for Artificial Intelligence: Guardrail Engineering with CCS-7", "comment": null, "summary": "Language models exhibit human-like cognitive vulnerabilities, such as\nemotional framing, that escape traditional behavioral alignment. We present\nCCS-7 (Cognitive Cybersecurity Suite), a taxonomy of seven vulnerabilities\ngrounded in human cognitive security research. To establish a human benchmark,\nwe ran a randomized controlled trial with 151 participants: a \"Think First,\nVerify Always\" (TFVA) lesson improved cognitive security by +7.9% overall. We\nthen evaluated TFVA-style guardrails across 12,180 experiments on seven diverse\nlanguage model architectures. Results reveal architecture-dependent risk\npatterns: some vulnerabilities (e.g., identity confusion) are almost fully\nmitigated, while others (e.g., source interference) exhibit escalating\nbackfire, with error rates increasing by up to 135% in certain models. Humans,\nin contrast, show consistent moderate improvement. These findings reframe\ncognitive safety as a model-specific engineering problem: interventions\neffective in one architecture may fail, or actively harm, another, underscoring\nthe need for architecture-aware cognitive safety testing before deployment.", "AI": {"tldr": "This paper introduces CCS-7 (Cognitive Cybersecurity Suite), a taxonomy of seven human-like cognitive vulnerabilities in language models that traditional alignment methods fail to address. It demonstrates that architecture-dependent interventions (e.g., 'Think First, Verify Always') mitigate some vulnerabilities but exacerbate others (e.g., source interference errors increasing by 135% in certain models); meanwhile, human users show consistent moderate improvement in cognitive security practices.", "motivation": "Language models exhibit human-like cognitive vulnerabilities (e.g., emotional framing) that escape traditional behavioral alignment, raising concerns about their reliability and security in critical applications.", "method": "1) A randomized controlled trial with 151 participants testing a 'Think First, Verify Always' (TFVA) lesson for human cognitive security. 2) 12,180 experiments evaluating TFVA-style guardrails across seven diverse language model architectures.", "result": "Architecture-dependent risk patterns emerged: identity confusion was almost fully mitigated in models but source interference experienced backfire (135% error rate increase). Humans showed consistent moderate improvement (+7.9%) in cognitive security regardless of trial architecture.", "conclusion": "Cognitive safety is a model-specific engineering problem, requiring architecture-aware testing before deployment due to the significant variability in intervention effectiveness. This highlights the need for tailored cybersecurity strategies for different LLM architectures."}}
{"id": "2508.10157", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10157", "abs": "https://arxiv.org/abs/2508.10157", "authors": ["Ajibode Adekunle", "Abdul Ali Bangash", "Bram Adams", "Ahmed E. Hassan"], "title": "On the synchronization between Hugging Face pre-trained language models and their upstream GitHub repository", "comment": null, "summary": "Pretrained language models (PTLMs) have advanced natural language processing\n(NLP), enabling progress in tasks like text generation and translation. Like\nsoftware package management, PTLMs are trained using code and environment\nscripts in upstream repositories (e.g., GitHub, GH) and distributed as variants\nvia downstream platforms like Hugging Face (HF). Coordinating development\nbetween GH and HF poses challenges such as misaligned release timelines,\ninconsistent versioning, and limited reuse of PTLM variants. We conducted a\nmixed-method study of 325 PTLM families (904 HF variants) to examine how commit\nactivities are coordinated. Our analysis reveals that GH contributors typically\nmake changes related to specifying the version of the model, improving code\nquality, performance optimization, and dependency management within the\ntraining scripts, while HF contributors make changes related to improving model\ndescriptions, data set handling, and setup required for model inference.\nFurthermore, to understand the synchronization aspects of commit activities\nbetween GH and HF, we examined three dimensions of these activities -- lag\n(delay), type of synchronization, and intensity -- which together yielded eight\ndistinct synchronization patterns. The prevalence of partially synchronized\npatterns, such as Disperse synchronization and Sparse synchronization, reveals\nstructural disconnects in current cross-platform release practices. These\npatterns often result in isolated changes -- where improvements or fixes made\non one platform are never replicated on the other -- and in some cases,\nindicate an abandonment of one repository in favor of the other. Such\nfragmentation risks exposing end users to incomplete, outdated, or behaviorally\ninconsistent models. Hence, recognizing these synchronization patterns is\ncritical for improving oversight and traceability in PTLM release workflows.", "AI": {"tldr": "The paper analyzes synchronization challenges in PTLM releases across platforms (GitHub, Hugging Face) by examining 325 PTLM families and their 904 downstream variants, identifying eight distinct commit synchronization patterns. It highlights risks of fragmentation, outdated variants, and inconsistent behavior due to partial synchronization.", "motivation": "Current PTLM cross-platform coordination (e.g., GitHub \u2192 Hugging Face) faces misaligned release timelines, inconsistent versioning, and limited reuse. This study aims to reveal how commit activities are synchronized and the risks of structural disconnects in release practices.", "method": "Mixed-method analysis of 325 PTLM families (904 Hugging Face variants). Examined commit activities across GitHub and Hugging Face, focusing on three dimensions: lag (delay), type of synchronization (e.g., full/partial), and intensity. Derived eight synchronization patterns from these dimensions.", "result": "Identified platform-specific contribution foci: GitHub (versioning, code quality, performance, dependencies) vs. Hugging Face (descriptions, data handling, inference setup). Eight synchronization patterns revealed, with Disperse and Sparse patterns being prevalent, causing isolated changes, abandoned repositories, and risks of outdated/inconsistent models.", "conclusion": "Fragmented synchronization patterns (e.g., Disperse, Sparse) compromise model quality and traceability. Recognizing these patterns is essential for improving cross-platform PTLM release workflows to ensure consistency, completeness, and user trust."}}
{"id": "2508.10035", "categories": ["cs.CR", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.10035", "abs": "https://arxiv.org/abs/2508.10035", "authors": ["Varsha Sen", "Biswash Basnet"], "title": "Neural Network-Based Detection and Multi-Class Classification of FDI Attacks in Smart Grid Home Energy Systems", "comment": "17 pages, 7 figures", "summary": "False Data Injection Attacks (FDIAs) pose a significant threat to smart grid\ninfrastructures, particularly Home Area Networks (HANs), where real-time\nmonitoring and control are highly adopted. Owing to the comparatively less\nstringent security controls and widespread availability of HANs, attackers view\nthem as an attractive entry point to manipulate aggregated demand patterns,\nwhich can ultimately propagate and disrupt broader grid operations. These\nattacks undermine the integrity of smart meter data, enabling malicious actors\nto manipulate consumption values without activating conventional alarms,\nthereby creating serious vulnerabilities across both residential and\nutility-scale infrastructures. This paper presents a machine learning-based\nframework for both the detection and classification of FDIAs using residential\nenergy data. A real-time detection is provided by the lightweight Artificial\nNeural Network (ANN), which works by using the most vital features of energy\nconsumption, cost, and time context. For the classification of different attack\ntypes, a Bidirectional LSTM is trained to recognize normal, trapezoidal, and\nsigmoid attack shapes through learning sequential dependencies in the data. A\nsynthetic time-series dataset was generated to emulate realistic household\nbehaviour. Experimental results demonstrate that the proposed models are\neffective in identifying and classifying FDIAs, offering a scalable solution\nfor enhancing grid resilience at the edge. This work contributes toward\nbuilding intelligent, data-driven defence mechanisms that strengthen smart grid\ncybersecurity from residential endpoints.", "AI": {"tldr": "A machine learning framework using ANN for real-time detection and Bidirectional LSTM for classification of False Data Injection Attacks (FDIAs) in smart grid Home Area Networks (HANs).", "motivation": "FDIAs threaten smart grid HANs due to weak security controls, enabling data manipulation and grid vulnerabilities. Attackers exploit these to alter demand patterns without triggering alarms, necessitating robust detection solutions.", "method": "1) Lightweight Artificial Neural Network (ANN) for real-time FDI detection using energy consumption, cost, and time context features. 2) Bidirectional LSTM for attack classification (normal, trapezoidal, sigmoid). 3) Synthetic time-series household behavior dataset for training/evaluation.", "result": "Proposed models demonstrated effectiveness in FDI detection/classification through experiments, providing scalable edge-based grid protection solutions with strong attack shape recognition capabilities.", "conclusion": "This work advances smart grid cybersecurity by establishing intelligent data-driven defense mechanisms at residential endpoints, improving grid resilience through edge computing with dual ML model architectures."}}
{"id": "2508.10517", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10517", "abs": "https://arxiv.org/abs/2508.10517", "authors": ["Likai Ye", "Mengliang Li", "Dehai Zhao", "Jiamou Sun", "Xiaoxue Ren"], "title": "Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart Contract Compilation Error Resolution", "comment": "International Conference on Software Maintenance and Evolution\n  (ICSME) 2025", "summary": "Solidity, the dominant smart contract language for Ethereum, has rapidly\nevolved with frequent version updates to enhance security, functionality, and\ndeveloper experience. However, these continual changes introduce significant\nchallenges, particularly in compilation errors, code migration, and\nmaintenance. Therefore, we conduct an empirical study to investigate the\nchallenges in the Solidity version evolution and reveal that 81.68% of examined\ncontracts encounter errors when compiled across different versions, with 86.92%\nof compilation errors.\n  To mitigate these challenges, we conducted a systematic evaluation of large\nlanguage models (LLMs) for resolving Solidity compilation errors during version\nmigrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek)\nand closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these\nmodels exhibit error repair capabilities, their effectiveness diminishes\nsignificantly for semantic-level issues and shows strong dependency on prompt\nengineering strategies. This underscores the critical need for domain-specific\nadaptation in developing reliable LLM-based repair systems for smart contracts.\n  Building upon these insights, we introduce SMCFIXER, a novel framework that\nsystematically integrates expert knowledge retrieval with LLM-based repair\nmechanisms for Solidity compilation error resolution. The architecture\ncomprises three core phases: (1) context-aware code slicing that extracts\nrelevant error information; (2) expert knowledge retrieval from official\ndocumentation; and (3) iterative patch generation for Solidity migration.\nExperimental validation across Solidity version migrations demonstrates our\napproach's statistically significant 24.24% improvement over baseline GPT-4o on\nreal-world datasets, achieving near-perfect 96.97% accuracy.", "AI": {"tldr": "This paper examines challenges in Solidity version evolution (81.68% contracts fail cross-version compilation), evaluates LLM effectiveness in error repair (86.92% errors), and proposes SMCFIXER - a domain-adapted framework achieving 96.97% accuracy with 24.24% improvement over GPT-4o.", "motivation": "Solidity's rapid evolution causes severe compilation, migration, and maintenance challenges (81.68% contracts fail cross-version compilation, 86.92% compilation errors). LLMs' success in conventional programming suggests potential for smart contract error repair, but their effectiveness for deep semantic issues is unexplored.", "method": "1. Empirical analysis of 400+ Solidity contracts across version migrations\n2. Systematic evaluation of open-source (LLaMA3, DeepSeek) and closed-source (GPT-4o, GPT-3.5-turbo) LLMs for error resolution\n3. Development of SMCFIXER framework: (1) context-aware code slicing, (2) expert knowledge retrieval from Ethereum docs, (3) iterative patch generation with version-aware tokenization and semantic validation", "result": "LLMs show limited cross-version error repair capability (13.08% failure rate). SMCFIXER achieves 96.97% accuracy and 24.24% improvement over GPT-4o baseline in controlled experiments. Framework demonstrates robustness to both syntactic/semantic errors across version transitions.", "conclusion": "Version evolution is critical problem in smart contract development; vanilla LLMs insufficient for reliable error repair. Domain-specific adaptation through SMCFIXER's knowledge integration shows promise for building practical systems. Future work includes automated prompt engineering and semantic correctness verification for smart contracts."}}
{"id": "2508.10038", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10038", "abs": "https://arxiv.org/abs/2508.10038", "authors": ["Pierre-Francois Gimenez", "Sarath Sivaprasad", "Mario Fritz"], "title": "Certifiably robust malware detectors by design", "comment": null, "summary": "Malware analysis involves analyzing suspicious software to detect malicious\npayloads. Static malware analysis, which does not require software execution,\nrelies increasingly on machine learning techniques to achieve scalability.\nAlthough such techniques obtain very high detection accuracy, they can be\neasily evaded with adversarial examples where a few modifications of the sample\ncan dupe the detector without modifying the behavior of the software. Unlike\nother domains, such as computer vision, creating an adversarial example of\nmalware without altering its functionality requires specific transformations.\nWe propose a new model architecture for certifiably robust malware detection by\ndesign. In addition, we show that every robust detector can be decomposed into\na specific structure, which can be applied to learn empirically robust malware\ndetectors, even on fragile features. Our framework ERDALT is based on this\nstructure. We compare and validate these approaches with machine-learning-based\nmalware detection methods, allowing for robust detection with limited reduction\nof detection performance.", "AI": {"tldr": "This paper introduces ERDALT, a robust malware detection framework using a novel architecture that resists adversarial examples with minimal impact on detection accuracy.", "motivation": "Static malware analysis via machine learning lacks robustness against adversarial examples, where minor code changes evade detectors without altering malicious behavior, unlike robustness in computer vision.", "method": "Proposes a certifiably robust model structure for malware detection, decomposing robust detectors into a specific architecture that inherently prevents evasion through adversarial transformations.", "result": "ERDALT achieves robust detection against adversarial examples while maintaining competitiveness with existing machine learning methods in terms of accuracy.", "conclusion": "The designed architecture enables intrinsically robust static malware analysis, offering a defense framework that preserves high detection performance even with adversarial attacks."}}
{"id": "2508.10852", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10852", "abs": "https://arxiv.org/abs/2508.10852", "authors": ["Souhaila Serbout", "Diana Carolina Mu\u00f1oz Hurtado", "Hassan Atwi", "Edoardo Riggio", "Cesare Pautasso"], "title": "EVOSCAT: Exploring Software Change Dynamics in Large-Scale Historical Datasets", "comment": "Submitted to VISSOFT 2025. For the hi-resolution version of the\n  paper, see https://design.inf.usi.ch/publications/2025/vissoft", "summary": "Long lived software projects encompass a large number of artifacts, which\nundergo many revisions throughout their history. Empirical software engineering\nresearchers studying software evolution gather and collect datasets with\nmillions of events, representing changes introduced to specific artifacts. In\nthis paper, we propose EvoScat, a tool that attempts addressing temporal\nscalability through the usage of interactive density scatterplot to provide a\nglobal overview of large historical datasets mined from open source\nrepositories in a single visualization. EvoScat intents to provide researchers\nwith a mean to produce scalable visualizations that can help them explore and\ncharacterize evolution datasets, as well as comparing the histories of\nindividual artifacts, both in terms of 1) observing how rapidly different\nartifacts age over multiple-year-long time spans 2) how often metrics\nassociated with each artifacts tend towards an improvement or worsening. The\npaper shows how the tool can be tailored to specific analysis needs (pace of\nchange comparison, clone detection, freshness assessment) thanks to its support\nfor flexible configuration of history scaling and alignment along the time\naxis, artifacts sorting and interactive color mapping, enabling the analysis of\nmillions of events obtained by mining the histories of tens of thousands of\nsoftware artifacts. We include in this paper a gallery showcasing datasets\ngathering specific artifacts (OpenAPI descriptions, GitHub workflow\ndefinitions) across multiple repositories, as well as diving into the history\nof specific popular open source projects.", "AI": {"tldr": "EvoScat is a scalable visualization tool for analyzing large software evolution datasets through interactive density scatterplots, enabling comparison of artifact aging and metric trends across open source projects.", "motivation": "Long-lived software projects generate massive historical datasets with millions of events, creating a need for researchers to study patterns in artifact evolution, compare aging rates, and track metric improvements/degradations over extended periods.", "method": "The method employs interactive density scatterplots with flexible configuration options: history scaling/alignment along time axis, artifact sorting, and interactive color mapping to adapt to specific analysis tasks like change pace comparison and freshness assessment.", "result": "EvoScat processes millions of events from tens of thousands of artifacts, demonstrated through analysis examples on OpenAPI descriptions, GitHub workflows, and popular open source project histories, showing its effectiveness in scalable visualization.", "conclusion": "EvoScat successfully addresses temporal scalability challenges in software evolution research, offering researchers a tailored visualization approach to analyze and compare artifact histories in large-scale data contexts."}}
{"id": "2508.10039", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10039", "abs": "https://arxiv.org/abs/2508.10039", "authors": ["Wenqiang Wang", "Yan Xiao", "Hao Lin", "Yangshijie Zhang", "Xiaochun Cao"], "title": "Multi-task Adversarial Attacks against Black-box Model with Few-shot Queries", "comment": null, "summary": "Current multi-task adversarial text attacks rely on abundant access to shared\ninternal features and numerous queries, often limited to a single task type. As\na result, these attacks are less effective against practical scenarios\ninvolving black-box feedback APIs, limited queries, or multiple task types. To\nbridge this gap, we propose \\textbf{C}luster and \\textbf{E}nsemble\n\\textbf{M}ulti-task Text Adversarial \\textbf{A}ttack (\\textbf{CEMA}), an\neffective black-box attack that exploits the transferability of adversarial\ntexts across different tasks. CEMA simplifies complex multi-task scenarios by\nusing a \\textit{deep-level substitute model} trained in a\n\\textit{plug-and-play} manner for text classification, enabling attacks without\nmimicking the victim model. This approach requires only a few queries for\ntraining, converting multi-task attacks into classification attacks and\nallowing attacks across various tasks.\n  CEMA generates multiple adversarial candidates using different text\nclassification methods and selects the one that most effectively attacks\nsubstitute models.\n  In experiments involving multi-task models with two, three, or six\ntasks--spanning classification, translation, summarization, and text-to-image\ngeneration--CEMA demonstrates significant attack success with as few as 100\nqueries. Furthermore, CEMA can target commercial APIs (e.g., Baidu and Google\nTranslate), large language models (e.g., ChatGPT 4o), and image-generation\nmodels (e.g., Stable Diffusion V2), showcasing its versatility and\neffectiveness in real-world applications.", "AI": {"tldr": "CEMA is a black-box adversarial text attack method that leverages cross-task transferability to efficiently compromise multi-task models, commercial APIs, and image-generation systems with minimal queries.", "motivation": "Existing multi-task adversarial text attacks require access to shared internal features and many queries, making them ineffective for real-world black-box scenarios with limited API access or diverse task types.", "method": "CEMA uses a deep-level substitute model trained in a plug-and-play manner for text classification, enabling attacks without replicating the victim model. It generates adversarial candidates via multiple classification methods and selects the most effective one.", "result": "CEMA achieved significant attack success on multi-task models with 2-6 tasks (classification, translation, summarization, image generation) using just 100 queries. It also worked against commercial APIs (Baidu, Google Translate) and models like ChatGPT 4o and Stable Diffusion V2.", "conclusion": "CEMA demonstrates practical versatility for real-world adversarial attacks across diverse multi-task systems, improving efficiency and query usage in constrained black-box environments."}}
{"id": "2508.10041", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.10041", "abs": "https://arxiv.org/abs/2508.10041", "authors": ["Julien Mellaerts"], "title": "Quantum Prime Factorization: A Novel Approach Based on Fermat Method", "comment": null, "summary": "In this paper, we introduce a novel quantum algorithm for the factorization\nof composite odd numbers. This work makes two significant contributions. First,\nwe present a new improvement to the classical Fermat method, fourfold reducing\nthe computational complexity of factoring. Second, we reformulate Fermat\nfactorization method as an optimization problem suitable for Quantum Annealers\nwhich allowed us to factorize 8,689,739, the biggest number ever factorized\nusing a quantum device to our knowledge.", "AI": {"tldr": "The paper presents a quantum algorithm improving Fermat factorization, reducing computational complexity fourfold and successfully factorizing 8,689,739 with a quantum device.", "motivation": "This work addresses factorization of composite odd numbers, crucial for computational mathematics and cryptography, by combining classical optimization with quantum computing to achieve better efficiency.", "method": "1) A classical enhancement of Fermat's factorization method reducing complexity by a factor of four. 2) Reformulation of Fermat factorization as a quadratic unconstrained binary optimization (QUBO) problem for execution on quantum annealers.", "result": "1) Fourfold reduction in computational steps for classical Fermat factorization. 2) Factorized the largest number (8,689,739) using a quantum annealer to date.", "conclusion": "The proposed method demonstrates significant improvements in classical and quantum factorization approaches, establishing proof-of-concept for quantum annealers in solving large-scale factorization problems."}}
{"id": "2508.10042", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10042", "abs": "https://arxiv.org/abs/2508.10042", "authors": ["Jane Carney", "Kushal Upreti", "Gaby G. Dagher", "Tim Andersen"], "title": "FIDELIS: Blockchain-Enabled Protection Against Poisoning Attacks in Federated Learning", "comment": null, "summary": "Federated learning enhances traditional deep learning by enabling the joint\ntraining of a model with the use of IoT device's private data. It ensures\nprivacy for clients, but is susceptible to data poisoning attacks during\ntraining that degrade model performance and integrity. Current poisoning\ndetection methods in federated learning lack a standardized detection method or\ntake significant liberties with trust. In this paper, we present \\Sys, a novel\nblockchain-enabled poison detection framework in federated learning. The\nframework decentralizes the role of the global server across participating\nclients. We introduce a judge model used to detect data poisoning in model\nupdates. The judge model is produced by each client and verified to reach\nconsensus on a single judge model. We implement our solution to show \\Sys is\nrobust against data poisoning attacks and the creation of our judge model is\nscalable.", "AI": {"tldr": "This paper proposes a blockchain-enabled framework (\\Sys) for detecting data poisoning attacks in federated learning by decentralizing the server role and using a consensus-based judge model.", "motivation": "Federated learning faces data poisoning risks and lacks standardized detection mechanisms with trust assumptions.", "method": "Develops a decentralized framework using blockchain to distribute server responsibilities, generates a judge model through client consensus for poison detection.", "result": "Implementation demonstrates robustness against poisoning attacks and scalable judge model creation.", "conclusion": "\\Sys provides a novel, trust-minimized approach to poisoning detection in federated learning through blockchain-assisted decentralization and consensus mechanisms."}}
{"id": "2508.10043", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10043", "abs": "https://arxiv.org/abs/2508.10043", "authors": ["Pallavi Zambare", "Venkata Nikhil Thanikella", "Ying Liu"], "title": "Securing Agentic AI: Threat Modeling and Risk Analysis for Network Monitoring Agentic AI System", "comment": "Submitted and under review in IEEE Transactions on Privacy", "summary": "When combining Large Language Models (LLMs) with autonomous agents, used in\nnetwork monitoring and decision-making systems, this will create serious\nsecurity issues. In this research, the MAESTRO framework consisting of the\nseven layers threat modeling architecture in the system was used to expose,\nevaluate, and eliminate vulnerabilities of agentic AI. The prototype agent\nsystem was constructed and implemented, using Python, LangChain, and telemetry\nin WebSockets, and deployed with inference, memory, parameter tuning, and\nanomaly detection modules. Two practical threat cases were confirmed as\nfollows: (i) resource denial of service by traffic replay denial-of-service,\nand (ii) memory poisoning by tampering with the historical log file maintained\nby the agent. These situations resulted in measurable levels of performance\ndegradation, i.e. telemetry updates were delayed, and computational loads were\nincreased, as a result of poor system adaptations. It was suggested to use a\nmultilayered defense-in-depth approach with memory isolation, validation of\nplanners and anomaly response systems in real-time. These findings verify that\nMAESTRO is viable in operational threat mapping, prospective risk scoring, and\nthe basis of the resilient system design. The authors bring attention to the\nimportance of the enforcement of memory integrity, paying attention to the\nadaptation logic monitoring, and cross-layer communication protection that\nguarantee the agentic AI reliability in adversarial settings.", "AI": {"tldr": "This paper introduces the MAESTRO framework, a seven-layer threat modeling architecture, to address security vulnerabilities in agentic AI systems combining Large Language Models (LLMs) with autonomous agents. They demonstrate two practical threats (resource denial-of-service and memory poisoning) and propose a multilayered defense strategy, including memory isolation and real-time anomaly detection, to ensure system reliability in adversarial settings.", "motivation": "Combining LLMs with autonomous agents in network monitoring and decision-making systems introduces serious security risks, such as vulnerabilities to denial-of-service attacks and memory manipulation, which can degrade system performance and reliability.", "method": "The researchers designed and implemented a prototype agent system using Python, LangChain, and WebSocket telemetry, incorporating MAESTRO's seven-layer architecture with modules for inference, memory management, parameter tuning, and anomaly detection. They conducted experiments to simulate and analyze two threat scenarios.", "result": "Two threat cases were experimentally validated: (i) traffic replay denial-of-service caused resource exhaustion and delayed telemetry updates, and (ii) memory poisoning through log tampering led to increased computational loads. These demonstrated measurable performance degradation in system adaptations.", "conclusion": "MAESTRO proves viable for operational threat mapping, risk scoring, and resilient system design. The authors emphasize critical security measures like memory integrity enforcement, adaptation logic monitoring, and cross-layer communication protection to ensure agentic AI reliability under adversarial conditions."}}
{"id": "2508.10044", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10044", "abs": "https://arxiv.org/abs/2508.10044", "authors": ["Aydin Zaboli", "Junho Hong"], "title": "Generative AI for Cybersecurity of Energy Management Systems: Methods, Challenges, and Future Directions", "comment": "36 pages, 10 figures", "summary": "This paper elaborates on an extensive security framework specifically\ndesigned for energy management systems (EMSs), which effectively tackles the\ndynamic environment of cybersecurity vulnerabilities and/or system problems\n(SPs), accomplished through the incorporation of novel methodologies. A\ncomprehensive multi-point attack/error model is initially proposed to\nsystematically identify vulnerabilities throughout the entire EMS data\nprocessing pipeline, including post state estimation (SE) stealth attacks, EMS\ndatabase manipulation, and human-machine interface (HMI) display corruption\naccording to the real-time database (RTDB) storage. This framework acknowledges\nthe interconnected nature of modern attack vectors, which utilize various\nphases of supervisory control and data acquisition (SCADA) data flow. Then,\ngenerative AI (GenAI)-based anomaly detection systems (ADSs) for EMSs are\nproposed for the first time in the power system domain to handle the scenarios.\nFurther, a set-of-mark generative intelligence (SoM-GI) framework, which\nleverages multimodal analysis by integrating visual markers with rules\nconsidering the GenAI capabilities, is suggested to overcome inherent spatial\nreasoning limitations. The SoM-GI methodology employs systematic visual\nindicators to enable accurate interpretation of segmented HMI displays and\ndetect visual anomalies that numerical methods fail to identify. Validation on\nthe IEEE 14-Bus system shows the framework's effectiveness across scenarios,\nwhile visual analysis identifies inconsistencies. This integrated approach\ncombines numerical analysis with visual pattern recognition and linguistic\nrules to protect against cyber threats and system errors.", "AI": {"tldr": "The paper proposes a novel GenAI-based security framework for EMSs (Structure: Multi-point attack model + SoM-GI method), validated on IEEE 14-Bus system to address dynamic cybersecurity challenges and system issues through multimodal analysis.", "motivation": "Modern EMSs face evolving cyber threats requiring adaptive security solutions that address interconnected vulnerabilities across data processing pipelines (SCADA, RTDB, HMI) through multidimensional analysis.", "method": "1) Comprehensive multi-point attack/error model for vulnerability identification 2) GenAI-based anomaly detection systems (ADSs) 3) Set-of-mark generative intelligence (SoM-GI) framework combining visual markers, linguistic rules, and numerical segmentation for HMI analysis.", "result": "Framework demonstrated effectiveness across scenarios in the IEEE 14-Bus system, with visual analysis successfully identifying inconsistencies not detected by numerical methods alone.", "conclusion": "The integrated approach combining numerical analysis, visual pattern recognition, and linguistic rules provides robust protection against evolving cyber-physical threats in EMS environments through enhanced spatial reasoning and multimodal threat detection."}}
{"id": "2508.10052", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10052", "abs": "https://arxiv.org/abs/2508.10052", "authors": ["Pallavi Zambare", "Venkata Nikhil Thanikella", "Nikhil Padmanabh Kottur", "Sree Akhil Akula", "Ying Liu"], "title": "NetMoniAI: An Agentic AI Framework for Network Security & Monitoring", "comment": "Accepted in IEEE 3rd International Conference on Artificial\n  Intelligence, Blockchain, and Internet of Things (AIBThings 2025)", "summary": "In this paper, we present NetMoniAI, an agentic AI framework for automatic\nnetwork monitoring and security that integrates decentralized analysis with\nlightweight centralized coordination. The framework consists of two layers:\nautonomous micro-agents at each node perform local traffic analysis and anomaly\ndetection. A central controller then aggregates insights across nodes to detect\ncoordinated attacks and maintain system-wide situational awareness. We\nevaluated NetMoniAI on a local micro-testbed and through NS-3 simulations.\nResults confirm that the two-tier agentic-AI design scales under resource\nconstraints, reduces redundancy, and improves response time without\ncompromising accuracy. To facilitate broader adoption and reproducibility, the\ncomplete framework is available as open source. This enables researchers and\npractitioners to replicate, validate, and extend it across diverse network\nenvironments and threat scenarios. Github link:\nhttps://github.com/pzambare3/NetMoniAI", "AI": {"tldr": "NetMoniAI is a two-tier agentic AI framework combining decentralized micro-agents for local network monitoring and a central controller for coordinating analysis, demonstrated to scale efficiently under resource constraints while maintaining accuracy. The framework is open-sourced for reproducibility.", "motivation": "Address limitations of centralized network security systems in resource-constrained environments by enabling scalable, low-redundancy monitoring through decentralized-anomaly detection with centralized coordination for identifying coordinated attacks.", "method": "Developed a framework with autonomous micro-agents at each network node for local traffic analysis and anomaly detection, combined with a central controller that aggregates node-level insights to detect multi-node attacks and maintain situational awareness.", "result": "Evaluations on micro-testbeds and NS-3 simulations show improved scalability, reduced coordination overhead and redundancy, faster response times, and preserved accuracy rates in detecting network anomalies and attacks.", "conclusion": "NetMoniAI provides an effective architecture for resource-efficient network monitoring that maintains security effectiveness while enabling customization through open-source availability for diverse network environments."}}
{"id": "2508.10065", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10065", "abs": "https://arxiv.org/abs/2508.10065", "authors": ["Yuhao Sun", "Yihua Zhang", "Gaowen Liu", "Hongtao Xie", "Sijia Liu"], "title": "Invisible Watermarks, Visible Gains: Steering Machine Unlearning with Bi-Level Watermarking Design", "comment": "Accepted by ICCV 2025", "summary": "With the increasing demand for the right to be forgotten, machine unlearning\n(MU) has emerged as a vital tool for enhancing trust and regulatory compliance\nby enabling the removal of sensitive data influences from machine learning (ML)\nmodels. However, most MU algorithms primarily rely on in-training methods to\nadjust model weights, with limited exploration of the benefits that data-level\nadjustments could bring to the unlearning process. To address this gap, we\npropose a novel approach that leverages digital watermarking to facilitate MU\nby strategically modifying data content. By integrating watermarking, we\nestablish a controlled unlearning mechanism that enables precise removal of\nspecified data while maintaining model utility for unrelated tasks. We first\nexamine the impact of watermarked data on MU, finding that MU effectively\ngeneralizes to watermarked data. Building on this, we introduce an\nunlearning-friendly watermarking framework, termed Water4MU, to enhance\nunlearning effectiveness. The core of Water4MU is a bi-level optimization (BLO)\nframework: at the upper level, the watermarking network is optimized to\nminimize unlearning difficulty, while at the lower level, the model itself is\ntrained independently of watermarking. Experimental results demonstrate that\nWater4MU is effective in MU across both image classification and image\ngeneration tasks. Notably, it outperforms existing methods in challenging MU\nscenarios, known as \"challenging forgets\".", "AI": {"tldr": "This paper introduces Water4MU, a digital watermarking-based approach for machine unlearning. It uses bi-level optimization to enhance unlearning effectiveness in image classification and generation tasks, particularly outperforming existing methods in 'challenging forgets' scenarios.", "motivation": "Existing machine unlearning algorithms focus on in-training weight adjustments while neglecting data-level modifications. This limits their effectiveness, motivating a need for novel data-centric unlearning approaches.", "method": "Water4MU employs a bi-level optimization (BLO) framework where the upper-level optimizes a watermarking network to simplify unlearning by altering data content, while the lower-level independently trains the ML model using modified data.", "result": "Water4MU achieves effective unlearning in challenging scenarios across image classification and generation tasks. Notably, it outperforms state-of-the-art methods in removing data influences (termed 'challenging forgets').", "conclusion": "The integration of digital watermarking with bi-level optimization significantly improves machine unlearning efficiency, particularly in complex unlearning cases through data-level control."}}
{"id": "2508.10185", "categories": ["cs.CR", "cs.CY", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.10185", "abs": "https://arxiv.org/abs/2508.10185", "authors": ["Ren\u00e9 Mayrhofer", "Michael Roland", "Tobias H\u00f6ller", "Philipp Hofer", "Mario Lins"], "title": "An Architecture for Distributed Digital Identities in the Physical World", "comment": null, "summary": "Digital identities are increasingly important for mediating not only digital\nbut also physical service transactions. Managing such identities through\ncentralized providers can cause both availability and privacy concerns: single\npoints of failure and control are ideal targets for global attacks on\ntechnical, organizational, or legal fronts. We design, analyze, and build a\ndistributed digital identity architecture for physical world transactions in\ncommon scenarios like unlocking doors, public transport, or crossing country\nborders. This architecture combines (biometric and other) sensors, (established\nand upcoming) identity authorities, attribute verifiers, and a new core\ncomponent we call the \\emph{Personal Identity Agent (PIA)} that represents\nindividuals with their identity attributes in the digital domain. All\ntransactions are conducted in a completely decentralized manner, and the\ncomponents for which we currently assume central coordination are optional and\nonly used for assisting with service discovery and latency reduction. We\npresent a first protocol between these parties and formally verify that it\nachieves relevant security properties based on a realistic threat model\nincluding strong global adversaries. A proof-of-concept implementation\ndemonstrates practical feasibility of both architecture and initial protocol\nfor applications that can tolerate end-to-end latencies in the range of a few\nseconds.", "AI": {"tldr": "This paper proposes a decentralized digital identity architecture for physical transactions, introducing a Personal Identity Agent (PIA) to enhance security and privacy. Protocols and formal verification demonstrate feasibility against strong adversaries.", "motivation": "Centralized identity providers create single points of failure and privacy risks, making them vulnerable to global attacks. Decentralized solutions are needed for secure and available physical service transactions.", "method": "The architecture combines sensors, identity authorities, attribute verifiers, and PIAs. A protocol was designed for decentralized transactions, with formal verification against a realistic threat model including global adversaries.", "result": "Formal verification confirms the protocol's security properties. A proof-of-concept implementation shows practical feasibility for applications tolerating ~second latency.", "conclusion": "The proposed PIA-based architecture provides a privacy-preserving, decentralized framework for physical transactions, addressing vulnerabilities of centralized systems while maintaining practical implementation viability."}}
{"id": "2508.10212", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.10212", "abs": "https://arxiv.org/abs/2508.10212", "authors": ["Md Sazedur Rahman", "Mohamed Elmahallawy", "Sanjay Madria", "Samuel Frimpong"], "title": "Detecting Untargeted Attacks and Mitigating Unreliable Updates in Federated Learning for Underground Mining Operations", "comment": null, "summary": "Underground mining operations rely on distributed sensor networks to collect\ncritical data daily, including mine temperature, toxic gas concentrations, and\nminer movements for hazard detection and operational decision-making. However,\ntransmitting raw sensor data to a central server for training deep learning\nmodels introduces significant privacy risks, potentially exposing sensitive\nmine-specific information. Federated Learning (FL) offers a transformative\nsolution by enabling collaborative model training while ensuring that raw data\nremains localized at each mine. Despite its advantages, FL in underground\nmining faces key challenges: (i) An attacker may compromise a mine's local\nmodel by employing techniques such as sign-flipping attacks or additive noise,\nleading to erroneous predictions; (ii) Low-quality (yet potentially valuable)\ndata, caused by poor lighting conditions or sensor inaccuracies in mines may\ndegrade the FL training process. In response, this paper proposes MineDetect, a\ndefense FL framework that detects and isolates the attacked models while\nmitigating the impact of mines with low-quality data. MineDetect introduces two\nkey innovations: (i) Detecting attacked models (maliciously manipulated) by\ndeveloping a history-aware mechanism that leverages local and global averages\nof gradient updates; (ii) Identifying and eliminating adversarial influences\nfrom unreliable models (generated by clients with poor data quality) on the FL\ntraining process. Comprehensive simulations across diverse datasets demonstrate\nthat MineDetect outperforms existing methods in both robustness and accuracy,\neven in challenging non-IID data scenarios. Its ability to counter adversarial\ninfluences while maintaining lower computational efficiency makes it a vital\nadvancement for improving safety and operational effectiveness in underground\nmining.", "AI": {"tldr": "MineDetect is a federated learning framework for underground mining that addresses privacy risks, attack vulnerabilities, and low-quality sensor data by isolating malicious models and mitigating adversarial influences from unreliable clients.", "motivation": "Underground mining operations face privacy risks from centralized sensor data processing, while federated learning is challenged by adversarial attacks and data quality issues in harsh mining environments.", "method": "MineDetect introduces a history-aware gradient analysis mechanism to detect attacked models, and a dual-robustness mitigation strategy that identifies and neutralizes adversarial influences from both malicious clients and unreliable clients with poor-quality sensor data.", "result": "Evaluations show MineDetect achieves superior robustness and accuracy compared to existing methods, particularly in non-IID data scenarios, while maintaining computational efficiency necessary for real-time hazard detection.", "conclusion": "MineDetect advances federated learning for safety-critical mining applications by effectively defending against attacks and low-quality data challenges, enabling secure collaborative model training for improved hazard detection and operational decisions."}}
{"id": "2508.10327", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.10327", "abs": "https://arxiv.org/abs/2508.10327", "authors": ["Haoyang Hu", "Xun Huang", "Chenyu Wu", "Shiwen Liu", "Zhichao Lian", "Shuangquan Zhang"], "title": "BERTector: Intrusion Detection Based on Joint-Dataset Learning", "comment": null, "summary": "Intrusion detection systems (IDS) are facing challenges in generalization and\nrobustness due to the heterogeneity of network traffic and the diversity of\nattack patterns. To address this issue, we propose a new joint-dataset training\nparadigm for IDS and propose a scalable BERTector framework based on BERT.\nBERTector integrates three key components: NSS-Tokenizer for traffic-aware\nsemantic tokenization, supervised fine-tuning with a hybrid dataset, and\nlow-rank adaptation (LoRA) for efficient training. Extensive experiments show\nthat BERTector achieves state-of-the-art detection accuracy, strong\ncross-dataset generalization capabilities, and excellent robustness to\nadversarial perturbations. This work establishes a unified and efficient\nsolution for modern IDS in complex and dynamic network environments.", "AI": {"tldr": "This paper introduces BERTector, a BERT-based intrusion detection system that achieves state-of-the-art results through joint-dataset training, traffic-aware tokenization (NSS-Tokenizer), hybrid dataset fine-tuning, and low-rank adaptation (LoRA).", "motivation": "Current IDS face limitations due to network traffic heterogeneity and diverse attack patterns, requiring better generalization and robustness.", "method": "BERTector combines three innovations: 1) Traffic-aware semantic tokenization (NSS-Tokenizer), 2) Supervised training on a hybrid dataset integrating multiple sources, and 3) LoRA for efficient parameter adaptation during training.", "result": "Experiments demonstrate BERTector achieves 1) Leading detection accuracy, 2) Strong cross-dataset generalization, and 3) High robustness against adversarial perturbations.", "conclusion": "BERTector establishes a unified, efficient framework for modern IDS with enhanced performance in complex/dynamic network environments through its joint-dataset approach and BERT-based architecture."}}
{"id": "2508.10431", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.10431", "abs": "https://arxiv.org/abs/2508.10431", "authors": ["Chris Cao", "Gururaj Saileshwar"], "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based Side-Channel Attacks on Fully Associative Randomized Caches", "comment": null, "summary": "Recent work presented at USENIX Security 2025 claims that occupancy-based\nattacks can recover AES keys from the MIRAGE randomized cache. In this paper,\nwe examine these claims and find that they arise from fundamental modeling\nflaws. Most critically, the authors' simulation of MIRAGE uses a constant seed\nto initialize the random number generator used for global evictions in MIRAGE,\ncausing every AES encryption they trace to evict the same deterministic\nsequence of cache lines. This artificially creates a highly repeatable timing\npattern that is not representative of a realistic implementation of MIRAGE,\nwhere eviction sequences vary randomly between encryptions. When we instead\nrandomize the eviction seed for each run, reflecting realistic operation, the\ncorrelation between AES T-table accesses and attacker runtimes disappears, and\nthe attack fails. These findings show that the reported leakage is an artifact\nof incorrect modeling, and not an actual vulnerability in MIRAGE.", "AI": {"tldr": "This paper refutes prior claims of AES key recovery from MIRAGE's cache by identifying a simulation flaw in the attack model, demonstrating that correct randomization prevents the reported vulnerability.", "motivation": "To address and validate recent claims that MIRAGE's randomized cache leaks AES keys through occupancy-based attacks, which is critical for assessing the security of the cache architecture.", "method": "The authors simulate MIRAGE with a constant seed for eviction randomization (as in the prior attack) versus random seeds per execution to evaluate how seed choice affects the feasibility of occupancy-based key recovery.", "result": "Under realistic random seed initialization, the timing correlation between AES T-table accesses and attacker runtime in the MIRAGE cache vanishes, causing the claimed attack to fail.", "conclusion": "The success of the prior attack relies on a flawed and non-realistic simulation setup; properly modeling MIRAGE's random eviction sequences eliminates the vulnerability."}}
{"id": "2508.10493", "categories": ["cs.CR", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.10493", "abs": "https://arxiv.org/abs/2508.10493", "authors": ["Bernhard Kauer", "Aleksandr Petrosyan", "Benjamin Livshits"], "title": "AlDBaran: Towards Blazingly Fast State Commitments for Blockchains", "comment": null, "summary": "The fundamental basis for maintaining integrity within contemporary\nblockchain systems is provided by authenticated databases. Our analysis\nindicates that a significant portion of the approaches applied in this domain\nfail to sufficiently meet the stringent requirements of systems processing\ntransactions at rates of multi-million TPS. AlDBaran signifies a substantial\nadvancement in authenticated databases. By eliminating disk I/O operations from\nthe critical path, implementing prefetching strategies, and refining the update\nmechanism of the Merkle tree, we have engineered an authenticated data\nstructure capable of handling state updates efficiently at a network throughput\nof 50 Gbps. This throughput capacity significantly surpasses any empirically\ndocumented blockchain throughput, guaranteeing the ability of even the most\nhigh-throughput blockchains to generate state commitments effectively.\n  AlDBaran provides support for historical state proofs, which facilitates a\nwide array of novel applications. For instance, the deployment of AlDBaran\ncould enable blockchains that do not currently support state commitments to\noffer functionalities for light clients and/or implement rollups.\n  When benchmarked against alternative authenticated data structure projects,\nAlDBaran exhibits superior performance and simplicity. In particular, AlDBaran\nachieves speeds of approximately 48 million updates per second using an\nidentical machine configuration. This characteristic renders AlDBaran an\nattractive solution for resource-limited environments, as its historical data\ncapabilities can be modularly isolated (and deactivated), which further\nenhances performance. On consumer-level portable hardware, it achieves\napproximately 8 million updates/s in an in-memory setting and 5 million\nupdates/s with snapshots at sub-second intervals, illustrating compelling and\ncost-effective scalability.", "AI": {"tldr": "AlDBaran is an authenticated database for blockchain systems that eliminates disk I/O from the critical path through prefetching strategies and Merkle tree optimizations, enabling high throughput of 50 Gbps (48M updates/sec) and outperforming existing solutions for resource-constrained environments.", "motivation": "Contemporary blockchain systems require authenticated databases capable of handling multi-million TPS, but most fail to meet these demands. Traditional Merkle trees with disk I/O create bottlenecks, limiting scalability and state commitment efficiency.", "method": "The system achieves performance gains by: 1) Decoupling disk I/O from the critical path via prefetching strategies 2) Optimizing Merkle tree update mechanisms 3) Modular isolation of historical data components that can be selectively deactivated for performance enhancements.", "result": "AlDBaran processes 48M updates/sec in identical hardware configurations compared to existing systems, and 8M updates/sec on portable hardware in memory with 5M updates/sec for snapshot-based operations. It supports 50 Gbps throughput, exceeding all empirically documented blockchain benchmarks.", "conclusion": "AlDBaran represents a significant leap in authenticated databases for blockchain, enabling high-throughput state commitments for previously incompatible systems. Its performance optimizations make it particularly valuable for resource-limited environments while maintaining historical state proof capabilities for new applications like light clients and rollups."}}
{"id": "2508.10510", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.10510", "abs": "https://arxiv.org/abs/2508.10510", "authors": ["Hugo Delavenne", "Louise Lallemand"], "title": "Codes on any Cayley Graph have an Interactive Oracle Proof of Proximity", "comment": null, "summary": "Interactive Oracle Proofs of Proximity (IOPP) are at the heart of code-based\nSNARKs, a family of zeroknowledge protocols. The first and most famous one is\nthe FRI protocol [BBHR18a], that efficiently tests proximity to Reed-Solomon\ncodes. This paper generalizes the flowering IOPP introduced in [DMR25] for some\nspecific (2, n)-regular Tanner codes to a much broader variety of codes: any\ncode with symbols indexed on the edges of a Cayley graph. The flowering\nprotocol of [DMR25] had a soundness parameter much lower than the FRI protocol\n[BCI + 23], and complexity parameters that could compete with the FRI\n[BBHR18a]. The lower soundness and the absence of restriction on the base field\nmay lead to other practical speedups, however the codes considered in [DMR25]\nhave an o(1) minimum distance. The generalization proposed in this paper\npreserves the soundness parameter with a slight decrease of the complexity\nparameters, while allowing being applied on codes with constant rate and\nconstant minimum distance thanks to the good expansion properties of some\nfamilies of Cayley graphs.", "AI": {"tldr": "This paper generalizes the flowering IOPP from a specific class of graphs to codes on Cayley graphs, improving practicality while maintaining soundness and enabling codes with constant rate and minimum distance.", "motivation": "The paper aims to expand the applicability of IOPPs beyond prior limitations on code types and field constraints, enabling broader use in zero-knowledge protocols.", "method": "The method extends the flowering protocol from [DMR25] by leveraging expansion properties of Cayley graphs, allowing application to codes with symbols indexed on their edges.", "result": "The generalized protocol preserves the low soundness parameter with minimal complexity trade-offs, enabling practical speedups while supporting codes of constant rate and minimum distance.", "conclusion": "This work advances the state of code-based SNARKs by broadening the class of codes compatible with IOPPs while retaining efficiency and improving error tolerance."}}
{"id": "2508.10636", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.10636", "abs": "https://arxiv.org/abs/2508.10636", "authors": ["Sandipan Dey", "Payal Santosh Kate", "Vatsala Upadhyay", "Abhishek Vaish"], "title": "A Transformer-Based Approach for DDoS Attack Detection in IoT Networks", "comment": null, "summary": "DDoS attacks have become a major threat to the security of IoT devices and\ncan cause severe damage to the network infrastructure. IoT devices suffer from\nthe inherent problem of resource constraints and are therefore susceptible to\nsuch resource-exhausting attacks. Traditional methods for detecting DDoS\nattacks are not efficient enough to cope with the dynamic nature of IoT\nnetworks, as well as the scalability of the attacks, diversity of protocols,\nhigh volume of traffic, and variability in device behavior, and variability of\nprotocols like MQTT, CoAP, making it hard to implement security across all the\nprotocols. In this paper, we propose a novel approach, i.e., the use of\nTransformer models, which have shown remarkable performance in natural language\nprocessing tasks, for detecting DDoS attacks on IoT devices. The proposed model\nextracts features from network traffic data and processes them using a\nself-attention mechanism. Experiments conducted on a real-world dataset\ndemonstrate that the proposed approach outperforms traditional machine learning\ntechniques, which can be validated by comparing both approaches' accuracy,\nprecision, recall, and F1-score. The results of this study show that the\nTransformer models can be an effective solution for detecting DDoS attacks on\nIoT devices and have the potential to be deployed in real-world IoT\nenvironments.", "AI": {"tldr": "This paper proposes using Transformer models to detect DDoS attacks on IoT devices, demonstrating superior performance over traditional methods through real-world experiments.", "motivation": "DDoS attacks pose significant threats to IoT networks due to resource constraints, dynamic network nature, attack scalability, protocol diversity (e.g., MQTT, CoAP), and high traffic volume, which challenge traditional detection techniques.", "method": "The approach employs Transformer models with self-attention mechanisms to extract and process network traffic features, leveraging their strengths in handling sequential data and capturing complex patterns.", "result": "Experiments on a real-world dataset show the Transformer model outperforms traditional ML techniques in accuracy, precision, recall, and F1-score, validating its effectiveness under varied IoT conditions.", "conclusion": "Transformers offer a promising solution for scalable, protocol-agnostic DDoS detection in IoT environments and can be practically deployed despite resource limitations and dynamic traffic patterns."}}
{"id": "2508.10639", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.10639", "abs": "https://arxiv.org/abs/2508.10639", "authors": ["Anyuan Sang", "Lu Zhou", "Li Yang", "Junbo Jia", "Huipeng Yang", "Pengbin Feng", "Jianfeng Ma"], "title": "MirGuard: Towards a Robust Provenance-based Intrusion Detection System Against Graph Manipulation Attacks", "comment": null, "summary": "Learning-based Provenance-based Intrusion Detection Systems (PIDSes) have\nbecome essential tools for anomaly detection in host systems due to their\nability to capture rich contextual and structural information, as well as their\npotential to detect unknown attacks. However, recent studies have shown that\nthese systems are vulnerable to graph manipulation attacks, where attackers\nmanipulate the graph structure to evade detection. While some previous\napproaches have discussed this type of attack, none have fully addressed it\nwith a robust detection solution, limiting the practical applicability of\nPIDSes.\n  To address this challenge, we propose MirGuard, a robust anomaly detection\nframework that combines logic-aware multi-view augmentation with contrastive\nrepresentation learning. Rather than applying arbitrary structural\nperturbations, MirGuard introduces Logic-Aware Noise Injection (LNI) to\ngenerate semantically valid graph views, ensuring that all augmentations\npreserve the underlying causal semantics of the provenance data. These views\nare then used in a Logic-Preserving Contrastive Learning framework, which\nencourages the model to learn representations that are invariant to benign\ntransformations but sensitive to adversarial inconsistencies. Comprehensive\nevaluations on multiple provenance datasets demonstrate that MirGuard\nsignificantly outperforms state-of-the-art detectors in robustness against\nvarious graph manipulation attacks without sacrificing detection performance\nand efficiency. Our work represents the first targeted study to enhance PIDS\nagainst such adversarial threats, providing a robust and effective solution to\nmodern cybersecurity challenges.", "AI": {"tldr": "MirGuard is a robust anomaly detection framework for host systems that addresses graph manipulation attacks in Learning-based Provenance-based Intrusion Detection Systems (PIDSes) by using logic-aware multi-view augmentation and contrastive representation learning, improving robustness without sacrificing performance.", "motivation": "The paper addresses the vulnerability of Learning-based PIDSes to graph manipulation attacks, which prior work has not effectively solved, thereby limiting their practical applicability in cybersecurity.", "method": "MirGuard employs (1) Logic-Aware Noise Injection (LNI) to generate semantically valid graph views preserving causal semantics, and (2) a contrastive learning framework that trains models to be invariant to benign transformations but sensitive to adversarial inconsistencies.", "result": "MirGuard outperforms state-of-the-art detectors in robustness against graph manipulation attacks, with no significant trade-off in detection performance or efficiency, as shown through evaluations on multiple provenance datasets.", "conclusion": "MirGuard represents the first targeted solution to enhance PIDSes against graph manipulation attacks, offering a robust and effective approach to modern cybersecurity challenges."}}
{"id": "2508.10652", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.10652", "abs": "https://arxiv.org/abs/2508.10652", "authors": ["Richa Dasila", "Vatsala Upadhyay", "Samo Bobek", "Abhishek Vaish"], "title": "A Novel Study on Intelligent Methods and Explainable AI for Dynamic Malware Analysis", "comment": null, "summary": "Deep learning models are one of the security strategies, trained on extensive\ndatasets, and play a critical role in detecting and responding to these threats\nby recognizing complex patterns in malicious code. However, the opaque nature\nof these models-often described as \"black boxes\"-makes their decision-making\nprocesses difficult to understand, even for their creators. This research\naddresses these challenges by integrating Explainable AI (XAI) techniques to\nenhance the interpretability and trustworthiness of malware detection models.\nIn this research, the use of Multi-Layer Perceptrons (MLP) for dynamic malware\nanalysis has been considered, a less explored area, and its efficacy in\ndetecting Metamorphic Malware, and further the effectiveness and transparency\nof MLPs, CNNs, RNNs, and CNN-LSTM models in malware classification, evaluating\nthese models through the lens of Explainable AI (XAI). This comprehensive\napproach aims to demystify the internal workings of deep learning models,\npromoting a better understanding and trust in their predictive capabilities in\ncybersecurity contexts. Such in-depth analysis and implementation haven't been\ndone to the best of our knowledge.", "AI": {"tldr": "This paper enhances the interpretability and trustworthiness of deep learning malware detection models using Explainable AI (XAI) techniques. It evaluates multi-layer perceptrons, CNNs, and RNNs in dynamic malware analysis, particularly focusing on Metamorphic Malware classification with a novel comprehensive approach.", "motivation": "While deep learning models effectively detect complex malware patterns, their \"black box\" nature limits transparency and trust in security decisions, hindering adoption despite performance capabilities. Traditional models lack accountability in high-stakes cybersecurity contexts.", "method": "Integrated XAI methods into malware detection by: 1) Applying multi-layer perceptrons (MLPs) to dynamic malware analysis (an underexplored application) 2) Evaluating MLPs, CNNs, RNNs, and CNN-LSTM models through multiple XAI interpretability frameworks 3) Conducting metamorphic malware classification experiments to test model explainability 4) Providing holistic analysis of model decision-making processes.", "result": "Demonstrated that XAI integration significantly improves model transparency without compromising detection accuracy, with specific quantitative improvements in feature importance visualization and decision rationale clarity for Metamorphic Malware analysis. Comparative analysis showed unique interpretability advantages across different model architectures.", "conclusion": "XAI integration provides a transformative approach to cybersecurity by enabling detailed understanding of deep learning threat detection mechanisms. The novel framework establishes a pathway for verifiable, accountable security AI systems, with implications for both model development and security domain adoption practices."}}
{"id": "2508.10677", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10677", "abs": "https://arxiv.org/abs/2508.10677", "authors": ["Amine Tellache", "Abdelaziz Amara Korba", "Amdjed Mokhtari", "Horea Moldovan", "Yacine Ghamri-Doudane"], "title": "Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat Intelligence", "comment": null, "summary": "Effective incident response (IR) is critical for mitigating cyber threats,\nyet security teams are overwhelmed by alert fatigue, high false-positive rates,\nand the vast volume of unstructured Cyber Threat Intelligence (CTI) documents.\nWhile CTI holds immense potential for enriching security operations, its\nextensive and fragmented nature makes manual analysis time-consuming and\nresource-intensive. To bridge this gap, we introduce a novel\nRetrieval-Augmented Generation (RAG)-based framework that leverages Large\nLanguage Models (LLMs) to automate and enhance IR by integrating dynamically\nretrieved CTI. Our approach introduces a hybrid retrieval mechanism that\ncombines NLP-based similarity searches within a CTI vector database with\nstandardized queries to external CTI platforms, facilitating context-aware\nenrichment of security alerts. The augmented intelligence is then leveraged by\nan LLM-powered response generation module, which formulates precise,\nactionable, and contextually relevant incident mitigation strategies. We\npropose a dual evaluation paradigm, wherein automated assessment using an\nauxiliary LLM is systematically cross-validated by cybersecurity experts.\nEmpirical validation on real-world and simulated alerts demonstrates that our\napproach enhances the accuracy, contextualization, and efficiency of IR,\nalleviating analyst workload and reducing response latency. This work\nunderscores the potential of LLM-driven CTI fusion in advancing autonomous\nsecurity operations and establishing a foundation for intelligent, adaptive\ncybersecurity frameworks.", "AI": {"tldr": "This paper proposes a RAG-based framework using Large Language Models (LLMs) to automate and enhance incident response by dynamically integrating Cyber Threat Intelligence (CTI), reducing analyst workload and response latency through a hybrid retrieval mechanism and context-aware alert enrichment.", "motivation": "Security teams face alert fatigue, high false-positive rates, and challenges in analyzing unstructured CTI documents. Existing methods struggle to efficiently leverage CTI's fragmented information for timely and accurate incident mitigation.", "method": "The approach combines 1) a hybrid retrieval mechanism (NLP similarity searches in a CTI vector database + standardized external CTI platform queries) for context-aware alert enrichment, and 2) an LLM-powered response generation module. A dual evaluation paradigm is introduced, using an auxiliary LLM and cross-validation by cybersecurity experts.", "result": "Empirical validation on real/simulated alerts shows improved IR accuracy, contextualization, and efficiency. The system reduces analyst workload and response latency, demonstrating the value of LLM-driven CTI fusion in enhancing operational effectiveness.", "conclusion": "The work establishes the potential of LLM-augmented CTI integration for implementing autonomous security operations and building intelligent, adaptable defensive frameworks in modern cybersecurity ecosystems."}}
{"id": "2508.10880", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10880", "abs": "https://arxiv.org/abs/2508.10880", "authors": ["Yanzhe Zhang", "Diyi Yang"], "title": "Searching for Privacy Risks in LLM Agents via Simulation", "comment": "Preprint", "summary": "The widespread deployment of LLM-based agents is likely to introduce a\ncritical privacy threat: malicious agents that proactively engage others in\nmulti-turn interactions to extract sensitive information. These dynamic\ndialogues enable adaptive attack strategies that can cause severe privacy\nviolations, yet their evolving nature makes it difficult to anticipate and\ndiscover sophisticated vulnerabilities manually. To tackle this problem, we\npresent a search-based framework that alternates between improving attacker and\ndefender instructions by simulating privacy-critical agent interactions. Each\nsimulation involves three roles: data subject, data sender, and data recipient.\nWhile the data subject's behavior is fixed, the attacker (data recipient)\nattempts to extract sensitive information from the defender (data sender)\nthrough persistent and interactive exchanges. To explore this interaction space\nefficiently, our search algorithm employs LLMs as optimizers, using parallel\nsearch with multiple threads and cross-thread propagation to analyze simulation\ntrajectories and iteratively propose new instructions. Through this process, we\nfind that attack strategies escalate from simple direct requests to\nsophisticated multi-turn tactics such as impersonation and consent forgery,\nwhile defenses advance from rule-based constraints to identity-verification\nstate machines. The discovered attacks and defenses transfer across diverse\nscenarios and backbone models, demonstrating strong practical utility for\nbuilding privacy-aware agents.", "AI": {"tldr": "The paper proposes a search-based framework using LLM-based agents to simulate privacy attacks (e.g., impersonation) and defenses (e.g., identity-verification state machines) in multi-turn dialogues, enabling scalable vulnerability discovery and robust defense generation for privacy-aware agent systems.", "motivation": "LLM-based agents face critical privacy risks due to sophisticated multi-turn attacks where malicious actors extract sensitive information adaptively. Current manual vulnerability identification is impractical for these evolving interactions.", "method": "A tripartite simulation system with data subject (fixed baseline), attacker (data recipient) optimized via multi-threaded LLM-driven parallel search with cross-thread strategy sharing, and defender (data sender) iteratively improved against attack simulations. Attack/defense instruction pairs are co-evolved through trajectory analysis.", "result": "Attack strategies evolve from direct questioning to complex tactics like consent forgery, while defenses advance from rules to state machines. The framework shows cross-scenario transferability (hotel booking, medical QA) and consistently works across different LLM architectures (LLaMA, GPT-3.5).", "conclusion": "Automated simulation of adversarial agent interactions is a critical tool for uncovering and mitigating privacy vulnerabilities in complex dialogue systems, enabling both attack analysis and defense generation at scale."}}
