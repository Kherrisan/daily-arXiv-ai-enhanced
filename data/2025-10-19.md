<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 3]
- [cs.SE](#cs.SE) [Total: 6]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Noisy Networks, Nosy Neighbors: Inferring Privacy Invasive Information from Encrypted Wireless Traffic](https://arxiv.org/abs/2510.13822)
*Bartosz Burgiel*

Main category: cs.CR

TL;DR: This study demonstrates that encrypted smart home wireless traffic can be passively analyzed to infer sensitive user activities and environmental layouts, highlighting new privacy risks beyond traditional data breaches.


<details>
  <summary>Details</summary>
Motivation: The proliferation of smart home devices creates privacy vulnerabilities through encrypted network traffic, which conventional security measures often overlook. This research addresses the risk of passive eavesdropping by neighbors or attackers using basic wireless observation tools.

Method: The researchers created a simulated neighboring environment to capture 802.11 packets and Bluetooth Low Energy advertisements. They used device fingerprinting, state inference algorithms, and RSSI trilateration to analyze encrypted traffic patterns without decrypting payloads.

Result: They successfully detected device usage patterns (e.g., streaming, sleeping, work hours), inferred human behaviors, and reconstructed approximate room layouts using only signal strength analysis from encrypted traffic. The system achieved location accuracy within 1-2 meters in testing scenarios.

Conclusion: Smart home privacy risks extend beyond data breaches to include passive traffic analysis. Even with encryption, temporal patterns and signal metadata can leak sensitive information about occupants' routines and environments, necessitating new countermeasures for location- and state-privacy protection.

Abstract: This thesis explores the extent to which passive observation of wireless
traffic in a smart home environment can be used to infer privacy-invasive
information about its inhabitants. Using a setup that mimics the capabilities
of a nosy neighbor in an adjacent flat, we analyze raw 802.11 packets and
Bluetooth Low Energy advertisemets. From this data, we identify devices, infer
their activity states and approximate their location using RSSI-based
trilateration. Despite the encrypted nature of the data, we demonstrate that it
is possible to detect active periods of multimedia devices, infer common
activities such as sleeping, working and consuming media, and even approximate
the layout of the neighbor's apartment. Our results show that privacy risks in
smart homes extend beyond traditional data breaches: a nosy neighbor behind the
wall can gain privacy-invasive insights into the lives of their neighbors
purely from encrypted network traffic.

</details>


### [2] [Multi-Layer Secret Sharing for Cross-Layer Attack Defense in 5G Networks: a COTS UE Demonstration](https://arxiv.org/abs/2510.13824)
*Wai Ming Chan,Remi Chou,Taejoon Kim*

Main category: cs.CR

TL;DR: First XOR-based multi-layer secret sharing implementation on COTS 5G devices, ensuring security against DoS attacks without infrastructure changes.


<details>
  <summary>Details</summary>
Motivation: Existing secret sharing schemes require infrastructure changes or pre-shared keys, limiting their applicability to COTS 5G devices.

Method: An XOR-based approach that distributes secret shares across network operators and distributed relays without requiring infrastructure modifications or pre-shared keys.

Result: Achieved perfect secret recovery and data confidentiality even when one network operator and one relay are compromised.

Conclusion: This paper demonstrates a practical and secure method for multi-layer secret sharing on standard 5G devices.

Abstract: This demo presents the first implementation of multi-layer secret sharing on
commercial-off-the-shelf (COTS) 5G user equipment (UE), operating without
infrastructure modifications or pre-shared keys. Our XOR-based approach
distributes secret shares across network operators and distributed relays,
ensuring perfect recovery and data confidentiality even if one network operator
and one relay are simultaneously lost (e.g., under denial of service (DoS) or
unanticipated attacks).

</details>


### [3] [A2AS: Agentic AI Runtime Security and Self-Defense](https://arxiv.org/abs/2510.13825)
*Eugene Neelou,Ivan Novikov,Max Moroz,Om Narayan,Tiffany Saade,Mika Ayenson,Ilya Kabanov,Jen Ozmen,Edward Lee,Vineeth Sai Narajala,Emmanuel Guilherme Junior,Ken Huang,Huseyin Gulsin,Jason Ross,Marat Vyshegorodtsev,Adelin Travers,Idan Habler,Rahul Jadav*

Main category: cs.CR

TL;DR: A2AS is a lightweight security framework for AI agents/LLMs, using the BASIC model to enforce behavior, ensure context integrity, and apply policies, aiming to set an industry security standard.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for securing AI/LLM systems against adversarial prompts and context window vulnerabilities while avoiding latency, architectural changes, or model retraining.

Method: A2AS employs the BASIC security model (Behavior certificates, Authenticated prompts, Security boundaries, In-context defenses, Codified policies) to enforce certified behavior, authenticate inputs, isolate risks, apply defenses during reasoning, and enforce application-specific rules.

Result: Introduces the BASIC model and A2AS framework, demonstrating their potential to become an industry standard for AI/LLM security without external dependencies or operational complexity.

Conclusion: The A2AS framework establishes a robust security model for AI agents and LLMs through the BASIC model, offering a defense-in-depth strategy without operational overheads, positioning itself as a candidate for industry standards.

Abstract: The A2AS framework is introduced as a security layer for AI agents and
LLM-powered applications, similar to how HTTPS secures HTTP. A2AS enforces
certified behavior, activates model self-defense, and ensures context window
integrity. It defines security boundaries, authenticates prompts, applies
security rules and custom policies, and controls agentic behavior, enabling a
defense-in-depth strategy. The A2AS framework avoids latency overhead, external
dependencies, architectural changes, model retraining, and operational
complexity. The BASIC security model is introduced as the A2AS foundation: (B)
Behavior certificates enable behavior enforcement, (A) Authenticated prompts
enable context window integrity, (S) Security boundaries enable untrusted input
isolation, (I) In-context defenses enable secure model reasoning, (C) Codified
policies enable application-specific rules. This first paper in the series
introduces the BASIC security model and the A2AS framework, exploring their
potential toward establishing the A2AS industry standard.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [From Craft to Constitution: A Governance-First Paradigm for Principled Agent Engineering](https://arxiv.org/abs/2510.13857)
*Qiang Xu,Xiangyu Wen,Changran Xu,Zeju Li,Jianyuan Zhong*

Main category: cs.SE

TL;DR: This paper proposes ArbiterOS, a governance-first architecture to address brittleness in AI agents by aligning engineering practices with LLMs' probabilistic nature.


<details>
  <summary>Details</summary>
Motivation: The work is motivated by the 'crisis of craft' in autonomous agent development, where deterministic software engineering practices yield brittle, unpredictable agents unsuitable for mission-critical applications.

Method: The paper introduces ArbiterOS, a formal architecture grounded in a governance-first paradigm that reorients agent design around probabilistic reasoning and risk management.

Result: The governance-first approach provides a principled framework that enables more trustworthy autonomous systems by addressing the paradigm mismatch between probabilistic LLMs and traditional design methods.

Conclusion: The paper concludes that a governance-first paradigm, implemented through ArbiterOS, can resolve the challenges posed by traditional deterministic approaches in agent engineering for probabilistic LLMs.

Abstract: The advent of powerful Large Language Models (LLMs) has ushered in an ``Age
of the Agent,'' enabling autonomous systems to tackle complex goals. However,
the transition from prototype to production is hindered by a pervasive ``crisis
of craft,'' resulting in agents that are brittle, unpredictable, and ultimately
untrustworthy in mission-critical applications. This paper argues this crisis
stems from a fundamental paradigm mismatch -- attempting to command inherently
probabilistic processors with the deterministic mental models of traditional
software engineering. To solve this crisis, we introduce a governance-first
paradigm for principled agent engineering, embodied in a formal architecture we
call ArbiterOS.

</details>


### [5] [Benchmarking Correctness and Security in Multi-Turn Code Generation](https://arxiv.org/abs/2510.13859)
*Ruchit Rawal,Jeffrey Yang Fan Chiang,Chihao Shen,Jeffery Siyuan Tian,Aastha Mahajan,Tom Goldstein,Yizheng Chen*

Main category: cs.SE

TL;DR: MT-Sec is a new benchmark for evaluating AI coding assistants in multi-turn scenarios, revealing significant drops in correctness and security compared to single-turn tasks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on single-turn code generation, ignoring the iterative nature of real-world development. Multi-turn evaluations are needed to assess correctness and security more realistically.

Method: Constructed synthetic multi-turn tasks from single-turn datasets using a pipeline that preserves semantic alignment. Evaluated 32 models and agent scaffolds across correctness, security, and code-diff generation.

Result: 20-27\% drop in 'correct and secure' performance in multi-turn settings. Code-diff generation showed higher error rates. Agent scaffolding improved single-turn but not multi-turn results.

Conclusion: Current models underperform in multi-turn workflows; benchmarks must account for iterative development to ensure real-world security and correctness in AI-assisted coding.

Abstract: AI coding assistants powered by large language models (LLMs) have transformed
software development, significantly boosting productivity. While existing
benchmarks evaluate the correctness and security of LLM-generated code, they
are typically limited to single-turn tasks that do not reflect the iterative
nature of real-world development. We introduce MT-Sec, the first benchmark to
systematically evaluate both correctness and security in multi-turn coding
scenarios. We construct this using a synthetic data pipeline that transforms
existing single-turn tasks into semantically aligned multi-turn interaction
sequences, allowing reuse of original test suites while modeling the complexity
of real-world coding processes. We evaluate 32 open- and closed-source models,
and three agent-scaffolding on MT-Sec and observe a consistent 20-27% drop in
"correct and secure" outputs from single-turn to multi-turn settings -- even
among state-of-the-art models. Beyond full-program generation, we also evaluate
models on multi-turn code-diff generation -- an unexplored yet practically
relevant setting -- and find that models perform worse here, with increased
rates of functionally incorrect and insecure outputs. Finally, we find that
while agent scaffoldings boost single-turn code generation performance, they
are not quite as effective in multi-turn evaluations. Together, these findings
highlight the need for benchmarks that jointly evaluate correctness and
security in multi-turn, real-world coding workflows.

</details>


### [6] [A11YN: aligning LLMs for accessible web UI code generation](https://arxiv.org/abs/2510.13914)
*Janghan Yoon,Jaegwan Cho,Junhyeok Kim,Jiwan Chung,Jaehyun Jeon,Youngjae Yu*

Main category: cs.SE

TL;DR: A11yn is a novel method that optimizes code-generating LLMs to produce web interfaces compliant with accessibility guidelines (WCAG), significantly reducing inaccessibility rates by 60%. It uses a severity-weighted reward function, a new training dataset (UIReq-6.8K), and a benchmark (RealUIReq-300).


<details>
  <summary>Details</summary>
Motivation: LLMs often replicate accessibility flaws from their training data, creating UIs that exclude users with diverse needs. Current models lack systematic alignment with accessibility standards like WCAG.

Method: A11yn introduces WCAG-compliant web UI optimization by penalizing guideline violations through a severity-scaled reward function trained on UIReq-6.8K (diverse web UI instructions). It evaluates performance using RealUIReq-300 (real-world UI requests).

Result: A11yn reduced inaccessibility rates by 60%, outperforming baselines, while maintaining semantic fidelity and visual quality. Both datasets (UIReq-6.8K and RealUIReq-300 show strong efficacy in training and evaluation.

Conclusion: Accessibility can be systematically optimized into code-generating LLMs through structured alignment with WCAG guidelines, demonstrating feasibility of responsible AI development for inclusive UI design.

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
in generating functional and aesthetic web interfaces directly from
instructions. However, these models often replicate accessibility flaws from
their training data, resulting in interfaces that exclude users with diverse
needs and contexts. To address this gap, we introduce A11yn, the first method
that aligns code-generating LLMs to reliably produce accessibility-compliant
web UIs. A11yn optimizes a novel reward function that penalizes violations of
the Web Content Accessibility Guidelines (WCAG), with penalties scaled to the
severity of each violation as identified by an accessibility testing engine. To
support training, we construct UIReq-6.8K, a dataset of 6,800 diverse
instructions for web UI generation. For evaluation, we introduce RealUIReq-300,
a benchmark of 300 real-world web UI requests grounded and manually curated
from public web pages, spanning a broad range of use cases. Empirical results
show that A11yn significantly outperforms strong baselines, lowering the
Inaccessibility Rate by 60% over the base model while preserving semantic
fidelity and visual quality of generated UIs. These findings demonstrate that
accessibility can be systematically optimized within LLMs, showing the
feasibility of aligning code generation for accessibility.

</details>


### [7] [Signature in Code Backdoor Detection, how far are we?](https://arxiv.org/abs/2510.13992)
*Quoc Hung Le,Thanh Le-Cong,Bach Le,Bowen Xu*

Main category: cs.SE

TL;DR: This paper re-evaluates Spectral Signature defenses for code backdoors, identifying suboptimal settings and introducing a new proxy metric to assess defense effectiveness without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing Spectral Signature methods for backdoor detection in code models lack optimal effectiveness, and there is a need to re-evaluate their applicability and identify factors impacting their performance in adversarial code model scenarios.

Method: The study systematically evaluates Spectral Signature-based defenses under various attack scenarios and defense configurations, analyzes their strengths and limitations, and examines the impact of different settings for key factors. A novel proxy metric is introduced for performance estimation.

Result: Findings reveal suboptimal performance of standard Spectral Signature settings in code-backdoor detection. The new proxy metric accurately predicts defense efficacy without retraining, and critical factors influencing Spectral Signature effectiveness were identified.

Conclusion: The paper concludes that the commonly used Spectral Signature settings for backdoor detection in code models are suboptimal. It proposes a new proxy metric to estimate performance without requiring model retraining after defense.

Abstract: As Large Language Models (LLMs) become increasingly integrated into software
development workflows, they also become prime targets for adversarial attacks.
Among these, backdoor attacks are a significant threat, allowing attackers to
manipulate model outputs through hidden triggers embedded in training data.
Detecting such backdoors remains a challenge, and one promising approach is the
use of Spectral Signature defense methods that identify poisoned data by
analyzing feature representations through eigenvectors. While some prior works
have explored Spectral Signatures for backdoor detection in neural networks,
recent studies suggest that these methods may not be optimally effective for
code models. In this paper, we revisit the applicability of Spectral
Signature-based defenses in the context of backdoor attacks on code models. We
systematically evaluate their effectiveness under various attack scenarios and
defense configurations, analyzing their strengths and limitations. We found
that the widely used setting of Spectral Signature in code backdoor detection
is often suboptimal. Hence, we explored the impact of different settings of the
key factors. We discovered a new proxy metric that can more accurately estimate
the actual performance of Spectral Signature without model retraining after the
defense.

</details>


### [8] [One Bug, Hundreds Behind: LLMs for Large-Scale Bug Discovery](https://arxiv.org/abs/2510.14036)
*Qiushi Wu,Yue Xiao,Dhilung Kirat,Kevin Eykholt,Jiyong Jang,Douglas Lee Schales*

Main category: cs.SE

TL;DR: BugStone combines LLVM and an LLM to detect recurring security bugs in large software, achieving 92.2% precision with high validation success in the Linux kernel. The system automates identification of systemic API misuse patterns, reducing manual effort and mitigating attack surfaces.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the labor-intensive nature of manually fixing recurring bugs in large programs, which risks missing vulnerabilities and expanding attack surfaces. RPBs, caused by a common root cause but unresolved in similar code segments, pose a critical security threat. Automated detection is essential to address these systemic issues efficiently and reduce reliance on manual efforts.

Method: The method involves developing BugStone, a program analysis system leveraging LLVM and a Large Language Model (LLM), which identifies RPBs by analyzing patched instances to detect consistent error patterns. The system scales to large codebases and uses a curated dataset of 1.9K security bugs with 80 recurring patterns to train and validate the approach. It employs pairwise accuracy metrics to evaluate detection effectiveness.

Result: BugStone identified 22,000 potential issues in the Linux kernel, with 246 confirmed as valid after manual analysis of 400 cases. It achieved 92.2% precision and 79.1% pairwise accuracy using a cost-efficient LLM. The dataset of 1.9K security bugs was annotated with 80 recurring patterns and 850 fixes, enabling robust evaluation of the approach.

Conclusion: This paper concludes that Recurring Pattern Bugs (RPBs) are a significant security threat, and the proposed BugStone system effectively detects and addresses these bugs with high precision, demonstrating practical impact through large-scale validation in the Linux kernel. The study highlights the potential of combining program analysis with LLMs to enhance software security at scale.

Abstract: Fixing bugs in large programs is a challenging task that demands substantial
time and effort. Once a bug is found, it is reported to the project
maintainers, who work with the reporter to fix it and eventually close the
issue. However, across the program, there are often similar code segments,
which may also contain the bug, but were missed during discovery. Finding and
fixing each recurring bug instance individually is labor intensive. Even more
concerning, bug reports can inadvertently widen the attack surface as they
provide attackers with an exploitable pattern that may be unresolved in other
parts of the program.
  In this paper, we explore these Recurring Pattern Bugs (RPBs) that appear
repeatedly across various code segments of a program or even in different
programs, stemming from a same root cause, but are unresolved. Our
investigation reveals that RPBs are widespread and can significantly compromise
the security of software programs. This paper introduces BugStone, a program
analysis system empowered by LLVM and a Large Language Model (LLM). The key
observation is that many RPBs have one patched instance, which can be leveraged
to identify a consistent error pattern, such as a specific API misuse. By
examining the entire program for this pattern, it is possible to identify
similar sections of code that may be vulnerable. Starting with 135 unique RPBs,
BugStone identified more than 22K new potential issues in the Linux kernel.
Manual analysis of 400 of these findings confirmed that 246 were valid. We also
created a dataset from over 1.9K security bugs reported by 23 recent top-tier
conference works. We manually annotate the dataset, identify 80 recurring
patterns and 850 corresponding fixes. Even with a cost-efficient model choice,
BugStone achieved 92.2% precision and 79.1% pairwise accuracy on the dataset.

</details>


### [9] [David vs. Goliath: A comparative study of different-sized LLMs for code generation in the domain of automotive scenario generation](https://arxiv.org/abs/2510.14115)
*Philipp Bauerfeind,Amir Salarpour,David Fernandez,Pedram MohajerAnsari,Johannes Reschke,Mert D. Pesé*

Main category: cs.SE

TL;DR: The paper introduces a new framework and dataset for evaluating natural language processing (NLP) to scenario programming (Scenic) in autonomous driving systems, termed NL2Scenic. It utilizes a new evaluation metric called EDIT-COMP to improve ranking consistency and benchmarks various LLMs, finding that mid-scale open-source models offer a balance of effectiveness and cost-efficiency.


<details>
  <summary>Details</summary>
Motivation: Scenario simulation is a critical but challenging component of autonomous driving system testing, with limitations in data availability, reproducibility, and consistent evaluation of natural language interfaces for scenario generation.

Method: The researchers developed an open dataset and evaluation framework called NL2Scenic. Key features include 146 paired natural language (NL) and Scenic examples, a test set of 30 cases with difficulty stratification, retrieval techniques via an Example Retriever, and a range of prompting strategies (Zero-shot, Few-shot, Chain-of-Thought, Self-Projection, Multiple Tree-of-Thought variants). Models were tested on large-scale and open-source LLMs including GPT-4o, GPT-5, Claude-Sonnet-4, Gemini-2.5-Pro, Qwen2.5-Coder (0.5B-32B), and CodeLlama (7B/13B/34B). They introduced the EDIT-SIM metric and further developed EDIT-COMP, a combined metric using F1 score to evaluate both output quality and executable correctness in generated Scenic code.

Result: GPT-4o outperformed all models in the benchmarks. The open-source Qwen2.5-Coder 14B model demonstrated 79.2 % of GPT-4o’s performance while being deployable on local hardware. Retrieval-augmented prompting enhanced the performance of small models effectively and reduced the gap with large models. EDIT-COMP correlated more strongly with human evaluations than existing text metrics, indicating its value as a proxy.

Conclusion: NL2Scenic provides a standardized approach for evaluating models in autonomous driving scenario coding. Mid-size open-source models demonstrate practicality and cost-effectiveness, while the newly proposed EDIT-COMP metric enhances non-human evaluation accuracy.

Abstract: Scenario simulation is central to testing autonomous driving systems. Scenic,
a domain-specific language (DSL) for CARLA, enables precise and reproducible
scenarios, but NL-to-Scenic generation with large language models (LLMs)
suffers from scarce data, limited reproducibility, and inconsistent metrics. We
introduce NL2Scenic, an open dataset and framework with 146 NL/Scenic pairs, a
difficulty-stratified 30-case test split, an Example Retriever, and 14
prompting variants (ZS, FS, CoT, SP, MoT). We evaluate 13 models: four
proprietary (GPT-4o, GPT-5, Claude-Sonnet-4, Gemini-2.5-pro) and nine
open-source code models (Qwen2.5Coder 0.5B-32B; CodeLlama 7B/13B/34B), using
text metrics (BLEU, ChrF, EDIT-SIM, CrystalBLEU) and execution metrics
(compilation and generation), and compare them with an expert study (n=11).
EDIT-SIM correlates best with human judgments; we also propose EDIT-COMP (F1 of
EDIT-SIM and compilation) as a robust dataset-level proxy that improves ranking
fidelity. GPT-4o performs best overall, while Qwen2.5Coder-14B reaches about 88
percent of its expert score on local hardware. Retrieval-augmented prompting,
Few-Shot with Example Retriever (FSER), consistently boosts smaller models, and
scaling shows diminishing returns beyond mid-size, with Qwen2.5Coder
outperforming CodeLlama at comparable scales. NL2Scenic and EDIT-COMP offer a
standardized, reproducible basis for evaluating Scenic code generation and
indicate that mid-size open-source models are practical, cost-effective options
for autonomous-driving scenario programming.

</details>
