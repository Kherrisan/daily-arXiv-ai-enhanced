{"id": "2509.14271", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14271", "abs": "https://arxiv.org/abs/2509.14271", "authors": ["Gustavo Sandoval", "Denys Fenchenko", "Junyao Chen"], "title": "Early Approaches to Adversarial Fine-Tuning for Prompt Injection Defense: A 2022 Study of GPT-3 and Contemporary Models", "comment": null, "summary": "This paper documents early research conducted in 2022 on defending against\nprompt injection attacks in large language models, providing historical context\nfor the evolution of this critical security domain. This research focuses on\ntwo adversarial attacks against Large Language Models (LLMs): prompt injection\nand goal hijacking. We examine how to construct these attacks, test them on\nvarious LLMs, and compare their effectiveness. We propose and evaluate a novel\ndefense technique called Adversarial Fine-Tuning. Our results show that,\nwithout this defense, the attacks succeeded 31\\% of the time on GPT-3 series\nmodels. When using our Adversarial Fine-Tuning approach, attack success rates\nwere reduced to near zero for smaller GPT-3 variants (Ada, Babbage, Curie),\nthough we note that subsequent research has revealed limitations of\nfine-tuning-based defenses. We also find that more flexible models exhibit\ngreater vulnerability to these attacks. Consequently, large models such as\nGPT-3 Davinci are more vulnerable than smaller models like GPT-2. While the\nspecific models tested are now superseded, the core methodology and empirical\nfindings contributed to the foundation of modern prompt injection defense\nresearch, including instruction hierarchy systems and constitutional AI\napproaches.", "AI": {"tldr": "This 2022 paper introduces Adversarial Fine-Tuning as an early defense against prompt injection attacks, demonstrating its partial effectiveness while highlighting vulnerabilities in larger LLMs and influencing later defenses like constitutional AI.", "motivation": "The paper addresses the need to understand and counteract adversarial prompts like prompt injection and goal hijacking, which pose significant security risks to LLMs.", "method": "The study proposes and evaluates Adversarial Fine-Tuning, a defense technique designed to mitigate prompt injection and goal hijacking attacks by training LLMs against adversarial data.", "result": "Adversarial Fine-Tuning reduced attack success rates to near zero for smaller GPT-3 models but failed to fully secure larger models. Larger, more flexible models (e.g., GPT-3 Davinci) showed greater vulnerability.", "conclusion": "The research lays the groundwork for modern prompt injection defense strategies, such as instruction hierarchy systems and constitutional AI, despite recognized limitations in fine-tuning-based approaches."}}
{"id": "2509.14275", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14275", "abs": "https://arxiv.org/abs/2509.14275", "authors": ["Nobin Sarwar", "Shubhashis Roy Dipta"], "title": "FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated LLMs in Mental Health", "comment": "(e.g.: 18 pages, 6 figures, 6 tables)", "summary": "Privacy-preserving adaptation of Large Language Models (LLMs) in sensitive\ndomains (e.g., mental health) requires balancing strict confidentiality with\nmodel utility and safety. We propose FedMentor, a federated fine-tuning\nframework that integrates Low-Rank Adaptation (LoRA) and domain-aware\nDifferential Privacy (DP) to meet per-domain privacy budgets while maintaining\nperformance. Each client (domain) applies a custom DP noise scale proportional\nto its data sensitivity, and the server adaptively reduces noise when utility\nfalls below a threshold. In experiments on three mental health datasets, we\nshow that FedMentor improves safety over standard Federated Learning without\nprivacy, raising safe output rates by up to three points and lowering toxicity,\nwhile maintaining utility (BERTScore F1 and ROUGE-L) within 0.5% of the\nnon-private baseline and close to the centralized upper bound. The framework\nscales to backbones with up to 1.7B parameters on single-GPU clients, requiring\n< 173 MB of communication per round. FedMentor demonstrates a practical\napproach to privately fine-tune LLMs for safer deployments in healthcare and\nother sensitive fields.", "AI": {"tldr": "FedMentor: Federated LLM adaptation with domain-aware privacy that improves safety metrics without significant utility loss in sensitive domains", "motivation": "Large Language Models deployed in sensitive domains require strict confidentiality while maintaining safety and utility - a critical challenge in fields like mental health care.", "method": "Proposes FedMentor framework combining federated fine-tuning with Low-Rank Adaptation (LoRA) and domain-specific Differential Privacy (DP) noise scaling, with adaptive noise reduction when utility thresholds are met.", "result": "Shows 3-point increase in safe output rates and toxicity reduction versus standard FL without privacy, while keeping BERTScore F1 and ROUGE-L within 0.5% of non-private baselines. Achieves 1.7B parameter model adaptation with <173 MB communication per training round.", "conclusion": "FedMentor demonstrates a practical framework for privacy-preserving LLM adaptation in sensitive domains, achieving high safety performance (elevated safe output rates, reduced toxicity) while maintaining model utility close to non-private benchmarks."}}
{"id": "2509.14278", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14278", "abs": "https://arxiv.org/abs/2509.14278", "authors": ["Yuntao Du", "Zitao Li", "Ninghui Li", "Bolin Ding"], "title": "Beyond Data Privacy: New Privacy Risks for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable progress in natural\nlanguage understanding, reasoning, and autonomous decision-making. However,\nthese advancements have also come with significant privacy concerns. While\nsignificant research has focused on mitigating the data privacy risks of LLMs\nduring various stages of model training, less attention has been paid to new\nthreats emerging from their deployment. The integration of LLMs into widely\nused applications and the weaponization of their autonomous abilities have\ncreated new privacy vulnerabilities. These vulnerabilities provide\nopportunities for both inadvertent data leakage and malicious exfiltration from\nLLM-powered systems. Additionally, adversaries can exploit these systems to\nlaunch sophisticated, large-scale privacy attacks, threatening not only\nindividual privacy but also financial security and societal trust. In this\npaper, we systematically examine these emerging privacy risks of LLMs. We also\ndiscuss potential mitigation strategies and call for the research community to\nbroaden its focus beyond data privacy risks, developing new defenses to address\nthe evolving threats posed by increasingly powerful LLMs and LLM-powered\nsystems.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.14282", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.14282", "abs": "https://arxiv.org/abs/2509.14282", "authors": ["Ali Al-kuwari", "Noureldin Mohamed", "Saif Al-kuwari", "Ahmed Farouk", "Bikash K. Behera"], "title": "Resisting Quantum Key Distribution Attacks Using Quantum Machine Learning", "comment": null, "summary": "The emergence of quantum computing poses significant risks to the security of\nmodern communication networks as it breaks today's public-key cryptographic\nalgorithms. Quantum Key Distribution (QKD) offers a promising solution by\nharnessing the principles of quantum mechanics to establish secure keys.\nHowever, practical QKD implementations remain vulnerable to hardware\nimperfections and advanced attacks such as Photon Number Splitting and\nTrojan-Horse attacks. In this work, we investigate the potential of using\nquantum machine learning (QML) to detect popular QKD attacks. In particular, we\npropose a Hybrid Quantum Long Short-Term Memory (QLSTM) model to improve the\ndetection of common QKD attacks. By combining quantum-enhanced learning with\nclassical deep learning, the model captures complex temporal patterns in QKD\ndata, improving detection accuracy. To evaluate the proposed model, we\nintroduce a realistic QKD dataset simulating normal QKD operations along with\nseven attack scenarios, Intercept-and-Resend, Photon-Number Splitting (PNS),\nTrojan-Horse attacks Random Number Generator (RNG), Detector Blinding,\nWavelength-dependent Trojan Horse, and Combined attacks. The dataset includes\nquantum security metrics such as Quantum Bit Error Rate (QBER), measurement\nentropy, signal and decoy loss rates, and time-based metrics, ensuring an\naccurate representation of real-world conditions. Our results demonstrate\npromising performance of the quantum machine learning approach compared to\ntraditional classical machine learning models, highlighting the potential of\nhybrid techniques to enhance the security of future quantum communication\nnetworks. The proposed Hybrid QLSTM model achieved an accuracy of 93.7.0\\%\nafter 50 training epochs, outperforming classical deep learning models such as\nLSTM, and CNN.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.14265", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14265", "abs": "https://arxiv.org/abs/2509.14265", "authors": ["Siyuan Chen", "Zhichao Lu", "Qingfu Zhang"], "title": "Evolution of Kernels: Automated RISC-V Kernel Optimization with Large Language Models", "comment": "Technical report", "summary": "Automated kernel design is critical for overcoming software ecosystem\nbarriers in emerging hardware platforms like RISC-V. While large language\nmodels (LLMs) have shown promise for automated kernel optimization,\ndemonstrating success in CUDA domains with comprehensive technical documents\nand mature codebases, their effectiveness remains unproven for reference-scarce\ndomains like RISC-V. We present Evolution of Kernels (EoK), a novel LLM-based\nevolutionary program search framework that automates kernel design for domains\nwith limited reference material. EoK mitigates reference scarcity by mining and\nformalizing reusable optimization ideas (general design principles + actionable\nthoughts) from established kernel libraries' development histories; it then\nguides parallel LLM explorations using these ideas, enriched via\nRetrieval-Augmented Generation (RAG) with RISC-V-specific context, prioritizing\nhistorically effective techniques. Empirically, EoK achieves a median 1.27x\nspeedup, surpassing human experts on all 80 evaluated kernel design tasks and\nimproving upon prior LLM-based automated kernel design methods by 20%. These\nresults underscore the viability of incorporating human experience into\nemerging domains and highlight the immense potential of LLM-based automated\nkernel optimization.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.14284", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14284", "abs": "https://arxiv.org/abs/2509.14284", "authors": ["Vaidehi Patil", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration", "comment": "Code: https://github.com/Vaidehi99/MultiAgentPrivacy", "summary": "As large language models (LLMs) become integral to multi-agent systems, new\nprivacy risks emerge that extend beyond memorization, direct inference, or\nsingle-turn evaluations. In particular, seemingly innocuous responses, when\ncomposed across interactions, can cumulatively enable adversaries to recover\nsensitive information, a phenomenon we term compositional privacy leakage. We\npresent the first systematic study of such compositional privacy leaks and\npossible mitigation methods in multi-agent LLM systems. First, we develop a\nframework that models how auxiliary knowledge and agent interactions jointly\namplify privacy risks, even when each response is benign in isolation. Next, to\nmitigate this, we propose and evaluate two defense strategies: (1)\nTheory-of-Mind defense (ToM), where defender agents infer a questioner's intent\nby anticipating how their outputs may be exploited by adversaries, and (2)\nCollaborative Consensus Defense (CoDef), where responder agents collaborate\nwith peers who vote based on a shared aggregated state to restrict sensitive\ninformation spread. Crucially, we balance our evaluation across compositions\nthat expose sensitive information and compositions that yield benign\ninferences. Our experiments quantify how these defense strategies differ in\nbalancing the privacy-utility trade-off. We find that while chain-of-thought\nalone offers limited protection to leakage (~39% sensitive blocking rate), our\nToM defense substantially improves sensitive query blocking (up to 97%) but can\nreduce benign task success. CoDef achieves the best balance, yielding the\nhighest Balanced Outcome (79.8%), highlighting the benefit of combining\nexplicit reasoning with defender collaboration. Together, our results expose a\nnew class of risks in collaborative LLM deployments and provide actionable\ninsights for designing safeguards against compositional, context-driven privacy\nleakage.", "AI": {"tldr": "This work identifies a new class of privacy risks in multi-agent LLM systems and introduces effective defenses, finding that CoDef best balances privacy protection with task utility.", "motivation": "Emerging privacy risks in multi-agent LLM systems go beyond memorization or direct inference, as cumulative benign responses across interactions can inadvertently enable sensitive information recovery by adversaries.", "method": "The authors develop a framework to model privacy risks from agent interactions and propose two defense strategies: Theory-of-Mind defense (ToM) and Collaborative Consensus Defense (CoDef).", "result": "ToM defense achieves 97% sensitive query blocking but reduces benign task success, while CoDef achieves 79.8% balanced outcomes by combining reasoning and collaboration, outperforming chain-of-thought defenses.", "conclusion": "The paper highlights compositional privacy leakage in multi-agent LLM systems and demonstrates that Collaborative Consensus Defense (CoDef) achieves the best balance between privacy and utility."}}
{"id": "2509.14273", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.14273", "abs": "https://arxiv.org/abs/2509.14273", "authors": ["Swapnil Sharma Sarker", "Tanzina Taher Ifty"], "title": "Automated and Context-Aware Code Documentation Leveraging Advanced LLMs", "comment": null, "summary": "Code documentation is essential to improve software maintainability and\ncomprehension. The tedious nature of manual code documentation has led to much\nresearch on automated documentation generation. Existing automated approaches\nprimarily focused on code summarization, leaving a gap in template-based\ndocumentation generation (e.g., Javadoc), particularly with publicly available\nLarge Language Models (LLMs). Furthermore, progress in this area has been\nhindered by the lack of a Javadoc-specific dataset that incorporates modern\nlanguage features, provides broad framework/library coverage, and includes\nnecessary contextual information. This study aims to address these gaps by\ndeveloping a tailored dataset and assessing the capabilities of publicly\navailable LLMs for context-aware, template-based Javadoc generation. In this\nwork, we present a novel, context-aware dataset for Javadoc generation that\nincludes critical structural and semantic information from modern Java\ncodebases. We evaluate five open-source LLMs (including LLaMA-3.1, Gemma-2,\nPhi-3, Mistral, Qwen-2.5) using zero-shot, few-shot, and fine-tuned setups and\nprovide a comparative analysis of their performance. Our results demonstrate\nthat LLaMA 3.1 performs consistently well and is a reliable candidate for\npractical, automated Javadoc generation, offering a viable alternative to\nproprietary systems.", "AI": {"tldr": "Researchers created a specialized Javadoc dataset and found LLaMA 3.1 to be the top public LLM for template-based documentation, promising as a free alternative to proprietary tools.", "motivation": "The paper addresses gaps in automated Javadoc generation by tackling the lack of a modern, context-enriched dataset for template-based documentation and the need to assess public LLMs as alternatives to proprietary systems.", "method": "The research introduces a novel, context-aware dataset for Javadoc generation that incorporates structural and semantic information from modern Java codebases. Five open-source LLMs are evaluated under zero-shot, few-shot, and fine-tuned setups, with performance comparisons conducted.", "result": "LLaMA 3.1 consistently outperforms evaluated models, showing strong potential for real-world Javadoc generation. The newly developed dataset supports modern Java features, framework coverage, and contextual information, enabling robust model evaluation.", "conclusion": "The study concludes that LLaMA 3.1 emerges as a reliable and effective open-source model for context-aware, template-based Javadoc generation, providing a practical alternative to proprietary systems."}}
{"id": "2509.14285", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14285", "abs": "https://arxiv.org/abs/2509.14285", "authors": ["S M Asif Hossain", "Ruksat Khan Shayoni", "Mohd Ruhul Ameen", "Akif Islam", "M. F. Mridha", "Jungpil Shin"], "title": "A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks", "comment": null, "summary": "Prompt injection attacks represent a major vulnerability in Large Language\nModel (LLM) deployments, where malicious instructions embedded in user inputs\ncan override system prompts and induce unintended behaviors. This paper\npresents a novel multi-agent defense framework that employs specialized LLM\nagents in coordinated pipelines to detect and neutralize prompt injection\nattacks in real-time. We evaluate our approach using two distinct\narchitectures: a sequential chain-of-agents pipeline and a hierarchical\ncoordinator-based system. Our comprehensive evaluation on 55 unique prompt\ninjection attacks, grouped into 8 categories and totaling 400 attack instances\nacross two LLM platforms (ChatGLM and Llama2), demonstrates significant\nsecurity improvements. Without defense mechanisms, baseline Attack Success\nRates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent\npipeline achieved 100% mitigation, reducing ASR to 0% across all tested\nscenarios. The framework demonstrates robustness across multiple attack\ncategories including direct overrides, code execution attempts, data\nexfiltration, and obfuscation techniques, while maintaining system\nfunctionality for legitimate queries.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.14279", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14279", "abs": "https://arxiv.org/abs/2509.14279", "authors": ["Robert Tjarko Lange", "Qi Sun", "Aaditya Prasad", "Maxence Faldor", "Yujin Tang", "David Ha"], "title": "Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization", "comment": "62 pages, 10 figures", "summary": "Recent advances in large language models (LLMs) demonstrate their\neffectiveness in scaling test-time compute for software engineering tasks.\nHowever, these approaches often focus on high-level solutions, with limited\nattention to optimizing low-level CUDA kernel implementations. Additionally,\nexisting kernel generation benchmarks suffer from exploitable loopholes and\ninsufficient diversity in testing conditions, hindering true generalization\nassessment. To address these limitations, we introduce robust-kbench, a new\nbenchmark for rigorous evaluation of kernel performance and correctness across\nvaried scenarios. Furthermore, we present a comprehensive agentic framework\nthat automates CUDA kernel discovery, verification, and optimization. This\npipeline enables frontier LLMs to translate torch code to CUDA kernels and\niteratively improve their runtime within our robust evaluation setting. Our\nsequential workflow first translates PyTorch code into equivalent CUDA kernels.\nIt then optimizes their runtime using a novel evolutionary meta-generation\nprocedure tailored to the CUDA ecosystem, guided by LLM-based verifiers for\ncorrectness and efficient filtering. Evaluated on robust-kbench, our approach\nproduces CUDA kernels outperforming torch implementations for practical\napplications, including forward and backward passes. It can fuse operations and\ndeploy various runtime optimization strategies. The verifier workflow\naccurately classifies incorrect kernels, enhancing hardware verification\nefficiency.", "AI": {"tldr": "The study presents a robust-kbench benchmark and an agentic framework for automated CUDA kernel discovery and optimization, showing improved performance in practical applications.", "motivation": "Current LLMs excels at high-level solutions but lack optimization for CUDA kernels. Benchmarks for this area are flawed, leading to unreliable assessments of generalization.", "method": "Introduce robust-kbench for rigorous testing and a comprehensive agentic framework. The framework includes translating PyTorch code to CUDA, evolutionary meta-generation for runtime optimization, and LLM-based verification and filtering.", "result": "On robust-kbench, the proposed approach outperforms torch in CUDA task, enables operation fusing and runtime optimization strategies, and achieves efficient verification via accurate kernel classification.", "conclusion": "The new benchmark and framework for CUDA kernel provide efficient and accurate solutions, demonstrating LLMs potential in low-level optimization with robust testing."}}
{"id": "2509.14297", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14297", "abs": "https://arxiv.org/abs/2509.14297", "authors": ["Xuan Luo", "Yue Wang", "Zefeng He", "Geng Tu", "Jing Li", "Ruifeng Xu"], "title": "A Simple and Efficient Jailbreak Method Exploiting LLMs' Helpfulness", "comment": null, "summary": "Safety alignment aims to prevent Large Language Models (LLMs) from responding\nto harmful queries. To strengthen safety protections, jailbreak methods are\ndeveloped to simulate malicious attacks and uncover vulnerabilities. In this\npaper, we introduce HILL (Hiding Intention by Learning from LLMs), a novel\njailbreak approach that systematically transforms imperative harmful requests\ninto learning-style questions with only straightforward hypotheticality\nindicators. Further, we introduce two new metrics to thoroughly evaluate the\nutility of jailbreak methods. Experiments on the AdvBench dataset across a wide\nrange of models demonstrate HILL's strong effectiveness, generalizability, and\nharmfulness. It achieves top attack success rates on the majority of models and\nacross malicious categories while maintaining high efficiency with concise\nprompts. Results of various defense methods show the robustness of HILL, with\nmost defenses having mediocre effects or even increasing the attack success\nrates. Moreover, the assessment on our constructed safe prompts reveals\ninherent limitations of LLMs' safety mechanisms and flaws in defense methods.\nThis work exposes significant vulnerabilities of safety measures against\nlearning-style elicitation, highlighting a critical challenge of balancing\nhelpfulness and safety alignments.", "AI": {"tldr": "HILL, a novel jailbreak method, exploits LLM safety systems using learning-style prompts, achieving strong performance while exposing critical limitations in existing safety measures.", "motivation": "Existing safety alignment methods for LLMs remain vulnerable to sophisticated jailbreak attacks. This work aims to identify and exploit these weaknesses through a systematic approach to stimulate safer, more resilient model designs.", "method": "HILL transforms imperative harmful requests into learning-style questions using straightforward hypothetical indicators. The approach introduces two novel evaluation metrics to comprehensively assess jailbreak effectiveness. Experiments on AdvBench demonstrate its superior performance across diverse models and malicious categories.", "result": "HILL achieves top attack success rates on AdvBench with high efficiency while concise prompts maintain effectiveness. Defense methods show limited efficacy, with some paradoxically improving attack success rates. Analysis of safe prompts reveals inherent flaws in current safety mechanisms.", "conclusion": "HILL's success in exploiting LLM safety mechanisms highlights the critical challenge of balancing helpfulness and safety in model alignment. The study underscores the need for robust, adaptive safety measures to address vulnerabilities exacerbated by learning-style elicitation tactics."}}
{"id": "2509.14281", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14281", "abs": "https://arxiv.org/abs/2509.14281", "authors": ["Xifeng Yao", "Dongyu Lang", "Wu Zhang", "Xintong Guo", "Huarui Xie", "Yinhao Ni", "Ping Liu", "Guang Shen", "Yi Bai", "Dandan Tu", "Changzheng Zhang"], "title": "SCoGen: Scenario-Centric Graph-Based Synthesis of Real-World Code Problems", "comment": null, "summary": "Significant advancements have been made in the capabilities of code large\nlanguage models, leading to their rapid adoption and application across a wide\nrange of domains. However, their further advancements are often constrained by\nthe scarcity of real-world coding problems. To bridge this gap, we propose a\nnovel framework for synthesizing code problems that emulate authentic\nreal-world scenarios. This framework systematically integrates domain\nknowledge, domain skills, and coding skills, all of which are meticulously\nextracted from real-world programming-related datasets, including Stack\nOverflow and Kaggle. The extracted elements serve as the foundational building\nblocks for constructing code problems. To align the generated problems with\npractical applications, application scenarios are also mined from the\naforementioned datasets. These scenarios are then utilized to construct a\nscenario-centric graph that interconnects domain knowledge, domain skills, and\ncoding skills. Based on this structured representation, a sampling strategy on\nthe graph is designed, which effectively controls the generation of a code\nproblem with complexity and diversity, reflects real-world challenges.\nExperimental results demonstrate that the proposed method consistently achieves\nsuperior performance over state-of-the-art open-source large language models of\nvarying sizes and functionalities, including both coders and general-purpose\nmodels, across a diverse set of real-world benchmarks.", "AI": {"tldr": "A novel framework synthesizes realistic code problems by integrating domain knowledge and coding skills from real-world data, enabling enhanced training and application of code LLMs.", "motivation": "Advances in code LLMs are hindered by limited real-world coding problems. Realistic synthetic benchmarks are needed to sustain progress.", "method": "The framework extracts domain/coding knowledge-skills from Stack Overflow/Kaggle, constructs a scenario-centric graph linking these elements, and employs a graph-based sampling strategy to control problem complexity/diversity.", "result": "Outperforms state-of-the-art coders and general LLMs across diverse real-world benchmarks, demonstrating superior performance across model scales and functionalities.", "conclusion": "Structured representation of real-world scenarios enables effective code problem generation, advancing code LLM capabilities through domain-informed synthetic data creation."}}
