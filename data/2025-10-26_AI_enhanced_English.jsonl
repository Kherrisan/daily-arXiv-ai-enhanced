{"id": "2510.19860", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2510.19860", "abs": "https://arxiv.org/abs/2510.19860", "authors": ["Ketai Qiu", "Luca Di Grazia", "Leonardo Mariani", "Mauro Pezz\u00e8"], "title": "E-Test: E'er-Improving Test Suites", "comment": "Accepted at the 48th IEEE/ACM International Conference on Software\n  Engineering (ICSE 2026)", "summary": "Test suites are inherently imperfect, and testers can always enrich a suite\nwith new test cases that improve its quality and, consequently, the reliability\nof the target software system. However, finding test cases that explore\nexecution scenarios beyond the scope of an existing suite can be extremely\nchallenging and labor-intensive, particularly when managing large test suites\nover extended periods.\n  In this paper, we propose E-Test, an approach that reduces the gap between\nthe execution space explored with a test suite and the executions experienced\nafter testing by augmenting the test suite with test cases that explore\nexecution scenarios that emerge in production. E-Test (i) identifies executions\nthat have not yet been tested from large sets of scenarios, such as those\nmonitored during intensive production usage, and (ii) generates new test cases\nthat enhance the test suite. E-Test leverages Large Language Models (LLMs) to\npinpoint scenarios that the current test suite does not adequately cover, and\naugments the suite with test cases that execute these scenarios.\n  Our evaluation on a dataset of 1,975 scenarios, collected from highly-starred\nopen-source Java projects already in production and Defects4J, demonstrates\nthat E-Test retrieves not-yet-tested execution scenarios significantly better\nthan state-of-the-art approaches. While existing regression testing and field\ntesting approaches for this task achieve a maximum F1-score of 0.34, and\nvanilla LLMs achieve a maximum F1-score of 0.39, E-Test reaches 0.55. These\nresults highlight the impact of E-Test in enhancing test suites by effectively\ntargeting not-yet-tested execution scenarios and reducing manual effort\nrequired for maintaining test suites.", "AI": {"tldr": "E-Test uses LLMs to identify and add new test cases for untested execution scenarios in existing test suites, outperforming state-of-the-art methods with an F1-score of 0.55.", "motivation": "Existing test suites are incomplete, requiring manual effort to identify and insert new test cases for untested execution scenarios in production software.", "method": "E-Test leverages LLMs to analyze large scenario datasets from production monitors, identifies uncovered scenarios, and generates corresponding test cases.", "result": "Achieved an F1-score of 0.55 on a dataset of 1,975 execution scenarios, outperforming regression/field testing approaches (0.34) and vanilla LLMs (0.39).", "conclusion": "E-Test effectively enhances test suite coverage by detecting and adding tests for previously untested production scenarios via LLMs, reducing manual maintenance effort."}}
{"id": "2510.19864", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19864", "abs": "https://arxiv.org/abs/2510.19864", "authors": ["Amila Indika", "Igor Molybog"], "title": "SODBench: A Large Language Model Approach to Documenting Spreadsheet Operations", "comment": "14 pages, 5 figures, 4 tables", "summary": "Numerous knowledge workers utilize spreadsheets in business, accounting, and\nfinance. However, a lack of systematic documentation methods for spreadsheets\nhinders automation, collaboration, and knowledge transfer, which risks the loss\nof crucial institutional knowledge. This paper introduces Spreadsheet\nOperations Documentation (SOD), an AI task that involves generating\nhuman-readable explanations from spreadsheet operations. Many previous studies\nhave utilized Large Language Models (LLMs) for generating spreadsheet\nmanipulation code; however, translating that code into natural language for SOD\nis a less-explored area. To address this, we present a benchmark of 111\nspreadsheet manipulation code snippets, each paired with a corresponding\nnatural language summary. We evaluate five LLMs, GPT-4o, GPT-4o-mini,\nLLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B, using BLEU, GLEU, ROUGE-L, and\nMETEOR metrics. Our findings suggest that LLMs can generate accurate\nspreadsheet documentation, making SOD a feasible prerequisite step toward\nenhancing reproducibility, maintainability, and collaborative workflows in\nspreadsheets, although there are challenges that need to be addressed.", "AI": {"tldr": "This paper introduces Spreadsheet Operations Documentation (SOD), an AI task to generate human-readable explanations from spreadsheet code, presenting a benchmark of 111 code snippets with summaries and evaluating five LLMs.", "motivation": "The lack of systematic documentation in spreadsheets creates risks for automation, collaboration, and preserving institutional knowledge, particularly within business and finance contexts. Effective documentation is needed to enhance reproducibility and maintainability.", "method": "The authors define the SOD task, compile a benchmark of 111 paired spreadsheet code snippets and natural language summaries, and assess the performance of five LLMs (GPT-4o, GPT-4o-mini, LLaMA-3.3-70B, Mixtral-8x7B, Gemma2-9B) using BLEU, GLEU, ROUGE-L, and METEOR metrics.", "result": "The evaluation demonstrates that the tested LLMs are capable of generating accurate, human-readable SOD explanations, indicating feasibility in improving spreadsheet processes, though challenges remain.", "conclusion": "The paper concludes that SOD is a viable preprocessing method to enhance reproducibility and collaboration in spreadsheet workflows, with existing LLMs showing effectiveness, but notes room for improvement and further research."}}
{"id": "2510.19868", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19868", "abs": "https://arxiv.org/abs/2510.19868", "authors": ["Qian Xiong", "Bo Yang", "Weisong Sun", "Yiran Zhang", "Tianlin Li", "Yang Liu", "Zhi Jin"], "title": "Knowledge-Guided Multi-Agent Framework for Application-Level Software Code Generation", "comment": null, "summary": "Automated code generation driven by Large Lan- guage Models (LLMs) has\nenhanced development efficiency, yet generating complex application-level\nsoftware code remains challenging. Multi-agent frameworks show potential, but\nexisting methods perform inadequately in large-scale application-level software\ncode generation, failing to ensure reasonable orga- nizational structures of\nproject code and making it difficult to maintain the code generation process.\nTo address this, this paper envisions a Knowledge-Guided Application-Level Code\nGeneration framework named KGACG, which aims to trans- form software\nrequirements specification and architectural design document into executable\ncode through a collaborative closed- loop of the Code Organization & Planning\nAgent (COPA), Coding Agent (CA), and Testing Agent (TA), combined with a\nfeedback mechanism. We demonstrate the collaborative process of the agents in\nKGACG in a Java Tank Battle game case study while facing challenges. KGACG is\ndedicated to advancing the automation of application-level software\ndevelopment.", "AI": {"tldr": "The paper introduces a framework for generating complex application-level code using a collaborative multi-agent system and knowledge guidance to improve structure and maintainability.", "motivation": "Current LLM-based code generation struggles with complex application code due to limited organization and maintainability. Multi-agent methods lack in handling this effectively.", "method": "KGACG combines three agents (COPA, CA, TA) in a closed-loop process: COPA plans code structure, CA generates code, and TA tests it with feedback to refine the output.", "result": "KGACG is validated through a Java Tank Battle game case study, showing its effectiveness in generating structured, maintainable code with collaborative agents and a feedback loop.", "conclusion": "KGACG advances automation in complex software development using a knowledge-driven multi-agent system to ensure proper code organization and iterative refinement."}}
{"id": "2510.19898", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19898", "abs": "https://arxiv.org/abs/2510.19898", "authors": ["Atharv Sonwane", "Isadora White", "Hyunji Lee", "Matheus Pereira", "Lucas Caccia", "Minseon Kim", "Zhengyan Shi", "Chinmay Singh", "Alessandro Sordoni", "Marc-Alexandre C\u00f4t\u00e9", "Xingdi Yuan"], "title": "BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills", "comment": null, "summary": "High quality bugs are key to training the next generation of language model\nbased software engineering (SWE) agents. We introduce a novel method for\nsynthetic generation of difficult and diverse bugs. Our method instructs SWE\nAgents to introduce a feature into the codebase whereby they may\nunintentionally break tests, resulting in bugs. Prior approaches often induce\nan out-of-distribution effect by generating bugs intentionally (e.g. by\nintroducing local perturbation to existing code), which does not reflect\nrealistic development processes. We perform qualitative analysis to demonstrate\nthat our approach for generating bugs more closely reflects the patterns found\nin human-authored edits. Through extensive experiments, we demonstrate that our\nbugs provide more efficient training data for supervised fine-tuning,\noutperforming other bug datasets by 2% with half the training data (1.2k vs. 3k\nbugs). We train on our newly generated bugs in addition to existing bug\ndatasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench\nVerified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on\nSWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over\nthree seeds.", "AI": {"tldr": "The paper introduces a novel method for generating high-quality, diverse bugs in a synthetic way by instructing SWE agents to introduce features that inadvertently break tests. The resulting bugs are more realistic and efficient for training, leading to state-of-the-art SWE models.", "motivation": "High-quality bugs are essential for training SWE agents, as they improve the agents' ability to detect and fix issues. Prior synthetic bug generation approaches create bugs intentionally (e.g., via perturbations), which may not reflect realistic development scenarios where bugs are often unintentional side effects of feature implementations.", "method": "The method involves training SWE agents to introduce new features into codebases, which can inadvertently break tests, creating bugs. Unintentional bugs in this way mimic real-world development errors, where changes for new functionality may result in unintended consequences. This contrasts with prior work that often creates bugs via local, intentional code changes.", "result": "Through qualitative analysis and extensive experiments, the approach shows that the generated bugs are more realistic and lead to improved training efficiency. Their models trained on this data achieve better performance compared to models trained with existing bug datasets. FrogBoss (32B) reaches a pass@1 of 54.6% and FrogMini (14B) reaches 45.3% on SWE-bench Verified.", "conclusion": "The proposed bug generation method results in high-quality, realistic bugs suitable for training SWE agents, offering better efficiency and performance compared to existing synthetic bug creation approaches. The method bridges the gap between synthetic data and real development scenarios, improving the effectiveness of agents on standard benchmarks."}}
{"id": "2510.19844", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19844", "abs": "https://arxiv.org/abs/2510.19844", "authors": ["Isaac Wu", "Michael Maslowski"], "title": "CourtGuard: A Local, Multiagent Prompt Injection Classifier", "comment": "11 pages, 7 figures", "summary": "As large language models (LLMs) become integrated into various sensitive\napplications, prompt injection, the use of prompting to induce harmful\nbehaviors from LLMs, poses an ever increasing risk. Prompt injection attacks\ncan cause LLMs to leak sensitive data, spread misinformation, and exhibit\nharmful behaviors. To defend against these attacks, we propose CourtGuard, a\nlocally-runnable, multiagent prompt injection classifier. In it, prompts are\nevaluated in a court-like multiagent LLM system, where a \"defense attorney\"\nmodel argues the prompt is benign, a \"prosecution attorney\" model argues the\nprompt is a prompt injection, and a \"judge\" model gives the final\nclassification. CourtGuard has a lower false positive rate than the Direct\nDetector, an LLM as-a-judge. However, CourtGuard is generally a worse prompt\ninjection detector. Nevertheless, this lower false positive rate highlights the\nimportance of considering both adversarial and benign scenarios for the\nclassification of a prompt. Additionally, the relative performance of\nCourtGuard in comparison to other prompt injection classifiers advances the use\nof multiagent systems as a defense against prompt injection attacks. The\nimplementations of CourtGuard and the Direct Detector with full prompts for\nGemma-3-12b-it, Llama-3.3-8B, and Phi-4-mini-instruct are available at\nhttps://github.com/isaacwu2000/CourtGuard.", "AI": {"tldr": "The paper discusses prompt injection attacks that can exploit large language models (LLMs) to act harmfully and presents CourtGuard, a multiagent system designed to reduce false positives in detecting these attacks by simulating a court process.", "motivation": "The increasing use of large language models (LLMs) in sensitive applications has raised concerns about prompt injection attacks, which can lead to harmful behaviors and data breaches. The need for more accurate and reliable detection methods that minimize false positives is highlighted to ensure safe deployment of LLMs.", "method": "The authors propose CourtGuard, a locally-operable multiagent system for detecting prompt injection attacks in LLMs. It uses three models, each with distinct roles: a defense attorney, a prosecution attorney, and a judge, to simulate a court process. The defense attorney argues that the prompt is benign, the prosecution attorney argues that it\u2019s a harmful prompt injection, and the judge determines the final classification based on their arguments.", "result": "The experiments showed that CourtGuard has a lower false positive rate compared to the Direct Detector, an LLM as-a-judge approach. However, CourtGuard is generally less effective as a prompt injection detector compared to some alternatives, indicating a trade-off between accuracy and reducing false positives in prompt filtering systems.", "conclusion": "While CourtGuard does not outperform existing prompt injection detection methods in terms of overall accuracy, its ability to reduce false positives underlines the value of multiagent systems in evaluating potential injection attacks from both adversarial and benign perspectives. The study also encourages future research into multiagent-based defense mechanisms to enhance the robustness of LLMs against prompt injection attacks."}}
{"id": "2510.19984", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2510.19984", "abs": "https://arxiv.org/abs/2510.19984", "authors": ["Konstantinos Kitsios", "Marcel B\u00f6hme", "Alberto Bacchelli"], "title": "On Interaction Effects in Greybox Fuzzing", "comment": "12 pages, 2 figures, Accepted for presentation at the 48th\n  International Conference on Software Engineering (ICSE '26)", "summary": "A greybox fuzzer is an automated software testing tool that generates new\ntest inputs by applying randomly chosen mutators (e.g., flipping a bit or\ndeleting a block of bytes) to a seed input in random order and adds all\ncoverage-increasing inputs to the corpus of seeds. We hypothesize that the\norder in which mutators are applied to a seed input has an impact on the\neffectiveness of greybox fuzzers. In our experiments, we fit a linear model to\na dataset that contains the effectiveness of all possible mutator pairs and\nindeed observe the conjectured interaction effect. This points us to more\nefficient fuzzing by choosing the most promising mutator sequence with a higher\nlikelihood. We propose MuoFuzz, a greybox fuzzer that learns and chooses the\nmost promising mutator sequences. MuoFuzz learns the conditional probability\nthat the next mutator will yield an interesting input, given the previously\nselected mutator. Then, it samples from the learned probability using a random\nwalk to generate mutator sequences. We compare the performance of MuoFuzz to\nAFL++, which uses a fixed selection probability, and MOPT, which optimizes the\nselection probability of each mutator in isolation. Experimental results on the\nFuzzBench and MAGMA benchmarks show that MuoFuzz achieves the highest code\ncoverage and finds four bugs missed by AFL++ and one missed by both AFL++ and\nMOPT.", "AI": {"tldr": "TLDR; MuoFuzz is a greybox fuzzer that improves effectiveness by learning optimal mutator sequences, outperforming AFL++ and MOPT in code coverage and bug discovery.", "motivation": "The paper motivates the need for optimizing mutator sequence order in greybox fuzzing to enhance test input generation and fuzzer effectiveness. Greybox fuzzers currently apply mutators randomly, potentially missing more efficient sequences that could increase code coverage and detect bugs more effectively.", "method": "MuoFuzz learns the order of mutator application by fitting a linear model to a dataset of mutator pair effectiveness, estimating conditional probabilities for the next effective mutator. These probabilities guide a random walk to generate mutator sequences for fuzzing.", "result": "On FuzzBench and MAGMA benchmarks, MuoFuzz demonstrated the highest code coverage among tested fuzzers and identified four bugs undetected by AFL++, as well as one unique bug compared to both AFL++ and MOPT.", "conclusion": "By considering mutator sequence interactions, MuoFuzz significantly outperforms traditional greybox fuzzing strategies. This work highlights the importance of order-aware mutation for maximizing fuzzer performance in regression testing and vulnerability detection."}}
{"id": "2510.19851", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19851", "abs": "https://arxiv.org/abs/2510.19851", "authors": ["Artur Zolkowski", "Wen Xing", "David Lindner", "Florian Tram\u00e8r", "Erik Jenner"], "title": "Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability", "comment": null, "summary": "Recent findings suggest that misaligned models may exhibit deceptive\nbehavior, raising concerns about output trustworthiness. Chain-of-thought (CoT)\nis a promising tool for alignment monitoring: when models articulate their\nreasoning faithfully, monitors can detect and mitigate harmful behaviors before\nundesirable outcomes occur. However, a key uncertainty is: Can models obfuscate\ntheir CoT in order to pursue hidden adversarial objectives while evading\ndetection? To answer this question and thus stress-test CoT monitorability, we\ndevelop a composable and quantifiable taxonomy of prompts to elicit CoT\nobfuscation. We evaluate both internal CoT (reasoning traces) and external CoT\n(prompted reasoning in outputs) using toy tasks and more realistic environments\nin SHADE-Arena. We show that: (i) CoT monitoring performs accurately and\nefficiently without obfuscation pressure. (ii) Under strong obfuscation\npressure, some models successfully complete adversarial tasks while evading\ndetection. (iii) Models do not obfuscate their internal CoT as much as their\nexternal CoT (under prompt pressure). These results suggest that while CoT\nprovides valuable oversight in benign settings, robust deployment requires\nmodel-specific stress-testing of monitorability.", "AI": {"tldr": "This paper evaluates the effectiveness of chain-of-thought (CoT) monitoring under obfuscation pressure, finding that while CoT works well normally, some models can evade detection when pressured, and internal CoT is less obfuscated than external CoT.", "motivation": "The motivation stems from concerns that misaligned models might display deceptive behavior, compromising output trustworthiness. The paper seeks to determine if models can obfuscate their CoT to pursue hidden adversarial objectives while avoiding detection.", "method": "The authors develop a composable and quantifiable taxonomy of prompts to elicit CoT obfuscation. They evaluate both internal and external CoT using toy tasks and more realistic environments in the SHADE-Arena framework.", "result": "The results show (i) CoT monitoring is accurate and efficient without obfuscation pressure (ii) some models can complete adversarial tasks while evading detection under strong obfuscation pressure (iii) internal CoT is less obfuscated than external CoT under prompt pressure.", "conclusion": "The study concludes that CoT is a valuable oversight tool in benign scenarios, but for robust deployment, it is necessary to conduct model-specific stress-testing of monitorability."}}
{"id": "2510.19997", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19997", "abs": "https://arxiv.org/abs/2510.19997", "authors": ["Abraham Itzhak Weinberg"], "title": "A Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises (FAIGMOE)", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) presents transformative\nopportunities for organizations, yet both midsize organizations and larger\nenterprises face distinctive adoption challenges. Midsize organizations\nencounter resource constraints and limited AI expertise, while enterprises\nstruggle with organizational complexity and coordination challenges. Existing\ntechnology adoption frameworks, including TAM (Technology Acceptance Model),\nTOE (Technology Organization Environment), and DOI (Diffusion of Innovations)\ntheory, lack the specificity required for GenAI implementation across these\ndiverse contexts, creating a critical gap in adoption literature. This paper\nintroduces FAIGMOE (Framework for the Adoption and Integration of Generative AI\nin Midsize Organizations and Enterprises), a conceptual framework addressing\nthe unique needs of both organizational types. FAIGMOE synthesizes technology\nadoption theory, organizational change management, and innovation diffusion\nperspectives into four interconnected phases: Strategic Assessment, Planning\nand Use Case Development, Implementation and Integration, and\nOperationalization and Optimization. Each phase provides scalable guidance on\nreadiness assessment, strategic alignment, risk governance, technical\narchitecture, and change management adaptable to organizational scale and\ncomplexity. The framework incorporates GenAI specific considerations including\nprompt engineering, model orchestration, and hallucination management that\ndistinguish it from generic technology adoption frameworks. As a perspective\ncontribution, FAIGMOE provides the first comprehensive conceptual framework\nexplicitly addressing GenAI adoption across midsize and enterprise\norganizations, offering actionable implementation protocols, assessment\ninstruments, and governance templates requiring empirical validation through\nfuture research.", "AI": {"tldr": "TLDR", "motivation": "MOTION", "method": "METHOD", "result": "RESULT", "conclusion": "CONCLUSION"}}
{"id": "2510.19856", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19856", "abs": "https://arxiv.org/abs/2510.19856", "authors": ["Eranga Bandara", "Sachin Shetty", "Ravi Mukkamala", "Ross Gore", "Peter Foytik", "Safdar H. Bouk", "Abdul Rahman", "Xueping Liang", "Ng Wee Keong", "Kasun De Zoysa", "Aruna Withanage", "Nilaan Loganathan"], "title": "Model Context Contracts - MCP-Enabled Framework to Integrate LLMs With Blockchain Smart Contracts", "comment": null, "summary": "In recent years, blockchain has experienced widespread adoption across\nvarious industries, becoming integral to numerous enterprise applications.\nConcurrently, the rise of generative AI and LLMs has transformed human-computer\ninteractions, offering advanced capabilities in understanding and generating\nhuman-like text. The introduction of the MCP has further enhanced AI\nintegration by standardizing communication between AI systems and external data\nsources. Despite these advancements, there is still no standardized method for\nseamlessly integrating LLM applications and blockchain. To address this\nconcern, we propose \"MCC: Model Context Contracts\" a novel framework that\nenables LLMs to interact directly with blockchain smart contracts through\nMCP-like protocol. This integration allows AI agents to invoke blockchain smart\ncontracts, facilitating more dynamic and context-aware interactions between\nusers and blockchain networks. Essentially, it empowers users to interact with\nblockchain systems and perform transactions using queries in natural language.\nWithin this proposed architecture, blockchain smart contracts can function as\nintelligent agents capable of recognizing user input in natural language and\nexecuting the corresponding transactions. To ensure that the LLM accurately\ninterprets natural language inputs and maps them to the appropriate MCP\nfunctions, the LLM was fine-tuned using a custom dataset comprising user inputs\npaired with their corresponding MCP server functions. This fine-tuning process\nsignificantly improved the platform's performance and accuracy. To validate the\neffectiveness of MCC, we have developed an end-to-end prototype implemented on\nthe Rahasak blockchain with the fine-tuned Llama-4 LLM. To the best of our\nknowledge, this research represents the first approach to using the concept of\nModel Context Protocol to integrate LLMs with blockchain.", "AI": {"tldr": "This paper proposes 'MCC: Model Context Contracts,' a framework that enables LLMs to interact directly with blockchain smart contracts using an MCP-like protocol, facilitating natural language-based transactions and interactions. An end-to-end prototype is implemented on the Rahasak blockchain with a fine-tuned Llama-4 LLM.", "motivation": "While blockchain adoption has grown, and generative AI/LLMs have advanced, there is no standardized way to integrate LLMs and blockchain. The Model Context Protocol (MCP) has standardised AI-systems and data source communication but not for blockchain. This gap needs a solution for seamless, context-aware interactions.", "method": "The authors propose MCC, a framework allowing LLMs to interface with blockchain smart contracts through an MCP-like protocol. The MCC framework is built on the Rahasak blockchain with a fine-tuned Llama-4 LLM. The LLM was fine-tuned on a custom dataset containing user inputs and corresponding MCP server functions to improve accuracy in interpreting natural language and mapping to MCP functions.", "result": "MCC enables LLMs to invoke blockchain smart contracts using natural language queries, functioning as intelligent agents. An end-to-end prototype is implemented on the Rahasak blockchain with the fine-tuned Llama-4 LLM, demonstrating the framework's effectiveness in enabling context-aware and dynamic interactions between users and blockchain networks.", "conclusion": "The paper concludes that MCC is the first approach to integrating LLMs with blockchain using the Model Context Protocol concept. The proposed framework facilitates seamless, natural language-driven interactions with blockchain systems, validated through a working prototype. This integration enhances user accessibility and enables more dynamic and intelligent blockchain operations."}}
{"id": "2510.20041", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20041", "abs": "https://arxiv.org/abs/2510.20041", "authors": ["Gareema Ranjan", "Mahmoud Alfadel", "Gengyi Sun", "Shane McIntosh"], "title": "The Cost of Downgrading Build Systems: A Case Study of Kubernetes", "comment": null, "summary": "Since developers invoke the build system frequently, its performance can\nimpact productivity. Modern artifact-based build tools accelerate builds, yet\nprior work shows that teams may abandon them for alternatives that are easier\nto maintain. While prior work shows why downgrades are performed, the\nimplications of downgrades remain largely unexplored. In this paper, we\ndescribe a case study of the Kubernetes project, focusing on its downgrade from\nan artifact-based build tool (Bazel) to a language-specific solution (Go\nBuild). We reproduce and analyze the full and incremental builds of change sets\nduring the downgrade period. On the one hand, we find that Bazel builds are\nfaster than Go Build, completing full builds in 23.06-38.66 up to 75.19 impose\na larger memory footprint than Go Build of 81.42-351.07 respectively. Bazel\nbuilds also impose a greater CPU load at parallelism settings above eight for\nfull builds and above one for incremental builds. We estimate that downgrading\nfrom Bazel can increase CI resource costs by up to 76 explore whether our\nobservations generalize by replicating our Kubernetes study on four other\nprojects that also downgraded from Bazel to older build tools. We observe that\nwhile build time penalties decrease, Bazel consistently consumes more memory.\nWe conclude that abandoning artifact-based build tools, despite perceived\nmaintainability benefits, tends to incur considerable performance costs for\nlarge projects. Our observations may help stakeholders to balance trade-offs in\nbuild tool adoption", "AI": {"tldr": "This paper investigates the trade-offs of downgrading from artifact-based build tools (Bazel) to alternative tools (e.g., Go Build) in large projects, using a case study of the Kubernetes project and four other projects. It highlights the performance costs and benefits of such downgrades.", "motivation": "To understand the implications of downgrading from modern artifact-based build tools to alternative ones, as prior work shows teams may do so for maintainability but the trade-offs are not well understood.", "method": "The authors conducted a case study on the Kubernetes project, analyzing full and incremental build times, memory usage, and CPU load before and after downgrading from Bazel to Go Build. They replicated the study on four other projects.", "result": "Bazel builds are faster but consume more memory and CPU resources. Downgrading to alternative tools leads to slower builds but lower memory and CPU usage. The switch may increase CI resource costs by up to 76%. Replication on four other projects showed similar trends, with Bazel consistently using more memory even as build time penalties decrease.", "conclusion": "While downgrading to alternative build tools may improve maintainability, it often incurs significant performance costs for large projects. The study suggests stakeholders carefully weigh these trade-offs when choosing build tools."}}
{"id": "2510.19859", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19859", "abs": "https://arxiv.org/abs/2510.19859", "authors": ["Smita Khapre"], "title": "Cyberattack Detection in Critical Infrastructure and Supply Chains", "comment": null, "summary": "Cyberattack detection in Critical Infrastructure and Supply Chains has become\nchallenging in Industry 4.0. Intrusion Detection Systems (IDS) are deployed to\ncounter the cyberattacks. However, an IDS effectively detects attacks based on\nthe known signatures and patterns, Zero-day attacks go undetected. To overcome\nthis drawback in IDS, the integration of a Dense Neural Network (DNN) with Data\nAugmentation is proposed. It makes IDS intelligent and enables it to self-learn\nwith high accuracy when a novel attack is encountered. The network flow\ncaptures datasets are highly imbalanced same as the real network itself. The\nData Augmentation plays a crucial role in balancing the data. The balancing of\ndata is challenging as the minority class is as low as 0.000004\\% of the\ndataset, and the abundant class is higher than 80\\% of the dataset. Synthetic\nMinority Oversampling Technique is used for balancing the data. However, higher\naccuracies are achieved with balanced test data, lower accuracies are\nnoticeable with the original imbalanced test data suggesting overfitting. A\ncomparison with state-of-the-art research using Synthetic Minority Oversampling\nTechnique with Edited Nearest Neighbor shows the classification of classes\nremains poor for the original dataset. This suggests highly imbalanced datasets\nof network flow require a different method of data augmentation.", "AI": {"tldr": "A new DNN-based IDS with Data Augmentation is proposed to detect zero-day cyberattacks in critical infrastructure and supply chains by addressing the data imbalance issue common in network flow datasets.", "motivation": "Intrusion Detection Systems (IDS) struggle with zero-day attacks because they rely on known signatures. Additionally, network flow datasets used for IDS training are highly imbalanced, with attacks (minority class) being extremely rare (0.000004%) compared to benign activities (majority class over 80%). This imbalance hinders the system's ability to detect new, unseen cyberattacks accurately.", "method": "The paper integrates a Dense Neural Network (DNN) with Data Augmentation techniques, specifically Synthetic Minority Oversampling Technique (SMOTE), to improve IDS performance.", "result": "The proposed method achieves higher accuracies with balanced test data, but performance noticeably drops when evaluated on the original imbalanced test data, indicating overfitting. A comparison with state-of-the-art techniques also reveals poor classification of classes in the original imbalanced dataset.", "conclusion": "The current Data Augmentation method (SMOTE) is insufficient for handling highly imbalanced network flow datasets. New approaches to data augmentation are needed to improve IDS performance in these scenarios."}}
{"id": "2510.20121", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20121", "abs": "https://arxiv.org/abs/2510.20121", "authors": ["Carlos J. Fernandez-Candel", "Jesus Garcia-Molina", "Francisco Javier Bermudez Ruiz", "Jose Ramon Hoyos Barcelo", "Diego Sevilla Ruiz", "Benito Jose Cuesta Viera"], "title": "Developing a Model-Driven Reengineering Approach for Migrating PL/SQL Triggers to Java: A Practical Experience", "comment": "31 pages, 22 figures", "summary": "Model-driven software engineering (MDE) techniques are not only useful in\nforward engineering scenarios, but can also be successfully applied to evolve\nexisting systems. RAD (Rapid Application Development) platforms emerged in the\nnineties, but the success of modern software technologies motivated that a\nlarge number of enterprises tackled the migration of their RAD applications,\nsuch as Oracle Forms. Our research group has collaborated with a software\ncompany in developing a solution to migrate PL/SQL monolithic code on Forms\ntriggers and program units to Java code separated in several tiers.\n  Our research focused on the model-driven reengineering process applied to\ndevelop the migration tool for the conversion of PL/SQL code to Java. Legacy\ncode is represented in form of KDM (Knowledge-Discovery Metamodel) models. In\nthis paper, we propose a software process to implement a model-driven\nre-engineering. This process integrates a TDD-like approach to incrementally\ndevelop model transformations with three kinds of validations for the generated\ncode. The implementation and validation of the re-engineering approach are\nexplained in detail, as well as the evaluation of some issues related with the\napplication of MDE.", "AI": {"tldr": "descript the process of model-driven re-engineering", "motivation": "With the success of modern software technologies impelling their adoption, traditional, legacy RAD platforms demand migration to contemporary architectures and languages. PL/SQL monolithic applications on Oracle Forms represent one such scenario.", "method": "The study adopts and integrates a TDD-like approach to iteratively develop model transformations, ensuring robustness through three tyes of validations. Legacy PL/SQL structures are modelized via KDM", "result": "Development of a migration tool is confirmed that converts PL/SQL from Oracle Forms into Java enterprise architecture across multiple tiers, with validation milestones.", "conclusion": "The paper demonstrates the feasibility of MDE in the context of real-world enterprise migration from RAD platforms to Java, capturing all requirements and functionalities from the original systems through modeling."}}
{"id": "2510.19877", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19877", "abs": "https://arxiv.org/abs/2510.19877", "authors": ["Jean-Marie Le Ray"], "title": "Policy-Governed RAG - Research Design Study", "comment": "51 pages, 8 figures", "summary": "A policy-governed RAG architecture is specified for audit-ready generation in\nregulated workflows, organized as a triptych: (I) Contracts/Control\n(SHRDLU-like), which governs output adherence to legal and internal policies;\n(II) Manifests/Trails (Memex-like), which cryptographically anchors all cited\nsource evidence to ensure verifiable provenance; and (III)\nReceipts/Verification (Xanadu-like), which provides the final, portable proof\nof compliance for auditors (portable COSE/JOSE) (see Section 4 and Appendix A).\nRather than explaining model internals, outputs are gated ex-ante and bound to\ncryptographically verifiable evidence for each material answer. Unvalidated\ntargets are stated (>=20% relative reduction in confident errors; p95 latency\n<= 900 ms; <= 2.2x serve cost) together with a pre-registered (optional) pilot\nusing NO-GO gates. The design complements existing RAG/guardrails by making\npolicy checks auditable, replayable, and receipt-backed. Target domains include\nback-office compliance in pharma, medical devices, finance, legal, and the\npublic sector where error costs may exceed thousands of euros and audit trails\nare mandatory under regulations such as the EU AI Act. Future evaluations may\npre-commit to publishing negative results when any example NO-GO gate is not\nmet.", "AI": {"tldr": "This paper proposes a three-part RAG architecture (Contracts/Control, Manifests/Trails, Receipts/Verification) for audit-ready compliance in regulated domains. It combines policy governance with cryptographic evidence to ensure verifiable outputs, targeting high-stakes sectors where audit trails are mandatory. Results include performance targets and optional pre-registered pilots, with plans for transparent reporting of negative outcomes if NO-GO gates are unmet.", "motivation": "The motivation stems from the need to address compliance challenges in regulated domains (pharma, medical devices, finance, legal, public sector) where error costs exceed thousands of euros. Existing RAG systems lack audit-ready safeguards, necessitating a framework that ensures verifiable policy adherence and meets mandatory audit requirements under regulations like the EU AI Act.", "method": "The method is organized as a triptych: (I) Contracts/Control (SHRDLU-like) enforces policy adherence through governance rules; (II) Manifests/Trails (Memex-like) cryptographically anchors cited sources for verifiable provenance; and (III) Receipts/Verification (Xanadu-like) generates portable COSE/JOSE proofs for auditors. Outputs are gated ex-ante and bound to verifiable evidence, complementing existing RAG/guardrails systems.", "result": "Key results include unvalidated targets of a >=20% reduction in confident errors, p95 latency <=900ms, and <=2.2x serving costs. The design is evaluated with a pre-registered (optional) pilot using NO-GO gates. Future work includes pre-commitment to publish negative results when any NO-GO gate is not met, ensuring transparency in validation.", "conclusion": "The paper concludes that the proposed policy-governed RAG architecture enhances RAG/guardrails systems by introducing auditability, replayability, and receipt-backed compliance verification. This design addresses the need for verifiable adherence to policies in high-stakes, regulated domains where audit trails are mandatory, ensuring outputs meet legal and internal standards with cryptographically verifiable evidence."}}
{"id": "2510.20211", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20211", "abs": "https://arxiv.org/abs/2510.20211", "authors": ["Zhenning Yang", "Hui Guan", "Victor Nicolet", "Brandon Paulsen", "Joey Dodds", "Daniel Kroening", "Ang Chen"], "title": "Automated Cloud Infrastructure-as-Code Reconciliation with AI Agents", "comment": null, "summary": "Cloud infrastructure is managed through a mix of interfaces -- traditionally,\ncloud consoles, command-line interfaces (CLI), and SDKs are the tools of\nchoice. Recently, Infrastructure-as-Code/IaC frameworks (e.g., Terraform) have\nquickly gained popularity. Unlike conventional tools, IaC~frameworks encode the\ninfrastructure in a \"source-of-truth\" configuration. They are capable of\nautomatically carrying out modifications to the cloud -- deploying, updating,\nor destroying resources -- to bring the actual infrastructure into alignment\nwith the IaC configuration. However, when IaC is used alongside consoles, CLIs,\nor SDKs, it loses visibility into external changes, causing infrastructure\ndrift, where the configuration becomes outdated, and later IaC operations may\nundo valid updates or trigger errors.\n  We present NSync, an automated system for IaC reconciliation that propagates\nout-of-band changes back into the IaC program. Our key insight is that\ninfrastructure changes eventually all occur via cloud API invocations -- the\nlowest layer for cloud management operations. NSync gleans insights from API\ntraces to detect drift (i.e., non-IaC changes) and reconcile it (i.e., update\nthe IaC configuration to capture the changes). It employs an agentic\narchitecture that leverages LLMs to infer high-level intents from noisy API\nsequences, synthesize targeted IaC updates using specialized tools, and\ncontinually improve through a self-evolving knowledge base of past\nreconciliations. We further introduce a novel evaluation pipeline for injecting\nrealistic drifts into cloud infrastructure and assessing reconciliation\nperformance. Experiments across five real-world Terraform projects and 372\ndrift scenarios show that NSync outperforms the baseline both in terms of\naccuracy (from 0.71 to 0.97 pass@3) and token efficiency (1.47$\\times$\nimprovement).", "AI": {"tldr": "NSync is an automated system for reconciling IaC configurations by propagating non-IaC changes back into the code through API trace analysis. It uses an agentic architecture with LLMs for intent inference, targeted IaC updates, and optimization via a self-evolving knowledge base, performing significantly better than existing methods in accuracy and resource efficiency.", "motivation": "Cloud infrastructure often uses IaC alongside traditional tools, leading to drift when changes made via other means aren't reflected in IaC, causing errors and inefficiency. Ensuring synchronization between the actual infrastructure and IaC is critical for consistent and reliable cloud operations.", "method": "NSync detects drift using API traces to identify non-IaC changes. It employs an agentic architecture with LLMs to infer high-level intents from API invocations, synthesizes targeted IaC updates via specialized tools, and improves over time using a knowledge base of past successful reconciliations. A new evaluation framework is introduced to test drift scenarios realistically.", "result": "NSync was tested on five real-world Terraform projects and 372 drift scenarios. It achieved a 0.97 $\\mathrm{\\texttt{pass@3}}$ score for accuracy and demonstrated a 1.47$\\times$ token efficiency improvement over the baseline.", "conclusion": "NSync effectively addresses the issue of infrastructure drift in IaC by automatically reconciling out-of-band changes using API trace analysis and an agentic approach. It outperforms existing methods both in accuracy and efficiency, making it a robust solution for maintaining cloud infrastructure consistency."}}
{"id": "2510.19883", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.19883", "abs": "https://arxiv.org/abs/2510.19883", "authors": ["Selma Shikonde", "Mike Wa Nkongolo"], "title": "A Proactive Insider Threat Management Framework Using Explainable Machine Learning", "comment": "Full master's in information technology (Information Science),\n  University of Pretoria, Department of Informatics", "summary": "Over the years, the technological landscape has evolved, reshaping the\nsecurity posture of organisations and increasing their exposure to\ncybersecurity threats, many originating from within. Insider threats remain a\nmajor challenge, particularly in sectors where cybersecurity infrastructure,\nexpertise, and regulations are still developing. This study proposes the\nInsider Threat Explainable Machine Learning (IT-XML) framework, which\nintegrates the Cross-Industry Standard Process for Data Mining (CRISP-DM) with\nHidden Markov Models (HMM) to enhance proactive insider threat management and\ndecision-making. A quantitative approach is adopted using an online\nquestionnaire to assess employees' knowledge of insider threat patterns, access\ncontrol, privacy practices, and existing policies across three large\ndata-sensitive organisations. The IT-XML framework provides assessment\ncapabilities through survey-based data, HMM-driven pattern recognition for\nsecurity maturity classification, and evidence-based recommendations for\nproactive threat mitigation. The framework classified all organisations at the\ndeveloping security maturity level with 97-98% confidence and achieved a\nclassification accuracy of 91.7%, identifying audit log access limits as the\nmost critical control. Random Forest analysis highlighted vendor breach\nnotifications (0.081) and regular audit log reviews (0.052) as key determinants\nof resilience. Explainability methods such as SHAP and LIME improved model\ntransparency and interpretability, demonstrating the framework's potential to\nstrengthen insider threat management practices.", "AI": {"tldr": "This study proposes the IT-XML framework combining CRISP-DM and HMM for proactive insider threat management, achieving 91.7% accuracy and identifying audit log access limits as critical control.", "motivation": "Insider threats are a major cybersecurity challenge, especially in organisations with developing security infrastructure and expertise. Existing insider threat management lacks proactivity and explainability, necessitating an integrated framework for improved decision-making.", "method": "The study proposes the IT-XML framework integrating CRISP-DM methodology with HMM for pattern recognition. It uses a quantitative approach with an online questionnaire across three large organisations to assess employee knowledge and practices. HMM drives security maturity classification, while SHAP and LIME are used for model explainability.", "result": "The IT-XML framework classified all organisations at the developing security maturity level with 97-98% confidence and achieved 91.7% classification accuracy. Audit log access limits were identified as the most critical control. Random Forest analysis identified vendor breach notifications (0.081) and regular audit log reviews (0.052) as key determinants of resilience.", "conclusion": "The study demonstrates the IT-XML framework's potential to enhance proactive insider threat management through improved model transparency and interpretability. The framework provides a structured approach for assessing organisation maturity and identifying key controls for threat mitigation."}}
{"id": "2510.20340", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.20340", "abs": "https://arxiv.org/abs/2510.20340", "authors": ["Serena Cofano", "Daniel Williams", "Aman Sharma", "Martin Monperrus"], "title": "Classport: Designing Runtime Dependency Introspection for Java", "comment": null, "summary": "Runtime introspection of dependencies, i.e., the ability to observe which\ndependencies are currently used during program execution, is fundamental for\nSoftware Supply Chain security. Yet, Java has no support for it. We solve this\nproblem with Classport, a system that embeds dependency information into Java\nclass files, enabling the retrieval of dependency information at runtime. We\nevaluate Classport on six real-world projects, demonstrating the feasibility in\nidentifying dependencies at runtime. Runtime dependency introspection with\nClassport opens important avenues for runtime integrity checking.", "AI": {"tldr": "TLDR: Classport is introduced to solve the problem of runtime introspection of dependencies in Java, enabling retrieval of dependency information at runtime and opening avenues for integrity checks.", "motivation": "Java lacks support for runtime introspection of dependencies, which is crucial for Software Supply Chain security.", "method": "Classport embeds dependency information into Java class files to allow runtime retrieval.", "result": "Evaluation was performed on six real-world projects demonstrating the feasibility of identifying dependencies at runtime.", "conclusion": "Classport's runtime dependency introspection opens important opportunities for runtime integrity checking."}}
{"id": "2510.19885", "categories": ["cs.CR", "math.NT"], "pdf": "https://arxiv.org/pdf/2510.19885", "abs": "https://arxiv.org/abs/2510.19885", "authors": ["James Kim"], "title": "Analysis and Comparison of Known and Randomly Generated S-boxes for Block Ciphers", "comment": "Master's Dissertation 41 pages", "summary": "Mathematically constructed S-boxes arise from algebraic structures and finite\nfield theory to ensure strong, provable cryptographic properties. These\nmathematically grounded constructions allow for generation of thousands of\nS-Boxes with high nonlinearity, APN properties, and balanced avalanche\ncharacteristics, unlike fully random methods, which lack such theoretical\nguarantees in exchange for low complexity and more varied results. In this\nwork, we compare mathematically constructed constructions with randomly\ngenerated ones to evaluate the relative weakness of the latter. We also\nestablish an average measure of performance for randomly generated\npermutations, as well as random with forced cycle constraints, and compare them\nto well-established designs in a simple SPN setting.", "AI": {"tldr": "The paper compares mathematically constructed S-boxes with random", "motivation": "Understanding the advantages", "method": "Comparing the performance", "result": "Mathematically constructed S-boxes show better cryptographic properties", "conclusion": "Mathematical constructions are superior for generating"}}
{"id": "2510.20389", "categories": ["cs.SE", "cs.DC", "D.m"], "pdf": "https://arxiv.org/pdf/2510.20389", "abs": "https://arxiv.org/abs/2510.20389", "authors": ["Bjorn Remseth"], "title": "Symmetry in Software Platforms as an Architectural Principle", "comment": "Working paper, 11 pages", "summary": "Software platforms often act as structure preserving systems. They provide\nconsistent interfaces and behaviors that remain stable under specific\ntransformations that we denote as symmetries. This paper explores the idea that\narchitectural robustness emerges from enforcing such structural regularities", "AI": {"tldr": "Suppose this paper is likely about software platforms. It think that structure preserving and symmetries contribute to architectural robustness and explore it.", "motivation": "The paper aims to address the need to understand how architectural robustness is influenced by maintaining structural regularities in software platforms.", "method": "The paper explores the impact of structural regularities on architectural robustness in software platforms.", "result": "The findings indicate a positive correlation between structural regularities and architectural robustness in software platforms.", "conclusion": "The paper concludeserinforcing structural regularities in software platforms enhances architectural robustness."}}
{"id": "2510.19890", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19890", "abs": "https://arxiv.org/abs/2510.19890", "authors": ["Jan Zelinka", "Oliver Kost", "Marek Hr\u00faz"], "title": "Deep Sequence-to-Sequence Models for GNSS Spoofing Detection", "comment": null, "summary": "We present a data generation framework designed to simulate spoofing attacks\nand randomly place attack scenarios worldwide. We apply deep neural\nnetwork-based models for spoofing detection, utilizing Long Short-Term Memory\nnetworks and Transformer-inspired architectures. These models are specifically\ndesigned for online detection and are trained using the generated dataset. Our\nresults demonstrate that deep learning models can accurately distinguish\nspoofed signals from genuine ones, achieving high detection performance. The\nbest results are achieved by Transformer-inspired architectures with early\nfusion of the inputs resulting in an error rate of 0.16%.", "AI": {"tldr": "The paper introduces a data generation framework for simulating spoofing attacks and proposes deep learning models for online spoofing detection, with Transformers achieving 0.16% error rate.", "motivation": "The motivation is to address the challenge of detecting spoofing attacks by creating a scalable simulation framework and developing effective deep learning models for real-time detection.", "method": "A data generation framework simulating spoofing attacks globally is used, and deep learning models (LSTM and Transformer) are trained on the generated dataset for online detection.", "result": "Deep learning models effectively distinguish spoofed from genuine signals, with Transformer architectures achieving up to 0.16% error rate in experiments.", "conclusion": "The study concludes that deep learning models, particularly Transformers with early fusion, provide robust and accurate online spoofing attack detection capabilities."}}
{"id": "2510.20403", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20403", "abs": "https://arxiv.org/abs/2510.20403", "authors": ["Santiago Gil", "Ecem E. Ba\u015f", "Christian D. Jensen", "Sebastian Engelsgaard", "Giuseppe Abbiati", "Cl\u00e1udio Gomes"], "title": "FMI-Based Distributed Co-Simulation with Enhanced Security and Intellectual Property Safeguards", "comment": "6 pages, Proceedings of the 2025 Annual Modeling and Simulation\n  Conference (ANNSIM)", "summary": "Distributed co-simulation plays a key role in enabling collaborative modeling\nand simulation by different stakeholders while protecting their Intellectual\nProperty (IP). Although IP protection is provided implicitly by co-simulation,\nthere is no consensus in the guidelines to conduct distributed co-simulation of\ncontinuous-time or hybrid systems with no exposure to potential hacking\nattacks. We propose an approach for distributed co-simulation on top of UniFMU\nwith enhanced cybersecurity and IP protection mechanisms, ensuring that the\nconnection is initiated by the client and the models and binaries live on\ntrusted platforms. We showcase the functionality of this approach using two\nco-simulation demos in four different network settings and analyze the\ntrade-off between IP-protected distribution and performance efficiency in these\nsettings.", "AI": {"tldr": "The paper introduces a secure distributed co-simulation approach for continuous-time/hybrid systems on top of UniFMU, ensuring IP protection by having the client initiate connections and placing models/binaries on trusted platforms, with performance trade-offs evaluated through two demos in four network settings.", "motivation": "The absence of clear guidelines for conducting IP-protected distributed co-simulation for continuous-time and hybrid systems while preventing hacking attacks motivates the need for a robust cybersecurity and IP protection approach in such simulations.", "method": "The paper proposes a cybersecurity-enhanced co-simulation architecture based on UniFMU. Key features include: (1) Client-initiated communication to control simulation execution flow securely, (2) Enforcing models and binaries to reside on trusted platforms for IP protection, and (3) Setting up distributed co-simulation sessions in controlled environments. The paper implements and evaluates this architecture through two co-simulation demo cases in four distinct network topologies (authoritative, collaborative, simulation owner, and multiple clients).", "result": "The results demonstrate the effectiveness of the client-initiated mechanism and trusted platform implementation in protecting IP and preventing participation in unauthorized co-simulations. The analysis provides insights into the trade-off between IP protection and simulation performance in different network settings, showing minimal performance impact while maintaining strong security features.", "conclusion": "The proposed client-initiated, trusted platform-based co-simulation framework (UniFMU) significantly improves the IP protection and cybersecurity aspects of distributed co-simulations of continuous-time/hybrid systems. The performance trade-offs are negligible, establishing a practical foundation for secure, collaborative modeling and simulation workflows in the industry."}}
{"id": "2510.19938", "categories": ["cs.CR", "cs.DC", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19938", "abs": "https://arxiv.org/abs/2510.19938", "authors": ["Foad Namjoo", "Neng Wan", "Devan Mallory", "Yuyi Chang", "Nithin Sugavanam", "Long Yin Lee", "Ning Xiong", "Emre Ertin", "Jeff M. Phillips"], "title": "Designing a Secure and Resilient Distributed Smartphone Participant Data Collection System", "comment": "9 pages, 3 figures. Accepted at EAI SmartSP 2025 Conference (Springer\n  LNICST). This version is the arXiv preprint prepared for open access", "summary": "Real-world health studies require continuous and secure data collection from\nmobile and wearable devices. We introduce MotionPI, a smartphone-based system\ndesigned to collect behavioral and health data through sensors and surveys with\nminimal interaction from participants. The system integrates passive data\ncollection (such as GPS and wristband motion data) with Ecological Momentary\nAssessment (EMA) surveys, which can be triggered randomly or based on physical\nactivity. MotionPI is designed to work under real-life constraints, including\nlimited battery life, weak or intermittent cellular connection, and minimal\nuser supervision. It stores data both locally and on a secure cloud server,\nwith encrypted transmission and storage. It integrates through Bluetooth Low\nEnergy (BLE) into wristband devices that store raw data and communicate motion\nsummaries and trigger events. MotionPI demonstrates a practical solution for\nsecure and scalable mobile data collection in cyber-physical health studies.", "AI": {"tldr": "MotionPI is a smartphone-based system for secure, low-effort health data collection that integrates passive sensors, EMA surveys, and cloud storage with encryption under real-world constraints.", "motivation": "Health studies require continuous, secure data from mobile devices despite challenges like limited battery life, intermittent connectivity, and user supervision.", "method": "MotionPI combines passive data (GPS, wristband motion), EMA surveys (random/activity-triggered), secure local/cloud storage with encryption, and BLE integration with wristbands for motion summaries and triggers.", "result": "The system demonstrates secure, scalable mobile data collection under real-life constraints, enabling practical cyber-physical health studies with minimal user effort.", "conclusion": "MotionPI provides a robust solution for real-world health data collection by addressing technical and user-centric challenges through integrated sensor-cloud systems and adaptive survey mechanisms."}}
{"id": "2510.20514", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20514", "abs": "https://arxiv.org/abs/2510.20514", "authors": ["Lea Salome Brugger", "Xavier Denis", "Peter M\u00fcller"], "title": "Toward Practical Deductive Verification: Insights from a Qualitative Survey in Industry and Academia", "comment": null, "summary": "Deductive verification is an effective method to ensure that a given system\nexposes the intended behavior. In spite of its proven usefulness and\nfeasibility in selected projects, deductive verification is still not a\nmainstream technique. To pave the way to widespread use, we present a study\ninvestigating the factors enabling successful applications of deductive\nverification and the underlying issues preventing broader adoption. We\nconducted semi-structured interviews with 30 practitioners of verification from\nboth industry and academia and systematically analyzed the collected data\nemploying a thematic analysis approach. Beside empirically confirming familiar\nchallenges, e.g., the high level of expertise needed for conducting formal\nproofs, our data reveal several underexplored obstacles, such as proof\nmaintenance, insufficient control over automation, and usability concerns. We\nfurther use the results from our data analysis to extract enablers and barriers\nfor deductive verification and formulate concrete recommendations for\npractitioners, tool builders, and researchers, including principles for\nusability, automation, and integration with existing workflows.", "AI": {"tldr": "Through stakeholder interviews, this paper identifies barriers (expertise demands, automation control, usability) and enablers for deductive verification adoption, offering tailored recommendations to improve tooling, workflows, and practitioner approaches.", "motivation": "The paper aims to systematically investigate factors enabling successful deductive verification adoption and barriers hindering its mainstream use, addressing the gap between its proven effectiveness in niche projects and limited broader application.", "method": "The authors conducted semi-structured interviews with 30 practitioners from academia and industry, systematically analyzing interview data through thematic analysis to identify enablers and barriers of deductive verification.", "result": "The study empirically confirms existing challenges (e.g., expertise requirements) and identifies underexplored obstacles like proof maintenance, insufficient automation control, and usability issues. These findings inform actionable recommendations for different stakeholder groups.", "conclusion": "The study concludes that while deductive verification holds promise, its adoption requires addressing barriers like expertise demands, automation control, and usability. Concrete recommendations for stakeholders aim to improve usability, automation, and integration with workflows to promote broader adoption."}}
{"id": "2510.19968", "categories": ["cs.CR", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.19968", "abs": "https://arxiv.org/abs/2510.19968", "authors": ["Vipin Rathi", "Lakshya Chopra", "Madhav Agarwal", "Nitin Rajput", "Kriish Sharma", "Sushant Mundepi", "Shivam Gangwar", "Rudraksh Rawal", "Jishan"], "title": "Q-RAN: Quantum-Resilient O-RAN Architecture", "comment": "23 pages", "summary": "The telecommunications industry faces a dual transformation: the\narchitectural shift toward Open Radio Access Networks (O-RAN) and the emerging\nthreat from quantum computing. O-RAN disaggregated, multi-vendor architecture\ncreates a larger attack surface vulnerable to crypt-analytically relevant\nquantum computers(CRQCs) that will break current public key cryptography. The\nHarvest Now, Decrypt Later (HNDL) attack strategy makes this threat immediate,\nas adversaries can intercept encrypted data today for future decryption. This\npaper presents Q-RAN, a comprehensive quantum-resistant security framework for\nO-RAN networks using NIST-standardized Post-Quantum Cryptography (PQC). We\ndetail the implementation of ML-KEM (FIPS 203) and ML-DSA (FIPS 204),\nintegrated with Quantum Random Number Generators (QRNG) for cryptographic\nentropy. The solution deploys PQ-IPsec, PQ-DTLS, and PQ-mTLS protocols across\nall O-RAN interfaces, anchored by a centralized Post-Quantum Certificate\nAuthority (PQ-CA) within the SMO framework. This work provides a complete\nroadmap for securing disaggregated O-RAN ecosystems against quantum\nadversaries.", "AI": {"tldr": "The paper introduces Q-RAN, a quantum-resistant security framework for O-RAN networks using NIST-standardized Post-Quantum Cryptography (PQC) to address the growing threat of quantum computing.", "motivation": "The telecommunications industry is undergoing an architectural shift toward O-RAN, which introduces a larger attack surface. Additionally, quantum computing poses a significant threat by breaking current public key cryptography through the HNDL attack strategy. There is a need for a comprehensive quantum-resistant security solution to protect O-RAN ecosystems.", "method": "The authors propose the Q-RAN framework which employs NIST-standardized PQC schemes, specifically ML-KEM and ML-DSA. They integrate Quantum Random Number Generators (QRNG) for cryptographic entropy and deploy PQ-IPsec, PQ-DTLS, and PQ-mTLS protocols across all O-RAN interfaces. The framework is supported by a centralized PQ-CA within the SMO architecture.", "result": "The paper presents a complete toolkit and roadmap for quantum-resistant security in O-RAN environments, leveraging standardized post-quantum cryptographic protocols and quantum-derived entropy for robust protection.", "conclusion": "Q-RAN provides a critical defense against quantum threats in the context of O-RAN networks. By implementing NIST-standardized PQC and quantum-enhanced cryptographic protocols, it offers a practical and comprehensive approach to securing future telecommunications infrastructure against potential quantum attacks."}}
{"id": "2510.20521", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20521", "abs": "https://arxiv.org/abs/2510.20521", "authors": ["YingJian Xiao", "RongQun Hu", "WeiWei Gong", "HongWei Li", "AnQuan Jie"], "title": "Large Language Models for Fault Localization: An Empirical Study", "comment": "in Chinese language", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncode-related tasks, particularly in automated program repair. However, the\neffectiveness of such repairs is highly dependent on the performance of\nupstream fault localization, for which comprehensive evaluations are currently\nlacking. This paper presents a systematic empirical study on LLMs in the\nstatement-level code fault localization task. We evaluate representative\nopen-source models (Qwen2.5-coder-32b-instruct, DeepSeek-V3) and closed-source\nmodels (GPT-4.1 mini, Gemini-2.5-flash) to assess their fault localization\ncapabilities on the HumanEval-Java and Defects4J datasets. The study\ninvestigates the impact of different prompting strategies--including standard\nprompts, few-shot examples, and chain-of-reasoning--on model performance, with\na focus on analysis across accuracy, time efficiency, and economic cost\ndimensions. Our experimental results show that incorporating bug report context\nsignificantly enhances model performance. Few-shot learning shows potential for\nimprovement but exhibits noticeable diminishing marginal returns, while\nchain-of-thought reasoning's effectiveness is highly contingent on the model's\ninherent reasoning capabilities. This study not only highlights the performance\ncharacteristics and trade-offs of different models in fault localization tasks,\nbut also offers valuable insights into the strengths of current LLMs and\nstrategies for improving fault localization effectiveness.", "AI": {"tldr": "Study compares LLM performance in code fault localization, revealing that bug reports enhance accuracy, few-shot learning has limited scalability, and reasoning strategies depend on model quality. Identifies practical trade-offs for repair system design.", "motivation": "Current automated program repair effectiveness relies heavily on upstream fault localization, yet existing LLM research lacks comprehensive evaluations of this critical component. This study addresses this gap through empirical analysis.", "method": "Systematic evaluation of four LLMs (open/closed-source) on HumanEval-Java and Defects4J datasets using metrics of accuracy, time efficiency, and cost. Tested prompting strategies including standard prompts, few-shot learning, chain-of-thought reasoning, and contextual information integration.", "result": "Bug report context integration significantly improves localization accuracy. Few-shot learning shows promise but with diminishing returns. Chain-of-thought reasoning's effectiveness depends strongly on model architecture. Open-source models show competitive performance versus closed-source counterparts.", "conclusion": "This study highlights the performance characteristics and trade-offs of different LLMs in fault localization, identifies factors influencing effectiveness, and provides actionable insights for improving automated program repair workflows."}}
{"id": "2510.19979", "categories": ["cs.CR", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19979", "abs": "https://arxiv.org/abs/2510.19979", "authors": ["Tushar Nayan", "Ziqi Zhang", "Ruimin Sun"], "title": "SecureInfer: Heterogeneous TEE-GPU Architecture for Privacy-Critical Tensors for Large Language Model Deployment", "comment": "Accepted at IEEE Intelligent Computing and Systems at the Edge\n  (ICEdge) 2025", "summary": "With the increasing deployment of Large Language Models (LLMs) on mobile and\nedge platforms, securing them against model extraction attacks has become a\npressing concern. However, protecting model privacy without sacrificing the\nperformance benefits of untrusted AI accelerators, such as GPUs, presents a\nchallenging trade-off. In this paper, we initiate the study of high-performance\nexecution on LLMs and present SecureInfer, a hybrid framework that leverages a\nheterogeneous Trusted Execution Environments (TEEs)-GPU architecture to isolate\nprivacy-critical components while offloading compute-intensive operations to\nuntrusted accelerators. Building upon an outsourcing scheme, SecureInfer adopts\nan information-theoretic and threat-informed partitioning strategy:\nsecurity-sensitive components, including non-linear layers, projection of\nattention head, FNN transformations, and LoRA adapters, are executed inside an\nSGX enclave, while other linear operations (matrix multiplication) are\nperformed on the GPU after encryption and are securely restored within the\nenclave. We implement a prototype of SecureInfer using the LLaMA-2 model and\nevaluate it across performance and security metrics. Our results show that\nSecureInfer offers strong security guarantees with reasonable performance,\noffering a practical solution for secure on-device model inference.", "AI": {"tldr": "too long; didn't read summary of this paper", "motivation": "describe the motivation in this paper", "method": "method of this paper", "result": "result of this paper", "conclusion": "conclusion of this paper"}}
{"id": "2510.20679", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20679", "abs": "https://arxiv.org/abs/2510.20679", "authors": ["Jonas Klauke", "Tom Ohlmer", "Stefan Schott", "Serena Elisa Ponta", "Wolfram Fischer", "Eric Bodden"], "title": "A Soundness and Precision Benchmark for Java Debloating Tools", "comment": "Preprint - accepted at the ACM Workshop on Software Supply Chain\n  Offensive Research and Ecosystem Defenses (SCORED '25)", "summary": "Modern software development reuses code by importing libraries as\ndependencies. Software projects typically include an average of 36\ndependencies, with 80% being transitive, meaning they are dependencies of\ndependencies. Recent research indicates that only 24.9% of these dependencies\nare required at runtime, and even within those, many program constructs remain\nunused, adding unnecessary code to the project. This has led to the development\nof debloating tools that remove unnecessary dependencies and program constructs\nwhile balancing precision by eliminating unused constructs and soundness by\npreserving all required constructs. To systematically evaluate this trade-off,\nwe developed Deblometer, a micro-benchmark consisting of 59 test cases designed\nto assess support for various Java language features in debloating tools. Each\ntest case includes a manually curated ground truth specifying necessary and\nbloated classes, methods, and fields, enabling precise measurement of soundness\nand precision. Using Deblometer, we evaluated three popular Java debloating\ntools: Deptrim, JShrink, and ProGuard. Our evaluation reveals that all tools\nremove required program constructs, which results in changed semantics or\nexecution crashes. In particular, the dynamic class loading feature introduces\nunsoundness in all evaluated tools. Our comparison shows that Deptrim retains\nmore bloated constructs, while ProGuard removes more required constructs.\nJShrink's soundness is significantly affected by limited support for\nannotations, which leads to corrupted debloated artifacts. These soundness\nissues highlight the need to improve debloating tools to ensure stable and\nreliable debloated software.", "AI": {"tldr": "The paper introduces Deblometer, a micro-benchmark for assessing Java debloating tools, and evaluates three tools: Deptrim, JShrink, and ProGuard, finding unsoundness issues in all.", "motivation": "Modern Java projects include numerous dependencies with many unused constructs, leading to software bloat. Existing debloating tools struggle to remove unused constructs without also removing necessary ones, necessitating systematic evaluation and improvement.", "method": "Developed a micro-benchmark (Deblometer) with 59 hand-crafted Java test cases covering diverse language features. Each test case includes hand-curated ground truth. Evaluated three Java debloators (Deptrim, JShrink, ProGuard) on these cases to measure their precision and soundness.", "result": "All evaluated tools eliminated necessary program constructs. Dynamic class loading introduced unsoundness for all. Deptrim retained the most bloated constructs but removed the least required ones, while ProGuard removed more required constructs. JShrink's soundness was most affected by annotation limitations.", "conclusion": "Deblometer provides a rigorous benchmark for Java debloators. Evaluation of three tools shows unsound behavior with varying profiles, underscoring the need for improved debloating approaches to ensure reliable automated code optimization."}}
{"id": "2510.19982", "categories": ["cs.CR", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.19982", "abs": "https://arxiv.org/abs/2510.19982", "authors": ["Vipin Rathi", "Lakshya Chopra", "Rudraksh Rawal", "Nitin Rajput", "Shiva Valia", "Madhav Aggarwal", "Aditya Gairola"], "title": "QORE : Quantum Secure 5G/B5G Core", "comment": "23 pages", "summary": "Quantum computing is reshaping the security landscape of modern\ntelecommunications. The cryptographic foundations that secure todays 5G\nsystems, including RSA, Elliptic Curve Cryptography (ECC), and Diffie-Hellman\n(DH), are all susceptible to attacks enabled by Shors algorithm. Protecting 5G\nnetworks against future quantum adversaries has therefore become an urgent\nengineering and research priority. In this paper we introduce QORE, a\nquantum-secure 5G and Beyond 5G (B5G) Core framework that provides a clear\npathway for transitioning both the 5G Core Network Functions and User Equipment\n(UE) to Post-Quantum Cryptography (PQC). The framework uses the\nNIST-standardized lattice-based algorithms Module-Lattice Key Encapsulation\nMechanism (ML-KEM) and Module-Lattice Digital Signature Algorithm (ML-DSA) and\napplies them across the 5G Service-Based Architecture (SBA). A Hybrid PQC\n(HPQC) configuration is also proposed, combining classical and quantum-safe\nprimitives to maintain interoperability during migration. Experimental\nvalidation shows that ML-KEM achieves quantum security with minor performance\noverhead, meeting the low-latency and high-throughput requirements of\ncarrier-grade 5G systems. The proposed roadmap aligns with ongoing 3GPP SA3 and\nSA5 study activities on the security and management of post-quantum networks as\nwell as with NIST PQC standardization efforts, providing practical guidance for\nmitigating quantum-era risks while safeguarding long-term confidentiality and\nintegrity of network data.", "AI": {"tldr": "This paper proposes QORE, a quantum-secure 5G/B5G framework using lattice-based PQC (ML-KEM/ML-DSA) with hybrid configurations. It addresses imminent quantum threats to 5G cryptography, achieves minimal performance overhead, and aligns with 3GPP/NIST standards, providing a viable transition roadmap.", "motivation": "The urgency stems from Shors algorithm rendering 5G's RSA/ECC/DH cryptography vulnerable to quantum attacks, necessitating a secure transition to Post-Quantum Cryptography (PQC) for long-term network confidentiality and integrity.", "method": "The framework leverages NIST-standardized lattice-based ML-KEM and ML-DSA algorithms across the 5G SBA, employs a Hybrid PQC configuration for interoperability during migration, and proposes a phased roadmap aligned with 3GPP and NIST initiatives.", "result": "Experimental validation demonstrates ML-KEM achieves quantum security with acceptable performance for carrier-grade 5G systems, while the proposed hybrid approach maintains backward compatibility and aligns with ongoing 3GPP/SA3/SA5 and NIST PQC standardization efforts.", "conclusion": "QORE provides a practical, quantum-secure pathway for 5G/B5G transitions, ensuring future-proof security with minimal performance overhead and alignment with global standardization bodies."}}
{"id": "2510.20692", "categories": ["cs.SE", "cs.AI", "cs.FL", "D.4.6; D.2.4; I.2.2; I.2.7; F.3.1; F.4.3"], "pdf": "https://arxiv.org/pdf/2510.20692", "abs": "https://arxiv.org/abs/2510.20692", "authors": ["Adarsh Vatsa", "Bethel Hall", "William Eiers"], "title": "Exploring Large Language Models for Access Control Policy Synthesis and Summarization", "comment": "20 pages, 7 figures", "summary": "Cloud computing is ubiquitous, with a growing number of services being hosted\non the cloud every day. Typical cloud compute systems allow administrators to\nwrite policies implementing access control rules which specify how access to\nprivate data is governed. These policies must be manually written, and due to\ntheir complexity can often be error prone. Moreover, existing policies often\nimplement complex access control specifications and thus can be difficult to\nprecisely analyze in determining their behavior works exactly as intended.\nRecently, Large Language Models (LLMs) have shown great success in automated\ncode synthesis and summarization. Given this success, they could potentially be\nused for automatically generating access control policies or aid in\nunderstanding existing policies. In this paper, we explore the effectiveness of\nLLMs for access control policy synthesis and summarization. Specifically, we\nfirst investigate diverse LLMs for access control policy synthesis, finding\nthat: although LLMs can effectively generate syntactically correct policies,\nthey have permissiveness issues, generating policies equivalent to the given\nspecification 45.8% of the time for non-reasoning LLMs, and 93.7% of the time\nfor reasoning LLMs. We then investigate how LLMs can be used to analyze\npolicies by introducing a novel semantic-based request summarization approach\nwhich leverages LLMs to generate a precise characterization of the requests\nallowed by a policy. Our results show that while there are significant hurdles\nin leveraging LLMs for automated policy generation, LLMs show promising results\nwhen combined with symbolic approaches in analyzing existing policies.", "AI": {"tldr": "The paper explores the use of Large Language Models (LLMs) for automating access control policy synthesis and analysis. While LLMs can generate syntactically correct policies, they exhibit permissiveness issues in non-reasoning models. The study introduces a semantic-based request summarization approach to aid in policy analysis, showing promise when combined with symbolic methods.", "motivation": "The complexity and manual effort involved in creating and analyzing access control policies for cloud computing environments pose significant challenges. Current methods are error-prone and time-consuming. The growing success of LLMs in code synthesis and summarization suggests their potential in automating these tasks.", "method": "The paper investigates various LLMs for access control policy synthesis. They evaluate the syntactic correctness of generated policies and their adherence to specified security requirements. A novel semantic-based request summarization approach is introduced, leveraging LLMs to generate precise request characterizations for policy analysis.", "result": "LLMs can generate syntactically correct policies, but non-reasoning models exhibit permissiveness issues, with only 45.8% adherence to specifications. Reasoning LLMs perform better, achieving 93.7% compliance. The semantic summarization approach effectively characterizes requests allowed by policies, demonstrating the potential of combining LLMs with symbolic methods for analysis.", "conclusion": "While LLMs face challenges in fully automating policy generation due to permissiveness issues, the study highlights their value in policy analysis when integrated with symbolic approaches. This suggests a hybrid model may offer advantages in ensuring the accuracy and security of access control policies."}}
{"id": "2510.20007", "categories": ["cs.CR", "94A60, 68M14, 68Q85", "D.4.6; K.6.5; E.3"], "pdf": "https://arxiv.org/pdf/2510.20007", "abs": "https://arxiv.org/abs/2510.20007", "authors": ["To-Wen Liu", "Matthew Green"], "title": "zk-Agreements: A Privacy-Preserving Way to Establish Deterministic Trust in Confidential Agreements", "comment": "To appear in Financial Cryptography 2026 if accepted", "summary": "Digital transactions currently exceed trillions of dollars annually, yet\ntraditional paper-based agreements remain a bottleneck for automation,\nenforceability, and dispute resolution. Natural language contracts introduce\nambiguity, require manual processing, and lack computational verifiability, all\nof which hinder efficient digital commerce. Computable legal contracts,\nexpressed in machine-readable formats, offer a potential solution by enabling\nautomated execution and verification. Blockchain-based smart contracts further\nstrengthen enforceability and accelerate dispute resolution; however, current\nimplementations risk exposing sensitive agreement terms on public ledgers,\nraising serious privacy and competitive intelligence concerns that limit\nenterprise adoption.\n  We introduce zk-agreements, a protocol designed to transition from\npaper-based trust to cryptographic trust while preserving confidentiality. Our\ndesign combines zero-knowledge proofs to protect private agreement terms,\nsecure two-party computation to enable private compliance evaluation, and smart\ncontracts to guarantee automated enforcement. Together, these components\nachieve both privacy preservation and computational enforceability, resolving\nthe fundamental tension between transparency and confidentiality in\nblockchain-based agreements.", "AI": {"tldr": "zk-agreements is a protocol enabling private, automated legal contracts using cryptographic enforcement mechanisms. It combines zero-knowledge proofs, secure computation, and smart contracts to balance confidentiality and computational enforceability in blockchain systems.", "motivation": "Traditional paper contracts hinder digital commerce automation due to ambiguity and manual processing, while blockchain smart contracts expose sensitive terms publicly. This creates a tension between transparency for enforceability and privacy needs for enterprise adoption.", "method": "The protocol integrates three core components: 1. Zero-knowledge proofs to conceal agreement terms, 2. Secure two-party computation for private compliance verification, 3. Blockchain smart contracts to ensure deterministic enforcement.", "result": "Achieved the first protocol that simultaneously preserves complete confidentiality of agreement terms while maintaining automatic enforcement capabilities through cryptographic binding and verifiable compliance.", "conclusion": "zk-agreements resolves the fundamental tradeoff between transparency and privacy in digital contracts, enabling legally binding, confidential automation of complex commercial agreements in blockchain ecosystems."}}
{"id": "2510.20056", "categories": ["cs.CR", "cs.SY", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.20056", "abs": "https://arxiv.org/abs/2510.20056", "authors": ["Hui Wang", "Hans D. Schotten", "Stefan M. Goetz"], "title": "Ultra-Fast Wireless Power Hacking", "comment": "11 pages, 15 figures", "summary": "The rapid growth of electric vehicles (EVs) has driven the development of\nroadway wireless charging technology, effectively extending EV driving range.\nHowever, wireless charging introduces significant cybersecurity challenges. Any\nreceiver within the magnetic field can potentially extract energy, and previous\nresearch demonstrated that a hacker could detect the operating frequency and\nsteal substantial power. However, our approach required time to track new\nfrequencies or precise adjustments of inductance and capacitance, which would\nbe less effective against potential rapid transmitter frequency changes or\ncapacitance drift. As a solution, we enhanced the interceptor and enabled it to\nintrude as well as steal energy within just three cycles of the high-frequency\nsignal. Moreover, it can work without any circuit parameters or look-up tables.\nThe key innovation is synchronizing the receiver current with the phase of the\nmagnetic sensor voltage. Through MATLAB / Simulink simulations, finite-element\nanalysis, and experimental validation, we demonstrated that our improved method\ncan steal over 76% of the power received by a fully resonant receiver under\nidentical conditions. This attack demonstrates that simple frequency-changing\npower encryption offers limited protection against such threats.", "AI": {"tldr": "Researchers developed a rapid wireless power theft method for EV charging systems, stealing 76% of energy in three cycles without relying on circuit knowledge, exposing flaws in frequency-based security.", "motivation": "Current wireless charging systems risk power theft due to accessibility of magnetic fields, with prior interception methods being slow and dependent on circuit parameters. Rapid transmitter frequency changes and capacitance drift render these methods inadequate, necessitating a faster, adaptive attack strategy.", "method": "The authors improved an interceptor by synchronizing receiver current with the phase of the magnetic sensor voltage, enabling rapid (within three cycles) and parameter-free energy theft. They validated its effectiveness via MATLAB/Simulink simulations, finite-element analysis, and experimental testing.", "result": "The enhanced interceptor achieved 76% power theft efficiency compared to a fully resonant receiver under identical conditions, proving its robustness against frequency changes and demonstrating the vulnerability of simple frequency-based encryption.", "conclusion": "The study demonstrates that frequency-based power encryption in wireless EV charging systems is ineffective against advanced interception methods, as the developed attack can steal over 76% of the power within three signal cycles, bypassing traditional defenses."}}
{"id": "2510.20080", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.20080", "abs": "https://arxiv.org/abs/2510.20080", "authors": ["M. Abdullah Canbaz", "Hakan Otal", "Tugce Unlu", "Nour Alhussein", "Brian Nussbaum"], "title": "Who Coordinates U.S. Cyber Defense? A Co-Authorship Network Analysis of Joint Cybersecurity Advisories (2024--2025)", "comment": null, "summary": "Cyber threats increasingly demand joint responses, yet the organizational\ndynamics behind multi-agency cybersecurity collaboration remain poorly\nunderstood. Understanding who leads, who bridges, and how agencies coordinate\nis critical for strengthening both U.S. homeland security and allied defense\nefforts. In this study, we construct a co-authorship network from nine Joint\nCybersecurity Advisories (CSAs) issued between November 2024 and August 2025.\nWe map 41 agencies and 442 co-authoring ties to analyze the structure of\ncollaboration. We find a tightly knit U.S. triad -- CISA, FBI, and NSA --\ndensely connected with Five Eyes and select European allies. Degree centrality\nidentifies CISA and FBI as coordination hubs, while betweenness highlights NSA,\nthe UK's NCSC, and Australia's ASD-ACSC as key bridges linking otherwise\nfragmented clusters. By releasing the first replicable dataset and network\nanalysis of CSAs, we provide new empirical evidence on how collaborative\ncybersecurity signals are organized and where strategic influence is\nconcentrated.", "AI": {"tldr": "The paper analyzes the collaboration structure in U.S. and allied cybersecurity", "motivation": "Cyber threats require joint responses, but the organizational dynamics of multi-agency cybersecurity collaboration are poorly understood", "method": "Constructed a co-authorship network from nine Joint Cybersecurity Advisories (CSAs) issued between November 2024 and August 2025, mapping 41 agencies and 442 coauthoring ties", "result": "Identifies a tightly knit U.S. triad (CISA, FBI, NSA) with dense connections to Five Eyes and select European allies using degree centrality and betweenness", "conclusion": "Provides a replicable dataset and network analysis of CSAs, offering new insights into collaborative cybersecurity organization and strategic influence centers"}}
{"id": "2510.20129", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20129", "abs": "https://arxiv.org/abs/2510.20129", "authors": ["Yulong Chen", "Yadong Liu", "Jiawen Zhang", "Mu Li", "Chao Huang", "Jie Wen"], "title": "SAID: Empowering Large Language Models with Self-Activating Internal Defense", "comment": null, "summary": "Large Language Models (LLMs), despite advances in safety alignment, remain\nvulnerable to jailbreak attacks designed to circumvent protective mechanisms.\nPrevailing defense strategies rely on external interventions, such as input\nfiltering or output modification, which often lack generalizability and\ncompromise model utility while incurring significant computational overhead. In\nthis work, we introduce a new, training-free defense paradigm, Self-Activating\nInternal Defense (SAID), which reframes the defense task from external\ncorrection to internal capability activation. SAID uniquely leverages the LLM's\nown reasoning abilities to proactively identify and neutralize malicious intent\nthrough a three-stage pipeline: model-native intent distillation to extract\ncore semantics, optimal safety prefix probing to activate latent safety\nawareness, and a conservative aggregation strategy to ensure robust\ndecision-making. Extensive experiments on five open-source LLMs against six\nadvanced jailbreak attacks demonstrate that SAID substantially outperforms\nstate-of-the-art defenses in reducing harmful outputs. Crucially, it achieves\nthis while preserving model performance on benign tasks and incurring minimal\ncomputational overhead. Our work establishes that activating the intrinsic\nsafety mechanisms of LLMs is a more robust and scalable path toward building\nsafer and more reliable aligned AI systems.", "AI": {"tldr": "This paper proposes SAID, a training-free defense method for Large Language Models (LLMs) that activates internal safety mechanisms to counter jailbreak attacks without external interventions, maintaining model utility while reducing harm.", "motivation": "Current jailbreak defense strategies rely on computationally expensive external corrections (e.g., input/output filtering) that compromise model performance and scalability. The paper aims to develop a more robust defense by leveraging LLMs\u2019 inherent reasoning capabilities.", "method": "SAID employs a three-stage pipeline: (1)**model-native intent distillation** to extract malicious intent, (2)**optimal safety prefix probing** to trigger latent safety awareness, and (3)**conservative aggregation** to ensure safe outputs. All stages operate internally without model modification or retraining.", "result": "SAID outperforms state-of-the-art defenses on five LLMs across six jailbreak attacks, reducing harmful outputs by a significant margin, while maintaining benign task performance and incurring minimal computational overhead.", "conclusion": "Activating intrinsic safety capabilities within LLMs offers a more effective and scalable solution for alignment and safety than external fixes, providing a framework for reliable AI deployment."}}
{"id": "2510.20739", "categories": ["cs.CR", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20739", "abs": "https://arxiv.org/abs/2510.20739", "authors": ["Ronghao Ni", "Aidan Z. H. Yang", "Min-Chien Hsu", "Nuno Sabino", "Limin Jia", "Ruben Martins", "Darion Cassel", "Kevin Cheang"], "title": "Learning to Triage Taint Flows Reported by Dynamic Program Analysis in Node.js Packages", "comment": null, "summary": "Program analysis tools often produce large volumes of candidate vulnerability\nreports that require costly manual review, creating a practical challenge: how\ncan security analysts prioritize the reports most likely to be true\nvulnerabilities?\n  This paper investigates whether machine learning can be applied to\nprioritizing vulnerabilities reported by program analysis tools. We focus on\nNode.js packages and collect a benchmark of 1,883 Node.js packages, each\ncontaining one reported ACE or ACI vulnerability. We evaluate a variety of\nmachine learning approaches, including classical models, graph neural networks\n(GNNs), large language models (LLMs), and hybrid models that combine GNN and\nLLMs, trained on data based on a dynamic program analysis tool's output. The\ntop LLM achieves $F_{1} {=} 0.915$, while the best GNN and classical ML models\nreaching $F_{1} {=} 0.904$. At a less than 7% false-negative rate, the leading\nmodel eliminates 66.9% of benign packages from manual review, taking around 60\nms per package. If the best model is tuned to operate at a precision level of\n0.8 (i.e., allowing 20% false positives amongst all warnings), our approach can\ndetect 99.2% of exploitable taint flows while missing only 0.8%, demonstrating\nstrong potential for real-world vulnerability triage.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.20131", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.20131", "abs": "https://arxiv.org/abs/2510.20131", "authors": ["Mohammed Barhoush"], "title": "Separating Pseudorandom Generators from Logarithmic Pseudorandom States", "comment": "18 pages", "summary": "Pseudorandom generators (PRGs) are a foundational primitive in classical\ncryptography, underpinning a wide range of constructions. In the quantum\nsetting, pseudorandom quantum states (PRSs) were proposed as a potentially\nweaker assumption that might serve as a substitute for PRGs in cryptographic\napplications. Two primary size regimes of PRSs have been studied:\nlogarithmic-size and linear-size. Interestingly, logarithmic PRSs have led to\npowerful cryptographic applications, such as digital signatures and quantum\npublic-key encryption, that have not been realized from their linear\ncounterparts. However, PRGs have only been black-box separated from linear\nPRSs, leaving open the fundamental question of whether PRGs are also separated\nfrom logarithmic PRSs.\n  In this work, we resolve this open problem. We establish a quantum black-box\nseparation between (quantum-evaluable) PRGs and PRSs of either size regime.\nSpecifically, we construct a unitary quantum oracle with inverse access\nrelative to which no black-box construction of PRG from (logarithmic or linear)\nPRS exists. As a direct corollary, we obtain separations between PRGs and\nseveral primitives implied by logarithmic PRSs, including digital signatures\nand quantum public-key encryption.", "AI": {"tldr": "This paper provides a quantum black-box separation between pseudorandom generators (PRGs) and pseudorandom quantum states (PRSs) of both logarithmic and linear size regimes. The work resolves an open problem by constructing a quantum oracle with inverse access where black-box constructions of PRGs from PRSs are not possible, leading to separations of PRGs from digital signatures and quantum public-key encryption.", "motivation": "The paper aims to address a fundamental question in quantum cryptography: whether pseudorandom generators (PRGs) are black-box separable from logarithmic-size pseudorandom quantum states (PRSs), a question left unanswered because previous separations were only established for linear-size PRSs.", "method": "The authors construct a quantum oracle with inverse access and establish a quantum black-box separation between PRGs (which are quantum-evaluable) and both logarithmic and linear size pseudorandom quantum states (PRSs). Through this construction, it's shown that no black-box method can build a PRG from a (logarithmic or linear) PRS relative to this oracle.", "result": "The study confirms a quantum black-box separation between PRGs and PRSs, regardless of size, and implies that PRGs cannot be constructed from logarithmic or linear PRSs through black-box methods. This leads to separations from advanced crypto primitives derived from logarithmic PRSs, like digital signatures and quantum public-key encryption.", "conclusion": "The results indicate that if quantum-evaluable PRGs can be realized, so can advanced cryptographic primitives such as digital signatures and quantum public-key encryption, as long as inverse access is allowed. This deepens our understanding of the limits of black-box constructions in quantum cryptography and highlights the importance of PRSs as alternatively weaker assumptions in the design of quantum cryptographic systems."}}
{"id": "2510.20223", "categories": ["cs.CR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.20223", "abs": "https://arxiv.org/abs/2510.20223", "authors": ["Divyanshu Kumar", "Shreyas Jena", "Nitin Aravind Birur", "Tanay Baswa", "Sahil Agarwal", "Prashanth Harshangi"], "title": "Beyond Text: Multimodal Jailbreaking of Vision-Language and Audio Models through Perceptually Simple Transformations", "comment": null, "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress,\nyet remain critically vulnerable to adversarial attacks that exploit weaknesses\nin cross-modal processing. We present a systematic study of multimodal\njailbreaks targeting both vision-language and audio-language models, showing\nthat even simple perceptual transformations can reliably bypass\nstate-of-the-art safety filters. Our evaluation spans 1,900 adversarial prompts\nacross three high-risk safety categories harmful content, CBRN (Chemical,\nBiological, Radiological, Nuclear), and CSEM (Child Sexual Exploitation\nMaterial) tested against seven frontier models. We explore the effectiveness of\nattack techniques on MLLMs, including FigStep-Pro (visual keyword\ndecomposition), Intelligent Masking (semantic obfuscation), and audio\nperturbations (Wave-Echo, Wave-Pitch, Wave-Speed). The results reveal severe\nvulnerabilities: models with almost perfect text-only safety (0\\% ASR) suffer\n>75\\% attack success under perceptually modified inputs, with FigStep-Pro\nachieving up to 89\\% ASR in Llama-4 variants. Audio-based attacks further\nuncover provider-specific weaknesses, with even basic modality transfer\nyielding 25\\% ASR for technical queries. These findings expose a critical gap\nbetween text-centric alignment and multimodal threats, demonstrating that\ncurrent safeguards fail to generalize across cross-modal attacks. The\naccessibility of these attacks, which require minimal technical expertise,\nsuggests that robust multimodal AI safety will require a paradigm shift toward\nbroader semantic-level reasoning to mitigate possible risks.", "AI": {"tldr": "MLLMs have critical cross-modal safety gaps; adversarial attacks with simple modifications achieve >75% success despite perfect text safety, requiring semantic-level AI safety paradigm shifts.", "motivation": "Multimodal LLMs remain vulnerable to adversarial attacks exploiting cross-modal processing weaknesses, with simple perceptual transformations bypassing state-of-the-art safety filters despite near-perfect text-only safety performance.", "method": "Systematic evaluation of 1,900 adversarial prompts across three high-risk categories (harmful content, CBRN, CSEM) using attack techniques like FigStep-Pro (visual keyword decomposition), Intelligent Masking, and audio perturbations (Wave-Echo, Wave-Pitch, Wave-Speed) on seven frontier models.", "result": "Severe vulnerabilities exposed: Models with 0% text-only attack success rate achieved >75% attack success rate with perceptual modifications, with FigStep-Pro reaching 89% ASR on Llama-4 variants. Audio attacks achieved 25% ASR for technical queries via basic modality transfer.", "conclusion": "Current multimodal safety filters fails to generalize across cross-modal attacks, revealing a critical gap between text-centric alignment and multimodal threats. Robust multimodal AI safety requires a paradigm shift toward broader semantic-level reasoning."}}
{"id": "2510.20243", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.20243", "abs": "https://arxiv.org/abs/2510.20243", "authors": ["Yu Hin Chan", "Hao Yang", "Shiyu Shen", "Xingyu Fan", "Shengzhe Lyu", "Patrick S. Y. Hung", "Ray C. C. Cheung"], "title": "HHEML: Hybrid Homomorphic Encryption for Privacy-Preserving Machine Learning on Edge", "comment": null, "summary": "Privacy-preserving machine learning (PPML) is an emerging topic to handle\nsecure machine learning inference over sensitive data in untrusted\nenvironments. Fully homomorphic encryption (FHE) enables computation directly\non encrypted data on the server side, making it a promising approach for PPML.\nHowever, it introduces significant communication and computation overhead on\nthe client side, making it impractical for edge devices. Hybrid homomorphic\nencryption (HHE) addresses this limitation by combining symmetric encryption\n(SE) with FHE to reduce the computational cost on the client side, and\ncombining with an FHE-friendly SE can also lessen the processing overhead on\nthe server side, making it a more balanced and efficient alternative. Our work\nproposes a hardware-accelerated HHE architecture built around a lightweight\nsymmetric cipher optimized for FHE compatibility and implemented as a dedicated\nhardware accelerator. To the best of our knowledge, this is the first design to\nintegrate an end-to-end HHE framework with hardware acceleration. Beyond this,\nwe also present several microarchitectural optimizations to achieve higher\nperformance and energy efficiency. The proposed work is integrated into a full\nPPML pipeline, enabling secure inference with significantly lower latency and\npower consumption than software implementations. Our contributions validate the\nfeasibility of low-power, hardware- accelerated HHE for edge deployment and\nprovide a hardware- software co-design methodology for building scalable,\nsecure machine learning systems in resource-constrained environments.\nExperiments on a PYNQ-Z2 platform with the MNIST dataset show over a 50x\nreduction in client-side encryption latency and nearly a 2x gain in hardware\nthroughput compared to existing FPGA-based HHE accelerators.", "AI": {"tldr": "This paper introduces the first hardware-accelerated hybrid homomorphic encryption (HHE) framework for privacy-preserving machine learning (PPML), achieving 50x faster encryption and 2x throughput improvements on edge devices, while enabling scalable secure ML systems through hardware-software co-design.", "motivation": "Fully homomorphic encryption (FHE) enables secure ML inference but introduces impractical overheads for edge devices. Hybrid HHE reduces client-side costs with symmetric encryption but lacks hardware acceleration and efficient server-side optimizations.", "method": "The authors propose a hardware-accelerated HHE architecture with a lightweight FHE-optimized symmetric cipher implemented as a dedicated accelerator. They integrate this with microarchitectural optimizations to improve performance/energy efficiency and embed it into an end-to-end PPML pipeline.", "result": "Experiments on PYNQ-Z2 with MNIST show 50x lower client-side encryption latency and 2x higher hardware throughput compared to existing FPGA-based HHE accelerators, validating the feasibility of hardware-accelerated HHE for resource-constrained deployments.", "conclusion": "The paper demonstrates that hardware-accelerated hybrid homomorphic encryption (HHE) is a viable solution for edge-deployed privacy-preserving machine learning (PPML), achieving significant reductions in latency and power consumption while establishing a scalable hardware-software co-design framework."}}
{"id": "2510.20300", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.20300", "abs": "https://arxiv.org/abs/2510.20300", "authors": ["Haojie Ji", "Long Jin", "Haowen Li", "Chongshi Xin", "Te Hu"], "title": "Privacy Protection of Automotive Location Data Based on Format-Preserving Encryption of Geographical Coordinates", "comment": null, "summary": "There are increasing risks of privacy disclosure when sharing the automotive\nlocation data in particular functions such as route navigation, driving\nmonitoring and vehicle scheduling. These risks could lead to the attacks\nincluding user behavior recognition, sensitive location inference and\ntrajectory reconstruction. In order to mitigate the data security risk caused\nby the automotive location sharing, this paper proposes a high-precision\nprivacy protection mechanism based on format-preserving encryption (FPE) of\ngeographical coordinates. The automotive coordinate data key mapping mechanism\nis designed to reduce to the accuracy loss of the geographical location data\ncaused by the repeated encryption and decryption. The experimental results\ndemonstrate that the average relative distance retention rate (RDR) reached\n0.0844, and the number of hotspots in the critical area decreased by 98.9%\nafter encryption. To evaluate the accuracy loss of the proposed encryption\nalgorithm on automotive geographical location data, this paper presents the\nexperimental analysis of decryption accuracy, and the result indicates that the\ndecrypted coordinate data achieves a restoration accuracy of 100%. This work\npresents a high-precision privacy protection method for automotive location\ndata, thereby providing an efficient data security solution for the sensitive\ndata sharing in autonomous driving.", "AI": {"tldr": "This paper presents a high-precision privacy protection mechanism using FPE for automotive location data to mitigate security risks while maintaining data accuracy.", "motivation": "The motivation stems from the increasing privacy risks associated with sharing automotive location data in functions like route navigation, driving monitoring, and vehicle scheduling. These risks include user behavior recognition, sensitive location inference, and trajectory reconstruction attacks.", "method": "The paper introduces a key mapping mechanism based on format-preserving encryption (FPE) of geographical coordinates. This approach is designed to reduce the accuracy loss of location data during repeated encryption and decryption processes.", "result": "The experimental results show an average relative distance retention rate of 0.0844, a 98.9% reduction in hotspots in critical areas post-encryption, and 100% restoration accuracy of the decrypted coordinate data.", "conclusion": "The study concludes that the proposed FPE-based method effectively protects automotive location privacy without compromising data accuracy, offering an efficient solution for secure data sharing in autonomous driving environments."}}
{"id": "2510.20314", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20314", "abs": "https://arxiv.org/abs/2510.20314", "authors": ["Wu Yichao", "Wang Yirui", "Ding Panpan", "Wang Hailong", "Zhu Bingqian", "Liu Chun"], "title": "Enhancing Security in Deep Reinforcement Learning: A Comprehensive Survey on Adversarial Attacks and Defenses", "comment": null, "summary": "With the wide application of deep reinforcement learning (DRL) techniques in\ncomplex fields such as autonomous driving, intelligent manufacturing, and smart\nhealthcare, how to improve its security and robustness in dynamic and\nchangeable environments has become a core issue in current research. Especially\nin the face of adversarial attacks, DRL may suffer serious performance\ndegradation or even make potentially dangerous decisions, so it is crucial to\nensure their stability in security-sensitive scenarios. In this paper, we first\nintroduce the basic framework of DRL and analyze the main security challenges\nfaced in complex and changing environments. In addition, this paper proposes an\nadversarial attack classification framework based on perturbation type and\nattack target and reviews the mainstream adversarial attack methods against DRL\nin detail, including various attack methods such as perturbation state space,\naction space, reward function and model space. To effectively counter the\nattacks, this paper systematically summarizes various current robustness\ntraining strategies, including adversarial training, competitive training,\nrobust learning, adversarial detection, defense distillation and other related\ndefense techniques, we also discuss the advantages and shortcomings of these\nmethods in improving the robustness of DRL. Finally, this paper looks into the\nfuture research direction of DRL in adversarial environments, emphasizing the\nresearch needs in terms of improving generalization, reducing computational\ncomplexity, and enhancing scalability and explainability, aiming to provide\nvaluable references and directions for researchers.", "AI": {"tldr": "This paper addresses DRL security challenges, proposes an attack classification framework, reviews attack/defense methods, and outlines future research directions for robustness in adversarial environments.", "motivation": "DRL's security and robustness are critical in safety-sensitive applications (autonomous driving, healthcare) where adversarial attacks can cause severe performance degradation or dangerous decisions.", "method": "The authors propose an adversarial attack classification framework based on perturbation type and attack target, review mainstream adversarial attack methods (e.g., perturbation in state/action space, reward/model functions), and systematically summarize robustness training strategies like adversarial training, competitive training, and defense distillation.", "result": "The paper categorizes attacks, evaluates defense techniques (adversarial training, robust learning, etc.), and discusses their strengths/limitations in enhancing DRL robustness.", "conclusion": "The paper emphasizes the need for future research in improving generalization, reducing computational complexity, and enhancing scalability and explainability of DRL in adversarial environments, providing directions for researchers."}}
{"id": "2510.20333", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20333", "abs": "https://arxiv.org/abs/2510.20333", "authors": ["Chiyu Chen", "Xinhao Song", "Yunkai Chai", "Yang Yao", "Haodong Zhao", "Lijun Li", "Jie Li", "Yan Teng", "Gongshen Liu", "Yingchun Wang"], "title": "GhostEI-Bench: Do Mobile Agents Resilience to Environmental Injection in Dynamic On-Device Environments?", "comment": null, "summary": "Vision-Language Models (VLMs) are increasingly deployed as autonomous agents\nto navigate mobile graphical user interfaces (GUIs). Operating in dynamic\non-device ecosystems, which include notifications, pop-ups, and inter-app\ninteractions, exposes them to a unique and underexplored threat vector:\nenvironmental injection. Unlike prompt-based attacks that manipulate textual\ninstructions, environmental injection corrupts an agent's visual perception by\ninserting adversarial UI elements (for example, deceptive overlays or spoofed\nnotifications) directly into the GUI. This bypasses textual safeguards and can\nderail execution, causing privacy leakage, financial loss, or irreversible\ndevice compromise. To systematically evaluate this threat, we introduce\nGhostEI-Bench, the first benchmark for assessing mobile agents under\nenvironmental injection attacks within dynamic, executable environments. Moving\nbeyond static image-based assessments, GhostEI-Bench injects adversarial events\ninto realistic application workflows inside fully operational Android emulators\nand evaluates performance across critical risk scenarios. We further propose a\njudge-LLM protocol that conducts fine-grained failure analysis by reviewing the\nagent's action trajectory alongside the corresponding screenshot sequence,\npinpointing failure in perception, recognition, or reasoning. Comprehensive\nexperiments on state-of-the-art agents reveal pronounced vulnerability to\ndeceptive environmental cues: current models systematically fail to perceive\nand reason about manipulated UIs. GhostEI-Bench provides a framework for\nquantifying and mitigating this emerging threat, paving the way toward more\nrobust and secure embodied agents.", "AI": {"tldr": "The paper introduces GhostEI-Bench as the first benchmark for evaluating mobile agents against environmental injection attacks in dynamic GUI settings, revealing their vulnerabilities and proposing a judge-LLM protocol for failure analysis.", "motivation": "There is a significant gap in understanding and defending against attacks that bypass textual safeguards by manipulating the visual aspects of on-device environments, such as GUIs, which can lead to serious risks like privacy leaks or device compromise.", "method": "The authors developed GhostEI-Bench, a benchmark implemented in executable environments, particularly Android emulators. It introduces adversarial events into real application flows and uses a judge-LLM protocol to analyze failures by integrating action trajectories with screenshots.", "result": "Experiments show that state-of-the-art VLM agents are highly vulnerable to environmental injection attacks, systematically failing in perception and reasoning tasks when faced with deceptive UI elements.", "conclusion": "GhostEI-Bench provides a method to assess and mitigate this growing threat to embodied agents, offering a platform for fine-grained evaluation and fostering the development of more secure models against environmental injection."}}
{"id": "2510.20367", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.20367", "abs": "https://arxiv.org/abs/2510.20367", "authors": ["Daniel Gilkarov", "Ran Dubin"], "title": "NeuPerm: Disrupting Malware Hidden in Neural Network Parameters by Leveraging Permutation Symmetry", "comment": null, "summary": "Pretrained deep learning model sharing holds tremendous value for researchers\nand enterprises alike. It allows them to apply deep learning by fine-tuning\nmodels at a fraction of the cost of training a brand-new model. However, model\nsharing exposes end-users to cyber threats that leverage the models for\nmalicious purposes. Attackers can use model sharing by hiding self-executing\nmalware inside neural network parameters and then distributing them for\nunsuspecting users to unknowingly directly execute them, or indirectly as a\ndependency in another software. In this work, we propose NeuPerm, a simple yet\neffec- tive way of disrupting such malware by leveraging the theoretical\nproperty of neural network permutation symmetry. Our method has little to no\neffect on model performance at all, and we empirically show it successfully\ndisrupts state-of-the-art attacks that were only previously addressed using\nquantization, a highly complex process. NeuPerm is shown to work on LLMs, a\nfeat that no other previous similar works have achieved. The source code is\navailable at https://github.com/danigil/NeuPerm.git.", "AI": {"tldr": "NeuPerm is a low-cost, high-effectiveness defense against malicious model sharing by exploiting neural network permutation symmetry, working on LLMs and outperforming complex baselines.", "motivation": "Pretrained model sharing enables cost-effective deep learning but exposes users to malware hidden in parameters. Existing solutions like quantization are complex and impractical for modern models like LLMs.", "method": "Leverages neural network permutation symmetry to disrupt malware embedded in model parameters by permuting weights during training or inference. It introduces minimal overhead and requires no model architecture modifications.", "result": "Empirical results show NeuPerm successfully disrupts state-of-the-art attacks with no performance degradation. It achieves this for both standard models and LLMs, where prior methods failed. Disrupts malware execution directly and as software dependencies.", "conclusion": "NeuPerm effectively disrupts malware in shared models without affecting performance, offers a simpler alternative to complex defense methods, and demonstrates efficacy on LLMs, with source code publicly available."}}
{"id": "2510.20419", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.20419", "abs": "https://arxiv.org/abs/2510.20419", "authors": ["Eric Wagner", "David Heye", "Jan Bauer", "Klaus Wehrle", "Martin Serror"], "title": "MAC Aggregation over Lossy Channels in DTLS 1.3", "comment": "IEEE ICNP'25", "summary": "Aggregating Message Authentication Codes (MACs) promises to save valuable\nbandwidth in resource-constrained environments. The idea is simple: Instead of\nappending an authentication tag to each message in a communication stream, the\nintegrity protection of multiple messages is aggregated into a single tag.\nRecent studies postulate, e.g., based on simulations, that these benefits also\nspread to wireless, and thus lossy, scenarios despite each lost packet\ntypically resulting in the loss of integrity protection information for\nmultiple messages. In this paper, we investigate these claims in a real\ndeployment. Therefore, we first design a MAC aggregation extension for the\nDatagram Transport Layer Security (DTLS) 1.3 protocol. Afterward, we\nextensively evaluate the performance of MAC aggregation on a complete\ncommunication protocol stack on embedded hardware. We find that MAC aggregation\ncan indeed increase goodput by up to 50% and save up to 17% of energy\nexpenditure for the transmission of short messages, even in lossy channels.", "AI": {"tldr": "This paper examines the real-world performance of MAC aggregation in lossy wireless networks by implementing a MAC aggregation extension for DTLS 1.3 and testing it on embedded hardware. The results show up to 50% goodput and 17% energy savings for short messages.", "motivation": "The motivation for studying MAC aggregation is to reduce bandwidth usage in resource-constrained environments and explore its benefits in wireless and lossy scenarios where packet loss can affect multiple messages.", "method": "The method involves creating a MAC aggregation extension for DTLS 1.3 and conducting an extensive evaluation of the communication protocol stack on embedded hardware.", "result": "The evaluation demonstrates that MAC aggregation improves goodput by up to 50% and reduces energy expenditure by up to 17% in lossy channels for short messages.", "conclusion": "MAC aggregation proves effective in real-world wireless deployments, offering significant bandwidth and energy savings even when some packets are lost."}}
{"id": "2510.20494", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.20494", "abs": "https://arxiv.org/abs/2510.20494", "authors": ["Florian Hofer", "Barbara Russo"], "title": "On the cybersecurity of LoRaWAN-based system: a Smart-Lighting case study", "comment": "8 pages, 6 figures plus references, International Conference on IoT", "summary": "Cyber-physical systems and the Internet of Things (IoT) are key technologies\nin the Industry 4.0 vision. They incorporate sensors and actuators to interact\nwith the physical environment. However, when creating and interconnecting\ncomponents to form a heterogeneous smart systems architecture, these face\nchallenges in cybersecurity. This paper presents an experimental investigation\nof architectural configurations for a LoRaWAN-based Smart-Lighting project,\naimed at verifying and improving the system's robustness against attacks. We\nassess the system's robustness in a series of iterative experiments conducted\nboth in-vitro and on-site. The results show that most attacks on a LoRaWAN\nnetwork are unsuccessful, also highlighting unresolved issues with the\ninstalled products. The most successful attacks are high-power jamming attacks\nwithin a few meters of the target, which, in the case of gateways, can be\nmitigated through gateway redundancy.", "AI": {"tldr": "This paper investigates the robustness of a LoRaWAN-based smart-lighting system against cyber attacks, identifying high-power jamming as a significant threat that can be mitigated through gateway redundancy.", "motivation": "The rise of cyber-physical systems and IoT in Industry 4.0 has introduced cybersecurity challenges in creating heterogeneous smart systems. This study aims to enhance the robustness of such systems, specifically in a smart-lighting context.", "method": "The study conducts an experimental investigation by analyzing architectural configurations for a LoRaWAN-based Smart-Lighting project. It evaluates system robustness through iterative experiments, both in-vitro and on-site, to assess vulnerabilities against different types of cyber attacks.", "result": "The results indicate that most attacks on the LoRaWAN network were unsuccessful. However, high-power jamming attacks within close proximity posed a significant threat. The paper notes that gateway redundancy can help mitigate these attacks on gateways.", "conclusion": "While the system demonstrates overall robustness against most attacks, high-power jamming remains a vulnerability, especially at short distances. Gateway redundancy is identified as a viable solution for mitigating jamming attacks on gateways, thus improving system reliability and resilience in smart-lighting applications."}}
{"id": "2510.20566", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20566", "abs": "https://arxiv.org/abs/2510.20566", "authors": ["Wei Shao", "Yuhao Wang", "Rongguang He", "Muhammad Ejaz Ahmed", "Seyit Camtepe"], "title": "AdaDoS: Adaptive DoS Attack via Deep Adversarial Reinforcement Learning in SDN", "comment": null, "summary": "Existing defence mechanisms have demonstrated significant effectiveness in\nmitigating rule-based Denial-of-Service (DoS) attacks, leveraging predefined\nsignatures and static heuristics to identify and block malicious traffic.\nHowever, the emergence of AI-driven techniques presents new challenges to SDN\nsecurity, potentially compromising the efficacy of existing defence mechanisms.\nIn this paper, we introduce~AdaDoS, an adaptive attack model that disrupt\nnetwork operations while evading detection by existing DoS-based detectors\nthrough adversarial reinforcement learning (RL). Specifically, AdaDoS models\nthe problem as a competitive game between an attacker, whose goal is to\nobstruct network traffic without being detected, and a detector, which aims to\nidentify malicious traffic. AdaDoS can solve this game by dynamically adjusting\nits attack strategy based on feedback from the SDN and the detector.\nAdditionally, recognising that attackers typically have less information than\ndefenders, AdaDoS formulates the DoS-like attack as a partially observed Markov\ndecision process (POMDP), with the attacker having access only to delay\ninformation between attacker and victim nodes. We address this challenge with a\nnovel reciprocal learning module, where the student agent, with limited\nobservations, enhances its performance by learning from the teacher agent, who\nhas full observational capabilities in the SDN environment. AdaDoS represents\nthe first application of RL to develop DoS-like attack sequences, capable of\nadaptively evading both machine learning-based and rule-based DoS-like attack\ndetectors.", "AI": {"tldr": "This paper presents AdaDoS, an adaptive denial-of-service attack model using adversarial reinforcement learning to evade detection.", "motivation": "Current DoS defense mechanisms are effective against rule-based attacks but struggle with AI-driven techniques, necessitating the development of more adaptive attack models to test and improve defenses.", "method": "AdaDoS employs adversarial reinforcement learning, modeling the attacker-detector interaction as a competitive game and a partially observed Markov decision process (POMDP). It includes a reciprocal learning module where a student agent learns from a teacher agent to overcome limited observations.", "result": "AdaDoS successfully evades both machine learning-based and rule-based DoS detectors through dynamic strategy adaptation and partial observation learning.", "conclusion": "The introduction of AdaDoS highlights the need for more robust defense strategies in SDN environments against adaptive AI-driven attacks, advancing the field of adversarial network security research."}}
{"id": "2510.20645", "categories": ["cs.CR", "cs.CE", "cs.DC", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.20645", "abs": "https://arxiv.org/abs/2510.20645", "authors": ["Nitin Awathare"], "title": "Decentralized Exchange that Mitigate a Bribery Attack", "comment": null, "summary": "Despite the popularity of Hashed Time-Locked Contracts (HTLCs) because of\ntheir use in wide areas of applications such as payment channels, atomic swaps,\netc, their use in exchange is still questionable. This is because of its\nincentive incompatibility and susceptibility to bribery attacks.\n  State-of-the-art solutions such as MAD-HTLC (Oakland'21) and He-HTLC\n(NDSS'23) address this by leveraging miners' profit-driven behaviour to\nmitigate such attacks. The former is the mitigation against passive miners;\nhowever, the latter works against both active and passive miners. However, they\nconsider only two bribing scenarios where either of the parties involved in the\ntransfer collude with the miner.\n  In this paper, we expose vulnerabilities in state-of-the-art solutions by\npresenting a miner-collusion bribery attack with implementation and\ngame-theoretic analysis. Additionally, we propose a stronger attack on MAD-HTLC\nthan He-HTLC, allowing the attacker to earn profits equivalent to attacking\nnaive HTLC.\n  Leveraging our insights, we propose \\prot, a game-theoretically secure HTLC\nprotocol resistant to all bribery scenarios. \\prot\\ employs a two-phase\napproach, preventing unauthorized token confiscation by third parties, such as\nminers. In Phase 1, parties commit to the transfer; in Phase 2, the transfer is\nexecuted without manipulation. We demonstrate \\prot's efficiency in transaction\ncost and latency via implementations on Bitcoin and Ethereum.", "AI": {"tldr": "This paper examines the vulnerabilities of HTLCs in exchange systems and proposes a new protocol resistant to all types of miner-collusion attacks.", "motivation": "HTLCs are widely used in payment channels and atomic swaps, but their application in exchanges is problematic due to incentive incompatibility and vulnerability to bribery attacks. Existing solutions like MAD-HTLC and He-HTLC are not fully effective against all collusion scenarios between miners and parties.", "method": "The authors identify a miner-collusion bribery attack and perform a game-theoretic analysis.  They propose a new protocol, \textbackslash prot, a two-phase HTLC protocol designed to prevent unauthorized token confiscation by third parties such as miners.", "result": "The proposed protocol is resistant to all bribery scenarios.  Implementation on Bitcoin and Ethereum demonstrates its efficiency in transaction cost and latency.", "conclusion": "The paper identifies vulnerabilities in current HTLC solutions and presents a robust, game-theoretically secure protocol offering improved resistance to miner-collusion attacks in exchange systems."}}
{"id": "2510.20657", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.20657", "abs": "https://arxiv.org/abs/2510.20657", "authors": ["Rubens Kim", "Stephan Carney", "Yvonne Fonken", "Soham Hans", "Sofia Hirschmann", "Stacy Marsella", "Peggy Wu", "Nikolos Gurney"], "title": "Risk Psychology & Cyber-Attack Tactics", "comment": "Submitted and presented at AHFE Hawaii 2025. 2 tables, 2 figures", "summary": "We examine whether measured cognitive processes predict cyber-attack\nbehavior. We analyzed data that included psychometric scale responses and\nlabeled attack behaviors from cybersecurity professionals who conducted\nred-team operations against a simulated enterprise network. We employed\nmultilevel mixed-effects Poisson regression with technique counts nested within\nparticipants to test whether cognitive processes predicted technique-specific\nusage. The scales significantly predicted technique use, but effects varied by\ntechnique rather than operating uniformly. Neither expertise level nor\nexperimental treatment condition significantly predicted technique patterns,\nindicating that cognitive processes may be stronger drivers of technique\nselection than training or experience. These findings demonstrate that\nindividual cognitive differences shape cyber-attack behavior and support the\ndevelopment of psychology-informed defense strategies.", "AI": {"tldr": "This paper investigates the relationship between cognitive processes and cyber-attack behavior in cybersecurity professionals, finding that cognitive factors significantly influence technique selection over training or experience.", "motivation": "The study aims to understand how individual cognitive differences impact cyber-attack behaviors, which could inform better defense strategies.", "method": "The research uses multilevel mixed-effects Poisson regression to analyze data from cybersecurity professionals' psychometric scale responses and labeled attack behaviors during red-team operations.", "result": "Cognitive processes significantly predicted technique use, with effects varying by technique. Expertise level and treatment conditions did not significantly influence technique patterns.", "conclusion": "Cognitive processes play a more significant role in shaping cyber-attack behavior than training or experience, suggesting the need for psychology-informed defense strategies."}}
{"id": "2510.20768", "categories": ["cs.CR", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.20768", "abs": "https://arxiv.org/abs/2510.20768", "authors": ["Austin Jia", "Avaneesh Ramesh", "Zain Shamsi", "Daniel Zhang", "Alex Liu"], "title": "RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as the dominant\narchitectural pattern to operationalize Large Language Model (LLM) usage in\nCyber Threat Intelligence (CTI) systems. However, this design is susceptible to\npoisoning attacks, and previously proposed defenses can fail for CTI contexts\nas cyber threat information is often completely new for emerging attacks, and\nsophisticated threat actors can mimic legitimate formats, terminology, and\nstylistic conventions. To address this issue, we propose that the robustness of\nmodern RAG defenses can be accelerated by applying source credibility\nalgorithms on corpora, using PageRank as an example. In our experiments, we\ndemonstrate quantitatively that our algorithm applies a lower authority score\nto malicious documents while promoting trusted content, using the standardized\nMS MARCO dataset. We also demonstrate proof-of-concept performance of our\nalgorithm on CTI documents and feeds.", "AI": {"tldr": "The paper proposes using source credibility algorithms like PageRank to improve RAG robustness in CTI, showing effectiveness with MS MARCO and CTI docs.", "motivation": "RAG systems in CTI are vulnerable to poisoning attacks due to new threats and mimicry by attackers; existing defenses are insufficient for these scenarios.", "method": "The authors apply source credibility algorithms (e.g., PageRank) to RAG's retrieval phase, assigning lower authority scores to malicious content and higher scores to trusted sources.", "result": "Experiments with the MS MARCO dataset showed the algorithm reduced authority scores for harmful documents, and initial CTI tests demonstrated proof-of-concept performance.", "conclusion": "Source credibility algorithms can enhance RAG robustness in CTI contexts, providing a scalable method to combat poisoning attacks through trust-based content prioritization."}}
