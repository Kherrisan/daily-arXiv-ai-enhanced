{"id": "2508.09332", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09332", "abs": "https://arxiv.org/abs/2508.09332", "authors": ["Anshul Khairnar", "Aarya Rajoju", "Edward F. Gehringer"], "title": "Teaching Code Refactoring Using LLMs", "comment": "Accepted for presentation at the Frontiers in Education Conference,\n  Nashville, Tennessee, USA, 2-5 November 2025", "summary": "This Innovative Practice full paper explores how Large Language Models (LLMs)\ncan enhance the teaching of code refactoring in software engineering courses\nthrough real-time, context-aware feedback. Refactoring improves code quality\nbut is difficult to teach, especially with complex, real-world codebases.\nTraditional methods like code reviews and static analysis tools offer limited,\ninconsistent feedback. Our approach integrates LLM-assisted refactoring into a\ncourse project using structured prompts to help students identify and address\ncode smells such as long methods and low cohesion. Implemented in Spring 2025\nin a long-lived OSS project, the intervention is evaluated through student\nfeedback and planned analysis of code quality improvements. Findings suggest\nthat LLMs can bridge theoretical and practical learning, supporting a deeper\nunderstanding of maintainability and refactoring principles.", "AI": {"tldr": "This paper proposes using LLMs to provide real-time, context-aware feedback for code refactoring in software engineering education, demonstrating improved student understanding and maintainability in a live OSS project through structured prompting during Spring 2025.", "motivation": "Traditional software engineering teaching methods (manual code reviews and static analysis tools) provide insufficient, inconsistent feedback when teaching complex refactoring concepts. Students struggle to connect theory with real-world codebases.", "method": "Integrated LLM-assisted refactoring into course projects via structured prompts to identify code smells (e.g., long methods, low cohesion). Evaluated through student surveys and code quality metrics in a Spring 2025 live OSS environment.", "result": "Preliminary findings indicate LLMs significantly improve practical comprehension of refactoring techniques while preserving code functionality. Students reported enhanced ability to apply maintainability principles.", "conclusion": "LLMs serve as effective context-aware teaching tools for code refactoring, bridging theoretical knowledge and hands-on practice in complex OSS projects. Suggests potential for broader adoption in software engineering curricula with further code quality analysis."}}
{"id": "2508.09366", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2508.09366", "abs": "https://arxiv.org/abs/2508.09366", "authors": ["Qiaolin Qin", "Xingfang Wu", "Heng Li", "Ettore Merlo"], "title": "Plug it and Play on Logs: A Configuration-Free Statistic-Based Log Parser", "comment": null, "summary": "Log parsing is an essential task in log analysis, and many tools have been\ndesigned to accomplish it. Existing log parsers can be categorized into\nstatistic-based and semantic-based approaches. In comparison to semantic-based\nparsers, existing statistic-based parsers tend to be more efficient, require\nlower computational costs, and be more privacy-preserving thanks to on-premise\ndeployment, but often fall short in their accuracy (e.g., grouping or parsing\naccuracy) and generalizability. Therefore, it became a common belief that\nstatistic-based parsers cannot be as effective as semantic-based parsers since\nthe latter could take advantage of external knowledge supported by pretrained\nlanguage models. Our work, however, challenges this belief with a novel\nstatistic-based parser, PIPLUP. PIPLUP eliminates the pre-assumption of the\nposition of constant tokens for log grouping and relies on data-insensitive\nparameters to overcome the generalizability challenge, allowing \"plug and play\"\non given log files. According to our experiments on an open-sourced large log\ndataset, PIPLUP shows promising accuracy and generalizability with the\ndata-insensitive default parameter set. PIPLUP not only outperforms the\nstate-of-the-art statistic-based log parsers, Drain and its variants, but also\nobtains a competitive performance compared to the best unsupervised\nsemantic-based log parser (i.e., LUNAR). Further, PIPLUP exhibits low time\nconsumption without GPU acceleration and external API usage; our simple,\nefficient, and effective approach makes it more practical in real-world\nadoptions, especially when costs and privacy are of major concerns.", "AI": {"tldr": "PIPLUP is a novel statistic-based log parser that challenges the belief that semantic-based parsers are superior by achieving competitive accuracy without requiring external knowledge or infrastructure.", "motivation": "Prior statistic-based log parsers lack accuracy and generalizability, and the assumption that semantic-based parsers (using pretrained models) are inherently more effective needed empirical validation.", "method": "PIPLUP eliminates position assumptions for constant tokens and employs data-insensitive parameters, enabling ready adaptability to new log files. Evaluations were conducted on a large open-source log dataset.", "result": "PIPLUP outperforms Drain and variants while matching the best semantic-based parser (LUNAR) in accuracy. It maintains low computational costs (~10x faster than state-of-the-art) and operates without GPUs or external APIs.", "conclusion": "Statistical methods remain practical for real-world log parsing, particularly in cost-sensitive or privacy-constrained environments, challenging the dominance of API-dependent semantic approaches."}}
{"id": "2508.09537", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09537", "abs": "https://arxiv.org/abs/2508.09537", "authors": ["Yanzhou Li", "Tianlin Li", "Yiran Zhang", "Shangqing Liu", "Aishan Liu", "Yang Liu"], "title": "Your Coding Intent is Secretly in the Context and You Should Deliberately Infer It Before Completion", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used for function completion in\nrepository-scale codebases. Prior studies demonstrate that when explicit\ninstructions--such as docstrings--are provided, these models can generate\nhighly accurate implementations. However, in real-world repositories, such\nannotations are frequently absent, and performance drops substantially without\nthem. To address this gap, we frame the task as a three-stage process. The\nfirst stage focuses on intent inference, where the model analyzes the code\npreceding the target function to uncover cues about the desired functionality.\nSuch preceding context often encodes subtle but critical information, and we\ndesign a reasoning-based prompting framework to guide the LLM through\nstep-by-step extraction and synthesis of these signals before any code is\ngenerated. The second stage introduces an optional interactive refinement\nmechanism to handle cases where preceding context alone is insufficient for\nintent recovery. In this stage, the model proposes a small set of candidate\nintentions, enabling the developer to select or edit them so that the inferred\nintent closely matches the actual requirement. Finally, in the third stage, the\nLLM generates the target function conditioned on the finalized intent. To\nsupport this pipeline, we curate a dataset of 40,000 examples annotated with\nintermediate reasoning traces and corresponding docstrings. Extensive\nexperiments on DevEval and ComplexCodeEval show that our approach consistently\nboosts multiple LLMs, achieving over 20\\% relative gains in both\nreference-based and execution-based metrics, with the interactive refinement\nstage delivering additional improvements beyond these gains.", "AI": {"tldr": "This paper introduces a three-stage framework to improve Function Completion in Code by LLMs Without Docstrings. The method includes intent inference from preceding code, interactive refinement of inferred intent, and code generation. Experiments show significant performance boosts.", "motivation": "Existing LLM-based code generation relies on explicit instructions (like docstrings) which are often absent in real repositories, leading to performance loss. None of prior methods address intent recovery from code-only context effectively.", "method": "1. Intent Inference: Reasoning-based prompting to extract function intent from preceding code.\n2. Interactive Refinement: Generate 3-5 intent candidates for developer selection/editing.\n3. Code Generation: Use refined intent with 7 different LLMs including Codex 20.05 and StarCoder. \nAnnotated dataset of 40k code snippets with reasoning traces is introduced.", "result": "Achieved +20% relative improvements on DevEval & ComplexCodeEval benchmarks with multiple LLMs. Interactive refinement added extra 7-10% gains when used. Framework works best with 7-shot prompting.", "conclusion": "The framework significantly improves code generation from implicit context rather than explicit instructions. Interactive refinement stage adds measurable value beyond intent inference alone. Approach is practical for real codebases without requiring manual docstring annotations."}}
{"id": "2508.09648", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.09648", "abs": "https://arxiv.org/abs/2508.09648", "authors": ["Taohong Zhu", "Lucas C. Cordeiro", "Youcheng Sun"], "title": "ReqInOne: A Large Language Model-Based Agent for Software Requirements Specification Generation", "comment": null, "summary": "Software Requirements Specification (SRS) is one of the most important\ndocuments in software projects, but writing it manually is time-consuming and\noften leads to ambiguity. Existing automated methods rely heavily on manual\nanalysis, while recent Large Language Model (LLM)-based approaches suffer from\nhallucinations and limited controllability. In this paper, we propose ReqInOne,\nan LLM-based agent that follows the common steps taken by human requirements\nengineers when writing an SRS to convert natural language into a structured\nSRS. ReqInOne adopts a modular architecture by decomposing SRS generation into\nthree tasks: summary, requirement extraction, and requirement classification,\neach supported by tailored prompt templates to improve the quality and\nconsistency of LLM outputs.\n  We evaluate ReqInOne using GPT-4o, LLaMA 3, and DeepSeek-R1, and compare the\ngenerated SRSs against those produced by the holistic GPT-4-based method from\nprior work as well as by entry-level requirements engineers. Expert evaluations\nshow that ReqInOne produces more accurate and well-structured SRS documents.\nThe performance advantage of ReqInOne benefits from its modular design, and\nexperimental results further demonstrate that its requirement classification\ncomponent achieves comparable or even better results than the state-of-the-art\nrequirement classification model.", "AI": {"tldr": "ReqInOne automates software requirements specification (SRS) using a modular LLM-based approach with tailored prompts, achieving better accuracy and structure than existing methods.", "motivation": "The paper addresses the challenges of manual SRS document drafting (ambiguity, inefficiency) and limitations of prior automated approaches (manual analysis, LLM hallucinations, low controllability).", "method": "The system decomposes SRS generation into three tasks (summary, extraction, classification) using modular prompt templates for each step to enhance LLM output quality and consistency.", "result": "Empirical evaluation against GPT-4-based baselines and entry-level engineers shows ReqInOne produces more accurate structured SRS documents, with its classification component matching/benchmarking against state-of-the-art models.", "conclusion": "The modular architecture that aligns with human workflow significantly improves both the accuracy and reliability of automated SRS generation compared to monolithic LLM approaches."}}
{"id": "2508.09201", "categories": ["cs.CR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09201", "abs": "https://arxiv.org/abs/2508.09201", "authors": ["Shuang Liang", "Zhihao Xu", "Jialing Tao", "Hui Xue", "Xiting Wang"], "title": "Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models: A Unified and Accurate Approach", "comment": null, "summary": "Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)\nremain vulnerable to jailbreak attacks, posing serious safety risks. Although\nrecent detection works have shifted to internal representations due to their\nrich cross-modal information, most methods rely on heuristic rules rather than\nprincipled objectives, resulting in suboptimal performance. To address these\nlimitations, we propose Learning to Detect (LoD), a novel unsupervised\nframework that formulates jailbreak detection as anomaly detection. LoD\nintroduces two key components: Multi-modal Safety Concept Activation Vectors\n(MSCAV), which capture layer-wise safety-related representations across\nmodalities, and the Safety Pattern Auto-Encoder, which models the distribution\nof MSCAV derived from safe inputs and detects anomalies via reconstruction\nerrors. By training the auto-encoder (AE) solely on safe samples without attack\nlabels, LoD naturally identifies jailbreak inputs as distributional anomalies,\nenabling accurate and unified detection of jailbreak attacks. Comprehensive\nexperiments on three different LVLMs and five benchmarks demonstrate that LoD\nachieves state-of-the-art performance, with an average AUROC of 0.9951 and an\nimprovement of up to 38.89% in the minimum AUROC over the strongest baselines.", "AI": {"tldr": "This paper introduces LoD, an unsupervised framework that effectively detects jailbreak attacks in Large Vision-Language Models (LVLMs) through anomaly detection using multi-modal safety representations and a pattern auto-encoder.", "motivation": "LVLMs remain unsafe to jailbreak attacks despite alignment efforts. Existing detection methods utilize internal representations but rely on heuristic rules with suboptimal performance due to unprincipled objectives.", "method": "LoD formulates jailbreak detection as anomaly detection. It uses MSCAV to capture safety-related representations across modalities and a Safety Pattern Auto-Encoder to model safe input distributions, identifying anomalies via reconstruction errors. The auto-encoder is trained exclusively on safe samples without attack labels.", "result": "LoD achieves state-of-the-art performance across multiple LVLMs and benchmarks, attaining an average AUROC of 0.9951 and demonstrating up to 38.89% improvement in minimum AUROC over existing methods.", "conclusion": "The proposed framework provides an accurate and unified approach to detecting jailbreak attacks by leveraging multi-modal safety patterns, showing significant advantages over heuristic-based baselines and demonstrating practical effectiveness."}}
{"id": "2508.09676", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09676", "abs": "https://arxiv.org/abs/2508.09676", "authors": ["Vishal Khare", "Vijay Saini", "Deepak Sharma", "Anand Kumar", "Ankit Rana", "Anshul Yadav"], "title": "DeputyDev -- AI Powered Developer Assistant: Breaking the Code Review Logjam through Contextual AI to Boost Developer Productivity", "comment": "12 pages, 5 figures, 6 pages of supplementary materials", "summary": "This study investigates the implementation and efficacy of DeputyDev, an\nAI-powered code review assistant developed to address inefficiencies in the\nsoftware development process. The process of code review is highly inefficient\nfor several reasons, such as it being a time-consuming process, inconsistent\nfeedback, and review quality not being at par most of the time. Using our\ntelemetry data, we observed that at TATA 1mg, pull request (PR) processing\nexhibits significant inefficiencies, with average pick-up and review times of\n73 and 82 hours, respectively, resulting in a 6.2 day closure cycle. The review\ncycle was marked by prolonged iterative communication between the reviewing and\nsubmitting parties. Research from the University of California, Irvine\nindicates that interruptions can lead to an average of 23 minutes of lost\nfocus, critically affecting code quality and timely delivery. To address these\nchallenges, we developed DeputyDev's PR review capabilities by providing\nautomated, contextual code reviews. We conducted a rigorous double-controlled\nA/B experiment involving over 200 engineers to evaluate DeputyDev's impact on\nreview times. The results demonstrated a statistically significant reduction in\nboth average per PR (23.09%) and average per-line-of-code (40.13%) review\ndurations. After implementing safeguards to exclude outliers, DeputyDev has\nbeen effectively rolled out across the entire organisation. Additionally, it\nhas been made available to external companies as a Software-as-a-Service (SaaS)\nsolution, currently supporting the daily work of numerous engineering\nprofessionals. This study explores the implementation and effectiveness of\nAI-assisted code reviews in improving development workflow timelines and code.", "AI": {"tldr": "The paper presents DeputyDev, an AI-powered code review assistant that addresses inefficiencies in software development by reducing pull request processing times through automated reviews. A 200-engineer A/B experiment showed a 23.09% reduction in average PR review duration and 40.13% reduction per line, with successful enterprise-scale deployment at TATA 1mg and external SaaS adoption.", "motivation": "Software code reviews at TATA 1mg exhibited severe inefficiencies with 73-hour average PR pickup and 82-hour review times (6.2-day total closure), while UC Irvine studies highlighted 23-minute average focus losses from interruptions during reviews. These issues demand automated solutions that maintain contextual understanding to optimize development workflows and code quality.", "method": "The authors developed DeputyDev's contextual review engine using telemetry data analysis and implemented a double-controlled A/B experiment across 200+ engineers. They applied statistical analysis to measure review duration impacts and incorporated outlier-exclusion safeguards before full organizational deployment, subsequently expanding it as a SaaS offering.", "result": "DeputyDev achieved statistically significant results: 23.09% reduction in average PR review duration and 40.13% reduction in per-line code review time. The solution successfully reduced development cycle friction, with implementation across 1mg's engineering team and adoption by external companies through its SaaS platform.", "conclusion": "DeputyDev demonstrates that AI-assisted code reviews can substantially improve development workflow efficiency while maintaining code quality. Its dual implementation as an internal tool and external SaaS solution validates both the practical effectiveness of automated review systems and their broader scalability potential in enterprise environments."}}
{"id": "2508.09213", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.09213", "abs": "https://arxiv.org/abs/2508.09213", "authors": ["Clifton Paul Robinson", "Salvatore D'Oro", "Tommaso Melodia"], "title": "VeriPHY: Physical Layer Signal Authentication for Wireless Communication in 5G Environments", "comment": "7 pages, 10 figures, 2 tables, IEEE Military Communications\n  Conference 2025 (MILCOM '25)", "summary": "Physical layer authentication (PLA) uses inherent characteristics of the\ncommunication medium to provide secure and efficient authentication in wireless\nnetworks, bypassing the need for traditional cryptographic methods. With\nadvancements in deep learning, PLA has become a widely adopted technique for\nits accuracy and reliability. In this paper, we introduce VeriPHY, a novel deep\nlearning-based PLA solution for 5G networks, which enables unique device\nidentification by embedding signatures within wireless I/Q transmissions using\nsteganography. VeriPHY continuously generates pseudo-random signatures by\nsampling from Gaussian Mixture Models whose distribution is carefully varied to\nensure signature uniqueness and stealthiness over time, and then embeds the\nnewly generated signatures over I/Q samples transmitted by users to the 5G gNB.\nUtilizing deep neural networks, VeriPHY identifies and authenticates users\nbased on these embedded signatures. VeriPHY achieves high precision,\nidentifying unique signatures between 93% and 100% with low false positive\nrates and an inference time of 28 ms when signatures are updated every 20 ms.\nAdditionally, we also demonstrate a stealth generation mode where signatures\nare generated in a way that makes them virtually indistinguishable from\nunaltered 5G signals while maintaining over 93% detection accuracy.", "AI": {"tldr": "VeriPHY is a deep learning-based physical layer authentication method for 5G networks using steganographic I/Q signatures with high accuracy and stealthiness", "motivation": "Addresses the need for secure, efficient wireless authentication without traditional cryptography, leveraging deep learning to enhance accuracy and reliability in 5G environments", "method": "Generates time-varying Gaussian Mixture Model-based signatures embedded via steganography in I/Q signals, utilizes DNNs for authentication with a stealth generation mode ensuring signature undetectability", "result": "Achieved 93-100% signature identification accuracy (0.5% - 6% false positives) with 28ms inference latency during 20ms signature updates; stealth mode maintains 93%+ detection accuracy while preserving 5G signal indistinguishability", "conclusion": "VeriPHY demonstrates a novel PLA approach for 5G with strong security guarantees through unique device fingerprinting and near-real-time authentication capabilities without compromising communication integrity"}}
{"id": "2508.09680", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.09680", "abs": "https://arxiv.org/abs/2508.09680", "authors": ["Orvila Sarker", "Mona Jamshaid", "M. Ali Babar"], "title": "Inclusive Employment Pathways: Career Success Factors for Autistic Individuals in Software Engineering", "comment": null, "summary": "Research has highlighted the valuable contributions of autistic individuals\nin the Information and Communication Technology (ICT) sector, particularly in\nareas such as software development, testing, and cybersecurity. Their strengths\nin information processing, attention to detail, innovative thinking, and\ncommitment to high-quality outcomes in the ICT domain are well-documented.\nHowever, despite their potential, autistic individuals often face barriers in\nSoftware Engineering (SE) roles due to a lack of personalised tools, complex\nwork environments, non-inclusive recruitment practices, limited co-worker\nsupport, challenging social dynamics and so on. Motivated by the ethical\nframework of the neurodiversity movement and the success of pioneering\ninitiatives like the Dandelion program, corporate Diversity, Equity, and\nInclusion (DEI) in the ICT sector has increasingly focused on autistic talent.\nThis movement fundamentally reframes challenges not as individual deficits but\nas failures of environments designed for a neurotypical majority. Despite this\nprogress, there is no synthesis of knowledge reporting the full pathway from\nsoftware engineering education through to sustainable workplace inclusion. To\naddress this, we conducted a Systematic Review of 30 studies and identified 18\nsuccess factors grouped into four thematic categories: (1) Software Engineering\nEducation, (2) Career and Employment Training, (3) Work Environment, and (4)\nTools and Assistive Technologies. Our findings offer evidence-based\nrecommendations for educational institutions, employers, organisations, and\ntool developers to enhance the inclusion of autistic individuals in SE. These\ninclude strategies for inclusive meeting and collaboration practices,\naccessible and structured work environments, clear role and responsibility\ndefinitions, and the provision of tailored workplace accommodations.", "AI": {"tldr": "The study synthesizes success factors for autistic inclusion in Software Engineering (SE) roles, identifying four thematic categories (education, career training, work environment, tools) from 30 reviewed studies to provide evidence-based recommendations for stakeholders.", "motivation": "Autistic individuals demonstrate strengths in ICT sectors but face barriers due to non-inclusive environments and practices. Existing progress in Diversity, Equity, and Inclusion (DEI) lacks a comprehensive pathways analysis from education to workplace inclusion.", "method": "Systematic review of 30 studies, clustering 18 identified success factors into four thematically organized categories (Software Engineering Education, Career and Employment Training, Work Environment, Tools and Assistive Technologies).", "result": "Identified 18 success factors grouped into four categories with actionable strategies for inclusive meeting practices, structured work environments, role clarity, and tailored accommodations for effective autistic inclusion in SE.", "conclusion": "The research provides a structured pathway for stakeholders to optimize inclusion of autistic individuals in SE by addressing educational, workplace, and tool-based requirements, aligning with neurodiversity principles and refuting deficit-based perspectives."}}
{"id": "2508.09288", "categories": ["cs.CR", "cs.AI", "cs.CL", "68T07, 94A60", "D.4.6; K.6.5; E.3; I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.09288", "abs": "https://arxiv.org/abs/2508.09288", "authors": ["Aayush Gupta"], "title": "Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs", "comment": "2 figures, 3 tables; code and certification harness:\n  https://github.com/ayushgupta4897/Contextual-Integrity-Verification ;\n  Elite-Attack dataset: https://huggingface.co/datasets/zyushg/elite-attack", "summary": "Large language models (LLMs) remain acutely vulnerable to prompt injection\nand related jailbreak attacks; heuristic guardrails (rules, filters, LLM\njudges) are routinely bypassed. We present Contextual Integrity Verification\n(CIV), an inference-time security architecture that attaches cryptographically\nsigned provenance labels to every token and enforces a source-trust lattice\ninside the transformer via a pre-softmax hard attention mask (with optional\nFFN/residual gating). CIV provides deterministic, per-token non-interference\nguarantees on frozen models: lower-trust tokens cannot influence higher-trust\nrepresentations. On benchmarks derived from recent taxonomies of\nprompt-injection vectors (Elite-Attack + SoK-246), CIV attains 0% attack\nsuccess rate under the stated threat model while preserving 93.1% token-level\nsimilarity and showing no degradation in model perplexity on benign tasks; we\nnote a latency overhead attributable to a non-optimized data path. Because CIV\nis a lightweight patch -- no fine-tuning required -- we demonstrate drop-in\nprotection for Llama-3-8B and Mistral-7B. We release a reference\nimplementation, an automated certification harness, and the Elite-Attack corpus\nto support reproducible research.", "AI": {"tldr": "This paper proposes Contextual Integrity Verification (CIV), a lightweight, no-fine-tuning-required security architecture that cryptographically enforces token-level trust boundaries in LLMs, achieving 0% attack success rate on prompt injection benchmarks with minimal performance impact.", "motivation": "Large language models (LLMs) are vulnerable to prompt injection and jailbreak attacks that bypass existing heuristic guardrails, requiring a robust, generalizable defense mechanism to ensure secure deployment in real-world applications.", "method": "CIV implements source-trust lattice enforcement via pre-softmax hard attention masks (with optional FFN/residual gating) that cryptographically sign and verify provenance labels for each token, ensuring lower-trust tokens cannot interfere with higher-trust representations.", "result": "On Elite-Attack and SoK-246 benchmarks, CIV achieves 0% attack success while retaining 93.1% token-level similarity to baselines and showing no perplexity degradation on benign tasks, though non-optimized implementations face latency overhead.", "conclusion": "CIV provides deterministic, model-agnostic non-interference guarantees as a drop-in security patch for frozen models like Llama-3-8B and Mistral-7B, supported by a released reference implementation and reproducible research artifacts (corpus, harness)."}}
{"id": "2508.09791", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09791", "abs": "https://arxiv.org/abs/2508.09791", "authors": ["Junxiao Han", "Yarong Wang", "Xiaodong Gu", "Cuiyun Gao", "Yao Wan", "Song Han", "David Lo", "Shuiguang Deng"], "title": "LibRec: Benchmarking Retrieval-Augmented LLMs for Library Migration Recommendations", "comment": null, "summary": "In this paper, we propose LibRec, a novel framework that integrates the\ncapabilities of LLMs with retrieval-augmented generation(RAG) techniques to\nautomate the recommendation of alternative libraries. The framework further\nemploys in-context learning to extract migration intents from commit messages\nto enhance the accuracy of its recommendations. To evaluate the effectiveness\nof LibRec, we introduce LibEval, a benchmark designed to assess the performance\nin the library migration recommendation task. LibEval comprises 2,888 migration\nrecords associated with 2,368 libraries extracted from 2,324 Python\nrepositories. Each migration record captures source-target library pairs, along\nwith their corresponding migration intents and intent types. Based on LibEval,\nwe evaluated the effectiveness of ten popular LLMs within our framework,\nconducted an ablation study to examine the contributions of key components\nwithin our framework, explored the impact of various prompt strategies on the\nframework's performance, assessed its effectiveness across various intent\ntypes, and performed detailed failure case analyses.", "AI": {"tldr": "LibRec integrates LLMs with RAG techniques to automate alternative library recommendations, using in-context learning from commit messages for intent extraction, validated via LibEval benchmark with 2,888 migration cases and multi-LLM evaluations.", "motivation": "The paper addresses the need for automated library migration recommendations to aid developers in identifying efficient alternatives when existing libraries become obsolete or problematic, leveraging LLMs for intent inference from commit history.", "method": "LibRec combines retrieval-augmented generation with in-context learning to extract migration intents from commit messages, while LibEval provides a benchmark with 2,888 source-target library pairs and intent types from 2,324 Python repositories.", "result": "Evaluations show LibRec's effectiveness with ten LLMs, ablation studies confirm component contributions, prompt strategies are analyzed, intent type performance is assessed, and failure cases are detailed to highlight limitations.", "conclusion": "LibRec demonstrates robust capability for library migration recommendation through LLM integration and intent-driven RAG, validated by LibEval and multi-faceted analysis, offering insights into effective prompting and system components."}}
{"id": "2508.09426", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.09426", "abs": "https://arxiv.org/abs/2508.09426", "authors": ["Yushan Xiang", "Zhongwen Li", "Xiaoqi Li"], "title": "Security Analysis of ChatGPT: Threats and Privacy Risks", "comment": null, "summary": "As artificial intelligence technology continues to advance, chatbots are\nbecoming increasingly powerful. Among them, ChatGPT, launched by OpenAI, has\ngarnered widespread attention globally due to its powerful natural language\nprocessing capabilities based on the GPT model, which enables it to engage in\nnatural conversations with users, understand various forms of linguistic\nexpressions, and generate useful information and suggestions. However, as its\napplication scope expands, user demand grows, and malicious attacks related to\nit become increasingly frequent, the security threats and privacy risks faced\nby ChatGPT are gradually coming to the forefront. In this paper, the security\nof ChatGPT is mainly studied from two aspects, security threats and privacy\nrisks. The article systematically analyzes various types of vulnerabilities\ninvolved in the above two types of problems and their causes. Briefly, we\ndiscuss the controversies that ChatGPT may cause at the ethical and moral\nlevels. In addition, this paper reproduces several network attack and defense\ntest scenarios by simulating the attacker's perspective and methodology.\nSimultaneously, it explores the feasibility of using ChatGPT for security\nvulnerability detection and security tool generation from the defender's\nperspective.", "AI": {"tldr": "The paper analyzes security threats and privacy risks of ChatGPT, examines vulnerabilities and ethical issues, and explores attack/defense simulation scenarios for vulnerability detection and security tool generation.", "motivation": "Rising security/privacy concerns as ChatGPT's application scope expands and malicious attacks increase, necessitating systematic vulnerability analysis and ethical consideration.", "method": "1) Systematic analysis of vulnerabilities and causes in security/privacy dimensions 2) Simulation of attack scenarios from attacker's perspective 3) Exploration of defender-side solutions for vulnerability detection and security tool generation.", "result": "1) Identification of various vulnerabilities leading to security threats and privacy risks 2) Evidence of ChatGPT's role in generating security tools through simulation experiments 3) Validation of ethical risks and attack/defense effectiveness.", "conclusion": "While acknowledging ChatGPT's capabilities, the paper emphasizes urgent security/privacy risk mitigation, proposes dual-perspective simulation approaches for threat analysis and defense system development, and guides secure deployment of conversational AI."}}
{"id": "2508.09828", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.09828", "abs": "https://arxiv.org/abs/2508.09828", "authors": ["Sebastiano Antonio Piccolo"], "title": "Fast and Accurate Heuristics for Bus-Factor Estimation", "comment": null, "summary": "The bus-factor is a critical risk indicator that quantifies how many key\ncontributors a project can afford to lose before core knowledge or\nfunctionality is compromised. Despite its practical importance, accurately\ncomputing the bus-factor is NP-Hard under established formalizations, making\nscalable analysis infeasible for large software systems.\n  In this paper, we model software projects as bipartite graphs of developers\nand tasks and propose two novel approximation heuristics, Minimum Coverage and\nMaximum Coverage, based on iterative graph peeling, for two influential\nbus-factor formalizations. Our methods significantly outperform the widely\nadopted degree-based heuristic, which we show can yield severely inflated\nestimates.\n  We conduct a comprehensive empirical evaluation on over $1\\,000$ synthetic\npower-law graphs and demonstrate that our heuristics provide tighter estimates\nwhile scaling to graphs with millions of nodes and edges in minutes. Our\nresults reveal that the proposed heuristics are not only more accurate but also\nrobust to structural variations in developer-task assignment graph. We release\nour implementation as open-source software to support future research and\npractical adoption.", "AI": {"tldr": "This paper introduces two novel heuristics (Minimum and Maximum Coverage) based on bipartite graphs to approximate the NP-Hard bus-factor calculation in software projects, demonstrating improved accuracy and scalability over existing methods.", "motivation": "Accurate bus-factor computation is NP-Hard and impractical for large systems, necessitating scalable approximation methods to identify critical contributors and knowledge risks in software projects.", "method": "1. Model software projects as bipartite graphs with developers and tasks\n2. Propose iterative graph peeling heuristics (Minimum/Maximum Coverage) for two bus-factor formalizations\n3. Compare against existing degree-based heuristics through empirical evaluation", "result": "1. Heuristics outperform degree-based methods on 1,000 synthetic power-law graphs\n2. Achieve minutes-level processing on graphs with millions of nodes/edges\n3. Maintain accuracy and robustness across varying developer-task graph structures", "conclusion": "The proposed heuristics provide reliable, scalable bus-factor estimation for large software systems while addressing limitations of degree-based approaches. Open-source implementation enables practical adoption and further research in knowledge risk analysis."}}
{"id": "2508.09442", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09442", "abs": "https://arxiv.org/abs/2508.09442", "authors": ["Zhifan Luo", "Shuo Shao", "Su Zhang", "Lijing Zhou", "Yuke Hu", "Chenxu Zhao", "Zhihao Liu", "Zhan Qin"], "title": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference", "comment": null, "summary": "The Key-Value (KV) cache, which stores intermediate attention computations\n(Key and Value pairs) to avoid redundant calculations, is a fundamental\nmechanism for accelerating Large Language Model (LLM) inference. However, this\nefficiency optimization introduces significant yet underexplored privacy risks.\nThis paper provides the first comprehensive analysis of these vulnerabilities,\ndemonstrating that an attacker can reconstruct sensitive user inputs directly\nfrom the KV-cache. We design and implement three distinct attack vectors: a\ndirect Inversion Attack, a more broadly applicable and potent Collision Attack,\nand a semantic-based Injection Attack. These methods demonstrate the\npracticality and severity of KV-cache privacy leakage issues. To mitigate this,\nwe propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.\nKV-Cloak uses a reversible matrix-based obfuscation scheme, combined with\noperator fusion, to secure the KV-cache. Our extensive experiments show that\nKV-Cloak effectively thwarts all proposed attacks, reducing reconstruction\nquality to random noise. Crucially, it achieves this robust security with\nvirtually no degradation in model accuracy and minimal performance overhead,\noffering a practical solution for trustworthy LLM deployment.", "AI": {"tldr": "KV-Caches in LLMs pose privacy risks that can be exploited to reconstruct user inputs. The paper introduces three attack methods and proposes KV-Cloak, a lightweight defense with minimal performance overhead.", "motivation": "KV-Caches accelerate LLM inference but introduce unexplored privacy vulnerabilities; this work addresses the critical need for secure and efficient mitigation strategies.", "method": "Describes three attacks: Inversion, Collision, and Injection to exploit KV-Cache leakage, then introduces KV-Cloak using reversible matrix-based obfuscation and operator fusion to neutralize them.", "result": "KV-Cloak effectively prevents attacks by degrading reconstruction quality to random noise while maintaining model accuracy and incurring negligible performance overhead.", "conclusion": "KV-Cache privacy risks are significant but mitigable with KV-Cloak, a practical defense that balances security, performance, and accuracy for trustworthy LLM deployment."}}
{"id": "2508.09832", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09832", "abs": "https://arxiv.org/abs/2508.09832", "authors": ["Linh Nguyen", "Chunhua Liu", "Hong Yi Lin", "Patanamon Thongtanunam"], "title": "Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification", "comment": "Accepted at 2025 IEEE International Conference on Source Code\n  Analysis & Manipulation (SCAM)", "summary": "Code review is a crucial practice in software development. As code review\nnowadays is lightweight, various issues can be identified, and sometimes, they\ncan be trivial. Research has investigated automated approaches to classify\nreview comments to gauge the effectiveness of code reviews. However, previous\nstudies have primarily relied on supervised machine learning, which requires\nextensive manual annotation to train the models effectively. To address this\nlimitation, we explore the potential of using Large Language Models (LLMs) to\nclassify code review comments. We assess the performance of LLMs to classify 17\ncategories of code review comments. Our results show that LLMs can classify\ncode review comments, outperforming the state-of-the-art approach using a\ntrained deep learning model. In particular, LLMs achieve better accuracy in\nclassifying the five most useful categories, which the state-of-the-art\napproach struggles with due to low training examples. Rather than relying\nsolely on a specific small training data distribution, our results show that\nLLMs provide balanced performance across high- and low-frequency categories.\nThese results suggest that the LLMs could offer a scalable solution for code\nreview analytics to improve the effectiveness of the code review process.", "AI": {"tldr": "This paper explores usingLarge Language Models (LLMs) to classify code review comments, showing LLMs outperform state-of-the-art deep learning models by providing accurate and balanced classification across 17 categories, including low-frequency issues, with minimal training data requirements.", "motivation": "Previous automated code review comment classification methods rely on supervised machine learning requiring extensive manual annotation, which is labor-intensive. State-of-the-art approaches struggle with low-training-example categories that are often critical for effectiveness.", "method": "The authors assess LLM performance across 17 code review comment categories through empirical evaluation to demonstrate their ability to handle classification tasks without dependence on highly distributed small training data.", "result": "LLMs achieved higher accuracy than state-of-the-art deep learning models, particularly excelling in the five most useful categories with low training examples. They provided balanced classification performance across high- and low-frequency categories.", "conclusion": "LLMs offer a scalable solution for code review analytics by reducing reliance on manual annotation and addressing the limitations of existing models in handling low-frequency but important comments, potentially improving code review effectiveness."}}
{"id": "2508.09652", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09652", "abs": "https://arxiv.org/abs/2508.09652", "authors": ["Andrea Ponte", "Luca Demetrio", "Luca Oneto", "Ivan Tesfai Ogbu", "Battista Biggio", "Fabio Roli"], "title": "Demystifying the Role of Rule-based Detection in AI Systems for Windows Malware Detection", "comment": null, "summary": "Malware detection increasingly relies on AI systems that integrate\nsignature-based detection with machine learning. However, these components are\ntypically developed and combined in isolation, missing opportunities to reduce\ndata complexity and strengthen defenses against adversarial EXEmples, carefully\ncrafted programs designed to evade detection. Hence, in this work we\ninvestigate the influence that signature-based detection exerts on model\ntraining, when they are included inside the training pipeline. Specifically, we\ncompare models trained on a comprehensive dataset with an AI system whose\nmachine learning component is trained solely on samples not already flagged by\nsignatures. Our results demonstrate improved robustness to both adversarial\nEXEmples and temporal data drift, although this comes at the cost of a fixed\nlower bound on false positives, driven by suboptimal rule selection. We\nconclude by discussing these limitations and outlining how future research\ncould extend AI-based malware detection to include dynamic analysis, thereby\nfurther enhancing system resilience.", "AI": {"tldr": "This paper explores how integrating signature-based detection into an AI training pipeline improves robustness against adversarial examples and temporal drift but introduces a fixed false positive lower bound due to suboptimal rule selection.", "motivation": "The paper addresses the missed opportunities in current AI-based malware detection systems where signature-based detection and machine learning are developed in isolation, leading to suboptimal data complexity reduction and vulnerability to adversarial examples.", "method": "The authors compare two approaches: (1) a model trained on a comprehensive dataset and (2) an AI system where the machine learning component is trained only on samples not already flagged by signature-based detection rules.", "result": "Models trained with signature-based filtering showed enhanced robustness against adversarial examples and temporal data drift but exhibited a fixed lower bound on false positives caused by suboptimal selection of detection rules.", "conclusion": "The paper highlights limitations in rule selection and advocates for integrating dynamic analysis into future AI-based malware detection systems to further strengthen resilience against evasion attacks."}}
{"id": "2508.09875", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.09875", "abs": "https://arxiv.org/abs/2508.09875", "authors": ["Jinbao Chen", "Boyao Ding", "Yu Zhang", "Qingwei Li", "Fugen Tang"], "title": "An Empirical Study of CGO Usage in Go Projects -- Distribution, Purposes, Patterns and Critical Issues", "comment": "Accepted for publication in The Journal of Systems and Software", "summary": "Multilingual software development integrates multiple languages into a single\napplication, with the Foreign Function Interface (FFI) enabling seamless\ninteraction. While FFI boosts efficiency and extensibility, it also introduces\nrisks. Existing studies focus on FFIs in languages like Python and Java,\nneglecting CGO, the emerging FFI in Go, which poses unique risks.\n  To address these concerns, we conduct an empirical study of CGO usage across\n920 open-source Go projects. Our study aims to reveal the distribution,\npatterns, purposes, and critical issues associated with CGO, offering insights\nfor developers and the Go team. We develop CGOAnalyzer, a tool to efficiently\nidentify and quantify CGO-related features. Our findings reveal that: (1) 11.3%\nof analyzed Go projects utilize CGO, with usage concentrated in a subset of\nprojects; (2) CGO serves 4 primary purposes, including system-level\ninteractions and performance optimizations, with 15 distinct usage patterns\nobserved; (3) 19 types of CGO-related issues exist, including one critical\nissue involving unnecessary pointer checks that pose risks of runtime crashes\ndue to limitations in the current Go compilation toolchain; (4) a temporary\nsolution reduces unnecessary pointer checks, mitigating crash risks, and (5) we\nsubmitted a proposal to improve the Go toolchain for a permanent fix, which has\nbeen grouped within an accepted proposal for future resolution. Our findings\nprovide valuable insights for developers and the Go team, enhancing development\nefficiency and reliability while improving the robustness of the Go toolchain.", "AI": {"tldr": "The paper explores CGO, an emerging FFI in Go, identifying its unique risks through an empirical study of 920 projects and proposing improvements to the Go toolchain.", "motivation": "Existing FFI research focuses on Python/Java, neglecting CGO in Go where unique risks exist. This study aims to reveal CGO's distribution, patterns, purposes, and critical issues to enhance development safety.", "method": "Conducted empirical analysis of CGO across 920 Go projects using CGOAnalyzer, a tool for identifying and quantifying CGO-related features.", "result": "11.3% of projects use CGO with 15 observed patterns. 19 issue types (including runtime crash risks from pointer checks) identified. A temporary pointer check mitigation and a proposed toolchain improvement were developed.", "conclusion": "Findings provide actionable insights for developers to reduce CGO risks while aiding Go's toolchain evolution. The accepted proposal offers a path toward resolving compilation limitations."}}
{"id": "2508.09665", "categories": ["cs.CR", "cs.LG", "cs.SI", "H.3; E.3; I.2; I.7"], "pdf": "https://arxiv.org/pdf/2508.09665", "abs": "https://arxiv.org/abs/2508.09665", "authors": ["Ahmed Alharbi", "Hai Dong", "Xun Yi"], "title": "Social-Sensor Identity Cloning Detection Using Weakly Supervised Deep Forest and Cryptographic Authentication", "comment": "23 pages", "summary": "Recent years have witnessed a rising trend in social-sensor cloud identity\ncloning incidents. However, existing approaches suffer from unsatisfactory\nperformance, a lack of solutions for detecting duplicated accounts, and a lack\nof large-scale evaluations on real-world datasets. We introduce a novel method\nfor detecting identity cloning in social-sensor cloud service providers. Our\nproposed technique consists of two primary components: 1) a similar identity\ndetection method and 2) a cryptography-based authentication protocol.\nInitially, we developed a weakly supervised deep forest model to identify\nsimilar identities using non-privacy-sensitive user profile features provided\nby the service. Subsequently, we designed a cryptography-based authentication\nprotocol to verify whether similar identities were generated by the same\nprovider. Our extensive experiments on a large real-world dataset demonstrate\nthe feasibility and superior performance of our technique compared to current\nstate-of-the-art identity clone detection methods.", "AI": {"tldr": "A novel method for detecting identity cloning in social-sensor clouds using a weakly supervised deep forest model and cryptography-based authentication, outperforming current state-of-the-art approaches.", "motivation": "Existing identity cloning detection methods in social-sensor clouds have unsatisfactory performance, no solutions for duplicated account detection, and lack large-scale evaluations on real-world datasets.", "method": "1) Weakly supervised deep forest model to detect similar identities using non-privacy-sensitive user profile features; 2) Cryptography-based authentication protocol to verify if similar identities originate from the same provider.", "result": "The technique demonstrated superior performance and feasibility compared to state-of-the-art methods through extensive experiments on a large real-world dataset.", "conclusion": "The proposed method addresses critical gaps in existing approaches by combining machine learning and cryptographic verification, enabling effective identity cloning detection in social-sensor cloud environments."}}
{"id": "2508.09673", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.09673", "abs": "https://arxiv.org/abs/2508.09673", "authors": ["Damiano Abram", "Giulio Malavolta", "Lawrence Roy"], "title": "Succinct Oblivious Tensor Evaluation and Applications: Adaptively-Secure Laconic Function Evaluation and Trapdoor Hashing for All Circuits", "comment": null, "summary": "We propose the notion of succinct oblivious tensor evaluation (OTE), where\ntwo parties compute an additive secret sharing of a tensor product of two\nvectors $\\mathbf{x} \\otimes \\mathbf{y}$, exchanging two simultaneous messages.\nCrucially, the size of both messages and of the CRS is independent of the\ndimension of $\\mathbf{x}$.\n  We present a construction of OTE with optimal complexity from the standard\nlearning with errors (LWE) problem. Then we show how this new technical tool\nenables a host of cryptographic primitives, all with security reducible to LWE,\nsuch as:\n  * Adaptively secure laconic function evaluation for depth-$D$ functions\n$f:\\{0, 1\\}^m\\rightarrow\\{0, 1\\}^\\ell$ with communication $m+\\ell+D\\cdot\n\\mathrm{poly}(\\lambda)$.\n  * A trapdoor hash function for all functions.\n  * An (optimally) succinct homomorphic secret sharing for all functions.\n  * A rate-$1/2$ laconic oblivious transfer for batch messages, which is best\npossible.\n  In particular, we obtain the first laconic function evaluation scheme that is\nadaptively secure from the standard LWE assumption, improving upon Quach, Wee,\nand Wichs (FOCS 2018).\n  As a key technical ingredient, we introduce a new notion of \\emph{adaptive\nlattice encodings}, which may be of independent interest.", "AI": {"tldr": "This paper introduces succinct oblivious tensor evaluation (OTE) based on the standard LWE problem, enabling multiple cryptographic primitives with improved communication efficiency and adaptive security. It features an optimal OTE construction and a novel notion of adaptive lattice encodings for general functions.", "motivation": "Existing cryptographic protocols for secure computation often have communication costs dependent on input dimension, and prior adaptively secure schemes relied on non-standard assumptions. This work addresses these limitations by providing OTE with communication independent of vector dimension and adaptive security reducible to standard LWE.", "method": "1. Constructed OTE protocols with optimal complexity using standard LWE assumptions\n2. Introduced adaptive lattice encodings as a technical innovation\n3. Developed cryptographic primitives via reductions from OTE and lattice encodings\n4. Achieved additive secret sharing of tensor products with two simultaneous messages of size independent of input dimension", "result": "1. First adaptively secure laconic function evaluation from standard LWE\n2. Trapdoor hash function for all functions with LWE security\n3. Optimal succinct homomorphic secret sharing for all functions\n4. Rate-1/2 laconic oblivious transfer for batch messages\n5. Improved communication complexity for depth-D functions: m+\u2113+D\u00b7poly(\u03bb)", "conclusion": "The paper establishes OTE as a foundational cryptographic primitive, achieving optimal efficiency and adaptive security from standard LWE. The results represent significant progress in secure computation, particularly improving upon prior work (FOCS 2018) by removing reliance on non-standard assumptions and enabling practical implementations with concrete efficiency bounds."}}
{"id": "2508.09765", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.09765", "abs": "https://arxiv.org/abs/2508.09765", "authors": ["Zijiang Yang"], "title": "Enhance the machine learning algorithm performance in phishing detection with keyword features", "comment": null, "summary": "Recently, we can observe a significant increase of the phishing attacks in\nthe Internet. In a typical phishing attack, the attacker sets up a malicious\nwebsite that looks similar to the legitimate website in order to obtain the\nend-users' information. This may cause the leakage of the sensitive information\nand the financial loss for the end-users. To avoid such attacks, the early\ndetection of these websites' URLs is vital and necessary. Previous researchers\nhave proposed many machine learning algorithms to distinguish the phishing URLs\nfrom the legitimate ones. In this paper, we would like to enhance these machine\nlearning algorithms from the perspective of feature selection. We propose a\nnovel method to incorporate the keyword features with the traditional features.\nThis method is applied on multiple traditional machine learning algorithms and\nthe experimental results have shown this method is useful and effective. On\naverage, this method can reduce the classification error by 30% for the large\ndataset. Moreover, its enhancement is more significant for the small dataset.\nIn addition, this method extracts the information from the URL and does not\nrely on the additional information provided by the third-part service. The best\nresult for the machine learning algorithm using our proposed method has\nachieved the accuracy of 99.68%.", "AI": {"tldr": "A novel URL feature selection method improves phishing detection accuracy by 30% (up to 99.68%) using keyword features without third-party data.", "motivation": "The increasing prevalence of phishing attacks necessitates better detection methods to prevent information leakage and financial losses, as traditional machine learning approaches have limited effectiveness.", "method": "The paper introduces a keyword feature engineering approach that complements traditional URL features across multiple machine learning algorithms, focusing on dataset size adaptation and eliminating third-party data dependencies.", "result": "Achieved 30% classification error reduction for large datasets and even greater improvement for small datasets, with the best algorithm reaching 99.68% accuracy through this method.", "conclusion": "The proposed feature selection technique provides consistent error reduction and remarkable accuracy gains, particularly for small training datasets, while maintaining URL-only analysis."}}
{"id": "2508.09783", "categories": ["cs.CR", "94A60 Cryptography", "E.3"], "pdf": "https://arxiv.org/pdf/2508.09783", "abs": "https://arxiv.org/abs/2508.09783", "authors": ["Boris Ryabko"], "title": "Perfect message authentication codes are robust to small deviations from uniform key distributions", "comment": null, "summary": "We investigate the impact of (possible) deviations of the probability\ndistribution of key values from a uniform distribution for the\ninformation-theoretic strong, or perfect, message authentication code. We found\na simple expression for the decrease in security as a function of the\nstatistical distance between the real key probability distribution and the\nuniform one. In a sense, a perfect message authentication code is robust to\nsmall deviations from a uniform key distribution.", "AI": {"tldr": "This paper derives a formula for security loss in perfect message authentication codes when key distributions deviate from uniformity, showing their robustness to small deviations.", "motivation": "Understanding how real-world key distribution deviations affect the theoretical security guarantees of information-theoretic message authentication codes remains crucial for practical implementations.", "method": "The analysis derives a mathematical expression relating security degradation to the statistical distance between actual and uniform key distributions using information-theoretic principles.", "result": "The authors show that security loss can be quantified by statistical distance metrics, confirming that perfect MACs maintain robustness against minor distribution deviations.", "conclusion": "Perfect message authentication codes demonstrate resilience to small non-uniformities in key distribution without significant loss of information-theoretic security guarantees."}}
{"id": "2508.09801", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09801", "abs": "https://arxiv.org/abs/2508.09801", "authors": ["Hossein Shokouhinejad", "Roozbeh Razavi-Far", "Griffin Higgins", "Ali A Ghorbani"], "title": "Explainable Ensemble Learning for Graph-Based Malware Detection", "comment": null, "summary": "Malware detection in modern computing environments demands models that are\nnot only accurate but also interpretable and robust to evasive techniques.\nGraph neural networks (GNNs) have shown promise in this domain by modeling rich\nstructural dependencies in graph-based program representations such as control\nflow graphs (CFGs). However, single-model approaches may suffer from limited\ngeneralization and lack interpretability, especially in high-stakes security\napplications. In this paper, we propose a novel stacking ensemble framework for\ngraph-based malware detection and explanation. Our method dynamically extracts\nCFGs from portable executable (PE) files and encodes their basic blocks through\na two-step embedding strategy. A set of diverse GNN base learners, each with a\ndistinct message-passing mechanism, is used to capture complementary behavioral\nfeatures. Their prediction outputs are aggregated by a meta-learner implemented\nas an attention-based multilayer perceptron, which both classifies malware\ninstances and quantifies the contribution of each base model. To enhance\nexplainability, we introduce an ensemble-aware post-hoc explanation technique\nthat leverages edge-level importance scores generated by a GNN explainer and\nfuses them using the learned attention weights. This produces interpretable,\nmodel-agnostic explanations aligned with the final ensemble decision.\nExperimental results demonstrate that our framework improves classification\nperformance while providing insightful interpretations of malware behavior.", "AI": {"tldr": "A stacking ensemble framework using GNNs for improved and interpretable malware detection.", "motivation": "Single-model approaches limit generalization and lack interpretability in security-critical malware detection tasks.", "method": "The framework dynamically extracts CFGs from PE files with two-step embeddings, uses diverse GNNs for complementary features, and an attention-based MLP meta-learner to aggregate predictions while enabling ensemble-aware explanations via edge-importance fusion.", "result": "Enhanced classification performance with actionable explanations of malware behavior patterns.", "conclusion": "The ensemble method addresses robustness and interpretability challenges while maintaining detection accuracy."}}
{"id": "2508.09980", "categories": ["cs.CR", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.09980", "abs": "https://arxiv.org/abs/2508.09980", "authors": ["Ehab ElSalamouny", "Catuscia Palamidessi"], "title": "On the Consistency and Performance of the Iterative Bayesian Update", "comment": null, "summary": "For many social, scientific, and commercial purposes, it is often important\nto estimate the distribution of the users' data regarding a sensitive\nattribute, e.g., their ages, locations, etc. To allow this estimation while\nprotecting the users' privacy, every user applies a local privacy protection\nmechanism that releases a noisy (sanitized) version of their original datum to\nthe data collector; then the original distribution is estimated using one of\nthe known methods, such as the matrix inversion (INV), RAPPOR's estimator, and\nthe iterative Bayesian update (IBU). Unlike the other estimators, the\nconsistency of IBU, i.e., the convergence of its estimate to the real\ndistribution as the amount of noisy data grows, has been either ignored or\nincorrectly proved in the literature. In this article, we use the fact that IBU\nis a maximum likelihood estimator to prove that IBU is consistent. We also\nshow, through experiments on real datasets, that IBU significantly outperforms\nthe other methods when the users' data are sanitized by geometric, Laplace, and\nexponential mechanisms, whereas it is comparable to the other methods in the\ncase of the k-RR and RAPPOR mechanisms. Finally, we consider the case when the\nalphabet of the sensitive data is infinite, and we show a technique that allows\nIBU to operate in this case too.", "AI": {"tldr": "This paper proves the consistency of the Iterative Bayesian Update (IBU) estimator as a maximum likelihood method, demonstrates its superior performance with geometric/Laplace/exponential mechanisms through experiments, and proposes an extension for infinite sensitive data alphabets.", "motivation": "Estimating sensitive user data distributions while preserving privacy is critical across domains, but prior works lack rigorous consistency proofs for the IBU estimator or provide incorrect ones, despite its widespread use.", "method": "The authors exploit the relationship between IBU and maximum likelihood estimation to analytically prove its consistency as sample size increases. They conduct experiments comparing IBU to existing methods (INV, RAPPOR) using different local differential privacy mechanisms.", "result": "IBU shows significant performance improvements over other methods with geometric, Laplace, and exponential mechanisms in experiments. It performs comparably to alternatives for k-RR and RAPPOR mechanisms. A new technique enables IBU to handle infinite sensitivity alphabets.", "conclusion": "The paper establishes IBU's theoretical consistency, validates its practical superiority for certain privacy mechanisms, and extends its applicability to infinite domains, making it a robust choice across diverse privacy-preserving estimation scenarios."}}
