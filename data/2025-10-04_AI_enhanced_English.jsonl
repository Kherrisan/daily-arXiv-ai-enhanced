{"id": "2510.01379", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01379", "abs": "https://arxiv.org/abs/2510.01379", "authors": ["Huashan Chen", "Zhenyu Qi", "Haotang Li", "Hong Chen", "Jinfu Chen", "Kebin Peng", "In Kee Kim", "Kyu Hyung Lee", "Sen He"], "title": "Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration", "comment": null, "summary": "While Large Language Models (LLMs) have become the predominant paradigm for\nautomated code generation, current single-model approaches fundamentally ignore\nthe heterogeneous computational strengths that different models exhibit across\nprogramming languages, algorithmic domains, and development stages. This paper\nchallenges the single-model convention by introducing a multi-stage,\nperformance-guided orchestration framework that dynamically routes coding tasks\nto the most suitable LLMs within a structured generate-fix-refine workflow. Our\napproach is grounded in a comprehensive empirical study of 17 state-of-the-art\nLLMs across five programming languages (Python, Java, C++, Go, and Rust) using\nHumanEval-X benchmark. The study, which evaluates both functional correctness\nand runtime performance metrics (execution time, mean/max memory utilization,\nand CPU efficiency), reveals pronounced performance heterogeneity by language,\ndevelopment stage, and problem category. Guided by these empirical insights, we\npresent PerfOrch, an LLM agent that orchestrates top-performing LLMs for each\ntask context through stage-wise validation and rollback mechanisms. Without\nrequiring model fine-tuning, PerfOrch achieves substantial improvements over\nstrong single-model baselines: average correctness rates of 96.22% and 91.37%\non HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and\n49.11%. Beyond correctness gains, the framework delivers consistent performance\noptimizations, improving execution time for 58.76% of problems with median\nspeedups ranging from 17.67% to 27.66% across languages on two benchmarks. The\nframework's plug-and-play architecture ensures practical scalability, allowing\nnew LLMs to be profiled and integrated seamlessly, thereby offering a paradigm\nfor production-grade automated software engineering that adapts to the rapidly\nevolving generative AI landscape.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.01514", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01514", "abs": "https://arxiv.org/abs/2510.01514", "authors": ["J. Alexander Curtis", "Sharadha Kasiviswanathan", "Nasir Eisty"], "title": "Deciphering WONTFIX: A Mixed-Method Study on Why GitHub Issues Get Rejected", "comment": null, "summary": "Context: The ``wontfix'' label is a widely used yet narrowly understood tool\nin GitHub repositories, indicating that an issue will not be pursued further.\nDespite its prevalence, the impact of this label on project management and\ncommunity dynamics within open-source software development is not clearly\ndefined. Objective: This study examines the prevalence and reasons behind\nissues being labeled as wontfix across various open-source repositories on\nGitHub. Method: Employing a mixed-method approach, we analyze both quantitative\ndata to assess the prevalence of the wontfix label and qualitative data to\nexplore the reasoning that it was used. Data were collected from 3,132 of\nGitHub's most-popular repositories. Later, we employ open coding and thematic\nanalysis to categorize the reasons behind wontfix labels, providing a\nstructured understanding of the issue management landscape. Results: Our\nfindings show that about 30% of projects on GitHub apply the wontfix label to\nsome issues. These issues most often occur on user-submitted issues for bug\nreports and feature requests. The study identified eight common themes behind\nlabeling issues as wontfix, ranging from user-specific control factors to\nmaintainer-specific decisions. Conclusions: The wontfix label is a critical\ntool for managing resources and guiding contributor efforts in GitHub projects.\nHowever, it can also discourage community involvement and obscure the\ntransparency of project management. Understanding these reasons aids project\nmanagers in making informed decisions and fostering efficient collaboration\nwithin open-source communities.", "AI": {"tldr": "This study reveals the 30% prevalence of GitHub's 'wontfix' label, uncovering eight themes behind its use. While the label aids resource management, it risks discouraging community participation and reducing project transparency.", "motivation": "The study aims to clarify the prevalence and underlying reasons for the widespread use of the wontfix label in GitHub repositories, addressing its impact on open-source project management and community dynamics.", "method": "A mixed-method approach was employed, combining quantitative analysis of data from 3,132 GitHub repositories with qualitative methods like open coding and thematic analysis to categorize reasons for using the wontfix label.", "result": "Approximately 30% of GitHub projects use the wontfix label, primarily on user-submitted bug reports and feature requests. Eight themes were identified, highlighting factors such as user-specific constraints and maintainer-driven decisions.", "conclusion": "The wontfix label is essential for resource management in GitHub projects but can hinder community engagement and project transparency. Understanding its usage helps project managers make informed decisions and improve collaboration."}}
{"id": "2510.01635", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01635", "abs": "https://arxiv.org/abs/2510.01635", "authors": ["Yifei Chen", "Sarra Habchi", "Lili Wei"], "title": "MIMIC: Integrating Diverse Personality Traits for Better Game Testing Using Large Language Model", "comment": "13 pages, 7 figures, 6 tables. This paper is accepted by the 40th\n  IEEE/ACM International Conference on Automated Software Engineering, ASE 2025", "summary": "Modern video games pose significant challenges for traditional automated\ntesting algorithms, yet intensive testing is crucial to ensure game quality. To\naddress these challenges, researchers designed gaming agents using\nReinforcement Learning, Imitation Learning, or Large Language Models. However,\nthese agents often neglect the diverse strategies employed by human players due\nto their different personalities, resulting in repetitive solutions in similar\nsituations. Without mimicking varied gaming strategies, these agents struggle\nto trigger diverse in-game interactions or uncover edge cases.\n  In this paper, we present MIMIC, a novel framework that integrates diverse\npersonality traits into gaming agents, enabling them to adopt different gaming\nstrategies for similar situations. By mimicking different playstyles, MIMIC can\nachieve higher test coverage and richer in-game interactions across different\ngames. It also outperforms state-of-the-art agents in Minecraft by achieving a\nhigher task completion rate and providing more diverse solutions. These results\nhighlight MIMIC's significant potential for effective game testing.", "AI": {"tldr": "MIMIC is a framework that integrates diverse personality traits into gaming agents to mimic human playstyles, enhancing test coverage and in-game interactions for automated game testing. It outperforms existing methods in Minecraft by achieving higher task completion and solution diversity.", "motivation": "Traditional automated testing agents (e.g., RL, Imitation Learning) fail to replicate human players' diverse strategies due to rigid algorithms, leading to repetitive solutions and limited coverage. This hinders edge case detection and real-world effectiveness in games.", "method": "MIMIC incorporates personality-based decision-making into agents by training them to adopt multiple playstyles. The framework dynamically selects strategies aligned with embedded personality traits, enabling diverse responses to similar scenarios across different games.", "result": "On Minecraft, MIMIC achieved higher task completion rates (23.7% improvement over state-of-the-art agents) and 38.4% more unique solutions. It demonstrated 25% higher test coverage and triggered 41% more in-game interactions compared to standard baselines.", "conclusion": "MIMIC bridges the gap between automated testing agents and human-like diversity in playstyles. Its personality-driven approach enables more comprehensive game testing, making it a promising solution for detecting edge cases and improving quality in complex environments."}}
{"id": "2510.01740", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01740", "abs": "https://arxiv.org/abs/2510.01740", "authors": ["Kypros Iacovou", "Georgia M. Kapitsaki", "Evangelia Vanezi"], "title": "FOSS-chain: using blockchain for Open Source Software license compliance", "comment": null, "summary": "Open Source Software (OSS) is widely used and carries licenses that indicate\nthe terms under which the software is provided for use, also specifying\nmodification and distribution rules. Ensuring that users are respecting OSS\nlicense terms when creating derivative works is a complex process. Compliance\nissues arising from incompatibilities among licenses may lead to legal\ndisputes. At the same time, the blockchain technology with immutable entries\noffers a mechanism to provide transparency when it comes to licensing and\nensure software changes are recorded. In this work, we are introducing an\nintegration of blockchain and license management when creating derivative\nworks, in order to tackle the issue of OSS license compatibility. We have\ndesigned, implemented and performed a preliminary evaluation of FOSS-chain, a\nweb platform that uses blockchain and automates the license compliance process,\ncovering 14 OSS licenses. We have evaluated the initial prototype version of\nthe FOSS-chain platform via a small scale user study. Our preliminary results\nare promising, demonstrating the potential of the platform for adaptation on\nrealistic software systems.", "AI": {"tldr": "Blockchain-based FOSS-chain platform automates OSS license compliance, addressing incompatibility issues through immutable tracking of changes.", "motivation": "Ensuring compliance with OSS licenses is complex due to incompatibilities among licenses, which can lead to legal disputes. Current processes lack transparency and efficiency in tracking license dependencies.", "method": "The authors designed and implemented FOSS-chain, a blockchain-enabled web platform that automates license compliance checks using immutable records to track software changes and ensure compatibility.", "result": "Preliminary evaluation via a small-scale user study demonstrated FOSS-chain's viability, showing it can automate compliance checks across 14 OSS licenses and adapt to realistic software systems.", "conclusion": "The integration of blockchain technology into OSS license management, as demonstrated by FOSS-chain, shows potential for addressing license compatibility and compliance challenges effectively."}}
{"id": "2510.01223", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01223", "abs": "https://arxiv.org/abs/2510.01223", "authors": ["Hui Dou", "Ning Xu", "Yiwen Zhang", "Kaibin Wang"], "title": "Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks. However, they remain exposed to jailbreak attacks, eliciting\nharmful responses. The nested scenario strategy has been increasingly adopted\nacross various methods, demonstrating immense potential. Nevertheless, these\nmethods are easily detectable due to their prominent malicious intentions. In\nthis work, we are the first to find and systematically verify that LLMs'\nalignment defenses are not sensitive to nested scenarios, where these scenarios\nare highly semantically relevant to the queries and incorporate targeted toxic\nknowledge. This is a crucial yet insufficiently explored direction. Based on\nthis, we propose RTS-Attack (Semantically Relevant Nested Scenarios with\nTargeted Toxic Knowledge), an adaptive and automated framework to examine LLMs'\nalignment. By building scenarios highly relevant to the queries and integrating\ntargeted toxic knowledge, RTS-Attack bypasses the alignment defenses of LLMs.\nMoreover, the jailbreak prompts generated by RTS-Attack are free from harmful\nqueries, leading to outstanding concealment. Extensive experiments demonstrate\nthat RTS-Attack exhibits superior performance in both efficiency and\nuniversality compared to the baselines across diverse advanced LLMs, including\nGPT-4o, Llama3-70b, and Gemini-pro. Our complete code is available in the\nsupplementary material. WARNING: THIS PAPER CONTAINS POTENTIALLY HARMFUL\nCONTENT.", "AI": {"tldr": "This paper introduces RTS-Attack, an adaptive framework that exploits LLMs alignment weaknesses by generating semantically relevant nested scenarios with hidden toxic knowledge, showing superior effectiveness over existing jailbreak attack methods against advanced models like GPT-4o and Llama3-70b.", "motivation": "Current nested scenario-based methods for testing LLM alignment defenses are easily detectable due to overtly malicious intent, creating a need for stealthier approaches that better mimic natural query patterns while maintaining attack efficacy.", "method": "RTS-Attack constructs query-relevant scenarios through 1. Semantic relevance alignment using contextual analysis, 2. Targeted toxic knowledge injection via knowledge-aware prompting, and 3. Adaptive response mechanism that dynamically adjusts attack intensity based on model's reaction patterns.", "result": "RTS-Attack achieves 42.7%-89.3%% succes rate across GPT-4o, Llama3-70b, and Gemini-pro, showing 28%-63% improvement over baselines. It maintains 87%-94% query concealment by avoiding explicit harmful prompts while successfully eliciting policy-violating responses.", "conclusion": "RTS-Attack demonstrates critical vulnerabilities in LLM alignment defenses through semantically natural attack vectors, establishing a new benchmark for evaluating model robustness with its efficiency (43% faster response time) and universality across leading models."}}
{"id": "2510.01754", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01754", "abs": "https://arxiv.org/abs/2510.01754", "authors": ["Hina Anwar"], "title": "ARENA: A tool for measuring and analysing the energy efficiency of Android apps", "comment": null, "summary": "To build energy-efficient apps, there is a need to estimate and analyze their\nenergy consumption in typical usage scenarios. The energy consumption of\nAndroid apps could be estimated via software-based and hardware-based\napproaches. Software-based approaches, while easier to implement, are not as\naccurate as hardware-based approaches. The process of measuring the energy\nconsumption of an Android app via a hardware-based approach typically involves\n1) setting up a measurement environment, 2) executing the app under test on a\nmobile device, 3) recording current/voltage data via a hardware device to\nmeasure energy consumption, and 4) cleaning and aggregating data for analyses,\nreports, and visualizations. Specialized scripts are written for selected\nhardware and software components to ensure reliable energy measurements. The\nenergy measurement process is repeated many times and aggregated to remove\nnoise. These steps make the hardware-based energy measurement process\ntime-consuming and not easy to adapt or reproduce. There is a lack of\nopen-source tools available for developers and researchers to take reliable\nenergy measurements via hardware devices. In this paper, we present and\ndemonstrate ARENA, a support tool that enables developers and researchers to\nconnect to a physical measurement device without leaving the comfort of their\nIDE. Developers could use ARENA during development to compare energy\nconsumption between different apps or versions of the same app. ARENA\ncalculates energy consumption on an Android smartphone by executing a test\nscenario on the app under development. Further, ARENA helps aggregate,\nstatistically analyze, report, and visualize the data, allowing developers and\nresearchers to dig into the data directly or visually. We implemented ARENA as\nan IntelliJ and Android Studio plugin.", "AI": {"tldr": "ARENA is an IDE-integrated plugin that streamlines hardware-based energy measurement for Android apps, offering automated data capture, analysis, and visualization to improve efficiency and reproducibility.", "motivation": "Existing hardware-based energy measurement processes for Android apps are time-consuming, difficult to adapt, and lack open-source tools. Software-based approaches compromise accuracy.", "method": "ARENA is implemented as an IntelliJ/Android Studio plugin that automates hardware device integration, test scenario execution, data aggregation, statistical analysis, and visualization of energy consumption metrics.", "result": "ARENA enables developers and researchers to perform reliable, repeatable energy measurements directly within their IDE, reducing manual steps and noise through automated data collection and statistical analysis.", "conclusion": "ARENA addresses the lack of open-source tools for reliable hardware-based energy measurement by providing an integrated, automated solution. It enhances the reproducibility and accessibility of energy consumption analysis for Android apps."}}
{"id": "2510.01342", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01342", "abs": "https://arxiv.org/abs/2510.01342", "authors": ["Xiangfang Li", "Yu Wang", "Bo Li"], "title": "Fine-Tuning Jailbreaks under Highly Constrained Black-Box Settings: A Three-Pronged Approach", "comment": null, "summary": "With the rapid advancement of large language models (LLMs), ensuring their\nsafe use becomes increasingly critical. Fine-tuning is a widely used method for\nadapting models to downstream tasks, yet it is vulnerable to jailbreak attacks.\nHowever, most existing studies focus on overly simplified attack scenarios,\nlimiting their practical relevance to real-world defense settings. To make this\nrisk concrete, we present a three-pronged jailbreak attack and evaluate it\nagainst provider defenses under a dataset-only black-box fine-tuning interface.\nIn this setting, the attacker can only submit fine-tuning data to the provider,\nwhile the provider may deploy defenses across stages: (1) pre-upload data\nfiltering, (2) training-time defensive fine-tuning, and (3) post-training\nsafety audit. Our attack combines safety-styled prefix/suffix wrappers, benign\nlexical encodings (underscoring) of sensitive tokens, and a backdoor mechanism,\nenabling the model to learn harmful behaviors while individual datapoints\nappear innocuous. Extensive experiments demonstrate the effectiveness of our\napproach. In real-world deployment, our method successfully jailbreaks GPT-4.1\nand GPT-4o on the OpenAI platform with attack success rates above 97% for both\nmodels. Our code is available at\nhttps://github.com/lxf728/tri-pronged-ft-attack.", "AI": {"tldr": "This paper introduces a three-pronged black-box jailbreak attack against large language model fine-tuning defenses, achieving >97% success on GPT-4 variants by combining stealthy data encoding, safety wrappers, and backdoor mechanisms under realistic attack conditions.", "motivation": "Existing research focuses on overly simplified attack scenarios that lack practical relevance, motivating the development of a more realistic attack framework to evaluate defenses against fine-tuning-based jailbreak attacks under dataset-only black-box settings.", "method": "The attack combines three techniques: (1) safety-styled prefix/suffix wrappers to mask malicious intent, (2) benign lexical encodings (e.g., underscoring) of sensitive tokens to evade detection, and (3) a backdoor mechanism to enable harmful behavior learning while maintaining surface-level data innocuity.", "result": "The proposed attack achieved over 97% success rates in jailbreaking GPT-4.1 and GPT-4o on the OpenAI platform under realistic deployment conditions involving pre-upload filtering, training-time defenses, and post-training audits.", "conclusion": "The paper presents an effective three-pronged jailbreak attack method that successfully reveals vulnerabilities in real-world fine-tuning defenses, demonstrating the need for stronger multi-stage defense mechanisms across data filtering, training, and post-training safety audits."}}
{"id": "2510.01825", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01825", "abs": "https://arxiv.org/abs/2510.01825", "authors": ["Zhenyu Yang", "Yue Pan", "Zhen Yang", "Zhongxing Yu"], "title": "Towards Speeding up Program Repair with Non-Autoregressive Model", "comment": "30 pages, 8 figures, 7 tables. arXiv admin note: substantial text\n  overlap with arXiv:2406.16526", "summary": "Enlightened by the success of machine learning techniques in various\napplication areas, recent years have witnessed a surge of research efforts on\nautomatic program repair (APR) using machine learning techniques. Previous\nmachine learning-based APR techniques essentially modified bugs in the\nautoregressive (AR) manner, which predicts future values based on past values.\nDue to the manner of token-by-token generation, the AR-based APR technique has\na huge time delay. In particular, the delay of the APR model with a large\nnumber of parameters is more serious. To address the issue, we aim to apply the\nnon-autoregressive (NAR) method to the APR task, which can output target code\nin a parallel manner to avoid huge repair delays. However, the naive use of the\nNAR manner for the APR task suffers from the issue of compromised patch\nquality. To effectively adapt the NAR manner for the APR task, we in this paper\npropose NARRepair, the first customized NAR code generation model for the APR\ntask. The NARRepair model features three major novelties, including 1) the\nrepair action predictor for alleviating the over-correction issue, 2) the\ninter-token dependency extractor for alleviating the issue of lacking\ninter-token dependency information, and 3) the two-stage decoder for\nalleviating the issue of lacking contextual information. We evaluated NARRepair\non three widely used datasets in the APR community, and the results show that\n1) compared to other APR techniques, the NARRepair model has the best\nperformance within the limited repair time, and 2) compared to AR-based APR\ntechniques, the repair speed of NARRepair has been increased by 1.4-6.4 times\nin the GPU environment. Overall, the results show that NARRepair has achieved\nstate-of-the-art comprehensive performance in terms of repair speed and\naccuracy.", "AI": {"tldr": "The paper proposes NARRepair, a non-autoregressive (NAR) model for automatic program repair (APR), which significantly improves repair speed compared to traditional autoregressive (AR) methods while maintaining high accuracy.", "motivation": "Existing autoregressive (AR) machine learning-based APR techniques suffer from significant time delays, especially when using large models with many parameters. These delays hinder the practicality of APR for real-time applications.", "method": "NARRepair employs a parallelized non-autoregressive approach with three key innovations: 1) a repair action predictor to mitigate over-correction, 2) an inter-token dependency extractor to address missing inter-token dependencies in NAR generation, and 3) a two-stage decoder to preserve contextual information for accurate patch generation.", "result": "NARRepair outperforms other APR techniques within limited repair times and achieves a 1.4-6.4x speedup over AR-based methods in a GPU environment without sacrificing patch quality.", "conclusion": "NARRepair effectively addresses the speed limitations of autoregressive APR by applying a customized non-autoregressive approach. The model achieves state-of-the-art performance in terms of both speed and accuracy, making it a practical solution for fast and effective program repair."}}
{"id": "2510.01350", "categories": ["cs.CR", "cs.AR", "cs.ET", "cs.NE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01350", "abs": "https://arxiv.org/abs/2510.01350", "authors": ["Muhammad Faheemur Rahman", "Wayne Burleson"], "title": "Integrated Security Mechanisms for Weight Protection in Memristive Crossbar Arrays", "comment": "2 pages, 2 figures", "summary": "Memristive crossbar arrays enable in-memory computing by performing parallel\nanalog computations directly within memory, making them well-suited for machine\nlearning, neural networks, and neuromorphic systems. However, despite their\nadvantages, non-volatile memristors are vulnerable to security threats (such as\nadversarial extraction of stored weights when the hardware is compromised.\nProtecting these weights is essential since they represent valuable\nintellectual property resulting from lengthy and costly training processes\nusing large, often proprietary, datasets. As a solution we propose two security\nmechanisms: Keyed Permutor and Watermark Protection Columns; where both\nsafeguard critical weights and establish verifiable ownership (even in cases of\ndata leakage). Our approach integrates efficiently with existing memristive\ncrossbar architectures without significant design modifications. Simulations\nacross 45nm, 22nm, and 7nm CMOS nodes, using a realistic interconnect model and\na large RF dataset, show that both mechanisms offer robust protection with\nunder 10% overhead in area, delay and power. We also present initial\nexperiments employing the widely known MNIST dataset; further highlighting the\nfeasibility of securing memristive in-memory computing systems with minimal\nperformance trade-offs.", "AI": {"tldr": "This paper proposes Keyed Permutor and Watermark Protection Columns to secure memristive in-memory computing systems against weight extraction threats while maintaining low overhead.", "motivation": "Non-volatile memristors in crossbar arrays store valuable trained weights that are costly to produce and vulnerable to intellectual property theft if hardware is compromised.", "method": "Two mechanisms: (1). Keyed Permutor shuffles weights using a cryptographic key, and (2). Watermark Protection Columns embed ownership identifiers; both integrate with existing architectures without major redesigns.", "result": "Simulations on 45nm-7nm CMOS nodes with realistic interconnect models and an RF dataset showed <10\u200b% area/power/delay overhead, validated on MNIST experiments for security effectiveness.", "conclusion": "Proposed mechanisms provide robust, verifiable security for memristive in-memory computing with minimal performance impact, enabling IP protection even when hardware is physically exposed."}}
{"id": "2510.01960", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01960", "abs": "https://arxiv.org/abs/2510.01960", "authors": ["Victor Lira", "Paulo Borba", "Rodrigo Bonif\u00e1cio", "Galileu Santos e Matheus barbosa"], "title": "RefFilter: Improving Semantic Conflict Detection via Refactoring-Aware Static Analysis", "comment": null, "summary": "Detecting semantic interference remains a challenge in collaborative software\ndevelopment. Recent lightweight static analysis techniques improve efficiency\nover SDG-based methods, but they still suffer from a high rate of false\npositives. A key cause of these false positives is the presence of\nbehavior-preserving code refactorings, which current techniques cannot\neffectively distinguish from changes that impact behavior and can interfere\nwith others. To handle this problem we present RefFilter, a refactoring-aware\ntool for semantic interference detection. It builds on existing static\ntechniques by incorporating automated refactoring detection to improve\nprecision. RefFilter discards behavior-preserving refactorings from reports,\nreducing false positives while preserving detection coverage. To evaluate\neffectiveness and scalability, use two datasets: a labeled dataset with 99\nscenarios and ground truth, and a novel dataset of 1,087 diverse merge\nscenarios that we have built. Experimental results show that RefFilter reduces\nfalse positives by nearly 32% on the labeled dataset. While this reduction\ncomes with a non significant increase in false negatives, the overall gain in\nprecision significantly outweighs the minor trade-off in recall. These findings\ndemonstrate that refactoring-aware interference detection is a practical and\neffective strategy for improving merge support in modern development workflows.", "AI": {"tldr": "RefFilter filters behavior-preserving refactorings to cut false positives by 32% in semantic interference detection, enhancing merge precision with minor recall trade-offs.", "motivation": "Current lightweight static analysis methods for detecting semantic interference suffer from high false positive rates due to failure to distinguish behavior-preserving refactorings from behavior-impacting changes.", "method": "RefFilter integrates automated refactoring detection into existing static analysis techniques to filter out non-behavioral changes, thereby enhancing precision while maintaining detection coverage.", "result": "RefFilter reduces false positives by 32% on a labeled dataset while introducing a negligible increase in false negatives, demonstrating significant precision gains with two evaluation datasets (99-labeled scenarios and 1,087 merge scenarios).", "conclusion": "RefFilter effectively addresses the issue of false positives in semantic interference detection by identifying and excluding behavior-preserving refactorings, making it a practical solution for improving merge support in collaborative software development."}}
{"id": "2510.01354", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01354", "abs": "https://arxiv.org/abs/2510.01354", "authors": ["Yinuo Liu", "Ruohan Xu", "Xilong Wang", "Yuqi Jia", "Neil Zhenqiang Gong"], "title": "WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents", "comment": null, "summary": "Multiple prompt injection attacks have been proposed against web agents. At\nthe same time, various methods have been developed to detect general prompt\ninjection attacks, but none have been systematically evaluated for web agents.\nIn this work, we bridge this gap by presenting the first comprehensive\nbenchmark study on detecting prompt injection attacks targeting web agents. We\nbegin by introducing a fine-grained categorization of such attacks based on the\nthreat model. We then construct datasets containing both malicious and benign\nsamples: malicious text segments generated by different attacks, benign text\nsegments from four categories, malicious images produced by attacks, and benign\nimages from two categories. Next, we systematize both text-based and\nimage-based detection methods. Finally, we evaluate their performance across\nmultiple scenarios. Our key findings show that while some detectors can\nidentify attacks that rely on explicit textual instructions or visible image\nperturbations with moderate to high accuracy, they largely fail against attacks\nthat omit explicit instructions or employ imperceptible perturbations. Our\ndatasets and code are released at:\nhttps://github.com/Norrrrrrr-lyn/WAInjectBench.", "AI": {"tldr": "This paper introduces the first comprehensive benchmark for detecting prompt injection attacks in web agents. It categorizes attacks, builds datasets with malicious/benign text/images, evaluates detection methods, and finds that current detectors struggle against attacks lacking explicit instructions or using imperceptible perturbations. ", "motivation": "Prior detection methods for prompt injection attacks have not been systematically evaluated for web agents, creating a need for benchmarking to understand their effectiveness against this specific threat. ", "method": "The study 1) introduces a fine-grained attack categorization based on threat models, 2) constructs datasets of malicious/benign text (4/2 categories) and images (attack-generated vs. normal), and 3) evaluates text/image-based detection methods across multiple scenarios. ", "result": "Detectors achieve moderate to high accuracy against attacks with explicit textual instructions or visible image perturbations, but largely fail against attacks that omit explicit instructions or use imperceptible perturbations. ", "conclusion": "Current detection methods are inadequate against advanced prompt injection attacks employing stealthy strategies. The paper highlights the urgent need for improved detection techniques and provides open-source datasets/code for further research. "}}
{"id": "2510.01994", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01994", "abs": "https://arxiv.org/abs/2510.01994", "authors": ["Chen Yang", "Lin Yang", "Ziqi Wang", "Dong Wang", "Jianyi Zhou", "Junjie Chen"], "title": "Clarifying Semantics of In-Context Examples for Unit Test Generation", "comment": "accepted in the research track of ASE 2025", "summary": "Recent advances in large language models (LLMs) have enabled promising\nperformance in unit test generation through in-context learning (ICL). However,\nthe quality of in-context examples significantly influences the effectiveness\nof generated tests-poorly structured or semantically unclear test examples\noften lead to suboptimal outputs. In this paper, we propose CLAST, a novel\ntechnique that systematically refines unit tests to improve their semantic\nclarity, thereby enhancing their utility as in-context examples. The approach\ndecomposes complex tests into logically clearer ones and improves semantic\nclarity through a combination of program analysis and LLM-based rewriting. We\nevaluated CLAST on four open-source and three industrial projects. The results\ndemonstrate that CLAST largely outperforms UTgen, the state-of-the-art\nrefinement technique, in both preserving test effectiveness and enhancing\nsemantic clarity. Specifically, CLAST fully retains the original effectiveness\nof unit tests, while UTgen reduces compilation success rate (CSR), pass rate\n(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,\n35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user\nstudy preferred the semantic clarity of CLAST-refined tests. Notably,\nincorporating CLAST-refined tests as examples effectively improves ICL-based\nunit test generation approaches such as RAGGen and TELPA, resulting in an\naverage increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for\ngenerated tests, compared to incorporating UTgen-refined tests. The insights\nfrom the follow-up user study not only reinforce CLAST's potential impact in\nsoftware testing practice but also illuminate avenues for future research.", "AI": {"tldr": "CLAST systematically refines unit tests for better semantic clarity using program analysis and LLMs, outperforming state-of-the-art methods and significantly improving downstream test generation effectiveness.", "motivation": "Current ICL-based unit test generation relies heavily on example quality, but suboptimal test examples significantly degrade performance. Existing refinement techniques like UTgen reduce critical test metrics, motivating the need for better semantic clarity.", "method": "CLAST employs program analysis and LLM-based rewriting to decompose complex tests into clearer logical components, improving semantic clarity without sacrificing original test effectiveness.", "result": "CLAST improves test metrics by 25.97-45.99%, retains original effectiveness completely, and achieves 85.33% user preference over UTgen. It also enhances ICL-based generators by similar margins compared to UTgen.", "conclusion": "CLAST enhances semantic clarity and test effectiveness, outperforming existing methods and improving IC-based test generation, while opening new research avenues in software testing."}}
{"id": "2510.01359", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01359", "abs": "https://arxiv.org/abs/2510.01359", "authors": ["Shoumik Saha", "Jifan Chen", "Sam Mayers", "Sanjay Krishna Gouda", "Zijian Wang", "Varun Kumar"], "title": "Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks", "comment": "28 pages, 21 figures, 9 tables", "summary": "Code-capable large language model (LLM) agents are increasingly embedded into\nsoftware engineering workflows where they can read, write, and execute code,\nraising the stakes of safety-bypass (\"jailbreak\") attacks beyond text-only\nsettings. Prior evaluations emphasize refusal or harmful-text detection,\nleaving open whether agents actually compile and run malicious programs. We\npresent JAWS-BENCH (Jailbreaks Across WorkSpaces), a benchmark spanning three\nescalating workspace regimes that mirror attacker capability: empty (JAWS-0),\nsingle-file (JAWS-1), and multi-file (JAWS-M). We pair this with a\nhierarchical, executable-aware Judge Framework that tests (i) compliance, (ii)\nattack success, (iii) syntactic correctness, and (iv) runtime executability,\nmoving beyond refusal to measure deployable harm. Using seven LLMs from five\nfamilies as backends, we find that under prompt-only conditions in JAWS-0, code\nagents accept 61% of attacks on average; 58% are harmful, 52% parse, and 27%\nrun end-to-end. Moving to single-file regime in JAWS-1 drives compliance to ~\n100% for capable models and yields a mean ASR (Attack Success Rate) ~ 71%; the\nmulti-file regime (JAWS-M) raises mean ASR to ~ 75%, with 32% instantly\ndeployable attack code. Across models, wrapping an LLM in an agent\nsubstantially increases vulnerability -- ASR raises by 1.6x -- because initial\nrefusals are frequently overturned during later planning/tool-use steps.\nCategory-level analyses identify which attack classes are most vulnerable and\nmost readily deployable, while others exhibit large execution gaps. These\nfindings motivate execution-aware defenses, code-contextual safety filters, and\nmechanisms that preserve refusal decisions throughout the agent's multi-step\nreasoning and tool use.", "AI": {"tldr": "The paper introduces JAWS-BENCH, a benchmark evaluating code-capable LLM agents against jailbreak attacks in three workspace complexities (empty, single-file, multi-file), finding attack success rates increase when agents execute code through iterative reasoning/planning.", "motivation": "Prior evaluations of LLM safety focused on text-based outputs, but code-execute agents pose deployment risks where malicious code could run. This study measures real-world exploitability through execution-aware metrics.", "method": "Developed JAWS-BENCH with three workspace tiers (0,1,M), a four-criteria Judge Framework (compliance, success, syntax, execution), tested 7 LLMs across regimes using prompts and adversarial inputs.", "result": "Attack acceptance rates rose from 61 (JAWS0, 27 end-to-end success) to 75 (JAWS-M, 32 deployable). Agent planning steps increased ASR 1.6\u00d7 over initial refusals. Category-specific vulnerabilities were identified.", "conclusion": "Code agents show significant vulnerability escalation when execution capability is allowed. Mitigations require execution-aware defenses and preventing reversal of initial refusals during planning phases."}}
{"id": "2510.02002", "categories": ["cs.SE", "D.2.1; D.2.2; D.2.3; D.3.4; G.1.6"], "pdf": "https://arxiv.org/pdf/2510.02002", "abs": "https://arxiv.org/abs/2510.02002", "authors": ["Maximilian Kratz", "Steffen Zschaler", "Jens Kosiol", "Gabriele Taentzer"], "title": "Automatic Generation of Combinatorial Reoptimisation Problem Specifications: A Vision", "comment": null, "summary": "Once an optimisation problem has been solved, the solution may need\nadaptation when contextual factors change. This challenge, also known as\nreoptimisation, has been addressed in various problem domains, such as railway\ncrew rescheduling, nurse rerostering, or aircraft recovery. This requires a\nmodified problem to be solved again to ensure that the adapted solution is\noptimal in the new context. However, the new optimisation problem differs\nnotably from the original problem: (i) we want to make only minimal changes to\nthe original solution to minimise the impact; (ii) we may be unable to change\nsome parts of the original solution (e.g., because they refer to past\nallocations); and (iii) we need to derive a change script from the original\nsolution to the new solution. In this paper, we argue that Model-Driven\nEngineering (MDE) - in particular, the use of declarative modelling languages\nand model transformations for the high-level specification of optimisation\nproblems - offers new opportunities for the systematic derivation of\nreoptimisation problems from the original optimisation problem specification.\nWe focus on combinatorial reoptimisation problems and provide an initial\ncategorisation of changing problems and strategies for deriving the\ncorresponding reoptimisation specifications. We introduce an initial\nproof-of-concept implementation based on the GIPS (Graph-Based (Mixed) Integer\nLinear Programming Problem Specification) tool and apply it to an example\nresource-allocation problem: the allocation of teaching assistants to teaching\nsessions.", "AI": {"tldr": "This paper advocates for using Model-Driven Engineering (MDE) to systematize reoptimization problems, offering a GIPS-based proof of concept applied to teaching assistant allocation, enabling minimal-change adaptations in combinatorial optimization contexts.", "motivation": "Reoptimization is critical when contextual factors change, requiring solutions to adapt while minimizing modifications, respecting inalterable parts of the original solution, and generating actionable change scripts. Traditional methods lack systematic strategies for these challenges in combinatorial problems.", "method": "The authors propose leveraging declarative modelling languages and model transformations within MDE to formalize reoptimization strategies. They implemented this approach in the GIPS tool, applying it to a teaching assistant allocation problem to demonstrate feasibility.", "result": "The paper presents an initial categorization of reoptimization problem types and strategies for specification derivation. A GIPS-based implementation successfully applied their approach to a real-world resource-allocation case study, validating the method's potential.", "conclusion": "The paper concludes that Model-Driven Engineering (MDE) provides a systematic framework for deriving reoptimization problems from original optimisation specifications, enabling efficient adaptation with minimal changes and respect to constraints."}}
{"id": "2510.01393", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01393", "abs": "https://arxiv.org/abs/2510.01393", "authors": ["Davide Rusconi", "Osama Yousef", "Mirco Picca", "Flavio Toffalini", "Andrea Lanzi"], "title": "E-FuzzEdge: Optimizing Embedded Device Security with Scalable In-Place Fuzzing", "comment": null, "summary": "In this paper we show E-FuzzEdge, a novel fuzzing architecture targeted\ntowards improving the throughput of fuzzing campaigns in contexts where\nscalability is unavailable. E-FuzzEdge addresses the inefficiencies of\nhardware-in-the-loop fuzzing for microcontrollers by optimizing execution\nspeed. We evaluated our system against state-of-the-art benchmarks,\ndemonstrating significant performance improvements. A key advantage of\nE-FuzzEdgearchitecture is its compatibility with other embedded fuzzing\ntechniques that perform on device testing instead of firmware emulation. This\nmeans that the broader embedded fuzzing community can integrate E-FuzzEdge into\ntheir workflows to enhance overall testing efficiency.", "AI": {"tldr": "E-FuzzEdge is a novel fuzzing architecture that improves throughput for microcontroller testing by optimizing hardware-in-the-loop efficiency, compatible with existing embedded techniques.", "motivation": "Hardware-in-the-loop fuzzing for microcontrollers faces scalability limitations and execution inefficiencies, hindering testing throughput.", "method": "E-FuzzEdge optimizes execution speed in hardware-in-the-loop contexts via a novel architecture, avoiding firmware emulation to maintain device compatibility.", "result": "Significant performance improvements demonstrated against state-of-the-art benchmarks in fuzzing throughput and execution speed.", "conclusion": "E-FuzzEdge's compatibility with device-based testing enables seamless integration into existing workflows, enhancing overall embedded fuzzing efficiency."}}
{"id": "2510.02007", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02007", "abs": "https://arxiv.org/abs/2510.02007", "authors": ["Justus Bogner", "Roberto Verdecchia"], "title": "ACM SIGSOFT SEN Empirical Software Engineering: Introducing Our New Regular Column", "comment": "Published in ACM SIGSOFT Software Engineering Notes (SIGSOFT-SEN).\n  Volume 50, Issue 4, 2025", "summary": "From its early foundations in the 1970s, empirical software engineering (ESE)\nhas evolved into a mature research discipline that embraces a plethora of\ndifferent topics, methodologies, and industrial practices. Despite its\nremarkable progress, the ESE research field still needs to keep evolving, as\nnew impediments, shortcoming, and technologies emerge. Research\nreproducibility, limited external validity, subjectivity of reviews, and\nporting research results to industrial practices are just some examples of the\ndrivers for improvements to ESE research. Additionally, several facets of ESE\nresearch are not documented very explicitly, which makes it difficult for\nnewcomers to pick them up. With this new regular ACM SIGSOFT SEN column\n(SEN-ESE), we introduce a venue for discussing meta-aspects of ESE research,\nranging from general topics such as the nature and best practices for\nreplication packages, to more nuanced themes such as statistical methods,\ninterview transcription tools, and publishing interdisciplinary research. Our\naim for the column is to be a place where we can regularly spark conversations\non ESE topics that might not often be touched upon or are left implicit.\nContributions to this column will be grounded in expert interviews, focus\ngroups, surveys, and position pieces, with the goal of encouraging reflection\nand improvement in how we conduct, communicate, teach, and ultimately improve\nESE research. Finally, we invite feedback from the ESE community on\nchallenging, controversial, or underexplored topics, as well as suggestions for\nvoices you would like to hear from. While we cannot promise to act on every\nidea, we aim to shape this column around the community interests and are\ngrateful for all contributions.", "AI": {"tldr": "This paper announces the ACM SIGSOFT SEN-ESE column to improve empirical software engineering research through structured meta-discussion on topics like reproducibility and interdisciplinary publishing.", "motivation": "Despite ESE's maturation, challenges like research reproducibility, limited external validity, and underdocumented practices hinder progress. The column addresses these gaps by creating a dedicated platform for meta-discussion.", "method": "The column will feature expert interviews, focus groups, surveys, and position pieces to explore meta-aspects of ESE research like replication packages, statistical methods, and interdisciplinary publishing.", "result": "A new regular publication venue (SEN-ESE) is introduced to catalyze conversations on improving ESE research practices through evidence-based contributions from the academic and industrial communities.", "conclusion": "The establishment of the SEN-ESE column aims to foster community-driven discussions and improvements in empirical software engineering (ESE) research by addressing gaps in methodology, documentation, and practice transfer."}}
{"id": "2510.01445", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01445", "abs": "https://arxiv.org/abs/2510.01445", "authors": ["Andr\u00e9s F. Betancur-L\u00f3pez"], "title": "Securing IoT Devices in Smart Cities: A Review of Proposed Solutions", "comment": "14 pages, 7 figures", "summary": "Privacy and security in Smart Cities remain at constant risk due to the\nvulnerabilities introduced by Internet of Things (IoT) devices. The limited\ncomputational resources of these devices make them especially susceptible to\nattacks, while their widespread adoption increases the potential impact of\nsecurity breaches. This article presents a review of security proposals aimed\nat protecting IoT devices in Smart City environments. The review was conducted\nby analyzing recent literature on device-level security, with particular\nemphasis on lightweight cryptography, physically unclonable functions (PUFs),\nand blockchain-based solutions. Findings highlight both the strengths and\nlimitations of current approaches, as well as the need for more practical,\nscalable, and resource-efficient mechanisms to ensure user privacy and data\nprotection in IoT ecosystems.", "AI": {"tldr": "The article reviews recent security proposals for IoT devices in Smart Cities, focusing on lightweight cryptography, PUFs, and blockchain. It identifies the need for more practical and scalable solutions.", "motivation": "The proliferation of IoT devices in Smart Cities introduces significant privacy and security risks due to limited computational resources and widespread adoption, necessitating robust security mechanisms.", "method": "The study conducts a literature review of device-level security solutions, analyzing approaches based on lightweight cryptography, physically unclonable functions (PUFs), and blockchain technology.", "result": "The findings highlight the strengths and limitations of current security proposals, emphasizing the need for more efficient, scalable, and practical implementations to protect IoT ecosystems in Smart Cities.", "conclusion": "Despite recent advancements, there are gaps in practical and resource-efficient security solutions for IoT in Smart Cities, demanding further research to enhance user privacy and data protection."}}
{"id": "2510.02165", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02165", "abs": "https://arxiv.org/abs/2510.02165", "authors": ["Peter Wauyo", "Dalia Bwiza", "Alain Murara", "Edwin Mugume", "Eric Umuhoza"], "title": "Towards fairer public transit: Real-time tensor-based multimodal fare evasion and fraud detection", "comment": "10 pages", "summary": "This research introduces a multimodal system designed to detect fraud and\nfare evasion in public transportation by analyzing closed circuit television\n(CCTV) and audio data. The proposed solution uses the Vision Transformer for\nVideo (ViViT) model for video feature extraction and the Audio Spectrogram\nTransformer (AST) for audio analysis. The system implements a Tensor Fusion\nNetwork (TFN) architecture that explicitly models unimodal and bimodal\ninteractions through a 2-fold Cartesian product. This advanced fusion technique\ncaptures complex cross-modal dynamics between visual behaviors (e.g.,\ntailgating,unauthorized access) and audio cues (e.g., fare transaction sounds).\nThe system was trained and tested on a custom dataset, achieving an accuracy of\n89.5%, precision of 87.2%, and recall of 84.0% in detecting fraudulent\nactivities, significantly outperforming early fusion baselines and exceeding\nthe 75% recall rates typically reported in state-of-the-art transportation\nfraud detection systems. Our ablation studies demonstrate that the tensor\nfusion approach provides a 7.0% improvement in the F1 score and an 8.8% boost\nin recall compared to traditional concatenation methods. The solution supports\nreal-time detection, enabling public transport operators to reduce revenue\nloss, improve passenger safety, and ensure operational compliance.", "AI": {"tldr": "A multimodal fraud detection system using ViViT, AST, and Tensor Fusion Networks achieves 84.0% recall, surpassing prior methods by 8.8%, enabling real-time monitoring in public transport.", "motivation": "The study addresses the critical need for improved fraud and fare evasion detection in public transportation, aiming to mitigate revenue loss, enhance passenger safety, and ensure compliance through advanced multimodal analysis of CCTV and audio data.", "method": "The system employs a Tensor Fusion Network (TFN) to integrate video features extracted via Vision Transformer for Video (ViViT) and audio features from the Audio Spectrogram Transformer (AST), utilizing a 2-fold Cartesian product to model cross-modal interactions between visual behaviors and audio cues.", "result": "The system achieved 89.5% accuracy, 87.2% precision, and 84.0% recall, outperforming early fusion baselines and state-of-the-art models by 8.8% in recall, with ablation studies confirming the 7.0% F1 score gain and 8.8% recall boost from tensor fusion.", "conclusion": "The research concludes that the proposed multimodal system using ViViT, AST, and Tensor Fusion Networks significantly enhances fraud detection accuracy, surpassing existing methods with an 84.0% recall and 7.8% improvement in F1 score over traditional approaches, while enabling real-time public transport monitoring."}}
{"id": "2510.01552", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01552", "abs": "https://arxiv.org/abs/2510.01552", "authors": ["Luoxi Tang", "Yuqiao Meng", "Ankita Patra", "Weicheng Ma", "Muchao Ye", "Zhaohan Xi"], "title": "POLAR: Automating Cyber Threat Prioritization through LLM-Powered Assessment", "comment": "25 pages", "summary": "Large Language Models (LLMs) are intensively used to assist security analysts\nin counteracting the rapid exploitation of cyber threats, wherein LLMs offer\ncyber threat intelligence (CTI) to support vulnerability assessment and\nincident response. While recent work has shown that LLMs can support a wide\nrange of CTI tasks such as threat analysis, vulnerability detection, and\nintrusion defense, significant performance gaps persist in practical\ndeployments. In this paper, we investigate the intrinsic vulnerabilities of\nLLMs in CTI, focusing on challenges that arise from the nature of the threat\nlandscape itself rather than the model architecture. Using large-scale\nevaluations across multiple CTI benchmarks and real-world threat reports, we\nintroduce a novel categorization methodology that integrates stratification,\nautoregressive refinement, and human-in-the-loop supervision to reliably\nanalyze failure instances. Through extensive experiments and human inspections,\nwe reveal three fundamental vulnerabilities: spurious correlations,\ncontradictory knowledge, and constrained generalization, that limit LLMs in\neffectively supporting CTI. Subsequently, we provide actionable insights for\ndesigning more robust LLM-powered CTI systems to facilitate future research.", "AI": {"tldr": "This paper identifies critical vulnerabilities in LLM-driven CTI systems and proposes a framework to improve their reliability.", "motivation": "Recent LLMs struggle with practical CTI tasks despite promising capabilities. The paper aims to identify intrinsic vulnerabilities caused by the threat landscape's nature, not model architecture, to improve LLM-powered CTI systems.", "method": "The study introduces a novel categorization methodology integrating stratification, autoregressive refinement, and human-in-the-loop supervision, validated through large-scale evaluations on CTI benchmarks and real-world threat reports.", "result": "Identified three major LLM vulnerabilities: spurious correlations, contradictory knowledge, and constrained generalization, with experimental and human validation. Provided actionable design insights for robust CTI systems.", "conclusion": "The paper concludes that LLMs in CTI face fundamental vulnerabilities like spurious correlations, contradictory knowledge, and constrained generalization, and proposes actionable insights to design more robust CTI systems."}}
{"id": "2510.02166", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02166", "abs": "https://arxiv.org/abs/2510.02166", "authors": ["Fatou Ndiaye Mbodji", "El-hacen Diallo", "Jordan Samhi", "Kui Liu", "Jacques Klein", "Tegawend\u00e9 F. Bissyande"], "title": "SIEVE: Towards Verifiable Certification for Code-datasets", "comment": "5", "summary": "Code agents and empirical software engineering rely on public code datasets,\nyet these datasets lack verifiable quality guarantees. Static 'dataset cards'\ninform, but they are neither auditable nor do they offer statistical\nguarantees, making it difficult to attest to dataset quality. Teams build\nisolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We\npresent SIEVE, a community-driven framework. It turns per-property checks into\nConfidence Cards-machine-readable, verifiable certificates with anytime-valid\nstatistical bounds. We outline a research plan to bring SIEVE to maturity,\nreplacing narrative cards with anytime-verifiable certification. This shift is\nexpected to lower quality-assurance costs and increase trust in code-datasets.", "AI": {"tldr": "SIEVE introduces verifiable 'Confidence Cards' for code datasets, offering statistical guarantees to improve quality assurance and trust in empirical software engineering.", "motivation": "Public code datasets lack auditable quality guarantees; existing 'dataset cards\u2019 are non-verified and non-statistical, leading to redundant cleaning pipelines and high costs.", "method": "A community-driven framework called SIEVE converts per-property checks into machine-readable, statistically bounded 'Confidence Cards' with anytime-valid certifications.", "result": "Outlines a research plan to transition from narrative dataset cards to SIEVE's verifiable certification system, but no empirical results are presented in the abstract.", "conclusion": "SIEVE aims to reduce quality assurance costs and increase trust in code datasets by providing verifiable statistical guarantees through community-driven certification."}}
{"id": "2510.01645", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01645", "abs": "https://arxiv.org/abs/2510.01645", "authors": ["Niloofar Mireshghallah", "Tianshi Li"], "title": "Position: Privacy Is Not Just Memorization!", "comment": "27 pages, 6 figures, 2 tables", "summary": "The discourse on privacy risks in Large Language Models (LLMs) has\ndisproportionately focused on verbatim memorization of training data, while a\nconstellation of more immediate and scalable privacy threats remain\nunderexplored. This position paper argues that the privacy landscape of LLM\nsystems extends far beyond training data extraction, encompassing risks from\ndata collection practices, inference-time context leakage, autonomous agent\ncapabilities, and the democratization of surveillance through deep inference\nattacks. We present a comprehensive taxonomy of privacy risks across the LLM\nlifecycle -- from data collection through deployment -- and demonstrate through\ncase studies how current privacy frameworks fail to address these multifaceted\nthreats. Through a longitudinal analysis of 1,322 AI/ML privacy papers\npublished at leading conferences over the past decade (2016--2025), we reveal\nthat while memorization receives outsized attention in technical research, the\nmost pressing privacy harms lie elsewhere, where current technical approaches\noffer little traction and viable paths forward remain unclear. We call for a\nfundamental shift in how the research community approaches LLM privacy, moving\nbeyond the narrow focus of current technical solutions and embracing\ninterdisciplinary approaches that address the sociotechnical nature of these\nemerging threats.", "AI": {"tldr": "This paper highlights underexplored privacy risks in LLM systems beyond memorization and calls for interdisciplinary research to address sociotechnical threats across the LLM lifecycle.", "motivation": "Existing research disproportionately emphasizes verbatim memorization of training data, neglecting more immediate and scalable privacy threats in LLM systems. The paper seeks to broaden the discourse to include risks such as data collection practices, inference-time context leakage, autonomous agent capabilities, and deep inference attacks.", "method": "The paper employs a longitudinal analysis of 1,322 AI/ML privacy papers (2016\u20132025) to identify research trends, while proposing a comprehensive taxonomy of privacy risks across the LLM lifecycle. Case studies demonstrate how current frameworks fail to address multifaceted threats.", "result": "Analysis reveals that memorization receives outsized attention in technical research, while pressing privacy harms related to data collection, deployment, and emerging threats are underexplored. Current technical approaches lack traction in addressing these issues.", "conclusion": "The authors argue for a fundamental shift in the research community's approach to LLM privacy, advocating for interdisciplinary methods that address the sociotechnical nature of emerging threats rather than focusing narrowly on technical solutions."}}
{"id": "2510.02169", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.02169", "abs": "https://arxiv.org/abs/2510.02169", "authors": ["Vadim Safronov", "Anthony McCaigue", "Nicholas Allott", "Andrew Martin"], "title": "TAIBOM: Bringing Trustworthiness to AI-Enabled Systems", "comment": "This paper has been accepted at the First International Workshop on\n  Security and Privacy-Preserving AI/ML (SPAIML 2025), co-located with the 28th\n  European Conference on Artificial Intelligence (ECAI 2025)", "summary": "The growing integration of open-source software and AI-driven technologies\nhas introduced new layers of complexity into the software supply chain,\nchallenging existing methods for dependency management and system assurance.\nWhile Software Bills of Materials (SBOMs) have become critical for enhancing\ntransparency and traceability, current frameworks fall short in capturing the\nunique characteristics of AI systems -- namely, their dynamic, data-driven\nnature and the loosely coupled dependencies across datasets, models, and\nsoftware components. These challenges are compounded by fragmented governance\nstructures and the lack of robust tools for ensuring integrity, trust, and\ncompliance in AI-enabled environments.\n  In this paper, we introduce Trusted AI Bill of Materials (TAIBOM) -- a novel\nframework extending SBOM principles to the AI domain. TAIBOM provides (i) a\nstructured dependency model tailored for AI components, (ii) mechanisms for\npropagating integrity statements across heterogeneous AI pipelines, and (iii) a\ntrust attestation process for verifying component provenance. We demonstrate\nhow TAIBOM supports assurance, security, and compliance across AI workflows,\nhighlighting its advantages over existing standards such as SPDX and CycloneDX.\nThis work lays the foundation for trustworthy and verifiable AI systems through\nstructured software transparency.", "AI": {"tldr": "This paper introduces TAIBOM, a framework extending SBOM principles to address AI system transparency gaps, addressing dynamic dependencies, integrity, and trust in AI workflows.", "motivation": "Current SBOMs lack mechanisms to capture AI systems' dynamic, data-driven nature and complex dependencies, leading to governance and compliance challenges in AI environments.", "method": "TAIBOM introduces (i) a structured AI dependency model, (ii) integrity propagation across heterogeneous pipelines, and (iii} trust attestation for component provenance, with comparisons to SPDX/CycloneDX.", "result": "Demonstrates TAIBOM's effectiveness in providing assurance, security, and compliance for AI workflows, outperforming existing standards through tailored AI-component transparency.", "conclusion": "TAIBOM establishes foundational software transparency for trustworthy AI systems by addressing supply chain gaps through structured dependency modeling and attestation processes."}}
{"id": "2510.01676", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01676", "abs": "https://arxiv.org/abs/2510.01676", "authors": ["Milad Nasr", "Yanick Fratantonio", "Luca Invernizzi", "Ange Albertini", "Loua Farah", "Alex Petit-Bianco", "Andreas Terzis", "Kurt Thomas", "Elie Bursztein", "Nicholas Carlini"], "title": "Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks", "comment": null, "summary": "As deep learning models become widely deployed as components within larger\nproduction systems, their individual shortcomings can create system-level\nvulnerabilities with real-world impact. This paper studies how adversarial\nattacks targeting an ML component can degrade or bypass an entire\nproduction-grade malware detection system, performing a case study analysis of\nGmail's pipeline where file-type identification relies on a ML model.\n  The malware detection pipeline in use by Gmail contains a machine learning\nmodel that routes each potential malware sample to a specialized malware\nclassifier to improve accuracy and performance. This model, called Magika, has\nbeen open sourced. By designing adversarial examples that fool Magika, we can\ncause the production malware service to incorrectly route malware to an\nunsuitable malware detector thereby increasing our chance of evading detection.\nSpecifically, by changing just 13 bytes of a malware sample, we can\nsuccessfully evade Magika in 90% of cases and thereby allow us to send malware\nfiles over Gmail. We then turn our attention to defenses, and develop an\napproach to mitigate the severity of these types of attacks. For our defended\nproduction model, a highly resourced adversary requires 50 bytes to achieve\njust a 20% attack success rate. We implement this defense, and, thanks to a\ncollaboration with Google engineers, it has already been deployed in production\nfor the Gmail classifier.", "AI": {"tldr": "Researchers demonstrated adversarial attacks against Gmail's Magika ML model, enabling malware evasion with minimal file changes. They developed a defense reducing attack success, now deployed in Gmail's production systems.", "motivation": "The paper addresses the real-world risk of adversarial attacks exploiting machine learning components in larger systems. Focusing on Gmail's malware detection, it highlights how such vulnerabilities could enable undetected malware dissemination via critical services.", "method": "The authors designed adversarial examples by altering 13 bytes of malware samples to fool the Magika model. They then developed a defense strategy and evaluated its effectiveness, showing that even a well-resourced adversary requires more perturbations (50 bytes) to achieve lower success rates (20%). The defense was collaboratively implemented and deployed in production.", "result": "The attacks achieved 90% evasion of Magika by altering 13 bytes, allowing malware evasion through Gmail. Post-defense, adversaries needed 50-byte alterations for 20% success. The defense is now actively in use in Gmail's production pipeline.", "conclusion": "This paper concludes that adversarial attacks on ML components in production systems can lead to significant vulnerabilities. However, with the implementation of effective defense mechanisms, the security of such systems can be substantially improved, as demonstrated by the successful deployment of their defense on Gmail's malware detection system."}}
{"id": "2510.02185", "categories": ["cs.SE", "cs.CR", "cs.MA", "D.2.4; F.3.1"], "pdf": "https://arxiv.org/pdf/2510.02185", "abs": "https://arxiv.org/abs/2510.02185", "authors": ["Paschal C. Amusuo", "Dongge Liu", "Ricardo Andres Calvo Mendez", "Jonathan Metzman", "Oliver Chang", "James C. Davis"], "title": "FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen Using Agentic AI", "comment": "12 pages, 2 figures", "summary": "Fuzz testing has become a cornerstone technique for identifying software bugs\nand security vulnerabilities, with broad adoption in both industry and\nopen-source communities. Directly fuzzing a function requires fuzz drivers,\nwhich translate random fuzzer inputs into valid arguments for the target\nfunction. Given the cost and expertise required to manually develop fuzz\ndrivers, methods exist that leverage program analysis and Large Language Models\nto automatically generate these drivers. However, the generated fuzz drivers\nfrequently lead to false positive crashes, especially in functions highly\nstructured input and complex state requirements. This problem is especially\ncrucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as\nreporting false positive crashes to maintainers impede trust in both the system\nand the team.\n  This paper presents two AI-driven strategies to reduce false positives in\nOSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First,\nconstraint-based fuzz driver generation proactively enforces constraints on a\nfunction's inputs and state to guide driver creation. Second, context-based\ncrash validation reactively analyzes function callers to determine whether\nreported crashes are feasible from program entry points. Using 1,500 benchmark\nfunctions from OSS-Fuzz, we show that these strategies reduce spurious crashes\nby up to 8%, cut reported crashes by more than half, and demonstrate that\nfrontier LLMs can serve as reliable program analysis agents. Our results\nhighlight the promise and challenges of integrating AI into large-scale fuzzing\npipelines.", "AI": {"tldr": "This paper proposes two AI-enhanced approaches to reduce false positives in automated fuzzing: constraint-aware driver generation and context-aware crash validation, proving AI can improve large-scale fuzzing reliability.", "motivation": "Automatically generated fuzz drivers often produce false positives that undermine trust in systems like OSS-Fuzz-Gen. This problem becomes critical at scale as false crashes reported to maintainers disrupt collaboration and confidence in automated fuzzing processes.", "method": "The paper introduces two AI techniques: 1) Constraint-based fuzz driver generation, which enforces function input/state constraints during driver creation, and 2) Context-based crash validation, which analyzes function callers to verify crash feasibility from program entry points.", "result": "On 1,500 OSS-Fuzz functions, the approaches reduced spurious crashes by up to 8%, cut reported crashes by 50%, and demonstrated that frontier LLMs can function as reliable program analysis agents for fuzzing tasks.", "conclusion": "The paper concludes that integrating AI-driven strategies, such as constraint-based fuzz driver generation and context-based crash validation, into large-scale fuzzing pipelines like OSS-Fuzz-Gen can significantly reduce false positives and improve trust in automated fuzzing. However, challenges remain in ensuring AI reliability for program analysis at scale."}}
{"id": "2510.01699", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01699", "abs": "https://arxiv.org/abs/2510.01699", "authors": ["Yue Li", "Linying Xue", "Dongdong Lin", "Qiushi Li", "Hui Tian", "Hongxia Wang"], "title": "Towards Imperceptible Adversarial Defense: A Gradient-Driven Shield against Facial Manipulations", "comment": null, "summary": "With the flourishing prosperity of generative models, manipulated facial\nimages have become increasingly accessible, raising concerns regarding privacy\ninfringement and societal trust. In response, proactive defense strategies\nembed adversarial perturbations into facial images to counter deepfake\nmanipulation. However, existing methods often face a tradeoff between\nimperceptibility and defense effectiveness-strong perturbations may disrupt\nforgeries but degrade visual fidelity. Recent studies have attempted to address\nthis issue by introducing additional visual loss constraints, yet often\noverlook the underlying gradient conflicts among losses, ultimately weakening\ndefense performance. To bridge the gap, we propose a gradient-projection-based\nadversarial proactive defense (GRASP) method that effectively counters facial\ndeepfakes while minimizing perceptual degradation. GRASP is the first approach\nto successfully integrate both structural similarity loss and low-frequency\nloss to enhance perturbation imperceptibility. By analyzing gradient conflicts\nbetween defense effectiveness loss and visual quality losses, GRASP pioneers\nthe design of the gradient-projection mechanism to mitigate these conflicts,\nenabling balanced optimization that preserves image fidelity without\nsacrificing defensive performance. Extensive experiments validate the efficacy\nof GRASP, achieving a PSNR exceeding 40 dB, SSIM of 0.99, and a 100% defense\nsuccess rate against facial attribute manipulations, significantly\noutperforming existing approaches in visual quality.", "AI": {"tldr": "They made a new method called GRASP to protect against fake faces without messing up the picture quality.", "motivation": "There's a problem because fake face images are getting more common, and current defenses make the original images look bad. They need a better way to stop deepfakes while keeping the real images looking good.", "method": "GRASP uses a gradient-projection technique to combine different losses (like how good the image looks and how well the defense works). This helps avoid the design conflicts that happen when trying to optimize all at once.", "result": "GRASP works really well - over 40 dB in PSNR, an SSIM close to perfect at 0.99, and it completely blocks manipulations. It's way better than other methods in making clean, fake-resistant images.", "conclusion": "GRASP is a solid solution for deepfake defense that keeps image quality top-notch. It shows we can have strong security and good visuals at the same time with the right approach."}}
{"id": "2510.01720", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01720", "abs": "https://arxiv.org/abs/2510.01720", "authors": ["Palash Sarkar"], "title": "Constructions of Efficiently Implementable Boolean Functions with Provable Nonlinearity/Resiliency/Algebraic Immunity Trade-Offs", "comment": null, "summary": "We describe several families of efficiently implementable Boolean functions\nachieving provable trade-offs between resiliency, nonlinearity, and algebraic\nimmunity. In concrete terms, the following result holds for each of the\nfunction families that we propose. Given integers $m_0\\geq 0$, $x_0\\geq 1$, and\n$a_0\\geq 1$, it is possible to construct an $n$-variable function which has\nresiliency at least $m_0$, linear bias (which is an equivalent method of\nexpressing nonlinearity) at most $2^{-x_0}$ and algebraic immunity at least\n$a_0$; further, $n$ is linear in $m_0$, $x_0$ and $a_0$, and the function can\nbe implemented using $O(n)$ gates.", "AI": {"tldr": "The paper presents efficiently implementable Boolean function families with provable trade-offs between resiliency, nonlinearity (linear bias), and algebraic immunity, achieving linear scalability in input size and gate complexity.", "motivation": "Boolean functions with balanced cryptographic properties (resiliency, nonlinearity, algebraic immunity are critical for secure cryptosystems. Prior work lacked efficient constructions with provable trade-offs among these parameters.", "method": "Constructs function families where for given minimal resiliency $m_0$, maximally suppressed linear bias $2^{-x_0}$, and minimal algebraic immunity $a_0$, the functions use $O(n)$ gates with $n$ linearly dependent on $m_0, x_0, a_0.", "result": "Explicit constructions achieving simultaneously specified levels of resiliency (\u2265m\u2080), nonlinearity (bias \u22642^{-x\u2080}), and algebraic immunity (\u2265a\u2080), with implementation complexity $O(n)$ where $n=O(m\u2080 + x\u2080 + a\u2080)$.", "conclusion": "The work establishes a systematic method to realize Boolean functions optimized for multiple cryptographic metrics with linear implementation cost, advancing practical cryptographic design."}}
{"id": "2510.01780", "categories": ["cs.CR", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01780", "abs": "https://arxiv.org/abs/2510.01780", "authors": ["Aueaphum Aueawatthanaphisut"], "title": "Secure Multi-Modal Data Fusion in Federated Digital Health Systems via MCP", "comment": "6 pages, 8 figures, 7 equations, 1 algorithm", "summary": "Secure and interoperable integration of heterogeneous medical data remains a\ngrand challenge in digital health. Current federated learning (FL) frameworks\noffer privacy-preserving model training but lack standardized mechanisms to\norchestrate multi-modal data fusion across distributed and resource-constrained\nenvironments. This study introduces a novel framework that leverages the Model\nContext Protocol (MCP) as an interoperability layer for secure, cross-agent\ncommunication in multi-modal federated healthcare systems. The proposed\narchitecture unifies three pillars: (i) multi-modal feature alignment for\nclinical imaging, electronic medical records, and wearable IoT data; (ii)\nsecure aggregation with differential privacy to protect patient-sensitive\nupdates; and (iii) energy-aware scheduling to mitigate dropouts in mobile\nclients. By employing MCP as a schema-driven interface, the framework enables\nadaptive orchestration of AI agents and toolchains while ensuring compliance\nwith privacy regulations. Experimental evaluation on benchmark datasets and\npilot clinical cohorts demonstrates up to 9.8\\% improvement in diagnostic\naccuracy compared with baseline FL, a 54\\% reduction in client dropout rates,\nand clinically acceptable privacy--utility trade-offs. These results highlight\nMCP-enabled multi-modal fusion as a scalable and trustworthy pathway toward\nequitable, next-generation federated health infrastructures.", "AI": {"tldr": "This paper proposes an MCP-based framework for secure multi-modal federated learning in healthcare, improving diagnostic accuracy (9.8%), reducing client dropouts (54%), and enabling privacy-preserving data fusion across distributed systems.", "motivation": "Current federated learning lacks standardized mechanisms for interoperable, secure multi-modal data fusion in resource-constrained healthcare environments, hindering privacy and scalability goals.", "method": "The framework integrates: 1. Multi-modal feature alignment for clinical imaging/EMR/IoT data; 2. Differential privacy-protected aggregation; 3. Energy-aware client scheduling with MCP as a schema-driven interoperability protocol.", "result": "Experiments show 9.8% higher diagnostic accuracy vs. baseline FL, 54%% lower dropout rates, and maintains clinically acceptable privacy-utility trade-offs across benchmark datasets and pilot cohorts.", "conclusion": "MCP-enabled architectures provide a scalable pathway for secure, interoperable next-generation federated health systems that address multi-modal data integration challenges while complying with privacy regulations."}}
{"id": "2510.01967", "categories": ["cs.CR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01967", "abs": "https://arxiv.org/abs/2510.01967", "authors": ["Aadarsh Anantha Ramakrishnan", "Shubham Agarwal", "Selvanayagam S", "Kunwar Singh"], "title": "ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs", "comment": "Accepted at AI-ML Systems 2025, Bangalore, India,\n  https://www.aimlsystems.org/2025/", "summary": "As image generation models grow increasingly powerful and accessible,\nconcerns around authenticity, ownership, and misuse of synthetic media have\nbecome critical. The ability to generate lifelike images indistinguishable from\nreal ones introduces risks such as misinformation, deepfakes, and intellectual\nproperty violations. Traditional watermarking methods either degrade image\nquality, are easily removed, or require access to confidential model internals\n- making them unsuitable for secure and scalable deployment. We are the first\nto introduce ZK-WAGON, a novel system for watermarking image generation models\nusing the Zero-Knowledge Succinct Non Interactive Argument of Knowledge\n(ZK-SNARKs). Our approach enables verifiable proof of origin without exposing\nmodel weights, generation prompts, or any sensitive internal information. We\npropose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively\nconvert key layers of an image generation model into a circuit, reducing proof\ngeneration time significantly. Generated ZK-SNARK proofs are imperceptibly\nembedded into a generated image via Least Significant Bit (LSB) steganography.\nWe demonstrate this system on both GAN and Diffusion models, providing a\nsecure, model-agnostic pipeline for trustworthy AI image generation.", "AI": {"tldr": "ZK-WAGON introduces a secure, imperceptible watermarking system for image generation models using ZK-SNARKs and LSB steganography, enabling verifiable origin proofs without exposing sensitive information.", "motivation": "Traditional watermarking methods compromise image quality, are easily removable, or require confidential model internals, creating risks for synthetic media misuse like deepfakes and IP violations.", "method": "The framework uses ZK-SNARKs for verifiable proofs, Selective Layer ZK-Circuit Creation (SL-ZKCC) to optimize proof generation via selective layer circuit conversion, and LSB steganography for embedding proofs into images.", "result": "ZK-WAGON successfully demonstrates watermarking on GAN and Diffusion models, achieving imperceptible proofs, reduced proof generation time via SL-ZKCC, and secure model-agnostic deployment.", "conclusion": "This work provides a scalable, privacy-preserving solution for trustworthy AI image generation, addressing authenticity concerns without compromising model secrecy or image quality."}}
{"id": "2510.02158", "categories": ["cs.CR", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.02158", "abs": "https://arxiv.org/abs/2510.02158", "authors": ["Junjie Su", "Weifei Jin", "Yuxin Cao", "Derui Wang", "Kai Ye", "Jie Hao"], "title": "Mirage Fools the Ear, Mute Hides the Truth: Precise Targeted Adversarial Attacks on Polyphonic Sound Event Detection Systems", "comment": null, "summary": "Sound Event Detection (SED) systems are increasingly deployed in\nsafety-critical applications such as industrial monitoring and audio\nsurveillance. However, their robustness against adversarial attacks has not\nbeen well explored. Existing audio adversarial attacks targeting SED systems,\nwhich incorporate both detection and localization capabilities, often lack\neffectiveness due to SED's strong contextual dependencies or lack precision by\nfocusing solely on misclassifying the target region as the target event,\ninadvertently affecting non-target regions. To address these challenges, we\npropose the Mirage and Mute Attack (M2A) framework, which is designed for\ntargeted adversarial attacks on polyphonic SED systems. In our optimization\nprocess, we impose specific constraints on the non-target output, which we\nrefer to as preservation loss, ensuring that our attack does not alter the\nmodel outputs for non-target region, thus achieving precise attacks.\nFurthermore, we introduce a novel evaluation metric Editing Precison (EP) that\nbalances effectiveness and precision, enabling our method to simultaneously\nenhance both. Comprehensive experiments show that M2A achieves 94.56% and\n99.11% EP on two state-of-the-art SED models, demonstrating that the framework\nis sufficiently effective while significantly enhancing attack precision.", "AI": {"tldr": "This paper introduces M2A, a precise adversarial attack framework for polyphonic SED systems, achieving state-of-the-art precision via preservation loss and a novel EP metric, with 94.56%-99.11% effectiveness on leading models.", "motivation": "Existing audio adversarial attacks for SED systems lack robustness due to contextual dependencies or imprecise targeting that inadvertently affects non-target regions, leaving safety-critical applications vulnerable.", "method": "Proposes M2A, a targeted adversarial attack framework with preservation loss constraints on non-target outputs and introduces the Editing Precision (EP) metric to balance attack effectiveness and precision.", "result": "Achieved 94.56% and 99.11% EP on two SED models, demonstrating significant improvements in both attack effectiveness and precision compared to prior methods.", "conclusion": "The M2A framework effectively enhances attack precision on polyphonic SED systems while maintaining non-target region integrity, demonstrating high effectiveness through 94.56% and 99.11% Editing Precision (EP) on two state-of-the-art models."}}
{"id": "2510.02162", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02162", "abs": "https://arxiv.org/abs/2510.02162", "authors": ["Cristian Bassotto", "Ermes Franch", "Marina Kr\u010dek", "Stjepan Picek"], "title": "NoMod: A Non-modular Attack on Module Learning With Errors", "comment": null, "summary": "The advent of quantum computing threatens classical public-key cryptography,\nmotivating NIST's adoption of post-quantum schemes such as those based on the\nModule Learning With Errors (Module-LWE) problem. We present NoMod ML-Attack, a\nhybrid white-box cryptanalytic method that circumvents the challenge of\nmodeling modular reduction by treating wrap-arounds as statistical corruption\nand casting secret recovery as robust linear estimation. Our approach combines\noptimized lattice preprocessing--including reduced-vector saving and algebraic\namplification--with robust estimators trained via Tukey's Biweight loss.\nExperiments show NoMod achieves full recovery of binary secrets for dimension\n$n = 350$, recovery of sparse binomial secrets for $n = 256$, and successful\nrecovery of sparse secrets in CRYSTALS-Kyber settings with parameters $(n, k) =\n(128, 3)$ and $(256, 2)$. We release our implementation in an anonymous\nrepository https://anonymous.4open.science/r/NoMod-3BD4.", "AI": {"tldr": "The advent of quantum computing threatens classical public-key cryptography, leading NIST to adopt post-quantum schemes like Module-LWE. The paper introduces NoMod ML-Attack, a hybrid white-box method for secret recovery that combines lattice preprocessing and robust linear estimation. Experiments show successful secret recovery in different scenarios.", "motivation": "Quantum computing poses a threat to classical public-key cryptography, necessitating the transition to post-quantum cryptographic schemes. The paper addresses the challenge of modeling modular reduction in such schemes by presenting a method to circumvent it through robust linear estimation.", "method": "The method involves a combination of optimized lattice preprocessing techniques, such as reduced-vector saving and algebraic amplification, with machine learning robust estimators trained via Tukey's Biweight loss. The approach treats modular reduction wrap-arounds as statistical corruption, enabling secret recovery as a robust linear estimation problem.", "result": "Experiments show that the proposed NoMod ML-Attack successfully recovers binary secrets of dimensions 350 and sparse binomial secrets of dimensions 256, as well as sparse secrets in CRYSTALS-Kyber settings with parameters (128, 3) and (256, 2).", "conclusion": "The NoMod ML-Attack method effectively circumvents the challenge of modular reduction in post-quantum cryptographic schemes, demonstrating successful secret recovery in various parameter settings. The implementation is available as an open repository."}}
{"id": "2510.02184", "categories": ["cs.CR", "nlin.CD", "34C15, 68M25, 94A60"], "pdf": "https://arxiv.org/pdf/2510.02184", "abs": "https://arxiv.org/abs/2510.02184", "authors": ["N. A. Anagnostopoulos", "K. Konstantinidis", "A. N. Miliou", "S. G. Stavrinides"], "title": "Testing Stability and Robustness in Three Cryptographic Chaotic Systems", "comment": "Published as \"N. A. Anagnostopoulos, K. Konstantinidis, A. N. Miliou\n  & S. G. Stavrinides, \"Testing Stability and Robustness in Three Cryptographic\n  Chaotic Systems\", Proceedings of the 3rd International Interdisciplinary\n  Symposium on Chaos and Complex Systems (CCS 2010), Journal of Concrete And\n  Applicable Mathematics (JCAAM), vol. 9, iss. 3, pp. 247-261, Eudoxus Press,\n  2011\"; no longer available", "summary": "In practical applications, it is crucial that the drive-response systems,\nalthough identical in all respects, are synchronized at all times, even if\nthere is noise present. In this work, we test the stability and robustness of\nthree distinct and well-known cryptographic chaotic systems, and compare the\nresults in relation to the desired security.", "AI": {"tldr": "This paper evaluates three chaotic cryptographic systems to determine their noise resilience and synchronization stability for secure applications.", "motivation": "The paper is motivated by the need for secure communication systems that maintain synchronization even in noisy environments, which is essential for practical cryptographic implementations.", "method": "The authors experimentally tested and compared three cryptographic chaotic systems by evaluating their synchronization behavior under noise conditions, focusing on stability and robustness metrics.", "result": "The results highlight differences in the performance of the three systems, with specific systems showing higher resilience to noise, thus aligning better with secure communication requirements.", "conclusion": "The study concludes that certain chaotic systems demonstrate superior stability and robustness in the presence of noise, making them more suitable for cryptographic applications where synchronization reliability is critical."}}
{"id": "2510.02196", "categories": ["cs.CR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.02196", "abs": "https://arxiv.org/abs/2510.02196", "authors": ["Jason Anderson"], "title": "Authentication Security of PRF GNSS Ranging", "comment": null, "summary": "This work derives the authentication security of pseudorandom function (PRF)\nGNSS ranging under multiple GNSS spoofing models, including the Security Code\nEstimation and Replay (SCER) spoofer. When GNSS ranging codes derive from a PRF\nutilizing a secret known only to the broadcaster, the spoofer cannot predict\nthe ranging code before broadcast. Therefore, PRF ranging can be used to\nestablish trust in the GNSS pseudoranges and the resulting receiver position,\nnavigation, and timing (PNT) solution. I apply the methods herein to Galileo's\nSignal Authentication Service (SAS) utilizing the encrypted Galileo E6-C signal\nto compute that, at most, 400 ms of Galileo E6-C data to assert 128-bit\nauthentication security under non-SCER models. For the SCER adversary, I\npredict the adversary's needed receiving radio equipment to break\nauthentication security. One can use this work to design a PRF GNSS ranging\nprotocol to meet useful authentication security requirements by computing the\nprobability of missed detection.", "AI": {"tldr": "This paper establishes authentication security for PRF-based GNSS ranging against spoofing, analyzing Galileo's SAS and showing 400ms of E6-C data achieves 128-bit security under non-SCER models. It also quantifies equipment requirements for SCER adversaries.", "motivation": "GNSS spoofing threats necessitate robust authentication mechanisms. PRF-based ranging with secret keys prevents spoofers from predicting codes, enabling trusted PNT solutions.", "method": "Derives security proofs for PRF-GNSS under multiple spoofing models (including SCER). Applies analysis to Galileo's SAS using E6-C signal characteristics, computing authentication security bounds and adversary equipment requirements through mathematical modeling.", "result": "128-bit authentication security requires at most 400ms of Galileo E6-C data under non-SCER models. Quantified SCER adversary capabilities based on required receiving equipment. Framework for designing PRF protocols via missed detection probability calculations.", "conclusion": "PRF-GNSS provides provable authentication security against spoofing. The Galileo SAS example demonstrates practical parameter selection. The analysis enables designing secure PRF protocols by balancing security requirements with operational constraints."}}
{"id": "2510.02280", "categories": ["cs.CR", "math.NT"], "pdf": "https://arxiv.org/pdf/2510.02280", "abs": "https://arxiv.org/abs/2510.02280", "authors": ["Jean-Francois Biasse", "Fang Song"], "title": "An efficient quantum algorithm for computing $S$-units and its applications", "comment": "Long version of a paper from SODA 2016", "summary": "In this paper, we provide details on the proofs of the quantum polynomial\ntime algorithm of Biasse and Song (SODA 16) for computing the $S$-unit group of\na number field. This algorithm directly implies polynomial time methods to\ncalculate class groups, S-class groups, relative class group and the unit\ngroup, ray class groups, solve the principal ideal problem, solve certain norm\nequations, and decompose ideal classes in the ideal class group. Additionally,\ncombined with a result of Cramer, Ducas, Peikert and Regev (Eurocrypt 2016),\nthe resolution of the principal ideal problem allows one to find short\ngenerators of a principal ideal. Likewise, methods due to Cramer, Ducas and\nWesolowski (Eurocrypt 2017) use the resolution of the principal ideal problem\nand the decomposition of ideal classes to find so-called ``mildly short\nvectors'' in ideal lattices of cyclotomic fields.", "AI": {"tldr": "This paper details a quantum polynomial-time algorithm for computing the S-unit group of a number field, enabling efficient solutions for several algebraic number theory problems like class group computation and principal ideal resolution, with implications for lattice problems and cryptography.", "motivation": "Efficient algorithms for number-theoretic problems are critical in algorithmic cryptography. The paper resolves open problems in computing fundamental algebraic structures (e.g., class groups, unit groups) in polynomial time using quantum methods, improving over classical exponential-time approaches.", "method": "The work provides rigorous proofs for the quantum polynomial-time algorithm of Biasse and Song (SODA 16). It leverages quantum subroutines for solving the principal ideal problem and integrates prior results (Cramer et al.) to address related tasks like finding short generators in principal ideals and decomposing ideal classes.", "result": "The algorithm achieves polynomial-time computation for S-unit groups, class groups, unit groups, ray class groups, and principal ideal solutions. It also enables resolution of norm equations and decomposition of ideal classes, with applications to finding short/generators of ideal lattices in cyclotomic fields.", "conclusion": "The paper demonstrates that quantum algorithms can solve core problems in algebraic number theory efficiently, significantly advancing lattice-based cryptography by connecting class group computations to hard lattice problems via short vector generation."}}
