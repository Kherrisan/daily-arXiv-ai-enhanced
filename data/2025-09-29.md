<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 30]
- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Design and Implementation of a Secure RAG-Enhanced AI Chatbot for Smart Tourism Customer Service: Defending Against Prompt Injection Attacks -- A Case Study of Hsinchu, Taiwan](https://arxiv.org/abs/2509.21367)
*Yu-Kai Shih,You-Kai Kang*

Main category: cs.CR

TL;DR: This paper presents a secure RAG chatbot for smart tourism, addressing prompt injection attacks with multi-layered defenses and achieving 95ppt accuracy on benign tasks with 85ppt attack detection using GPT-5 benchmarking.


<details>
  <summary>Details</summary>
Motivation: AI chatbots in smart tourism face prompt injection attacks risking sensitive data leaks and harmful content. Secure, context-aware solutions are critical for sustainability and traveler safety.

Method: Designed a RAG chatbot integrating API calls, multi-layered linguistic analysis (lexical/semantic/pragmatic), tiered response strategies, intent decomposition, and reverse RAG. Combined RAG with system norms and gatekeeper guardrails for injection prevention.

Result: 95ppt accuracy on 223 benign queries, 85ppt attack detection on 674 adversarial prompts. GPT-5 variant showed improved robustness but underscored layered defense necessity.

Conclusion: Provides a practical secure chatbot framework for smart tourism enhancing sustainability, multilingual access, and ethical AI. Demonstrates progress in resilient AI but highlights ongoing injection risk mitigation needs.

Abstract: As smart tourism evolves, AI-powered chatbots have become indispensable for
delivering personalized, real-time assistance to travelers while promoting
sustainability and efficiency. However, these systems are increasingly
vulnerable to prompt injection attacks, where adversaries manipulate inputs to
elicit unintended behaviors such as leaking sensitive information or generating
harmful content. This paper presents a case study on the design and
implementation of a secure retrieval-augmented generation (RAG) chatbot for
Hsinchu smart tourism services. The system integrates RAG with API function
calls, multi-layered linguistic analysis, and guardrails against injections,
achieving high contextual awareness and security. Key features include a tiered
response strategy, RAG-driven knowledge grounding, and intent decomposition
across lexical, semantic, and pragmatic levels. Defense mechanisms include
system norms, gatekeepers for intent judgment, and reverse RAG text to
prioritize verified data. We also benchmark a GPT-5 variant (released
2025-08-07) to assess inherent robustness. Evaluations with 674 adversarial
prompts and 223 benign queries show over 95% accuracy on benign tasks and
substantial detection of injection attacks. GPT-5 blocked about 85% of attacks,
showing progress yet highlighting the need for layered defenses. Findings
emphasize contributions to sustainable tourism, multilingual accessibility, and
ethical AI deployment. This work offers a practical framework for deploying
secure chatbots in smart tourism and contributes to resilient, trustworthy AI
applications.

</details>


### [2] [Towards Adapting Federated & Quantum Machine Learning for Network Intrusion Detection: A Survey](https://arxiv.org/abs/2509.21389)
*Devashish Chaudhary,Sutharshan Rajasegarar,Shiva Raj Pokhrel*

Main category: cs.CR

TL;DR: This paper surveys FL integration with NIDS, focusing on deep learning and quantum ML approaches to address data privacy, model efficiency, and robustness in network security contexts.


<details>
  <summary>Details</summary>
Motivation: Network security requires decentralized, privacy-preserving intrusion detection systems. Existing FL-NIDS solutions lack systematic analysis of architectures, privacy techniques, and quantum-enhanced approaches.

Method: Comprehensive analysis of FL architectures/protocols/aggregation methods for NIDS, comparative evaluation of classical vs quantum FL techniques (QFL), and identification of research gaps through real-world deployment analysis.

Result: Detailed taxonomy of privacy-preserving methods, attack-specific FL solutions (DDoS, MITM), and pioneering QFL framework with quantum feature encoding and speedup-potential algorithms for traffic pattern recognition.

Conclusion: Provides adoption roadmap for FL-NIDS in industrial settings while establishing quantum FL foundations for future cybersecurity, emphasizing practical implementation challenges and performance trade-offs.

Abstract: This survey explores the integration of Federated Learning (FL) with Network
Intrusion Detection Systems (NIDS), with particular emphasis on deep learning
and quantum machine learning approaches. FL enables collaborative model
training across distributed devices while preserving data privacy-a critical
requirement in network security contexts where sensitive traffic data cannot be
centralized. Our comprehensive analysis systematically examines the full
spectrum of FL architectures, deployment strategies, communication protocols,
and aggregation methods specifically tailored for intrusion detection. We
provide an in-depth investigation of privacy-preserving techniques, model
compression approaches, and attack-specific federated solutions for threats
including DDoS, MITM, and botnet attacks. The survey further delivers a
pioneering exploration of Quantum FL (QFL), discussing quantum feature
encoding, quantum machine learning algorithms, and quantum-specific aggregation
methods that promise exponential speedups for complex pattern recognition in
network traffic. Through rigorous comparative analysis of classical and quantum
approaches, identification of research gaps, and evaluation of real-world
deployments, we outline a concrete roadmap for industrial adoption and future
research directions. This work serves as an authoritative reference for
researchers and practitioners seeking to enhance privacy, efficiency, and
robustness of federated intrusion detection systems in increasingly complex
network environments, while preparing for the quantum-enhanced cybersecurity
landscape of tomorrow.

</details>


### [3] [Dynamic Dual-level Defense Routing for Continual Adversarial Training](https://arxiv.org/abs/2509.21392)
*Wenxuan Wang,Chenglei Wang,Xuelin Qian*

Main category: cs.CR

TL;DR: The paper introduces DDeR, a dual-level defense routing framework for continuous adversarial training (CAT). It addresses vulnerabilities in existing CAT methods by autonomously selecting defense experts and routers to adapt to evolving adversarial attacks without catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: Existing CAT approaches using data replay or optimization strategies struggle with catastrophic forgetting during continual learning against diverse adversarial examples. This limits their ability to maintain prior defense knowledge while adapting to new attacks.

Method: DDeR employs a two-level routing system: 1. First-level routing dynamically combines defense experts via independent routers to process attacked features 2. Second-level routing uses an Adversarial Sentinel Network (ASN). Additionally, Pseudo-task Substitution Training (PST)' leverages data distribution discrepancies to enable inter-router communication without historical data storage.

Result: Extensive experiments show DDeR achieves superior continuous defense performance and classification accuracy compared to state-of-the-art CAT methods, demonstrating better adaptation to adversarial attack evolution.

Conclusion: DDeR provides an effective solution for continuous adversarial training by enabling autonomous adaptation to new threats while preserving prior defense capabilities, significantly outperforming existing approaches in maintaining robust performance.

Abstract: As adversarial attacks continue to evolve, defense models face the risk of
recurrent vulnerabilities, underscoring the importance of continuous
adversarial training (CAT). Existing CAT approaches typically balance decision
boundaries by either data replay or optimization strategy to constrain shared
model parameters. However, due to the diverse and aggressive nature of
adversarial examples, these methods suffer from catastrophic forgetting of
previous defense knowledge after continual learning. In this paper, we propose
a novel framework, called Dual-level Defense Routing or DDeR, that can
autonomously select appropriate routers to integrate specific defense experts,
thereby adapting to evolving adversarial attacks. Concretely, the first-level
defense routing comprises multiple defense experts and routers, with each
router dynamically selecting and combining suitable experts to process attacked
features. Routers are independently incremented as continuous adversarial
training progresses, and their selections are guided by an Adversarial Sentinel
Network (ASN) in the second-level defense routing. To compensate for the
inability to test due to the independence of routers, we further present a
Pseudo-task Substitution Training (PST) strategy, which leverages
distributional discrepancy in data to facilitate inter-router communication
without storing historical data. Extensive experiments demonstrate that DDeR
achieves superior continuous defense performance and classification accuracy
compared to existing methods.

</details>


### [4] [SafeSteer: Adaptive Subspace Steering for Efficient Jailbreak Defense in Vision-Language Models](https://arxiv.org/abs/2509.21400)
*Xiyu Zeng,Siyuan Liang,Liming Lu,Haotian Zhu,Enguang Liu,Jisheng Dang,Yongbin Zhou,Shuchao Pang*

Main category: cs.CR

TL;DR: The paper introduces SafeSteer, a lightweight, inference-time defense framework for Vision Language Models (VLMs) that uses Singular Value Decomposition (SVD) to create a 'safety subspace.' It effectively reduces jailbreak attack success by 60% with minimal latency and preserves model utility.  


<details>
  <summary>Details</summary>
Motivation: Existing VLM defenses either compromise model utility or introduce significant inference latency, creating a gap for solutions that maintain both safety and efficiency.  

Method: SafeSteer constructs a low-dimensional safety subspace via SVD during inference. Steering vectors (inputs) are projected into and reconstructed from this subspace to filter harmful signals while retaining benign functionality, all within a single inference pass. Model weights remain unaltered.  

Result: Experiments show SafeSteer achieves >60% reduction in attack success rates, 1-2% accuracy improvement on normal tasks, and negligible latency increase, validating its efficiency and robustness against diverse jailbreak attacks.

Conclusion: SafeSteer demonstrates a viable path for practical VLM protection through simple, efficient inference-time control, resolving the safety-utility-latency trade-off inherent in prior methods.

Abstract: As the capabilities of Vision Language Models (VLMs) continue to improve,
they are increasingly targeted by jailbreak attacks. Existing defense methods
face two major limitations: (1) they struggle to ensure safety without
compromising the model's utility; and (2) many defense mechanisms significantly
reduce the model's inference efficiency. To address these challenges, we
propose SafeSteer, a lightweight, inference-time steering framework that
effectively defends against diverse jailbreak attacks without modifying model
weights. At the core of SafeSteer is the innovative use of Singular Value
Decomposition to construct a low-dimensional "safety subspace." By projecting
and reconstructing the raw steering vector into this subspace during inference,
SafeSteer adaptively removes harmful generation signals while preserving the
model's ability to handle benign inputs. The entire process is executed in a
single inference pass, introducing negligible overhead. Extensive experiments
show that SafeSteer reduces the attack success rate by over 60% and improves
accuracy on normal tasks by 1-2%, without introducing significant inference
latency. These results demonstrate that robust and practical jailbreak defense
can be achieved through simple, efficient inference-time control.

</details>


### [5] [Designing Ethereum's Geographical (De)Centralization Beyond the Atlantic](https://arxiv.org/abs/2509.21475)
*Sen Yang,Burak Öz,Fei Wu,Fan Zhang*

Main category: cs.CR

TL;DR: The paper explores how protocol design influences geographic decentralization in Ethereum by modeling block-building paradigms, revealing that Multi-Source Paradigms (MSPs) accelerate geographic centralization compared to Single-Source Paradigms due to location-dependent marginal value and latency advantages.


<details>
  <summary>Details</summary>
Motivation: Conventional decentralization metrics overlook geographic distribution's impact on resilience and fairness. The paper aims to understand how latency and protocol design affect validator geography, critical as Ethereum's validators currently cluster latency-advantaged regions like the Atlantic.

Method: A latency-calibrated agent-based model compares two Ethereum paradigms: Single-Source (SSP, relay-dependent) and Multi-Source (MSP, aggregation-based). Simulations evaluate how latency, source placement, and consensus settings influence validator clustering and centralization dynamics.

Result: SSP centralizes slowly around relay locations, while MSP centralizes faster due to location-dependent value maximization, favoring latency minima. North America consistently becomes the focal hub. Post-clustering, source placement has minimal impact on decentralization. Consensus settings modulate these effects.

Conclusion: Protocol design significantly shapes validator geography, with MSP amplifying centralization under latency constraints. The findings highlight levers for mitigating geographic concentration and promoting decentralization, emphasizing latency and value dispersion in protocol choices.

Abstract: Decentralization has a geographic dimension that conventional metrics such as
stake distribution overlook. Where validators run affects resilience to
regional shocks (outages, disasters, government intervention) and fairness in
reward access. Yet in permissionless systems, locations cannot be mandated, but
they emerge from incentives. Today, Ethereum's validators cluster along the
Atlantic (EU and U.S. East Coast), where latency is structurally favorable.
This raises a key question: when some regions already enjoy latency advantages,
how does protocol design shape validator incentives and the geography of
(de)centralization? We develop a latency-calibrated agent-based model and
compare two Ethereum block-building paradigms: a Single-Source Paradigm (SSP),
akin to MEV-Boost, where proposers fetch full blocks from a relay that also
propagates them; and a Multi-Source Paradigm (MSP), where proposers aggregate
value from multiple sources and broadcast the block themselves. Simulations
show that SSP concentrates around relay placement but more slowly, since
proximity mainly affects propagation, and the marginal value of time is
relatively uniform across regions. MSP centralizes faster: aggregating across
sources makes marginal value location-dependent, amplifying payoff dispersion
and migration toward latency minima. Source placement and consensus settings
can dampen or intensify these effects, though once validators are already
clustered, the impact of source placement on decentralization is marginal. In
most cases, North America consistently emerges as the focal hub. These findings
show that protocol design materially shapes validator geography and offer
levers for promoting geographical decentralization.

</details>


### [6] [Functional Encryption in Secure Neural Network Training: Data Leakage and Practical Mitigations](https://arxiv.org/abs/2509.21497)
*Alexandru Ioniţă,Andreea Ioniţă*

Main category: cs.CR

TL;DR: This paper analyzes vulnerabilities in Functional Encryption (FE)-based secure neural network training, demonstrates a linear programming attack to reconstruct encrypted data, and proposes two client-involved solutions for secure training/inference without relying on FE encryption.


<details>
  <summary>Details</summary>
Motivation: ML-as-a-Service (MLaaS)

Method: 1) Attack method: Uses linear programming to reconstruct original inputs from FE-encrypted data during neural network training. 2. Defense methods: (a) Client-participating computation without encryption, (b) Function-hiding inner-product techniques with client involvement. Both defenses ensure data confidentiality during training.

Result: Successfully demonstrates attack on FE-based secure training systems, proving prior security assumptions invalid. Proposed solutions effectively mitigate vulnerabilities while maintaining training accuracy and client data privacy during computation phases. One solution eliminates encryption reliance entirely, the other employs advanced FE techniques for secure inner products.

Conclusion: Exposed critical security gaps in current FE-based ML training approaches, proving that input reconstruction is feasible. The proposed client-involved solutions (encryption-free and function-hiding methods) provide practical alternatives for secure MLaaS while maintaining system utility. Highlights need for re-evaluation of FE-based security models in machine learning scenarios.

Abstract: With the increased interest in artificial intelligence, Machine Learning as a
Service provides the infrastructure in the Cloud for easy training, testing,
and deploying models. However, these systems have a major privacy issue:
uploading sensitive data to the Cloud, especially during training. Therefore,
achieving secure Neural Network training has been on many researchers' minds
lately. More and more solutions for this problem are built around a main
pillar: Functional Encryption (FE). Although these approaches are very
interesting and offer a new perspective on ML training over encrypted data,
some vulnerabilities do not seem to be taken into consideration. In our paper,
we present an attack on neural networks that uses FE for secure training over
encrypted data. Our approach uses linear programming to reconstruct the
original input, unveiling the previous security promises. To address the
attack, we propose two solutions for secure training and inference that involve
the client during the computation phase. One approach ensures security without
relying on encryption, while the other uses function-hiding inner-product
techniques.

</details>


### [7] [From Indexing to Coding: A New Paradigm for Data Availability Sampling](https://arxiv.org/abs/2509.21586)
*Moritz Grundei,Aayush Rajasekaran,Kishori Konwar,Muriel Medard*

Main category: cs.CR

TL;DR: This paper proposes a new and more efficient data availability sampling method for blockchains by modularizing the coding and commitment process, resulting in stronger assurances for light nodes.


<details>
  <summary>Details</summary>
Motivation: Blockchain systems like Ethereum face challenges with data availability, which affects their accessibility and scalability. Current methods using fixed-rate erasure codes are limited in efficiency, necessitating a more effective and self-sufficient approach.

Method: The proposed approach introduces a modular data availability sampling framework where light nodes commit to uncoded data and perform on-the-fly coding with random linear network coding (RLNC), improving the efficiency and expressiveness of the sampling process.

Result: Using RLNC, the authors demonstrate concrete implementations that provide light nodes with significantly stronger data availability assurances compared to traditional DAS methods such as those based on Reed Solomon or low density parity check codes.

Conclusion: The paper concludes that the new method enhances data availability sampling in blockchain systems by employing modular coding and commitment, offering a substantial improvement in security and assurance without relying on fixed-rate redundancy codes.

Abstract: The data availability problem is a central challenge in blockchain systems
and lies at the core of the accessibility and scalability issues faced by
platforms such as Ethereum. Modern solutions employ several approaches, with
data availability sampling (DAS) being the most self-sufficient and
minimalistic in its security assumptions. Existing DAS methods typically form
cryptographic commitments on codewords of fixed-rate erasure codes, which
restrict light nodes to sampling from a predetermined set of coded symbols.
  In this paper, we introduce a new approach to DAS that modularizes the coding
and commitment process by committing to the uncoded data while performing
sampling through on-the-fly coding. The resulting samples are significantly
more expressive, enabling light nodes to obtain, in concrete implementations,
up to multiple orders of magnitude stronger assurances of data availability
than from sampling pre-committed symbols from a fixed-rate redundancy code as
done in established DAS schemes using Reed Solomon or low density parity check
codes. We present a concrete protocol that realizes this paradigm using random
linear network coding (RLNC).

</details>


### [8] [It's not Easy: Applying Supervised Machine Learning to Detect Malicious Extensions in the Chrome Web Store](https://arxiv.org/abs/2509.21590)
*Ben Rosenzweig,Valentino Dalla Valle,Giovanni Apruzzese,Aurore Fass*

Main category: cs.CR

TL;DR: This paper analyzes the effectiveness of supervised ML in detecting malicious Chrome extensions, revealing high lab accuracy (98%) but significant concept drift in real-world scenarios (over 1k false positives), and highlights the ongoing challenge of browser extension security despite Google's vetting.


<details>
  <summary>Details</summary>
Motivation: Malicious browser extensions bypass Google's CWS vetting, endangering user security. Current automated detection systems (including commercial tools) fail to address this systematically.

Method: Collected 7,140 malicious and 63,598 benign extensions pre-2023 to train ML classifiers. Tested on 35,462 post-2023 extensions with unknown status, discovering 68 confirmed malicious extensions.

Result: Classifiers showed 98% lab accuracy but detected >1k potential malicious extensions in real-world data, demonstrating severe concept drift. Commercial detectors like VirusTotal had poor performance.

Conclusion: Browser extension malware detection is fundamentally challenging due to concept drift and evolving attack patterns. Requires collaborative research efforts and potential revisions to Google's vetting processes.

Abstract: Google Chrome is the most popular Web browser. Users can customize it with
extensions that enhance their browsing experience. The most well-known
marketplace of such extensions is the Chrome Web Store (CWS). Developers can
upload their extensions on the CWS, but such extensions are made available to
users only after a vetting process carried out by Google itself. Unfortunately,
some malicious extensions bypass such checks, putting the security and privacy
of downstream browser extension users at risk.
  Here, we scrutinize the extent to which automated mechanisms reliant on
supervised machine learning (ML) can be used to detect malicious extensions on
the CWS. To this end, we first collect 7,140 malicious extensions published in
2017--2023. We combine this dataset with 63,598 benign extensions published or
updated on the CWS before 2023, and we develop three supervised-ML-based
classifiers. We show that, in a "lab setting", our classifiers work well (e.g.,
98% accuracy). Then, we collect a more recent set of 35,462 extensions from the
CWS, published or last updated in 2023, with unknown ground truth. We were
eventually able to identify 68 malicious extensions that bypassed the vetting
process of the CWS. However, our classifiers also reported >1k likely malicious
extensions. Based on this finding (further supported with empirical evidence),
we elucidate, for the first time, a strong concept drift effect on browser
extensions. We also show that commercial detectors (e.g., VirusTotal) work
poorly to detect known malicious extensions. Altogether, our results highlight
that detecting malicious browser extensions is a fundamentally hard problem.
This requires additional work both by the research community and by Google
itself -- potentially by revising their approaches. In the meantime, we
informed Google of our discoveries, and we release our artifacts.

</details>


### [9] [World's First Authenticated Satellite Pseudorange from Orbit](https://arxiv.org/abs/2509.21601)
*Jason Anderson*

Main category: cs.CR

TL;DR: This paper introduces Pulsar, a cryptographic ranging authentication service using orbital transmissions. It presents a watermark design for secure, key-leakage-free spoofing detection in satellite pseudorange measurements, validated with 2025 orbital data.


<details>
  <summary>Details</summary>
Motivation: Current satellite ranging systems lack authentication without key secrecy assumptions. This work addresses spoofing threats by establishing a mathematically justified, key-agnostic security framework for space-based navigation.

Method: The paper develops the Pulsar watermark with: (19) security analysis of missed detection/false alarm probabilities, (10) receiver processing requirements, (10) orbital transmission validation, and (8) spoofing scenario testing using authentic orbit-sourced signals.

Result: Demonstrated watermark efficacy via (14) orbital data validation and (8) spoofing resistance in controlled scenarios. Proves mathematical security guarantees without symmetric key assumptions, enabling claims of July 2025 world-first authenticated pseudorange from orbit.

Conclusion: Pulsar establishes a foundational, secure ranging authentication paradigm by combining orbital signal watermarking with rigorous security analysis, fulfilling the first practical satellite-validated cryptographic ranging solution.

Abstract: Cryptographic Ranging Authentication is here! We present initial results on
the Pulsar authenticated ranging service broadcast from space with Pulsar-0
utilizing a recording taken at Xona headquarters in Burlingame, CA. No
assumptions pertaining to the ownership or leakage of encryption keys are
required. This work discusses the Pulsar watermark design and security
analysis. We derive the Pulsar watermark's probabilities of missed detection
and false alarm, and we discuss the required receiver processing needed to
utilize the Pulsar watermark. We present validation results of the Pulsar
watermark utilizing the transmissions from orbit. Lastly, we provide results
that demonstrate the spoofing detection efficacy with a spoofing scenario that
incorporates the authentic transmissions from orbit. Because we make no
assumption about the leakage of symmetric encryption keys, this work provides
mathematical justification of the watermark's security, and our July 2025
transmissions from orbit, we claim the world's first authenticated satellite
pseudorange from orbit.

</details>


### [10] [MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G Open RANs](https://arxiv.org/abs/2509.21634)
*Prakhar Sharma,Haohuang Wen,Vinod Yegneswaran,Ashish Gehani,Phillip Porras,Zhiqiang Lin*

Main category: cs.CR

TL;DR: Develops MobiLLM, an agentic AI framework for automated threat mitigation in 6G O-RAN environments using multi-agent LLMs and RAG.


<details>
  <summary>Details</summary>
Motivation: O-RAN's openness expands attack surfaces and demands autonomous security solutions; legacy systems are reactive and inadequate for 6G's complexity.

Method: Modular multi-agent system (Threat Analysis, RAG-based Classification Agent, Threat Response Agent) integrated with MITRE FiGHT/3GPP standards and safety guardrails for real-time, closed-loop mitigation.

Result: Initial evaluations show effective strategy orchestration and 63.2% reduction in response latency, validating autonomous threat mitigation feasibility.

Conclusion: MobiLLM establishes a scalable, trustworthy blueprint for end-to-end AI-driven security in O-RAN 6G ecosystems, addressing critical gaps in automated defense.

Abstract: The evolution toward 6G networks is being accelerated by the Open Radio
Access Network (O-RAN) paradigm -- an open, interoperable architecture that
enables intelligent, modular applications across public telecom and private
enterprise domains. While this openness creates unprecedented opportunities for
innovation, it also expands the attack surface, demanding resilient, low-cost,
and autonomous security solutions. Legacy defenses remain largely reactive,
labor-intensive, and inadequate for the scale and complexity of next-generation
systems. Current O-RAN applications focus mainly on network optimization or
passive threat detection, with limited capability for closed-loop, automated
response.
  To address this critical gap, we present an agentic AI framework for fully
automated, end-to-end threat mitigation in 6G O-RAN environments. MobiLLM
orchestrates security workflows through a modular multi-agent system powered by
Large Language Models (LLMs). The framework features a Threat Analysis Agent
for real-time data triage, a Threat Classification Agent that uses
Retrieval-Augmented Generation (RAG) to map anomalies to specific
countermeasures, and a Threat Response Agent that safely operationalizes
mitigation actions via O-RAN control interfaces. Grounded in trusted knowledge
bases such as the MITRE FiGHT framework and 3GPP specifications, and equipped
with robust safety guardrails, MobiLLM provides a blueprint for trustworthy
AI-driven network security. Initial evaluations demonstrate that MobiLLM can
effectively identify and orchestrate complex mitigation strategies,
significantly reducing response latency and showcasing the feasibility of
autonomous security operations in 6G.

</details>


### [11] [Not My Agent, Not My Boundary? Elicitation of Personal Privacy Boundaries in AI-Delegated Information Sharing](https://arxiv.org/abs/2509.21712)
*Bingcan Guo,Eryue Xu,Zhiping Zhang,Tianshi Li*

Main category: cs.CR

TL;DR: The paper investigates the complexity of aligning AI systems with individual privacy preferences through a task-based elicitation approach, revealing how communication roles and delegation affect privacy boundaries.


<details>
  <summary>Details</summary>
Motivation: Understanding individual privacy preferences is crucial for aligning AI systems, but this is complicated by context-dependent decisions and trade-offs, which make traditional norms insufficient.

Method: The researchers conducted a between-subjects study with 169 participants, varying communication roles and AI delegation conditions to elicit privacy boundaries for specific scenarios.

Result: Quantitative analysis showed that communication roles impact what individuals accept as privacy-relevant information; AI delegation increases sensitivity to identifiable data with less inter-individual consensus.

Conclusion: Real-world data flow contexts are essential for effective privacy preference elicitation, and nuanced, context-aware boundaries should guide future AI system alignment.

Abstract: Aligning AI systems with human privacy preferences requires understanding
individuals' nuanced disclosure behaviors beyond general norms. Yet eliciting
such boundaries remains challenging due to the context-dependent nature of
privacy decisions and the complex trade-offs involved. We present an AI-powered
elicitation approach that probes individuals' privacy boundaries through a
discriminative task. We conducted a between-subjects study that systematically
varied communication roles and delegation conditions, resulting in 1,681
boundary specifications from 169 participants for 61 scenarios. We examined how
these contextual factors and individual differences influence the boundary
specification. Quantitative results show that communication roles influence
individuals' acceptance of detailed and identifiable disclosure, AI delegation
and individuals' need for privacy heighten sensitivity to disclosed
identifiers, and AI delegation results in less consensus across individuals.
Our findings highlight the importance of situating privacy preference
elicitation within real-world data flows. We advocate using nuanced privacy
boundaries as an alignment goal for future AI systems.

</details>


### [12] [Backdoor Attribution: Elucidating and Controlling Backdoor in Language Models](https://arxiv.org/abs/2509.21761)
*Miao Yu,Zhenhong Zhou,Moayad Aloqaily,Kun Wang,Biwei Huang,Stephen Wang,Yueming Jin,Qingsong Wen*

Main category: cs.CR

TL;DR: This paper introduces Backdoor Attribution (BkdAttr), a tripartite causal analysis framework to interpret and control LLM backdoors via attention head analysis and backdoor vector manipulation, achieving significant reductions in attack success rates with minimal model alterations.


<details>
  <summary>Details</summary>
Motivation: Existing LLM safety research overlooks backdoor mechanisms, leaving them as black boxes. Understanding these mechanisms is critical for effective backdoor mitigation.

Method: 1) Backdoor Probe: Identifies learnable backdoor features in model representations. 2} Backdoor Attention Head Attribution (BAHA): Pinpoints attention heads processing these features. 3} Backdoor Vector: A master controller derived from critical heads to manipulate attack success rates through single-point interventions.

Result: Removing ~3% of attention heads reduces Attack Success Rate (ASR:]90%. Backdoor Vector enables 100%/0% ASR manipulation on clean/triggered inputs via single representation intervention.

Conclusion: The paper pioneers mechanistic interpretability for LLM backdoors, demonstrating a powerful control method through sparse attention head targeting, providing actionable insights for backdoor analysis and mitigation.

Abstract: Fine-tuned Large Language Models (LLMs) are vulnerable to backdoor attacks
through data poisoning, yet the internal mechanisms governing these attacks
remain a black box. Previous research on interpretability for LLM safety tends
to focus on alignment, jailbreak, and hallucination, but overlooks backdoor
mechanisms, making it difficult to understand and fully eliminate the backdoor
threat. In this paper, aiming to bridge this gap, we explore the interpretable
mechanisms of LLM backdoors through Backdoor Attribution (BkdAttr), a
tripartite causal analysis framework. We first introduce the Backdoor Probe
that proves the existence of learnable backdoor features encoded within the
representations. Building on this insight, we further develop Backdoor
Attention Head Attribution (BAHA), efficiently pinpointing the specific
attention heads responsible for processing these features. Our primary
experiments reveals these heads are relatively sparse; ablating a minimal
\textbf{$\sim$ 3%} of total heads is sufficient to reduce the Attack Success
Rate (ASR) by \textbf{over 90%}. More importantly, we further employ these
findings to construct the Backdoor Vector derived from these attributed heads
as a master controller for the backdoor. Through only \textbf{1-point}
intervention on \textbf{single} representation, the vector can either boost ASR
up to \textbf{$\sim$ 100% ($\uparrow$)} on clean inputs, or completely
neutralize backdoor, suppressing ASR down to \textbf{$\sim$ 0% ($\downarrow$)}
on triggered inputs. In conclusion, our work pioneers the exploration of
mechanistic interpretability in LLM backdoors, demonstrating a powerful method
for backdoor control and revealing actionable insights for the community.

</details>


### [13] [PSRT: Accelerating LRM-based Guard Models via Prefilled Safe Reasoning Traces](https://arxiv.org/abs/2509.21768)
*Jiawei Zhao,Yuang Qi,Weiming Zhang,Nenghai Yu,Kejiang Chen*

Main category: cs.CR

TL;DR: This paper introduces PSRT, a method that replaces LLMs' reasoning processes with pre-filled 'safe' reasoning traces to reduce inference costs while maintaining classification accuracy for harmful-query detection.


<details>
  <summary>Details</summary>
Motivation: LRMs excel at tasks like mathematics and code generation but suffer from high inference costs due to long reasoning traces. They are effective guard models for harmful query detection, but computational overhead limits deployment.

Method: PSRT pre-fills 'safe reasoning virtual tokens from a constructed dataset, trains on their embeddings, and uses indicator tokens to enable single-pass harmful query detection while retaining LRM classification capabilities.

Result: PSRT eliminates reasoning token generation overhead across 7 models, 13 datasets, and 8 jailbreak methods. Accuracy remains nearly identical with only 0.015 F1 drop on average across 7 models and 5 datasets.

Conclusion: PSRT demonstrates that pre-filled reasoning traces can dramatically reduce LRM inference costs with minimal performance loss, making LRM-based harmful query detection more computationally feasible.

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on
tasks such as mathematics and code generation. Motivated by these strengths,
recent work has empirically demonstrated the effectiveness of LRMs as guard
models in improving harmful query detection. However, LRMs typically generate
long reasoning traces during inference, causing substantial computational
overhead. In this paper, we introduce PSRT, a method that replaces the model's
reasoning process with a Prefilled Safe Reasoning Trace, thereby significantly
reducing the inference cost of LRMs. Concretely, PSRT prefills "safe reasoning
virtual tokens" from a constructed dataset and learns over their continuous
embeddings. With the aid of indicator tokens, PSRT enables harmful-query
detection in a single forward pass while preserving the classification
effectiveness of LRMs. We evaluate PSRT on 7 models, 13 datasets, and 8
jailbreak methods. In terms of efficiency, PSRT completely removes the overhead
of generating reasoning tokens during inference. In terms of classification
performance, PSRT achieves nearly identical accuracy, with only a minor average
F1 drop of 0.015 across 7 models and 5 datasets.

</details>


### [14] [PhishLumos: An Adaptive Multi-Agent System for Proactive Phishing Campaign Mitigation](https://arxiv.org/abs/2509.21772)
*Daiki Chiba,Hiroki Nakano,Takashi Koide*

Main category: cs.CR

TL;DR: PhishLumos is a proactive system using LLM agents to detect phishing campaigns early by analyzing infrastructure patterns, identifying 100% of campaigns a week before experts confirm them.


<details>
  <summary>Details</summary>
Motivation: Phishing harms vulnerable groups and undermines trust in digital services. Current defenses are reactive, failing against tactics like cloaking that hide malicious content. There's an imbalance in scalability between attacks and defenses.

Method: PhishLumos is an adaptive multi-agent system powered by LLMs. It analyzes shared hosting, certificates, and domain registration patterns to identify entire attack campaigns.

Result: In tests, PhishLumos identified 100% of phishing campaigns in the median case, over a week earlier than confirmation by cybersecurity experts.

Conclusion: PhishLumos shifts the paradigm from reactive URL blocking to proactive campaign mitigation. It protects users before attacks occur and enhances digital safety for vulnerable populations.

Abstract: Phishing attacks are a significant societal threat, disproportionately
harming vulnerable populations and eroding trust in essential digital services.
Current defenses are often reactive, failing against modern evasive tactics
like cloaking that conceal malicious content. To address this, we introduce
PhishLumos, an adaptive multi-agent system that proactively mitigates entire
attack campaigns. It confronts a core cybersecurity imbalance: attackers can
easily scale operations, while defense remains an intensive expert task.
Instead of being blocked by evasion, PhishLumos treats it as a critical signal
to investigate the underlying infrastructure. Its Large Language Model
(LLM)-powered agents uncover shared hosting, certificates, and domain
registration patterns. On real-world data, our system identified 100% of
campaigns in the median case, over a week before their confirmation by
cybersecurity experts. PhishLumos demonstrates a practical shift from reactive
URL blocking to proactive campaign mitigation, protecting users before they are
harmed and making the digital world safer for all.

</details>


### [15] [Lattice-Based Dynamic $k$-times Anonymous Authentication](https://arxiv.org/abs/2509.21786)
*Junjie Song,Jinguang Han,Man Ho Au,Rupeng Yang,Chao Sun*

Main category: cs.CR

TL;DR: Proposed a lattice-based dynamic $k$-TAA that supports anonymous, dynamic, and post-quantum secure authentication.


<details>
  <summary>Details</summary>
Motivation: Privacy is important with the internet growth, and existing anonymous authentication schemes have limitations in quantum security and dynamic user management.

Method: We present a lattice-based cryptographic construction for $k$-times anonymous authentication, utilizing post-quantum secure assumptions to allow dynamic granting and revoking of users without compromising anonymity.

Result: The proposed scheme achieves efficient communication costs and is reduced to standard hardness assumptions. It is the first of its kind in being dynamic while maintaining sufficient anonymity and post-quantum security.

Conclusion: The new dynamic lattice-based $k$-TAA is a significant improvement over previous schemes, offering a practical application for privacy-preserving services in the post-quantum era.

Abstract: With the development of Internet, privacy has become a close concern of
users. Anonymous authentication plays an important role in privacy-preserving
systems. $k$-times anonymous authentication ($k$-TAA) scheme allows members of
a group to be authenticated anonymously by application providers up to $k$
times. Considering quantum computing attacks, lattice-based $k$-TAA was
introduced. However, existing schemes do not support dynamically granting and
revoking users. In this paper, we construct the first lattice-based dynamic
$k$-TAA, which offers limited times anonymous authentication, dynamic member
management, and post-quantum security. We present a concrete construction, and
reduce its security to standard complexity assumptions. Notably, compared with
existing lattice-based $k$-TAA, our scheme is efficient in terms of
communication cost.

</details>


### [16] [SoK: Potentials and Challenges of Large Language Models for Reverse Engineering](https://arxiv.org/abs/2509.21821)
*Xinyu Hu,Zhiwei Fu,Shaocong Xie,Steven H. H. Ding,Philippe Charland*

Main category: cs.CR

TL;DR: This paper systematizes the application of Large Language Models (LLMs) in Reverse Engineering (RE) by organizing 44 research papers and 18 open-source projects into a taxonomy, identifying strengths/limitations, and proposing future research directions for security-relevant LLM applications in RE.


<details>
  <summary>Details</summary>
Motivation: Reverse Engineering (RE) is critical for software security but remains labor-intensive and expertise-dependent. Prior work on deep learning and LLMs applied to RE lacks clarity on comparative effectiveness, reproducibility, and cumulative progress due to methodological diversity and inconsistent evaluation practices.

Method: The paper reviews 44 research papers and 18 open-source projects applying LLMs to RE. It proposes a taxonomy categorizing work by objective, target, method, evaluation strategy, and data scale, alongside analysis of reproducibility, evaluation gaps, and emerging risks.

Result: The analysis reveals strengths and limitations of LLMs in RE, highlights reproducibility and evaluation gaps, and identifies risks associated with current practices. A structured taxonomy and detailed synthesis of existing approaches are presented.

Conclusion: The paper concludes by outlining open challenges and future research directions to foster coherent, security-focused advancements in leveraging LLMs for RE tasks, emphasizing reproducibility and standardized evaluation practices.

Abstract: Reverse Engineering (RE) is central to software security, enabling tasks such
as vulnerability discovery and malware analysis, but it remains labor-intensive
and requires substantial expertise. Earlier advances in deep learning start to
automate parts of RE, particularly for malware detection and vulnerability
classification. More recently, a rapidly growing body of work has applied Large
Language Models (LLMs) to similar purposes. Their role compared to prior
machine learning remains unclear, since some efforts simply adapt existing
pipelines with minimal change while others seek to exploit broader reasoning
and generative abilities. These differences, combined with varied problem
definitions, methods, and evaluation practices, limit comparability,
reproducibility, and cumulative progress. This paper systematizes the field by
reviewing 44 research papers, including peer-reviewed publications and
preprints, and 18 additional open-source projects that apply LLMs in RE. We
propose a taxonomy that organizes existing work by objective, target, method,
evaluation strategy, and data scale. Our analysis identifies strengths and
limitations, highlights reproducibility and evaluation gaps, and examines
emerging risks. We conclude with open challenges and future research directions
that aim to guide more coherent and security-relevant applications of LLMs in
RE.

</details>


### [17] [The Dark Art of Financial Disguise in Web3: Money Laundering Schemes and Countermeasures](https://arxiv.org/abs/2509.21831)
*Hesam Sarkhosh,Uzma Maroof,Diogo Barradas*

Main category: cs.CR

TL;DR: The survey explores money laundering techniques in Web3 and DeFi, highlighted by the absence of centralized control and regulatory loopholes, and outlines ways to enhance transparency and tackle these issues.


<details>
  <summary>Details</summary>
Motivation: The rise of Web3 and DeFi, due to their trustless and borderless nature, has led to increased opportunities for financial crimes like money laundering, necessitating a comprehensive analysis to address regulatory and security needs.

Method: The survey reviews existing literature and cases to form a taxonomy of strategies and mechanisms used for money laundering in Web3, combining analysis of pseudonymity, weak regulations, and potential vulnerabilities in the DeFi ecosystem.

Result: The study identifies specific laundering techniques, outlines the challenges in detecting and preventing them, and provides a structured understanding of how criminals exploit DeFi's characteristics for illicit purposes.

Conclusion: To mitigate the risks of money laundering in Web3, the paper suggests developing robust detection frameworks and promoting future research to strengthen the ecosystem's transparency and regulatory compliance.

Abstract: The rise of Web3 and Decentralized Finance (DeFi) has enabled borderless
access to financial services empowered by smart contracts and blockchain
technology. However, the ecosystem's trustless, permissionless, and borderless
nature presents substantial regulatory challenges. The absence of centralized
oversight and the technical complexity create fertile ground for financial
crimes. Among these, money laundering is particularly concerning, as in the
event of successful scams, code exploits, and market manipulations, it
facilitates covert movement of illicit gains. Beyond this, there is a growing
concern that cryptocurrencies can be leveraged to launder proceeds from drug
trafficking, or to transfer funds linked to terrorism financing.
  This survey aims to outline a taxonomy of high-level strategies and
underlying mechanisms exploited to facilitate money laundering in Web3. We
examine how criminals leverage the pseudonymous nature of Web3, alongside weak
regulatory frameworks, to obscure illicit financial activities. Our study seeks
to bridge existing knowledge gaps on laundering schemes, identify open
challenges in the detection and prevention of such activities, and propose
future research directions to foster a more transparent Web3 financial
ecosystem -- offering valuable insights for researchers, policymakers, and
industry practitioners.

</details>


### [18] [SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models](https://arxiv.org/abs/2509.21843)
*Jingkai Guo,Chaitali Chakrabarti,Deliang Fan*

Main category: cs.CR

TL;DR: This paper proposes SBFA, a sneaky bit-flip attack that collapses Large Language Models (LLMs)' performance with a single bit flip while maintaining parameter stealthiness. It introduces ImpactScore (gradient sensitivity + weight distribution constraints) and a SKIP algorithm to enable efficient, undetectable attacks across Qwen, LLaMA, and Gemma models in BF16/INT8 formats.


<details>
  <summary>Details</summary>
Motivation: Existing bit-flip attacks on LLMs either limit attack flexibility by targeting specific data formats (integers/floating-point), cause non-stealthy parameter perturbations, or trigger numerical runtime errors. This creates a critical gap in practical, stealthy LLM security threats that require only minimal bit flips.

Method: SBFA combines (1). ImpactScore: a sensitivity metric integrating gradient sensitivity with layer-wise distribution constraints to prioritize stealthy bits; and (2). SKIP algorithm: a lightweight search method reducing complexity through incremental pruning and resampling, enabling efficient large-scale bit-flip discovery in tens of minutes.

Result: SBFA achieves: 1. >99% accuracy degradation to below random levels on MMLU/SST-2 with 1 bit flip; 2. Stealthiness by keeping perturbed values within benign weight distributions; 3. Success across Qwen, LLaMA, and Gemma in both BF16 and INT8 formats without triggering NaN/Inf errors.

Conclusion: Demonstrates severe security vulnerabilities in state-of-the-art LLMs, highlighting their vulnerability to minuscule, stealthy bit-flip manipulations undetectable by existing defenses. Urges development of new robustness mechanisms for deployed LLMs.

Abstract: Model integrity of Large language models (LLMs) has become a pressing
security concern with their massive online deployment. Prior Bit-Flip Attacks
(BFAs) -- a class of popular AI weight memory fault-injection techniques -- can
severely compromise Deep Neural Networks (DNNs): as few as tens of bit flips
can degrade accuracy toward random guessing. Recent studies extend BFAs to LLMs
and reveal that, despite the intuition of better robustness from modularity and
redundancy, only a handful of adversarial bit flips can also cause LLMs'
catastrophic accuracy degradation. However, existing BFA methods typically
focus on either integer or floating-point models separately, limiting attack
flexibility. Moreover, in floating-point models, random bit flips often cause
perturbed parameters to extreme values (e.g., flipping in exponent bit), making
it not stealthy and leading to numerical runtime error (e.g., invalid tensor
values (NaN/Inf)). In this work, for the first time, we propose SBFA (Sneaky
Bit-Flip Attack), which collapses LLM performance with only one single bit flip
while keeping perturbed values within benign layer-wise weight distribution. It
is achieved through iterative searching and ranking through our defined
parameter sensitivity metric, ImpactScore, which combines gradient sensitivity
and perturbation range constrained by the benign layer-wise weight
distribution. A novel lightweight SKIP searching algorithm is also proposed to
greatly reduce searching complexity, which leads to successful SBFA searching
taking only tens of minutes for SOTA LLMs. Across Qwen, LLaMA, and Gemma
models, with only one single bit flip, SBFA successfully degrades accuracy to
below random levels on MMLU and SST-2 in both BF16 and INT8 data formats.
Remarkably, flipping a single bit out of billions of parameters reveals a
severe security concern of SOTA LLM models.

</details>


### [19] [You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors](https://arxiv.org/abs/2509.21884)
*Bochuan Cao,Changjiang Li,Yuanpu Cao,Yameng Ge,Ting Wang,Jinghui Chen*

Main category: cs.CR

TL;DR: This paper presents a prompt leaking attack to extract system prompts from large language models (LLMs) and proposes SysVec, a method that encodes system prompts as vectors to enhance security and improve instruction following.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the growing concern for the security risks associated with system prompt leakage, particularly with the exposure of sensitive training data and proprietary information as LLMs become more integrated into applications. Current defenses are ineffective against new or hidden attack methods, highlighting the need for a robust solution.

Method: The paper introduces two key methods. First, a novel prompt leakage attack is presented to extract the system prompt and determine the presence of an LLM across various applications. Second, the authors propose SysVec, which encodes system prompts as internal representation vectors instead of using raw text, thereby reducing the risk of unauthorized disclosure while preserving the model's functionality.

Result: The results show that the proposed prompt leaking attack is numerous LLM-based applications and even on leading models like GPT-4o or Claude 3.5 Sonnet. Additionally, SysVec effectively mitigates prompt leakage attacks, maintains the LLM's functional integrity, and helps reduce the forgetting effect in long-context settings.

Conclusion: The paper concludes that current methodological approaches to secure system prompt for LLMs are insufficient against new attacks. The proposed SysVec represents a significant advancement, offering enhanced security by internalizing prompt representation while simultaneously improving the model's performance in handling long-context instructions.

Abstract: Large language models (LLMs) have been widely adopted across various
applications, leveraging customized system prompts for diverse tasks. Facing
potential system prompt leakage risks, model developers have implemented
strategies to prevent leakage, primarily by disabling LLMs from repeating their
context when encountering known attack patterns. However, it remains vulnerable
to new and unforeseen prompt-leaking techniques. In this paper, we first
introduce a simple yet effective prompt leaking attack to reveal such risks.
Our attack is capable of extracting system prompts from various LLM-based
application, even from SOTA LLM models such as GPT-4o or Claude 3.5 Sonnet. Our
findings further inspire us to search for a fundamental solution to the
problems by having no system prompt in the context. To this end, we propose
SysVec, a novel method that encodes system prompts as internal representation
vectors rather than raw text. By doing so, SysVec minimizes the risk of
unauthorized disclosure while preserving the LLM's core language capabilities.
Remarkably, this approach not only enhances security but also improves the
model's general instruction-following abilities. Experimental results
demonstrate that SysVec effectively mitigates prompt leakage attacks, preserves
the LLM's functional integrity, and helps alleviate the forgetting issue in
long-context scenarios.

</details>


### [20] [Eliminating Exponential Key Growth in PRG-Based Distributed Point Functions](https://arxiv.org/abs/2509.22022)
*Marc Damie,Florian Hahn,Andreas Peter,Jan Ramon*

Main category: cs.CR

TL;DR: The paper optimizes multi-party DPFs for better efficiency and practical key sizes.


<details>
  <summary>Details</summary>
Motivation: DPFs are important for privacy-preserving technologies, but multi-party PRG-based DPFs have had impractically large keys due to exponential growth with more parties and field size.

Method: They improved Boyle et al.'s PRG-based DPF by leveraging the honest-majority assumption to eliminate exponential factors.

Result: Their key sizes are up to 3x smaller than the best existing solutions, making multi-party DPFs more practical.

Conclusion: With optimization, PRG-based multi-party DPFs can now achieve practical and top performance.

Abstract: Distributed Point Functions (DPFs) enable sharing secret point functions
across multiple parties, supporting privacy-preserving technologies such as
Private Information Retrieval, and anonymous communications. While 2-party
PRG-based schemes with logarithmic key sizes have been known for a decade,
extending these solutions to multi-party settings has proven challenging. In
particular, PRG-based multi-party DPFs have historically struggled with
practicality due to key sizes growing exponentially with the number of parties
and the field size.
  Our work addresses this efficiency bottleneck by optimizing the PRG-based
multi-party DPF scheme of Boyle et al. (EUROCRYPT'15). By leveraging the
honest-majority assumption, we eliminate the exponential factor present in this
scheme. Our construction is the first PRG-based multi-party DPF scheme with
practical key sizes, and provides key up to 3x smaller than the best known
multi-party DPF. This work demonstrates that with careful optimization,
PRG-based multi-party DPFs can achieve practical performances, and even obtain
top performances.

</details>


### [21] ["Your AI, My Shell": Demystifying Prompt Injection Attacks on Agentic AI Coding Editors](https://arxiv.org/abs/2509.22040)
*Yue Liu,Yanjie Zhao,Yunbo Lyu,Ting Zhang,Haoyu Wang,David Lo*

Main category: cs.CR

TL;DR: This paper presents the first empirical analysis of prompt injection attacks against high-privilege agentic AI coding editors (e.g., Cursor and GitHub Copilot), revealing attackers can hijack AI agents to execute malicious commands with up to 84% success rates using the proposed AIShellJack framework.


<details>
  <summary>Details</summary>
Motivation: As agentic AI coding editors gain system privileges for complex tasks, they become critical yet vulnerable targets. Existing prompt injection research focuses on low-privilege systems, leaving high-privilege editor vulnerabilities unexplored despite their potential for system-level exploitation.

Method: The authors developed AIShellJack, an automated testing framework containing 314 attack payloads mapped to 70 MITRE ATT&CK techniques. They evaluated its effectiveness against Cursor and GitHub Copilot by injecting malicious prompts into external development resources to trigger AI agent behavior.

Result: 84% attack success rate in executing malicious commands across both editors. The attacks proved effective for multi-stage objectives including initial access, system discovery, credential theft, and data exfiltration, demonstrating real-world exploitability of these vulnerabilities.

Conclusion: Agentic AI coding editors are significantly vulnerable to prompt injection attacks, enabling attackers to fully hijack systems through poisoned resources. The findings highlight urgent security risks in emerging AI-first development tools and emphasize the need for robust input sanitization and attack surface reduction in high-privilege AI integration.

Abstract: Agentic AI coding editors driven by large language models have recently
become more popular due to their ability to improve developer productivity
during software development. Modern editors such as Cursor are designed not
just for code completion, but also with more system privileges for complex
coding tasks (e.g., run commands in the terminal, access development
environments, and interact with external systems). While this brings us closer
to the "fully automated programming" dream, it also raises new security
concerns. In this study, we present the first empirical analysis of prompt
injection attacks targeting these high-privilege agentic AI coding editors. We
show how attackers can remotely exploit these systems by poisoning external
development resources with malicious instructions, effectively hijacking AI
agents to run malicious commands, turning "your AI" into "attacker's shell". To
perform this analysis, we implement AIShellJack, an automated testing framework
for assessing prompt injection vulnerabilities in agentic AI coding editors.
AIShellJack contains 314 unique attack payloads that cover 70 techniques from
the MITRE ATT&CK framework. Using AIShellJack, we conduct a large-scale
evaluation on GitHub Copilot and Cursor, and our evaluation results show that
attack success rates can reach as high as 84% for executing malicious commands.
Moreover, these attacks are proven effective across a wide range of objectives,
ranging from initial access and system discovery to credential theft and data
exfiltration.

</details>


### [22] [NanoTag: Systems Support for Efficient Byte-Granular Overflow Detection on ARM MTE](https://arxiv.org/abs/2509.22027)
*Mingkai Li,Hang Ye,Joseph Devietti,Suman Jana,Tanvir Ahmed Khan*

Main category: cs.CR

TL;DR: The paper introduces NanoTag, a system that combines ARM MTE with tripwire mechanisms to detect memory safety bugs at byte granularity. It achieves ASAN-like precision while maintaining MTE's low overhead through a hybrid software-hardware approach.


<details>
  <summary>Details</summary>
Motivation: ARM MTE hardware detects memory bugs efficiently but lacks fine-grained precision due to its 16-byte granularity. Software-based ASAN offers better precision but with high overhead, creating a need for a balanced solution.

Method: NanoTag uses a tripwire strategy: it places special markers in memory regions requiring fine-grained checking. When access occurs beyond MTE's hardware detection (triggering the tripwire), software-level checks are performed for intra-granule overflows. This integrates MTE for general detection and targeted software checks for critical areas.

Result: NanoTag detected nearly as many bugs as ASAN across benchmarks and real-world applications, while maintaining runtime overhead comparable to the default Scudo Hardened Allocator in MTE SYNC mode.

Conclusion: NanoTag effectively bridges the precision gap in ARM MTE, offering a practical solution that matches software detectors' accuracy but with significantly lower overhead, enabling robust memory safety in real-world deployments.

Abstract: Memory safety bugs, such as buffer overflows and use-after-frees, are the
leading causes of software safety issues in production. Software-based
approaches, e.g., Address Sanitizer (ASAN), can detect such bugs with high
precision, but with prohibitively high overhead. ARM's Memory Tagging Extension
(MTE) offers a promising alternative to detect these bugs in hardware with a
much lower overhead. However, in this paper, we perform a thorough
investigation of Google Pixel 8, the first production implementation of ARM
MTE, and show that MTE can only achieve coarse precision in bug detection
compared with software-based approaches such as ASAN, mainly due to its 16-byte
tag granularity. To address this issue, we present NanoTag, a system to detect
memory safety bugs in unmodified binaries at byte granularity with ARM MTE.
NanoTag detects intra-granule buffer overflows by setting up a tripwire for tag
granules that may require intra-granule overflow detection. The memory access
to the tripwire causes additional overflow detection in the software while
using MTE's hardware to detect bugs for the rest of the accesses. We implement
NanoTag based on the Scudo Hardened Allocator, the default memory allocator on
Android since Android 11. Our evaluation results across popular benchmarks and
real-world case studies show that NanoTag detects nearly as many memory safety
bugs as ASAN while incurring similar run-time overhead to Scudo Hardened
Allocator in MTE SYNC mode.

</details>


### [23] [Guidance Watermarking for Diffusion Models](https://arxiv.org/abs/2509.22126)
*Enoal Gesny,Eva Giboulot,Teddy Furon,Vivien Chappelier*

Main category: cs.CR

TL;DR: A gradient-guided watermarking method for diffusion models enhances robustness against attacks without retraining, converting post-hoc schemes into in-generation embedding.


<details>
  <summary>Details</summary>
Motivation: Post-hoc watermarking lacks attack robustness; existing in-generation methods require model modifications or retraining, necessitating a flexible, robust solution.

Method: Uses gradients from a watermark decoder (with augmentation-aware computation) during diffusion process to embed watermarks, preserving diffusion dynamics and compatibility with any off-the-shelf detector.

Result: Validated across models/detectors: maintains image quality/diversity while resisting attacks beyond decoder's original robustness scope, including those not explicitly trained against.

Conclusion: Gradient-guided approach provides attack-resilient, flexible watermarking compatible with both post-hoc detectors and existing VAE-based methods; eliminates need for fine-tuning.

Abstract: This paper introduces a novel watermarking method for diffusion models. It is
based on guiding the diffusion process using the gradient computed from any
off-the-shelf watermark decoder. The gradient computation encompasses different
image augmentations, increasing robustness to attacks against which the decoder
was not originally robust, without retraining or fine-tuning. Our method
effectively convert any \textit{post-hoc} watermarking scheme into an
in-generation embedding along the diffusion process. We show that this approach
is complementary to watermarking techniques modifying the variational
autoencoder at the end of the diffusion process. We validate the methods on
different diffusion models and detectors. The watermarking guidance does not
significantly alter the generated image for a given seed and prompt, preserving
both the diversity and quality of generation.

</details>


### [24] [The Express Lane to Spam and Centralization: An Empirical Analysis of Arbitrum's Timeboost](https://arxiv.org/abs/2509.22143)
*Johnnatan Messias,Christof Ferreira Torres*

Main category: cs.CR

TL;DR: This paper evaluates Arbitrum's Timeboost auction-based MEV mitigation system using 11.5 million transactions and 151 thousand auctions, concluding it fails to achieve fairness, decentralization, or spam reduction while entrenching centralization.


<details>
  <summary>Details</summary>
Motivation: DeFi MEV extraction threatens transaction fairness, yet auction-based mechanisms like Timeboost lack verification of whether they achieve their stated goals of decentralizing MEV revenue and reducing spam.

Method: Large-scale empirical analysis of over 11.5 million express lane transactions and 151 thousand auctions across April-July 2025, examining key metrics like bid patterns, block inclusion dynamics, transaction reversion rates, and auction competitiveness.

Result: 1.92.67% of express lane auctions won by top two entities. 2.MEV remains clustered at block ends despite priority access. 3.22.43%% time-boosted transaction reversion rate. 4.Secondary markets for express lane rights failed economically. 5.Auction competition decreasing over time, reducing DAO revenue by 38.7%.html

Conclusion: Auction-based transaction ordering fails to deliver on decentralization, MEV mitigation, or spam protection goals. Timeboost entrenches centralization, enables spam, and features deteriorating economics, highlighting fundamental flaws in market-based transaction sequencing as a decentralization mechanism.

Abstract: DeFi applications are vulnerable to MEV, where specialized actors profit by
reordering or inserting transactions. To mitigate latency races and internalize
MEV revenue, Arbitrum introduced Timeboost, an auction-based transaction
sequencing mechanism that grants short-term priority access to an express lane.
In this paper we present the first large-scale empirical study of Timeboost,
analyzing over 11.5 million express lane transactions and 151 thousand auctions
between April and July 2025. Our results reveal five main findings. First,
express lane control is highly centralized, with two entities winning more than
90% of auctions. Second, while express lane access provides earlier inclusion,
profitable MEV opportunities cluster at the end of blocks, limiting the value
of priority access. Third, approximately 22% of time-boosted transactions are
reverted, indicating that the Timeboost does not effectively mitigate spam.
Fourth, secondary markets for reselling express lane rights have collapsed due
to poor execution reliability and unsustainable economics. Finally, auction
competition declined over time, leading to steadily reduced revenue for the
Arbitrum DAO. Taken together, these findings show that Timeboost fails to
deliver on its stated goals of fairness, decentralization, and spam reduction.
Instead, it reinforces centralization and narrows adoption, highlighting the
limitations of auction-based ordering as a mechanism for fair transaction
sequencing in rollups.

</details>


### [25] [Collusion-Driven Impersonation Attack on Channel-Resistant RF Fingerprinting](https://arxiv.org/abs/2509.22154)
*Zhou Xu,Guyue Li,Zhe Peng,Aiqun Hu*

Main category: cs.CR

TL;DR: This paper proposes a collusion-driven RF-level mimicry attack to break RFF identification systems by exploiting CLPS feature similarities through a VAE-based signal generation method, achieving 95+% success rates across channel variations.


<details>
  <summary>Details</summary>
Motivation: Existing RFF security research focuses on basic spoofing (e.g., MAC tampering), but advanced mimicry resilience remains unexplored, leaving critical vulnerabilities unaddressed.

Method: The attack synthesizes colluding receiver CLPS matching with a VAE network using multi-objective loss functions to generate deceptive signals, tested under AWGN, fading, and Doppler conditions.

Result: Simulation results demonstrate 95+% success rates in cross-channel attacks across diverse environments, validating the method's effectiveness against RFF systems.

Conclusion: The proposed attack reveals significant vulnerabilities in RFF security against advanced mimicry, necessitating improved defensive strategies against distributed spoofing attacks.

Abstract: Radio frequency fingerprint (RFF) is a promising device identification
technology, with recent research shifting from robustness to security due to
growing concerns over vulnerabilities. To date, while the security of RFF
against basic spoofing such as MAC address tampering has been validated, its
resilience to advanced mimicry remains unknown. To address this gap, we propose
a collusion-driven impersonation attack that achieves RF-level mimicry,
successfully breaking RFF identification systems across diverse environments.
Specifically, the attacker synchronizes with a colluding receiver to match the
centralized logarithmic power spectrum (CLPS) of the legitimate transmitter;
once the colluder deems the CLPS identical, the victim receiver will also
accept the forged fingerprint, completing RF-level spoofing. Given that the
distribution of CLPS features is relatively concentrated and has a clear
underlying structure, we design a spoofed signal generation network that
integrates a variational autoencoder (VAE) with a multi-objective loss function
to enhance the similarity and deceptive capability of the generated samples. We
carry out extensive simulations, validating cross-channel attacks in
environments that incorporate standard channel variations including additive
white Gaussian noise (AWGN), multipath fading, and Doppler shift. The results
indicate that the proposed attack scheme essentially maintains a success rate
of over 95% under different channel conditions, revealing the effectiveness of
this attack.

</details>


### [26] [Accuracy-First Rényi Differential Privacy and Post-Processing Immunity](https://arxiv.org/abs/2509.22213)
*Ossi Räisä,Antti Koskela,Antti Honkela*

Main category: cs.CR

TL;DR: The paper addresses gaps in the accuracy-first differential privacy approach by proving post-processing immunity for certain definitions, proposing a new Rényi DP-based definition with practical tools, and demonstrating its effectiveness in synthetic data generation.


<details>
  <summary>Details</summary>
Motivation: Existing accuracy-first differential privacy methods ignore post-processing immunity, a critical property that prevents adversaries from weakening privacy guarantees through post-processing. This jeopardizes privacy integrity in real-world applications.

Method: 1) Analyze post-processing immunity of existing accuracy-first definitions
2.Propose a new Rényi DP-based definition maintaining post-processing immunity
3.Develop practical tools including an ex-post Gaussian mechanism analogy and accuracy validation algorithms
4.Empirically test on synthetic data generation tasks

Result: Show new definition retains post-processing immunity while enabling practical tools. Demonstrate successful privacy bound adaptation in synthetic data generation until validation accuracy thresholds are met.

Conclusion: The proposed Rényi DP-based approach fills critical gaps in accuracy-first differential privacy, combining theoretical robustness with practical utility advantages for real-world deployment.

Abstract: The accuracy-first perspective of differential privacy addresses an important
shortcoming by allowing a data analyst to adaptively adjust the quantitative
privacy bound instead of sticking to a predetermined bound. Existing works on
the accuracy-first perspective have neglected an important property of
differential privacy known as post-processing immunity, which ensures that an
adversary is not able to weaken the privacy guarantee by post-processing. We
address this gap by determining which existing definitions in the
accuracy-first perspective have post-processing immunity, and which do not. The
only definition with post-processing immunity, pure ex-post privacy, lacks
useful tools for practical problems, such as an ex-post analogue of the
Gaussian mechanism, and an algorithm to check if accuracy on separate private
validation set is high enough. To address this, we propose a new definition
based on R\'enyi differential privacy that has post-processing immunity, and we
develop basic theory and tools needed for practical applications. We
demonstrate the practicality of our theory with an application to synthetic
data generation, where our algorithm successfully adjusts the privacy bound
until an accuracy threshold is met on a private validation dataset.

</details>


### [27] [Learn, Check, Test -- Security Testing Using Automata Learning and Model Checking](https://arxiv.org/abs/2509.22215)
*Stefan Marksteiner,Mikael Sjödin,Marjan Sirjani*

Main category: cs.CR

TL;DR: This paper proposes a method to create formal models for cyber-physical systems using active black box learning, which are then translated into model checker-compatible formats to verify security properties.


<details>
  <summary>Details</summary>
Motivation: Cyber-physical systems are integral to industrial systems and critical infrastructure, necessitating systematic and automated verification of their correctness and security. Traditional methods are insufficient when system internals are inaccessible, which common in real-world scenarios due to black box constraints.

Method: The authors employ active black box learning techniques to infer behavioral models of cyber-physical systems as annotated Mealy machines. These models are annotated using Context-based Proposition Maps (CPMs) to relate protocol context information to the model. A template is then defined to convert the Mealy machines into a format compatible with model checking tools.

Result: The generated annotated Mealy machines are converted into model checker-compatible structures, enabling verification of generic security properties with protocol-specific context through CPMs. The flexibility of the framework allows for introducing non-deterministic behaviors or faults for further analysis.

Conclusion: The approach demonstrates its effectiveness and versatility through case studies on communication protocols like NFC and UDS, using a unified tool chain and a consistent set of security properties for verification.

Abstract: Cyber-physical systems are part of industrial systems and critical
infrastructure. Therefore, they should be examined in a comprehensive manner to
verify their correctness and security. At the same time, the complexity of such
systems demands such examinations to be systematic and, if possible, automated
for efficiency and accuracy. A method that can be useful in this context is
model checking. However, this requires a model that faithfully represents the
behavior of the examined system. Obtaining such a model is not trivial, as many
of these systems can be examined only in black box settings due to, e.g., long
supply chains or secrecy. We therefore utilize active black box learning
techniques to infer behavioral models in the form of Mealy machines of such
systems and translate them into a form that can be evaluated using a model
checker. To this end, we will investigate a cyber-physical systems as a black
box using its external communication interface. We first annotate the model
with propositions by mapping context information from the respective protocol
to the model using Context-based Proposition Maps (CPMs). We gain annotated
Mealy machines that resemble Kripke structures. We then formally define a
template, to transfer the structures model checker-compatible format. We
further define generic security properties based on basic security
requirements. Due to the used CPMs, we can instantiate these properties with a
meaningful context to check a specific protocol, which makes the approach
flexible and scalable. The gained model can be easily altered to introduce
non-deterministic behavior (like timeouts) or faults and examined if the
properties still. Lastly, we demonstrate the versatility of the approach by
providing case studies of different communication protocols (NFC and UDS),
checked with the same tool chain and the same security properties.

</details>


### [28] [Secure and Efficient Access Control for Computer-Use Agents via Context Space](https://arxiv.org/abs/2509.22256)
*Haochen Gong,Chenxiao Li,Rui Chang,Wenbo Shen*

Main category: cs.CR

TL;DR: CSAgent, a static policy-based access control framework for LLM-based computer-use agents, addresses security risks through intent-aware policies, achieving 99.36 attack defense with 6.83 performance overhead.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents pose security risks due to action uncertainty, with existing solutions lacking in usability, security, and performance.

Method: CSAgent introduces intent- and context-aware policies enforced by an optimized OS service, supporting API/CLI/GUI interfaces, and includes an automated toolchain for policy development refinement.

Result: 99.36% attack defense rate achieved, with <6.83% performance overhead across diverse agent interaction methods.

Conclusion: CSAgent provides effective security for LLM agents through policy enforcement while addressing dynamic context limitations of prior work.

Abstract: Large language model (LLM)-based computer-use agents represent a convergence
of AI and OS capabilities, enabling natural language to control system- and
application-level functions. However, due to LLMs' inherent uncertainty issues,
granting agents control over computers poses significant security risks. When
agent actions deviate from user intentions, they can cause irreversible
consequences. Existing mitigation approaches, such as user confirmation and
LLM-based dynamic action validation, still suffer from limitations in
usability, security, and performance. To address these challenges, we propose
CSAgent, a system-level, static policy-based access control framework for
computer-use agents. To bridge the gap between static policy and dynamic
context and user intent, CSAgent introduces intent- and context-aware policies,
and provides an automated toolchain to assist developers in constructing and
refining them. CSAgent enforces these policies through an optimized OS service,
ensuring that agent actions can only be executed under specific user intents
and contexts. CSAgent supports protecting agents that control computers through
diverse interfaces, including API, CLI, and GUI. We implement and evaluate
CSAgent, which successfully defends against more than 99.36% of attacks while
introducing only 6.83% performance overhead.

</details>


### [29] [A Global Analysis of Cyber Threats to the Energy Sector: "Currents of Conflict" from a Geopolitical Perspective](https://arxiv.org/abs/2509.22280)
*Gustavo Sánchez,Ghada Elbez,Veit Hagenmeyer*

Main category: cs.CR

TL;DR: This paper examines cyber threats in the energy sector using generative AI for threat analysis, geopolitical comparisons of threat actors, and evaluation of learning-based cybersecurity tools.


<details>
  <summary>Details</summary>
Motivation: The growing sophistication of cyber threats, particularly in critical domains like energy, necessitates advanced analytical frameworks combining geopolitical insights and technical detection capabilities to address existing knowledge gaps.

Method: 1) Generative AI extracts structured information from raw cyber threat data. 2.1 Geopolitical analysis compares threat actor origins and targets across databases. 3. Evaluation of learning-based cybersecurity tools for detecting energy-sector attack indicators.

Result: Identified emerging threat trends, validated effectiveness of machine learning approaches in energy threat detection, and mapped geopolitical patterns in cyber attack attribution.

Conclusion: The integrated approach enhances understanding of energy-targeted cyber threats through geopolitical-technical analysis, offering actionable intelligence for multi-stakeholder decision-making in cybersecurity strategy.

Abstract: The escalating frequency and sophistication of cyber threats increased the
need for their comprehensive understanding. This paper explores the
intersection of geopolitical dynamics, cyber threat intelligence analysis, and
advanced detection technologies, with a focus on the energy domain. We leverage
generative artificial intelligence to extract and structure information from
raw cyber threat descriptions, enabling enhanced analysis. By conducting a
geopolitical comparison of threat actor origins and target regions across
multiple databases, we provide insights into trends within the general threat
landscape. Additionally, we evaluate the effectiveness of cybersecurity tools
-- with particular emphasis on learning-based techniques -- in detecting
indicators of compromise for energy-targeted attacks. This analysis yields new
insights, providing actionable information to researchers, policy makers, and
cybersecurity professionals.

</details>


### [30] [Privacy Mechanism Design based on Empirical Distributions](https://arxiv.org/abs/2509.22428)
*Leonhard Grosse,Sara Saeidian,Mikael Skoglund,Tobias J. Oechtering*

Main category: cs.CR

TL;DR: This paper proposes a framework for pointwise maximal leakage (PML) privacy assessment and mechanism design under uncertain data distributions. It extends PML to handle distribution ambiguity, provides distribution-independent guarantees via large-deviation bounds, and demonstrates utility improvements over local differential privacy through optimal mechanism design.


<details>
  <summary>Details</summary>
Motivation: Traditional PML relies on precise knowledge of the data-generating distribution. However, real-world applications require empirical estimates, introducing uncertainty. The paper addresses how to provide robust privacy guarantees when the distribution is imprecisely known.

Method: The approach extends PML to account for sets of data-generating distributions, derives worst-case leakage bounds, and integrates large-deviation bounds to form distribution-independent $(\varepsilon,\delta)$-PML guarantees. Mechanism design with distributional uncertainty is reduced to a linearly constrained convex program.

Result: An optimal binary mechanism is proposed, validated through theoretical analysis and numerical experiments showing up to significant utility gains compared to local differential privacy for Laplace and Gaussian mechanisms for binary data.

Conclusion: The framework enables robust PML privacy assessment under distributional uncertainty, offering both strong privacy guarantees and improved utility. The approach bridges the gap between theoretical PML and practical implementation with empirical data.

Abstract: Pointwise maximal leakage (PML) is a per-outcome privacy measure based on
threat models from quantitative information flow. Privacy guarantees with PML
rely on knowledge about the distribution that generated the private data. In
this work, we propose a framework for PML privacy assessment and mechanism
design with empirical estimates of this data-generating distribution. By
extending the PML framework to consider sets of data-generating distributions,
we arrive at bounds on the worst-case leakage within a given set. We use these
bounds alongside large-deviation bounds from the literature to provide a method
for obtaining distribution-independent $(\varepsilon,\delta)$-PML guarantees
when the data-generating distribution is estimated from available data samples.
We provide an optimal binary mechanism, and show that mechanism design with
this type of uncertainty about the data-generating distribution reduces to a
linearly constrained convex program. Further, we show that optimal mechanisms
designed for a distribution estimate can be used. Finally, we apply these tools
to leakage assessment of the Laplace mechanism and the Gaussian mechanism for
binary private data, and numerically show that the presented approach to
mechanism design can yield significant utility increase compared to local
differential privacy, while retaining similar privacy guarantees.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [31] [Extracting Conceptual Knowledge to Locate Software Issues](https://arxiv.org/abs/2509.21427)
*Ying Wang,Wenjun Mao,Chong Wang,Zhenhao Zhou,Yicheng Zhou,Wenyun Zhao,Yiling Lou,Xin Peng*

Main category: cs.SE

TL;DR: RepoLens improves LLM-based bug localization in large repositories by abstracting code into high-level concerns, overcoming issues of logic dispersion and complexity. It achieves significant gains in accuracy across multiple tools and models.


<details>
  <summary>Details</summary>
Motivation: Existing LLM and LLM-agent approaches struggle with large-scale repositories due to concern mixing (buried logic in large functions/complex code) and scattering (分散 logic across files), leading to poor localization accuracy.

Method: RepoLens extracts and leverages conceptual knowledge from code in two stages: 1. Offline - Documents repository-wide knowledge base by decomposing functionalities into semantic clusters ('concerns'). 2. Online - Retrieves relevant terms, ranks concerns by relevance, and integrates insights into workflows via enhanced prompts without major model modifications.

Result: Improves state-of-the-art tools (AgentLess, OpenHands, mini-SWE-agent) by +22-46%, generalizes across GPT-4o variants (up to 504% Hit@1, 376% Recall@10 gains). Ablation studies and manual validation confirm effectiveness.

Conclusion: RepoLens demonstrates that structuring code knowledge into semantic concerns significantly enhances LLM-based bug localization in complex repositories, providing scalable and reliable improvements across models and tools.

Abstract: Issue localization, which identifies faulty code elements such as files or
functions, is critical for effective bug fixing. While recent LLM-based and
LLM-agent-based approaches improve accuracy, they struggle in large-scale
repositories due to concern mixing, where relevant logic is buried in large
functions, and concern scattering, where related logic is dispersed across
files.
  To address these challenges, we propose RepoLens, a novel approach that
abstracts and leverages conceptual knowledge from code repositories. RepoLens
decomposes fine-grained functionalities and recomposes them into high-level
concerns, semantically coherent clusters of functionalities that guide LLMs. It
operates in two stages: an offline stage that extracts and enriches conceptual
knowledge into a repository-wide knowledge base, and an online stage that
retrieves issue-specific terms, clusters and ranks concerns by relevance, and
integrates them into localization workflows via minimally intrusive prompt
enhancements. We evaluate RepoLens on SWE-Lancer-Loc, a benchmark of 216 tasks
derived from SWE-Lancer. RepoLens consistently improves three state-of-the-art
tools, namely AgentLess, OpenHands, and mini-SWE-agent, achieving average gains
of over 22% in Hit@k and 46% in Recall@k for file- and function-level
localization. It generalizes across models (GPT-4o, GPT-4o-mini, GPT-4.1) with
Hit@1 and Recall@10 gains up to 504% and 376%, respectively. Ablation studies
and manual evaluation confirm the effectiveness and reliability of the
constructed concerns.

</details>


### [32] [Lost in Transition: The Struggle of Women Returning to Software Engineering Research after Career Breaks](https://arxiv.org/abs/2509.21533)
*Shalini Chakraborty,Sebastian Baltes*

Main category: cs.SE

TL;DR: The paper proposes a multicultural research project to investigate challenges faced by women in software engineering returning to academia after career breaks, comparing academia's limited support to industry initiatives like returnship programs, and offers recommendations for transparent hiring practices.


<details>
  <summary>Details</summary>
Motivation: Women with software engineering backgrounds experience career disruptions (e.g., pregnancy, immigration, inflexible work) that hinder their re-entry into academia, yet academic institutions lack visible gender diversity policies compared to industry sectors.

Method: The study will analyze challenges and institutional policies across multiple universities and countries through comparative research, examining differences in support structures, existing return-to-academia opportunities, and regional policy variations.

Result: Expected outcomes include insights into challenges women face in academia versus industry, a comparative analysis of country-specific policies, and evidence-based hiring practice recommendations for universities to retain/employ returning women researchers.

Conclusion: The research aims to bridge the support gap by identifying systemic barriers and proposing actionable solutions for academic institutions to adopt flexible, transparent, and inclusive re-entry frameworks

Abstract: The IT industry provides supportive pathways such as returnship programs,
coding boot camps, and buddy systems for women re-entering their job after a
career break. Academia, however, offers limited opportunities to motivate women
to return. We propose a diverse multicultural research project investigating
the challenges faced by women with software engineering (SE) backgrounds
re-entering academia or related research roles after a career break. Career
disruptions due to pregnancy, immigration status, or lack of flexible work
options can significantly impact women's career progress, creating barriers for
returning as lecturers, professors, or senior researchers. Although many
companies promote gender diversity policies, such measures are less prominent
and often under-recognized within academic institutions. Our goal is to explore
the specific challenges women encounter when re-entering academic roles
compared to industry roles; to understand the institutional perspective,
including a comparative analysis of existing policies and opportunities in
different countries for women to return to the field; and finally, to provide
recommendations that support transparent hiring practices. The research project
will be carried out in multiple universities and in multiple countries to
capture the diverse challenges and policies that vary by location.

</details>


### [33] [No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials](https://arxiv.org/abs/2509.21816)
*Yuhang Xie,Jian Mu,Xiaojun Ma,Chaoyun Zhang,Lu Wang,Mengyu Zhou,Mugeng Liu,Si Qin,Qingwei Lin,Saravan Rajmohan,Shi Han,Dongmei Zhang*

Main category: cs.SE

TL;DR: The paper presents the first automated framework for generating Excel tutorials from natural language tasks, improving success rates and reducing time costs significantly.


<details>
  <summary>Details</summary>
Motivation: Despite Excel's widespread use, current tutorials are manually created, costly, and outdated after software updates, leading to a need for automated generation. Existing methods rely on human-crafted sequences or materials, which still require significant input.

Method: The framework includes an Execution Agent that automatically plans and executes Excel solutions, collecting intermediate artifacts for tutorials. These artifacts are then transformed into Excel documents and video demos. A tutorial corpus of 1,559 real-world tasks was used for testing, with evaluations from both LLMs and human experts.

Result: The framework outperforms state-of-the-art baselines by 8.5% in task execution success rates. The generated tutorials exhibit high readability and effectiveness, rivaling or exceeding expert-created ones. The automation reduces time costs by a factor of 20.

Conclusion: The paper concludes that the framework enables scalable and high-quality tutorial generation for Excel, significantly improving efficiency and reducing reliance on manual labor.

Abstract: Excel is one of the most widely used productivity tools across domains,
offering rich functionality but also overwhelming users with its complexity.
This creates a persistent demand for tutorials to support effective usage.
However, existing tutorials are manually authored by experts, require frequent
updates after each software release, and incur substantial labor costs. Prior
work has not achieved fully automated tutorial generation, since existing
methods still depend on handcrafted operation sequences or example materials.
In this paper, we present the first framework for automatically generating
Excel tutorials directly from natural language task descriptions. Our framework
first instantiates the task. Then a central component of this framework,
Execution Agent, plans and executes the solution in Excel, and collects the
intermediate artifacts required for tutorial construction. These artifacts are
then transformed into both structured Excel documents and video demonstrations.
To build a comprehensive tutorial corpus, we collected 1,559 task descriptions
from real-world scenarios. In addition, we designed a systematic evaluation
framework that integrates assessments from both large language models (LLMs)
and human reviewers. Experimental results show that our framework improves task
execution success rates by 8.5% over state-of-the-art baselines. Moreover, the
generated tutorials demonstrate superior readability and instructional
effectiveness, often approaching or surpassing expert-authored materials.
Importantly, the automated pipeline eliminates manual labor and reduces time
costs to 1/20 of expert authoring, making scalable and high-quality tutorial
generation practical for the first time.

</details>


### [34] [Software Engineering Data Analytics: A Framework Based on a Multi-Layered Abstraction Mechanism](https://arxiv.org/abs/2509.21881)
*Chaman Wijesiriwardana,Prasad Wimalaratne*

Main category: cs.SE

TL;DR: A domain-specific framework for software analytics that enables querying, modeling, and integrating diverse software repositories is proposed and evaluated through a case study.


<details>
  <summary>Details</summary>
Motivation: There is a need for frameworks that can effectively handle the querying, modeling, and integration of heterogeneous software repositories to improve software analytics.

Method: The authors developed a domain-specific framework that includes a multi-layered abstraction mechanism with domain-specific operators to facilitate software analytics tasks.

Result: The case study demonstrated the effectiveness and potential of the proposed domain-specific framework for querying, modeling, and integrating heterogeneous software repositories.

Conclusion: The domain-specific framework for software analytics shows promise in enhancing the ability to work with diverse software repositories, and future work may expand its capabilities and real-world applications.

Abstract: This paper presents a concept of a domain-specific framework for software
analytics by enabling querying, modeling, and integration of heterogeneous
software repositories. The framework adheres to a multi-layered abstraction
mechanism that consists of domain-specific operators. We showcased the
potential of this approach by employing a case study.

</details>


### [35] [AgentPack: A Dataset of Code Changes, Co-Authored by Agents and Humans](https://arxiv.org/abs/2509.21891)
*Yangtian Zi,Zixuan Wu,Aleksander Boruch-Gruszecki,Jonathan Bell,Arjun Guha*

Main category: cs.SE

TL;DR: The paper introduces AgentPack, a curated corpus of 1.3M code edits from code editing agents with cleaner data than past human-only commit data. It finds that agent co-authoring leads to more focused edits with detailed commit messages and shows models trained on AgentPack outperform prior models.


<details>
  <summary>Details</summary>
Motivation: The paper's motivation is to address the previously noisy and low-quality data in code editing fine-tuning by leveraging high-quality edits generated by software engineering agents, which result in cleaner, well-scoped commits with more detailed context.

Method: The method includes a curation pipeline for filtering and cleaning agent-generated code changes, quantitative analysis of adoption trends for specific agents across public GitHub projects, and evaluation of model performance comparing fine-tuning on AgentPack versus human-only data.

Result: Results show that models fine-tuned on AgentPack outperform those trained on prior human-only commit datasets, demonstrating the benefits of agent-generated, human-verified code edits for improving code-editing model accuracy.

Conclusion: The conclusion emphasizes the potential of using public data from software engineering agents for training code-editing models and highlights that AgentPack provides a cleaner and more informative dataset for such training, validated by improved performance over human-only data.

Abstract: Fine-tuning large language models for code editing has typically relied on
mining commits and pull requests. The working hypothesis has been that commit
messages describe human intent in natural language, and patches to code
describe the changes that implement that intent. However, much of the
previously collected data is noisy: commit messages are terse, human-written
commits commingle several unrelated edits, and many commits come from simple,
rule-based bots.
  The recent adoption of software engineering agents changes this landscape.
Code changes co-authored by humans and agents tend to be more narrowly scoped
and focused on clearer goals. Their commit messages, generated by LLMs,
articulate intent and rationale in much greater detail. Moreover, when these
changes land in public repositories, they are implicitly filtered by humans:
maintainers discard low-quality commits to their projects.
  We present AgentPack, a corpus of 1.3M code edits co-authored by Claude Code,
OpenAI Codex, and Cursor Agent across public GitHub projects up to mid-August
2025. We describe the identification and curation pipeline, quantify adoption
trends of these agents, and analyze the structural properties of the edits.
Finally, we show that models fine-tuned on AgentPack can outperform models
trained on prior human-only commit corpora, highlighting the potential of using
public data from software engineering agents to train future code-editing
models.

</details>


### [36] [Unveiling Many Faces of Surrogate Models for Configuration Tuning: A Fitness Landscape Analysis Perspective](https://arxiv.org/abs/2509.21945)
*Pengzhou Chen,Hongyuan Liang,Tao Chen*

Main category: cs.SE

TL;DR: This paper challenges the assumption that accurate surrogate models are optimal for configuration tuning, proposing Model4Tune—a tool to predict effective model-tuner pairs using fitness landscape analysis. It demonstrates Model4Tune outperforms random guessing in 79–82\% of cases through an extensive empirical study (27,000 cases).


<details>
  <summary>Details</summary>
Motivation: Prior work found model accuracy alone does not guarantee effective tuning, leaving unanswered questions about surrogate models' role. The study aims to address this gap by redefining model usefulness beyond accuracy and providing a practical solution for practitioners.

Method: The paper introduces a theory-based framework of fitness landscape analysis, conducts an empirical study across 27,000 scenarios, and develops Model4Tune, which automates model-tuner pairing selection without requiring costly tuner profiling.

Result: Model4Tune achieves 79–82\% improvement over random selection in identifying effective model-tuner pairs, validating the proposed theory and empirical framework. The study reveals surrogate models' performance is decoupled from accuracy, with landscape analysis as a better indicator.

Conclusion: The work reframes surrogate model evaluation in configuration tuning, offering both theoretical insights and a practical tool. Model4Tune marks a first step toward automating model selection, with implications for future research in model-tuner dynamics and system optimization.

Abstract: To efficiently tune configuration for better system performance (e.g.,
latency), many tuners have leveraged a surrogate model to expedite the process
instead of solely relying on the profoundly expensive system measurement. As
such, it is naturally believed that we need more accurate models. However, the
fact of accuracy can lie-a somewhat surprising finding from prior work-has left
us many unanswered questions regarding what role the surrogate model plays in
configuration tuning. This paper provides the very first systematic exploration
and discussion, together with a resolution proposal, to disclose the many faces
of surrogate models for configuration tuning, through the novel perspective of
fitness landscape analysis. We present a theory as an alternative to accuracy
for assessing the model usefulness in tuning, based on which we conduct an
extensive empirical study involving up to 27,000 cases. Drawing on the above,
we propose Model4Tune, an automated predictive tool that estimates which
model-tuner pairs are the best for an unforeseen system without expensive tuner
profiling. Our results suggest that Moldel4Tune, as one of the first of its
kind, performs significantly better than random guessing in 79%-82% of the
cases. Our results not only shed light on the possible future research
directions but also offer a practical resolution that can assist practitioners
in evaluating the most useful model for configuration tuning.

</details>


### [37] [SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios](https://arxiv.org/abs/2509.22097)
*Junkai Chen,Huihui Huang,Yunbo Lyu,Junwen An,Jieke Shi,Chengran Yang,Ting Zhang,Haoye Tian,Yikun Li,Zhenhao Li,Xin Zhou,Xing Hu,David Lo*

Main category: cs.SE

TL;DR: TL;DR: The paper introduces SecureAgentBench, a new benchmark for evaluating code agents on secure code generation through multi-file edits and real-world vulnerabilities, revealing that current agents struggle to produce both correct and secure code despite state-of-the-art LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for code agents often neglect the realistic context of vulnerabilities and use narrow evaluation protocols that do not adequately assess functional correctness or the introduction of new vulnerabilities.

Method: The paper introduces SecureAgentBench, a benchmark with 105 coding tasks: (i) realistic settings requiring multi-file edits in large repositories, (ii) contexts based on real-world open-source vulnerabilities, and (iii) a comprehensive evaluation combining functionality testing, vulnerability checking via exploits, and static analysis for new vulnerabilities. They evaluate three agents (SWE-agent, OpenHands, Aider) with three state-of-the-art LLMs (Claude 3.7, GPT-4.1, DeepSeek-V3).

Result: Results show that even the best agent (SWE-agent + DeepSeek-V3) has only 15.2% correct-and-secure solutions. Some agents produce correct code but introduce new vulnerabilities. Adding security instructions doesn't significantly improve secure coding outcomes.

Conclusion: SecureAgentBench provides a rigorous evaluation for secure code generation. The low performance of current agents indicates that further research is needed to ensure secure and correct LLM-based code generation for more reliable software development.

Abstract: Large language model (LLM) powered code agents are rapidly transforming
software engineering by automating tasks such as testing, debugging, and
repairing, yet the security risks of their generated code have become a
critical concern. Existing benchmarks have offered valuable insights but remain
insufficient: they often overlook the genuine context in which vulnerabilities
were introduced or adopt narrow evaluation protocols that fail to capture
either functional correctness or newly introduced vulnerabilities. We therefore
introduce SecureAgentBench, a benchmark of 105 coding tasks designed to
rigorously evaluate code agents' capabilities in secure code generation. Each
task includes (i) realistic task settings that require multi-file edits in
large repositories, (ii) aligned contexts based on real-world open-source
vulnerabilities with precisely identified introduction points, and (iii)
comprehensive evaluation that combines functionality testing, vulnerability
checking through proof-of-concept exploits, and detection of newly introduced
vulnerabilities using static analysis. We evaluate three representative agents
(SWE-agent, OpenHands, and Aider) with three state-of-the-art LLMs (Claude 3.7
Sonnet, GPT-4.1, and DeepSeek-V3.1). Results show that (i) current agents
struggle to produce secure code, as even the best-performing one, SWE-agent
supported by DeepSeek-V3.1, achieves merely 15.2% correct-and-secure solutions,
(ii) some agents produce functionally correct code but still introduce
vulnerabilities, including new ones not previously recorded, and (iii) adding
explicit security instructions for agents does not significantly improve secure
coding, underscoring the need for further research. These findings establish
SecureAgentBench as a rigorous benchmark for secure code generation and a step
toward more reliable software development with LLMs.

</details>


### [38] [SK2Decompile: LLM-based Two-Phase Binary Decompilation from Skeleton to Skin](https://arxiv.org/abs/2509.22114)
*Hanzhuo Tan,Weihao Li,Xiaolong Tian,Siyi Wang,Jiaming Liu,Jing Li,Yuqun Zhang*

Main category: cs.SE

TL;DR: This paper proposes SK2Decompile, a two-phase LLM-based decompiler that first reconstructs program structure (skeleton) via IR translation with reinforcement learning, then generates meaningful identifiers (skin) using a separate model with semantic similarity rewards, achieving state-of-the-art results on two benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based decompilers struggle to preserve a program's original structure and identifiers effectively, resulting in obfuscated outputs that sacrifice readability and correct source code representation.

Method: 1) Structure Recovery: Translates binary to IR preserving control flow/data structures while obfuscating identifiers using RL to enforce compiler-compliant structures. 2) Identifier Naming: Trains a separate model with RL rewards for semantic similarity between predicted and reference identifiers.

Result: Outperforms GPT-5-mini by 21.6% in re-executability on HumanEval, and Idioms by 29.4% in R2I metric on GitHub2025 benchmark.

Conclusion: The proposed two-phase approach enables independent optimization of decompilation correctness and readability, establishing a new benchmark for structure-preserving and semantically meaningful binary decompilation.

Abstract: Large Language Models (LLMs) have emerged as a promising approach for binary
decompilation. However, the existing LLM-based decompilers still are somewhat
limited in effectively presenting a program's source-level structure with its
original identifiers. To mitigate this, we introduce SK2Decompile, a novel
two-phase approach to decompile from the skeleton (semantic structure) to the
skin (identifier) of programs. Specifically, we first apply a Structure
Recovery model to translate a program's binary code to an Intermediate
Representation (IR) as deriving the program's "skeleton", i.e., preserving
control flow and data structures while obfuscating all identifiers with generic
placeholders. We also apply reinforcement learning to reward the model for
producing program structures that adhere to the syntactic and semantic rules
expected by compilers. Second, we apply an Identifier Naming model to produce
meaningful identifiers which reflect actual program semantics as deriving the
program's "skin". We train the Identifier Naming model with a separate
reinforcement learning objective that rewards the semantic similarity between
its predictions and the reference code. Such a two-phase decompilation process
facilitates advancing the correctness and readability of decompilation
independently. Our evaluations indicate that SK2Decompile, significantly
outperforms the SOTA baselines, achieving 21.6% average re-executability rate
gain over GPT-5-mini on the HumanEval dataset and 29.4% average R2I improvement
over Idioms on the GitHub2025 benchmark.

</details>


### [39] [Leveraging LLM Agents for Automated Video Game Testing](https://arxiv.org/abs/2509.22170)
*Chengjia Wang,Lanling Tang,Ming Yuan,Jiongchi Yu,Xiaofei Xie,Jiajun Bu*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Testing MMORPGs (Massively Multiplayer Online Role-Playing Games) is a
critical yet labor-intensive task in game development due to their complexity
and frequent updating nature. Traditional automated game testing approaches
struggle to achieve high state coverage and efficiency in these rich,
open-ended environments, while existing LLM-based game-playing approaches are
limited to shallow reasoning ability in understanding complex game state-action
spaces and long-complex tasks. To address these challenges, we propose TITAN,
an effective LLM-driven agent framework for intelligent MMORPG testing. TITAN
incorporates four key components to: (1) perceive and abstract high-dimensional
game states, (2) proactively optimize and prioritize available actions, (3)
enable long-horizon reasoning with action trace memory and reflective
self-correction, and (4) employ LLM-based oracles to detect potential
functional and logic bugs with diagnostic reports.
  We implement the prototype of TITAN and evaluate it on two large-scale
commercial MMORPGs spanning both PC and mobile platforms. In our experiments,
TITAN achieves significantly higher task completion rates (95%) and bug
detection performance compared to existing automated game testing approaches.
An ablation study further demonstrates that each core component of TITAN
contributes substantially to its overall performance. Notably, TITAN detects
four previously unknown bugs that prior testing approaches fail to identify. We
provide an in-depth discussion of these results, which offer guidance for new
avenues of advancing intelligent, general-purpose testing systems. Moreover,
TITAN has been deployed in eight real-world game QA pipelines, underscoring its
practical impact as an LLM-driven game testing framework.

</details>


### [40] [Library Hallucinations in LLMs: Risk Analysis Grounded in Developer Queries](https://arxiv.org/abs/2509.22202)
*Lukas Twist,Jie M. Zhang,Mark Harman,Helen Yannakoudakis*

Main category: cs.SE

TL;DR: This paper investigates how prompt variations trigger library hallucinations in code generated by large language models (LLMs), revealing vulnerabilities where minor input changes lead to high hallucination rates (e.g., 99% for fake library names).


<details>
  <summary>Details</summary>
Motivation: LLM-generated code often contains non-existent library references, creating security and operational risks. Previous understudied how real-world prompt differences influence hallucination rates, motivating a systematic analysis.

Method: The study evaluates six LLMs across two hallucination types (library name invalidity and member invalidity) using developer forum prompts and manipulated errors (e.g., misspellings). Hallucination rates were measured under realistic input variations.

Result: Results show 26% hallucination rates for one-character misspellings, 99.1% for fake library names, and 84% for time-related prompts. Prompt engineering reduces hallucinations inconsistently across models.

Conclusion: LLMs exhibit fragility to natural prompt variations, emphasizing urgent safeguards against exploitable library hallucinations, particularly as code generation becomes more prevalent.

Abstract: Large language models (LLMs) are increasingly used to generate code, yet they
continue to hallucinate, often inventing non-existent libraries. Such library
hallucinations are not just benign errors: they can mislead developers, break
builds, and expose systems to supply chain threats such as slopsquatting.
Despite increasing awareness of these risks, little is known about how
real-world prompt variations affect hallucination rates. Therefore, we present
the first systematic study of how user-level prompt variations impact library
hallucinations in LLM-generated code. We evaluate six diverse LLMs across two
hallucination types: library name hallucinations (invalid imports) and library
member hallucinations (invalid calls from valid libraries). We investigate how
realistic user language extracted from developer forums and how user errors of
varying degrees (one- or multi-character misspellings and completely fake
names/members) affect LLM hallucination rates. Our findings reveal systemic
vulnerabilities: one-character misspellings in library names trigger
hallucinations in up to 26% of tasks, fake library names are accepted in up to
99% of tasks, and time-related prompts lead to hallucinations in up to 84% of
tasks. Prompt engineering shows promise for mitigating hallucinations, but
remains inconsistent and LLM-dependent. Our results underscore the fragility of
LLMs to natural prompt variation and highlight the urgent need for safeguards
against library-related hallucinations and their potential exploitation.

</details>


### [41] [Green Prompt Engineering: Investigating the Energy Impact of Prompt Design in Software Engineering](https://arxiv.org/abs/2509.22320)
*Vincenzo De Martino,Mohammad Amin Zadenoori,Xavier Franch,Alessio Ferrari*

Main category: cs.SE

TL;DR: This paper explores how linguistic complexity of prompts impacts energy consumption and performance of language models (LLMs), introducing 'Green Prompt Engineering' as a sustainability-focused approach. Simpler prompts reduce energy costs with minimal F1-score loss.


<details>
  <summary>Details</summary>
Motivation: Environmental concerns from LLM inference and the lack of research on linguistic complexity as a sustainability factor. Prior studies focused on hardware and prompt length, not linguistic simplicity.

Method: Empirical study on requirement classification using open-source small language models, varying prompt readability to assess energy consumption/performance trade-offs.

Result: Readability significantly affects energy consumption and performance trade-offs. Simpler prompts enable energy savings without substantial F1-score reduction in practical tasks.

Conclusion: Green Prompt Engineering offers cost-effective energy savings for practitioners and opens research avenues for sustainable AI guidelines. Simpler prompts align with the Green AI agenda.

Abstract: Language Models are increasingly applied in software engineering, yet their
inference raises growing environmental concerns. Prior work has examined
hardware choices and prompt length, but little attention has been paid to
linguistic complexity as a sustainability factor. This paper introduces Green
Prompt Engineering, framing linguistic complexity as a design dimension that
can influence energy consumption and performance. We conduct an empirical study
on requirement classification using open-source Small Language Models, varying
the readability of prompts. Our results reveal that readability affects
environmental sustainability and performance, exposing trade-offs between them.
For practitioners, simpler prompts can reduce energy costs without a
significant F1-score loss; for researchers, it opens a path toward guidelines
and studies on sustainable prompt design within the Green AI agenda.

</details>


### [42] [GPU-Accelerated Loopy Belief Propagation for Program Analysis](https://arxiv.org/abs/2509.22337)
*Haoyu Feng,Xin Zhang*

Main category: cs.SE

TL;DR: The paper introduces a GPU-accelerated Loopy Belief Propagation (LBP) algorithm for program analysis with flexible update strategies and logical constraint integration, achieving significant speedups over existing methods while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: LBP faces computational challenges in large-scale program analysis, and existing GPU-based solutions lack flexible update strategies and logical constraint integration, resulting in suboptimal performance.

Method: The authors propose a unified representation for user-defined update strategies, a dependency analysis algorithm, and message grouping to minimize GPU warp divergence by leveraging Horn clause structures.

Result: Experiments on Java program datarace analysis show an average 2.14× speedup over sequential approaches and 5.56× over state-of-the-art GPU-based methods, with high accuracy maintained.

Conclusion: The proposed GPU-LBP algorithm effectively addresses scalability issues in program analysis through optimized parallelism and constraint integration.

Abstract: Loopy Belief Propagation (LBP) is a widely used approximate inference
algorithm in probabilistic graphical models, with applications in computer
vision, error correction codes, protein folding, program analysis, etc.
However, LBP faces significant computational challenges when applied to
large-scale program analysis. While GPU (Graphics Processing Unit) parallel
computing provides a promising solution, existing approaches lack support for
flexible update strategies and have yet to integrate logical constraints with
GPU acceleration, leading to suboptimal practical performance.
  This paper presents a GPU-accelerated LBP algorithm for program analysis. To
support the diverse update strategies required by users, we propose a unified
representation for specifying arbitrary user-defined update strategies, along
with a dependency analysis algorithm. Furthermore, building on previous work
that leverages the local structure of Horn clauses to simplify message passing,
we group messages to minimize warp divergence and better utilize GPU resources.
Experimental results on datarace analysis over eight real-world Java programs
show that our approach achieves an average speedup of $2.14\times$ over the
state-of-the-art sequential approach and $5.56\times$ over the state-of-the-art
GPU-based approach, while maintaining high accuracy.

</details>


### [43] [A Multi-Modality Evaluation of the Reality Gap in Autonomous Driving Systems](https://arxiv.org/abs/2509.22379)
*Stefano Carlo Lambertenghi,Mirena Flores Valdez,Andrea Stocco*

Main category: cs.SE

TL;DR: The paper compares four autonomous driving testing modalities (SiL, ViL, MR, real-world), finding that MR testing improves perceptual realism without safety trade-offs, while identifying conditions where test failures do not transfer across modalities due to the reality gap.


<details>
  <summary>Details</summary>
Motivation: Discrepancies between simulated and real-world autonomous driving test results threaten the safety and reliability of deployed systems, necessitating empirical evaluation of testing modality transferability.

Method: Evaluation of two ADS architectures using a physical vehicle and digital twin across indoor scenarios, systematically analyzing reality gap dimensions (actuation, perception, behavioral fidelity).

Result: MR testing shows superior perceptual realism vs. SiL/ViL, but reveals modality-specific failure modes. Systematic identification of reality gap dimensions causing non-transferrable failures.

Conclusion: Provides actionable insights on testing modality strengths/weaknesses and proposes paths toward more transferable autonomous driving validation. Highlighted importance of MR for bridging reality gaps in perception-critical scenarios.

Abstract: Simulation-based testing is a cornerstone of Autonomous Driving System (ADS)
development, offering safe and scalable evaluation across diverse driving
scenarios. However, discrepancies between simulated and real-world behavior,
known as the reality gap, challenge the transferability of test results to
deployed systems. In this paper, we present a comprehensive empirical study
comparing four representative testing modalities: Software-in-the-Loop (SiL),
Vehicle-in-the-Loop (ViL), Mixed-Reality (MR), and full real-world testing.
Using a small-scale physical vehicle equipped with real sensors (camera and
LiDAR) and its digital twin, we implement each setup and evaluate two ADS
architectures (modular and end-to-end) across diverse indoor driving scenarios
involving real obstacles, road topologies, and indoor environments. We
systematically assess the impact of each testing modality along three
dimensions of the reality gap: actuation, perception, and behavioral fidelity.
Our results show that while SiL and ViL setups simplify critical aspects of
real-world dynamics and sensing, MR testing improves perceptual realism without
compromising safety or control. Importantly, we identify the conditions under
which failures do not transfer across testing modalities and isolate the
underlying dimensions of the gap responsible for these discrepancies. Our
findings offer actionable insights into the respective strengths and
limitations of each modality and outline a path toward more robust and
transferable validation of autonomous driving systems.

</details>


### [44] [Context-Specific Instruction: A Longitudinal Study on Debugging Skill Acquisition and Retention for Novice Programmers](https://arxiv.org/abs/2509.22420)
*Ziyi Zhang,Devjeet Roy,Venera Arnaoudova*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Bug localization is a critical skill, yet novices often lack systematic
approaches. Prior work tested abstract guidelines and general concrete steps;
the impact of context-specific instruction is unclear. We ran an eight-week
longitudinal study with four conditions: no instruction (G1), abstract
guidelines (G2), concrete steps (G3), and our context-specific instruction that
pairs concrete bug-localization steps with problem-specific details (G4).
Forty-four undergraduates participated; 41 completed all five sessions (S1-S5).
Each session included 2-3 debugging tasks to identify the minimal code element
containing a seeded logical fault. We measured correctness (binary), time to
completion, self-perceived scores (stress, difficulty, satisfaction, and
strategy adherence). G4 achieved higher correctness and shorter time to
completion: it reached 80% correctness after one session (vs. 20-44% for other
groups) and maintained 80% after three weeks, outperforming all groups (p <
0.05); its time to completion stabilized at 13-15 minutes in S1, whereas other
groups took 2-3 sessions to stabilize at 22-27 minutes. Qualitative responses
showed lower stress and higher satisfaction in G4, with participants
internalizing strategies via contextual examples. We conclude that
context-specific instruction yields faster skill acquisition and stronger
retention than abstract guidelines or context-agnostic steps. Even 1-2 sessions
produced significant gains, while extended practice optimized and stabilized
performance. Integrating contextual examples with abstract principles may
bridge theory-practice gaps in bug-localization education and provide a more
equitable path for novices.

</details>


### [45] [TreeMind: Automatically Reproducing Android Bug Reports via LLM-empowered Monte Carlo Tree Search](https://arxiv.org/abs/2509.22431)
*Zhengyu Chen,Zhaoyi Meng,Wenxiang Zhao,Wansen Wang,Haoyang Zhao,Jiahao Zhan,Jie Cui,Hong Zhong*

Main category: cs.SE

TL;DR: TreeMind combines large language models (LLMs) with a customized Monte Carlo Tree Search (MCTS) to improve Android bug reproduction by strategically exploring complex UI spaces and reconstructing missing user actions.


<details>
  <summary>Details</summary>
Motivation: Existing methods (e.g., reinforcement learning, LLMs) struggle to infer unobserved steps and navigate high-complexity modern UIs in crash reproduction from incomplete textual bug reports due to limited goal-directed planning.

Method: TreeMind integrates LLMs with MCTS via two agents: (1)'Expander' generates top-k actions based on UI state and history, and (2) 'Simulator,' which estimates action success likelihood using LLM semantic reasoning. Multi-modal UI inputs and feedback-aware navigation enable incremental path reconstruction.

Result: TreeMind outperformed four state-of-the-art baselines on 93 real-world Android reports from three benchmarks, achieving significantly higher reproduction success rates, and a real-world case study validated its effectiveness.

Conclusion: TreeMind demonstrates that combining LLM semantic reasoning with MCTS-based planning is a novel, effective approach for automated bug reproduction, addressing limitations of prior methods in complex, incomplete scenarios.

Abstract: Automatically reproducing Android app crashes from textual bug reports is
challenging, particularly when the reports are incomplete and the modern UI
exhibits high combinatorial complexity. Existing approaches based on
reinforcement learning or large language models (LLMs) exhibit limitations in
such scenarios. They struggle to infer unobserved steps and reconstruct the
underlying user action sequences to navigate the vast UI interaction space,
primarily due to limited goal-directed reasoning and planning. We present
TreeMind, a novel technique that integrates LLMs with a customized Monte Carlo
Tree Search (MCTS) algorithm to achieve strategic UI exploration in bug
reproduction. To the best of our knowledge, this is the first work to combine
external decision-making with LLM semantic reasoning for reliable bug
reproduction. We formulate the reproduction task as a target-driven search
problem, leveraging MCTS as the core planning mechanism to iteratively refine
action sequences. To enhance MCTS with semantic reasoning, we introduce two
LLM-guided agents with distinct roles: Expander generates top-k promising
actions based on the current UI state and exploration history, while Simulator
estimates the likelihood that each action leads toward successful reproduction.
By incorporating multi-modal UI inputs and advanced prompting techniques,
TreeMind conducts feedback-aware navigation that identifies missing but
essential user actions and incrementally reconstructs the reproduction paths.
We evaluate TreeMind on a dataset of 93 real-world Android bug reports from
three widely-used benchmarks. Experimental results show that it significantly
outperforms four state-of-the-art baselines in reproduction success rate. A
real-world case study indicates that integrating LLM reasoning with MCTS-based
planning is a compelling direction for automated bug reproduction.

</details>


### [46] [Boosting Pointer Analysis With Large Language Model-Enhanced Allocation Function Detection](https://arxiv.org/abs/2509.22530)
*Baijun Cheng,Kailong Wang,Ling Shi,Haoyu Wang,Peng Di,Yao Guo,Ding Li,Xiangqun Chen*

Main category: cs.SE

TL;DR: The paper introduces AFD, a method that automates modeling custom allocation functions in C/C++ programs to improve pointer analysis precision and scalability.


<details>
  <summary>Details</summary>
Motivation: Pointer analysis struggles with imprecisely modeling heap allocations, especially user-defined allocators (AFs) in C/C++ systems, leading to high aliasing and low precision.

Method: AFD combines value-flow analysis to detect simple AF wrappers and Large Language Models (LLMs) for complex allocators involving side effects, enabling precise heap modeling.

Result: On 15 C projects, AFD identified 600+ custom AFs, increased modeled heap objects by 26x, reduced alias set sizes by 39%, and uncovered 17 new memory bugs with minimal runtime overhead.

Conclusion: AFD demonstrates that accurately modeling user-defined allocators can significantly improve pointer analysis precision and is scalable for real-world software.

Abstract: Pointer analysis is foundational for many static analysis tasks, yet its
effectiveness is often hindered by imprecise modeling of heap allocations,
particularly in C/C++ programs where user-defined allocation functions (AFs)
are pervasive. Existing approaches largely overlook these custom allocators,
leading to coarse aliasing and reduced analysis precision. In this paper, we
present AFD, a novel technique that enhances pointer analysis by automatically
identifying and modeling custom allocation functions. AFD employs a hybrid
approach: it uses value-flow analysis to detect straightforward wrappers and
leverages Large Language Models (LLMs) to reason about more complex allocation
patterns with side effects. This targeted enhancement enables precise modeling
of heap objects at each call site, achieving context-sensitivity-like benefits
without the associated overhead. We evaluate AFD on 15 real-world C projects,
identifying over 600 custom AFs. Integrating AFD into a baseline pointer
analysis yields a 26x increase in modeled heap objects and a 39% reduction in
alias set sizes, with only 1.4x runtime overhead. Furthermore, our enhanced
analysis improves indirect call resolution and uncovers 17 previously
undetected memory bugs. These results demonstrate that precise modeling of
custom allocation functions offers a scalable and practical path to improving
pointer analysis in large software systems.

</details>
