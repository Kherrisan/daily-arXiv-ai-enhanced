<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 18]
- [cs.SE](#cs.SE) [Total: 6]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [WatchWitch: Interoperability, Privacy, and Autonomy for the Apple Watch](https://arxiv.org/abs/2507.07210)
*Nils Rollshausen,Alexander Heinrich,Matthias Hollick,Jiska Classen*

Main category: cs.CR

TL;DR: This paper explores the security vulnerabilities in Apple Watch's proprietary system and introduces WatchWitch, an Android-based alternative, to enhance user privacy and interoperability in the smartwatch market.


<details>
  <summary>Details</summary>
Motivation: Apple Watch's restricted ecosystem limits user control over health/fitness data and locks users into Apple's proprietary software/cloud services, creating privacy and choice concerns.

Method: Public reverse-engineering of Apple Watch's wireless protocols to identify security flaws, followed by development of WatchWitch - a privacy-focused Android reimplementation with interoperability.

Result: 1) Multiple security issues discovered in Apple's implementation  2) Created functional Android reimplementation (WatchWitch) 3) Demonstrated practical interoperability 4) Achieved enhanced privacy controls

Conclusion: The research establishes precedent for breaking proprietary smartwatch ecosystems while maintaining functionality, enabling users to prioritize privacy through choice in platform and data handling.

Abstract: Smartwatches such as the Apple Watch collect vast amounts of intimate health
and fitness data as we wear them. Users have little choice regarding how this
data is processed: The Apple Watch can only be used with Apple's iPhones, using
their software and their cloud services. We are the first to publicly
reverse-engineer the watch's wireless protocols, which led to discovering
multiple security issues in Apple's proprietary implementation. With
WatchWitch, our custom Android reimplementation, we break out of Apple's walled
garden -- demonstrating practical interoperability with enhanced privacy
controls and data autonomy. We thus pave the way for more consumer choice in
the smartwatch ecosystem, offering users more control over their devices.

</details>


### [2] [Automated Attack Testflow Extraction from Cyber Threat Report using BERT for Contextual Analysis](https://arxiv.org/abs/2507.07244)
*Faissal Ahmadou,Sepehr Ghaffarzadegan,Boubakr Nour,Makan Pourzandi,Mourad Debbabi,Chadi Assi*

Main category: cs.CR

TL;DR: FLOWGUARDIAN automates extraction of attack testflows from unstructured threat reports using BERT and NLP, enabling efficient and accurate cybersecurity threat analysis and incident response.


<details>
  <summary>Details</summary>
Motivation: Manual extraction of attack testflows from threat reports is time-consuming, error-prone, and requires specialized knowledge, limiting the efficiency of cybersecurity testing and response.

Method: FLOWGUARDIAN leverages BERT language models and NLP techniques to systematically analyze security event context, reconstruct attack sequences, and generate structured testflows from unstructured reports.

Result: Empirical validation shows FLOWGUARDIAN achieves high accuracy and efficiency in testflow extraction, demonstrably improving security teams' capabilities in proactive threat hunting and incident response.

Conclusion: Automated testflow extraction via FLOWGUARDIAN reduces manual effort, minimizes errors, and enhances comprehensive coverage in cybersecurity testing, with promising practical applications for security operations.

Abstract: In the ever-evolving landscape of cybersecurity, the rapid identification and
mitigation of Advanced Persistent Threats (APTs) is crucial. Security
practitioners rely on detailed threat reports to understand the tactics,
techniques, and procedures (TTPs) employed by attackers. However, manually
extracting attack testflows from these reports requires elusive knowledge and
is time-consuming and prone to errors. This paper proposes FLOWGUARDIAN, a
novel solution leveraging language models (i.e., BERT) and Natural Language
Processing (NLP) techniques to automate the extraction of attack testflows from
unstructured threat reports. FLOWGUARDIAN systematically analyzes and
contextualizes security events, reconstructs attack sequences, and then
generates comprehensive testflows. This automated approach not only saves time
and reduces human error but also ensures comprehensive coverage and robustness
in cybersecurity testing. Empirical validation using public threat reports
demonstrates FLOWGUARDIAN's accuracy and efficiency, significantly enhancing
the capabilities of security teams in proactive threat hunting and incident
response.

</details>


### [3] [Disa: Accurate Learning-based Static Disassembly with Attentions](https://arxiv.org/abs/2507.07246)
*Peicheng Wang,Monika Santra,Mingyu Liu,Cong Sun,Dongrui Zeng,Gang Tan*

Main category: cs.CR

TL;DR: This paper proposes Disa, a deep learning-based disassembly method that improves function entry-point identification and control flow graph accuracy by leveraging superset instructions with multi-head self-attention, outperforming prior approaches on obfuscated binaries.


<details>
  <summary>Details</summary>
Motivation: Traditional disassembly methods using file-format assumptions and architecture-specific heuristics fail to handle obfuscated binaries, leading to incomplete/incorrect instruction boundary identification crucial for vulnerability detection, malware analysis, and binary hardening.

Method: Disa uses multi-head self-attention with superset instruction information to learn instruction correlations, identifying function entry-points and instruction boundaries. It incorporates memory block boundary detection to enhance control flow graph accuracy through advanced value-set analysis.

Result: Disa achieves 9.1% and 13.2% F1-score improvements on binaries obfuscated by desynchronizing techniques and source-level obfuscators respectively. It improves memory block precision by 18.5%, reducing Average Indirect Call Targets (AICT) by 4.4% compared to heuristic-based approaches.

Conclusion: Deep learning-based Disa effectively addresses disassembly challenges in obfuscated binaries, outperforming both prior deep learning and state-of-the-art heuristic methods in function boundary identification and control flow graph generation precision.

Abstract: For reverse engineering related security domains, such as vulnerability
detection, malware analysis, and binary hardening, disassembly is crucial yet
challenging. The fundamental challenge of disassembly is to identify
instruction and function boundaries. Classic approaches rely on file-format
assumptions and architecture-specific heuristics to guess the boundaries,
resulting in incomplete and incorrect disassembly, especially when the binary
is obfuscated. Recent advancements of disassembly have demonstrated that deep
learning can improve both the accuracy and efficiency of disassembly. In this
paper, we propose Disa, a new learning-based disassembly approach that uses the
information of superset instructions over the multi-head self-attention to
learn the instructions' correlations, thus being able to infer function
entry-points and instruction boundaries. Disa can further identify instructions
relevant to memory block boundaries to facilitate an advanced block-memory
model based value-set analysis for an accurate control flow graph (CFG)
generation. Our experiments show that Disa outperforms prior deep-learning
disassembly approaches in function entry-point identification, especially
achieving 9.1% and 13.2% F1-score improvement on binaries respectively
obfuscated by the disassembly desynchronization technique and popular
source-level obfuscator. By achieving an 18.5% improvement in the memory block
precision, Disa generates more accurate CFGs with a 4.4% reduction in Average
Indirect Call Targets (AICT) compared with the state-of-the-art heuristic-based
approach.

</details>


### [4] [Semi-fragile watermarking of remote sensing images using DWT, vector quantization and automatic tiling](https://arxiv.org/abs/2507.07250)
*Jordi Serra-Ruiz,David Meg√≠as*

Main category: cs.CR

TL;DR: This paper proposes a semi-fragile watermarking scheme for multi-band images using a tree-structured vector quantization approach to embed marks into pixel signatures, enabling detection of significant modifications with lossy compression robustness.


<details>
  <summary>Details</summary>
Motivation: Traditional separation-based processing of multi-band remote sensing images may inadequately address integrity verification needs, motivated by the demand for watermarking schemes that can distinguish between acceptable compressions and malicious forgeries while leveraging image signature characteristics.

Method: The method involves 1) segmenting images into 3D blocks containing all bands, 2) constructing tree-structured vector quantizers for each block using pixel signatures, and 3) applying an iterative algorithm to manipulate these trees until watermark embedding criteria are met while maintaining signature correlations.

Result: Experimental results demonstrate the scheme successfully preserves watermarks under lossy compression (above a defined threshold) and simultaneously detects forged blocks by identifying non-compliant signature deviations in multi-band images.

Conclusion: The proposed semi-fragile watermarking approach for multi-band images enables effective authentication while accommodating natural compressive losses, with potential applications in remote sensing data integrity verification where multi-spectral/hyper-spectral analysis requires robust yet forgery-sensitive security mechanisms.

Abstract: A semi-fragile watermarking scheme for multiple band images is presented in
this article. We propose to embed a mark into remote sensing images applying a
tree-structured vector quantization approach to the pixel signatures instead of
processing each band separately. The signature of the multispectral or
hyperspectral image is used to embed the mark in it order to detect any
significant modification of the original image. The image is segmented into
three-dimensional blocks, and a tree-structured vector quantizer is built for
each block. These trees are manipulated using an iterative algorithm until the
resulting block satisfies a required criterion, which establishes the embedded
mark. The method is shown to be able to preserve the mark under lossy
compression (above a given threshold) but, at the same time, it detects
possibly forged blocks and their position in the whole image.

</details>


### [5] [FedP3E: Privacy-Preserving Prototype Exchange for Non-IID IoT Malware Detection in Cross-Silo Federated Learning](https://arxiv.org/abs/2507.07258)
*Rami Darwish,Mahmoud Abdelsalam,Sajad Khorsandroo,Kaushik Roy*

Main category: cs.CR

TL;DR: The paper introduces FedP3E, a privacy-preserving federated learning framework for IoT malware detection, addressing class imbalance and non-IID data challenges through prototype-based representation sharing and SMOTE augmentation.


<details>
  <summary>Details</summary>
Motivation: IoT ecosystems face sophisticated malware attacks, requiring privacy-preserving frameworks robust to data heterogeneity. Standard FL methods like FedAvg/FedProx fail in real-world scenarios with class imbalance and disjoint malware classes.

Method: FedP3E uses Gaussian Mixture Models to generate class-wise prototypes, adds Gaussian noise to them for privacy, shares compact summaries via aggregation, and integrates SMOTE-based data augmentation to improve training on imbalanced and non-IID client data.

Result: FedP3E is evaluated on N-BaIoT datasets with cross-silo scenarios and varying data imbalances, demonstrating its effectiveness with reduced communication overhead.

Conclusion: FedP3E outperforms existing FL methods in IoT malware detection under non-IID and imbalanced data conditions, achieving privacy-preservation and robustness through indirect representation sharing and structural pattern enrichment.

Abstract: As IoT ecosystems continue to expand across critical sectors, they have
become prominent targets for increasingly sophisticated and large-scale malware
attacks. The evolving threat landscape, combined with the sensitive nature of
IoT-generated data, demands detection frameworks that are both
privacy-preserving and resilient to data heterogeneity. Federated Learning (FL)
offers a promising solution by enabling decentralized model training without
exposing raw data. However, standard FL algorithms such as FedAvg and FedProx
often fall short in real-world deployments characterized by class imbalance and
non-IID data distributions -- particularly in the presence of rare or disjoint
malware classes. To address these challenges, we propose FedP3E
(Privacy-Preserving Prototype Exchange), a novel FL framework that supports
indirect cross-client representation sharing while maintaining data privacy.
Each client constructs class-wise prototypes using Gaussian Mixture Models
(GMMs), perturbs them with Gaussian noise, and transmits only these compact
summaries to the server. The aggregated prototypes are then distributed back to
clients and integrated into local training, supported by SMOTE-based
augmentation to enhance representation of minority malware classes. Rather than
relying solely on parameter averaging, our prototype-driven mechanism enables
clients to enrich their local models with complementary structural patterns
observed across the federation -- without exchanging raw data or gradients.
This targeted strategy reduces the adverse impact of statistical heterogeneity
with minimal communication overhead. We evaluate FedP3E on the N-BaIoT dataset
under realistic cross-silo scenarios with varying degrees of data imbalance.

</details>


### [6] [Shuffling for Semantic Secrecy](https://arxiv.org/abs/2507.07401)
*Fupei Chen,Liyao Xiang,Haoxiang Sun,Hei Victor Cheng,Kaiming Shen*

Main category: cs.CR

TL;DR: This paper proposes a semantic security communication system using random shuffling patterns as shared keys to distort data semantics and enhance secure transmission, particularly in noisy channels.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need for improving conventional secure coding schemes by achieving a better tradeoff between transmission rate and leakage rate in wiretap channels while minimizing semantic error probability.

Method: The authors devised a novel system utilizing random shuffling patterns as shared secret keys to permute feature sequences, obscuring semantic information from eavesdroppers. This approach is flexible enough to integrate as a plugin into existing semantic communication frameworks.

Result: Simulation results demonstrate that the proposed shuffling method significantly outperforms benchmarks in secure transmission, especially in channels with strong noise and unpredictable fading conditions.

Conclusion: The conclusion highlights that the random shuffling-based security framework effectively protects data semantics against eavesdropping, offering a practical and adaptable solution for enhancing security in semantic communication systems.

Abstract: Deep learning draws heavily on the latest progress in semantic
communications. The present paper aims to examine the security aspect of this
cutting-edge technique from a novel shuffling perspective. Our goal is to
improve upon the conventional secure coding scheme to strike a desirable
tradeoff between transmission rate and leakage rate. To be more specific, for a
wiretap channel, we seek to maximize the transmission rate while minimizing the
semantic error probability under the given leakage rate constraint. Toward this
end, we devise a novel semantic security communication system wherein the
random shuffling pattern plays the role of the shared secret key. Intuitively,
the permutation of feature sequences via shuffling would distort the semantic
essence of the target data to a sufficient extent so that eavesdroppers cannot
access it anymore. The proposed random shuffling method also exhibits its
flexibility in working for the existing semantic communication system as a
plugin. Simulations demonstrate the significant advantage of the proposed
method over the benchmark in boosting secure transmission, especially when
channels are prone to strong noise and unpredictable fading.

</details>


### [7] [Phishing Detection in the Gen-AI Era: Quantized LLMs vs Classical Models](https://arxiv.org/abs/2507.07406)
*Jikesh Thapa,Gurrehmat Chahal,Serban Voinea Gabreanu,Yazan Otoum*

Main category: cs.CR

TL;DR: This paper compares traditional Machine Learning (ML), Deep Learning (DL), and quantized LLMs for phishing detection. While LLMs lag in raw accuracy, they excel at identifying context-based phishing cues and lightweight LLMs like Q8_0 models offer cost-efficient deployment options with over 80% accuracy at 17GB VRAM. LLM-based methods also provide interpretable explanations but suffer from performance degradation when handling rephrased emails.


<details>
  <summary>Details</summary>
Motivation: Phishing attacks are becoming more sophisticated, necessitating detection systems that balance accuracy with computational efficiency. Traditional AI approaches may lack interpretability, while unoptimized LLMs face high resource constraints. The study explores optimized LLM applications to address these challenges in cybersecurity contexts.

Method: Experiments were conducted on a curated phishing detection dataset, comparing traditional ML/DL models against quantized LLMs (e.g., DeepSeek R1 Distill Qwen 14B Q8_0). The evaluation assessed accuracy, VRAM requirements, adversarial robustness, cost-performance tradeoffs, and the impact of zero-shot/few-shot prompting strategies using LLM-rephrased emails as adversarial examples.

Result: 1) Quantized LLMs achieved >80% accuracy using 17GB VRAM. 2) LLMs outperformed ML/DL for subtle contextual phishing cues while underperforming in direct accuracy comparisons. 3) Zero-shot/few-shot prompting significantly reduced detection accuracy for both ML and LLM detectors. 4) Lightweight LLMs provided concise, human-interpretable explanations for phishing decisions.

Conclusion: While traditionally behind ML/DL approaches in accuracy, optimized LLMs demonstrate sufficient phishing detection accuracy with major advantages in interpretability and cost efficiency. The findings suggest lightweight LLMs can be strategically integrated into multi-layered cybersecurity systems as explainable, resource-conscious detectors for context-aware phishing threats.

Abstract: Phishing attacks are becoming increasingly sophisticated, underscoring the
need for detection systems that strike a balance between high accuracy and
computational efficiency. This paper presents a comparative evaluation of
traditional Machine Learning (ML), Deep Learning (DL), and quantized
small-parameter Large Language Models (LLMs) for phishing detection. Through
experiments on a curated dataset, we show that while LLMs currently
underperform compared to ML and DL methods in terms of raw accuracy, they
exhibit strong potential for identifying subtle, context-based phishing cues.
We also investigate the impact of zero-shot and few-shot prompting strategies,
revealing that LLM-rephrased emails can significantly degrade the performance
of both ML and LLM-based detectors. Our benchmarking highlights that models
like DeepSeek R1 Distill Qwen 14B (Q8_0) achieve competitive accuracy, above
80%, using only 17GB of VRAM, supporting their viability for cost-efficient
deployment. We further assess the models' adversarial robustness and
cost-performance tradeoffs, and demonstrate how lightweight LLMs can provide
concise, interpretable explanations to support real-time decision-making. These
findings position optimized LLMs as promising components in phishing defence
systems and offer a path forward for integrating explainable, efficient AI into
modern cybersecurity frameworks.

</details>


### [8] [Hybrid LLM-Enhanced Intrusion Detection for Zero-Day Threats in IoT Networks](https://arxiv.org/abs/2507.07413)
*Mohammad F. Al-Hammouri,Yazan Otoum,Rasha Atwa,Amiya Nayak*

Main category: cs.CR

TL;DR: This paper introduces a hybrid Intrusion Detection System (IDS) that combines signature-based methods with GPT-2's semantic analysis, achieving 6.3% better detection accuracy and 9.0% fewer false positives while maintaining near real-time performance.


<details>
  <summary>Details</summary>
Motivation: To address rising cyber threats in IoT and other complex environments, where traditional signature-based IDSs struggle with zero-day attacks but machine learning models like GPT-2 can provide contextual threat understanding through unstructured data analysis.

Method: Proposed a hybrid IDS framework merging signature-based threat detection with GPT-2-powered semantic analysis for anomaly detection in network traffic, using experiments on a benchmark intrusion dataset to validate performance improvements.

Result: Demonstrated 6.3% increased detection accuracy, 9.0% reduced false-positive rate, and maintained near real-time responsiveness compared to traditional methods in experimental evaluations.

Conclusion: Integration of large language models with conventional IDS approaches enhances threat detection capabilities while maintaining efficiency, suggesting a viable path toward intelligent, adaptive cybersecurity systems for modern distributed environments.

Abstract: This paper presents a novel approach to intrusion detection by integrating
traditional signature-based methods with the contextual understanding
capabilities of the GPT-2 Large Language Model (LLM). As cyber threats become
increasingly sophisticated, particularly in distributed, heterogeneous, and
resource-constrained environments such as those enabled by the Internet of
Things (IoT), the need for dynamic and adaptive Intrusion Detection Systems
(IDSs) becomes increasingly urgent. While traditional methods remain effective
for detecting known threats, they often fail to recognize new and evolving
attack patterns. In contrast, GPT-2 excels at processing unstructured data and
identifying complex semantic relationships, making it well-suited to uncovering
subtle, zero-day attack vectors. We propose a hybrid IDS framework that merges
the robustness of signature-based techniques with the adaptability of
GPT-2-driven semantic analysis. Experimental evaluations on a representative
intrusion dataset demonstrate that our model enhances detection accuracy by
6.3%, reduces false positives by 9.0%, and maintains near real-time
responsiveness. These results affirm the potential of language model
integration to build intelligent, scalable, and resilient cybersecurity
defences suited for modern connected environments.

</details>


### [9] [Autonomous AI-based Cybersecurity Framework for Critical Infrastructure: Real-Time Threat Mitigation](https://arxiv.org/abs/2507.07416)
*Jenifer Paulraj,Brindha Raghuraman,Nagarani Gopalakrishnan,Yazan Otoum*

Main category: cs.CR

TL;DR: This paper analyzes cybersecurity vulnerabilities in interconnected critical infrastructure systems and proposes a hybrid AI-based framework for real-time threat detection and mitigation, addressing adversarial AI and regulatory challenges to enhance security resilience.


<details>
  <summary>Details</summary>
Motivation: Critical infrastructure systems are increasingly exposed to cyber threats due to interconnectivity, necessitating improved security measures to ensure societal stability and economic resilience.

Method: The study proposes a hybrid AI-driven cybersecurity framework integrating real-time vulnerability detection, threat modeling, and automated remediation capabilities.

Result: The findings provide actionable insights for strengthening critical infrastructure security against emerging threats through AI implementation.

Conclusion: The hybrid AI framework offers a robust solution to address evolving cyber threats in critical infrastructure while managing adversarial AI risks and compliance complexities.

Abstract: Critical infrastructure systems, including energy grids, healthcare
facilities, transportation networks, and water distribution systems, are
pivotal to societal stability and economic resilience. However, the increasing
interconnectivity of these systems exposes them to various cyber threats,
including ransomware, Denial-of-Service (DoS) attacks, and Advanced Persistent
Threats (APTs). This paper examines cybersecurity vulnerabilities in critical
infrastructure, highlighting the threat landscape, attack vectors, and the role
of Artificial Intelligence (AI) in mitigating these risks. We propose a hybrid
AI-driven cybersecurity framework to enhance real-time vulnerability detection,
threat modelling, and automated remediation. This study also addresses the
complexities of adversarial AI, regulatory compliance, and integration. Our
findings provide actionable insights to strengthen the security and resilience
of critical infrastructure systems against emerging cyber threats.

</details>


### [10] [May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks](https://arxiv.org/abs/2507.07417)
*Nishit V. Pandya,Andrey Labunets,Sicun Gao,Earlence Fernandes*

Main category: cs.CR

TL;DR: This paper demonstrates that existing fine-tuning defenses against prompt injection attacks in LLMs (SecAlign and StruQ) are vulnerable in the whitebox setting via novel attention-based attacks achieving ~70% success rates with minimal token budget increases. The analysis highlights gaps in claimed security properties and provides public code for evaluated attacks.


<details>
  <summary>Details</summary>
Motivation: Current instruction separation defenses for LLMs lack thorough whitebox attack evaluation, creating uncertainty about their security guarantees. Understanding these weaknesses is critical for developing robust mitigation strategies.

Method: Developed an attention-based optimization attack framework that exploits visibility patterns between instruction tokens and model internals. Applied this method to analyze SecAlign and StruQ through iterative adversarial perturbation with token budget constraints.

Result: Achieved up to 70% successful prompt injection attack rates against both defenses with only a 10-20% token budget increase from baseline attacks, demonstrating significant vulnerability despite architectural modifications.

Conclusion: Popular instruction separation defenses are fundamentally vulnerable to whitebox optimization-based attacks requiring minimal resources. This work establishes a benchmark for evaluating LLM injection defenses and emphasizes the need for stronger architecture-level solutions.

Abstract: A popular class of defenses against prompt injection attacks on large
language models (LLMs) relies on fine-tuning the model to separate instructions
and data, so that the LLM does not follow instructions that might be present
with data. There are several academic systems and production-level
implementations of this idea. We evaluate the robustness of this class of
prompt injection defenses in the whitebox setting by constructing strong
optimization-based attacks and showing that the defenses do not provide the
claimed security properties. Specifically, we construct a novel attention-based
attack algorithm for text-based LLMs and apply it to two recent whitebox
defenses SecAlign (CCS 2025) and StruQ (USENIX Security 2025), showing attacks
with success rates of up to 70% with modest increase in attacker budget in
terms of tokens. Our findings make fundamental progress towards understanding
the robustness of prompt injection defenses in the whitebox setting. We release
our code and attacks at https://github.com/nishitvp/better_opts_attacks

</details>


### [11] [RADAR: a Radio-based Analytics for Dynamic Association and Recognition of pseudonyms in VANETs](https://arxiv.org/abs/2507.07732)
*Giovanni Gambigliani Zoccoli,Filip Valgimigli,Dario Stabili,Mirco Marchetti*

Main category: cs.CR

TL;DR: RADAR is a vehicle tracking algorithm that combines DSRC and Wi-Fi signals to improve de-anonymization in VANETs by exploiting pseudonym-breaking opportunities when attacker coverage is limited.


<details>
  <summary>Details</summary>
Motivation: Existing pseudonym schemes in VANETs preserve location privacy by rotating identifiers, but standard de-anonymization methods relying solely on DSRC signals fail under partial observation scenarios common in real-world C-ITS implementations.

Method: The approach combines Dedicated Short Range Communication (DSRC) with Wi-Fi probe request messages using a three-metric evaluation framework (Count, Statistical RSSI, and Pearson RSSI) to associate pseudonyms and Wi-Fi identifiers despite pseudonym rotation.

Result: Experimental evaluation across multiple scenarios demonstrates Pearson RSSI metric outperforms previous methods in tracking performance, establishing robust advantages over standard Count and Statistical RSSI approaches.

Conclusion: This work advances vehicle tracking capabilities in C-ITS through multi-signal correlation while contributing to research transparency by publicly releasing all implementations and simulation scenarios.

Abstract: This paper presents RADAR, a tracking algorithm for vehicles participating in
Cooperative Intelligent Transportation Systems (C-ITS) that exploits multiple
radio signals emitted by a modern vehicle to break privacy-preserving pseudonym
schemes deployed in VANETs. This study shows that by combining Dedicated Short
Range Communication (DSRC) and Wi-Fi probe request messages broadcast by the
vehicle, it is possible to improve tracking over standard de-anonymization
approaches that only leverage DSRC, especially in realistic scenarios where the
attacker does not have full coverage of the entire vehicle path. The
experimental evaluation compares three different metrics for pseudonym and
Wi-Fi probe identifier association (Count, Statistical RSSI, and Pearson RSSI),
demonstrating that the Pearson RSSI metric is better at tracking vehicles under
pseudonym-changing schemes in all scenarios and against previous works. As an
additional contribution to the state-of-the-art, we publicly release all
implementations and simulation scenarios used in this work.

</details>


### [12] [Rainbow Artifacts from Electromagnetic Signal Injection Attacks on Image Sensors](https://arxiv.org/abs/2507.07773)
*Youqian Zhang,Xinyu Ji,Zhihao Wang,Qinhong Jiang*

Main category: cs.CR

TL;DR: The paper identifies and evaluates a novel electromagnetic interference attack on CMOS image sensors, which introduces rainbow-like color artifacts in raw visual data and leads to significant mispredictions in object detection systems. This highlights a critical vulnerability in the physical layer of image sensors used in safety/industrial applications.


<details>
  <summary>Details</summary>
Motivation: Image sensors are crucial in safety- and security-critical systems (e.g., autonomous vehicles, surveillance infrastructure), and their data integrity is essential for valid system decisions. Current digital integrity checks fail to detect analog-domain attacks. The paper aims to uncover physical-layer vulnerabilities to improve security.

Method: The researchers inject electromagnetic signals into the analog domain of CMOS image sensors to manipulate raw visual input. They exploit undocumented attack phenomena by tuning electromagnetic interference to induce controlled rainbow-like color artifacts. These artifacts are then analyzed to determine their impact on object detection pipelines.

Result: The injected artifacts bypass conventional digital integrity checks, propagate through image signal processing pipelines, and cause significant mispredictions in state-of-the-art object detection models (e.g., misclassifying objects, introducing false positives). This demonstrates vulnerability to physical-layer attacks in the visual perception stack.

Conclusion: The paper concludes that physical-layer attacks targeting the analog domain of image sensors represent a critical and underexplored vulnerability. It emphasizes the need to develop robust defenses for the visual perception stack to prevent adversarial manipulation of raw visual data in safety-critical systems.

Abstract: Image sensors are integral to a wide range of safety- and security-critical
systems, including surveillance infrastructure, autonomous vehicles, and
industrial automation. These systems rely on the integrity of visual data to
make decisions. In this work, we investigate a novel class of electromagnetic
signal injection attacks that target the analog domain of image sensors,
allowing adversaries to manipulate raw visual inputs without triggering
conventional digital integrity checks. We uncover a previously undocumented
attack phenomenon on CMOS image sensors: rainbow-like color artifacts induced
in images captured by image sensors through carefully tuned electromagnetic
interference. We further evaluate the impact of these attacks on
state-of-the-art object detection models, showing that the injected artifacts
propagate through the image signal processing pipeline and lead to significant
mispredictions. Our findings highlight a critical and underexplored
vulnerability in the visual perception stack, highlighting the need for more
robust defenses against physical-layer attacks in such systems.

</details>


### [13] [Mitigating Watermark Stealing Attacks in Generative Models via Multi-Key Watermarking](https://arxiv.org/abs/2507.07871)
*Toluwani Aremu,Noor Hussein,Munachiso Nwadike,Samuele Poppi,Jie Zhang,Karthik Nandakumar,Neil Gong,Nils Lukas*

Main category: cs.CR

TL;DR: The paper proposes a multi-key watermarking strategy to prevent unauthorized forging of watermarks in generative AI outputs, offering theoretical/empirical guarantees and modeling threats via security games.


<details>
  <summary>Details</summary>
Motivation: GenAI providers face risks from watermark stealing attacks where users forge watermarks without access to secret keys to misattribute harmful content, undermining provenance verification.

Method: A multi-key extension with black-box watermarking compatibility is introduced, enabling dynamic key selection during verification. Threats are formalized as security games to evaluate model vulnerabilities.

Result: The method significantly reduces forging success rates across datasets with theoretical security bounds validated experimentally, demonstrating robustness against simulated stealing attacks.

Conclusion: The proposed framework provides a practical defense against watermark stealing by decoupling key management from watermark design, while establishing a game-theoretic model for future attack/defense research.

Abstract: Watermarking offers a promising solution for GenAI providers to establish the
provenance of their generated content. A watermark is a hidden signal embedded
in the generated content, whose presence can later be verified using a secret
watermarking key. A threat to GenAI providers are \emph{watermark stealing}
attacks, where users forge a watermark into content that was \emph{not}
generated by the provider's models without access to the secret key, e.g., to
falsely accuse the provider. Stealing attacks collect \emph{harmless}
watermarked samples from the provider's model and aim to maximize the expected
success rate of generating \emph{harmful} watermarked samples. Our work focuses
on mitigating stealing attacks while treating the underlying watermark as a
black-box. Our contributions are: (i) Proposing a multi-key extension to
mitigate stealing attacks that can be applied post-hoc to any watermarking
method across any modality. (ii) We provide theoretical guarantees and
demonstrate empirically that our method makes forging substantially less
effective across multiple datasets, and (iii) we formally define the threat of
watermark forging as the task of generating harmful, watermarked content and
model this threat via security games.

</details>


### [14] [The Trust Fabric: Decentralized Interoperability and Economic Coordination for the Agentic Web](https://arxiv.org/abs/2507.07901)
*Sree Bhargavi Balija,Rekha Singal,Abhishek Singh,Ramesh Raskar,Erfan Darzi,Raghu Bala,Thomas Hardjono,Ken Huang*

Main category: cs.CR

TL;DR: Nanda addresses AI agent ecosystem fragmentation via DID discovery, semantic agent cards, dynamic trust layers, X42 H42 micropayments, and MAESTRO security for a trust-anchored IoA.


<details>
  <summary>Details</summary>
Motivation: Current protocols (MCP, A2A, ACP, AGP) fail to address interoperability, trust, and economic coordination challenges in fragmented AI agent ecosystems at scale.

Method: Architecture features: 1) DID-based distributed registries for fast agent discovery 2) Semantic agent cards with verifiable credentials/composability profiles 3) Trust layer combining behavioral attestations/policy compliance 4) X42/H42 micropayment coordination 5) MAESTRO security (AgentTalk protocol + secure containers).

Result: Real-world deployments show 99.9% healthcare compliance and strong monthly transaction volumes with privacy guarantees, demonstrating transactional viability.

Conclusion: Unifies MIT's trust research with production deployments to create a trust-anchored, globally interoperable Internet of Agents (IoA) across enterprise/Web3 ecosystems using cryptographic proofs and policy-as-code.

Abstract: The fragmentation of AI agent ecosystems has created urgent demands for
interoperability, trust, and economic coordination that current protocols --
including MCP (Hou et al., 2025), A2A (Habler et al., 2025), ACP (Liu et al.,
2025), and Cisco's AGP (Edwards, 2025) -- cannot address at scale. We present
the Nanda Unified Architecture, a decentralized framework built around three
core innovations: fast DID-based agent discovery through distributed
registries, semantic agent cards with verifiable credentials and composability
profiles, and a dynamic trust layer that integrates behavioral attestations
with policy compliance. The system introduces X42/H42 micropayments for
economic coordination and MAESTRO, a security framework incorporating
Synergetics' patented AgentTalk protocol (US Patent 12,244,584 B1) and secure
containerization. Real-world deployments demonstrate 99.9 percent compliance in
healthcare applications and substantial monthly transaction volumes with strong
privacy guarantees. By unifying MIT's trust research with production
deployments from Cisco and Synergetics, we show how cryptographic proofs and
policy-as-code transform agents into trust-anchored participants in a
decentralized economy (Lakshmanan, 2025; Sha, 2025). The result enables a
globally interoperable Internet of Agents where trust becomes the native
currency of collaboration across both enterprise and Web3 ecosystems.

</details>


### [15] [Can Large Language Models Improve Phishing Defense? A Large-Scale Controlled Experiment on Warning Dialogue Explanations](https://arxiv.org/abs/2507.07916)
*Federico Maria Cau,Giuseppe Desolda,Francesco Greco,Lucio Davide Spano,Luca Vigan√≤*

Main category: cs.CR

TL;DR: This paper evaluates using Large Language Models (LLMs) to generate phishing warning explanations, demonstrating LLMs can match or exceed manual explanations in reducing phishing susceptibility while being scalable and adaptive.


<details>
  <summary>Details</summary>
Motivation: Traditional phishing warnings with static content and limited clarity fail to effectively mitigate risks by addressing human factors, necessitating scalable, context-aware solutions.

Method: A large-scale between-subjects user study (N=750) comparing manually generated vs. LLM (Claude 3.5 Sonnet, Llama 3.3 70B) generated explanations using two styles (feature-based and counterfactual) to measure click-through rates and user perceptions like trust/risk.

Result: LLM-generated explanations equalled/exceeded manual ones in reducing phishing susceptibility; feature-based explanations outperformed counterfactual for genuine phishing attempts, while counterfactual reduced false-positive rates. Variables like workload, gender, and prior familiarity with warnings significantly influenced outcomes.

Conclusion: LLMs enable the automatic creation of scalable, context-sensitive phishing explanations that maintain human-centred effectiveness while offering adaptability, demonstrating potential to enhance cybersecurity defenses through personalized user interaction.

Abstract: Phishing has become a prominent risk in modern cybersecurity, often used to
bypass technological defences by exploiting predictable human behaviour.
Warning dialogues are a standard mitigation measure, but the lack of
explanatory clarity and static content limits their effectiveness. In this
paper, we report on our research to assess the capacity of Large Language
Models (LLMs) to generate clear, concise, and scalable explanations for
phishing warnings. We carried out a large-scale between-subjects user study (N
= 750) to compare the influence of warning dialogues supplemented with manually
generated explanations against those generated by two LLMs, Claude 3.5 Sonnet
and Llama 3.3 70B. We investigated two explanatory styles (feature-based and
counterfactual) for their effects on behavioural metrics (click-through rate)
and perceptual outcomes (e.g., trust, risk, clarity). The results indicate that
well-constructed LLM-generated explanations can equal or surpass manually
crafted explanations in reducing susceptibility to phishing; Claude-generated
warnings exhibited particularly robust performance. Feature-based explanations
were more effective for genuine phishing attempts, whereas counterfactual
explanations diminished false-positive rates. Other variables such as workload,
gender, and prior familiarity with warning dialogues significantly moderated
warning effectiveness. These results indicate that LLMs can be used to
automatically build explanations for warning users against phishing, and that
such solutions are scalable, adaptive, and consistent with human-centred
values.

</details>


### [16] [KeyDroid: A Large-Scale Analysis of Secure Key Storage in Android Apps](https://arxiv.org/abs/2507.07927)
*Jenny Blessing,Ross J. Anderson,Alastair R. Beresford*

Main category: cs.CR

TL;DR: This paper analyzes the adoption and performance of Android's hardware-backed key storage across 490,119 apps. 56.3% of apps handling sensitive data don't use trusted hardware, and only 5.03% use the strongest secure element form. Secure elements show performance limitations for large symmetric encryption and all asymmetric encryption.


<details>
  <summary>Details</summary>
Motivation: Hardware-backed storage protects sensitive data from OS-level attacks, but adoption patterns and performance implications remain poorly understood despite industry initiatives.

Method: Examined 490,119 Android apps for hardware usage, cross-referencing with self-reported data safety labels. Conducted empirical performance analysis of cryptographic operations in hardware vs software keystores across 24 devices.

Result: 56.3% of sensitive data processing apps used no hardware protection. 5.03% used secure elements. Coprocessor-based storage was viable for common operations, but secure elements showed 5-20x slower symmetric encryption for large payloads and 50-100x slower asymmetric encryption.

Conclusion: Industry efforts to promote secure hardware adoption have limited success. Security vs performance tradeoffs in existing implementations (especially secure elements) create challenges for practical deployment of defense-in-depth strategies.

Abstract: Most contemporary mobile devices offer hardware-backed storage for
cryptographic keys, user data, and other sensitive credentials. Such hardware
protects credentials from extraction by an adversary who has compromised the
main operating system, such as a malicious third-party app. Since 2011, Android
app developers can access trusted hardware via the Android Keystore API. In
this work, we conduct the first comprehensive survey of hardware-backed key
storage in Android devices. We analyze 490 119 Android apps, collecting data on
how trusted hardware is used by app developers (if used at all) and
cross-referencing our findings with sensitive user data collected by each app,
as self-reported by developers via the Play Store's data safety labels.
  We find that despite industry-wide initiatives to encourage adoption, 56.3%
of apps self-reporting as processing sensitive user data do not use Android's
trusted hardware capabilities at all, while just 5.03% of apps collecting some
form of sensitive data use the strongest form of trusted hardware, a secure
element distinct from the main processor. To better understand the potential
downsides of using secure hardware, we conduct the first empirical analysis of
trusted hardware performance in mobile devices, measuring the runtime of common
cryptographic operations across both software- and hardware-backed keystores.
We find that while hardware-backed key storage using a coprocessor is viable
for most common cryptographic operations, secure elements capable of preventing
more advanced attacks make performance infeasible for symmetric encryption with
non-negligible payloads and any kind of asymmetric encryption.

</details>


### [17] [EinHops: Einsum Notation for Expressive Homomorphic Operations on RNS-CKKS Tensors](https://arxiv.org/abs/2507.07972)
*Karthik Garimella,Austin Ebel,Brandon Reagen*

Main category: cs.CR

TL;DR: EinHops addresses FHE limitations in multi-dimensional tensor operations by leveraging Einstein summation notation, providing a transparent and general system for encrypted tensor computations.


<details>
  <summary>Details</summary>
Motivation: FHE's 1-D operation constraints force tensor packing into vectors, with existing systems hiding critical decisions in abstractions. This hinders debugging and optimization, necessitating a transparent approach to tensor operations.

Method: The authors decompose einsum expressions into FHE-compatible operations, implementing EinHops to explicitly expose packing strategies through einsum's dimensional syntax. This avoids abstracted packing decisions in prior systems.

Result: EinHops was evaluated on diverse tensor operations, showing it achieves simplicity, generality, and interpretability. The open-source implementation provides a foundation for further encrypted computing advancements.

Conclusion: EinHops demonstrates that einsum's explicitness enables simple, interpretable FHE tensor systems with full visibility into packing strategies, validated across operations from transposes to contractions.

Abstract: Fully Homomorphic Encryption (FHE) is an encryption scheme that allows for
computation to be performed directly on encrypted data, effectively closing the
loop on secure and outsourced computing. Data is encrypted not only during rest
and transit, but also during processing. However, FHE provides a limited
instruction set: SIMD addition, SIMD multiplication, and cyclic rotation of 1-D
vectors. This restriction makes performing multi-dimensional tensor operations
challenging. Practitioners must pack these tensors into 1-D vectors and map
tensor operations onto this one-dimensional layout rather than their
traditional nested structure. And while prior systems have made significant
strides in automating this process, they often hide critical packing decisions
behind layers of abstraction, making debugging, optimizing, and building on top
of these systems difficult.
  In this work, we approach multi-dimensional tensor operations in FHE through
Einstein summation (einsum) notation. Einsum notation explicitly encodes
dimensional structure and operations in its syntax, naturally exposing how
tensors should be packed and transformed. We decompose einsum expressions into
a fixed set of FHE-friendly operations. We implement our design and present
EinHops, a minimalist system that factors einsum expressions into a fixed
sequence of FHE operations. EinHops enables developers to perform encrypted
tensor operations using FHE while maintaining full visibility into the
underlying packing strategy. We evaluate EinHops on a range of tensor
operations from a simple transpose to complex multi-dimensional contractions.
We show that the explicit nature of einsum notation allows us to build an FHE
tensor system that is simple, general, and interpretable. We open-source
EinHops at the following repository: https://github.com/baahl-nyu/einhops.

</details>


### [18] [Defending Against Prompt Injection With a Few DefensiveTokens](https://arxiv.org/abs/2507.07974)
*Sizhe Chen,Yizhu Wang,Nicholas Carlini,Chawin Sitawarin,David Wagner*

Main category: cs.CR

TL;DR: Introduces DefensiveTokens for test-time LLM security against prompt injection attacks, balancing high security and utility without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing test-time LLM defenses are ineffective compared to retraining-based approaches. There is a need for effective test-time security with minimal utility loss that can be optionally activated.

Method: Proposes DefensiveTokens: special tokens with optimized embeddings inserted before LLM inputs. These tokens are designed to neutralize prompt injection attacks by contextually overriding malicious inputs without altering model parameters.

Result: Achieves security comparable to training-time defenses when using DefensiveTokens, while maintaining low utility impact when security is disabled. Developers can dynamically enable/disable security.

Conclusion: DefensiveTokens provide a test-time defense against prompt injection attacks, enabling LLM systems to switch between SOTA utility and near-SOTA security without model parameter changes, offering flexibility for developers. The method's availability via GitHub demonstrates its practical implementation.

Abstract: When large language model (LLM) systems interact with external data to
perform complex tasks, a new attack, namely prompt injection, becomes a
significant threat. By injecting instructions into the data accessed by the
system, the attacker is able to override the initial user task with an
arbitrary task directed by the attacker. To secure the system, test-time
defenses, e.g., defensive prompting, have been proposed for system developers
to attain security only when needed in a flexible manner. However, they are
much less effective than training-time defenses that change the model
parameters. Motivated by this, we propose DefensiveToken, a test-time defense
with prompt injection robustness comparable to training-time alternatives.
DefensiveTokens are newly inserted as special tokens, whose embeddings are
optimized for security. In security-sensitive cases, system developers can
append a few DefensiveTokens before the LLM input to achieve security with a
minimal utility drop. In scenarios where security is less of a concern,
developers can simply skip DefensiveTokens; the LLM system remains the same as
there is no defense, generating high-quality responses. Thus, DefensiveTokens,
if released alongside the model, allow a flexible switch between the
state-of-the-art (SOTA) utility and almost-SOTA security at test time. The code
is available at https://github.com/Sizhe-Chen/DefensiveToken.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [19] [A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering](https://arxiv.org/abs/2507.07325)
*Martin Obaidi,Marc Herrmann,Elisa Schmid,Raymond Ochsner,Kurt Schneider,Jil Kl√ºnder*

Main category: cs.SE

TL;DR: This paper introduces a German dataset of 5,949 developer statements annotated with six basic emotions to address the lack of domain-specific sentiment analysis tools in German software engineering contexts. The dataset shows high reliability and is intended to support German-speaking communities.


<details>
  <summary>Details</summary>
Motivation: Current sentiment analysis tools in software engineering rely on English or non-German gold-standard datasets, leaving a gap for German-speaking developer communities. This limits the availability of accurate, context-specific emotional analysis in German contexts.

Method: The dataset was collected from the German developer forum Android-Hilfe.de. Developer statements were annotated with six basic emotions (Shaver et al.'s model) by four German-speaking computer science students. Inter-rater agreement and reliability were evaluated to validate the dataset.

Result: The German dataset exhibited high inter-rater agreement and reliability, confirming its validity. Evaluations revealed that existing German sentiment analysis tools lack domain specificity for software engineering.

Conclusion: The presented dataset provides a reliable foundation for sentiment analysis in German software engineering contexts. It highlights the need for domain-specific tools and proposes approaches to optimize annotation processes along with additional use cases.

Abstract: Sentiment analysis is an essential technique for investigating the emotional
climate within developer teams, contributing to both team productivity and
project success. Existing sentiment analysis tools in software engineering
primarily rely on English or non-German gold-standard datasets. To address this
gap, our work introduces a German dataset of 5,949 unique developer statements,
extracted from the German developer forum Android-Hilfe.de. Each statement was
annotated with one of six basic emotions, based on the emotion model by Shaver
et al., by four German-speaking computer science students. Evaluation of the
annotation process showed high interrater agreement and reliability. These
results indicate that the dataset is sufficiently valid and robust to support
sentiment analysis in the German-speaking software engineering community.
Evaluation with existing German sentiment analysis tools confirms the lack of
domain-specific solutions for software engineering. We also discuss approaches
to optimize annotation and present further use cases for the dataset.

</details>


### [20] [Automatic Generation of Explainability Requirements and Software Explanations From User Reviews](https://arxiv.org/abs/2507.07344)
*Martin Obaidi,Jannik Fischbach,Jakob Droste,Hannah Deters,Marc Herrmann,Jil Kl√ºnder,Steffen Kr√§tzig,Hugo Villamizar,Kurt Schneider*

Main category: cs.SE

TL;DR: This paper introduces an automated tool-supported approach to derive explainability requirements from user reviews and generate aligned explanations, while highlighting the tradeoff between AI-generated clarity/style vs human validation for correctness.


<details>
  <summary>Details</summary>
Motivation: Explainability requirements are critical for transparency and trust but current methods lack systematic approaches to translate user feedback into structured requirements and explanations.

Method: Developed an industry-validated tool to automatically parse 58 annotated user reviews, extract requirements, and generate explanations while benchmarking AI output against human-crafted ones

Result: AI-generated explanations were preferred for clarity/style (78% acceptance) but requirements lacked relevance/correctness (62% acceptable). Human validation remains essential for correctness despite AI benefits. Dataset released for future research.

Conclusion: The work presents three contributions: 1) Automated explainability requirements derivation pipeline, 2) Empirical analysis of AI vs human artifacts, 3) Publicly available annotated dataset enabling research in this area.

Abstract: Explainability has become a crucial non-functional requirement to enhance
transparency, build user trust, and ensure regulatory compliance. However,
translating explanation needs expressed in user feedback into structured
requirements and corresponding explanations remains challenging. While existing
methods can identify explanation-related concerns in user reviews, there is no
established approach for systematically deriving requirements and generating
aligned explanations. To contribute toward addressing this gap, we introduce a
tool-supported approach that automates this process. To evaluate its
effectiveness, we collaborated with an industrial automation manufacturer to
create a dataset of 58 user reviews, each annotated with manually crafted
explainability requirements and explanations. Our evaluation shows that while
AI-generated requirements often lack relevance and correctness compared to
human-created ones, the AI-generated explanations are frequently preferred for
their clarity and style. Nonetheless, correctness remains an issue,
highlighting the importance of human validation. This work contributes to the
advancement of explainability requirements in software systems by (1)
introducing an automated approach to derive requirements from user reviews and
generate corresponding explanations, (2) providing empirical insights into the
strengths and limitations of automatically generated artifacts, and (3)
releasing a curated dataset to support future research on the automatic
generation of explainability requirements.

</details>


### [21] [Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN](https://arxiv.org/abs/2507.07468)
*Sten Gr√ºner,Nafise Eskandani*

Main category: cs.SE

TL;DR: This paper proposes a distributed AAS copy-on-write infrastructure combined with BPMN workflows to enhance secure, scalable, and automated engineering processes across organizations.


<details>
  <summary>Details</summary>
Motivation: Industry 4.0 demands automated plant/process engineering workflows enabled by interoperable Digital Twins, but existing solutions lack robust security, scalability, and cross-organizational collaboration capabilities.

Method: The authors develop a distributed copy-on-write AAS infrastructure to address security and scalability challenges, while implementing a workflow management prototype driven by BPMN to automate AAS operations.

Result: The proposed infrastructure enables seamless AAS engineering automation, cross-organizational collaboration, and improved process traceability, validated through a workflow management prototype demonstrating enhanced efficiency.

Conclusion: The integration of distributed AAS infrastructure with BPMN workflows offers a promising approach to advancing interoperable, automated engineering processes in Industry 4.0 contexts.

Abstract: The integration of Industry 4.0 technologies into engineering workflows is an
essential step toward automating and optimizing plant and process engineering
processes. The Asset Administration Shell (AAS) serves as a key enabler for
creating interoperable Digital Twins that facilitate engineering data exchange
and automation. This paper explores the use of AAS within engineering
workflows, particularly in combination with Business Process Model and Notation
(BPMN) to define structured and automated processes. We propose a distributed
AAS copy-on-write infrastructure that enhances security and scalability while
enabling seamless cross organizational collaboration. We also introduce a
workflow management prototype automating AAS operations and engineering
workflows, improving efficiency and traceability.

</details>


### [22] [From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering](https://arxiv.org/abs/2507.07548)
*Jonathan Ullrich,Matthias Koch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: This paper investigates how developers use requirements and design artifacts with LLMs for code generation.


<details>
  <summary>Details</summary>
Motivation: The shift in envisioning traditional software engineering due to advanced LLM code generation capabilities necessitates understanding current developer practices in incorporating requirements-a largely unexplored area.

Method: Qualitative analysis through interviews with 18 practitioners from 14 companies about their use of requirements and design artifacts in LLM code generation.

Result: A theory proposing that 'requirements must be manually decomposed into programming tasks and enriched with design decisions/architectural constraints before being used for LLM prompting'.

Conclusion: While LLMs enable new workflows, fundamental requirements engineering work remains essential for effective code generation, highlighting ongoing needs for RE-AI integration research.

Abstract: With the advent of generative LLMs and their advanced code generation
capabilities, some people already envision the end of traditional software
engineering, as LLMs may be able to produce high-quality code based solely on
the requirements a domain expert feeds into the system. The feasibility of this
vision can be assessed by understanding how developers currently incorporate
requirements when using LLMs for code generation-a topic that remains largely
unexplored. We interviewed 18 practitioners from 14 companies to understand how
they (re)use information from requirements and other design artifacts to feed
LLMs when generating code. Based on our findings, we propose a theory that
explains the processes developers employ and the artifacts they rely on. Our
theory suggests that requirements, as typically documented, are too abstract
for direct input into LLMs. Instead, they must first be manually decomposed
into programming tasks, which are then enriched with design decisions and
architectural constraints before being used in prompts. Our study highlights
that fundamental RE work is still necessary when LLMs are used to generate
code. Our theory is important for contextualizing scientific approaches to
automating requirements-centric SE tasks.

</details>


### [23] [Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap](https://arxiv.org/abs/2507.07682)
*Kaicheng Huang,Fanyu Wang,Yutan Huang,Chetan Arora*

Main category: cs.SE

TL;DR: This paper presents a systematic review and roadmap for integrating prompt engineering in requirements engineering, addressing current fragmentation and offering a taxonomy of techniques.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of controllability and uncertainty in LLM applications for RE tasks, highlighting the need for structured guidance to enable trustworthy implementation in RE.

Method: Conducted a systematic literature review following Kitchenham and Petersen's protocol, screening 867 records across six digital libraries and analyzing 35 primary studies. Developed a hybrid taxonomy and two research questions with five sub-questions.

Result: The review proposes a hybrid taxonomy linking prompt engineering techniques to RE tasks and identifies current limitations and research gaps. A roadmap is outlined to transition from ad-hoc PE prototypes to structured workflows.

Conclusion: The authors emphasize the importance of transitioning from ad-hoc prompt engineering methods to reproducible, practitioner-friendly workflows as a foundational step for LLM adoption in requirements engineering.

Abstract: Advancements in large language models (LLMs) have led to a surge of prompt
engineering (PE) techniques that can enhance various requirements engineering
(RE) tasks. However, current LLMs are often characterized by significant
uncertainty and a lack of controllability. This absence of clear guidance on
how to effectively prompt LLMs acts as a barrier to their trustworthy
implementation in the RE field. We present the first roadmap-oriented
systematic literature review of Prompt Engineering for RE (PE4RE). Following
Kitchenham's and Petersen's secondary-study protocol, we searched six digital
libraries, screened 867 records, and analyzed 35 primary studies. To bring
order to a fragmented landscape, we propose a hybrid taxonomy that links
technique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented
RE roles (elicitation, validation, traceability). Two research questions, with
five sub-questions, map the tasks addressed, LLM families used, and prompt
types adopted, and expose current limitations and research gaps. Finally, we
outline a step-by-step roadmap showing how today's ad-hoc PE prototypes can
evolve into reproducible, practitioner-friendly workflows.

</details>


### [24] [From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry](https://arxiv.org/abs/2507.07689)
*Chetan Arora,Fanyu Wang,Chakkrit Tantithamthavorn,Aldeida Aleti,Shaun Kenyon*

Main category: cs.SE

TL;DR: This paper proposes using Retrieval-Augmented Generation (RAG) and large language models (LLMs) to semi-automate requirements engineering (RE) in the space industry, reducing manual effort and improving compliance for smaller organizations.


<details>
  <summary>Details</summary>
Motivation: Smaller space organizations face challenges in deriving actionable requirements from unstructured documents due to complexity, standards alignment, and mission-specific constraints, necessitating automation.

Method: A modular AI-driven approach that preprocesses mission documents, classifies content, retrieves relevant standards, and generates draft requirements using LLMs.

Result: Preliminary results show reduced manual effort, improved requirement coverage, and enhanced compliance alignment. A practical implementation was tested with industry partner Starbound Space Solutions.

Conclusion: The authors outline a roadmap for integrating AI into RE workflows to lower barriers for smaller space organizations in safety-critical missions.

Abstract: Requirements engineering (RE) in the space industry is inherently complex,
demanding high precision, alignment with rigorous standards, and adaptability
to mission-specific constraints. Smaller space organisations and new entrants
often struggle to derive actionable requirements from extensive, unstructured
documents such as mission briefs, interface specifications, and regulatory
standards. In this innovation opportunity paper, we explore the potential of
Retrieval-Augmented Generation (RAG) models to support and (semi-)automate
requirements generation in the space domain. We present a modular, AI-driven
approach that preprocesses raw space mission documents, classifies them into
semantically meaningful categories, retrieves contextually relevant content
from domain standards, and synthesises draft requirements using large language
models (LLMs). We apply the approach to a real-world mission document from the
space domain to demonstrate feasibility and assess early outcomes in
collaboration with our industry partner, Starbound Space Solutions. Our
preliminary results indicate that the approach can reduce manual effort,
improve coverage of relevant requirements, and support lightweight compliance
alignment. We outline a roadmap toward broader integration of AI in RE
workflows, intending to lower barriers for smaller organisations to participate
in large-scale, safety-critical missions.

</details>
