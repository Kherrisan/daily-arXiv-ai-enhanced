<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 23]
- [cs.SE](#cs.SE) [Total: 20]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [When and Where do Data Poisons Attack Textual Inversion?](https://arxiv.org/abs/2507.10578)
*Jeremy Styborski,Mingzhi Lyu,Jiayou Lu,Nupur Kapur,Adams Kong*

Main category: cs.CR

TL;DR: This paper analyzes poisoning attacks on textual inversion (TI) in diffusion models, introduces Semantic Sensitivity Maps for visualization, and proposes Safe-Zone Training (SZT) with JPEG compression, high-timestep restriction, and loss masking to enhance robustness against poisoning, outperforming prior defenses in generative quality.


<details>
  <summary>Details</summary>
Motivation: The robustness of diffusion models (DMs) to poisoning attacks is a critical challenge, especially for TI techniques, as adversarial signals in training data can corrupt personalization processes. Understanding attack patterns in timesteps or text embeddings is required for effective defense.

Method: 1) Proposed Semantic Sensitivity Maps to identify poisoning effects on text embeddings. 2) Analyzed non-uniform learning behavior across timesteps, where poisoning attacks affect lower-noise regions. 3) Developed Safe-Zone Training (SZT) with three components: JPEG compression to reduce high-frequency poison signals, restricting training to high timesteps to avoid adversarial influence, and loss masking to focus learning on relevant data regions.

Result: SZT significantly improves TI robustness against all tested poisoning attacks. Experiments show enhanced generative quality compared to existing defenses, with reduced corruption to training data from adversarial signals and better preservation of personalized features.

Conclusion: The study reveals poisoning attack patterns in TI for diffusion models and introduces SZT as a comprehensive defense mechanism addressing these vulnerabilities. By leveraging semantic visualization, temporal learning bias, and spatial constraints, SZT achieves state-of-the-art robustness while maintaining high generative quality.

Abstract: Poisoning attacks pose significant challenges to the robustness of diffusion
models (DMs). In this paper, we systematically analyze when and where poisoning
attacks textual inversion (TI), a widely used personalization technique for
DMs. We first introduce Semantic Sensitivity Maps, a novel method for
visualizing the influence of poisoning on text embeddings. Second, we identify
and experimentally verify that DMs exhibit non-uniform learning behavior across
timesteps, focusing on lower-noise samples. Poisoning attacks inherit this bias
and inject adversarial signals predominantly at lower timesteps. Lastly, we
observe that adversarial signals distract learning away from relevant concept
regions within training data, corrupting the TI process. Based on these
insights, we propose Safe-Zone Training (SZT), a novel defense mechanism
comprised of 3 key components: (1) JPEG compression to weaken high-frequency
poison signals, (2) restriction to high timesteps during TI training to avoid
adversarial signals at lower timesteps, and (3) loss masking to constrain
learning to relevant regions. Extensive experiments across multiple poisoning
methods demonstrate that SZT greatly enhances the robustness of TI against all
poisoning attacks, improving generative quality beyond prior published
defenses. Code: www.github.com/JStyborski/Diff_Lab Data:
www.github.com/JStyborski/NC10

</details>


### [2] [Breaking a 5-Bit Elliptic Curve Key using a 133-Qubit Quantum Computer](https://arxiv.org/abs/2507.10592)
*Steve Tippeconnic*

Main category: cs.CR

TL;DR: A 5-bit elliptic curve key was broken using a Shor-style quantum attack on a 133-qubit IBM processor, revealing k=7 through 16,384 shots of a 15-qubit circuit without direct encoding.


<details>
  <summary>Details</summary>
Motivation: Demonstrates practical feasibility of quantum attacks against elliptic curve cryptography, highlighting vulnerabilities without direct key encoding in a resource-efficient implementation using current quantum hardware.

Method: Executed a 15-qubit Shor-style circuit (10 logical + 5 ancilla) on IBM's ibm_torino with Qiskit Runtime 2.0. Utilized a 67,000-layer deep circuit over an order-32 subgroup to solve Q = kP via quantum interference in a 32×32 QFT space.

Result: Found a diagonal ridge in QFT outcomes; classical post-processing identified k=7 as the secret scalar from 16,384 quantum shots with a 15-qubit circuit on current NISQ hardware.

Conclusion: Experimental validation shows quantum key attacks (e.g., Shor-style) can work with ~10 logical qubits, not requiring direct key encoding or quantum error correction, underscoring urgent need for post-quantum cryptographic solutions.

Abstract: This experiment breaks a 5-bit elliptic curve cryptographic key using a
Shor-style quantum attack. Executed on IBM's 133-qubit ibm_torino with Qiskit
Runtime 2.0, a 15-qubit circuit, comprised of 10 logical qubits and 5 ancilla,
interferes over an order-32 elliptic curve subgroup to extract the secret
scalar k from the public key relation Q = kP, without ever encoding k directly
into the oracle. From 16,384 shots, the quantum interference reveals a diagonal
ridge in the 32 x 32 QFT outcome space. The quantum circuit, over 67,000 layers
deep, produced valid interference patterns despite extreme circuit depth, and
classical post-processing revealed k = 7 in the top 100 invertible (a, b)
results. All code, circuits, and raw data are publicly available for
replication.

</details>


### [3] [LaSM: Layer-wise Scaling Mechanism for Defending Pop-up Attack on GUI Agents](https://arxiv.org/abs/2507.10610)
*Zihe Yan,Zhuosheng Zhang*

Main category: cs.CR

TL;DR: This paper proposes LaSM, a layer-wise scaling mechanism to defend GUI agents against pop-up attacks by aligning attention with task-relevant regions, achieving over 98% robustness without retraining.


<details>
  <summary>Details</summary>
Motivation: Current GUI agents using MLLMs are vulnerable to environmental injection attacks via pop-ups, requiring costly retraining for defense.

Method: LaSM selectively amplifies attention and MLP modules in critical layers to realign model saliency with task-relevant regions during inference.

Result: Experiments with 12 pop-up types across 4 MLLM backbones show consistent defense improvement (98%+ robustness with prompt-level alerts).

Conclusion: Attention misalignment is the core vulnerability in MLLM agents, and selective layer-wise modulation effectively enhances robustness against environmental attacks.

Abstract: Graphical user interface (GUI) agents built on multimodal large language
models (MLLMs) have recently demonstrated strong decision-making abilities in
screen-based interaction tasks. However, they remain highly vulnerable to
pop-up-based environmental injection attacks, where malicious visual elements
divert model attention and lead to unsafe or incorrect actions. Existing
defense methods either require costly retraining or perform poorly under
inductive interference. In this work, we systematically study how such attacks
alter the attention behavior of GUI agents and uncover a layer-wise attention
divergence pattern between correct and incorrect outputs. Based on this
insight, we propose \textbf{LaSM}, a \textit{Layer-wise Scaling Mechanism} that
selectively amplifies attention and MLP modules in critical layers. LaSM
improves the alignment between model saliency and task-relevant regions without
additional training. Extensive experiments across 12 types of pop-up
perturbations and 4 different model backbones show that LaSM consistently
enhances the defense success rate. When combined with prompt-level alerts, LaSM
achieves over 98\% robustness even under strong inductive attacks. Our findings
reveal that attention misalignment is a core vulnerability in MLLM agents and
can be effectively addressed through selective layer-wise modulation.

</details>


### [4] [Game Theory Meets LLM and Agentic AI: Reimagining Cybersecurity for the Age of Intelligent Threats](https://arxiv.org/abs/2507.10621)
*Quanyan Zhu*

Main category: cs.CR

TL;DR: This paper explores integrating game theory, large language models (LLMs), and agentic AI to bridge the disconnect between cybersecurity theory and practical implementation, offering proactive and adaptive defense systems.


<details>
  <summary>Details</summary>
Motivation: Traditional cybersecurity methods rely on outdated manual responses and rigid heuristics. Recent advances in AI and game theory provide opportunities to develop intelligent, adaptive systems that model adversarial behavior and operationalize strategic defenses.

Method: The authors analyze game-theoretic frameworks (static, dynamic, Bayesian, signaling games) and demonstrate how LLM-powered agents can operationalize abstract strategies into decisions. They introduce LLM-driven game models and examine multi-agent workflows for coordination across complex cyber workflows.

Result: The study reveals that LLM agents enhance cyber defense through modular, trust-aware decision-making while challenging classical game-theoretic assumptions (e.g., perfect rationality), enabling new models aligned with computational realities.

Conclusion: The convergence of game theory, agentic AI, and cybersecurity creates secure, intelligent, and adaptive systems through theoretical advancements and practical implementation of LLM-driven multi-agent coordination.

Abstract: Protecting cyberspace requires not only advanced tools but also a shift in
how we reason about threats, trust, and autonomy. Traditional cybersecurity
methods rely on manual responses and brittle heuristics. To build proactive and
intelligent defense systems, we need integrated theoretical frameworks and
software tools. Game theory provides a rigorous foundation for modeling
adversarial behavior, designing strategic defenses, and enabling trust in
autonomous systems. Meanwhile, software tools process cyber data, visualize
attack surfaces, verify compliance, and suggest mitigations. Yet a disconnect
remains between theory and practical implementation.
  The rise of Large Language Models (LLMs) and agentic AI offers a new path to
bridge this gap. LLM-powered agents can operationalize abstract strategies into
real-world decisions. Conversely, game theory can inform the reasoning and
coordination of these agents across complex workflows. LLMs also challenge
classical game-theoretic assumptions, such as perfect rationality or static
payoffs, prompting new models aligned with cognitive and computational
realities. This co-evolution promises richer theoretical foundations and novel
solution concepts. Agentic AI also reshapes software design: systems must now
be modular, adaptive, and trust-aware from the outset.
  This chapter explores the intersection of game theory, agentic AI, and
cybersecurity. We review key game-theoretic frameworks (e.g., static, dynamic,
Bayesian, and signaling games) and solution concepts. We then examine how LLM
agents can enhance cyber defense and introduce LLM-driven games that embed
reasoning into AI agents. Finally, we explore multi-agent workflows and
coordination games, outlining how this convergence fosters secure, intelligent,
and adaptive cyber systems.

</details>


### [5] [Spectral Feature Extraction for Robust Network Intrusion Detection Using MFCCs](https://arxiv.org/abs/2507.10622)
*HyeYoung Lee,Muhammad Nadeem,Pavel Tsoi*

Main category: cs.CR

TL;DR: The paper proposes an IoT network anomaly detection framework combining learnable MFCCs and ResNet-18 for adaptive spectral feature extraction and robust multiclass classification.


<details>
  <summary>Details</summary>
Motivation: IoT network expansion has created security vulnerabilities, necessitating improved adaptive techniques for detecting and classifying anomalies in heterogeneous environments.

Method: Uses learnable MFCCs to transform raw network signals into higher-dimensional acoustic features, exploiting ResNet-18's feature extraction capabilities for temporal pattern analysis and class separation.

Result: Demonstrated effectiveness through evaluation on CICIoT2023, NSL-KDD, and IoTID20 datasets, showing improved multiclass classification performance compared to traditional methods.

Conclusion: Integration of adaptive signal processing with deep learning architectures provides a scalable solution for robust anomaly detection in complex IoT network landscapes.

Abstract: The rapid expansion of Internet of Things (IoT) networks has led to a surge
in security vulnerabilities, emphasizing the critical need for robust anomaly
detection and classification techniques. In this work, we propose a novel
approach for identifying anomalies in IoT network traffic by leveraging the
Mel-frequency cepstral coefficients (MFCC) and ResNet-18, a deep learning model
known for its effectiveness in feature extraction and image-based tasks.
Learnable MFCCs enable adaptive spectral feature representation, capturing the
temporal patterns inherent in network traffic more effectively than traditional
fixed MFCCs. We demonstrate that transforming raw signals into MFCCs maps the
data into a higher-dimensional space, enhancing class separability and enabling
more effective multiclass classification. Our approach combines the strengths
of MFCCs with the robust feature extraction capabilities of ResNet-18, offering
a powerful framework for anomaly detection. The proposed model is evaluated on
three widely used IoT intrusion detection datasets: CICIoT2023, NSL-KDD, and
IoTID20. The experimental results highlight the potential of integrating
adaptive signal processing techniques with deep learning architectures to
achieve robust and scalable anomaly detection in heterogeneous IoT network
landscapes.

</details>


### [6] [Crypto-Assisted Graph Degree Sequence Release under Local Differential Privacy](https://arxiv.org/abs/2507.10627)
*Xiaojian Zhang,Junqing Wang,Kerui Chen,Peiyuan Zhao,Huiyuan Bai*

Main category: cs.CR

TL;DR: This paper proposes CADR-LDP, a framework that enhances local differential privacy for graph degree sequences by optimizing threshold parameter selection and edge addition mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing graph projection techniques based on edge deletion suffer from a suboptimal threshold trade-off: high thresholds introduce excessive noise while low thresholds unnecessarily remove edges. Traditional threshold selection methods also impose high communication costs.

Method: CADR-LDP combines encryption techniques with differential privacy through two components: (1) crypto-assisted Optimal-θ-Selection for choosing the threshold parameter with low communication cost, and (2) LPEA-LOW method, which adds edges locally in projected graphs by prioritizing low-degree nodes to minimize projection errors.

Result: Theoretical validation confirms 

-ε-node local differential privacy, while experiments on eight graph datasets demonstrate superior performance compared to state-of-the-art methods in maintaining degree distribution accuracy.

Conclusion: This research resolves the threshold parameter optimization problem in graph degree sequence release with local differential privacy, achieving better accuracy while maintaining privacy guarantees through a novel edge addition-based projection approach.

Abstract: Given a graph $G$ defined in a domain $\mathcal{G}$, we investigate locally
differentially private mechanisms to release a degree sequence on $\mathcal{G}$
that accurately approximates the actual degree distribution. Existing solutions
for this problem mostly use graph projection techniques based on edge deletion
process, using a threshold parameter $\theta$ to bound node degrees. However,
this approach presents a fundamental trade-off in threshold parameter
selection. While large $\theta$ values introduce substantial noise in the
released degree sequence, small $\theta$ values result in more edges removed
than necessary. Furthermore, $\theta$ selection leads to an excessive
communication cost. To remedy existing solutions' deficiencies, we present
CADR-LDP, an efficient framework incorporating encryption techniques and
differentially private mechanisms to release the degree sequence. In CADR-LDP,
we first use the crypto-assisted Optimal-$\theta$-Selection method to select
the optimal parameter with a low communication cost. Then, we use the LPEA-LOW
method to add some edges for each node with the edge addition process in local
projection. LPEA-LOW prioritizes the projection with low-degree nodes, which
can retain more edges for such nodes and reduce the projection error.
Theoretical analysis shows that CADR-LDP satisfies $\epsilon$-node local
differential privacy. The experimental results on eight graph datasets show
that our solution outperforms existing methods.

</details>


### [7] [Access Control for Information-Theoretically Secure Key-Document Stores](https://arxiv.org/abs/2507.10730)
*Yin Li,Sharad Mehrota,Shantanu Sharma,Komal Kumari*

Main category: cs.CR

TL;DR: A key-based access control method for secure outsourced key-value stores using Shamir's secret-sharing to enable keyword searches without exposing data, access rights, or query output volume.


<details>
  <summary>Details</summary>
Motivation: Secure outsourcing of key-value stores requires protecting data confidentiality, access rights, and query execution transparency, especially against malicious clients and servers.

Method: The approach leverages Shamir's secret-sharing for information-theoretic security, allowing keyword-based document retrieval while maintaining data privacy and enabling detection of unauthorized access attempts.

Result: The technique achieves secure document access with 231.5ms query performance over 5,000 keywords and 500,000 files, and can detect malicious clients/servers preventing unauthorized data access or manipulation.

Conclusion: The proposed method effectively balances information-theoretic security with practical efficiency for outsourced key-value storage, offering robust protection against data leakage and malicious entity actions.

Abstract: This paper presents a novel key-based access control technique for secure
outsourcing key-value stores where values correspond to documents that are
indexed and accessed using keys. The proposed approach adopts Shamir's
secret-sharing that offers unconditional or information-theoretic security. It
supports keyword-based document retrieval while preventing leakage of the data,
access rights of users, or the size (\textit{i}.\textit{e}., volume of the
output that satisfies a query). The proposed approach allows servers to detect
(and abort) malicious clients from gaining unauthorized access to data, and
prevents malicious servers from altering data undetected while ensuring
efficient access -- it takes 231.5ms over 5,000 keywords across 500,000 files.

</details>


### [8] [3S-Attack: Spatial, Spectral and Semantic Invisible Backdoor Attack Against DNN Models](https://arxiv.org/abs/2507.10733)
*Jianyao Yin,Luca Arnaboldi,Honglong Chen,Pascal Berrang*

Main category: cs.CR

TL;DR: We propose a novel backdoor attack called 3S-attack, which is stealthy across spatial, spectral, and semantic domains by using semantic features for trigger extraction, embedding in the spectral domain, and minimizing differences via pixel-level restrictions. Extensive experiments and analysis demonstrate its effectiveness and highlight the need for stronger AI defenses.


<details>
  <summary>Details</summary>
Motivation: Existing backdoor attacks focus on stealth in spatial and spectral domains but neglect the semantic (feature) domain. Incorporating all three domains can evade detection by current defenses and human inspection, posing a significant threat to AI security.

Method: 3S-attack combines Grad-CAM and a preliminary model to extract semantic features as triggers. The trigger is embedded in the spectral domain, followed by spatial-domain pixel-level restrictions to minimize the distance between poisoned and benign samples.

Result: The 3S-attack achieves high stealthiness across all three domains, as validated through extensive experiments on multiple datasets and theoretical analysis, demonstrating its ability to bypass existing detection methods.

Conclusion: Our findings underscore the necessity for advanced defense mechanisms to address backdoor attacks operating stealthily across spatial, spectral, and semantic domains. Code is publicly available for further research.

Abstract: Backdoor attacks involve either poisoning the training data or directly
modifying the model in order to implant a hidden behavior, that causes the
model to misclassify inputs when a specific trigger is present. During
inference, the model maintains high accuracy on benign samples but
misclassifies poisoned samples into an attacker-specified target class.
Existing research on backdoor attacks has explored developing triggers in the
spatial, spectral (frequency), and semantic (feature) domains, aiming to make
them stealthy. While some approaches have considered designing triggers that
are imperceptible in both spatial and spectral domains, few have incorporated
the semantic domain. In this paper, we propose a novel backdoor attack, termed
3S-attack, which is stealthy across the spatial, spectral, and semantic
domains. The key idea is to exploit the semantic features of benign samples as
triggers, using Gradient-weighted Class Activation Mapping (Grad-CAM) and a
preliminary model for extraction. The trigger is then embedded in the spectral
domain, followed by pixel-level restrictions after converting the samples back
to the spatial domain. This process minimizes the distance between poisoned and
benign samples, making the attack harder to detect by existing defenses and
human inspection. Extensive experiments on various datasets, along with
theoretical analysis, demonstrate the stealthiness of 3S-attack and highlight
the need for stronger defenses to ensure AI security. Our code is available at:
https://anonymous.4open.science/r/anon-project-3776/

</details>


### [9] [Contrastive-KAN: A Semi-Supervised Intrusion Detection Framework for Cybersecurity with scarce Labeled Data](https://arxiv.org/abs/2507.10808)
*Mohammad Alikhani,Reza Kazemi*

Main category: cs.CR

TL;DR: This paper proposes a real-time intrusion detection system using semi-supervised contrastive learning with a Kolmogorov-Arnold Network (KAN) to address data imbalances and limited supervision in IoT/IIoT environments.


<details>
  <summary>Details</summary>
Motivation: Cybersecurity in safety-critical IoT/IIoT systems is hindered by scarce labeled attack data and the need for rapid detection capabilities to prevent infrastructure disruptions.

Method: A semi-supervised contrastive learning framework employing KAN leverages unlabeled data to distinguish normal vs. attack patterns, validated on three benchmark datasets using minimal labeled samples.

Result: Outperforms existing contrastive methods, shows superior detection accuracy and robustness compared to MLP in limited supervision scenarios, and provides interpretable model behavior through visualizations of complex relationships.

Conclusion: The KAN-based approach demonstrates effectiveness in multi-class intrusion detection for imbalanced datasets and safety-critical applications, offering reliable performance with high practical value.

Abstract: In the era of the Fourth Industrial Revolution, cybersecurity and intrusion
detection systems are vital for the secure and reliable operation of IoT and
IIoT environments. A key challenge in this domain is the scarcity of labeled
cyber-attack data, as most industrial systems operate under normal conditions.
This data imbalance, combined with the high cost of annotation, hinders the
effective training of machine learning models. Moreover, rapid detection of
attacks is essential, especially in critical infrastructure, to prevent
large-scale disruptions. To address these challenges, we propose a real-time
intrusion detection system based on a semi-supervised contrastive learning
framework using the Kolmogorov-Arnold Network (KAN). Our method leverages
abundant unlabeled data to distinguish between normal and attack behaviors
effectively. We validate our approach on three benchmark datasets: UNSW-NB15,
BoT-IoT, and Gas Pipeline, using only 2.20 percent, 1.28 percent, and 8 percent
of labeled samples, respectively, to simulate real-world conditions.
Experimental results show that our method outperforms existing contrastive
learning-based approaches. We further compare KAN with a traditional multilayer
perceptron (MLP), demonstrating KAN's superior performance in both detection
accuracy and robustness under limited supervision. KAN's ability to model
complex relationships and its learnable activation functions are also explored
and visualized, offering interpretability and potential for rule extraction.
The method supports multi-class classification and proves effective in
safety-critical environments where reliability is paramount.

</details>


### [10] [Reporte de vulnerabilidades en IIoT. Proyecto DEFENDER](https://arxiv.org/abs/2507.10819)
*Pedro Almansa Jiménez,Lorenzo Fernández Maimó,Ángel Luis Peráles Gómez*

Main category: cs.CR

TL;DR: This paper studies IIoT devices, their operational scenarios, security vulnerabilities, attack phases, and countermeasures, emphasizing Machine Learning for industrial cybersecurity.


<details>
  <summary>Details</summary>
Motivation: Understanding IIoT device security vulnerabilities is critical for protecting industrial systems from cyber threats that could disrupt operations or cause safety failures.

Method: The study identifies and examines IIoT device classes, analyzes vulnerabilities and attack vectors, classifies real-world incidents using a taxonomy, and evaluates recent security countermeasures with a focus on Machine Learning.

Result: Results include a taxonomy of IIoT vulnerabilities, detailed analysis of attack phases, documented case classifications, and evaluation of security countermeasure effectiveness.

Conclusion: The report concludes that robust security strategies are essential for industrial environments and highlights Machine Learning's potential to enhance IIoT system protection.

Abstract: The main objective of this technical report is to conduct a comprehensive
study on devices operating within Industrial Internet of Things (IIoT)
environments, describing the scenarios that define this category and analysing
the vulnerabilities that compromise their security. To this end, the report
seeks to identify and examine the main classes of IIoT devices, detailing their
characteristics, functionalities, and roles within industrial systems. This
analysis enables a better understanding of how these devices interact and
fulfil the requirements of critical industrial environments. The report also
explores the specific contexts in which these devices operate, highlighting the
distinctive features of industrial scenarios and the conditions under which the
devices function. Furthermore, it analyses the vulnerabilities affecting IIoT
devices, outlining their vectors, targets, impact, and consequences. The report
then describes the typical phases of an attack, along with a selection of
real-world documented incidents. These cases are classified according to the
taxonomy presented in Section 3, providing a comprehensive view of the
potential threats to security and assessing the impact these vulnerabilities
may have on industrial environments. Finally, the report presents a compilation
of some of the most recent and effective security countermeasures as potential
solutions to the security challenges faced by industrial systems. Special
emphasis is placed on the role of Machine Learning in the development of these
approaches, underscoring its importance in enhancing industrial cybersecurity.

</details>


### [11] [REAL-IoT: Characterizing GNN Intrusion Detection Robustness under Practical Adversarial Attack](https://arxiv.org/abs/2507.10836)
*Zhonghao Zhan,Huichi Zhou,Hamed Haddadi*

Main category: cs.CR

TL;DR: This paper introduces REAL-IoT, a framework to evaluate GNN-based NIDS in IoT environments using realistic datasets and techniques to enhance robustness via LLM-based filtering.


<details>
  <summary>Details</summary>
Motivation: Existing GNN-based network intrusion detection systems are flawed: they overestimate robustness due to evaluation on single datasets and synthetic adversarial perturbations, creating a measurement gap.

Method: REAL-IoT combines canonical datasets to assess generalization under distribution drift, adds a new intrusion dataset from a physical IoT testbed, and employs LLM-based flow filtering to mitigate adversarial examples.

Result: GNN model performance drops significantly on REAL-IoT vs. standard benchmarks, confirming their vulnerability to real-world drift/attacks. LLM filtering demonstrates robustness improvement potential.

Conclusion: The framework exposes critical limitations of current GNN-based NIDS, emphasizing the need for realistic threat modeling, rigorous evaluation practices, and innovative robustness solutions like LLM-based filtering.

Abstract: Graph Neural Network (GNN)-based network intrusion detection systems (NIDS)
are often evaluated on single datasets, limiting their ability to generalize
under distribution drift. Furthermore, their adversarial robustness is
typically assessed using synthetic perturbations that lack realism. This
measurement gap leads to an overestimation of GNN-based NIDS resilience. To
address the limitations, we propose \textbf{REAL-IoT}, a comprehensive
framework for robustness evaluation of GNN-based NIDS in IoT environments. Our
framework presents a methodology that creates a unified dataset from canonical
datasets to assess generalization under drift. In addition, it features a novel
intrusion dataset collected from a physical IoT testbed, which captures network
traffic and attack scenarios under real-world settings. Furthermore, using
REAL-IoT, we explore the usage of Large Language Models (LLMs) to analyze
network data and mitigate the impact of adversarial examples by filtering
suspicious flows. Our evaluations using REAL-IoT reveal performance drops in
GNN models compared to results from standard benchmarks, quantifying their
susceptibility to drift and realistic attacks. We also demonstrate the
potential of LLM-based filtering to enhance robustness. These findings
emphasize the necessity of realistic threat modeling and rigorous measurement
practices for developing resilient IoT intrusion detection systems.

</details>


### [12] [BandFuzz: An ML-powered Collaborative Fuzzing Framework](https://arxiv.org/abs/2507.10845)
*Wenxuan Shi,Hongwei Li,Jiahao Yu,Xinqian Sun,Wenbo Guo,Xinyu Xing*

Main category: cs.CR

TL;DR: Collaborative fuzzing combines multiple fuzzers for robust performance across programs but is hindered by high resource demands and inefficient allocation.


<details>
  <summary>Details</summary>
Motivation: Existing collaborative fuzzing frameworks face limitations due to excessive computational resources and poor resource allocation efficiency, preventing them from achieving optimal effectiveness.

Method: The paper introduces collaborative fuzzing techniques that dynamically select fuzzer combinations while addressing resource allocation inefficiencies among component fuzzers.

Result: The proposed approach demonstrates improved performance over traditional methods by maintaining robustness while mitigating resource-intensive drawbacks.

Conclusion: Collaborative fuzzing offers a path to generic fuzzing solutions but requires further optimization to balance resource allocation for practical use.

Abstract: Collaborative fuzzing has recently emerged as a technique that combines
multiple individual fuzzers and dynamically chooses the appropriate
combinations suited for different programs. Unlike individual fuzzers, which
rely on specific assumptions to maintain their effectiveness, collaborative
fuzzing relaxes the assumptions on target programs, providing constant and
robust performance across various programs. Ideally, collaborative fuzzing
should be a more promising direction toward generic fuzzing solutions, as it
mitigates the need for manual cherry-picking of individual fuzzers. However,
the effectiveness of existing collaborative fuzzing frameworks is limited by
major challenges, such as the need for additional computational resources
compared to individual fuzzers and the inefficient allocation of resources
among the various fuzzers.

</details>


### [13] [PhreshPhish: A Real-World, High-Quality, Large-Scale Phishing Website Dataset and Benchmark](https://arxiv.org/abs/2507.10854)
*Thomas Dalton,Hemanth Gowda,Girish Rao,Sachin Pargi,Alireza Hadj Khodabakhshi,Joseph Rombs,Stephan Jou,Manish Marwah*

Main category: cs.CR

TL;DR: PhreshPhish introduces a large-scale, high-quality phishing dataset that addresses issues in existing datasets like poor-quality, leakage, and unrealistic base rates, providing realistic benchmarks for improved phishing detection research.


<details>
  <summary>Details</summary>
Motivation: Progress in phishing detection is hindered by flawed datasets (poor quality, leakage, unrealistic base rates) that lead to overoptimistic results, necessitating reliable data for accurate model evaluation.

Method: The authors created PhreshPhish by collecting phishing websites with substantial size and quality, minimising leakage, increasing task difficulty, enhancing diversity, and adjusting base rates to real-world scenarios. They also proposed benchmark datasets and trained multiple models to establish baselines.

Result: PhreshPhish is significantly larger and higher quality than existing datasets, with reduced invalid/mislabeled data. The benchmarks enabled realistic evaluation, revealing performance closer to theoretical maximums and providing baseline metrics for comparison.

Conclusion: The availability of PhreshPhish and its benchmarks will facilitate standardized, realistic model comparison, fostering future advances in phishing detection and enabling reproducible research through accessible data on Hugging Face.

Abstract: Phishing remains a pervasive and growing threat, inflicting heavy economic
and reputational damage. While machine learning has been effective in real-time
detection of phishing attacks, progress is hindered by lack of large,
high-quality datasets and benchmarks. In addition to poor-quality due to
challenges in data collection, existing datasets suffer from leakage and
unrealistic base rates, leading to overly optimistic performance results. In
this paper, we introduce PhreshPhish, a large-scale, high-quality dataset of
phishing websites that addresses these limitations. Compared to existing public
datasets, PhreshPhish is substantially larger and provides significantly higher
quality, as measured by the estimated rate of invalid or mislabeled data
points. Additionally, we propose a comprehensive suite of benchmark datasets
specifically designed for realistic model evaluation by minimizing leakage,
increasing task difficulty, enhancing dataset diversity, and adjustment of base
rates more likely to be seen in the real world. We train and evaluate multiple
solution approaches to provide baseline performance on the benchmark sets. We
believe the availability of this dataset and benchmarks will enable realistic,
standardized model comparison and foster further advances in phishing
detection. The datasets and benchmarks are available on Hugging Face
(https://huggingface.co/datasets/phreshphish/phreshphish).

</details>


### [14] [From Alerts to Intelligence: A Novel LLM-Aided Framework for Host-based Intrusion Detection](https://arxiv.org/abs/2507.10873)
*Danyu Sun,Jinghuai Zhang,Jiacen Xu,Yu Zheng,Yuan Tian,Zhou Li*

Main category: cs.CR

TL;DR: This paper introduces SHIELD, an LLM-based intrusion detection system that combines techniques like event-level MAE and DDA to address limitations in current HIDS, achieving better performance on log datasets.


<details>
  <summary>Details</summary>
Motivation: Current HIDS face challenges like high false-positives and inconsistent results; LLMs offer semantic analysis and attack knowledge potential despite their limitations.

Method: SHIELD integrates event-level Autoencoder for attack window detection, attack evidence handling, Deterministic Data Augmentation for profiling, and custom prompting for precise investigations.

Result: Experiments on three datasets showed SHIELD outperforms five other HIDS with consistent results and reduced false-positives.

Conclusion: LLMs can be effectively customized for intrusion detection; SHIELD demonstrates practical solutions for token limits, noise confusion, and provides interpretable security results.

Abstract: Host-based intrusion detection system (HIDS) is a key defense component to
protect the organizations from advanced threats like Advanced Persistent
Threats (APT). By analyzing the fine-grained logs with approaches like data
provenance, HIDS has shown successes in capturing sophisticated attack traces.
Despite the progresses embarked by the research community and industry, HIDS
still frequently encounters backlash from their operators in the deployed
environments, due to issues like high false-positive rate, inconsistent
outcomes across environments and human-unfriendly detection results. Large
Language Models (LLMs) have great potentials to advance the state of HIDS,
given their extensive knowledge of attack techniques and their ability to
detect anomalies through semantic analysis, anchored by recent studies. Yet,
our preliminary analysis indicates that building an HIDS by naively prompting
an LLM is unlikely to succeed. In this work, we explore the direction of
building a customized LLM pipeline for HIDS and develop a system named SHIELD.
SHIELD addresses challenges related to LLM's token limits, confusion of
background noises, etc., by integrating a variety of techniques like
event-level Masked Autoencoder (MAE) for attack window detection, attack
evidence identification and expansion, Deterministic Data Augmentation (DDA)
for profiling normal activities, and multi-purpose prompting that guides the
LLM to conduct precise and interpretable attack investigations. Extensive
experiments on three log datasets (DARPA-E3, NodLink-simulated-data and
ATLASv2) show that SHIELD consistently achieves outstanding performance in
comparison with 5 representative HIDS. These findings highlight the potential
of LLMs as powerful tools for intrusion detection and pave the way for future
research in this domain.

</details>


### [15] [MalCodeAI: Autonomous Vulnerability Detection and Remediation via Language Agnostic Code Reasoning](https://arxiv.org/abs/2507.10898)
*Jugal Gajjar,Kamalasankari Subramaniakuppusamy,Noha El Kachach*

Main category: cs.CR

TL;DR: MalCodeAI is a language-independent, multi-stage AI framework for autonomous code security analysis and remediation, optimized using LoRA in the MLX framework. It achieves low validation losses (0.397 and 0.199) across 14 programming languages and shows high practical value for real-world software security with positive developer feedback in usability testing.


<details>
  <summary>Details</summary>
Motivation: Traditional vulnerability detection tools are insufficient against increasingly complex cyber threats. There is a critical need for scalable, accurate security solutions that work across programming languages while producing developer-actionable results.

Method: The framework uses two-phase AI pipeline: 1) Functional decomposition and code summarization with fine-tuned Qwen2.5-Coder-3B-Instruct models (200 iterations, 6 trainable layers, 2x10^-5 learning rate) 2) Vulnerability detection and remediation with enhanced training parameters (4x10^-5 learning rate). Supports red-hat exploit tracing, CVSS risk scoring, and zero-shot anomaly detection using Low-Rank Adaptation optimization.

Result: Phase 1 achieved 0.397 validation loss for code decomposition. Phase 2 reached 0.199 validation loss for vulnerability detection. Developer study (n=15) reported 8.06/10 for usefulness, 7.40 for interpretability, and 7.53 for output readability. Demonstrated capabilities for cross-language analysis and zero-day vulnerability detection.

Conclusion: MalCodeAI represents a significant advancement in code security automation, offering an explainable, scalable solution that meets developers' practical needs. The framework sets a new standard for language-agnostic vulnerability detection with its multi-phase design and demonstrated real-world effectiveness.

Abstract: The growing complexity of cyber threats and the limitations of traditional
vulnerability detection tools necessitate novel approaches for securing
software systems. We introduce MalCodeAI, a language-agnostic, multi-stage AI
pipeline for autonomous code security analysis and remediation. MalCodeAI
combines code decomposition and semantic reasoning using fine-tuned
Qwen2.5-Coder-3B-Instruct models, optimized through Low-Rank Adaptation (LoRA)
within the MLX framework, and delivers scalable, accurate results across 14
programming languages. In Phase 1, the model achieved a validation loss as low
as 0.397 for functional decomposition and summarization of code segments after
200 iterations, 6 trainable layers, and a learning rate of 2 x 10^(-5). In
Phase 2, for vulnerability detection and remediation, it achieved a best
validation loss of 0.199 using the same number of iterations and trainable
layers but with an increased learning rate of 4 x 10^(-5), effectively
identifying security flaws and suggesting actionable fixes. MalCodeAI supports
red-hat-style exploit tracing, CVSS-based risk scoring, and zero-shot
generalization to detect complex, zero-day vulnerabilities. In a qualitative
evaluation involving 15 developers, the system received high scores in
usefulness (mean 8.06/10), interpretability (mean 7.40/10), and readability of
outputs (mean 7.53/10), confirming its practical value in real-world
development workflows. This work marks a significant advancement toward
intelligent, explainable, and developer-centric software security solutions.

</details>


### [16] [DVFS: A Dynamic Verifiable Fuzzy Search Service for Encrypted Cloud Data](https://arxiv.org/abs/2507.10927)
*Jie Zhang,Xiaohong Li,Man Zheng,Zhe Hou,Guangdong Bai,Ruitao Feng*

Main category: cs.CR

TL;DR: The paper proposes DVFS, a dynamic verifiable fuzzy search service, to resolve privacy challenges in cloud storage by combining adaptive-secure fuzzy search, dual-repository version control, and blockchain-based verification, achieving sublinear search time and preventing leakage.


<details>
  <summary>Details</summary>
Motivation: Cloud storage's privacy challenges require secure yet efficient encrypted data retrieval. Existing solutions face trade-offs: linear-search mechanisms are secure but inefficient, while tree-based indexes are faster but vulnerable to branch leakage.

Method: DVFS introduces three innovations: (1) Adaptive-secure fuzzy search using locality-sensitive hashing (LSH) and virtual binary trees to reduce search complexity to O(log n) and eliminate branch leakage. (2) Dual-repository version control for dynamic updates with forward privacy. (3) Blockchain-based verification via smart contracts for correctness and completeness with O(log n) complexity.

Result: The authors demonstrate a solution that simultaneously addresses the security-performance paradox in encrypted retrieval, enabling scalable, adaptive, and verifiable search for dynamic cloud data without sacrificing privacy or efficiency.

Conclusion: DVFS advances secure encrypted retrieval in cloud storage by resolving fundamental trade-offs between security and performance, achieving verifiable and leakage-free dynamic operations while maintaining sublinear complexity.

Abstract: Cloud storage introduces critical privacy challenges for encrypted data
retrieval, where fuzzy multi-keyword search enables approximate matching while
preserving data confidentiality. Existing solutions face fundamental trade-offs
between security and efficiency: linear-search mechanisms provide adaptive
security but incur prohibitive overhead for large-scale data, while tree-based
indexes improve performance at the cost of branch leakage vulnerabilities.
  To address these limitations, we propose DVFS - a dynamic verifiable fuzzy
search service with three core innovations: (1) An \textit{adaptive-secure
fuzzy search} method integrating locality-sensitive hashing with virtual binary
trees, eliminating branch leakage while reducing search complexity from linear
to sublinear ($O(\log n)$ time); (2) A \textit{dual-repository version control}
mechanism supporting dynamic updates with forward privacy, preventing
information leakage during operations; (3) A \textit{blockchain-based
verification system} that ensures correctness and completeness via smart
contracts, achieving $O(\log n)$ verification complexity.
  Our solution advances secure encrypted retrieval by simultaneously resolving
the security-performance paradox and enabling trustworthy dynamic operations.

</details>


### [17] [Hashed Watermark as a Filter: Defeating Forging and Overwriting Attacks in Weight-based Neural Network Watermarking](https://arxiv.org/abs/2507.11137)
*Yuan Yao,Jin Song,Jian Jin*

Main category: cs.CR

TL;DR: NeuralMark is a robust neural network watermarking method using a hashed watermark filter to defend against forging, overwriting, and other attacks, validated across 13 architectures and multiple tasks.


<details>
  <summary>Details</summary>
Motivation: Existing weight-based neural network watermarking methods lack resilience against forging and overwriting attacks, necessitating a more secure solution.

Method: NeuralMark employs a hash function to generate an irreversible binary watermark from a secret key, which acts as a filter for parameter embedding. This intertwines embedding parameters with the watermark for robustness, complemented by average pooling to resist fine-tuning and pruning.

Result: The method was empirically validated on 13 Convolutional and Transformer architectures across five image classification tasks and one text generation task, demonstrating robustness against forging, overwriting, fine-tuning, and pruning attacks. Theoretical security bounds were established.

Conclusion: NeuralMark provides a secure and broadly applicable watermarking solution for deep neural networks, effectively addressing existing vulnerabilities in weight-based approaches with minimal computational overhead and open-source availability.

Abstract: As valuable digital assets, deep neural networks necessitate robust ownership
protection, positioning neural network watermarking (NNW) as a promising
solution. Among various NNW approaches, weight-based methods are favored for
their simplicity and practicality; however, they remain vulnerable to forging
and overwriting attacks. To address those challenges, we propose NeuralMark, a
robust method built around a hashed watermark filter. Specifically, we utilize
a hash function to generate an irreversible binary watermark from a secret key,
which is then used as a filter to select the model parameters for embedding.
This design cleverly intertwines the embedding parameters with the hashed
watermark, providing a robust defense against both forging and overwriting
attacks. An average pooling is also incorporated to resist fine-tuning and
pruning attacks. Furthermore, it can be seamlessly integrated into various
neural network architectures, ensuring broad applicability. Theoretically, we
analyze its security boundary. Empirically, we verify its effectiveness and
robustness across 13 distinct Convolutional and Transformer architectures,
covering five image classification tasks and one text generation task. The
source codes are available at https://github.com/AIResearch-Group/NeuralMark.

</details>


### [18] [FacialMotionID: Identifying Users of Mixed Reality Headsets using Abstract Facial Motion Representations](https://arxiv.org/abs/2507.11138)
*Adriano Castro,Simon Hanisch,Matin Fallahi,Thorsten Strufe*

Main category: cs.CR

TL;DR: This study demonstrates that avatar-linked facial motion data from mixed reality headsets can re-identify individuals with 98% balanced accuracy and infer emotional states with 86% accuracy, revealing significant privacy risks in MR environments despite not using raw video data.


<details>
  <summary>Details</summary>
Motivation: The proliferation of immersive mixed reality systems necessitates understanding privacy risks associated with facial motion data, a behavioral biometric that now drives avatar animation for virtual interactions.

Method: 116 participants used three headset types across three sessions for verbal/non-verbal tasks, collecting abstract facial, eye, and head motion representations (not raw video) for analysis.

Result: 98% re-identification accuracy across devices using abstract motion representations, 86% emotional state inference accuracy from motion data alone.

Conclusion: Facial motion tracking for avatar animation poses substantial privacy risks as it enables accurate user identification and sensitive attribute inference, requiring proactive privacy safeguards in MR systems.

Abstract: Facial motion capture in mixed reality headsets enables real-time avatar
animation, allowing users to convey non-verbal cues during virtual
interactions. However, as facial motion data constitutes a behavioral
biometric, its use raises novel privacy concerns. With mixed reality systems
becoming more immersive and widespread, understanding whether face motion data
can lead to user identification or inference of sensitive attributes is
increasingly important.
  To address this, we conducted a study with 116 participants using three types
of headsets across three sessions, collecting facial, eye, and head motion data
during verbal and non-verbal tasks. The data used is not raw video, but rather,
abstract representations that are used to animate digital avatars. Our analysis
shows that individuals can be re-identified from this data with up to 98%
balanced accuracy, are even identifiable across device types, and that
emotional states can be inferred with up to 86% accuracy. These results
underscore the potential privacy risks inherent in face motion tracking in
mixed reality environments.

</details>


### [19] [Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities](https://arxiv.org/abs/2507.11155)
*Yiting Qu,Michael Backes,Yang Zhang*

Main category: cs.CR

TL;DR: This paper evaluates vision-language models (VLMs) for identifying unsafe concepts across modalities and proposes a PPO-based RL approach to improve alignment without human preference data, showing efficiency over SFT/DPO.


<details>
  <summary>Details</summary>
Motivation: Despite VLMs' ethical standards and reasoning abilities, it remains unclear how well they recognize unsafe concepts in both text and images, with open-source models showing a consistent modality gap.

Method: 1) Curated UnsafeConcepts dataset (75 unsafe concepts, 1.5K images); 2) Systematically evaluated 8 VLMs on perception (concept recognition) and alignment (ethical reasoning); 3) Introduced a PPO-based reinforcement learning approach using VLM-generated rewards to bridge the modality gap.

Result: Most VLMs perceive unsafe concepts accurately but occasionally misclassify them as safe. The proposed RL method outperforms supervised fine-tuning (SFT) and direct preference optimization (DPO) in improving image-based alignment while preserving general capabilities.

Conclusion: The study highlights the importance of modality-specific alignment in VLMs and demonstrates that PPO-based RL with internal reward signals effectively addresses safety gaps, offering a scalable solution for safer vision-language systems.

Abstract: Vision-language models (VLMs) are increasingly applied to identify unsafe or
inappropriate images due to their internal ethical standards and powerful
reasoning abilities. However, it is still unclear whether they can recognize
various unsafe concepts when presented in different modalities, such as text
and images. To address this, we first compile the UnsafeConcepts dataset,
featuring 75 unsafe concepts, i.e., ``Swastika,'' ``Sexual Harassment,'' and
``Assaults,'' along with associated 1.5K images. We then conduct a systematic
evaluation of VLMs' perception (concept recognition) and alignment (ethical
reasoning) capabilities. We assess eight popular VLMs and find that, although
most VLMs accurately perceive unsafe concepts, they sometimes mistakenly
classify these concepts as safe. We also identify a consistent modality gap
among open-source VLMs in distinguishing between visual and textual unsafe
concepts. To bridge this gap, we introduce a simplified reinforcement learning
(RL)-based approach using proximal policy optimization (PPO) to strengthen the
ability to identify unsafe concepts from images. Our approach uses reward
scores based directly on VLM responses, bypassing the need for collecting
human-annotated preference data to train a new reward model. Experimental
results show that our approach effectively enhances VLM alignment on images
while preserving general capabilities. It outperforms baselines such as
supervised fine-tuning (SFT) and direct preference optimization (DPO). We hope
our dataset, evaluation findings, and proposed alignment solution contribute to
the community's efforts in advancing safe VLMs.

</details>


### [20] [LRCTI: A Large Language Model-Based Framework for Multi-Step Evidence Retrieval and Reasoning in Cyber Threat Intelligence Credibility Verification](https://arxiv.org/abs/2507.11310)
*Fengxiao Tang,Huan Li,Ming Zhao,Zongzong Wu,Shisong Peng,Tao Yin*

Main category: cs.CR

TL;DR: LRCTI is an LLM-based framework for multi-step Cyber Threat Intelligence (CTI) credibility verification, incorporating text summarization, adaptive evidence retrieval, and interpretable NLI evaluation. It achieves significant improvements in F1 scores compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional CTI verification methods (static classification, handcrafted features, or isolated models) struggle with incomplete/heterogeneous/noisy data and lack decision transparency, reducing their real-world effectiveness.

Method: The framework uses three modules: (1) Text summarization to extract threat claims, (2) Adaptive multi-step evidence retrieval with LLM feedback to refine supporting information from a CTI corpus, and (3) Prompt-based NLI to evaluate claim credibility and generate justifications.

Result: LRCTI outperforms state-of-the-art baselines by 5+ on CTI-200 and PolitiFact datasets, achieving 90.9% F1-Macro and 93.6% F1-Micro scores.

Conclusion: LRCTI addresses critical limitations of prior methods through a scalable, accurate, and explainable approach to CTI credibility verification, making it more robust and transparent for practical threat environments.

Abstract: Verifying the credibility of Cyber Threat Intelligence (CTI) is essential for
reliable cybersecurity defense. However, traditional approaches typically treat
this task as a static classification problem, relying on handcrafted features
or isolated deep learning models. These methods often lack the robustness
needed to handle incomplete, heterogeneous, or noisy intelligence, and they
provide limited transparency in decision-making-factors that reduce their
effectiveness in real-world threat environments. To address these limitations,
we propose LRCTI, a Large Language Model (LLM)-based framework designed for
multi-step CTI credibility verification. The framework first employs a text
summarization module to distill complex intelligence reports into concise and
actionable threat claims. It then uses an adaptive multi-step evidence
retrieval mechanism that iteratively identifies and refines supporting
information from a CTI-specific corpus, guided by LLM feedback. Finally, a
prompt-based Natural Language Inference (NLI) module is applied to evaluate the
credibility of each claim while generating interpretable justifications for the
classification outcome. Experiments conducted on two benchmark datasets,
CTI-200 and PolitiFact show that LRCTI improves F1-Macro and F1-Micro scores by
over 5%, reaching 90.9% and 93.6%, respectively, compared to state-of-the-art
baselines. These results demonstrate that LRCTI effectively addresses the core
limitations of prior methods, offering a scalable, accurate, and explainable
solution for automated CTI credibility verification

</details>


### [21] [A Review of Privacy Metrics for Privacy-Preserving Synthetic Data Generation](https://arxiv.org/abs/2507.11324)
*Frederik Marinus Trudslev,Matteo Lissandrini,Juan Manuel Rodriguez,Martin Bøgsted,Daniele Dell'Aglio*

Main category: cs.CR

TL;DR: This paper analyzes 17 privacy metrics for evaluating privacy risks in synthetic data generation, addressing challenges in interpreting differential privacy's ε parameter.


<details>
  <summary>Details</summary>
Motivation: Differential privacy's ε parameter is hard to interpret, and existing privacy metrics (PMs) inherit assumptions from the PP-SDG mechanisms they assess, necessitating clear PM definitions.

Method: Systematic review and documentation of assumptions and mathematical formulations for 17 established privacy metrics.

Result: Comprehensive presentation of assumptions and equations for 17 PMs, enabling standardized privacy risk assessment.

Conclusion: Thorough documentation of privacy metrics is critical for transparent and accurate evaluation of PP-SDG mechanisms beyond differential privacy's ε.

Abstract: Privacy Preserving Synthetic Data Generation (PP-SDG) has emerged to produce
synthetic datasets from personal data while maintaining privacy and utility.
Differential privacy (DP) is the property of a PP-SDG mechanism that
establishes how protected individuals are when sharing their sensitive data. It
is however difficult to interpret the privacy loss ($\varepsilon$) expressed by
DP. To make the actual risk associated with the privacy loss more transparent,
multiple privacy metrics (PMs) have been proposed to assess the privacy risk of
the data. These PMs are utilized in separate studies to assess newly introduced
PP-SDG mechanisms. Consequently, these PMs embody the same assumptions as the
PP-SDG mechanism they were made to assess. Therefore, a thorough definition of
how these are calculated is necessary. In this work, we present the assumptions
and mathematical formulations of 17 distinct privacy metrics.

</details>


### [22] [Demo: Secure Edge Server for Network Slicing and Resource Allocation in Open RAN](https://arxiv.org/abs/2507.11499)
*Adhwaa Alchaab,Ayman Younis,Dario Pompili*

Main category: cs.CR

TL;DR: SnSRIC is a secure, intelligent network slicing framework for Open RAN environments that dynamically allocates resources and mitigates DDoS attacks through AI-driven xApp and E2 interface throttling.


<details>
  <summary>Details</summary>
Motivation: NGRAN must address diverse vertical application demands with strict security, latency, and SLA requirements, yet faces challenges in infrastructure security, dynamic resource allocation, and real-time reconfiguration.

Method: The framework uses an AI-driven xApp to dynamically allocate PRBs while enforcing slice-level security, detects anomalies, distinguishes benign/malicious devices, and leverages the E2 interface for rogue signaling mitigation.

Result: SnSRIC successfully mitigates DDoS attacks in Open RAN environments while maintaining service continuity for legitimate users through adaptive resource allocation and intelligent threat detection.

Conclusion: SnSRIC demonstrates a viable solution for securing network slicing in NGRAN by combining AI-based resource management with proactive countermeasures against anomalous signaling behavior.

Abstract: Next-Generation Radio Access Networks (NGRAN) aim to support diverse vertical
applications with strict security, latency, and Service-Level Agreement (SLA)
requirements. These demands introduce challenges in securing the
infrastructure, allocating resources dynamically, and enabling real-time
reconfiguration. This demo presents SnSRIC, a secure and intelligent network
slicing framework that mitigates a range of Distributed Denial-of-Service
(DDoS) attacks in Open RAN environments. SnSRIC incorporates an AI-driven xApp
that dynamically allocates Physical Resource Blocks (PRBs) to active users
while enforcing slice-level security. The system detects anomalous behavior,
distinguishes between benign and malicious devices, and uses the E2 interface
to throttle rogue signaling while maintaining service continuity for legitimate
users.

</details>


### [23] [ARMOR: Aligning Secure and Safe Large Language Models via Meticulous Reasoning](https://arxiv.org/abs/2507.11500)
*Zhengyue Zhao,Yingzi Ma,Somesh Jha,Marco Pavone,Chaowei Xiao*

Main category: cs.CR

TL;DR: This paper introduces ARMOR, a reasoning-based safety alignment framework for Large Language Models (LLMs) that enhances robustness against adaptive jailbreak attacks by structuring the safety verification process to align with human intent evaluation and policy-grounded analysis.


<details>
  <summary>Details</summary>
Motivation: Despite post-training safety alignment methods, LLMs remain vulnerable to malicious instructions bypassing existing safety constraints. Current inference-time safety reasoning lacks alignment with structured human processes for intent discernment and risk evaluation, leaving models exposed to sophisticated jailbreak prompts.

Method: ARMOR builds a defense framework with three steps: (1) detecting jailbreak strategies, (2) extracting core user intent while removing deceptive language, and (3) executing policy-oriented safety analyses on the refined request. It replaces ad-hoc reasoning with human-aligned structured thought processes during inference.

Result: Experiments show ARMOR outperforms state-of-the-art reasoning-based LLMs on safety benchmarks and provides significantly enhanced robustness against adaptive jailbreak attacks. Performance improvements further demonstrate benefits of test-time scaling.

Conclusion: Structured human-aligned reasoning in safety alignment provides superior defense capabilities against advanced adversarial prompts, making ARMOR a promising solution for secure LLM deployment.

Abstract: Large Language Models (LLMs) have demonstrated remarkable generative
capabilities. However, their susceptibility to misuse has raised significant
safety concerns. While post-training safety alignment methods have been widely
adopted, LLMs remain vulnerable to malicious instructions that can bypass
safety constraints. Recent efforts have introduced inference-time safety
reasoning (system-2 alignment), where LLMs conduct a reasoning process to
perform safety verification before final response. We show, however, that these
checks are driven by ad-hoc reasoning that diverges from the structured human
process, where they first discern a user's true intent, then evaluate the
associated risk based on the true intent. Consequently, these defenses remain
vulnerable to sophisticated jailbreak prompts that cloak harmful goals in
seemingly benign language. To build secure and safe LLMs, we propose a
reasoning-based safety alignment framework, ARMOR, that replaces the ad-hoc
chains of thought reasoning process with human-aligned, structured one. At
inference, ARMOR (1) detects likely jailbreak strategies, (2) extracts the
user's core intent while discarding deceptive instructions, and (3) applies a
policy-grounded safety analysis to the purified request. ARMOR is evaluated on
adaptive jailbreak attacks and multiple safety benchmarks, and a test-time
scaling is conducted to further improve its performance. Results demonstrate
that ARMOR significantly enhances the robustness against state-of-the-art
adaptive jailbreak attacks and outperforms recent reasoning-based aligned
models across various safety benchmarks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [24] [$\texttt{Droid}$: A Resource Suite for AI-Generated Code Detection](https://arxiv.org/abs/2507.10583)
*Daniil Orel,Indraneil Paul,Iryna Gurevych,Preslav Nakov*

Main category: cs.SE

TL;DR: This paper introduces DroidCollection, a large open dataset for training machine-generated code detectors, and DroidDetect, an encoder-only detection suite. The study highlights generalization failures in existing detectors across diverse domains/languages, shows improved robustness via adversarial training, and validates metric learning and uncertainty-based resampling for noisy data.


<details>
  <summary>Details</summary>
Motivation: Existing code detectors fail to generalize beyond their narrow training data and are vulnerable to output distribution humanization using superficial prompting techniques, necessitating robust training approaches for diverse domains and languages.

Method: 1) Curated DroidCollection with 1M+ samples across 7 languages, 43 coding models, and 3 domains including fully AI-generated, human-AI co-authored, and adversarial samples. 2) Developed DroidDetect using multi-task encoder training on the dataset with 1) domain/languages

Result: Experimental results demonstrate: 1) Existing detectors' poor generalization across domains/languages 2) Their detectors remain robust against superficial prompting attacks 3) Metric learning and uncertainty-based resampling effectively handle noisy distributions while maintaining strong performance (metrics not specified).

Conclusion: DroidCollection enables more robust training of code detectors across domains/languages and provides a benchmark for evaluating adversarial robustness. The proposed training strategies significantly improve detector resilience to surface-level prompting techniques while maintaining accuracy through effective handling of noisy samples.

Abstract: In this work, we compile $\textbf{$\texttt{DroidCollection}$}$, the most
extensive open data suite for training and evaluating machine-generated code
detectors, comprising over a million code samples, seven programming languages,
outputs from 43 coding models, and over three real-world coding domains.
Alongside fully AI-generated samples, our collection includes human-AI
co-authored code, as well as adversarial samples explicitly crafted to evade
detection. Subsequently, we develop $\textbf{$\texttt{DroidDetect}$}$, a suite
of encoder-only detectors trained using a multi-task objective over
$\texttt{DroidCollection}$. Our experiments show that existing detectors'
performance fails to generalise to diverse coding domains and programming
languages outside of their narrow training data. Additionally, we demonstrate
that while most detectors are easily compromised by humanising the output
distributions using superficial prompting and alignment approaches, this
problem can be easily amended by training on a small amount of adversarial
data. Finally, we demonstrate the effectiveness of metric learning and
uncertainty-based resampling as means to enhance detector training on possibly
noisy distributions.

</details>


### [25] [ARPaCCino: An Agentic-RAG for Policy as Code Compliance](https://arxiv.org/abs/2507.10584)
*Francesco Romeo,Luigi Arena,Francesco Blefari,Francesco Aurelio Pironti,Matteo Lupinacci,Angelo Furfaro*

Main category: cs.SE

TL;DR: ARPaCCino is an agentic system combining LLMs, RAG, and validation tools to automate and improve Policy as Code (PaC) generation and verification in Infrastructure as Code (IaC) environments.


<details>
  <summary>Details</summary>
Motivation: Adoption of Policy as Code (PaC) is limited by policy language complexity and misconfiguration risks, prompting the need for automated generation and verification solutions.

Method: ARPaCCino uses Large Language Models (LLMs) to translate natural language policy descriptions into formal Rego rules, leverages Retrieval-Augmented-Generation (RAG) for context-aware policy creation, and integrates tool-based validation to assess IaC compliance and iteratively refine configurations.

Result: In experiments, ARPaCCino successfully generated syntactically/semantically correct PaC policies (Rego rules), identified non-compliant IaC infrastructure, and applied configuration fixes, even with smaller open-weight LLMs in Terraform-based scenarios.

Conclusion: Agentic RAG architectures enable scalable, reliable PaC automation that works across diverse IaC technologies, including niche/emerging frameworks, improving automation efficiency and policy enforcement accuracy.

Abstract: Policy as Code (PaC) is a paradigm that encodes security and compliance
policies into machine-readable formats, enabling automated enforcement in
Infrastructure as Code (IaC) environments. However, its adoption is hindered by
the complexity of policy languages and the risk of misconfigurations. In this
work, we present ARPaCCino, an agentic system that combines Large Language
Models (LLMs), Retrieval-Augmented-Generation (RAG), and tool-based validation
to automate the generation and verification of PaC rules. Given natural
language descriptions of the desired policies, ARPaCCino generates formal Rego
rules, assesses IaC compliance, and iteratively refines the IaC configurations
to ensure conformance. Thanks to its modular agentic architecture and
integration with external tools and knowledge bases, ARPaCCino supports policy
validation across a wide range of technologies, including niche or emerging IaC
frameworks. Experimental evaluation involving a Terraform-based case study
demonstrates ARPaCCino's effectiveness in generating syntactically and
semantically correct policies, identifying non-compliant infrastructures, and
applying corrective modifications, even when using smaller, open-weight LLMs.
Our results highlight the potential of agentic RAG architectures to enhance the
automation, reliability, and accessibility of PaC workflows.

</details>


### [26] [Repairing Language Model Pipelines by Meta Self-Refining Competing Constraints at Runtime](https://arxiv.org/abs/2507.10590)
*Mojtaba Eshghie*

Main category: cs.SE

TL;DR: The paper introduces Meta Self-Refining, a framework that addresses competing soft constraints in Language Model pipelines by adding a meta-corrective layer which monitors execution history and utilizes a meta-repairer LM to resolve backtracking loops, enhancing efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing Language Model pipelines fail to handle competing soft constraints effectively, causing inefficient backtracking loops where satisfying one constraint violates another, necessitating a solution to improve runtime efficiency.

Method: Meta Self-Refining employs a meta-corrective layer that monitors execution history to detect oscillatory failures. When such failures are identified, a separate meta-repairer LM analyzes backtracking attempts and generates strategic instructions to resolve constraint conflicts during runtime.

Result: Experimental results demonstrate that Meta Self-Refining successfully repairs backtracking loops caused by competing constraints, leading to increased efficiency in executing LM programs.

Conclusion: Meta Self-Refining provides a runtime solution to constraint conflict resolution in Language Models by leveraging a meta-corrective layer, improving efficiency and reliability of LM pipelines.

Abstract: Language Model (LM) pipelines can dynamically refine their outputs against
programmatic constraints. However, their effectiveness collapses when faced
with competing soft constraints, leading to inefficient backtracking loops
where satisfying one constraint violates another. We introduce Meta
Self-Refining, a framework that equips LM pipelines with a meta-corrective
layer to repair these competitions at runtime/inference-time. Our approach
monitors the pipeline's execution history to detect oscillatory failures. Upon
detection, it invokes a meta-repairer LM that analyzes the holistic state of
the backtracking attempts and synthesizes a strategic instruction to balance
the competing requirements. This self-repair instruction guides the original LM
out of a failing refining loop towards a successful output. Our results show
Meta Self-Refining can successfully repair these loops, leading to more
efficient LM programs.

</details>


### [27] [ToolRegistry: A Protocol-Agnostic Tool Management Library for Function-Calling LLMs](https://arxiv.org/abs/2507.10593)
*Peng Ding*

Main category: cs.SE

TL;DR: Toolregistry is a protocol-agnostic library for simplifying LLM tool integration, reducing code by 60-80%, improving performance 3.1x, and maintaining OpenAI compatibility.


<details>
  <summary>Details</summary>
Motivation: Current LLM tool integration methods face fragmentation, protocol limitations, and high implementation complexity, causing significant development overhead.

Method: The authors developed a unified tool management interface that abstracts protocol differences, enabling streamlined registration, execution, and lifecycle management of tools.

Result: 60-80% fewer lines of code for integrations, 3.1x execution speedup with concurrency, 100% OpenAI compatibility, and improved code maintainability in case studies.

Conclusion: Toolregistry provides a robust, open-source solution for efficient and sustainable LLM tool integration, available with documentation at the specified repositories.

Abstract: Large Language Model (LLM) applications are increasingly relying on external
tools to extend their capabilities beyond text generation. However, current
tool integration approaches suffer from fragmentation, protocol limitations,
and implementation complexity, leading to substantial development overhead.
This paper presents Toolregistry, a protocol-agnostic tool management library
that simplifies tool registration, representation, execution, and lifecycle
management via a unified interface. Our evaluation demonstrates that
\toolregistry achieves 60-80% reduction in tool integration code, up to 3.1x
performance improvements through concurrent execution, and 100% compatibility
with OpenAI function calling standards. Real-world case studies show
significant improvements in development efficiency and code maintainability
across diverse integration scenarios. \toolregistry is open-source and
available at https://github.com/Oaklight/ToolRegistry, with comprehensive
documentation at https://toolregistry.readthedocs.io/.

</details>


### [28] [SENSOR: An ML-Enhanced Online Annotation Tool to Uncover Privacy Concerns from User Reviews in Social-Media Applications](https://arxiv.org/abs/2507.10640)
*Labiba Farah,Mohammad Ridwan Kabir,Shohel Ahmed,MD Mohaymen Ul Anam,Md. Sakibul Islam*

Main category: cs.SE

TL;DR: This paper presents SENSOR, an automated annotation tool using the GRACE model (GRU-based Attention with CBOW Embedding) to classify social media app reviews into privacy-related feature requests, bug reports, or irrelevant categories. Evaluated across 16,000 reviews from seven apps, GRACE achieves high accuracy (95.10%) despite class imbalance.


<details>
  <summary>Details</summary>
Motivation: Social media apps face privacy concerns expressed in user reviews, but manual classification of these reviews into privacy-specific categories is labor-intensive and imprecise. Existing tools lack focus on distinguishing between privacy feature requests, bug reports, and irrelevant content.

Method: 1) Collected 16,000 reviews from Google Play Store apps (Instagram, Facebook, WhatsApp, etc.). 2) Annotated by two raters with 0.87 Cohen's Kappa inter-rater agreement. 3) Proposed GRACE model combining GRU with attention and CBOW embeddings for automated annotation.

Result: GRACE achieved 95.10% accuracy, 0.9434 macro F1-score, and 0.9934 macro ROC-AUC - highest among tested models. Performance remains strong despite class imbalance (privacy-related reviews being fewer).

Conclusion: SENSOR with GRACE demonstrates potential to help developers systematically address privacy-related user feedback from reviews, improving privacy outcomes through efficient categorization of feature requests and bug reports.

Abstract: The widespread use of social media applications has raised significant
privacy concerns, often highlighted in user reviews. These reviews also provide
developers with valuable insights into improving apps by addressing issues and
introducing better features. However, the sheer volume and nuanced nature of
reviews make manual identification and prioritization of privacy-related
concerns challenging for developers. Previous studies have developed software
utilities to automatically classify user reviews as privacy-relevant,
privacy-irrelevant, bug reports, feature requests, etc., using machine
learning. Notably, there is a lack of focus on classifying reviews specifically
as privacy-related feature requests, privacy-related bug reports, or
privacy-irrelevant. This paper introduces SENtinel SORt (SENSOR), an automated
online annotation tool designed to help developers annotate and classify user
reviews into these categories. For automating the annotation of such reviews,
this paper introduces the annotation model, GRACE (GRU-based Attention with
CBOW Embedding), using Gated Recurrent Units (GRU) with Continuous Bag of Words
(CBOW) and Attention mechanism. Approximately 16000 user reviews from seven
popular social media apps on Google Play Store, including Instagram, Facebook,
WhatsApp, Snapchat, X (formerly Twitter), Facebook Lite, and Line were
analyzed. Two annotators manually labelled the reviews, achieving a Cohen's
Kappa value of 0.87, ensuring a labeled dataset with high inter-rater agreement
for training machine learning models. Among the models tested, GRACE
demonstrated the best performance (macro F1-score: 0.9434, macro ROC-AUC:
0.9934, and accuracy: 95.10%) despite class imbalance. SENSOR demonstrates
significant potential to assist developers with extracting and addressing
privacy-related feature requests or bug reports from user reviews, enhancing
user privacy and trust.

</details>


### [29] [A Code Comprehension Benchmark for Large Language Models for Code](https://arxiv.org/abs/2507.10641)
*Jayant Havare,Saurav Chaudhary,Ganesh Ramakrishnan,Kaushik Maharajan,Srikanth Tamilselvam*

Main category: cs.SE

TL;DR: The paper proposes fine-tuning large language models for code comprehension to enhance semantic understanding beyond surface-level syntax, showing improvements in model performance across various code comprehension tasks, including a 13.47% increase for QWQ-32B and a 87.66% micro-accuracy for Codestral-22B after DPO fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current large language models excel in syntactic pattern matching for code tasks but struggle with semantic understanding, limiting their performance in tasks like debugging and optimization.

Method: Fine-tune code models on a large-scale dataset specifically designed for code comprehension tasks to strengthen semantic understanding rather than relying solely on syntactic patterns.

Result: All models showed improved performance post-fine-tuning. QWQ-32B's accuracy increased from 70% to 83.47%, and Codestral-22B (DPO-fine-tuned) achieved the highest micro-accuracy of 87.66% on the Subjectivity Grading Task.

Conclusion: Fine-tuning on semantic code comprehension tasks effectively enhances models' understanding of code semantics, with significant performance gains across models, particularly the Codestral-22B using the DPO method.

Abstract: Large Language Models have shown impressive capabilities in coding tasks like
code generation and code completion, as they have been trained on a large
amount of code data. Also, since one of the core pretraining objectives is Next
Token Prediction, these models tends to learn surface-level syntactic patterns
in code. However, this does not guarantee code comprehension ability i.e. the
ability to capture the semantics of the code. In our opinion, this is the
reason why these models often underperform on tasks that require deeper
semantic understanding, such as code debugging and code optimization. To
address this, we propose fine-tuning these models specifically for code
comprehension tasks using large-scale datasets, enabling them to develop a more
robust understanding of code semantics. We evaluate three code models of
varying sizes on a suite of code comprehension tasks designed to assess
semantic understanding beyond surface-level syntactic pattern matching. In
particular, we analyze performance on the Subjectivity Grading Task and observe
that model performance improves after fine-tuning on relevant downstream tasks.
The most significant improvement is seen in the QWQ-32B model, where accuracy
increases from 70% to 83.47%. A similar or explainable trend is observed across
other models, clearly indicating an enhancement in code comprehension ability.
Among the models studied, the DPO-fine-tuned Codestral-22B achieves the highest
micro-accuracy of 87.66% on the Subjectivity Grading Task.

</details>


### [30] [CodeAssistBench (CAB): Dataset & Benchmarking for Multi-turn Chat-Based Code Assistance](https://arxiv.org/abs/2507.10646)
*Myeongsoo Kim,Shweta Garg,Baishakhi Ray,Varun Kumar,Anoop Deoras*

Main category: cs.SE

TL;DR: CodeAssistBench (CAB) addresses the gap in evaluating multi-turn programming assistance in real project environments by automatically generating scalable datasets from GitHub issues and containerizing codebases. Evaluation shows leading LLMs perform poorly (16.49% success) on CAB's real-world issues compared to 70-83% on Stack Overflow benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for programming assistants focus on code generation tasks and single-turn interactions, using Stack Overflow data or isolated problems. These approaches fail to simulate complete project environments, are manually curated, and lack scalability for real-world codebase evaluation.

Method: CAB automates dataset creation via configurable GitHub issue parameters (repo date, stars, language) and containerizes codebases for evaluation. It simulates 3,286 real-world questions across 231 repositories in seven programming languages, enabling model interactions with full codebase access.

Result: Leading LLMs achieved 70-83% success rates on Stack Overflow-style benchmarks but only up to 16.49% on CAB's recent GitHub issues. This discrepancy reveals a significant capability gap in handling multi-turn assistance within complex codebase contexts.

Conclusion: Current large language models struggle with real-world programming tasks requiring multi-turn interactions in authentic project environments, highlighting the need for benchmarks like CAB that expose contextual challenges absent in isolated question-answer formats.

Abstract: Programming assistants powered by large language models have transformed
software development, yet most benchmarks focus narrowly on code generation
tasks. Recent efforts like InfiBench and StackEval attempt to address this gap
using Stack Overflow data but remain limited to single-turn interactions in
isolated contexts, require significant manual curation, and fail to represent
complete project environments. We introduce CodeAssistBench (CAB), the first
benchmark framework for evaluating multi-turn programming assistance in
realistic settings that address real-world questions about actual codebases.
Unlike existing programming Q&A benchmarks, CAB automatically generates
scalable datasets from question-related GitHub issues using configurable
parameters (e.g., repository creation date, star count, programming languages),
and includes automatic containerization of codebases for evaluation. It then
evaluates models through simulated users in these containerized environments
with full codebase access. Using this framework, we constructed a test set of
3,286 real-world programming questions across 231 repositories, spanning seven
programming languages and diverse problem domains. Our evaluation of leading
LLMs reveals a substantial capability gap: while models perform well on Stack
Overflow questions with success rates of 70-83%, they resolve only up to 16.49%
of CAB's recent issues. This discrepancy highlights the challenges of providing
assistance in complex, project-specific contexts versus answering standalone
questions.

</details>


### [31] [Toward Realistic Evaluations of Just-In-Time Vulnerability Prediction](https://arxiv.org/abs/2507.10729)
*Duong Nguyen,Thanh Le-Cong,Triet Huynh Minh Le,M. Ali Babar,Quyet-Thang Huynh*

Main category: cs.SE

TL;DR: The paper evaluates JIT-VP techniques under realistic class-imbalanced conditions using a large public dataset of 1 million commits, revealing a dramatic performance decline and suggesting standard balancing methods like oversampling/undersampling are ineffective for vulnerability prediction.


<details>
  <summary>Details</summary>
Motivation: Current JIT-VP evaluations use artificially balanced datasets containing only vulnerability-related commits, creating an overly optimistic assessment and ignoring class imbalance issues inherent to real-world software development where most commits are neutral.

Method: The study introduces a large-scale public dataset with 1 million+ commits from FFmpeg and Linux kernel including both vulnerability-related and neutral commits. Eight state-of-the-art JIT-VP techniques were evaluated using PR-AUC and traditional balancing strategies were tested for effectiveness.

Result: Predictive performance of JIT-VP techniques dropped to PR-AUC=0.016 (Linux) from ideally 0.805, with standard balancing techniques showing no significant improvement despite severe class imbalance (vulnerability commits <1% of total commits).

Conclusion: Standard data balancing techniques fail to address JIT-VP performance issues in realistic imbalanced settings. The findings highlight the critical need for domain-specific solutions and emphasize the necessity of rigorous, real-world evaluation methodologies for vulnerability prediction systems.

Abstract: Modern software systems are increasingly complex, presenting significant
challenges in quality assurance. Just-in-time vulnerability prediction (JIT-VP)
is a proactive approach to identifying vulnerable commits and providing early
warnings about potential security risks. However, we observe that current
JIT-VP evaluations rely on an idealized setting, where the evaluation datasets
are artificially balanced, consisting exclusively of vulnerability-introducing
and vulnerability-fixing commits.
  To address this limitation, this study assesses the effectiveness of JIT-VP
techniques under a more realistic setting that includes both
vulnerability-related and vulnerability-neutral commits. To enable a reliable
evaluation, we introduce a large-scale public dataset comprising over one
million commits from FFmpeg and the Linux kernel. Our empirical analysis of
eight state-of-the-art JIT-VP techniques reveals a significant decline in
predictive performance when applied to real-world conditions; for example, the
average PR-AUC on Linux drops 98\% from 0.805 to 0.016. This discrepancy is
mainly attributed to the severe class imbalance in real-world datasets, where
vulnerability-introducing commits constitute only a small fraction of all
commits.
  To mitigate this issue, we explore the effectiveness of widely adopted
techniques for handling dataset imbalance, including customized loss functions,
oversampling, and undersampling. Surprisingly, our experimental results
indicate that these techniques are ineffective in addressing the imbalance
problem in JIT-VP. These findings underscore the importance of realistic
evaluations of JIT-VP and the need for domain-specific techniques to address
data imbalance in such scenarios.

</details>


### [32] [GenAI-Enabled Backlog Grooming in Agile Software Projects: An Empirical Study](https://arxiv.org/abs/2507.10753)
*Kasper Lien Oftebro,Anh Nguyen-Duc,Kai-Kristian Kemell*

Main category: cs.SE

TL;DR: This paper explores using a generative-AI assistant to automate Agile backlog grooming, achieving 100% precision and 45% time reduction through a Jira plugin with vector databases and GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Product backlogs often grow cluttered with redundant/outdated tasks, complicating prioritization and decision-making in Agile projects. Current manual grooming processes are time-intensive and error-prone.

Method: A Jira plug-in was developed via Design Science cycles, embedding backlog issues in a vector database. It detects duplicates using cosine similarity and leverages GPT-4o to suggest merges, deletions, or new issues for automated grooming.

Result: AI-assisted backlog grooming achieved 100% precision and 45% reduction in time-to-completion, demonstrating effectiveness in streamlining refinement processes while improving user experiences.

Conclusion: The GenAI tool successfully automates backlog grooming without sacrificing accuracy, offering significant time savings and usability improvements for Agile project management.

Abstract: Effective backlog management is critical for ensuring that development teams
remain aligned with evolving requirements and stakeholder expectations.
However, as product backlogs consistently grow in scale and complexity, they
tend to become cluttered with redundant, outdated, or poorly defined tasks,
complicating prioritization and decision making processes. This study
investigates whether a generative-AI (GenAI) assistant can automate backlog
grooming in Agile software projects without sacrificing accuracy or
transparency. Through Design Science cycles, we developed a Jira plug-in that
embeds backlog issues with the vector database, detects duplicates via cosine
similarity, and leverage the GPT-4o model to propose merges, deletions, or new
issues. We found that AI-assisted backlog grooming achieved 100 percent
precision while reducing the time-to-completion by 45 percent. The findings
demonstrated the tool's potential to streamline backlog refinement processes
while improving user experiences.

</details>


### [33] [Towards a Closer Collaboration Between Practice and Research in Agile Software Development Workshop: A Summary and Research Agenda](https://arxiv.org/abs/2507.10785)
*Michael Neumann,Eva-Maria Schön,Mali Senapathi,Maria Rauschenberger,Tiago Silva da Silva*

Main category: cs.SE

TL;DR: The paper investigates the gap between agile software development research and practice through an international workshop, identifying themes, strategies to bridge the divide, and unresolved challenges for future research.


<details>
  <summary>Details</summary>
Motivation: Despite widespread adoption of agile software development principles, there is a significant gap between academic research and practical implementation, necessitating collaboration to address this divide.

Method: Conducted the first international workshop to facilitate dialogue between researchers and practitioners in agile software development, analyzing participant discussions to identify themes and strategies.

Result: Identified key factors contributing to the research-practice gap in agile methodologies, proposed actionable strategies for bridging it, and highlighted specific challenges requiring further investigation.

Conclusion: The workshop outcomes emphasize the importance of sustained collaboration between researchers and practitioners to address persistent challenges in aligning agile research with practical application.

Abstract: Agile software development principles and values have been widely adopted
across various industries, influencing products and services globally. Despite
its increasing popularity, a significant gap remains between research and
practical implementation. This paper presents the findings of the first
international workshop designed to foster collaboration between research and
practice in agile software development. We discuss the main themes and factors
identified by the workshop participants that contribute to this gap, strategies
to bridge it, and the challenges that require further research attention.

</details>


### [34] [How Robust are LLM-Generated Library Imports? An Empirical Study using Stack Overflow](https://arxiv.org/abs/2507.10818)
*Jasmine Latendresse,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: The paper evaluates how six leading LLMs recommend Python libraries for real-world coding tasks, revealing a preference for third-party libraries but highlighting unresolved dependency issues (4.6% failure rate) and limited installation guidance (only 2 models).


<details>
  <summary>Details</summary>
Motivation: Understanding LLM library recommendations is critical as developers increasingly use LLMs for programming, with implications for code functionality, security, and maintainability.

Method: Conducted empirical analysis by prompting six proprietary/open-source LLMs to solve Stack Overflow Python problems, examining their library import patterns, dependencies' characteristics, and usability of recommendations.

Result: LLMs favor mature, popular, and permissively-licensed third-party libraries, but 4.6% of recommendations fail due to import-package mismatches, and only two models offer installation instructions despite technically valid code.

Conclusion: Highlights critical usability gaps in LLM-generated code related to dependency resolution while providing actionable insights for improving reliability through better licensing awareness, name mapping, and installation guidance integration.

Abstract: Software libraries are central to the functionality, security, and
maintainability of modern code. As developers increasingly turn to Large
Language Models (LLMs) to assist with programming tasks, understanding how
these models recommend libraries is essential. In this paper, we conduct an
empirical study of six state-of-the-art LLMs, both proprietary and open-source,
by prompting them to solve real-world Python problems sourced from Stack
Overflow. We analyze the types of libraries they import, the characteristics of
those libraries, and the extent to which the recommendations are usable out of
the box. Our results show that LLMs predominantly favour third-party libraries
over standard ones, and often recommend mature, popular, and permissively
licensed dependencies. However, we also identify gaps in usability: 4.6% of the
libraries could not be resolved automatically due to structural mismatches
between import names and installable packages, and only two models (out of six)
provided installation guidance. While the generated code is technically valid,
the lack of contextual support places the burden of manually resolving
dependencies on the user. Our findings offer actionable insights for both
developers and researchers, and highlight opportunities to improve the
reliability and usability of LLM-generated code in the context of software
dependencies.

</details>


### [35] [Past, Present and Future: Exploring Adaptive AI in Software Development Bots](https://arxiv.org/abs/2507.10822)
*Omar Elsisi,Glaucia Melo*

Main category: cs.SE

TL;DR: The paper explores the role of adaptive AI-powered conversational agents in software development, discussing their evolution from rule-based systems to advanced tools like GitHub Copilot, benefits in productivity, challenges like privacy and ethics, and their potential to enhance development efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the growing adoption of conversational agents in software development and the need to evaluate their dynamic, context-aware capabilities compared to traditional systems, addressing integration challenges and ethical concerns.

Method: The study employs a qualitative exploration of existing tools (e.g., GitHub Copilot), integration challenges, and ethical issues through analysis of system evolution and practical applications.

Result: Adaptive AI chatbots demonstrate real-time, personalized support improving development efficiency, while challenges such as data privacy and ethical implications highlight areas needing further investigation.

Conclusion: Adaptive AI conversational agents hold significant potential to revolutionize software development by providing responsive assistance and optimizing workflows, though future research must address current limitations and ethical considerations.

Abstract: Conversational agents, such as chatbots and virtual assistants, have become
essential in software development, boosting productivity, collaboration, and
automating various tasks. This paper examines the role of adaptive AI-powered
conversational agents in software development, highlighting their ability to
offer dynamic, context-aware assistance to developers. Unlike traditional
rule-based systems, adaptive AI agents use machine learning and natural
language processing to learn from interactions and improve over time, providing
more personalized and responsive help. We look at how these tools have evolved
from simple query-based systems to advanced AI-driven solutions like GitHub
Copilot and Microsoft Teams bots. We also explore the challenges of integrating
adaptive AI into software development processes. The study aims to assess the
benefits and limitations of these systems, address concerns like data privacy
and ethical issues, and offer insights into their future use in the field.
Ultimately, adaptive AI chatbots have great potential to revolutionize software
development by delivering real-time, customized support and enhancing the
efficiency of development cycles.

</details>


### [36] [Evaluating Generated Commit Messages with Large Language Models](https://arxiv.org/abs/2507.10906)
*Qunhong Zeng,Yuxia Zhang,Zexiong Ma,Bo Jiang,Ningyuan Sun,Klaas-Jan Stol,Xingyu Mou,Hui Liu*

Main category: cs.SE

TL;DR: This paper explores using LLMs as automated evaluators for commit message quality, showing they can achieve near-human-level performance with improved scalability and effectiveness over traditional metrics.


<details>
  <summary>Details</summary>
Motivation: Commit messages often have poor quality, and existing automated metrics (BLEU, ROUGE-L, METEOR) are inadequate due to their one-to-one code-message mapping assumptions. Human evaluation is accurate but resource-intensive.

Method: The study tests various prompt strategies and state-of-the-art LLMs, employing Chain-of-Thought reasoning and few-shot demonstrations to evaluate generated commit messages.

Result: LLMs with Chain-of-Thought and few-shot examples outperform traditional metrics in evaluating commit messages, achieving near-human proficiency with acceptable reproducibility, robustness, and fairness.

Conclusion: LLM-based evaluation offers a scalable, high-quality alternative to human assessment for commit message evaluation, addressing the shortcomings of existing automated metrics.

Abstract: Commit messages are essential in software development as they serve to
document and explain code changes. Yet, their quality often falls short in
practice, with studies showing significant proportions of empty or inadequate
messages. While automated commit message generation has advanced significantly,
particularly with Large Language Models (LLMs), the evaluation of generated
messages remains challenging. Traditional reference-based automatic metrics
like BLEU, ROUGE-L, and METEOR have notable limitations in assessing commit
message quality, as they assume a one-to-one mapping between code changes and
commit messages, leading researchers to rely on resource-intensive human
evaluation. This study investigates the potential of LLMs as automated
evaluators for commit message quality. Through systematic experimentation with
various prompt strategies and state-of-the-art LLMs, we demonstrate that LLMs
combining Chain-of-Thought reasoning with few-shot demonstrations achieve near
human-level evaluation proficiency. Our LLM-based evaluator significantly
outperforms traditional metrics while maintaining acceptable reproducibility,
robustness, and fairness levels despite some inherent variability. This work
conducts a comprehensive preliminary study on using LLMs for commit message
evaluation, offering a scalable alternative to human assessment while
maintaining high-quality evaluation.

</details>


### [37] [SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks](https://arxiv.org/abs/2507.11059)
*Pavel Adamenko,Mikhail Ivanov,Aidar Valeev,Rodion Levichev,Pavel Zadorozhny,Ivan Lopatin,Dmitry Babayev,Alena Fenogenova,Valentin Malykh*

Main category: cs.SE

TL;DR: Introduces SWE-MERA: A dynamic benchmark for LLMs in software engineering addressing SWE-bench contamination issues through GitHub-based automated task collection and quality validation.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks like SWE-bench suffer from solution leakage (32.67%) and insufficient test case rigor (31.08%), compromising fair model evaluation in software engineering tasks.

Method: Developed an automated pipeline to collect GitHub issues with 10,000 potential tasks, implementing dynamic updates and quality validation to minimize contamination and ensure reliability.

Result: SWE-MERA demonstrates strong model discrimination via Aider coding agent evaluations, with performance results showing effectiveness across 12 recent LLMs (tasks collected September 2024-June 2025).

Conclusion: SWE-MERA establishes a foundational benchmark for LLM software engineering evaluation by addressing contamination through automation and validation, enabling accurate model comparison.

Abstract: The rapid advancement of Large Language Models (LLMs) in software engineering
has revealed critical limitations in existing benchmarks, particularly the
widely used SWE-bench dataset. Recent studies have uncovered severe data
contamination issues, e.g. SWE-bench reports 32.67% of successful patches
involve direct solution leakage and 31.08\% pass due to inadequate test cases.
We introduce SWE-MERA, a dynamic, continuously updated benchmark designed to
address these fundamental challenges through an automated collection of
real-world GitHub issues and rigorous quality validation. Our approach
implements a reliable pipeline that ensures quality while minimizing
contamination risks, resulting in approximately 10,000 potential tasks with 300
samples currently available. Evaluation using the Aider coding agent
demonstrates strong discriminative power in state-of-the-art models. We report
performance across a dozen recent LLMs evaluated on tasks collected between
September 2024 and June 2025.

</details>


### [38] [MT4DP: Data Poisoning Attack Detection for DL-based Code Search Models via Metamorphic Testing](https://arxiv.org/abs/2507.11092)
*Gong Chen,Wenjie Liu,Xiaoyuan Xie,Xunzhu Tang,Tegawendé F. Bissyandé,Songqiang Chen*

Main category: cs.SE

TL;DR: The paper proposes MT4DP, a data poisoning attack detection framework for DL-based code search models using metamorphic testing. It introduces SE-MRs to identify ranking inconsistencies caused by malicious training data patterns, demonstrating 191% and 265% improvement in F1 score and precision over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current data poisoning attacks on code search models can inject malicious patterns into training data, causing models to rank vulnerable code higher during inference. Existing detection methods are insufficiently effective against these threats.

Method: MT4DP identifies high-frequency search query terms as potential poisoning targets, generates semantically equivalent follow-up queries for each source query, re-ranks retrieval results using semantic similarity with these queries, and detects attacks through inconsistencies in ranking variances violating the SE-MR.

Result: Experiments show MT4DP significantly improves detection performance, achieving 191% higher average F1 score and 265% higher average precision compared to existing detection methods for code search model poisoning attacks.

Conclusion: MT4DP demonstrates the effectiveness of metamorphic testing for detecting data poisoning in code search models, providing a strong foundation for developing more robust mitigation techniques in this security-critical domain.

Abstract: Recently, several studies have indicated that data poisoning attacks pose a
severe security threat to deep learning-based (DL-based) code search models.
Attackers inject carefully crafted malicious patterns into the training data,
misleading the code search model to learn these patterns during training.
During the usage of the poisoned code search model for inference, once the
malicious pattern is triggered, the model tends to rank the vulnerability code
higher. However, existing detection methods for data poisoning attacks on
DL-based code search models remain insufficiently effective. To address this
critical security issue, we propose MT4DP, a Data Poisoning Attack Detection
Framework for DL-based Code Search Models via Metamorphic Testing. MT4DP
introduces a novel Semantically Equivalent Metamorphic Relation (SE-MR)
designed to detect data poisoning attacks on DL-based code search models.
Specifically, MT4DP first identifies the high-frequency words from search
queries as potential poisoning targets and takes their corresponding queries as
the source queries. For each source query, MT4DP generates two semantically
equivalent follow-up queries and retrieves its source ranking list. Then, each
source ranking list is re-ranked based on the semantic similarities between its
code snippets and the follow-up queries. Finally, variances between the source
and re-ranked lists are calculated to reveal violations of the SE-MR and warn
the data poisoning attack. Experimental results demonstrate that MT4DP
significantly enhances the detection of data poisoning attacks on DL-based code
search models, outperforming the best baseline by 191% on average F1 score and
265% on average precision. Our work aims to promote further research into
effective techniques for mitigating data poisoning threats on DL-based code
search models.

</details>


### [39] [Automata Models for Effective Bug Description](https://arxiv.org/abs/2507.11146)
*Tom Yaacov,Gera Weiss,Gal Amram,Avi Hayoun*

Main category: cs.SE

TL;DR: This paper introduces Failure Explanations (FE), Eventual Failure Explanations (EFE), and Early Detection (ED) using automata learning techniques to generate concise, informative bug descriptions for complex systems.


<details>
  <summary>Details</summary>
Motivation: Debugging complex systems is crucial yet time-consuming, requiring effective methods to identify and summarize failure patterns without being overwhelmed by irrelevant details.

Method: The approach employs automata learning and testing methods to extract FE/EFE/ED patterns by abstracting test sequences and isolating critical failure-inducing behaviors through model-based analysis.

Result: Evaluation on test patterns and real-world benchmarks demonstrated the capability to produce compact bug descriptions that maintain essential diagnostic information while eliminating noise.

Conclusion: By focusing on essential test patterns and leveraging automata learning, the proposed techniques significantly improve bug detection efficiency and failure understanding in complex systems.

Abstract: Debugging complex systems is a crucial yet time-consuming task. This paper
presents the use of automata learning and testing techniques to obtain concise
and informative bug descriptions. We introduce the concepts of Failure
Explanations (FE), Eventual Failure Explanations (EFE), and Early Detection
(ED) to provide meaningful summaries of failing behavior patterns. By factoring
out irrelevant information and focusing on essential test patterns, our
approach aims to enhance bug detection and understanding. We evaluate our
methods using various test patterns and real-world benchmarks, demonstrating
their effectiveness in producing compact and informative bug descriptions.

</details>


### [40] [New Formulation of DNN Statistical Mutation Killing for Ensuring Monotonicity: A Technical Report](https://arxiv.org/abs/2507.11199)
*Jinhan Kim,Nargiz Humbatova,Gunel Jahangirova,Shin Yoo,Paolo Tonella*

Main category: cs.SE

TL;DR: This paper proposes a Fisher exact test-based statistical mutant killing criterion that preserves statistical rigor while addressing the non-monotonicity issue in DeepCrime.


<details>
  <summary>Details</summary>
Motivation: Existing mutation testing approaches for DNNs like DeepCrime violate monotonicity, leading to inconsistent mutant killing classification when test sets are expanded.

Method: The authors reformulate statistical mutant killing using Fisher's exact test framework, leveraging hypothesis testing to ensure monotonicity by analyzing count distributions of input classifications between original and mutated models.

Result: The new approach guarantees that expanding test sets maintains previously killed mutants' status while achieving equivalent or improved statistical reliability compared to DeepCrime's method.

Conclusion: Fisher exact test-based mutation analysis solves the critical monotonicity violation problem while maintaining rigorous statistical validation, enabling more consistent DNN test suite evaluation.

Abstract: Mutation testing has emerged as a powerful technique for evaluating the
effectiveness of test suites for Deep Neural Networks. Among existing
approaches, the statistical mutant killing criterion of DeepCrime has leveraged
statistical testing to determine whether a mutant significantly differs from
the original model. However, it suffers from a critical limitation: it violates
the monotonicity property, meaning that expanding a test set may result in
previously killed mutants no longer being classified as killed. In this
technical report, we propose a new formulation of statistical mutant killing
based on Fisher exact test that preserves the statistical rigour of it while
ensuring monotonicity.

</details>


### [41] [An Empirical Study of Multi-Agent RAG for Real-World University Admissions Counseling](https://arxiv.org/abs/2507.11272)
*Anh Nguyen-Duc,Chien Vu Manh,Bao Anh Tran,Viet Phuong Ngo,Luan Le Chi,Anh Quang Nguyen*

Main category: cs.SE

TL;DR: MARAUS is a real-world university admission counseling AI system in Vietnam that combines hybrid retrieval, multi-agent orchestration, and LLMs to achieve 92% accuracy, reduce hallucinations from 15% to 1.45%, and maintain low costs (11.58 USD for two weeks).


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based admissions solutions are limited to prototypes or synthetic benchmarks, lacking real-world deployment. This paper addresses the need for practical, accurate, and cost-effective AI systems in university admissions, particularly in low-resource educational settings.

Method: MARAUS integrates hybrid retrieval-augmented generation with multi-agent orchestration, developed through a two-phase collaboration with UTT. It processes six admission-related query categories using LLM-based generation while incorporating retrieval mechanisms for factual consistency.

Result: The system handled 6,000+ real user interactions with 92% average accuracy, 1.45% hallucination rate (vs. 15% in LLM-only baselines), <4 second response times, and a low operational cost of 11.58 USD over two weeks using GPT-4o mini.

Conclusion: MARAUS demonstrates the effectiveness of agentic RAG systems in real-world university admissions, showing they can achieve high accuracy and low hallucination rates while remaining cost-effective. This provides a scalable solution for low-resource educational institutions.

Abstract: This paper presents MARAUS (Multi-Agent and Retrieval-Augmented University
Admission System), a real-world deployment of a conversational AI platform for
higher education admissions counseling in Vietnam. While large language models
(LLMs) offer potential for automating advisory tasks, most existing solutions
remain limited to prototypes or synthetic benchmarks. MARAUS addresses this gap
by combining hybrid retrieval, multi-agent orchestration, and LLM-based
generation into a system tailored for real-world university admissions. In
collaboration with the University of Transport Technology (UTT) in Hanoi, we
conducted a two-phase study involving technical development and real-world
evaluation. MARAUS processed over 6,000 actual user interactions, spanning six
categories of queries. Results show substantial improvements over LLM-only
baselines: on average 92 percent accuracy, hallucination rates reduced from 15
precent to 1.45 percent, and average response times below 4 seconds. The system
operated cost-effectively, with a two-week deployment cost of 11.58 USD using
GPT-4o mini. This work provides actionable insights for the deployment of
agentic RAG systems in low-resource educational settings.

</details>


### [42] [RefModel: Detecting Refactorings using Foundation Models](https://arxiv.org/abs/2507.11346)
*Pedro Simões,Rohit Gheyi,Rian Melo,Jonhnanthan Oliveira,Márcio Ribeiro,Wesley K. G. Assunção*

Main category: cs.SE

TL;DR: This paper evaluates the use of foundation models (like Claude 3.5 Sonnet, Gemini 2.5 Pro) in refactoring detection through the RefModel tool. These models demonstrate competitive performance with traditional tools, achieving 97% detection on real-world refactorings while generalizing across programming languages and requiring simple, single-sentence refactoring definitions.


<details>
  <summary>Details</summary>
Motivation: Traditional refactoring detection tools rely on language-specific rules and static analysis, limiting their extensibility to other programming languages.

Method: The study evaluates multiple foundation models on 858 synthetic Java refactorings and 44 real-world refactorings from open-source projects, comparing their performance to ReExtractor+, RefactoringMiner, and RefDiff.

Result: RefModel matches traditional tools' performance while achieving 97% detection on real-world cases using Claude 3.5 Sonnet and Gemini 2.5 Pro. The models generalize to Python and Golang and require minimal definitions.

Conclusion: Foundation models offer a flexible, language-agnostic approach to refactoring detection with high accuracy and explainability through natural language outputs.

Abstract: Refactoring is a common software engineering practice that improves code
quality without altering program behavior. Although tools like ReExtractor+,
RefactoringMiner, and RefDiff have been developed to detect refactorings
automatically, they rely on complex rule definitions and static analysis,
making them difficult to extend and generalize to other programming languages.
In this paper, we investigate the viability of using foundation models for
refactoring detection, implemented in a tool named RefModel. We evaluate
Phi4-14B, and Claude 3.5 Sonnet on a dataset of 858 single-operation
transformations applied to artificially generated Java programs, covering
widely-used refactoring types. We also extend our evaluation by including
Gemini 2.5 Pro and o4-mini-high, assessing their performance on 44 real-world
refactorings extracted from four open-source projects. These models are
compared against RefactoringMiner, RefDiff, and ReExtractor+. RefModel is
competitive with, and in some cases outperform, traditional tools. In
real-world settings, Claude 3.5 Sonnet and Gemini 2.5 Pro jointly identified
97% of all refactorings, surpassing the best-performing static-analysis-based
tools. The models showed encouraging generalization to Python and Golang. They
provide natural language explanations and require only a single sentence to
define each refactoring type.

</details>


### [43] [Security Debt in Practice: Nuanced Insights from Practitioners](https://arxiv.org/abs/2507.11362)
*Chaima Boufaied,Taher Ghaleb,Zainab Masood*

Main category: cs.SE

TL;DR: The paper investigates how software practitioners perceive and manage Security Debts (SDs) through interviews, highlighting the need for stronger security integration in the SDLC and balanced priorities between functionality and security.


<details>
  <summary>Details</summary>
Motivation: The study addresses the lack of empirical evidence on the real-world perception, management, and communication of security vulnerabilities (SDs) in software development, where constraints like deadlines and resources often prioritize functionality over security.

Method: A qualitative empirical study using semi-structured interviews with 22 software practitioners from diverse roles, organizations, and countries to explore four research questions on SD awareness, behavior, mitigation tools, and communication practices.

Result: Practitioners exhibit varied approaches to SDs, with some prioritizing delivery speed over security and others maintaining consistent security priorities. The findings stress gaps in SDLC security integration and highlight tools/strategies for mitigation and risk communication.

Conclusion: Stronger integration of security practices within the SDLC, consistent mitigation strategies, and balancing deadlines/resources with the CIA triad are critical to addressing security debts more effectively in real-world software development.

Abstract: With the increasing reliance on software and automation nowadays, tight
deadlines, limited resources, and prioritization of functionality over security
can lead to insecure coding practices. When not handled properly, these
constraints cause unaddressed security vulnerabilities to accumulate over time,
forming Security Debts (SDs). Despite their critical importance, there is
limited empirical evidence on how software practitioners perceive, manage, and
communicate SDs in real-world settings. In this paper, we present a qualitative
empirical study based on semi-structured interviews with 22 software
practitioners across various roles, organizations, and countries. We address
four research questions: i) we assess software practitioners' knowledge of SDs
and awareness of associated security risks, ii) we investigate their behavior
towards SDs, iii) we explore common tools and strategies used to mitigate SDs,
and iv) we analyze how security risks are communicated within teams and to
decision makers. We observe variations in how practitioners perceive and manage
SDs, with some prioritizing delivery speed over security, while others
consistently maintain security as a priority. Our findings emphasize the need
for stronger integration of security practices across the Software Development
Life Cycle (SDLC), more consistent use of mitigation strategies, better
balancing of deadlines, resources, and security-related tasks, with attention
to the Confidentiality, Integrity, and Availability (CIA) triad.

</details>
