{"id": "2510.15953", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15953", "abs": "https://arxiv.org/abs/2510.15953", "authors": ["Sisir Doppalapudi"], "title": "Hierarchical Multi-Modal Threat Intelligence Fusion Without Aligned Data: A Practical Framework for Real-World Security Operations", "comment": null, "summary": "Multi-modal threat detection faces a fundamental challenge that involves\nsecurity tools operating in isolation, and this creates streams of network,\nemail, and system data with no natural alignment or correlation. We present\nHierarchical Multi-Modal Threat Intelligence Fusion (HM-TIF), a framework\nexplicitly designed for this realistic scenario where naturally aligned\nmulti-modal attack data does not exist. Unlike prior work that assumes or\ncreates artificial alignment, we develop principled methods for correlating\nindependent security data streams while maintaining operational validity. Our\narchitecture employs hierarchical cross-attention with dynamic weighting that\nadapts to data availability and threat context, coupled with a novel temporal\ncorrelation protocol that preserves statistical independence. Evaluation on\nUNSW-NB15, CSE-CIC-IDS2018, and CICBell-DNS2021 datasets demonstrates that\nHM-TIF achieves 88.7% accuracy with a critical 32% reduction in false positive\nrates, even without true multi-modal training data. The framework maintains\nrobustness when modalities are missing, making it immediately deployable in\nreal security operations where data streams frequently have gaps. Our\ncontributions include: (i) the first multi-modal security framework explicitly\ndesigned for non-aligned data, (ii) a temporal correlation protocol that avoids\ncommon data leakage pitfalls, (iii) empirical validation that multi-modal\nfusion provides operational benefits even without perfect alignment, and (iv)\npractical deployment guidelines for security teams facing heterogeneous,\nuncoordinated data sources. Index Terms: multi-modal learning, threat\nintelligence, non-aligned data, operational security, cross-attention\nmechanisms, practical deployment"}
{"id": "2510.15971", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15971", "abs": "https://arxiv.org/abs/2510.15971", "authors": ["Md. Ifthekhar Hossain", "Kazi Abdullah Al Arafat", "Bryce Shepard", "Kayd Craig", "Imtiaz Parvez"], "title": "A Graph-Attentive LSTM Model for Malicious URL Detection", "comment": "Planned to be submitted", "summary": "Malicious URLs pose significant security risks as they facilitate phishing\nattacks, distribute malware, and empower attackers to deface websites.\nBlacklist detection methods fail to identify new or obfuscated URLs because\nthey depend on pre-existing patterns. This work presents a hybrid deep learning\nmodel named GNN-GAT-LSTM that combines Graph Neural Networks (GNNs) with Graph\nAttention Networks (GATs) and Long Short-Term Memory (LSTM) networks. The\nproposed architecture extracts both the structural and sequential patterns of\nthe features from data. The model transforms URLs into graphs through a process\nwhere characters become nodes that connect through edges. It applies one-hot\nencoding to represent node features. The model received training and testing\ndata from a collection of 651,191 URLs, which were classified into benign,\nphishing, defacement, and malware categories. The preprocessing stage included\nboth feature engineering and data balancing techniques, which addressed the\nclass imbalance issue to enhance model learning. The GNN-GAT-LSTM model\nachieved outstanding performance through its test accuracy of 0.9806 and its\nweighted F1-score of 0.9804. It showed excellent precision and recall\nperformance across most classes, particularly for benign and defacement URLs.\nOverall, the model provides an efficient and scalable system for detecting\nmalicious URLs while demonstrating strong potential for real-world\ncybersecurity applications."}
{"id": "2510.15973", "categories": ["cs.CR", "cs.AI", "cs.CY", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.15973", "abs": "https://arxiv.org/abs/2510.15973", "authors": ["Tiarnaigh Downey-Webb", "Olamide Jogunola", "Oluwaseun Ajao"], "title": "Safeguarding Efficacy in Large Language Models: Evaluating Resistance to Human-Written and Algorithmic Adversarial Prompts", "comment": "10 pages, 4 pages manuscript submitted to the Language Resources and\n  Evaluation Conference (LREC 2026)", "summary": "This paper presents a systematic security assessment of four prominent Large\nLanguage Models (LLMs) against diverse adversarial attack vectors. We evaluate\nPhi-2, Llama-2-7B-Chat, GPT-3.5-Turbo, and GPT-4 across four distinct attack\ncategories: human-written prompts, AutoDAN, Greedy Coordinate Gradient (GCG),\nand Tree-of-Attacks-with-pruning (TAP). Our comprehensive evaluation employs\n1,200 carefully stratified prompts from the SALAD-Bench dataset, spanning six\nharm categories. Results demonstrate significant variations in model\nrobustness, with Llama-2 achieving the highest overall security (3.4% average\nattack success rate) while Phi-2 exhibits the greatest vulnerability (7.0%\naverage attack success rate). We identify critical transferability patterns\nwhere GCG and TAP attacks, though ineffective against their target model\n(Llama-2), achieve substantially higher success rates when transferred to other\nmodels (up to 17% for GPT-4). Statistical analysis using Friedman tests reveals\nsignificant differences in vulnerability across harm categories ($p < 0.001$),\nwith malicious use prompts showing the highest attack success rates (10.71%\naverage). Our findings contribute to understanding cross-model security\nvulnerabilities and provide actionable insights for developing targeted defense\nmechanisms"}
{"id": "2510.15975", "categories": ["cs.CR", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.15975", "abs": "https://arxiv.org/abs/2510.15975", "authors": ["Zaixi Zhang", "Souradip Chakraborty", "Amrit Singh Bedi", "Emilin Mathew", "Varsha Saravanan", "Le Cong", "Alvaro Velasquez", "Sheng Lin-Gibson", "Megan Blewett", "Dan Hendrycs", "Alex John London", "Ellen Zhong", "Ben Raphael", "Jian Ma", "Eric Xing", "Russ Altman", "George Church", "Mengdi Wang"], "title": "Generative AI for Biosciences: Emerging Threats and Roadmap to Biosecurity", "comment": null, "summary": "The rapid adoption of generative artificial intelligence (GenAI) in the\nbiosciences is transforming biotechnology, medicine, and synthetic biology. Yet\nthis advancement is intrinsically linked to new vulnerabilities, as GenAI\nlowers the barrier to misuse and introduces novel biosecurity threats, such as\ngenerating synthetic viral proteins or toxins. These dual-use risks are often\noverlooked, as existing safety guardrails remain fragile and can be\ncircumvented through deceptive prompts or jailbreak techniques. In this\nPerspective, we first outline the current state of GenAI in the biosciences and\nemerging threat vectors ranging from jailbreak attacks and privacy risks to the\ndual-use challenges posed by autonomous AI agents. We then examine urgent gaps\nin regulation and oversight, drawing on insights from 130 expert interviews\nacross academia, government, industry, and policy. A large majority ($\\approx\n76$\\%) expressed concern over AI misuse in biology, and 74\\% called for the\ndevelopment of new governance frameworks. Finally, we explore technical\npathways to mitigation, advocating a multi-layered approach to GenAI safety.\nThese defenses include rigorous data filtering, alignment with ethical\nprinciples during development, and real-time monitoring to block harmful\nrequests. Together, these strategies provide a blueprint for embedding security\nthroughout the GenAI lifecycle. As GenAI becomes integrated into the\nbiosciences, safeguarding this frontier requires an immediate commitment to\nboth adaptive governance and secure-by-design technologies."}
{"id": "2510.16059", "categories": ["cs.SE", "cs.CL", "D.2.2; D.2.3"], "pdf": "https://arxiv.org/pdf/2510.16059", "abs": "https://arxiv.org/abs/2510.16059", "authors": ["Xin Cao", "Nan Yu"], "title": "SIADAFIX: issue description response for adaptive program repair", "comment": "20 pages, 3 figures", "summary": "We propose utilizing fast and slow thinking to enhance the capabilities of\nlarge language model-based agents on complex tasks such as program repair. In\nparticular, we design an adaptive program repair method based on issue\ndescription response, called SIADAFIX. The proposed method utilizes slow\nthinking bug fix agent to complete complex program repair tasks, and employs\nfast thinking workflow decision components to optimize and classify issue\ndescriptions, using issue description response results to guide the\norchestration of bug fix agent workflows. SIADAFIX adaptively selects three\nrepair modes, i.e., easy, middle and hard mode, based on problem complexity. It\nemploys fast generalization for simple problems and test-time scaling\ntechniques for complex problems. Experimental results on the SWE-bench Lite\nshow that the proposed method achieves 60.67% pass@1 performance using the\nClaude-4 Sonnet model, reaching state-of-the-art levels among all open-source\nmethods. SIADAFIX effectively balances repair efficiency and accuracy,\nproviding new insights for automated program repair. Our code is available at\nhttps://github.com/liauto-siada/siada-cli."}
{"id": "2510.15976", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15976", "abs": "https://arxiv.org/abs/2510.15976", "authors": ["Chenrui Wang", "Junyi Shu", "Billy Chiu", "Yu Li", "Saleh Alharbi", "Min Zhang", "Jing Li"], "title": "Learning to Watermark: A Selective Watermarking Framework for Large Language Models via Multi-Objective Optimization", "comment": "28 pages, 11 figures, NeurIPS 2025 Poster", "summary": "The rapid development of LLMs has raised concerns about their potential\nmisuse, leading to various watermarking schemes that typically offer high\ndetectability. However, existing watermarking techniques often face trade-off\nbetween watermark detectability and generated text quality. In this paper, we\nintroduce Learning to Watermark (LTW), a novel selective watermarking framework\nthat leverages multi-objective optimization to effectively balance these\ncompeting goals. LTW features a lightweight network that adaptively decides\nwhen to apply the watermark by analyzing sentence embeddings, token entropy,\nand current watermarking ratio. Training of the network involves two\nspecifically constructed loss functions that guide the model toward\nPareto-optimal solutions, thereby harmonizing watermark detectability and text\nquality. By integrating LTW with two baseline watermarking methods, our\nexperimental evaluations demonstrate that LTW significantly enhances text\nquality without compromising detectability. Our selective watermarking approach\noffers a new perspective for designing watermarks for LLMs and a way to\npreserve high text quality for watermarks. The code is publicly available at:\nhttps://github.com/fattyray/learning-to-watermark"}
{"id": "2510.16242", "categories": ["cs.SE", "cs.DL"], "pdf": "https://arxiv.org/pdf/2510.16242", "abs": "https://arxiv.org/abs/2510.16242", "authors": ["Eva Maxfield Brown", "Isaac Slaughter", "Nicholas Weber"], "title": "Code Contribution and Credit in Science", "comment": null, "summary": "Software development has become essential to scientific research, but its\nrelationship to traditional metrics of scholarly credit remains poorly\nunderstood. We develop a dataset of approximately 140,000 paired research\narticles and code repositories, as well as a predictive model that matches\nresearch article authors with software repository developer accounts. We use\nthis data to investigate how software development activities influence credit\nallocation in collaborative scientific settings. Our findings reveal\nsignificant patterns distinguishing software contributions from traditional\nauthorship credit. We find that nearly 30% of articles include non-author code\ncontributors- individuals who participated in software development but received\nno formal authorship recognition. While code-contributing authors show a modest\n$\\sim$4.2% increase in article citations, this effect becomes non-significant\nwhen controlling for domain, article type, and open access status. First\nauthors are significantly more likely to be code contributors than other author\npositions. Notably, we identify a negative relationship between coding\nfrequency and scholarly impact metrics. Authors who contribute code more\nfrequently exhibit progressively lower h-indices than non-coding colleagues,\neven when controlling for publication count, author position, domain, and\narticle type. These results suggest a disconnect between software contributions\nand credit, highlighting important implications for institutional reward\nstructures and science policy."}
{"id": "2510.15989", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.15989", "abs": "https://arxiv.org/abs/2510.15989", "authors": ["Keshav Sood", "Sanjay Selvaraj", "Youyang Qu"], "title": "Meta-Guardian: An Early Evaluation of an On-device Application to Mitigate Psychography Data Leakage in Immersive Technologies", "comment": null, "summary": "The use of Immersive Technologies has shown its potential to revolutionize\nmany sectors such as health, entertainment, education, and industrial sectors.\nImmersive technologies such as Virtual Reality (VR), Augmented reality (AR),\nand Mixed Reality (MR) have redefined user interaction through real-time\nbiometric and behavioral tracking. Although Immersive Technologies (XR)\nessentially need the collection of the biometric data which acts as a baseline\nto create immersive experience, however, this ongoing feedback information\n(includes biometrics) poses critical privacy concerns due to the sensitive\nnature of the data collected. A comprehensive review of recent literature\nexplored the technical dimensions of related problem; however, they largely\noverlook the challenge particularly the intricacies of real-time biometric data\nfiltering within head-mounted display system. Motivated from this, in this\nwork, we propose a novel privacy-preserving system architecture that identifies\nand filters biometric signals (within the VR headset) in real-time before\ntransmission or storage. Implemented as a modular Unity Software-development\nKit (SDK) compatible with major immersive platforms, our solution (named\nMeta-Guardian) employs machine learning models for signal classification and a\nfiltering mechanism to block sensitive data. This framework aims to enable\ndevelopers to embed privacy-by-design principles into immersive experiences on\nvarious headsets and applications."}
{"id": "2510.16357", "categories": ["cs.SE", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.16357", "abs": "https://arxiv.org/abs/2510.16357", "authors": ["Jugal Gajjar", "Kamalasankari Subramaniakuppusamy"], "title": "MLCPD: A Unified Multi-Language Code Parsing Dataset with Universal AST Schema", "comment": "12 pages, 7 figures, 4 tables, 2 algorithms, and 34 references.\n  HuggingFace:\n  https://huggingface.co/datasets/jugalgajjar/MultiLang-Code-Parser-Dataset\n  GitHub: https://github.com/JugalGajjar/MultiLang-Code-Parser-Dataset", "summary": "We introduce the MultiLang Code Parser Dataset (MLCPD), a large-scale,\nlanguage-agnostic dataset unifying syntactic and structural representations of\ncode across ten major programming languages. MLCPD contains over seven million\nparsed source files normalized under our proposed universal Abstract Syntax\nTree (AST) schema, enabling consistent cross-language reasoning, structural\nlearning, and multilingual software analysis. Unlike existing corpora that\nfocus purely on token-level code or isolated parsers, MLCPD provides both\nhierarchical tree representations and rich metadata for every file, ensuring\nlossless syntactic coverage and structural uniformity. Each entry includes a\nnormalized schema, language-level metadata, and abstracted node semantics\nstored in Parquet format for scalable retrieval. Empirical analyses reveal\nstrong cross-language structural regularities-demonstrating that syntactic\ngraphs from languages as diverse as Python, Java, and Go can be aligned under a\nshared schema. We release the dataset publicly on Hugging Face and the\naccompanying codebase on GitHub, which includes complete pipelines for dataset\nreproduction, grammar compilation, and a visualization tool for exploring the\nunified AST across languages. Together, these resources establish MLCPD as an\nopen, reproducible foundation for future research in cross-language\nrepresentation learning and program analysis."}
{"id": "2510.15994", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15994", "abs": "https://arxiv.org/abs/2510.15994", "authors": ["Dongsen Zhang", "Zekun Li", "Xu Luo", "Xuannan Liu", "Peipei Li", "Wenjun Xu"], "title": "MCP Security Bench (MSB): Benchmarking Attacks Against Model Context Protocol in LLM Agents", "comment": null, "summary": "The Model Context Protocol (MCP) standardizes how large language model (LLM)\nagents discover, describe, and call external tools. While MCP unlocks broad\ninteroperability, it also enlarges the attack surface by making tools\nfirst-class, composable objects with natural-language metadata, and\nstandardized I/O. We present MSB (MCP Security Benchmark), the first end-to-end\nevaluation suite that systematically measures how well LLM agents resist\nMCP-specific attacks throughout the full tool-use pipeline: task planning, tool\ninvocation, and response handling. MSB contributes: (1) a taxonomy of 12\nattacks including name-collision, preference manipulation, prompt injections\nembedded in tool descriptions, out-of-scope parameter requests,\nuser-impersonating responses, false-error escalation, tool-transfer, retrieval\ninjection, and mixed attacks; (2) an evaluation harness that executes attacks\nby running real tools (both benign and malicious) via MCP rather than\nsimulation; and (3) a robustness metric that quantifies the trade-off between\nsecurity and performance: Net Resilient Performance (NRP). We evaluate nine\npopular LLM agents across 10 domains and 400+ tools, producing 2,000 attack\ninstances. Results reveal the effectiveness of attacks against each stage of\nMCP. Models with stronger performance are more vulnerable to attacks due to\ntheir outstanding tool calling and instruction following capabilities. MSB\nprovides a practical baseline for researchers and practitioners to study,\ncompare, and harden MCP agents."}
{"id": "2510.16384", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16384", "abs": "https://arxiv.org/abs/2510.16384", "authors": ["Yuwei Zhao", "Yuan-An Xiao", "Qianyu Xiao", "Zhao Zhang", "Yingfei Xiong"], "title": "SemOpt: LLM-Driven Code Optimization via Rule-Based Analysis", "comment": null, "summary": "Automated code optimization aims to improve performance in programs by\nrefactoring code, and recent studies focus on utilizing LLMs for the\noptimization. Typical existing approaches mine optimization commits from\nopen-source codebases to construct a large-scale knowledge base, then employ\ninformation retrieval techniques such as BM25 to retrieve relevant optimization\nexamples for hotspot code locations, thereby guiding LLMs to optimize these\nhotspots. However, since semantically equivalent optimizations can manifest in\nsyntactically dissimilar code snippets, current retrieval methods often fail to\nidentify pertinent examples, leading to suboptimal optimization performance.\nThis limitation significantly reduces the effectiveness of existing\noptimization approaches.\n  To address these limitations, we propose SemOpt, a novel framework that\nleverages static program analysis to precisely identify optimizable code\nsegments, retrieve the corresponding optimization strategies, and generate the\noptimized results. SemOpt consists of three key components: (1) A strategy\nlibrary builder that extracts and clusters optimization strategies from\nreal-world code modifications. (2) A rule generator that generates Semgrep\nstatic analysis rules to capture the condition of applying the optimization\nstrategy. (3) An optimizer that utilizes the strategy library to generate\noptimized code results. All the three components are powered by LLMs.\n  On our benchmark containing 151 optimization tasks, SemOpt demonstrates its\neffectiveness under different LLMs by increasing the number of successful\noptimizations by 1.38 to 28 times compared to the baseline. Moreover, on\npopular large-scale C/C++ projects, it can improve individual performance\nmetrics by 5.04% to 218.07%, demonstrating its practical utility."}
{"id": "2510.16005", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16005", "abs": "https://arxiv.org/abs/2510.16005", "authors": ["Giacomo Bertollo", "Naz Bodemir", "Jonah Burgess"], "title": "Breaking Guardrails, Facing Walls: Insights on Adversarial AI for Defenders & Researchers", "comment": null, "summary": "Analyzing 500 CTF participants, this paper shows that while participants\nreadily bypassed simple AI guardrails using common techniques, layered\nmulti-step defenses still posed significant challenges, offering concrete\ninsights for building safer AI systems."}
{"id": "2510.16395", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16395", "abs": "https://arxiv.org/abs/2510.16395", "authors": ["Xin Peng", "Chong Wang"], "title": "Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex Software Development", "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated strong\ncapabilities in software engineering tasks, raising expectations of\nrevolutionary productivity gains. However, enterprise software development is\nlargely driven by incremental evolution, where challenges extend far beyond\nroutine coding and depend critically on tacit knowledge, including design\ndecisions at different levels and historical trade-offs. To achieve effective\nAI-powered support for complex software development, we should align emerging\nAI capabilities with the practical realities of enterprise development. To this\nend, we systematically identify challenges from both software and LLM\nperspectives. Alongside these challenges, we outline opportunities where AI and\nstructured knowledge frameworks can enhance decision-making in tasks such as\nissue localization and impact analysis. To address these needs, we propose the\nCode Digital Twin, a living framework that models both the physical and\nconceptual layers of software, preserves tacit knowledge, and co-evolves with\nthe codebase. By integrating hybrid knowledge representations, multi-stage\nextraction pipelines, incremental updates, LLM-empowered applications, and\nhuman-in-the-loop feedback, the Code Digital Twin transforms fragmented\nknowledge into explicit and actionable representations. Our vision positions it\nas a bridge between AI advancements and enterprise software realities,\nproviding a concrete roadmap toward sustainable, intelligent, and resilient\ndevelopment and evolution of ultra-complex systems."}
{"id": "2510.16024", "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16024", "abs": "https://arxiv.org/abs/2510.16024", "authors": ["Abdulrahman Alhaidari", "Balaji Palanisamy", "Prashant Krishnamurthy"], "title": "On-Chain Decentralized Learning and Cost-Effective Inference for DeFi Attack Mitigation", "comment": "Published in the 7th Conference on Advances in Financial Technologies\n  (AFT 2025)", "summary": "Billions of dollars are lost every year in DeFi platforms by transactions\nexploiting business logic or accounting vulnerabilities. Existing defenses\nfocus on static code analysis, public mempool screening, attacker contract\ndetection, or trusted off-chain monitors, none of which prevents exploits\nsubmitted through private relays or malicious contracts that execute within the\nsame block. We present the first decentralized, fully on-chain learning\nframework that: (i) performs gas-prohibitive computation on Layer-2 to reduce\ncost, (ii) propagates verified model updates to Layer-1, and (iii) enables\ngas-bounded, low-latency inference inside smart contracts. A novel\nProof-of-Improvement (PoIm) protocol governs the training process and verifies\neach decentralized micro update as a self-verifying training transaction.\nUpdates are accepted by \\textit{PoIm} only if they demonstrably improve at\nleast one core metric (e.g., accuracy, F1-score, precision, or recall) on a\npublic benchmark without degrading any of the other core metrics, while\nadversarial proposals get financially penalized through an adaptable test set\nfor evolving threats. We develop quantization and loop-unrolling techniques\nthat enable inference for logistic regression, SVM, MLPs, CNNs, and gated RNNs\n(with support for formally verified decision tree inference) within the\nEthereum block gas limit, while remaining bit-exact to their off-chain\ncounterparts, formally proven in Z3. We curate 298 unique real-world exploits\n(2020 - 2025) with 402 exploit transactions across eight EVM chains,\ncollectively responsible for \\$3.74 B in losses."}
{"id": "2510.16433", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16433", "abs": "https://arxiv.org/abs/2510.16433", "authors": ["Tatsuya Shirai", "Olivier Nourry", "Yutaro Kashiwa", "Kenji Fujiwara", "Yasutaka Kamei", "Hajimu Iida"], "title": "Large-Scale Empirical Analysis of Continuous Fuzzing: Insights from 1 Million Fuzzing Sessions", "comment": null, "summary": "Software vulnerabilities are constantly being reported and exploited in\nsoftware products, causing significant impacts on society. In recent years, the\nmain approach to vulnerability detection, fuzzing, has been integrated into the\ncontinuous integration process to run in short and frequent cycles. This\ncontinuous fuzzing allows for fast identification and remediation of\nvulnerabilities during the development process. Despite adoption by thousands\nof projects, however, it is unclear how continuous fuzzing contributes to\nvulnerability detection. This study aims to elucidate the role of continuous\nfuzzing in vulnerability detection. Specifically, we investigate the coverage\nand the total number of fuzzing sessions when fuzzing bugs are discovered. We\ncollect issue reports, coverage reports, and fuzzing logs from OSS-Fuzz, an\nonline service provided by Google that performs fuzzing during continuous\nintegration. Through an empirical study of a total of approximately 1.12\nmillion fuzzing sessions from 878 projects participating in OSS-Fuzz, we reveal\nthat (i) a substantial number of fuzzing bugs exist prior to the integration of\ncontinuous fuzzing, leading to a high detection rate in the early stages; (ii)\ncode coverage continues to increase as continuous fuzzing progresses; and (iii)\nchanges in coverage contribute to the detection of fuzzing bugs. This study\nprovides empirical insights into how continuous fuzzing contributes to fuzzing\nbug detection, offering practical implications for future strategies and tool\ndevelopment in continuous fuzzing."}
{"id": "2510.16025", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16025", "abs": "https://arxiv.org/abs/2510.16025", "authors": ["Denis Ovichinnikov", "Hemant Kavadia", "Satya Keerti Chand Kudupudi", "Ilya Rempel", "Vineet Chadha", "Marty Franz", "Paul Master", "Craig Gentry", "Darlene Kindler", "Alberto Reyes", "Muthu Annamalai"], "title": "Resource Estimation of CGGI and CKKS scheme workloads on FracTLcore Computing Fabric", "comment": "5 tables, 2 figures, conference style", "summary": "Cornami Mx2 accelerates of Fully Homomorphic Encryption (FHE) applications,\nenabled by breakthrough work [1], which are otherwise compute limited. Our\nprocessor architecture is based on the systolic array of cores with in-memory\ncompute capability and a network on chip (NoC) processor architecture called\nthe \"FracTLcore compute fabric processor\" (Mx2). Here, we describe the work to\nestimate processor resources to compute workload in CGGI (TFHE-rs) or CKKS\nscheme during construction of our compiler backend for this architecture [2].\nThese processors are available for running applications in both the TFHE-rs\nBoolean scheme and CKKS scheme FHE applications."}
{"id": "2510.16502", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16502", "abs": "https://arxiv.org/abs/2510.16502", "authors": ["Sebastián Pizard", "Ramiro Moreira", "Federico Galiano", "Ignacio Sastre", "Lorena Etcheverry"], "title": "On the Use of Large Language Models for Qualitative Synthesis", "comment": null, "summary": "Large language models (LLMs) show promise for supporting systematic reviews\n(SR), even complex tasks such as qualitative synthesis (QS). However, applying\nthem to a stage that is unevenly reported and variably conducted carries\nimportant risks: misuse can amplify existing weaknesses and erode confidence in\nthe SR findings. To examine the challenges of using LLMs for QS, we conducted a\ncollaborative autoethnography involving two trials. We evaluated each trial for\nmethodological rigor and practical usefulness, and interpreted the results\nthrough a technical lens informed by how LLMs are built and their current\nlimitations."}
{"id": "2510.16028", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16028", "abs": "https://arxiv.org/abs/2510.16028", "authors": ["Jianzhu Yao", "Hongxu Su", "Taobo Liao", "Zerui Cheng", "Huan Zhang", "Xuechao Wang", "Pramod Viswanath"], "title": "Nondeterminism-Aware Optimistic Verification for Floating-Point Neural Networks", "comment": "17 pages, 7 figures", "summary": "Neural networks increasingly run on hardware outside the user's control\n(cloud GPUs, inference marketplaces). Yet ML-as-a-Service reveals little about\nwhat actually ran or whether returned outputs faithfully reflect the intended\ninputs. Users lack recourse against service downgrades (model swaps,\nquantization, graph rewrites, or discrepancies like altered ad embeddings).\nVerifying outputs is hard because floating-point(FP) execution on heterogeneous\naccelerators is inherently nondeterministic. Existing approaches are either\nimpractical for real FP neural networks or reintroduce vendor trust. We present\nNAO: a Nondeterministic tolerance Aware Optimistic verification protocol that\naccepts outputs within principled operator-level acceptance regions rather than\nrequiring bitwise equality. NAO combines two error models: (i) sound\nper-operator IEEE-754 worst-case bounds and (ii) tight empirical percentile\nprofiles calibrated across hardware. Discrepancies trigger a Merkle-anchored,\nthreshold-guided dispute game that recursively partitions the computation graph\nuntil one operator remains, where adjudication reduces to a lightweight\ntheoretical-bound check or a small honest-majority vote against empirical\nthresholds. Unchallenged results finalize after a challenge window, without\nrequiring trusted hardware or deterministic kernels. We implement NAO as a\nPyTorch-compatible runtime and a contract layer currently deployed on Ethereum\nHolesky testnet. The runtime instruments graphs, computes per-operator bounds,\nand runs unmodified vendor kernels in FP32 with negligible overhead (0.3% on\nQwen3-8B). Across CNNs, Transformers and diffusion models on A100, H100,\nRTX6000, RTX4090, empirical thresholds are $10^2-10^3$ times tighter than\ntheoretical bounds, and bound-aware adversarial attacks achieve 0% success. NAO\nreconciles scalability with verifiability for real-world heterogeneous ML\ncompute."}
{"id": "2510.16579", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16579", "abs": "https://arxiv.org/abs/2510.16579", "authors": ["Wendkûuni C. Ouédraogo", "Yinghua Li", "Xueqi Dang", "Pawel Borsukiewicz", "Xin Zhou", "Anil Koyuncu", "Jacques Klein", "David Lo", "Tegawendé F. Bissyandé"], "title": "Human-Aligned Code Readability Assessment with Large Language Models", "comment": null, "summary": "Code readability is crucial for software comprehension and maintenance, yet\ndifficult to assess at scale. Traditional static metrics often fail to capture\nthe subjective, context-sensitive nature of human judgments. Large Language\nModels (LLMs) offer a scalable alternative, but their behavior as readability\nevaluators remains underexplored. We introduce CoReEval, the first large-scale\nbenchmark for evaluating LLM-based code readability assessment, comprising over\n1.4 million model-snippet-prompt evaluations across 10 state of the art LLMs.\nThe benchmark spans 3 programming languages (Java, Python, CUDA), 2 code types\n(functional code and unit tests), 4 prompting strategies (ZSL, FSL, CoT, ToT),\n9 decoding settings, and developer-guided prompts tailored to junior and senior\npersonas. We compare LLM outputs against human annotations and a validated\nstatic model, analyzing numerical alignment (MAE, Pearson's, Spearman's) and\njustification quality (sentiment, aspect coverage, semantic clustering). Our\nfindings show that developer-guided prompting grounded in human-defined\nreadability dimensions improves alignment in structured contexts, enhances\nexplanation quality, and enables lightweight personalization through persona\nframing. However, increased score variability highlights trade-offs between\nalignment, stability, and interpretability. CoReEval provides a robust\nfoundation for prompt engineering, model alignment studies, and human in the\nloop evaluation, with applications in education, onboarding, and CI/CD\npipelines where LLMs can serve as explainable, adaptable reviewers."}
{"id": "2510.16037", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16037", "abs": "https://arxiv.org/abs/2510.16037", "authors": ["Peini Cheng", "Amir Bahmani"], "title": "Membership Inference over Diffusion-models-based Synthetic Tabular Data", "comment": null, "summary": "This study investigates the privacy risks associated with diffusion-based\nsynthetic tabular data generation methods, focusing on their susceptibility to\nMembership Inference Attacks (MIAs). We examine two recent models, TabDDPM and\nTabSyn, by developing query-based MIAs based on the step-wise error comparison\nmethod. Our findings reveal that TabDDPM is more vulnerable to these attacks.\nTabSyn exhibits resilience against our attack models. Our work underscores the\nimportance of evaluating the privacy implications of diffusion models and\nencourages further research into robust privacy-preserving mechanisms for\nsynthetic data generation."}
{"id": "2510.16665", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16665", "abs": "https://arxiv.org/abs/2510.16665", "authors": ["Mohamed Sami Rakha", "Andriy Miranskyy", "Daniel Alencar da Costa"], "title": "Contrasting the Hyperparameter Tuning Impact Across Software Defect Prediction Scenarios", "comment": "Accepted to IEEE Transactions on Software Engineering", "summary": "Software defect prediction (SDP) is crucial for delivering high-quality\nsoftware products. Recent research has indicated that prediction performance\nimprovements in SDP are achievable by applying hyperparameter tuning to a\nparticular SDP scenario. However, the positive impact resulting from the\nhyperparameter tuning step may differ based on the targeted SDP scenario.\nComparing the impact of hyperparameter tuning across SDP scenarios is necessary\nto provide comprehensive insights and enhance the robustness, generalizability,\nand, eventually, the practicality of SDP modeling for quality assurance.\n  Therefore, in this study, we contrast the impact of hyperparameter tuning\nacross two pivotal and consecutive SDP scenarios: (1) Inner Version Defect\nPrediction (IVDP) and (2) Cross Version Defect Prediction (CVDP). The main\ndistinctions between the two scenarios lie in the scope of defect prediction\nand the selected evaluation setups. This study's experiments use common\nevaluation setups, 28 machine learning (ML) algorithms, 53 post-release\nsoftware datasets, two tuning algorithms, and five optimization metrics. We\napply statistical analytics to compare the SDP performance impact differences\nby investigating the overall impact, the single ML algorithm impact, and\nvariations across different software dataset sizes.\n  The results indicate that the SDP gains within the IVDP scenario are\nsignificantly larger than those within the CVDP scenario. The results reveal\nthat asserting performance gains for up to 24 out of 28 ML algorithms may not\nhold across multiple SDP scenarios. Furthermore, we found that small software\ndatasets are more susceptible to larger differences in performance impacts.\nOverall, the study findings recommend software engineering researchers and\npractitioners to consider the effect of the selected SDP scenario when\nexpecting performance gains from hyperparameter tuning."}
{"id": "2510.16044", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16044", "abs": "https://arxiv.org/abs/2510.16044", "authors": ["Zeng Zhang", "Wenjie Yin", "Xiaoqi Li"], "title": "A Novel GPT-Based Framework for Anomaly Detection in System Logs", "comment": null, "summary": "Identification of anomalous events within system logs constitutes a pivotal\nelement within the frame- work of cybersecurity defense strategies. However,\nthis process faces numerous challenges, including the management of substantial\ndata volumes, the distribution of anomalies, and the precision of con-\nventional methods. To address this issue, the present paper puts forward a\nproposal for an intelligent detection method for system logs based on Genera-\ntive Pre-trained Transformers (GPT). The efficacy of this approach is\nattributable to a combination of structured input design and a Focal Loss op-\ntimization strategy, which collectively result in a substantial enhancement of\nthe performance of log anomaly detection. The initial approach involves the\nconversion of raw logs into event ID sequences through the use of the Drain\nparser. Subsequently, the Focal Loss loss function is employed to address the\nissue of class imbalance. The experimental re- sults demonstrate that the\noptimized GPT-2 model significantly outperforms the unoptimized model in a\nrange of key metrics, including precision, recall, and F1 score. In specific\ntasks, comparable or superior performance has been demonstrated to that of the\nGPT-3.5 API."}
{"id": "2510.16779", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16779", "abs": "https://arxiv.org/abs/2510.16779", "authors": ["Xiaoyu Guo", "Minggu Wang", "Jianjun Zhao"], "title": "QuanBench: Benchmarking Quantum Code Generation with Large Language Models", "comment": "This paper was accepted by ASE2025", "summary": "Large language models (LLMs) have demonstrated good performance in general\ncode generation; however, their capabilities in quantum code generation remain\ninsufficiently studied. This paper presents QuanBench, a benchmark for\nevaluating LLMs on quantum code generation. QuanBench includes 44 programming\ntasks that cover quantum algorithms, state preparation, gate decomposition, and\nquantum machine learning. Each task has an executable canonical solution and is\nevaluated by functional correctness (Pass@K) and quantum semantic equivalence\n(Process Fidelity). We evaluate several recent LLMs, including general-purpose\nand code-specialized models. The results show that current LLMs have limited\ncapability in generating the correct quantum code, with overall accuracy below\n40% and frequent semantic errors. We also analyze common failure cases, such as\noutdated API usage, circuit construction errors, and incorrect algorithm logic.\nQuanBench provides a basis for future work on improving quantum code generation\nwith LLMs."}
{"id": "2510.16054", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16054", "abs": "https://arxiv.org/abs/2510.16054", "authors": ["Zheng Hui", "Yijiang River Dong", "Sanhanat Sivapiromrat", "Ehsan Shareghi", "Nigel Collier"], "title": "PrivacyPAD: A Reinforcement Learning Framework for Dynamic Privacy-Aware Delegation", "comment": null, "summary": "When users submit queries to Large Language Models (LLMs), their prompts can\noften contain sensitive data, forcing a difficult choice: Send the query to a\npowerful proprietary LLM providers to achieving state-of-the-art performance\nand risk data exposure, or relying on smaller, local models guarantees data\nprivacy but often results in a degradation of task performance. Prior\napproaches have relied on static pipelines that use LLM rewriting, which\nshatters linguistic coherence and indiscriminately removes privacy-sensitive\ninformation, including task-critical content. We reformulate this challenge\n(Privacy-Conscious Delegation) as a sequential decision-making problem and\nintroduce a novel reinforcement learning (RL) framework called PrivacyPAD to\nsolve it. Our framework trains an agent to dynamically route text chunks,\nlearning a policy that optimally balances the trade-off between privacy leakage\nand task performance. It implicitly distinguishes between replaceable\nPersonally Identifiable Information (PII) (which it shields locally) and\ntask-critical PII (which it strategically sends to the remote model for maximal\nutility). To validate our approach in complex scenarios, we also introduce a\nnew medical dataset with high PII density. Our framework achieves a new\nstate-of-the-art on the privacy-utility frontier, demonstrating the necessity\nof learned, adaptive policies for deploying LLMs in sensitive environments."}
{"id": "2510.16786", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16786", "abs": "https://arxiv.org/abs/2510.16786", "authors": ["Pengfei Gao", "Chao Peng"], "title": "More with Less: An Empirical Study of Turn-Control Strategies for Efficient Coding Agents", "comment": null, "summary": "LLM-powered coding agents, which operate in iterative loops (turns) to solve\nsoftware engineering tasks, are becoming increasingly powerful. However, their\npractical deployment is hindered by significant and unpredictable costs. This\nchallenge arises from a combination of factors: quadratically growing token\ncounts with each turn, the high price of models, the large number of turns\nrequired for real-world tasks, and the tendency of agents to take inefficient\nor unnecessary actions. While existing research focuses on optimizing\nindividual turns, the strategic control of the total number of turns remains an\nunderexplored area for managing agent performance and cost. To address this\ngap, we conduct a comprehensive empirical study on SWE-bench using three\nstate-of-the-art models and evaluate the impact of three distinct turn-control\nstrategies: an unrestricted baseline, a fixed-turn limit with reminders, and a\nnovel dynamic-turn strategy that grants extensions on-demand. Our findings\nfirst reveal a fundamental trade-off in the unrestricted setting, where no\nsingle model excels across performance, cost, and turn efficiency. We then show\nthat a fixed-turn limit, specifically at the 75th percentile of the baseline,\nserves as a \"sweet spot\", substantially reducing costs (by 24%-68%) with\nminimal impact on solve rates. Most significantly, the dynamic-turn strategy\nconsistently outperforms fixed-limit approaches, achieving comparable or better\nsolve rates while further reducing costs by an additional 12%-24% by\nintelligently allocating resources only to tasks that need them. This work\nprovides the first systematic analysis of turn-control strategies, offering\nsimple yet effective guidelines for developers to balance cost and efficacy. We\ndemonstrate that dynamic resource allocation is a superior, easy-to-implement\napproach for deploying powerful yet economically viable coding agents."}
{"id": "2510.16067", "categories": ["cs.CR", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.16067", "abs": "https://arxiv.org/abs/2510.16067", "authors": ["Saurabh Deochake", "Ryan Murphy", "Jeremiah Gearheart"], "title": "A Multi-Cloud Framework for Zero-Trust Workload Authentication", "comment": "Cyber Security Experimentation and Test (CSET) at the Annual Computer\n  Security Applications Conference (ACSAC) 2025", "summary": "Static, long-lived credentials for workload authentication create untenable\nsecurity risks that violate Zero-Trust principles. This paper presents a\nmulti-cloud framework using Workload Identity Federation (WIF) and OpenID\nConnect (OIDC) for secretless authentication. Our approach uses\ncryptographically-verified, ephemeral tokens, allowing workloads to\nauthenticate without persistent private keys and mitigating credential theft.\nWe validate this framework in an enterprise-scale Kubernetes environment, which\nsignificantly reduces the attack surface. The model offers a unified solution\nto manage workload identities across disparate clouds, enabling future\nimplementation of robust, attribute-based access control."}
{"id": "2510.16809", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL", "68T50, 68N30, 68W40", "I.2.7; D.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.16809", "abs": "https://arxiv.org/abs/2510.16809", "authors": ["Amirkia Rafiei Oskooei", "Kaan Baturalp Cosdan", "Husamettin Isiktas", "Mehmet S. Aktas"], "title": "When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation", "comment": null, "summary": "Large Language Models (LLMs) with vast context windows offer new avenues for\nin-context learning (ICL), where providing many examples (\"many-shot\"\nprompting) is often assumed to enhance performance. We investigate this\nassumption for the complex task of code translation. Through a large-scale\nempirical study of over 90,000 translations, we systematically evaluate the\nimpact of scaling in-context examples from zero-shot to many-shot\nconfigurations of up to 625 examples, with prompts spanning from approximately\n100,000 to 800,000 tokens. Our findings reveal a \"many-shot paradox\": while\nstatic similarity metrics may modestly improve with more examples, functional\ncorrectness consistently peaks with few-shot prompting (5-25 examples).\nProviding substantially more examples often degrades this crucial functional\nperformance. This study highlights that for code translation, the quality of a\nfew well-chosen examples outweighs sheer quantity, challenging the universal\nefficacy of \"more is better\" for ICL and underscoring the task-dependent nature\nof optimal prompting strategies. Our results have significant implications for\neffectively leveraging LLMs in software engineering."}
{"id": "2510.16078", "categories": ["cs.CR", "cs.AI", "cs.CV", "68T10, 68T45, 94A60", "I.4.8; I.5.4; I.2.10"], "pdf": "https://arxiv.org/pdf/2510.16078", "abs": "https://arxiv.org/abs/2510.16078", "authors": ["Abdelilah Ganmati", "Karim Afdel", "Lahcen Koutti"], "title": "ISO/IEC-Compliant Match-on-Card Face Verification with Short Binary Templates", "comment": "~14 pages, 6 figures, 6 tables. Source uses elsarticle class; all\n  figures included as PNG/PDF. Primary: cs.CV", "summary": "We present a practical match-on-card design for face verification in which\ncompact 64/128-bit templates are produced off-card by PCA-ITQ and compared\non-card via constant-time Hamming distance. We specify ISO/IEC 7816-4 and\n14443-4 command APDUs with fixed-length payloads and decision-only status words\n(no score leakage), together with a minimal per-identity EEPROM map. Using real\nbinary codes from a CelebA working set (55 identities, 412 images), we (i)\nderive operating thresholds from ROC/DET, (ii) replay enroll->verify\ntransactions at those thresholds, and (iii) bound end-to-end time by pure link\nlatency plus a small constant on-card budget. Even at the slowest contact rate\n(9.6 kbps), total verification time is 43.9 ms (64 b) and 52.3 ms (128 b); at\n38.4 kbps both are <14 ms. At FAR = 1%, both code lengths reach TPR = 0.836,\nwhile 128 b lowers EER relative to 64 b. An optional +6 B helper (targeted\nsymbol-level parity over empirically unstable bits) is latency-negligible.\nOverall, short binary templates, fixed-payload decision-only APDUs, and\nconstant-time matching satisfy ISO/IEC transport constraints with wide timing\nmargin and align with ISO/IEC 24745 privacy goals. Limitations: single-dataset\nevaluation and design-level (pre-hardware) timing; we outline AgeDB/CFP-FP and\non-card microbenchmarks as next steps."}
{"id": "2510.16823", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16823", "abs": "https://arxiv.org/abs/2510.16823", "authors": ["Yue Liu", "Zhenchang Xing", "Shidong Pan", "Chakkrit Tantithamthavorn"], "title": "When AI Takes the Wheel: Security Analysis of Framework-Constrained Program Generation", "comment": null, "summary": "In recent years, the AI wave has grown rapidly in software development. Even\nnovice developers can now design and generate complex framework-constrained\nsoftware systems based on their high-level requirements with the help of Large\nLanguage Models (LLMs). However, when LLMs gradually \"take the wheel\" of\nsoftware development, developers may only check whether the program works. They\noften miss security problems hidden in how the generated programs are\nimplemented.\n  In this work, we investigate the security properties of framework-constrained\nprograms generated by state-of-the-art LLMs. We focus specifically on Chrome\nextensions due to their complex security model involving multiple privilege\nboundaries and isolated components. To achieve this, we built ChromeSecBench, a\ndataset with 140 prompts based on known vulnerable extensions. We used these\nprompts to instruct nine state-of-the-art LLMs to generate complete Chrome\nextensions, and then analyzed them for vulnerabilities across three dimensions:\nscenario types, model differences, and vulnerability categories. Our results\nshow that LLMs produced vulnerable programs at alarmingly high rates (18%-50%),\nparticularly in Authentication & Identity and Cookie Management scenarios (up\nto 83% and 78% respectively). Most vulnerabilities exposed sensitive browser\ndata like cookies, history, or bookmarks to untrusted code. Interestingly, we\nfound that advanced reasoning models performed worse, generating more\nvulnerabilities than simpler models. These findings highlight a critical gap\nbetween LLMs' coding skills and their ability to write secure\nframework-constrained programs."}
{"id": "2510.16087", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16087", "abs": "https://arxiv.org/abs/2510.16087", "authors": ["Sabbir M Saleh", "Nazim Madhavji", "John Steinbacher"], "title": "Towards a Blockchain-Based CI/CD Framework to Enhance Security in Cloud Environments", "comment": "8 pages, 5 figures, conference", "summary": "Security is becoming a pivotal point in cloud platforms. Several divisions,\nsuch as business organisations, health care, government, etc., have experienced\ncyber-attacks on their infrastructures. This research focuses on security\nissues within Continuous Integration and Deployment (CI/CD) pipelines in a\ncloud platform as a reaction to recent cyber breaches. This research proposes a\nblockchain-based solution to enhance CI/CD pipeline security. This research\naims to develop a framework that leverages blockchain's distributed ledger\ntechnology and tamper-resistant features to improve CI/CD pipeline security.\nThe goal is to emphasise secure software deployment by integrating threat\nmodelling frameworks and adherence to coding standards. It also aims to employ\ntools to automate security testing to detect publicly disclosed vulnerabilities\nand flaws, such as an outdated version of Java Spring Framework, a JavaScript\nlibrary from an unverified source, or a database library that allows SQL\ninjection attacks in the deployed software through the framework."}
{"id": "2510.17056", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.17056", "abs": "https://arxiv.org/abs/2510.17056", "authors": ["Luis F. G. Campos", "Leonardo C. Marques", "Walter T. Nakamura"], "title": "Will AI also replace inspectors? Investigating the potential of generative AIs in usability inspection", "comment": "Accepted and to be published in SBQS25 - Brazilian Symposium on\n  Software Quality 2025", "summary": "Usability inspection is a well-established technique for identifying\ninteraction issues in software interfaces, thereby contributing to improved\nproduct quality. However, it is a costly process that requires time and\nspecialized knowledge from inspectors. With advances in Artificial Intelligence\n(AI), new opportunities have emerged to support this task, particularly through\ngenerative models capable of interpreting interfaces and performing inspections\nmore efficiently. This study examines the performance of generative AIs in\nidentifying usability problems, comparing them to those of experienced human\ninspectors. A software prototype was evaluated by four specialists and two AI\nmodels (GPT-4o and Gemini 2.5 Flash), using metrics such as precision, recall,\nand F1-score. While inspectors achieved the highest levels of precision and\noverall coverage, the AIs demonstrated high individual performance and\ndiscovered many novel defects, but with a higher rate of false positives and\nredundant reports. The combination of AIs and human inspectors produced the\nbest results, revealing their complementarity. These findings suggest that AI,\nin its current stage, cannot replace human inspectors but can serve as a\nvaluable augmentation tool to improve efficiency and expand defect coverage.\nThe results provide evidence based on quantitative analysis to inform the\ndiscussion on the role of AI in usability inspections, pointing to viable paths\nfor its complementary use in software quality assessment contexts."}
{"id": "2510.16122", "categories": ["cs.CR", "cs.CL", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16122", "abs": "https://arxiv.org/abs/2510.16122", "authors": ["Owais Makroo", "Siva Rajesh Kasa", "Sumegh Roychowdhury", "Karan Gupta", "Nikhil Pattisapu", "Santhosh Kasa", "Sumit Negi"], "title": "The Hidden Cost of Modeling P(X): Vulnerability to Membership Inference Attacks in Generative Text Classifiers", "comment": null, "summary": "Membership Inference Attacks (MIAs) pose a critical privacy threat by\nenabling adversaries to determine whether a specific sample was included in a\nmodel's training dataset. Despite extensive research on MIAs, systematic\ncomparisons between generative and discriminative classifiers remain limited.\nThis work addresses this gap by first providing theoretical motivation for why\ngenerative classifiers exhibit heightened susceptibility to MIAs, then\nvalidating these insights through comprehensive empirical evaluation. Our study\nencompasses discriminative, generative, and pseudo-generative text classifiers\nacross varying training data volumes, evaluated on nine benchmark datasets.\nEmploying a diverse array of MIA strategies, we consistently demonstrate that\nfully generative classifiers which explicitly model the joint likelihood\n$P(X,Y)$ are most vulnerable to membership leakage. Furthermore, we observe\nthat the canonical inference approach commonly used in generative classifiers\nsignificantly amplifies this privacy risk. These findings reveal a fundamental\nutility-privacy trade-off inherent in classifier design, underscoring the\ncritical need for caution when deploying generative classifiers in\nprivacy-sensitive applications. Our results motivate future research directions\nin developing privacy-preserving generative classifiers that can maintain\nutility while mitigating membership inference vulnerabilities."}
{"id": "2510.17110", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17110", "abs": "https://arxiv.org/abs/2510.17110", "authors": ["Xiaoyu Guo", "Shinobu Saito", "Jianjun Zhao"], "title": "M2QCode: A Model-Driven Framework for Generating Multi-Platform Quantum Programs", "comment": "This paper was accepted by ASE2025", "summary": "With the growing interest in quantum computing, the emergence of quantum\nsupremacy has marked a pivotal milestone in the field. As a result, numerous\nquantum programming languages (QPLs) have been introduced to support the\ndevelopment of quantum algorithms. However, the application of Model-Driven\nDevelopment (MDD) in quantum system engineering remains largely underexplored.\nThis paper presents an MDD-based approach to support the structured design and\nimplementation of quantum systems. Our framework enables the automatic\ngeneration of quantum code for multiple QPLs, thereby enhancing development\nefficiency and consistency across heterogeneous quantum platforms. The\neffectiveness and practicality of our approach have been demonstrated through\nmultiple case studies."}
{"id": "2510.16128", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.16128", "abs": "https://arxiv.org/abs/2510.16128", "authors": ["Kate Glazko", "Jennifer Mankoff"], "title": "Prompt injections as a tool for preserving identity in GAI image descriptions", "comment": "Accepted as a poster to Soups 2025", "summary": "Generative AI risks such as bias and lack of representation impact people who\ndo not interact directly with GAI systems, but whose content does: indirect\nusers. Several approaches to mitigating harms to indirect users have been\ndescribed, but most require top down or external intervention. An emerging\nstrategy, prompt injections, provides an empowering alternative: indirect users\ncan mitigate harm against them, from within their own content. Our approach\nproposes prompt injections not as a malicious attack vector, but as a tool for\ncontent/image owner resistance. In this poster, we demonstrate one case study\nof prompt injections for empowering an indirect user, by retaining an image\nowner's gender and disabled identity when an image is described by GAI."}
{"id": "2510.17130", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17130", "abs": "https://arxiv.org/abs/2510.17130", "authors": ["Shuzheng Gao", "Chaozheng Wang", "Cuiyun Gao", "Michael R. Lyu"], "title": "SEER: Enhancing Chain-of-Thought Code Generation through Self-Exploring Deep Reasoning", "comment": "The paper was completed in Feb. 2025, submitted to ICSE 2026 in Mar.\n  2025, received a major revision in Jun. 2025, and was finally accepted in\n  Oct. 2025", "summary": "Code generation, the task of creating executable programs from natural\nlanguage requirements, has recently seen tremendous advances through\nChain-of-Thought (CoT) reasoning, which enables Large Language Models (LLMs) to\ndevelop high-level reasoning plans before writing code. Recent research has\nproposed various methods to enhance models' CoT reasoning for code generation\nsuch as prompt engineering and supervised fine-tuning. However, existing\napproaches still face three critical limitations: (1) limited exploration of\ndiverse reasoning paths, which constrains generalization across various\nprogramming scenarios, (2) lack of quality assessment for intermediate\nreasoning steps, which hampers the reliability of the generated plans and code,\nand (3) the potential negative impact of \"overthinking\", potentially leading to\nunnecessarily complex and incorrect solutions. To address these limitations, we\nframe CoT code generation as a decision making problem and present SEER, a\nSElf-Exploring deep Reasoning framework that enables accurate and adaptive\nreasoning for code generation. SEER introduces three key components: (1)\nDiverse reasoning path exploration, which aims at exploring diverse reasoning\npaths and annotating intermediate steps without relying on manual experts or\nclosed-source proprietary models; (2) Reasoning quality-aware model training,\nwhich trains a policy model for generating candidate reasoning steps and a\nvalue model for assessing their quality; and (3) Adaptive CoT reasoning, which\ndynamically switches between direct generation and step-by-step reasoning for\ndifferent problems."}
{"id": "2510.16168", "categories": ["cs.CR", "C.2.0; C.2.2; K.6.5"], "pdf": "https://arxiv.org/pdf/2510.16168", "abs": "https://arxiv.org/abs/2510.16168", "authors": ["Ahmed Fouad Kadhim Koysha", "Aytug Boyaci", "Rafet Akdeniz"], "title": "WebRTC Metadata and IP Leakage in Modern Browsers: A Cross-Platform Measurement Study", "comment": "14 pages, 7 figures. This preprint is under review at a Taylor &\n  Francis journal", "summary": "Web Real-Time Communication (WebRTC) enables real-time peer-to-peer\ncommunication, but its Interactive Connectivity Establishment (ICE) process can\nunintentionally expose internal and public IP addresses as metadata. This paper\npresents a cross-platform measurement study of WebRTC metadata leakage using\ncurrent (2025) builds of Chrome, Brave, Firefox, and Tor on desktop and mobile\nplatforms. Experiments were conducted across semi-trusted Wi-Fi and untrusted\nmobile carrier networks. Results show that Chrome remains the most\nleakage-prone, disclosing LAN or Carrier-Grade NAT (CGNAT) addresses on mobile\nand metadata on desktop; Brave avoids direct IP leaks but exposes\nsession-stable mDNS identifiers; Firefox provides strong protection on desktop\nbut leaks internal IPs on Android; and Tor consistently prevents all forms of\nleakage. We introduce a structured threat model for semi-trusted environments\nand evaluate the limitations of mDNS obfuscation. Finally, we propose layered\nmitigation strategies combining browser defaults, institutional safeguards, and\nuser controls. Findings demonstrate that while direct LAN leakage is declining,\nemerging vectors such as mDNS and CGNAT create persistent privacy risks\nrequiring protocol-level redesign and policy action."}
{"id": "2510.17142", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17142", "abs": "https://arxiv.org/abs/2510.17142", "authors": ["Xiaoxue Ren", "Jun Wan", "Yun Peng", "Zhongxin Liu", "Ming Liang", "Dajun Chen", "Wei Jiang", "Yong Li"], "title": "PEACE: Towards Efficient Project-Level Efficiency Optimization via Hybrid Code Editing", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant capability in code\ngeneration, but their potential in code efficiency optimization remains\nunderexplored. Previous LLM-based code efficiency optimization approaches\nexclusively focus on function-level optimization and overlook interaction\nbetween functions, failing to generalize to real-world development scenarios.\nCode editing techniques show great potential for conducting project-level\noptimization, yet they face challenges associated with invalid edits and\nsuboptimal internal functions. To address these gaps, we propose Peace, a novel\nhybrid framework for Project-level code Efficiency optimization through\nAutomatic Code Editing, which also ensures the overall correctness and\nintegrity of the project. Peace integrates three key phases: dependency-aware\noptimizing function sequence construction, valid associated edits\nidentification, and efficiency optimization editing iteration. To rigorously\nevaluate the effectiveness of Peace, we construct PeacExec, the first benchmark\ncomprising 146 real-world optimization tasks from 47 high-impact GitHub Python\nprojects, along with highly qualified test cases and executable environments.\nExtensive experiments demonstrate Peace's superiority over the state-of-the-art\nbaselines, achieving a 69.2% correctness rate (pass@1), +46.9% opt rate, and\n0.840 speedup in execution efficiency. Notably, our Peace outperforms all\nbaselines by significant margins, particularly in complex optimization tasks\nwith multiple functions. Moreover, extensive experiments are also conducted to\nvalidate the contributions of each component in Peace, as well as the rationale\nand effectiveness of our hybrid framework design."}
{"id": "2510.16219", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16219", "abs": "https://arxiv.org/abs/2510.16219", "authors": ["Yang Feng", "Xudong Pan"], "title": "SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based Dynamic Threat Detection", "comment": null, "summary": "Malicious agents pose significant threats to the reliability and\ndecision-making capabilities of Multi-Agent Systems (MAS) powered by Large\nLanguage Models (LLMs). Existing defenses often fall short due to reactive\ndesigns or centralized architectures which may introduce single points of\nfailure. To address these challenges, we propose SentinelNet, the first\ndecentralized framework for proactively detecting and mitigating malicious\nbehaviors in multi-agent collaboration. SentinelNet equips each agent with a\ncredit-based detector trained via contrastive learning on augmented adversarial\ndebate trajectories, enabling autonomous evaluation of message credibility and\ndynamic neighbor ranking via bottom-k elimination to suppress malicious\ncommunications. To overcome the scarcity of attack data, it generates\nadversarial trajectories simulating diverse threats, ensuring robust training.\nExperiments on MAS benchmarks show SentinelNet achieves near-perfect detection\nof malicious agents, close to 100% within two debate rounds, and recovers 95%\nof system accuracy from compromised baselines. By exhibiting strong\ngeneralizability across domains and attack patterns, SentinelNet establishes a\nnovel paradigm for safeguarding collaborative MAS."}
{"id": "2510.17163", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17163", "abs": "https://arxiv.org/abs/2510.17163", "authors": ["Shuzheng Gao", "Eric John Li", "Man Ho Lam", "Jingyu Xiao", "Yuxuan Wan", "Chaozheng Wang", "Ng Man Tik", "Michael R. Lyu"], "title": "TREAT: A Code LLMs Trustworthiness / Reliability Evaluation and Testing Framework", "comment": null, "summary": "Large foundation models are fundamentally transforming the software\nengineering landscape, demonstrating exceptional capabilities across diverse\ntasks such as code generation, debugging, and testing. Despite this rapid\nprogress, a significant gap remains in how to comprehensively evaluate these\nmodels' trustworthiness in real-world software engineering scenarios. Existing\nbenchmarks suffer from limited task scope and fail to incorporate critical\nevaluation aspects such as the robustness and reliability of models. To bridge\nthis gap, we present an evaluation framework called TREAT (Code LLMs\nTrustworthiness / Reliability Evaluation And Testing) that provides a holistic\nassessment of model performance in code intelligence tasks. Our evaluation\nframework addresses key limitations in existing approaches with four main\nimprovements: (1) Multi-Task Holistic Evaluation that spans diverse software\nengineering activities rather than limited coding tasks; (2) Multi-Language and\nMulti-Modality Assessment that extends beyond traditional single-language,\ntext-only benchmarks to include multi-modality coding tasks; (3) Robustness\nAssessment that evaluates model reliability under semantically-preserving code\ntransformations; and (4) Rigorous Evaluation Methodology that enhances the\ntrustworthiness of evaluation results through diverse evaluation prompts and\nadaptive solution extraction. Based on this evaluation framework, we assess 26\nstate-of-the-art models and uncover both their strengths and limitations,\nyielding several key insights:(1) Current models show substantial performance\nvariation across programming tasks; (2) Multi-modal language models demonstrate\nspecific performance limitations in UI code generation and edit;"}
{"id": "2510.16229", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16229", "abs": "https://arxiv.org/abs/2510.16229", "authors": ["Vienna Li", "Justin Villa", "Dan Diessner", "Jayson Clifford", "Laxima Niure Kandel"], "title": "C/N0 Analysis-Based GPS Spoofing Detection with Variable Antenna Orientations", "comment": null, "summary": "GPS spoofing poses a growing threat to aviation by falsifying satellite\nsignals and misleading aircraft navigation systems. This paper demonstrates a\nproof-of-concept spoofing detection strategy based on analyzing satellite\nCarrier-to-Noise Density Ratio (C/N$_0$) variation during controlled static\nantenna orientations. Using a u-blox EVK-M8U receiver and a GPSG-1000 satellite\nsimulator, C/N$_0$ data is collected under three antenna orientations flat,\nbanked right, and banked left) in both real-sky (non-spoofed) and spoofed\nenvironments. Our findings reveal that under non-spoofed signals, C/N$_0$\nvalues fluctuate naturally with orientation, reflecting true geometric\ndependencies. However, spoofed signals demonstrate a distinct pattern: the flat\norientation, which directly faces the spoofing antenna, consistently yielded\nthe highest C/N$_0$ values, while both banked orientations showed reduced\nC/N$_0$ due to misalignment with the spoofing source. These findings suggest\nthat simple maneuvers such as brief banking to induce C/N$_0$ variations can\nprovide early cues of GPS spoofing for general aviation and UAV systems."}
{"id": "2510.17164", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17164", "abs": "https://arxiv.org/abs/2510.17164", "authors": ["Maria Deolinda Santana", "Cleyton Magalhaes", "Ronnie de Souza Santos"], "title": "Software Testing with Large Language Models: An Interview Study with Practitioners", "comment": null, "summary": "\\textit{Background:} The use of large language models in software testing is\ngrowing fast as they support numerous tasks, from test case generation to\nautomation, and documentation. However, their adoption often relies on informal\nexperimentation rather than structured guidance. \\textit{Aims:} This study\ninvestigates how software testing professionals use LLMs in practice to propose\na preliminary, practitioner-informed guideline to support their integration\ninto testing workflows. \\textit{Method:} We conducted a qualitative study with\n15 software testers from diverse roles and domains. Data were collected through\nsemi-structured interviews and analyzed using grounded theory-based processes\nfocused on thematic analysis. \\textit{Results:} Testers described an iterative\nand reflective process that included defining testing objectives, applying\nprompt engineering strategies, refining prompts, evaluating outputs, and\nlearning over time. They emphasized the need for human oversight and careful\nvalidation, especially due to known limitations of LLMs such as hallucinations\nand inconsistent reasoning. \\textit{Conclusions:} LLM adoption in software\ntesting is growing, but remains shaped by evolving practices and caution around\nrisks. This study offers a starting point for structuring LLM use in testing\ncontexts and invites future research to refine these practices across teams,\ntools, and tasks."}
{"id": "2510.16251", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16251", "abs": "https://arxiv.org/abs/2510.16251", "authors": ["Changyu Zhao", "Yohan Beugin", "Jean-Charles Noirot Ferrand", "Quinn Burke", "Guancheng Li", "Patrick McDaniel"], "title": "LibIHT: A Hardware-Based Approach to Efficient and Evasion-Resistant Dynamic Binary Analysis", "comment": "Accepted in Proceedings of the 2025 Workshop on Software\n  Understanding and Reverse Engineering (SURE'25), October 13-17, 2025, Taipei,\n  Taiwan", "summary": "Dynamic program analysis is invaluable for malware detection, debugging, and\nperformance profiling. However, software-based instrumentation incurs high\noverhead and can be evaded by anti-analysis techniques. In this paper, we\npropose LibIHT, a hardware-assisted tracing framework that leverages on-CPU\nbranch tracing features (Intel Last Branch Record and Branch Trace Store) to\nefficiently capture program control-flow with minimal performance impact. Our\napproach reconstructs control-flow graphs (CFGs) by collecting hardware\ngenerated branch execution data in the kernel, preserving program behavior\nagainst evasive malware. We implement LibIHT as an OS kernel module and\nuser-space library, and evaluate it on both benign benchmark programs and\nadversarial anti-instrumentation samples. Our results indicate that LibIHT\nreduces runtime overhead by over 150x compared to Intel Pin (7x vs 1,053x\nslowdowns), while achieving high fidelity in CFG reconstruction (capturing over\n99% of execution basic blocks and edges). Although this hardware-assisted\napproach sacrifices the richer semantic detail available from full software\ninstrumentation by capturing only branch addresses, this trade-off is\nacceptable for many applications where performance and low detectability are\nparamount. Our findings show that hardware-based tracing captures control flow\ninformation significantly faster, reduces detection risk and performs dynamic\nanalysis with minimal interference."}
{"id": "2510.17184", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17184", "abs": "https://arxiv.org/abs/2510.17184", "authors": ["Nicolas Robert", "Fabien Gandon", "Maxime Lefrançois"], "title": "OLIVAW: ACIMOV's GitHub robot assisting agile collaborative ontology development", "comment": null, "summary": "Agile and collaborative approaches to ontologies design are crucial because\nthey contribute to making them userdriven, up-to-date, and able to evolve\nalongside the systems they support, hence proper continuous validation tooling\nis required to ensure ontologies match developers' requirements all along their\ndevelopment. We propose OLIVAW (Ontology Long-lived Integration Via ACIMOV\nWorkflow), a tool supporting the ACIMOV methodology on GitHub. It relies on W3C\nStandards to assist the development of modular ontologies through GitHub\nComposite Actions, pre-commit hooks, or a command line interface. OLIVAW was\ntested on several ontology projects to ensure its usefulness, genericity and\nreusability. A template repository is available for a quick start. OLIVAW is"}
{"id": "2510.16255", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16255", "abs": "https://arxiv.org/abs/2510.16255", "authors": ["Sarah Egler", "John Schulman", "Nicholas Carlini"], "title": "Detecting Adversarial Fine-tuning with Auditing Agents", "comment": null, "summary": "Large Language Model (LLM) providers expose fine-tuning APIs that let end\nusers fine-tune their frontier LLMs. Unfortunately, it has been shown that an\nadversary with fine-tuning access to an LLM can bypass safeguards. Particularly\nconcerning, such attacks may avoid detection with datasets that are only\nimplicitly harmful. Our work studies robust detection mechanisms for\nadversarial use of fine-tuning APIs. We introduce the concept of a fine-tuning\nauditing agent and show it can detect harmful fine-tuning prior to model\ndeployment. We provide our auditing agent with access to the fine-tuning\ndataset, as well as the fine-tuned and pre-fine-tuned models, and request the\nagent assigns a risk score for the fine-tuning job. We evaluate our detection\napproach on a diverse set of eight strong fine-tuning attacks from the\nliterature, along with five benign fine-tuned models, totaling over 1400\nindependent audits. These attacks are undetectable with basic content\nmoderation on the dataset, highlighting the challenge of the task. With the\nbest set of affordances, our auditing agent achieves a 56.2% detection rate of\nadversarial fine-tuning at a 1% false positive rate. Most promising, the\nauditor is able to detect covert cipher attacks that evade safety evaluations\nand content moderation of the dataset. While benign fine-tuning with\nunintentional subtle safety degradation remains a challenge, we establish a\nbaseline configuration for further work in this area. We release our auditing\nagent at https://github.com/safety-research/finetuning-auditor."}
{"id": "2510.17376", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17376", "abs": "https://arxiv.org/abs/2510.17376", "authors": ["Yongmin Li", "Jia Li", "Ge Li", "Zhi Jin"], "title": "AdapTrack: Constrained Decoding without Distorting LLM's Output Intent", "comment": "to be published in ICSE 2026", "summary": "Language model-based code generation and completion tools have been widely\nadopted, but they may sometimes produce code that does not meet necessary\nconstraints, such as syntactic correctness or API existence. Constrained\ndecoding techniques are developed to help the model generate code adhering to\nthe constraints by greedily eliminating generation options that violate\nconstraints at each step of the generation process. However, there is a severe\nlimitation of constrained decoding, that it distorts the model's output intent,\nforcing it to produce code that may satisfy the constraint but does not match\nthe development intent and is therefore incorrect. In response to this\nchallenge, we propose AdapTrack. By incorporating backtracking into the\ngeneration process, AdapTrack avoids distorting the output intent of the model,\nthereby producing results that are not only constraint-compliant but also more\nsemantically aligned with model's output intent. On our synthetic API\ncompletion dataset, AdapTrack can achieve up to 360.87% improvement compared to\nconstrained decoding; on the real-world API completion dataset we collect that\nexhibits similar issues, AdapTrack can achieve up to 38.93% improvement over\nconstrained decoding; in general code genration benchmarks, compared to\nconstrained decoding, AdapTrack can achieve up to 7.84% improvement on\nHumanEval, and up to 6.42% improvement on MBPP. This indicates that, simply by\nbetter adhering to the model's output intent, AdapTrack can achieve significant\nimprovements. We provide a theoretical proof that the distribution produced by\nAdapTrack aligns with the model's distribution given the generated tokens,\nthereby ensuring that the model's output intent is not distorted. Experiments\non DSL problems show that, compared to existing methods, our approach can\nprovide generation results that are more consistent with the language model's\ndistribution."}
{"id": "2510.16331", "categories": ["cs.CR", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.16331", "abs": "https://arxiv.org/abs/2510.16331", "authors": ["Fatemeh Jafarian Dehkordi", "Elahe Vedadi", "Alireza Feizbakhsh", "Yasaman Keshtkarjahromi", "Hulya Seferoglu"], "title": "Efficient and Privacy-Preserving Binary Dot Product via Multi-Party Computation", "comment": null, "summary": "Striking a balance between protecting data privacy and enabling collaborative\ncomputation is a critical challenge for distributed machine learning. While\nprivacy-preserving techniques for federated learning have been extensively\ndeveloped, methods for scenarios involving bitwise operations, such as\ntree-based vertical federated learning (VFL), are still underexplored.\nTraditional mechanisms, including Shamir's secret sharing and multi-party\ncomputation (MPC), are not optimized for bitwise operations over binary data,\nparticularly in settings where each participant holds a different part of the\nbinary vector. This paper addresses the limitations of existing methods by\nproposing a novel binary multi-party computation (BiMPC) framework. The BiMPC\nmechanism facilitates privacy-preserving bitwise operations, with a particular\nfocus on dot product computations of binary vectors, ensuring the privacy of\neach individual bit. The core of BiMPC is a novel approach called Dot Product\nvia Modular Addition (DoMA), which uses regular and modular additions for\nefficient binary dot product calculation. To ensure privacy, BiMPC uses random\nmasking in a higher field for linear computations and a three-party oblivious\ntransfer (triot) protocol for non-linear binary operations. The privacy\nguarantees of the BiMPC framework are rigorously analyzed, demonstrating its\nefficiency and scalability in distributed settings."}
{"id": "2510.17430", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17430", "abs": "https://arxiv.org/abs/2510.17430", "authors": ["Kuniaki Kudo", "Sherine Devi"], "title": "Scalable CI/CD for Legacy Modernization: An Industrial Experience Addressing Internal Challenges Related to the 2025 Japan Cliff", "comment": null, "summary": "We have developed a Scalable CI/CD Pipeline to address internal challenges\nrelated to Japan 2025 cliff problem, a critical issue where the mass end of\nservice life of legacy core IT systems threatens to significantly increase the\nmaintenance cost and black box nature of these system also leads to difficult\nupdate moreover replace, which leads to lack of progress in Digital\nTransformation (DX). If not addressed, Japan could potentially lose up to 12\ntrillion yen per year after 2025, which is 3 times more than the cost in\nprevious years. Asahi also faced the same internal challenges regarding legacy\nsystem, where manual maintenance workflows and limited QA environment have left\ncritical systems outdated and difficult to update. Middleware and OS version\nhave remained unchanged for years, leading to now its nearing end of service\nlife which require huge maintenance cost and effort to continue its operation.\nTo address this problem, we have developed and implemented a Scalable CI/CD\nPipeline where isolated development environments can be created and deleted\ndynamically and is scalable as needed. This Scalable CI/CD Pipeline incorporate\nGitHub for source code control and branching, Jenkins for pipeline automation,\nAmazon Web Services for scalable environment, and Docker for environment\ncontainerization. This paper presents the design and architecture of the\nScalable CI/CD Pipeline, with the implementation along with some use cases.\nThrough Scalable CI/CD, developers can freely and safely test maintenance\nprocedures and do experiments with new technology in their own environment,\nreducing maintenance cost and drive Digital Transformation (DX).\n  key words: 2025 Japan Cliff, Scalable CI/CD, DevOps, Legacy IT Modernization."}
{"id": "2510.16367", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16367", "abs": "https://arxiv.org/abs/2510.16367", "authors": ["Shuai Li", "Kejiang Chen", "Jun Jiang", "Jie Zhang", "Qiyi Yao", "Kai Zeng", "Weiming Zhang", "Nenghai Yu"], "title": "EditMark: Watermarking Large Language Models based on Model Editing", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities, but\ntheir training requires extensive data and computational resources, rendering\nthem valuable digital assets. Therefore, it is essential to watermark LLMs to\nprotect their copyright and trace unauthorized use or resale. Existing methods\nfor watermarking LLMs primarily rely on training LLMs with a watermarked\ndataset, which entails burdensome training costs and negatively impacts the\nLLM's performance. In addition, their watermarked texts are not logical or\nnatural, thereby reducing the stealthiness of the watermark. To address these\nissues, we propose EditMark, the first watermarking method that leverages model\nediting to embed a training-free, stealthy, and performance-lossless watermark\nfor LLMs. We observe that some questions have multiple correct answers.\nTherefore, we assign each answer a unique watermark and update the weights of\nLLMs to generate corresponding questions and answers through the model editing\ntechnique. In addition, we refine the model editing technique to align with the\nrequirements of watermark embedding. Specifically, we introduce an adaptive\nmulti-round stable editing strategy, coupled with the injection of a noise\nmatrix, to improve both the effectiveness and robustness of the watermark\nembedding. Extensive experiments indicate that EditMark can embed 32-bit\nwatermarks into LLMs within 20 seconds (Fine-tuning: 6875 seconds) with a\nwatermark extraction success rate of 100%, which demonstrates its effectiveness\nand efficiency. External experiments further demonstrate that EditMark has\nfidelity, stealthiness, and a certain degree of robustness against common\nattacks."}
{"id": "2510.16461", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.16461", "abs": "https://arxiv.org/abs/2510.16461", "authors": ["Minjae Seo", "Jaehan Kim", "Eduard Marin", "Myoungsung You", "Taejune Park", "Seungsoo Lee", "Seungwon Shin", "Jinwoo Kim"], "title": "Heimdallr: Fingerprinting SD-WAN Control-Plane Architecture via Encrypted Control Traffic", "comment": "14 pages, 14 figures", "summary": "Software-defined wide area network (SD-WAN) has emerged as a new paradigm for\nsteering a large-scale network flexibly by adopting distributed\nsoftware-defined network (SDN) controllers. The key to building a logically\ncentralized but physically distributed control-plane is running diverse cluster\nmanagement protocols to achieve consistency through an exchange of control\ntraffic. Meanwhile, we observe that the control traffic exposes unique\ntime-series patterns and directional relationships due to the operational\nstructure even though the traffic is encrypted, and this pattern can disclose\nconfidential information such as control-plane topology and protocol\ndependencies, which can be exploited for severe attacks. With this insight, we\npropose a new SD-WAN fingerprinting system, called Heimdallr. It analyzes\nperiodical and operational patterns of SD-WAN cluster management protocols and\nthe context of flow directions from the collected control traffic utilizing a\ndeep learning-based approach, so that it can classify the cluster management\nprotocols automatically from miscellaneous control traffic datasets. Our\nevaluation, which is performed in a realistic SD-WAN environment consisting of\ngeographically distant three campus networks and one enterprise network shows\nthat Heimdallr can classify SD-WAN control traffic with $\\geq$ 93%, identify\nindividual protocols with $\\geq$ 80% macro F-1 scores, and finally can infer\ncontrol-plane topology with $\\geq$ 70% similarity."}
{"id": "2510.16544", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16544", "abs": "https://arxiv.org/abs/2510.16544", "authors": ["Weijie Chen", "Shan Tang", "Yulin Tang", "Xiapu Luo", "Yinqian Zhang", "Weizhong Qiang"], "title": "$ρ$Hammer: Reviving RowHammer Attacks on New Architectures via Prefetching", "comment": "Accepted for publication in the 58th IEEE/ACM International Symposium\n  on Microarchitecture (MICRO '25). This is the author's version of the paper", "summary": "Rowhammer is a critical vulnerability in dynamic random access memory (DRAM)\nthat continues to pose a significant threat to various systems. However, we\nfind that conventional load-based attacks are becoming highly ineffective on\nthe most recent architectures such as Intel Alder and Raptor Lake. In this\npaper, we present $\\rho$Hammer, a new Rowhammer framework that systematically\novercomes three core challenges impeding attacks on these new architectures.\nFirst, we design an efficient and generic DRAM address mapping\nreverse-engineering method that uses selective pairwise measurements and\nstructured deduction, enabling recovery of complex mappings within seconds on\nthe latest memory controllers. Second, to break through the activation rate\nbottleneck of load-based hammering, we introduce a novel prefetch-based\nhammering paradigm that leverages the asynchronous nature of x86 prefetch\ninstructions and is further enhanced by multi-bank parallelism to maximize\nthroughput. Third, recognizing that speculative execution causes more severe\ndisorder issues for prefetching, which cannot be simply mitigated by memory\nbarriers, we develop a counter-speculation hammering technique using\ncontrol-flow obfuscation and optimized NOP-based pseudo-barriers to maintain\nprefetch order with minimal overhead. Evaluations across four latest Intel\narchitectures demonstrate $\\rho$Hammer's breakthrough effectiveness: it induces\nup to 200K+ additional bit flips within 2-hour attack pattern fuzzing processes\nand has a 112x higher flip rate than the load-based hammering baselines on\nComet and Rocket Lake. Also, we are the first to revive Rowhammer attacks on\nthe latest Raptor Lake architecture, where baselines completely fail, achieving\nstable flip rates of 2,291/min and fast end-to-end exploitation."}
{"id": "2510.16558", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16558", "abs": "https://arxiv.org/abs/2510.16558", "authors": ["Xiaofan Li", "Xing Gao"], "title": "Toward Understanding Security Issues in the Model Context Protocol Ecosystem", "comment": null, "summary": "The Model Context Protocol (MCP) is an emerging open standard that enables\nAI-powered applications to interact with external tools through structured\nmetadata. A rapidly growing ecosystem has formed around MCP, including a wide\nrange of MCP hosts (i.e., Cursor, Windsurf, Claude Desktop, and Cline), MCP\nregistries (i.e., mcp.so, MCP Market, MCP Store, Pulse MCP, Smithery, and npm),\nand thousands of community-contributed MCP servers. Although the MCP ecosystem\nis gaining traction, there has been little systematic study of its architecture\nand associated security risks. In this paper, we present the first\ncomprehensive security analysis of the MCP ecosystem. We decompose MCP\necosystem into three core components: hosts, registries, and servers, and study\nthe interactions and trust relationships among them. Users search for servers\non registries and configure them in the host, which translates LLM-generated\noutput into external tool invocations provided by the servers and executes\nthem. Our qualitative analysis reveals that hosts lack output verification\nmechanisms for LLM-generated outputs, enabling malicious servers to manipulate\nmodel behavior and induce a variety of security threats, including but not\nlimited to sensitive data exfiltration. We uncover a wide range of\nvulnerabilities that enable attackers to hijack servers, due to the lack of a\nvetted server submission process in registries. To support our analysis, we\ncollect and analyze a dataset of 67,057 servers from six public registries. Our\nquantitative analysis demonstrates that a substantial number of servers can be\nhijacked by attackers. Finally, we propose practical defense strategies for MCP\nhosts, registries, and users. We responsibly disclosed our findings to affected\nhosts and registries."}
{"id": "2510.16581", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16581", "abs": "https://arxiv.org/abs/2510.16581", "authors": ["Xinfeng Li", "Shengyuan Pang", "Jialin Wu", "Jiangyi Deng", "Huanlong Zhong", "Yanjiao Chen", "Jie Zhang", "Wenyuan Xu"], "title": "Patronus: Safeguarding Text-to-Image Models against White-Box Adversaries", "comment": "14 pages, 18 figures, 7 tables", "summary": "Text-to-image (T2I) models, though exhibiting remarkable creativity in image\ngeneration, can be exploited to produce unsafe images. Existing safety\nmeasures, e.g., content moderation or model alignment, fail in the presence of\nwhite-box adversaries who know and can adjust model parameters, e.g., by\nfine-tuning. This paper presents a novel defensive framework, named Patronus,\nwhich equips T2I models with holistic protection to defend against white-box\nadversaries. Specifically, we design an internal moderator that decodes unsafe\ninput features into zero vectors while ensuring the decoding performance of\nbenign input features. Furthermore, we strengthen the model alignment with a\ncarefully designed non-fine-tunable learning mechanism, ensuring the T2I model\nwill not be compromised by malicious fine-tuning. We conduct extensive\nexperiments to validate the intactness of the performance on safe content\ngeneration and the effectiveness of rejecting unsafe content generation.\nResults also confirm the resilience of Patronus against various fine-tuning\nattacks by white-box adversaries."}
{"id": "2510.16593", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16593", "abs": "https://arxiv.org/abs/2510.16593", "authors": ["Khandaker Akramul Haque", "Katherine R. Davis"], "title": "DESTinE Block: Private Blockchain Based Data Storage Framework for Power System", "comment": null, "summary": "This paper presents DESTinE Block, a blockchain-based data storage framework\ndesigned for power systems and optimized for resource-constrained environments,\nincluding grid-edge devices such as single-board computers. The proposed\narchitecture leverages the InterPlanetary File System (IPFS) for storing large\nfiles while maintaining secure and traceable metadata on a custom blockchain\nnamed DESTinE Block. The metadata, comprising the IPFS Content Identifier\n(CID), uploader identity, administrator verification, and timestamp; is\nimmutably recorded on-chain to ensure authenticity and integrity. DESTinE Block\nadopts a dual-blockchain abstraction, where the blockchain remains unaware of\nthe IPFS storage layer to enhance security and limit the exposure of sensitive\nfile data. The consensus mechanism is based on Proof of Authority (PoA), where\nboth an administrator and an uploader with distinct cryptographic key pairs are\nrequired to create a block collaboratively. Each block contains verified\nsignatures of both parties and is designed to be computationally efficient,\nenabling deployment on devices like the Raspberry Pi 5. The framework was\ntested on both an x86-based device and an ARM64-based Raspberry Pi,\ndemonstrating its potential for secure, decentralized logging and measurement\nstorage in smart grid applications. Moreover, DESTinE Block is compared with a\nsimilar framework based on Multichain. The results indicate that DESTinE Block\nprovides a promising solution for tamper-evident data retention in distributed\npower system infrastructure while maintaining minimal hardware requirements."}
{"id": "2510.16610", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16610", "abs": "https://arxiv.org/abs/2510.16610", "authors": ["Bruno Lourenço", "Pedro Adão", "João F. Ferreira", "Mario Monteiro Marques", "Cátia Vaz"], "title": "Structuring Security: A Survey of Cybersecurity Ontologies, Semantic Log Processing, and LLMs Application", "comment": null, "summary": "This survey investigates how ontologies, semantic log processing, and Large\nLanguage Models (LLMs) enhance cybersecurity. Ontologies structure domain\nknowledge, enabling interoperability, data integration, and advanced threat\nanalysis. Security logs, though critical, are often unstructured and complex.\nTo address this, automated construction of Knowledge Graphs (KGs) from raw logs\nis emerging as a key strategy for organizing and reasoning over security data.\nLLMs enrich this process by providing contextual understanding and extracting\ninsights from unstructured content. This work aligns with European Union (EU)\nefforts such as NIS 2 and the Cybersecurity Taxonomy, highlighting challenges\nand opportunities in intelligent ontology-driven cyber defense."}
{"id": "2510.16637", "categories": ["cs.CR", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.16637", "abs": "https://arxiv.org/abs/2510.16637", "authors": ["Alireza Heshmati", "Saman Soleimani Roudi", "Sajjad Amini", "Shahrokh Ghaemmaghami", "Farokh Marvasti"], "title": "A Versatile Framework for Designing Group-Sparse Adversarial Attacks", "comment": null, "summary": "Existing adversarial attacks often neglect perturbation sparsity, limiting\ntheir ability to model structural changes and to explain how deep neural\nnetworks (DNNs) process meaningful input patterns. We propose ATOS (Attack\nThrough Overlapping Sparsity), a differentiable optimization framework that\ngenerates structured, sparse adversarial perturbations in element-wise,\npixel-wise, and group-wise forms. For white-box attacks on image classifiers,\nwe introduce the Overlapping Smoothed L0 (OSL0) function, which promotes\nconvergence to a stationary point while encouraging sparse, structured\nperturbations. By grouping channels and adjacent pixels, ATOS improves\ninterpretability and helps identify robust versus non-robust features. We\napproximate the L-infinity gradient using the logarithm of the sum of\nexponential absolute values to tightly control perturbation magnitude. On\nCIFAR-10 and ImageNet, ATOS achieves a 100% attack success rate while producing\nsignificantly sparser and more structurally coherent perturbations than prior\nmethods. The structured group-wise attack highlights critical regions from the\nnetwork's perspective, providing counterfactual explanations by replacing\nclass-defining regions with robust features from the target class."}
{"id": "2510.16706", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16706", "abs": "https://arxiv.org/abs/2510.16706", "authors": ["Hongjie Zhang", "Zhiqi Zhao", "Hanzhou Wu", "Zhihua Xia", "Athanasios V. Vasilakos"], "title": "Rotation, Scale, and Translation Resilient Black-box Fingerprinting for Intellectual Property Protection of EaaS Models", "comment": null, "summary": "Feature embedding has become a cornerstone technology for processing\nhigh-dimensional and complex data, which results in that Embedding as a Service\n(EaaS) models have been widely deployed in the cloud. To protect the\nintellectual property of EaaS models, existing methods apply digital\nwatermarking to inject specific backdoor triggers into EaaS models by modifying\ntraining samples or network parameters. However, these methods inevitably\nproduce detectable patterns through semantic analysis and exhibit\nsusceptibility to geometric transformations including rotation, scaling, and\ntranslation (RST). To address this problem, we propose a fingerprinting\nframework for EaaS models, rather than merely refining existing watermarking\ntechniques. Different from watermarking techniques, the proposed method\nestablishes EaaS model ownership through geometric analysis of embedding\nspace's topological structure, rather than relying on the modified training\nsamples or triggers. The key innovation lies in modeling the victim and\nsuspicious embeddings as point clouds, allowing us to perform robust spatial\nalignment and similarity measurement, which inherently resists RST attacks.\nExperimental results evaluated on visual and textual embedding tasks verify the\nsuperiority and applicability. This research reveals inherent characteristics\nof EaaS models and provides a promising solution for ownership verification of\nEaaS models under the black-box scenario."}
{"id": "2510.16716", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16716", "abs": "https://arxiv.org/abs/2510.16716", "authors": ["Asmita Mohanty", "Gezheng Kang", "Lei Gao", "Murali Annavaram"], "title": "DistilLock: Safeguarding LLMs from Unauthorized Knowledge Distillation on the Edge", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong performance across\ndiverse tasks, but fine-tuning them typically relies on cloud-based,\ncentralized infrastructures. This requires data owners to upload potentially\nsensitive data to external servers, raising serious privacy concerns. An\nalternative approach is to fine-tune LLMs directly on edge devices using local\ndata; however, this introduces a new challenge: the model owner must transfer\nproprietary models to the edge, which risks intellectual property (IP) leakage.\nTo address this dilemma, we propose DistilLock, a TEE-assisted fine-tuning\nframework that enables privacy-preserving knowledge distillation on the edge.\nIn DistilLock, a proprietary foundation model is executed within a trusted\nexecution environment (TEE) enclave on the data owner's device, acting as a\nsecure black-box teacher. This setup preserves both data privacy and model IP\nby preventing direct access to model internals. Furthermore, DistilLock employs\na model obfuscation mechanism to offload obfuscated weights to untrusted\naccelerators for efficient knowledge distillation without compromising\nsecurity. We demonstrate that DistilLock prevents unauthorized knowledge\ndistillation processes and model-stealing attacks while maintaining high\ncomputational efficiency, but offering a secure and practical solution for\nedge-based LLM personalization."}
{"id": "2510.16744", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16744", "abs": "https://arxiv.org/abs/2510.16744", "authors": ["Srinivas Vivek"], "title": "Cryptanalysis of a Privacy-Preserving Ride-Hailing Service from NSS 2022", "comment": "9 pages", "summary": "Ride-Hailing Services (RHS) match a ride request initiated by a rider with a\nsuitable driver responding to the ride request. A Privacy-Preserving RHS\n(PP-RHS) aims to facilitate ride matching while ensuring the privacy of riders'\nand drivers' location data w.r.t. the Service Provider (SP). At NSS 2022, Xie\net al. proposed a PP-RHS. In this work, we demonstrate a passive attack on\ntheir PP-RHS protocol. Our attack allows the SP to completely recover the\nlocations of the rider as well as that of the responding drivers in every ride\nrequest. Further, our attack is very efficient as it is independent of the\nsecurity parameter."}
{"id": "2510.16794", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16794", "abs": "https://arxiv.org/abs/2510.16794", "authors": ["Jie Zhang", "Meng Ding", "Yang Liu", "Jue Hong", "Florian Tramèr"], "title": "Black-box Optimization of LLM Outputs by Asking for Directions", "comment": null, "summary": "We present a novel approach for attacking black-box large language models\n(LLMs) by exploiting their ability to express confidence in natural language.\nExisting black-box attacks require either access to continuous model outputs\nlike logits or confidence scores (which are rarely available in practice), or\nrely on proxy signals from other models. Instead, we demonstrate how to prompt\nLLMs to express their internal confidence in a way that is sufficiently\ncalibrated to enable effective adversarial optimization. We apply our general\nmethod to three attack scenarios: adversarial examples for vision-LLMs,\njailbreaks and prompt injections. Our attacks successfully generate malicious\ninputs against systems that only expose textual outputs, thereby dramatically\nexpanding the attack surface for deployed LLMs. We further find that better and\nlarger models exhibit superior calibration when expressing confidence, creating\na concerning security paradox where model capability improvements directly\nenhance vulnerability. Our code is available at this\n[link](https://github.com/zj-jayzhang/black_box_llm_optimization)."}
{"id": "2510.16830", "categories": ["cs.CR", "cs.CL", "68T07, 94A60, 68Q25", "I.2.6; G.1.6; E.3; C.2.4"], "pdf": "https://arxiv.org/pdf/2510.16830", "abs": "https://arxiv.org/abs/2510.16830", "authors": ["Hasan Akgul", "Daniel Borg", "Arta Berisha", "Amina Rahimova", "Andrej Novak", "Mila Petrov"], "title": "Verifiable Fine-Tuning for LLMs: Zero-Knowledge Training Proofs Bound to Data Provenance and Policy", "comment": "20 pages, 10 figures", "summary": "Large language models are often adapted through parameter efficient fine\ntuning, but current release practices provide weak assurances about what data\nwere used and how updates were computed. We present Verifiable Fine Tuning, a\nprotocol and system that produces succinct zero knowledge proofs that a\nreleased model was obtained from a public initialization under a declared\ntraining program and an auditable dataset commitment. The approach combines\nfive elements. First, commitments that bind data sources, preprocessing,\nlicenses, and per epoch quota counters to a manifest. Second, a verifiable\nsampler that supports public replayable and private index hiding batch\nselection. Third, update circuits restricted to parameter efficient fine tuning\nthat enforce AdamW style optimizer semantics and proof friendly approximations\nwith explicit error budgets. Fourth, recursive aggregation that folds per step\nproofs into per epoch and end to end certificates with millisecond\nverification. Fifth, provenance binding and optional trusted execution property\ncards that attest code identity and constants. On English and bilingual\ninstruction mixtures, the method maintains utility within tight budgets while\nachieving practical proof performance. Policy quotas are enforced with zero\nviolations, and private sampling windows show no measurable index leakage.\nFederated experiments demonstrate that the system composes with probabilistic\naudits and bandwidth constraints. These results indicate that end to end\nverifiable fine tuning is feasible today for real parameter efficient\npipelines, closing a critical trust gap for regulated and decentralized\ndeployments."}
{"id": "2510.16835", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16835", "abs": "https://arxiv.org/abs/2510.16835", "authors": ["Hongpeng Bai", "Minhong Dong", "Yao Zhang", "Shunzhe Zhao", "Haobo Zhang", "Lingyue Li", "Yude Bai", "Guangquan Xu"], "title": "ThreatIntel-Andro: Expert-Verified Benchmarking for Robust Android Malware Research", "comment": null, "summary": "The rapidly evolving Android malware ecosystem demands high-quality,\nreal-time datasets as a foundation for effective detection and defense. With\nthe widespread adoption of mobile devices across industrial systems, they have\nbecome a critical yet often overlooked attack surface in industrial\ncybersecurity. However, mainstream datasets widely used in academia and\nindustry (e.g., Drebin) exhibit significant limitations: on one hand, their\nheavy reliance on VirusTotal's multi-engine aggregation results introduces\nsubstantial label noise; on the other hand, outdated samples reduce their\ntemporal relevance. Moreover, automated labeling tools (e.g., AVClass2) suffer\nfrom suboptimal aggregation strategies, further compounding labeling errors and\npropagating inaccuracies throughout the research community."}
{"id": "2510.16871", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16871", "abs": "https://arxiv.org/abs/2510.16871", "authors": ["Anirban Chakraborty", "Nimish Mishra", "Sayandeep Saha", "Sarani Bhattacharya", "Debdeep Mukhopadhyay"], "title": "Addendum: Systematic Evaluation of Randomized Cache Designs against Cache Occupancy", "comment": null, "summary": "In the main text published at USENIX Security 2025, we presented a systematic\nanalysis of the role of cache occupancy in the design considerations for\nrandomized caches (from the perspectives of performance and security). On the\nperformance front, we presented a uniform benchmarking strategy that allows for\na fair comparison among different randomized cache designs. Likewise, from the\nsecurity perspective, we presented three threat assumptions: (1) covert\nchannels; (2) process fingerprinting side-channel; and (3) AES key recovery.\nThe main takeaway of our work is an open problem of designing a randomized\ncache of comparable efficiency with modern set-associative LLCs, while still\nresisting both contention-based and occupancy-based attacks. This note is meant\nas an addendum to the main text in light of the observations made in [2]. To\nsummarize, the authors in [2] argue that (1) L1d cache size plays a role in\nadversarial success, and that (2) a patched version of MIRAGE with randomized\ninitial seeding of global eviction map prevents leakage of AES key. We discuss\nthe same in this addendum."}
{"id": "2510.16873", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16873", "abs": "https://arxiv.org/abs/2510.16873", "authors": ["Jacob Leiken", "Sunoo Park"], "title": "On the Credibility of Deniable Communication in Court", "comment": null, "summary": "Over time, cryptographically deniable systems have come to be associated in\ncomputer-science literature with the idea of \"denying\" evidence in court -\nspecifically, with the ability to convincingly forge evidence in courtroom\nscenarios and an inability to authenticate evidence in such contexts.\nEvidentiary processes in courts, however, have been developed over centuries to\naccount for the reality that evidence has always been forgeable, and relies on\nfactors outside of cryptographic models to seek the truth \"as well as possible\"\nwhile acknowledging that all evidence is imperfect. We argue that deniability\ndoes not and need not change this paradigm.\n  Our analysis highlights a gap between technical deniability notions and their\napplication to the real world. There will always be factors outside a\ncryptographic model that influence perceptions of a message's authenticity, in\nrealistic situations. We propose the broader concept of credibility to capture\nthese factors. The credibility of a system is determined by (1) a threshold of\nquality that a forgery must pass to be \"believable\" as an original\ncommunication, which varies based on sociotechnical context and threat model,\n(2) the ease of creating a forgery that passes this threshold, which is also\ncontext- and threat-model-dependent, and (3) default system retention policy\nand retention settings. All three aspects are important for designing secure\ncommunication systems for real-world threat models, and some aspects of (2) and\n(3) may be incorporated directly into technical system design. We hope that our\nmodel of credibility will facilitate system design and deployment that\naddresses threats that are not and cannot be captured by purely technical\ndefinitions and existing cryptographic models, and support more nuanced\ndiscourse on the strengths and limitations of cryptographic guarantees within\nspecific legal and sociotechnical contexts."}
{"id": "2510.16923", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16923", "abs": "https://arxiv.org/abs/2510.16923", "authors": ["Mansi Phute", "Matthew Hull", "Haoran Wang", "Alec Helbling", "ShengYun Peng", "Willian Lunardi", "Martin Andreoni", "Wenke Lee", "Polo Chau"], "title": "UNDREAM: Bridging Differentiable Rendering and Photorealistic Simulation for End-to-end Adversarial Attacks", "comment": null, "summary": "Deep learning models deployed in safety critical applications like autonomous\ndriving use simulations to test their robustness against adversarial attacks in\nrealistic conditions. However, these simulations are non-differentiable,\nforcing researchers to create attacks that do not integrate simulation\nenvironmental factors, reducing attack success. To address this limitation, we\nintroduce UNDREAM, the first software framework that bridges the gap between\nphotorealistic simulators and differentiable renderers to enable end-to-end\noptimization of adversarial perturbations on any 3D objects. UNDREAM enables\nmanipulation of the environment by offering complete control over weather,\nlighting, backgrounds, camera angles, trajectories, and realistic human and\nobject movements, thereby allowing the creation of diverse scenes. We showcase\na wide array of distinct physically plausible adversarial objects that UNDREAM\nenables researchers to swiftly explore in different configurable environments.\nThis combination of photorealistic simulation and differentiable optimization\nopens new avenues for advancing research of physical adversarial attacks."}
{"id": "2510.16959", "categories": ["cs.CR", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.16959", "abs": "https://arxiv.org/abs/2510.16959", "authors": ["Surendra Ghentiyala"], "title": "Efficient derandomization of differentially private counting queries", "comment": "Accepted to SOSA'26", "summary": "Differential privacy for the 2020 census required an estimated 90 terabytes\nof randomness [GL20], an amount which may be prohibitively expensive or\nentirely infeasible to generate. Motivated by these practical concerns, [CSV25]\ninitiated the study of the randomness complexity of differential privacy, and\nin particular, the randomness complexity of $d$ counting queries. This is the\ntask of outputting the number of entries in a dataset that satisfy predicates\n$\\mathcal{P}_1, \\dots, \\mathcal{P}_d$ respectively. They showed the rather\nsurprising fact that though any reasonably accurate,\n$\\varepsilon$-differentially private mechanism for one counting query requires\n$1-O(\\varepsilon)$ bits of randomness in expectation, there exists a fairly\naccurate mechanism for $d$ counting queries which requires only $O(\\log d)$\nbits of randomness in expectation.\n  The mechanism of [CSV25] is inefficient (not polynomial time) and relies on a\ncombinatorial object known as rounding schemes. Here, we give a polynomial time\nmechanism which achieves nearly the same randomness complexity versus accuracy\ntradeoff as that of [CSV25]. Our construction is based on the following simple\nobservation: after a randomized shift of the answer to each counting query, the\nanswer to many counting queries remains the same regardless of whether we add\nnoise to that coordinate or not. This allows us to forgo the step of adding\nnoise to the result of many counting queries. Our mechanism does not make use\nof rounding schemes. Therefore, it provides a different -- and, in our opinion,\nclearer -- insight into the origins of the randomness savings that can be\nobtained by batching $d$ counting queries. Therefore, it provides a different\n-- and, in our opinion, clearer -- insight into the origins of the randomness\nsavings that can be obtained by batching $d$ counting queries."}
{"id": "2510.17000", "categories": ["cs.CR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17000", "abs": "https://arxiv.org/abs/2510.17000", "authors": ["Masahiro Kaneko", "Timothy Baldwin"], "title": "Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs", "comment": "NeurIPS 2025 (spotlight)", "summary": "Adversarial attacks by malicious users that threaten the safety of large\nlanguage models (LLMs) can be viewed as attempts to infer a target property $T$\nthat is unknown when an instruction is issued, and becomes knowable only after\nthe model's reply is observed. Examples of target properties $T$ include the\nbinary flag that triggers an LLM's harmful response or rejection, and the\ndegree to which information deleted by unlearning can be restored, both\nelicited via adversarial instructions. The LLM reveals an \\emph{observable\nsignal} $Z$ that potentially leaks hints for attacking through a response\ncontaining answer tokens, thinking process tokens, or logits. Yet the scale of\ninformation leaked remains anecdotal, leaving auditors without principled\nguidance and defenders blind to the transparency--risk trade-off. We fill this\ngap with an information-theoretic framework that computes how much information\ncan be safely disclosed, and enables auditors to gauge how close their methods\ncome to the fundamental limit. Treating the mutual information $I(Z;T)$ between\nthe observation $Z$ and the target property $T$ as the leaked bits per query,\nwe show that achieving error $\\varepsilon$ requires at least\n$\\log(1/\\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak\nrate and only logarithmically with the desired accuracy. Thus, even a modest\nincrease in disclosure collapses the attack cost from quadratic to logarithmic\nin terms of the desired accuracy. Experiments on seven LLMs across\nsystem-prompt leakage, jailbreak, and relearning attacks corroborate the\ntheory: exposing answer tokens alone requires about a thousand queries; adding\nlogits cuts this to about a hundred; and revealing the full thinking process\ntrims it to a few dozen. Our results provide the first principled yardstick for\nbalancing transparency and security when deploying LLMs."}
{"id": "2510.17033", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.17033", "abs": "https://arxiv.org/abs/2510.17033", "authors": ["Leixu Huang", "Zedian Shao", "Teodora Baluta"], "title": "Watermark Robustness and Radioactivity May Be at Odds in Federated Learning", "comment": "9 pages, 4 figures (not including citation and appendix) submitted to\n  ICLR 2026", "summary": "Federated learning (FL) enables fine-tuning large language models (LLMs)\nacross distributed data sources. As these sources increasingly include\nLLM-generated text, provenance tracking becomes essential for accountability\nand transparency. We adapt LLM watermarking for data provenance in FL where a\nsubset of clients compute local updates on watermarked data, and the server\naverages all updates into the global LLM. In this setup, watermarks are\nradioactive: the watermark signal remains detectable after fine-tuning with\nhigh confidence. The $p$-value can reach $10^{-24}$ even when as little as\n$6.6\\%$ of data is watermarked. However, the server can act as an active\nadversary that wants to preserve model utility while evading provenance\ntracking. Our observation is that updates induced by watermarked synthetic data\nappear as outliers relative to non-watermark updates. Our adversary thus\napplies strong robust aggregation that can filter these outliers, together with\nthe watermark signal. All evaluated radioactive watermarks are not robust\nagainst such an active filtering server. Our work suggests fundamental\ntrade-offs between radioactivity, robustness, and utility."}
{"id": "2510.17087", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17087", "abs": "https://arxiv.org/abs/2510.17087", "authors": ["Ziqing Zhu"], "title": "Quantum Key Distribution for Virtual Power Plant Communication: A Lightweight Key-Aware Scheduler with Provable Stability", "comment": null, "summary": "Virtual power plants (VPPs) are becoming a cornerstone of future grids,\naggregating distributed PV, wind, storage, and flexible loads for market\nparticipation and real-time balancing. As operations move to minute-- and\nsecond--level feedback, communication security shifts from a compliance item to\nan operational constraint: latency, reliability, and confidentiality jointly\ndetermine whether dispatch, protection, and settlement signals arrive on time.\nConventional PKI and key-rotation schemes struggle with cross-domain,\nhigh-frequency messaging and face long-term quantum threats. Quantum key\ndistribution (QKD) offers information-theoretic key freshness, but its key\nyield is scarce and stochastic, often misaligned with bursty VPP traffic. This\npaper proposes a key-aware priority and quota framework that treats quantum\nkeys as first-class scheduling resources. The design combines (i)\nforecast-driven long-term quotas and short-term tokens, (ii) key-aware\ndeficit-round-robin arbitration, (iii) a preemptive emergency key reserve, and\n(iv) graceful degradation via encryption-mode switching and controlled\ndown-sampling for non-critical traffic. A drift-plus-penalty analysis\nestablishes strong stability under average supply--demand balance with\nquantifiable bounds on backlog and tail latency, providing interpretable\noperating guarantees. We build a reproducible testbed on IEEE 33- and 123-bus\nVPP systems and evaluate normal, degraded, and outage regimes with\nindustry-consistent message classes and TTLs. Against FIFO, fixed-priority, and\nstatic-quota baselines, the proposed scheme consistently reduces tail delay and\npassive timeouts for critical messages, improves per-bit key utility, and\nenhances power-tracking reliability during key scarcity and regime switches."}
{"id": "2510.17098", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17098", "abs": "https://arxiv.org/abs/2510.17098", "authors": ["Elias Hossain", "Swayamjit Saha", "Somshubhra Roy", "Ravi Prasad"], "title": "Can Transformer Memory Be Corrupted? Investigating Cache-Side Vulnerabilities in Large Language Models", "comment": null, "summary": "Even when prompts and parameters are secured, transformer language models\nremain vulnerable because their key-value (KV) cache during inference\nconstitutes an overlooked attack surface. This paper introduces Malicious Token\nInjection (MTI), a modular framework that systematically perturbs cached key\nvectors at selected layers and timesteps through controlled magnitude and\nfrequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A\ntheoretical analysis quantifies how these perturbations propagate through\nattention, linking logit deviations to the Frobenius norm of corruption and\nsoftmax Lipschitz dynamics. Empirical results show that MTI significantly\nalters next-token distributions and downstream task performance across GPT-2\nand LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic\nreasoning pipelines. These findings identify cache integrity as a critical yet\nunderexplored vulnerability in current LLM deployments, positioning cache\ncorruption as a reproducible and theoretically grounded threat model for future\nrobustness and security research."}
{"id": "2510.17175", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17175", "abs": "https://arxiv.org/abs/2510.17175", "authors": ["Muhammad Wahid Akram", "Keshav Sood", "Muneeb Ul Hassan"], "title": "QRïS: A Preemptive Novel Method for Quishing Detection Through Structural Features of QR", "comment": "13 pages, 11 figures, and 7 tables", "summary": "Globally, individuals and organizations employ Quick Response (QR) codes for\nswift and convenient communication. Leveraging this, cybercriminals embed\nfalsify and misleading information in QR codes to launch various phishing\nattacks which termed as Quishing. Many former studies have introduced defensive\napproaches to preclude Quishing such as by classifying the embedded content of\nQR codes and then label the QR codes accordingly, whereas other studies\nclassify them using visual features (i.e., deep features, histogram density\nanalysis features). However, these approaches mainly rely on black-box\ntechniques which do not clearly provide interpretability and transparency to\nfully comprehend and reproduce the intrinsic decision process; therefore,\nhaving certain obvious limitations includes the approaches' trust,\naccountability, issues in bias detection, and many more. We proposed QR\\\"iS,\nthe pioneer method to classify QR codes through the comprehensive structural\nanalysis of a QR code which helps to identify phishing QR codes beforehand. Our\nclassification method is clearly transparent which makes it reproducible,\nscalable, and easy to comprehend. First, we generated QR codes dataset (i.e.\n400,000 samples) using recently published URLs datasets [1], [2]. Then, unlike\nblack-box models, we developed a simple algorithm to extract 24 structural\nfeatures from layout patterns present in QR codes. Later, we train the machine\nlearning models on the harvested features and obtained accuracy of up to\n83.18%. To further evaluate the effectiveness of our approach, we perform the\ncomparative analysis of proposed method with relevant contemporary studies.\nLastly, for real-world deployment and validation, we developed a mobile app\nwhich assures the feasibility of the proposed solution in real-world scenarios\nwhich eventually strengthen the applicability of the study."}
{"id": "2510.17220", "categories": ["cs.CR", "cs.LO", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.17220", "abs": "https://arxiv.org/abs/2510.17220", "authors": ["Giulia Giusti"], "title": "Exploiting the Potential of Linearity in Automatic Differentiation and Computational Cryptography", "comment": null, "summary": "The concept of linearity plays a central role in both mathematics and\ncomputer science, with distinct yet complementary meanings. In mathematics,\nlinearity underpins functions and vector spaces, forming the foundation of\nlinear algebra and functional analysis. In computer science, it relates to\nresource-sensitive computation. Linear Logic (LL), for instance, models\nassumptions that must be used exactly once, providing a natural framework for\ntracking computational resources such as time, memory, or data access. This\ndual perspective makes linearity essential to programming languages, type\nsystems, and formal models that express both computational complexity and\ncomposability. Bridging these interpretations enables rigorous yet practical\nmethodologies for analyzing and verifying complex systems.\n  This thesis explores the use of LL to model programming paradigms based on\nlinearity. It comprises two parts: ADLL and CryptoBLL. The former applies LL to\nAutomatic Differentiation (AD), modeling linear functions over the reals and\nthe transposition operation. The latter uses LL to express complexity\nconstraints on adversaries in computational cryptography.\n  In AD, two main approaches use linear type systems: a theoretical one\ngrounded in proof theory, and a practical one implemented in JAX, a Python\nlibrary developed by Google for machine learning research. In contrast,\nframeworks like PyTorch and TensorFlow support AD without linear types. ADLL\naims to bridge theory and practice by connecting JAX's type system to LL.\n  In modern cryptography, several calculi aim to model cryptographic proofs\nwithin the computational paradigm. These efforts face a trade-off between\nexpressiveness, to capture reductions, and simplicity, to abstract probability\nand complexity. CryptoBLL addresses this tension by proposing a framework for\nthe automatic analysis of protocols in computational cryptography."}
{"id": "2510.17277", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.17277", "abs": "https://arxiv.org/abs/2510.17277", "authors": ["Xinkai Wang", "Beibei Li", "Zerui Shao", "Ao Liu", "Shouling Ji"], "title": "Multimodal Safety Is Asymmetric: Cross-Modal Exploits Unlock Black-Box MLLMs Jailbreaks", "comment": null, "summary": "Multimodal large language models (MLLMs) have demonstrated significant\nutility across diverse real-world applications. But MLLMs remain vulnerable to\njailbreaks, where adversarial inputs can collapse their safety constraints and\ntrigger unethical responses. In this work, we investigate jailbreaks in the\ntext-vision multimodal setting and pioneer the observation that visual\nalignment imposes uneven safety constraints across modalities in MLLMs, thereby\ngiving rise to multimodal safety asymmetry. We then develop PolyJailbreak, a\nblack-box jailbreak method grounded in reinforcement learning. Initially, we\nprobe the model's attention dynamics and latent representation space, assessing\nhow visual inputs reshape cross-modal information flow and diminish the model's\nability to separate harmful from benign inputs, thereby exposing exploitable\nvulnerabilities. On this basis, we systematize them into generalizable and\nreusable operational rules that constitute a structured library of Atomic\nStrategy Primitives, which translate harmful intents into jailbreak inputs\nthrough step-wise transformations. Guided by the primitives, PolyJailbreak\nemploys a multi-agent optimization process that automatically adapts inputs\nagainst the target models. We conduct comprehensive evaluations on a variety of\nopen-source and closed-source MLLMs, demonstrating that PolyJailbreak\noutperforms state-of-the-art baselines."}
{"id": "2510.17284", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.17284", "abs": "https://arxiv.org/abs/2510.17284", "authors": ["Jiri Gavenda", "Petr Svenda", "Stanislav Bobon", "Vladimir Sedlacek"], "title": "Analysis of Input-Output Mappings in Coinjoin Transactions with Arbitrary Values", "comment": null, "summary": "A coinjoin protocol aims to increase transactional privacy for Bitcoin and\nBitcoin-like blockchains via collaborative transactions, by violating\nassumptions behind common analysis heuristics. Estimating the resulting privacy\ngain is a crucial yet unsolved problem due to a range of influencing factors\nand large computational complexity.\n  We adapt the BlockSci on-chain analysis software to coinjoin transactions,\ndemonstrating a significant (10-50%) average post-mix anonymity set size\ndecrease for all three major designs with a central coordinator: Whirlpool,\nWasabi 1.x, and Wasabi 2.x. The decrease is highest during the first day and\nnegligible after one year from a coinjoin creation.\n  Moreover, we design a precise, parallelizable privacy estimation method,\nwhich takes into account coinjoin fees, implementation-specific limitations and\nusers' post-mix behavior. We evaluate our method in detail on a set of emulated\nand real-world Wasabi 2.x coinjoins and extrapolate to its largest real-world\ncoinjoins with hundreds of inputs and outputs. We conclude that despite the\nusers' undesirable post-mix behavior, correctly attributing the coins to their\nowners is still very difficult, even with our improved analysis algorithm."}
{"id": "2510.17308", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.17308", "abs": "https://arxiv.org/abs/2510.17308", "authors": ["Reo Eriguchi", "Kazumasa Shinagawa"], "title": "Single-Shuffle Full-Open Card-Based Protocols for Any Function", "comment": null, "summary": "A card-based secure computation protocol is a method for $n$ parties to\ncompute a function $f$ on their private inputs $(x_1,\\ldots,x_n)$ using\nphysical playing cards, in such a way that the suits of revealed cards leak no\ninformation beyond the value of $f(x_1,\\ldots,x_n)$. A \\textit{single-shuffle\nfull-open} protocol is a minimal model of card-based secure computation in\nwhich, after the parties place face-down cards representing their inputs, a\nsingle shuffle operation is performed and then all cards are opened to derive\nthe output. Despite the simplicity of this model, the class of functions known\nto admit single-shuffle full-open protocols has been limited to a few small\nexamples. In this work, we prove for the first time that every function admits\na single-shuffle full-open protocol. We present two constructions that offer a\ntrade-off between the number of cards and the complexity of the shuffle\noperation. These feasibility results are derived from a novel connection\nbetween single-shuffle full-open protocols and a cryptographic primitive known\nas \\textit{Private Simultaneous Messages} protocols, which has rarely been\nstudied in the context of card-based cryptography. We also present variants of\nsingle-shuffle protocols in which only a subset of cards are revealed. These\nprotocols reduce the complexity of the shuffle operation compared to existing\nprotocols in the same setting."}
{"id": "2510.17311", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.17311", "abs": "https://arxiv.org/abs/2510.17311", "authors": ["Eduard Marin", "Jinwoo Kim", "Alessio Pavoni", "Mauro Conti", "Roberto Di Pietro"], "title": "The Hidden Dangers of Public Serverless Repositories: An Empirical Security Assessment", "comment": "Accepted at ESORICS 2025", "summary": "Serverless computing has rapidly emerged as a prominent cloud paradigm,\nenabling developers to focus solely on application logic without the burden of\nmanaging servers or underlying infrastructure. Public serverless repositories\nhave become key to accelerating the development of serverless applications.\nHowever, their growing popularity makes them attractive targets for\nadversaries. Despite this, the security posture of these repositories remains\nlargely unexplored, exposing developers and organizations to potential risks.\nIn this paper, we present the first comprehensive analysis of the security\nlandscape of serverless components hosted in public repositories. We analyse\n2,758 serverless components from five widely used public repositories popular\namong developers and enterprises, and 125,936 Infrastructure as Code (IaC)\ntemplates across three widely used IaC frameworks. Our analysis reveals\nsystemic vulnerabilities including outdated software packages, misuse of\nsensitive parameters, exploitable deployment configurations, susceptibility to\ntypo-squatting attacks and opportunities to embed malicious behaviour within\ncompressed serverless components. Finally, we provide practical recommendations\nto mitigate these threats."}
{"id": "2510.17403", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17403", "abs": "https://arxiv.org/abs/2510.17403", "authors": ["Stella N. Arinze", "Patrick U. Okafor", "Onyekachi M. Egwuagu", "Augustine O. Nwajana"], "title": "Process Automation Architecture Using RFID for Transparent Voting Systems", "comment": "7 pages, 5 figures, 1 table", "summary": "This paper presents the development of a process automation architecture\nleveraging Radio Frequency Identification (RFID) technology for secure,\ntransparent and efficient voting systems. The proposed architecture automates\nthe voting workflow through RFID-enabled voter identification, encrypted vote\ncasting, and secure data transmission. Each eligible voter receives a smart\nRFID card containing a uniquely encrypted identifier, which is verified using\nan RC522 reader interfaced with a microcontroller. Upon successful\nverification, the voter interacts with a touchscreen interface to cast a vote,\nwhich is then encrypted using AES-128 and securely stored on a local SD card or\ntransmitted via GSM to a central server. A tamper-proof monitoring mechanism\nrecords each session with time-stamped digital signatures, ensuring\nauditability and data integrity. The architecture is designed to function in\nboth online and offline modes, with an automated batch synchronization\nmechanism that updates vote records once network connectivity is restored.\nSystem testing in simulated environments confirmed 100% voter authentication\naccuracy, minimized latency (average voting time of 11.5 seconds), and\nrobustness against cloning, double voting, and data interception. The\nintegration of real-time monitoring and secure process control modules enables\nelectoral authorities to automate data logging, detect anomalies, and validate\nsystem integrity dynamically. This work demonstrates a scalable,\nautomation-driven solution for voting infrastructure, offering enhanced\ntransparency, resilience, and deployment flexibility, especially in\nenvironments where digital transformation of electoral processes is critically\nneeded."}
{"id": "2510.17521", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.17521", "abs": "https://arxiv.org/abs/2510.17521", "authors": ["Francesco Balassone", "Víctor Mayoral-Vilches", "Stefan Rass", "Martin Pinzger", "Gaetano Perrone", "Simon Pietro Romano", "Peter Schartner"], "title": "Cybersecurity AI: Evaluating Agentic Cybersecurity in Attack/Defense CTFs", "comment": null, "summary": "We empirically evaluate whether AI systems are more effective at attacking or\ndefending in cybersecurity. Using CAI (Cybersecurity AI)'s parallel execution\nframework, we deployed autonomous agents in 23 Attack/Defense CTF\nbattlegrounds. Statistical analysis reveals defensive agents achieve 54.3%\nunconstrained patching success versus 28.3% offensive initial access\n(p=0.0193), but this advantage disappears under operational constraints: when\ndefense requires maintaining availability (23.9%) and preventing all intrusions\n(15.2%), no significant difference exists (p>0.05). Exploratory taxonomy\nanalysis suggests potential patterns in vulnerability exploitation, though\nlimited sample sizes preclude definitive conclusions. This study provides the\nfirst controlled empirical evidence challenging claims of AI attacker\nadvantage, demonstrating that defensive effectiveness critically depends on\nsuccess criteria, a nuance absent from conceptual analyses but essential for\ndeployment. These findings underscore the urgency for defenders to adopt\nopen-source Cybersecurity AI frameworks to maintain security equilibrium\nagainst accelerating offensive automation."}
{"id": "2510.17552", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.17552", "abs": "https://arxiv.org/abs/2510.17552", "authors": ["Persefoni Konteli", "Nikolaos Makris", "Evgenia Niovi Sassalou", "Stylianos A. Kazazis", "Alkinoos Papageorgopoulos", "Stefanos Vasileiadis", "Konstantinos Tsimvrakidis", "Symeon Tsintzos", "Georgios M. Nikolopoulos", "George T. Kanellos"], "title": "Dynamic Switched Quantum Key Distribution Networkwith PUF-based authentication", "comment": null, "summary": "We demonstrate a centrally controlled dynamic switched-QKD network,\nwithintegrated PUF-based dynamic authentication for each QKD link. The\nperformance of the dynamicswitched-QKD network with real-time PUF-based\nauthentication is analyzed."}
{"id": "2510.17621", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17621", "abs": "https://arxiv.org/abs/2510.17621", "authors": ["Vincenzo Carletti", "Pasquale Foggia", "Carlo Mazzocca", "Giuseppe Parrella", "Mario Vento"], "title": "GUIDE: Enhancing Gradient Inversion Attacks in Federated Learning with Denoising Models", "comment": null, "summary": "Federated Learning (FL) enables collaborative training of Machine Learning\n(ML) models across multiple clients while preserving their privacy. Rather than\nsharing raw data, federated clients transmit locally computed updates to train\nthe global model. Although this paradigm should provide stronger privacy\nguarantees than centralized ML, client updates remain vulnerable to privacy\nleakage. Adversaries can exploit them to infer sensitive properties about the\ntraining data or even to reconstruct the original inputs via Gradient Inversion\nAttacks (GIAs). Under the honest-butcurious threat model, GIAs attempt to\nreconstruct training data by reversing intermediate updates using\noptimizationbased techniques. We observe that these approaches usually\nreconstruct noisy approximations of the original inputs, whose quality can be\nenhanced with specialized denoising models. This paper presents Gradient Update\nInversion with DEnoising (GUIDE), a novel methodology that leverages diffusion\nmodels as denoising tools to improve image reconstruction attacks in FL. GUIDE\ncan be integrated into any GIAs that exploits surrogate datasets, a widely\nadopted assumption in GIAs literature. We comprehensively evaluate our approach\nin two attack scenarios that use different FL algorithms, models, and datasets.\nOur results demonstrate that GUIDE integrates seamlessly with two state-ofthe-\nart GIAs, substantially improving reconstruction quality across multiple\nmetrics. Specifically, GUIDE achieves up to 46% higher perceptual similarity,\nas measured by the DreamSim metric."}
{"id": "2510.17687", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17687", "abs": "https://arxiv.org/abs/2510.17687", "authors": ["Xu Zhang", "Hao Li", "Zhichao Lu"], "title": "CrossGuard: Safeguarding MLLMs against Joint-Modal Implicit Malicious Attacks", "comment": "14 pages, 8 figures, 2 tables", "summary": "Multimodal Large Language Models (MLLMs) achieve strong reasoning and\nperception capabilities but are increasingly vulnerable to jailbreak attacks.\nWhile existing work focuses on explicit attacks, where malicious content\nresides in a single modality, recent studies reveal implicit attacks, in which\nbenign text and image inputs jointly express unsafe intent. Such joint-modal\nthreats are difficult to detect and remain underexplored, largely due to the\nscarcity of high-quality implicit data. We propose ImpForge, an automated\nred-teaming pipeline that leverages reinforcement learning with tailored reward\nmodules to generate diverse implicit samples across 14 domains. Building on\nthis dataset, we further develop CrossGuard, an intent-aware safeguard\nproviding robust and comprehensive defense against both explicit and implicit\nthreats. Extensive experiments across safe and unsafe benchmarks, implicit and\nexplicit attacks, and multiple out-of-domain settings demonstrate that\nCrossGuard significantly outperforms existing defenses, including advanced\nMLLMs and guardrails, achieving stronger security while maintaining high\nutility. This offers a balanced and practical solution for enhancing MLLM\nrobustness against real-world multimodal threats."}
{"id": "2510.17759", "categories": ["cs.CR", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17759", "abs": "https://arxiv.org/abs/2510.17759", "authors": ["Qilin Liao", "Anamika Lochab", "Ruqi Zhang"], "title": "VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models", "comment": "18 pages, 7 Figures,", "summary": "Vision-Language Models (VLMs) extend large language models with visual\nreasoning, but their multimodal design also introduces new, underexplored\nvulnerabilities. Existing multimodal red-teaming methods largely rely on\nbrittle templates, focus on single-attack settings, and expose only a narrow\nsubset of vulnerabilities. To address these limitations, we introduce VERA-V, a\nvariational inference framework that recasts multimodal jailbreak discovery as\nlearning a joint posterior distribution over paired text-image prompts. This\nprobabilistic view enables the generation of stealthy, coupled adversarial\ninputs that bypass model guardrails. We train a lightweight attacker to\napproximate the posterior, allowing efficient sampling of diverse jailbreaks\nand providing distributional insights into vulnerabilities. VERA-V further\nintegrates three complementary strategies: (i) typography-based text prompts\nthat embed harmful cues, (ii) diffusion-based image synthesis that introduces\nadversarial signals, and (iii) structured distractors to fragment VLM\nattention. Experiments on HarmBench and HADES benchmarks show that VERA-V\nconsistently outperforms state-of-the-art baselines on both open-source and\nfrontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the\nbest baseline on GPT-4o."}
{"id": "2510.16823", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16823", "abs": "https://arxiv.org/abs/2510.16823", "authors": ["Yue Liu", "Zhenchang Xing", "Shidong Pan", "Chakkrit Tantithamthavorn"], "title": "When AI Takes the Wheel: Security Analysis of Framework-Constrained Program Generation", "comment": null, "summary": "In recent years, the AI wave has grown rapidly in software development. Even\nnovice developers can now design and generate complex framework-constrained\nsoftware systems based on their high-level requirements with the help of Large\nLanguage Models (LLMs). However, when LLMs gradually \"take the wheel\" of\nsoftware development, developers may only check whether the program works. They\noften miss security problems hidden in how the generated programs are\nimplemented.\n  In this work, we investigate the security properties of framework-constrained\nprograms generated by state-of-the-art LLMs. We focus specifically on Chrome\nextensions due to their complex security model involving multiple privilege\nboundaries and isolated components. To achieve this, we built ChromeSecBench, a\ndataset with 140 prompts based on known vulnerable extensions. We used these\nprompts to instruct nine state-of-the-art LLMs to generate complete Chrome\nextensions, and then analyzed them for vulnerabilities across three dimensions:\nscenario types, model differences, and vulnerability categories. Our results\nshow that LLMs produced vulnerable programs at alarmingly high rates (18%-50%),\nparticularly in Authentication & Identity and Cookie Management scenarios (up\nto 83% and 78% respectively). Most vulnerabilities exposed sensitive browser\ndata like cookies, history, or bookmarks to untrusted code. Interestingly, we\nfound that advanced reasoning models performed worse, generating more\nvulnerabilities than simpler models. These findings highlight a critical gap\nbetween LLMs' coding skills and their ability to write secure\nframework-constrained programs."}
