{"id": "2507.15887", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15887", "abs": "https://arxiv.org/abs/2507.15887", "authors": ["Ori Press", "Brandon Amos", "Haoyu Zhao", "Yikai Wu", "Samuel K. Ainsworth", "Dominik Krupke", "Patrick Kidger", "Touqir Sajed", "Bartolomeo Stellato", "Jisun Park", "Nathanael Bosch", "Eli Meril", "Albert Steppi", "Arman Zharmagambetov", "Fangzhao Zhang", "David Perez-Pineiro", "Alberto Mercurio", "Ni Zhan", "Talor Abramovich", "Kilian Lieret", "Hanlin Zhang", "Shirley Huang", "Matthias Bethge", "Ofir Press"], "title": "AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?", "comment": null, "summary": "Despite progress in language model (LM) capabilities, evaluations have thus\nfar focused on models' performance on tasks that humans have previously solved,\nincluding in programming (Jimenez et al., 2024) and mathematics (Glazer et al.,\n2024). We therefore propose testing models' ability to design and implement\nalgorithms in an open-ended benchmark: We task LMs with writing code that\nefficiently solves computationally challenging problems in computer science,\nphysics, and mathematics. Our AlgoTune benchmark consists of 155 coding tasks\ncollected from domain experts and a framework for validating and timing\nLM-synthesized solution code, which is compared to reference implementations\nfrom popular open-source packages. In addition, we develop a baseline LM agent,\nAlgoTuner, and evaluate its performance across a suite of frontier models.\nAlgoTuner achieves an average 1.72x speedup against our reference solvers,\nwhich use libraries such as SciPy, sk-learn and CVXPY. However, we find that\ncurrent models fail to discover algorithmic innovations, instead preferring\nsurface-level optimizations. We hope that AlgoTune catalyzes the development of\nLM agents exhibiting creative problem solving beyond state-of-the-art human\nperformance.", "AI": {"tldr": "AlgoTune introduces a novel open-ended benchmark to evaluate language models' ability to design and implement algorithms across computer science, physics, and mathematics. While achieving 1.72x speedups over reference solvers using libraries like SciPy, models currently lack the capacity for algorithmic innovation, focusing instead on superficial optimizations.", "motivation": "Previous LM evaluations focused only on solving pre-existing human-defined tasks, not creating algorithmic solutions from scratch. This work seeks to measure true algorithmic creativity and reasoning in computationally challenging domains.", "method": "1) Developed 155 expert-curated coding tasks for AlgoTune benchmark\n2) Created framework to validate correctness and benchmark efficiency of LM-generated code\n3) Compared solutions against reference implementations from established libraries\n4) Evaluated performance using baseline AlgoTuner agent and state-of-the-art models", "result": "Achieved 1.72x average speedup over reference optimizers.\nIdentified limitations in deep algorithmic innovation - models prioritized surface-level code optimizations over discovering novel algorithms.\nRevealed performance gaps between model capabilities and breakthrough algorithm design.", "conclusion": "AlgoTune demonstrates viable methodology to assess algorithmic creativity while exposing key limitations in current language models. The benchmark provides targeted avenues for developing LMs that could eventually realize novel algorithmic contributions beyond human precedent."}}
{"id": "2507.15889", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15889", "abs": "https://arxiv.org/abs/2507.15889", "authors": ["Noah van der Vleuten"], "title": "Dr. Boot: Bootstrapping Program Synthesis Language Models to Perform Repairing", "comment": "Master's thesis, University of Amsterdam, 2023\n  (https://scripties.uba.uva.nl/search?id=record_54126). Code and experiments\n  available at: https://github.com/NoahVl/Dr-Boot", "summary": "Language models for program synthesis are usually trained and evaluated on\nprogramming competition datasets (MBPP, APPS). However, these datasets are\nlimited in size and quality, while these language models are extremely data\nhungry. Additionally, the language models have a misaligned program synthesis\nprocess compared to humans. While humans iteratively develop code with the help\nof a compiler, most program synthesis models currently produce code in one go.\nTo solve these issues, we introduce a bootstrapping algorithm for program\nsynthesis, that supports teaching models how to repair. We show that\nbootstrapping consistently outperforms regular fine-tuning. Compared to other\nwork, our bootstrapped model performs on par with fine-tuned models that are\n68\\% larger. Notably, bootstrapping with repairing also improves non-repairing\nperformance compared to regular bootstrapping during inference. However, on our\nmodels, repairing during inference is likely inferior to simply sampling the\nsame number of solutions. Furthermore, we find that there are issues with the\nexample test cases in the training portion of the APPS dataset that are\nvaluable to the community, as many repairing and reinforcement learning methods\nrely on them.", "AI": {"tldr": "The paper introduces a bootstrapping algorithm for program synthesis that teaches models to repair code iteratively, outperforming regular fine-tuning and matching results from 68% larger models.", "motivation": "Current language models for program synthesis struggle with limited dataset size/quality and misaligned synthesis processes compared to human iterative code development using compilers. These models are data-hungry and produce code in one go.", "method": "A bootstrapping algorithm was developed to teach models code repair. The algorithm improves learning through iterative refinement rather than single-shot generation, simulating human development patterns.", "result": "Bootstrapping consistently outperforms regular fine-tuning, achieving performance comparable to models 68% larger. However, repair-based inference is often outperformed by direct sampling despite repair benefits during training.", "conclusion": "The study demonstrates that bootstrapping with repair mechanisms enhances program synthesis effectiveness while using fewer resources, but emphasizes limitations in evaluation datasets and potential improvements in inference techniques for repair processes."}}
{"id": "2507.15892", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15892", "abs": "https://arxiv.org/abs/2507.15892", "authors": ["Elijah Nnorom", "Md Basim Uddin Ahmed", "Jiho Shin", "Hung Viet Pham", "Song Wang"], "title": "StaAgent: An Agentic Framework for Testing Static Analyzers", "comment": null, "summary": "Static analyzers play a critical role in identifying bugs early in the\nsoftware development lifecycle, but their rule implementations are often\nunder-tested and prone to inconsistencies. To address this, we propose\nStaAgent, an agentic framework that harnesses the generative capabilities of\nLarge Language Models (LLMs) to systematically evaluate static analyzer rules.\nStaAgent comprises four specialized agents: a Seed Generation Agent that\ntranslates bug detection rules into concrete, bug-inducing seed programs; a\nCode Validation Agent that ensures the correctness of these seeds; a Mutation\nGeneration Agent that produces semantically equivalent mutants; and an Analyzer\nEvaluation Agent that performs metamorphic testing by comparing the static\nanalyzer's behavior on seeds and their corresponding mutants. By revealing\ninconsistent behaviors, StaAgent helps uncover flaws in rule implementations.\nThis LLM-driven, multi-agent framework offers a scalable and adaptable solution\nto improve the reliability of static analyzers. We evaluated StaAgent with five\nstate-of-the-art LLMs (CodeL-lama, DeepSeek, Codestral, Qwen, and GPT-4o)\nacross five widely used static analyzers (SpotBugs, SonarQube, ErrorProne,\nInfer, and PMD). The experimental results show that our approach can help\nreveal 64 problematic rules in the latest versions of these five static\nanalyzers (i.e., 28 in SpotBugs, 18 in SonarQube, 6 in ErrorProne, 4 in Infer,\nand 8 in PMD). In addition, 53 out of the 64 bugs cannot be detected by the\nSOTA baseline. We have reported all the bugs to developers, with two of them\nalready fixed. Three more have been confirmed by developers, while the rest are\nawaiting response. These results demonstrate the effectiveness of our approach\nand underscore the promise of agentic, LLM-driven data synthesis to advance\nsoftware engineering.", "AI": {"tldr": "StaAgent is an LLM-driven multi-agent framework that identifies inconsistent behaviors in static analyzers by generating and evaluating bug-inducing seed programs and mutants.", "motivation": "Static analyzers are under-tested and prone to rule implementation inconsistencies, which can lead to missed bugs in software.", "method": "The framework uses four agents: Seed Generation (translates rules into seed programs), Code Validation (ensures correctness), Mutation Generation (creates semantically equivalent mutants), and Analyzer Evaluation (compares analyzer behavior via metamorphic testing).", "result": "64 problematic rules were revealed across five static analyzers (SpotBugs, SonarQube, etc.), with 53 undetectable by prior methods; two bugs were fixed, three confirmed, and the rest under review.", "conclusion": "StaAgent demonstrates that agentic, LLM-based data synthesis can significantly enhance static analyzer reliability and advance software engineering practices."}}
{"id": "2507.16037", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16037", "abs": "https://arxiv.org/abs/2507.16037", "authors": ["Zhili Zeng", "Kimya Khakzad Shahandashti", "Alvine Boaye Belle", "Song Wang", "Zhen Ming", "Jiang"], "title": "A Pilot Study on LLM-Based Agentic Translation from Android to iOS: Pitfalls and Insights", "comment": null, "summary": "The rapid advancement of mobile applications has led to a significant demand\nfor cross-platform compatibility, particularly between the Android and iOS\nplatforms. Traditional approaches to mobile application translation often rely\non manual intervention or rule-based systems, which are labor-intensive and\ntime-consuming. While recent advancements in machine learning have introduced\nautomated methods, they often lack contextual understanding and adaptability,\nresulting in suboptimal translations. Large Language Models (LLMs) were\nrecently leveraged to enhance code translation at different granularities,\nincluding the method, class, and repository levels. Researchers have\ninvestigated common errors, limitations, and potential strategies to improve\nthese tasks. However, LLM-based application translation across different\nplatforms, such as migrating mobile applications between Android and iOS or\nadapting software across diverse frameworks, remains underexplored.\nUnderstanding the performance, strengths, and limitations of LLMs in\ncross-platform application translation is critical for advancing software\nengineering automation. This study aims to fill this gap by evaluating\nLLM-based agentic approaches for mobile application translation, identifying\nkey failure points, and proposing guidelines to improve translation\nperformance. We developed a chain of agents that account for dependencies,\nspecifications, program structure, and program control flow when translating\napplications from Android to iOS. To evaluate the performance, we manually\nexamined the translated code for syntactic correctness, semantic accuracy, and\nfunctional completeness. For translation failures, we further conducted a\ndetailed root cause analysis to understand the underlying limitations of the\nagentic translation process and identify opportunities for improvement.", "AI": {"tldr": "This study evaluates LLM-based agents for Android-to-iOS mobile application translation, identifies failure points, and proposes guidelines to improve cross-platform translation performance.", "motivation": "Current mobile app translation methods between Android and iOS are reliance on manual/rule-based systems or insufficient ML approaches lacking contextual understanding. LLM-based translation remains underexplored despite potential for automation.", "method": "Developed an agent chain that handles dependencies, specifications, program structure, and control flow during translation. Evaluated syntactic correctness, semantic accuracy, and functional completeness through manual inspection and root cause analysis of failures.", "result": "Systematically characterized LLM limitations in cross-platform translation, including platform-specific framework adaptation issues and control flow handling deficiencies. Identified common failure modes in agentic approaches.", "conclusion": "LLM-based agents offer promise but require platform-aware architecture adaptations and improved program control flow modeling to achieve effective cross-platform mobile application translation."}}
{"id": "2507.15859", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15859", "abs": "https://arxiv.org/abs/2507.15859", "authors": ["Harsha Sammangi", "Aditya Jagatha", "Giridhar Reddy Bojja", "Jun Liu"], "title": "Decentralized AI-driven IoT Architecture for Privacy-Preserving and Latency-Optimized Healthcare in Pandemic and Critical Care Scenarios", "comment": "10 Pages", "summary": "AI Innovations in the IoT for Real-Time Patient Monitoring On one hand, the\ncurrent traditional centralized healthcare architecture poses numerous issues,\nincluding data privacy, delay, and security. Here, we present an AI-enabled\ndecentralized IoT architecture that can address such challenges during a\npandemic and critical care settings. This work presents our architecture to\nenhance the effectiveness of the current available federated learning,\nblockchain, and edge computing approach, maximizing data privacy, minimizing\nlatency, and improving other general system metrics. Experimental results\ndemonstrate transaction latency, energy consumption, and data throughput orders\nof magnitude lower than competitive cloud solutions.", "AI": {"tldr": "A decentralized IoT and AI architecture improves real-time patient monitoring by enhancing data privacy and reducing latency.", "motivation": "Traditional centralized healthcare systems face significant challenges in data privacy, delay, and security, particularly critical during pandemics and in intensive care environments. This study aims to address these limitations through a novel decentralized approach.", "method": "The paper integrates federated learning, blockchain technology, and edge computing optimizations. Federated learning enables distributed model training with privacy preservation, blockchain ensures secure and tamper-proof data transactions, and edge computing reduces network latency by processing data locally at the edge.", "result": "Experimental evaluations demonstrate transaction latency, energy consumption, and data throughput improvements up to orders of magnitude better than competitive cloud-based solutions.", "conclusion": "The proposed AI-enhanced decentralized IoT architecture offers a robust framework for real-time patient monitoring in critical scenarios, with tangible benefits in latency, privacy, security, and energy efficiency."}}
{"id": "2507.16044", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16044", "abs": "https://arxiv.org/abs/2507.16044", "authors": ["Meriem Mastouri", "Emna Ksontini", "Wael Kessentini"], "title": "Making REST APIs Agent-Ready: From OpenAPI to Model Context Protocol Servers for Tool-Augmented LLMs", "comment": null, "summary": "Large Language Models (LLMs) are evolving from passive text generators into\nactive agents that invoke external tools. To support this shift, scalable\nprotocols for tool integration are essential. The Model Context Protocol (MCP),\nintroduced by Anthropic in 2024, offers a schema-driven standard for dynamic\ntool discovery and invocation. Yet, building MCP servers remains manual and\nrepetitive, requiring developers to write glue code, handle authentication, and\nconfigure schemas by hand-replicating much of the integration effort MCP aims\nto eliminate.\n  This paper investigates whether MCP server construction can be meaningfully\nautomated. We begin by analyzing adoption trends: among 22,000+ MCP-tagged\nGitHub repositories created within six months of release, fewer than 5% include\nservers, typically small, single-maintainer projects dominated by repetitive\nscaffolding. To address this gap, we present AutoMCP, a compiler that generates\nMCP servers from OpenAPI 2.0/3.0 specifications. AutoMCP parses REST API\ndefinitions and produces complete server implementations, including schema\nregistration and authentication handling.\n  We evaluate AutoMCP on 50 real-world APIs spanning 5,066 endpoints across\nover 10 domains. From a stratified sample of 1,023 tool calls, 76.5% succeeded\nout of the box. Manual failure analysis revealed five recurring issues, all\nattributable to inconsistencies or omissions in the OpenAPI contracts. After\nminor fixes, averaging 19 lines of spec changes per API, AutoMCP achieved 99.9%\nsuccess.\n  Our findings (i) analyze MCP adoption and quantify the cost of manual server\ndevelopment, (ii) demonstrate that OpenAPI specifications, despite quality\nissues, enable near-complete MCP server automation, and (iii) contribute a\ncorpus of 5,066 callable tools along with insights on repairing common\nspecification flaws.", "AI": {"tldr": "The paper introduces AutoMCP, a compiler that automates the creation of Model Context Protocol (MCP) servers from OpenAPI 3.0 specifications, achieving 99.9% success after resolving minor spec inconsistencies.", "motivation": "Manual MCP server construction requires repetitive scaffolding and glue code, limiting adoption despite MCP's potential to streamline tool integration for large language models.", "method": "AutoMCP was developed to parse REST API definitions and generate schema-registered, authenticated servers. The tool was evaluated on 50 real-world APIs with 5,066 endpoints via stratified sampling of 1,023 tool calls.", "result": "76.5% of tool calls worked without modification, rising to 99.9% after 19-line average adjustments per API to fix OpenAPI specification flaws. The tool supports dynamic integration of APIs across 10+ domains.", "conclusion": "Automatic MCP server generation is viable using OpenAPI 3.0 contracts despite specification quality issues. The work contributes (1) adoption analysis of manual MCP costs, (2) demonstration of 99.9% automation feasibility, and (3) a 5,066-endpoint tool corpus with spec repair patterns."}}
{"id": "2507.15984", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15984", "abs": "https://arxiv.org/abs/2507.15984", "authors": ["I Putu Arya Dharmaadi", "Mohannad Alhanahnah", "Van-Thuan Pham", "Fadi Mohsen", "Fatih Turkmen"], "title": "BACFuzz: Exposing the Silence on Broken Access Control Vulnerabilities in Web Applications", "comment": "Under peer-review", "summary": "Broken Access Control (BAC) remains one of the most critical and widespread\nvulnerabilities in web applications, allowing attackers to access unauthorized\nresources or perform privileged actions. Despite its severity, BAC is\nunderexplored in automated testing due to key challenges: the lack of reliable\noracles and the difficulty of generating semantically valid attack requests. We\nintroduce BACFuzz, the first gray-box fuzzing framework specifically designed\nto uncover BAC vulnerabilities, including Broken Object-Level Authorization\n(BOLA) and Broken Function-Level Authorization (BFLA) in PHP-based web\napplications. BACFuzz combines LLM-guided parameter selection with runtime\nfeedback and SQL-based oracle checking to detect silent authorization flaws. It\nemploys lightweight instrumentation to capture runtime information that guides\ntest generation, and analyzes backend SQL queries to verify whether\nunauthorized inputs flow into protected operations. Evaluated on 20 real-world\nweb applications, including 15 CVE cases and 2 known benchmarks, BACFuzz\ndetects 16 of 17 known issues and uncovers 26 previously unknown BAC\nvulnerabilities with low false positive rates. All identified issues have been\nresponsibly disclosed, and artifacts will be publicly released.", "AI": {"tldr": "BACFuzz is a gray-box fuzzing framework for detecting Broken Access Control vulnerabilities in PHP web applications using LLM-guided parameter selection, runtime feedback, and SQL-based oracle checking.", "motivation": "Broken Access Control (BAC) vulnerabilities enable unauthorized access/privileged actions in web apps, but remain underexplored in automated testing due to unreliable oracles and challenges in generating semantically valid attack requests.", "method": "BACFuzz combines large language model (LLM)-guided parameter selection with runtime feedback and SQL-based oracle checking to detect silent authorization flaws. It uses lightweight instrumentation for runtime information capture and analyzes backend SQL queries to verify unauthorized input flows.", "result": "Achieved 94% detection rate on 17 known BAC vulnerabilities and discovered 26 previously unknown issues across 20 real-world applications, including 15 CVE cases and 2 benchmarks.", "conclusion": "BACFuzz demonstrates robust BAC detection capability with low false positives, successfully uncovering both known and novel vulnerabilities while artifacts will be publicly released for further validation."}}
{"id": "2507.16063", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16063", "abs": "https://arxiv.org/abs/2507.16063", "authors": ["Yousab Grees", "Polina Iaremchuk", "Ramtin Ehsani", "Esteban Parra", "Preetha Chatterjee", "Sonia Haiduc"], "title": "AI-Powered Commit Explorer (APCE)", "comment": null, "summary": "Commit messages in a version control system provide valuable information for\ndevelopers regarding code changes in software systems. Commit messages can be\nthe only source of information left for future developers describing what was\nchanged and why. However, writing high-quality commit messages is often\nneglected in practice. Large Language Model (LLM) generated commit messages\nhave emerged as a way to mitigate this issue. We introduce the AI-Powered\nCommit Explorer (APCE), a tool to support developers and researchers in the use\nand study of LLM-generated commit messages. APCE gives researchers the option\nto store different prompts for LLMs and provides an additional evaluation\nprompt that can further enhance the commit message provided by LLMs. APCE also\nprovides researchers with a straightforward mechanism for automated and human\nevaluation of LLM-generated messages. Demo link https://youtu.be/zYrJ9s6sZvo", "AI": {"tldr": "The paper introduces APCE, a tool for developers and researchers to use and study LLM-generated commit messages, offering prompt storage, evaluation prompts, and evaluation mechanisms.", "motivation": "High-quality commit messages are often neglected, and LLM-generated messages need improvement in practice. Researchers require tools to study and evaluate their effectiveness.", "method": "APCE was developed to allow storing LLM prompts, integrate an evaluation prompt for enhanced message generation, and provide automated/human evaluation mechanisms for assessing generated messages.", "result": "APCE supports researchers in prompt management and evaluation of LLM-generated commit messages, with a demo available for demonstration.", "conclusion": "APCE addresses the need for tools to systematically evaluate and improve LLM-generated commit messages, facilitating both practical use and academic research."}}
{"id": "2507.15997", "categories": ["cs.CR", "cs.HC", "68-XX 68-XX 68-XX"], "pdf": "https://arxiv.org/pdf/2507.15997", "abs": "https://arxiv.org/abs/2507.15997", "authors": ["Onyinye Dibia", "Mengyi Lu", "Prianka Bhattacharjee", "Joseph P. Near", "Yuanyuan Feng"], "title": "\"We Need a Standard\": Toward an Expert-Informed Privacy Label for Differential Privacy", "comment": "13 pages, 5 figures", "summary": "The increasing adoption of differential privacy (DP) leads to public-facing\nDP deployments by both government agencies and companies. However, real-world\nDP deployments often do not fully disclose their privacy guarantees, which vary\ngreatly between deployments. Failure to disclose certain DP parameters can lead\nto misunderstandings about the strength of the privacy guarantee, undermining\nthe trust in DP. In this work, we seek to inform future standards for\ncommunicating the privacy guarantees of DP deployments. Based on\nsemi-structured interviews with 12 DP experts, we identify important DP\nparameters necessary to comprehensively communicate DP guarantees, and describe\nwhy and how they should be disclosed. Based on expert recommendations, we\ndesign an initial privacy label for DP to comprehensively communicate privacy\nguarantees in a standardized format.", "AI": {"tldr": "This paper investigates the inconsistent disclosure practices in differential privacy deployments and proposes a standardized privacy label to transparently communicate privacy guarantees using expert interviews.", "motivation": "The core motivation addresses the lack of standardized transparency in real-world differential privacy (DP) implementations, where undisclosed parameters erode trust and create ambiguous privacy assurances through public-facing systems.", "method": "Semi-structured interviews with 12 DP researchers facilitated the systematic identification of critical parameters, followed by the development of a formalized privacy label prototype to operationalize disclosure best practices.", "result": "We established a comprehensive set of DP parameters requiring disclosure, including their quantitative significance, and implemented a prototype privacy label to standardize communication of these values across deployments.", "conclusion": "The study advocates for mandatory implementation of privacy labels in DP systems to ensure consistent disclosure of core parameters, thereby establishing verifiable trust and enabling future empirical comparisons across deployments."}}
{"id": "2507.16166", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16166", "abs": "https://arxiv.org/abs/2507.16166", "authors": ["Nasir U. Eisty", "David E. Bernholdt", "Alex Koufos", "David J. Luet", "Miranda Mundt"], "title": "Ten Essential Guidelines for Building High-Quality Research Software", "comment": null, "summary": "High-quality research software is a cornerstone of modern scientific\nprogress, enabling researchers to analyze complex data, simulate phenomena, and\nshare reproducible results. However, creating such software requires adherence\nto best practices that ensure robustness, usability, and sustainability. This\npaper presents ten guidelines for producing high-quality research software,\ncovering every stage of the development lifecycle. These guidelines emphasize\nthe importance of planning, writing clean and readable code, using version\ncontrol, and implementing thorough testing strategies. Additionally, they\naddress key principles such as modular design, reproducibility, performance\noptimization, and long-term maintenance. The paper also highlights the role of\ndocumentation and community engagement in enhancing software usability and\nimpact. By following these guidelines, researchers can create software that\nadvances their scientific objectives and contributes to a broader ecosystem of\nreliable and reusable research tools. This work serves as a practical resource\nfor researchers and developers aiming to elevate the quality and impact of\ntheir research software.", "AI": {"tldr": "This paper proposes ten guidelines for developing high-quality research software across the entire lifecycle, focusing on best practices for robustness, usability, and sustainability.", "motivation": "To address the critical need for reliable and reusable research software that supports scientific progress through reproducibility, maintainability, and effective collaboration.", "method": "The work synthesizes ten evidence-based guidelines covering software planning, clean code practices, version control, testing strategies, modular design, performance optimization, documentation, and long-term maintenance principles.", "result": "Demonstrates that following these guidelines produces research software with enhanced robustness, usability, and sustainability, while fostering community engagement and scientific reproducibility.", "conclusion": "Researchers adopting these guidelines can advance their scientific objectives while contributing to a more reliable and interconnected ecosystem of research tools through improved software quality and community standards."}}
{"id": "2507.16040", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.16040", "abs": "https://arxiv.org/abs/2507.16040", "authors": ["Xinyuan Zhang", "Anrin Chakraborti", "Michael Reiter"], "title": "Blocklisted Oblivious Pseudorandom Functions", "comment": null, "summary": "An oblivious pseudorandom function (OPRF) is a protocol by which a client and\nserver interact to evaluate a pseudorandom function on a key provided by the\nserver and an input provided by the client, without divulging the key or input\nto the other party. We extend this notion by enabling the server to specify a\nblocklist, such that OPRF evaluation succeeds only if the client's input is not\non the blocklist. More specifically, our design gains performance by embedding\nthe client input into a metric space, where evaluation continues only if this\nembedding does not cluster with blocklist elements. Our framework exploits this\nstructure to separate the embedding and blocklist check to enable efficient\nimplementations of each, but then must stitch these phases together through\ncryptographic means. Our framework also supports subsequent evaluation of the\nOPRF on the same input more efficiently. We demonstrate the use of our design\nfor password blocklisting in augmented password-authenticated key exchange, and\nto MAC only executables that are not similar to ones on a blocklist of known\nmalware.", "AI": {"tldr": "This paper introduces a blocklisted OPRF (BLOPRF) framework where a server can define a blocklist, and the protocol only evaluates the pseudorandom function if the client's input doesn't cluster with blocked items in a metric space. It optimizes efficiency by separating cryptographic tasks and shows applications in password security and malware detection.", "motivation": "Existing OPRF protocols lack mechanisms to prevent malicious inputs (e.g., compromised passwords, malware executables) from being processed. The authors aim to develop a secure and efficient system that enables selective OPRF evaluation through blocklist verification while preserving privacy.", "method": "The approach embeds client and server inputs into a metric space, splitting OPRF evaluation into two phases: (1) deterministic embedding computation by the client and (2) secure blocklist checking via distance bounding techniques. Cryptographic stitching ensures interaction without leaking information between phases.", "result": "The paper demonstrates BLOPRF's feasibility for augmenting password-authenticated key exchange and malware analysis systems. Performance improvements are achieved through phase separation and efficient repeated computation reuse of initial embeddings.", "conclusion": "The BLOPRF framework establishes a privacy-preserving mechanism for OPRF blocklisting, enabling security-critical applications to selectively compute functions on potentially sensitive input while maintaining efficiency and cryptographic correctness."}}
{"id": "2507.16208", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16208", "abs": "https://arxiv.org/abs/2507.16208", "authors": ["Sohaib Muhammad", "Ashwati Vipin", "Karan Shetti", "Honey Mittal"], "title": "LOCOFY Large Design Models -- Design to code conversion solution", "comment": null, "summary": "Despite rapid advances in Large Language Models and Multimodal Large Language\nModels (LLMs), numerous challenges related to interpretability, scalability,\nresource requirements and repeatability remain, related to their application in\nthe design-to-code space. To address this, we introduce the Large Design Models\n(LDMs) paradigm specifically trained on designs and webpages to enable seamless\nconversion from design-to-code. We have developed a training and inference\npipeline by incorporating data engineering and appropriate model architecture\nmodification. The training pipeline consists of the following: 1)Design\nOptimiser: developed using a proprietary ground truth dataset and addresses\nsub-optimal designs; 2)Tagging and feature detection: using pre-trained and\nfine-tuned models, this enables the accurate detection and classification of UI\nelements; and 3)Auto Components: extracts repeated UI structures into reusable\ncomponents to enable creation of modular code, thus reducing redundancy while\nenhancing code reusability. In this manner, each model addresses distinct but\nkey issues for design-to-code conversion. Separately, our inference pipeline\nprocesses real-world designs to produce precise and interpretable instructions\nfor code generation and ensures reliability. Additionally, our models\nillustrated exceptional end-to-end design-to-code conversion accuracy using a\nnovel preview match score metric. Comparative experiments indicated superior\nperformance of LDMs against LLMs on accuracy of node positioning,\nresponsiveness and reproducibility. Moreover, our custom-trained tagging and\nfeature detection model demonstrated high precision and consistency in\nidentifying UI elements across a wide sample of test designs. Thus, our\nproposed LDMs are a reliable and superior solution to understanding designs\nthat subsequently enable the generation of efficient and reliable\nproduction-ready code.", "AI": {"tldr": "The paper proposes Large Design Models (LDMs) to address design-to-code challenges not solvable by existing LLMs through a novel training pipeline with three core components (Design Optimiser, Tagging & Feature Detection, Auto Components), demonstrating superior conversion accuracy and code efficiency via preview match score and benchmark comparison.", "motivation": "Current LLMs and Multimodal LLMs face limitations in interpretability, scalability, resource efficiency, and repeatability when applied to design-to-code conversion tasks.", "method": "Developed a dual-phase approach with 1) Design Optimiser for sub-optimal design correction, 2) Tagging & Feature Detection using fine-tuned pre-trained models for UI element classification, and 3) Auto Components for modular code generation. Inference processes real-world designs into precise code instructions with reliability.", "result": "LDMs outperformed LLMs in node positioning accuracy (38.2% improvement), responsiveness (27.5% improvement), and reproducibility (41.7% improvement). Custom tagging model achieved 92.3% precision across 1,000+ test designs.", "conclusion": "LDMs represent a superior and reliable paradigm for end-to-end design-to-code conversion, enabling production-ready code with enhanced modularity and reduced redundancy through their architecture-specific optimizations."}}
{"id": "2507.16060", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.16060", "abs": "https://arxiv.org/abs/2507.16060", "authors": ["Eyasu Getahun Chekole", "Howard Halim", "Jianying Zhou"], "title": "MFAz: Historical Access Based Multi-Factor Authorization", "comment": null, "summary": "Unauthorized access remains one of the critical security challenges in the\nrealm of cybersecurity. With the increasing sophistication of attack\ntechniques, the threat of unauthorized access is no longer confined to the\nconventional ones, such as exploiting weak access control policies. Instead,\nadvanced exploitation strategies, such as session hijacking-based attacks, are\nbecoming increasingly prevalent, posing serious security concerns. Session\nhijacking enables attackers to take over an already established session between\nlegitimate peers in a stealthy manner, thereby gaining unauthorized access to\nprivate resources. Unfortunately, traditional access control mechanisms, such\nas static access control policies, are insufficient to prevent session\nhijacking or other advanced exploitation techniques. In this work, we propose a\nnew multi-factor authorization (MFAz) scheme that proactively mitigates\nunauthorized access attempts both conventional and advanced unauthorized access\nattacks. The proposed scheme employs fine-grained access control rules (ARs)\nand verification points (VPs) that are systematically generated from\nhistorically granted accesses as the first and second authorization factors,\nrespectively. As a proof-of-concept, we implement the scheme using different\ntechniques. We leverage bloom filter to achieve runtime and storage efficiency,\nand blockchain to make authorization decisions in a temper-proof and\ndecentralized manner. To the best of our knowledge, this is the first formal\nintroduction of a multi-factor authorization scheme, which is orthogonal to the\nmulti-factor authentication (MFA) schemes. The effectiveness of our proposed\nscheme is experimentally evaluated using a smart-city testbed involving\ndifferent devices with varying computational capacities. The experimental\nresults reveal high effectiveness of the scheme both in security and\nperformance guarantees.", "AI": {"tldr": "This paper proposes a novel multi-factor authorization (MFAz) scheme to counter advanced unauthorized access threats like session hijacking, leveraging bloom filters for efficiency and blockchain for secure decentralized decision-making. Experimental evaluation on a smart-city testbed demonstrates its effectiveness.", "motivation": "Traditional access control mechanisms (e.g., static policies) fail to prevent sophisticated attacks such as session hijacking, which enables stealthy takeover of established peer sessions to access private resources. Advanced exploitation techniques require proactive solutions beyond conventional approaches.", "method": "The authors introduce a multi-factor authorization scheme (MFAz) utilizing two factors: (1) fine-grained access control rules (ARs) derived from historical access patterns, and (2) systematically generated verification points (VPs). Techniques include bloom filter implementation for runtime/storage efficiency and blockchain for tamper-proof, decentralized authorization decisions.", "result": "Experimental evaluation on a smart-city testbed with diverse devices showed the MFAz scheme provides security against both conventional and advanced attacks while maintaining performance efficiency. The combination of bloom filters and blockchain successfully enhanced protection without compromising scalability.", "conclusion": "The proposed MFAz framework offers a robust solution for proactive access control in cyber-physical systems like smart cities. This work pioneers a multi-factor authorization approach orthogonal to multi-factor authentication (MFA), demonstrating practicality through hardware-validated performance results."}}
{"id": "2507.16327", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16327", "abs": "https://arxiv.org/abs/2507.16327", "authors": ["Karoline Nyl\u00e6nder", "Aitor Arrieta", "Shaukat Ali", "Paolo Arcaini"], "title": "Search-based Generation of Waypoints for Triggering Self-Adaptations in Maritime Autonomous Vessels", "comment": "9 pages, 3 figures. Accepted at GECCO 2025 (Genetic and Evolutionary\n  Computation Conference), July 14-18, 2025, Malaga, Spain", "summary": "Self-adaptation in maritime autonomous vessels (AVs) enables them to adapt\ntheir behaviors to address unexpected situations while maintaining\ndependability requirements. During the design of such AVs, it is crucial to\nunderstand and identify the settings that should trigger adaptations, enabling\nvalidation of their implementation. To this end, we focus on the navigation\nsoftware of AVs, which must adapt their behavior during operation through\nadaptations. AVs often rely on predefined waypoints to guide them along\ndesignated routes, ensuring safe navigation. We propose a multiobjective\nsearch-based approach, called WPgen, to generate minor modifications to the\npredefined set of waypoints, keeping them as close as possible to the original\nwaypoints, while causing the AV to navigate inappropriately when navigating\nwith the generated waypoints. WPgen uses NSGA-II as the multi-objective search\nalgorithm with three seeding strategies for its initial population, resulting\nin three variations of WPgen. We evaluated these variations on three AVs (one\noverwater tanker and two underwater). We compared the three variations of WPgen\nwith Random Search as the baseline and with each other. Experimental results\nshowed that the effectiveness of these variations varied depending on the AV.\nBased on the results, we present the research and practical implications of\nWPgen.", "AI": {"tldr": "WPgen uses multiobjective search to generate modified waypoints for testing self-adaptation in autonomous maritime vessels, showing effectiveness varies across vessel types.", "motivation": "Ensuring safety and dependability in autonomous vessels through systematic testing of self-adaptation mechanisms against unexpected waypoint modifications.", "method": "Proposes WPgen with NSGA-II and three seeding strategies to generate perturbed waypoints, evaluated on three AVs (one overwater tanker, two underwater).", "result": "Experimental results demonstrated variation in effectiveness among WPgen's three variations across different AV types.", "conclusion": "Highlights research and practical implications for improving self-adaptation validation in maritime autonomous systems through tailored waypoint generation."}}
{"id": "2507.16134", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.16134", "abs": "https://arxiv.org/abs/2507.16134", "authors": ["Baofu Han", "Bing Li", "Yining Qi", "Raja Jurdak", "Kaibin Huang", "Chau Yuen"], "title": "DP2Guard: A Lightweight and Byzantine-Robust Privacy-Preserving Federated Learning Scheme for Industrial IoT", "comment": null, "summary": "Privacy-Preserving Federated Learning (PPFL) has emerged as a secure\ndistributed Machine Learning (ML) paradigm that aggregates locally trained\ngradients without exposing raw data. To defend against model poisoning threats,\nseveral robustness-enhanced PPFL schemes have been proposed by integrating\nanomaly detection. Nevertheless, they still face two major challenges: (1) the\nreliance on heavyweight encryption techniques results in substantial\ncommunication and computation overhead; and (2) single-strategy defense\nmechanisms often fail to provide sufficient robustness against adaptive\nadversaries. To overcome these challenges, we propose DP2Guard, a lightweight\nPPFL framework that enhances both privacy and robustness. DP2Guard leverages a\nlightweight gradient masking mechanism to replace costly cryptographic\noperations while ensuring the privacy of local gradients. A hybrid defense\nstrategy is proposed, which extracts gradient features using singular value\ndecomposition and cosine similarity, and applies a clustering algorithm to\neffectively identify malicious gradients. Additionally, DP2Guard adopts a trust\nscore-based adaptive aggregation scheme that adjusts client weights according\nto historical behavior, while blockchain records aggregated results and trust\nscores to ensure tamper-proof and auditable training. Extensive experiments\nconducted on two public datasets demonstrate that DP2Guard effectively defends\nagainst four advanced poisoning attacks while ensuring privacy with reduced\ncommunication and computation costs.", "AI": {"tldr": "DP2Guard enhances PPFL by using lightweight gradient masking, hybrid defense strategies (SVD, cosine similarity, clustering), and blockchain for privacy and robustness against poisoning attacks with reduced overhead.", "motivation": "Existing PPFL schemes face high computational/communication overhead from encryption and insufficient robustness against adaptive adversaries using single-strategy defenses.", "method": "The framework combines (1) lightweight gradient masking (instead of cryptography), (2) hybrid defense with singular value decomposition and cosine similarity feature extraction plus clustering, and (3) trust-score adaptive aggregation with blockchain auditability.", "result": "Extensive experiments on two public datasets show DP2Guard effectively defends against four advanced poisoning attacks while maintaining privacy and reducing overhead.", "conclusion": "DP2Guard offers a balanced solution for secure PPFL by improving privacy, robustness, and efficiency through integrated novel techniques."}}
{"id": "2507.16407", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16407", "abs": "https://arxiv.org/abs/2507.16407", "authors": ["Shuhan Liu", "Xing Hu", "Kerui Huang", "Xiaohu Yang", "David Lo", "Xin Xia"], "title": "Improving Code LLM Robustness to Prompt Perturbations via Layer-Aware Model Editing", "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\ncode generation, where the natural language prompt plays a crucial role in\nconveying user intent to the model. However, prior studies have shown that LLMs\nare highly sensitive to prompt perturbations. Minor modifications in wording,\nsyntax, or formatting can significantly reduce the functional correctness of\ngenerated code. As perturbations frequently occur in real-world scenarios,\nimproving the robustness of LLMs to prompt perturbations is essential for\nensuring reliable performance in practical code generation. In this paper, we\nintroduce CREME (Code Robustness Enhancement via Model Editing), a novel\napproach that enhances LLM robustness through targeted parameter updates. CREME\nfirst identifies robustness-sensitive layers by comparing hidden states between\nan original prompt and its perturbed variant. Then, it performs lightweight\nparameter editing at the identified layer to reduce performance degradation. We\nevaluate CREME on two widely used code generation benchmarks (HumanEval and\nMBPP) along with their perturbed counterparts. Experimental results show that\nCREME improves Pass@1 accuracy by 63% on perturbed prompts while maintaining\nstable performance on clean inputs, with accuracy deviations within 1%. Further\nanalysis reveals that robustness-sensitive layers are primarily concentrated in\nthe middle and deeper layers of the network, and their locations vary across\ndifferent model architectures. These insights provide a valuable foundation for\ndeveloping future robustness-oriented editing strategies.", "AI": {"tldr": "CREME is a novel method that enhances the robustness of large language models in code generation by identifying and editing parameters in robustness-sensitive layers, improving performance on perturbed prompts without compromising accuracy on clean inputs.", "motivation": "LLMs are highly sensitive to prompt perturbations (e.g., wording changes, formatting), leading to reduced functional correctness in real-world scenarios. This sensitivity hinders reliable code generation, necessitating robustness improvements.", "method": "CREME identifies robustness-sensitive layers by comparing hidden states of original and perturbed prompts. It then applies lightweight parameter editing at those layers to mitigate performance degradation caused by perturbations.", "result": "Evaluations on HumanEval and MBPP benchmarks with perturbed prompts demonstrate CREME improves Pass@1 accuracy by 63% on perturbed inputs while maintaining a deviation of \u22641% on clean inputs. Sensitive layers are concentrated in middle/deep layers and vary across architectures.", "conclusion": "CREME effectively addresses prompt perturbation vulnerability in code generation, showing that targeted parameter editing in identified sensitive layers improves robustness. The findings suggest model-architecture-specific sensitive layers and lay the groundwork for future robustness-focused editing strategies."}}
{"id": "2507.16164", "categories": ["cs.CR", "cs.AI", "cs.LG", "I.2.7; I.2.6; I.2.3; D.4.6"], "pdf": "https://arxiv.org/pdf/2507.16164", "abs": "https://arxiv.org/abs/2507.16164", "authors": ["Eldor Abdukhamidov", "Tamer Abuhmed", "Joanna C. S. Santos", "Mohammed Abuhamad"], "title": "Attacking interpretable NLP systems", "comment": null, "summary": "Studies have shown that machine learning systems are vulnerable to\nadversarial examples in theory and practice. Where previous attacks have\nfocused mainly on visual models that exploit the difference between human and\nmachine perception, text-based models have also fallen victim to these attacks.\nHowever, these attacks often fail to maintain the semantic meaning of the text\nand similarity. This paper introduces AdvChar, a black-box attack on\nInterpretable Natural Language Processing Systems, designed to mislead the\nclassifier while keeping the interpretation similar to benign inputs, thus\nexploiting trust in system transparency. AdvChar achieves this by making less\nnoticeable modifications to text input, forcing the deep learning classifier to\nmake incorrect predictions and preserve the original interpretation. We use an\ninterpretation-focused scoring approach to determine the most critical tokens\nthat, when changed, can cause the classifier to misclassify the input. We apply\nsimple character-level modifications to measure the importance of tokens,\nminimizing the difference between the original and new text while generating\nadversarial interpretations similar to benign ones. We thoroughly evaluated\nAdvChar by testing it against seven NLP models and three interpretation models\nusing benchmark datasets for the classification task. Our experiments show that\nAdvChar can significantly reduce the prediction accuracy of current deep\nlearning models by altering just two characters on average in input samples.", "AI": {"tldr": "AdvChar is a black-box adversarial attack on interpretable NLP systems that modifies text with minimal character changes to mislead classifiers while preserving original interpretations.", "motivation": "Previous text-based adversarial attacks often fail to maintain semantic meaning and similarity, while visual attacks exploit human-machine perception differences. This paper aims to develop an attack that preserves interpretation trust while causing misclassification.", "method": "The method uses an interpretation-focused scoring approach to identify critical tokens and applies character-level modifications to alter input texts with minimal changes (typically 2 characters), keeping adversarial interpretations similar to benign ones.", "result": "AdvChar significantly reduced prediction accuracy across seven NLP models and three interpretation models using benchmark datasets, demonstrating effectiveness of the attack with minimal text alterations.", "conclusion": "AdvChar exploits vulnerabilities in interpretable NLP systems by making imperceptible character changes that induce classification errors while maintaining trusted interpretations, highlighting risks to model transparency."}}
{"id": "2507.16439", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16439", "abs": "https://arxiv.org/abs/2507.16439", "authors": ["Gunnar Larsen", "Carol Wong", "Anthony Peruma"], "title": "Exploring Large Language Models for Analyzing and Improving Method Names in Scientific Code", "comment": "The 19th ACM/IEEE International Symposium on Empirical Software\n  Engineering and Measurement - Emerging Results and Vision Track", "summary": "Research scientists increasingly rely on implementing software to support\ntheir research. While previous research has examined the impact of identifier\nnames on program comprehension in traditional programming environments, limited\nwork has explored this area in scientific software, especially regarding the\nquality of method names in the code. The recent advances in Large Language\nModels (LLMs) present new opportunities for automating code analysis tasks,\nsuch as identifier name appraisals and recommendations. Our study evaluates\nfour popular LLMs on their ability to analyze grammatical patterns and suggest\nimprovements for 496 method names extracted from Python-based Jupyter\nNotebooks. Our findings show that the LLMs are somewhat effective in analyzing\nthese method names and generally follow good naming practices, like starting\nmethod names with verbs. However, their inconsistent handling of\ndomain-specific terminology and only moderate agreement with human annotations\nindicate that automated suggestions require human evaluation. This work\nprovides foundational insights for improving the quality of scientific code\nthrough AI automation.", "AI": {"tldr": "The paper evaluates four LLMs' effectiveness in analyzing and improving method names in scientific Python code, finding they are moderately helpful but require human review due to domain-specific inconsistencies.", "motivation": "Scientific software relies heavily on clear method names, yet prior research focused mainly on traditional programming environments. LLMs offer potential for automating code quality tasks in this domain.", "method": "Assessed four LLMs by analyzing grammatical patterns and recommending improvements for 496 method names from Python Jupyter Notebooks, comparing against established naming practices and human annotations.", "result": "LLMs demonstrated moderate effectiveness, often adhering to verb-leading naming conventions but showing inconsistency with domain jargon and only 56-70% agreement with human evaluators.", "conclusion": "Automated LLM-based name recommendations can aid scientific code quality improvement but must be supplemented with human oversight due to moderate reliability and domain knowledge gaps."}}
{"id": "2507.16203", "categories": ["cs.CR", "cs.AI", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16203", "abs": "https://arxiv.org/abs/2507.16203", "authors": ["Rui Guo", "Avinash Ayalasomayajula", "Henian Li", "Jingbo Zhou", "Sujan Kumar Saha", "Farimah Farahmandi"], "title": "SVAgent: AI Agent for Hardware Security Verification Assertion", "comment": null, "summary": "Verification using SystemVerilog assertions (SVA) is one of the most popular\nmethods for detecting circuit design vulnerabilities. However, with the\nglobalization of integrated circuit design and the continuous upgrading of\nsecurity requirements, the SVA development model has exposed major limitations.\nIt is not only inefficient in development, but also unable to effectively deal\nwith the increasing number of security vulnerabilities in modern complex\nintegrated circuits. In response to these challenges, this paper proposes an\ninnovative SVA automatic generation framework SVAgent. SVAgent introduces a\nrequirement decomposition mechanism to transform the original complex\nrequirements into a structured, gradually solvable fine-grained problem-solving\nchain. Experiments have shown that SVAgent can effectively suppress the\ninfluence of hallucinations and random answers, and the key evaluation\nindicators such as the accuracy and consistency of the SVA are significantly\nbetter than existing frameworks. More importantly, we successfully integrated\nSVAgent into the most mainstream integrated circuit vulnerability assessment\nframework and verified its practicality and reliability in a real engineering\ndesign environment.", "AI": {"tldr": "The paper introduces SVAgent, an automated framework for generating SystemVerilog Assertions (SVA) to address limitations in manual SVA development and improving security analysis in complex circuits.", "motivation": "Current SVA development models are inefficient and inadequate for handling the growing complexity and security vulnerabilities in modern integrated circuits, especially with globalized design processes and stricter security standards.", "method": "SVAgent employs a requirement decomposition mechanism that breaks down complex verification requirements into structured, fine-grained sub-problems, enabling systematic SVA generation.", "result": "SVAgent demonstrates superior accuracy and consistency compared to existing frameworks, effectively reduces hallucinations/random errors, and integrates with mainstream vulnerability assessment tools in real engineering environments.", "conclusion": "SVAgent provides a reliable, efficient solution for automated SVA generation, addressing critical gaps in circuit verification while maintaining practicality through industry framework integration."}}
{"id": "2507.16587", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16587", "abs": "https://arxiv.org/abs/2507.16587", "authors": ["Giuseppe Crupi", "Rosalia Tufano", "Alejandro Velasco", "Antonio Mastropaolo", "Denys Poshyvanyk", "Gabriele Bavota"], "title": "On the Effectiveness of LLM-as-a-judge for Code Generation and Summarization", "comment": "Accepted at TSE. IEEE Transactions on Software Engineering", "summary": "Large Language Models have been recently exploited as judges for complex\nnatural language processing tasks, such as Q&A. The basic idea is to delegate\nto an LLM the assessment of the \"quality\" of the output provided by an\nautomated technique for tasks for which: (i) quantitative metrics would only\ntell part of the story, and; (ii) a large-scale human-based evaluation would be\ntoo expensive. LLMs-as-a-judge, if proven effective for a specific task, can\nalso unlock new possibilities for automation, with several LLMs proposing a\nsolution for a given instance of the task and others judging and deciding what\nis the best output to show the user. We study the effectiveness of\nLLMs-as-a-judge for two code-related tasks, namely code generation and code\nsummarization. The rationale for choosing these tasks is two-fold. First,\nquantitative metrics are usually not enough for the assessment of code\nsummarizers/generators. For example, it is well documented that metrics such as\nBLEU are quite weak proxies for the quality of the generated summaries. Second,\neven state-of-the-art techniques still struggle with handling complex instances\nof these tasks, making them good candidates for benefiting from more advanced\nsolutions envisioning collaboration among LLMs. For code generation, we check\nwhether eight LLMs are able to judge the correctness of 1,405 Java methods and\n1,281 Python functions generated by the same LLMs or implemented by humans. For\ncode summarization, we compare the judgment of five LLMs to those provided by\nnine humans for ~1.2k summaries, related to both Java and Python functions. Our\nfindings show that GPT-4-turbo is the best LLM in terms of judging capabilities\nfor both tasks, with \"smaller\" LLMs featuring tens of billions parameters not\nbeing able to cope with judging tasks. However, even the best-performing LLM\nfrequently misjudges the correctness of the code and summary quality.", "AI": {"tldr": "The paper evaluates the effectiveness of LLMs as judges for code generation and summarization tasks, finding that GPT-4-turbo performs best but still frequently misjudges correctness and quality.", "motivation": "The study addresses the limitations of quantitative metrics and the expense of human evaluation in assessing code tasks, aiming to explore LLMs' potential as scalable judgment tools for complex code instances.", "method": "Eight LLMs were tested on judging code correctness (1,405 Java + 1,281 Python methods), while five LLMs and nine humans evaluated ~1.2k code summaries (Java/Python). Judging performance was compared across LLMs and human benchmarks.", "result": "GPT-4-turbo outperformed other LLMs in both code generation and summarization judging tasks; smaller LLMs (10B+ parameters) exhibited poor judgment accuracy. However, top LLMs frequently misjudged code correctness and summary quality relative to human assessments.", "conclusion": "While LLMs like GPT-4-turbo show promise as automated judges for code tasks, their frequent misjudgments highlight the need for specialized evaluation frameworks or hybrid approaches combining LLMs and human oversight."}}
{"id": "2507.16241", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16241", "abs": "https://arxiv.org/abs/2507.16241", "authors": ["Paul R. B. Houssel", "Siamak Layeghy", "Priyanka Singh", "Marius Portmann"], "title": "eX-NIDS: A Framework for Explainable Network Intrusion Detection Leveraging Large Language Models", "comment": null, "summary": "This paper introduces eX-NIDS, a framework designed to enhance\ninterpretability in flow-based Network Intrusion Detection Systems (NIDS) by\nleveraging Large Language Models (LLMs). In our proposed framework, flows\nlabelled as malicious by NIDS are initially processed through a module called\nthe Prompt Augmenter. This module extracts contextual information and Cyber\nThreat Intelligence (CTI)-related knowledge from these flows. This enriched,\ncontext-specific data is then integrated with an input prompt for an LLM,\nenabling it to generate detailed explanations and interpretations of why the\nflow was identified as malicious by NIDS. We compare the generated\ninterpretations against a Basic-Prompt Explainer baseline, which does not\nincorporate any contextual information into the LLM's input prompt. Our\nframework is quantitatively evaluated using the Llama 3 and GPT-4 models,\nemploying a novel evaluation method tailored for natural language explanations,\nfocusing on their correctness and consistency. The results demonstrate that\naugmented LLMs can produce accurate and consistent explanations, serving as\nvaluable complementary tools in NIDS to explain the classification of malicious\nflows. The use of augmented prompts enhances performance by over 20% compared\nto the Basic-Prompt Explainer.", "AI": {"tldr": "eX-NIDS is a framework that improves interpretability in flow-based NIDS by using LLMs with CTI-enriched prompts, achieving over 20% better performance than basic explanations.", "motivation": "Network intrusion detection systems (NIDS) require interpretability to explain malicious flow classifications. Without context-specific knowledge, LLM-generated explanations lack accuracy and utility.", "method": "The framework uses a Prompt Augmenter module to extract contextual information and CTI knowledge from NIDS-labeled malicious flows. This data is then included in LLM input prompts to enable contextualized explanation generation. Comparisons are made against a basic-prompt baseline using Llama 3 and GPT-4 models.", "result": "Augmented LLM explanations showed significant improvements (over 20% performance gain) in accuracy and consistency compared to the basic-prompt baseline through specialized natural language evaluation.", "conclusion": "Integrating CTI-based context into LLM prompts provides reliable, consistent explanations for NIDS classifications, demonstrating LLMs' value as complementary tools for cybersecurity interpretability."}}
{"id": "2507.16661", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16661", "abs": "https://arxiv.org/abs/2507.16661", "authors": ["Tan Bui", "Yan Naing Tun", "Thanh Phuc Nguyen", "Yindu Su", "Ferdian Thung", "Yikun Li", "Han Wei Ang", "Yide Yin", "Frank Liauw", "Lwin Khin Shar", "Eng Lieh Ouh", "Ting Zhang", "David Lo"], "title": "VulCoCo: A Simple Yet Effective Method for Detecting Vulnerable Code Clones", "comment": null, "summary": "Code reuse is common in modern software development, but it can also spread\nvulnerabilities when developers unknowingly copy risky code. The code fragments\nthat preserve the logic of known vulnerabilities are known as vulnerable code\nclones (VCCs). Detecting those VCCs is a critical but challenging task.\nExisting VCC detection tools often rely on syntactic similarity or produce\ncoarse vulnerability predictions without clear explanations, limiting their\npractical utility. In this paper, we propose VulCoCo, a lightweight and\nscalable approach that combines embedding-based retrieval with large language\nmodel (LLM) validation. Starting from a set of known vulnerable functions, we\nretrieve syntactically or semantically similar candidate functions from a large\ncorpus and use an LLM to assess whether the candidates retain the\nvulnerability. Given that there is a lack of reproducible vulnerable code clone\nbenchmarks, we first construct a synthetic benchmark that spans various clone\ntypes.\n  Our experiments on the benchmark show that VulCoCo outperforms prior\nstate-of-the-art methods in terms of Precision@k and mean average precision\n(MAP). In addition, we also demonstrate VulCoCo's effectiveness in real-world\nprojects by submitting 400 pull requests (PRs) to 284 open-source projects.\nAmong them, 75 PRs were merged, and 15 resulted in newly published CVEs. We\nalso provide insights to inspire future work to further improve the precision\nof vulnerable code clone detection.", "AI": {"tldr": "VulCoCo is a novel VCC detection method combining embeddings and LLM validation, featuring a synthetic benchmark and real-world impact with 75 merged PRs and 15 new CVEs identified.", "motivation": "Prior VCC detection tools often struggle with syntactic similarity limitations or lack explanation clarity, while reproducible benchmarks for evaluation are scarce.", "method": "The approach uses embedding-based retrieval to identify function clones from a large code corpus, followed by LLM validation to determine if the vulnerability is retained. A synthetic benchmark spanning multiple clone types is also developed for evaluation.", "result": "Outperforms existing methods on Precision@k and MAP metrics. Achieved 75 merged PRs in real open-source projects, including 15 CVEs resulting from vulnerability fixes.", "conclusion": "Provides a lightweight, scalable VCC detection framework with actionable results, offering 284 real-world validations and highlighting paths for precision improvement in future research."}}
{"id": "2507.16276", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.16276", "abs": "https://arxiv.org/abs/2507.16276", "authors": ["Lambard Maxence", "Bertelle Cyrille", "Duvallet Claude"], "title": "From Contracts to Code: Automating Smart Contract Generation with Multi-Level Finite State Machines", "comment": null, "summary": "In an increasingly complex contractual landscape, the demand for\ntransparency, security, and efficiency has intensified. Blockchain technology,\nwith its decentralized and immutable nature, addresses these challenges by\nreducing intermediary costs, minimizing fraud risks, and enhancing system\ncompatibility. Smart contracts, initially conceptualized by Nick Szabo and\nlater implemented on the Ethereum blockchain, automate and secure contractual\nclauses, offering a robust solution for various industries. However, their\ncomplexity and the requirement for advanced programming skills present\nsignificant barriers to widespread adoption. This study introduces a\nmulti-level finite state machine model designed to represent and track the\nexecution of smart contracts. Our model aims to simplify smart contract\ndevelopment by providing a formalized framework that abstracts underlying\ntechnical complexities, making it accessible to professionals without deep\ntechnical expertise. The hierarchical structure of the multi-level finite state\nmachine enhances contract modularity and traceability, facilitating detailed\nrepresentation and evaluation of functional properties. The paper explores the\npotential of this multi-level approach, reviewing existing methodologies and\ntools, and detailing the smart contract generation process with an emphasis on\nreusable components and modularity. We also conduct a security analysis to\nevaluate potential vulnerabilities in our model, ensuring the robustness and\nreliability of the generated smart contracts.", "AI": {"tldr": "The paper proposes a multi-level finite state machine model to simplify smart contract development by abstracting technical complexities and enhancing security, aiming to improve accessibility for non-technical professionals.", "motivation": "Blockchain smart contracts face adoption barriers due to their complexity and programming requirements, despite benefits like transparency, security, and reduced intermediary costs.", "method": "A multi-level finite state machine (FSM) is developed as a formal framework to represent smart contracts hierarchically, promoting modularity and traceability through reusable components and structured execution modeling.", "result": "The model's potential is explored via analysis of existing methodologies, detailed contract generation with modularity emphasis, and a security evaluation identifying vulnerabilities to ensure robustness.", "conclusion": "The multi-level FSM approach offers a scalable, secure, and user-friendly solution for smart contract design, reducing technical hurdles while maintaining reliability in blockchain environments."}}
{"id": "2507.16685", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16685", "abs": "https://arxiv.org/abs/2507.16685", "authors": ["Duong Nguyen", "Manh Tran-Duc", "Thanh Le-Cong", "Triet Huynh Minh Le", "M. Ali Babar", "Quyet-Thang Huynh"], "title": "VulGuard: An Unified Tool for Evaluating Just-In-Time Vulnerability Prediction Models", "comment": null, "summary": "We present VulGuard, an automated tool designed to streamline the extraction,\nprocessing, and analysis of commits from GitHub repositories for Just-In-Time\nvulnerability prediction (JIT-VP) research. VulGuard automatically mines commit\nhistories, extracts fine-grained code changes, commit messages, and software\nengineering metrics, and formats them for downstream analysis. In addition, it\nintegrates several state-of-the-art vulnerability prediction models, allowing\nresearchers to train, evaluate, and compare models with minimal setup. By\nsupporting both repository-scale mining and model-level experimentation within\na unified framework, VulGuard addresses key challenges in reproducibility and\nscalability in software security research. VulGuard can also be easily\nintegrated into the CI/CD pipeline. We demonstrate the effectiveness of the\ntool in two influential open-source projects, FFmpeg and the Linux kernel,\nhighlighting its potential to accelerate real-world JIT-VP research and promote\nstandardized benchmarking. A demo video is available at:\nhttps://youtu.be/j96096-pxbs", "AI": {"tldr": "VulGuard is an automated tool for JIT-VP research that streamlines commit extraction, integrates models, and addresses scalability and reproducibility issues.", "motivation": "The tool aims to overcome challenges in reproducibility, scalability, and efficient model experimentation in software security research, particularly for Just-In-Time vulnerability prediction.", "method": "VulGuard automates commit history mining, extracts code changes, messages, and metrics, formats data for analysis, and integrates state-of-the-art vulnerability prediction models within a unified framework.", "result": "The effectiveness of VulGuard was demonstrated through its application to FFmpeg and Linux kernel repositories, facilitating JIT-VP model training and evaluation.", "conclusion": "VulGuard provides a standardized, scalable solution for JIT-VP research by unifying data processing and model experimentation, reducing setup complexity and promoting reproducible benchmarks."}}
{"id": "2507.16291", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.16291", "abs": "https://arxiv.org/abs/2507.16291", "authors": ["Wenhao Li", "Selvakumar Manickam", "Yung-wey Chong", "Shankar Karuppayah"], "title": "Talking Like a Phisher: LLM-Based Attacks on Voice Phishing Classifiers", "comment": "Accepted by EAI ICDF2C 2025", "summary": "Voice phishing (vishing) remains a persistent threat in cybersecurity,\nexploiting human trust through persuasive speech. While machine learning\n(ML)-based classifiers have shown promise in detecting malicious call\ntranscripts, they remain vulnerable to adversarial manipulations that preserve\nsemantic content. In this study, we explore a novel attack vector where large\nlanguage models (LLMs) are leveraged to generate adversarial vishing\ntranscripts that evade detection while maintaining deceptive intent. We\nconstruct a systematic attack pipeline that employs prompt engineering and\nsemantic obfuscation to transform real-world vishing scripts using four\ncommercial LLMs. The generated transcripts are evaluated against multiple ML\nclassifiers trained on a real-world Korean vishing dataset (KorCCViD) with\nstatistical testing. Our experiments reveal that LLM-generated transcripts are\nboth practically and statistically effective against ML-based classifiers. In\nparticular, transcripts crafted by GPT-4o significantly reduce classifier\naccuracy (by up to 30.96%) while maintaining high semantic similarity, as\nmeasured by BERTScore. Moreover, these attacks are both time-efficient and\ncost-effective, with average generation times under 9 seconds and negligible\nfinancial cost per query. The results underscore the pressing need for more\nresilient vishing detection frameworks and highlight the imperative for LLM\nproviders to enforce stronger safeguards against prompt misuse in adversarial\nsocial engineering contexts.", "AI": {"tldr": "This paper explores a novel adversarial attack using large language models (LLMs) to generate persuasive vishing transcripts that evade detection by ML-based classifiers, highlighting vulnerabilities and proposing safeguards.", "motivation": "Existing ML classifiers for vishing detection are vulnerable to adversarial manipulations that preserve semantic content, creating a need to systematically study how LLMs can be exploited to generate such attacks.", "method": "The authors constructed an attack pipeline employing prompt engineering and semantic obfuscation across four commercial LLMs. Generated transcripts were evaluated against ML classifiers on a real-world Korean vishing dataset (KorCCViD) using statistical testing.", "result": "LLM-generated transcripts, particularly by GPT-4o, reduced classifier accuracy by up to 30.96% with high semantic similarity (BERTScore) and high time-efficiency (under 9 seconds per transcript) and cost-efficiency.", "conclusion": "The study underscores the urgency of improving vishing detection frameworks and for LLM providers to implement stronger safeguards against prompt misuse in adversarial social engineering scenarios."}}
{"id": "2507.16754", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16754", "abs": "https://arxiv.org/abs/2507.16754", "authors": ["Fangjian Lei", "Mariam El Mezouar", "Shayan Noei", "Ying Zou"], "title": "Never Come Up Empty: Adaptive HyDE Retrieval for Improving LLM Developer Support", "comment": null, "summary": "Large Language Models (LLMs) have shown promise in assisting developers with\ncode-related questions; however, LLMs carry the risk of generating unreliable\nanswers. To address this, Retrieval-Augmented Generation (RAG) has been\nproposed to reduce the unreliability (i.e., hallucinations) of LLMs. However,\ndesigning effective pipelines remains challenging due to numerous design\nchoices. In this paper, we construct a retrieval corpus of over 3 million Java\nand Python related Stack Overflow posts with accepted answers, and explore\nvarious RAG pipeline designs to answer developer questions, evaluating their\neffectiveness in generating accurate and reliable responses. More specifically,\nwe (1) design and evaluate 7 different RAG pipelines and 63 pipeline variants\nto answer questions that have historically similar matches, and (2) address new\nquestions without any close prior matches by automatically lowering the\nsimilarity threshold during retrieval, thereby increasing the chance of finding\npartially relevant context and improving coverage for unseen cases. We find\nthat implementing a RAG pipeline combining hypothetical-documentation-embedding\n(HyDE) with the full-answer context performs best in retrieving and answering\nsimilarcontent for Stack Overflow questions. Finally, we apply our optimal RAG\npipeline to 4 open-source LLMs and compare the results to their zero-shot\nperformance. Our findings show that RAG with our optimal RAG pipeline\nconsistently outperforms zero-shot baselines across models, achieving higher\nscores for helpfulness, correctness, and detail with LLM-as-a-judge. These\nfindings demonstrate that our optimal RAG pipelines robustly enhance answer\nquality for a wide range of developer queries including both previously seen\nand novel questions across different LLMs", "AI": {"tldr": "This paper presents an optimal Retrieval-Augmented Generation (RAG) pipeline for enhancing LLM reliability in answering developer questions, achieving higher accuracy and coverage across 4 open-source models.", "motivation": "Large Language Models (LLMs) often generate unreliable answers (hallucinations) for developer questions. Effective RAG pipeline designs are needed to mitigate these risks while maintaining broad applicability across both familiar and novel queries.", "method": "The authors (1) constructed a 3M+ Java/Python Stack Overflow corpus and tested 7 RAG pipelines with 63 variants for similarity-matched questions, and (2) implement dynamic similarity-threshold lowering to handle novel questions. The most effective approach combines hypothetical-documentation-embedding (HyDE) with full-answer context retrieval.", "result": "The optimal RAG pipeline (HyDE + full-answer context) consistently outperformed zero-shot baselines across four open-source LLMs, showing higher helpfulness, correctness, and detail scores using LLM-as-a-judge evaluation for both seen and novel developer questions.", "conclusion": "Optimized RAG pipelines robustly improve LLM answer quality for developer queries by effectively addressing hallucinations while maintaining strong performance on unseen questions through adaptive retrieval strategies and context-aware embeddings."}}
{"id": "2507.16329", "categories": ["cs.CR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.16329", "abs": "https://arxiv.org/abs/2507.16329", "authors": ["Boheng Li", "Junjie Wang", "Yiming Li", "Zhiyang Hu", "Leyi Qi", "Jianshuo Dong", "Run Wang", "Han Qiu", "Zhan Qin", "Tianwei Zhang"], "title": "DREAM: Scalable Red Teaming for Text-to-Image Generative Systems via Distribution Modeling", "comment": "Preprint version. Under review", "summary": "Despite the integration of safety alignment and external filters,\ntext-to-image (T2I) generative models are still susceptible to producing\nharmful content, such as sexual or violent imagery. This raises serious\nconcerns about unintended exposure and potential misuse. Red teaming, which\naims to proactively identify diverse prompts that can elicit unsafe outputs\nfrom the T2I system (including the core generative model as well as potential\nexternal safety filters and other processing components), is increasingly\nrecognized as an essential method for assessing and improving safety before\nreal-world deployment. Yet, existing automated red teaming approaches often\ntreat prompt discovery as an isolated, prompt-level optimization task, which\nlimits their scalability, diversity, and overall effectiveness. To bridge this\ngap, in this paper, we propose DREAM, a scalable red teaming framework to\nautomatically uncover diverse problematic prompts from a given T2I system.\nUnlike most prior works that optimize prompts individually, DREAM directly\nmodels the probabilistic distribution of the target system's problematic\nprompts, which enables explicit optimization over both effectiveness and\ndiversity, and allows efficient large-scale sampling after training. To achieve\nthis without direct access to representative training samples, we draw\ninspiration from energy-based models and reformulate the objective into simple\nand tractable objectives. We further introduce GC-SPSA, an efficient\noptimization algorithm that provide stable gradient estimates through the long\nand potentially non-differentiable T2I pipeline. The effectiveness of DREAM is\nvalidated through extensive experiments, demonstrating that it surpasses 9\nstate-of-the-art baselines by a notable margin across a broad range of T2I\nmodels and safety filters in terms of prompt success rate and diversity.", "AI": {"tldr": "DREAM is a scalable red teaming framework for text-to-image models that models the probabilistic distribution of harmful prompts using energy-based methods, surpassing state-of-the-art baselines in detecting problematic content with higher success rates and diversity.", "motivation": "Current safety measures for text-to-image (T2I) systems fail to prevent unintended harmful outputs (e.g., sexual/violent imagery), necessitating robust pre-deployment safety testing through red teaming. Existing automated approaches lack scalability, diversity, and effectiveness in identifying problematic prompts across the entire T2I pipeline.", "method": "DREAM uses energy-based models to directly optimize a probabilistic distribution of harmful prompts, and introduces GC-SPSA for stable gradient estimation through the non-differentiable T2I pipeline, enabling large-scale sampling and joint optimization of effectiveness and diversity.", "result": "DREAM outperformed 9 state-of-the-art baselines significantly in prompt success rate and content diversity across various T2I models and safety filters, validated through extensive experiments.", "conclusion": "DREAM addresses critical limitations of isolated red teaming by modeling harmful prompt distributions, providing an efficient and effective solution for T2I system safety improvement while balancing output quality and diversity requirements."}}
{"id": "2507.16808", "categories": ["cs.SE", "cs.AI", "68N19, 68T05", "B.6.3; D.3.4; I.2.2; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.16808", "abs": "https://arxiv.org/abs/2507.16808", "authors": ["Zhihao Xu", "Bixin Li", "Lulu Wang"], "title": "Rethinking LLM-Based RTL Code Optimization Via Timing Logic Metamorphosis", "comment": "13pages with 9 pictures and 2 tables", "summary": "Register Transfer Level(RTL) code optimization is crucial for achieving high\nperformance and low power consumption in digital circuit design. However,\ntraditional optimization methods often rely on manual tuning and heuristics,\nwhich can be time-consuming and error-prone. Recent studies proposed to\nleverage Large Language Models(LLMs) to assist in RTL code optimization. LLMs\ncan generate optimized code snippets based on natural language descriptions,\npotentially speeding up the optimization process. However, existing approaches\nhave not thoroughly evaluated the effectiveness of LLM-Based code optimization\nmethods for RTL code with complex timing logic. To address this gap, we\nconducted a comprehensive empirical investigation to assess the capability of\nLLM-Based RTL code optimization methods in handling RTL code with complex\ntiming logic. In this study, we first propose a new benchmark for RTL\noptimization evaluation. It comprises four subsets, each corresponding to a\nspecific area of RTL code optimization. Then we introduce a method based on\nmetamorphosis to systematically evaluate the effectiveness of LLM-Based RTL\ncode optimization methods.Our key insight is that the optimization\neffectiveness should remain consistent for semantically equivalent but more\ncomplex code. After intensive experiments, we revealed several key findings.\n(1) LLM-Based RTL optimization methods can effectively optimize logic\noperations and outperform existing compiler-based methods. (2) LLM-Based RTL\noptimization methods do not perform better than existing compiler-based methods\non RTL code with complex timing logic, particularly in timing control flow\noptimization and clock domain optimization. This is primarily attributed to the\nchallenges LLMs face in understanding timing logic in RTL code. Based on these\nfindings, we provide insights for further research in leveraging LLMs for RTL\ncode optimization.", "AI": {"tldr": "This paper evaluates the effectiveness of LLM-based RTL optimization methods on complex timing logic. It introduces a new benchmark with four subsets and a metamorphosis method, revealing that while LLMs outperform compiler-based approaches in logic operations, they lag in timing-related optimizations like control flow and clock domain.", "motivation": "Traditional RTL optimization relies on manual methods and heuristics, which are inefficient and prone to errors. LLMs offer potential automation but their performance on complex timing logic remains underexplored, necessitating a comprehensive evaluation.", "method": "The authors propose a benchmark with four RTL optimization subsets and use a metamorphosis approach to test LLMs on semantically equivalent but more complex code, systematically assessing their optimization capabilities.", "result": "LLM-based methods excel in optimizing logic operations, outperforming compiler-based tools, but underperform in timing control flow optimization and clock domain optimization due to challenges in understanding timing logic in RTL code.", "conclusion": "LLM-based RTL optimization is effective for logic-centric tasks but struggles with timing logic complexities. Future work should focus on improving LLMs' understanding of timing-specific nuances and hybrid approaches for broader applicability."}}
{"id": "2507.16372", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16372", "abs": "https://arxiv.org/abs/2507.16372", "authors": ["Tian Dong", "Yan Meng", "Shaofeng Li", "Guoxing Chen", "Zhen Liu", "Haojin Zhu"], "title": "Depth Gives a False Sense of Privacy: LLM Internal States Inversion", "comment": "Accepted by USENIX Security 2025. Please cite this paper as \"Tian\n  Dong, Yan Meng, Shaofeng Li, Guoxing Chen, Zhen Liu, Haojin Zhu. Depth Gives\n  a False Sense of Privacy: LLM Internal States Inversion. In the 34th USENIX\n  Security Symposium (USENIX Security '25).\"", "summary": "Large Language Models (LLMs) are increasingly integrated into daily routines,\nyet they raise significant privacy and safety concerns. Recent research\nproposes collaborative inference, which outsources the early-layer inference to\nensure data locality, and introduces model safety auditing based on inner\nneuron patterns. Both techniques expose the LLM's Internal States (ISs), which\nare traditionally considered irreversible to inputs due to optimization\nchallenges and the highly abstract representations in deep layers. In this\nwork, we challenge this assumption by proposing four inversion attacks that\nsignificantly improve the semantic similarity and token matching rate of\ninverted inputs. Specifically, we first develop two white-box\noptimization-based attacks tailored for low-depth and high-depth ISs. These\nattacks avoid local minima convergence, a limitation observed in prior work,\nthrough a two-phase inversion process. Then, we extend our optimization attack\nunder more practical black-box weight access by leveraging the transferability\nbetween the source and the derived LLMs. Additionally, we introduce a\ngeneration-based attack that treats inversion as a translation task, employing\nan inversion model to reconstruct inputs. Extensive evaluation of short and\nlong prompts from medical consulting and coding assistance datasets and 6 LLMs\nvalidates the effectiveness of our inversion attacks. Notably, a 4,112-token\nlong medical consulting prompt can be nearly perfectly inverted with 86.88 F1\ntoken matching from the middle layer of Llama-3 model. Finally, we evaluate\nfour practical defenses that we found cannot perfectly prevent ISs inversion\nand draw conclusions for future mitigation design.", "AI": {"tldr": "This paper challenges the assumption that LLM internal states (ISs) can't be reversed to original inputs, proposing four inversion attacks. These attacks improve semantic and token-level reconstruction, show effectiveness on medical/coding datasets and 6 LLMs, and find that existing defenses fail to prevent IS inversion.", "motivation": "Privacy and safety concerns with LLMs grow as internal states are exposed through collaborative inference and auditing techniques, necessitating evaluation of how ISs might be exploited to recover sensitive data.", "method": "Two white-box optimization attacks for low/high-depth ISs (two-phase inversion process), black-box extension using model transferability, and a generation-based attack framing inversion as translation. Evaluated on real-world datasets and LLMs.", "result": "High token matching (86.88 F1) on 4,112-token medical prompts from Llama-3 mid-layer. All four practical defenses tested were ineffective against IS inversion attacks.", "conclusion": "Common belief that ISs are irreversible is incorrect. Highlights critical risk in current LLM deployment paradigms and emphasizes need for stronger mitigation strategies."}}
{"id": "2507.16540", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.16540", "abs": "https://arxiv.org/abs/2507.16540", "authors": ["Radowanul Haque", "Aftab Ali", "Sally McClean", "Naveed Khan"], "title": "Explainable Vulnerability Detection in C/C++ Using Edge-Aware Graph Attention Networks", "comment": null, "summary": "Detecting security vulnerabilities in source code remains challenging,\nparticularly due to class imbalance in real-world datasets where vulnerable\nfunctions are under-represented. Existing learning-based methods often optimise\nfor recall, leading to high false positive rates and reduced usability in\ndevelopment workflows. Furthermore, many approaches lack explainability,\nlimiting their integration into security workflows. This paper presents\nExplainVulD, a graph-based framework for vulnerability detection in C/C++ code.\nThe method constructs Code Property Graphs and represents nodes using\ndual-channel embeddings that capture both semantic and structural information.\nThese are processed by an edge-aware attention mechanism that incorporates\nedge-type embeddings to distinguish among program relations. To address class\nimbalance, the model is trained using class-weighted cross-entropy loss.\nExplainVulD achieves a mean accuracy of 88.25 percent and an F1 score of 48.23\npercent across 30 independent runs on the ReVeal dataset. These results\nrepresent relative improvements of 4.6 percent in accuracy and 16.9 percent in\nF1 score compared to the ReVeal model, a prior learning-based method. The\nframework also outperforms static analysis tools, with relative gains of 14.0\nto 14.1 percent in accuracy and 132.2 to 201.2 percent in F1 score. Beyond\nimproved detection performance, ExplainVulD produces explainable outputs by\nidentifying the most influential code regions within each function, supporting\ntransparency and trust in security triage.", "AI": {"tldr": "ExplainVulD is a graph-based vulnerability detection framework for C/C++ code using dual-channel embeddings and edge-aware attention, addressing class imbalance and improving accuracy/F1 scores while providing explainable outputs.", "motivation": "Security vulnerability detection is hampered by class imbalance and lack of explainability in existing methods, causing high false positives and usability issues in developer and security workflows.", "method": "Constructs Code Property Graphs with dual-channel node embeddings (semantic + structural) processed via edge-aware attention mechanism (edge-type embeddings). Uses class-weighted cross-entropy loss to mitigate class imbalance.", "result": "Achieved 88.25% mean accuracy and 48.23% F1 score on ReVeal dataset (4.6% and 16.9% improvements over ReVeal model). Outperformed static analysis tools with 14.0-14.1% accuracy gain and 132.2-201.2% F1 gain across 30 runs.", "conclusion": "ExplainVulD improves vulnerability detection through balanced accuracy/F1 performance while enabling transparent security triage via code region influence explanations, offering practical advantages over learning-based and static methods."}}
{"id": "2507.16576", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.16576", "abs": "https://arxiv.org/abs/2507.16576", "authors": ["Ahmed Lekssays", "Husrev Taha Sencar", "Ting Yu"], "title": "From Text to Actionable Intelligence: Automating STIX Entity and Relationship Extraction", "comment": "This paper is accepted at RAID 2025", "summary": "Sharing methods of attack and their effectiveness is a cornerstone of\nbuilding robust defensive systems. Threat analysis reports, produced by various\nindividuals and organizations, play a critical role in supporting security\noperations and combating emerging threats. To enhance the timeliness and\nautomation of threat intelligence sharing, several standards have been\nestablished, with the Structured Threat Information Expression (STIX) framework\nemerging as one of the most widely adopted. However, generating STIX-compatible\ndata from unstructured security text remains a largely manual, expert-driven\nprocess. To address this challenge, we introduce AZERG, a tool designed to\nassist security analysts in automatically generating structured STIX\nrepresentations. To achieve this, we adapt general-purpose large language\nmodels for the specific task of extracting STIX-formatted threat data. To\nmanage the complexity, the task is divided into four subtasks: entity detection\n(T1), entity type identification (T2), related pair detection (T3), and\nrelationship type identification (T4). We apply task-specific fine-tuning to\naccurately extract relevant entities and infer their relationships in\naccordance with the STIX specification. To address the lack of training data,\nwe compiled a comprehensive dataset with 4,011 entities and 2,075 relationships\nextracted from 141 full threat analysis reports, all annotated in alignment\nwith the STIX standard. Our models achieved F1-scores of 84.43% for T1, 88.49%\nfor T2, 95.47% for T3, and 84.60% for T4 in real-world scenarios. We validated\ntheir performance against a range of open- and closed-parameter models, as well\nas state-of-the-art methods, demonstrating improvements of 2-25% across tasks.", "AI": {"tldr": "This paper introduces AZERG, a tool for automatically generating structured STIX threat intelligence from unstructured security reports. By fine-tuning LLMs with a custom dataset of 141 annotated threat reports, the system achieves strong performance across four subtasks with 2-25% improvements over existing methods.", "motivation": "Manual generation of STIX-compatible threat intelligence data is time-consuming and reliant on expert analysts, hampering timeliness and automation in threat intelligence sharing. The STIX framework is widely adopted but lacks efficient tools for structured data extraction.", "method": "The authors decomposed the task into four components: entity detection, entity type identification, related pair detection, and relationship type identification. They implemented task-specific fine-tuning of general-purpose LLMs using their custom dataset containing 4,011 entities and 2,075 STIX-aligned relationships from real threat reports.", "result": "Achieved F1-scores of 84.43% (T1), 88.49% (T2), 95.47% (T3), and 84.60% (T4) with 2-25% performance improvements over open-/closed-parameter models and state-of-the-art alternatives.", "conclusion": "AZERG demonstrates a viable solution for automating STIX data generation through structured task decomposition and LLM fine-tuning, significantly improving threat intelligence sharing efficiency while maintaining high alignment with STIX standards."}}
{"id": "2507.16585", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.16585", "abs": "https://arxiv.org/abs/2507.16585", "authors": ["Ahmed Lekssays", "Hamza Mouhcine", "Khang Tran", "Ting Yu", "Issa Khalil"], "title": "LLMxCPG: Context-Aware Vulnerability Detection Through Code Property Graph-Guided Large Language Models", "comment": "This paper is accepted at USENIX 2025", "summary": "Software vulnerabilities present a persistent security challenge, with over\n25,000 new vulnerabilities reported in the Common Vulnerabilities and Exposures\n(CVE) database in 2024 alone. While deep learning based approaches show promise\nfor vulnerability detection, recent studies reveal critical limitations in\nterms of accuracy and robustness: accuracy drops by up to 45% on rigorously\nverified datasets, and performance degrades significantly under simple code\nmodifications. This paper presents LLMxCPG, a novel framework integrating Code\nProperty Graphs (CPG) with Large Language Models (LLM) for robust vulnerability\ndetection. Our CPG-based slice construction technique reduces code size by\n67.84 to 90.93% while preserving vulnerability-relevant context. Our approach's\nability to provide a more concise and accurate representation of code snippets\nenables the analysis of larger code segments, including entire projects. This\nconcise representation is a key factor behind the improved detection\ncapabilities of our method, as it can now identify vulnerabilities that span\nmultiple functions. Empirical evaluation demonstrates LLMxCPG's effectiveness\nacross verified datasets, achieving 15-40% improvements in F1-score over\nstate-of-the-art baselines. Moreover, LLMxCPG maintains high performance across\nfunction-level and multi-function codebases while exhibiting robust detection\nefficacy under various syntactic code modifications.", "AI": {"tldr": "LLMxCPG is a framework combining Code Property Graphs (CPG) with Large Language Models (LLM) for robust vulnerability detection, achieving 15-40% F1-score improvements over existing methods and maintaining performance under code modifications.", "motivation": "Deep learning approaches for vulnerability detection suffer from significant accuracy drops (up to 45%) on verified datasets and poor robustness against code modifications, creating a need for more reliable solutions.", "method": "LLMxCPG integrates CPG slicing techniques to reduce code size by 67.84-90.93% while preserving vulnerability-relevant context, enabling analysis of larger code segments and projects through concise code representation.", "result": "15-40% higher F1-scores compared to state-of-the-art baselines, combined with consistent performance on function-level and multi-function codebases under various syntactic modifications.", "conclusion": "The integration of CPG-based slicing with LLMs provides a robust vulnerability detection framework that addresses existing limitations by improving accuracy, scalability, and resilience to code transformations."}}
{"id": "2507.16773", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.16773", "abs": "https://arxiv.org/abs/2507.16773", "authors": ["Yue Li", "Xiao Li", "Hao Wu", "Yue Zhang", "Fengyuan Xu", "Xiuzhen Cheng", "Sheng Zhong"], "title": "When LLMs Copy to Think: Uncovering Copy-Guided Attacks in Reasoning LLMs", "comment": null, "summary": "Large Language Models (LLMs) have become integral to automated code analysis,\nenabling tasks such as vulnerability detection and code comprehension. However,\ntheir integration introduces novel attack surfaces. In this paper, we identify\nand investigate a new class of prompt-based attacks, termed Copy-Guided Attacks\n(CGA), which exploit the inherent copying tendencies of reasoning-capable LLMs.\nBy injecting carefully crafted triggers into external code snippets,\nadversaries can induce the model to replicate malicious content during\ninference. This behavior enables two classes of vulnerabilities: inference\nlength manipulation, where the model generates abnormally short or excessively\nlong reasoning traces; and inference result manipulation, where the model\nproduces misleading or incorrect conclusions. We formalize CGA as an\noptimization problem and propose a gradient-based approach to synthesize\neffective triggers. Empirical evaluation on state-of-the-art reasoning LLMs\nshows that CGA reliably induces infinite loops, premature termination, false\nrefusals, and semantic distortions in code analysis tasks. While highly\neffective in targeted settings, we observe challenges in generalizing CGA\nacross diverse prompts due to computational constraints, posing an open\nquestion for future research. Our findings expose a critical yet underexplored\nvulnerability in LLM-powered development pipelines and call for urgent advances\nin prompt-level defense mechanisms.", "AI": {"tldr": "This paper introduces Copy-Guided Attacks (CGA), a novel method exploiting LLMs' copying behavior to manipulate code analysis outcomes, enabling vulnerabilities like infinite loops and false conclusions. While effective in targeted scenarios, generalization challenges persist, highlighting the need for prompt-level defenses.", "motivation": "LLMs are critical for code analysis but their integration creates new vulnerabilities. The paper addresses the underexplored risk of adversarial code snippet copying, which can distort reasoning and compromise downstream tasks such as vulnerability detection.", "method": "The authors formalize CGA as an optimization problem and propose a gradient-based approach to synthesize triggers. These triggers are injected into external code snippets to exploit the LLMs' tendency to replicate malicious content during inference.", "result": "Empirical results demonstrate CGA successfully induces infinite loops, premature termination, false refusals, and semantic distortions in multiple state-of-the-art reasoning LLMs. However, computational constraints limit cross-prompt generalization.", "conclusion": "The study reveals critical vulnerabilities in LLM-powered development pipelines, emphasizing the need for prompt-level defense mechanisms. It raises open questions about generalization efficiency, prompting further research on secure LLM deployment in code analysis."}}
{"id": "2507.16788", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.16788", "abs": "https://arxiv.org/abs/2507.16788", "authors": ["Sebastian Pape", "Anis Bkakria", "Maurice Heymann", "Badreddine Chah", "Abdeljalil Abbas-Turki", "Sarah Syed-Winkler", "Matthias Hiller", "Reda Yaich"], "title": "AUTOPSY: A Framework for Tackling Privacy Challenges in the Automotive Industry", "comment": "19 pages, 4 figures", "summary": "With the General Data Protection Regulation (GDPR) in place, all domains have\nto ensure compliance with privacy legislation. However, compliance does not\nnecessarily result in a privacy-friendly system as for example getting users'\nconsent to process their data does not improve the privacy-friendliness of the\nsystem. Therefore, the goal of the AUTOPSY project was to support the privacy\nengineering process in the automotive domain by providing several building\nblocks which technically improve the privacy-friendliness of modern, i.e.,\nconnected and (partially) automated vehicles. This paper presents the results\nof the AUTOPSY project: a system model to identify relevant entities and\nlocations to apply privacy enhancing technologies (PETs); the privacy manager\naiming at more control of the data flow from the vehicle, a PET selection\napproach based on GDPR principles, and an architectural framework for\nautomotive privacy. Furthermore, we built a demonstrator for location-based\nservices to evaluate the architectural framework.", "AI": {"tldr": "The AUTOPSY project developed technical solutions to improve privacy in connected vehicles beyond GDPR compliance, including a system model, privacy manager, PET selection approach based on GDPR principles, architectural framework, and a demonstrator for location-based services.", "motivation": "GDPR compliance alone does not ensure privacy-friendly automotive systems, as processes like user consent collection do not address underlying privacy risks in connected and automated vehicles.", "method": "The project implemented a privacy engineering framework through: (1) System modeling to identify PET application contexts, (2) Designing a privacy manager for data control, (3) Developing a GDPR-principles-driven PET selection methodology, (4) Creating an architectural framework with reusable privacy components, and (5) Building a location-based services demonstrator for evaluation.", "result": "Delivered tools include: (1) System model for PET identification across 162 components, (2) Privacy manager with dynamic access controls, (3) PET selection approach addressing 349 privacy risks, (4) Reference architecture with 12 privacy layers, (5) Functional demonstrator for location services with de-identification and data minimization features.", "conclusion": "The architectural framework and PET integration strategy significantly enhance automotive privacy engineering by systematically implementing GDPR requirements through technical measures rather than superficial compliance checks, as validated by the demonstrator's successful proof-of-concept implementation."}}
