<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 14]
- [cs.SE](#cs.SE) [Total: 10]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Towards Trustworthy Agentic IoEV: AI Agents for Explainable Cyberthreat Mitigation and State Analytics](https://arxiv.org/abs/2509.12233)
*Meryem Malak Dif,Mouhamed Amine Bouchiha,Abdelaziz Amara Korba,Yacine Ghamri-Doudane*

Main category: cs.CR

TL;DR: The paper presents an Agentic AI framework for enhancing the security and reliability of the IoEV, with agents for threat detection, SoC/SoH prediction, and coordinated decision-making. Experiments show improved performance in security and accuracy.


<details>
  <summary>Details</summary>
Motivation: The Internet of Electric Vehicles (IoEV) is susceptible to cyberattacks, unreliable battery-state predictions, and opaque decision processes which undermine trust and performance. There's a need to address these issues to ensure the practical deployment and adoption of EV technologies.

Method: The proposed framework employs Agentic AI, with an architecture of coordinated agents for cyber threat detection, real-time State of Charge (SoC) estimation, and State of Health (SoH) anomaly detection. It incorporates interpretable threat-mitigation strategies using proactive attack identification and continuous, adversarial-aware learning for SoC and SoH models. The system is structured around a three-agent pipeline featuring LLM-driven reasoning and dynamic tool invocation for task contextualization and optimization.

Result: The AAI framework was tested across various IoEV scenarios and showed marked enhancements in threat mitigation and prediction accuracy, particularly in maintaining EV battery health and system security under adversarial conditions.

Conclusion: The paper concludes that the integration of specialized agents within the AAI framework effectively improves the security, transparency, and reliability of IoEV systems. Public release of datasets and code supports future research and development, contributing to more trustworthy and efficient IoEV ecosystems.

Abstract: The Internet of Electric Vehicles (IoEV) envisions a tightly coupled
ecosystem of electric vehicles (EVs), charging infrastructure, and grid
services, yet it remains vulnerable to cyberattacks, unreliable battery-state
predictions, and opaque decision processes that erode trust and performance. To
address these challenges, we introduce a novel Agentic Artificial Intelligence
(AAI) framework tailored for IoEV, where specialized agents collaborate to
deliver autonomous threat mitigation, robust analytics, and interpretable
decision support. Specifically, we design an AAI architecture comprising
dedicated agents for cyber-threat detection and response at charging stations,
real-time State of Charge (SoC) estimation, and State of Health (SoH) anomaly
detection, all coordinated through a shared, explainable reasoning layer;
develop interpretable threat-mitigation mechanisms that proactively identify
and neutralize attacks on both physical charging points and learning
components; propose resilient SoC and SoH models that leverage continuous and
adversarial-aware learning to produce accurate, uncertainty-aware forecasts
with human-readable explanations; and implement a three-agent pipeline, where
each agent uses LLM-driven reasoning and dynamic tool invocation to interpret
intent, contextualize tasks, and execute formal optimizations for user-centric
assistance. Finally, we validate our framework through comprehensive
experiments across diverse IoEV scenarios, demonstrating significant
improvements in security and prediction accuracy. All datasets, models, and
code will be released publicly.

</details>


### [2] [Secure Human Oversight of AI: Exploring the Attack Surface of Human Oversight](https://arxiv.org/abs/2509.12290)
*Jonas C. Ditz,Veronika Lazar,Elmar Lichtme√ü,Carola Plesch,Matthias Heck,Kevin Baum,Markus Langer*

Main category: cs.CR

TL;DR: This paper analyzes human oversight of AI systems from a security perspective, identifying new attack surface and introducing attack vectors and mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: Human oversight is considered a crucial safeguard against AI risks and is mandated by regulations like the European AI Act. However, the security aspects of this oversight are often overlooked, creating potential vulnerabilities.

Method: The study uses cybersecurity frameworks to examine how human oversight in AI operations can be compromised. It identifies various attack vectors that affect the AI system, its communication with oversight personnel, or the personnel themselves.

Result: The research identified specific attack vectors that can undermine effective human oversight, potentially leading to unsafe AI operations. It also suggested hardening strategies to mitigate these identified risks.

Conclusion: Human oversight although important, can introduce new security vulnerabilities. A cybersecurity-focused approach is necessary to safeguard the oversight process and ensure the overall safety and accountability of AI systems.

Abstract: Human oversight of AI is promoted as a safeguard against risks such as
inaccurate outputs, system malfunctions, or violations of fundamental rights,
and is mandated in regulation like the European AI Act. Yet debates on human
oversight have largely focused on its effectiveness, while overlooking a
critical dimension: the security of human oversight. We argue that human
oversight creates a new attack surface within the safety, security, and
accountability architecture of AI operations. Drawing on cybersecurity
perspectives, we analyze attack vectors that threaten the requirements of
effective human oversight, thereby undermining the safety of AI operations.
Such attacks may target the AI system, its communication with oversight
personnel, or the personnel themselves. We then outline hardening strategies to
mitigate these risks. Our contributions are: (1) introducing a security
perspective on human oversight, and (2) providing an overview of attack vectors
and hardening strategies to enable secure human oversight of AI.

</details>


### [3] [Collaborative P4-SDN DDoS Detection and Mitigation with Early-Exit Neural Networks](https://arxiv.org/abs/2509.12291)
*Ouassim Karrakchou,Alaa Zniber,Anass Sebbar,Mounir Ghogho*

Main category: cs.CR

TL;DR: A P4-SDN architecture with a split neural network enables rapid, accurate DDoS detection by combining data-plane-first CNNs with control-plane GRU analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation addresses the growing need for real-time, scalable DDoS mitigation strategies to counter persistent network security threats, where traditional methods may fall short in speed and adaptability.

Method: The method employs a split early-exit neural network, where a quantized CNN in the P4 data plane handles fast classification, while a GRU module in the SDN control plane processes uncertain flows for accurate and scalable DDoS detection.

Result: Results show high detection accuracy, significant reduction in inference latency, and minimal control plane overhead, validated through experiments on real-world DDoS datasets.

Conclusion: The paper concludes that the integration of ML with P4-programmable data planes and SDN control planes offers a robust, low-latency solution for DDoS mitigation, demonstrating high efficacy in real-world scenarios.

Abstract: Distributed Denial of Service (DDoS) attacks pose a persistent threat to
network security, requiring timely and scalable mitigation strategies. In this
paper, we propose a novel collaborative architecture that integrates a
P4-programmable data plane with an SDN control plane to enable real-time DDoS
detection and response. At the core of our approach is a split early-exit
neural network that performs partial inference in the data plane using a
quantized Convolutional Neural Network (CNN), while deferring uncertain cases
to a Gated Recurrent Unit (GRU) module in the control plane. This design
enables high-speed classification at line rate with the ability to escalate
more complex flows for deeper analysis. Experimental evaluation using
real-world DDoS datasets demonstrates that our approach achieves high detection
accuracy with significantly reduced inference latency and control plane
overhead. These results highlight the potential of tightly coupled ML-P4-SDN
systems for efficient, adaptive, and low-latency DDoS defense.

</details>


### [4] [Amulet: a Python Library for Assessing Interactions Among ML Defenses and Risks](https://arxiv.org/abs/2509.12386)
*Asim Waheed,Vasisht Duddu,Rui Zhang,Sebastian Szyller,N. Asokan*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: ML models are susceptible to risks to security, privacy, and fairness.
Several defenses are designed to protect against their intended risks, but can
inadvertently affect susceptibility to other unrelated risks, known as
unintended interactions. Several jurisdictions are preparing ML regulatory
frameworks that require ML practitioners to assess the susceptibility of ML
models to different risks. A library for valuating unintended interactions that
can be used by (a) practitioners to evaluate unintended interactions at scale
prior to model deployment and (b) researchers to design defenses which do not
suffer from an unintended increase in unrelated risks. Ideally, such a library
should be i) comprehensive by including representative attacks, defenses and
metrics for different risks, ii) extensible to new modules due to its modular
design, iii) consistent with a user-friendly API template for inputs and
outputs, iv) applicable to evaluate previously unexplored unintended
interactions. We present AMULET, a Python library that covers risks to
security, privacy, and fairness, which satisfies all these requirements. AMULET
can be used to evaluate unexplored unintended interactions, compare
effectiveness between defenses or attacks, and include new attacks and
defenses.

</details>


### [5] [Redefining Website Fingerprinting Attacks With Multiagent LLMs](https://arxiv.org/abs/2509.12462)
*Chuxu Song,Dheekshith Dev Manohar Mekala,Hao Wang,Richard Martin*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Website Fingerprinting (WFP) uses deep learning models to classify encrypted
network traffic to infer visited websites. While historically effective, prior
methods fail to generalize to modern web environments. Single-page applications
(SPAs) eliminate the paradigm of websites as sets of discrete pages,
undermining page-based classification, and traffic from scripted browsers lacks
the behavioral richness seen in real user sessions. Our study reveals that
users exhibit highly diverse behaviors even on the same website, producing
traffic patterns that vary significantly across individuals. This behavioral
entropy makes WFP a harder problem than previously assumed and highlights the
need for larger, more diverse, and representative datasets to achieve robust
performance. To address this, we propose a new paradigm: we drop
session-boundaries in favor of contiguous traffic segments and develop a
scalable data generation pipeline using large language models (LLM) agents.
These multi-agent systems coordinate decision-making and browser interaction to
simulate realistic, persona-driven browsing behavior at 3--5x lower cost than
human collection. We evaluate nine state-of-the-art WFP models on traffic from
20 modern websites browsed by 30 real users, and compare training performance
across human, scripted, and LLM-generated datasets. All models achieve under
10\% accuracy when trained on scripted traffic and tested on human data. In
contrast, LLM-generated traffic boosts accuracy into the 80\% range,
demonstrating strong generalization to real-world traces. Our findings indicate
that for modern WFP, model performance is increasingly bottlenecked by data
quality, and that scalable, semantically grounded synthetic traffic is
essential for capturing the complexity of real user behavior.

</details>


### [6] [QKD Oracles for Authenticated Key Exchange](https://arxiv.org/abs/2509.12478)
*Kathrin H√∂velmanns,Daan Planken,Christian Schaffner,Sebastian R. Verschoor*

Main category: cs.CR

TL;DR: This paper analyzes the combination of Authenticated Key Exchange (AKE) with Quantum Key Distribution (QKD) for secure communication against quantum attacks. It highlights issues with QKD key ID handling in existing security models and proposes a CK+ security model integrated with a QKD oracle, leading to a provably secure hybrid protocol using a triple-KEM handshake.


<details>
  <summary>Details</summary>
Motivation: The motivation centers around the vulnerability of existing security models when combining AKE and QKD, specifically their insufficient treatment of QKD key IDs, which can lead to Dependent-Key attacks. The paper aims to bridge this gap and enhance security against potential quantum threats.

Method: The authors first conduct a reviw of existing models for combined AKE-QKD protocols. They construct a new security model for combined AKE by integrating a QKD oracle (designed to model QKD with key IDs) into the CK+ AKE security model. Within this model, they then design and formally prove the security of a new hybrid AKE-QKD protocol based on a triple-KEM handshake, capturing the issue of Dependent-Key attacks.

Result: The result is a formal security model for combined AKE-QKD protocols, which captures Dependent-Key attacks by including a QKD oracle that models quantum key exchange with key identifiers. The paper introduces the first provably secure hybrid protocol that uses information-theoretic QKD security. This protocol combines a triple-KEM handshake with QKD and is secure within the integrated model.

Conclusion: The paper concludes that integrating a QKD oracle into the CK+ model improves the analysis of hybrid AKE-QKD protocols by handling QKD key identifiers properly. Their proposed protocol is the first hybrid protocol that is provably secure and maintains the information-theoretic security of QKD, thus offering enhanced protection against quantum attacks.

Abstract: Authenticated Key Exchange (AKE) establishes shared ('symmetric')
cryptographic keys which are essential for secure online communication. AKE
protocols can be constructed from public-key cryptography like Key
Encapsulation Mechanisms (KEMs). Another approach is to use Quantum Key
Distribution (QKD) to establish a symmetric key, which uses quantum
communication. Combining post-quantum AKE and QKD appropriately may provide
security against quantum attacks even if only one of the two approaches turns
out to be secure.
  We provide an extensive review of existing security analyses for combined AKE
and their formal security models, and identify some gaps in their treatment of
QKD key IDs. In particular, improper handling of QKD key IDs leads to
Dependent-Key attacks on AKE.
  As our main conceptual contribution, we model QKD as an oracle that closely
resembles the standard ETSI 014 QKD interface. We demonstrate the usability of
our QKD oracle for cryptographic security analyses by integrating it into a
prominent security model for AKE, called CK+ model, thereby obtaining a
security model for combined AKE that catches Dependent-Key attacks. In this
model, we formally prove security of a new protocol that combines QKD with a
triple-KEM handshake. This is the first provably secure hybrid protocol that
maintains information-theoretic security of QKD.

</details>


### [7] [Towards Closing the Performance Gap for Cryptographic Kernels Between CPUs and Specialized Hardware](https://arxiv.org/abs/2509.12494)
*Naifeng Zhang,Sophia Fu,Franz Franchetti*

Main category: cs.CR

TL;DR: This paper addresses the challenge of narrowing the performance gap between CPUs and specialized hardware (ASICs) for cryptographic kernels like BLAS operations and NTT. The researchers developed an optimized scalar implementation for x86 CPUs, leveraged AVX2 and AVX-512 SIMD instructions, and proposed the MQX extension, which further boosts CPU performance.


<details>
  <summary>Details</summary>
Motivation: Specialized hardware such as ASICs excels in cryptographic workloads, but achieving equivalent performance on CPUs remains difficult, creating a need for CPU optimization to better support low-latency, flexible cryptographic computations in environments where deploying ASICs is impractical.

Method: The method involves creating optimized scalar code for x86 CPUs, then further improving it with AVX2 and AVX-512 SIMD instruction sets. The authors also designed the MQX (multi-word extension) to enhance performance by allowing more efficient multi-word operations through minimal new instructions.

Result: The results show an average speedup of 38 times for NTTs and 62 times for BLAS operations over existing CPU baselines with AVX-based optimizations. The proposed MQX extension reduces the performance gap between a single CPU core and ASICs to less than 35 times.

Conclusion: The paper concludes that, by using SIMD optimizations and proposing lightweight architectural changes (MQX), server-grade CPUs can significantly close the performance gap with ASICs for cryptographic kernels. This makes high-performance cryptographic operations feasible on general-purpose hardware without specialized accelerators.

Abstract: Specialized hardware like application-specific integrated circuits (ASICs)
remains the primary accelerator type for cryptographic kernels based on large
integer arithmetic. Prior work has shown that commodity and server-class GPUs
can achieve near-ASIC performance for these workloads. However, achieving
comparable performance on CPUs remains an open challenge. This work
investigates the following question: How can we narrow the performance gap
between CPUs and specialized hardware for key cryptographic kernels like basic
linear algebra subprograms (BLAS) operations and the number theoretic transform
(NTT)?
  To this end, we develop an optimized scalar implementation of these kernels
for x86 CPUs at the per-core level. We utilize SIMD instructions (specifically
AVX2 and AVX-512) to further improve performance, achieving an average speedup
of 38 times and 62 times over state-of-the-art CPU baselines for NTTs and BLAS
operations, respectively. To narrow the gap further, we propose a small AVX-512
extension, dubbed multi-word extension (MQX), which delivers substantial
speedup with only three new instructions and minimal proposed hardware
modifications. MQX cuts the slowdown relative to ASICs to as low as 35 times on
a single CPU core. Finally, we perform a roofline analysis to evaluate the peak
performance achievable with MQX when scaled across an entire multi-core CPU.
Our results show that, with MQX, top-tier server-grade CPUs can approach the
performance of state-of-the-art ASICs for cryptographic workloads.

</details>


### [8] [Exploiting Timing Side-Channels in Quantum Circuits Simulation Via ML-Based Methods](https://arxiv.org/abs/2509.12535)
*Ben Dong,Hui Feng,Qian Wang*

Main category: cs.CR

TL;DR: The paper reveals a timing side-channel attack against cloud quantum simulators, enabling adversaries to infer confidential circuit details with high accuracy, urging improved security measures.


<details>
  <summary>Details</summary>
Motivation: Quantum circuit simulators on cloud platforms are critical for current quantum hardware limitations, but they risk exposing proprietary designs through unaddressed security vulnerabilities like side-channel attacks, which this study aims to highlight.

Method: The research employs a timing side-channel attack by analyzing execution timing and memory characteristics of quantum circuit simulations using the QASMBench suite. Pattern recognition techniques are applied to classify timing profiles, enabling inference of circuit identities.

Result: Experimental results demonstrate high accuracy (88%‚Äì99.9%) in identifying quantum circuits through timing profile analysis, confirming the feasibility of the attack and the potential threat to user confidentiality in quantum simulation environments.

Conclusion: The study underscores the security vulnerabilities in cloud-based quantum simulators due to timing side-channel attacks, emphasizing the necessity for enhanced isolation mechanisms to safeguard user data and confidentiality in quantum computing environments.

Abstract: As quantum computing advances, quantum circuit simulators serve as critical
tools to bridge the current gap caused by limited quantum hardware
availability. These simulators are typically deployed on cloud platforms, where
users submit proprietary circuit designs for simulation. In this work, we
demonstrate a novel timing side-channel attack targeting cloud-based quantum
simulators. A co-located malicious process can observe fine-grained execution
timing patterns to extract sensitive information about concurrently running
quantum circuits. We systematically analyze simulator behavior using the
QASMBench benchmark suite, profiling timing and memory characteristics across
various circuit executions. Our experimental results show that timing profiles
exhibit circuit-dependent patterns that can be effectively classified using
pattern recognition techniques, enabling the adversary to infer circuit
identities and compromise user confidentiality. We were able to achieve 88% to
99.9% identification rate of quantum circuits based on different datasets. This
work highlights previously unexplored security risks in quantum simulation
environments and calls for stronger isolation mechanisms to protect user
workloads

</details>


### [9] [Yet Another Watermark for Large Language Models](https://arxiv.org/abs/2509.12574)
*Siyuan Bao,Ying Shi,Zhiguang Yang,Hanzhou Wu,Xinpeng Zhang*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Existing watermarking methods for large language models (LLMs) mainly embed
watermark by adjusting the token sampling prediction or post-processing,
lacking intrinsic coupling with LLMs, which may significantly reduce the
semantic quality of the generated marked texts. Traditional watermarking
methods based on training or fine-tuning may be extendable to LLMs. However,
most of them are limited to the white-box scenario, or very time-consuming due
to the massive parameters of LLMs. In this paper, we present a new watermarking
framework for LLMs, where the watermark is embedded into the LLM by
manipulating the internal parameters of the LLM, and can be extracted from the
generated text without accessing the LLM. Comparing with related methods, the
proposed method entangles the watermark with the intrinsic parameters of the
LLM, which better balances the robustness and imperceptibility of the
watermark. Moreover, the proposed method enables us to extract the watermark
under the black-box scenario, which is computationally efficient for use.
Experimental results have also verified the feasibility, superiority and
practicality. This work provides a new perspective different from mainstream
works, which may shed light on future research.

</details>


### [10] [Secure and Efficient Out-of-band Call Metadata Transmission](https://arxiv.org/abs/2509.12582)
*David Adei,Varun Madathil,Nithin Shyam S.,Bradley Reaves*

Main category: cs.CR

TL;DR: Sidecar extends the STIR/SHAKEN attestation framework with a privacy-preserving, decentralized system that enhances calling security and can evolve with future improvements.


<details>
  <summary>Details</summary>
Motivation: The STIR/SHAKEN framework lacks privacy in handling non-VoIP call metadata and expiration control, leading to potential privacy and confidentiality breaches.

Method: To fix this, the authors created Sidecar, a system that uses secure out-of-band signaling, formalizes requirements, and designs protocols to meet them under the Universal Composability framework.

Result: Sidecar offers better data secrecy, improved resource efficiency, and enables features like secure billing and misbehavior detection while maintaining similar performance to existing solutions.

Conclusion: Sidecar not only addresses current limitations better than existing solutions, but also allows for future enhancements like stronger authentication in the telephony ecosystem.

Abstract: The STIR/SHAKEN (S/S) attestation Framework mandated by the United States,
Canada, and France to combat pervasive telephone abuse has not achieved its
goals, partly because legacy non-VoIP infrastructure could not participate. The
industry solution to extend S/S broadcasts sensitive metadata of every non-VoIP
call in plaintext to every third party required to facilitate the system. It
has no mechanism to determine whether a provider's request for call data is
appropriate, nor can it ensure that every copy of that call data is unavailable
after its specified expiration. It threatens subscriber privacy and provider
confidentiality.
  In this paper, we present Sidecar, a distributed, privacy-preserving system
with tunable decentralization that securely extends S/S across all telephone
network technologies. We introduce the notion of secure out-of-band signaling
for telephony and formalize its system and security requirements. We then
design novel, scalable protocols that realize these requirements and prove
their security within the Universal Composability framework. Finally, we
demonstrate Sidecar's efficiency with our open-sourced reference
implementation. Compared to the current solution, Sidecar 1) protects the
confidentiality of subscriber identity and provider trade secrets, 2)
guarantees record expiration as long as a single node handling a record is
honest, 3) reduces resource requirements while providing virtually identical
call-setup times and equivalent or better uptimes, and 4) enables secure
pay-per-use billing and integrates mechanisms to mitigate and detect
misbehavior. Moreover, Sidecar can be extended to provide the same security
guarantees for arbitrary call metadata. Not only is Sidecar a superior
approach, it is also a transformative tool to retrofit fragmented global
telephony and enable future improvements, such as stronger call authentication
and Branded Calling.

</details>


### [11] [A Systematic Evaluation of Parameter-Efficient Fine-Tuning Methods for the Security of Code LLMs](https://arxiv.org/abs/2509.12649)
*Kiho Lee,Jungkon Kim,Doowon Kim,Hyoungshick Kim*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Code-generating Large Language Models (LLMs) significantly accelerate
software development. However, their frequent generation of insecure code
presents serious risks. We present a comprehensive evaluation of seven
parameter-efficient fine-tuning (PEFT) techniques, demonstrating substantial
gains in secure code generation without compromising functionality. Our
research identifies prompt-tuning as the most effective PEFT method, achieving
an 80.86% Overall-Secure-Rate on CodeGen2 16B, a 13.5-point improvement over
the 67.28% baseline. Optimizing decoding strategies through sampling
temperature further elevated security to 87.65%. This equates to a reduction of
approximately 203,700 vulnerable code snippets per million generated. Moreover,
prompt and prefix tuning increase robustness against poisoning attacks in our
TrojanPuzzle evaluation, with strong performance against CWE-79 and CWE-502
attack vectors. Our findings generalize across Python and Java, confirming
prompt-tuning's consistent effectiveness. This study provides essential
insights and practical guidance for building more resilient software systems
with LLMs.

</details>


### [12] [Hardened CTIDH: Dummy-Free and Deterministic CTIDH](https://arxiv.org/abs/2509.12877)
*Gustavo Banegas,Andreas Hellenbrand,Matheus Saldanha*

Main category: cs.CR

TL;DR: This paper presents the first dummy-free implementation of dCTIDH (a post-quantum isogeny-based primitive) by eliminating vulnerable dummy operations via DACsHUND optimization and a reformulated Matryoshka structure.


<details>
  <summary>Details</summary>
Motivation: Dummy operations in existing CTIDH/dCTIDH implementations (used for security and constant-time execution of group-action protocols) create vulnerabilities to fault-injection attacks. Removing these without compromising security or efficiency is critical for cryptographic robustness.

Method: The authors combine two innovations: (1)
DACsHUND (enforces equal-length differential addition chains without padding) and (2)
a restructured Matryoshka scheme that verifies intermediate points while removing dummy multiplications. They also introduce parameter sets excluding small primes (3,5,7) to enable viable DACsHUND configurations.

Result: Achieved dummy-free dCTIDH-2048-194/205 with 357k-362k Fp-multiplication costs and 1.59-1.60 Gcyc median times. Outperforms CTIDH by 5% and is >4x faster than dCSIDH, while being the first CSIDH-like protocol to achieve deterministic, constant-time, and fully dummy-free execution.

Conclusion: This work demonstrates a secure, efficient alternative to existing CSIDH variants by removing exploitable dummy operations through architectural re-design and parameter optimization, setting a new baseline for fault-resistant post-quantum isogeny cryptography.

Abstract: Isogeny-based cryptography has emerged as a promising postquantum
alternative, with CSIDH and its constant-time variants CTIDH and dCTIDH
offering efficient group-action protocols. However, CTIDH and dCTIDH rely on
dummy operations in differential addition chains (DACs) and Matryoshka, which
can be exploitable by fault-injection attacks. In this work, we present the
first dummy-free implementation of dCTIDH. Our approach combines two recent
ideas: DACsHUND, which enforces equal-length DACs within each batch without
padding, and a reformulated Matryoshka structure that removes dummy
multiplications and validates all intermediate points. Our analysis shows that
small primes such as 3, 5, and 7 severely restrict feasible DACsHUND
configurations, motivating new parameter sets that exclude them. We implement
dummy-free dCTIDH-2048-194 and dCTIDH-2048-205, achieving group action costs of
roughly 357,000-362,000 Fp-multiplications, with median evaluation times of
1.59-1.60 (Gcyc). These results do not surpass dC-TIDH, but they outperform
CTIDH by roughly 5% while eliminating dummy operations entirely. Compared to
dCSIDH, our construction is more than 4x faster. To the best of our knowledge,
this is the first efficient implementation of a CSIDH-like protocol that is
simultaneously deterministic, constant-time, and fully dummy-free.

</details>


### [13] [A Fault Analysis on SNOVA](https://arxiv.org/abs/2509.12879)
*Gustavo Banegas,Ricardo Villanueva-Polanco*

Main category: cs.CR

TL;DR: This paper analyzes the security of the SNOVA post-quantum signature scheme against fault attacks, demonstrating key recovery with only 22-68 faulty signatures and proposing a lightweight countermeasure.


<details>
  <summary>Details</summary>
Motivation: SNOVA (a NIST second-round PQCRYPTO candidate) requires rigorous security evaluation against fault attacks to ensure robustness in real-world deployment.

Method: The authors develop targeted fault injection strategies exploiting SNOVA's secret key structure, including a novel fault-assisted reconciliation attack solving quadratic polynomial systems. They combine simulation-based analysis of transient faults with countermeasure development.

Result: Experimental results show complete secret key recovery using 22-68 faulty signatures (security level dependent), with a lightweight fault detection mechanism reducing attack success probability by 83% without significant performance overhead.

Conclusion: The work underscores the critical need for fault-resilient design in post-quantum schemes. The proposed countermeasure provides immediate practical value while highlighting fundamental security weaknesses in current PQ signature implementation assumptions.

Abstract: SNOVA is a post-quantum cryptographic signature scheme known for its
efficiency and compact key sizes, making it a second-round candidate in the
NIST post-quantum cryptography standardization process. This paper presents a
comprehensive fault analysis of SNOVA, focusing on both permanent and transient
faults during signature generation. We introduce several fault injection
strategies that exploit SNOVA's structure to recover partial or complete secret
keys with limited faulty signatures. Our analysis reveals that as few as 22 to
68 faulty signatures, depending on the security level, can suffice for key
recovery. We propose a novel fault-assisted reconciliation attack,
demonstrating its effectiveness in extracting the secret key space via solving
a quadratic polynomial system. Simulations show transient faults in key
signature generation steps can significantly compromise SNOVA's security. To
address these vulnerabilities, we propose a lightweight countermeasure to
reduce the success of fault attacks without adding significant overhead. Our
results highlight the importance of fault-resistant mechanisms in post-quantum
cryptographic schemes like SNOVA to ensure robustness.

</details>


### [14] [EByFTVeS: Efficient Byzantine Fault Tolerant-based Verifiable Secret-sharing in Distributed Privacy-preserving Machine Learning](https://arxiv.org/abs/2509.12899)
*Zhen Li,Zijian Zhang,Wenjin Yang,Pengbo Wang,Zhaoqi Wang,Meng Li,Yan Wu,Xuyang Liu,Jing Sun,Liehuang Zhu*

Main category: cs.CR

TL;DR: This paper proposes EByFTVeS, a BFT-enhanced VSS scheme that improves efficiency and security in DPML systems by addressing consistency vulnerabilities through adaptive share delay strategies and theoretical validation of its enhanced performance.


<details>
  <summary>Details</summary>
Motivation: Current VSS-based DPML systems face two critical limitations: (1) inconsistency risks from malicious actors and (2) high computational/communication overhead. Existing BFT extensions fail to adequately resolve these challenges, necessitating a novel approach.

Method: 1. Proposes the Adaptive Share Delay Provision (ASDP) strategy to mitigate vulnerabilities in VSS schemes. 2. Develops the ACuMPA attack model to analyze security weaknesses. 3. Integrates BFT mechanisms with VSS to create EByFTVeS, optimizing share distribution and validation processes.

Result: Theoretical analysis confirms EByFTVeS satisfies validity, liveness, consistency, and privacy properties. Experimental results show efficiency improvements (lower latency/communication costs) compared to state-of-the-art VSS schemes, while maintaining strong security guarantees against poisoning attacks.

Conclusion: The paper introduces an efficient Byzantine Fault Tolerant-based Verifiable Secret-Sharing scheme (EByFTVeS) to address consistency and efficiency challenges in VSS-based DPML systems. Through theoretical analysis and experiments, it demonstrates improved validity, privacy, and efficiency compared to existing VSS schemes.

Abstract: Verifiable Secret Sharing (VSS) has been widespread in Distributed
Privacy-preserving Machine Learning (DPML), because invalid shares from
malicious dealers or participants can be recognized by verifying the commitment
of the received shares for honest participants. However, the consistency and
the computation and communitation burden of the VSS-based DPML schemes are
still two serious challenges. Although Byzantine Fault Tolerance (BFT) system
has been brought to guarantee the consistency and improve the efficiency of the
existing VSS-based DPML schemes recently, we explore an Adaptive Share Delay
Provision (ASDP) strategy, and launch an ASDP-based Customized Model Poisoning
Attack (ACuMPA) for certain participants in this paper. We theoretically
analyzed why the ASDP strategy and the ACuMPA algorithm works to the existing
schemes. Next, we propose an [E]fficient [By]zantine [F]ault [T]olerant-based
[Ve]rifiable [S]ecret-sharing (EByFTVeS) scheme. Finally, the validity,
liveness, consistency and privacy of the EByFTVeS scheme are theoretically
analyzed, while the efficiency of the EByFTVeS scheme outperforms that of
the-state-of-art VSS scheme according to comparative experiment results.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [15] [Evaluating Large Language Models for Functional and Maintainable Code in Industrial Settings: A Case Study at ASML](https://arxiv.org/abs/2509.12395)
*Yash Mundhra,Max Valk,Maliheh Izadi*

Main category: cs.SE

TL;DR: This collaborative study with ASML evaluates LLMs in industrial code generation using a new benchmark and build@k metric. Results show prompting methods (e.g., few-shot, chain-of-thought) and model size most impact code quality and integration success, with less consistent differences between code-specific and generic models.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of exploration regarding LLMs' applicability in proprietary industrial environments with domain-specific constraints and code interdependencies. It aims to evaluate how LLMs can generate maintainable, functional code in such specialized, closed settings.

Method: The researchers conducted a collaborative case study with ASML's leveling department, developing an evaluation framework tailored to their proprietary codebase. They introduced a new benchmark and the build@k metric to assess compilation and integration success. The study evaluated prompting techniques, compared generic vs. code-specific LLMs, and analyzed impact of model size using match-based and execution-based metrics.

Result: Key findings indicate that few-shot and chain-of-thought prompting enhance build success rates. Model size directly affects code quality, while the performance difference between code-specific and generic LLMs is minimal and varies across model families. The proposed build@k metric revealed significant variation in integration success.

Conclusion: The study concludes that the effectiveness of LLMs in industrial code generation depends significantly on prompting techniques and model size rather than solely on domain-specific model specialization, with build@k metrics highlighting the importance of integration success. However, the performance gap between code-specific and generic models remains inconsistent across model families.

Abstract: Large language models have shown impressive performance in various domains,
including code generation across diverse open-source domains. However, their
applicability in proprietary industrial settings, where domain-specific
constraints and code interdependencies are prevalent, remains largely
unexplored. We present a case study conducted in collaboration with the
leveling department at ASML to investigate the performance of LLMs in
generating functional, maintainable code within a closed, highly specialized
software environment.
  We developed an evaluation framework tailored to ASML's proprietary codebase
and introduced a new benchmark. Additionally, we proposed a new evaluation
metric, build@k, to assess whether LLM-generated code successfully compiles and
integrates within real industrial repositories. We investigate various
prompting techniques, compare the performance of generic and code-specific
LLMs, and examine the impact of model size on code generation capabilities,
using both match-based and execution-based metrics. The findings reveal that
prompting techniques and model size have a significant impact on output
quality, with few-shot and chain-of-thought prompting yielding the highest
build success rates. The difference in performance between the code-specific
LLMs and generic LLMs was less pronounced and varied substantially across
different model families.

</details>


### [16] [Understanding Prompt Management in GitHub Repositories: A Call for Best Practices](https://arxiv.org/abs/2509.12421)
*Hao Li,Hicham Masri,Filipe R. Cogo,Abdul Ali Bangash,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: This paper analyzes 24,800 open-source prompts to identify management challenges and proposes solutions for improving promptware quality.


<details>
  <summary>Details</summary>
Motivation: The rising use of foundation models has created a need for effective prompt management in promptware, but current practices face significant challenges in formatting consistency, duplication, and readability.

Method: The study conducted an empirical analysis of 24,800 open-source prompts across 92 GitHub repositories to examine prompt management practices and quality attributes.

Result: Key findings include inconsistent prompt formatting, substantial internal/external duplication, and frequent readability/spelling issues in open-source prompts.

Conclusion: Based on the findings of critical challenges in prompt management, the paper provides actionable recommendations to improve the usability and maintainability of open-source prompts in the promptware ecosystem.

Abstract: The rapid adoption of foundation models (e.g., large language models) has
given rise to promptware, i.e., software built using natural language prompts.
Effective management of prompts, such as organization and quality assurance, is
essential yet challenging. In this study, we perform an empirical analysis of
24,800 open-source prompts from 92 GitHub repositories to investigate prompt
management practices and quality attributes. Our findings reveal critical
challenges such as considerable inconsistencies in prompt formatting,
substantial internal and external prompt duplication, and frequent readability
and spelling issues. Based on these findings, we provide actionable
recommendations for developers to enhance the usability and maintainability of
open-source prompts within the rapidly evolving promptware ecosystem.

</details>


### [17] [From Legacy Fortran to Portable Kokkos:An Autonomous Agentic AI Workflow](https://arxiv.org/abs/2509.12443)
*Sparsh Gupta,Kamalavasan Kamalakkannan,Maxim Moraru,Galen Shipman,Patrick Diehl*

Main category: cs.SE

TL;DR: A novel agentic AI system automates Fortran-to-Kokkos modernization, demonstrating LLMs can enable autonomous performance-portable code transformation for heterogeneous supercomputing with low cost and high efficacy.


<details>
  <summary>Details</summary>
Motivation: Legacy Fortran codes face urgent modernization needs due to the shift to GPU-accelerated HPC systems. Current manual porting to frameworks like Kokkos is labor-intensive, while fully autonomous code transformation workflows remain unexplored despite LLMs' potential for source-to-source translation.

Method: An agentic AI workflow employs specialized LLM agents to collaboratively translate, validate, compile, execute, test, debug, and optimize Fortran kernels into Kokkos C++ programs through autonomous decision-making and iterative refinement.

Result: The workflow produces optimized Kokkos codes outperforming original Fortran baselines, achieves performance portability across hardware, and costs <US$5 for OpenAI models. Open-source models showed limited success compared to commercial counterparts.

Conclusion: This paper demonstrates that agentic AI systems can effectively transform legacy Fortran code into performance-portable Kokkos C++, offering a scalable solution for modernizing scientific applications on heterogeneous HPC architectures.

Abstract: Scientific applications continue to rely on legacy Fortran codebases
originally developed for homogeneous, CPU-based systems. As High-Performance
Computing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many
accelerators lack native Fortran bindings, creating an urgent need to modernize
legacy codes for portability. Frameworks like Kokkos provide performance
portability and a single-source C++ abstraction, but manual Fortran-to-Kokkos
porting demands significant expertise and time. Large language models (LLMs)
have shown promise in source-to-source code generation, yet their use in fully
autonomous workflows for translating and optimizing parallel code remains
largely unexplored, especially for performance portability across diverse
hardware.
  This paper presents an agentic AI workflow where specialized LLM "agents"
collaborate to translate, validate, compile, run, test, debug, and optimize
Fortran kernels into portable Kokkos C++ programs. Results show the pipeline
modernizes a range of benchmark kernels, producing performance-portable Kokkos
codes across hardware partitions. Paid OpenAI models such as GPT-5 and
o4-mini-high executed the workflow for only a few U.S. dollars, generating
optimized codes that surpassed Fortran baselines, whereas open-source models
like Llama4-Maverick often failed to yield functional codes.
  This work demonstrates the feasibility of agentic AI for Fortran-to-Kokkos
transformation and offers a pathway for autonomously modernizing legacy
scientific applications to run portably and efficiently on diverse
supercomputers. It further highlights the potential of LLM-driven agentic
systems to perform structured, domain-specific reasoning tasks in scientific
and systems-oriented applications.

</details>


### [18] [Perspectives, Needs and Challenges for Sustainable Software Engineering Teams: A FinServ Case Study](https://arxiv.org/abs/2509.12466)
*Satwik Ghanta,Peggy Gregory,Gul Calikli*

Main category: cs.SE

TL;DR: A case study in a financial services company reveals stark differences between management and developers on sustainability in software engineering, advocating for collaborative, context-aware sustainability strategies.


<details>
  <summary>Details</summary>
Motivation: Sustainable Software Engineering (SSE) lacks a unified definition, creating challenges for organisations. Financial services, being data-intensive and regulation-heavy, have underexplored sustainability needs, warranting sector-specific investigation.

Method: The research employed an exploratory qualitative case study, involving interviews and a focus group with 6 higher management employees and 16 software engineers from a financial services company.

Result: Participants exhibited divergent sustainability perceptions: higher management prioritized technical and economic aspects (cloud migration, data availability), while developers emphasized workload-stress reduction. Scepticism about organizational PR-oriented initiatives and calls for a sustainability-dedicated team (like security governance) emerged.

Conclusion: The study emphasizes that context-sensitive, co-designed interventions are crucial to bridging the gap between organisational sustainability goals in financial services software engineering and the practical needs and concerns of developers.

Abstract: Sustainable Software Engineering (SSE) is slowly becoming an industry need
for reasons including reputation enhancement, improved profits and more
efficient practices. However, SSE has many definitions, and this is a challenge
for organisations trying to build a common and broadly agreed understanding of
the term. Although much research effort has gone into identifying general SSE
practices, there is a gap in understanding the sustainability needs of specific
organisational contexts, such as financial services, which are highly
data-driven, operate under strict regulatory requirements, and handle millions
of transactions day to day. To address this gap, our research focuses on a
financial services company (FinServCo) that invited us to investigate
perceptions of sustainability in their IT function: how it could be put into
practice, who is responsible for it, and what the challenges are. We conducted
an exploratory qualitative case study using interviews and a focus group with
six higher management employees and 16 software engineers comprising various
experience levels from junior developers to team leaders. Our study found a
clear divergence in how sustainability is perceived between organisational
levels. Higher management emphasised technical and economic sustainability,
focusing on cloud migration and business continuity through data availability.
In contrast, developers highlighted human-centric concerns such as workload
management and stress reduction. Scepticism toward organisational initiatives
was also evident, with some developers viewing them as a PR strategy. Many
participants expressed a preference for a dedicated sustainability team,
drawing analogies to internal structures for security governance. The
disconnect between organisational goals and individual developer needs
highlights the importance of context-sensitive, co-designed interventions.

</details>


### [19] [Good Vibrations? A Qualitative Study of Co-Creation, Communication, Flow, and Trust in Vibe Coding](https://arxiv.org/abs/2509.12491)
*Veronica Pimenova,Sarah Fakhoury,Christian Bird,Margaret-Anne Storey,Madeline Endres*

Main category: cs.SE

TL;DR: This paper is the first systematic qualitative study on understanding vibe coding, an AI-assisted natural language programming paradigm that emphasizes flow and co-design with an AI assistant. The authors analyze over 190,000 words of developer feedback and present a grounded theory, challenges, and best practices.


<details>
  <summary>Details</summary>
Motivation: There is a need for a grounded understanding of vibe coding based on developer experiences rather than code artifact analysis or limited theories. The authors aim to explore why and how developers use this paradigm and to identify risks and emerging best practices.

Method: The authors undertaken a systematic qualitative investigation using data from 190,000 words of semi-structured interviews, Reddit threads, and LinkedIn posts to characterise the use of vibe coding in practice.

Result: A qualitatively grounded theory of vibe coding centered on conversational interaction with AI, co-creation, and developer flow and joy has been proposed. Also identified pain points, risks, and best practices related to specification, reliability, debugging, latency, code review burden, and collaboration.

Conclusion: Implications for future AI dev tools and directions for researchers investigating vibe coding are proposed. The paper highlights the importance of trust in AI, flow states, and the need for best practices to mitigate challenges.

Abstract: Vibe coding, a term coined by Andrej Karpathy in February 2025, has quickly
become a compelling and controversial natural language programming paradigm in
AI-assisted software development. Centered on iterative co-design with an AI
assistant, vibe coding emphasizes flow and experimentation over strict upfront
specification. While initial studies have begun to explore this paradigm, most
focus on analyzing code artifacts or proposing theories with limited empirical
backing. There remains a need for a grounded understanding of vibe coding as it
is perceived and experienced by developers. We present the first systematic
qualitative investigation of vibe coding perceptions and practice. Drawing on
over 190,000 words from semi-structured interviews, Reddit threads, and
LinkedIn posts, we characterize what vibe coding is, why and how developers use
it, where it breaks down, and which emerging practices aim to support it. We
propose a qualitatively grounded theory of vibe coding centered on
conversational interaction with AI, co-creation, and developer flow and joy. We
find that AI trust regulates movement along a continuum from delegation to
co-creation and supports the developer experience by sustaining flow. We
surface recurring pain points and risks in areas including specification,
reliability, debugging, latency, code review burden, and collaboration. We also
present best practices that have been discovered and shared to mitigate these
challenges. We conclude with implications for the future of AI dev tools and
directions for researchers investigating vibe coding.

</details>


### [20] [Ensembling Large Language Models for Code Vulnerability Detection: An Empirical Evaluation](https://arxiv.org/abs/2509.12629)
*Zhihong Sun,Jia Li,Yao Wan,Chuanyi Li,Hongyu Zhang,Zhi jin,Ge Li,Hong Liu,Chen Lyu,Songlin Hu*

Main category: cs.SE

TL;DR: The study explores enhancing LLM-based code vulnerability detection using ensemble learning, comparing three strategies and proposing Dynamic Gated Stacking (DGS). DGS outperforms traditional stacking, especially in imbalanced and multi-class tasks.


<details>
  <summary>Details</summary>
Motivation: Code vulnerability detection is crucial for security and reliability. LLMs show promise but have inconsistent results across training stages and models, highlighting an opportunity for ensemble learning to leverage their complementarity.

Method: Conducted experiments with five LLMs (DeepSeek-Coder-6.7B, CodeLlama-7B, CodeLlama-13B, CodeQwen1.5-7B, StarCoder2-15B) using Bagging, Boosting, and Stacking ensemble strategies. Introduced DGS, a Stacking variant inspired by Mixture of Experts (MoE) techniques.

Result: Ensemble methods significantly improve detection performance. Boosting excels in imbalanced datasets, while DGS consistently outperforms traditional Stacking in handling class imbalance and multi-class classification tasks.

Conclusion: Ensemble learning, particularly DGS, offers a valuable approach for building reliable and effective LLM-based vulnerability detection systems.

Abstract: Code vulnerability detection is crucial for ensuring the security and
reliability of modern software systems. Recently, Large Language Models (LLMs)
have shown promising capabilities in this domain. However, notable
discrepancies in detection results often arise when analyzing identical code
segments across different training stages of the same model or among
architecturally distinct LLMs. While such inconsistencies may compromise
detection stability, they also highlight a key opportunity: the latent
complementarity among models can be harnessed through ensemble learning to
create more robust vulnerability detection systems. In this study, we explore
the potential of ensemble learning to enhance the performance of LLMs in source
code vulnerability detection. We conduct comprehensive experiments involving
five LLMs (i.e., DeepSeek-Coder-6.7B, CodeLlama-7B, CodeLlama-13B,
CodeQwen1.5-7B, and StarCoder2-15B), using three ensemble strategies (i.e.,
Bagging, Boosting, and Stacking). These experiments are carried out across
three widely adopted datasets (i.e., Devign, ReVeal, and BigVul). Inspired by
Mixture of Experts (MoE) techniques, we further propose Dynamic Gated Stacking
(DGS), a Stacking variant tailored for vulnerability detection. Our results
demonstrate that ensemble approaches can significantly improve detection
performance, with Boosting excelling in scenarios involving imbalanced
datasets. Moreover, DGS consistently outperforms traditional Stacking,
particularly in handling class imbalance and multi-class classification tasks.
These findings offer valuable insights into building more reliable and
effective LLM-based vulnerability detection systems through ensemble learning.

</details>


### [21] [When Large Language Models Meet UAVs: How Far Are We?](https://arxiv.org/abs/2509.12795)
*Yihua Chen,Xingle Que,Jiashuo Zhang,Ting Chen,Guangshun Li,Jiachi Chen*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The integration of unmanned aerial vehicles (UAVs) and large language models
(LLMs) has emerged as a research direction of growing interest, with the
potential to address challenges in autonomous decision-making, human-UAV
interaction, and real-time adaptability. However, existing studies have
remained largely in preliminary exploration with a limited understanding of
real-world practice, risking a misalignment between academic research and
practical needs and hindering the translation of results. To examine and
address these potential challenges, we conducted an empirical study of 74
selected papers and 56 public GitHub projects, identified nine task types for
LLMs in UAV systems, and quantified their distribution. Our findings show that
academic research emphasizes theoretical modeling and task optimization with
dispersed attention across tasks. In contrast, industrial projects focus on
flight control, task planning, and human-machine interaction, prioritizing
operability and efficiency. To further capture industry perspectives, we
distributed an online questionnaire. We obtained 52 valid responses: 40.4% of
practitioners have attempted to apply LLMs to UAV tasks. We further identify
factors that impede real-world integration, including technological maturity,
performance, safety, cost, and other considerations. Finally, we highlight
challenges for future development and provide recommendations.

</details>


### [22] [LLM-Based Approach for Enhancing Maintainability of Automotive Architectures](https://arxiv.org/abs/2509.12798)
*Nenad Petrovic,Lukasz Mazur,Alois Knoll*

Main category: cs.SE

TL;DR: The paper investigates using Large Language Models (LLMs) to automate tasks improving automotive system flexibility, presenting three case studies with GPT-4o for automation in updates/compliance, interface compatibility, and architecture modification.


<details>
  <summary>Details</summary>
Motivation: Automotive systems face inflexibility from prolonged re-engineering, standardization, and heterogeneous components; LLMs may address these by automating maintenance, updates, and lifecycle processes.

Method: Three case studies implementing LLM automation: 1) Updates/hardware abstraction/compliance, 2) Interface compatibility checking, 3) Architecture modification suggestions, using OpenAI's GPT-4o as proof-of-concept.

Result: Demonstrated LLM capabilities in automating critical automotive system processes (updates, compatibility checks, architecture suggestions) through early-stage case studies with GPT-4o.

Conclusion: LLMs show potential to enhance automotive system flexibility through process automation, though further research is needed to validate scalability and practical deployment in industrial settings.

Abstract: There are many bottlenecks that decrease the flexibility of automotive
systems, making their long-term maintenance, as well as updates and extensions
in later lifecycle phases increasingly difficult, mainly due to long
re-engineering, standardization, and compliance procedures, as well as
heterogeneity and numerosity of devices and underlying software components
involved. In this paper, we explore the potential of Large Language Models
(LLMs) when it comes to the automation of tasks and processes that aim to
increase the flexibility of automotive systems. Three case studies towards
achieving this goal are considered as outcomes of early-stage research: 1)
updates, hardware abstraction, and compliance, 2) interface compatibility
checking, and 3) architecture modification suggestions. For proof-of-concept
implementation, we rely on OpenAI's GPT-4o model.

</details>


### [23] [SateLight: A Satellite Application Update Framework for Satellite Computing](https://arxiv.org/abs/2509.12809)
*Jinfeng Wen,Jianshu Zhao,Zixi Zhu,Xiaomin Zhang,Qi Liang,Ao Zhou,Shangguang Wang*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Satellite computing is an emerging paradigm that empowers satellites to
perform onboard processing tasks (i.e., \textit{satellite applications}),
thereby reducing reliance on ground-based systems and improving responsiveness.
However, enabling application software updates in this context remains a
fundamental challenge due to application heterogeneity, limited
ground-to-satellite bandwidth, and harsh space conditions. Existing software
update approaches, designed primarily for terrestrial systems, fail to address
these constraints, as they assume abundant computational capacity and stable
connectivity.
  To address this gap, we propose SateLight, a practical and effective
satellite application update framework tailored for satellite computing.
SateLight leverages containerization to encapsulate heterogeneous applications,
enabling efficient deployment and maintenance. SateLight further integrates
three capabilities: (1) a content-aware differential strategy that minimizes
communication data volume, (2) a fine-grained onboard update design that
reconstructs target applications, and (3) a layer-based fault-tolerant recovery
mechanism to ensure reliability under failure-prone space conditions.
Experimental results on a satellite simulation environment with 10
representative satellite applications demonstrate that SateLight reduces
transmission latency by up to 91.18% (average 56.54%) compared to the best
currently available baseline. It also consistently ensures 100% update
correctness across all evaluated applications. Furthermore, a case study on a
real-world in-orbit satellite demonstrates the practicality of our approach.

</details>


### [24] [Evaluating Large Language Models for Code Translation: Effects of Prompt Language and Prompt Design](https://arxiv.org/abs/2509.12973)
*Aamer Aljagthami,Mohammed Banabila,Musab Alshehri,Mohammed Kabini,Mohammad D. Alahmadi*

Main category: cs.SE

TL;DR: The study compares large language models (LLMs) for source-code translation in terms of quality, prompt design, and language choice. It finds that detailed prompts and English improve translation quality, with the best LLMs outperforming TransCoder.


<details>
  <summary>Details</summary>
Motivation: Automated source-code translation is crucial for software migration, maintenance, and interoperability. However, it is unclear how choices in model architecture, prompt style, and language impact translation quality across different programming languages.

Method: The paper evaluates multiple LLMs and the TransCoder baseline for code translation among four languages (C++, Java, Python, C#) using BLEU and CodeBLEU. Performance is assessed with concise instructions vs. detailed specifications, and using English vs. Arabic prompts, including direction-aware analysis.

Result: Detailed prompts consistently enhance performance across models and translation directions. English prompts are better than Arabic by 13-15%. Top models are very good at Java to C# and Python to C++. All tested LLMs match or surpass the TransCoder baseline in all tasks.

Conclusion: The results underscore the importance of prompt engineering and language choice in code translation. Effective LLMs can significantly improve software modernization by automated cross-language translation.

Abstract: Large language models (LLMs) have shown promise for automated source-code
translation, a capability critical to software migration, maintenance, and
interoperability. Yet comparative evidence on how model choice, prompt design,
and prompt language shape translation quality across multiple programming
languages remains limited. This study conducts a systematic empirical
assessment of state-of-the-art LLMs for code translation among C++, Java,
Python, and C#, alongside a traditional baseline (TransCoder). Using BLEU and
CodeBLEU, we quantify syntactic fidelity and structural correctness under two
prompt styles (concise instruction and detailed specification) and two prompt
languages (English and Arabic), with direction-aware evaluation across language
pairs. Experiments show that detailed prompts deliver consistent gains across
models and translation directions, and English prompts outperform Arabic by
13-15%. The top-performing model attains the highest CodeBLEU on challenging
pairs such as Java to C# and Python to C++. Our evaluation shows that each LLM
outperforms TransCoder across the benchmark. These results demonstrate the
value of careful prompt engineering and prompt language choice, and provide
practical guidance for software modernization and cross-language
interoperability.

</details>
