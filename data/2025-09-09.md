<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 35]
- [cs.SE](#cs.SE) [Total: 23]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Towards Log Analysis with AI Agents: Cowrie Case Study](https://arxiv.org/abs/2509.05306)
*Enis Karaarslan,Esin Güler,Efe Emir Yüce,Cagatay Coban*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The scarcity of real-world attack data significantly hinders progress in
cybersecurity research and education. Although honeypots like Cowrie
effectively collect live threat intelligence, they generate overwhelming
volumes of unstructured and heterogeneous logs, rendering manual analysis
impractical. As a first step in our project on secure and efficient AI
automation, this study explores the use of AI agents for automated log
analysis. We present a lightweight and automated approach to process Cowrie
honeypot logs. Our approach leverages AI agents to intelligently parse,
summarize, and extract insights from raw data, while also considering the
security implications of deploying such an autonomous system. Preliminary
results demonstrate the pipeline's effectiveness in reducing manual effort and
identifying attack patterns, paving the way for more advanced autonomous
cybersecurity analysis in future work.

</details>


### [2] [Large Language Model Integration with Reinforcement Learning to Augment Decision-Making in Autonomous Cyber Operations](https://arxiv.org/abs/2509.05311)
*Konur Tholl,François Rivest,Mariam El Mezouar,Ranwa Al Mallah*

Main category: cs.CR

TL;DR: LLM-guided RL agents learn faster, avoid harmful actions, and outperform baselines in autonomous cyber operations through early training guidance from cybersecurity expertise.


<details>
  <summary>Details</summary>
Motivation: Standard RL agents in cybersecurity learn from scratch through environment interaction, often executing undesirable actions to learn their consequences, which is inefficient and risky for real-world applications.

Method: A Large Language Model (LLM) pretrained on cybersecurity data is integrated with an RL agent to guide decision-making during initial training stages, avoiding the need for trial-and-error exploration of harmful behaviors.

Result: The LLM-integrated agent achieved over 2x higher rewards during early training and converged to a favorable policy 4,500 episodes faster than the baseline in simulated cybersecurity experiments.

Conclusion: Integrating a cybersecurity-pretrained LLM with RL agents improves autonomous cyber operations by reducing harmful exploratory actions, accelerating policy convergence, and achieving higher early training rewards.

Abstract: Reinforcement Learning (RL) has shown great potential for autonomous
decision-making in the cybersecurity domain, enabling agents to learn through
direct environment interaction. However, RL agents in Autonomous Cyber
Operations (ACO) typically learn from scratch, requiring them to execute
undesirable actions to learn their consequences. In this study, we integrate
external knowledge in the form of a Large Language Model (LLM) pretrained on
cybersecurity data that our RL agent can directly leverage to make informed
decisions. By guiding initial training with an LLM, we improve baseline
performance and reduce the need for exploratory actions with obviously negative
outcomes. We evaluate our LLM-integrated approach in a simulated cybersecurity
environment, and demonstrate that our guided agent achieves over 2x higher
rewards during early training and converges to a favorable policy approximately
4,500 episodes faster than the baseline.

</details>


### [3] [Backdoor Samples Detection Based on Perturbation Discrepancy Consistency in Pre-trained Language Models](https://arxiv.org/abs/2509.05318)
*Zuquan Peng,Jianming Fu,Lixin Zou,Li Zheng,Yanzhen Ren,Guojun Peng*

Main category: cs.CR

TL;DR: Researchers developed NERTE, a novel backdoor detection method that uses perturbation discrepancy analysis to identify malicious samples in pre-trained models without needing special resources.


<details>
  <summary>Details</summary>
Motivation: Existing backdoor detection methods require poisoned models, extra clean samples, or significant computational resources, making them impractical for real-world applications.

Method: NERTE leverages perturbation discrepancy consistency evaluation by computing log probabilities with a pre-trained model and using an automated mask-filling strategy to generate perturbations. It measures curvature to assess discrepancy consistency for backdoor detection.

Result: NERTE outperforms state-of-the-art zero-shot black-box detection methods across experiments involving four typical attacks and five large language model backdoor attack scenarios.

Conclusion: The proposed NERTE method effectively detects backdoor samples in pre-trained models with minimal resource requirements and without needing access to poisoned models or additional clean data. It demonstrates superior performance over existing zero-shot black-box detection methods.

Abstract: The use of unvetted third-party and internet data renders pre-trained models
susceptible to backdoor attacks. Detecting backdoor samples is critical to
prevent backdoor activation during inference or injection during training.
However, existing detection methods often require the defender to have access
to the poisoned models, extra clean samples, or significant computational
resources to detect backdoor samples, limiting their practicality. To address
this limitation, we propose a backdoor sample detection method based on
perturbatio\textbf{N} discr\textbf{E}pancy consis\textbf{T}ency
\textbf{E}valuation (\NETE). This is a novel detection method that can be used
both pre-training and post-training phases. In the detection process, it only
requires an off-the-shelf pre-trained model to compute the log probability of
samples and an automated function based on a mask-filling strategy to generate
perturbations. Our method is based on the interesting phenomenon that the
change in perturbation discrepancy for backdoor samples is smaller than that
for clean samples. Based on this phenomenon, we use curvature to measure the
discrepancy in log probabilities between different perturbed samples and input
samples, thereby evaluating the consistency of the perturbation discrepancy to
determine whether the input sample is a backdoor sample. Experiments conducted
on four typical backdoor attacks and five types of large language model
backdoor attacks demonstrate that our detection strategy outperforms existing
zero-shot black-box detection methods.

</details>


### [4] [Privacy-Preserving Offloading for Large Language Models in 6G Vehicular Networks](https://arxiv.org/abs/2509.05320)
*Ikhlasse Badidi,Nouhaila El Khiyaoui,Aya Riany,Badr Ben Elallid,Amine Abouaomar*

Main category: cs.CR

TL;DR: This paper proposes a FL-DP hybrid framework for privacy-preserving LLM offloading in 6G vehicular networks, achieving strong accuracy (75%) and ε=0.8 DP while maintaining low communication overhead.


<details>
  <summary>Details</summary>
Motivation: LLM offloading in 6G vehicular networks risks exposing sensitive user data. Existing methods lack privacy-protection mechanisms for such environments, necessitating a solution that balances privacy and system performance.

Method: A privacy-preserving framework combining federated learning (FL) and differential privacy (DP) is introduced. It uses privacy-aware task partitioning to balance local and edge computation, along with a secure communication protocol for model updates.

Result: The approach achieves 75% global accuracy (2-3% lower than non-privacy methods) with ε=0.8 DP guarantees. Communication overhead remains stable at 2.1MB/round, and computation dominates 90% of processing time, confirming efficiency for resource-constrained vehicles.

Conclusion: The proposed framework effectively addresses privacy concerns in LLM-integrated vehicular networks through a hybrid FL-DP approach, achieving high accuracy and efficiency in resource-constrained environments.

Abstract: The integration of Large Language Models (LLMs) in 6G vehicular networks
promises unprecedented advancements in intelligent transportation systems.
However, offloading LLM computations from vehicles to edge infrastructure poses
significant privacy risks, potentially exposing sensitive user data. This paper
presents a novel privacy-preserving offloading framework for LLM-integrated
vehicular networks. We introduce a hybrid approach combining federated learning
(FL) and differential privacy (DP) techniques to protect user data while
maintaining LLM performance. Our framework includes a privacy-aware task
partitioning algorithm that optimizes the trade-off between local and edge
computation, considering both privacy constraints and system efficiency. We
also propose a secure communication protocol for transmitting model updates and
aggregating results across the network. Experimental results demonstrate that
our approach achieves 75\% global accuracy with only a 2-3\% reduction compared
to non-privacy-preserving methods, while maintaining DP guarantees with an
optimal privacy budget of $\varepsilon = 0.8$. The framework shows stable
communication overhead of approximately 2.1MB per round with computation
comprising over 90\% of total processing time, validating its efficiency for
resource-constrained vehicular environments.

</details>


### [5] [Zero-Knowledge Proofs in Sublinear Space](https://arxiv.org/abs/2509.05326)
*Logan Nye*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Modern zero-knowledge proof (ZKP) systems, essential for privacy and
verifiable computation, suffer from a fundamental limitation: the prover
typically uses memory that scales linearly with the computation's trace length
T, making them impractical for resource-constrained devices and prohibitively
expensive for large-scale tasks. This paper overcomes this barrier by
constructing, to our knowledge, the first sublinear-space ZKP prover. Our core
contribution is an equivalence that reframes proof generation as an instance of
the classic Tree Evaluation problem. Leveraging a recent space-efficient
tree-evaluation algorithm, we design a streaming prover that assembles the
proof without ever materializing the full execution trace. The approach reduces
prover memory from linear in T to O(sqrt(T)) (up to O(log T) lower-order terms)
while preserving proof size, verifier time, and the transcript/security
guarantees of the underlying system. This enables a shift from specialized,
server-bound proving to on-device proving, opening applications in
decentralized systems, on-device machine learning, and privacy-preserving
technologies.

</details>


### [6] [ForensicsData: A Digital Forensics Dataset for Large Language Models](https://arxiv.org/abs/2509.05331)
*Youssef Chakir,Iyad Lahsen-Cherif*

Main category: cs.CR

TL;DR: Introduces ForensicsData, a Q-C-A dataset from malware analysis reports to address limited public resources in digital forensics.


<details>
  <summary>Details</summary>
Motivation: Lack of realistic datasets due to privacy/ethical constraints hinders forensic research and tool development.

Method: Workflow combining structured data extraction, LLM-based Q-C-A transformation, and specialized evaluation for quality assurance.

Result:  Generated 5,000+ Q-C-A triplets; Gemini 2 Flash showed best performance in forensic terminology alignment.

Conclusion: ForensicsData enables reproducible experiments and fosters collaboration by providing standardized, realistic forensic data.

Abstract: The growing complexity of cyber incidents presents significant challenges for
digital forensic investigators, especially in evidence collection and analysis.
Public resources are still limited because of ethical, legal, and privacy
concerns, even though realistic datasets are necessary to support research and
tool developments. To address this gap, we introduce ForensicsData, an
extensive Question-Context-Answer (Q-C-A) dataset sourced from actual malware
analysis reports. It consists of more than 5,000 Q-C-A triplets. A unique
workflow was used to create the dataset, which extracts structured data, uses
large language models (LLMs) to transform it into Q-C-A format, and then uses a
specialized evaluation process to confirm its quality. Among the models
evaluated, Gemini 2 Flash demonstrated the best performance in aligning
generated content with forensic terminology. ForensicsData aims to advance
digital forensics by enabling reproducible experiments and fostering
collaboration within the research community.

</details>


### [7] [Integrated Simulation Framework for Adversarial Attacks on Autonomous Vehicles](https://arxiv.org/abs/2509.05332)
*Christos Anagnostopoulos,Ioulia Kapsali,Alexandros Gkillas,Nikos Piperigkos,Aris S. Lalos*

Main category: cs.CR

TL;DR: A new simulation framework is introduced for testing autonomous vehicles against multi-domain adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: AVs face risks from adversarial attacks on their perception and communication systems, and current tools don't adequately model these threats across domains.

Method: Developed an open-source simulation framework with a unified core that integrates physical environment, traffic, V2X networking, and supports perception and communication attacks via a single config file.

Result: Tested the framework with a real-world 3D object detector, showing detectable performance issues in realistic adversarial scenarios.

Conclusion: The framework enables detailed testing for AV robustness against multi-domain attacks and is made open-source for research use.

Abstract: Autonomous vehicles (AVs) rely on complex perception and communication
systems, making them vulnerable to adversarial attacks that can compromise
safety. While simulation offers a scalable and safe environment for robustness
testing, existing frameworks typically lack comprehensive supportfor modeling
multi-domain adversarial scenarios. This paper introduces a novel, open-source
integrated simulation framework designed to generate adversarial attacks
targeting both perception and communication layers of AVs. The framework
provides high-fidelity modeling of physical environments, traffic dynamics, and
V2X networking, orchestrating these components through a unified core that
synchronizes multiple simulators based on a single configuration file. Our
implementation supports diverse perception-level attacks on LiDAR sensor data,
along with communication-level threats such as V2X message manipulation and GPS
spoofing. Furthermore, ROS 2 integration ensures seamless compatibility with
third-party AV software stacks. We demonstrate the framework's effectiveness by
evaluating the impact of generated adversarial scenarios on a state-of-the-art
3D object detector, revealing significant performance degradation under
realistic conditions.

</details>


### [8] [Ensembling Membership Inference Attacks Against Tabular Generative Models](https://arxiv.org/abs/2509.05350)
*Joshua Ward,Yuxuan Yang,Chi-Hua Wang,Guang Cheng*

Main category: cs.CR

TL;DR: This paper addresses the challenge of selecting effective Membership Inference Attacks (MIAs) under uncertainty for auditing synthetic data privacy. It proposes unsupervised ensembling of MIAs as a robust, regret-minimizing strategy superior to individual attacks.


<details>
  <summary>Details</summary>
Motivation: Existing MIAs lack a universally dominant method across diverse model architectures and datasets, creating a gap where adversaries must choose an empirically optimal attack without a priori knowledge. Decision theory under uncertainty motivates the need for robust strategies.

Method: 1) Frame MIA selection as a decision-theoretic problem under uncertainty. 2) Conduct the largest synthetic data privacy benchmark to date. 3) Propose unsupervised ensemble methods combining individual MIAs.

Result: 1) No single MIA strictly dominates across all model/dataset combinations. 2) Unsupervised ensembles consistently outperform individual attacks by minimizing regret (26-43% improvement in precision). 3) Ensemble robustness holds across 12 models (GANs, VAEs) and 6 diverse datasets.

Conclusion: Ensemble MIAs provide empirically more robust privacy auditing under uncertainty. Results demonstrate the necessity of ensemble approaches to account for model/dataset variability in adversarial privacy analysis.

Abstract: Membership Inference Attacks (MIAs) have emerged as a principled framework
for auditing the privacy of synthetic data generated by tabular generative
models, where many diverse methods have been proposed that each exploit
different privacy leakage signals. However, in realistic threat scenarios, an
adversary must choose a single method without a priori guarantee that it will
be the empirically highest performing option. We study this challenge as a
decision theoretic problem under uncertainty and conduct the largest synthetic
data privacy benchmark to date. Here, we find that no MIA constitutes a
strictly dominant strategy across a wide variety of model architectures and
dataset domains under our threat model. Motivated by these findings, we propose
ensemble MIAs and show that unsupervised ensembles built on individual attacks
offer empirically more robust, regret-minimizing strategies than individual
attacks.

</details>


### [9] [AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and Conversational Scambaiting by Leveraging LLMs and Federated Learning](https://arxiv.org/abs/2509.05362)
*Ismail Hossain,Sai Puppala,Sajedul Talukder,Md Jahangir Alam*

Main category: cs.CR

TL;DR: The paper introduces a privacy-preserving AI framework that proactively detects and disrupts real-time social engineering scams using instruction-tuned AI, federated learning, and a safety-aware utility function, achieving high engagement and low privacy leakage.


<details>
  <summary>Details</summary>
Motivation: Existing scam defenses are reactive and ineffective during active interactions. Real-time social engineering attacks require proactive, privacy-preserving solutions to protect users dynamically across digital platforms.

Method: The system combines instruction-tuned AI with a safety-aware utility function that balances engagement and harm minimization. Federated learning enables model updates without raw data sharing, while guard models (e.g., LlamaGuard) moderate safety. The framework operates in real time to disrupt scam conversations.

Result: Experimental results show fluent responses (perplexity 22.3, engagement ≈0.80), strong relevance (≈0.74), and minimal PII leakage (≤0.0085) over 30 federated rounds. Human studies confirm improved realism/safety over baselines. Stricter moderation reduces privacy risks but limits engagement, while relaxed settings enhance scam detection but increase privacy trade-offs.

Conclusion: The framework is the first to unify real-time scam baiting, federated privacy preservation, and calibrated safety moderation, demonstrating effective proactive defense without sacrificing performance. It highlights actionable trade-offs between privacy and engagement in safety-critical systems.

Abstract: Scams exploiting real-time social engineering -- such as phishing,
impersonation, and phone fraud -- remain a persistent and evolving threat
across digital platforms. Existing defenses are largely reactive, offering
limited protection during active interactions. We propose a privacy-preserving,
AI-in-the-loop framework that proactively detects and disrupts scam
conversations in real time. The system combines instruction-tuned artificial
intelligence with a safety-aware utility function that balances engagement with
harm minimization, and employs federated learning to enable continual model
updates without raw data sharing. Experimental evaluations show that the system
produces fluent and engaging responses (perplexity as low as 22.3, engagement
$\approx$0.80), while human studies confirm significant gains in realism,
safety, and effectiveness over strong baselines. In federated settings, models
trained with FedAvg sustain up to 30 rounds while preserving high engagement
($\approx$0.80), strong relevance ($\approx$0.74), and low PII leakage
($\leq$0.0085). Even with differential privacy, novelty and safety remain
stable, indicating that robust privacy can be achieved without sacrificing
performance. The evaluation of guard models (LlamaGuard, LlamaGuard2/3,
MD-Judge) shows a straightforward pattern: stricter moderation settings reduce
the chance of exposing personal information, but they also limit how much the
model engages in conversation. In contrast, more relaxed settings allow longer
and richer interactions, which improve scam detection, but at the cost of
higher privacy risk. To our knowledge, this is the first framework to unify
real-time scam-baiting, federated privacy preservation, and calibrated safety
moderation into a proactive defense paradigm.

</details>


### [10] [A Framework for Detection and Classification of Attacks on Surveillance Cameras under IoT Networks](https://arxiv.org/abs/2509.05366)
*Umair Amjid,M. Umar Khan,S. A. Manan Kirmani*

Main category: cs.CR

TL;DR: This paper proposes an AI-driven framework to secure IoT cameras from attacks like brute force and DOS by analyzing network traffic with machine learning, trained on real-world datasets to enhance threat detection and response.


<details>
  <summary>Details</summary>
Motivation: The study addresses the growing security vulnerabilities in IoT networks, particularly surveillance cameras, which are susceptible to unauthorized access, spying, and Denial of Service attacks due to increasing IoT adoption.

Method: The method involves training a machine learning-based framework on real-world datasets to analyze network traffic, identify anomalous behavior, and detect potential intrusions like brute force, zero-day, and DOS attacks.

Result: The framework demonstrates improved intrusion detection capabilities by learning from historical security data, enabling rapid identification and mitigation of threats in IoT camera networks.

Conclusion: The paper concludes that an AI-based framework using machine learning algorithms effectively enhances security for IoT surveillance cameras by detecting and responding to intrusion threats in real-time.

Abstract: The increasing use of Internet of Things (IoT) devices has led to a rise in
security related concerns regarding IoT Networks. The surveillance cameras in
IoT networks are vulnerable to security threats such as brute force and
zero-day attacks which can lead to unauthorized access by hackers and potential
spying on the users activities. Moreover, these cameras can be targeted by
Denial of Service (DOS) attacks, which will make it unavailable for the user.
The proposed AI based framework will leverage machine learning algorithms to
analyze network traffic and detect anomalous behavior, allowing for quick
detection and response to potential intrusions. The framework will be trained
and evaluated using real-world datasets to learn from past security incidents
and improve its ability to detect potential intrusion.

</details>


### [11] [Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs](https://arxiv.org/abs/2509.05367)
*Shei Pern Chua,Thai Zhen Leng,Teh Kai Jun,Xiao Li,Xiaolin Hu*

Main category: cs.CR

TL;DR: This paper introduces TRIAL, a context-aware adversarial framework exploiting LLMs' ethical reasoning to bypass safety safeguards, highlighting urgent gaps in current AI alignment strategies.


<details>
  <summary>Details</summary>
Motivation: Traditional jailbreak attacks rely on single-step methods, leaving multi-turn, context-aware adversarial strategies underexplored despite sophisticated LLM reasoning abilities that may introduce new security risks.

Method: TRIAL (Trolley-problem Reasoning for Interactive Attack Logic) embeds adversarial goals within ethical dilemmas modeled on the trolley problem, leveraging LLMs' ethical reasoning capabilities to bypass safeguards through multi-turn, context-aware interactions.

Result: TRIAL demonstrates high jailbreak success rates against both open-source and closed-source large language models, validating its effectiveness in exploiting ethical reasoning frameworks to bypass safety measures.

Conclusion: The paper underscores a critical limitation in AI safety alignment as models gain advanced reasoning abilities, allowing more covert vulnerabilities to be exploited. It urges reevaluating safety oversight strategies to address context-aware adversarial attacks.

Abstract: Large language models (LLMs) have undergone safety alignment efforts to
mitigate harmful outputs. However, as LLMs become more sophisticated in
reasoning, their intelligence may introduce new security risks. While
traditional jailbreak attacks relied on singlestep attacks, multi-turn
jailbreak strategies that adapt dynamically to context remain underexplored. In
this work, we introduce TRIAL (Trolley-problem Reasoning for Interactive Attack
Logic), a framework that leverages LLMs ethical reasoning to bypass their
safeguards. TRIAL embeds adversarial goals within ethical dilemmas modeled on
the trolley problem. TRIAL demonstrates high jailbreak success rates towards
both open and close-source models. Our findings underscore a fundamental
limitation in AI safety: as models gain advanced reasoning abilities, the
nature of their alignment may inadvertently allow for more covert security
vulnerabilities to be exploited. TRIAL raises an urgent need in reevaluating
safety alignment oversight strategies, as current safeguards may prove
insufficient against context-aware adversarial attack.

</details>


### [12] [Quantum AI Algorithm Development for Enhanced Cybersecurity: A Hybrid Approach to Malware Detection](https://arxiv.org/abs/2509.05370)
*Tanya Joshi,Krishnendu Guha*

Main category: cs.CR

TL;DR: This paper demonstrates quantum machine learning's superiority in cybersecurity threat detection, achieving 94-95% accuracy through QML algorithms that exploit quantum principles. It introduces a real-time malware analysis framework with quantum feature extraction and explainable AI, offering interpretable results for complex malware classification tasks.


<details>
  <summary>Details</summary>
Motivation: Classical machine learning struggles with obfuscated malware patterns and high-dimensional cybersecurity data. This work addresses these limitations by exploring quantum machine learning's potential to identify complex patterns imperceptible to classical models.

Method: The study implements QML algorithms (QNN, QSVM, QCNN) using quantum superposition/entanglement principles. It proposes a real-time malware analysis framework employing Quantum Fourier Transform for feature extraction, quantum feature maps, and variational quantum circuits for classification. Explainable AI methods (GradCAM++, ScoreCAM) are integrated for interpretability.

Result: QML methods achieved 95% accuracy for Quantum Neural Networks (QNN) and 94% for Quantum Support Vector Machines (QSVM) in malware classification, surpassing classical approaches. Quantum methods effectively identified obfuscated malware patterns in datasets with 56-57 memory-based features from 58,596 samples.

Conclusion: Quantum Machine Learning (QML) outperforms classical methods in detecting sophisticated malware and intrusion patterns, leveraging principles like superposition and entanglement. The paper introduces a novel real-time framework integrating quantum feature extraction and explainable AI techniques (GradCAM++, ScoreCAM) for interpretable threat detection.

Abstract: This study explores the application of quantum machine learning (QML)
algorithms to enhance cybersecurity threat detection, particularly in the
classification of malware and intrusion detection within high-dimensional
datasets. Classical machine learning approaches encounter limitations when
dealing with intricate, obfuscated malware patterns and extensive network
intrusion data. To address these challenges, we implement and evaluate various
QML algorithms, including Quantum Neural Networks (QNN), Quantum Support Vector
Machines (QSVM), and hybrid Quantum Convolutional Neural Networks (QCNN) for
malware detection tasks. Our experimental analysis utilized two datasets: the
Intrusion dataset, comprising 150 samples with 56 memory-based features derived
from Volatility framework analysis, and the ObfuscatedMalMem2022 dataset,
containing 58,596 samples with 57 features representing benign and malicious
software. Remarkably, our QML methods demonstrated superior performance
compared to classical approaches, achieving accuracies of 95% for QNN and 94%
for QSVM. These quantum-enhanced methods leveraged quantum superposition and
entanglement principles to accurately identify complex patterns within highly
obfuscated malware samples that were imperceptible to classical methods. To
further advance malware analysis, we propose a novel real-time malware analysis
framework that incorporates Quantum Feature Extraction using Quantum Fourier
Transform, Quantum Feature Maps, and Classification using Variational Quantum
Circuits. This system integrates explainable AI methods, including GradCAM++
and ScoreCAM algorithms, to provide interpretable insights into the quantum
decision-making processes.

</details>


### [13] [Privacy Preservation and Identity Tracing Prevention in AI-Driven Eye Tracking for Interactive Learning Environments](https://arxiv.org/abs/2509.05376)
*Abdul Rehman,Are Dæhlen,Ilona Heldal,Jerry Chun-wei Lin*

Main category: cs.CR

TL;DR: This paper proposes a privacy-preserving human-centered framework for AI-powered eye-tracking in education, balancing pedagogical value with identity protection through anonymization, ethical design, and Federated Learning (FL).


<details>
  <summary>Details</summary>
Motivation: Eye-tracking in educational AI poses privacy risks by linking sensitive data to individuals, necessitating solutions that maintain diagnostic utility while preventing identity backtracking.

Method: A two-phase framework: Phase 1 evaluates backtracking risks via four scenarios (diagnosis prediction, student ID prediction, randomization, K-Means classification). Phase 2 implements privacy measures using FL, dummy IDs, and administrator-controlled access.

Result: Phase 1 achieved 99.3-99.7 % diagnostic accuracy while limiting student ID prediction (63 %). Phase 2 achieved 99.40 % accuracy overall with full backtracking prevention and secure identity management.

Conclusion: The framework successfully preserves privacy and diagnostic capabilities in eye-tracking for education, demonstrating compliance with GDPR and ethical AI through technical-philosophical integration.

Abstract: Eye-tracking technology can aid in understanding neurodevelopmental disorders
and tracing a person's identity. However, this technology poses a significant
risk to privacy, as it captures sensitive information about individuals and
increases the likelihood that data can be traced back to them. This paper
proposes a human-centered framework designed to prevent identity backtracking
while preserving the pedagogical benefits of AI-powered eye tracking in
interactive learning environments. We explore how real-time data anonymization,
ethical design principles, and regulatory compliance (such as GDPR) can be
integrated to build trust and transparency. We first demonstrate the potential
for backtracking student IDs and diagnoses in various scenarios using serious
game-based eye-tracking data. We then provide a two-stage privacy-preserving
framework that prevents participants from being tracked while still enabling
diagnostic classification. The first phase covers four scenarios: I) Predicting
disorder diagnoses based on different game levels. II) Predicting student IDs
based on different game levels. III) Predicting student IDs based on randomized
data. IV) Utilizing K-Means for out-of-sample data. In the second phase, we
present a two-stage framework that preserves privacy. We also employ Federated
Learning (FL) across multiple clients, incorporating a secure identity
management system with dummy IDs and administrator-only access controls. In the
first phase, the proposed framework achieved 99.3% accuracy for scenario 1, 63%
accuracy for scenario 2, and 99.7% accuracy for scenario 3, successfully
identifying and assigning a new student ID in scenario 4. In phase 2, we
effectively prevented backtracking and established a secure identity management
system with dummy IDs and administrator-only access controls, achieving an
overall accuracy of 99.40%.

</details>


### [14] [ThreatGPT: An Agentic AI Framework for Enhancing Public Safety through Threat Modeling](https://arxiv.org/abs/2509.05379)
*Sharif Noor Zisad,Ragib Hasan*

Main category: cs.CR

TL;DR: The proposed ThreatGPT is an AI assistant designed to help individuals like engineers, safety officers, and policymakers analyze threats in smart city systems using popular cybersecurity frameworks such as STRIDE, MITRE ATT&CK, CVE reports, NIST, and CISA. It allows users to generate smart threat models with minimal cybersecurity expertise by simply describing system components and offering actionable prevention strategies.


<details>
  <summary>Details</summary>
Motivation: The complexity of smart city systems increases the risk of security threats, which can impact not only machines but also the safety of real people. These systems require expert knowledge to identify and address threats, which may be a barrier for non-experts such as policymakers or safety officers attempting to evaluate threat models for their urban infrastructure.

Method: ThreatGPT is an agentic AI assistant that utilizes few-shot learning to understand system components based on user descriptions. It can interactively use popular cybersecurity frameworks (e.g., MITRE ATT&CK, NIST) to generate customized threat models. The AI does not require complex querying or deep cybersecurity expertise, as it learns from examples and adapts to the needs of different users such as engineers, policymakers, and safety officers.

Result: ThreatGPT simplifies threat analysis in smart city and public safety infrastructure by allowing users to generate comprehensive threat models using intuitive inputs. The system enables users to evaluate how attackers might exploit vulnerabilities and gain insights into prevention strategies efficiently, even without deep cybersecurity knowledge.

Conclusion: ThreatGPT represents a step towards making AI and cybersecurity more accessible, enhancing the safety of smart city systems by bridging the expertise gap. It empowers users with timely, accurate, and actionable threat analysis for various public safety domains, demonstrating the potential of agentic AI as a collaborative tool.

Abstract: As our cities and communities become smarter, the systems that keep us safe,
such as traffic control centers, emergency response networks, and public
transportation, also become more complex. With this complexity comes a greater
risk of security threats that can affect not just machines but real people's
lives. To address this challenge, we present ThreatGPT, an agentic Artificial
Intelligence (AI) assistant built to help people whether they are engineers,
safety officers, or policy makers to understand and analyze threats in public
safety systems. Instead of requiring deep cybersecurity expertise, it allows
users to simply describe the components of a system they are concerned about,
such as login systems, data storage, or communication networks. Then, with the
click of a button, users can choose how they want the system to be analyzed by
using popular frameworks such as STRIDE, MITRE ATT&CK, CVE reports, NIST, or
CISA. ThreatGPT is unique because it does not just provide threat information,
but rather it acts like a knowledgeable partner. Using few-shot learning, the
AI learns from examples and generates relevant smart threat models. It can
highlight what might go wrong, how attackers could take advantage, and what can
be done to prevent harm. Whether securing a city's infrastructure or a local
health service, this tool adapts to users' needs. In simple terms, ThreatGPT
brings together AI and human judgment to make our public systems safer. It is
designed not just to analyze threats, but to empower people to understand and
act on them, faster, smarter, and with more confidence.

</details>


### [15] [Behind the Mask: Benchmarking Camouflaged Jailbreaks in Large Language Models](https://arxiv.org/abs/2509.05471)
*Youjia Zheng,Mohammad Zandsalimy,Shanu Sushmita*

Main category: cs.CR

TL;DR: A study reveals LLMs' vulnerability to camouflaged jailbreak prompts, which evade existing safeguards. A new dataset and framework highlight major shortcomings in safety measures, urging improved defenses to secure real-world LLM use.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the escalating threat of camouflaged jailbreaking, where malicious intent is hidden in seemingly benign prompts to bypass safety mechanisms. Existing keyword-based defenses fail to detect such subtle attacks, motivating the need for advanced evaluation strategies.

Method: The paper introduces a benchmark dataset (500 examples) and a multi-faceted evaluation framework with seven dimensions (e.g., Safety Awareness, Harmful Potential) to rigorously test LLMs' responses to camouflaged jailbreak prompts.

Result: LLMs exhibit high safety and content quality with benign inputs but show significant decline in performance and safety when tested against camouflaged jailbreak prompts. This reveals a critical vulnerability in current defense systems.

Conclusion: The paper concludes that current LLM safety measures are insufficient against camouflaged jailbreaking attacks. It highlights the urgent need for adaptive, nuanced security strategies to address vulnerabilities in handling contextually deceptive prompts, ensuring safe real-world deployment of LLMs.

Abstract: Large Language Models (LLMs) are increasingly vulnerable to a sophisticated
form of adversarial prompting known as camouflaged jailbreaking. This method
embeds malicious intent within seemingly benign language to evade existing
safety mechanisms. Unlike overt attacks, these subtle prompts exploit
contextual ambiguity and the flexible nature of language, posing significant
challenges to current defense systems. This paper investigates the construction
and impact of camouflaged jailbreak prompts, emphasizing their deceptive
characteristics and the limitations of traditional keyword-based detection
methods. We introduce a novel benchmark dataset, Camouflaged Jailbreak Prompts,
containing 500 curated examples (400 harmful and 100 benign prompts) designed
to rigorously stress-test LLM safety protocols. In addition, we propose a
multi-faceted evaluation framework that measures harmfulness across seven
dimensions: Safety Awareness, Technical Feasibility, Implementation Safeguards,
Harmful Potential, Educational Value, Content Quality, and Compliance Score.
Our findings reveal a stark contrast in LLM behavior: while models demonstrate
high safety and content quality with benign inputs, they exhibit a significant
decline in performance and safety when confronted with camouflaged jailbreak
attempts. This disparity underscores a pervasive vulnerability, highlighting
the urgent need for more nuanced and adaptive security strategies to ensure the
responsible and robust deployment of LLMs in real-world applications.

</details>


### [16] [What is Cybersecurity in Space?](https://arxiv.org/abs/2509.05496)
*Charbel Mattar,Jacques Bou Abdo,Abdallah Makhoul,Benoit Piranda,Jacques Demerjian*

Main category: cs.CR

TL;DR: This paper identifies 11 research gaps in space cybersecurity for satellites/drones/5G, proposes a multi-agent AI defense system, and outlines a 5-year roadmap focused on proactive resilience.


<details>
  <summary>Details</summary>
Motivation: Modern space-critical infrastructure lacks cyber resilience as ground stations can be hacked, GPS jammed, and no shared vulnerability database exists, risking global services like air traffic and finance.

Method: Mapped 11 research gaps with challenge/impact/question triads, recommending agentic AI (multi-agent systems) over monolithic models for onboard defense coordination.

Result: Framework of research priorities including post-quantum crypto trials, open cyber-testing ranges, and multi-agent deployment timelines to enable proactive defense strategies.

Conclusion: A transition from reactive patching to proactive resilience is achievable through standardized vulnerability sharing, QKD experiments, and multi-agent architectures within 5 years.

Abstract: Satellites, drones, and 5G space links now support
  critical services such as air traffic, finance, and weather. Yet most
  were not built to resist modern cyber threats. Ground stations
  can be breached, GPS jammed, and supply chains compromised,
  while no shared list of vulnerabilities or safe testing range exists.
  This paper maps eleven research gaps, including secure
  routing, onboard intrusion detection, recovery methods, trusted
  supply chains, post-quantum encryption, zero-trust architectures,
  and real-time impact monitoring. For each, we outline the
  challenge, why it matters, and a guiding research question. We
  also highlight an agentic (multi-agent) AI approach where small,
  task-specific agents share defense tasks onboard instead of one
  large model.
  Finally, we propose a five-year roadmap: post-quantum and
  QKD flight trials, open cyber-ranges, clearer vulnerability shar ing, and
early multi-agent deployments. These steps move space
  cybersecurity from reactive patching toward proactive resilience.

</details>


### [17] [Secure and Efficient $L^p$-Norm Computation for Two-Party Learning Applications](https://arxiv.org/abs/2509.05552)
*Ali Arastehfard,Weiran Liu,Joshua Lee,Bingyu Liu,Xuegang Ban,Yuan Hong*

Main category: cs.CR

TL;DR: The paper proposes Crypto-$L^p$, the first comprehensive secure two-party $L^p$-norm computation framework for $L^1$, $L^2$, and $L^\infty$, achieving significant performance improvements over existing methods and enabling efficient secure machine learning inference.


<details>
  <summary>Details</summary>
Motivation: Existing cryptographic systems lack a general framework for secure $L^p$-norm computation, focus only on $L^2$, and use inefficient black-box approaches. Real-world applications like machine learning and location-based services require optimized protocols for all common $L^p$-norms.

Method: Designed, implemented, and evaluated a tailored framework (Crypto-$L^p$) supporting all $L^p$-norms. Conducted benchmarks against state-of-the-art protocols using real-world datasets and applications.

Result: Achieves 82×-42× runtime improvements and 36×-21× communication reductions compared to prior work. Reduces machine learning inference communication costs by 3× over SOTA while maintaining comparable runtime and accuracy.

Conclusion: Crypto-$L^p$ establishes the first practical, secure $L^p$-norm framework that outperforms existing solutions across all measured metrics. It enables efficient secure machine learning inference and provides a versatile foundation for privacy-preserving applications.

Abstract: Secure norm computation is becoming increasingly important in many real-world
learning applications. However, existing cryptographic systems often lack a
general framework for securely computing the $L^p$-norm over private inputs
held by different parties. These systems often treat secure norm computation as
a black-box process, neglecting to design tailored cryptographic protocols that
optimize performance. Moreover, they predominantly focus on the $L^2$-norm,
paying little attention to other popular $L^p$-norms, such as $L^1$ and
$L^\infty$, which are commonly used in practice, such as machine learning tasks
and location-based services.
  To our best knowledge, we propose the first comprehensive framework for
secure two-party $L^p$-norm computations ($L^1$, $L^2$, and $L^\infty$),
denoted as \mbox{Crypto-$L^p$}, designed to be versatile across various
applications. We have designed, implemented, and thoroughly evaluated our
framework across a wide range of benchmarking applications, state-of-the-art
(SOTA) cryptographic protocols, and real-world datasets to validate its
effectiveness and practical applicability. In summary, \mbox{Crypto-$L^p$}
outperforms prior works on secure $L^p$-norm computation, achieving $82\times$,
$271\times$, and $42\times$ improvements in runtime while reducing
communication overhead by $36\times$, $4\times$, and $21\times$ for $p=1$, $2$,
and $\infty$, respectively. Furthermore, we take the first step in adapting our
Crypto-$L^p$ framework for secure machine learning inference, reducing
communication costs by $3\times$ compared to SOTA systems while maintaining
comparable runtime and accuracy.

</details>


### [18] [Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints](https://arxiv.org/abs/2509.05608)
*Waris Gill,Natalie Isak,Matthew Dressman*

Main category: cs.CR

TL;DR: BinaryShield is a privacy-preserving threat intelligence system for LLMs that allows secure sharing of attack fingerprints across compliance boundaries using a unique pipeline of PII redaction, semantic embedding, binary quantization, and randomized response, achieving better performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: Organizations face a security challenge with LLMs as they can't share threat intelligence about prompt injection attacks due to privacy regulations, leading to prolonged undetected threats in other services.

Method: BinaryShield employs a pipeline combining four steps: PII redaction to remove sensitive information, semantic embedding to capture attack context, binary quantization to compress embeddings, and a randomized response mechanism to enhance privacy while preserving attack patterns.

Result: BinaryShield achieves an impressive F1-score of 0.94 for identifying malicious prompts, a 0.77 F1-score baseline for SimHash, and results in a 64x reduction in storage requirements and 38x faster similarity search compared to using dense embeddings.

Conclusion: BinaryShield advances the state of the art for secure threat intelligence sharing among LLM services by effectively preserving privacy while accurately detecting prompt injection attacks, offering a robust solution to a critical industry problem.

Abstract: The widespread deployment of LLMs across enterprise services has created a
critical security blind spot. Organizations operate multiple LLM services
handling billions of queries daily, yet regulatory compliance boundaries
prevent these services from sharing threat intelligence about prompt injection
attacks, the top security risk for LLMs. When an attack is detected in one
service, the same threat may persist undetected in others for months, as
privacy regulations prohibit sharing user prompts across compliance boundaries.
  We present BinaryShield, the first privacy-preserving threat intelligence
system that enables secure sharing of attack fingerprints across compliance
boundaries. BinaryShield transforms suspicious prompts through a unique
pipeline combining PII redaction, semantic embedding, binary quantization, and
randomized response mechanism to potentially generate non-invertible
fingerprints that preserve attack patterns while providing privacy. Our
evaluations demonstrate that BinaryShield achieves an F1-score of 0.94,
significantly outperforming SimHash (0.77), the privacy-preserving baseline,
while achieving 64x storage reduction and 38x faster similarity search compared
to dense embeddings.

</details>


### [19] [FuzzBox: Blending Fuzzing into Emulation for Binary-Only Embedded Targets](https://arxiv.org/abs/2509.05643)
*Carmine Cesarano,Roberto Natella*

Main category: cs.CR

TL;DR: FuzzBox enables fuzzing in closed-source industrial systems via runtime instrumentation, overcoming recompilation and hardware dependency limitations.


<details>
  <summary>Details</summary>
Motivation: Traditional coverage-guided fuzzing is hindered in industrial systems due to closed-source toolchains, proprietary compilers, and lack of source code access, limiting its ability to detect zero-day vulnerabilities in such environments.

Method: FuzzBox integrates emulation with fuzzing through dynamic instrumentation in a virtualized environment, allowing fuzz input injection, failure detection, and coverage analysis without hardware dependencies or recompilation.

Result: FuzzBox demonstrates effectiveness on a proprietary MILS hypervisor for industrial applications and shows broad portability across commercial IoT firmware.

Conclusion: FuzzBox effectively addresses the limitations of traditional fuzzing in industrial systems by enabling dynamic instrumentation without requiring source code recompilation, making vulnerability detection feasible in proprietary environments.

Abstract: Coverage-guided fuzzing has been widely applied to address zero-day
vulnerabilities in general-purpose software and operating systems. This
approach relies on instrumenting the target code at compile time. However,
applying it to industrial systems remains challenging, due to proprietary and
closed-source compiler toolchains and lack of access to source code. FuzzBox
addresses these limitations by integrating emulation with fuzzing: it
dynamically instruments code during execution in a virtualized environment, for
the injection of fuzz inputs, failure detection, and coverage analysis, without
requiring source code recompilation and hardware-specific dependencies. We show
the effectiveness of FuzzBox through experiments in the context of a
proprietary MILS (Multiple Independent Levels of Security) hypervisor for
industrial applications. Additionally, we analyze the applicability of FuzzBox
across commercial IoT firmware, showcasing its broad portability.

</details>


### [20] [SEASONED: Semantic-Enhanced Self-Counterfactual Explainable Detection of Adversarial Exploiter Contracts](https://arxiv.org/abs/2509.05681)
*Xng Ai,Shudan Lin,Zecheng Li,Kai Zhou,Bixin Li,Bin Xiao*

Main category: cs.CR

TL;DR: This paper addresses the explainable detection of Adversarial Exploiter Contracts (AECs) in DeFi by introducing SEASONED, a framework combining semantic relation graphs and self-counterfactual explanations.


<details>
  <summary>Details</summary>
Motivation: Existing AEC detection methods fail to capture semantic dependencies and lack interpretability, creating critical gaps in threat analysis.

Method: SEASONED constructs semantic relation graphs from contract bytecode and uses a self-counterfactual detector (SCFED) to classify SRGs while generating attack-explaining logic.

Result: Experiments show SEASONED achieves strong detection performance, robustness, generalizability, and data efficiency, validated by theoretical analysis.

Conclusion: SEASONED provides a robust, explainable AEC detection framework, supported by a released dataset of 359 AECs for future research.

Abstract: Decentralized Finance (DeFi) attacks have resulted in significant losses,
often orchestrated through Adversarial Exploiter Contracts (AECs) that exploit
vulnerabilities in victim smart contracts. To proactively identify such
threats, this paper targets the explainable detection of AECs.
  Existing detection methods struggle to capture semantic dependencies and lack
interpretability, limiting their effectiveness and leaving critical knowledge
gaps in AEC analysis. To address these challenges, we introduce SEASONED, an
effective, self-explanatory, and robust framework for AEC detection.
  SEASONED extracts semantic information from contract bytecode to construct a
semantic relation graph (SRG), and employs a self-counterfactual explainable
detector (SCFED) to classify SRGs and generate explanations that highlight the
core attack logic. SCFED further enhances robustness, generalizability, and
data efficiency by extracting representative information from these
explanations. Both theoretical analysis and experimental results demonstrate
the effectiveness of SEASONED, which showcases outstanding detection
performance, robustness, generalizability, and data efficiency learning
ability. To support further research, we also release a new dataset of 359
AECs.

</details>


### [21] [KnowHow: Automatically Applying High-Level CTI Knowledge for Interpretable and Accurate Provenance Analysis](https://arxiv.org/abs/2509.05698)
*Yuhan Meng,Shaofei Li,Jiaping Gui,Peng Jiang,Ding Li*

Main category: cs.CR

TL;DR: KnowHow is a CTI-knowledge-driven approach that bridges the semantic gap between high-level natural language attack knowledge and low-level system events for APT detection, achieving high accuracy and robustness against false positives.


<details>
  <summary>Details</summary>
Motivation: Automatically applying high-level CTI knowledge (e.g., ATT&CK) to low-level system events remains a challenge due to semantic gaps, and manual approaches are labor-intensive and error-prone.

Method: KnowHow introduces gIoC, a structured attack knowledge representation, to lift system identifiers (e.g., file paths) to natural language terms. It matches system events to gIoC and techniques from CTI reports, then reasons through temporal logic to detect APTs.

Result: KnowHow detects all 16 APT campaigns in datasets with no false positives, reduces node-level false positives by up to 90% compared to existing methods, and demonstrates robustness against unknown/mimic attacks.

Conclusion: KnowHow effectively applies CTI knowledge to system events, enabling accurate and efficient APT detection while outperforming existing approaches in precision and robustness.

Abstract: High-level natural language knowledge in CTI reports, such as the ATT&CK
framework, is beneficial to counter APT attacks. However, how to automatically
apply the high-level knowledge in CTI reports in realistic attack detection
systems, such as provenance analysis systems, is still an open problem. The
challenge stems from the semantic gap between the knowledge and the low-level
security logs: while the knowledge in CTI reports is written in natural
language, attack detection systems can only process low-level system events
like file accesses or network IP manipulations. Manual approaches can be
labor-intensive and error-prone.
  In this paper, we propose KnowHow, a CTI-knowledge-driven online provenance
analysis approach that can automatically apply high-level attack knowledge from
CTI reports written in natural languages to detect low-level system events. The
core of KnowHow is a novel attack knowledge representation, gIoC, that
represents the subject, object, and actions of attacks. By lifting system
identifiers, such as file paths, in system events to natural language terms,
KnowHow can match system events to gIoC and further match them to techniques
described in natural languages. Finally, based on the techniques matched to
system events, KnowHow reasons about the temporal logic of attack steps and
detects potential APT attacks in system events. Our evaluation shows that
KnowHow can accurately detect all 16 APT campaigns in the open-source and
industrial datasets, while existing approaches all introduce large numbers of
false positives. Meanwhile, our evaluation also shows that KnowHow reduces at
most 90% of node-level false positives while having a higher node-level recall
and is robust against several unknown attacks and mimicry attacks.

</details>


### [22] [VehiclePassport: A GAIA-X-Aligned, Blockchain-Anchored Privacy-Preserving, Zero-Knowledge Digital Passport for Smart Vehicles](https://arxiv.org/abs/2509.06133)
*Pradyumna Kaushal*

Main category: cs.CR

TL;DR: Blockchain-based vehicle passport system with ZKPs ensures verifiable, private lifecycle records at <$0.02/event for global mobility applications.


<details>
  <summary>Details</summary>
Motivation: Fragmented vehicle records across stakeholders enable fraud and lack verification; paper-based KYC systems are incompatible with modern data privacy requirements.

Method: Leverages Polygon zkEVM for hash anchoring, Groth16 proofs for validation, and short-lived JWTs for selective disclosure in a GAIA-X-aligned architecture.

Result: Achieves <$0.02/event cost, sub-10ms proof validation, and scalability for 100M+ vehicles through open-source optimizations on zkEVM infrastructure.

Conclusion: VehiclePassport establishes a trustless blockchain framework with ZKPs to secure vehicle lifecycle data, enabling GDPR-compliant, fraud-resistant applications across mobility markets.

Abstract: Modern vehicles accumulate fragmented lifecycle records across OEMs, owners,
and service centers that are difficult to verify and prone to fraud. We propose
VehiclePassport, a GAIA-X-aligned digital passport anchored on blockchain with
zero-knowledge proofs (ZKPs) for privacy-preserving verification.
VehiclePassport immutably commits to manufacturing, telemetry, and service
events while enabling selective disclosure via short-lived JWTs and Groth16
proofs. Our open-source reference stack anchors hashes on Polygon zkEVM at
<$0.02 per event, validates proofs in <10 ms, and scales to millions of
vehicles. This architecture eliminates paper-based KYC, ensures GDPR-compliant
traceability, and establishes a trustless foundation for insurance, resale, and
regulatory applications in global mobility data markets.

</details>


### [23] [Larger-scale Nakamoto-style Blockchains Offer Better Security](https://arxiv.org/abs/2509.05708)
*Junjie Hu,Na Ruan*

Main category: cs.CR

TL;DR: This paper introduces a dual-delay framework to correct traditional blockchain security analysis by quantifying how internal communication delays between adversarial and honest nodes affect consensus security, showing the 50% adversarial power assumption can be exceeded


<details>
  <summary>Details</summary>
Motivation: Traditional models assume instantaneous malicious node coordination, ignoring real-world network delays that critically impact blockchain security thresholds and consensus stability

Method: 1. Static delay model: M/D/1 queuing analysis to quantify private chain growth constraints
2. Dynamic delay model: Integrates probabilistic corruption, scale-dependent delays, and logarithmic delay growth in honest nodes

Result: Security threshold β* increases with adversarial delay Δ_a (exceeding 51% when Δ_a>Δ_h). Adversarial power decays linearly with network scale, with P(β ≤ β*)→1 as n→∞

Conclusion: The dual-delay framework establishes a theoretical foundation for scaling Nakamoto-style blockchains by characterizing the interaction between network scale, communication delays, and security boundaries

Abstract: Traditional security models for Nakamoto-style blockchains overestimate
adversarial coordination by assuming instantaneous synchronization among
malicious nodes, neglecting the critical impact of internal communication
delays on security. This paper introduces a dual-delay framework to revisit
security analysis, addressing this oversight through two key innovations.
First, the static delay model quantifies how adversarial communication delays
(\(\Delta_a\)) constrain the effective growth rate of private chains, derived
via an M/D/1 queuing model as \(\lambda_{eff} = \lambda_a / (1 + \lambda_a
\Delta_a)\). This model reveals that the security threshold (\(\beta^*\)), the
maximum adversarial power the system tolerates, increases with \(\Delta_a\),
even exceeding the classic 51\% boundary when \(\Delta_a \textgreater \Delta\)
(honest nodes' delay), breaking the long-standing 50\% assumption. Second, the
dynamic delay model integrates probabilistic corruption and scale-dependent
delays to characterize the total adversarial delay window (\(\Delta_{total} =
\Delta(n) e^{-k\beta} + c \log(1 + \beta n)\)), where \(\Delta(n) \in
\Theta(\log n)\) captures honest nodes' logarithmic delay growth. Asymptotic
analysis shows adversarial power decays linearly with network scale, ensuring
the probability of \(\beta \leq \beta^*\) approaches 1 as \(n \to \infty\). By
exposing the interplay between network scale, communication delays, and power
dilution, we provide a theoretical foundation for optimizing consensus
protocols and assessing robustness in large-scale Nakamoto-style blockchains.

</details>


### [24] [Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated](https://arxiv.org/abs/2509.05739)
*Hanna Foerster,Ilia Shumailov,Yiren Zhao,Harsh Chaudhari,Jamie Hayes,Robert Mullins,Yarin Gal*

Main category: cs.CR

TL;DR: The paper explores decomposed reasoning poison attacks, which alter the reasoning paths in LLMs without changing the final answer. It notes that while these backdoors can be injected they are hard to reliably activate because models recover from them.


<details>
  <summary>Details</summary>
Motivation: Recent LLMs with step-by-step reasoning introduce new vulnerabilities in the chain-of-thought process. The paper studies decomposed reasoning poison attacks to understand how perturbations in the reasoning path affect model behavior, possibly revealing a form of backdoor robustness.

Method: An adversarial example is created by modifying the reasoning path of a model's chain-of-thought while keeping the prompt and final answer intact. The trigger is split into harmless components across the subproblems.

Result: These decomposed poisons can be injected easily, but reliably activating them to alter the final answer is difficult. LLMs appear to recover from CoT-based backdoors, despite the attack affecting their internal reasoning.

Conclusion: LLMs may be developing an emergent form of robustness in handling backdoors by separating reasoning and answer generation. This improved resilience implies that more sophisticated or persistent methods might be necessary for attacks on advanced LLMs.

Abstract: Early research into data poisoning attacks against Large Language Models
(LLMs) demonstrated the ease with which backdoors could be injected. More
recent LLMs add step-by-step reasoning, expanding the attack surface to include
the intermediate chain-of-thought (CoT) and its inherent trait of decomposing
problems into subproblems. Using these vectors for more stealthy poisoning, we
introduce ``decomposed reasoning poison'', in which the attacker modifies only
the reasoning path, leaving prompts and final answers clean, and splits the
trigger across multiple, individually harmless components.
  Fascinatingly, while it remains possible to inject these decomposed poisons,
reliably activating them to change final answers (rather than just the CoT) is
surprisingly difficult. This difficulty arises because the models can often
recover from backdoors that are activated within their thought processes.
Ultimately, it appears that an emergent form of backdoor robustness is
originating from the reasoning capabilities of these advanced LLMs, as well as
from the architectural separation between reasoning and final answer
generation.

</details>


### [25] [Tell-Tale Watermarks for Explanatory Reasoning in Synthetic Media Forensics](https://arxiv.org/abs/2509.05753)
*Ching-Chun Chang,Isao Echizen*

Main category: cs.CR

TL;DR: Develops interpretable watermarks that evolve with synthetic media to trace manipulation history, solving digital forensics' inverse problem in an era of deepfakes.


<details>
  <summary>Details</summary>
Motivation: Synthetic media's increasing sophistication undermines public trust and obscures media provenance. Traditional forensic methods fail to address complex transformation chains involving semantic edits, color adjustments, and geometric projections, necessitating a new approach to trace generation history and detect intentional manipulations.

Method: The paper proposes an inverse problem-solving approach using transformation-class-specific tell-tale watermarks. These watermarks evolve dynamically with the synthetic media through its lifecycle, leaving interpretable traces rather than being strictly robust or fragile. Explanatory reasoning is employed to infer transformation sequences across parameter spaces.

Result: Experimental evaluations validate the system's effectiveness through three dimensions: (1) fidelity maintaining watermark integrity through transformations, (2) synchronicity ensuring watermark evolution parallels media transformations, and (3) traceability enabling reconstruction of transformation chains. The approach successfully enables forensic reconstruction of synthetic media provenance.

Conclusion: This study presents a novel solution to the challenge of forensic analysis in synthetic media by introducing interpretable tell-tale watermarks. These watermarks provide explanatory reasoning about media transformations, offering critical insights into criminal intent and enhancing forensic investigation capabilities.

Abstract: The rise of synthetic media has blurred the boundary between reality and
fabrication under the evolving power of artificial intelligence, fueling an
infodemic that erodes public trust in cyberspace. For digital imagery, a
multitude of editing applications further complicates the forensic analysis,
including semantic edits that alter content, photometric adjustments that
recalibrate colour characteristics, and geometric projections that reshape
viewpoints. Collectively, these transformations manipulate and control
perceptual interpretation of digital imagery. This susceptibility calls for
forensic enquiry into reconstructing the chain of events, thereby revealing
deeper evidential insight into the presence or absence of criminal intent. This
study seeks to address an inverse problem of tracing the underlying generation
chain that gives rise to the observed synthetic media. A tell-tale watermarking
system is developed for explanatory reasoning over the nature and extent of
transformations across the lifecycle of synthetic media. Tell-tale watermarks
are tailored to different classes of transformations, responding in a manner
that is neither strictly robust nor fragile but instead interpretable. These
watermarks function as reference clues that evolve under the same
transformation dynamics as the carrier media, leaving interpretable traces when
subjected to transformations. Explanatory reasoning is then performed to infer
the most plausible account across the combinatorial parameter space of
composite transformations. Experimental evaluations demonstrate the validity of
tell-tale watermarking with respect to fidelity, synchronicity and
traceability.

</details>


### [26] [Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based Agentic System](https://arxiv.org/abs/2509.05755)
*Yu Liu,Yuchong Xie,Mingyu Luo,Zesen Liu,Zhixiang Zhang,Kaikai Zhang,Zongjie Li,Ping Chen,Shuai Wang,Dongdong She*

Main category: cs.CR

TL;DR: Paper uncovers critical TIP security flaws in major LLM systems, demonstrates RCE/DoS attacks via TEW exploitation workflow, and establishes defenses to secure agentic tool interactions.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based agentic systems lack robust security mechanisms for tool invocation controls (TIPs), leaving them exposed to severe attacks like code execution and service disruption despite their widespread use in sensitive domains.

Method: The authors conduct a systematic analysis of TIP design flaws using a Threat Exploitation Workflow (TEW), demonstrating how malicious tool invocations can hijack external tool behaviors. They combine vulnerability scanning, attack pattern analysis, and TEW-based exploit validation.

Result: The study discovers that major systems like Cursor and Claude Code are vulnerable to TIP-based attacks, achieving remote code execution and denial of service through crafted tool invocations. Their proposed defenses significantly reduce these risks while maintaining system functionality.

Conclusion: This work highlights critical security vulnerabilities in Tool Invocation Prompts (TIPs) of LLM-based systems, demonstrating practical attacks like RCE and DoS while offering effective defense mechanisms to secure agentic workflows.

Abstract: LLM-based agentic systems leverage large language models to handle user
queries, make decisions, and execute external tools for complex tasks across
domains like chatbots, customer service, and software engineering. A critical
component of these systems is the Tool Invocation Prompt (TIP), which defines
tool interaction protocols and guides LLMs to ensure the security and
correctness of tool usage. Despite its importance, TIP security has been
largely overlooked. This work investigates TIP-related security risks,
revealing that major LLM-based systems like Cursor, Claude Code, and others are
vulnerable to attacks such as remote code execution (RCE) and denial of service
(DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate
external tool behavior hijacking via manipulated tool invocations. We also
propose defense mechanisms to enhance TIP security in LLM-based agentic
systems.

</details>


### [27] [Secure and Trustful Cross-domain Communication with Decentralized Identifiers in 5G and Beyond](https://arxiv.org/abs/2509.05797)
*Hai Dinh-Tuan,Sandro Rodriguez Garzon,Jianeng Fu*

Main category: cs.CR

TL;DR: This paper proposes DID-based communication for 5G+ networks, showing enhanced security over TCP/TLS but with performance trade-offs, indicating promise for future optimization.


<details>
  <summary>Details</summary>
Motivation: Future mobile networks require secure, trustful cross-domain communication to support dynamic interactions among network components, necessitating robust solutions like W3C-endorsed DIDs to address vulnerabilities in existing protocols.

Method: The paper introduces a DID-based communication agent integrated with 5G-standardized functions, employs a DID-based application layer transport protocol, and evaluates two protocol iterations through comparative analysis and overhead assessment against TCP/TLS.

Result: The latest DID protocol version demonstrates compatibility advantages, while evaluations reveal security improvements over TCP/TLS at the cost of increased communication overhead, underscoring both the viability and performance trade-offs of DID-based communication.

Conclusion: The study highlights the potential of DID-based communication in enhancing security for future mobile networks but identifies the need for further optimization to mitigate performance losses compared to traditional protocols.

Abstract: In the evolving landscape of future mobile networks, there is a critical need
for secure and trustful communication modalities to support dynamic
interactions among core network components of different network domains. This
paper proposes the application of W3C-endorsed Decentralized Identifiers (DIDs)
to establish secure and trustful communication channels among network functions
in 5G and subsequent generations. A new communication agent is introduced that
integrates seamlessly with 5G-standardized network functions and utilizes a
DID-based application layer transport protocol to ensure confidentiality,
integrity, and authenticity for cross-domain interactions. A comparative
analysis of the two different versions of the DID-based communication protocol
for inter network function communication reveals compatibility advantages of
the latest protocol iteration. Furthermore, a comprehensive evaluation of the
communication overhead caused by both protocol iterations compared to
traditional TCP/TLS shows the benefits of using DIDs to improve communication
security, albeit with performance loses compared to TCP/TLS. These results
uncover the potential of DID-based communication for future mobile networks but
also point out areas for optimization.

</details>


### [28] [Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization](https://arxiv.org/abs/2509.05831)
*Ishaan Verma*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) are increasingly integrated into web-based
systems for content summarization, yet their susceptibility to prompt injection
attacks remains a pressing concern. In this study, we explore how non-visible
HTML elements such as <meta>, aria-label, and alt attributes can be exploited
to embed adversarial instructions without altering the visible content of a
webpage. We introduce a novel dataset comprising 280 static web pages, evenly
divided between clean and adversarial injected versions, crafted using diverse
HTML-based strategies. These pages are processed through a browser automation
pipeline to extract both raw HTML and rendered text, closely mimicking
real-world LLM deployment scenarios. We evaluate two state-of-the-art
open-source models, Llama 4 Scout (Meta) and Gemma 9B IT (Google), on their
ability to summarize this content. Using both lexical (ROUGE-L) and semantic
(SBERT cosine similarity) metrics, along with manual annotations, we assess the
impact of these covert injections. Our findings reveal that over 29% of
injected samples led to noticeable changes in the Llama 4 Scout summaries,
while Gemma 9B IT showed a lower, yet non-trivial, success rate of 15%. These
results highlight a critical and largely overlooked vulnerability in LLM driven
web pipelines, where hidden adversarial content can subtly manipulate model
outputs. Our work offers a reproducible framework and benchmark for evaluating
HTML-based prompt injection and underscores the urgent need for robust
mitigation strategies in LLM applications involving web content.

</details>


### [29] [Yours or Mine? Overwriting Attacks against Neural Audio Watermarking](https://arxiv.org/abs/2509.05835)
*Lingfeng Yao,Chenpei Huang,Shengyao Wang,Junpei Xue,Hanqing Guo,Jiang Liu,Phone Lin,Tomoaki Ohtsuki,Miao Pan*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As generative audio models are rapidly evolving, AI-generated audios
increasingly raise concerns about copyright infringement and misinformation
spread. Audio watermarking, as a proactive defense, can embed secret messages
into audio for copyright protection and source verification. However, current
neural audio watermarking methods focus primarily on the imperceptibility and
robustness of watermarking, while ignoring its vulnerability to security
attacks. In this paper, we develop a simple yet powerful attack: the
overwriting attack that overwrites the legitimate audio watermark with a forged
one and makes the original legitimate watermark undetectable. Based on the
audio watermarking information that the adversary has, we propose three
categories of overwriting attacks, i.e., white-box, gray-box, and black-box
attacks. We also thoroughly evaluate the proposed attacks on state-of-the-art
neural audio watermarking methods. Experimental results demonstrate that the
proposed overwriting attacks can effectively compromise existing watermarking
schemes across various settings and achieve a nearly 100% attack success rate.
The practicality and effectiveness of the proposed overwriting attacks expose
security flaws in existing neural audio watermarking systems, underscoring the
need to enhance security in future audio watermarking designs.

</details>


### [30] [Multimodal Prompt Injection Attacks: Risks and Defenses for Modern LLMs](https://arxiv.org/abs/2509.05883)
*Andrew Yeo,Daeseon Choi*

Main category: cs.CR

TL;DR: This paper evaluates the security vulnerabilities of commercial LLMs (e.g., prompt injection attacks) through experiments on eight models, revealing exploitable weaknesses and emphasizing the need for enhanced defenses. Claude 3 showed relative robustness but still required additional safeguards like input normalization.


<details>
  <summary>Details</summary>
Motivation: LLMs' widespread adoption creates critical security risks (e.g., prompt injection, jailbreak attacks). Systematic evaluation is needed to identify vulnerabilities and establish robust defense mechanisms for industries relying on these models.

Method: The study tested eight commercial LLMs without supplementary sanitization, relying solely on built-in safeguards. Four attack types were analyzed: direct/indirect injection, image-based injection, and prompt leakage, using empirical experiments to assess model robustness.

Result: All models exhibited exploitable vulnerabilities to external prompt injection attacks. Claude 3 demonstrated the highest robustness among tested models, but no model achieved reliable security without additional measures like input normalization.

Conclusion: Current built-in safeguards of commercial LLMs are insufficient to prevent prompt injection attacks. The study confirms the necessity of layering technical defenses (e.g., input normalization) to achieve practical security for real-world deployment.

Abstract: Large Language Models (LLMs) have seen rapid adoption in recent years, with
industries increasingly relying on them to maintain a competitive advantage.
These models excel at interpreting user instructions and generating human-like
responses, leading to their integration across diverse domains, including
consulting and information retrieval. However, their widespread deployment also
introduces substantial security risks, most notably in the form of prompt
injection and jailbreak attacks.
  To systematically evaluate LLM vulnerabilities -- particularly to external
prompt injection -- we conducted a series of experiments on eight commercial
models. Each model was tested without supplementary sanitization, relying
solely on its built-in safeguards. The results exposed exploitable weaknesses
and emphasized the need for stronger security measures. Four categories of
attacks were examined: direct injection, indirect (external) injection,
image-based injection, and prompt leakage. Comparative analysis indicated that
Claude 3 demonstrated relatively greater robustness; nevertheless, empirical
findings confirm that additional defenses, such as input normalization, remain
necessary to achieve reliable protection.

</details>


### [31] [Introduction to Number Theoretic Transform](https://arxiv.org/abs/2509.05884)
*Banhirup Sengupta,Peenal Gupta,Souvik Sengupta*

Main category: cs.CR

TL;DR: The paper introduces fast Number Theoretic Transforms (NTTs) for cyclic and negacyclic convolutions, enhancing polynomial multiplication efficiency in lattice-based cryptography and post-quantum/homomorphic encryption applications.


<details>
  <summary>Details</summary>
Motivation: Traditional Fourier Transform methods rely on continuous signals and sine/cosine waves, but cryptographic applications require discrete operations over finite fields. Efficient polynomial multiplication is critical for scalable lattice cryptography.

Method: Proposes fast NTT variants for cyclic/negacyclic convolutions. Leverages group-theoretic and finite field mathematics where polynomials replace sinusoids. Extends FFT-style divide-and-conquer strategies to discrete algebraic structures.

Result: Demonstrates quasilinear complexity for polynomial multiplication in cryptographic contexts. Achieves significant computational efficiency gains compared to quadratic-time methods.

Conclusion: Fast NTT is foundational for efficient lattice-based cryptographic systems. Its algebraic structure enables both theoretical advantages and practical performance improvements over classical approaches in post-quantum security paradigms.

Abstract: The Number Theoretic Transform (NTT) can be regarded as a variant of the
Discrete Fourier Transform. NTT has been quite a powerful mathematical tool in
developing Post-Quantum Cryptography and Homomorphic Encryption. The Fourier
Transform essentially decomposes a signal into its frequencies. They are
traditionally sine or cosine waves. NTT works more over groups or finite fields
rather than on a continuous signal and polynomials work as the analog of sine
waves in case of NTT. Fast Fourier Trnasform (FFT) style NTT or fast NTT has
been proven to be useful in lattice-based cryptography due to its ability to
reduce the complexity of polynomial multiplication from quadratic to
quasilinear. We have introduced the concepts of cyclic, negacyclic convolutions
along with NTT and its inverse and their fast versions.

</details>


### [32] [MemTraceDB: Reconstructing MySQL User Activity Using ActiviTimeTrace Algorithm](https://arxiv.org/abs/2509.05891)
*Mahfuzul I. Nissan*

Main category: cs.CR

TL;DR: MemTraceDB is a forensic tool that reconstructs user activity from MySQL memory snapshots, bypassing tampered disk logs. It identifies MySQL’s query stack limit (~9,997 queries) to optimize memory snapshot collection, enabling reliable timeline reconstruction.


<details>
  <summary>Details</summary>
Motivation: Traditional database logs are vulnerable to tampering by privileged attackers, creating blind spots in forensic investigations. Memory analysis addresses this by providing volatile, untampered data about recent user activity.

Method: MemTraceDB analyzes raw MySQL memory snapshots using the ActiviTimeTrace algorithm, which extracts and correlates user connections and executed queries. Experiments validate its effectiveness.

Result: Discovery of MySQL’s finite query stack capacity (~9,997 queries) and a formula to determine optimal snapshot collection frequency, enabling actionable guidelines for investigators.

Conclusion: MemTraceDB provides a forensically sound, log-independent reconstruction of user activity, addressing vulnerabilities in disk-based audit trails.

Abstract: Database audit and transaction logs are fundamental to forensic
investigations, but they are vulnerable to tampering by privileged attackers.
Malicious insiders or external threats with administrative access can alter,
purge, or temporarily disable logging mechanisms, creating significant blind
spots and rendering disk-based records unreliable. Memory analysis offers a
vital alternative, providing investigators direct access to volatile artifacts
that represent a ground-truth source of recent user activity, even when log
files have been compromised.
  This paper introduces MemTraceDB, a tool that reconstructs user activity
timelines by analyzing raw memory snapshots from the MySQL database process.
MemTraceDB utilizes a novel algorithm, ActiviTimeTrace, to systematically
extract and correlate forensic artifacts such as user connections and executed
queries. Through a series of experiments, I demonstrate MemTraceDB's
effectiveness and reveal a critical empirical finding: the MySQL query stack
has a finite operational capacity of approximately 9,997 queries. This
discovery allows me to establish a practical, data-driven formula for
determining the optimal frequency for memory snapshot collection, providing a
clear, actionable guideline for investigators. The result is a
forensically-sound reconstruction of user activity, independent of compromised
disk-based logs.

</details>


### [33] [Wrangling Entropy: Next-Generation Multi-Factor Key Derivation, Credential Hashing, and Credential Generation Functions](https://arxiv.org/abs/2509.05893)
*Colin Roberts,Vivek Nair,Dawn Song*

Main category: cs.CR

TL;DR: This paper introduces ESTMF to fix entropy-leak vulnerabilities in MFKDF, resulting in a secure next-generation multi-factor key derivation function (MFKDF2) with broader usability.


<details>
  <summary>Details</summary>
Motivation: The original MFKDF, while innovative for client-side key management, suffered from security degradation due to cryptanalysis, necessitating a robust method (ESTMF) to uncover entropy leaks and develop a secure successor (MFKDF2).

Method: The authors introduce the ESTMF framework to detect entropy leaks across multiple invocations of cryptographic functions, using it to identify vulnerabilities in MFKDF and redesign it into MFKDF2 with provable security. They extend MFKDF2 to support additional authentication factors and usability features.

Result: ESTMF successfully identifies all known vulnerabilities in MFKDF, enables MFKDF2 to achieve end-to-end security, and demonstrates scalability through additional authentication factors and usability improvements.

Conclusion: The paper proposes MFKDF2, an improved and end-to-end secure multi-factor key derivation function based on the Entropy State Transition Modeling Framework (ESTMF), which addresses vulnerabilities in the original MFKDF and provides generalizable best practices for future KDFs.

Abstract: The Multi-Factor Key Derivation Function (MFKDF) offered a novel solution to
the classic problem of usable client-side key management by incorporating
multiple popular authentication factors into a key derivation process, but was
later shown to be vulnerable to cryptanalysis that degraded its security over
multiple invocations. In this paper, we present the Entropy State Transition
Modeling Framework (ESTMF), a novel cryptanalytic technique designed to reveal
pernicious leaks of entropy across multiple invocations of a cryptographic key
derivation or hash function, and show that it can be used to correctly identify
each of the known vulnerabilities in the original MFKDF construction. We then
use these findings to propose a new construction for ``MFKDF2,'' a
next-generation multi-factor key derivation function that can be proven to be
end-to-end secure using the ESTMF. Finally, we discuss how MFKDF2 can be
extended to support more authentication factors and usability features than the
previous MFKDF construction, and derive several generalizable best-practices
for the construction of new KDFs in the future.

</details>


### [34] [Dataset Ownership in the Era of Large Language Models](https://arxiv.org/abs/2509.05921)
*Kun Li,Cheng Wang,Minghui Xu,Yue Zhang,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: This survey categorizes and evaluates technical methods for dataset copyright protection, highlighting challenges and future research needs in securing data within modern machine learning systems.


<details>
  <summary>Details</summary>
Motivation: Traditional legal mechanisms fail to address technical complexities of digital data replication and unauthorized use in opaque or decentralized environments, necessitating technical solutions.

Method: The authors conduct a systematic survey, categorizing technical approaches into non-intrusive, minimally-intrusive, and maximally-intrusive methods, and analyze their strengths and limitations.

Result: Synthesis of key techniques, analysis of their trade-offs, and identification of open research challenges in dataset copyright protection.

Conclusion: The paper provides an organized perspective on current dataset copyright protection methods and suggests future directions for unified, scalable solutions in complex machine learning ecosystems.

Abstract: As datasets become critical assets in modern machine learning systems,
ensuring robust copyright protection has emerged as an urgent challenge.
Traditional legal mechanisms often fail to address the technical complexities
of digital data replication and unauthorized use, particularly in opaque or
decentralized environments. This survey provides a comprehensive review of
technical approaches for dataset copyright protection, systematically
categorizing them into three main classes: non-intrusive methods, which detect
unauthorized use without modifying data; minimally-intrusive methods, which
embed lightweight, reversible changes to enable ownership verification; and
maximally-intrusive methods, which apply aggressive data alterations, such as
reversible adversarial examples, to enforce usage restrictions. We synthesize
key techniques, analyze their strengths and limitations, and highlight open
research challenges. This work offers an organized perspective on the current
landscape and suggests future directions for developing unified, scalable, and
ethically sound solutions to protect datasets in increasingly complex machine
learning ecosystems.

</details>


### [35] [DCMI: A Differential Calibration Membership Inference Attack Against Retrieval-Augmented Generation](https://arxiv.org/abs/2509.06026)
*Xinyu Gao,Xiangtao Meng,Yingkai Dong,Zheng Li,Shanqing Guo*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While Retrieval-Augmented Generation (RAG) effectively reduces hallucinations
by integrating external knowledge bases, it introduces vulnerabilities to
membership inference attacks (MIAs), particularly in systems handling sensitive
data. Existing MIAs targeting RAG's external databases often rely on model
responses but ignore the interference of non-member-retrieved documents on RAG
outputs, limiting their effectiveness. To address this, we propose DCMI, a
differential calibration MIA that mitigates the negative impact of
non-member-retrieved documents. Specifically, DCMI leverages the sensitivity
gap between member and non-member retrieved documents under query perturbation.
It generates perturbed queries for calibration to isolate the contribution of
member-retrieved documents while minimizing the interference from
non-member-retrieved documents. Experiments under progressively relaxed
assumptions show that DCMI consistently outperforms baselines--for example,
achieving 97.42% AUC and 94.35% Accuracy against the RAG system with Flan-T5,
exceeding the MBA baseline by over 40%. Furthermore, on real-world RAG
platforms such as Dify and MaxKB, DCMI maintains a 10%-20% advantage over the
baseline. These results highlight significant privacy risks in RAG systems and
emphasize the need for stronger protection mechanisms. We appeal to the
community's consideration of deeper investigations, like ours, against the data
leakage risks in rapidly evolving RAG systems. Our code is available at
https://github.com/Xinyu140203/RAG_MIA.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [36] [Adversarial Bug Reports as a Security Risk in Language Model-Based Automated Program Repair](https://arxiv.org/abs/2509.05372)
*Piotr Przymus,Andreas Happe,Jürgen Cito*

Main category: cs.SE

TL;DR: This paper exposes security vulnerabilities in LLM-based APR systems via adversarial bug reports. Despite existing defenses failing to block >90% of attacks, the study highlights a cost imbalance between attack generation and defense, calling for robust solutions and future research on trustworthy repair.


<details>
  <summary>Details</summary>
Motivation: As LLM-based APR systems become integral to software workflows, their dependence on untrusted user input introduces unexplored security risks. Adversarial bug reports could compromise system integrity, motivating the exploration of this novel attack vector.

Method: The authors developed a threat model and conducted an empirical evaluation using 51 adversarial bug reports generated through manual and automated methods. They tested state-of-the-art APR systems with pre-repair defenses (e.g., LlamaGuard variants, PromptGuard) and post-repair detectors (e.g., GitHub Copilot, CodeQL). A prototype framework for scalable adversarial input generation is introduced.

Result: 90% of adversarial bug reports triggered harmful patches. Pre-repair filters blocked only 47%, and post-repair analysis detected threats in 58% of cases. Current defenses are insufficient for mitigating adversarial attacks.

Conclusion: The study reveals a structural asymmetry where adversarial bug report generation is inexpensive but defending against them is costly and error-prone. It emphasizes the need for improved robustness in APR systems and outlines future research directions for trustworthy automated repair.

Abstract: Large Language Model (LLM) - based Automated Program Repair (APR) systems are
increasingly integrated into modern software development workflows, offering
automated patches in response to natural language bug reports. However, this
reliance on untrusted user input introduces a novel and underexplored attack
surface. In this paper, we investigate the security risks posed by adversarial
bug reports -- realistic-looking issue submissions crafted to mislead APR
systems into producing insecure or harmful code changes. We develop a
comprehensive threat model and conduct an empirical study to evaluate the
vulnerability of state-of-the-art APR systems to such attacks. Our
demonstration comprises 51 adversarial bug reports generated across a spectrum
of strategies, from manual curation to fully automated pipelines. We test these
against leading APR model and assess both pre-repair defenses (e.g., LlamaGuard
variants, PromptGuard variants, Granite-Guardian, and custom LLM filters) and
post-repair detectors (GitHub Copilot, CodeQL). Our findings show that current
defenses are insufficient: 90\% of crafted bug reports triggered
attacker-aligned patches. The best pre-repair filter blocked only 47\%, while
post-repair analysis-often requiring human oversight-was effective in just 58\%
of cases. To support scalable security testing, we introduce a prototype
framework for automating the generation of adversarial bug reports. Our
analysis exposes a structural asymmetry: generating adversarial inputs is
inexpensive, while detecting or mitigating them remains costly and error-prone.
We conclude with practical recommendations for improving the robustness of APR
systems against adversarial misuse and highlight directions for future work on
trustworthy automated repair.

</details>


### [37] [Reverse Browser: Vector-Image-to-Code Generator](https://arxiv.org/abs/2509.05394)
*Zoltan Toth-Czifra*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Automating the conversion of user interface design into code (image-to-code
or image-to-UI) is an active area of software engineering research. However,
the state-of-the-art solutions do not achieve high fidelity to the original
design, as evidenced by benchmarks. In this work, I approach the problem
differently: I use vector images instead of bitmaps as model input. I create
several large datasets for training machine learning models. I evaluate the
available array of Image Quality Assessment (IQA) algorithms and introduce a
new, multi-scale metric. I then train a large open-weights model and discuss
its limitations.

</details>


### [38] [Combining TSL and LLM to Automate REST API Testing: A Comparative Study](https://arxiv.org/abs/2509.05540)
*Thiago Barradas,Aline Paes,Vânia de Oliveira Neves*

Main category: cs.SE

TL;DR: RestTSLLM uses LLMs and TSL to automate REST API testing, with Claude 3.5 Sonnet being the top model for generating effective tests from OpenAPI specs.


<details>
  <summary>Details</summary>
Motivation: REST API testing faces challenges from distributed system complexity, limited time, and impractical exhaustive scenarios, leading to undetected failures and low coverage, necessitating automation.

Method: RestTSLLM combines TSL with prompt engineering and an automated pipeline to evaluate LLMs' ability to generate REST API tests from OpenAPI specifications, using metrics like success rate, test coverage, and mutation score.

Result: Top-performing LLMs (Claude 3.5 Sonnet, Deepseek R1, Qwen 2.5 32b, Sabia 3) consistently produced robust tests, with Claude 3.5 Sonnet excelling in all metrics, proving LLMs' potential for automated API test generation.

Conclusion: The study demonstrates that LLMs, particularly Claude 3.5 Sonnet, effectively automate REST API test generation using RestTSLLM and OpenAPI specifications, achieving high success rates, coverage, and mutation scores.

Abstract: The effective execution of tests for REST APIs remains a considerable
challenge for development teams, driven by the inherent complexity of
distributed systems, the multitude of possible scenarios, and the limited time
available for test design. Exhaustive testing of all input combinations is
impractical, often resulting in undetected failures, high manual effort, and
limited test coverage. To address these issues, we introduce RestTSLLM, an
approach that uses Test Specification Language (TSL) in conjunction with Large
Language Models (LLMs) to automate the generation of test cases for REST APIs.
The approach targets two core challenges: the creation of test scenarios and
the definition of appropriate input data. The proposed solution integrates
prompt engineering techniques with an automated pipeline to evaluate various
LLMs on their ability to generate tests from OpenAPI specifications. The
evaluation focused on metrics such as success rate, test coverage, and mutation
score, enabling a systematic comparison of model performance. The results
indicate that the best-performing LLMs - Claude 3.5 Sonnet (Anthropic),
Deepseek R1 (Deepseek), Qwen 2.5 32b (Alibaba), and Sabia 3 (Maritaca) -
consistently produced robust and contextually coherent REST API tests. Among
them, Claude 3.5 Sonnet outperformed all other models across every metric,
emerging in this study as the most suitable model for this task. These findings
highlight the potential of LLMs to automate the generation of tests based on
API specifications.

</details>


### [39] [Natural Language-Programming Language Software Traceability Link Recovery Needs More than Textual Similarity](https://arxiv.org/abs/2509.05585)
*Zhiyuan Zou,Bangchao Wang,Peng Liang,Tingting Bi,Huan Jin*

Main category: cs.SE

TL;DR: This paper introduces multi-strategy integration in HGT and Gemini 2.5 Pro models to overcome textual similarity limitations in NL-PL traceability, achieving significant performance gains in requirements-code TLR.


<details>
  <summary>Details</summary>
Motivation: Textual similarity alone is insufficient for NL-PL traceability due to semantic gaps between natural and programming languages, motivating the exploration of enhanced strategies to address these limitations.

Method: The authors propose incorporating domain-specific auxiliary strategies into two models: HGT through edge types and Gemini 2.5 Pro via additional input information, validated through extensive experiments on requirements-to-code tasks.

Result: Multi-strategy HGT and Gemini 2.5 Pro achieved average F1-score improvements of 3.68% and 8.84% over baseline models and current SOTA HGNNLink across twelve open-source projects.

Conclusion: The study demonstrates that integrating multiple domain-specific auxiliary strategies into models like HGT and Gemini 2.5 Pro significantly enhances NL-PL traceability link recovery performance, as evidenced by F1-score improvements over existing methods.

Abstract: In the field of software traceability link recovery (TLR), textual similarity
has long been regarded as the core criterion. However, in tasks involving
natural language and programming language (NL-PL) artifacts, relying solely on
textual similarity is limited by their semantic gap. To this end, we conducted
a large-scale empirical evaluation across various types of TLR tasks, revealing
the limitations of textual similarity in NL-PL scenarios. To address these
limitations, we propose an approach that incorporates multiple domain-specific
auxiliary strategies, identified through empirical analysis, into two models:
the Heterogeneous Graph Transformer (HGT) via edge types and the prompt-based
Gemini 2.5 Pro via additional input information. We then evaluated our approach
using the widely studied requirements-to-code TLR task, a representative case
of NL-PL TLR. Experimental results show that both the multi-strategy HGT and
Gemini 2.5 Pro models outperformed their original counterparts without strategy
integration. Furthermore, compared to the current state-of-the-art method
HGNNLink, the multi-strategy HGT and Gemini 2.5 Pro models achieved average
F1-score improvements of 3.68% and 8.84%, respectively, across twelve
open-source projects, demonstrating the effectiveness of multi-strategy
integration in enhancing overall model performance for the requirements-code
TLR task.

</details>


### [40] [Verifying Correctness of PLC Software during System Evolution using Model Containment Approach](https://arxiv.org/abs/2509.05596)
*Soumyadip Bandyopadhyay,Santonu Sarkar*

Main category: cs.SE

TL;DR: This paper introduces a Petri net-based verification framework for ensuring PLC software upgrade correctness through symbolic path equivalence, achieving 4x faster verification than existing tools on 80 real-world benchmarks.


<details>
  <summary>Details</summary>
Motivation: PLC software upgrades are common but verifying correctness of existing functionality in updated versions remains a significant industry challenge.

Method: Converts sequential function charts (SFCs) to Petri net models and uses a novel containment checking algorithm based on symbolic path equivalence.

Result: Experimental evaluation on 80 real-world benchmarks shows a 4x performance improvement compared to verifAPS and validates the framework's scalability and effectiveness.

Conclusion: The framework demonstrates scalability and effectiveness through 80 real-world benchmarks, with a 4x performance improvement over existing tools.

Abstract: Upgradation of Programmable Logic Controller (PLC) software is quite common
to accommodate evolving industrial requirements. Verifying the correctness of
such upgrades remains a significant challenge. In this paper, we propose a
verification-based approach to ensure the correctness of the existing
functionality in the upgraded version of a PLC software. The method converts
the older and the newer versions of the sequential function chart (SFC) into
two Petri net models. We then verify whether one model is contained within
another, based on a novel containment checking algorithm grounded in symbolic
path equivalence. For this purpose, we have developed a home-grown Petri
net-based containment checker. Experimental evaluation on 80 real-world
benchmarks from the OSCAT library highlights the scalability and effectiveness
of the framework. We have compared our approach with verifAPS, a popular tool
used for software upgradation, and observed nearly 4x performance improvement.

</details>


### [41] [Automating API Documentation with LLMs: A BERTopic Approach](https://arxiv.org/abs/2509.05749)
*AmirHossein Naghshzan*

Main category: cs.SE

TL;DR: This study summarizes 3.6M Android-related Stack Overflow posts using BERTopic and extractive methods to improve API documentation accessibility and developer productivity.


<details>
  <summary>Details</summary>
Motivation: Official Android API documentation is often lengthy, complex, or incomplete, prompting developers to seek practical insights from community forums like Stack Overflow.

Method: The study utilized BERTopic for topic extraction from 3.6 million Stack Overflow posts and applied extractive summarization techniques to create concise, code-inclusive summaries.

Result: A user study (n=30 Android developers) demonstrated improved productivity through summaries evaluated for coherence, relevance, informativeness, and satisfaction.

Conclusion: Integrating formal API knowledge with community-generated content enhances documentation, making API resources more accessible and actionable.

Abstract: Developers rely on API documentation, but official sources are often lengthy,
complex, or incomplete. Many turn to community-driven forums like Stack
Overflow for practical insights. We propose automating the summarization of
informal sources, focusing on Android APIs. Using BERTopic, we extracted
prevalent topics from 3.6 million Stack Overflow posts and applied extractive
summarization techniques to generate concise summaries, including code
snippets. A user study with 30 Android developers assessed the summaries for
coherence, relevance, informativeness, and satisfaction, showing improved
productivity. Integrating formal API knowledge with community-generated content
enhances documentation, making API resources more accessible and actionable
work.

</details>


### [42] [IoT Miner: Intelligent Extraction of Event Logs from Sensor Data for Process Mining](https://arxiv.org/abs/2509.05769)
*Edyta Brzychczy,Urszula Jessen,Krzysztof Kluza,Sridhar Sriram,Manuel Vargas Nettelnstroth*

Main category: cs.SE

TL;DR: IoT Miner is a framework that automates generating structured event logs from unstructured industrial sensor data using a four-stage pipeline involving unsupervised clustering and LLM-based labeling, enabling process mining in data-scarce environments.


<details>
  <summary>Details</summary>
Motivation: Real-world industrial settings like mining/manufacturing often lack standard event logs, requiring structured/semantic sensor data analysis for process mining applications which cannot be addressed by traditional methods.

Method: Four-stage pipeline: (1) data preprocessing, (2) unsupervised clustering, (3) LLM-based labeling using domain-specific prompts to derive activity labels from cluster statistics, (4) event log construction

Result: Evaluation on Load-Haul-Dump mining machine data showed: (1) Similarity-Weighted Accuracy metric effectiveness, (2) enriched prompts significantly improve label accuracy and consistency

Conclusion: IoT Miner demonstrates a scalable, interpretable methodology for transforming raw IoT data into usable event logs by combining AI with domain-aware processing, overcoming the lack of traditional logs in industrial environments.

Abstract: This paper presents IoT Miner, a novel framework for automatically creating
high-level event logs from raw industrial sensor data to support process
mining. In many real-world settings, such as mining or manufacturing, standard
event logs are unavailable, and sensor data lacks the structure and semantics
needed for analysis. IoT Miner addresses this gap using a four-stage pipeline:
data preprocessing, unsupervised clustering, large language model (LLM)-based
labeling, and event log construction. A key innovation is the use of LLMs to
generate meaningful activity labels from cluster statistics, guided by
domain-specific prompts. We evaluate the approach on sensor data from a
Load-Haul-Dump (LHD) mining machine and introduce a new metric,
Similarity-Weighted Accuracy, to assess labeling quality. Results show that
richer prompts lead to more accurate and consistent labels. By combining AI
with domain-aware data processing, IoT Miner offers a scalable and
interpretable method for generating event logs from IoT data, enabling process
mining in settings where traditional logs are missing.

</details>


### [43] [GeoAnalystBench: A GeoAI benchmark for assessing large language models for spatial analysis workflow and code generation](https://arxiv.org/abs/2509.05881)
*Qianheng Zhang,Song Gao,Chen Wei,Yibo Zhao,Ying Nie,Ziru Chen,Shijie Chen,Yu Su,Huan Sun*

Main category: cs.SE

TL;DR: The paper introduces GeoAnalystBench, a benchmark for evaluating large language models' (LLMs) capabilities in geospatial analysis. It highlights a performance gap between proprietary models (e.g., ChatGPT-4o-mini) and open-source alternatives (e.g., DeepSeek-R1-7B) in terms of workflow validity and code quality, while underscoring remaining challenges in spatial reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: The study addresses the need for rigorous evaluation frameworks to assess LLM capabilities in GIS automation, as current claims about their efficacy lack systemic validation, risking overestimation of their readiness for real-world geoprocessing tasks.

Method: Authors developed GeoAnalystBench, a benchmark with 50 Python-based, expert-validated geospatial tasks, each paired with minimum deliverables and evaluated using four metrics: workflow validity, structural alignment, semantic similarity, and CodeBLEU.

Result: Proprietary models (validity: 95%, CodeBLEU: 0.39) outperformed open-source models (validity: 48.5%, CodeBLEU: 0.272). Deep spatial reasoning tasks (e.g., site selection) were most challenging. Proprietary models showed stronger code alignment but gaps persist for complex workflows.

Conclusion: While current LLMs show promise for GIS automation, performance limitations in spatial reasoning and consistency highlight critical challenges. The benchmark provides a human-in-the-loop framework to advance GeoAI research and development.

Abstract: Recent advances in large language models (LLMs) have fueled growing interest
in automating geospatial analysis and GIS workflows, yet their actual
capabilities remain uncertain. In this work, we call for rigorous evaluation of
LLMs on well-defined geoprocessing tasks before making claims about full GIS
automation. To this end, we present GeoAnalystBench, a benchmark of 50
Python-based tasks derived from real-world geospatial problems and carefully
validated by GIS experts. Each task is paired with a minimum deliverable
product, and evaluation covers workflow validity, structural alignment,
semantic similarity, and code quality (CodeBLEU). Using this benchmark, we
assess both proprietary and open source models. Results reveal a clear gap:
proprietary models such as ChatGPT-4o-mini achieve high validity 95% and
stronger code alignment (CodeBLEU 0.39), while smaller open source models like
DeepSeek-R1-7B often generate incomplete or inconsistent workflows (48.5%
validity, 0.272 CodeBLEU). Tasks requiring deeper spatial reasoning, such as
spatial relationship detection or optimal site selection, remain the most
challenging across all models. These findings demonstrate both the promise and
limitations of current LLMs in GIS automation and provide a reproducible
framework to advance GeoAI research with human-in-the-loop support.

</details>


### [44] [Code2MCP: A Multi-Agent Framework for Automated Transformation of Code Repositories into Model Context Protocol Services](https://arxiv.org/abs/2509.05941)
*Chaoqian Ouyang,Ling Yue,Shimin Di,Libin Zheng,Shaowu Pan,Min-Ling Zhang*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The proliferation of Large Language Models (LLMs) has created a significant
integration challenge in the AI agent ecosystem, often called the "$N \times M$
problem," where N models require custom integrations for M tools. This
fragmentation stifles innovation and creates substantial development overhead.
While the Model Context Protocol (MCP) has emerged as a standard to resolve
this, its adoption is hindered by the manual effort required to convert the
vast universe of existing software into MCP-compliant services. This is
especially true for the millions of open-source repositories on GitHub, the
world's largest collection of functional code. This paper introduces Code2MCP,
a highly automated, agentic framework designed to transform any GitHub
repository into a functional MCP service with minimal human intervention. Our
system employs a multi-stage workflow that automates the entire process, from
code analysis and environment configuration to service generation and
deployment. A key innovation of our framework is an LLM-driven, closed-loop
"Run--Review--Fix" cycle, which enables the system to autonomously debug and
repair the code it generates. Code2MCP produces not only deployable services
but also comprehensive technical documentation, acting as a catalyst to
accelerate the MCP ecosystem by systematically unlocking the world's largest
open-source code repository and automating the critical last mile of tool
integration. The code is open-sourced at
https://github.com/DEFENSE-SEU/MCP-Github-Agent.

</details>


### [45] [GRACE: Graph-Guided Repository-Aware Code Completion through Hierarchical Code Fusion](https://arxiv.org/abs/2509.05980)
*Xingliang Wang,Baoyi Wang,Chen Zhi,Junxiao Han,Xinkui Zhao,Jianwei Yin,Shuiguang Deng*

Main category: cs.SE

TL;DR: GRACE improves repository-level code generation by integrating structural code relationships into a graph-based RAG system, outperforming existing methods with 8.19%+ accuracy gains.


<details>
  <summary>Details</summary>
Motivation: Current code generation methods struggle with repository-level tasks due to context window limitations and neglect of structural relationships like call chains and inheritance, relying instead on text similarity and concatenating snippets in a structure-agnostic manner.

Method: GRACE constructs a multi-level code graph combining file structures, ASTs, call graphs, class hierarchies, and data flow graphs. It uses a Hybrid Graph Retriever with structural/text similarity, a graph-attention re-ranker, and a structural fusion mechanism to merge retrieved subgraphs with local context while preserving key dependencies.

Result: GRACE outperforms state-of-the-art methods using DeepSeek-V3 by 8.19% (EM) and 7.51% (ES) across all benchmarks, demonstrating superior handling of complex code dependencies.

Conclusion: GRACE enhances code generation by integrating structural data into Retrieval-Augmented Generation, significantly improving repository-level code tasks through a multi-level code graph and hybrid retrieval mechanisms.

Abstract: LLMs excel in localized code completion but struggle with repository-level
tasks due to limited context windows and complex semantic and structural
dependencies across codebases. While Retrieval-Augmented Generation (RAG)
mitigates context scarcity by retrieving relevant code snippets, current
approaches face significant limitations. They overly rely on textual similarity
for retrieval, neglecting structural relationships such as call chains and
inheritance hierarchies, and lose critical structural information by naively
concatenating retrieved snippets into text sequences for LLM input. To address
these shortcomings, GRACE constructs a multi-level, multi-semantic code graph
that unifies file structures, abstract syntax trees, function call graphs,
class hierarchies, and data flow graphs to capture both static and dynamic code
semantics. For retrieval, GRACE employs a Hybrid Graph Retriever that
integrates graph neural network-based structural similarity with textual
retrieval, refined by a graph attention network-based re-ranker to prioritize
topologically relevant subgraphs. To enhance context, GRACE introduces a
structural fusion mechanism that merges retrieved subgraphs with the local code
context and preserves essential dependencies like function calls and
inheritance. Extensive experiments on public repository-level benchmarks
demonstrate that GRACE significantly outperforms state-of-the-art methods
across all metrics. Using DeepSeek-V3 as the backbone LLM, GRACE surpasses the
strongest graph-based RAG baselines by 8.19% EM and 7.51% ES points on every
dataset. The code is available at
https://anonymous.4open.science/r/grace_icse-C3D5.

</details>


### [46] [Students' Perception of LLM Use in Requirements Engineering Education: An Empirical Study Across Two Universities](https://arxiv.org/abs/2509.05995)
*Sharon Guardado,Risha Parveen,Zheying Zhang,Maruf Rayhan,Nirnaya Tripathi*

Main category: cs.SE

TL;DR: This study evaluates how integrating LLMs into RE coursework affects student learning. It finds that LLMs improve concept understanding but highlights concerns like academic integrity and overuse. Individual assignments yielded higher perceived benefits than team projects, suggesting context matters for AI integration in education.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the growing integration of LLMs in RE education to improve student engagement, practical skill development, and professional preparedness, while empirically evaluating the effectiveness and challenges of such integration.

Method: The study conducted an empirical evaluation using survey data from 179 students across two RE courses at two universities. LLMs were integrated through individual assignments and team-based Agile projects to compare their impact on learning experiences and perceived benefits/challenges.

Result: Findings show LLMs improved comprehension of RE concepts (e.g., requirements elicitation) but raised concerns about academic integrity, AI overreliance, and content integration challenges. Students in individual assignments reported greater benefits than those in team projects, emphasizing the need for context-aware AI integration.

Conclusion: The study concludes that integrating LLMs into RE education can enhance learning outcomes but requires careful consideration of academic integrity, overreliance risks, and contextual integration strategies. It provides actionable recommendations for educators and outlines future research directions to balance AI-assisted learning with critical thinking and collaboration.

Abstract: The integration of Large Language Models (LLMs) in Requirements Engineering
(RE) education is reshaping pedagogical approaches, seeking to enhance student
engagement and motivation while providing practical tools to support their
professional future. This study empirically evaluates the impact of integrating
LLMs in RE coursework. We examined how the guided use of LLMs influenced
students' learning experiences, and what benefits and challenges they perceived
in using LLMs in RE practices. The study collected survey data from 179
students across two RE courses in two universities. LLMs were integrated into
coursework through different instructional formats, i.e., individual
assignments versus a team-based Agile project. Our findings indicate that LLMs
improved students' comprehension of RE concepts, particularly in tasks like
requirements elicitation and documentation. However, students raised concerns
about LLMs in education, including academic integrity, overreliance on AI, and
challenges in integrating AI-generated content into assignments. Students who
worked on individual assignments perceived that they benefited more than those
who worked on team-based assignments, highlighting the importance of contextual
AI integration. This study offers recommendations for the effective integration
of LLMs in RE education. It proposes future research directions for balancing
AI-assisted learning with critical thinking and collaborative practices in RE
courses.

</details>


### [47] [A Rapid Review Regarding the Concept of Legal Requirements in Requirements Engineering](https://arxiv.org/abs/2509.06012)
*Jukka Ruohonen*

Main category: cs.SE

TL;DR: This paper reviews the concept of legal requirements in requirements engineering, revealing conceptual confusion, lack of empirical evidence, and persistent challenges in their management despite widespread recognition of their complexity.


<details>
  <summary>Details</summary>
Motivation: Driven by personal confusion, peer review feedback, and observed inconsistencies in the literature, the paper aims to clarify the conceptual and practical understanding of legal requirements in requirements engineering.

Method: The paper conducts a rapid literature review, analyzing existing definitions, operationalizations, and characterizations of legal requirements in RE research through a systematic review of the literature.

Result: The study identifies conceptual inconsistencies, a lack of standardized definitions, challenges in implementing legal requirements (e.g., vagueness, complexity, overlapping requirements), and a reliance on minimal baseline implementations without sufficient empirical validation.

Conclusion: The review highlights the need for more empirical evidence and clear conceptual frameworks to resolve existing ambiguities in understanding and managing legal requirements within requirements engineering.

Abstract: Out of a personal puzzlement, recent peer review comments, and demonstrable
confusion in the existing literature, the paper presents a rapid review of the
concept of legal requirements (LRs) in requirements engineering (RE) research.
According to reviewing results, a normative understanding of LRs has often been
present, although proper definitions and conceptual operationalizations are
lacking. Some papers also see LRs as functional and others as non-functional
requirements. Legal requirements are often characterized as being vague and
complex, requiring a lot of effort to elicit, implement, and validate. These
characterizations supposedly correlate with knowledge gaps among requirements
engineers. LRs are also seen to often change and overlap. They may be also
prioritized. According to the literature, they seem to be also reluctantly
implemented, often providing only a minimal baseline for other requirements.
With these and other observations, the review raises critical arguments about
apparent knowledge gaps, including a lack of empirical evidence backing the
observations and enduring conceptual confusion.

</details>


### [48] [Empirical Study of Code Large Language Models for Binary Security Patch Detection](https://arxiv.org/abs/2509.06052)
*Qingyuan Li,Binchang Li,Cuiyun Gao,Shuzheng Gao,Zongjie Li*

Main category: cs.SE

TL;DR: This work bridges the gap between code LLMs and binary security patch detection by constructing a large binary dataset and demonstrating that fine-tuned LLMs using pseudo-code achieve superior patch detection performance.


<details>
  <summary>Details</summary>
Motivation: Existing learning-based security patch detection (SPD) approaches rely on source code, which is inaccessible in closed-source applications dominating real-world software. Despite code LLMs' success in low-level code tasks, their potential for binary SPD remains unexplored, highlighting a critical research gap.

Method: The study constructs a large-scale binary patch dataset with two levels of representation (assembly code and pseudo-code), systematically evaluates 19 code LLMs using prompting techniques, and investigates fine-tuning strategies to inject domain knowledge through these representations.

Result: Vanilla code LLMs struggle with binary SPD tasks without domain knowledge, while fine-tuned models achieve outstanding performance with the best results on pseudo-code representation. Advanced prompting techniques fail to mitigate the knowledge gap without explicit fine-tuning.

Conclusion: Fine-tuning code large language models (LLMs) with binary security patch detection domain knowledge, particularly through pseudo-code representations, bridges the gap between their low-level code understanding capabilities and the critical task of binary SPD.

Abstract: Security patch detection (SPD) is crucial for maintaining software security,
as unpatched vulnerabilities can lead to severe security risks. In recent
years, numerous learning-based SPD approaches have demonstrated promising
results on source code. However, these approaches typically cannot be applied
to closed-source applications and proprietary systems that constitute a
significant portion of real-world software, as they release patches only with
binary files, and the source code is inaccessible. Given the impressive
performance of code large language models (LLMs) in code intelligence and
binary analysis tasks such as decompilation and compilation optimization, their
potential for detecting binary security patches remains unexplored, exposing a
significant research gap between their demonstrated low-level code
understanding capabilities and this critical security task. To address this
gap, we construct a large-scale binary patch dataset containing \textbf{19,448}
samples, with two levels of representation: assembly code and pseudo-code, and
systematically evaluate \textbf{19} code LLMs of varying scales to investigate
their capability in binary SPD tasks. Our initial exploration demonstrates that
directly prompting vanilla code LLMs struggles to accurately identify security
patches from binary patches, and even state-of-the-art prompting techniques
fail to mitigate the lack of domain knowledge in binary SPD within vanilla
models. Drawing on the initial findings, we further investigate the fine-tuning
strategy for injecting binary SPD domain knowledge into code LLMs through two
levels of representation. Experimental results demonstrate that fine-tuned LLMs
achieve outstanding performance, with the best results obtained on the
pseudo-code representation.

</details>


### [49] [Software Dependencies 2.0: An Empirical Study of Reuse and Integration of Pre-Trained Models in Open-Source Projects](https://arxiv.org/abs/2509.06085)
*Jerin Yasmin,Wenxin Jiang,James C. Davis,Yuan Tian*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Pre-trained models (PTMs) are machine learning models that have been trained
in advance, often on large-scale data, and can be reused for new tasks, thereby
reducing the need for costly training from scratch. Their widespread adoption
introduces a new class of software dependency, which we term Software
Dependencies 2.0, extending beyond conventional libraries to learned behaviors
embodied in trained models and their associated artifacts. The integration of
PTMs as software dependencies in real projects remains unclear, potentially
threatening maintainability and reliability of modern software systems that
increasingly rely on them. Objective: In this study, we investigate Software
Dependencies 2.0 in open-source software (OSS) projects by examining the reuse
of PTMs, with a focus on how developers manage and integrate these models.
Specifically, we seek to understand: (1) how OSS projects structure and
document their PTM dependencies; (2) what stages and organizational patterns
emerge in the reuse pipelines of PTMs within these projects; and (3) the
interactions among PTMs and other learned components across pipeline stages. We
conduct a mixed-methods analysis of a statistically significant random sample
of 401 GitHub repositories from the PeaTMOSS dataset (28,575 repositories
reusing PTMs from Hugging Face and PyTorch Hub). We quantitatively examine PTM
reuse by identifying patterns and qualitatively investigate how developers
integrate and manage these models in practice.

</details>


### [50] [Agentic Software Engineering: Foundational Pillars and a Research Roadmap](https://arxiv.org/abs/2509.06216)
*Ahmed E. Hassan,Hao Li,Dayi Lin,Bram Adams,Tse-Hsun Chen,Yutaro Kashiwa,Dong Qiu*

Main category: cs.SE

TL;DR: This paper introduces SE 3.0 through a dual-modality framework that redefines software engineering with intelligent agents, proposing ACE (human coordination hub) and AEE (agent workspace) environments. It establishes structured collaboration processes between humans and agents while providing a research roadmap for the field's evolution.


<details>
  <summary>Details</summary>
Motivation: The rise of intelligent agents in software engineering demands a paradigm shift from code-centric practices to structured, goal-oriented systems engineering. The duality of human-agent collaboration creates complex trust, scalability, and coordination challenges currently lacking in foundational SE frameworks.

Method: The paper introduces two symbiotic workbench environments: (1) Agent Command Environment (ACE) for human-agent orchestration and (2) Agent Execution Environment (AEE) for autonomous agent tasks with human intervention. These frameworks redefine actors, processes, tools, and artifacts in agentic SE.

Result: The authors present the Structured Agentic Software Engineering (SASE) vision, defining standardized artifacts like Merge-Readiness Packs and Consultation Request Packs. The paper establishes a research roadmap targeting key technical challenges and educational implications for this emerging field.

Conclusion: This paper proposes a conceptual framework for Agentic Software Engineering (SE 3.0) through the dual modality approach of SE for Humans and SE for Agents. It outlines foundational pillars, introduces ACE and AEE environments, and emphasizes the need for community-driven dialogue to redefine traditional SE principles.

Abstract: Agentic Software Engineering (SE 3.0) represents a new era where intelligent
agents are tasked not with simple code generation, but with achieving complex,
goal-oriented SE objectives. To harness these new capabilities while ensuring
trustworthiness, we must recognize a fundamental duality within the SE field in
the Agentic SE era, comprising two symbiotic modalities: SE for Humans and SE
for Agents. This duality demands a radical reimagining of the foundational
pillars of SE (actors, processes, tools, and artifacts) which manifest
differently across each modality. We propose two purpose-built workbenches to
support this vision. The Agent Command Environment (ACE) serves as a command
center where humans orchestrate and mentor agent teams, handling outputs such
as Merge-Readiness Packs (MRPs) and Consultation Request Packs (CRPs). The
Agent Execution Environment (AEE) is a digital workspace where agents perform
tasks while invoking human expertise when facing ambiguity or complex
trade-offs. This bi-directional partnership, which supports agent-initiated
human callbacks and handovers, gives rise to new, structured engineering
activities (i.e., processes) that redefine human-AI collaboration, elevating
the practice from agentic coding to true agentic software engineering. This
paper presents the Structured Agentic Software Engineering (SASE) vision,
outlining several of the foundational pillars for the future of SE. The paper
culminates in a research roadmap that identifies a few key challenges and
opportunities while briefly discussing the resulting impact of this future on
SE education. Our goal is not to offer a definitive solution, but to provide a
conceptual scaffold with structured vocabulary to catalyze a community-wide
dialogue, pushing the SE community to think beyond its classic, human-centric
tenets toward a disciplined, scalable, and trustworthy agentic future.

</details>


### [51] [Learning From Software Failures: A Case Study at a National Space Research Center](https://arxiv.org/abs/2509.06301)
*Dharun Anandayuvaraj,Zain Hammadeh,Andreas Lund,Alexandra Holloway,James C. Davis*

Main category: cs.SE

TL;DR: Case studies in HROs show failure learning is informal and fragmented, leading to recurring issues. Structured processes and addressing organizational challenges could enhance reliability.


<details>
  <summary>Details</summary>
Motivation: To address the lack of in-depth industry insights into failure learning practices in high-reliability software systems and improve reliability processes.

Method: Conducted 10 in-depth interviews at a national space research center and 5 additional interviews at other HROs, examining failure learning practices.

Result: Three key findings: (1) Failure learning is informal and inconsistent in SDLC integration; (2) Recurring failures due to unstructured processes; (3) Challenges include time constraints, knowledge loss, and weak process enforcement.

Conclusion: The study reveals that failure learning in software HROs is hindered by informal processes, recurring failures, and organizational challenges, offering guidance to improve failure management practices.

Abstract: Software failures can have significant consequences, making learning from
failures a critical aspect of software engineering. While software
organizations are recommended to conduct postmortems, the effectiveness and
adoption of these practices vary widely. Understanding how engineers gather,
document, share, and apply lessons from failures is essential for improving
reliability and preventing recurrence. High-reliability organizations (HROs)
often develop software systems where failures carry catastrophic risks,
requiring continuous learning to ensure reliability. These organizations
provide a valuable setting to examine practices and challenges for learning
from software failures. Such insight could help develop processes and tools to
improve reliability and prevent recurrence. However, we lack in-depth industry
perspectives on the practices and challenges of learning from failures.
  To address this gap, we conducted a case study through 10 in-depth interviews
with research software engineers at a national space research center. We
examine how they learn from failures: how they gather, document, share, and
apply lessons. To assess transferability, we include data from 5 additional
interviews at other HROs. Our findings provide insight into how engineers learn
from failures in practice. To summarize: (1) failure learning is informal, ad
hoc, and inconsistently integrated into SDLC; (2) recurring failures persist
due to absence of structured processes; and (3) key challenges, including time
constraints, knowledge loss from turnover and fragmented documentation, and
weak process enforcement, undermine systematic learning. Our findings deepen
understanding of how software engineers learn from failures and offer guidance
for improving failure management practices.

</details>


### [52] [A Generic and Efficient Python Runtime Verification System and its Large-scale Evaluation](https://arxiv.org/abs/2509.06324)
*Zhuohang Shen,Mohammed Yaseen,Denini Silva,Kevin Guan,Junho Lee,Marcelo d'Amorim,Owolabi Legunsen*

Main category: cs.SE

TL;DR: PyMOP is a generic, efficient runtime verification (RV ) system for Python that addresses the limitations of existing tools, achieving significant speed improvements and finding/fxing real-world bugs.


<details>
  <summary>Details</summary>
Motivation: Existing Python RV systems are either domain-specific, limited to particular logics, or slow. PyMOP aims to provide a scalable, extensible solution for Python's growing ecosystem to enable robust dynamic analysis and bug detection.

Method: PyMOP implements five monitoring algorithms, supports five logics, includes 73 API specs, and three instrumentation strategies. It was evaluated across 290,133 unit tests in 1,463 GitHub projects, comparing performance to existing systems and validating bug-finding capabilities.

Result: 1) Default Java RV algorithms are often suboptimal for Python. 2) PyMOP is up to 1,168.3× faster than recent dynamic analysis tools. 3) 44/121 bugs found by PyMOP were fxed by developers. 4) Supports broad extensibility for monitoring logics and APIs.

Conclusion: PyMOP establishes a foundational platform for advancing Python RV, demonstrating superior efficiency and practical bug-finding capabilities while enabling future research through its modular design and comprehensive API coverage.

Abstract: Runtime verification (RV) now scales for testing thousands of open-source
Java projects, helping find hundreds of bugs. The popular Python ecosystem
could use such benefits. But, today's Python RV systems are limited to a domain
or specification logic, or slow. We propose PyMOP, a generic, extensible, and
efficient RV system for Python. PyMOP supports five logics, implements five
existing monitoring algorithms, ships with 73 API specs of Python and
widely-used libraries, supports three instrumentation strategies, and users can
easily add more of these. On 290,133 unit tests in 1,463 GitHub projects, we
find mainly that (i) the default monitoring algorithm for Java is often not the
fastest for Python; (ii) PyMOP is up to 1,168.3x faster than two recent dynamic
analysis systems; and (iii) 44 of 121 bugs that PyMOP helped find so far were
fixed by developers. PyMOP's generality and efficiency position it well as an
excellent platform for the next advances on RV for Python.

</details>


### [53] [Analyzing the Instability of Large Language Models in Automated Bug Injection and Correction](https://arxiv.org/abs/2509.06429)
*Mehmet Bilal Er,Nagehan İlhan,Umut Kuran*

Main category: cs.SE

TL;DR: This study evaluates ChatGPT's instability in bug-fixing tasks across different temperature settings, finding that higher temperatures lead to increased instability and functional failures. Syntax and functional consistency metrics highlight the need for caution when relying on LLMs for software development.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the growing use of LLMs in software engineering for tasks like bug fixing and code generation, coupled with the lack of thorough analysis on their instability in bug-fixing contexts. The study aims to address this gap and inform reliable application of LLM-based tools.

Method: The study evaluates structural, syntactic, and functional variations in ChatGPT's bug-fixing outputs using 20 code problems, three temperature settings (0, 0.5, 1), and nine outputs per problem. Syntax Similarity and Output Equivalence Rate (OER) metrics were employed to measure consistency.

Result: Results show that higher temperature settings significantly increase instability and functional failure rates in ChatGPT's outputs. At low temperatures, structural similarity remains high, but structural differences become pronounced at high temperatures. Metrics reveal a strong correlation between temperature and output variability.

Conclusion: The study concludes that LLMs like ChatGPT exhibit significant instability in bug-fixing tasks, particularly at higher temperature settings, which raises doubts about their reliability for software development processes. It emphasizes the need for methodological improvements to ensure consistent application of LLM-based error correction systems.

Abstract: The use of Large Language Models (LLMs) in software engineering tasks is
growing, especially in the areas of bug fixing and code generation.
Nevertheless, these models often yield unstable results; when executed at
different times with the same input, they can generate radically different
code. The consistency of LLMs in bug-fixing tasks has not yet been thoroughly
assessed, despite the fact that this instability has typically been discussed
in the literature in relation to code generation. The purpose of this study is
to look into how unstable an LLM like ChatGPT is when it comes to fixing code
bugs. We examine the structural, syntactic, and functional variations among
several fix recommendations made in response to the same prompt using code
samples with various error types. Additionally, we assess how instability is
affected by the temperature settings (0, 0.5, and 1) used for the model's
deterministic operation. For a total of 20 problems in the experimental
analysis, the model produced three fix suggestions at each temperature value,
comparing nine distinct outputs for each problem. The Syntax Similarity and
Output Equivalence Rate (OER) metrics were used to assess the outputs'
structural and functional consistency. The results demonstrate that the model's
outputs become much more unstable and variable as the temperature rises, with
high temperatures showing especially high rates of functional failure.
According to syntax similarity analyses, the suggested fixes show notable
structural differences at high temperatures but are fairly similar at low
temperatures. The purpose of this study is to provide important methodological
insights into how LLM-based error correction systems can be applied more
consistently in software development processes while also casting doubt on
their dependability.

</details>


### [54] [Modeling in the Design Multiverse](https://arxiv.org/abs/2509.06530)
*Sylvain Guérin,Salvador Martinez,Ciprian Teodorov*

Main category: cs.SE

TL;DR: The paper introduces the Design Multiverse, a concept to manage design variability in multi-stakeholder systems by integrating revisions and variants into the modeling space, enabling traceability and analysis of design decisions.


<details>
  <summary>Details</summary>
Motivation: Current modeling tools cannot handle temporal-spatial variability in design processes involving multiple stakeholders, requiring external tools. This limits seamless management of design paths, variants, and dependencies.

Method: The authors propose a conceptual framework for the Design Multiverse, describe usage scenarios (model product lines, co-evolution of models/metamodels), and suggest an implementation using model federation.

Result: The Design Multiverse provides theoretical foundations and implementation guidelines for managing design artifacts as a unified space of revisions and variants, though practical validation is beyond the paper's scope.

Conclusion: The Design Multiverse offers a promising approach to integrate, trace, and analyze complex design ecosystems across stakeholders and time, bridging gaps between modeling tools and real-world design evolution.

Abstract: Real-world design processes often involve the evolution and divergence of
design paths (by branching, revising, merging, etc.), especially when multiple
stakeholders or teams operate concurrently and/or explore different
alternatives for complex and heterogeneous systems. Unfortunately, this
variability in time and space can not be directly managed in current modeling
spaces but requires resorting to external tools and methodologies.
  In order to tackle this problem, we introduce the Design Multiverse. The
Design Multiverse aims to integrate in the modeling space a selection of
revisions and variants, representing snapshots of a design state composed of
multiple artifacts. This enables stakeholders to seamlessly trace, analyze, and
manage design decisions, system variants, and their interdependencies.
Concretely, in this paper we present a conceptual definition of the Design
Multiverse, discuss usage scenarios such as model product lines and
model/metamodel co-evolution, and propose an implementation leveraging the
model federation paradigm.

</details>


### [55] [Design and Implementation of a Domain-specific Language for Modelling Evacuation Scenarios Using Eclipse EMG/GMF Tool](https://arxiv.org/abs/2509.06688)
*Heerok Banerjee*

Main category: cs.SE

TL;DR: The paper introduces Bmod, a domain-specific language (DSL) for evacuation scenario modeling built with Eclipse EMF/GMF, and compares it with other tools like AToMPM and Sirius.


<details>
  <summary>Details</summary>
Motivation: Existing DSLs require improved modeling frameworks with graphical interfaces and hierarchical structures to reduce complexity for novice users in enterprise settings.

Method: Developed Bmod using Eclipse EMF/GMF for graphical modeling. Conducted a comparative analysis of EMF/GMF with tools like AToMPM, metaDepth, and Sirius, evaluating expressiveness, learning curve, and performance.

Result: Bmod enables hierarchical evacuation modeling with graphical interfaces. The comparison highlights EMF/GMF's strengths in expressiveness and usability relative to alternatives.

Conclusion: Bmod addresses gaps in DSL tooling for enterprise scenario modeling, while the comparison provides guidelines for selecting modeling tools based on project requirements.

Abstract: Domain-specific languages (DSLs) play a crucial role in resolving internal
dependencies across enterprises and boosts their upfront business management
processes. Yet, a lot of development is needed to build modelling frameworks
which support graphical interfaces (canvas, pallettes etc.), hierarchical
structures and easy implementation to shorten the gap for novice users. In this
paper, a DSL namely, Bmod is introduced, which can be used to model evacuation
scenarios. The language is built using Eclipse Modelling Framework (EMF) and
Eclipse Graphical Modelling Framework (GMF). Furthermore, a comparison is also
shown between Eclipse EMF/GMF and other modelling tools such as AToMPM,
metaDepth, Sirius etc with respect to expressiveness, learning curve and
performance.

</details>


### [56] [Efficiently Ranking Software Variants with Minimal Benchmarks](https://arxiv.org/abs/2509.06716)
*Théo Matricon,Mathieu Acher,Helge Spieker,Arnaud Gotlieb*

Main category: cs.SE

TL;DR: This paper proposes BISS, a benchmark reduction method that maintains ranking stability while reducing computational costs by up to 99% through optimized test suite sampling.


<details>
  <summary>Details</summary>
Motivation: Benchmark execution is computationally expensive, necessitating methods to reduce costs while maintaining reliable performance comparisons across software variants.

Method: BISS employs test suite optimization through strategic retention of critical tests and a divide-and-conquer sampling approach to reduce benchmark subsets.

Result: BISS achieves 44% average computational cost reduction (up to 99% in some cases) across LLM leaderboards, SAT competitions, and configurable systems without compromising ranking stability.

Conclusion: The paper concludes that BISS effectively reduces benchmarking costs while preserving ranking stability, outperforming baseline methods.

Abstract: Benchmarking is a common practice in software engineering to assess the
qualities and performance of software variants, coming from multiple competing
systems or from configurations of the same system. Benchmarks are used notably
to compare and understand variant performance, fine-tune software, detect
regressions, or design new software systems. The execution of benchmarks to get
a complete picture of software variants is highly costly in terms of
computational resources and time. In this paper, we propose a novel approach
for reducing benchmarks while maintaining stable rankings, using test suite
optimization techniques. That is, we remove instances from the benchmarks while
trying to keep the same rankings of the variants on all tests. Our method,
BISection Sampling, BISS, strategically retains the most critical tests and
applies a novel divide-and-conquer approach to efficiently sample among
relevant remaining tests. We experiment with datasets and use cases from LLM
leaderboards, SAT competitions, and configurable systems for performance
modeling. Our results show that our method outperforms baselines even when
operating on a subset of variants. Using BISS, we reduce the computational cost
of the benchmarks on average to 44% and on more than half the benchmarks by up
to 99% without loss in ranking stability.

</details>


### [57] [OpenCoderRank: AI-Driven Technical Assessments Made Easy](https://arxiv.org/abs/2509.06774)
*Hridoy Sankar Dutta,Sana Ansari,Swati Kumari,Shounak Ravi Bhalerao*

Main category: cs.SE

TL;DR: OpenCoderRank is an open-source platform for conducting customizable, cost-effective technical coding assessments, addressing both problem-setting and solving challenges in resource-constrained environments.


<details>
  <summary>Details</summary>
Motivation: Technical assessments require balancing problem difficulty and integrity, especially with LLMs potentially exposing solutions. Organizations need accessible, customizable tools for evaluating coding skills.

Method: The platform provides a self-hosted, no-cost interface for problem setters to design assessments and problem solvers to practice under time constraints, with features to ensure assessment security.

Result: OpenCoderRank enables seamless, low-barrier technical evaluations without compromising problem integrity while supporting diverse educational and professional use cases.

Conclusion: The platform offers a scalable solution for fair, practical coding assessments in resource-limited settings by eliminating financial and technical barriers to implementation.

Abstract: Organizations and educational institutions use time-bound assessment tasks to
evaluate coding and problem-solving skills. These assessments measure not only
the correctness of the solutions, but also their efficiency. Problem setters
(educator/interviewer) are responsible for crafting these challenges, carefully
balancing difficulty and relevance to create meaningful evaluation experiences.
Conversely, problem solvers (student/interviewee) apply coding efficiency and
logical thinking to arrive at correct solutions. In the era of Large Language
Models (LLMs), LLMs assist problem setters in generating diverse and
challenging questions, but they can undermine assessment integrity for problem
solvers by providing easy access to solutions. This paper introduces
OpenCoderRank, an easy-to-use platform designed to simulate technical
assessments. It acts as a bridge between problem setters and problem solvers,
helping solvers prepare for time constraints and unfamiliar problems while
allowing setters to self-host assessments, offering a no-cost and customizable
solution for technical assessments in resource-constrained environments.

</details>


### [58] [Hypergraph-Guided Regex Filter Synthesis for Event-Based Anomaly Detection](https://arxiv.org/abs/2509.06911)
*Margarida Ferreira,Victor Nicolet,Luan Pham,Joey Dodds,Daniel Kroening,Ines Lynce,Ruben Martins*

Main category: cs.SE

TL;DR: HyGLAD is an interpretable algorithm for event-based anomaly detection that outperforms deep-learning methods in precision and recall while being more efficient.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for interpretable anomaly detection in event data from real-world stationary systems, where deviations indicate potential malicious activities. Existing deep-learning approaches, while effective, lack interpretability, making it hard to understand and act on detected anomalies.

Method: HyGLAD infers equivalence classes of entities with similar behavior from event data and constructs interpretable regular expressions to model these classes. This contrasts with deep-learning techniques by providing transparency through regular expressions, allowing for clear anomaly explanations.

Result: HyGLAD was evaluated against seven unsupervised anomaly detection methods from DeepOD on five real-world datasets. It demonstrated a 1.2x improvement in precision and a 1.3x improvement in recall compared to the second-best baseline. Training and inference were significantly more efficient (single CPU vs GPU).

Conclusion: HyGLAD offers a new approach to event-based anomaly detection that is both more accurate and interpretable than deep-learning models while maintaining higher efficiency. This suggests it is well-suited for environments where model interpretability and performance are both critical.

Abstract: We propose HyGLAD, a novel algorithm that automatically builds a set of
interpretable patterns that model event data. These patterns can then be used
to detect event-based anomalies in a stationary system, where any deviation
from past behavior may indicate malicious activity. The algorithm infers
equivalence classes of entities with similar behavior observed from the events,
and then builds regular expressions that capture the values of those entities.
As opposed to deep-learning approaches, the regular expressions are directly
interpretable, which also translates to interpretable anomalies. We evaluate
HyGLAD against all 7 unsupervised anomaly detection methods from DeepOD on five
datasets from real-world systems. The experimental results show that on average
HyGLAD outperforms existing deep-learning methods while being an order of
magnitude more efficient in training and inference (single CPU vs GPU).
Precision improved by 1.2x and recall by 1.3x compared to the second-best
baseline.

</details>
