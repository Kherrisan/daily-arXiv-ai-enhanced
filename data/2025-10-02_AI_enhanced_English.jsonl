{"id": "2510.00151", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00151", "abs": "https://arxiv.org/abs/2510.00151", "authors": ["Valentin Barbaza", "Alan Rodrigo Diaz-Rizo", "Hassan Aboushady", "Spyridon Raptis", "Haralampos-G. Stratigopoulos"], "title": "Stealing AI Model Weights Through Covert Communication Channels", "comment": null, "summary": "AI models are often regarded as valuable intellectual property due to the\nhigh cost of their development, the competitive advantage they provide, and the\nproprietary techniques involved in their creation. As a result, AI model\nstealing attacks pose a serious concern for AI model providers. In this work,\nwe present a novel attack targeting wireless devices equipped with AI hardware\naccelerators. The attack unfolds in two phases. In the first phase, the\nvictim's device is compromised with a hardware Trojan (HT) designed to covertly\nleak model weights through a hidden communication channel, without the victim\nrealizing it. In the second phase, the adversary uses a nearby wireless device\nto intercept the victim's transmission frames during normal operation and\nincrementally reconstruct the complete weight matrix. The proposed attack is\nagnostic to both the AI model architecture and the hardware accelerator used.\nWe validate our approach through a hardware-based demonstration involving four\ndiverse AI models of varying types and sizes. We detail the design of the HT\nand the covert channel, highlighting their stealthy nature. Additionally, we\nanalyze the impact of bit error rates on the reception and propose an error\nmitigation technique. The effectiveness of the attack is evaluated based on the\naccuracy of the reconstructed models with stolen weights and the time required\nto extract them. Finally, we explore potential defense mechanisms.", "AI": {"tldr": "The paper discusses an AI model stealing attack through hardware Trojans and wireless interception, validated with various models.", "motivation": "High cost and proprietary value of AI models make them targets for stealing attacks, creating a need for understanding their vulnerabilities.", "method": "The attack compromises a device with a hardware Trojan to leak weights via hidden communication channels then uses wireless devices to intercept and reconstruct the stolen data.", "result": "Demonstration showed the attack works across multiple AI models, with techniques addressing bit error rates to enhance mitigation and recovery.", "conclusion": "The research underscores the vulnerability of AI hardware and wireless systems to stealing, with implications for future defenses against such covert attacks."}}
{"id": "2510.00164", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.00164", "abs": "https://arxiv.org/abs/2510.00164", "authors": ["Dominik Apel", "Zeta Avarikioti", "Matteo Maffei", "Yuheng Wang"], "title": "Calyx: Privacy-Preserving Multi-Token Optimistic-Rollup Protocol", "comment": null, "summary": "Rollup protocols have recently received significant attention as a promising\nclass of Layer 2 (L2) scalability solutions. By utilizing the Layer 1 (L1)\nblockchain solely as a bulletin board for a summary of the executed\ntransactions and state changes, rollups enable secure off-chain execution while\navoiding the complexity of other L2 mechanisms. However, to ensure data\navailability, current rollup protocols require the plaintext of executed\ntransactions to be published on-chain, resulting in inherent privacy\nlimitations.\n  In this paper, we address this problem by introducing Calyx, the first\nprivacy-preserving multi-token optimistic-Rollup protocol. Calyx guarantees\nfull payment privacy for all L2 transactions, revealing no information about\nthe sender, recipient, transferred amount, or token type. The protocol further\nsupports atomic execution of multiple multi-token transactions and introduces a\ntransaction fee scheme to enable broader application scenarios while ensuring\nthe sustainable operation of the protocol. To enforce correctness, Calyx adopts\nan efficient one-step fraud-proof mechanism. We analyze the security and\nprivacy guarantees of the protocol and provide an implementation and\nevaluation. Our results show that executing a single transaction costs\napproximately $0.06 (0.00002 ETH) and incurs only constant-size on-chain cost\nin asymptotic terms.", "AI": {"tldr": "Introduce Calyx, a multi-token optimistic-Rollup protocol ensuring full L2 transaction privacy and efficient fraud-proofing with low on-chain costs.", "motivation": "Current rollups need transaction plaintext on-chain for data availability, leading to privacy issues. There's a lack of solutions that provide transaction privacy while maintaining the benefits of rollup scalability.", "method": "Calyx uses a one-step fraud-proof mechanism and other techniques to guarantee sender, recipient, amount, and token-type privacy. It also supports atomic multi-token transactions and includes a transaction fee scheme for sustainability.", "result": "Single transaction cost is ~$0.06 (0.00002 ETH) with constant on-chain cost asymptotically. Implementation is analyzed and evaluated.", "conclusion": "Calyx is the first protocol to enable private assets without compromising performance in a multi-token optimistic rollup setting, proving feasibility of private on-chain asset transfers with moderate cost."}}
{"id": "2510.00181", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00181", "abs": "https://arxiv.org/abs/2510.00181", "authors": ["Luis Burbano", "Diego Ortiz", "Qi Sun", "Siwei Yang", "Haoqin Tu", "Cihang Xie", "Yinzhi Cao", "Alvaro A Cardenas"], "title": "CHAI: Command Hijacking against embodied AI", "comment": null, "summary": "Embodied Artificial Intelligence (AI) promises to handle edge cases in\nrobotic vehicle systems where data is scarce by using common-sense reasoning\ngrounded in perception and action to generalize beyond training distributions\nand adapt to novel real-world situations. These capabilities, however, also\ncreate new security risks. In this paper, we introduce CHAI (Command Hijacking\nagainst embodied AI), a new class of prompt-based attacks that exploit the\nmultimodal language interpretation abilities of Large Visual-Language Models\n(LVLMs). CHAI embeds deceptive natural language instructions, such as\nmisleading signs, in visual input, systematically searches the token space,\nbuilds a dictionary of prompts, and guides an attacker model to generate Visual\nAttack Prompts. We evaluate CHAI on four LVLM agents; drone emergency landing,\nautonomous driving, and aerial object tracking, and on a real robotic vehicle.\nOur experiments show that CHAI consistently outperforms state-of-the-art\nattacks. By exploiting the semantic and multimodal reasoning strengths of\nnext-generation embodied AI systems, CHAI underscores the urgent need for\ndefenses that extend beyond traditional adversarial robustness.", "AI": {"tldr": "CHAI is a new class of prompt-based attacks exploiting multimodal reasoning in embodied AI, demonstrating superior effectiveness against LVLMs and emphasizing the need for advanced defenses.", "motivation": "Next-generation embodied AI systems' security risks need investigation due to their multimodal language interpretation capabilities introducing novel vulnerabilities.", "method": "CHAI employs deceptive visual inputs, token space search, prompt dictionaries, and attacker model guidance to generate Visual Attack Prompts targeting LVLMs.", "result": "CHAI outperformed state-of-the-art attacks across drone landing, autonomous driving, object tracking, and real robotic vehicle scenarios.", "conclusion": "The paper highlights the urgent need for defenses beyond traditional adversarial robustness to counter attacks leveraging the multimodal reasoning strengths of embodied AI systems."}}
