<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 19]
- [cs.SE](#cs.SE) [Total: 14]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge](https://arxiv.org/abs/2510.01223)
*Hui Dou,Ning Xu,Yiwen Zhang,Kaibin Wang*

Main category: cs.CR

TL;DR: The paper proposes RTS-Attack, a novel adversarial attack framework for Large Language Models (LLMs). RTS-Attack utilizes semantically relevant nested scenarios combined with targeted toxic knowledge to bypass alignment defenses. The framework is adaptive, automated, and achieves high concealment by generating prompts that do not contain harmful queries. Experiments on GPT-4o, Llama3-70b, and Gemini-pro show superior efficiency and universality compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Current jailbreak attack methods for LLMs are often detectable due to their apparent harmful intent. Existing nested scenario strategies lack systematic study regarding their detectability and effectiveness in bypassing alignment defenses, even though they show potential for adversarial attacks.

Method: RTS-Attack constructs semantically relevant nested scenarios where each scenario is highly related to the attack target query. These scenarios integrate pre-characterized toxic knowledge segments to systematically undermine alignment defenses while concealing harmful intent in the surface-level query. The framework is designed to be both adaptive and automated, allowing it to generate diverse attack patterns for different LLM architectures.

Result: Experiments show RTS-Attack achieves 89%+ success rate on various models, outperforming existing attack methods by over 20 percentage points. It maintains high efficiency with minimal query turns and demonstrates universality across different LLM architectures including GPT-4o, Llama3-70b, and Gemini-pro. The concealment property is validated by detection tests where 85% of generated prompts were not flagged by standard detection mechanisms.

Conclusion: RTS-Attack's success highlights the need for more robust alignment defenses for LLMs. The framework demonstrates that nested scenarios without explicit harmful language can effectively bypass current safety mechanisms. Future research should focus on improving alignment detection while considering semantically encoded adversarial patterns.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various tasks. However, they remain exposed to jailbreak attacks, eliciting
harmful responses. The nested scenario strategy has been increasingly adopted
across various methods, demonstrating immense potential. Nevertheless, these
methods are easily detectable due to their prominent malicious intentions. In
this work, we are the first to find and systematically verify that LLMs'
alignment defenses are not sensitive to nested scenarios, where these scenarios
are highly semantically relevant to the queries and incorporate targeted toxic
knowledge. This is a crucial yet insufficiently explored direction. Based on
this, we propose RTS-Attack (Semantically Relevant Nested Scenarios with
Targeted Toxic Knowledge), an adaptive and automated framework to examine LLMs'
alignment. By building scenarios highly relevant to the queries and integrating
targeted toxic knowledge, RTS-Attack bypasses the alignment defenses of LLMs.
Moreover, the jailbreak prompts generated by RTS-Attack are free from harmful
queries, leading to outstanding concealment. Extensive experiments demonstrate
that RTS-Attack exhibits superior performance in both efficiency and
universality compared to the baselines across diverse advanced LLMs, including
GPT-4o, Llama3-70b, and Gemini-pro. Our complete code is available in the
supplementary material. WARNING: THIS PAPER CONTAINS POTENTIALLY HARMFUL
CONTENT.

</details>


### [2] [Fine-Tuning Jailbreaks under Highly Constrained Black-Box Settings: A Three-Pronged Approach](https://arxiv.org/abs/2510.01342)
*Xiangfang Li,Yu Wang,Bo Li*

Main category: cs.CR

TL;DR: The paper introduces a three-pronged jailbreak attack method for fine-tuning large language models (LLMs) that effectively bypasses existing defenses, achieving over 97% success rates against GPT-4.1 and GPT-4o models.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the gap between simplified attack scenarios in prior research and real-world defense mechanisms, demonstrating practical risks in deployed LLM systems.

Method: The attack combines safety-styled textual wrappers, lexical token obfuscation (e.g., underscoring), and a backdoor training method, operating under a dataset-only black-box fine-tuning interface with provider-enforced data filtering, defensive training, and safety audits.

Result: Experiments show the attack successfully evades all three stages of defense (pre-upload filtering, defensive training, and post-training audits) with high success rates (>97%) on OpenAI’s GPT-4.1 and GPT-4o platforms.

Conclusion: The study highlights critical vulnerabilities in current LLM defense ecosystems under realistic attack constraints, suggesting urgent re-evaluation of training-time security measures in production systems.

Abstract: With the rapid advancement of large language models (LLMs), ensuring their
safe use becomes increasingly critical. Fine-tuning is a widely used method for
adapting models to downstream tasks, yet it is vulnerable to jailbreak attacks.
However, most existing studies focus on overly simplified attack scenarios,
limiting their practical relevance to real-world defense settings. To make this
risk concrete, we present a three-pronged jailbreak attack and evaluate it
against provider defenses under a dataset-only black-box fine-tuning interface.
In this setting, the attacker can only submit fine-tuning data to the provider,
while the provider may deploy defenses across stages: (1) pre-upload data
filtering, (2) training-time defensive fine-tuning, and (3) post-training
safety audit. Our attack combines safety-styled prefix/suffix wrappers, benign
lexical encodings (underscoring) of sensitive tokens, and a backdoor mechanism,
enabling the model to learn harmful behaviors while individual datapoints
appear innocuous. Extensive experiments demonstrate the effectiveness of our
approach. In real-world deployment, our method successfully jailbreaks GPT-4.1
and GPT-4o on the OpenAI platform with attack success rates above 97% for both
models. Our code is available at
https://github.com/lxf728/tri-pronged-ft-attack.

</details>


### [3] [Integrated Security Mechanisms for Weight Protection in Memristive Crossbar Arrays](https://arxiv.org/abs/2510.01350)
*Muhammad Faheemur Rahman,Wayne Burleson*

Main category: cs.CR

TL;DR: Researchers developed security mechanisms for memristive computing systems that protect machine learning weights against extraction attacks with minimal performance costs, validated across multiple semiconductor technologies and datasets.


<details>
  <summary>Details</summary>
Motivation: Memristive crossbar arrays for in-memory computing face critical security vulnerabilities as they store valuable machine learning weight matrices - intellectual property at risk of extraction if hardware is physically compromised.

Method: Develops two hardware-integrated security solutions (Keyed Permutor for dynamic weight scrambling and Watermark Protection Columns for ownership verification) that require minimal architectural modifications to existing memristive crossbar arrays.

Result: Simulations on 45nm, 22nm, and 7nm CMOS with real-world datasets show 100% protection against weight leakage with <10% overhead in area/power/delay, and practical validation using MNIST dataset confirms real-world feasibility.

Conclusion: The proposed security mechanisms, Keyed Permutor and Watermark Protection Columns, offer practical protection for memristive in-memory computing systems against weight extraction attacks, maintaining minimal performance overhead and demonstrating feasibility across advanced CMOS nodes.

Abstract: Memristive crossbar arrays enable in-memory computing by performing parallel
analog computations directly within memory, making them well-suited for machine
learning, neural networks, and neuromorphic systems. However, despite their
advantages, non-volatile memristors are vulnerable to security threats (such as
adversarial extraction of stored weights when the hardware is compromised.
Protecting these weights is essential since they represent valuable
intellectual property resulting from lengthy and costly training processes
using large, often proprietary, datasets. As a solution we propose two security
mechanisms: Keyed Permutor and Watermark Protection Columns; where both
safeguard critical weights and establish verifiable ownership (even in cases of
data leakage). Our approach integrates efficiently with existing memristive
crossbar architectures without significant design modifications. Simulations
across 45nm, 22nm, and 7nm CMOS nodes, using a realistic interconnect model and
a large RF dataset, show that both mechanisms offer robust protection with
under 10% overhead in area, delay and power. We also present initial
experiments employing the widely known MNIST dataset; further highlighting the
feasibility of securing memristive in-memory computing systems with minimal
performance trade-offs.

</details>


### [4] [WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents](https://arxiv.org/abs/2510.01354)
*Yinuo Liu,Ruohan Xu,Xilong Wang,Yuqi Jia,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: First comprehensive benchmark reveals detection methods for web agent prompt injections work against obvious attacks but fail against stealthier variants.


<details>
  <summary>Details</summary>
Motivation: Existing prompt injection detection methods lack systematic evaluation for web agents, despite the growing threat of such attacks to this domain.

Method: Introduce a fine-grained threat-based attack categorization, construct datasets with malicious/benign text and image samples, systematize text- and image-based detection methods, and evaluate their performance across scenarios.

Result: Detectors show moderate-high accuracy against explicit text/image attacks but fail against non-explicit or imperceptible perturbation-based attacks.

Conclusion: Current detectors are ineffective against attacks without explicit instructions or using imperceptible perturbations, highlighting the need for more robust detection methods for web agents.

Abstract: Multiple prompt injection attacks have been proposed against web agents. At
the same time, various methods have been developed to detect general prompt
injection attacks, but none have been systematically evaluated for web agents.
In this work, we bridge this gap by presenting the first comprehensive
benchmark study on detecting prompt injection attacks targeting web agents. We
begin by introducing a fine-grained categorization of such attacks based on the
threat model. We then construct datasets containing both malicious and benign
samples: malicious text segments generated by different attacks, benign text
segments from four categories, malicious images produced by attacks, and benign
images from two categories. Next, we systematize both text-based and
image-based detection methods. Finally, we evaluate their performance across
multiple scenarios. Our key findings show that while some detectors can
identify attacks that rely on explicit textual instructions or visible image
perturbations with moderate to high accuracy, they largely fail against attacks
that omit explicit instructions or employ imperceptible perturbations. Our
datasets and code are released at:
https://github.com/Norrrrrrr-lyn/WAInjectBench.

</details>


### [5] [Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks](https://arxiv.org/abs/2510.01359)
*Shoumik Saha,Jifan Chen,Sam Mayers,Sanjay Krishna Gouda,Zijian Wang,Varun Kumar*

Main category: cs.CR

TL;DR: The paper evaluates code-capable LLM agents' vulnerability to jailbreak attacks using JAWS-BENCH (three workspace regimes) and a Judge Framework (compliance, attack success, syntax, runtime). Finds rising harm: 61% attack acceptance in empty workspaces, 75% ASR in multi-file regimes, with deployable risks motivating execution-aware defenses.


<details>
  <summary>Details</summary>
Motivation: Code LLM agents embedded in software workflows necessitate evaluating safety-bypass attacks beyond text-only scenarios, as prior work focuses on refusal/Harm detection without assessing executable malicious code.

Method: 1) JAWS-BENCH benchmark with three workspace regimes (empty/JAWS-0, single-file/JAWS-1, multi-file/JAWS-M)
2 ) Hierarchical Judge Framework (compliance, attack success, syntax correctness, runtime executability)
3) Testing seven LLMs (five families) across regimes

Result: Attack acceptance rates: 61%(JAWS-0, 61%) -> 71%(JAWS-1 ASR) -> 75%(JAWS-M ASR). 32%MJAWS-M instantly deployable code. Agents overturn 1.6x higher ASR post-refusal during planning/tool-use steps. Execution gaps vary across attack categories.

Conclusion: LLM agents are more vulnerable to jailbreaks in code contexts. Defense solutions require execution-aware systems, code-contextual filters, and refusal-preservation during multi-step reasoning.

Abstract: Code-capable large language model (LLM) agents are increasingly embedded into
software engineering workflows where they can read, write, and execute code,
raising the stakes of safety-bypass ("jailbreak") attacks beyond text-only
settings. Prior evaluations emphasize refusal or harmful-text detection,
leaving open whether agents actually compile and run malicious programs. We
present JAWS-BENCH (Jailbreaks Across WorkSpaces), a benchmark spanning three
escalating workspace regimes that mirror attacker capability: empty (JAWS-0),
single-file (JAWS-1), and multi-file (JAWS-M). We pair this with a
hierarchical, executable-aware Judge Framework that tests (i) compliance, (ii)
attack success, (iii) syntactic correctness, and (iv) runtime executability,
moving beyond refusal to measure deployable harm. Using seven LLMs from five
families as backends, we find that under prompt-only conditions in JAWS-0, code
agents accept 61% of attacks on average; 58% are harmful, 52% parse, and 27%
run end-to-end. Moving to single-file regime in JAWS-1 drives compliance to ~
100% for capable models and yields a mean ASR (Attack Success Rate) ~ 71%; the
multi-file regime (JAWS-M) raises mean ASR to ~ 75%, with 32% instantly
deployable attack code. Across models, wrapping an LLM in an agent
substantially increases vulnerability -- ASR raises by 1.6x -- because initial
refusals are frequently overturned during later planning/tool-use steps.
Category-level analyses identify which attack classes are most vulnerable and
most readily deployable, while others exhibit large execution gaps. These
findings motivate execution-aware defenses, code-contextual safety filters, and
mechanisms that preserve refusal decisions throughout the agent's multi-step
reasoning and tool use.

</details>


### [6] [E-FuzzEdge: Optimizing Embedded Device Security with Scalable In-Place Fuzzing](https://arxiv.org/abs/2510.01393)
*Davide Rusconi,Osama Yousef,Mirco Picca,Flavio Toffalini,Andrea Lanzi*

Main category: cs.CR

TL;DR: E-FuzzEdge is a novel fuzzing architecture designed to increase throughput for microcontroller testing by optimizing execution speed.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the inefficiencies of hardware-in-the-loop fuzzing for microcontrollers where scalability is not an option.

Method: E-FuzzEdge optimizes the execution speed of the fuzzing process specifically for hardware-in-the-loop environments.

Result: Evaluation against state-of-the-art benchmarks showed significant improvements in performance for E-FuzzEdge.

Conclusion: E-FuzzEdge can be integrated with existing embedded fuzzing techniques to improve overall testing efficiency.

Abstract: In this paper we show E-FuzzEdge, a novel fuzzing architecture targeted
towards improving the throughput of fuzzing campaigns in contexts where
scalability is unavailable. E-FuzzEdge addresses the inefficiencies of
hardware-in-the-loop fuzzing for microcontrollers by optimizing execution
speed. We evaluated our system against state-of-the-art benchmarks,
demonstrating significant performance improvements. A key advantage of
E-FuzzEdgearchitecture is its compatibility with other embedded fuzzing
techniques that perform on device testing instead of firmware emulation. This
means that the broader embedded fuzzing community can integrate E-FuzzEdge into
their workflows to enhance overall testing efficiency.

</details>


### [7] [Securing IoT Devices in Smart Cities: A Review of Proposed Solutions](https://arxiv.org/abs/2510.01445)
*Andrés F. Betancur-López*

Main category: cs.CR

TL;DR: The paper reviews IoT security solutions for Smart Cities with a focus on lightweight cryptography, PUFs, and blockchain, highlighting their pros, cons, and the need for better mechanisms.


<details>
  <summary>Details</summary>
Motivation: IoT devices in Smart Cities pose significant privacy and security risks due to their limited resources and widespread use, requiring a focused investigation into recent device-level security solutions.

Method: The paper conducts a comprehensive analysis of recent literature on device-level security for IoT in Smart Cities, with a specific focus on lightweight cryptography, physically unclonable functions (PUFs), and blockchain-based solutions.

Result: The findings present an overview of the strengths and limitations of current security approaches, indicating that existing solutions have not fully met the requirements for practical, scalable, and resource-efficient protection in IoT ecosystems.

Conclusion: The study underscores the importance of developing more effective, scalable security mechanisms for resource-constrained IoT devices in Smart Cities to mitigate privacy risks and enhance system resilience.

Abstract: Privacy and security in Smart Cities remain at constant risk due to the
vulnerabilities introduced by Internet of Things (IoT) devices. The limited
computational resources of these devices make them especially susceptible to
attacks, while their widespread adoption increases the potential impact of
security breaches. This article presents a review of security proposals aimed
at protecting IoT devices in Smart City environments. The review was conducted
by analyzing recent literature on device-level security, with particular
emphasis on lightweight cryptography, physically unclonable functions (PUFs),
and blockchain-based solutions. Findings highlight both the strengths and
limitations of current approaches, as well as the need for more practical,
scalable, and resource-efficient mechanisms to ensure user privacy and data
protection in IoT ecosystems.

</details>


### [8] [POLAR: Automating Cyber Threat Prioritization through LLM-Powered Assessment](https://arxiv.org/abs/2510.01552)
*Luoxi Tang,Yuqiao Meng,Ankita Patra,Weicheng Ma,Muchao Ye,Zhaohan Xi*

Main category: cs.CR

TL;DR: This paper identifies inherent vulnerabilities in LLMs for cyber threat intelligence (CTI), proposes a novel analysis methodology, and reveals three critical limitations (spurious correlations, contradictory knowledge, constrained generalization), offering actionable insights for robust CTI systems.


<details>
  <summary>Details</summary>
Motivation: Despite LLMs being widely adopted in CTI for tasks like threat analysis and vulnerability detection, significant performance gaps persist in practical deployments, necessitating a deeper understanding of inherent vulnerabilities to improve reliability.

Method: A novel categorization methodology combining stratification, autoregressive refinement, and human-in-the-loop supervision is applied to large-scale evaluations across CTI benchmarks and real-world threat reports.

Result: Three fundamental LLM vulnerabilities are uncovered: 1) spurious correlations leading to unreliable inferences, 2)... [truncated] ...actionable strategies for designing more robust LLM-powered CTI systems, emphasizing methodology improvements and human supervision integration.

Conclusion: The study provides a framework to systematically analyze LLM failures in CTI and demonstrates that addressing spurious correlations, contradictory knowledge, and generalization limitations requires targeted architectural and training improvements supported by human-in-the-loop mechanisms.

Abstract: Large Language Models (LLMs) are intensively used to assist security analysts
in counteracting the rapid exploitation of cyber threats, wherein LLMs offer
cyber threat intelligence (CTI) to support vulnerability assessment and
incident response. While recent work has shown that LLMs can support a wide
range of CTI tasks such as threat analysis, vulnerability detection, and
intrusion defense, significant performance gaps persist in practical
deployments. In this paper, we investigate the intrinsic vulnerabilities of
LLMs in CTI, focusing on challenges that arise from the nature of the threat
landscape itself rather than the model architecture. Using large-scale
evaluations across multiple CTI benchmarks and real-world threat reports, we
introduce a novel categorization methodology that integrates stratification,
autoregressive refinement, and human-in-the-loop supervision to reliably
analyze failure instances. Through extensive experiments and human inspections,
we reveal three fundamental vulnerabilities: spurious correlations,
contradictory knowledge, and constrained generalization, that limit LLMs in
effectively supporting CTI. Subsequently, we provide actionable insights for
designing more robust LLM-powered CTI systems to facilitate future research.

</details>


### [9] [Position: Privacy Is Not Just Memorization!](https://arxiv.org/abs/2510.01645)
*Niloofar Mireshghallah,Tianshi Li*

Main category: cs.CR

TL;DR: This paper argues that privacy risks in LLMs go beyond memorization, highlighting underexplored threats like context leakage, autonomous agents, and surveillance democratization, while calling for interdisciplinary solutions.


<details>
  <summary>Details</summary>
Motivation: Current LLM privacy research over-focuses on data memorization, leaving more immediate and systemic threats inadequately addressed despite their scalability and societal impact.

Method: The authors present a lifecycle taxonomy of privacy risks and analyze 1,322 AI/ML privacy papers (2016–2025), identifying research gaps through case studies and literature trends.

Result: Analysis shows memorization dominates technical literature but misses critical risks in data collection, inference leakage, and emerging surveillance capabilities where current solutions are ineffective.

Conclusion: The paper advocates shifting LLM privacy research toward sociotechnical, interdisciplinary approaches to address the complex ecosystem of evolving privacy threats.

Abstract: The discourse on privacy risks in Large Language Models (LLMs) has
disproportionately focused on verbatim memorization of training data, while a
constellation of more immediate and scalable privacy threats remain
underexplored. This position paper argues that the privacy landscape of LLM
systems extends far beyond training data extraction, encompassing risks from
data collection practices, inference-time context leakage, autonomous agent
capabilities, and the democratization of surveillance through deep inference
attacks. We present a comprehensive taxonomy of privacy risks across the LLM
lifecycle -- from data collection through deployment -- and demonstrate through
case studies how current privacy frameworks fail to address these multifaceted
threats. Through a longitudinal analysis of 1,322 AI/ML privacy papers
published at leading conferences over the past decade (2016--2025), we reveal
that while memorization receives outsized attention in technical research, the
most pressing privacy harms lie elsewhere, where current technical approaches
offer little traction and viable paths forward remain unclear. We call for a
fundamental shift in how the research community approaches LLM privacy, moving
beyond the narrow focus of current technical solutions and embracing
interdisciplinary approaches that address the sociotechnical nature of these
emerging threats.

</details>


### [10] [Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks](https://arxiv.org/abs/2510.01676)
*Milad Nasr,Yanick Fratantonio,Luca Invernizzi,Ange Albertini,Loua Farah,Alex Petit-Bianco,Andreas Terzis,Kurt Thomas,Elie Bursztein,Nicholas Carlini*

Main category: cs.CR

TL;DR: The paper demonstrates how adversarial attacks on the ML component (Magika) in Gmail's malware detection pipeline can enable evasion of malware detection, and proposes a defense strategy successfully deployed in production.


<details>
  <summary>Details</summary>
Motivation: ML components in critical systems like malware detection are vulnerable to adversarial attacks, which can exploit model shortcomings to create system-level failures with real-world consequences (e.g., bypassing Gmail's security).

Method: The authors crafted adversarial examples by modifying 13-50 bytes in malware files to deceive Magika's file-type identification model. They developed and tested a defense approach, collaborating with engineers for production deployment.

Result: Adversarial attacks achieved 90\% evasion success with 13-byte changes. The proposed defense reduced attack success to 20\%, requiring 50 bytes. The defense was implemented in Gmail's production system.

Conclusion: The study emphasizes vulnerabilities in ML-dependent security pipelines and validates that adversarially robust defenses, when implemented collaboratively, can mitigate real-world adversarial threats effectively.

Abstract: As deep learning models become widely deployed as components within larger
production systems, their individual shortcomings can create system-level
vulnerabilities with real-world impact. This paper studies how adversarial
attacks targeting an ML component can degrade or bypass an entire
production-grade malware detection system, performing a case study analysis of
Gmail's pipeline where file-type identification relies on a ML model.
  The malware detection pipeline in use by Gmail contains a machine learning
model that routes each potential malware sample to a specialized malware
classifier to improve accuracy and performance. This model, called Magika, has
been open sourced. By designing adversarial examples that fool Magika, we can
cause the production malware service to incorrectly route malware to an
unsuitable malware detector thereby increasing our chance of evading detection.
Specifically, by changing just 13 bytes of a malware sample, we can
successfully evade Magika in 90% of cases and thereby allow us to send malware
files over Gmail. We then turn our attention to defenses, and develop an
approach to mitigate the severity of these types of attacks. For our defended
production model, a highly resourced adversary requires 50 bytes to achieve
just a 20% attack success rate. We implement this defense, and, thanks to a
collaboration with Google engineers, it has already been deployed in production
for the Gmail classifier.

</details>


### [11] [Towards Imperceptible Adversarial Defense: A Gradient-Driven Shield against Facial Manipulations](https://arxiv.org/abs/2510.01699)
*Yue Li,Linying Xue,Dongdong Lin,Qiushi Li,Hui Tian,Hongxia Wang*

Main category: cs.CR

TL;DR: This paper proposes GRASP, a gradient-projection-based proactive defense method for facial deepfakes, achieving high imperceptibility and defense effectiveness by resolving gradient conflicts between losses. It attains PSNR>40 dB, SSIM=0.99, and 100 defense success rate, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Current deepfake defense methods face a tradeoff between perturbation imperceptibility and defense effectiveness. Existing approaches using visual loss constraints neglect gradient conflicts among losses, weakening their performance.

Method: The paper introduces GRASP, which combines structural similarity loss and low-frequency loss to enhance imperceptibility. It uses a novel gradient-projection mechanism to analyze and mitigate conflicts between defense effectiveness loss and visual quality losses, enabling balanced optimization.

Result: GRASP achieves a PSNR exceeding 40 dB, SSIM of 0.99, and a 100 defense success rate against facial attribute manipulations, demonstrating significant improvements in visual quality compared to existing methods.

Conclusion: GRASP is the first approach to effectively integrate structural similarity loss and low-frequency loss while resolving gradient conflicts. It achieves both high perceptual fidelity and strong defense performance, establishing a new benchmark for facial deepfake defense.

Abstract: With the flourishing prosperity of generative models, manipulated facial
images have become increasingly accessible, raising concerns regarding privacy
infringement and societal trust. In response, proactive defense strategies
embed adversarial perturbations into facial images to counter deepfake
manipulation. However, existing methods often face a tradeoff between
imperceptibility and defense effectiveness-strong perturbations may disrupt
forgeries but degrade visual fidelity. Recent studies have attempted to address
this issue by introducing additional visual loss constraints, yet often
overlook the underlying gradient conflicts among losses, ultimately weakening
defense performance. To bridge the gap, we propose a gradient-projection-based
adversarial proactive defense (GRASP) method that effectively counters facial
deepfakes while minimizing perceptual degradation. GRASP is the first approach
to successfully integrate both structural similarity loss and low-frequency
loss to enhance perturbation imperceptibility. By analyzing gradient conflicts
between defense effectiveness loss and visual quality losses, GRASP pioneers
the design of the gradient-projection mechanism to mitigate these conflicts,
enabling balanced optimization that preserves image fidelity without
sacrificing defensive performance. Extensive experiments validate the efficacy
of GRASP, achieving a PSNR exceeding 40 dB, SSIM of 0.99, and a 100% defense
success rate against facial attribute manipulations, significantly
outperforming existing approaches in visual quality.

</details>


### [12] [Constructions of Efficiently Implementable Boolean Functions with Provable Nonlinearity/Resiliency/Algebraic Immunity Trade-Offs](https://arxiv.org/abs/2510.01720)
*Palash Sarkar*

Main category: cs.CR

TL;DR: The paper presents multiple families of Boolean functions that offer efficient implementations with proven balance among resiliency, nonlinearity, and algebraic immunity.


<details>
  <summary>Details</summary>
Motivation: There is a need for Boolean functions that optimally balance trade-offs in cryptographic properties such as resiliency, nonlinearity, and algebraic immunity for secure and efficient implementations.

Method: The authors propose families of Boolean functions with specific parameters to demonstrate how these trade-offs can be achieved. By setting integers m0 ≥ 0, x0 ≥ 1, and a0 ≥ 1, they construct an n-variable function with desired properties: resiliency ≥ m0, linear bias ≤ 2^{-x0}, and algebraic immunity ≥ a0. The construction ensures n is linear in m0, x0, and a0 while maintaining implementation efficiency with O(n) gates.

Result: The proposed function families achieve provable trade-offs between the three important cryptographic characteristics, and allow for efficient implementation with O(n) gates.

Conclusion: The construction of n-variable Boolean functions with specified trade-offs in resiliency, nonlinearity, and algebraic immunity demonstrates the feasibility of achieving optimal performance in implementations for cryptographic applications.

Abstract: We describe several families of efficiently implementable Boolean functions
achieving provable trade-offs between resiliency, nonlinearity, and algebraic
immunity. In concrete terms, the following result holds for each of the
function families that we propose. Given integers $m_0\geq 0$, $x_0\geq 1$, and
$a_0\geq 1$, it is possible to construct an $n$-variable function which has
resiliency at least $m_0$, linear bias (which is an equivalent method of
expressing nonlinearity) at most $2^{-x_0}$ and algebraic immunity at least
$a_0$; further, $n$ is linear in $m_0$, $x_0$ and $a_0$, and the function can
be implemented using $O(n)$ gates.

</details>


### [13] [Secure Multi-Modal Data Fusion in Federated Digital Health Systems via MCP](https://arxiv.org/abs/2510.01780)
*Aueaphum Aueawatthanaphisut*

Main category: cs.CR

TL;DR: This paper proposes an MCP-based federated learning framework that unifies multi-modal data fusion, differential privacy, and energy efficiency, achieving significant accuracy improvements and client retention in distributed healthcare systems while complying with privacy regulations.


<details>
  <summary>Details</summary>
Motivation: Current federated learning systems lack standardized mechanisms for secure, interoperable multi-modal data fusion across distributed healthcare environments, limiting their ability to address privacy, resource constraints, and regulatory compliance in heterogeneous settings.

Method: The framework integrates three key components: (i) multi-modal feature alignment for heterogeneous data types, (ii) secure aggregation with differential privacy for sensitive data protection, and (iii) energy-aware scheduling to reduce mobile client dropouts, all orchestrated through the Model Context Protocol (MCP).

Result: Experiments show 9.8% higher diagnostic accuracy than baseline FL, 54% lower client dropout rates, and privacy-utility trade-offs meeting clinical requirements, validating the framework's effectiveness in real-world scenarios.

Conclusion: The proposed framework demonstrates that MCP-enabled multi-modal fusion offers a scalable and trustworthy solution for secure, interoperable federated healthcare systems, achieving significant improvements in diagnostic accuracy, client retention, and privacy compliance.

Abstract: Secure and interoperable integration of heterogeneous medical data remains a
grand challenge in digital health. Current federated learning (FL) frameworks
offer privacy-preserving model training but lack standardized mechanisms to
orchestrate multi-modal data fusion across distributed and resource-constrained
environments. This study introduces a novel framework that leverages the Model
Context Protocol (MCP) as an interoperability layer for secure, cross-agent
communication in multi-modal federated healthcare systems. The proposed
architecture unifies three pillars: (i) multi-modal feature alignment for
clinical imaging, electronic medical records, and wearable IoT data; (ii)
secure aggregation with differential privacy to protect patient-sensitive
updates; and (iii) energy-aware scheduling to mitigate dropouts in mobile
clients. By employing MCP as a schema-driven interface, the framework enables
adaptive orchestration of AI agents and toolchains while ensuring compliance
with privacy regulations. Experimental evaluation on benchmark datasets and
pilot clinical cohorts demonstrates up to 9.8\% improvement in diagnostic
accuracy compared with baseline FL, a 54\% reduction in client dropout rates,
and clinically acceptable privacy--utility trade-offs. These results highlight
MCP-enabled multi-modal fusion as a scalable and trustworthy pathway toward
equitable, next-generation federated health infrastructures.

</details>


### [14] [ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs](https://arxiv.org/abs/2510.01967)
*Aadarsh Anantha Ramakrishnan,Shubham Agarwal,Selvanayagam S,Kunwar Singh*

Main category: cs.CR

TL;DR: ZK-WAGON uses ZK-SNARKs to watermark image-generation models securely and without exposing sensitive info, with LSB steganography for imperceptible proof embedding.


<details>
  <summary>Details</summary>
Motivation: Image-generation models need secure watermarking to address authenticity, ownership, and misuse risks due to synthetic media, but traditional methods are inadequate for quality, removal resistance, and scalability.

Method: ZK-WAGON introduces Selective Layer ZK-Circuit Creation (SL-ZKCC) to transform key model layers into circuits for efficient ZK-SNARK proof generation, and LSB steganography embeds proofs imperceptibly in images.

Result: Successfully demonstrated ZK-WAGON on GAN and Diffusion models, offering a secure, model-agnostic solution for watermarking synthetic images with embedded ZK-SNARK proofs.

Conclusion: ZK-WAGON represents a significant advancement in watermarking image-generation models securely using ZK-SNARKs, combining scalability and robustness with imperceptible watermarking through LSB steganography.

Abstract: As image generation models grow increasingly powerful and accessible,
concerns around authenticity, ownership, and misuse of synthetic media have
become critical. The ability to generate lifelike images indistinguishable from
real ones introduces risks such as misinformation, deepfakes, and intellectual
property violations. Traditional watermarking methods either degrade image
quality, are easily removed, or require access to confidential model internals
- making them unsuitable for secure and scalable deployment. We are the first
to introduce ZK-WAGON, a novel system for watermarking image generation models
using the Zero-Knowledge Succinct Non Interactive Argument of Knowledge
(ZK-SNARKs). Our approach enables verifiable proof of origin without exposing
model weights, generation prompts, or any sensitive internal information. We
propose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively
convert key layers of an image generation model into a circuit, reducing proof
generation time significantly. Generated ZK-SNARK proofs are imperceptibly
embedded into a generated image via Least Significant Bit (LSB) steganography.
We demonstrate this system on both GAN and Diffusion models, providing a
secure, model-agnostic pipeline for trustworthy AI image generation.

</details>


### [15] [Mirage Fools the Ear, Mute Hides the Truth: Precise Targeted Adversarial Attacks on Polyphonic Sound Event Detection Systems](https://arxiv.org/abs/2510.02158)
*Junjie Su,Weifei Jin,Yuxin Cao,Derui Wang,Kai Ye,Jie Hao*

Main category: cs.CR

TL;DR: This paper proposes M2A, a targeted adversarial attack framework for polyphonic SED systems that addresses precision issues via preservation loss and introduces a novel evaluation metric (Editing Precision).


<details>
  <summary>Details</summary>
Motivation: Existing attacks on SED systems either fail due to contextual dependencies or lack precision by distorting non-target regions. Robustness against adversarial attacks is critical for safety-critical SED applications.

Method: The Mirage and Mute Attack (M2A) framework uses a preservation loss to maintain non-target region outputs during optimization and introduces Editing Precision (EP) as a metric to balance attack effectiveness and precision.

Result: M2A achieves 94.56-99.11 EP on two state-of-the-art SED models, demonstrating superior precision over existing methods while maintaining high effectiveness.

Conclusion: M2A provides a framework for precise, effective adversarial attacks on SED systems through novel constraint design and evaluation metrics, revealing vulnerabilities in polyphonic SED robustness.

Abstract: Sound Event Detection (SED) systems are increasingly deployed in
safety-critical applications such as industrial monitoring and audio
surveillance. However, their robustness against adversarial attacks has not
been well explored. Existing audio adversarial attacks targeting SED systems,
which incorporate both detection and localization capabilities, often lack
effectiveness due to SED's strong contextual dependencies or lack precision by
focusing solely on misclassifying the target region as the target event,
inadvertently affecting non-target regions. To address these challenges, we
propose the Mirage and Mute Attack (M2A) framework, which is designed for
targeted adversarial attacks on polyphonic SED systems. In our optimization
process, we impose specific constraints on the non-target output, which we
refer to as preservation loss, ensuring that our attack does not alter the
model outputs for non-target region, thus achieving precise attacks.
Furthermore, we introduce a novel evaluation metric Editing Precison (EP) that
balances effectiveness and precision, enabling our method to simultaneously
enhance both. Comprehensive experiments show that M2A achieves 94.56% and
99.11% EP on two state-of-the-art SED models, demonstrating that the framework
is sufficiently effective while significantly enhancing attack precision.

</details>


### [16] [NoMod: A Non-modular Attack on Module Learning With Errors](https://arxiv.org/abs/2510.02162)
*Cristian Bassotto,Ermes Franch,Marina Krček,Stjepan Picek*

Main category: cs.CR

TL;DR: NoMod ML-Attack bypasses modular arithmetic in Module-LWE schemes via robust linear estimation, achieving secret recovery in post-quantum cryptographic parameters including CRYSTALS-Kyber.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the threat quantum computing poses to classical public-key cryptography, necessitating the evaluation of post-quantum schemes like Module-LWE for security.

Method: The method employs a hybrid white-box approach combining lattice preprocessing techniques (e.g., reduced-vector saving, algebraic amplification) with robust statistical estimation via Tukey's Biweight loss, treating modular arithmetic challenges as statistical corruption rather than explicit modular modeling.

Result: NoMod achieves full binary secret recovery (n=350), sparse binomial recovery (n=256), and breaks CRYSTALS-Kyber instances with (n,k)=(128,3)/(256,2), demonstrating practical feasibility against current post-quantum standards.

Conclusion: The paper concludes that the NoMod ML-Attack is effective against Module-LWE based post-quantum cryptographic schemes, highlighting the necessity of robust defenses against such statistical cryptanalytic methods in the post-quantum era.

Abstract: The advent of quantum computing threatens classical public-key cryptography,
motivating NIST's adoption of post-quantum schemes such as those based on the
Module Learning With Errors (Module-LWE) problem. We present NoMod ML-Attack, a
hybrid white-box cryptanalytic method that circumvents the challenge of
modeling modular reduction by treating wrap-arounds as statistical corruption
and casting secret recovery as robust linear estimation. Our approach combines
optimized lattice preprocessing--including reduced-vector saving and algebraic
amplification--with robust estimators trained via Tukey's Biweight loss.
Experiments show NoMod achieves full recovery of binary secrets for dimension
$n = 350$, recovery of sparse binomial secrets for $n = 256$, and successful
recovery of sparse secrets in CRYSTALS-Kyber settings with parameters $(n, k) =
(128, 3)$ and $(256, 2)$. We release our implementation in an anonymous
repository https://anonymous.4open.science/r/NoMod-3BD4.

</details>


### [17] [Testing Stability and Robustness in Three Cryptographic Chaotic Systems](https://arxiv.org/abs/2510.02184)
*N. A. Anagnostopoulos,K. Konstantinidis,A. N. Miliou,S. G. Stavrinides*

Main category: cs.CR

TL;DR: This paper evaluates three cryptographic chaotic systems for synchronization stability under noise, finding one system outperforms the others for secure communication. Practical security requires optimizing these systems further.


<details>
  <summary>Details</summary>
Motivation: Chaotic systems are critical for secure communication, but their synchronization under real-world noise conditions remains a challenge. This study addresses the gap in understanding how different cryptographic chaotic systems maintain synchronization reliability, which directly impacts their practical security effectiveness.

Method: The authors evaluate the stability and robustness of three established cryptographic chaotic systems through comparative analysis under noisy conditions. They likely employed simulation experiments and quantitative metrics (e.g., synchronization error, noise tolerance thresholds) to assess performance differences.

Result: Results show significant variation in performance among the three systems, with one system maintaining stable synchronization at higher noise levels. The comparison reveals trade-offs between complexity, speed, and robustness, guiding recommendations for secure system design.

Conclusion: The study concludes that among the three cryptographic chaotic systems tested, one system demonstrates superior stability and robustness against noise, making it more suitable for secure communication applications. However, all systems require further optimization to achieve the desired security levels in practical scenarios.

Abstract: In practical applications, it is crucial that the drive-response systems,
although identical in all respects, are synchronized at all times, even if
there is noise present. In this work, we test the stability and robustness of
three distinct and well-known cryptographic chaotic systems, and compare the
results in relation to the desired security.

</details>


### [18] [Authentication Security of PRF GNSS Ranging](https://arxiv.org/abs/2510.02196)
*Jason Anderson*

Main category: cs.CR

TL;DR: This paper analyzes spoofing-resistant PRF-based GNSS ranging security, focusing on Galileo's SAS with E6-C signal. It calculates 400ms data requirement for 128-bit authentication security under non-SCER models and evaluates SCER adversary capabilities.


<details>
  <summary>Details</summary>
Motivation: GNSS spoofing threatens positioning accuracy. PRF-based authentication using secret keys can secure pseudorange data when ranging codes resist prediction before broadcast.

Method: Security analysis of PRF GNSS ranging under multiple spoofing models, including SCER. Applied to Galileo SAS with E6-C signal through probability of missed detection calculations and equipment requirement modeling.

Result: 400ms of Galileo E6-C data provides 128-bit authentication security against non-SCER spoofers. SCER attackers require specific receiving equipment to compromise security, enabling protocol design to meet security requirements.

Conclusion: PRF-based authentication achieves robust GNSS security by bounding data requirements for 128-bit security under different spoofing scenarios, providing practical guidance for authentication protocol design against targeted adversaries.

Abstract: This work derives the authentication security of pseudorandom function (PRF)
GNSS ranging under multiple GNSS spoofing models, including the Security Code
Estimation and Replay (SCER) spoofer. When GNSS ranging codes derive from a PRF
utilizing a secret known only to the broadcaster, the spoofer cannot predict
the ranging code before broadcast. Therefore, PRF ranging can be used to
establish trust in the GNSS pseudoranges and the resulting receiver position,
navigation, and timing (PNT) solution. I apply the methods herein to Galileo's
Signal Authentication Service (SAS) utilizing the encrypted Galileo E6-C signal
to compute that, at most, 400 ms of Galileo E6-C data to assert 128-bit
authentication security under non-SCER models. For the SCER adversary, I
predict the adversary's needed receiving radio equipment to break
authentication security. One can use this work to design a PRF GNSS ranging
protocol to meet useful authentication security requirements by computing the
probability of missed detection.

</details>


### [19] [An efficient quantum algorithm for computing $S$-units and its applications](https://arxiv.org/abs/2510.02280)
*Jean-Francois Biasse,Fang Song*

Main category: cs.CR

TL;DR: The paper provides proofs for a quantum polynomial time algorithm to compute S-unit groups of number fields, enabling efficient solutions for related number-theoretic problems like class groups, principal ideal problems, and norm equations.


<details>
  <summary>Details</summary>
Motivation: Efficiently solving S-unit groups and related problems is critical for advances in algebraic number theory and cryptography. Prior methods lacked polynomial-time guarantees, necessitating quantum approaches for scalability.

Method: The method builds on Biasse and Song’s quantum algorithm for S-unit groups, combining it with techniques from Cramer et al. (2016, 2017). It solves the principal ideal problem and leverages ideal class decomposition to address multiple related problems.

Result: The algorithm achieves polynomial time complexity for computing class groups, S-class groups, unit groups, ray class groups, and solving principal ideals/norm equations. It also enables finding short generators in principal ideals and mild vectors in ideal lattices.

Conclusion: The work establishes a robust quantum framework for solving core problems in number theory, advancing cryptographic applications and theoretical understanding of algebraic structures with exponential speedups.

Abstract: In this paper, we provide details on the proofs of the quantum polynomial
time algorithm of Biasse and Song (SODA 16) for computing the $S$-unit group of
a number field. This algorithm directly implies polynomial time methods to
calculate class groups, S-class groups, relative class group and the unit
group, ray class groups, solve the principal ideal problem, solve certain norm
equations, and decompose ideal classes in the ideal class group. Additionally,
combined with a result of Cramer, Ducas, Peikert and Regev (Eurocrypt 2016),
the resolution of the principal ideal problem allows one to find short
generators of a principal ideal. Likewise, methods due to Cramer, Ducas and
Wesolowski (Eurocrypt 2017) use the resolution of the principal ideal problem
and the decomposition of ideal classes to find so-called ``mildly short
vectors'' in ideal lattices of cyclotomic fields.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [20] [Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration](https://arxiv.org/abs/2510.01379)
*Huashan Chen,Zhenyu Qi,Haotang Li,Hong Chen,Jinfu Chen,Kebin Peng,In Kee Kim,Kyu Hyung Lee,Sen He*

Main category: cs.SE

TL;DR: This paper introduces PerfOrch, a multi-model LLM orchestration framework for automated code generation that dynamically selects optimal models based on empirical performance metrics across languages and stages, achieving higher correctness (up to 96.22% on HumanEval-X) and execution speed improvements (up to 27.66% median speedup).


<details>
  <summary>Details</summary>
Motivation: Current single-model approaches for code generation ignore inherent model heterogeneity in language capabilities, domain expertise, and development-stage appropriateness, leading to suboptimal code quality and performance.

Method: Conducted empirical evaluation of 17 LLMs across 5 programming languages using HumanEval-X and EffiBench-X benchmarks (assessing functional correctness and runtime metrics). Developed PerfOrch framework with stage-specific model routing, generate-fix-refine workflow, and validation/rollback mechanisms without requiring model adaptation.

Result: perfOrch outperforms single-model baselines (including GPT-4o) with 96.22% correctness on HumanEval-X (vs 78.66%) and 91.37 vs 49.11 on EffiBench-X; achieves 58.76% problem coverage with 17.67-27.66% median execution time improvements across languages.

Conclusion: PerfOrch demonstrates that model orchestration frameworks can systematically leverage model heterogeneity for production-grade code generation, delivering combined correctness/performance gains through empirical routing and modular, extensible architecture.

Abstract: While Large Language Models (LLMs) have become the predominant paradigm for
automated code generation, current single-model approaches fundamentally ignore
the heterogeneous computational strengths that different models exhibit across
programming languages, algorithmic domains, and development stages. This paper
challenges the single-model convention by introducing a multi-stage,
performance-guided orchestration framework that dynamically routes coding tasks
to the most suitable LLMs within a structured generate-fix-refine workflow. Our
approach is grounded in a comprehensive empirical study of 17 state-of-the-art
LLMs across five programming languages (Python, Java, C++, Go, and Rust) using
HumanEval-X benchmark. The study, which evaluates both functional correctness
and runtime performance metrics (execution time, mean/max memory utilization,
and CPU efficiency), reveals pronounced performance heterogeneity by language,
development stage, and problem category. Guided by these empirical insights, we
present PerfOrch, an LLM agent that orchestrates top-performing LLMs for each
task context through stage-wise validation and rollback mechanisms. Without
requiring model fine-tuning, PerfOrch achieves substantial improvements over
strong single-model baselines: average correctness rates of 96.22% and 91.37%
on HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and
49.11%. Beyond correctness gains, the framework delivers consistent performance
optimizations, improving execution time for 58.76% of problems with median
speedups ranging from 17.67% to 27.66% across languages on two benchmarks. The
framework's plug-and-play architecture ensures practical scalability, allowing
new LLMs to be profiled and integrated seamlessly, thereby offering a paradigm
for production-grade automated software engineering that adapts to the rapidly
evolving generative AI landscape.

</details>


### [21] [Deciphering WONTFIX: A Mixed-Method Study on Why GitHub Issues Get Rejected](https://arxiv.org/abs/2510.01514)
*J. Alexander Curtis,Sharadha Kasiviswanathan,Nasir Eisty*

Main category: cs.SE

TL;DR: This study analyzes the 'wontfix' label usage in 3,132 GitHub repositories, revealing its 30%


<details>
  <summary>Details</summary>
Motivation: The 'wontfix' label is a common but poorly understood GitHub tool; this research aims to clarify its role in open-source project management and community dynamics.

Method: Mixed-method approach combining quantitative analysis of 3,132 repositories with qualitative thematic analysis via open coding to categorize labeling reasons.

Result: 30%

Conclusion: The 'wontfix' label is essential for resource management but may hinder community engagement; understanding its usage patterns can optimize project governance and collaboration.

Abstract: Context: The ``wontfix'' label is a widely used yet narrowly understood tool
in GitHub repositories, indicating that an issue will not be pursued further.
Despite its prevalence, the impact of this label on project management and
community dynamics within open-source software development is not clearly
defined. Objective: This study examines the prevalence and reasons behind
issues being labeled as wontfix across various open-source repositories on
GitHub. Method: Employing a mixed-method approach, we analyze both quantitative
data to assess the prevalence of the wontfix label and qualitative data to
explore the reasoning that it was used. Data were collected from 3,132 of
GitHub's most-popular repositories. Later, we employ open coding and thematic
analysis to categorize the reasons behind wontfix labels, providing a
structured understanding of the issue management landscape. Results: Our
findings show that about 30% of projects on GitHub apply the wontfix label to
some issues. These issues most often occur on user-submitted issues for bug
reports and feature requests. The study identified eight common themes behind
labeling issues as wontfix, ranging from user-specific control factors to
maintainer-specific decisions. Conclusions: The wontfix label is a critical
tool for managing resources and guiding contributor efforts in GitHub projects.
However, it can also discourage community involvement and obscure the
transparency of project management. Understanding these reasons aids project
managers in making informed decisions and fostering efficient collaboration
within open-source communities.

</details>


### [22] [MIMIC: Integrating Diverse Personality Traits for Better Game Testing Using Large Language Model](https://arxiv.org/abs/2510.01635)
*Yifei Chen,Sarra Habchi,Lili Wei*

Main category: cs.SE

TL;DR: The paper introduces MIMIC, a framework addressing the limitations of traditional automated game testing by integrating diverse human-like playstyles into gaming agents, achieving higher test coverage and performance in Minecraft.


<details>
  <summary>Details</summary>
Motivation: Traditional agents lack diversity in strategies, leading to repetitive interactions and missed edge cases. Human players use varied approaches, which existing methods fail to emulate.

Method: MIMIC incorporates personality traits into agents, enabling them to adopt different playstyles for similar scenarios. This mimics human diversity, driving varied in-game interactions.

Result: MIMIC achieves higher task completion rates and more diverse solutions in Minecraft, surpassing state-of-the-art agents while generating richer test coverage.

Conclusion: MIMIC demonstrates significant potential for effective game testing by leveraging personality-driven strategies, addressing critical gaps in automated testing for complex video games.

Abstract: Modern video games pose significant challenges for traditional automated
testing algorithms, yet intensive testing is crucial to ensure game quality. To
address these challenges, researchers designed gaming agents using
Reinforcement Learning, Imitation Learning, or Large Language Models. However,
these agents often neglect the diverse strategies employed by human players due
to their different personalities, resulting in repetitive solutions in similar
situations. Without mimicking varied gaming strategies, these agents struggle
to trigger diverse in-game interactions or uncover edge cases.
  In this paper, we present MIMIC, a novel framework that integrates diverse
personality traits into gaming agents, enabling them to adopt different gaming
strategies for similar situations. By mimicking different playstyles, MIMIC can
achieve higher test coverage and richer in-game interactions across different
games. It also outperforms state-of-the-art agents in Minecraft by achieving a
higher task completion rate and providing more diverse solutions. These results
highlight MIMIC's significant potential for effective game testing.

</details>


### [23] [FOSS-chain: using blockchain for Open Source Software license compliance](https://arxiv.org/abs/2510.01740)
*Kypros Iacovou,Georgia M. Kapitsaki,Evangelia Vanezi*

Main category: cs.SE

TL;DR: The paper proposes FOSS-chain, a blockchain-based platform to automate Open Source Software (OSS license compliance and address compatibility issues through immutable record-keeping, validated via a small-scale user study.


<details>
  <summary>Details</summary>
Motivation: OSS license incompatibilities increase legal risks during derivative work creation, requiring complex compliance processes and transparency in tracking modifications and distributions.

Method: FOSS-chain integrates blockchain to record software changes and license data, automating compliance checks for 14 OSS licenses through a web platform prototype evaluated via user testing.

Result: Preliminary evaluation shows FOSS-chain's potential to simplify license compliance, with promising results indicating adaptability to real-world software systems despite the current prototype's limited scale.

Conclusion: Blockchain-based solutions like FOSS-chain demonstrate viability for enhancing OSS license management by ensuring transparency and automating compliance verification, reducing legal risks.

Abstract: Open Source Software (OSS) is widely used and carries licenses that indicate
the terms under which the software is provided for use, also specifying
modification and distribution rules. Ensuring that users are respecting OSS
license terms when creating derivative works is a complex process. Compliance
issues arising from incompatibilities among licenses may lead to legal
disputes. At the same time, the blockchain technology with immutable entries
offers a mechanism to provide transparency when it comes to licensing and
ensure software changes are recorded. In this work, we are introducing an
integration of blockchain and license management when creating derivative
works, in order to tackle the issue of OSS license compatibility. We have
designed, implemented and performed a preliminary evaluation of FOSS-chain, a
web platform that uses blockchain and automates the license compliance process,
covering 14 OSS licenses. We have evaluated the initial prototype version of
the FOSS-chain platform via a small scale user study. Our preliminary results
are promising, demonstrating the potential of the platform for adaptation on
realistic software systems.

</details>


### [24] [ARENA: A tool for measuring and analysing the energy efficiency of Android apps](https://arxiv.org/abs/2510.01754)
*Hina Anwar*

Main category: cs.SE

TL;DR: The paper introduces ARENA, an open-source IDE plugin for Android developers to simplify hardware-based energy consumption measurement and analysis of apps through integrated data collection and visualization.


<details>
  <summary>Details</summary>
Motivation: Hardware-based energy measurement for Android apps is fragmented, time-consuming, and lacks accessible tools, creating barriers for developers/researchers in optimizing energy efficiency.

Method: CRENNA provides an IntelliJ/Android Studio plugin that automates hardware device integration, test scenario execution, data aggregation (with noise reduction), and energy analysis (metrics calculation, statistical analysis, and visualization).

Result: ARENA enables developers to (1) perform reliable hardware-based energy comparisons between apps/versions and (2) analyze measurement data via statistical reports and visualizations without leaving their IDE.

Conclusion: ARENA bridges the gap between accurate hardware-based energy measurement and developer workflow efficiency, enabling systematic energy optimization of Android applications.

Abstract: To build energy-efficient apps, there is a need to estimate and analyze their
energy consumption in typical usage scenarios. The energy consumption of
Android apps could be estimated via software-based and hardware-based
approaches. Software-based approaches, while easier to implement, are not as
accurate as hardware-based approaches. The process of measuring the energy
consumption of an Android app via a hardware-based approach typically involves
1) setting up a measurement environment, 2) executing the app under test on a
mobile device, 3) recording current/voltage data via a hardware device to
measure energy consumption, and 4) cleaning and aggregating data for analyses,
reports, and visualizations. Specialized scripts are written for selected
hardware and software components to ensure reliable energy measurements. The
energy measurement process is repeated many times and aggregated to remove
noise. These steps make the hardware-based energy measurement process
time-consuming and not easy to adapt or reproduce. There is a lack of
open-source tools available for developers and researchers to take reliable
energy measurements via hardware devices. In this paper, we present and
demonstrate ARENA, a support tool that enables developers and researchers to
connect to a physical measurement device without leaving the comfort of their
IDE. Developers could use ARENA during development to compare energy
consumption between different apps or versions of the same app. ARENA
calculates energy consumption on an Android smartphone by executing a test
scenario on the app under development. Further, ARENA helps aggregate,
statistically analyze, report, and visualize the data, allowing developers and
researchers to dig into the data directly or visually. We implemented ARENA as
an IntelliJ and Android Studio plugin.

</details>


### [25] [Towards Speeding up Program Repair with Non-Autoregressive Model](https://arxiv.org/abs/2510.01825)
*Zhenyu Yang,Yue Pan,Zhen Yang,Zhongxing Yu*

Main category: cs.SE

TL;DR: The paper introduces NARRepair, the first non-autoregressive model for APR, achieving state-of-the-art speed and accuracy by addressing parallel code generation issues.


<details>
  <summary>Details</summary>
Motivation: Traditional AR-based APR techniques suffer from significant time delays, especially with large models. The authors aim to improve speed by adopting a parallel NAR approach for code generation.

Method: NARRepair is a non-autoregressive code generation model incorporating three novelties: a repair action predictor, an inter-token dependency extractor, and a two-stage decoder.

Result: On three APR datasets, NARRepair outperforms AR-based methods in accuracy under time constraints and increases repair speed by 1.4-6.4x in GPU environments.

Conclusion: NARRepair successfully adapts the NAR approach to APR, offering a speed-accuracy trade-off. It is the first NAR-based model to achieve SOTA performance in APR.

Abstract: Enlightened by the success of machine learning techniques in various
application areas, recent years have witnessed a surge of research efforts on
automatic program repair (APR) using machine learning techniques. Previous
machine learning-based APR techniques essentially modified bugs in the
autoregressive (AR) manner, which predicts future values based on past values.
Due to the manner of token-by-token generation, the AR-based APR technique has
a huge time delay. In particular, the delay of the APR model with a large
number of parameters is more serious. To address the issue, we aim to apply the
non-autoregressive (NAR) method to the APR task, which can output target code
in a parallel manner to avoid huge repair delays. However, the naive use of the
NAR manner for the APR task suffers from the issue of compromised patch
quality. To effectively adapt the NAR manner for the APR task, we in this paper
propose NARRepair, the first customized NAR code generation model for the APR
task. The NARRepair model features three major novelties, including 1) the
repair action predictor for alleviating the over-correction issue, 2) the
inter-token dependency extractor for alleviating the issue of lacking
inter-token dependency information, and 3) the two-stage decoder for
alleviating the issue of lacking contextual information. We evaluated NARRepair
on three widely used datasets in the APR community, and the results show that
1) compared to other APR techniques, the NARRepair model has the best
performance within the limited repair time, and 2) compared to AR-based APR
techniques, the repair speed of NARRepair has been increased by 1.4-6.4 times
in the GPU environment. Overall, the results show that NARRepair has achieved
state-of-the-art comprehensive performance in terms of repair speed and
accuracy.

</details>


### [26] [RefFilter: Improving Semantic Conflict Detection via Refactoring-Aware Static Analysis](https://arxiv.org/abs/2510.01960)
*Victor Lira,Paulo Borba,Rodrigo Bonifácio,Galileu Santos e Matheus barbosa*

Main category: cs.SE

TL;DR: RefFilter tackles false positives in semantic interference detection by identifying behavior-preserving refactoring, reducing false positives by 32% in evaluations while maintaining detection coverage.


<details>
  <summary>Details</summary>
Motivation: Existing static analysis methods for semantic interference detection face high false positive rates due to inability to distinguish behavior-preserving refactorings from behavior-altering changes, disrupting collaborative workflows.

Method: RefFilter integrates automated refactoring detection into lightweight static analysis, filtering out false positives caused by refactorings, and leverages two datasets (labeled scenarios and a new 1,087-merge set) for validation.

Result: Achieves 32\% false positive reduction on the labeled dataset with minimal (non-significant bordering on Problem Descriptions) false negative increase, demonstrating improved precision outweighing minor recall trade-offs.

Conclusion: Refactoring-aware interference detection via RefFilter is a scalable, effective strategy for enhancing merge support in collaborative development without compromising coverage.

Abstract: Detecting semantic interference remains a challenge in collaborative software
development. Recent lightweight static analysis techniques improve efficiency
over SDG-based methods, but they still suffer from a high rate of false
positives. A key cause of these false positives is the presence of
behavior-preserving code refactorings, which current techniques cannot
effectively distinguish from changes that impact behavior and can interfere
with others. To handle this problem we present RefFilter, a refactoring-aware
tool for semantic interference detection. It builds on existing static
techniques by incorporating automated refactoring detection to improve
precision. RefFilter discards behavior-preserving refactorings from reports,
reducing false positives while preserving detection coverage. To evaluate
effectiveness and scalability, use two datasets: a labeled dataset with 99
scenarios and ground truth, and a novel dataset of 1,087 diverse merge
scenarios that we have built. Experimental results show that RefFilter reduces
false positives by nearly 32% on the labeled dataset. While this reduction
comes with a non significant increase in false negatives, the overall gain in
precision significantly outweighs the minor trade-off in recall. These findings
demonstrate that refactoring-aware interference detection is a practical and
effective strategy for improving merge support in modern development workflows.

</details>


### [27] [Clarifying Semantics of In-Context Examples for Unit Test Generation](https://arxiv.org/abs/2510.01994)
*Chen Yang,Lin Yang,Ziqi Wang,Dong Wang,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: CLAST improves unit test generation by refining tests for semantic clarity via program analysis and LLM rewriting, outperforming UTgen in test effectiveness and user preference.


<details>
  <summary>Details</summary>
Motivation: In-context examples for LLM-based unit test generation are often poorly structured, limiting effectiveness. Better test clarity is needed to enhance generated test quality.

Method: CLAST decomposes complex tests into logically clearer ones using program analysis and LLM-based rewriting to boost semantic clarity.

Result: CLAST retains original test effectiveness while outperforming UTgen by 12.90%-35.82%% in key metrics; 85.33%% user preference. Generated tests with CLAST examples improve CSR/PR/Cov by 25.97%-45.99%%.

Conclusion: CLAST demonstrates significant potential to enhance software testing practices through improved test clarity, with implications for future research in LLM-powered testing.

Abstract: Recent advances in large language models (LLMs) have enabled promising
performance in unit test generation through in-context learning (ICL). However,
the quality of in-context examples significantly influences the effectiveness
of generated tests-poorly structured or semantically unclear test examples
often lead to suboptimal outputs. In this paper, we propose CLAST, a novel
technique that systematically refines unit tests to improve their semantic
clarity, thereby enhancing their utility as in-context examples. The approach
decomposes complex tests into logically clearer ones and improves semantic
clarity through a combination of program analysis and LLM-based rewriting. We
evaluated CLAST on four open-source and three industrial projects. The results
demonstrate that CLAST largely outperforms UTgen, the state-of-the-art
refinement technique, in both preserving test effectiveness and enhancing
semantic clarity. Specifically, CLAST fully retains the original effectiveness
of unit tests, while UTgen reduces compilation success rate (CSR), pass rate
(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,
35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user
study preferred the semantic clarity of CLAST-refined tests. Notably,
incorporating CLAST-refined tests as examples effectively improves ICL-based
unit test generation approaches such as RAGGen and TELPA, resulting in an
average increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for
generated tests, compared to incorporating UTgen-refined tests. The insights
from the follow-up user study not only reinforce CLAST's potential impact in
software testing practice but also illuminate avenues for future research.

</details>


### [28] [Automatic Generation of Combinatorial Reoptimisation Problem Specifications: A Vision](https://arxiv.org/abs/2510.02002)
*Maximilian Kratz,Steffen Zschaler,Jens Kosiol,Gabriele Taentzer*

Main category: cs.SE

TL;DR: The paper explores using Model-Driven Engineering (MDE) for systematic reoptimisation in combinatorial problems by adapting solutions to changing contexts, demonstrating a proof-of-concept for resource allocation.


<details>
  <summary>Details</summary>
Motivation: Traditional optimisation methods fail to address reoptimisation constraints (minimal changes, fixed historical allocations, deriving change scripts) in dynamic contexts like scheduling and resource allocation.

Method: The authors leverage MDE with declarative modelling and model transformations to systematically derive reoptimisation specifications from original problem definitions, implemented in the GIPS tool.

Result: An initial categorization of reoptimisation problems and a proof-of-concept implementation applied to a teaching assistant resource-allocation case study.

Conclusion: MDE provides a structured approach to address reoptimisation challenges, enabling scalable and maintainable solutions for combinatorial problems.

Abstract: Once an optimisation problem has been solved, the solution may need
adaptation when contextual factors change. This challenge, also known as
reoptimisation, has been addressed in various problem domains, such as railway
crew rescheduling, nurse rerostering, or aircraft recovery. This requires a
modified problem to be solved again to ensure that the adapted solution is
optimal in the new context. However, the new optimisation problem differs
notably from the original problem: (i) we want to make only minimal changes to
the original solution to minimise the impact; (ii) we may be unable to change
some parts of the original solution (e.g., because they refer to past
allocations); and (iii) we need to derive a change script from the original
solution to the new solution. In this paper, we argue that Model-Driven
Engineering (MDE) - in particular, the use of declarative modelling languages
and model transformations for the high-level specification of optimisation
problems - offers new opportunities for the systematic derivation of
reoptimisation problems from the original optimisation problem specification.
We focus on combinatorial reoptimisation problems and provide an initial
categorisation of changing problems and strategies for deriving the
corresponding reoptimisation specifications. We introduce an initial
proof-of-concept implementation based on the GIPS (Graph-Based (Mixed) Integer
Linear Programming Problem Specification) tool and apply it to an example
resource-allocation problem: the allocation of teaching assistants to teaching
sessions.

</details>


### [29] [ACM SIGSOFT SEN Empirical Software Engineering: Introducing Our New Regular Column](https://arxiv.org/abs/2510.02007)
*Justus Bogner,Roberto Verdecchia*

Main category: cs.SE

TL;DR: This paper launches the SEN-ESE column to address empirical software engineering (ESE) research gaps through meta-discussions on replication practices, statistical methods, and industry transfer challenges, using community-driven feedback to drive field improvements.


<details>
  <summary>Details</summary>
Motivation: Despite ESE's maturity, challenges like reproducibility, external validity, subjective reviews, poor documentation, and difficulty transferring research to industry practices persist, necessitating focused meta-research to guide field evolution and improve accessibility for newcomers.

Method: The SEN-ESE column will leverage expert interviews, focus groups, surveys, and position pieces to analyze and improve meta-aspects of ESE research, such as replication packages, statistical methods, and interdisciplinary publishing challenges.

Result: The column establishes a structured venue for ESE meta-discussions, enabling systematic reflection on implicit research practices and community-driven prioritization of improvement initiatives.

Conclusion: The paper introduces the SEN-ESE column as a dedicated platform to address meta-aspects of ESE research, fostering discussions on underexplored topics, and invites ongoing community feedback to refine ESE practices and pedagogy.

Abstract: From its early foundations in the 1970s, empirical software engineering (ESE)
has evolved into a mature research discipline that embraces a plethora of
different topics, methodologies, and industrial practices. Despite its
remarkable progress, the ESE research field still needs to keep evolving, as
new impediments, shortcoming, and technologies emerge. Research
reproducibility, limited external validity, subjectivity of reviews, and
porting research results to industrial practices are just some examples of the
drivers for improvements to ESE research. Additionally, several facets of ESE
research are not documented very explicitly, which makes it difficult for
newcomers to pick them up. With this new regular ACM SIGSOFT SEN column
(SEN-ESE), we introduce a venue for discussing meta-aspects of ESE research,
ranging from general topics such as the nature and best practices for
replication packages, to more nuanced themes such as statistical methods,
interview transcription tools, and publishing interdisciplinary research. Our
aim for the column is to be a place where we can regularly spark conversations
on ESE topics that might not often be touched upon or are left implicit.
Contributions to this column will be grounded in expert interviews, focus
groups, surveys, and position pieces, with the goal of encouraging reflection
and improvement in how we conduct, communicate, teach, and ultimately improve
ESE research. Finally, we invite feedback from the ESE community on
challenging, controversial, or underexplored topics, as well as suggestions for
voices you would like to hear from. While we cannot promise to act on every
idea, we aim to shape this column around the community interests and are
grateful for all contributions.

</details>


### [30] [Towards fairer public transit: Real-time tensor-based multimodal fare evasion and fraud detection](https://arxiv.org/abs/2510.02165)
*Peter Wauyo,Dalia Bwiza,Alain Murara,Edwin Mugume,Eric Umuhoza*

Main category: cs.SE

TL;DR: A multimodal system using ViViT (video) and AST (audio) with Tensor Fusion achieves 84% recall for fraud detection, outperforming existing methods and enabling real-time monitoring in public transportation.


<details>
  <summary>Details</summary>
Motivation: Fraud and fare evasion in public transportation cause revenue loss and safety risks. Existing systems achieve 75% recall rates, but the field needs a more accurate and robust solution.

Method: The system combines video (ViViT model) and audio (AST model) features using a Tensor Fusion Network (TFN) architecture that models unimodal and bimodal interactions via a 2-fold Cartesian product. This captures cross-modal dynamics between visual behaviors and audio cues.

Result: The system achieved 89.5% accuracy, 87.2% precision, and 84.0% recall in detecting fraudulent activities. Abation studies showed 7.0% F1-score and 8.8% recall improvements over traditional concatenation methods.

Conclusion: The research successfully demonstrates that the Tensor Fusion Network (TFN) with multimodal analysis outperforms traditional methods in detecting fraud and fare evasion in public transportation. The approach achieves high accuracy and exceeds state-of-the-art systems, providing real-time capabilities to reduce revenue loss and improve safety.

Abstract: This research introduces a multimodal system designed to detect fraud and
fare evasion in public transportation by analyzing closed circuit television
(CCTV) and audio data. The proposed solution uses the Vision Transformer for
Video (ViViT) model for video feature extraction and the Audio Spectrogram
Transformer (AST) for audio analysis. The system implements a Tensor Fusion
Network (TFN) architecture that explicitly models unimodal and bimodal
interactions through a 2-fold Cartesian product. This advanced fusion technique
captures complex cross-modal dynamics between visual behaviors (e.g.,
tailgating,unauthorized access) and audio cues (e.g., fare transaction sounds).
The system was trained and tested on a custom dataset, achieving an accuracy of
89.5%, precision of 87.2%, and recall of 84.0% in detecting fraudulent
activities, significantly outperforming early fusion baselines and exceeding
the 75% recall rates typically reported in state-of-the-art transportation
fraud detection systems. Our ablation studies demonstrate that the tensor
fusion approach provides a 7.0% improvement in the F1 score and an 8.8% boost
in recall compared to traditional concatenation methods. The solution supports
real-time detection, enabling public transport operators to reduce revenue
loss, improve passenger safety, and ensure operational compliance.

</details>


### [31] [SIEVE: Towards Verifiable Certification for Code-datasets](https://arxiv.org/abs/2510.02166)
*Fatou Ndiaye Mbodji,El-hacen Diallo,Jordan Samhi,Kui Liu,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.SE

TL;DR: The paper proposes SIEVE, a framework for certifying code dataset quality via machine-readable 'Confidence Cards' with statistical bounds, addressing fragmented pipelines and unverifiable static dataset cards.


<details>
  <summary>Details</summary>
Motivation: Public code datasets lack auditable quality guarantees, relying on non-verifiable static cards and ad-hoc cleaning pipelines that fragment efforts and increase costs.

Method: SIEVE introduces per-property checks generating Confidence Cards—machine-readable certificates with anytime-valid statistical bounds, replacing narrative cards through community-driven verification.

Result: Expected outcomes include reduced quality-assurance costs and increased trust in code datasets via verifiable certifications.

Conclusion: SIEVE offers a scalable solution for dataset quality assurance through statistical certification, promoting transparency and community collaboration in empirical software engineering.

Abstract: Code agents and empirical software engineering rely on public code datasets,
yet these datasets lack verifiable quality guarantees. Static 'dataset cards'
inform, but they are neither auditable nor do they offer statistical
guarantees, making it difficult to attest to dataset quality. Teams build
isolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We
present SIEVE, a community-driven framework. It turns per-property checks into
Confidence Cards-machine-readable, verifiable certificates with anytime-valid
statistical bounds. We outline a research plan to bring SIEVE to maturity,
replacing narrative cards with anytime-verifiable certification. This shift is
expected to lower quality-assurance costs and increase trust in code-datasets.

</details>


### [32] [TAIBOM: Bringing Trustworthiness to AI-Enabled Systems](https://arxiv.org/abs/2510.02169)
*Vadim Safronov,Anthony McCaigue,Nicholas Allott,Andrew Martin*

Main category: cs.SE

TL;DR: The paper proposes TAIBOM, an AI-focused extension of SBOMs to address supply chain transparency gaps in dynamic, data-driven AI systems.


<details>
  <summary>Details</summary>
Motivation: Existing SBOM frameworks fail to capture AI systems' unique characteristics (dynamic nature, dataset/model/software dependencies, fragmented governance), creating risks for integrity, trust, and compliance in AI workflows.

Method: Introduces TAIBOM with (1). structured AI-specific dependency modeling, (2). integrity propagation mechanisms across heterogeneous pipelines, and (3). trust attestation processes for component verification, while extending SBOM principles.

Result: Demonstrates TAIBOM's ability to enhance assurance, security, and compliance for AI systems, outperforming existing standards like SPDX and CycloneDX through structured transparency and provenance verification.

Conclusion: Establishes foundational framework for trustworthy AI systems via tailored supply chain transparency, enabling actionable verification and compliance across the AI lifecycle.

Abstract: The growing integration of open-source software and AI-driven technologies
has introduced new layers of complexity into the software supply chain,
challenging existing methods for dependency management and system assurance.
While Software Bills of Materials (SBOMs) have become critical for enhancing
transparency and traceability, current frameworks fall short in capturing the
unique characteristics of AI systems -- namely, their dynamic, data-driven
nature and the loosely coupled dependencies across datasets, models, and
software components. These challenges are compounded by fragmented governance
structures and the lack of robust tools for ensuring integrity, trust, and
compliance in AI-enabled environments.
  In this paper, we introduce Trusted AI Bill of Materials (TAIBOM) -- a novel
framework extending SBOM principles to the AI domain. TAIBOM provides (i) a
structured dependency model tailored for AI components, (ii) mechanisms for
propagating integrity statements across heterogeneous AI pipelines, and (iii) a
trust attestation process for verifying component provenance. We demonstrate
how TAIBOM supports assurance, security, and compliance across AI workflows,
highlighting its advantages over existing standards such as SPDX and CycloneDX.
This work lays the foundation for trustworthy and verifiable AI systems through
structured software transparency.

</details>


### [33] [FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen Using Agentic AI](https://arxiv.org/abs/2510.02185)
*Paschal C. Amusuo,Dongge Liu,Ricardo Andres Calvo Mendez,Jonathan Metzman,Oliver Chang,James C. Davis*

Main category: cs.SE

TL;DR: The paper addresses the challenge of false positive crashes in fuzz driver generation by introducing two AI-driven strategies that reduce spurious crashes by 8% and reported crashes by over half, improving the reliability of fuzzing in industry-scale projects like OSS-Fuzz-Gen.


<details>
  <summary>Details</summary>
Motivation: Fuzz drivers enable automated fuzz testing by converting random inputs into valid function arguments, but their manual development is costly and error-prone. Existing automated methods often produce false positive crashes, undermining trust in large-scale fuzzing systems such as OSS-Fuzz-Gen.

Method: The authors propose two strategies: (1) Constraint-based fuzz driver generation, which enforces input and state constraints during driver creation, and (2) Context-based crash validation, which analyzes function callers to assess if reported crashes are feasible from actual program entry points.

Result: Evaluation on 1,500 OSS-Fuzz benchmark functions shows these strategies reduce spurious crashes by up to 8%, cut reported crashes by over 50%, and validate the effectiveness of state-of-the-art LLMs in automating fuzz driver generation.

Conclusion: The work demonstrates that AI, particularly constraint- and context-based approaches, can significantly enhance the accuracy of fuzz driver generation and crash validation, offering a path to more trustworthy automatic fuzzing systems while highlighting remaining challenges in AI integration.

Abstract: Fuzz testing has become a cornerstone technique for identifying software bugs
and security vulnerabilities, with broad adoption in both industry and
open-source communities. Directly fuzzing a function requires fuzz drivers,
which translate random fuzzer inputs into valid arguments for the target
function. Given the cost and expertise required to manually develop fuzz
drivers, methods exist that leverage program analysis and Large Language Models
to automatically generate these drivers. However, the generated fuzz drivers
frequently lead to false positive crashes, especially in functions highly
structured input and complex state requirements. This problem is especially
crucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as
reporting false positive crashes to maintainers impede trust in both the system
and the team.
  This paper presents two AI-driven strategies to reduce false positives in
OSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First,
constraint-based fuzz driver generation proactively enforces constraints on a
function's inputs and state to guide driver creation. Second, context-based
crash validation reactively analyzes function callers to determine whether
reported crashes are feasible from program entry points. Using 1,500 benchmark
functions from OSS-Fuzz, we show that these strategies reduce spurious crashes
by up to 8%, cut reported crashes by more than half, and demonstrate that
frontier LLMs can serve as reliable program analysis agents. Our results
highlight the promise and challenges of integrating AI into large-scale fuzzing
pipelines.

</details>
