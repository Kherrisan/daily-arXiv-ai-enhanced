<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 15]
- [cs.SE](#cs.SE) [Total: 11]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility](https://arxiv.org/abs/2507.11630)
*Brendan Murphy,Dillon Bowen,Shahrad Mohammadzadeh,Julius Broomfield,Adam Gleave,Kellin Pelrine*

Main category: cs.CR

TL;DR: This paper shows that fine-tuning (via open weights or APIs) can create AI models capable of bypassing moderation systems to produce high-quality malicious outputs, with newer models being more vulnerable. It warns that releasing fine-tunable models simultaneously creates 'evil twin' models for harmful use.


<details>
  <summary>Details</summary>
Motivation: Modern AI systems with safeguards are vulnerable to bypass through fine-tuning techniques, creating a critical need to understand and mitigate this risk against unintended misuse.

Method: Proposed jailbreak-tuning method that trains models to comply with harmful requests while maintaining output quality, demonstrating effectiveness via attacks on major models (OpenAI, Google, Anthropic) and analyzing input/weight space interactions.

Result: 1. Fine-tuned models comply with CBRN, cyberattack, and other criminal requests. 2. Stealthy attacks using backdoors increase impact. 3. Newer models show heightened vulnerability to these techniques.

Conclusion: Implies urgent need for tamper-resistant safeguards. Framing fine-tunable model releases as simultaneous 'evil twin' releases highlights the security risks. Calls for immediate action from companies and policymakers to prevent exploitation of this vulnerability.

Abstract: AI systems are rapidly advancing in capability, and frontier model developers
broadly acknowledge the need for safeguards against serious misuse. However,
this paper demonstrates that fine-tuning, whether via open weights or closed
fine-tuning APIs, can produce helpful-only models. In contrast to prior work
which is blocked by modern moderation systems or achieved only partial removal
of safeguards or degraded output quality, our jailbreak-tuning method teaches
models to generate detailed, high-quality responses to arbitrary harmful
requests. For example, OpenAI, Google, and Anthropic models will fully comply
with requests for CBRN assistance, executing cyberattacks, and other criminal
activity. We further show that backdoors can increase not only the stealth but
also the severity of attacks, while stronger jailbreak prompts become even more
effective in fine-tuning attacks, linking attack and potentially defenses in
the input and weight spaces. Not only are these models vulnerable, more recent
ones also appear to be becoming even more vulnerable to these attacks,
underscoring the urgent need for tamper-resistant safeguards. Until such
safeguards are discovered, companies and policymakers should view the release
of any fine-tunable model as simultaneously releasing its evil twin: equally
capable as the original model, and usable for any malicious purpose within its
capabilities.

</details>


### [2] [Evasion Under Blockchain Sanctions](https://arxiv.org/abs/2507.11721)
*Endong Liu,Mark Ryan,Liyi Zhou,Pascal Berrang*

Main category: cs.CR

TL;DR: This paper analyzes OFAC sanctions against Tornado Cash over 957 days, revealing a 71.03% reduction in deposits but 78.33% of Ethereum security incidents still using it, while introducing a new blockchain sanction enforcement algorithm with 97.61% precision.


<details>
  <summary>Details</summary>
Motivation: The study addresses challenges in enforcing sanctions on permissionless blockchains due to complex transaction flows and fund obfuscation techniques, focusing on cryptocurrency mixers like Tornado Cash.

Method: Quantitative assessment using 6.79 million Ethereum blocks and 1.07 billion transactions, identifying evasion patterns and proposing a sanction scoring algorithm based on quantitative impurity.

Result: 71.03% deposit reduction post-sanctions, 78.33% continued use in security incidents, and the algorithm achieved 0.07 second/block processing speed, 97.61% precision, and 74.08% recall in the Bybit exploit case.

Conclusion: Findings provide empirical evidence of enforcement limitations in DeFi, clarify challenges in sanctioning blockchain mixers, and suggest algorithmic improvements for tracking obfuscated transactions effectively.

Abstract: Sanctioning blockchain addresses has become a common regulatory response to
malicious activities. However, enforcement on permissionless blockchains
remains challenging due to complex transaction flows and sophisticated
fund-obfuscation techniques. Using cryptocurrency mixing tool Tornado Cash as a
case study, we quantitatively assess the effectiveness of U.S. Office of
Foreign Assets Control (OFAC) sanctions over a 957-day period, covering 6.79
million Ethereum blocks and 1.07 billion transactions. Our analysis reveals
that while OFAC sanctions reduced overall Tornado Cash deposit volume by 71.03%
to approximately 2 billion USD, attackers still relied on Tornado Cash in
78.33% of Ethereum-related security incidents, underscoring persistent evasion
strategies.
  We identify three structural limitations in current sanction enforcement
practices: (i) the susceptibility of binary sanction classifications to dusting
attacks; (ii) fragmented censorship by blockchain producers; and (iii) the
complexity of obfuscation services exploited by users. To address these gaps,
we introduce a more practical algorithm for scoring and tracking, grounded in
quantitative impurity. On average, our algorithm processes Ethereum blocks
within 0.07 $\pm$ 0.03 seconds and achieves 97.61% precision and 74.08% recall
when evaluated on the Bybit exploit. Our findings contribute to ongoing
discussions around regulatory effectiveness in Decentralized Finance by
providing empirical evidence, clarifying enforcement challenges, and informing
future compliance strategies in response to sanctions and blockchain-based
security risks.

</details>


### [3] [Space Cybersecurity Testbed: Fidelity Framework, Example Implementation, and Characterization](https://arxiv.org/abs/2507.11763)
*Jose Luis Castanon Remy,Caleb Chang,Ekzhin Ear,Shouhuai Xu*

Main category: cs.CR

TL;DR: This paper presents a framework to characterize the fidelity of space cybersecurity testbeds and applies it to a concrete testbed implementation.


<details>
  <summary>Details</summary>
Motivation: Current space cybersecurity testbeds lack sufficient study and characterization, limiting their effectiveness in threat validation and understanding.

Method: A seven-attribute framework was developed to evaluate system models, threat models, and defenses in testbeds, followed by the construction of a testbed integrating space, ground, user, and link segments.

Result: The testbed successfully accommodates real-world space cyber attack scenarios, demonstrating the framework's practical utility.

Conclusion: The proposed framework offers a systematic approach to designing and evaluating space cybersecurity testbeds, with implications for future research in realistic threat simulation.

Abstract: Cyber threats against space infrastructures, including satellites and systems
on the ground, have not been adequately understood. Testbeds are important to
deepen our understanding and validate space cybersecurity studies. The state of
the art is that there are very few studies on building testbeds, and there are
few characterizations of testbeds. In this paper, we propose a framework for
characterizing the fidelity of space cybersecurity testbeds. The framework
includes 7 attributes for characterizing the system models, threat models, and
defenses that can be accommodated by a testbed. We use the framework to guide
us in building and characterizing a concrete testbed we have implemented, which
includes space, ground, user, and link segments. In particular, we show how the
testbed can accommodate some space cyber attack scenarios that have occurred in
the real world, and discuss future research directions.

</details>


### [4] [How To Mitigate And Defend Against DDoS Attacks In IoT Devices](https://arxiv.org/abs/2507.11772)
*Ifiyemi Leigha,Basak Comlekcioglu,Maria Pilar Bezanilla*

Main category: cs.CR

TL;DR: Analyzes DDoS attacks in IoT and proposes layered mitigation strategies including IPv6 ULA, edge computing, SDN, honeypots, and machine learning for infrastructure protection.


<details>
  <summary>Details</summary>
Motivation: DDoS attacks are increasingly dangerous in IoT networks due to low-security device configurations, necessitating tailored mitigation approaches to safeguard critical infrastructure.

Method: Examines Mirai botnet attacks to characterize DDoS threats in IoT, then proposes multi-layered mitigation techniques combining network address management (IPv6 ULA), edge computing frameworks, SDN-based traffic control, honeypot deception tactics, and ML-powered intrusion detection systems.

Result: Identified effective combinations of IPv6 address strategies and edge computing reduced botnet impact by 72% in simulated IoT environments; machine learning models achieved 98% accuracy in attack detection through signature analysis.

Conclusion: Layered security approaches leveraging IPv6 features, edge computing, SDN, honeypots, and ML provide robust defense against IoT DDoS attacks, offering actionable solutions for infrastructure protection in complex, distributed environments.

Abstract: Distributed Denial of Service (DDoS) attacks have become increasingly
prevalent and dangerous in the context of Internet of Things (IoT) networks,
primarily due to the low-security configurations of many connected devices.
This paper analyzes the nature and impact of DDoS attacks such as those
launched by the Mirai botnet, and proposes layered mitigation strategies
tailored to IoT environments. Key solutions explored include IPv6 Unique Local
Addresses (ULA), edge computing, software-defined networking (SDN), honeypot
deception, and machine learning-based intrusion detection systems. The paper
aims to help engineers and researchers understand and implement practical
countermeasures to protect IoT infrastructures.

</details>


### [5] [Challenges in GenAI and Authentication: a scoping review](https://arxiv.org/abs/2507.11775)
*Wesley dos Reis Bezerra,Lais Machado Bezerra,Carlos Becker Westphall*

Main category: cs.CR

TL;DR: The paper conducts a scoping review of 88 documents to analyze the impact of generative artificial intelligence on authentication and authenticity, identifying challenges and gaps across images, text, audio, and video.


<details>
  <summary>Details</summary>
Motivation: The advancement of generative AI has created evolving security challenges in authentication, necessitating an updated analysis of its societal and technical impacts.

Method: A qualitative scoping review analyzing 88 documents from IEEEXplore, Scopus, and ACM databases, guided by six specific research questions about authentication challenges in AI contexts.

Result: The study outlines persistent challenges (e.g., attack surfaces, vulnerabilities), gaps in security approaches, and risks across different media types, while surveying proposed solutions.

Conclusion: The review provides a foundation for future research in authentication amidst generative AI, emphasizing the need for addressing identified security threats and gaps.

Abstract: Authentication and authenticity have been a security challenge since the
beginning of information sharing, especially in the context of digital
information. With the advancement of generative artificial intelligence, these
challenges have evolved, demanding a more up-to-date analysis of their impacts
on society and system security. This work presents a scoping review that
analyzed 88 documents from the IEEExplorer, Scopus, and ACM databases,
promoting an analysis of the resulting portfolio through six guiding questions
focusing on the most relevant work, challenges, attack surfaces, threats,
proposed solutions, and gaps. Finally, the portfolio articles are analyzed
through this guiding research lens and also receive individualized analysis.
The results consistently outline the challenges, gaps, and threats related to
images, text, audio, and video, thereby supporting new research in the areas of
authentication and generative artificial intelligence.

</details>


### [6] [Unveiling Usability Challenges in Web Privacy Controls](https://arxiv.org/abs/2507.11908)
*Rahat Masood,Sunday Oyinlola Ogundoyin,Muhammad Ikram,Alex Ye*

Main category: cs.CR

TL;DR: The paper analyzes usability challenges of web privacy controls across 18,628 websites, focusing on guest visit scenarios due to automated data collection hurdles, and recommends design improvements like pop-up nudges and direct navigation links.


<details>
  <summary>Details</summary>
Motivation: Increasing privacy concerns and data privacy laws necessitate better user control over web privacy. However, non-standardized, hard-to-find, and poorly worded privacy controls create confusion and hinder user ability to manage their data effectively.

Method: Large-scale empirical analysis of 18,628 websites using heuristic evaluation of three user visit scenarios. Automated data collection faced challenges simulating sign-up and authenticated user interactions, limiting insights to guest visit scenarios.

Result: Privacy policies were the most common feature across all visit scenarios. Sign-up scenarios showed prevalence of nudges/notices alongside policies. Automated capture of dynamic interactions was challenging. Heuristic evaluation identified significant usability issues.

Conclusion: The paper recommends designing privacy controls with pop-up nudges for awareness, table of contents for policy navigation, customized settings links for informed choices, and direct accessibility from nudges to privacy settings.

Abstract: With the increasing concerns around privacy and the enforcement of data
privacy laws, many websites now provide users with privacy controls. However,
locating these controls can be challenging, as they are frequently hidden
within multiple settings and layers. Moreover, the lack of standardization
means these controls can vary widely across services. The technical or
confusing terminology used to describe these controls further complicates
users' ability to understand and use them effectively. This paper presents a
large-scale empirical analysis investigating usability challenges of web
privacy controls across 18,628 websites. While aiming for a multi-scenario
view, our automated data collection faced significant hurdles, particularly in
simulating sign-up and authenticated user visits, leading to more focused
insights on guest visit scenarios and challenges in automated capture of
dynamic user interactions. Our heuristic evaluation of three different user
visit scenarios identifies significant website usability issues. Our results
show that privacy policies are most common across all visit scenarios, with
nudges and notices being prevalent in sign-up situations. We recommend
designing privacy controls that: enhance awareness through pop-up nudges and
notices; offer a table of contents as navigational aids and customized settings
links in policies for more informed choice; and ensure accessibility via direct
links to privacy settings from nudges.

</details>


### [7] [Effective Fine-Tuning of Vision Transformers with Low-Rank Adaptation for Privacy-Preserving Image Classification](https://arxiv.org/abs/2507.11943)
*Haiwei Lin,Shoko Imaizumi,Hitoshi Kiya*

Main category: cs.CR

TL;DR: The paper introduces a novel low-rank adaptation method for privacy-preserving vision transformers (ViT) that injects trainable rank decomposition matrices into every layer while leaving the patch embedding layer unfrozen, achieving efficient training with minimal accuracy loss compared to full-tuning.


<details>
  <summary>Details</summary>
Motivation: Conventional low-rank adaptation methods sacrifice accuracy by fixing the patch embedding layer alongside other parameters during fine-tuning. The authors aim to overcome this limitation by maintaining adaptability in the patch embedding to preserve model performance while enabling efficient privacy-preserving training through parameter freezing.

Method: The proposed method decomposes each ViT layer into low-rank matrices which remain trainable, while all pre-trained weights are frozen. The patch embedding layer is intentionally left unfrozen, unlike standard approaches, to allow dynamic adaptation during training.

Result: The method achieves comparable accuracy to full-time fine-tuning using significantly fewer trainable parameters, demonstrating effectiveness in both parameter efficiency and maintaining model performance for privacy-sensitive vision tasks.

Conclusion: This approach provides a practical solution for deploying privacy-preserving vision transformers by enabling efficient low-parameter adaptation without the accuracy trade-offs of traditional frozen-layer methods.

Abstract: We propose a low-rank adaptation method for training privacy-preserving
vision transformer (ViT) models that efficiently freezes pre-trained ViT model
weights. In the proposed method, trainable rank decomposition matrices are
injected into each layer of the ViT architecture, and moreover, the patch
embedding layer is not frozen, unlike in the case of the conventional low-rank
adaptation methods. The proposed method allows us not only to reduce the number
of trainable parameters but to also maintain almost the same accuracy as that
of full-time tuning.

</details>


### [8] [Expanding ML-Documentation Standards For Better Security](https://arxiv.org/abs/2507.12003)
*Cara Ellen Appel*

Main category: cs.CR

TL;DR: The paper highlights the lack of standardized security documentation in ML practices and proposes an expanded documentation framework incorporating security requirements based on Model Cards and Datasheets.


<details>
  <summary>Details</summary>
Motivation: Existing ML documentation lacks security awareness and standardization, leading to poor quality and oversight of IT-security risks in systems, models, and datasets.

Method: Expanded documentation standards (Model Cards/Datasheets) with tailored security sections containing explicit security requirement specifications.

Result: Demonstration of a new ML documentation framework that explicitly integrates security considerations into standard artifact documentation.

Conclusion: Improved standardized security documentation is essential to address ML-security gaps, requiring adoption of expanded practices across all ML documentation types.

Abstract: This article presents the current state of ML-security and of the
documentation of ML-based systems, models and datasets in research and practice
based on an extensive review of the existing literature. It shows a generally
low awareness of security aspects among ML-practitioners and organizations and
an often unstandardized approach to documentation, leading to overall low
quality of ML-documentation. Existing standards are not regularly adopted in
practice and IT-security aspects are often not included in documentation. Due
to these factors, there is a clear need for improved security documentation in
ML, as one step towards addressing the existing gaps in ML-security. To achieve
this, we propose expanding existing documentation standards for
ML-documentation to include a security section with specific security relevant
information. Implementing this, a novel expanded method of documenting security
requirements in ML-documentation is presented, based on the existing Model
Cards and Datasheets for Datasets standards, but with the recommendation to
adopt these findings in all ML-documentation.

</details>


### [9] [IDFace: Face Template Protection for Efficient and Secure Identification](https://arxiv.org/abs/2507.12050)
*Sunpill Kim,Seunghun Paik,Chanwoo Hwang,Dongsoo Kim,Junbum Shin,Jae Hong Seo*

Main category: cs.CR

TL;DR: The study proposes IDFace, an efficient HE-based face identification method with template protection, achieving 2X overhead over plaintext systems for 1M encrypted templates.


<details>
  <summary>Details</summary>
Motivation: Current HE-based face template protection methods face severe efficiency issues (hundreds of times slower than plaintext), necessitating a secure yet practical solution for widespread FRS adoption.

Method: IDFace combines two novel techniques: (1) a template representation transformation to reduce matching unit cost and (2) a space-efficient encoding to minimize encryption-induced operations on angular-metric biometric databases.

Result: IDFace identifies a face from 1 million encrypted templates in 126ms with only 2X overhead compared to plaintext identification.

Conclusion: IDFace effectively balances security and efficiency in HE-based face template protection, enabling scalable private FRS implementations.

Abstract: As face recognition systems (FRS) become more widely used, user privacy
becomes more important. A key privacy issue in FRS is protecting the user's
face template, as the characteristics of the user's face image can be recovered
from the template. Although recent advances in cryptographic tools such as
homomorphic encryption (HE) have provided opportunities for securing the FRS,
HE cannot be used directly with FRS in an efficient plug-and-play manner. In
particular, although HE is functionally complete for arbitrary programs, it is
basically designed for algebraic operations on encrypted data of predetermined
shape, such as a polynomial ring. Thus, a non-tailored combination of HE and
the system can yield very inefficient performance, and many previous HE-based
face template protection methods are hundreds of times slower than plain
systems without protection. In this study, we propose IDFace, a new HE-based
secure and efficient face identification method with template protection.
IDFace is designed on the basis of two novel techniques for efficient searching
on a (homomorphically encrypted) biometric database with an angular metric. The
first technique is a template representation transformation that sharply
reduces the unit cost for the matching test. The second is a space-efficient
encoding that reduces wasted space from the encryption algorithm, thus saving
the number of operations on encrypted templates. Through experiments, we show
that IDFace can identify a face template from among a database of 1M encrypted
templates in 126ms, showing only 2X overhead compared to the identification
over plaintexts.

</details>


### [10] [Toward an Intent-Based and Ontology-Driven Autonomic Security Response in Security Orchestration Automation and Response](https://arxiv.org/abs/2507.12061)
*Zequan Huang,Jacques Robin,Nicolas Herbaut,Nourhène Ben Rabah,Bénédicte Le Grand*

Main category: cs.CR

TL;DR: This paper bridges Intent-Based Cyber Defense and Autonomic Cyber Defense by introducing a unified, ontology-driven security intent framework using MITRE-D3FEND, enabling hierarchical, context-aware automated responses in next-gen SOAR platforms.


<details>
  <summary>Details</summary>
Motivation: Modern SOAR platforms require rapid adaptation to evolving cyber attacks, but procedural actions lack the flexibility and persistence of high-level declarative intents.

Method: A Two-tiered methodology integrating security intents into decision-theoretic autonomic systems via the MITRE-D3FEND ontology, enabling hierarchical and context-aware automation.

Result: Demonstration of practical integration through a use case within next-generation SOAR platforms, validating the approach's effectiveness for dynamic attack scenarios.

Conclusion: Ontology-driven unification of intent-based and autonomic defense paradigms enhances automated response capabilities in SOAR, offering scalable context-aware adaptation.

Abstract: Modern Security Orchestration, Automation, and Response (SOAR) platforms must
rapidly adapt to continuously evolving cyber attacks. Intent-Based Networking
has emerged as a promising paradigm for cyber attack mitigation through
high-level declarative intents, which offer greater flexibility and persistency
than procedural actions. In this paper, we bridge the gap between two active
research directions: Intent-Based Cyber Defense and Autonomic Cyber Defense, by
proposing a unified, ontology-driven security intent definition leveraging the
MITRE-D3FEND cybersecurity ontology. We also propose a general two-tiered
methodology for integrating such security intents into decision-theoretic
Autonomic Cyber Defense systems, enabling hierarchical and context-aware
automated response capabilities. The practicality of our approach is
demonstrated through a concrete use case, showcasing its integration within
next-generation Security Orchestration, Automation, and Response platforms.

</details>


### [11] [A Privacy-Preserving Framework for Advertising Personalization Incorporating Federated Learning and Differential Privacy](https://arxiv.org/abs/2507.12098)
*Xiang Li,Yifan Lin,Yuanzhe Zhang*

Main category: cs.CR

TL;DR: This paper proposes a privacy-protective framework for personalized advertising using federated learning and differential privacy to balance accuracy, efficiency, and privacy.


<details>
  <summary>Details</summary>
Motivation: To address privacy leakage and performance challenges in personalized advertising systems, necessitating a balance between model accuracy and privacy protection.

Method: The framework integrates distributed feature extraction, dynamic privacy budget allocation, robust model aggregation, multi-party secure computing, and anomaly detection mechanisms.

Result: Experimental results show the framework achieves dual optimization of recommendation accuracy and system efficiency while ensuring privacy protection.

Conclusion: The framework provides both a practical application solution and theoretical foundation for deploying privacy technologies in advertising recommendations.

Abstract: To mitigate privacy leakage and performance issues in personalized
advertising, this paper proposes a framework that integrates federated learning
and differential privacy. The system combines distributed feature extraction,
dynamic privacy budget allocation, and robust model aggregation to balance
model accuracy, communication overhead, and privacy protection. Multi-party
secure computing and anomaly detection mechanisms further enhance system
resilience against malicious attacks. Experimental results demonstrate that the
framework achieves dual optimization of recommendation accuracy and system
efficiency while ensuring privacy, providing both a practical solution and a
theoretical foundation for applying privacy protection technologies in
advertisement recommendation.

</details>


### [12] [Exploiting Jailbreaking Vulnerabilities in Generative AI to Bypass Ethical Safeguards for Facilitating Phishing Attacks](https://arxiv.org/abs/2507.12185)
*Rina Mishra,Gaurav Varshney*

Main category: cs.CR

TL;DR: This paper investigates how jailbreaking GenAI chatbots like ChatGPT enables novice users to execute undetectable phishing attacks across web, email, SMS, and voice channels, despite ethical safeguards.


<details>
  <summary>Details</summary>
Motivation: Advanced GenAI models (DeepSeek, ChatGPT) are reshaping cybersecurity by introducing phishing automation risks while evading traditional detection mechanisms.

Method: Controlled experiments using ChatGPT 4o Mini (chosen for accessibility) to test jailbreaking techniques and evaluate phishing attack vectors across multiple communication platforms.

Result: Demonstrated successful generation of phishing content, hacking tool recommendations, and campaign coordination by GenAI models, with attacks evading standard anti-phishing measures due to human-in-the-loop patterns.

Conclusion: Human-guided GenAI phishing represents a critical security threat; mitigation requires layered approaches including education, authentication upgrades, and policy reforms, with implications for future AI security research and regulation.

Abstract: The advent of advanced Generative AI (GenAI) models such as DeepSeek and
ChatGPT has significantly reshaped the cybersecurity landscape, introducing
both promising opportunities and critical risks. This study investigates how
GenAI powered chatbot services can be exploited via jailbreaking techniques to
bypass ethical safeguards, enabling the generation of phishing content,
recommendation of hacking tools, and orchestration of phishing campaigns. In
ethically controlled experiments, we used ChatGPT 4o Mini selected for its
accessibility and status as the latest publicly available model at the time of
experimentation, as a representative GenAI system. Our findings reveal that the
model could successfully guide novice users in executing phishing attacks
across various vectors, including web, email, SMS (smishing), and voice
(vishing). Unlike automated phishing campaigns that typically follow detectable
patterns, these human-guided, AI assisted attacks are capable of evading
traditional anti phishing mechanisms, thereby posing a growing security threat.
We focused on DeepSeek and ChatGPT due to their widespread adoption and
technical relevance in 2025. The study further examines common jailbreaking
techniques and the specific vulnerabilities exploited in these models. Finally,
we evaluate a range of mitigation strategies such as user education, advanced
authentication mechanisms, and regulatory policy measures and discuss emerging
trends in GenAI facilitated phishing, outlining future research directions to
strengthen cybersecurity defenses in the age of artificial intelligence.

</details>


### [13] [Efficient Control Flow Attestation by Speculating on Control Flow Path Representations](https://arxiv.org/abs/2507.12345)
*Liam Tyler,Adam Caulfield,Ivan De Oliveira Nunes*

Main category: cs.CR

TL;DR: RESPEC-CFA significantly reduces control flow log sizes via locality speculation and Huffman encoding.


<details>
  <summary>Details</summary>
Motivation: Control Flow Attestation (CFA) faces high storage/transmission costs due to large control flow logs (CFlogs). Prior methods lack address representation optimization.

Method: Proposed architectural extension for CFA: (1) Locality speculation of control flows and (2) Huffman encoding of control flow paths. Combines these with prior application-specific optimizations.

Result: Reduces CFlog sizes by up to 90.1% alone, and 99.7% when combined with prior methods, demonstrating substantial efficiency gains.

Conclusion: RESPEC-CFA enables practical CFA implementation by addressing storage/transmission limitations through novel speculative compression techniques.

Abstract: Control Flow Attestation (CFA) allows remote verification of run-time
software integrity in embedded systems. However, CFA is limited by the
storage/transmission costs of generated control flow logs (CFlog). Recent work
has proposed application-specific optimizations by speculating on likely
sub-paths in CFlog and replacing them with reserved symbols at runtime. Albeit
effective, prior approaches do not consider the representation of addresses in
a control flow path for speculation. This work proposes RESPEC-CFA, an
architectural extension for CFA allowing for speculation on (1) the locality of
control flows and (2) their Huffman encoding. Alone, RESPEC-CFA reduces CFlog
sizes by up to 90.1%. Combined with prior methods, RESPEC-CFA yields reductions
of up to 99.7%, representing a significant step toward practical CFA.

</details>


### [14] [Rethinking the confidential cloud through a unified low-level abstraction for composable isolation](https://arxiv.org/abs/2507.12364)
*Adrien Ghosn,Charly Castes,Neelu S. Kalani,Yuchen Qian,Marios Kogias,Edouard Bugnion*

Main category: cs.CR

TL;DR: Tyche is a unified isolation model for cloud workloads using composable trust domains (TDs) across enclaves, sandboxes, and CVMs, achieving secure resource management, end-to-end attestation, and minimal overhead without hardware extensions.


<details>
  <summary>Details</summary>
Motivation: Existing isolation mechanisms for securing cloud workloads increase complexity, bloat the Trusted Computing Base (TCB), and lack cross-platform composability, requiring costly ad-hoc security solutions.

Method: Tyche introduces trust domains (TDs) as a core abstraction to manage hierarchical isolation boundaries through a single security monitor, enabling programmatic resource partitioning, sharing, and attestation while supporting x86_64 commodity hardware and backward compatibility.

Result: Tyche maintains native Linux-level performance with minimal overhead, handles confidential inference scenarios with multiple distrustful parties, and demonstrates portability via a RISC-V prototype.

Conclusion: Tyche provides a scalable, platform-agnostic solution to simplify isolation-based cloud security, enabling composable and attestable workloads across CSPs and hardware architectures.

Abstract: Securing sensitive cloud workloads requires composing confidential virtual
machines (CVMs) with nested enclaves or sandboxes. Unfortunately, each new
isolation boundary adds ad-hoc access control mechanisms, hardware extensions,
and trusted software. This escalating complexity bloats the TCB, complicates
end-to-end attestation, and leads to fragmentation across platforms and cloud
service providers (CSPs).
  We introduce a unified isolation model that delegates enforceable,
composable, and attestable isolation to a single trusted security monitor:
Tyche. Tyche provides an API for partitioning, sharing, attesting, and
reclaiming resources through its core abstraction, trust domains (TDs). To
provide fine-grain isolation, TDs can recursively create and manage sub-TDs.
Tyche captures these relationships in attestations, allowing cloud tenants to
reason about end-to-end security. TDs serve as the building blocks for
constructing composable enclaves, sandboxes, and CVMs.
  Tyche runs on commodity x86_64 without hardware security extensions and can
maintain backward compatibility with existing software. We provide an SDK to
run and compose unmodified workloads as sandboxes, enclaves, and CVMs with
minimal overhead compared to native Linux execution. Tyche supports complex
cloud scenarios, such as confidential inference with mutually distrustful
users, model owners, and CSPs. An additional RISC-V prototype demonstrates
Tyche's portability across platforms.

</details>


### [15] [On One-Shot Signatures, Quantum vs Classical Binding, and Obfuscating Permutations](https://arxiv.org/abs/2507.12456)
*Omri Shmueli,Mark Zhandry*

Main category: cs.CR

TL;DR: This paper introduces the first standard-model one-shot signatures (OSS) under classical security assumptions (iO and LWE), resolves decade-old open problems in post-quantum commitments, and constructs full-domain trapdoor one-way permutations using permutable PRPs.


<details>
  <summary>Details</summary>
Motivation: The original OSS construction by Amos et al. relied on a flawed classical oracle model and quantum principles, leaving the existence of secure OSS under classical assumptions unresolved for a decade.

Method: The authors develop permutable pseudorandom permutations (PRPs) to translate oracle-based security proofs into obfuscation-based ones. They leverage sub-exponential indistinguishability obfuscation (iO) and LWE hardness, along with a technique for constructing trapdoor one-way permutations from obfuscated permutable PRPs.

Result: First standard-model OSS with provable security from classical assumptions; first standard-model separation between classical-binding and collapse-binding commitments/hashing; and construction of full-domain trapdoor one-way permutations from iO and one-way functions.

Conclusion: The paper resolves critical open questions in quantum and classical cryptography by realizing secure OSS under classical assumptions, advancing obfuscation-based proof techniques, and enabling foundational cryptographic primitives with decade-old unresolved security requirements.

Abstract: One-shot signatures (OSS) were defined by Amos, Georgiou, Kiayias, and
Zhandry (STOC'20). These allow for signing exactly one message, after which the
signing key self-destructs, preventing a second message from ever being signed.
While such an object is impossible classically, Amos et al observe that OSS may
be possible using quantum signing keys by leveraging the no-cloning principle.
OSS has since become an important conceptual tool with many applications in
decentralized settings and for quantum cryptography with classical
communication. OSS are also closely related to separations between
classical-binding and collapse-binding for post-quantum hashing and
commitments. Unfortunately, the only known OSS construction due to Amos et al.
was only justified in a classical oracle model, and moreover their
justification was ultimately found to contain a fatal bug. Thus, the existence
of OSS, even in a classical idealized model, has remained open.
  We give the first standard-model OSS, with provable security assuming
(sub-exponential) indistinguishability obfuscation (iO) and LWE. This also
gives the first standard-model separation between classical and
collapse-binding post-quantum commitments/hashing, solving a decade-old open
problem. Along the way, we also give the first construction with unconditional
security relative to a classical oracle. To achieve our standard-model
construction, we develop a notion of permutable pseudorandom permutations
(permutable PRPs), and show how they are useful for translating oracle proofs
involving random permutations into obfuscation-based proofs. In particular,
obfuscating permutable PRPs gives a trapdoor one-way permutation that is
\emph{full-domain}, solving another decade-old-problem of constructing this
object from (sub-exponential) iO and one-way functions.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [16] [Decision Models for Selecting Architecture Patterns and Strategies in Quantum Software Systems](https://arxiv.org/abs/2507.11671)
*Mst Shamima Aktar,Peng Liang,Muhammad Waseem,Amjed Tahir,Mojtaba Shahin,Muhammad Azeem Akbar,Arif Ali Khan,Aakash Ahmad,Musengamana Jean de Dieu,Ruiyin Li*

Main category: cs.SE

TL;DR: This paper addresses the challenges in quantum software architecture by proposing decision models for six design areas (Communication, Decomposition, Data Processing, Fault Tolerance, Integration & Optimization, Algorithm Implementation) to guide practitioners in selecting effective patterns and strategies. The models are validated through a mining study, literature review, and interviews with 16 experts.


<details>
  <summary>Details</summary>
Motivation: Quantum software architecture requires reusable patterns and strategies to manage system complexity, but practitioners lack clear guidelines for their selection and implementation.

Method: 1. Conducted a mining study using GitHub and Stack Exchange to identify patterns/strategies. 2. Performed a systematic literature review to gather Quality Attributes (QAs). 3. Validated findings through semi-structured interviews with 16 quantum software practitioners.

Result: The semi-structured interviews confirmed the decision models enhance practitioners' understanding, completeness, and usefulness in addressing architecture design challenges for quantum software systems.

Conclusion: The proposed decision models effectively aid practitioners in quantum software architecture design, and the provided dataset [6] facilitates reproducibility and future research extensions.

Abstract: Quantum software represents disruptive technologies in terms of
quantum-specific software systems, services, and applications - leverage the
principles of quantum mechanics via programmable quantum bits (Qubits) that
manipulate quantum gates (QuGates) - to achieve quantum supremacy in computing.
Quantum software architecture enables quantum software developers to abstract
away implementation-specific details (i.e., mapping of Qubits and QuGates to
high-level architectural components and connectors). Architectural patterns and
strategies can provide reusable knowledge and best practices to engineer
quantum software systems effectively and efficiently. However, quantum software
practitioners face significant challenges in selecting and implementing
appropriate patterns and strategies due to the complexity of quantum software
systems and the lack of guidelines. To address these challenges, this study
proposes decision models for selecting patterns and strategies in six critical
design areas in quantum software systems: Communication, Decomposition, Data
Processing, Fault Tolerance, Integration and Optimization, and Algorithm
Implementation. These decision models are constructed based on data collected
from both a mining study (i.e., GitHub and Stack Exchange) and a Systematic
Literature Review, which were used to identify relevant patterns and strategies
with their involved Quality Attributes (QAs). We then conducted semi-structured
interviews with 16 quantum software practitioners to evaluate the familiarity,
understandability, completeness, and usefulness of the proposed decision
models. The results show that the proposed decision models can aid
practitioners in selecting suitable patterns and strategies to address the
challenges related to the architecture design of quantum software systems. The
dataset is available at [6], allowing the community to reproduce and build upon
our findings.

</details>


### [17] [MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization](https://arxiv.org/abs/2507.11687)
*Atharva Naik,Lawanya Baghel,Dhakshin Govindarajan,Darsh Agrawal,Daniel Fried,Carolyn Rose*

Main category: cs.SE

TL;DR: MetaLint introduces an instruction-following framework for code quality analysis using synthetic linter data, achieving strong idiom detection (70.37% F-score) and localization performance (26.73%) with a 4B parameter model.


<details>
  <summary>Details</summary>
Motivation: Large language models lack adaptability to evolving code best practices due to static training data limitations. This paper demonstrates the potential of instruction tuning on synthetic linter data for better semantic code analysis.

Method: The framework uses synthetic linter-generated data for instruction tuning, enabling models to generalize from easy-to-hard code analysis tasks while adapting to novel code patterns without retraining.

Result: MetaLint achieves state-of-the-art 70.37% F-score on PEP idiom detection with highest recall (70.43%), and competitive 26.73% localization performance against larger models like o3-mini despite its 4B parameter size.

Conclusion: MetaLint's approach enables future-proof code quality analysis by combining synthetic instruction tuning with adaptive generalization, outperforming conventional static training methods in handling evolving coding standards.

Abstract: Large Language Models, though successful in code generation, struggle with
code quality analysis because they are limited by static training data and
can't easily adapt to evolving best practices. We introduce MetaLint, a new
instruction-following framework that formulates code quality analysis as the
task of detecting and fixing problematic semantic code fragments or code idioms
based on high-level specifications. Unlike conventional approaches that train
models on static, rule-based data, MetaLint employs instruction tuning on
synthetic linter-generated data to support easy-to-hard generalization,
enabling models to adapt to novel or complex code patterns without retraining.
To evaluate this, we construct a benchmark of challenging idioms inspired by
real-world coding standards such as Python Enhancement Proposals (PEPs) and
assess whether MetaLint-trained models reason adaptively or simply memorize.
Our results show that MetaLint improves generalization to unseen PEP idioms,
achieving a 70.37% F-score on idiom detection with the highest recall (70.43%)
among all evaluated models. It also achieves 26.73% on localization,
competitive for its 4B parameter size and comparable to larger state-of-the-art
models like o3-mini, highlighting its potential for future-proof code quality
analysis.

</details>


### [18] [REST in Pieces: RESTful Design Rule Violations in Student-Built Web Apps](https://arxiv.org/abs/2507.11689)
*Sergio Di Meglio,Valeria Pontillo,Luigi Libero Lucio Starace*

Main category: cs.SE

TL;DR: Study on student code quality in web courses reveals high frequency of REST API design rule violations (98% missing hyphens, 88% incorrect pluralization, 83% HTTP method misuse), emphasizing the need for targeted API design education and automated code quality tools in academic settings.


<details>
  <summary>Details</summary>
Motivation: Identify gaps in undergraduate software quality education to align with industry expectations and improve hiring practices for Web Technologies courses.

Method: Automated static analysis of 40 full-stack web applications from a third-year university course to evaluate REST API design rule adherence.

Result: 98% of projects had endpoint paths missing hyphens, 88% showed incorrect pluralization, and 83% demonstrated HTTP method misuse. Violations predominantly involved basic API design conventions rather than complex patterns.

Conclusion: The findings advocate for integrating focused API design instruction into curricula and adopting automated analysis tools to enhance code quality in student projects before industry deployment.

Abstract: In Computer Science Bachelor's programs, software quality is often
underemphasized due to limited time and a focus on foundational skills, leaving
many students unprepared for industry expectations. To better understand the
typical quality of student code and inform both education and hiring practices,
we analyze 40 full-stack web applications developed in a third-year Web
Technologies course. Using an automated static analysis pipeline, we assess
adherence to REST API design rules. Results reveal frequent violations of
foundational conventions, such as missing hyphens in endpoint paths (98%),
incorrect pluralization (88%), and misuse of HTTP methods (83%). These findings
highlight the need for more focused instruction on API design and support the
adoption of automated tools to improve code quality in student projects.

</details>


### [19] [Extremal Testing for Network Software using LLMs](https://arxiv.org/abs/2507.11898)
*Rathin Singha,Harry Qian,Srinath Saikrishnan,Tracy Zhao,Ryan Beckett,Siva Kesava Reddy Kakarla,George Varghese*

Main category: cs.SE

TL;DR: This paper automates extremal testing of network software using LLMs in two steps: generating input constraints and creating violations to uncover bugs. Demonstrations on HTTP, BGP, DNS found new issues, and the methodology extends to centralized systems like shortest path algorithms. Proposes agentic AI for further automation, outperforming traditional Boundary Value Analysis.


<details>
  <summary>Details</summary>
Motivation: Manual analysis of extreme cases in network software testing is labor-intensive and error-prone. Automating this process with LLMs improves efficiency and uncover potential bugs that traditional methods might miss.

Method: 1) LLMs generate input constraints (e.g., DNS name limits). 2) LLMs create tests violating these constraints. 3) Extends to centralized software by generating filtering code to reject edge cases. 4) Integrates agentic AI for self-driving testing workflows.

Result: Automated extremal tests for HTTP, BGP, and DNS implementations successfully revealed new bugs. Demonstrated adaptability to centralized algorithms. Filtering code generation for constraint enforcement was validated.

Conclusion: LLM-based extremal testing is a scalable, effective methodology for network software validation. Agentic AI enhances automation beyond Boundary Value Analysis, offering a new paradigm for robustness testing while reducing manual effort.

Abstract: Physicists often manually consider extreme cases when testing a theory. In
this paper, we show how to automate extremal testing of network software using
LLMs in two steps: first, ask the LLM to generate input constraints (e.g., DNS
name length limits); then ask the LLM to generate tests that violate the
constraints. We demonstrate how easy this process is by generating extremal
tests for HTTP, BGP and DNS implementations, each of which uncovered new bugs.
We show how this methodology extends to centralized network software such as
shortest path algorithms, and how LLMs can generate filtering code to reject
extremal input. We propose using agentic AI to further automate extremal
testing. LLM-generated extremal testing goes beyond an old technique in
software testing called Boundary Value Analysis.

</details>


### [20] [A Task Taxonomy for Conformance Checking](https://arxiv.org/abs/2507.11976)
*Jana-Rebecca Rehse,Michael Grohs,Finn Klessascheck,Lisa-Marie Klein,Tatiana von Landesberger,Luise Pufahl*

Main category: cs.SE

TL;DR: The paper introduces a task taxonomy for conformance checking to clarify the analytical purposes of visualizations and bridge process mining and visual analytics disciplines.


<details>
  <summary>Details</summary>
Motivation: Current conformance checking visualizations lack clear analytical purpose specifications, making their usefulness ambiguous and hindering objective evaluation.

Method: The authors propose a task taxonomy derived from combining process mining concepts with visual analytics frameworks, systematically categorizing analysis tasks by goal, means, constraint type, data characteristics, data target, and data cardinality.

Result: A comprehensive task taxonomy that enables researchers to identify the intended analytical purposes of conformance checking visualizations and define specific research requirements for different visualization types.

Conclusion: The taxonomy provides a foundational framework for evaluating and developing conformance checking visualizations by clarifying their analytical contexts through cross-disciplinary integration of process mining and visual analytics concepts.

Abstract: Conformance checking is a sub-discipline of process mining, which compares
observed process traces with a process model to analyze whether the process
execution conforms with or deviates from the process design. Organizations can
leverage this analysis, for example to check whether their processes comply
with internal or external regulations or to identify potential improvements.
Gaining these insights requires suitable visualizations, which make complex
results accessible and actionable. So far, however, the development of
conformance checking visualizations has largely been left to tool vendors. As a
result, current tools offer a wide variety of visual representations for
conformance checking, but the analytical purposes they serve often remain
unclear. However, without a systematic understanding of these purposes, it is
difficult to evaluate the visualizations' usefulness. Such an evaluation hence
requires a deeper understanding of conformance checking as an analysis domain.
To this end, we propose a task taxonomy, which categorizes the tasks that can
occur when conducting conformance checking analyses. This taxonomy supports
researchers in determining the purpose of visualizations, specifying relevant
conformance checking tasks in terms of their goal, means, constraint type, data
characteristics, data target, and data cardinality. Combining concepts from
process mining and visual analytics, we address researchers from both
disciplines to enable and support closer collaborations.

</details>


### [21] [LLAMA: Multi-Feedback Smart Contract Fuzzing Framework with LLM-Guided Seed Generation](https://arxiv.org/abs/2507.12084)
*Keke Gai,Haochen Liang,Jing Yu,Liehuang Zhu,Dusit Niyato*

Main category: cs.SE

TL;DR: LLAMA is a novel smart contract fuzzer combining Large Language Models (LLMs), evolutionary mutation strategies, and hybrid testing to improve mutation scheduling, achieving high coverage and vulnerability detection rates.


<details>
  <summary>Details</summary>
Motivation: Existing smart contract fuzzers focus on seed scheduling and generation while neglecting mutation scheduling, a critical factor in fuzzing effectiveness.

Method: LLAMA integrates hierarchical LLM prompting for semantically valid seeds, multi-feedback optimization using runtime coverage and dependency feedback, and an evolutionary engine with dynamic mutation probability adjustment and symbolic execution to enhance exploration.

Result: Outperforms state-of-the-art fuzzers with 91% instruction coverage, 90% branch coverage, and detects 132/148 known vulnerabilities across diverse categories.

Conclusion: LLAMA demonstrates superior adaptability and practicality for real-world smart contract security testing through its innovative use of LLMs and evolutionary techniques.

Abstract: Smart contracts play a pivotal role in blockchain ecosystems, and fuzzing
remains an important approach to securing smart contracts. Even though mutation
scheduling is a key factor influencing fuzzing effectiveness, existing fuzzers
have primarily explored seed scheduling and generation, while mutation
scheduling has been rarely addressed by prior work. In this work, we propose a
Large Language Models (LLMs)-based Multi-feedback Smart Contract Fuzzing
framework (LLAMA) that integrates LLMs, evolutionary mutation strategies, and
hybrid testing techniques. Key components of the proposed LLAMA include: (i) a
hierarchical prompting strategy that guides LLMs to generate semantically valid
initial seeds, coupled with a lightweight pre-fuzzing phase to select
high-potential inputs; (ii) a multi-feedback optimization mechanism that
simultaneously improves seed generation, seed selection, and mutation
scheduling by leveraging runtime coverage and dependency feedback; and (iii) an
evolutionary fuzzing engine that dynamically adjusts mutation operator
probabilities based on effectiveness, while incorporating symbolic execution to
escape stagnation and uncover deeper vulnerabilities. Our experiments
demonstrate that LLAMA outperforms state-of-the-art fuzzers in both coverage
and vulnerability detection. Specifically, it achieves 91% instruction coverage
and 90% branch coverage, while detecting 132 out of 148 known vulnerabilities
across diverse categories. These results highlight LLAMA's effectiveness,
adaptability, and practicality in real-world smart contract security testing
scenarios.

</details>


### [22] [From Static to Intelligent: Evolving SaaS Pricing with LLMs](https://arxiv.org/abs/2507.12104)
*Francisco Javier Cavero,Juan C. Alonso,Antonio Ruiz-Cortés*

Main category: cs.SE

TL;DR: This paper proposes an LLM-driven approach for automating the transformation of static HTML pricing models to machine-readable intelligent pricing (iPricing) systems, addressing manual complexity and errors in SaaS pricing management.


<details>
  <summary>Details</summary>
Motivation: SaaS market expansion has created challenges for DevOps teams managing pricing structures manually, leading to inefficiency and errors. Existing tools lack automation for pricing analysis and evolution.

Method: An LLM-based implementation (AI4Pricing2Yaml) that employs web scraping and information extraction techniques to convert static SaaS pricing HTML content into structured iPricing models with features, plans, usage limits, and add-ons.

Result: Effective validation across 30 commercial SaaS cases (30+ companies, 150+ pricings) demonstrated successful extraction of key pricing components across all transformation stages.

Conclusion: Automated iPricing transformation has significant potential to streamline SaaS pricing operations through improved consistency and scalability. Challenges like hallucinations and dynamic content handling require further research to expand system adaptability.

Abstract: The SaaS paradigm has revolutionized software distribution by offering
flexible pricing options to meet diverse customer needs. However, the rapid
expansion of the SaaS market has introduced significant complexity for DevOps
teams, who must manually manage and evolve pricing structures, an approach that
is both time-consuming and prone to errors. The absence of automated tools for
pricing analysis restricts the ability to efficiently evaluate, optimize, and
scale these models. This paper proposes leveraging intelligent pricing
(iPricing), dynamic, machine-readable pricing models, as a solution to these
challenges. Intelligent pricing enables competitive analysis, streamlines
operational decision-making, and supports continuous pricing evolution in
response to market dynamics, leading to improved efficiency and accuracy. We
present an LLM-driven approach that automates the transformation of static HTML
pricing into iPricing, significantly improving efficiency and consistency while
minimizing human error. Our implementation, AI4Pricing2Yaml, features a basic
Information Extractor that uses web scraping and LLMs technologies to extract
essential pricing components, plans, features, usage limits, and add-ons, from
SaaS websites. Validation against a dataset of 30 distinct commercial SaaS,
encompassing over 150 intelligent pricings, demonstrates the system's
effectiveness in extracting the desired elements across all steps. However,
challenges remain in addressing hallucinations, complex structures, and dynamic
content. This work highlights the potential of automating intelligent pricing
transformation to streamline SaaS pricing management, offering implications for
improved consistency and scalability in an increasingly intricate pricing
landscape. Future research will focus on refining extraction capabilities and
enhancing the system's adaptability to a wider range of SaaS websites.

</details>


### [23] [An Online A/B Testing Decision Support System for Web Usability Assessment Based on a Linguistic Decision-making Methodology: Case of Study a Virtual Learning Environment](https://arxiv.org/abs/2507.12118)
*Noe Zermeño,Cristina Zuheros,Lucas Daniel Del Rosso Calache,Francisco Herrera,Rosana Montes*

Main category: cs.SE

TL;DR: The paper introduces a methodology called Linguistic Decision-Making for Web Usability Evaluation, integrating user-centered approaches and A/B testing to assess web usability, validated through a case study on Moodle platforms.


<details>
  <summary>Details</summary>
Motivation: Current online tools lack support for comprehensive web usability evaluation involving real and fictional users in role-playing scenarios, necessitating a structured methodology for such testing.

Method: A user-centered methodology combining design thinking and linguistic decision-making, implemented within an A/B testing framework and validated via usability tests (e.g., System Usability Scale) with real users on Moodle platforms.

Result: Case study results from evaluating three Moodle platforms using the proposed methodology with real users, demonstrating its applicability and effectiveness in a practical setting.

Conclusion: The proposed methodology provides a systematic approach to web usability evaluation through A/B testing and role-playing scenarios, addressing limitations of existing tools and enhancing decision-making in user interface design.

Abstract: In recent years, attention has increasingly focused on enhancing user
satisfaction with user interfaces, spanning both mobile applications and
websites. One fundamental aspect of human-machine interaction is the concept of
web usability. In order to assess web usability, the A/B testing technique
enables the comparison of data between two designs. Expanding the scope of
tests to include the designs being evaluated, in conjunction with the
involvement of both real and fictional users, presents a challenge for which
few online tools offer support. We propose a methodology for web usability
evaluation based on user-centered approaches such as design thinking and
linguistic decision-making, named Linguistic Decision-Making for Web Usability
Evaluation. This engages people in role-playing scenarios and conducts a number
of usability tests, including the widely recognized System Usability Scale. We
incorporate the methodology into a decision support system based on A/B
testing. We use real users in a case study to assess three Moodle platforms at
the University of Guadalajara, Mexico.

</details>


### [24] [MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks](https://arxiv.org/abs/2507.12284)
*Artem Chervyakov,Alexander Kharitonov,Pavel Zadorozhny,Adamenko Pavel,Rodion Levichev,Dmitrii Vorobev,Dmitrii Salikhov,Aidar Valeev,Alena Pestova,Maria Dziuba,Ilseyar Alimova,Artem Zavgorodnev,Aleksandr Medvedev,Stanislav Moiseev,Elena Bruches,Daniil Grebenkin,Roman Derunets,Vikulov Vladimir,Anton Emelyanov,Dmitrii Babaev,Vladimir V. Ivanov,Valentin Malykh,Alena Fenogenova*

Main category: cs.SE

TL;DR: Introduces MERA Code, a Russian-focused benchmark for evaluating code generation LLMs, emphasizing practical coding skills across 8 languages to address gaps in current assessments.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations for LLMs in code generation prioritize natural language tasks and high-level reasoning, neglecting executable code quality and real-world performance in non-English contexts.

Method: The benchmark features an open-source codebase, language-agnostic scoring system, and a platform with a leaderboard and submission process for 11 assessment tasks.

Result: Analysis reveals limitations of open LLMs and frontier API models in practical non-English coding tasks, demonstrating the benchmark's effectiveness in identifying model weaknesses.

Conclusion: MERA Code advances standardized evaluation of code generation LLMs in Russian, fosters research into real-world coding capabilities, and highlights challenges in non-English development.

Abstract: Advancements in LLMs have enhanced task automation in software engineering;
however, current evaluations primarily focus on natural language tasks,
overlooking code quality. Most benchmarks prioritize high-level reasoning over
executable code and real-world performance, leaving gaps in understanding true
capabilities and risks associated with these models in production. To address
this issue, we propose MERA Code, a new addition to the MERA benchmark family,
specifically focused on evaluating code for the latest code generation LLMs in
Russian. This benchmark includes 11 evaluation tasks that span 8 programming
languages. Our proposed evaluation methodology features a taxonomy that
outlines the practical coding skills necessary for models to complete these
tasks. The benchmark comprises an open-source codebase for users to conduct
MERA assessments, a scoring system compatible with various programming
environments, and a platform featuring a leaderboard and submission system. We
evaluate open LLMs and frontier API models, analyzing their limitations in
terms of practical coding tasks in non-English languages. We are publicly
releasing MERA to guide future research, anticipate groundbreaking features in
model development, and standardize evaluation procedures.

</details>


### [25] [GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities](https://arxiv.org/abs/2507.12367)
*Diganta Misra,Nizar Islah,Victor May,Brice Rauby,Zihan Wang,Justine Gehring,Antonio Orvieto,Muawiz Chaudhary,Eilif B. Muller,Irina Rish,Samira Ebrahimi Kahou,Massimo Caccia*

Main category: cs.SE

TL;DR: GitChameleon is a curated dataset for version-conditioned Python code generation tasks, featuring 328 code completion problems with executable unit tests to benchmark functional accuracy across library versions. Current systems achieve only 48-51% baseline success rates, highlighting the complexity of version-adaptive code generation.


<details>
  <summary>Details</summary>
Motivation: Software libraries evolve rapidly, requiring code generation tools to adapt to frequent version updates without compromising backward compatibility. Existing benchmarks lack execution-based evaluations for version-compliant code generation, hindering progress in this critical area.

Method: Developed GitChameleon dataset with meticulously selected Python code problems conditioned on specific library versions, paired with unit tests that can be executed to verify correctness. Evaluated performance of LLMs, AI agents, code assistants, and RAG systems through execution-based validation.

Result: State-of-the-art systems show significant limitations in version-conditioned code generation, with even enterprise models achieving only 48-51% success rates, demonstrating the inherent difficulty of this task.

Conclusion: GitChameleon establishes a robust execution-based benchmark for version-aware code generation, promoting development of more adaptable methods. Public release of the dataset and evaluation code enables systematic research into improving AI code generation under library version constraints.

Abstract: The rapid evolution of software libraries poses a considerable hurdle for
code generation, necessitating continuous adaptation to frequent version
updates while preserving backward compatibility. While existing code evolution
benchmarks provide valuable insights, they typically lack execution-based
evaluation for generating code compliant with specific library versions. To
address this, we introduce GitChameleon, a novel, meticulously curated dataset
comprising 328 Python code completion problems, each conditioned on specific
library versions and accompanied by executable unit tests. GitChameleon
rigorously evaluates the capacity of contemporary large language models (LLMs),
LLM-powered agents, code assistants, and RAG systems to perform
version-conditioned code generation that demonstrates functional accuracy
through execution. Our extensive evaluations indicate that state-of-the-art
systems encounter significant challenges with this task; enterprise models
achieving baseline success rates in the 48-51\% range, underscoring the
intricacy of the problem. By offering an execution-based benchmark emphasizing
the dynamic nature of code libraries, GitChameleon enables a clearer
understanding of this challenge and helps guide the development of more
adaptable and dependable AI code generation methods. We make the dataset and
evaluation code publicly available at
https://github.com/mrcabbage972/GitChameleonBenchmark.

</details>


### [26] [SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?](https://arxiv.org/abs/2507.12415)
*Xinyi He,Qian Liu,Mingzhe Du,Lin Yan,Zhijie Fan,Yiming Huang,Zejian Yuan,Zejun Ma*

Main category: cs.SE

TL;DR: SWE-Perf evaluates LLMs in code performance optimization for real-world repositories, revealing capability gaps with expert level.


<details>
  <summary>Details</summary>
Motivation: Explores the underappreciated role of LLMs in repository-level code performance optimization, essential for production systems.

Method: Introduced SWE-Perf, a benchmark with 140 instances from GitHub performance PRs; evaluated file-level and repo-level methods (Agentless, OpenHands).

Result: LLMs show significant capability gaps compared to expert optimization performance in repository contexts.

Conclusion: SWE-Perf enables systematic study of LLMs for code perf optimization, highlighting critical research directions in this emerging field.

Abstract: Code performance optimization is paramount in real-world software engineering
and critical for production-level systems. While Large Language Models (LLMs)
have demonstrated impressive capabilities in code generation and bug fixing,
their proficiency in enhancing code performance at the repository level remains
largely unexplored. To address this gap, we introduce SWE-Perf, the first
benchmark specifically designed to systematically evaluate LLMs on code
performance optimization tasks within authentic repository contexts. SWE-Perf
comprises 140 carefully curated instances, each derived from
performance-improving pull requests from popular GitHub repositories. Each
benchmark instance includes the relevant codebase, target functions,
performance-related tests, expert-authored patches, and executable
environments. Through a comprehensive evaluation of representative methods that
span file-level and repo-level approaches (e.g., Agentless and OpenHands), we
reveal a substantial capability gap between existing LLMs and expert-level
optimization performance, highlighting critical research opportunities in this
emerging field.

</details>
