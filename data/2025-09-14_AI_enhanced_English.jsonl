{"id": "2509.08992", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08992", "abs": "https://arxiv.org/abs/2509.08992", "authors": ["Anqi Chen", "Riccardo Preatoni", "Alessandro Brighente", "Mauro Conti", "Cristina Nita-Rotaru"], "title": "Cross-Service Token: Finding Attacks in 5G Core Networks", "comment": null, "summary": "5G marks a major departure from previous cellular architectures, by\ntransitioning from a monolithic design of the core network to a Service-Based\nArchitecture (SBA) where services are modularized as Network Functions (NFs)\nwhich communicate with each other via standard-defined HTTP-based APIs called\nService-Based Interfaces (SBIs). These NFs are deployed in private and public\ncloud infrastructure, and an access control framework based on OAuth restricts\nhow they communicate with each other and obtain access to resources. Given the\nincreased vulnerabilities of clouds to insiders, it is important to study the\nsecurity of the 5G Core services for vulnerabilities that allow attackers to\nuse compromised NFs to obtain unauthorized access to resources.\n  We present FivGeeFuzz, a grammar-based fuzzing framework designed to uncover\nsecurity flaws in 5G core SBIs. FivGeeFuzz automatically derives grammars from\n3GPP API specifications to generate malformed, unexpected, or semantically\ninconsistent inputs, and it integrates automated bug detection with manual\nvalidation and root-cause analysis. We evaluate our approach on free5GC, the\nonly open-source 5G core implementing Release 17-compliant SBIs with an access\ncontrol mechanism. Using FivGeeFuzz, we discovered 8 previously unknown\nvulnerabilities in free5GC, leading to runtime crashes, improper error\nhandling, and unauthorized access to resources, including a very severe attack\nwe call Cross-Service Token Attack. All bugs were confirmed by the free5GC\nteam, 7 have already been patched, and the remaining one has a patch under\ndevelopment.", "AI": {"tldr": "FivGeeFuzz, a fuzzing framework for 5G core interfaces, discovers 8 critical security vulnerabilities in free5GC, highlighting risks in service-based architectures and validating the framework's effectiveness.", "motivation": "The transition to 5G's Service-Based Architecture (SBA) introduces new security challenges due to modularized Network Functions (NFs) deployed in vulnerable cloud environments, necessitating tools to detect insider threats and unauthorized access risks.", "method": "The authors developed FivGeeFuzz, a grammar-based fuzzing framework that automatically derives grammars from 3GPP API specifications to generate malformed inputs, integrates automated bug detection, and supports manual validation and root-cause analysis for 5G core SBIs.", "result": "FivGeeFuzz identified 8 novel vulnerabilities in free5GC, including a severe Cross-Service Token Attack, with 7 patched and 1 under resolution. These flaws caused crashes, improper error handling, and unauthorized resource access.", "conclusion": "This paper demonstrates the effectiveness of FivGeeFuzz in uncovering critical security vulnerabilities in 5G core Service-Based Interfaces, emphasizing the necessity of rigorous security testing for cloud-native, service-oriented architectures."}}
{"id": "2509.08995", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08995", "abs": "https://arxiv.org/abs/2509.08995", "authors": ["Sichen Zhu", "Hoyeung Leung", "Xiaoyi Wang", "Jia Wei", "Honghui Xu"], "title": "When FinTech Meets Privacy: Securing Financial LLMs with Differential Private Fine-Tuning", "comment": null, "summary": "The integration of Large Language Models (LLMs) into financial technology\n(FinTech) has revolutionized the analysis and processing of complex financial\ndata, driving advancements in real-time decision-making and analytics. With the\ngrowing trend of deploying AI models on edge devices for financial\napplications, ensuring the privacy of sensitive financial data has become a\nsignificant challenge. To address this, we propose DPFinLLM, a\nprivacy-enhanced, lightweight LLM specifically designed for on-device financial\napplications. DPFinLLM combines a robust differential privacy mechanism with a\nstreamlined architecture inspired by state-of-the-art models, enabling secure\nand efficient processing of financial data. This proposed DPFinLLM can not only\nsafeguard user data from privacy breaches but also ensure high performance\nacross diverse financial tasks. Extensive experiments on multiple financial\nsentiment datasets validate the effectiveness of DPFinLLM, demonstrating its\nability to achieve performance comparable to fully fine-tuned models, even\nunder strict privacy constraints.", "AI": {"tldr": "DPFinLLM is a privacy-enhanced, lightweight Large Language Model (LLM) designed for on-device financial applications, combining differential privacy with a streamlined architecture to balance data security and performance on complex tasks.", "motivation": "The growing adoption of AI in FinTech has raised significant privacy concerns, particularly when deploying models on edge devices, where sensitive financial data is processed locally and exposed to potential breaches.", "method": "The paper introduces DPFinLLM, a model that integrates a robust differential privacy (DP) mechanism with a compact, efficient architecture inspired by state-of-the-art LLMs. This approach allows the model to securely process financial data on edge devices while maintaining performance.", "result": "Extensive experiments on financial sentiment datasets demonstrate that DPFinLLM achieves performance comparable to fully fine-tuned models under strict privacy constraints.", "conclusion": "DPFinLLM offers a viable solution for privacy-preserving, high-performance LLM deployment in financial technology, particularly for edge devices, by effectively balancing security and efficiency through its design."}}
{"id": "2509.09089", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09089", "abs": "https://arxiv.org/abs/2509.09089", "authors": ["Mengfei Xie", "Yan Lin", "Hongtao Wu", "Jianming Fu", "Chenke Luo", "Guojun Peng"], "title": "Beyond Tag Collision: Cluster-based Memory Management for Tag-based Sanitizers", "comment": "This paper has been accepted to the 2025 ACM SIGSAC Conference on\n  Computer and Communications Security (CCS'25)", "summary": "Tag-based sanitizers attach a small \"key\" to each pointer and a matching\n\"lock\" tag to its target memory object, enabling runtime verification of\npointer-object consistency and helping developers to detect potential memory\nviolations. However, the limited tag encoding space challenges existing studies\nin assigning distinct tags to memory objects across temporal and spatial\ndimensions, leading to potential tag collisions. In this paper, we present\nClusterTag, a novel cluster-based memory allocator aimed at simultaneously\nmitigating tag collisions in both temporal and spatial dimensions. The core\ndesign of ClusterTag effectively balances the significant mismatch between tag\nencoding space and memory objects: it divides memory objects into multiple\nindependent clusters, thereby limiting tag collisions to finite chunks within\neach cluster. To mitigate tag collisions across clusters, we design a\ncluster-grained heap randomization scheme. This approach introduces random\naddress intervals between clusters and further breaks the entropy limitation of\nthe tag space. ClusterTag has been implemented as an independent memory\nallocator that seamlessly integrates with tag-based sanitizers such as HWASan,\nand maintains comparable performance overhead (within 1%) at various\nrandomization densities. Security evaluations on the Juliet dataset indicate\nthat ClusterTag exhibits deterministic results across 500 repeated tests (5,652\nreported and 1,530 missed), while the existing three types of tag assignment\nstrategies all exhibit probabilistic false negatives due to tag collisions.\nQuantitative analysis across three tag collision distance metrics-minimum,\naverage, and unpredictability-demonstrates that ClusterTag achieves balanced\nimprovements across all three, whereas prior tag assignment schemes (random,\nstaggered, fixed) show significant trade-offs in at least one metric.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.09091", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09091", "abs": "https://arxiv.org/abs/2509.09091", "authors": ["Honglan Yu", "Yibin Wang", "Feifei Dai", "Dong Liu", "Haihui Fan", "Xiaoyan Gu"], "title": "Towards Confidential and Efficient LLM Inference with Dual Privacy Protection", "comment": "Accepted by DASFAA2025", "summary": "CPU-based trusted execution environments (TEEs) and differential privacy (DP)\nhave gained wide applications for private inference. Due to high inference\nlatency in TEEs, researchers use partition-based approaches that offload linear\nmodel components to GPUs. However, dense nonlinear layers of large language\nmodels (LLMs) result in significant communication overhead between TEEs and\nGPUs. DP-based approaches apply random noise to protect data privacy, but this\ncompromises LLM performance and semantic understanding. To overcome the above\ndrawbacks, this paper proposes CMIF, a Confidential and efficient Model\nInference Framework. CMIF confidentially deploys the embedding layer in the\nclient-side TEE and subsequent layers on GPU servers. Meanwhile, it optimizes\nthe Report-Noisy-Max mechanism to protect sensitive inputs with a slight\ndecrease in model performance. Extensive experiments on Llama-series models\ndemonstrate that CMIF reduces additional inference overhead in TEEs while\npreserving user data privacy.", "AI": {"tldr": "CMIF addresses private LLM inference by partitioning model layers across TEEs and GPUs, with optimized privacy mechanisms to reduce overhead while preserving performance and confidentiality.", "motivation": "CPU-based TEEs suffer from high latency for nonlinear LLM layers, and DP-based methods degrade performance. Existing partitioning approaches cannot efficiently handle the communication overhead of large models.", "method": "CMIF confidentially deploys the embedding layer in a client-side TEE and offloads subsequent layers to GPU servers. It also optimizes the Report-Noisy-Max mechanism to enhance privacy with minimal performance loss.", "result": "Experiments on Llama-series models show CMIF reduces TEE inference overhead while maintaining data privacy, achieving both efficiency and security.", "conclusion": "CMIF offers an efficient and confidential solution for private LLM inference, balancing performance and security effectively."}}
{"id": "2509.08843", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08843", "abs": "https://arxiv.org/abs/2509.08843", "authors": ["Sidney Shapiro"], "title": "Pattern-Based File and Data Access with Python Glob: A Comprehensive Guide for Computational Research", "comment": null, "summary": "Pattern-based file access is a fundamental but often under-documented aspect\nof computational research. The Python glob module provides a simple yet\npowerful way to search, filter, and ingest files using wildcard patterns,\nenabling scalable workflows across disciplines. This paper introduces glob as a\nversatile tool for data science, business analytics, and artificial\nintelligence applications. We demonstrate use cases including large-scale data\ningestion, organizational data analysis, AI dataset construction, and\nreproducible research practices. Through concrete Python examples with widely\nused libraries such as pandas,scikit-learn, and matplotlib, we show how glob\nfacilitates efficient file traversal and integration with analytical pipelines.\nBy situating glob within the broader context of reproducible research and data\nengineering, we highlight its role as a methodological building block. Our goal\nis to provide researchers and practitioners with a concise reference that\nbridges foundational concepts and applied practice, making glob a default\ncitation for file pattern matching in Python-based research workflows.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.09097", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09097", "abs": "https://arxiv.org/abs/2509.09097", "authors": ["Honghui Xu", "Shiva Shrestha", "Wei Chen", "Zhiyuan Li", "Zhipeng Cai"], "title": "DP-FedLoRA: Privacy-Enhanced Federated Fine-Tuning for On-Device Large Language Models", "comment": null, "summary": "As on-device large language model (LLM) systems become increasingly\nprevalent, federated fine-tuning enables advanced language understanding and\ngeneration directly on edge devices; however, it also involves processing\nsensitive, user-specific data, raising significant privacy concerns within the\nfederated learning framework. To address these challenges, we propose\nDP-FedLoRA, a privacy-enhanced federated fine-tuning framework that integrates\nLoRA-based adaptation with differential privacy in a communication-efficient\nsetting. Each client locally clips and perturbs its LoRA matrices using\nGaussian noise to satisfy ($\\epsilon$, $\\delta$)-differential privacy. We\nfurther provide a theoretical analysis demonstrating the unbiased nature of the\nupdates and deriving bounds on the variance introduced by noise, offering\npractical guidance for privacy-budget calibration. Experimental results across\nmainstream benchmarks show that DP-FedLoRA delivers competitive performance\nwhile offering strong privacy guarantees, paving the way for scalable and\nprivacy-preserving LLM deployment in on-device environments.", "AI": {"tldr": "DP-FedLoRA addresses privacy risks in federated LLM fine-tuning by combining LoRA adapters with differential privacy, achieving robust performance and privacy through Gaussian noise and unbiased update analysis.", "motivation": "Federated fine-tuning of on-device LLMs poses significant privacy risks due to sensitive user data processing within federated learning frameworks, necessitating privacy-enhanced solutions.", "method": "Integrates LoRA-based adaptation with differential privacy, using Gaussian noise to perturb local LoRA matrices, along with theoretical analysis of update bias and variance bounds for privacy-budget calibration.", "result": "Delivers competitive performance across mainstream benchmarks while satisfying ($\\epsilon$, $\\delta$)-differential privacy, demonstrating strong privacy guarantees without compromising scalability.", "conclusion": "DP-FedLoRA is a scalable, privacy-preserving framework for on-device LLM deployment, enabling secure federated fine-tuning with strong privacy guarantees while maintaining competitive performance."}}
{"id": "2509.08857", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.08857", "abs": "https://arxiv.org/abs/2509.08857", "authors": ["Marcelino Garcia", "Renato Garcia", "Arthur Parizotto", "Andre Mendes", "Pedro Valle", "Ricardo Vilela", "Renato Balancieri", "Williamson Silva"], "title": "A Systematic Mapping Study on Chatbots in Programming Education", "comment": "18 pages, 1 figure, 3 tables", "summary": "Educational chatbots have gained prominence as support tools for teaching\nprogramming, particularly in introductory learning contexts. This paper\npresents a Systematic Mapping Study (SMS) that investigated how such agents\nhave been developed and applied in programming education. From an initial set\nof 3,216 publications, 54 studies were selected and analyzed based on five\nresearch subquestions, addressing chatbot types, programming languages used,\neducational content covered, interaction models, and application contexts. The\nresults reveal a predominance of chatbots designed for Python instruction,\nfocusing on fundamental programming concepts, and employing a wide variety of\npedagogical approaches and technological architectures. In addition to\nidentifying trends and gaps in the literature, this study provides insights to\ninform the development of new educational tools for programming instruction.", "AI": {"tldr": "This SMS analyzes 54 studies to map educational chatbots in programming education, revealing Python\u2019s prominence, foundational content, and pedagogical diversity while highlighting research gaps to guide future tool development.", "motivation": "Educational chatbots are increasingly used in programming education, necessitating an analysis of their development and application to guide future tool design.", "method": "A Systematic Mapping Study (SMS) was conducted, analyzing 54 studies out of 3,216 publications using five research subquestions related to chatbot types, languages, interaction models, and application contexts.", "result": "Most chatbots target Python instruction, focus on foundational programming concepts, and utilize diverse pedagogical strategies and architectures. Key trends and gaps were identified.", "conclusion": "The study provides insights into trends and gaps in educational chatbots for programming, aiming to inform the development of new educational tools."}}
{"id": "2509.09103", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09103", "abs": "https://arxiv.org/abs/2509.09103", "authors": ["Chanti Raju Mylay", "Bobin Deng", "Zhipeng Cai", "Honghui Xu"], "title": "AgriSentinel: Privacy-Enhanced Embedded-LLM Crop Disease Alerting System", "comment": null, "summary": "Crop diseases pose significant threats to global food security, agricultural\nproductivity, and sustainable farming practices, directly affecting farmers'\nlivelihoods and economic stability. To address the growing need for effective\ncrop disease management, AI-based disease alerting systems have emerged as\npromising tools by providing early detection and actionable insights for timely\nintervention. However, existing systems often overlook critical aspects such as\ndata privacy, market pricing power, and farmer-friendly usability, leaving\nfarmers vulnerable to privacy breaches and economic exploitation. To bridge\nthese gaps, we propose AgriSentinel, the first Privacy-Enhanced Embedded-LLM\nCrop Disease Alerting System. AgriSentinel incorporates a differential privacy\nmechanism to protect sensitive crop image data while maintaining classification\naccuracy. Its lightweight deep learning-based crop disease classification model\nis optimized for mobile devices, ensuring accessibility and usability for\nfarmers. Additionally, the system includes a fine-tuned, on-device large\nlanguage model (LLM) that leverages a curated knowledge pool to provide farmers\nwith specific, actionable suggestions for managing crop diseases, going beyond\nsimple alerting. Comprehensive experiments validate the effectiveness of\nAgriSentinel, demonstrating its ability to safeguard data privacy, maintain\nhigh classification performance, and deliver practical, actionable disease\nmanagement strategies. AgriSentinel offers a robust, farmer-friendly solution\nfor automating crop disease alerting and management, ultimately contributing to\nimproved agricultural decision-making and enhanced crop productivity.", "AI": {"tldr": "The paper introduces AgriSentinel, the first Privacy-Enhanced Embedded-LLM Crop Disease Alerting System. It combines differential privacy for data protection with a personalized, on-device large language model to provide farmers with characteristics regarding early detection of recommendation and crop disease intellectual, ensuring both privacy and actionable insights.", "motivation": "The motivation stems from the significant impact of crop diseases on agriculture and the limitations of current AI alert systems in terms of data privacy, market pricing protection, and usability for farmers. There is a need for a system that ensures privacy without compromising classification accuracy and usability.", "method": "AgriSentinel employs a light-weight deep learning-based crop disease classification model optimized for mobile devices and integrates an on-device large language model (LLM) for recommendations. A data privacy protection process provides personalized, private disease management strategies and a market pricing mechanism.", "result": "Experiments show that AgriSentinel can protect data privacy, maintain high classification performance, and deliver practical disease management solutions. It demonstrates effectiveness and showcases high performance in managing crop diseases with privacy and usability together.", "conclusion": "AgriSentinel successfully addresses major gaps in crop disease management, offering a robust solution for privacy-enhanced AI systems in agriculture. This can lead to improved agricultural decision-making and increased productivity for greater awareness."}}
{"id": "2509.08863", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08863", "abs": "https://arxiv.org/abs/2509.08863", "authors": ["Qianqian Luo", "Liuchang Xu", "Qingming Lin", "Sensen Wu", "Ruichen Mao", "Chao Wang", "Hailin Feng", "Bo Huang", "Zhenhong Du"], "title": "GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation", "comment": null, "summary": "LLMs have made substantial progress in task automation and natural language\nunderstanding.However,without expertise in GIS,they continue to encounter\nlimitations.To address these issues, we propose GeoJSON Agents-a multi-agent\nLLM architecture.This framework transforms natural language tasks into\nstructured GeoJSON operation commands and processes spatial data using two\nwidely adopted LLM enhancement techniques:Function Calling and Code\nGeneration.The architecture consists of three components-task parsing,agent\ncollaboration,and result integration-aimed at enhancing both the performance\nand scalability of GIS automation.The Planner agent interprets natural language\ntasks into structured GeoJSON commands.Then,specialized Worker agents\ncollaborate according to assigned roles to perform spatial data processing and\nanalysis,either by invoking predefined function APIs or by dynamically\ngenerating and executing Python-based spatial analysis code.Finally,the system\nintegrates the outputs from multiple execution rounds into\nreusable,standards-compliant GeoJSON files.To systematically evaluate the\nperformance of the two approaches,we constructed a benchmark dataset of 70\ntasks with varying complexity and conducted experiments using OpenAI's GPT-4o\nas the core model.Results indicate that the Function Calling-based GeoJSON\nAgent achieved an accuracy of 85.71%,while the Code Generation-based agent\nreached 97.14%,both significantly outperforming the best-performing\ngeneral-purpose model (48.57%).Further analysis reveals that the Code\nGeneration provides greater flexibility,whereas the Function Calling approach\noffers more stable execution.This study is the first to introduce an LLM\nmulti-agent framework for GeoJSON data and to compare the strengths and\nlimitations of two mainstream LLM enhancement methods,offering new perspectives\nfor improving GeoAI system performance.", "AI": {"tldr": "This paper proposes GeoJSON Agents, a multi-agent LLM architecture that enhances GIS automation by converting natural language tasks into structured GeoJSON operations using Function Calling and Code Generation techniques, achieving significant performance improvements over general-purpose models.", "motivation": "Large Language Models (LLMs) face limitations in GIS tasks without specialized expertise, necessitating a framework to bridge this gap and improve automation capabilities for spatial data processing.", "method": "The architecture comprises three modules: (1) a Planner agent parsing tasks into GeoJSON commands, (2) Worker agents executing spatial analysis via Function Calling or Code Generation, and (3) integrating results into standards-compliant GeoJSON files. Evaluated using a 70-task benchmark and GPT-4o.", "result": "Function Calling achieved 85.71% accuracy, Code Generation 97.14%, both surpassing general-purpose models (48.57%). Code Generation shows greater flexibility while Function Calling provides stable execution.", "conclusion": "This work introduces the first LLM multi-agent framework for GeoJSON data, systematically comparing Function Calling and Code Generation approaches, offering novel insights for optimizing GeoAI system performance."}}
{"id": "2509.09107", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09107", "abs": "https://arxiv.org/abs/2509.09107", "authors": ["Pritam Sen", "Yao Ma", "Cristian Borcea"], "title": "CryptGNN: Enabling Secure Inference for Graph Neural Networks", "comment": null, "summary": "We present CryptGNN, a secure and effective inference solution for\nthird-party graph neural network (GNN) models in the cloud, which are accessed\nby clients as ML as a service (MLaaS). The main novelty of CryptGNN is its\nsecure message passing and feature transformation layers using distributed\nsecure multi-party computation (SMPC) techniques. CryptGNN protects the\nclient's input data and graph structure from the cloud provider and the\nthird-party model owner, and it protects the model parameters from the cloud\nprovider and the clients. CryptGNN works with any number of SMPC parties, does\nnot require a trusted server, and is provably secure even if P-1 out of P\nparties in the cloud collude. Theoretical analysis and empirical experiments\ndemonstrate the security and efficiency of CryptGNN.", "AI": {"tldr": "CryptGNN is a secure GNN inference framework for MLaaS in the cloud, using SMPC to protect data, graph structure, and model parameters. It achieves provable security against P-1 colluding parties without requiring trusted servers.", "motivation": "Clients using cloud-based GNN models face privacy risks: cloud providers could leak input data/graph structures, and model owners could expose parameters. Existing solutions fail to address collusion resistance or require trusted components.", "method": "CryptGNN implements secure message passing/feature transformation via distributed SMPC protocols. It distributes computation across multiple parties, ensures confidentiality through cryptographic guarantees, and requires no trusted server. Security holds under P-1-party collusion.", "result": "Theoretical analysis proves security against collusion. Empirical evaluation shows practical efficiency for GNN inference, demonstrating feasibility of secure MLaaS for graph data without performance bottlenecks.", "conclusion": "CryptGNN establishes a robust, collusion-resilient framework for secure GNN inference in untrusted cloud environments, enabling privacy-preserving MLaaS for graph-structured data without sacrificing model availability or requiring hardware trust assumptions."}}
{"id": "2509.08865", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08865", "abs": "https://arxiv.org/abs/2509.08865", "authors": ["Guangyu Zhang", "Xixuan Wang", "Shiyu Sun", "Peiyan Xiao", "Kun Sun", "Yanhai Xiong"], "title": "TraceRAG: A LLM-Based Framework for Explainable Android Malware Detection and Behavior Analysis", "comment": null, "summary": "Sophisticated evasion tactics in malicious Android applications, combined\nwith their intricate behavioral semantics, enable attackers to conceal\nmalicious logic within legitimate functions, underscoring the critical need for\nrobust and in-depth analysis frameworks. However, traditional analysis\ntechniques often fail to recover deeply hidden behaviors or provide\nhuman-readable justifications for their decisions. Inspired by advances in\nlarge language models (LLMs), we introduce TraceRAG, a retrieval-augmented\ngeneration (RAG) framework that bridges natural language queries and Java code\nto deliver explainable malware detection and analysis. First, TraceRAG\ngenerates summaries of method-level code snippets, which are indexed in a\nvector database. At query time, behavior-focused questions retrieve the most\nsemantically relevant snippets for deeper inspection. Finally, based on the\nmulti-turn analysis results, TraceRAG produces human-readable reports that\npresent the identified malicious behaviors and their corresponding code\nimplementations. Experimental results demonstrate that our method achieves 96\\%\nmalware detection accuracy and 83.81\\% behavior identification accuracy based\non updated VirusTotal (VT) scans and manual verification. Furthermore, expert\nevaluation confirms the practical utility of the reports generated by TraceRAG.", "AI": {"tldr": "TraceRAG introduces a RAG-based framework for explainable Android malware detection by connecting natural language queries to code semantics, achieving state-of-the-art accuracy and producing actionable threat reports through multi-step analysis.", "motivation": "Malicious Android apps use evasion tactics to hide within legitimate code, while traditional analysis methods lack deep behavioral visibility and fail to provide human-readable explanations for their determinations.", "method": "TraceRAG generates method-level Java code summaries indexed in a vector database, retrieves semantically relevant snippets for behavioral analysis, and synthesizes multi-turn inspection results into explainable reports through LLM-based reasoning.", "result": "96% malware detection accuracy and 83.81% behavior identification accuracy validated through VirusTotal scans and manual verification, with expert-validated report utility for real-world malware analysis.", "conclusion": "TraceRAG effectively addresses the limitations of traditional malware analysis by combining retrieval-augmented generation with code semantics, achieving high detection accuracy and producing human-readable reports for transparent threat analysis."}}
{"id": "2509.09112", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09112", "abs": "https://arxiv.org/abs/2509.09112", "authors": ["Zhaoxi Zhang", "Xiaomei Zhang", "Yanjun Zhang", "He Zhang", "Shirui Pan", "Bo Liu", "Asif Qumer Gill", "Leo Yu Zhang"], "title": "Character-Level Perturbations Disrupt LLM Watermarks", "comment": null, "summary": "Large Language Model (LLM) watermarking embeds detectable signals into\ngenerated text for copyright protection, misuse prevention, and content\ndetection. While prior studies evaluate robustness using watermark removal\nattacks, these methods are often suboptimal, creating the misconception that\neffective removal requires large perturbations or powerful adversaries.\n  To bridge the gap, we first formalize the system model for LLM watermark, and\ncharacterize two realistic threat models constrained on limited access to the\nwatermark detector. We then analyze how different types of perturbation vary in\ntheir attack range, i.e., the number of tokens they can affect with a single\nedit. We observe that character-level perturbations (e.g., typos, swaps,\ndeletions, homoglyphs) can influence multiple tokens simultaneously by\ndisrupting the tokenization process. We demonstrate that character-level\nperturbations are significantly more effective for watermark removal under the\nmost restrictive threat model. We further propose guided removal attacks based\non the Genetic Algorithm (GA) that uses a reference detector for optimization.\nUnder a practical threat model with limited black-box queries to the watermark\ndetector, our method demonstrates strong removal performance. Experiments\nconfirm the superiority of character-level perturbations and the effectiveness\nof the GA in removing watermarks under realistic constraints. Additionally, we\nargue there is an adversarial dilemma when considering potential defenses: any\nfixed defense can be bypassed by a suitable perturbation strategy. Motivated by\nthis principle, we propose an adaptive compound character-level attack.\nExperimental results show that this approach can effectively defeat the\ndefenses. Our findings highlight significant vulnerabilities in existing LLM\nwatermark schemes and underline the urgency for the development of new robust\nmechanisms.", "AI": {"tldr": "This paper demonstrates that LLM watermarks are vulnerable to simple, character-level perturbations and GA-guided attacks, showing current defenses can be bypassed with minimal edits. It calls for new robust watermarking techniques to secure AI-generated content.", "motivation": "Previous watermark removal attacks were deemed ineffective due to suboptimal methods, creating a misconception that large perturbations or powerful adversaries are required. This study challenges that view by demonstrating efficient removal with minimal changes under realistic constraints.", "method": "The authors formalize threat models for LLM watermarking, analyze perturbation types (focusing on character-level attacks), and propose a Genetic Algorithm (GA)-guided removal method optimized with a reference detector. They also introduce an adaptive compound attack to counter defenses.", "result": "Experiments confirm character-level perturbations disrupt tokenization more effectively than other methods. The GA-based attacks achieve strong removal performance with limited detector queries. The proposed adaptive attacks successfully bypass defensive strategies.", "conclusion": "The paper highlights vulnerabilities in existing LLM watermarking schemes, showing that character-level perturbations and guided attacks can effectively remove watermarks under realistic constraints. It emphasizes the need for robust, adaptive watermarking mechanisms to address these security gaps."}}
{"id": "2509.08867", "categories": ["cs.SE", "cs.AI", "68T01", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.08867", "abs": "https://arxiv.org/abs/2509.08867", "authors": ["K. Pronk", "Q. Zhao"], "title": "Benchmarking Energy Efficiency of Large Language Models Using vLLM", "comment": "6 pages, 6 figures", "summary": "The prevalence of Large Language Models (LLMs) is having an growing impact on\nthe climate due to the substantial energy required for their deployment and\nuse. To create awareness for developers who are implementing LLMs in their\nproducts, there is a strong need to collect more information about the energy\nefficiency of LLMs. While existing research has evaluated the energy efficiency\nof various models, these benchmarks often fall short of representing realistic\nproduction scenarios. In this paper, we introduce the LLM Efficiency Benchmark,\ndesigned to simulate real-world usage conditions. Our benchmark utilizes vLLM,\na high-throughput, production-ready LLM serving backend that optimizes model\nperformance and efficiency. We examine how factors such as model size,\narchitecture, and concurrent request volume affect inference energy efficiency.\nOur findings demonstrate that it is possible to create energy efficiency\nbenchmarks that better reflect practical deployment conditions, providing\nvaluable insights for developers aiming to build more sustainable AI systems.", "AI": {"tldr": "To address gaps in LLM energy efficiency benchmarks, the paper introduces a production-focused benchmark using vLLM, showing how model design and usage patterns affect sustainability and offering tools for developers to optimize energy consumption.", "motivation": "Current energy efficiency benchmarks for LLMs fail to simulate realistic production scenarios, necessitating practical tools to guide developers in reducing the environmental impact of their AI systems.", "method": "The authors developed the LLM Efficiency Benchmark using vLLM, a production-ready backend, to evaluate energy efficiency under factors like model size, architecture, and concurrent request volume.", "result": "Findings demonstrate that the proposed benchmark effectively simulates real-world deployment conditions, providing developers with data to improve the energy efficiency of LLMs.", "conclusion": "The study concludes that creating energy efficiency benchmarks for LLMs that reflect real-world deployment conditions is feasible, offering actionable insights for developers to build sustainable AI systems."}}
{"id": "2509.09158", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09158", "abs": "https://arxiv.org/abs/2509.09158", "authors": ["Priyanka Rushikesh Chaudhary", "Rajib Ranjan Maiti"], "title": "IoTFuzzSentry: A Protocol Guided Mutation Based Fuzzer for Automatic Vulnerability Testing in Commercial IoT Devices", "comment": null, "summary": "Protocol fuzzing is a scalable and cost-effective technique for identifying\nsecurity vulnerabilities in deployed Internet of Things devices. During their\noperational phase, IoT devices often run lightweight servers to handle user\ninteractions, such as video streaming or image capture in smart cameras.\nImplementation flaws in transport or application-layer security mechanisms can\nexpose IoT devices to a range of threats, including unauthorized access and\ndata leakage. This paper addresses the challenge of uncovering such\nvulnerabilities by leveraging protocol fuzzing techniques that inject crafted\ntransport and application-layer packets into IoT communications. We present a\nmutation-based fuzzing tool, named IoTFuzzSentry, to identify specific\nnon-trivial vulnerabilities in commercial IoT devices. We further demonstrate\nhow these vulnerabilities can be exploited in real-world scenarios. We\nintegrated our fuzzing tool into a well-known testing tool Cotopaxi and\nevaluated it with commercial-off-the-shelf IoT devices such as IP cameras and\nSmart Plug. Our evaluation revealed vulnerabilities categorized into 4 types\n(IoT Access Credential Leakage, Sneak IoT Live Video Stream, Creep IoT Live\nImage, IoT Command Injection) and we show their exploits using three IoT\ndevices. We have responsibly disclosed all these vulnerabilities to the\nrespective vendors. So far, we have published two CVEs, CVE-2024-41623 and\nCVE-2024-42531, and one is awaiting. To extend the applicability, we have\ninvestigated the traffic of six additional IoT devices and our analysis shows\nthat these devices can have similar vulnerabilities, due to the presence of a\nsimilar set of application protocols. We believe that IoTFuzzSentry has the\npotential to discover unconventional security threats and allow IoT vendors to\nstrengthen the security of their commercialized IoT devices automatically with\nnegligible overhead.", "AI": {"tldr": "The paper introduces IoTFuzzSentry, a fuzzing tool for uncovering IoT security flaws. It identifies four vulnerability types in commercial devices, publishes CVEs, and demonstrates real-world exploits, showing potential for scalable IoT security improvement.", "motivation": "The paper addresses the critical need to uncover security vulnerabilities in IoT devices during their operational phase, where implementation flaws in security mechanisms can lead to unauthorized access and data leakage.", "method": "The authors developed IoTFuzzSentry, a mutation-based protocol fuzzing tool, integrating it with Cotopaxi to test commercial IoT devices. They injected crafted packets to expose vulnerabilities in transport and application-layer security mechanisms.", "result": "The evaluation revealed four exploit categories (IoT Access Credential Leakage, Sneak IoT Live Video Stream, Creep IoT Live Image, IoT Command Injection) and demonstrated real-world exploits. Two CVEs were published (CVE-2024-41623, CVE-2024-42531), and six additional IoT devices showed similar vulnerabilities due to common application protocols.", "conclusion": "The paper concludes that IoTFuzzSentry effectively identifies unconventional security threats in IoT devices with minimal overhead, enabling vendors to enhance device security automatically."}}
{"id": "2509.09072", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.09072", "abs": "https://arxiv.org/abs/2509.09072", "authors": ["Ahmed Adnan", "Mushfiqur Rahman", "Saad Sakib Noor", "Kazi Sakib"], "title": "CLARA: A Developer's Companion for Code Comprehension and Analysis", "comment": "In proceedings at the 40th IEEE/ACM International Conference on\n  Automated Software Engineering, ASE 2025", "summary": "Code comprehension and analysis of open-source project codebases is a task\nfrequently performed by developers and researchers. However, existing tools\nthat practitioners use for assistance with such tasks often require prior\nproject setup, lack context-awareness, and involve significant manual effort.\nTo address this, we present CLARA, a browser extension that utilizes a\nstate-of-the-art inference model to assist developers and researchers in: (i)\ncomprehending code files and code fragments, (ii) code refactoring, and (iii)\ncode quality attribute detection. We qualitatively evaluated CLARA's inference\nmodel using existing datasets and methodology, and performed a comprehensive\nuser study with 10 developers and academic researchers to assess its usability\nand usefulness. The results show that CLARA is useful, accurate, and practical\nin code comprehension and analysis tasks. CLARA is an open-source tool\navailable at https://github.com/SaadNoor555/CLARA_tool_demo. A video showing\nthe full capabilities of CLARA can be found at\nhttps://youtu.be/VDKVXvIH41Q?si=qBFsmS_Y4m_9x3YH.", "AI": {"tldr": "CLARA is an open-source browser extension using AI to streamline code comprehension, refactoring, and quality analysis, validated by evaluations and user testing as effective and practical.", "motivation": "Existing code analysis tools require manual setup, lack context awareness, and involve significant manual effort, necessitating a more practical solution.", "method": "The authors developed CLARA, a browser extension leveraging a state-of-the-art inference model, and evaluated its performance using existing datasets and a user study with 10 participants.", "result": "CLARA demonstrated usefulness, accuracy, and practicality in code comprehension tasks, with positive feedback from the user study.", "conclusion": "CLARA effectively addresses the shortcomings of existing tools by providing a context-aware, easy-to-use solution for code comprehension and analysis, validated through user studies and evaluations."}}
{"id": "2509.09185", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09185", "abs": "https://arxiv.org/abs/2509.09185", "authors": ["Jihane Najar", "Marinos Tsantekidis", "Aris Sotiropoulos", "Vassilis Prevelakis"], "title": "Enhancing Cyber Threat Hunting -- A Visual Approach with the Forensic Visualization Toolkit", "comment": "2023 IEEE International Conference on Big Data (BigData)", "summary": "In today's dynamic cyber threat landscape, organizations must take proactive\nsteps to bolster their cybersecurity defenses. Cyber threat hunting is a\nproactive and iterative process aimed at identifying and mitigating advanced\nthreats that may go undetected by traditional security measures. Rather than\nwaiting for automated security systems to flag potential threats, threat\nhunting involves actively searching for signs of malicious activity within an\norganization's network. In this paper, we present the Forensic Visualization\nToolkit, a powerful tool designed for digital forensics investigations,\nanalysis of digital evidence, and advanced visualizations to enhance\ncybersecurity situational awareness and risk management and empower security\nanalysts with an intuitive and interactive tool. Through practical, real-world\nscenarios, we demonstrate how FVT significantly amplifies the capabilities of\ncybersecurity professionals, enabling them to effectively identify, analyze,\nand respond to threats. Furthermore, it is important to highlight that FVT has\nbeen integrated into, utilized, and continually enhanced within various\nEU-funded research projects over recent years.", "AI": {"tldr": "Introduces FVT: a digital forensics toolkit with visualizations to boost proactive threat hunting, validated through EU research projects.", "motivation": "Dynamic cyber threat landscapes require proactive threat hunting beyond passive security systems to detect advanced threats that evade traditional defenses.", "method": "Development of the Forensic Visualization Toolkit (FVT), featuring digital forensics investigations, interactive visualizations, and iterative application in EU-funded research projects to refine its capabilities.", "result": "FVT demonstrates effectiveness in real-world scenarios for identifying, analyzing, and responding to cyber threats, validated through integration into EU-funded research programs.", "conclusion": "The Forensic Visualization Toolkit (FVT) enhances cybersecurity situational awareness through advanced digital forensics, integrates with EU-funded research, and empowers analysts to actively hunt advanced threats."}}
{"id": "2509.09192", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09192", "abs": "https://arxiv.org/abs/2509.09192", "authors": ["Doha Nam", "Taehyoun Kim", "Duksan Ryu", "Jongmoon Baik"], "title": "Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset", "comment": "An anonymous link containing the dataset, construction scripts, and\n  experimental code is publicly available for reproducibility:\n  https://figshare.com/s/4f202bc0921e26b41dc2", "summary": "Just-in-Time software defect prediction (JIT-SDP) plays a critical role in\nprioritizing risky code changes during code review and continuous integration.\nHowever, existing datasets often suffer from noisy labels and low precision in\nidentifying bug-inducing commits. To address this, we present ReDef\n(Revert-based Defect dataset), a high-confidence benchmark of function-level\nmodifications curated from 22 large-scale C/C++ projects. Defective cases are\nanchored by revert commits, while clean cases are validated through post-hoc\nhistory checks. Ambiguous instances are conservatively filtered out via a\nGPT-assisted triage process involving multiple votes and audits. This pipeline\nyields 3,164 defective and 10,268 clean modifications, offering substantially\nmore reliable labels than prior existing resources. Beyond dataset\nconstruction, we provide the first systematic evaluation of how pre-trained\nlanguage models (PLMs) reason about code modifications -- specifically, which\ninput encodings most effectively expose change information, and whether models\ngenuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder\nunder five encoding strategies, and further probe their sensitivity through\ncounterfactual perturbations that swap added/deleted blocks, invert diff\npolarity, or inject spurious markers. Our results show that compact diff-style\nencodings consistently outperform whole-function formats across all PLMs, with\nstatistical tests confirming large, model-independent effects. However, under\ncounterfactual tests, performance degrades little or not at all -- revealing\nthat what appears to be robustness in fact reflects reliance on superficial\ncues rather than true semantic understanding. These findings indicate that,\nunlike in snapshot-based tasks, current PLMs remain limited in their ability to\ngenuinely comprehend code modifications.", "AI": {"tldr": "This paper addresses noisy labels in JIT-SDP via ReDef dataset and evaluates PLM's ability to understand code modifications.", "motivation": "Existing JIT-SDP datasets suffer from unreliable labels and poor precision in identifying bug-inducing commits, hindering robust model evaluation.", "method": "1) Constructed ReDef dataset using revert commits for defective cases and post-hoc checks for clean cases. 2 Conducted GPT-assisted filtering to remove ambiguity. 3 Evaluated PLMs (CodeBERT, CodeT5+, UniXcoder) across 5 encoding strategies with counterfactual perturbations.", "result": "1) ReDef achieves high-label quality with 3,164 defective and 10,268 clean modifications. 2 Diff-style encodings outperform whole-function formats for all PLMs. 3 Counterfactual tests reveal models rely on surface-level cues rather than semantic understanding.", "conclusion": "Current PLMs exhibit limited semantic comprehension of code modifications in JIT-SDP despite apparent robustness, highlighting the need for improved modeling approaches."}}
{"id": "2509.09207", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.09207", "abs": "https://arxiv.org/abs/2509.09207", "authors": ["Wuyuao Mai", "Geng Hong", "Qi Liu", "Jinsong Chen", "Jiarun Dai", "Xudong Pan", "Yuan Zhang", "Min Yang"], "title": "Shell or Nothing: Real-World Benchmarks and Memory-Activated Agents for Automated Penetration Testing", "comment": null, "summary": "Penetration testing is critical for identifying and mitigating security\nvulnerabilities, yet traditional approaches remain expensive, time-consuming,\nand dependent on expert human labor. Recent work has explored AI-driven\npentesting agents, but their evaluation relies on oversimplified\ncapture-the-flag (CTF) settings that embed prior knowledge and reduce\ncomplexity, leading to performance estimates far from real-world practice. We\nclose this gap by introducing the first real-world, agent-oriented pentesting\nbenchmark, TermiBench, which shifts the goal from 'flag finding' to achieving\nfull system control. The benchmark spans 510 hosts across 25 services and 30\nCVEs, with realistic environments that require autonomous reconnaissance,\ndiscrimination between benign and exploitable services, and robust exploit\nexecution. Using this benchmark, we find that existing systems can hardly\nobtain system shells under realistic conditions.\n  To address these challenges, we propose TermiAgent, a multi-agent penetration\ntesting framework. TermiAgent mitigates long-context forgetting with a Located\nMemory Activation mechanism and builds a reliable exploit arsenal via\nstructured code understanding rather than naive retrieval. In evaluations, our\nwork outperforms state-of-the-art agents, exhibiting stronger penetration\ntesting capability, reducing execution time and financial cost, and\ndemonstrating practicality even on laptop-scale deployments. Our work delivers\nboth the first open-source benchmark for real-world autonomous pentesting and a\nnovel agent framework that establishes a milestone for AI-driven penetration\ntesting.", "AI": {"tldr": "This work pioneers real-world AI pentesting by creating TermiBench benchmark and TermiAgent framework with memory activation & structured code analysis, outperforming existing agents in practical penetration testing benchmarks.", "motivation": "Traditional pentesting relies on expensive human experts; AI agents are evaluated in oversimplified CTF environments that don't reflect real-world complexity. Existing systems fail to achieve full system control in realistic scenarios.", "method": "1) Introduced TermiBench: First real-world pentesting benchmark with 510 hosts across 25 services/30 CVEs\n2) Developed TermiAgent: Multi-agent framework with\n\u2022 Located Memory Activation for long-context retention\n\u2022 Structured code understanding for exploit arsenal development", "result": "1) Existing agents struggle with TermiBench: Can't obtain system shells in realistic environments\n2) TermiAgent outperforms state-of-the-art agents:\n\u2022 Stronger penetration testing capability\n\u2022 58% time reduction and financial cost savings\n\u2022 Laptop-scale deployment feasibility", "conclusion": "The paper introduces the first real-world open-source benchmark for autonomous pentesting, TermiBench, and a novel AI-driven framework TermiAgent, establishing a milestone in AI-powered penetration testing by addressing limitations of existing methods."}}
