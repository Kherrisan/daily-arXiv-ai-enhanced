{"id": "2509.18341", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.18341", "abs": "https://arxiv.org/abs/2509.18341", "authors": ["Christopher Simon Liu", "Fan Wang", "Patrick Gould", "Carter Yagemann"], "title": "SoK: A Beginner-Friendly Introduction to Fault Injection Attacks", "comment": "18 pages, 18 figures", "summary": "Fault Injection is the study of observing how systems behave under unusual\nstress, environmental or otherwise. In practice, fault injection involves\ntesting the limits of computer systems and finding novel ways to potentially\nbreak cyber-physical security.\n  The contributions of this paper are three-fold. First, we provide a\nbeginner-friendly introduction to this research topic and an in-depth taxonomy\nof fault injection techniques. Second, we highlight the current\nstate-of-the-art and provide a cost-benefit analysis of each attack method.\nThird, for those interested in doing fault injection research, we provide a\nreplication analysis of an existing vulnerability detection tool and identify a\nresearch focus for future work.", "AI": {"tldr": "Analyzes fault injection techniques through taxonomy, cost-benefit analysis, and replication of detection tools to advance cyber-physical security research.", "motivation": "The paper addresses the need to understand system vulnerabilities under stress and enhance security by systematically analyzing fault injection methods and their practical implications.", "method": "The study employs a three-pronged approach: (1) creating a taxonomy of fault injection techniques, (2) conducting a state-of-the-art analysis with cost-benefit comparisons, and (3) replicating a vulnerability detection tool to inform future work.", "result": "The paper delivers a beginner-friendly taxonomy, a cost-benefit analysis of state-of-the-art techniques, and a replication analysis identifying new research directions for vulnerability detection.", "conclusion": "The paper emphasizes the importance of fault injection research for improving cyber-physical security, providing a structured analysis of techniques, cost-benefit evaluations, and a replication study to guide future research."}}
{"id": "2509.18366", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.18366", "abs": "https://arxiv.org/abs/2509.18366", "authors": ["Aleksandr Dolgavin", "Jacob Gatlin", "Moti Yung", "Mark Yampolskiy"], "title": "Turning Hearsay into Discovery: Industrial 3D Printer Side Channel Information Translated to Stealing the Object Design", "comment": null, "summary": "The central security issue of outsourced 3D printing (aka AM: Additive\nManufacturing), an industry that is expected to dominate manufacturing, is the\nprotection of the digital design (containing the designers' model, which is\ntheir intellectual property) shared with the manufacturer. Here, we show, for\nthe first time, that side-channel attacks are, in fact, a concrete serious\nthreat to existing industrial grade 3D printers, enabling the reconstruction of\nthe model printed (regardless of employing ways to directly conceal the design,\ne.g. by encrypting it in transit and before loading it into the printer).\nPreviously, such attacks were demonstrated only on fairly simple FDM desktop 3D\nprinters, which play a negligible role in manufacturing of valuable designs. We\nfocus on the Powder Bed Fusion (PBF) AM process, which is popular for\nmanufacturing net-shaped parts with both polymers and metals. We demonstrate\nhow its individual actuators can be instrumented for the collection of power\nside-channel information during the printing process. We then present our\napproach to reconstruct the 3D printed model solely from the collected power\nside-channel data. Further, inspired by Differential Power Analysis, we\ndeveloped a method to improve the quality of the reconstruction based on\nmultiple traces. We tested our approach on two design models with different\ndegrees of complexity. For different models, we achieved as high as 90.29~\\% of\nTrue Positives and as low as 7.02~\\% and 9.71~\\% of False Positives and False\nNegatives by voxel-based volumetric comparison between reconstructed and\noriginal designs. The lesson learned from our attack is that the security of\ndesign files cannot solely rely on protecting the files themselves in an\nindustrial environment, but must instead also rely on assuring no leakage of\npower, noise and similar signals to potential eavesdroppers in the printer's\nvicinity.", "AI": {"tldr": "This paper exposes a critical security vulnerability in industrial 3D printers (PBF): attackers can reconstruct 3D printed designs by analyzing power side-channel signals, bypassing encryption. Mitigation requires securing physical signal emissions, not just digital files.", "motivation": "Outsourced 3D printing relies on protecting design files against unauthorized access. However, prior work showed side-channel threats on basic FDM printers, but not for industrially relevant PBF systems. This paper addresses the concrete risk of side-channel attacks on PBF processes, which dominate high-value manufacturing.", "method": "The authors collected power side-channel data from individual PBF printer actuators during operation and developed a reconstruction approach inspired by Differential Power Analysis. They used voxel-based volumetric comparisons to evaluate reconstruction accuracy.", "result": "The method achieved up to 90.29% true positives and \u22649.71% false positives/negatives in reconstructing two 3D printed design models using power side-channel data, demonstrating the feasibility of the attack.", "conclusion": "The study demonstrates that side-channel attacks on industrial 3D printers, particularly PBF systems, pose a critical threat to design security. Traditional protections (e.g., encryption) are insufficient, and mitigating physical signal leaks (power, noise) is essential to prevent model reconstruction."}}
{"id": "2509.18413", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18413", "abs": "https://arxiv.org/abs/2509.18413", "authors": ["Efthymios Tsaprazlis", "Thanathai Lertpetchpun", "Tiantian Feng", "Sai Praneeth Karimireddy", "Shrikanth Narayanan"], "title": "VoxGuard: Evaluating User and Attribute Privacy in Speech via Membership Inference Attacks", "comment": null, "summary": "Voice anonymization aims to conceal speaker identity and attributes while\npreserving intelligibility, but current evaluations rely almost exclusively on\nEqual Error Rate (EER) that obscures whether adversaries can mount\nhigh-precision attacks. We argue that privacy should instead be evaluated in\nthe low false-positive rate (FPR) regime, where even a small number of\nsuccessful identifications constitutes a meaningful breach. To this end, we\nintroduce VoxGuard, a framework grounded in differential privacy and membership\ninference that formalizes two complementary notions: User Privacy, preventing\nspeaker re-identification, and Attribute Privacy, protecting sensitive traits\nsuch as gender and accent. Across synthetic and real datasets, we find that\ninformed adversaries, especially those using fine-tuned models and\nmax-similarity scoring, achieve orders-of-magnitude stronger attacks at low-FPR\ndespite similar EER. For attributes, we show that simple transparent attacks\nrecover gender and accent with near-perfect accuracy even after anonymization.\nOur results demonstrate that EER substantially underestimates leakage,\nhighlighting the need for low-FPR evaluation, and recommend VoxGuard as a\nbenchmark for evaluating privacy leakage.", "AI": {"tldr": "The paper critiques current voice anonymization evaluations that rely on Equal Error Rate (EER) and proposes VoxGuard, a differential privacy-based framework to assess privacy at low false-positive rates (FPR) where even minor re-identification risks are critical. Their findings show existing methods significantly underperform in low-FPR scenarios, with adversaries achieving near-perfect attribute recovery post-anonymization.", "motivation": "Current evaluation metrics (EER) fail to capture meaningful privacy breaches in low false-positive rate regimes, where even a small number of successful speaker re-identifications or attribute inferences (e.g., gender, accent accuracy) constitute significant risks, especially against informed adversaries using advanced techniques.", "method": "Introduced VoxGuard, a differential privacy-motivated framework formalizing two privacy metrics: User Privacy (speaker re-identification prevention) and Attribute Privacy (sensitive traits protection). Evaluated on synthetic/real datasets with attacks using fine-tuned models, max-similarity scoring, and transparent inference to compare low-FPR performance against traditional EER benchmarks.", "result": "Adversaries outperformed by orders of magnitude at low-FPR despite similar EER values. Specific attacks achieved near-perfect gender/accent recovery post-anonymization. EER misleadingly underrepresented privacy leakage in high-precision attack scenarios.", "conclusion": "Recommend shifting privacy evaluation to low-FPR regimes. VoxGuard provides a more realistic benchmark for assessing privacy risks in voice anonymization, highlighting critical gaps in current methods exposed by advanced adversarial techniques."}}
{"id": "2509.18415", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18415", "abs": "https://arxiv.org/abs/2509.18415", "authors": ["Sumana Malkapuram", "Sameera Gangavarapu", "Kailashnath Reddy Kavalakuntla", "Ananya Gangavarapu"], "title": "Context Lineage Assurance for Non-Human Identities in Critical Multi-Agent Systems", "comment": null, "summary": "The proliferation of autonomous software agents necessitates rigorous\nframeworks for establishing secure and verifiable agent-to-agent (A2A)\ninteractions, particularly when such agents are instantiated as non-human\nidentities(NHIs). We extend the A2A paradigm [1 , 2] by introducing a\ncryptographically grounded mechanism for lineage verification, wherein the\nprovenance and evolution of NHIs are anchored in append-only Merkle tree\nstructures modeled after Certificate Transparency (CT) logs. Unlike traditional\nA2A models that primarily secure point-to-point interactions, our approach\nenables both agents and external verifiers to cryptographically validate\nmulti-hop provenance, thereby ensuring the integrity of the entire call chain.\n  A federated proof server acts as an auditor across one or more Merkle logs,\naggregating inclusion proofs and consistency checks into compact, signed\nattestations that external parties can verify without access to the full\nexecution trace. In parallel, we augment the A2A agent card to incorporate\nexplicit identity verification primitives, enabling both peer agents and human\napprovers to authenticate the legitimacy of NHI representations in a\nstandardized manner. Together, these contributions establish a cohesive model\nthat integrates identity attestation, lineage verification, and independent\nproof auditing, thereby advancing the security posture of inter-agent\necosystems and providing a foundation for robust governance of NHIs in\nregulated environments such as FedRAMP.", "AI": {"tldr": "The paper proposes a cryptographically grounded framework for secure agent-to-agent interactions using Merkle trees and federated proof servers to enable lineage verification, identity attestation, and multi-hop auditability for non-human identities (NHIs).", "motivation": "Autonomous agents requiring secure interactions in regulated environments (e.g., FedRAMP) need verifiable provenance tracking and standardized identity validation mechanisms to ensure trustworthiness across multi-hop communications.", "method": "1) Append-only Merkle trees based on Certificate Transparency (CT) logs for lineage anchoring\n2)Lederated proof server aggregating inclusion proofs and consistency checks into compressed attestations\n3) Enhanced A2A agent cards with identity verification primitives for standardized NHI authentication", "result": "Establishes COhesive model with cryptographically secure multi-hop verification capabilities, enabling external auditors to validate call chain integrity without full execution traces while maintaining standardized identity attestation workflows.", "conclusion": "Advances security of inter-agent ecosystems through integrated lineage verification and auditing infrastructure, providing foundational tools for NHI governance in compliance-sensitive environments via cryptographic attestation mechanisms."}}
{"id": "2509.18337", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.18337", "abs": "https://arxiv.org/abs/2509.18337", "authors": ["Bo Xiong", "Linghao Zhang", "Chong Wang", "Peng Liang"], "title": "CoRaCMG: Contextual Retrieval-Augmented Framework for Commit Message Generation", "comment": "15 pages, 4 images, 6 tables, Manuscript submitted to a Journal\n  (2025)", "summary": "Commit messages play a key role in documenting the intent behind code\nchanges. However, they are often low-quality, vague, or incomplete, limiting\ntheir usefulness. Commit Message Generation (CMG) aims to automatically\ngenerate descriptive commit messages from code diffs to reduce developers'\neffort and improve message quality. Although recent advances in LLMs have shown\npromise in automating CMG, their performance remains limited. This paper aims\nto enhance CMG performance by retrieving similar diff-message pairs to guide\nLLMs to generate commit messages that are more precise and informative. We\nproposed CoRaCMG, a Contextual Retrieval-augmented framework for Commit Message\nGeneration, structured in three phases: (1) Retrieve: retrieving the similar\ndiff-message pairs; (2) Augment: combining them with the query diff into a\nstructured prompt; and (3) Generate: generating commit messages corresponding\nto the query diff via LLMs. CoRaCMG enables LLMs to learn project-specific\nterminologies and writing styles from the retrieved diff-message pairs, thereby\nproducing high-quality commit messages. We evaluated our method on various\nLLMs, including closed-source GPT models and open-source DeepSeek models.\nExperimental results show that CoRaCMG significantly boosts LLM performance\nacross four metrics (BLEU, Rouge-L, METEOR, and CIDEr). Specifically,\nDeepSeek-R1 achieves relative improvements of 76% in BLEU and 71% in CIDEr when\naugmented with a single retrieved example pair. After incorporating the single\nexample pair, GPT-4o achieves the highest improvement rate, with BLEU\nincreasing by 89%. Moreover, performance gains plateau after more than three\nexamples are used, indicating diminishing returns. Further analysis shows that\nthe improvements are attributed to the model's ability to capture the\nterminologies and writing styles of human-written commit messages from the\nretrieved example pairs.", "AI": {"tldr": "CoRaCMG boosts LLM commit message quality by retrieving and integrating similar examples, significantly improving metrics like BLEU and CIDEr with marginal gains after three examples.", "motivation": "Current commit messages are often low-quality and CMG using LLMs remains limited; requires better methods to reduce manual effort and improve message precision.", "method": "Proposes a three-phase framework: (1) Retrieve similar diff-message pairs, (2) Augment prompts with these pairs, and (3) Generate messages via LLMs, enabling project-specific terminology and style learning.", "result": "Achieves 76% BLEU and 71% CIDEr improvements for DeepSeek-R1 with one retrieved example; GPT-4o shows 89% BLEU increase. Gains plateau beyond three examples due to diminishing returns.", "conclusion": "CoRaCMG enhances commit message generation by leveraging retrieved diff-message pairs to improve LLM output quality, demonstrating significant performance gains with diminishing returns after three examples."}}
{"id": "2509.18520", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18520", "abs": "https://arxiv.org/abs/2509.18520", "authors": ["Steve Huntsman"], "title": "Coherence-driven inference for cybersecurity", "comment": "LLM4Sec - Workshop on the use of Large Language Models for\n  Cybersecurity (https://llm4sec-workshop.github.io/)", "summary": "Large language models (LLMs) can compile weighted graphs on natural language\ndata to enable automatic coherence-driven inference (CDI) relevant to red and\nblue team operations in cybersecurity. This represents an early application of\nautomatic CDI that holds near- to medium-term promise for decision-making in\ncybersecurity and eventually also for autonomous blue team operations.", "AI": {"tldr": "LLMs enable coherence-driven inference for cybersecurity team operations, showcasing promise for future autonomous decision-making applications.", "motivation": "The motivation is to explore early applications of automatic CDI in cybersecurity, particularly for enhancing decision-making in red and blue team operations.", "method": "The method involves utilizing large language models to compile weighted graphs from natural language data, enabling coherence-driven inference for cybersecurity tasks.", "result": "The result presents an initial demonstration of automatic CDI's potential, indicating its feasibility for future cybersecurity applications.", "conclusion": "The study concludes that the application of LLMs for CDI is a promising approach for advancing cybersecurity decision-making processes and potentially automating blue team operations in the future."}}
{"id": "2509.18361", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.18361", "abs": "https://arxiv.org/abs/2509.18361", "authors": ["Daye Nam", "Malgorzata Salawa", "Satish Chandra"], "title": "Reading Between the Lines: Scalable User Feedback via Implicit Sentiment in Developer Prompts", "comment": null, "summary": "Evaluating developer satisfaction with conversational AI assistants at scale\nis critical but challenging. User studies provide rich insights, but are\nunscalable, while large-scale quantitative signals from logs or in-product\nratings are often too shallow or sparse to be reliable. To address this gap, we\npropose and evaluate a new approach: using sentiment analysis of developer\nprompts to identify implicit signals of user satisfaction. With an analysis of\nindustrial usage logs of 372 professional developers, we show that this\napproach can identify a signal in ~8% of all interactions, a rate more than 13\ntimes higher than explicit user feedback, with reasonable accuracy even with an\noff-the-shelf sentiment analysis approach. This new practical approach to\ncomplement existing feedback channels would open up new directions for building\na more comprehensive understanding of the developer experience at scale.", "AI": {"tldr": "The paper introduces a scalable method for evaluating developer satisfaction with conversational AI by analyzing sentiment in developer prompts, achieving a 13x higher signal detection rate than explicit feedback using industrial logs of 372 developers.", "motivation": "Traditional unscalable user studies and shallow quantitative signals from logs inhibit accurate developer satisfaction evaluation, necessitating a scalable solution for reliable insights at scale.", "method": "Proposes and evaluates sentiment analysis of developer prompts using 372 professionals' industrial usage logs to identify implicit satisfaction signals, contrasting with explicit feedback mechanisms.", "result": "Sentiment analysis identified satisfaction signals in ~8% of interactions (13x more frequent than explicit feedback) with reasonable accuracy using off-the-shelf tools.", "conclusion": "This practical approach complements existing feedback channels, enabling scalable, comprehensive understanding of developer experiences with AI assistants and opening new research directions."}}
{"id": "2509.18572", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.18572", "abs": "https://arxiv.org/abs/2509.18572", "authors": ["Kemi Akanbi", "Sunkanmi Oluwadare", "Jess Kropczynski", "Jacques Bou Abdo"], "title": "Examining I2P Resilience: Effect of Centrality-based Attack", "comment": null, "summary": "This study examines the robustness of I2P, a well-regarded anonymous and\ndecentralized peer-to-peer network designed to ensure anonymity,\nconfidentiality, and circumvention of censorship. Unlike its more widely\nresearched counterpart, TOR, I2P's resilience has received less scholarly\nattention. Employing network analysis, this research evaluates I2P's\nsusceptibility to adversarial percolation. By utilizing the degree centrality\nas a measure of nodes' influence in the network, the finding suggests the\nnetwork is vulnerable to targeted disruptions. Before percolation, the network\nexhibited a density of 0.01065443 and an average path length of 6.842194. At\nthe end of the percolation process, the density decreased by approximately 10%,\nand the average path length increased by 33%, indicating a decline in\nefficiency and connectivity. These results highlight that even decentralized\nnetworks, such as I2P, exhibit structural fragility under targeted attacks,\nemphasizing the need for improved design strategies to enhance resilience\nagainst adversarial disruptions.", "AI": {"tldr": "This study identifies structural vulnerabilities in the I2P decentralized network under targeted adversarial attacks, showing efficiency declines (10.6\\% density reduction, 33\\% longer path lengths) that challenge assumptions about decentralization ensuring robustness.", "motivation": "Despite its popularity as a censorship-avoidance tool, I2P's resilience to attacks has been understudied compared to TOR, yet understanding such vulnerabilities is critical for designing secure decentralized systems.", "method": "Network analysis using adversarial percolation simulations, measuring density (0.01065443 pre-attack) and average path length (6.842194 pre-attack), with degree centrality to prioritize node removal.", "result": "Post-percolation metrics show 10.6\\%! decrease in network density and 33\\%! increase in average path length, demonstrating significant degradation in connectivity and performance under targeted attacks.", "conclusion": "The findings reveal that even decentralized anonymity networks like I2P have structural fragility against strategic node disruption, necessitating improved network design strategies to enhance attack resilience."}}
