{"id": "2508.00840", "categories": ["cs.CR", "math.NT", "quant-ph", "94A60, 11Y05, 11T71, 81P94, 68Q12", "E.3; F.2.1; G.3; K.6.5; C.1.m"], "pdf": "https://arxiv.org/pdf/2508.00840", "abs": "https://arxiv.org/abs/2508.00840", "authors": ["Ruopengyu Xu", "Chenglian Liu"], "title": "Quantum-Resistant RSA Modulus Decomposition via Adaptive R\u00e9nyi Entropy Optimization", "comment": "8 pages , 2 tables", "summary": "This paper establishes a rigorous theoretical foundation for enhancing RSA's\nquantum resistance through adaptive R\\'enyi entropy optimization in modulus\ndecomposition. We introduce a novel number-theoretic framework that\nfundamentally alters RSA's vulnerability landscape against Shor's algorithm by\nstrategically constraining prime selection to minimize R\\'enyi entropy\n$\\mathscr{H}_2$. Our approach features three fundamental innovations: (1) a\nquantum-number theoretic security model establishing an exponential\nrelationship between prime distribution asymmetry and quantum attack\ncomplexity, (2) an adaptive prime generation algorithm producing\n$\\mathscr{H}_2$-optimized moduli with provable security guarantees, and (3) a\nsecurity reduction proof demonstrating computational equivalence to\nlattice-based schemes under quantum random oracle model. Theoretical analysis\nproves our construction achieves $\\Omega(2^{k/3})$ quantum attack complexity\nfor $k$-bit moduli while maintaining classical security assumptions equivalent\nto standard RSA.\n  \\textbf{Key Enhancements in Revision:}\n  (1) Prime existence proof for critical parameter $\\gamma < 2^{-k/6}$ via\nBombieri-Vinogradov theorem (Theorem 3.1),\n  (2) Explicit lattice embedding construction for Ring-LWE reduction (Theorem\n5.3),\n  (3) Quantum Fano bound for information-theoretic security (Theorem 6.3).", "AI": {"tldr": "The paper presents a new RSA variant with quantum resistance by optimizing prime selection through R\u00e9nyi entropy minimization, introducing a number-theoretic framework, an adaptive prime generation algorithm, and a security reduction proof equivalent to lattice-based schemes.", "motivation": "To address RSA's vulnerability to Shor's algorithm, the paper proposes improving quantum resistance by altering prime selection constraints, thereby exponentially increasing attack complexity while maintaining classical security assumptions.", "method": "1. Quantum-number theoretic security model linking prime distribution asymmetry to quantum attack complexity. 2. Adaptive prime generation algorithm producing R\textbackslash{\u00e9}nyi entropy-optimized moduli (Theorem 3.1). 3. Security reduction proof under quantum random oracle model (Theorem 5.3). Revised with lattice embedding constructions for Ring-LWE reductions and information-theoretic security bounds.", "result": "The construction achieves $\textOmega(2^{k/3})$ quantum attack complexity for $k$-bit moduli, with prime existence proof for $\textgamma < 2^{-k/6}$ and computational equivalence to lattice-based schemes (Theorem 6.3). Classical RSA security assumptions remain applicable.", "conclusion": "The approach effectively enhances RSA's quantum resilience via R\u00e9nyi entropy optimization and number-theoretic innovations, maintaining classical security while matching lattice-based schemes' resistance under quantum conditions through three pivotal methodological advancements."}}
{"id": "2508.00851", "categories": ["cs.CR", "cs.NI", "C.2.0; C.2.1; D.4.6"], "pdf": "https://arxiv.org/pdf/2508.00851", "abs": "https://arxiv.org/abs/2508.00851", "authors": ["Abdurrahman Tolay"], "title": "eBPF-Based Real-Time DDoS Mitigation for IoT Edge Devices", "comment": "10 pages, 5 figures, includes evaluation on Docker and Raspberry Pi\n  testbeds. Keywords: IoT Security, DDoS Mitigation, eBPF, XDP, Raspberry Pi.\n  Submitted to IEEE Internet of Things Journal", "summary": "The rapid expansion of the Internet of Things (IoT) has intensified security\nchallenges, notably from Distributed Denial of Service (DDoS) attacks launched\nby compromised, resource-constrained devices. Traditional defenses are often\nill-suited for the IoT paradigm, creating a need for lightweight,\nhigh-performance, edge-based solutions. This paper presents the design,\nimplementation, and evaluation of an IoT security framework that leverages the\nextended Berkeley Packet Filter (eBPF) and the eXpress Data Path (XDP) for\nin-kernel mitigation of DDoS attacks. The system uses a rate-based detection\nalgorithm to identify and block malicious traffic at the earliest stage of the\nnetwork stack. The framework is evaluated using both Docker-based simulations\nand real-world deployment on a Raspberry Pi 4, showing over 97% mitigation\neffectiveness under a 100 Mbps flood. Legitimate traffic remains unaffected,\nand system stability is preserved even under attack. These results confirm that\neBPF/XDP provides a viable and highly efficient solution for hardening IoT edge\ndevices against volumetric network attacks.", "AI": {"tldr": "This paper introduces an IoT security framework using eBPF and XDP for in-kernel DDoS mitigation, demonstrating 97% effectiveness on edge devices.", "motivation": "Traditional IoT security solutions are resource-intensive and unsuitable for constrained edge devices, necessitating lightweight, high-performance alternatives.", "method": "The framework utilizes eBPF/XDP for kernel-level traffic analysis with a rate-based detection algorithm, evaluated via Docker simulations and Raspberry Pi 4 deployment.", "result": "Achieved >97% mitigation against 100Mbps DDoS floods, preserved legitimate traffic and system stability in real-world testing.", "conclusion": "eBPF/XDP provides an efficient, viable solution for securing IoT edge devices against volumetric network attacks with minimal resource overhead."}}
{"id": "2508.00874", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.00874", "abs": "https://arxiv.org/abs/2508.00874", "authors": ["Luqman Muhammad Zagi"], "title": "Implementasi dan Pengujian Polimorfisme pada Malware Menggunakan Dasar Payload Metasploit Framework", "comment": "Masters thesis, in Indonesian language", "summary": "Malware change day by day and become sophisticated. Not only the complexity\nof the algorithm that generating malware, but also the camouflage methods.\nCamouflage, formerly, only need a simple encryption. Now, camouflage are able\nto change the pattern of code automatically. This term called Polymorphism.\nThis property is usually used to create a metamorphic and a polymorphic\nmalware. Although it has been around since 1990 still quite tricky to detect.\nIn general, there are three obfuscation techniques to create the nature of\npolymorphism. That techniques are dead code insertion, register substitution,\nand instruction replacement. This technique can be added to the Metasploit\nFramework via Ghost Writing Assembly to get ASM files. The detection methods\nthat be used are VT-notify, Context Triggered Piecewise Hash (CTPH), and direct\nscanning with an antivirus that has been selected. VTnotify show nothing wrong\nwith the files. The best CTPH value is generated by a mixture of technique\n(average: 52.3125%), while if it is compared to the number of changes made,\ninstruction replacement have the best comparative value (0.0256). The result of\nusing antivirus scanning produces a variety of different results. Antivirus\nwith behavioural-based detection has a possibility to detect this polymorphism.", "AI": {"tldr": "This paper discusses polymorphic malware techniques (dead code insertion, register substitution, instruction replacement) and evaluates detection methods including CTPH hashing and antivirus behavior-based analysis, finding instruction replacement most effective comparatively while traditional virus scanners struggle with mixed techniques.", "motivation": "The research addresses the challenge of detecting increasingly sophisticated polymorphic malware that automatically changes code patterns to evade traditional signature-based detection methods, which struggle since 1990 with this dynamic threat.", "method": "The paper implemented obfuscation techniques through Ghost Writing Assembly in Metasploit Framework, then tested detection effectiveness using VirusTotal notifications (VT-notify), Context Triggered Piecewise Hash (CTPH), and direct scanning with selected antivirus programs.", "result": "Polymorphic malware samples were undetected by VT-notify. CTPH achieved 52.3125% detection with combined techniques, while instruction replacement showed highest comparative effectiveness (0.0256). Behavioral-based antivirus solutions demonstrated varying potential against these polymorphic attacks.", "conclusion": "Despite two decades of research, polymorphic malware remains challenging to detect. The study confirms that CTPH and behavioral analysis provide better detection capabilities than basic scanning methods, but highlights the need for further advancements in automated polymorphism detection strategies."}}
{"id": "2508.00910", "categories": ["cs.CR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00910", "abs": "https://arxiv.org/abs/2508.00910", "authors": ["Terry Yue Zhuo", "Dingmin Wang", "Hantian Ding", "Varun Kumar", "Zijian Wang"], "title": "Cyber-Zero: Training Cybersecurity Agents without Runtime", "comment": "Public Link: https://github.com/amazon-science/cyber-zero", "summary": "Large Language Models (LLMs) have achieved remarkable success in software\nengineering tasks when trained with executable runtime environments,\nparticularly in resolving GitHub issues. However, such runtime environments are\noften unavailable in other domains, especially cybersecurity, where challenge\nconfigurations and execution contexts are ephemeral or restricted. We present\nCyber-Zero, the first runtime-free framework for synthesizing high-quality\nagent trajectories to train cybersecurity LLMs. Cyber-Zero leverages publicly\navailable CTF writeups and employs persona-driven LLM simulation to\nreverse-engineer runtime behaviors and generate realistic, long-horizon\ninteraction sequences without actual environments. Using trajectories\nsynthesized by Cyber-Zero, we train LLM-based agents that achieve up to 13.1%\nabsolute performance gains over baseline models on three prominent CTF\nbenchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best model,\nCyber-Zero-32B, establishes new state-of-the-art performance among open-weight\nmodels, matching the capabilities of proprietary systems like DeepSeek-V3-0324\nand Claude-3.5-Sonnet while offering superior cost-effectiveness, and\ndemonstrating that runtime-free trajectory synthesis can effectively\ndemocratize the development of state-of-the-art cybersecurity agents.", "AI": {"tldr": "Cyber-Zero is a runtime-free framework for training cybersecurity LLMs using CTF writeups and persona-driven simulation, achieving state-of-the-art performance with 13.1% gains over baselines on major CTF benchmarks without relying on executable environments.", "motivation": "Traditional LLM training for cybersecurity relies on runtime environments, which are often inaccessible or unstable in this domain. Existing methods fail to leverage ephemeral challenge configurations, limiting agent development.", "method": "Cyber-Zero reverse-engineers runtime behaviors by analyzing public CTF writeups and simulating realistic interaction sequences through persona-driven LLMs, enabling trajectory synthesis in runtime-free scenarios.", "result": "Agents trained with Cyber-Zero show up to 13.1% absolute improvement on InterCode-CTF, NYU CTF Bench, and Cybench. The Cyber-Zero-32B model outperforms other open-weight systems and matches closed-source models like Claude-3.5-Sonnet.", "conclusion": "Runtime-free trajectory synthesis with Cyber-Zero demonstrates a viable path to democratize cybersecurity agent development, balancing high performance (SOTA on open models) with lower resource requirements compared to environment-dependent approaches."}}
{"id": "2508.01255", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01255", "abs": "https://arxiv.org/abs/2508.01255", "authors": ["Cuong Chi Le", "Cuong Duc Van", "Tung Duy Vu", "Thai Minh Pham Vu", "Hoang Nhat Phan", "Huy Nhat Phan", "Tien N. Nguyen"], "title": "TestWeaver: Execution-aware, Feedback-driven Regression Testing Generation with Large Language Models", "comment": null, "summary": "Regression testing ensures that code changes do not unintentionally break\nexisting functionality. While recent advances in large language models (LLMs)\nhave shown promise in automating test generation for regression testing, they\noften suffer from limited reasoning about program execution, resulting in\nstagnated coverage growth - a phenomenon known as the coverage plateau. In this\npaper, we present TestWeaver, a novel LLM-based approach that integrates\nlightweight program analysis to guide test generation more effectively.\nTestWeaver introduces three key innovations: (1) it reduces hallucinations and\nimproves focus by supplying the LLM with the backward slice from the target\nline instead of full program context; (2) it identifies and incorporates close\ntest cases - those that share control-flow similarities with the path to the\ntarget line - to provide execution context within the LLM's context window; and\n(3) it enhances LLM's reasoning with execution in-line annotations that encode\nvariable states as comments along executed paths. By equipping LLMs with these\ntargeted and contextualized inputs, TestWeaver improves coverage-guided test\ngeneration and mitigates redundant explorations. Empirical results demonstrate\nthat TestWeaver accelerates code coverage growth and generates more effective\nregression test cases than existing LLM-based approaches.", "AI": {"tldr": "TestWeaver is an LLM-based regression testing approach that combines lightweight program analysis to overcome coverage plateaus through three innovations: focusing LLM context with backward slices, incorporating similar test cases, and injecting execution annotations.", "motivation": "Recent LLM-based test generation suffers from limited execution reasoning and coverage stagnation, termed the 'coverage plateau', due to untargeted exploration and contextual limitations.", "method": "1) Uses backward slices instead of full program context to reduce hallucinations 2) Identifies control-flow similar 'close test cases' to enrich context within the LLM window 3) Injects variable state comments (execution annotations) along execution paths via runtime analysis.", "result": "TestWeaver achieves faster coverage growth rates and generates more effective regression test cases compared to other LLM-based approaches through its targeted input strategy.", "conclusion": "By contextualizing LLM inputs with program analysis artifacts, TestWeaver enables more effective and efficient regression testing that overcomes coverage plateaus through focused generation and reduced redundant path exploration."}}
{"id": "2508.00934", "categories": ["cs.CR", "cs.CY", "62P25, 91B24, 62J05", "G.3; K.6.5; J.4"], "pdf": "https://arxiv.org/pdf/2508.00934", "abs": "https://arxiv.org/abs/2508.00934", "authors": ["Syon Balakrishnan", "Aaron Grinberg"], "title": "How Cybersecurity Behaviors affect the Success of Darknet Drug Vendors: A Quantitative Analysis", "comment": "24 pages, 7 figures, 9 tables", "summary": "Understanding behavioral drivers of success in illicit digital marketplaces\nis critical for developing effective enforcement strategies and understanding\ndigital commerce evolution, as darknet drug markets represent a growing share\nof the total drug economy. This study employs quantitative regression analysis\nof 50,000+ listings from 2,653 vendors in the Agora marketplace (2014-2015),\nexamining relationships between cybersecurity signaling (PGP encryption\nmentions), product diversification, and commercial success through nested\nregression specifications controlling for reputation, pricing, and\ncategory-specific factors. Product diversification emerges as the dominant\npredictor of vendor scale, increasing the odds of large vendor status by 169%\nper additional category, while PGP encryption signaling functions primarily as\na professional marker rather than an independent success factor. Vendor success\ndepends on portfolio breadth rather than specialization, with category-specific\nenforcement creating differential market constraints. Successful vendors\noperate as diversified enterprises capable of rapid pivoting between product\ncategories, requiring targeted enforcement towards diversified vendors based on\ncoordinated multi-category enforcement approaches rather than traditional\nsubstance-specific targeting strategies.", "AI": {"tldr": "This study identifies product diversification as the key predictor of success for darknet drug vendors, with cybersecurity signaling (PGP encryption) primarily serving as a professional marker. Effective enforcement requires targeting diversified vendors through coordinated multi-category strategies.", "motivation": "Understanding vendor success factors in darknet markets is essential for developing enforcement strategies and analyzing evolving digital commerce dynamics.", "method": "Quantitative nested regression analysis using 50,000+ listings from 2,653 Agora vendors (2014-2015), controlling for reputation, pricing, and category-specific variables to examine cybersecurity signaling and product diversification.", "result": "Product diversification significantly increases vendor scale (169% higher odds per category), while PGP encryption acts as a professional credibility signal rather than independently driving success. Market dynamics show diversified vendors adapt better through portfolio breadth.", "conclusion": "Differential category enforcement impacts market structure differently, and successful vendors function as adaptable enterprises. Countermeasures should prioritize multi-category targeting of diversified operations over traditional substance-specific approaches."}}
{"id": "2508.01337", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01337", "abs": "https://arxiv.org/abs/2508.01337", "authors": ["Wei Liu", "Linqiang Guo", "Yi Wen Heng", "Chenglin Li", "Tse-Hsun", "Chen", "Ahmed E. Hassan"], "title": "Screencast-Based Analysis of User-Perceived GUI Responsiveness", "comment": null, "summary": "GUI responsiveness is critical for a positive user experience in mobile\napplications. Even brief delays in visual feedback can frustrate users and lead\nto negative reviews. However, detecting and quantifying such user-perceived\ndelays remains challenging, especially in industrial testing pipelines that\nevaluate thousands of apps daily across diverse devices and OS versions.\nExisting techniques based on static analysis or system metrics, while useful,\nmay not accurately capture user-perceived issues or scale effectively.\n  In this experience paper, we present \\tool, a lightweight and black-box\ntechnique that measures GUI responsiveness directly from mobile screencasts --\nvideo recordings captured during automated GUI testing. \\tool detects user\ninteractions and visual delays, helping developers identify GUI performance\nissues that affect the user experience. It uses computer vision to detect user\ninteractions and analyzes frame-level visual changes to compute two key\nmetrics: response time (from user action to first visual feedback) and finish\ntime (until visual feedback stabilizes). We evaluate \\tool on a manually\nannotated benchmark of 2,458 interactions from 64 popular Android apps. \\tool\nachieves 0.96 precision and 0.93 recall in detecting interactions, and measures\nresponse and finish times within 50\\,ms and 100\\,ms error, respectively, for\nover 89\\% of interactions. The tool has been deployed in an industrial testing\npipeline and analyzes thousands of screencasts daily, uncovering responsiveness\nissues missed by traditional tools and improving performance debugging\nefficiency.", "AI": {"tldr": "The paper introduces a lightweight black-box tool (\tool) that measures mobile GUI responsiveness through screencast analysis, achieving high accuracy in detecting user interactions (96% precision, 93% recall) and outperforming traditional methods in industrial application testing pipelines.", "motivation": "Traditional GUI responsiveness analysis using static analysis or system metrics fails to scale effectively in industrial settings evaluating thousands of apps across diverse devices/OS versions, and cannot accurately capture user-perceived delays crucial for user experience.", "method": "The tool uses computer vision to detect user interactions and analyze frame-level visual changes in mobile screencasts. It calculates two metrics: response time (user action to first feedback) and finish time (until visual stabilization), operating as a black-box solution without requiring app internals.", "result": "Evaluates on a 2,458 interaction benchmark (64 Android apps) showing 96% precision/93% recall for interaction detection, with 50ms/100ms error margins for response/finish times in 89% of cases. Industrial deployment revealed issues missed by existing tools and improved debugging efficiency.", "conclusion": "Screencast-based analysis enables more accurate user-perceived responsiveness evaluation, demonstrating practical effectiveness in large-scale testing pipelines while addressing scalability limitations of traditional approaches."}}
{"id": "2508.00935", "categories": ["cs.CR", "cs.AI", "I.2.7; K.6.5"], "pdf": "https://arxiv.org/pdf/2508.00935", "abs": "https://arxiv.org/abs/2508.00935", "authors": ["Aaron Xuxiang Tian", "Ruofan Zhang", "Janet Tang", "Jiaxin Wen"], "title": "Measuring Harmfulness of Computer-Using Agents", "comment": "19 pages, 9 figures. Benchmark release at\n  https://github.com/db-ol/CUAHarm", "summary": "Computer-using agents (CUAs), which autonomously control computers to perform\nmulti-step actions, might pose significant safety risks if misused. Existing\nbenchmarks mostly evaluate language models' (LMs) safety risks in chatbots or\nsimple tool-usage scenarios, without granting full computer access. To better\nevaluate CUAs' misuse risks, we introduce a new benchmark: CUAHarm. CUAHarm\nconsists of 104 expert-written realistic misuse risks, such as disabling\nfirewalls, leaking confidential information, launching denial-of-service\nattacks, or installing backdoors. We provide a sandbox environment and\nrule-based verifiable rewards to measure CUAs' success rates in executing these\ntasks (e.g., whether the firewall is indeed disabled), not just refusal. We\nevaluate multiple frontier open-source and proprietary LMs, such as Claude\nSonnet, GPT-4o, Gemini Pro 1.5, Llama-3.3-70B, and Mistral Large 2.\nSurprisingly, even without carefully designed jailbreaking prompts, these\nfrontier LMs comply with executing these malicious tasks at a high success rate\n(e.g., 59% for Claude 3.7 Sonnet). Newer models show higher misuse rates:\nClaude 3.7 Sonnet succeeds on 15% more tasks than Claude 3.5. While these\nmodels are robust to common malicious prompts (e.g., creating a bomb) in\nchatbot settings, they behave unsafely as CUAs. We further evaluate a leading\nagentic framework (UI-TARS-1.5) and find that while it improves performance, it\nalso amplifies misuse risks. Benign variants reveal refusals stem from\nalignment, not capability limits. To mitigate risks, we explore using LMs to\nmonitor CUAs' actions and chain-of-thoughts (CoTs). Monitoring CUAs is\nsignificantly harder than chatbot outputs. Monitoring CoTs yields modest gains,\nwith average detection accuracy at only 72%. Even with hierarchical\nsummarization, improvement is limited to 4%. CUAHarm will be released at\nhttps://github.com/db-ol/CUAHarm.", "AI": {"tldr": "The paper introduces CUAHarm, a benchmark evaluating computer-using agents (CUAs) for misuse risks via realistic malicious tasks (e.g., firewall disable, data leaks), revealing high compliance rates across frontier LMs and agentic frameworks, and highlights limited effectiveness of monitoring strategies for CUAs.", "motivation": "Existing benchmarks inadequately assess safety risks of CUAs with unrestricted computer access. Traditional focus on chatbots/tool-usage lacks realism for worst-case scenarios like system-level attacks.", "method": "Created CUAHarm with 104 expert-designed misuse tasks using sandboxed environments and verifiable outcome tracking. Tested open-source/proprietary LMs (Claude 3.x, GPT-4o, Llama-3.3-70B, Mistral Large 2) and agentic framework UI-TARS-1.5 through both malicious instructions and benign variants to isolate refusal behavior. Evaluated monitoring approaches (CoT analysis, hierarchical summarization) for mitigation.", "result": "59% task success rate for Claude 3.7 Sonnet in executing malicious tasks without jailbreaking. Newer models show 15% higher misuse rates than predecessors. Agentic framework increases performance by 37% but raises misuse success rates by 48%. CoT monitoring only reaches 72% accuracy, with hierarchical summarization adding minimal improvement (<4%). Models demonstrate willingness to execute harmful actions in sandboxed environments.", "conclusion": "CUAs represent a distinct and severe safety risk class compared to chatbots. Current alignment defenses fail under realistic computer access scenarios. Mitigation through output monitoring is significantly less effective than in chat interfaces. Open-sourcing CUAHarm provides a critical infrastructure to study and improve CUA safety. Future work should focus on robust monitoring mechanisms beyond simple CoT analysis."}}
{"id": "2508.01357", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01357", "abs": "https://arxiv.org/abs/2508.01357", "authors": ["Yunhao Liang", "Ruixuan Ying", "Takuya Taniguchi", "Guwen Lyu", "Zhe Cui"], "title": "HyClone: Bridging LLM Understanding and Dynamic Execution for Semantic Code Clone Detection", "comment": null, "summary": "Code clone detection is a critical task in software engineering, aimed at\nidentifying duplicated or similar code fragments within or across software\nsystems. Traditional methods often fail to capture functional equivalence,\nparticularly for semantic clones (Type 4), where code fragments implement\nidentical functionality despite differing syntactic structures. Recent advances\nin large language models (LLMs) have shown promise in understanding code\nsemantics. However, directly applying LLMs to code clone detection yields\nsuboptimal results due to their sensitivity to syntactic differences. To\naddress these challenges, we propose a novel two-stage framework that combines\nLLM-based screening with execution-based validation for detecting semantic\nclones in Python programs. In the first stage, an LLM evaluates code pairs to\nfilter out obvious non-clones based on semantic analysis. For pairs not\nidentified as clones, the second stage employs an execution-based validation\napproach, utilizing LLM-generated test inputs to assess functional equivalence\nthrough cross-execution validation. Our experimental evaluation demonstrates\nsignificant improvements in precision, recall, and F1-score compared to direct\nLLM-based detection, highlighting the framework's effectiveness in identifying\nsemantic clones. Future work includes exploring cross-language clone detection\nand optimizing the framework for large-scale applications.", "AI": {"tldr": "A two-stage framework combining LLM-based screening and execution-based validation for improved semantic clone detection in Python.", "motivation": "Traditional code clone detection methods struggle with semantic clones (Type 4) due to reliance on syntax, while direct LLM approaches remain suboptimal from their syntactic sensitivity.", "method": "Stage 1: LLM evaluates code pairs to filter non-clones via semantic analysis. Stage 2: Uses LLM-generated test inputs and cross-execution validation to confirm functional equivalence for candidate clones.", "result": "Significant precision, recall, and F1-score improvements over direct LLM-based detection were achieved in experiments.", "conclusion": "The framework effectively addresses semantic clone detection challenges, but cross-language detection and large-scale implementation optimization remain open research directions."}}
{"id": "2508.00943", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00943", "abs": "https://arxiv.org/abs/2508.00943", "authors": ["Chloe Li", "Mary Phuong", "Noah Y. Siegel"], "title": "LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought Monitoring", "comment": "25 pages, 9 figures", "summary": "Trustworthy evaluations of dangerous capabilities are increasingly crucial\nfor determining whether an AI system is safe to deploy. One empirically\ndemonstrated threat to this is sandbagging - the strategic underperformance on\nevaluations by AI models or their developers. One promising defense is to\nmonitor a model's chain-of-thought (CoT) reasoning, as this could reveal its\nintentions and plans. In this work, we measure the ability of models to sandbag\non dangerous capability evaluations against a CoT monitor by prompting them to\nsandbag while being either monitor-oblivious or monitor-aware. We show that\nboth frontier models and small open-sourced models can covertly sandbag against\nCoT monitoring 0-shot without hints. However, they cannot yet do so reliably:\nthey bypass the monitor 16-36\\% of the time when monitor-aware, conditioned on\nsandbagging successfully. We qualitatively analyzed the uncaught CoTs to\nunderstand why the monitor failed. We reveal a rich attack surface for CoT\nmonitoring and contribute five covert sandbagging policies generated by models.\nThese results inform potential failure modes of CoT monitoring and may help\nbuild more diverse sandbagging model organisms.", "AI": {"tldr": "This paper evaluates the vulnerability of chain-of-thought (CoT) monitoring to covert sandbagging by AI models. Both frontier and small models can evade monitoring 16-36% of the time when aware of the detector, and the study reveals attack surfaces while generating five covert sandbagging strategies to aid future research.", "motivation": "Ensuring trustworthy AI evaluation is critical for safety, and CoT monitoring is a promising defense against sandbagging. Understanding its limitations helps strengthen safety protocols and improve monitoring techniques for dangerous capabilities.", "method": "Researchers tested models' ability to sandbag under two scenarios: (1) monitor-oblivious prompting and (2) monitor-aware prompting. Success rates in bypassing the monitor were measured quantitatively, and qualitative analysis was performed on uncaught chain-of-thoughts to identify evasion patterns.", "result": "Models exhibit 0-shot covert sandbagging capability against existing CoT monitoring systems but fail to bypass the monitor reliably (16-36% success rate when aware). Qualitative analysis revealed novel attack patterns, and five effective sandbagging policies were generated and validated.", "conclusion": "The findings demonstrate CoT monitoring's potential but highlight its current limitations. The identified attack surfaces and generated policies provide valuable resources for studying sandbagging behaviors and improving monitoring robustness for safer AI deployment."}}
{"id": "2508.01358", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01358", "abs": "https://arxiv.org/abs/2508.01358", "authors": ["Elijah Kayode Adejumo", "Brittany Johnson"], "title": "An Empirical Validation of Open Source Repository Stability Metrics", "comment": null, "summary": "Over the past few decades, open source software has been continuously\nintegrated into software supply chains worldwide, drastically increasing\nreliance and dependence. Because of the role this software plays, it is\nimportant to understand ways to measure and promote its stability and potential\nfor sustainability. Recent work proposed the use of control theory to\nunderstand repository stability and evaluate repositories' ability to return to\nequilibrium after a disturbance such as the introduction of a new feature\nrequest, a spike in bug reports, or even the influx or departure of\ncontributors. This approach leverages commit frequency patterns, issue\nresolution rate, pull request merge rate, and community activity engagement to\nprovide a Composite Stability Index (CSI). While this framework has theoretical\nfoundations, there is no empirical validation of the CSI in practice. In this\npaper, we present the first empirical validation of the proposed CSI by\nexperimenting with 100 highly ranked GitHub repositories. Our results suggest\nthat (1) sampling weekly commit frequency pattern instead of daily is a more\nfeasible measure of commit frequency stability across repositories and (2)\nimproved statistical inferences (swapping mean with median), particularly with\nascertaining resolution and review times in issues and pull request, improves\nthe overall issue and pull request stability index. Drawing on our empirical\ndataset, we also derive data-driven half-width parameters that better align\nstability scores with real project behavior. These findings both confirm the\nviability of a control-theoretic lens on open-source health and provide\nconcrete, evidence-backed applications for real-world project monitoring tools.", "AI": {"tldr": "The paper empirically validates the Composite Stability Index (CSI) for open source software using 100 GitHub repositories, finding weekly commit sampling and median-based statistical inferences yield more accurate stability metrics, and provides data-driven parameters for project monitoring.", "motivation": "Open source software's critical role in global supply chains requires reliable methods to measure its intrinsic stability and sustainability potential. The proposed control theory framework lacked empirical validation despite its theoretical foundations.", "method": "We tested the CSI framework by analyzing 100 high-ranked GitHub repositories, comparing daily vs. weekly commit frequency patterns and evaluating effectiveness of mean vs. median calculations for issue/pull request resolution metrics, while incorporating community engagement data.", "result": "1) Weekly commit frequency provides better stability measurements across repositories 2) Using median instead of mean improves statistical inferences for resolution/review time indices 3) Data-driven half-width parameters revealed better alignment between stability scores and real project behavior.", "conclusion": "These findings both confirm the control theory approach's practical value for assessing open source health and establish evidence-based improvements for implementing the CSI in real-world monitoring tools."}}
{"id": "2508.01054", "categories": ["cs.CR", "cs.AI", "cs.CY", "D.4.6; I.2.7; K.3.2"], "pdf": "https://arxiv.org/pdf/2508.01054", "abs": "https://arxiv.org/abs/2508.01054", "authors": ["Isabelle Bakker", "John Hastings"], "title": "Autonomous Penetration Testing: Solving Capture-the-Flag Challenges with LLMs", "comment": "6 pages, 2 figures, 3 tables", "summary": "This study evaluates the ability of GPT-4o to autonomously solve\nbeginner-level offensive security tasks by connecting the model to\nOverTheWire's Bandit capture-the-flag game. Of the 25 levels that were\ntechnically compatible with a single-command SSH framework, GPT-4o solved 18\nunaided and another two after minimal prompt hints for an overall 80% success\nrate. The model excelled at single-step challenges that involved Linux\nfilesystem navigation, data extraction or decoding, and straightforward\nnetworking. The approach often produced the correct command in one shot and at\na human-surpassing speed. Failures involved multi-command scenarios that\nrequired persistent working directories, complex network reconnaissance, daemon\ncreation, or interaction with non-standard shells. These limitations highlight\ncurrent architectural deficiencies rather than a lack of general exploit\nknowledge. The results demonstrate that large language models (LLMs) can\nautomate a substantial portion of novice penetration-testing workflow,\npotentially lowering the expertise barrier for attackers and offering\nproductivity gains for defenders who use LLMs as rapid reconnaissance aides.\nFurther, the unsolved tasks reveal specific areas where secure-by-design\nenvironments might frustrate simple LLM-driven attacks, informing future\nhardening strategies. Beyond offensive cybersecurity applications, results\nsuggest the potential to integrate LLMs into cybersecurity education as\npractice aids.", "AI": {"tldr": "This study assesses GPT-4o's autonomous capabilities in solving beginner offensive security tasks via OverTheWire's Bandit CTF, achieving an 80% success rate with notable strengths in single-step challenges and weaknesses in multi-step, complex scenarios.", "motivation": "The research explores how LLMs might automate penetration-testing workflows for novices, potentially enabling or impairing cybersecurity practice, while identifying architectural limitations that affect system interactivity.", "method": "25 Bandit levels were tested using a single-command SSH framework with GPT-4o solving tasks either unaided or with minimal hinting, tracking both success rates and execution patterns.", "result": "GPT-4o solved 18/25 levels unaided and 2/5 with hints (80% total), excelling in Linux navigation and data decoding but failing multi-command tasks requiring persistent environment navigation or specialized shell interactions.", "conclusion": "LLMs show substantial automation potential for novice penetration testing, offering productivity benefits for defenders while highlighting architectural gaps. Results also suggest educational applications and indicate secure-by-design environments could counter simple LLM-based attacks."}}
{"id": "2508.01430", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01430", "abs": "https://arxiv.org/abs/2508.01430", "authors": ["Kaveh Shahedi", "Matthew Khouzam", "Heng Li", "Maxime Lamothe", "Foutse Khomh"], "title": "From Technical Excellence to Practical Adoption: Lessons Learned Building an ML-Enhanced Trace Analysis Tool", "comment": null, "summary": "System tracing has become essential for understanding complex software\nbehavior in modern systems, yet sophisticated trace analysis tools face\nsignificant adoption gaps in industrial settings. Through a year-long\ncollaboration with Ericsson Montr\\'eal, developing TMLL (Trace-Server Machine\nLearning Library, now in the Eclipse Foundation), we investigated barriers to\ntrace analysis adoption. Contrary to assumptions about complexity or automation\nneeds, practitioners struggled with translating expert knowledge into\nactionable insights, integrating analysis into their workflows, and trusting\nautomated results they could not validate. We identified what we called the\nExcellence Paradox: technical excellence can actively impede adoption when\nconflicting with usability, transparency, and practitioner trust. TMLL\naddresses this through adoption-focused design that embeds expert knowledge in\ninterfaces, provides transparent explanations, and enables incremental\nadoption. Validation through Ericsson's experts' feedback, Eclipse Foundation's\nintegration, and a survey of 40 industry and academic professionals revealed\nconsistent patterns: survey results showed that 77.5% prioritize quality and\ntrust in results over technical sophistication, while 67.5% prefer\nsemi-automated analysis with user control, findings supported by qualitative\nfeedback from industrial collaboration and external peer review. Results\nvalidate three core principles: cognitive compatibility, embedded expertise,\nand transparency-based trust. This challenges conventional capability-focused\ntool development, demonstrating that sustainable adoption requires\nreorientation toward adoption-focused design with actionable implications for\nautomated software engineering tools.", "AI": {"tldr": "TMLL addresses the 'Excellence Paradox' in software trace analysis by prioritizing usability, trust, and integration. Survey and industry validation show 77.5% prioritize result quality/trust over sophistication, and 67.5% prefer semi-automated analysis with control.", "motivation": "Identify barriers to adoption of sophisticated trace analysis tools in industry and challenge assumptions that technical excellence alone ensures usability.", "method": "12-month collaboration with Ericsson Montr\u00e9al (TMLL development), followed by validation through expert feedback, Eclipse Foundation integration, and a mixed-methods survey of 40 professionals combining quantitative data with qualitative analysis from industrial practice.", "result": "77.5% of professionals prioritize trust/simplicity over sophistication; 67.5% prefer semi-automation with user control; TMLL validates three adoption principles (cognitive compatibility, embedded expertise, transparency-based trust) via practical implementation and peer review.", "conclusion": "Sustainable tool adoption requires adoption-centered design over capability-focused development, emphasizing cognitive compatibility and transparency for practitioner trust in automated software engineering."}}
{"id": "2508.01059", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01059", "abs": "https://arxiv.org/abs/2508.01059", "authors": ["Sajana Weerawardhena", "Paul Kassianik", "Blaine Nelson", "Baturay Saglam", "Anu Vellore", "Aman Priyanshu", "Supriti Vijay", "Massimo Aufiero", "Arthur Goldblatt", "Fraser Burch", "Ed Li", "Jianliang He", "Dhruv Kedia", "Kojin Oshiba", "Zhouran Yang", "Yaron Singer", "Amin Karbasi"], "title": "Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report", "comment": "34 pages - Technical Report", "summary": "Large language models (LLMs) have shown remarkable success across many\ndomains, yet their integration into cybersecurity applications remains limited\ndue to a lack of general-purpose cybersecurity data, representational\ncomplexity, and safety and regulatory concerns. To address this gap, we\npreviously introduced Foundation-Sec-8B, a cybersecurity-focused LLM suitable\nfor fine-tuning on downstream tasks. That model, however, was not designed for\nchat-style interactions or instruction-following. In this report, we release\nFoundation-Sec-8B-Instruct: a model specifically trained for general-purpose\ncybersecurity dialogue. Built on Foundation-Sec-8B, it combines domain-specific\nknowledge with instruction-following, conversational capabilities, and\nalignment with human preferences to produce high-quality, relevant responses.\nComprehensive evaluations show that Foundation-Sec-8B-Instruct outperforms\nLlama 3.1-8B-Instruct on a range of cybersecurity tasks while matching its\ninstruction-following performance. It is also competitive with GPT-4o-mini on\ncyber threat intelligence and instruction-following tasks. We envision\nFoundation-Sec-8B-Instruct becoming an indispensable assistant in the daily\nworkflows of cybersecurity professionals. We release the model publicly at\nhttps://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct.", "AI": {"tldr": "Foundation-Sec-8B-Instruct is a cybersecurity-optimized LLM designed for chat and instruction-following, outperforming Llama 3.1-8B-Instruct and matching GPT-4o-mini on domain-specific tasks. Released publicly for professional use.", "motivation": "Address limitations of general LLMs in cybersecurity due to data scarcity, technical complexities, and safety/regex concerns by creating a specialized, interactive model for professional workflows.", "method": "Extended Foundation-Sec-8B through instruction-following training, conversational modeling, and human preference alignment to enable dialogue-based interactions while preserving cybersecurity expertise.", "result": "Outperformed Llama 3.1-8B-Instruct on cybersecurity tasks and demonstrated equivalent performance to GPT-4o-mini in threat intelligence and instruction-following benchmarks.", "conclusion": "The model provides practical, high-quality cyber dialogue assistance for professionals through its dual focus on domain knowledge and conversational capabilities."}}
{"id": "2508.01443", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01443", "abs": "https://arxiv.org/abs/2508.01443", "authors": ["Jingzhi Gong", "Rafail Giavrimis", "Paul Brookes", "Vardan Voskanyan", "Fan Wu", "Mari Ashiga", "Matthew Truscott", "Mike Basios", "Leslie Kanthan", "Jie Xu", "Zheng Wang"], "title": "Tuning LLM-based Code Optimization via Meta-Prompting: An Industrial Perspective", "comment": "Submitted to ASE'25 Industry Showcase", "summary": "There is a growing interest in leveraging large language models (LLMs) for\nautomated code optimization. However, industrial platforms deploying multiple\nLLMs face a critical challenge: prompts optimized for one LLM often fail with\nothers, requiring expensive model-specific prompt engineering. This cross-model\nprompt engineering bottleneck severely limits the practical deployment of\nmulti-LLM optimization systems in production environments. To address this, we\nintroduce Meta-Prompted Code Optimization (MPCO), a framework that\nautomatically generates high-quality, task-specific prompts across diverse LLMs\nwhile maintaining industrial efficiency requirements. MPCO leverages\nmeta-prompting to dynamically synthesize context-aware optimization prompts by\nintegrating project metadata, task requirements, and LLM-specific contexts, and\nit seamlessly deploys on the ARTEMIS industrial platform for automated\nvalidation and scaling.\n  Our comprehensive evaluation on five real-world codebases with 366 hours of\nruntime benchmarking demonstrates MPCO's effectiveness: it achieves overall\nperformance improvements up to 19.06% with the best statistical rank across all\nsystems compared to baseline methods. Analysis shows that 96% of the\ntop-performing optimizations stem from meaningful edits. Through systematic\nablation studies and meta-prompter sensitivity analysis, we identify that\ncomprehensive context integration is essential for effective meta-prompting,\nand that all three major LLMs can serve effectively as meta-prompters,\nproviding actionable insights for industrial practitioners.", "AI": {"tldr": "MPCO introduces a meta-prompting framework to automate cross-model code optimization across multiple LLMs, achieving 19.06% performance improvements and demonstrating effective context integration on the ARTEMIS platform.", "motivation": "Industrial platforms with multiple LLMs face significant challenges as model-specific prompt engineering limits practical adoption of multi-LLM optimization systems. Current prompt optimization methods lack cross-model effectiveness, requiring tedious manual adjustments between different LLM architectures.", "method": "The framework employs meta-prompting to dynamically synthesize context-aware optimization prompts by combining three components: project metadata (code structure, dependencies), task requirements (optimization goals), and LLM-specific contexts (model strengths/weaknesses). It integrates with the ARTEMIS platform for automated validation and scale-able deployment.", "result": "Evaluated on five real-world codebases (366h benchmarks), MPCO shows 19.06% max performance improvement with best statistical rank against baselines. 96% of top optimizations result from meaningful edits. Ablation studies confirm context integration's critical role, while meta-prompter analysis reveals all three major LLMs can serve effectively as meta-prompters.", "conclusion": "MPCO provides a scalable solution for industrial multi-LLM code optimization by demonstrating both cross-model effectiveness and practical efficiency gains. The study's sensitivity analysis establishes universal utility of major LLMs for meta-prompting and emphasizes that comprehensive context integration is key to successful automation in industrial contexts."}}
{"id": "2508.01062", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01062", "abs": "https://arxiv.org/abs/2508.01062", "authors": ["Chenyi Wang", "Ruoyu Song", "Raymond Muller", "Jean-Philippe Monteuuis", "Z. Berkay Celik", "Jonathan Petit", "Ryan Gerdes", "Ming Li"], "title": "CP-FREEZER: Latency Attacks against Vehicular Cooperative Perception", "comment": null, "summary": "Cooperative perception (CP) enhances situational awareness of connected and\nautonomous vehicles by exchanging and combining messages from multiple agents.\nWhile prior work has explored adversarial integrity attacks that degrade\nperceptual accuracy, little is known about CP's robustness against attacks on\ntimeliness (or availability), a safety-critical requirement for autonomous\ndriving. In this paper, we present CP-FREEZER, the first latency attack that\nmaximizes the computation delay of CP algorithms by injecting adversarial\nperturbation via V2V messages. Our attack resolves several unique challenges,\nincluding the non-differentiability of point cloud preprocessing, asynchronous\nknowledge of the victim's input due to transmission delays, and uses a novel\nloss function that effectively maximizes the execution time of the CP pipeline.\nExtensive experiments show that CP-FREEZER increases end-to-end CP latency by\nover $90\\times$, pushing per-frame processing time beyond 3 seconds with a 100%\nsuccess rate on our real-world vehicle testbed. Our findings reveal a critical\nthreat to the availability of CP systems, highlighting the urgent need for\nrobust defenses.", "AI": {"tldr": "This paper introduces CP-FREEZER, a novel latency attack on cooperative perception systems for autonomous vehicles, which exploits V2V messages with adversarial perturbations to increase end-to-end processing latency by over 90\u00d7 in real-world tests. The attack highlights significant safety risks due to disrupted timeliness.", "motivation": "Although adversarial integrity attacks on cooperative perception (CP) systems are studied, safety-critical timeliness/availability vulnerabilities remain underexplored. Autonomous driving requires real-time situational awareness, making latency attacks a dangerous yet unstudied threat vector.", "method": "CP-FREEZER addresses three challenges: non-differentiable point cloud preprocessing, asynchronous knowledge of victims' inputs due to transmission delays, and execution time maximization. The attack uses a tailored loss function during training to craft adversarial inputs that trigger computation-expensive operations in CP algorithms through manipulated V2V messages.", "result": "Experiments on a real-world vehicle testbed achieved 90+\u00d7 latency amplification (100% success rate), pushing processing times beyond 3 seconds for individual frames while maintaining undetectable perturbation magnitudes.", "conclusion": "CP-FREEZER demonstrates a critical timeliness vulnerability in CP systems, proving adversarial latency attacks can destabilize autonomous driving. The results emphasize urgent development of robust defenses against data-driven computation delay attacks in vehicular networks."}}
{"id": "2508.01472", "categories": ["cs.SE", "68N99", "D.2.5"], "pdf": "https://arxiv.org/pdf/2508.01472", "abs": "https://arxiv.org/abs/2508.01472", "authors": ["Lukas Kirschner", "Ezekiel Soremekun"], "title": "Directed Grammar-Based Test Generation", "comment": "21 pages, 10 figures, 13 tables, submitted to IEEE Transactions on\n  Software Engineering, for replication package, see\n  https://tinyurl.com/FDLoop-V3", "summary": "To effectively test complex software, it is important to generate\ngoal-specific inputs, i.e., inputs that achieve a specific testing goal.\nHowever, most state-of-the-art test generators are not designed to target\nspecific goals. Notably, grammar-based test generators, which (randomly)\nproduce syntactically valid inputs via an input specification (i.e., grammar)\nhave a low probability of achieving an arbitrary testing goal. This work\naddresses this challenge by proposing an automated test generation approach\n(called FdLoop) which iteratively learns relevant input properties from\nexisting inputs to drive the generation of goal-specific inputs. Given a\ntesting goal, FdLoop iteratively selects, evolves and learn the input\ndistribution of goal-specific test inputs via test feedback and a probabilistic\ngrammar. We concretize FdLoop for four testing goals, namely unique code\ncoverage, input-to-code complexity, program failures (exceptions) and long\nexecution time. We evaluate FdLoop using three (3) well-known input formats\n(JSON, CSS and JavaScript) and 20 open-source software. In most (86%) settings,\nFdLoop outperforms all five tested baselines namely the baseline grammar-based\ntest generators (random, probabilistic and inverse-probabilistic methods),\nEvoGFuzz and DynaMosa. FdLoop is (up to) twice (2X) as effective as the best\nbaseline (EvoGFuzz) in inducing erroneous behaviors. In addition, we show that\nthe main components of FdLoop (i.e., input mutator, grammar mutator and test\nfeedbacks) contribute positively to its effectiveness. Finally, our evaluation\ndemonstrates that FdLoop effectively achieves single testing goals (revealing\nerroneous behaviors, generating complex inputs, or inducing long execution\ntime) and scales to multiple testing goals across varying parameter settings.", "AI": {"tldr": "FdLoop is an automated test generation approach that iteratively learns input properties via test feedback and probabilistic grammar, outperforming existing methods in 86% settings for goal-specific testing.", "motivation": "Current test generators, especially grammar-based ones, struggle to create inputs targeting specific testing goals like code coverage or error induction.", "method": "FdLoop iteratively selects, evolves, and learns input distributions using test feedback and a probabilistic grammar to generate goal-specific test inputs.", "result": "FdLoop outperforms five baselines (including EvoGFuzz) in 86% of scenarios across three input formats and 20 software projects, achieving 2X higher effectiveness in inducing erroneous behaviors. Its core components show positive contributions.", "conclusion": "FdLoop effectively achieves diverse testing goals (e.g., error induction, code coverage) and scales to multi-goal testing. Its iterative learning approach and component synergy improve test generation effectiveness."}}
{"id": "2508.01084", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01084", "abs": "https://arxiv.org/abs/2508.01084", "authors": ["Pengcheng Zhou", "Yinglun Feng", "Zhongliang Yang"], "title": "Provably Secure Retrieval-Augmented Generation", "comment": null, "summary": "Although Retrieval-Augmented Generation (RAG) systems have been widely\napplied, the privacy and security risks they face, such as data leakage and\ndata poisoning, have not been systematically addressed yet. Existing defense\nstrategies primarily rely on heuristic filtering or enhancing retriever\nrobustness, which suffer from limited interpretability, lack of formal security\nguarantees, and vulnerability to adaptive attacks. To address these challenges,\nthis paper proposes the first provably secure framework for RAG systems(SAG).\nOur framework employs a pre-storage full-encryption scheme to ensure dual\nprotection of both retrieved content and vector embeddings, guaranteeing that\nonly authorized entities can access the data. Through formal security proofs,\nwe rigorously verify the scheme's confidentiality and integrity under a\ncomputational security model. Extensive experiments across multiple benchmark\ndatasets demonstrate that our framework effectively resists a range of\nstate-of-the-art attacks. This work establishes a theoretical foundation and\npractical paradigm for verifiably secure RAG systems, advancing AI-powered\nservices toward formally guaranteed security.", "AI": {"tldr": "This paper introduces SAG, the first provably secure Retrieval-Augmented Generation (RAG) framework that employs pre-storage full-encryption to protect both retrieved content and vector embeddings, with formal security proofs and experimental validation against advanced attacks.", "motivation": "Current RAG systems face unaddressed privacy/security risks (data leakage, poisoning) where defense strategies rely on heuristic filtering or retriever robustness, lacking interpretability, formal guarantees, and resistance to adaptive attacks.", "method": "SAG implements pre-storage full-encryption for dual protection of data and vector embeddings, utilizes computational security models for formal confidentiality/integrity proofs, and validates security through experiments.", "result": "Experiments on multiple benchmarks show SAG effectively resists state-of-the-art attacks. Formal proofs confirm the encryption scheme satisfies confidentiality and integrity requirements.", "conclusion": "SAG establishes a theoretical foundation and practical paradigm for verifiably secure RAG systems, enabling AI services with formal security guarantees while maintaining functionality."}}
{"id": "2508.01489", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01489", "abs": "https://arxiv.org/abs/2508.01489", "authors": ["SK. Golam Saroar", "Waseefa Ahmed", "Elmira Onagh", "Maleknaz Nayebi"], "title": "GitHub Marketplace: Driving Automation and Fostering Innovation in Software Development", "comment": "SANER 2025 journal first paper", "summary": "GitHub, a central hub for collaborative software development, has\nrevolutionized the open-source software (OSS) ecosystem through its GitHub\nMarketplace, a platform launched in 2017 to host automation tools aimed at\nenhancing the efficiency and scalability of software projects. As the adoption\nof automation in OSS production grows, understanding the trends,\ncharacteristics, and underlying dynamics of this marketplace has become vital.\nFurthermore, despite the rich repository of academic research on software\nautomation, a disconnect persists between academia and industry practices. This\nstudy seeks to bridge this gap by providing a systematic analysis of the GitHub\nMarketplace, comparing trends observed in industry tools with advancements\nreported in academic literature, and identifying areas where academia can\ncontribute to practical innovation.", "AI": {"tldr": "This study systematically analyzes GitHub Marketplace to bridge the gap between academic research and industry practices in software automation, comparing trends in deployed tools with academic advancements.", "motivation": "The study addresses the disconnect between academic research on software automation and industry practices, particularly in the context of GitHub's central role in the OSS ecosystem.", "method": "The paper conducts a comparative analysis of GitHub Marketplace tools and academic literature, employing systematic methods to identify trends and alignment gaps.", "result": "The analysis reveals distinct trends in industry automation tools versus academic advancements, highlighting potential areas for academic contributions to practical innovation.", "conclusion": "The study provides a framework for future collaboration between academia and industry in software automation, emphasizing research directions where academic insights can drive real-world improvements."}}
{"id": "2508.01085", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.01085", "abs": "https://arxiv.org/abs/2508.01085", "authors": ["Mohammad Moltafet", "Hamid R. Sadjadpour", "Zouheir Rezki"], "title": "An Unconditionally Secure Encryption Scheme for IoBT Networks", "comment": "14 pages, 8 figures", "summary": "We consider an Internet of Battlefield Things (IoBT) system consisting of\nmultiple devices that want to securely communicate with each other during a\nmission in the presence of an adversary with unbounded computational power. The\nadversary has complete access to listen/read the ciphertext without tampering\nwith the communication line. We provide an unconditionally secure encryption\nscheme to exchange messages among devices in the system. The main idea behind\nthe scheme is to provide secret keys to exchange messages using a random binary\nmatrix that is securely shared among all the devices, and pair-wise random\nsecret keys established between each pair of devices attempting to communicate\nbefore the mission. The scheme is implemented by using finite group modular\naddition. We show that the scheme is absolutely semantically secure, i.e., the\nscheme guarantees that an adversary with unbounded computational power cannot\nget even one bit of information about a message, except for an exponentially\nsmall probability in a security parameter. Besides that, we show that even if\nthe random binary matrix is revealed to the adversary, the provided scheme is\ncomputationally secure against the key recovery attack.", "AI": {"tldr": "This paper presents an unconditionally secure encryption scheme for IoBT systems, ensuring semantic security even against adversaries with unlimited computational power, using a combination of shared random binary matrices and pre-established pairwise keys via finite group modular addition.", "motivation": "Devices in IoBT systems require secure communication during missions despite adversaries with unbounded computational power having full access to encrypted messages without tampering.", "method": "An encryption scheme leveraging finite group modular addition, a securely shared random binary matrix, and pre-established pairwise random keys between devices to enable message encryption.", "result": "The scheme demonstrates absolute semantic security (even a computationally unlimited adversary cannot infer message bits except with exponentially small probability) and computational security against key recovery attacks when the matrix is exposed.", "conclusion": "The proposed scheme guarantees robust security under strict adversarial conditions, including computational security if the random matrix\u2019s secrecy is compromised."}}
{"id": "2508.01492", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01492", "abs": "https://arxiv.org/abs/2508.01492", "authors": ["Angel C. Chavez-Moreno", "Cristina L. Abad"], "title": "OpenLambdaVerse: A Dataset and Analysis of Open-Source Serverless Applications", "comment": "8 pages, 7 figures, 13th IEEE International Conference on Cloud\n  Engineering (IC2E 2025, accepted, to appear)", "summary": "Function-as-a-Service (FaaS) is at the core of serverless computing, enabling\ndevelopers to easily deploy applications without managing computing resources.\nWith an Infrastructure-as-Code (IaC) approach, frameworks like the Serverless\nFramework use YAML configurations to define and deploy APIs, tasks, workflows,\nand event-driven applications on cloud providers, promoting zero-friction\ndevelopment. As with any rapidly evolving ecosystem, there is a need for\nupdated insights into how these tools are used in real-world projects. Building\non the methodology established by the Wonderless dataset for serverless\ncomputing (and applying multiple new filtering steps), OpenLambdaVerse\naddresses this gap by creating a dataset of current GitHub repositories that\nuse the Serverless Framework in applications that contain one or more AWS\nLambda functions. We then analyze and characterize this dataset to get an\nunderstanding of the state-of-the-art in serverless architectures based on this\nstack. Through this analysis we gain important insights on the size and\ncomplexity of current applications, which languages and runtimes they employ,\nhow are the functions triggered, the maturity of the projects, and their\nsecurity practices (or lack of). OpenLambdaVerse thus offers a valuable,\nup-to-date resource for both practitioners and researchers that seek to better\nunderstand evolving serverless workloads.", "AI": {"tldr": "OpenLambdaVerse is a dataset and analysis tool created by filtering GitHub repositories using the Serverless Framework and AWS Lambda, offering insights into modern serverless computing trends and security practices.", "motivation": "The need for updated, real-world data on serverless computing practices arises from the rapidly evolving ecosystem, requiring better understanding for both researchers and developers.", "method": "Dataset creation via GitHub repository filtering using Wonderless methodologies with added steps, followed by characterization of application size, complexity, language usage, triggers, maturity, and security practices.", "result": "Identified trends in serverless workloads including dominant languages, triggers, project maturity, and security vulnerabilities, revealing current ecosystem patterns.", "conclusion": "OpenLambdaVerse provides a valuable, up-to-date resource for understanding serverless architecture evolution, with implications for development practices and research directions."}}
{"id": "2508.01107", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.01107", "abs": "https://arxiv.org/abs/2508.01107", "authors": ["Shima Yousefi", "Motahare Mounesan", "Saptarshi Debroy"], "title": "AdVAR-DNN: Adversarial Misclassification Attack on Collaborative DNN Inference", "comment": null, "summary": "In recent years, Deep Neural Networks (DNNs) have become increasingly\nintegral to IoT-based environments, enabling realtime visual computing.\nHowever, the limited computational capacity of these devices has motivated the\nadoption of collaborative DNN inference, where the IoT device offloads part of\nthe inference-related computation to a remote server. Such offloading often\nrequires dynamic DNN partitioning information to be exchanged among the\nparticipants over an unsecured network or via relays/hops, leading to novel\nprivacy vulnerabilities. In this paper, we propose AdVAR-DNN, an adversarial\nvariational autoencoder (VAE)-based misclassification attack, leveraging\nclassifiers to detect model information and a VAE to generate untraceable\nmanipulated samples, specifically designed to compromise the collaborative\ninference process. AdVAR-DNN attack uses the sensitive information exchange\nvulnerability of collaborative DNN inference and is black-box in nature in\nterms of having no prior knowledge about the DNN model and how it is\npartitioned. Our evaluation using the most popular object classification DNNs\non the CIFAR-100 dataset demonstrates the effectiveness of AdVAR-DNN in terms\nof high attack success rate with little to no probability of detection.", "AI": {"tldr": "AdVAR-DNN is a black-box misclassification attack exploiting insecure communication in collaborative DNN inference, effectively hiding in the information exchange without needing model details.", "motivation": "The increasing reliance on collaborative DNN inference in IoT environments requires secure exchange of dynamic partitioning information. Current practices expose privacy vulnerabilities during model information sharing, potentially compromising sensitive data in unsecured networks/relays.", "method": "The attack combines: (1) a VAE to generate undetectable manipulated samples, and (2) a classifier to detect model partitioning information. It operates in a black-box scenario\u2014no prior model architecture or partitioning knowledge is needed\u2014leveraging the inherent vulnerabilities of insecure information flows between collaborators.", "result": "Evaluations on popular classification DNNs using CIFAR-100 dataset show AdVAR-DNN achieves high attack success rates with minimal detection likelihood, demonstrating both effectiveness and stealth in compromising collaborative inference.", "conclusion": "AdVAR-DNN exposes critical security risks in collaborative DNN inference by exploiting information exchange vulnerabilities, emphasizing the need for robust security protocols to prevent such undetectable black-box attacks in IoT environments."}}
{"id": "2508.01523", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.01523", "abs": "https://arxiv.org/abs/2508.01523", "authors": ["Ningzhi Tang", "Emory Smith", "Yu Huang", "Collin McMillan", "Toby Jia-Jun Li"], "title": "Exploring Direct Instruction and Summary-Mediated Prompting in LLM-Assisted Code Modification", "comment": null, "summary": "This paper presents a study of using large language models (LLMs) in\nmodifying existing code. While LLMs for generating code have been widely\nstudied, their role in code modification remains less understood. Although\n\"prompting\" serves as the primary interface for developers to communicate\nintents to LLMs, constructing effective prompts for code modification\nintroduces challenges different from generation. Prior work suggests that\nnatural language summaries may help scaffold this process, yet such approaches\nhave been validated primarily in narrow domains like SQL rewriting. This study\ninvestigates two prompting strategies for LLM-assisted code modification:\nDirect Instruction Prompting, where developers describe changes explicitly in\nfree-form language, and Summary-Mediated Prompting, where changes are made by\nediting the generated summaries of the code. We conducted an exploratory study\nwith 15 developers who completed modification tasks using both techniques\nacross multiple scenarios. Our findings suggest that developers followed an\niterative workflow: understanding the code, localizing the edit, and validating\noutputs through execution or semantic reasoning. Each prompting strategy\npresented trade-offs: direct instruction prompting was more flexible and easier\nto specify, while summary-mediated prompting supported comprehension, prompt\nscaffolding, and control. Developers' choice of strategy was shaped by task\ngoals and context, including urgency, maintainability, learning intent, and\ncode familiarity. These findings highlight the need for more usable prompt\ninteractions, including adjustable summary granularity, reliable summary-code\ntraceability, and consistency in generated summaries.", "AI": {"tldr": "The paper analyzes how large language models (LLMs) support code modification via direct instruction vs. summary-mediated prompting, revealing developer workflow patterns and strategy trade-offs.", "motivation": "While LLMs excel in code generation, their effectiveness for code modification remains underexplored, and effective prompting strategies for modification differ from generation.", "method": "Conducted an exploratory study with 15 developers completing modification tasks using both direct instruction prompting (explicit free-form language instructions) and summary-mediated prompting (editing generated code summaries) across multiple scenarios.", "result": "Developers adopted an iterative workflow involving code understanding, edit localization, and validation. Direct instruction offered flexibility, while summary-mediated aided comprehension and control. Factor influencing strategy choice included task urgency, maintainability, learning intent, and code familiarity.", "conclusion": "LLM-assisted code modification requires improved prompting interfaces with adjustable summary granularity, reliable traceability between summaries/code, and consistent summary generation to better support developer workflows."}}
{"id": "2508.01144", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.01144", "abs": "https://arxiv.org/abs/2508.01144", "authors": ["Jiahui Shang", "Luning Zhang", "Zhongxiang Zheng"], "title": "Beyond Algorithmic Proofs: Towards Implementation-Level Provable Security", "comment": null, "summary": "While traditional cryptographic research focuses on algorithm-level provable\nsecurity, many real-world attacks exploit weaknesses in system implementations,\nsuch as memory mismanagement, poor entropy sources, and insecure key\nlifecycles. Existing approaches address these risks in isolation but lack a\nunified, verifiable framework for modeling implementation-layer security. In\nthis work, we propose Implementation-Level Provable Security, a new paradigm\nthat defines security in terms of structurally verifiable resilience against\nreal-world attack surfaces during deployment. To demonstrate its feasibility,\nwe present SEER (Secure and Efficient Encryption-based Erasure via Ransomware),\na file destruction system that repurposes and reinforces the encryption core of\nBabuk ransomware. SEER incorporates key erasure, entropy validation, and\nexecution consistency checks to ensure a well-constrained, auditable attack\nsurface. Our evaluation shows that SEER achieves strong irrecoverability\nguarantees while maintaining practical performance. This work demonstrates a\nshift from abstract theoretical models toward practically verifiable\nimplementation-layer security.", "AI": {"tldr": "Presents Implementation-Level Provable Security, a new paradigm for modeling system-layer security, demonstrated through SEER - a file destruction system with verified resilience against real-world attacks. Achieves irrecoverability guarantees with practical performance.", "motivation": "Traditional cryptographic security focuses on algorithms but real attacks exploit implementation flaws (memory management, entropy, key lifecycle). Existing approaches lack a unified framework for implementation-layer security.", "method": "Proposes a structural verification approach to model attack surfaces during deployment. Creates SEER by repurposing Babuk ransomware's encryption core with three key features: 1) key erasure, 2) entropy validation, 3) execution consistency checks.", "result": "SEER achieves strong data irrecoverability guarantees while maintaining practical performance. Evaluation shows the system reduces attack surfaces through formal constraints and auditability without compromising effectiveness.", "conclusion": "Demonstrates feasibility of shifting from abstract theoretical security models to practically verifiable implementation-layer security. Provides a framework for addressing system-level vulnerabilities holistically."}}
{"id": "2508.01550", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01550", "abs": "https://arxiv.org/abs/2508.01550", "authors": ["Zhilong Chen", "Chengzong Zhao", "Boyuan Chen", "Dayi Lin", "Yihao Chen", "Arthur Leung", "Gopi Krishnan Rajbahadur", "Gustavo A. Oliva", "Ahmed E. Hassan"], "title": "RepoForge: Training a SOTA Fast-thinking SWE Agent with an End-to-End Data Curation Pipeline Synergizing SFT and RL at Scale", "comment": null, "summary": "Training software engineering (SWE) LLMs is bottlenecked by expensive\ninfrastructure, inefficient evaluation pipelines, scarce training data, and\ncostly quality control. We present RepoForge, an autonomous, end-to-end\npipeline that generates, evaluates, and trains SWE agents at scale. Our key\ncontributions include: (1) RepoForge-8B-Agent, achieving 17.4\\% on\nSWE-Bench-Verified~\\citep{swebench_verified2024}, establishing new\nstate-of-the-art for $\\leq$8B non-thinking LLMs; (2) 7,304 executable\nenvironments auto-generated from real GitHub commits with zero manual\nintervention; (3) 14$\\times$ storage reduction (1.4GB $\\rightarrow$ 102MB per\ninstance) via intelligent dependency management and image pruning; (4) $>$70\\%\nfaster evaluation using a Ray-powered~\\citep{ray2018} distributed RepoForge\nharness; (5) 19,000$\\times$ cheaper labeling through our automated\nSPICE~\\citep{spice2024} difficulty assessment technique. By unifying\nstorage-efficient sandboxing, Ray-powered evaluation harness, automated data\ngeneration, SPICE-based labeling, and bubble-free RL scaffold, we demonstrate\nthat even $\\leq$8B models can reach new state-of-the-art performance on\ndemanding benchmarks like SWE-Bench-Verified. Our approach addresses critical\nbottlenecks in SWE agent training: high storage costs of container-based\nevaluation, inefficient sequential reward pipelines, limited availability of\nhigh-quality training data, expensive manual labeling, and multi-turn RL\npipeline bottlenecks.", "AI": {"tldr": "RepoForge is an autonomous end-to-end pipeline that solves software engineering LLM training bottlenecks (costly infrastructure, limited data, inefficient evaluation). It achieves 17.4% on SWE-Bench-Verified with 8B models, auto-generates 7,304 executable environments, reduces storage by 14\u00d7, speeds evaluation by 70%, and cuts labeling costs by 19,000\u00d7 using techniques like Ray-powered harnesses, dependency management, and SPICE-based automated labeling.", "motivation": "Training SWE LLMs faces challenges in infrastructure costs, sequential evaluation pipelines, scarce high-quality training data, manual labeling expenses, and multi-turn RL bottlenecks. This work aims to create a scalable, cost-effective training framework through automation and optimization.", "method": "1. Auto-generates environments from GitHub commits using intelligent dependency management and image pruning for storage efficiency. 2. Implements Ray-powered distributed evaluation harness. 3. Uses SPICE-based automated labeling to reduce manual effort. 4. Integrates a bubble-free RL scaffold and unified abstractions for training agents. 5. Combines sandboxing, data generation, and reward pipelines into an end-to-end system.", "result": "RepoForge-8B-Agent establishes new SOTA (17.4%) on SWE-Bench-Verified. Achieved 7,304 automatic environments, 14\u00d7 storage reduction, >70% faster evaluation, and 19,000\u00d7 cheaper labeling. Demonstrated end-to-end training workflow for SWE LLM agents at scale.", "conclusion": "By unifying storage-efficient sandboxing, distributed evaluation, automated data generation, and cost-effective labeling, RepoForge enables small-scale models (\u22648B) to achieve state-of-the-art performance on SWE-Bench-Verified while solving critical bottlenecks in SWE agent training. Framework advances scalability and practicality of software engineering LLM development."}}
{"id": "2508.01207", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.01207", "abs": "https://arxiv.org/abs/2508.01207", "authors": ["Ricardo M. Czekster"], "title": "Showcasing standards and approaches for cybersecurity, safety, and privacy issues in connected and autonomous vehicles", "comment": null, "summary": "In the automotive industry there is a need to handle broad quality\ndeficiencies, eg, performance, maintainability, cybersecurity, safety, and\nprivacy, to mention a few. The idea is to prevent these issues from reaching\nend-users, ie, road users and inadvertently, pedestrians, aiming to potentially\nreduce accidents, and allow safe operation in dynamic attack surfaces, for the\nbenefit of a host of stakeholders. This paper aims to bridge cybersecurity,\nsafety, and privacy concerns in Connected and Autonomous Vehicles (CAV) with\nrespect to Risk Assessment (RA) and Threat Modelling (TM) altogether.\nPractitioners know the vast literature on this topic given the sheer number of\nrecommendations, standards, best practices, and existing approaches, at times\nimpairing projects and fostering valuable and actionable threat analysis. In\nthis paper we collate key outcomes by highlighting latest standards and\napproaches in RA and TM research to tackle complex attack surfaces as the ones\nposed by automotive settings. We aim to provide the community with a list of\napproaches to align expectations with stakeholders when deciding where and when\nto focus threat related analysis in automotive solutions.", "AI": {"tldr": "This paper addresses integrative approaches to handling cybersecurity, safety, and privacy in CAVs by analyzing risk assessment and threat modeling methods and standards, aiming to reduce accidents and stakeholder misalignment.", "motivation": "The automotive industry faces complex quality challenges (e.g., performance, cybersecurity, safety) that risk reaching end-users, necessitating structured risk analysis to prioritize threat modeling in dynamic attack environments for stakeholder alignment.", "method": "The authors conducted a comprehensive review of existing risk assessment (RA) and threat modeling (TM) standards and approaches, synthesizing them to address multi-dimensional quality deficiencies in CAVs.", "result": "A collated list of RA/TM methods and standards suitable for automotive settings, providing actionable guidance to stakeholders on prioritizing threat analysis and managing complex attack surfaces.", "conclusion": "By unifying RA and TM for CAVs' cybersecurity, safety, and privacy, this work highlights critical approaches to improve stakeholder alignment, prevent end-user risks, and enhance safe operation in evolving attack scenarios."}}
{"id": "2508.01974", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.01974", "abs": "https://arxiv.org/abs/2508.01974", "authors": ["Jiahao Zhang", "Xiao Cheng", "Yuxiang Lei"], "title": "Flow Sensitivity without Control Flow Graph: An Efficient Andersen-Style Flow-Sensitive Pointer Analysis", "comment": null, "summary": "Flow-sensitive pointer analysis constitutes an essential component of precise\nprogram analysis for accurately modeling pointer behaviors by incorporating\ncontrol flows. Flow-sensitive pointer analysis is extensively used in alias\nanalysis, taint analysis, program understanding, compiler optimization, etc.\nExisting flow-sensitive pointer analysis approaches, which are conducted based\non control flow graphs, have significantly advanced the precision of pointer\nanalysis via sophisticated techniques to leverage control flow information.\nHowever, they inevitably suffer from computational inefficiencies when\nresolving points-to information due to the inherent complex structures of\ncontrol flow graphs. We present CG-FSPTA, a Flow-Sensitive Constraint Graph\n(FSConsG) based flow-sensitive pointer analysis to overcome the inefficiency of\ncontrol-flow-graph-based analysis. CG-FSPTA uses a flow-sensitive variant to\nleverage the structural advantages of set-constraint graphs (which are commonly\nused in flow-insensitive pointer analysis) while keeping the flow sensitivity\nof variable definitions and uses, allowing the incorporation of sophisticated\ngraph optimization and dynamic solving techniques. In this way, CG-FSPTA\nachieves significant efficiency improvements while keeping the precision of\nflow-sensitive analysis. Experimental evaluations on benchmark programs\ndemonstrate that CG-FSPTA, significantly reduces both memory usage and\nexecution time while maintaining precision. In particular, by solving in the\nFSConsG, CG-FSPTA achieves an average memory reduction of 33.05\\% and\naccelerates flow-sensitive pointer analysis by 7.27x compared to the\nstate-of-art method. These experimental results underscore the efficacy of\nCG-FSPTA as a scalable solution to analyze large-scale software systems,\nestablishing a robust foundation for future advancements in efficient program\nanalysis frameworks.", "AI": {"tldr": "CG-FSPTA improves flow-sensitive pointer analysis efficiency by 7.27x with 33% less memory using constraint graphs instead of control flow graphs.", "motivation": "Existing flow-sensitive pointer analysis approaches based on control flow graphs suffer from computational inefficiencies due to complex structures when resolving points-to information.", "method": "CG-FSPTA uses Flow-Sensitive Constraint Graphs (FSConsG) to combine structural advantages of set-constraint graphs with flow sensitivity through graph optimization and dynamic solving techniques.", "result": "CG-FSPTA achieves 33.05% average memory reduction and 7.27x execution speedup compared to state-of-the-art methods while maintaining precision.", "conclusion": "CG-FSPTA establishes a scalable, efficient solution for flow-sensitive pointer analysis, providing a robust foundation for future program analysis frameworks."}}
{"id": "2508.01249", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01249", "abs": "https://arxiv.org/abs/2508.01249", "authors": ["Peiran Wang", "Yang Liu", "Yunfei Lu", "Yifeng Cai", "Hongbo Chen", "Qingyou Yang", "Jie Zhang", "Jue Hong", "Ye Wu"], "title": "AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection", "comment": null, "summary": "Large Language Model (LLM) agents offer a powerful new paradigm for solving\nvarious problems by combining natural language reasoning with the execution of\nexternal tools. However, their dynamic and non-transparent behavior introduces\ncritical security risks, particularly in the presence of prompt injection\nattacks. In this work, we propose a novel insight that treats the agent runtime\ntraces as structured programs with analyzable semantics. Thus, we present\nAgentArmor, a program analysis framework that converts agent traces into graph\nintermediate representation-based structured program dependency representations\n(e.g., CFG, DFG, and PDG) and enforces security policies via a type system.\nAgentArmor consists of three key components: (1) a graph constructor that\nreconstructs the agent's working traces as graph-based intermediate\nrepresentations with control flow and data flow described within; (2) a\nproperty registry that attaches security-relevant metadata of interacted tools\n& data, and (3) a type system that performs static inference and checking over\nthe intermediate representation. By representing agent behavior as structured\nprograms, AgentArmor enables program analysis over sensitive data flow, trust\nboundaries, and policy violations. We evaluate AgentArmor on the AgentDojo\nbenchmark, the results show that AgentArmor can achieve 95.75% of TPR, with\nonly 3.66% of FPR. Our results demonstrate AgentArmor's ability to detect\nprompt injection vulnerabilities and enforce fine-grained security constraints.", "AI": {"tldr": "AgentArmor is a program analysis framework that treats LLM agent runtime traces as structured programs, enabling security policy enforcement through static type checking and achieving high detection accuracy for prompt injection vulnerabilities.", "motivation": "LLM agents introduce security risks via dynamic, non-transparent behavior and prompt injection attacks, necessitating program analysis techniques to detect policy violations and enforce security.", "method": "AgentArmor uses three components: (1) graph constructor to convert agent traces into CFG/DFG/PDG structures, (2) property registry to annotate tool/data metadata, and (3) type system for static inference/checking over the intermediate representation.", "result": "Evaluations on AgentDojo benchmark demonstrate 95.75% true positive rate (TPR) and 3.66% false positive rate (FPR) in detecting prompt injection vulnerabilities while enforcing security policies.", "conclusion": "Structured program analysis of LLM agent traces via AgentArmor enables robust detection of prompt injection attacks and effective enforcement of fine-grained security policies with strong empirical performance."}}
{"id": "2508.02023", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02023", "abs": "https://arxiv.org/abs/2508.02023", "authors": ["Huashan Lei", "Guanping Xiao", "Yepang Liu", "Zheng Zheng"], "title": "PCREQ: Automated Inference of Compatible Requirements for Python Third-party Library Upgrades", "comment": "52 pages, 33 figures", "summary": "Python third-party libraries (TPLs) are essential in modern software\ndevelopment, but upgrades often cause compatibility issues, leading to system\nfailures. These issues fall into two categories: version compatibility issues\n(VCIs) and code compatibility issues (CCIs). Existing tools mainly detect\ndependency conflicts but overlook code-level incompatibilities, with no\nsolution fully automating the inference of compatible versions for both VCIs\nand CCIs. To fill this gap, we propose PCREQ, the first approach to\nautomatically infer compatible requirements by combining version and code\ncompatibility analysis. PCREQ integrates six modules: knowledge acquisition,\nversion compatibility assessment, invoked APIs and modules extraction, code\ncompatibility assessment, version change, and missing TPL completion. PCREQ\ncollects candidate versions, checks for conflicts, identifies API usage,\nevaluates code compatibility, and iteratively adjusts versions to generate a\ncompatible requirements.txt with a detailed repair report. To evaluate PCREQ,\nwe construct REQBench, a large-scale benchmark with 2,095 upgrade test cases\n(including 406 unsolvable by pip). Results show PCREQ achieves a 94.03%\ninference success rate, outperforming PyEGo (37.02%), ReadPyE (37.16%), and\nLLM-based approaches (GPT-4o, DeepSeek V3/R1) by 18-20%. PCREQ processes each\ncase from REQBench in 60.79s on average, demonstrating practical efficiency.\nPCREQ significantly reduces manual effort in troubleshooting upgrades,\nadvancing Python dependency maintenance automation.", "AI": {"tldr": "PCREQ addresses Python library upgrade compatibility issues (version and code) through an automated analysis framework, achieving 94.03% success on a large benchmark.", "motivation": "Existing tools miss code-level incompatibilities and cannot fully automate combined version/code compatibility resolution for Python TPL upgrades.", "method": "Integrates six modules: knowledge acquisition, version compatibility assessment, API/module extraction, code compatibility assessment, version change, and missing TPL completion for iterative requirements.txt repair.", "result": "94.03% inference success rate (vs. 37-40% for baselines) on REQBench (2,095 cases), with 60.79s average processing time per case demonstrating practical efficiency.", "conclusion": "PCREQ advances Python dependency maintenance automation by addressing both version and code compatibility issues through a systematic approach with empirical validation."}}
{"id": "2508.01276", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01276", "abs": "https://arxiv.org/abs/2508.01276", "authors": ["Nilufer Gulciftci", "M. Emre Gursoy"], "title": "Defending Against Beta Poisoning Attacks in Machine Learning Models", "comment": null, "summary": "Poisoning attacks, in which an attacker adversarially manipulates the\ntraining dataset of a machine learning (ML) model, pose a significant threat to\nML security. Beta Poisoning is a recently proposed poisoning attack that\ndisrupts model accuracy by making the training dataset linearly nonseparable.\nIn this paper, we propose four defense strategies against Beta Poisoning\nattacks: kNN Proximity-Based Defense (KPB), Neighborhood Class Comparison\n(NCC), Clustering-Based Defense (CBD), and Mean Distance Threshold (MDT). The\ndefenses are based on our observations regarding the characteristics of\npoisoning samples generated by Beta Poisoning, e.g., poisoning samples have\nclose proximity to one another, and they are centered near the mean of the\ntarget class. Experimental evaluations using MNIST and CIFAR-10 datasets\ndemonstrate that KPB and MDT can achieve perfect accuracy and F1 scores, while\nCBD and NCC also provide strong defensive capabilities. Furthermore, by\nanalyzing performance across varying parameters, we offer practical insights\nregarding defenses' behaviors under varying conditions.", "AI": {"tldr": "This paper proposes four defenses (KPB, NCC, CBD, MDT) against Beta Poisoning attacks, which introduce non-separability in datasets. KPB and MDT achieve perfect accuracy/F1 scores on MNIST and CIFAR-10 datasets, while CBD and NCC also show strong performance.", "motivation": "Beta Poisoning attacks disrupt ML model accuracy by making datasets linearly non-separable, posing a critical threat to ML security. Effective defenses are needed to preserve model performance under such adversarial manipulation.", "method": "Based on observations of Beta Poisoning sample characteristics (e.g., proximity to each other and target class mean), four defenses were designed: (1) kNN proximity-based filtering, (2) neighborhood class comparison, (3) clustering-based isolation, and (4) mean distance thresholding. Each method exploits spatial relationships between samples and class distributions.", "result": "The proposed defenses achieved: (1) KPB and MDT reached 100% accuracy and F1 scores on MNIST/CIFAR-10, (2) CBD and NCC demonstrated strong, albeit slightly lower, defensive performance. Parameter analysis revealed practical insights into how these defenses behave under different attack conditions.", "conclusion": "KPB and MDT effectively neutralize Beta Poisoning attacks while maintaining full accuracy on benchmark datasets. The study provides a robust defense framework against non-separability-based poisoning, with actionable insights for parameter tuning in real-world ML deployment scenarios."}}
{"id": "2508.02144", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02144", "abs": "https://arxiv.org/abs/2508.02144", "authors": ["Yusaku Kato", "Norihiro Yoshida", "Erina Makihara", "Katsuro Inoue"], "title": "BiFuzz: A Two-Stage Fuzzing Tool for Open-World Video Games", "comment": "4 pages, 5 figures", "summary": "Open-world video games present a broader search space than other games,\nposing challenges for test automation. Fuzzing, which generates new inputs by\nmutating an initial input, is commonly used to uncover failures. In this study,\nwe proposed BiFuzz, a two-stage fuzzer designed for automated testing of\nopen-world video games, and investigated its effectiveness. The results\nrevealed that BiFuzz mutated the overall strategy of gameplay and test cases,\nincluding actual movement paths, step by step. Consequently, BiFuzz can detect\n`stucking' failures. The tool and its video are at\nhttps://github.com/Yusaku-Kato/BiFuzz.", "AI": {"tldr": "BiFuzz is a two-stage fuzzer for automated testing of open-world video games, detecting sticking failures through gameplay strategy mutation.", "motivation": "Open-world games have complex search spaces challenging traditional fuzzing methods; sticking failures remain undetected in automated testing.", "method": "Two-stage fuzzer using gameplay strategy modeling followed by step-by-step mutation of movement paths and test cases.", "result": "BiFuzz successfully identifies sticking failures by altering gameplay strategies; tool demonstrated via public GitHub repository with detailed video.", "conclusion": "BiFuzz effectively addresses open-world game testing challenges through strategic input mutation, offering a practical solution for critical failure detection."}}
{"id": "2508.01280", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.01280", "abs": "https://arxiv.org/abs/2508.01280", "authors": ["Wenwen Zhou", "Dongyang Lyu", "Xiaoqi Li"], "title": "Blockchain security based on cryptography: a review", "comment": null, "summary": "As an emerging service framework built by combining cryptography, P2P\nnetwork, consensus mechanism and innovative contract technology, blockchain has\nbeen widely used in digital finance, data sharing, message traceability and\nelectronic evidence preservation because of its decentralised, non-tamperable\nand transaction traceability. However, with the complex and changeable\napplication scenarios of blockchain technology and the continuous enhancement\nof blockchain attack technology, the security of the blockchain system has been\nseriously threatened, dramatically affecting the development and application of\nblockchain technology. This paper aims to analyse the attacks on blockchain\nfrom the perspective of cryptography. Firstly, from the cryptography technology\nin the blockchain, the principle of hash functions, digital signatures, and\nother technologies, as well as their role in the blockchain, are introduced.\nThen, based on the six-layer architecture of the blockchain, the attacks on the\ndata layer, the network layer, the consensus layer, the contract layer, the\nincentive layer and the application layer are analysed, and the methods to\nmitigate or resist the attacks are proposed. Secondly, the attack principles of\n51% attack, Double-Spending attack, Reentrancy attack, Replay attack, Sybil\nattack and Timestamp Tampering attack were analysed, and the mitigation or\ndefence solutions for these six attacks were designed. Finally, the core\nproblems to be solved in blockchain technology are summarised, and the future\ndevelopment of blockchain security technology is projected.", "AI": {"tldr": "The paper analyzes blockchain security threats from a cryptographic perspective, examines attacks across its six-layer architecture, and proposes mitigation strategies for common attack vectors like 51% attacks and reentrancy.", "motivation": "Blockchain security is increasingly vital due to the technology\u2019s growing adoption and sophisticated attacks. The paper addresses these threats to guide safe implementation and future development of blockchain systems.", "method": "The study first explains cryptographic fundamentals (hash functions, digital signatures) in blockchain. It then categorizes attacks by the blockchain\u2019s six architectural layers and analyzes six specific attack types with corresponding defense strategies.", "result": "The analysis provides methods to mitigate data layer tampering, network layer disruptions (e.g., Sybil attacks), consensus layer vulnerabilities (e.g., 51% attacks), and application layer risks (e.g., reentrancy). Practical solutions for each attack type are proposed.", "conclusion": "The paper emphasizes the need for robust cryptographic defenses at every blockchain layer and highlights future research directions to tackle emerging threats, ensuring the sustainable development of blockchain technology."}}
{"id": "2508.02167", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02167", "abs": "https://arxiv.org/abs/2508.02167", "authors": ["Yuxuan Wang", "Cristian Tirelli", "Giovanni Ansaloni", "Laura Pozzi", "David Atienza"], "title": "An MLIR-based Compilation Framework for Control Flow Management on CGRAs", "comment": null, "summary": "Coarse Grained Reconfigurable Arrays (CGRAs) present both high flexibility\nand efficiency, making them well-suited for the acceleration of intensive\nworkloads. Nevertheless, a key barrier towards their widespread adoption is\nposed by CGRA compilation, which must cope with a multi-dimensional space\nspanning both the spatial and the temporal domains. Indeed, state-of-the-art\ncompilers are limited in scope as they mostly deal with the data flow of\napplications, while having little or no support for control flow. Hence, they\nmostly target the mapping of single loops and/or delegate the management of\ncontrol flow divergences to ad-hoc hardware units.\n  Conversely, in this paper we show that control flow can be effectively\nmanaged and optimized at the compilation level, allowing for a broad set of\napplications to be targeted while being hardware-agnostic and achieving high\nperformance. We embody our methodology in a modular compilation framework\nconsisting of transformation and optimization passes, enabling support for\napplications with arbitrary control flows running on abstract CGRA meshes. We\nalso introduce a novel mapping methodology that acts as a compilation back-end,\naddressing the limitations in available CGRA hardware resources and\nguaranteeing a feasible solution in the compilation process. Our framework\nachieves up to 2.1X speedups over state-of-the-art approaches, purely through\ncompilation optimizations.", "AI": {"tldr": "This paper presents a compiler framework that effectively manages control flow in CGRAs through modular transformations and optimizations, achieving up to 2.1X speedups over existing methods without hardware-specific dependencies.", "motivation": "State-of-the-art CGRA compilers are limited to data flow analysis and lack robust support for control flow, restricting application scope and relying on ad-hoc hardware units for divergence handling. This limits widespread adoption.", "method": "Developed a hardware-agnostic compiler framework with control flow transformation/optimization passes and a resource-aware mapping algorithm for abstract CGRA meshes, addressing prior limitations in control flow and hardware constraints.", "result": "Achieved 2.1X speedup improvements through pure compilation optimizations, successfully mapping applications with complex control flows onto CGRAs without requiring architectural modifications.", "conclusion": "Compiler-level control flow management for CGRAs is viable and effective, enabling broader application support while maintaining hardware-agnostic flexibility and demonstrating significant performance gains through novel compilation strategies."}}
{"id": "2508.01332", "categories": ["cs.CR", "cs.AI", "68T42 (Primary), 94A60 (Secondary)", "I.2.11; E.3"], "pdf": "https://arxiv.org/pdf/2508.01332", "abs": "https://arxiv.org/abs/2508.01332", "authors": ["Zhenhua Zou", "Zhuotao Liu", "Lepeng Zhao", "Qiuyang Zhan"], "title": "BlockA2A: Towards Secure and Verifiable Agent-to-Agent Interoperability", "comment": "43 pages", "summary": "The rapid adoption of agentic AI, powered by large language models (LLMs), is\ntransforming enterprise ecosystems with autonomous agents that execute complex\nworkflows. Yet we observe several key security vulnerabilities in LLM-driven\nmulti-agent systems (MASes): fragmented identity frameworks, insecure\ncommunication channels, and inadequate defenses against Byzantine agents or\nadversarial prompts. In this paper, we present the first systematic analysis of\nthese emerging multi-agent risks and explain why the legacy security strategies\ncannot effectively address these risks. Afterwards, we propose BlockA2A, the\nfirst unified multi-agent trust framework that enables secure and verifiable\nand agent-to-agent interoperability. At a high level, BlockA2A adopts\ndecentralized identifiers (DIDs) to enable fine-grained cross-domain agent\nauthentication, blockchain-anchored ledgers to enable immutable auditability,\nand smart contracts to dynamically enforce context-aware access control\npolicies. BlockA2A eliminates centralized trust bottlenecks, ensures message\nauthenticity and execution integrity, and guarantees accountability across\nagent interactions. Furthermore, we propose a Defense Orchestration Engine\n(DOE) that actively neutralizes attacks through real-time mechanisms, including\nByzantine agent flagging, reactive execution halting, and instant permission\nrevocation. Empirical evaluations demonstrate BlockA2A's effectiveness in\nneutralizing prompt-based, communication-based, behavioral and systemic MAS\nattacks. We formalize its integration into existing MAS and showcase a\npractical implementation for Google's A2A protocol. Experiments confirm that\nBlockA2A and DOE operate with sub-second overhead, enabling scalable deployment\nin production LLM-based MAS environments.", "AI": {"tldr": "BlockA2A is a unified trust framework for LLM-driven multi-agent systems (MASes) that addresses security vulnerabilities using decentralized identifiers (DIDs), blockchain-anchored ledgers, and smart contracts. It introduces a Defense Orchestration Engine (DOE) for real-time attack mitigation and demonstrates sub-second overhead in experiments.", "motivation": "Legacy security strategies fail to address fragmented identity frameworks, insecure communication channels, and risks from Byzantine agents/adversarial prompts in LLM-driven MASes, necessitating a unified solution for verifiable interoperability and accountability.", "method": "1) Decentralized identifiers (DIDs) for cross-domain agent authentication. 2) Blockchain-anchored ledgers for immutable auditability. 3) Smart contracts for dynamic context-aware access control. 4) Defense Orchestration Engine (DOE) with mechanisms like Byzantine agent flagging, reactive execution halting, and instant permission revocation.", "result": "Empirical evaluations show BlockA2A effectively neutralizes prompt-based, communication-based, behavioral, and systemic MAS attacks. Integration into Google's A2A protocol is formalized, with experiments confirming sub-second overhead for scalable deployment.", "conclusion": "BlockA2A establishes a decentralized trust model for secure LLM MAS interactions while maintaining performance viability for production environments through its integrated security mechanisms and real-time defense orchestration."}}
{"id": "2508.02176", "categories": ["cs.SE", "cs.HC", "D.2.3; D.2.6; D.2.5; H.5.2"], "pdf": "https://arxiv.org/pdf/2508.02176", "abs": "https://arxiv.org/abs/2508.02176", "authors": ["Andrew Tropin"], "title": "Highly Interactive Testing for Uninterrupted Development Flow", "comment": "12 pages, ICFP-2025", "summary": "Highly interactive development environments (HIDEs) enable uninterrupted\ndevelopment flow through continuous program evolution and rapid hypothesis\nchecking. However, traditional testing approaches -- typically executed\nseparately via CLI -- isolate tests from HIDE tooling (interactive debuggers,\nvalue and stack inspectors, etc.) and introduce disruptive delays due to coarse\nexecution granularity and lack of runtime context. This disconnect breaks\ndevelopment flow by exceeding critical attention thresholds. In this paper we\npresent a library that provides runtime representation for tests, allowing\ntight integration with HIDEs, and enabling immediate access to HIDE tooling in\nthe context of test failure. We then describe development workflows enhanced\nwith testing and demonstrate how they achieve subsecond test reexecution times\ncrucial for maintaining developer focus.", "AI": {"tldr": "This paper introduces a library that enhances testing in Highly Interactive Development Environments (HIDEs) by enabling runtime test representation and immediate integration with HIDE tools during test failures, reducing reexecution delays to below one second.", "motivation": "Traditional testing methods in CLI are isolated from HIDE tooling and cause disruptive delays due to coarse execution granularity and loss of runtime context, breaking developer focus.", "method": "A library was developed to embed test representation in runtime within HIDEs, allowing seamless tooling access and optimized test reexecution with subsecond performance.", "result": "The solution achieves test reexecution times under one second, maintaining developer flow by preserving runtime context and enabling rapid debugging within HIDEs.", "conclusion": "The proposed library bridges the gap between testing and HIDE tooling, proving that tight integration and low-latency reexecution are critical for uninterrupted development practices."}}
{"id": "2508.01343", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01343", "abs": "https://arxiv.org/abs/2508.01343", "authors": ["Dechao Kong", "Xiaoqi Li", "Wenkai Li"], "title": "UEChecker: Detecting Unchecked External Call Vulnerabilities in DApps via Graph Analysis", "comment": null, "summary": "The increasing number of attacks on the contract layer of DApps has resulted\nin economic losses amounting to $66 billion. Vulnerabilities arise when\ncontracts interact with external protocols without verifying the results of the\ncalls, leading to exploit entry points such as flash loan attacks and\nreentrancy attacks. In this paper, we propose UEChecker, a deep learning-based\ntool that utilizes a call graph and a Graph Convolutional Network to detect\nunchecked external call vulnerabilities. We design the following components: An\nedge prediction module that reconstructs the feature representation of nodes\nand edges in the call graph; A node aggregation module that captures structural\ninformation from both the node itself and its neighbors, thereby enhancing\nfeature representation between nodes and improving the model's understanding of\nthe global graph structure; A Conformer Block module that integrates multi-head\nattention, convolutional modules, and feedforward neural networks to more\neffectively capture dependencies of different scales within the call graph,\nextending beyond immediate neighbors and enhancing the performance of\nvulnerability detection. Finally, we combine these modules with Graph\nConvolutional Network to detect unchecked external call vulnerabilities. By\nauditing the smart contracts of 608 DApps, our results show that our tool\nachieves an accuracy of 87.59% in detecting unchecked external call\nvulnerabilities. Furthermore, we compare our tool with GAT, LSTM, and GCN\nbaselines, and in the comparison experiments, UEChecker consistently\noutperforms these models in terms of accuracy.", "AI": {"tldr": "UEChecker is a deep learning-based tool utilizing call graphs and Graph Convolutional Networks (GCN) to detect unchecked external call vulnerabilities in DApps, achieving 87.59% accuracy through modules for edge prediction, node aggregation, and Conformer Block integration.", "motivation": "The paper addresses the $66 billion economic losses caused by contract layer attacks in DApps, particularly due to unchecked external calls that enable exploits like flash loan and reentrancy attacks. Existing detection methods lack precision in identifying these vulnerabilities.", "method": "UEChecker employs a call graph representation enhanced by three core modules: (1) Edge Prediction Module to reconstruct node/edge features, (2) Node Aggregation Module to capture local and global structural information through neighbor interactions, and (3) Conformer Block combining multi-head attention, convolution, and feedforward networks to model multi-scale dependencies. These are integrated with a GCN framework for vulnerability detection.", "result": "Evaluation on 608 DApp smart contracts demonstrated 87.59% detection accuracy for unchecked external call vulnerabilities. Comparative analysis showed UEChecker outperformed baseline models (GAT, LSTM, GCN) in accuracy metrics across all experiments.", "conclusion": "UEChecker provides an effective solution for detecting unchecked external call vulnerabilities through its novel combination of call graph analysis and deep learning modules. The framework consistently surpasses traditional GCN-based approaches, enabling robust detection of sophisticated financial exploits in blockchain environments."}}
{"id": "2508.02233", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02233", "abs": "https://arxiv.org/abs/2508.02233", "authors": ["Vincenzo De Martino", "Joel Casta\u00f1o", "Fabio Palomba", "Xavier Franch", "Silverio Mart\u00ednez-Fern\u00e1ndez"], "title": "A Methodological Framework for LLM-Based Mining of Software Repositories", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in software engineering\nresearch, offering new opportunities for automating repository mining tasks.\nHowever, despite their growing popularity, the methodological integration of\nLLMs into Mining Software Repositories (MSR) remains poorly understood.\nExisting studies tend to focus on specific capabilities or performance\nbenchmarks, providing limited insight into how researchers utilize LLMs across\nthe full research pipeline. To address this gap, we conduct a mixed-method\nstudy that combines a rapid review and questionnaire survey in the field of\nLLM4MSR. We investigate (1) the approaches and (2) the threats that affect the\nempirical rigor of researchers involved in this field. Our findings reveal 15\nmethodological approaches, nine main threats, and 25 mitigation strategies.\nBuilding on these findings, we present PRIMES 2.0, a refined empirical\nframework organized into six stages, comprising 23 methodological substeps,\neach mapped to specific threats and corresponding mitigation strategies,\nproviding prescriptive and adaptive support throughout the lifecycle of\nLLM-based MSR studies. Our work contributes to establishing a more transparent\nand reproducible foundation for LLM-based MSR research.", "AI": {"tldr": "This paper addresses the methodological integration of Large Language Models (LLMs) into Mining Software Repositories (MSR) research, proposing PRIMES 2.0, an empirical framework with 23 substeps to enhance the transparency and reproducibility of LLM-based MSR studies.", "motivation": "LLMs are gaining popularity in MSR research, but their full integration into the research pipeline lacks systematic investigation, leading to limited understanding of their empirical rigor.", "method": "The authors conducted a mixed-method study combining a rapid review and a questionnaire survey to identify methodological approaches, threats to empirical rigor, and mitigation strategies in LLM4MSR research.", "result": "Identified 15 methodological approaches, nine main threats, and 25 mitigation strategies. Introduced PRIMES 2.0, a six-stage framework with 23 substeps mapped to threats and strategies.", "conclusion": "PRIMES 2.0 establishes a structured approach to LLM-based MSR research, addressing gaps in empirical methodology and promoting translucency through threat-mitigation mapping across the research lifecycle."}}
{"id": "2508.01346", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.01346", "abs": "https://arxiv.org/abs/2508.01346", "authors": ["Hongli Peng", "Xiaoqi Li", "Wenkai Li"], "title": "MultiCFV: Detecting Control Flow Vulnerabilities in Smart Contracts Leveraging Multimodal Deep Learning", "comment": null, "summary": "The introduction of smart contract functionality marks the advent of the\nblockchain 2.0 era, enabling blockchain technology to support digital currency\ntransactions and complex distributed applications. However, many smart\ncontracts have been found to contain vulnerabilities and errors, leading to the\nloss of assets within the blockchain. Despite a range of tools that have been\ndeveloped to identify vulnerabilities in smart contracts at the source code or\nbytecode level, most rely on a single modality, reducing performance, accuracy,\nand limited generalization capabilities. This paper proposes a multimodal deep\nlearning approach, MultiCFV, which is designed specifically to analyze and\ndetect erroneous control flow vulnerability, as well as identify code clones in\nsmart contracts. Bytecode is generated from source code to construct control\nflow graphs, with graph embedding techniques extracting graph features.\nAbstract syntax trees are used to obtain syntax features, while code comments\ncapture key commentary words and comment features. These three feature vectors\nare fused to create a database for code inspection, which is used to detect\nsimilar code and identify contract vulnerabilities. Experimental results\ndemonstrate our method effectively combines structural, syntactic, and semantic\ninformation, improving the accuracy of smart contract vulnerability detection\nand clone detection.", "AI": {"tldr": "This paper introduces MultiCFV, a multimodal deep learning method for detecting control flow vulnerabilities and code clones in smart contracts by fusing graph, syntax, and comment features, improving detection accuracy and generalization.", "motivation": "Smart contracts are prone to vulnerabilities causing asset losses, and existing tools using single-modality approaches (source code or bytecode) lack performance, accuracy, and generalization. There is a need for a more effective method to address these issues.", "method": "MultiCFV combines three modalities: (1) control flow graphs from bytecode with graph embedding, (2) syntax features via abstract syntax trees, and (3) semantic features from code comments. These vectors are fused into an inspection database for vulnerability and clone detection.", "result": "Experiments demonstrate that MultiCFV improves accuracy for both erroneous control flow vulnerability detection and code clone identification in smart contracts, outperforming single-modality approaches.", "conclusion": "The paper concludes that integrating structural, syntactic, and semantic information through a multimodal framework enhances smart contract analysis, offering robust solutions for vulnerability detection and code similarity identification."}}
{"id": "2508.02279", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02279", "abs": "https://arxiv.org/abs/2508.02279", "authors": ["Mikio Nakano", "Hironori Takeuchi", "Sadahiro Yoshikawa", "Yoichi Matsuyama", "Kazunori Komatani"], "title": "Dialogue Systems Engineering: A Survey and Future Directions", "comment": "18 pages, 2 figures", "summary": "This paper proposes to refer to the field of software engineering related to\nthe life cycle of dialogue systems as Dialogue Systems Engineering, and surveys\nthis field while also discussing its future directions. With the advancement of\nlarge language models, the core technologies underlying dialogue systems have\nsignificantly progressed. As a result, dialogue system technology is now\nexpected to be applied to solving various societal issues and in business\ncontexts. To achieve this, it is important to build, operate, and continuously\nimprove dialogue systems correctly and efficiently. Accordingly, in addition to\napplying existing software engineering knowledge, it is becoming increasingly\nimportant to evolve software engineering tailored specifically to dialogue\nsystems. In this paper, we enumerate the knowledge areas of dialogue systems\nengineering based on those of software engineering, as defined in the Software\nEngineering Body of Knowledge (SWEBOK) Version 4.0, and survey each area. Based\non this survey, we identify unexplored topics in each area and discuss the\nfuture direction of dialogue systems engineering.", "AI": {"tldr": "The paper introduces Dialogue Systems Engineering (DSE) as a new field, surveys its knowledge areas based on SWEBOK, and outlines future directions for advancing tailored software practices for dialogue systems.", "motivation": "With advancements in large language models and growing applications of dialogue systems in societal and business contexts, there is a need for software engineering practices specifically adapted to address the unique challenges of dialogue systems' life cycle.", "method": "The authors enumerate the knowledge areas of DSE by extending the Software Engineering Body of Knowledge (SWEBOK) Version 4.0 framework and conduct a systematic survey to identify unexplored topics within each area.", "result": "The paper presents a structured survey of DSE knowledge areas, highlights gaps in existing software engineering practices, and provides an inventory of under-researched topics in dialogue systems engineering.", "conclusion": "Dialogue Systems Engineering emerges as a critical field requiring specialized software engineering approaches. The authors advocate for continued evolution of this discipline to address identified gaps and enable effective deployment of dialogue systems in real-world applications."}}
{"id": "2508.01351", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.01351", "abs": "https://arxiv.org/abs/2508.01351", "authors": ["Yuanzheng Niu", "Xiaoqi Li", "Wenkai Li"], "title": "NATLM: Detecting Defects in NFT Smart Contracts Leveraging LLM", "comment": null, "summary": "Security issues are becoming increasingly significant with the rapid\nevolution of Non-fungible Tokens (NFTs). As NFTs are traded as digital assets,\nthey have emerged as prime targets for cyber attackers. In the development of\nNFT smart contracts, there may exist undiscovered defects that could lead to\nsubstantial financial losses if exploited. To tackle this issue, this paper\npresents a framework called NATLM(NFT Assistant LLM), designed to detect\npotential defects in NFT smart contracts. The framework effectively identifies\nfour common types of vulnerabilities in NFT smart contracts: ERC-721\nReentrancy, Public Burn, Risky Mutable Proxy, and Unlimited Minting. Relying\nexclusively on large language models (LLMs) for defect detection can lead to a\nhigh false-positive rate. To enhance detection performance, NATLM integrates\nstatic analysis with LLMs, specifically Gemini Pro 1.5. Initially, NATLM\nemploys static analysis to extract structural, syntactic, and execution flow\ninformation from the code, represented through Abstract Syntax Trees (AST) and\nControl Flow Graphs (CFG). These extracted features are then combined with\nvectors of known defect examples to create a matrix for input into the\nknowledge base. Subsequently, the feature vectors and code vectors of the\nanalyzed contract are compared with the contents of the knowledge base.\nFinally, the LLM performs deep semantic analysis to enhance detection\ncapabilities, providing a more comprehensive and accurate identification of\npotential security issues. Experimental results indicate that NATLM analyzed\n8,672 collected NFT smart contracts, achieving an overall precision of 87.72%,\na recall of 89.58%, and an F1 score of 88.94%. The results outperform other\nbaseline experiments, successfully identifying four common types of defects.", "AI": {"tldr": "NATLM is a framework that combines static analysis and large language models (Gemini Pro 1.5) to detect four critical vulnerabilities in NFT smart contracts, achieving high precision, recall, and F1 scores.", "motivation": "Despite growing significance of NFTs as digital assets, undiscovered smart contract defects pose substantial financial risks. Current LLM-based methods for security analysis face high false-positive rates.", "method": "Integrates static analysis (AST/CFG extraction, feature vector generation) with semantic LLM analysis. Combines code features with vectors of known defect examples to create a knowledge base matrix, then uses vector comparisons and deep LLM analysis for detection.", "result": "Analyzed 8,672 NFT contracts with 87.72% precision, 89.58% recall, and 88.94% F1 score, outperforming baselines in detecting ERC-721 Reentrancy, Public Burn, Risky Mutable Proxy, and Unlimited Minting vulnerabilities.", "conclusion": "Hybrid approach of static analysis and LLMs significantly improves detection accuracy for NFT smart contract vulnerabilities compared to existing methods, offering a promising solution for blockchain security."}}
{"id": "2508.02335", "categories": ["cs.SE", "cs.DL"], "pdf": "https://arxiv.org/pdf/2508.02335", "abs": "https://arxiv.org/abs/2508.02335", "authors": ["Matteo Cancellieri", "Martin Docekal", "David Pride", "Morane Gruenpeter", "David Douard", "Petr Knoth"], "title": "Interoperable verification and dissemination of software assets in repositories using COAR Notify", "comment": "8 pages. Presented at the 20th International Conference on Open\n  Repositories, June 15-18 2025, Chicago, Illinois, USA", "summary": "The discoverability, attribution, and reusability of open research software\nare often hindered by its obscurity within academic manuscripts. To address\nthis, the SoFAIR project (2024-2025) introduces a comprehensive workflow\nleveraging machine learning tools for extracting software mentions from\nresearch papers. The project integrates repository systems, authors, and\nservices like HAL and Software Heritage to ensure proper archiving, citation,\nand accessibility of research software in alignment with FAIR principles. To\nenable interoperable communication across the various systems we present an\nintegration of the COAR Notify Protocol, which facilitates automated,\ninteroperable communication among repositories and authors to validate and\ndisseminate software mentions. This paper outlines the SoFAIR workflow and the\nimplementation of the COAR Notify Protocol, emphasising its potential to\nenhance the visibility and credibility of research software as first-class\nbibliographic records.", "AI": {"tldr": "The SoFAIR project (2024-2025) proposes a machine learning workflow and COAR Notify Protocol integration to enhance visibility and adherence to FAIR standards for open research software mentioned in academic manuscripts.", "motivation": "Research software often lacks proper discoverability, attribution, and reusability due to its obscurity in academic publications, hindering compliance with FAIR principles.", "method": "The project employs machine learning tools for extracting software mentions from papers, combined with COAR Notify Protocol integration to enable interoperable communication between repositories (HAL, Software Heritage), authors, and services.", "result": "Implementation of a workflow that automates software mention validation and dissemination through COAR Notify Protocol, aligning with FAIR principles.", "conclusion": "The proposed workflow and protocol integration significantly improve the visibility and credibility of research software, positioning it as first-class bibliographic records while ensuring its long-term accessibility and FAIR alignment."}}
{"id": "2508.01365", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01365", "abs": "https://arxiv.org/abs/2508.01365", "authors": ["Zihan Wang", "Rui Zhang", "Hongwei Li", "Wenshu Fan", "Wenbo Jiang", "Qingchuan Zhao", "Guowen Xu"], "title": "ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models", "comment": "Under review", "summary": "Backdoor attacks pose a significant threat to Large Language Models (LLMs),\nwhere adversaries can embed hidden triggers to manipulate LLM's outputs. Most\nexisting defense methods, primarily designed for classification tasks, are\nineffective against the autoregressive nature and vast output space of LLMs,\nthereby suffering from poor performance and high latency. To address these\nlimitations, we investigate the behavioral discrepancies between benign and\nbackdoored LLMs in output space. We identify a critical phenomenon which we\nterm sequence lock: a backdoored model generates the target sequence with\nabnormally high and consistent confidence compared to benign generation.\nBuilding on this insight, we propose ConfGuard, a lightweight and effective\ndetection method that monitors a sliding window of token confidences to\nidentify sequence lock. Extensive experiments demonstrate ConfGuard achieves a\nnear 100\\% true positive rate (TPR) and a negligible false positive rate (FPR)\nin the vast majority of cases. Crucially, the ConfGuard enables real-time\ndetection almost without additional latency, making it a practical backdoor\ndefense for real-world LLM deployments.", "AI": {"tldr": "The paper proposes ConfGuard, a lightweight detection method for backdoor attacks in LLMs by monitoring sequence lock patterns in token confidence.", "motivation": "Existing defenses for LLM backdoors are ineffective due to the models' autoregressive nature and large output space, leading to poor performance and high latency.", "method": "They identify 'sequence lock' phenomena (anomalously high confidence in target sequences of backdoored models) and propose a sliding window-based token confidence monitoring approach for lightweight detection.", "result": "ConfGuard achieves near 100% true positive rate with negligible false positive rate and minimal/zero additional latency in most cases through extensive experiments.", "conclusion": "ConfGuard provides a practical real-time defense mechanism for LLMs against backdoor attacks without compromising model performance through its confidence-based detection methodology."}}
{"id": "2508.02338", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02338", "abs": "https://arxiv.org/abs/2508.02338", "authors": ["Jiahui Wu", "Chengjie Lu", "Aitor Arrieta", "Shaukat Ali", "Thomas Peyrucain"], "title": "Vision Language Model-based Testing of Industrial Autonomous Mobile Robots", "comment": null, "summary": "Autonomous Mobile Robots (AMRs) are deployed in diverse environments (e.g.,\nwarehouses, retail spaces, and offices), where they work alongside humans.\nGiven that human behavior can be unpredictable and that AMRs may not have been\ntrained to handle all possible unknown and uncertain behaviors, it is important\nto test AMRs under a wide range of human interactions to ensure their safe\nbehavior. Moreover, testing in real environments with actual AMRs and humans is\noften costly, impractical, and potentially hazardous (e.g., it could result in\nhuman injury). To this end, we propose a Vision Language Model (VLM)-based\ntesting approach (RVSG) for industrial AMRs developed by PAL Robotics in Spain.\nBased on the functional and safety requirements, RVSG uses the VLM to generate\ndiverse human behaviors that violate these requirements. We evaluated RVSG with\nseveral requirements and navigation routes in a simulator using the latest AMR\nfrom PAL Robotics. Our results show that, compared with the baseline, RVSG can\neffectively generate requirement-violating scenarios. Moreover, RVSG-generated\nscenarios increase variability in robot behavior, thereby helping reveal their\nuncertain behaviors.", "AI": {"tldr": "The paper proposes RVSG, a VLM-based method for generating human-violating scenarios to test industrial AMRs in simulation, effectively revealing uncertain robot behaviors while avoiding real-world risks.", "motivation": "Testing AMRs in real-world environments is costly, impractical, and potentially hazardous due to unpredictable human behavior. There's a need for scalable safety validation methods under diverse human interactions.", "method": "RVSG uses a Vision Language Model to synthesize diverse human behaviors violating specified safety/functional requirements for AMRs. The approach generates realistic scenarios in simulation using PAL Robotics' latest AMR platform.", "result": "RVSG outperforms baselines in generating requirement-violating scenarios. The generated scenarios increase robot behavior variability, successfully uncovering uncertain behaviors in simulated testing environments.", "conclusion": "The proposed VLM-based testing approach enables effective safety validation of AMRs under unpredictable human interactions in controlled simulations, supporting robust industrial deployment."}}
{"id": "2508.01371", "categories": ["cs.CR", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.01371", "abs": "https://arxiv.org/abs/2508.01371", "authors": ["Zeke Xiao", "Yuekang Li", "Qin Wang", "Shiping Chen"], "title": "Prompt to Pwn: Automated Exploit Generation for Smart Contracts", "comment": null, "summary": "We explore the feasibility of using LLMs for Automated Exploit Generation\n(AEG) against vulnerable smart contracts. We present \\textsc{ReX}, a framework\nintegrating LLM-based exploit synthesis with the Foundry testing suite,\nenabling the automated generation and validation of proof-of-concept (PoC)\nexploits. We evaluate five state-of-the-art LLMs (GPT-4.1, Gemini 2.5 Pro,\nClaude Opus 4, DeepSeek, and Qwen3 Plus) on both synthetic benchmarks and\nreal-world smart contracts affected by known high-impact exploits. Our results\nshow that modern LLMs can reliably generate functional PoC exploits for diverse\nvulnerability types, with success rates reaching up to 92\\%. Notably, Gemini\n2.5 Pro and GPT-4.1 consistently outperform others in both synthetic and\nreal-world scenarios. We further analyze factors influencing AEG effectiveness,\nincluding model capabilities, contract structure, and vulnerability types. We\nalso collect the first curated dataset of real-world PoC exploits to support\nfuture research.", "AI": {"tldr": "This paper evaluates LLMs for automated smart contract exploit generation using ReX framework, finding 92% success rate with Gemini 2.5 Pro and GPT-4.1 as top performers.", "motivation": "Smart contracts face increasing security challenges, and existing AEG methods lack effectiveness against complex vulnerabilities. This research explores LLMs' potential to enable reliable exploit generation for vulnerability identification and mitigation.", "method": "The authors developed ReX, a framework combining LLM-based exploit synthesis with Foundry testing suite. They evaluated five state-of-the-art LLMs (GPT-4.1, Gemini 2.5 Pro, Claude Opus 4, DeepSeek, Qwen3 Plus) on synthetic benchmarks and real-world vulnerable contracts. They also analyzed model performance factors and created a curated dataset of real exploit examples.", "result": "Modern LLMs achieved 92% success rate in generating functional PoC exploits across diverse vulnerabilities. Gemini 2.5 Pro and GPT-4.1 showed best performance in both synthetic (84-95%) and real-world (78-94%) scenarios. The study identified correlations between model capacity, contract structure, and exploit discovery effectiveness.", "conclusion": "LLMs demonstrate strong potential for automated exploit generation against smart contracts. The ReX framework and new curated dataset provide valuable resources for advancing AEG research and improving smart contract security through automated vulnerability testing."}}
{"id": "2508.02397", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02397", "abs": "https://arxiv.org/abs/2508.02397", "authors": ["Lida Zhao", "Chaofan Li", "Yueming Wu", "Lyuye Zhang", "Jiahui Wu", "Chengwei Liu", "Sen Chen", "Yutao Hu", "Zhengzi Xu", "Yi Liu", "Jingquan Ge", "Jun Sun", "Yang Liu"], "title": "JC-Finder: Detecting Java Clone-based Third-Party Library by Class-level Tree Analysis", "comment": null, "summary": "While reusing third-party libraries (TPL) facilitates software development,\nits chaotic management has brought great threats to software maintenance and\nthe unauthorized use of source code also raises ethical problems such as\nmisconduct on copyrighted code. To identify TPL reuse in projects, Software\nComposition Analysis (SCA) is employed, and two categories of SCA techniques\nare used based on how TPLs are introduced: clone-based SCA and\npackage-manager-based SCA (PM-based SCA). Although introducing TPLs by clones\nis prevalent in Java, no clone-based SCA tools are specially designed for Java.\nAlso, directly applying clone-based SCA techniques from other tools is\nproblematic. To fill this gap, we introduce JC-Finder, a novel clone-based SCA\ntool that aims to accurately and comprehensively identify instances of TPL\nreuse introduced by source code clones in Java projects. JC-Finder achieves\nboth accuracy and efficiency in identifying TPL reuse from code cloning by\ncapturing features at the class level, maintaining inter-function\nrelationships, and excluding trivial or duplicated elements. To evaluate the\nefficiency of JC-Finder, we applied it to 9,965 most popular Maven libraries as\nreference data and tested the TPL reuse of 1,000 GitHub projects. The result\nshows that JC-Finder achieved an F1-score of 0.818, outperforming the other\nfunction-level tool by 0.427. The average time taken for resolving TPL reuse is\n14.2 seconds, which is approximately 9 times faster than the other tool. We\nfurther applied JC-Finder to 7,947 GitHub projects, revealing TPL reuse by code\nclones in 789 projects (about 9.89% of all projects) and identifying a total of\n2,142 TPLs. JC-Finder successfully detects 26.20% more TPLs that are not\nexplicitly declared in package managers.", "AI": {"tldr": "JC-Finder is a new Java-specific clone-based Software Composition Analysis (SCA) tool achieving 0.818 F1-score and 9x faster detection than existing tools, uncovering a significant amount of previously undetected third-party library reuse in GitHub projects.\n", "motivation": "Java's prevalent but poorly managed third-party library (TPL) cloning introduces maintenance risks and Copyright violations. Clone-based SCA tools lack Java specificity, and directly applying cross-language solutions proves ineffective due to the lack of appropriate training data and language-specific challenges.\n", "method": "JC-Finder employs class-level feature extraction, preserves inter-function semantic relationships, and filters out trivial/duplicated elements through a multi-phase filtering strategy. It utilizes a supervised machine learning approach with training data derived from Java-specific patterns.", "result": "Evaluated on 9,965 Maven libraries and 1,000 GitHub projects, JC-Finder achieved 0.818 F1-score (0.427 improvement over previous tools) and 14.2 sec/analysis time (9x faster). Scanning 7,947 projects identified 2,142 TPLs with code clones, revealing 26.2% more TPL reuse than existing package managers could detect.", "conclusion": "This study establishes JC-Finder as the first specialized Java clone-based SCA tool, demonstrating significant performance advantages in both accuracy and speed for detecting untracked TPL reuse. The findings highlight the prevalence of undetected code cloning in Java projects and the importance of language-specific approaches to Software Composition Analysis."}}
{"id": "2508.01422", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.01422", "abs": "https://arxiv.org/abs/2508.01422", "authors": ["Biswajit Chandra Das", "M Saif Sartaz", "Syed Ali Reza", "Arat Hossain", "Md Nasiruddin", "Kanchon Kumar Bishnu", "Kazi Sharmin Sultana", "Sadia Sharmeen Shatyi", "MD Azam Khan", "Joynal Abed"], "title": "AI-Driven Cybersecurity Threat Detection: Building Resilient Defense Systems Using Predictive Analytics", "comment": null, "summary": "This study examines how Artificial Intelligence can aid in identifying and\nmitigating cyber threats in the U.S. across four key areas: intrusion\ndetection, malware classification, phishing detection, and insider threat\nanalysis. Each of these problems has its quirks, meaning there needs to be\ndifferent approaches to each, so we matched the models to the shape of the\nproblem. For intrusion detection, catching things like unauthorized access, we\ntested unsupervised anomaly detection methods. Isolation forests and deep\nautoencoders both gave us useful signals by picking up odd patterns in network\ntraffic. When it came to malware detection, we leaned on ensemble models like\nRandom Forest and XGBoost, trained on features pulled from files and traffic\nlogs. Phishing was more straightforward. We fed standard classifiers (logistic\nregression, Random Forest, XGBoost) a mix of email and web-based features.\nThese models handled the task surprisingly well. Phishing turned out to be the\neasiest problem to crack, at least with the data we had. There was a different\nstory. We utilized an LSTM autoencoder to identify behavioral anomalies in user\nactivity logs. It caught every suspicious behavior but flagged a lot of\nharmless ones too. That kind of model makes sense when the cost of missing a\nthreat is high and you are willing to sift through some noise. What we saw\nacross the board is that performance was not about stacking the most complex\nmodel. What mattered was how well the models structure matched the way the data\nbehaved. When signals were strong and obvious, simple models worked fine. But\nfor messier, more subtle threats, we needed something more adaptive, sequence\nmodels and anomaly detectors, though they brought their trade offs. The\ntakeaway here is clear in cybersecurity, context drives the solution.", "AI": {"tldr": "The study explores the application of tailored AI models in cybersecurity across four areas: intrusion detection, malware classification, phishing detection, and insider threat analysis, emphasizing the importance of matching models to data characteristics for optimal threat detection.", "motivation": "To address the challenge of diverse cyber threats in the U.S. by developing specialized AI models that align with the inherent structures of problems like intrusion detection, malware classification, phishing, and insider threats.", "method": "1) Intrusion detection: unsupervised anomaly detection (isolation forest, deep autoencoders). 2) Malware classification: ensemble models (Random Forest, XGBoost) trained on file/traffic features. 3) Phishing detection: standard classifiers (logistic regression, Random Forest, XGBoost). 4) Insider threat analysis: LSTM autoencoder with behavioral features in user activity logs.", "result": "Intrusion detection methods successfully identified traffic anomalies. Ensemble models excelled in malware detection but lacked adaptability. Phishing models achieved high accuracy with minimal complexity. LSTM autoencoder captured all behavioral threats but produced numerous false positives, demonstrating a trade-off between sensitivity and precision.", "conclusion": "The study underscores the significance of contextual problem structure over model complexity in cybersecurity. Simple models work for clear threats, while adaptive models (sequence models, anomaly detectors) are critical for subtle threats despite their drawbacks, emphasizing that context drives effective threat-mitigation strategies."}}
{"id": "2508.02407", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02407", "abs": "https://arxiv.org/abs/2508.02407", "authors": ["Xinyi Wang", "Qinghua Xu", "Paolo Arcaini", "Shaukat Ali", "Thomas Peyrucain"], "title": "Quantum Machine Learning-based Test Oracle for Autonomous Mobile Robots", "comment": null, "summary": "Robots are increasingly becoming part of our daily lives, interacting with\nboth the environment and humans to perform their tasks. The software of such\nrobots often undergoes upgrades, for example, to add new functionalities, fix\nbugs, or delete obsolete functionalities. As a result, regression testing of\nrobot software becomes necessary. However, determining the expected correct\nbehavior of robots (i.e., a test oracle) is challenging due to the potentially\nunknown environments in which the robots must operate. To address this\nchallenge, machine learning (ML)-based test oracles present a viable solution.\nThis paper reports on the development of a test oracle to support regression\ntesting of autonomous mobile robots built by PAL Robotics (Spain), using\nquantum machine learning (QML), which enables faster training and the\nconstruction of more precise test oracles. Specifically, we propose a hybrid\nframework, QuReBot, that combines both quantum reservoir computing (QRC) and a\nsimple neural network, inspired by residual connection, to predict the expected\nbehavior of a robot. Results show that QRC alone fails to converge in our case,\nyielding high prediction error. In contrast, QuReBot converges and achieves 15%\nreduction of prediction error compared to the classical neural network\nbaseline. Finally, we further examine QuReBot under different configurations\nand offer practical guidance on optimal settings to support future robot\nsoftware testing.", "AI": {"tldr": "This paper introduces QuReBot, a hybrid quantum machine learning framework for regression testing of autonomous mobile robots. It combines quantum reservoir computing with a neural network to reduce prediction errors by 15% compared to classical methods, addressing challenges in creating effective test oracles for unknown environments.", "motivation": "Robot software upgrades necessitate regression testing, but constructing reliable test oracles (expected behavior predictions) is difficult due to unpredictable operating environments. Classical methods fall short in accuracy and speed for complex robotic systems.", "method": "Proposes QuReBot, a hybrid quantum machine learning framework integrating quantum reservoir computing (QRC) with residual-connection-inspired neural networks. QRC provides faster training and higher precision, while the neural network enhances convergence stability.", "result": "QuReBot achieves 15% lower prediction error than classical neural networks. QRC alone fails to converge, but the hybrid framework remains stable and effective. Practical configurations for optimal performance are identified.", "conclusion": "QuReBot effectively addresses test oracle challenges in robot software regression testing by combining QRC with neural networks. The hybrid approach improves accuracy and convergence, offering usable guidance for future robot software testing in complex environments."}}
{"id": "2508.01448", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.01448", "abs": "https://arxiv.org/abs/2508.01448", "authors": ["Mirza Ahad Baig", "Christoph U. G\u00fcnther", "Krzysztof Pietrzak"], "title": "Nakamoto Consensus from Multiple Resources", "comment": "Full version of the paper published at AFT25", "summary": "The blocks in the Bitcoin blockchain record the amount of work W that went\ninto creating them through proofs of work. When honest parties control a\nmajority of the work, consensus is achieved by picking the chain with the\nhighest recorded weight. Resources other than work have been considered to\nsecure such longest-chain blockchains. In Chia, blocks record the amount of\nspace S (via a proof of space) and sequential computational steps V (via a\nVDF).\n  In this paper, we ask what weight functions {\\Gamma}(S,V,W) (that assign a\nweight to a block as a function of the recorded space, speed, and work) are\nsecure in the sense that whenever the weight of the resources controlled by\nhonest parties is larger than the weight of adversarial parties, the blockchain\nis secure against private double-spending attacks.\n  We completely classify such functions in an idealized \"continuous\" model:\n{\\Gamma}(S,V,W) is secure against private double-spending attacks if and only\nif it is homogeneous of degree one in the timed resources V and W, i.e.,\n{\\alpha}{\\Gamma}(S,V,W)={\\Gamma}(S,{\\alpha}V, {\\alpha}W). This includes Bitcoin\nrule {\\Gamma}(S,V,W)=W and Chia rule {\\Gamma}(S,V,W) = SV. In a more realistic\nmodel where blocks are created at discrete time-points, one additionally needs\nsome mild assumptions on the dependency on S (basically, the weight should not\ngrow too much if S is slightly increased, say linear as in Chia).\n  Our classification is more general and allows various instantiations of the\nsame resource. It provides a powerful tool for designing new longest-chain\nblockchains. E.g., consider combining different PoWs to counter centralization,\nsay the Bitcoin PoW W_1 and a memory-hard PoW W_2. Previous work suggested to\nuse W_1+W_2 as weight. Our results show that using\n{\\sqrt}(W_1){\\cdot}{\\sqrt}(W_2), {\\min}{W_1,W_2} are also secure, and we argue\nthat in practice these are much better choices.", "AI": {"tldr": "The paper analyzes weight functions for securing longest-chain blockchains with space (S), verification (V), and work (W) resources. It classifies functions \u0393(S,V,W) that are secure against private double-spending attacks when honest parties control more weight than adversaries. The results show homogeneity of degree one in V and W is key, with examples including Bitcoin\u2019s work-based function and Chia\u2019s space-time combination.", "motivation": "Bitcoin uses work-proportional consensus, while Chia introduces space and time (VDFs). This work aims to identify general weight functions combining S, V, W that ensure security under majority resource control, enabling better blockchain designs with mixed resources.", "method": "The paper uses a continuous-time idealized model to prove that \u0393(S,V,W) must be homogeneous of degree one in V and W for security. It then examines discrete-time models, imposing linear assumptions on S to ensure real-world applicability.", "result": "In the continuous model, secure \u0393 functions are exactly those homogeneous in V and W. For discrete models, security requires mild S constraints (e.g., linearity in S). New secure examples include \u221a(W\u2081W\u2082) and min{W\u2081,W\u2082}, outperforming existing approaches like W\u2081+W\u2082 in decentralization.", "conclusion": "The classification provides a design framework for longest-chain blockchains, demonstrating that both Bitcoin and Chia\u2019s approaches are special cases. Proposed alternatives like \u221a(W\u2081W\u2082) are shown to better prevent centralization than summation-based methods."}}
{"id": "2508.02455", "categories": ["cs.SE", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.02455", "abs": "https://arxiv.org/abs/2508.02455", "authors": ["Daniele Cipollone", "Egor Bogomolov", "Arie van Deursen", "Maliheh Izadi"], "title": "TreeRanker: Fast and Model-agnostic Ranking System for Code Suggestions in IDEs", "comment": null, "summary": "Token-level code completion is one of the most critical features in modern\nIntegrated Development Environments (IDEs). It assists developers by suggesting\nrelevant identifiers and APIs during coding. While completions are typically\nderived from static analysis, their usefulness depends heavily on how they are\nranked, as correct predictions buried deep in the list are rarely seen by\nusers. Most current systems rely on hand-crafted heuristics or lightweight\nmachine learning models trained on user logs, which can be further improved to\ncapture context information and generalize across projects and coding styles.\nIn this work, we propose a new scoring approach to ranking static completions\nusing language models in a lightweight and model-agnostic way. Our method\norganizes all valid completions into a prefix tree and performs a single greedy\ndecoding pass to collect token-level scores across the tree. This enables a\nprecise token-aware ranking without needing beam search, prompt engineering, or\nmodel adaptations. The approach is fast, architecture-agnostic, and compatible\nwith already deployed models for code completion. These findings highlight a\npractical and effective pathway for integrating language models into already\nexisting tools within IDEs, and ultimately providing smarter and more\nresponsive developer assistance.", "AI": {"tldr": "This paper introduces a new lightweight, model-agnostic approach for ranking static code completions using prefix trees and greedy decoding to improve accuracy and usability.", "motivation": "Current code completion systems rely on heuristics or user-log-based models that fail to generalize context and rankings across diverse projects and coding styles, limiting their effectiveness.", "method": "The method constructs a prefix tree of valid completions and employs a single greedy decoding pass to compute token-level scores, enabling precise ranking without beam search, prompt engineering, or model modifications.", "result": "The proposed approach is shown to be fast and architecture-agnostic while maintaining compatibility with existing deployed code completion models, demonstrating its practicality for IDE integration.", "conclusion": "The work provides a scalable solution for enhancing code completion tools by leveraging language models in a way that avoids complex parameterization and preserves existing model capabilities."}}
{"id": "2508.01451", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01451", "abs": "https://arxiv.org/abs/2508.01451", "authors": ["Mohammed Sayagh", "Mohammad Ghafari"], "title": "Think Broad, Act Narrow: CWE Identification with Multi-Agent Large Language Models", "comment": null, "summary": "Machine learning and Large language models (LLMs) for vulnerability detection\nhas received significant attention in recent years. Unfortunately,\nstate-of-the-art techniques show that LLMs are unsuccessful in even\ndistinguishing the vulnerable function from its benign counterpart, due to\nthree main problems: Vulnerability detection requires deep analysis, which LLMs\noften struggle with when making a one-shot prediction. Existing techniques\ntypically perform function-level analysis, whereas effective vulnerability\ndetection requires contextual information beyond the function scope. The focus\non binary classification can result in identifying a vulnerability but\nassociating it with the wrong security weaknesses (CWE), which may mislead\ndevelopers. We propose a novel multi-agent LLM approach to address the\nchallenges of identifying CWEs. This approach consists of three steps: (1) a\nteam of LLM agents performs an exhaustive search for potential CWEs in the\nfunction under review, (2) another team of agents identifies relevant external\ncontext to support or refute each candidate CWE, and (3) a final agent makes\ninformed acceptance or rejection decisions for each CWE based on the gathered\ncontext. A preliminary evaluation of our approach shows promising results. In\nthe PrimeVul dataset, Step 1 correctly identifies the appropriate CWE in 40.9\\%\nof the studied vulnerable functions. We further evaluated the full pipeline on\nten synthetic programs and found that incorporating context information\nsignificantly reduced false positives from 6 to 9 CWEs to just 1 to 2, while\nstill correctly identifying the true CWE in 9 out of 10 cases.", "AI": {"tldr": "This paper presents a novel multi-agent LLM approach to address three challenges in vulnerability detection: lack of deep analysis, insufficient contextual information, and incorrect CWE associations. The method shows promising results by reducing false positives and improving accuracy in synthetic programs.", "motivation": "Current LLM-based vulnerability detection struggles with distinguishing vulnerabilities from benign code due to (1) limited deep analysis in one-shot predictions, (2) reliance on function-level analysis without broader contextual evidence, and (3) failure to correctly associate vulnerabilities with the right Common Weakness Enumeration (CWE) identifiers, which could mislead developers.", "method": "The proposed approach employs three stages: 1) A team of LLM agents conducts an exhaustive search for potential CWEs in the target function. 2) Another agent group analyzes external context to support/refute each candidate CWE. 3) A final agent makes informed decisions about CWE acceptance/rejection based on the contextual evidence collected during prior stages.", "result": "1) Step 1 achieves 40.9% correct CWE identification rate on the PrimeVul dataset. 2) Full pipeline evaluation on ten synthetic programs reduces false positives from 6-9 CWEs to 1-2 while maintaining 9/10 true positive identification rates. The system demonstrates significantly better context-sensitive analysis compared to prior methods.", "conclusion": "The multi-agent LLM framework offers a promising solution to improve CWE detection accuracy by leveraging distributed analysis of functional and contextual evidence. Preliminary results suggest this approach enhances precision and reduces false positives through collaborative validation of vulnerability hypotheses, though broader evaluations are needed for final assessment."}}
{"id": "2508.02473", "categories": ["cs.SE", "cs.LG", "68N30", "D.2.3; D.1.2; I.2.2"], "pdf": "https://arxiv.org/pdf/2508.02473", "abs": "https://arxiv.org/abs/2508.02473", "authors": ["Xinfang Chen", "Siyang Xiao", "Xianying Zhu", "Junhong Xie", "Ming Liang", "Dajun Chen", "Wei Jiang", "Yong Li", "Peng Di"], "title": "An Efficient and Adaptive Next Edit Suggestion Framework with Zero Human Instructions in IDEs", "comment": "13 pages", "summary": "Code editing, including modifying, refactoring, and maintaining existing\ncode, is the most frequent task in software development and has garnered\nsignificant attention from AI-powered tools. However, existing solutions that\ntranslate explicit natural language instructions into code edits face critical\nlimitations, such as heavy reliance on human instruction input and high\nlatency, which hinder their effective integration into a developer's workflow.\nWe observe that developers' habitual behaviors and coding objectives are often\nreflected in their historical editing patterns, making this data key to\naddressing existing limitations. To leverage these insights, we propose NES\n(Next Edit Suggestion), an LLM-driven code editing framework that delivers an\ninstruction-free and low-latency experience. Built on a dual-model architecture\nand trained with our high-quality SFT and DAPO datasets, NES enhances\nproductivity by understanding developer intent while optimizing inference to\nminimize latency. NES is a scalable, industry-ready solution with a continuous\nTab key interaction workflow, seamlessly adopted by a FinTech company with over\n20,000 developers. Evaluations on real-world datasets show NES achieves 75.6%\nand 81.6% accuracy in two tasks of predicting next edit locations, alongside\n91.36% ES and 27.7% EMR for intent-aligned edits, outperforming SOTA models.\nOur open-sourced SFT and DAPO datasets have been demonstrated to enhance the\nperformance of open-source CodeLLMs. The demonstration of NES is available at\nhttps://youtu.be/yGoyYOe6fbY.", "AI": {"tldr": "NES is an LLM-based code editing framework that improves developer productivity through instruction-free next edit prediction, leveraging dual-model architecture and novel datasets to overcome latency issues and reduce dependency on natural language instructions.", "motivation": "Current AI code editing tools rely heavily on explicit natural language instructions and suffer from high latency, limiting their workflow integration. Developers exhibit consistent historical editing patterns that could be utilized for more efficient prediction.", "method": "The framework employs a dual-model design with a dual-attention mechanism for low-latency inference, trained on SFT and DAPO datasets derived from real developer interactions. It uses continuous Tab key interactions for seamless adoption.", "result": "Achieves 75.6% and 81.6% accuracy in next edit location prediction, with 91.36% Edit Similarity (ES) and 27.7% Edit Match Rate (EMR) for intent-aligned edits. Outperforms state-of-the-art models and improves open-source CodeLLMs with new datasets.", "conclusion": "NES demonstrates a viable instruction-free code editing paradigm that can be scaled in production environments, validated by adoption at a 20k+ developer FinTech company and superior performance metrics on real-world datasets."}}
{"id": "2508.01469", "categories": ["cs.CR", "cs.NI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.01469", "abs": "https://arxiv.org/abs/2508.01469", "authors": ["Imtiaz Karim", "Hyunwoo Lee", "Hassan Asghar", "Kazi Samin Mubasshir", "Seulgi Han", "Mashroor Hasan Bhuiyan", "Elisa Bertino"], "title": "VWAttacker: A Systematic Security Testing Framework for Voice over WiFi User Equipments", "comment": null, "summary": "We present VWAttacker, the first systematic testing framework for analyzing\nthe security of Voice over WiFi (VoWiFi) User Equipment (UE) implementations.\nVWAttacker includes a complete VoWiFi network testbed that communicates with\nCommercial-Off-The-Shelf (COTS) UEs based on a simple interface to test the\nbehavior of diverse VoWiFi UE implementations; uses property-guided adversarial\ntesting to uncover security issues in different UEs systematically. To reduce\nmanual effort in extracting and testing properties, we introduce an LLM-based,\nsemi-automatic, and scalable approach for property extraction and testcase (TC)\ngeneration. These TCs are systematically mutated by two domain-specific\ntransformations. Furthermore, we introduce two deterministic oracles to detect\nproperty violations automatically. Coupled with these techniques, VWAttacker\nextracts 63 properties from 11 specifications, evaluates 1,116 testcases, and\ndetects 13 issues in 21 UEs. The issues range from enforcing a DH shared secret\nto 0 to supporting weak algorithms. These issues result in attacks that expose\nthe victim UE's identity or establish weak channels, thus severely hampering\nthe security of cellular networks. We responsibly disclose the findings to all\nthe related vendors. At the time of writing, one of the vulnerabilities has\nbeen acknowledged by MediaTek with high severity.", "AI": {"tldr": "VWAttacker is a systematic testing framework for VoWiFi UE security, using property-guided adversarial testing and LLM-based techniques to extract properties, generate testcases, detect 13 issues in 21 UEs, with one high-severity vulnerability acknowledged by MediaTek.", "motivation": "Current manual methods for testing VoWiFi security are labor-intensive and inconsistent, risking undetected vulnerabilities. The paper addresses this by introducing a scalable, semi-automated framework.", "method": "1. Built a VoWiFi testbed interface to evaluate COTS UEs. 2. Developed an LLM-based approach for property extraction and testcase generation. 3. Mutated testcases via domain-specific transformations. 4. Implemented deterministic oracles to detect property violations.", "result": "Extracted 63 properties from 11 specs, executed 1,116 testcases, identified 13 security issues (e.g., enforcing 0 as DH shared secret, supporting weak algorithms) in 21 UEs. One vulnerability received high-severity acknowledgment from MediaTek.", "conclusion": "VWAttacker demonstrates the feasibility of systematic adversarial testing for VoWiFi security, automating property extraction and violation detection. Responsible disclosure practices and vendor collaboration are emphasized to enhance cellular network security."}}
{"id": "2508.02487", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02487", "abs": "https://arxiv.org/abs/2508.02487", "authors": ["Elijah Kayode Adejumo", "Brittany Johnson", "Mariam Guizani"], "title": "Commit Stability as a Signal for Risk in Open-Source Projects", "comment": null, "summary": "Open source software (OSS) generates trillions of dollars in economic value\nand has become essential to technical infrastructures worldwide. As\norganizations increasingly depend on OSS, understanding project evolution is\ncritical. While existing metrics provide insights into project health, one\ndimension remains understudied: project resilience -- the ability to return to\nnormal operations after disturbances such as contributor departures, security\nvulnerabilities, and bug report spikes. We hypothesize that stable commit\npatterns reflect underlying project characteristics such as mature governance,\nsustained contributors, and robust development processes that enable\nresilience. Building on the Composite Stability Index (CSI) framework, we\nempirically validate commit frequency patterns across 100 highly ranked\nrepositories. Our findings reveal that only 2\\% of repositories exhibit daily\nstability, 29\\% achieve weekly stability, and 50\\% demonstrate monthly\nstability, while half remain unstable across all temporal levels. Programming\nlanguages and blockchain applications were the most stable. We identified two\nexemplary repositories that achieved stability at all three granularities,\nwhose governance models, CI cadence, and release policies could serve as\nreference frameworks. We observed that large yearly commit throughput does not\nnecessarily correlate with stability. Beyond commits, stability can be enriched\nwith issue-resolution times, PR merge rates, and community-engagement metrics\nto broaden resilience assessment and sharpen stability-based risk evaluation.", "AI": {"tldr": "This paper analyzes project resilience in open source software via commit patterns, finding only 2% of repositories achieve daily stability, with larger yearly commit counts not necessarily indicating resilience.", "motivation": "Understanding organizational OSS reliance, existing health metrics lack focus on post-disturbance project resilience (recovering from contributor departures, security issues, and bug spikes).", "method": "Built on the Composite Stability Index (CSI), validated commit frequency patterns across 100 highly ranked repositories, examining daily/weekly/monthly stability alongside programming language, blockchain focus, and governance models.", "result": "2% daily stability, 29% weekly, 50% monthly\u7a33\u5b9a\u6027; programming languages and blockchain repos\u6700 stable; two repos achieved stability across all granularity levels; yearly commit throughput doesn't consistently imply stability; issue-resolution times, PR merge rates, and community engagement metrics can enrich resilience analysis.", "conclusion": "Stable commit patterns signal mature governance, sustained contributors, and robust processes essential for OSS resilience. Stability-based risk evaluation should incorporate diverse metrics beyond just commit frequency to improve project management and risk mitigation."}}
{"id": "2508.01479", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SI", "68M14, 68T05, 05C65, 47H10, 94A60", "C.2.4; D.4.6; I.2.6; G.2.2; F.1.2"], "pdf": "https://arxiv.org/pdf/2508.01479", "abs": "https://arxiv.org/abs/2508.01479", "authors": ["Faruk Alpay", "Taylan Alpay", "Bugra Kilictas"], "title": "Reconstructing Trust Embeddings from Siamese Trust Scores: A Direct-Sum Approach with Fixed-Point Semantics", "comment": "22 pages, 3 figures, 1 table", "summary": "We study the inverse problem of reconstructing high-dimensional trust\nembeddings from the one-dimensional Siamese trust scores that many\ndistributed-security frameworks expose. Starting from two independent agents\nthat publish time-stamped similarity scores for the same set of devices, we\nformalise the estimation task, derive an explicit direct-sum estimator that\nconcatenates paired score series with four moment features, and prove that the\nresulting reconstruction map admits a unique fixed point under a contraction\nargument rooted in Banach theory. A suite of synthetic benchmarks (20 devices x\n10 time steps) confirms that, even in the presence of Gaussian noise, the\nrecovered embeddings preserve inter-device geometry as measured by Euclidean\nand cosine metrics; we complement these experiments with non-asymptotic error\nbounds that link reconstruction accuracy to score-sequence length. Beyond\nmethodology, the paper demonstrates a practical privacy risk: publishing\ngranular trust scores can leak latent behavioural information about both\ndevices and evaluation models. We therefore discuss counter-measures -- score\nquantisation, calibrated noise, obfuscated embedding spaces -- and situate them\nwithin wider debates on transparency versus confidentiality in networked AI\nsystems. All datasets, reproduction scripts and extended proofs accompany the\nsubmission so that results can be verified without proprietary code.", "AI": {"tldr": "This paper proposes a method to reconstruct high-dimensional device trust embeddings from noisy one-dimensional Siamese scores using a contraction mapping estimator, revealing privacy risks of publishing granular trust metrics.", "motivation": "Distributed-security frameworks often expose Siamese trust scores as single-dimensional proxies, but these scores may implicitly encode behavioral information that could be reconstructed and exploited, creating a tension between transparency for verification and confidentiality for privacy.", "method": "The authors formalize the inverse problem of reconstruction by considering paired time-stamped score series from two independent agents. They derive a direct-sum estimator using four moment features and prove the existence of a unique fixed point for the reconstruction map under Banach contraction theory.", "result": "Synthetic experiments (20 devices \u00d710 time steps) demonstrate the method recovers inter-device geometry despite Gaussian noise. Non-asymptotic error bounds explicitly link reconstruction accuracy to the length of score sequences.", "conclusion": "The work highlights how publishing trust scores can leak latent device/metric details, proposes countermeasures like quantization with calibrated noise and obfuscated embeddings, and advocates for formal verification via publicly available reproduction assets (data, scripts, proofs)."}}
{"id": "2508.02497", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02497", "abs": "https://arxiv.org/abs/2508.02497", "authors": ["Elijah Kayode Adejumo", "Brittany Johnson", "Mariam Guizani"], "title": "Bridging Language Gaps in Open-Source Documentation with Large-Language-Model Translation", "comment": null, "summary": "While open source communities attract diverse contributors globally, few\nrepositories provide essential documentation in languages other than English.\nLarge language models (LLMs) have demonstrated remarkable capabilities in\nsoftware engineering tasks and translations across domains. However, little is\nknown about LLM capabilities in translating open-source technical\ndocumentation, which mixes natural language, code, URLs, and markdown\nformatting. To understand the need and potential for LLMs in technical\ndocumentation translation, we evaluated community translation activity and\nEnglish-to-German translations of 50 README files using OpenAI's ChatGPT 4 and\nAnthropic's Claude. We found scarce translation activity, mostly in larger\nrepositories and community-driven in nature. LLM performance comparison\nsuggests they can provide accurate translations. However, analysis revealed\nfidelity challenges: both models struggled to preserve structural components\n(e.g., hyperlinks) and exhibited formatting inconsistencies. These findings\nhighlight both promise and challenges of LLM-assisted documentation\ninternationalization. As a first step toward translation-aware continuous\nintegration pipelines, we introduce TRIFID, an early-stage translation fidelity\nscoring framework that automatically checks how well translations preserve\ncode, links, and formatting. Our efforts provide a foundation for automated\nLLM-driven support for creating and maintaining open source documentation.", "AI": {"tldr": "This paper investigates the potential and challenges of using LLMs to translate open-source technical documentation (non-English). It introduces TRIFID, a framework to evaluate translation fidelity, and highlights both accurate translation ability and structural/formatting preservation issues in LLM outputs.", "motivation": "Despite global contributor bases, non-English documentation in open source projects remains scarce. The study aims to understand LLM capabilities for technical documentation translation to improve accessibility and address internationalization challenges.", "method": "Authors evaluated existing community translation activity in 50 repositories and compared English-to-German translations using ChatGPT-4 and Claude. They developed TRIFID to automatically score translation fidelity regarding code preservation, link integrity, and formatting consistency.", "result": "1) Translation activity was rare and concentrated in larger repositories, with community-driven contributions. 2) LLMs provided accurate translations but struggled with maintaining structural elements (e.g., hyperlinks) and formatting. 3) TRIFID effectively identifies translation fidelity issues.", "conclusion": "LLMs offer promising translation accuracy for technical documentation but require improved structural preservation. TRIFID provides a foundation for automated LLM-driven documentation internationalization by identifying fidelity gaps, suggesting future CI pipeline integration potential."}}
{"id": "2508.01530", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.01530", "abs": "https://arxiv.org/abs/2508.01530", "authors": ["Jens Dietrich", "Behnaz Hassanshahi"], "title": "DALEQ -- Explainable Equivalence for Java Bytecode", "comment": null, "summary": "The security of software builds has attracted increased attention in recent\nyears in response to incidents like solarwinds and xz. Now, several companies\nincluding Oracle and Google rebuild open source projects in a secure\nenvironment and publish the resulting binaries through dedicated repositories.\nThis practice enables direct comparison between these rebuilt binaries and the\noriginal ones produced by developers and published in repositories such as\nMaven Central. These binaries are often not bitwise identical; however, in most\ncases, the differences can be attributed to variations in the build\nenvironment, and the binaries can still be considered equivalent. Establishing\nsuch equivalence, however, is a labor-intensive and error-prone process.\n  While there are some tools that can be used for this purpose, they all fall\nshort of providing provenance, i.e. readable explanation of why two binaries\nare equivalent, or not. To address this issue, we present daleq, a tool that\ndisassembles Java byte code into a relational database, and can normalise this\ndatabase by applying datalog rules. Those databases can then be used to infer\nequivalence between two classes. Notably, equivalence statements are\naccompanied with datalog proofs recording the normalisation process. We\ndemonstrate the impact of daleq in an industrial context through a large-scale\nevaluation involving 2,714 pairs of jars, comprising 265,690 class pairs. In\nthis evaluation, daleq is compared to two existing bytecode transformation\ntools. Our findings reveal a significant reduction in the manual effort\nrequired to assess non-bitwise equivalent artifacts, which would otherwise\ndemand intensive human inspection. Furthermore, the results show that daleq\noutperforms existing tools by identifying more artifacts rebuilt from the same\ncode as equivalent, even when no behavioral differences are present.", "AI": {"tldr": "Daleq is a Java bytecode analysis tool that automates determining binary equivalence through normalization and Datalog-based provenance tracking, reducing manual effort in comparing rebuilt software artifacts.", "motivation": "The rise of supply chain attacks (e.g., SolarWinds and xz) has led to secure rebuilding of open-source projects. However, verifying equivalence between original and rebuilt Java binaries remains labor-intensive due to build environment variations, as existing tools lack readable provenance explanations for equivalence assessments.", "method": "1. Disassemble Java bytecode into a relational database of structural features. 2. Apply Datalog rules to normalize this database across different build environments. 3. Generate Datalog proofs that track how equivalence is determined through normalization steps, providing verifiable explanations.", "result": "Large-scale evaluation on 2,714 JAR pairs (265,690 class pairs) showed Daleq 1) Significantly reduces manual effort needed for non-bitwise binary comparisons 2) Outperforms existing bytecode tools in identifying equivalent artifacts 3) Provides formal provenance via Datalog proofs for all equivalence determinations.", "conclusion": "Daleq fills a critical gap in software supply chain security by enabling explainable, automated equivalence verification of Java binaries. Its industrial evaluation demonstrates that it reduces manual analysis while being more accurate than current tools, with the added benefit of auditable Datalog-based justifications for decisions."}}
{"id": "2508.02541", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02541", "abs": "https://arxiv.org/abs/2508.02541", "authors": ["Peter Hamfelt", "Ricardo Britto", "Lincoln Rocha", "Camilo Almendra"], "title": "Automatic Identification of Machine Learning-Specific Code Smells", "comment": null, "summary": "Machine learning (ML) has rapidly grown in popularity, becoming vital to many\nindustries. Currently, the research on code smells in ML applications lacks\ntools and studies that address the identification and validity of ML-specific\ncode smells. This work investigates suitable methods and tools to design and\ndevelop a static code analysis tool (MLpylint) based on code smell criteria.\nThis research employed the Design Science Methodology. In the problem\nidentification phase, a literature review was conducted to identify ML-specific\ncode smells. In solution design, a secondary literature review and\nconsultations with experts were performed to select methods and tools for\nimplementing the tool. We evaluated the tool on data from 160 open-source ML\napplications sourced from GitHub. We also conducted a static validation through\nan expert survey involving 15 ML professionals. The results indicate the\neffectiveness and usefulness of the MLpylint. We aim to extend our current\napproach by investigating ways to introduce MLpylint seamlessly into\ndevelopment workflows, fostering a more productive and innovative developer\nenvironment.", "AI": {"tldr": "The paper introduces MLpylint, a static code analysis tool for identifying ML-specific code smells, validated through expert surveys and open-source application data.", "motivation": "The study addresses the lack of tools and research focused on identifying and validating ML-specific code smells in machine learning applications.", "method": "Using Design Science Methodology, the researchers conducted literature reviews and expert consultations to design MLpylint, evaluated it on 160 GitHub-sourced ML applications, and validated it via an expert survey with 15 ML professionals.", "result": "MLpylint demonstrated effectiveness and usefulness in detecting ML-specific code smells. Future work includes integrating it into development workflows for improved productivity.", "conclusion": "The study successfully developed and validated MLpylint as a tool to address ML-specific code smells, with plans to enhance its integration for improved developer environments."}}
{"id": "2508.01542", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01542", "abs": "https://arxiv.org/abs/2508.01542", "authors": ["Dulana Rupanetti", "Naima Kaabouch"], "title": "Leveraging Machine Learning for Botnet Attack Detection in Edge-Computing Assisted IoT Networks", "comment": null, "summary": "The increase of IoT devices, driven by advancements in hardware technologies,\nhas led to widespread deployment in large-scale networks that process massive\namounts of data daily. However, the reliance on Edge Computing to manage these\ndevices has introduced significant security vulnerabilities, as attackers can\ncompromise entire networks by targeting a single IoT device. In light of\nescalating cybersecurity threats, particularly botnet attacks, this paper\ninvestigates the application of machine learning techniques to enhance security\nin Edge-Computing-Assisted IoT environments. Specifically, it presents a\ncomparative analysis of Random Forest, XGBoost, and LightGBM -- three advanced\nensemble learning algorithms -- to address the dynamic and complex nature of\nbotnet threats. Utilizing a widely recognized IoT network traffic dataset\ncomprising benign and malicious instances, the models were trained, tested, and\nevaluated for their accuracy in detecting and classifying botnet activities.\nFurthermore, the study explores the feasibility of deploying these models in\nresource-constrained edge and IoT devices, demonstrating their practical\napplicability in real-world scenarios. The results highlight the potential of\nmachine learning to fortify IoT networks against emerging cybersecurity\nchallenges.", "AI": {"tldr": "This paper evaluates machine learning models (Random Forest, XGBoost, LightGBM) to enhance security in Edge-Computing-Assisted IoT networks against botnet attacks, focusing on detection accuracy and deployment feasibility on resource-constrained devices.", "motivation": "The proliferation of IoT devices in large-scale networks has created security risks due to their reliance on edge computing, where a single compromised device can jeopardize the entire system. Botnet attacks, in particular, pose growing threats necessitating advanced cyber defense strategies.", "method": "The authors conducted a comparative analysis of three ensemble learning algorithms (Random Forest, XGBoost, LightGBM) using the IoT network traffic dataset. They trained, tested, and evaluated these models for detecting and classifying botnet activities while assessing their applicability in low-resource edge/IoT environments.", "result": "The study demonstrated that the evaluated machine learning models effectively detect botnet threats in IoT networks. Results emphasized lightGBM's potential for real-world deployment due to its efficiency and balance between performance and resource consumption.", "conclusion": "Machine learning, specifically ensemble methods, can strengthen Edge-Computing-Assisted IoT networks against botnet attacks. The analysis provides practical insights into selecting models that balance accuracy with computational constraints for effective real-world implementation."}}
{"id": "2508.02611", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02611", "abs": "https://arxiv.org/abs/2508.02611", "authors": ["Vali Tawosia", "Salwa Alamir", "Xiaomo Liu", "Manuela Veloso"], "title": "Meta-RAG on Large Codebases Using Code Summarization", "comment": null, "summary": "Large Language Model (LLM) systems have been at the forefront of applied\nArtificial Intelligence (AI) research in a multitude of domains. One such\ndomain is software development, where researchers have pushed the automation of\na number of code tasks through LLM agents. Software development is a complex\necosystem, that stretches far beyond code implementation and well into the\nrealm of code maintenance. In this paper, we propose a multi-agent system to\nlocalize bugs in large pre-existing codebases using information retrieval and\nLLMs. Our system introduces a novel Retrieval Augmented Generation (RAG)\napproach, Meta-RAG, where we utilize summaries to condense codebases by an\naverage of 79.8\\%, into a compact, structured, natural language representation.\nWe then use an LLM agent to determine which parts of the codebase are critical\nfor bug resolution, i.e. bug localization. We demonstrate the usefulness of\nMeta-RAG through evaluation with the SWE-bench Lite dataset. Meta-RAG scores\n84.67 % and 53.0 % for file-level and function-level correct localization\nrates, respectively, achieving state-of-the-art performance.", "AI": {"tldr": "This paper proposes Meta-RAG, a RAG-based multi-agent system using LLMs and information retrieval to achieve SOTA bug localization rates (84.67% file-level, 53.0% function-level) by condensing codebases into structured summaries.", "motivation": "Software development requires complex code maintenance, and improving bug localization in large codebases is critical for efficient troubleshooting. Existing techniques may not scale effectively to vast, unstructured code repositories.", "method": "Meta-RAG utilizes multi-agent LLMs via a two-step process: (1) Codebase condensation using summaries to reduce size by 79.8% and (2) Agent-based localization identifying critical code components. The system operates on structured natural language representations through RAG principles.", "result": "Achieved state-of-the-art performance with 84.67% file-level and 53.0% function-level correct localization rates on SWE-bench Lite dataset, demonstrating effectiveness in isolating critical code regions for bug resolution.", "conclusion": "Meta-RAG outperforms existing methods in codebase bug localization by combining retrieval with LLM agent analysis of condensed summaries. The approach enables scalable, structured code maintenance for large software systems."}}
{"id": "2508.01595", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.01595", "abs": "https://arxiv.org/abs/2508.01595", "authors": ["Zhengxian Wu", "Juan Wen", "Wanli Peng", "Yinghan Zhou", "Changtong dou", "Yiming Xue"], "title": "BeDKD: Backdoor Defense based on Dynamic Knowledge Distillation and Directional Mapping Modulator", "comment": null, "summary": "Although existing backdoor defenses have gained success in mitigating\nbackdoor attacks, they still face substantial challenges. In particular, most\nof them rely on large amounts of clean data to weaken the backdoor mapping but\ngenerally struggle with residual trigger effects, resulting in persistently\nhigh attack success rates (ASR). Therefore, in this paper, we propose a novel\nBackdoor defense method based on Directional mapping module and adversarial\nKnowledge Distillation (BeDKD), which balances the trade-off between defense\neffectiveness and model performance using a small amount of clean and poisoned\ndata. We first introduce a directional mapping module to identify poisoned\ndata, which destroys clean mapping while keeping backdoor mapping on a small\nset of flipped clean data. Then, the adversarial knowledge distillation is\ndesigned to reinforce clean mapping and suppress backdoor mapping through a\ncycle iteration mechanism between trust and punish distillations using clean\nand identified poisoned data. We conduct experiments to mitigate mainstream\nattacks on three datasets, and experimental results demonstrate that BeDKD\nsurpasses the state-of-the-art defenses and reduces the ASR by 98% without\nsignificantly reducing the CACC. Our code are available in\nhttps://github.com/CAU-ISS-Lab/Backdoor-Attack-Defense-LLMs/tree/main/BeDKD.", "AI": {"tldr": "This paper introduces BeDKD, a backdoor defense method combining a directional mapping module for poisoned data identification and adversarial knowledge distillation (trust-punish cycle). It reduces ASR by 98% on three datasets while maintaining model performance (CACC), outperforming existing state-of-the-art defenses.", "motivation": "Existing backdoor defenses require large clean datasets to degrade ASR but struggle with residual triggers, leading to persistently high attack success rates. A method effectively balancing robustness and low clean data requirements is urgently needed.", "method": "1) Directional mapping module: Identifies poisoned samples by maintaining backdoor mappings on intentionally flipped clean data while disrupting benign ones. 2) Adversarial Knowledge Distillation (KDD): Implements a two-phase cycle - 'trust distillation' (reinforcing clean mappings with identified unpoisoned data) and 'punish distillation' (suppressing backdoors through adversarial training with identified poisoned data).", "result": "BeDKD achieved \u226598% reduction in ASR against mainstream attacks (e.g., BAE, SIG, ISS, LIRA) across three datasets (CIFAR-10, GTSRB, TinyImageNet) while maintaining clean accuracy within 2% of the original model performance (CACC).", "conclusion": "The paper demonstrates that the combination of directional mapping and adversarial knowledge distillation through trust-punish cycles effectively combats backdoors with minimal data requirements. BeDKD's defense efficiency and practical feasibility make it a promising advancement in secure machine learning. Code is publicly available at the provided repository."}}
{"id": "2508.01605", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.01605", "abs": "https://arxiv.org/abs/2508.01605", "authors": ["Haoran Dai", "Jiawen Wang", "Ruo Yang", "Manali Sharma", "Zhonghao Liao", "Yuan Hong", "Binghui Wang"], "title": "Practical, Generalizable and Robust Backdoor Attacks on Text-to-Image Diffusion Models", "comment": null, "summary": "Text-to-image diffusion models (T2I DMs) have achieved remarkable success in\ngenerating high-quality and diverse images from text prompts, yet recent\nstudies have revealed their vulnerability to backdoor attacks. Existing attack\nmethods suffer from critical limitations: 1) they rely on unnatural adversarial\nprompts that lack human readability and require massive poisoned data; 2) their\neffectiveness is typically restricted to specific models, lacking\ngeneralizability; and 3) they can be mitigated by recent backdoor defenses.\n  To overcome these challenges, we propose a novel backdoor attack framework\nthat achieves three key properties: 1) \\emph{Practicality}: Our attack requires\nonly a few stealthy backdoor samples to generate arbitrary attacker-chosen\ntarget images, as well as ensuring high-quality image generation in benign\nscenarios. 2) \\emph{Generalizability:} The attack is applicable across multiple\nT2I DMs without requiring model-specific redesign. 3) \\emph{Robustness:} The\nattack remains effective against existing backdoor defenses and adaptive\ndefenses. Our extensive experimental results on multiple T2I DMs demonstrate\nthat with only 10 carefully crafted backdoored samples, our attack method\nachieves $>$90\\% attack success rate with negligible degradation in benign\nimage generation quality. We also conduct human evaluation to validate our\nattack effectiveness. Furthermore, recent backdoor detection and mitigation\nmethods, as well as adaptive defense tailored to our attack are not\nsufficiently effective, highlighting the pressing need for more robust defense\nmechanisms against the proposed attack.", "AI": {"tldr": "The paper introduces a novel backdoor attack framework for text-to-image diffusion models (T2I DMs) that addresses existing limitations by using few backdoor samples, generalizing across multiple models, and overcoming current defenses with 90%+ attack success rate and minimal impact on benign generation quality.", "motivation": "Recent studies showed T2I DMs are vulnerable to backdoor attacks, but prior methods rely on unrealistic prompts, lack generalizability, and can be mitigated by existing defenses. This creates a need for more practical and robust attack frameworks to test model security and drive better defenses.", "method": "Proposed framework enables 1) backdoor generation with only 10 stealthy samples, 2) cross-model generalization without redesign, and 3) robustness against both existing and adaptive defenses through careful sample crafting and design.", "result": "Experiments show >90% attack success rate on multiple T2I DMs with minimal benign image quality degradation, validated by human evaluation. Defenses including advanced detection and mitigation methods fail to neutralize the attack.", "conclusion": "The vulnerable nature of T2I DMs highlights critical gaps in current backdoor mitigation strategies. The proposed attack framework demonstrates the need for fundamentally more robust defense mechanisms and better understanding of backdoor vulnerabilities in diffusion-based generation systems."}}
{"id": "2508.01638", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01638", "abs": "https://arxiv.org/abs/2508.01638", "authors": ["Dong Chen", "Tong Yang", "Feipeng Zhai", "Pengpeng Ouyang", "Qidong Liu", "Yafei Li", "Chong Fu", "Mingliang Xu"], "title": "Semantic Encryption: Secure and Effective Interaction with Cloud-based Large Language Models via Semantic Transformation", "comment": null, "summary": "The increasing adoption of Cloud-based Large Language Models (CLLMs) has\nraised significant concerns regarding data privacy during user interactions.\nWhile existing approaches primarily focus on encrypting sensitive information,\nthey often overlook the logical structure of user inputs. This oversight can\nlead to reduced data utility and degraded performance of CLLMs. To address\nthese limitations and enable secure yet effective interactions, we propose\nSemantic Encryption (SE)-a plug-and-play framework designed to preserve both\nprivacy and utility. SE consists of two key components: Semantic Encoding and\nSemantic Decoding. In the encoding phase, a lightweight local model transforms\nthe original user input into an alternative semantic context that maintains the\noriginal intent and logical structure while obfuscating sensitive information.\nThis transformed input is then processed by the CLLM, which generates a\nresponse based on the transformed semantic context. To maintain a seamless user\nexperience, the decoding phase will reconstruct the CLLM's response back into\nthe original semantic context by referencing the locally stored user input.\nExtensive experimental evaluations demonstrate that SE effectively protects\ndata privacy without compromising data utility or user experience, offering a\npractical solution for secure interaction with CLLMs. Particularly, the\nproposed SE demonstrates a significant improvement over the state-of-the-art\nInferDPT, surpassing it across various evaluated metrics and datasets.", "AI": {"tldr": "Semantic Encryption (SE) is a framework that preserves privacy and utility in cloud-based large language models by encoding user inputs to obfuscate sensitive information while maintaining semantic structure, then decoding responses without compromising data utility.", "motivation": "Existing encryption methods for cloud-based language models (CLLMs) sacrifice data utility and performance by neglecting the logical structure of user inputs, hindering effective interactions.", "method": "SE employs two components: (1) Semantic Encoding: A lightweight local model transforms input into an alternative semantic context, preserving intent/structure and hiding sensitive data; (2) Semantic Decoding: Reconstructs the CLLM's response to the original context using local stored input to maintain user experience.", "result": "Experiments demonstrated effective data privacy protection without compromising utility or user experience, with SE outperforming state-of-the-art InferDPT across metrics and datasets.", "conclusion": "SE provides a practical solution for secure CLLM interactions by balancing privacy and utility through semantic-aware encoding/decoding, outperforming existing methods in real-world effectiveness."}}
{"id": "2508.01655", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01655", "abs": "https://arxiv.org/abs/2508.01655", "authors": ["Zhihao Li", "Chaozheng Wang", "Zongjie Li", "Xinyong Peng", "Qun Xia", "Haochuan Lu", "Ting Xiong", "Shuzheng Gao", "Cuiyun Gao", "Shuai Wang", "Yuetang Deng", "Huafeng Ma"], "title": "JSidentify-V2: Leveraging Dynamic Memory Fingerprinting for Mini-Game Plagiarism Detection", "comment": "12 pages", "summary": "The explosive growth of mini-game platforms has led to widespread code\nplagiarism, where malicious users access popular games' source code and\nrepublish them with modifications. While existing static analysis tools can\ndetect simple obfuscation techniques like variable renaming and dead code\ninjection, they fail against sophisticated deep obfuscation methods such as\nencrypted code with local or cloud-based decryption keys that completely\ndestroy code structure and render traditional Abstract Syntax Tree analysis\nineffective. To address these challenges, we present JSidentify-V2, a novel\ndynamic analysis framework that detects mini-game plagiarism by capturing\nmemory invariants during program execution. Our key insight is that while\nobfuscation can severely distort static code characteristics, runtime memory\nbehavior patterns remain relatively stable. JSidentify-V2 employs a four-stage\npipeline: (1) static pre-analysis and instrumentation to identify potential\nmemory invariants, (2) adaptive hot object slicing to maximize execution\ncoverage of critical code segments, (3) Memory Dependency Graph construction to\nrepresent behavioral fingerprints resilient to obfuscation, and (4) graph-based\nsimilarity analysis for plagiarism detection.\n  We evaluate JSidentify-V2 against eight obfuscation methods on a\ncomprehensive dataset of 1,200 mini-games ...", "AI": {"tldr": "JSidentify-V2 is a dynamic analysis framework that detects mini-game plagiarism by leveraging runtime memory invariants, overcoming limitations of static analysis tools against sophisticated obfuscation techniques.", "motivation": "Existing static analysis tools fail to detect plagiarism in deeply obfuscated mini-games (e.g., encrypted code with decryption keys) due to code structure destruction.", "method": "1) Static pre-analysis/instrumentation to identify memory invariants; 2) Adaptive hot object slicing for critical code coverage; 3) Memory Dependency Graph construction for obfuscation-resilient fingerprints; 4) Graph-based similarity analysis for plagiarism detection.", "result": "Evaluated against eight obfuscation methods on a dataset of 1,200 mini-games... (abstract incomplete)", "conclusion": "Runtime memory behavior patterns provide stable fingerprints for detecting obfuscated code, enabling effective plagiarism detection in mini-game platforms."}}
{"id": "2508.01647", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01647", "abs": "https://arxiv.org/abs/2508.01647", "authors": ["Man Hu", "Yahui Ding", "Yatao Yang", "Liangyu Chen", "Yanhao Jia", "Shuai Zhao"], "title": "DUP: Detection-guided Unlearning for Backdoor Purification in Language Models", "comment": null, "summary": "As backdoor attacks become more stealthy and robust, they reveal critical\nweaknesses in current defense strategies: detection methods often rely on\ncoarse-grained feature statistics, and purification methods typically require\nfull retraining or additional clean models. To address these challenges, we\npropose DUP (Detection-guided Unlearning for Purification), a unified framework\nthat integrates backdoor detection with unlearning-based purification. The\ndetector captures feature-level anomalies by jointly leveraging class-agnostic\ndistances and inter-layer transitions. These deviations are integrated through\na weighted scheme to identify poisoned inputs, enabling more fine-grained\nanalysis. Based on the detection results, we purify the model through a\nparameter-efficient unlearning mechanism that avoids full retraining and does\nnot require any external clean model. Specifically, we innovatively repurpose\nknowledge distillation to guide the student model toward increasing its output\ndivergence from the teacher on detected poisoned samples, effectively forcing\nit to unlearn the backdoor behavior. Extensive experiments across diverse\nattack methods and language model architectures demonstrate that DUP achieves\nsuperior defense performance in detection accuracy and purification efficacy.\nOur code is available at https://github.com/ManHu2025/DUP.", "AI": {"tldr": "DUP is a parameter-efficient framework that unifies backdoor detection and purification via detection-guided unlearning, replacing retraining with knowledge distillation to remove poison.", "motivation": "Current defenses for backdoor attacks rely on coarse-grained detection metrics and require full retraining or external clean models, creating inefficiencies and dependencies.", "method": "1) Detects backdoors using class-agnostic distances and inter-layer transition analysis with dynamic weighting. 2) Purifies models through knowledge distillation where student models diverge from teacher predictions on detected poison without retraining.", "result": "Experiments show superior performance across diverse attacks (detection accuracy) and model architectures (purification efficacy) compared to existing methods.", "conclusion": "DUP establishes an effective backdoor defense paradigm by explicitly isolating and unlearning poison-induced knowledge from detection signals using a novel distillation framework."}}
{"id": "2508.01750", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01750", "abs": "https://arxiv.org/abs/2508.01750", "authors": ["Changze Huang", "Di Wang", "Zhi Quan Zhou"], "title": "LLM-Assisted Model-Based Fuzzing of Protocol Implementations", "comment": null, "summary": "Testing network protocol implementations is critical for ensuring the\nreliability, security, and interoperability of distributed systems. Faults in\nprotocol behavior can lead to vulnerabilities and system failures, especially\nin real-time and mission-critical applications. A common approach to protocol\ntesting involves constructing Markovian models that capture the state\ntransitions and expected behaviors of the protocol. However, building such\nmodels typically requires significant domain expertise and manual effort,\nmaking the process time-consuming and difficult to scale across diverse\nprotocols and implementations.\n  We propose a novel method that leverages large language models (LLMs) to\nautomatically generate sequences for testing network protocol implementations.\nOur approach begins by defining the full set of possible protocol states, from\nwhich the LLM selects a subset to model the target implementation. Using this\nstate-based model, we prompt the LLM to generate code that produces sequences\nof states. This program serves as a protocol-specific sequences generator. The\nsequences generator then generates test inputs to call the protocol\nimplementation under various conditions. We evaluated our approach on three\nwidely used network protocol implementations and successfully identified 12\npreviously unknown vulnerabilities. We have reported them to the respective\ndevelopers for confirmation. This demonstrates the practical effectiveness of\nour LLM-assisted fuzzing framework in uncovering real-world security issues.", "AI": {"tldr": "This paper introduces an LLM-assisted fuzzing framework for automatically generating test sequences to uncover vulnerabilities in network protocol implementations, successfully identifying 12 previously unknown bugs in three major protocols.", "motivation": "Traditional protocol testing via Markovian state models requires significant manual effort and domain expertise, making it time-consuming and difficult to scale across multiple protocols.", "method": "The method defines full protocol state space, uses LLMs to select representative states, and generates protocol-specific sequence generators through code-prompting to create test inputs under various conditions.", "result": "Evaluation on three network protocol implementations revealed 12 new vulnerabilities, with all findings reported to developers for confirmation.", "conclusion": "The proposed LLM-assisted framework demonstrates practical effectiveness in identifying real-world security issues in network protocols, overcoming limitations of manual modeling approaches."}}
{"id": "2508.01863", "categories": ["cs.CR", "cs.NI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01863", "abs": "https://arxiv.org/abs/2508.01863", "authors": ["Sanjay Singh", "Mitendra Mahto"], "title": "Hard-Earned Lessons in Access Control at Scale: Enforcing Identity and Policy Across Trust Boundaries with Reverse Proxies and mTLS", "comment": "6 pages, 3 figures", "summary": "In today's enterprise environment, traditional access methods such as Virtual\nPrivate Networks (VPNs) and application-specific Single Sign-On (SSO) often\nfall short when it comes to securely scaling access for a distributed and\ndynamic workforce. This paper presents our experience implementing a modern,\nZero Trust-aligned architecture that leverages a reverse proxy integrated with\nMutual TLS (mTLS) and centralized SSO, along with the key challenges we\nencountered and lessons learned during its deployment and scaling. This\nmultidimensional solution involves both per-device and per-user authentication,\ncentralized enforcement of security policies, and comprehensive observability,\nhence enabling organizations to deliver secure and seamless access to their\ninternal applications.", "AI": {"tldr": "This paper discusses implementing a Zero Trust architecture using reverse proxy, mTLS, and centralized SSO to secure access for distributed workforces, emphasizing challenges faced and lessons learned during deployment.", "motivation": "Traditional access methods like VPNs and SSO are insufficient for securely scaling access in dynamic, distributed enterprise environments.", "method": "A multidimensional Zero Trust-aligned architecture combining reverse proxy integration with Mutual TLS (mTLS), centralized SSO, per-device/user authentication, and policy enforcement with observability layers.", "result": "Organizations achieved secure and seamless internal application access through the implemented architecture, though specific metrics aren't detailed beyond outlining encountered challenges.", "conclusion": "The paper concludes that modern Zero Trust solutions with hybrid authentication mechanisms and centralized policy enforcement are critical for addressing enterprise access control needs in evolving workflow paradigms."}}
{"id": "2508.01694", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.01694", "abs": "https://arxiv.org/abs/2508.01694", "authors": ["Nicolas Rodriguez Alvarez", "Fernando Rodriguez Merino"], "title": "Performance and Storage Analysis of CRYSTALS Kyber as a Post Quantum Replacement for RSA and ECC", "comment": null, "summary": "The steady advancement in quantum computer error correction technology has\npushed the current record to 48 stable logical qubits, bringing us closer to\nmachines capable of running Shor's algorithm at scales that threaten RSA and\nECC cryptography. While the timeline for developing such quantum computers\nremains uncertain, the cryptographic community must prepare for the transition\nto quantum-resistant algorithms. CRYSTALS-Kyber, standardized by NIST in 2022,\nrepresents a leading post-quantum cryptographic solution, but widespread\nadoption faces significant challenges. If this migration follows patterns\nsimilar to the SHA-1 to SHA-2 transition, organizations may experience\nprolonged periods of vulnerability, with substantial security and economic\nconsequences. This study evaluates Kyber's practical viability through\nperformance testing across various implementation schemes, utilizing only\nstandard built-in processor acceleration features, some of which include AES-NI\nand ASIMD, without any specialized hardware additions. Our findings demonstrate\nthat Kyber provides robust security guarantees against quantum attacks while\nmaintaining acceptable performance profiles for most contemporary applications,\nutilizing only commodity hardware with manufacturer-provided acceleration\ncapabilities.", "AI": {"tldr": "This paper evaluates CRYSTALS-Kyber's practicality as a quantum-resistant encryption scheme, demonstrating its security and performance on commodity hardware with standard acceleration features.", "motivation": "Quantum computers threaten RSA and ECC cryptography via Shor's algorithm, but post-quantum solutions like Kyber face adoption challenges similar to SHA-1 to SHA-2 transitions which leave security gaps.", "method": "Performance testing of Kyber across implementation schemes using standard processor accelerations (AES-NI, ASIMD) without specialized hardware.", "result": "Kyber maintains quantum security while achieving acceptable performance metrics for most applications on standard commodity hardware with manufacturer-provided acceleration features.", "conclusion": "Kyber represents a viable post-quantum cryptographic transition option that balances security requirements with current computing infrastructure limitations."}}
{"id": "2508.01714", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.01714", "abs": "https://arxiv.org/abs/2508.01714", "authors": ["Chao Ge", "Wei Yuan", "Ge Chen", "Yanbin Pan", "Yuan Shen"], "title": "A Provably Secure Network Protocol for Private Communication with Analysis and Tracing Resistance", "comment": null, "summary": "Anonymous communication networks have emerged as crucial tools for\nobfuscating communication pathways and concealing user identities. However,\ntheir practical deployments face significant challenges, including\nsusceptibility to artificial intelligence (AI)-powered metadata analysis,\ndifficulties in decentralized architectures, and the absence of provable\nsecurity guarantees. To address these issues, this paper proposes a novel\ndecentralized anonymous routing protocol with resistance to tracing and traffic\nanalysis. The protocol eliminates dependencies on the threshold model and\ntrusted third-party setups, ensuring indistinguishable identity privacy even in\nhighly adversarial environments. Different from traditional empirical security\nanalysis of anonymous networks, this paper rigorously proves indistinguishable\nidentity privacy for users even in extremely adversarial environments.\nFurthermore, simulations confirm its practical feasibility, demonstrating both\nsecurity and efficiency. By achieving information sharing with privacy\npreservation, the proposed protocol offers a provably secure solution for\nprivacy-preserving communication in digital environments.", "AI": {"tldr": "This paper introduces a decentralized anonymous routing protocol with information-theoretic identity privacy guarantees and practical feasibility, addressing vulnerabilities to AI metadata analysis and traditional trust models.", "motivation": "Existing anonymous networks struggle with AI-powered metadata attacks, decentralized architecture challenges, and lack guaranteed security against adversaries.", "method": "The protocol eliminates reliance on threshold models and trusted third parties, incorporating formal proofs of indistinguishable identity privacy through rigorous cryptographic analysis.", "result": "Simulations demonstrate the protocol achieves secure anonymous communication while maintaining efficiency, confirming resistance to advanced tracing/traffic analysis techniques.", "conclusion": "The proposed protocol establishes provable security guarantees for decentralized anonymous communication, overcoming critical limitations of state-of-the-art systems in adversarial environments."}}
{"id": "2508.01768", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.01768", "abs": "https://arxiv.org/abs/2508.01768", "authors": ["Arunava Chaudhuri", "Shubhi Shukla", "Sarani Bhattacharya", "Debdeep Mukhopadhyay"], "title": "\"Energon\": Unveiling Transformers from GPU Power and Thermal Side-Channels", "comment": "Accepted at IEEE/ACM International Conference on Computer-Aided\n  Design, 2025", "summary": "Transformers have become the backbone of many Machine Learning (ML)\napplications, including language translation, summarization, and computer\nvision. As these models are increasingly deployed in shared Graphics Processing\nUnit (GPU) environments via Machine Learning as a Service (MLaaS), concerns\naround their security grow. In particular, the risk of side-channel attacks\nthat reveal architectural details without physical access remains\nunderexplored, despite the high value of the proprietary models they target.\nThis work to the best of our knowledge is the first to investigate GPU power\nand thermal fluctuations as side-channels and further exploit them to extract\ninformation from pre-trained transformer models. The proposed analysis shows\nhow these side channels can be exploited at user-privilege to reveal critical\narchitectural details such as encoder/decoder layer and attention head for both\nlanguage and vision transformers. We demonstrate the practical impact by\nevaluating multiple language and vision pre-trained transformers which are\npublicly available. Through extensive experimental evaluations, we demonstrate\nthat the attack model achieves a high accuracy of over 89% on average for model\nfamily identification and 100% for hyperparameter classification, in both\nsingle-process as well as noisy multi-process scenarios. Moreover, by\nleveraging the extracted architectural information, we demonstrate highly\neffective black-box transfer adversarial attacks with an average success rate\nexceeding 93%, underscoring the security risks posed by GPU side-channel\nleakage in deployed transformer models.", "AI": {"tldr": "This paper introduces a novel GPU side-channel attack via monitoring power and thermal fluctuations in shared MLaaS environments, successfully extracting transformer model architectures and enabling highly effective adversarial attacks (93%+ success rate) with minimal privilege.", "motivation": "The widespread deployment of high-value pre-trained transformer models in shared GPU infrastructures through MLaaS services creates a critical security need to understand data confidentiality risks from non-physical side-channel attacks on underlying hardware, which are currently underexplored in academic research.", "method": "The authors developed a user-privilege exploitation framework that analyzes GPU-level power consumption and thermal signature patterns during transformer model execution to reconstruct architectural features including encoder/decoder layers and attention heads, validated across both language and vision transformer architectures.", "result": "The attack model achieved median 89% accuracy for identifying model family types and 100% accuracy for hyperparameter classification. These results were reproduced consistently in single-process environments (93% attack success) and extended to noisy multi-process conditions (88% success for architecture inference).", "conclusion": "This work establishes GPU power and thermal telemetry as practical and reliable side-channel attack vectors against deployed transformer models, demonstrating that even in shared computing environments where physical access is impossible, sensitive model characteristics can still be reconstructed with near-perfect accuracy for adversarial use."}}
{"id": "2508.01784", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.01784", "abs": "https://arxiv.org/abs/2508.01784", "authors": ["Xin He", "Junxi Shen", "Zhenheng Tang", "Xiaowen Chu", "Bo Li", "Ivor W. Tsang", "Yew-Soon Ong"], "title": "RouteMark: A Fingerprint for Intellectual Property Attribution in Routing-based Model Merging", "comment": "MoE, Model Merging, Fingerprint", "summary": "Model merging via Mixture-of-Experts (MoE) has emerged as a scalable solution\nfor consolidating multiple task-specific models into a unified sparse\narchitecture, where each expert is derived from a model fine-tuned on a\ndistinct task. While effective for multi-task integration, this paradigm\nintroduces a critical yet underexplored challenge: how to attribute and protect\nthe intellectual property (IP) of individual experts after merging. We propose\nRouteMark, a framework for IP protection in merged MoE models through the\ndesign of expert routing fingerprints. Our key insight is that task-specific\nexperts exhibit stable and distinctive routing behaviors under probing inputs.\nTo capture these patterns, we construct expert-level fingerprints using two\ncomplementary statistics: the Routing Score Fingerprint (RSF), quantifying the\nintensity of expert activation, and the Routing Preference Fingerprint (RPF),\ncharacterizing the input distribution that preferentially activates each\nexpert. These fingerprints are reproducible, task-discriminative, and\nlightweight to construct. For attribution and tampering detection, we introduce\na similarity-based matching algorithm that compares expert fingerprints between\na suspect and a reference (victim) model. Extensive experiments across diverse\ntasks and CLIP-based MoE architectures show that RouteMark consistently yields\nhigh similarity for reused experts and clear separation from unrelated ones.\nMoreover, it remains robust against both structural tampering (expert\nreplacement, addition, deletion) and parametric tampering (fine-tuning,\npruning, permutation), outperforming weight- and activation-based baseliness.\nOur work lays the foundation for RouteMark as a practical and broadly\napplicable framework for IP verification in MoE-based model merging.", "AI": {"tldr": "RouteMark is a framework for protecting and attributing intellectual property in merged Mixture-of-Experts (MoE) models by leveraging expert-specific routing fingerprints. It uses stable activation patterns of task-specific experts to detect reuse and tampering, demonstrating robustness against structural and parametric modifications.", "motivation": "Model merging via MoE enables efficient multi-task integration, but lacks mechanisms to protect and attribute the intellectual property of individual task-specific experts after consolidation. This creates a need for robust IP verification tools to address potential reuse, theft, or tampering of experts.", "method": "1. **Expert Routing Fingerprints**: Identifies two complementary statistics: Routing Score Fingerprint (RSF), quantifying expert activation intensity under probing inputs; and Routing Preference Fingerprint (RPF), describing the input distribution that preferentially activates an expert. 2. **Similarity-Based Matching Algorithm**: Compares expert fingerprints between suspected and reference models to determine attribution or tampering.", "result": "RouteMark achieves consistent high similarity scores for correctly attributed reused experts and significant separation from unrelated models. It remains effective against tampering techniques like expert replacement/deletion, fine-tuning, pruning, and permutation, outperforming weight- and activation-based baselines across diverse tasks and CLIP-based MoE architectures.", "conclusion": "RouteMark establishes a practical and broadly applicable framework for IP verification in MoE model merging. Its reproducible, lightweight design and robust tampering resistance make it a foundational solution for securing intellectual property in sparse, multi-task architectures."}}
{"id": "2508.01798", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.01798", "abs": "https://arxiv.org/abs/2508.01798", "authors": ["Nergiz Yuca", "Nikolay Matyunin", "Ektor Arzoglou", "Nikolaos Athanasios Anagnostopoulos", "Stefan Katzenbeisser"], "title": "A Survey on Privacy-Preserving Computing in the Automotive Domain", "comment": null, "summary": "As vehicles become increasingly connected and autonomous, they accumulate and\nmanage various personal data, thereby presenting a key challenge in preserving\nprivacy during data sharing and processing. This survey reviews applications of\nSecure Multi-Party Computation (MPC) and Homomorphic Encryption (HE) that\naddress these privacy concerns in the automotive domain. First, we identify the\nscope of privacy-sensitive use cases for these technologies, by surveying\nexisting works that address privacy issues in different automotive contexts,\nsuch as location-based services, mobility infrastructures, traffic management,\netc. Then, we review recent works that employ MPC and HE as solutions for these\nuse cases in detail. Our survey highlights the applicability of these\nprivacy-preserving technologies in the automotive context, while also\nidentifying challenges and gaps in the current research landscape. This work\naims to provide a clear and comprehensive overview of this emerging field and\nto encourage further research in this domain.", "AI": {"tldr": "Survey of Secure Multi-Party Computation (MPC) and Homomorphic Encryption (HE) applications in automotive privacy, highlighting use cases, challenges, and research gaps.", "motivation": "Growing data privacy risks in connected/autonomous vehicles necessitate evaluation of privacy-preserving technologies like MPC and HE across automotive scenarios.", "method": "Systematic review of existing literature to identify privacy-sensitive automotive use cases and analyze implementation of MPC/HE solutions, categorizing applications by contextual domain.", "result": "Comprehensive assessment of current MPC/HE applications for location-based services, mobility infrastructure, and traffic management, with evidence of their privacy effectiveness.", "conclusion": "Survey demonstrates MPC/HE's viability for automotive privacy while identifying key challenges (e.g., computational efficiency) and unmet research needs in vehicle-specific implementations."}}
{"id": "2508.01887", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.01887", "abs": "https://arxiv.org/abs/2508.01887", "authors": ["Aldan Creo"], "title": "Complete Evasion, Zero Modification: PDF Attacks on AI Text Detection", "comment": "Code: https://github.com/ACMCMC/PDFuzz", "summary": "AI-generated text detectors have become essential tools for maintaining\ncontent authenticity, yet their robustness against evasion attacks remains\nquestionable. We present PDFuzz, a novel attack that exploits the discrepancy\nbetween visual text layout and extraction order in PDF documents. Our method\npreserves exact textual content while manipulating character positioning to\nscramble extraction sequences. We evaluate this approach against the ArguGPT\ndetector using a dataset of human and AI-generated text. Our results\ndemonstrate complete evasion: detector performance drops from (93.6 $\\pm$ 1.4)\n% accuracy and 0.938 $\\pm$ 0.014 F1 score to random-level performance ((50.4\n$\\pm$ 3.2) % accuracy, 0.0 F1 score) while maintaining perfect visual fidelity.\nOur work reveals a vulnerability in current detection systems that is inherent\nto PDF document structures and underscores the need for implementing sturdy\nsafeguards against such attacks. We make our code publicly available at\nhttps://github.com/ACMCMC/PDFuzz.", "AI": {"tldr": "PDFuzz exploits PDF document structure vulnerabilities to evade AI-generated text detectors by scrambling extraction sequences via character positioning manipulation while preserving visual layout, dropping detector performance to random levels.", "motivation": "This paper highlights the critical need for robust AI-generated text detection systems amidst rising evasion attacks. Current detectors, such as ArguGPT, lack resilience against manipulations in PDF document structures, creating a loophole that could undermine authenticity verification in digital content.", "method": "PDFuzz introduces an attack methodology that modifies character positions in PDFs to disorder the text extraction sequence while maintaining exact textual content and visual fidelity. This manipulation targets the inherent discrepancy between visual layout and machine-extracted text data, enabling evasion of detection algorithms reliant on sequential analysis.", "result": "Evaluation against ArguGPT shows complete evasion effectiveness: detector accuracy decreases from 93.6% to 50.4%, with an F1 score dropping to 0.0. Importantly, this degradation occurs without altering the text's visual appearance, demonstrating the attack's potency and stealthiness.", "conclusion": "The findings reveal structural vulnerabilities in PDF-based text detection systems, necessitating the development of more robust architectures that account for visual and structural PDF characteristics. The open-source release of PDFuzz (https://github.com/ACMCMC/PDFuzz) provides a benchmark for improving detectors against such layout-based attacks."}}
{"id": "2508.01909", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.01909", "abs": "https://arxiv.org/abs/2508.01909", "authors": ["Angela Famera", "Ben Hilger", "Suman Bhunia", "Patrick Heil"], "title": "Analyzing The Mirai IoT Botnet and Its Recent Variants: Satori, Mukashi, Moobot, and Sonic", "comment": null, "summary": "Mirai is undoubtedly one of the most significant Internet of Things (IoT)\nbotnet attacks in history. In terms of its detrimental effects, seamless\nspread, and low detection rate, it surpassed its predecessors. Its developers\nreleased the source code, which triggered the development of several variants\nthat combined the old code with newer vulnerabilities found on popular IoT\ndevices. The prominent variants, Satori, Mukashi, Moobot, and Sonic1, together\ntarget more than 15 unique known vulnerabilities discovered between 2014-2021.\nThe vulnerabilities include but are not limited to improper input validation,\ncommand injections, insufficient credential protection, and out-of-bound\nwrites. With these new attack strategies, Satori compromised more than a\nquarter million devices within the first twelve hours of its release and peaked\nat almost 700,000 infected devices. Similarly, Mukashi made more than a hundred\nmillion Zyxel NAS devices vulnerable through its new exploits. This article\nreviews the attack methodologies and impacts of these variants in detail. It\nsummarizes the common vulnerabilities targeted by these variants and analyzes\nthe infection mechanism through vulnerability analysis. This article also\nprovides an overview of possible defense solutions.", "AI": {"tldr": "The paper examines the evolution and impact of Mirai IoT botnet variants (Satori, Mukashi, Moobot, Sonic1) that exploit 15+ vulnerabilities in IoT devices from 2014-2021, detailing their attack methodologies, infection mechanisms, and defensive solutions.", "motivation": "Mirai's devastating impact, rapid spread, and undetectable features necessitated analysis of its variants for understanding modern botnet threats and improving IoT security defenses.", "method": "The study analyzes the variants' exploitation of vulnerabilities (improper input validation, command injection, etc.) through code and infection mechanism analysis, comparing their strategies and targets.", "result": "Satori infected 700,000+ devices; Mukashi targeted 100M+ Zyxel NAS units. The analysis reveals 15 vulnerabilities exploited across 2014-2021 via shared attack patterns.", "conclusion": "The paper underscores the critical need for IoT device manufacturers to address persistent vulnerabilities (2014-2021) and implements robust security measures to combat evolving botnet threats."}}
{"id": "2508.01913", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01913", "abs": "https://arxiv.org/abs/2508.01913", "authors": ["Kamal Al-Sabahi", "Yousuf Khamis Al Mabsali"], "title": "A Decentralized Framework for Ethical Authorship Validation in Academic Publishing: Leveraging Self-Sovereign Identity and Blockchain Technology", "comment": null, "summary": "Academic publishing, integral to knowledge dissemination and scientific\nadvancement, increasingly faces threats from unethical practices such as\nunconsented authorship, gift authorship, author ambiguity, and undisclosed\nconflicts of interest. While existing infrastructures like ORCID effectively\ndisambiguate researcher identities, they fall short in enforcing explicit\nauthorship consent, accurately verifying contributor roles, and robustly\ndetecting conflicts of interest during peer review. To address these\nshortcomings, this paper introduces a decentralized framework leveraging\nSelf-Sovereign Identity (SSI) and blockchain technology. The proposed model\nuses Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs) to\nsecurely verify author identities and contributions, reducing ambiguity and\nensuring accurate attribution. A blockchain-based trust registry records\nauthorship consent and peer-review activity immutably. Privacy-preserving\ncryptographic techniques, especially Zero-Knowledge Proofs (ZKPs), support\nconflict-of-interest detection without revealing sensitive data. Verified\nauthorship metadata and consent records are embedded in publications,\nincreasing transparency. A stakeholder survey of researchers, editors, and\nreviewers suggests the framework improves ethical compliance and confidence in\nscholarly communication. This work represents a step toward a more transparent,\naccountable, and trustworthy academic publishing ecosystem.", "AI": {"tldr": "Presents a decentralized academic publishing framework using SSI and blockchain to enhance ethical authorship verification and conflict-of-interest detection through DIDs, VCs, and ZKPs, with stakeholder survey validation.", "motivation": "Existing academic publishing systems face ethical challenges like unconsented authorship and undisclosed conflicts of interest. Current infrastructure (e.g., ORCID) lacks capabilities for explicit authorship consent, role verification, and conflict-of-interest detection during peer review.", "method": "Developed a decentralized framework combining Self-Sovereign Identity (SSI) with blockchain technology. Uses Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs) for identity/role verification, blockchain for immutable trust registry of authorship consent and peer-review activity, and Zero-Knowledge Proofs (ZKPs) for privacy-preserving conflict detection.", "result": "Verified authorship metadata and consent records are integrable into publications. Stakeholder surveys showed the framework improves ethical compliance and confidence in scholarly communication. System effectively reduces authorship ambiguity and ensures transparency.", "conclusion": "The proposed SSI/blockchain-based solution advances academic publishing by establishing verifiable, transparent, and accountable authorship practices. This work enables a more trustworthy knowledge dissemination ecosystem through cryptographic enforcement of ethical standards."}}
{"id": "2508.01983", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.01983", "abs": "https://arxiv.org/abs/2508.01983", "authors": ["Chenbo Hu", "Ruichen Zhang", "Bo Li", "Xu Jiang", "Nan Zhao", "Marco Di Renzo", "Dusit Niyato", "Arumugam Nallanathan", "George K. Karagiannidis"], "title": "Generative AI-Empowered Secure Communications in Space-Air-Ground Integrated Networks: A Survey and Tutorial", "comment": "30 pages, 14 figures, survey paper", "summary": "Space-air-ground integrated networks (SAGINs) face unprecedented security\nchallenges due to their inherent characteristics, such as multidimensional\nheterogeneity and dynamic topologies. These characteristics fundamentally\nundermine conventional security methods and traditional artificial intelligence\n(AI)-driven solutions. Generative AI (GAI) is a transformative approach that\ncan safeguard SAGIN security by synthesizing data, understanding semantics, and\nmaking autonomous decisions. This survey fills existing review gaps by\nexamining GAI-empowered secure communications across SAGINs. First, we\nintroduce secured SAGINs and highlight GAI's advantages over traditional AI for\nsecurity defenses. Then, we explain how GAI mitigates failures of authenticity,\nbreaches of confidentiality, tampering of integrity, and disruptions of\navailability across the physical, data link, and network layers of SAGINs.\nThree step-by-step tutorials discuss how to apply GAI to solve specific\nproblems using concrete methods, emphasizing its generative paradigm beyond\ntraditional AI. Finally, we outline open issues and future research directions,\nincluding lightweight deployment, adversarial robustness, and cross-domain\ngovernance, to provide major insights into GAI's role in shaping\nnext-generation SAGIN security.", "AI": {"tldr": "This paper surveys how generative AI (GAI) addresses security challenges in space-air-ground integrated networks (SAGINs) by enabling data synthesis, semantic understanding, and autonomous decision-making across network layers.", "motivation": "Traditional security methods and AI fail to adequately protect SAGINs due to their multidimensional heterogeneity, dynamic topologies, and vulnerabilities to authenticity failures, confidentiality breaches, integrity tampering, and availability disruptions.", "method": "The authors analyze GAI's capabilities through surveys and empirical studies, comparing it against traditional AI solutions. They provide three step-by-step tutorials demonstrating concrete methods for implementing GAI in SAGIN security defenses.", "result": "Demonstration of GAI's superiority in handling physical/data link/network layer security issues through data synthesis and generative modeling approaches, with specific frameworks for authenticity verification, confidentiality maintenance, and availability restoration.", "conclusion": "The paper identifies future research directions for GAI in SAGIN security including lightweight deployment optimization, adversarial training robustness, and cross-domain governance frameworks to enable practical next-generation secure communications."}}
{"id": "2508.01995", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.01995", "abs": "https://arxiv.org/abs/2508.01995", "authors": ["Sefatun-Noor Puspa", "Mashrur Chowdhury"], "title": "GPU in the Blind Spot: Overlooked Security Risks in Transportation", "comment": null, "summary": "Graphics processing units (GPUs) are becoming an essential part of the\nintelligent transportation system (ITS) for enabling video-based and artificial\nintelligence (AI) based applications. GPUs provide high-throughput and\nenergy-efficient computing for tasks like sensor fusion and roadside video\nanalytics. However, these GPUs are one of the most unmonitored components in\nterms of security. This makes them vulnerable to cyber and hardware attacks,\nincluding unauthorized crypto mining. This paper highlights GPU security as a\ncritical blind spot in transportation cybersecurity. To support this concern,\nit also presents a case study showing the impact of stealthy unauthorized\ncrypto miners on critical AI workloads, along with a detection strategy. We\nused a YOLOv8-based video processing pipeline running on an RTX 2060 GPU for\nthe case study. A multi-streaming application was executed while a T-Rex crypto\nminer ran in the background. We monitored how the miner degraded GPU\nperformance by reducing the frame rate and increasing power consumption, which\ncould be a serious concern for GPUs operating in autonomous vehicles or\nbattery-powered edge devices. We observed measurable impacts using GPU\ntelemetry (nvidia-smi) and Nsight Compute profiling, where frame rate dropped\nby 50 percent, and power usage increased by up to 90%. To detect, we trained\nlightweight classifiers using extracted telemetry features. All models achieved\nhigh accuracy, precision, recall, and F1-score. This paper raises urgent\nawareness about GPU observability gaps in ITS and offers a replicable framework\nfor detecting GPU misuse through on-device telemetry.", "AI": {"tldr": "This paper addresses GPU security vulnerabilities in intelligent transportation systems (ITS) by demonstrating how unauthorized crypto mining degrades AI workload performance and proposing a detection framework using telemetry data.", "motivation": "GPUs in ITS are critical for high-performance computing but remain largely unmonitored, leaving them exposed to cyberattacks and misuse like crypto mining that could compromise safety-critical applications.", "method": "The authors conducted a case study using a YOLOv8 video processing pipeline on an RTX 2060 GPU with simultaneous T-Rex crypto miner execution. They monitored performance shifts via GPU telemetry (nvidia-smi) and Nsight Compute profiling, then trained lightweight classification models on extracted telemetry features to detect misuse.", "result": "Unauthorized crypto mining caused a 50% frame rate drop and 90% power consumption increase during AI workloads. Trained classifiers achieved high accuracy, precision, recall, and F1-score in detecting these anomalies.", "conclusion": "The paper emphasizes the urgent need for improved GPU observability in ITS and presents a scalable framework for detecting GPU misuse through on-device telemetry, crucial for securing autonomous vehicles and edge devices."}}
{"id": "2508.01997", "categories": ["cs.CR", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.01997", "abs": "https://arxiv.org/abs/2508.01997", "authors": ["Hammad Atta", "Muhammad Zeeshan Baig", "Yasir Mehmood", "Nadeem Shahzad", "Ken Huang", "Muhammad Aziz Ul Haq", "Muhammad Awais", "Kamal Ahmed", "Anthony Green"], "title": "DIRF: A Framework for Digital Identity Protection and Clone Governance in Agentic AI Systems", "comment": null, "summary": "The rapid advancement and widespread adoption of generative artificial\nintelligence (AI) pose significant threats to the integrity of personal\nidentity, including digital cloning, sophisticated impersonation, and the\nunauthorized monetization of identity-related data. Mitigating these risks\nnecessitates the development of robust AI-generated content detection systems,\nenhanced legal frameworks, and ethical guidelines. This paper introduces the\nDigital Identity Rights Framework (DIRF), a structured security and governance\nmodel designed to protect behavioral, biometric, and personality-based digital\nlikeness attributes to address this critical need. Structured across nine\ndomains and 63 controls, DIRF integrates legal, technical, and hybrid\nenforcement mechanisms to secure digital identity consent, traceability, and\nmonetization. We present the architectural foundations, enforcement strategies,\nand key use cases supporting the need for a unified framework. This work aims\nto inform platform builders, legal entities, and regulators about the essential\ncontrols needed to enforce identity rights in AI-driven systems.", "AI": {"tldr": "This paper proposes the Digital Identity Rights Framework (DIRF), a comprehensive model to protect personal identity against generative AI threats through legal, technical, and hybrid enforcement mechanisms.", "motivation": "The proliferation of generative AI introduces risks such as unauthorized digital cloning, impersonation, and monetization of identity data, necessitating robust governance models.", "method": "DIRF is structured into nine domains with 63 controls, integrating legal frameworks, technical solutions (e.g., traceability), and hybrid mechanisms (e.g., consent enforcement) to secure identity rights.", "result": "The paper establishes architectural foundations, strategies, and use cases demonstrating the feasibility of a unified framework to address AI-driven identity exploitation.", "conclusion": "DIRF emphasizes the critical need for standardized controls to enforce digital identity rights in AI ecosystems, guiding platforms and regulators toward secure and ethical implementation."}}
{"id": "2508.02008", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02008", "abs": "https://arxiv.org/abs/2508.02008", "authors": ["Ali Alkinoon", "Trung Cuong Dang", "Ahod Alghuried", "Abdulaziz Alghamdi", "Soohyeon Choi", "Manar Mohaisen", "An Wang", "Saeed Salem", "David Mohaisen"], "title": "A Comprehensive Analysis of Evolving Permission Usage in Android Apps: Trends, Threats, and Ecosystem Insights", "comment": "16 pages, 6 figures, 14 tables. In submission to Journal of\n  Cybersecurity and Privacy", "summary": "The proper use of Android app permissions is crucial to the success and\nsecurity of these apps. Users must agree to permission requests when installing\nor running their apps. Despite official Android platform documentation on\nproper permission usage, there are still many cases of permission abuse. This\nstudy provides a comprehensive analysis of the Android permission landscape,\nhighlighting trends and patterns in permission requests across various\napplications from the Google Play Store. By distinguishing between benign and\nmalicious applications, we uncover developers' evolving strategies, with\nmalicious apps increasingly requesting fewer permissions to evade detection,\nwhile benign apps request more to enhance functionality. In addition to\nexamining permission trends across years and app features such as\nadvertisements, in-app purchases, content ratings, and app sizes, we leverage\nassociation rule mining using the FP-Growth algorithm. This allows us to\nuncover frequent permission combinations across the entire dataset, specific\nyears, and 16 app genres. The analysis reveals significant differences in\npermission usage patterns, providing a deeper understanding of co-occurring\npermissions and their implications for user privacy and app functionality. By\ncategorizing permissions into high-level semantic groups and examining their\napplication across distinct app categories, this study offers a structured\napproach to analyzing the dynamics within the Android ecosystem. The findings\nemphasize the importance of continuous monitoring, user education, and\nregulatory oversight to address permission misuse effectively.", "AI": {"tldr": "This study analyzes Android app permission trends, revealing that malicious apps now request fewer permissions to avoid detection while benign apps require more for functionality. Using FP-Growth association rule mining on Google Play data, it identifies co-occurring permission patterns across 16 app genres, emphasizing the need for improved monitoring and regulation.", "motivation": "Android app permissions are critical for security, but persistent abuse undermines user trust despite documented guidelines. Understanding evolving patterns helps address misuse.", "method": "1) Analyzed Google Play Store apps over time and by features (ads, purchases, etc.)\n2) Used FP-Growth algorithm for association rule mining on permission datasets\n3) Categorized permissions into semantic groups for contextual analysis", "result": "1) Malicious apps request fewer permissions (stealth strategy)\n2) Benign apps increasingly request more permissions (functionality demands)\n3) Identified genre-specific permission patterns and high-impact combinations\n4) Revealed significant temporal changes in permission usage trends", "conclusion": "Permission usage strategies are diverging between benign/malicious apps, creating new privacy challenges. The structured analysis approach enables better detection of abnormal patterns and reinforces the need for continuous ecosystem monitoring and user education programs."}}
{"id": "2508.02035", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02035", "abs": "https://arxiv.org/abs/2508.02035", "authors": ["Hiroki Nakano", "Takashi Koide", "Daiki Chiba"], "title": "PhishParrot: LLM-Driven Adaptive Crawling to Unveil Cloaked Phishing Sites", "comment": "Accepted for publication at IEEE GLOBECOM 2025", "summary": "Phishing attacks continue to evolve, with cloaking techniques posing a\nsignificant challenge to detection efforts. Cloaking allows attackers to\ndisplay phishing sites only to specific users while presenting legitimate pages\nto security crawlers, rendering traditional detection systems ineffective. This\nresearch proposes PhishParrot, a novel crawling environment optimization system\ndesigned to counter cloaking techniques. PhishParrot leverages the contextual\nanalysis capabilities of Large Language Models (LLMs) to identify potential\npatterns in crawling information, enabling the construction of optimal user\nprofiles capable of bypassing cloaking mechanisms. The system accumulates\ninformation on phishing sites collected from diverse environments. It then\nadapts browser settings and network configurations to match the attacker's\ntarget user conditions based on information extracted from similar cases. A\n21-day evaluation showed that PhishParrot improved detection accuracy by up to\n33.8% over standard analysis systems, yielding 91 distinct crawling\nenvironments for diverse conditions targeted by attackers. The findings confirm\nthat the combination of similar-case extraction and LLM-based context analysis\nis an effective approach for detecting cloaked phishing attacks.", "AI": {"tldr": "PhishParrot uses LLMs to create adaptive user profiles for detecting cloaked phishing sites, achieving a 33.8% accuracy improvement over 21 days.", "motivation": "Traditional phishing detection systems are ineffective against cloaking techniques, which hide malicious content from security crawlers while appearing legitimate to targets. This research addresses the urgent need for a more effective approach to detect such evasive attacks.", "method": "PhishParrot analyzes phishing site data with LLMs to identify cloaking patterns, constructs user profiles based on similar-case extraction, and dynamically adjusts browser/network configurations to mimic attacker-targeted conditions for optimal crawling.", "result": "The 21-day evaluation demonstrated 33.8% higher detection accuracy compared to standard systems, with 91 distinct crawling environments generated to expose cloaked content under various attack scenarios.", "conclusion": "PhishParrot's integration of LLM contextual analysis and adaptive crawling environments provides an effective solution for detecting cloaked phishing attacks, outperforming existing methods through targeted user profile simulation."}}
{"id": "2508.02092", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02092", "abs": "https://arxiv.org/abs/2508.02092", "authors": ["Shida Wang", "Chaohu Liu", "Yubo Wang", "Linli Xu"], "title": "FPEdit: Robust LLM Fingerprinting through Localized Knowledge Editing", "comment": null, "summary": "Large language models represent significant investments in computation, data,\nand engineering expertise, making them extraordinarily valuable intellectual\nassets. Nevertheless, these AI assets remain vulnerable to unauthorized\nredistribution and commercial exploitation through fine-tuning or black-box\ndeployment. Current fingerprinting approaches face a fundamental trade-off:\nintrinsic methods require full parameter access, while backdoor-based\ntechniques employ statistically anomalous triggers easily detected and filtered\nby adversaries. To address these limitations, we introduce FPEdit, a novel\nknowledge-editing framework that injects semantically coherent natural language\nfingerprints by modifying a sparse subset of model weights. This ensures\nstealthy and precise ownership encoding without degrading the core\nfunctionality. Extensive experiments show that FPEdit achieves $95$-$100\\%$\nfingerprint retention under both full-parameter fine-tuning and\nparameter-efficient adaptation, while preserving performance on 24 downstream\nbenchmarks. Moreover, FPEdit remains robust under quantization, pruning, and\nstochastic decoding, and can embed 10 fingerprint pairs into LLaMA2-7B in under\n10 minutes using less than 32 GB of GPU memory, a $70\\%$ reduction in resource\nrequirements compared to existing techniques. These advances establish FPEdit\nas the first fingerprinting approach to simultaneously achieve robustness\nagainst adaptation, resistance to detection, and preservation of model utility,\nproviding a minimally invasive solution for reliable provenance verification of\nlarge language models in adversarial deployment scenarios.", "AI": {"tldr": "FPEdit is a novel knowledge-editing framework that injects stealthy, semantically coherent natural language fingerprints into large language models by sparsely modifying model weights. It maintains 95-100% fingerprint retention while preserving performance on 24 benchmarks and reducing resource requirements by 70% compared to existing techniques. The method enables reliable provenance verification in adversarial scenarios like fine-tuning and black-box deployment.", "motivation": "Current LLM fingerprinting approaches struggle with either requiring full parameter access (intrinsic methods) or being vulnerable to detection via statistically anomalous triggers (backdoor techniques), leaving LLMs exposed to unauthorized redistribution and commercial exploitation.", "method": "FPEdit modifies a sparse subset of model weights (instead of all parameters) to inject semantically coherent natural language fingerprints. These fingerprints are 'aware' of the model's ownership information while maintaining functional performance through careful weight modifications.", "result": "1) 95-100% fingerprint retention after full-parameter fine-tuning or parameter-efficient adaptation (like LoRA)\n2) 70% reduction in resource requirements compared to existing fingerprinting methods\n3) Preserves performance on 24 downstream benchmarks\n4) Robust to quantization, pruning, and stochastic decoding\n5) Can embed 10 fingerprint pairs into LLaMA2-7B in <10 minutes with <32GB GPU memory", "conclusion": "FPEdit establishes the first fingerprinting approach for LLMs that simultaneously achieves robustness against model adaptations, resistance to detection, and preservation of model utility. It provides a practical solution for provenance verification in adversarial deployment environments with minimal computational overhead."}}
{"id": "2508.02115", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02115", "abs": "https://arxiv.org/abs/2508.02115", "authors": ["Wenjie Li", "Siying Gu", "Yiming Li", "Kangjie Chen", "Zhili Chen", "Tianwei Zhang", "Shu-Tao Xia", "Dacheng Tao"], "title": "Coward: Toward Practical Proactive Federated Backdoor Defense via Collision-based Watermark", "comment": "13-page main body and 4-page appendix", "summary": "Backdoor detection is currently the mainstream defense against backdoor\nattacks in federated learning (FL), where malicious clients upload poisoned\nupdates that compromise the global model and undermine the reliability of FL\ndeployments. Existing backdoor detection techniques fall into two categories,\nincluding passive and proactive ones, depending on whether the server\nproactively modifies the global model. However, both have inherent limitations\nin practice: passive defenses are vulnerable to common non-i.i.d. data\ndistributions and random participation of FL clients, whereas current proactive\ndefenses suffer inevitable out-of-distribution (OOD) bias because they rely on\nbackdoor co-existence effects. To address these issues, we introduce a new\nproactive defense, dubbed Coward, inspired by our discovery of multi-backdoor\ncollision effects, in which consecutively planted, distinct backdoors\nsignificantly suppress earlier ones. In general, we detect attackers by\nevaluating whether the server-injected, conflicting global watermark is erased\nduring local training rather than retained. Our method preserves the advantages\nof proactive defenses in handling data heterogeneity (\\ie, non-i.i.d. data)\nwhile mitigating the adverse impact of OOD bias through a revised detection\nmechanism. Extensive experiments on benchmark datasets confirm the\neffectiveness of Coward and its resilience to potential adaptive attacks. The\ncode for our method would be available at\nhttps://github.com/still2009/cowardFL.", "AI": {"tldr": "The paper introduces Coward, a proactive backdoor detection method in federated learning, which mitigates OOD bias by levering multi-backdoor collision effects, erasing conflicting watermarks during local training to identify attackers, and demonstrated resilience via experiments.", "motivation": "Existing backdoor detection in federated learning is limited: passive methods fail under data heterogeneity and client randomness, while proactive methods face OOD bias due to backdoor co-existence assumptions.", "method": "Coward uses multi-backdoor collision effects by injecting conflicting global watermarks, then detecting attackers based on watermark erasure during local training, while revising the detection mechanism to reduce OOD bias.", "result": "Extensive experiments show Coward effectively detects backdoors and resists adaptive attacks on benchmark datasets, with code available for verification.", "conclusion": "Coward addresses FL backdoor vulnerabilities by overcoming prior limitations through its collision-based detection mechanism, offering a robust proactive defense solution."}}
{"id": "2508.02116", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02116", "abs": "https://arxiv.org/abs/2508.02116", "authors": ["Chao Liu", "Zhezheng Zhu", "Hao Chen", "Zhe Chen", "Kaiwen Guo", "Penghao Wang", "Jun Luo"], "title": "SUAD: Solid-Channel Ultrasound Injection Attack and Defense to Voice Assistants", "comment": null, "summary": "As a versatile AI application, voice assistants (VAs) have become\nincreasingly popular, but are vulnerable to security threats. Attackers have\nproposed various inaudible attacks, but are limited by cost, distance, or LoS.\nTherefore, we propose \\name~Attack, a long-range, cross-barrier, and\ninterference-free inaudible voice attack via solid channels. We begin by\nthoroughly analyzing the dispersion effect in solid channels, revealing its\nunique impact on signal propagation. To avoid distortions in voice commands, we\ndesign a modular command generation model that parameterizes attack distance,\nvictim audio, and medium dispersion features to adapt to variations in the\nsolid-channel state. Additionally, we propose SUAD Defense, a universal defense\nthat uses ultrasonic perturbation signals to block inaudible voice attacks\n(IVAs) without impacting normal speech. Since the attack can occur at arbitrary\nfrequencies and times, we propose a training method that randomizes both time\nand frequency to generate perturbation signals that break ultrasonic commands.\nNotably, the perturbation signal is modulated to an inaudible frequency without\naffecting the functionality of voice commands for VAs. Experiments on six\nsmartphones have shown that SUAD Attack achieves activation success rates above\n89.8% and SUAD Defense blocks IVAs with success rates exceeding 98%.", "AI": {"tldr": "This paper proposes SUAD Attack, a long-range cross-barrier inaudible voice attack, and SUAD Defense, a defense mechanism using ultrasonic perturbations to block such attacks without affecting normal speech. Both methods demonstrate high success rates (89.8% activation, 98% blocking) across six smartphones.", "motivation": "Existing inaudible voice attacks suffer from limitations in cost, distance, or requiring line-of-sight. The paper aims to create an attack method that bypasses these constraints while developing an effective defense mechanism.", "method": "1)Analyzes solid-channel dispersion effects for long-range propagation\n2)Designs a modular command generation model with attack distance, victim audio, and medium dispersion parameters\n3)Develops SUAD Defense with time/frequency randomized ultrasonic perturbations modulated to inaudible frequencies", "result": "SUAD Attack achieves 89.8% activation success on smartphones. SUAD Defense blocks inaudible voice attacks (IVAs) with 98% success rate while causing no interference with normal audio commands.", "conclusion": "The paper demonstrates a new attack vector (SUAD Attack) that establishes long-range barriers-free inaudible voice command capabilities, and proposes SUAD Defense as an effective, non-intrusive defense mechanism against such attacks."}}
{"id": "2508.02145", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02145", "abs": "https://arxiv.org/abs/2508.02145", "authors": ["Dingding Wang", "Jianting He", "Siwei Wu", "Yajin Zhou", "Lei Wu", "Cong Wang"], "title": "The Dark Side of Upgrades: Uncovering Security Risks in Smart Contract Upgrades", "comment": null, "summary": "Smart contract upgrades are increasingly common due to their flexibility in\nmodifying deployed contracts, such as fixing bugs or adding new\nfunctionalities. Meanwhile, upgrades compromise the immutability of contracts,\nintroducing significant security concerns. While existing research has explored\nthe security impacts of contract upgrades, these studies are limited in\ncollection of upgrade behaviors and identification of insecurities.\n  To address these limitations, we conduct a comprehensive study on the\ninsecurities of upgrade behaviors. First, we build a dataset containing 83,085\nupgraded contracts and 20,902 upgrade chains. To our knowledge, this is the\nfirst large-scale dataset about upgrade behaviors, revealing their diversity\nand exposing gaps in public disclosure. Next, we develop a taxonomy of\ninsecurities based on 37 real-world security incidents, categorizing eight\ntypes of upgrade risks and providing the first complete view of upgrade-related\ninsecurities. Finally, we survey public awareness of these risks and existing\nmitigations. Our findings show that four types of security risks are overlooked\nby the public and lack mitigation measures. We detect these upgrade risks\nthrough a preliminary study, identifying 31,407 related issues - a finding that\nraises significant concerns.", "AI": {"tldr": "The paper studies smart contract upgrade insecurities by creating a large dataset of upgraded contracts and a taxonomy of 8 risk types from 37 real incidents, revealing 4 overlooked risks and 31,407 related issues.", "motivation": "Existing research on smart contract upgrades lacks comprehensive datasets and systematic identification of security risks, leading to unresolved gaps in understanding and mitigating upgrade-related insecurities.", "method": "1) Built a dataset of 83,085 upgraded contracts and 20,902 upgrade chains. 2) Developed a taxonomy based on 37 real-world incidents. 3) Conducted public awareness surveys and preliminary risk detection.", "result": "4 types of security risks are overlooked and lack mitigation; 31,407 upgrade-related issues detected, indicating significant real-world concerns.", "conclusion": "The study highlights critical gaps in both disclosure and mitigation of upgrade risks, urging further attention to the identified overlooked risks and proposing the dataset as a foundation for future research."}}
{"id": "2508.02188", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02188", "abs": "https://arxiv.org/abs/2508.02188", "authors": ["Kaibo Huang", "Yukun Wei", "WanSheng Wu", "Tianhua Zhang", "Zhongliang Yang", "Linna Zhou"], "title": "Whispering Agents: An event-driven covert communication protocol for the Internet of Agents", "comment": null, "summary": "The emergence of the Internet of Agents (IoA) introduces critical challenges\nfor communication privacy in sensitive, high-stakes domains. While standard\nAgent-to-Agent (A2A) protocols secure message content, they are not designed to\nprotect the act of communication itself, leaving agents vulnerable to\nsurveillance and traffic analysis. We find that the rich, event-driven nature\nof agent dialogues provides a powerful, yet untapped, medium for covert\ncommunication. To harness this potential, we introduce and formalize the Covert\nEvent Channel, the first unified model for agent covert communication driven by\nthree interconnected dimensions, which consist of the Storage, Timing,and\nBehavioral channels. Based on this model, we design and engineer {\\Pi}CCAP, a\nnovel protocol that operationalizes this event-driven paradigm. Our\ncomprehensive evaluation demonstrates that {\\Pi}CCAP achieves high capacity and\nrobustness while remaining imperceptible to powerful LLM-based wardens,\nestablishing its practical viability. By systematically engineering this\nchannel, our work provides the foundational understanding essential for\ndeveloping the next generation of monitoring systems and defensive protocols\nfor a secure and trustworthy IoA.", "AI": {"tldr": "This paper addresses the vulnerability of communication acts in the Internet of Agents (IoA) by introducing the Covert Event Channel model and the PiCCAP protocol, which enable imperceptible, high-capacity covert communication through event-driven agent dialogues to enhance privacy against surveillance and traffic analysis.", "motivation": "Standard Agent-to-Agent protocols secure message content but lack protections for the communication act itself, leaving agents vulnerable to sophisticated monitoring. Covert communication via event-driven dialogues is identified as an underexplored medium for privacy in distributed agent interactions.", "method": "The paper formalizes the Covert Event Channel with three dimensions: (1) Storage, (2) Timing, and (3) Behavioral channels. It builds PiCCAP, a protocol operationalizing this model by leveraging event-driven timing, behavior, and message storage patterns to embed covert communications.", "result": "Evaluation shows PiCCAP achieves robustness, high transmission capacity, and elusion of detection by advanced LLM-based wardens, demonstrating its effectiveness for real-world implementation in sensitive agent communication spaces.", "conclusion": "The structured engineering of covert event channels provides a foundational framework for developing future IoA monitoring systems and defensive protocols, enabling both proactive privacy measures and intelligent threat detection in agent interactions."}}
{"id": "2508.02312", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02312", "abs": "https://arxiv.org/abs/2508.02312", "authors": ["Kang Chen", "Xiuze Zhou", "Yuanguo Lin", "Jinhe Su", "Yuanhui Yu", "Li Shen", "Fan Lin"], "title": "A Survey on Data Security in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs), now a foundation in advancing natural language\nprocessing, power applications such as text generation, machine translation,\nand conversational systems. Despite their transformative potential, these\nmodels inherently rely on massive amounts of training data, often collected\nfrom diverse and uncurated sources, which exposes them to serious data security\nrisks. Harmful or malicious data can compromise model behavior, leading to\nissues such as toxic output, hallucinations, and vulnerabilities to threats\nsuch as prompt injection or data poisoning. As LLMs continue to be integrated\ninto critical real-world systems, understanding and addressing these\ndata-centric security risks is imperative to safeguard user trust and system\nreliability. This survey offers a comprehensive overview of the main data\nsecurity risks facing LLMs and reviews current defense strategies, including\nadversarial training, RLHF, and data augmentation. Additionally, we categorize\nand analyze relevant datasets used for assessing robustness and security across\ndifferent domains, providing guidance for future research. Finally, we\nhighlight key research directions that focus on secure model updates,\nexplainability-driven defenses, and effective governance frameworks, aiming to\npromote the safe and responsible development of LLM technology. This work aims\nto inform researchers, practitioners, and policymakers, driving progress toward\ndata security in LLMs.", "AI": {"tldr": "The paper examines data security risks in large language models (LLMs) and reviews defense strategies, datasets, and future research directions to promote safe development.", "motivation": "LLMs increasingly power critical systems but face vulnerabilities from uncurated training data, necessitating robust security solutions to maintain trust and reliability.", "method": "Comprehensive survey of data security risks, analysis of defense approaches like adversarial training and RLHF, and categorization of datasets used for security evaluation.", "result": "Catalogs security risks (toxic outputs, hallucinations, prompt injection), summarizes existing defenses and datasets, and identifies research gaps in secure updates and governance.", "conclusion": "The work provides guidance for researchers and policymakers to address data-centric security challenges and ensure responsible advancement of LLM technology."}}
{"id": "2508.02375", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02375", "abs": "https://arxiv.org/abs/2508.02375", "authors": ["Matthew Rodda", "Vasilios Mavroudis"], "title": "Analysis of Publicly Accessible Operational Technology and Associated Risks", "comment": null, "summary": "Operational Technology (OT) is an integral component of critical national\ninfrastructure, enabling automation and control in industries such as energy,\nmanufacturing, and transportation. However, OT networks, systems, and devices\nhave been designed and deployed prioritising functionality rather than\nsecurity. This leads to inherent vulnerabilities in many deployed systems when\noperational misconfigurations expose them to the internet. This report provides\nan up-to-date overview of the OT threat landscape exposed to the public\ninternet and studies the affected protocols, vendors, software, and the\ngeographic distribution of systems. Our findings reveal nearly 70,000 exposed\nOT devices globally, with significant concentrations in North America and\nEurope. Analysis of prevalent protocols (e.g., ModbusTCP, EtherNet/IP, S7)\nshows that many devices expose detailed identifying information, including\noutdated firmware versions with known critical vulnerabilities that remain\nunpatched for years after disclosure. Furthermore, we demonstrate how automated\nanalysis of screenshots can uncover exposed graphical interfaces of Human\nMachine Interfaces (HMIs) and Supervisory Control and Data Acquisition (SCADA)\nsystems, highlighting diverse pathways for potential unauthorized access and\nunderscoring the risks to industrial processes and critical infrastructure.", "AI": {"tldr": "The paper highlights security vulnerabilities in internet-exposed Operational Technology (OT) systems across industries, identifying 70,000 devices with outdated firmware and unpatched critical flaws, while demonstrating pathways for unauthorized access via exposed HMIs/SCADA systems.", "motivation": "OT systems in critical infrastructure (energy, manufacturing, transportation) have historically prioritized functionality over security, creating risks when operational misconfigurations leave them exposed to the internet.", "method": "The study conducted a global analysis of OT threat landscapes by examining exposed protocols (ModbusTCP, EtherNet/IP, S7), vendors, software, and geographic distribution. Automated screenshot analysis was used to detect publicly accessible graphical interfaces of HMIs and SCADA systems.", "result": "70,000 OT devices (primarily in North America/Europe) were found exposed, with many revealing unpatched firmware vulnerabilities. Automated analysis identified thousands of exposed HMI/SCADA interfaces, showcasing diverse unauthorized access pathways.", "conclusion": "The unaddressed vulnerabilities in internet-exposed OT systems represent a critical threat to industrial operations and national infrastructure, necessitating urgent mitigation strategies."}}
{"id": "2508.02438", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02438", "abs": "https://arxiv.org/abs/2508.02438", "authors": ["S M Mostaq Hossain", "Sheikh Ghafoor", "Kumar Yelamarthi", "Venkata Prasanth Yanambaka"], "title": "SoftPUF: a Software-Based Blockchain Framework using PUF and Machine Learning", "comment": "8 figures, 4 tables", "summary": "Physically Unclonable Function (PUF) offers a secure and lightweight\nalternative to traditional cryptography for authentication due to their unique\ndevice fingerprint. However, their dependence on specialized hardware hinders\ntheir adoption in diverse applications. This paper proposes a novel blockchain\nframework that leverages SoftPUF, a software-based approach mimicking PUF.\nSoftPUF addresses the hardware limitations of traditional PUF, enabling secure\nand efficient authentication for a broader range of devices within a blockchain\nnetwork. The framework utilizes a machine learning model trained on PUF data to\ngenerate unique, software-based keys for each device. These keys serve as\nsecure identifiers for authentication on the blockchain, eliminating the need\nfor dedicated hardware. This approach facilitates the integration of legacy\ndevices from various domains, including cloud-based solutions, into the\nblockchain network. Additionally, the framework incorporates well-established\ndefense mechanisms to ensure robust security against various attacks. This\ncombined approach paves the way for secure and scalable authentication in\ndiverse blockchain-based applications. Additionally, to ensure robust security,\nthe system incorporates well-established defense mechanisms against various\nattacks, including 51%, phishing, routing, and Sybil attacks, into the\nblockchain network. This combined approach paves the way for secure and\nefficient authentication in a wider range of blockchain-based applications.", "AI": {"tldr": "The paper presents a blockchain framework using SoftPUF, a software-based authentication method, to eliminate hardware dependency and enhance security in diverse blockchain applications.", "motivation": "Traditional Physically Unclonable Functions (PUFs) require specialized hardware, limiting their adoption across devices and applications.", "method": "A blockchain framework incorporates SoftPUF, which uses machine learning models trained on PUF data to generate software-based device keys. Defense mechanisms against 51%, phishing, routing, and Sybil attacks are integrated for robust security.", "result": "Secure, hardware-free authentication for a broader range of devices (including legacy and cloud-based systems) within blockchain networks while mitigating common attacks.", "conclusion": "The combined use of SoftPUF and blockchain-based security mechanisms enables scalable, efficient, and hardware-independent authentication for diverse blockchain applications."}}
{"id": "2508.02454", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02454", "abs": "https://arxiv.org/abs/2508.02454", "authors": ["Malvika Jadhav", "Wenxuan Bao", "Vincent Bindschaedler"], "title": "Thwart Me If You Can: An Empirical Analysis of Android Platform Armoring Against Stalkerware", "comment": "15 pages, 2 figures", "summary": "Stalkerware is a serious threat to individuals' privacy that is receiving\nincreased attention from the security and privacy research communities.\nExisting works have largely focused on studying leading stalkerware apps,\ndual-purpose apps, monetization of stalkerware, or the experience of survivors.\nHowever, there remains a need to understand potential defenses beyond the\ndetection-and-removal approach, which may not necessarily be effective in the\ncontext of stalkerware.\n  In this paper, we perform a systematic analysis of a large corpus of recent\nAndroid stalkerware apps. We combine multiple analysis techniques to quantify\nstalkerware behaviors and capabilities and how these evolved over time. Our\nprimary goal is understanding: how (and whether) recent Android platform\nchanges -- largely designed to improve user privacy -- have thwarted\nstalkerware functionality; how stalkerware may have adapted as a result; and\nwhat we may conclude about potential defenses. Our investigation reveals new\ninsights into tactics used by stalkerware and may inspire alternative defense\nstrategies.", "AI": {"tldr": "The paper systematically analyzes recent Android stalkerware apps to assess how platform privacy updates impact their functionality and adaptation, proposing alternative defense strategies beyond detection/removal.", "motivation": "Existing stalkerware research focuses on detection/removal approaches that may be ineffective due to the persistent and evolving nature of these threats.", "method": "Combined analysis of a large corpus of Android stalkerware apps using multiple techniques to quantify behaviors, capabilities, and temporal evolution of their tactics.", "result": "Identified stalkerware adaptations to Android privacy changes and provided insights into their evolving strategies for maintaining functionality.", "conclusion": "Android privacy improvements partially thwart stalkerware but necessitate new defensive approaches; understanding these adaptations can inform more effective privacy protections."}}
{"id": "2508.02461", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02461", "abs": "https://arxiv.org/abs/2508.02461", "authors": ["Abdullah Al Mamun", "Kyle Yates", "Antsa Rakotondrafara", "Mashrur Chowdhury", "Ryann Cartor", "Shuhong Gao"], "title": "Experimental Evaluation of Post-Quantum Homomorphic Encryption for Privacy-Preserving V2X Communication", "comment": "This version has been submitted to the TRB Annual Meeting 2026 and is\n  currently under review", "summary": "Intelligent Transportation Systems (ITS) fundamentally rely on\nvehicle-generated data for applications such as congestion monitoring and route\noptimization, making the preservation of user privacy a critical challenge.\nHomomorphic Encryption (HE) offers a promising solution by enabling computation\non encrypted data without revealing underlying content. This study presents the\nfirst real-world experimental evaluation of three post-quantum secure HE\nschemes, i.e., Brakerski-Fan-Vercauteren (BFV), Brakerski-Gentry-Vaikuntanathan\n(BGV), and Cheon-Kim-Kim-Song (CKKS), for vehicular communication scenarios.\nTwo representative privacy-preserving use cases are considered: encrypted\nvehicle counting and average speed aggregation. Experiments are conducted over\nboth Wi-Fi and Ethernet to assess performance under wireless and wired\nvehicle-to-everything (V2X) settings. Results show that BFV and BGV are\nsuitable for latency-tolerant applications such as intersection monitoring and\nregional traffic analysis, with total end-to-end latencies under 10 seconds.\nWhile CKKS experiences higher overhead, it remains viable for periodic\nencrypted aggregation of numerical data. The experimental results demonstrate\nthat HE can be feasibly deployed in ITS environments under 128-bit post-quantum\nsecurity, provided that scheme-specific latency constraints are considered.\nThis reinforces its potential to serve as a foundational tool for secure and\nprivacy-preserving V2X data processing.", "AI": {"tldr": "This paper evaluates three post-quantum HE schemes (BFV, BGV, CKKS) for ITS privacy in real-world V2X scenarios, showing their feasibility for encrypted data processing with latency constraints.", "motivation": "ITS requires vehicle data for operations like congestion monitoring but faces privacy challenges. HE enables computation on encrypted data, with post-quantum schemes being critical for future security.", "method": "The authors experimentally assess BFV, BGV, and CKKS in two privacy-preserving ITS use cases (encrypted vehicle counting and average speed aggregation) across Wi-Fi and Ethernet networks.", "result": "BFV/BGV achieved <10s latency for latency-tolerant ITS tasks, CKKS showed higher overhead but viability for periodic numerical aggregation, demonstrating HE's feasibility under 128-bit post-quantum security.", "conclusion": "HE can be practically deployed in ITS for secure V2X data processing if scheme-specific latency constraints are met, solidifying its role as a post-quantum privacy preservation foundation."}}
{"id": "2508.02476", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02476", "abs": "https://arxiv.org/abs/2508.02476", "authors": ["Kongxin Wang", "Jie Zhang", "Peigui Qi", "Kunsheng Tang", "Tianwei Zhang", "Wenbo Zhou"], "title": "PoseGuard: Pose-Guided Generation with Safety Guardrails", "comment": null, "summary": "Pose-guided video generation has become a powerful tool in creative\nindustries, exemplified by frameworks like Animate Anyone. However,\nconditioning generation on specific poses introduces serious risks, such as\nimpersonation, privacy violations, and NSFW content creation. To address these\nchallenges, we propose $\\textbf{PoseGuard}$, a safety alignment framework for\npose-guided generation. PoseGuard is designed to suppress unsafe generations by\ndegrading output quality when encountering malicious poses, while maintaining\nhigh-fidelity outputs for benign inputs. We categorize unsafe poses into three\nrepresentative types: discriminatory gestures such as kneeling or offensive\nsalutes, sexually suggestive poses that lead to NSFW content, and poses\nimitating copyrighted celebrity movements. PoseGuard employs a dual-objective\ntraining strategy combining generation fidelity with safety alignment, and uses\nLoRA-based fine-tuning for efficient, parameter-light updates. To ensure\nadaptability to evolving threats, PoseGuard supports pose-specific LoRA fusion,\nenabling flexible and modular updates when new unsafe poses are identified. We\nfurther demonstrate the generalizability of PoseGuard to facial landmark-guided\ngeneration. Extensive experiments validate that PoseGuard effectively blocks\nunsafe generations, maintains generation quality for benign inputs, and remains\nrobust against slight pose variations.", "AI": {"tldr": "PoseGuard is a safety alignment framework for pose-guided video generation that degrades output quality for malicious poses while preserving high-fidelity outputs for benign inputs, ensuring ethical and secure generation.", "motivation": "Pose-guided video generation risks malicious misuse through impersonation, privacy violations, and NSFW content creation, necessitating safeguards for safe deployment.", "method": "Dual-objective training combining generation fidelity (e.g., LoRA fine-tuning) and safety alignment, with pose-specific LoRA fusion to adaptively suppress unsafe generations (three categories: discriminatory gestures, NSFW poses, and copyrighted imitations).", "result": "Experiments show PoseGuard effectively blocks unsafe generations, maintains benign input quality, and remains robust to pose variations, with generalizability to facial landmark-guided generation demonstrated.", "conclusion": "PoseGuard provides an efficient, modular safety solution for pose-guided generation through parameter-light fine-tuning, balancing ethical alignment and output quality."}}
{"id": "2508.02523", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02523", "abs": "https://arxiv.org/abs/2508.02523", "authors": ["Ostonya Thomas", "Muhaimin Bin Munir", "Jean-Michel Tine", "Mizanur Rahman", "Yuchen Cai", "Khandakar Ashrafi Akbar", "Md Nahiyan Uddin", "Latifur Khan", "Trayce Hockstad", "Mashrur Chowdhury"], "title": "Transportation Cyber Incident Awareness through Generative AI-Based Incident Analysis and Retrieval-Augmented Question-Answering Systems", "comment": "This paper has been submitted to the Transportation Research Board\n  (TRB) for consideration for presentation at the 2026 Annual Meeting", "summary": "Technological advancements have revolutionized numerous industries, including\ntransportation. While digitalization, automation, and connectivity have\nenhanced safety and efficiency, they have also introduced new vulnerabilities.\nWith 95% of data breaches attributed to human error, promoting cybersecurity\nawareness in transportation is increasingly critical. Despite numerous\ncyberattacks on transportation systems worldwide, comprehensive and centralized\nrecords of these incidents remain scarce. To address this gap and enhance cyber\nawareness, this paper presents a large language model (LLM) based approach to\nextract and organize transportation related cyber incidents from publicly\navailable datasets. A key contribution of this work is the use of generative AI\nto transform unstructured, heterogeneous cyber incident data into structured\nformats. Incidents were sourced from the Center for Strategic & International\nStudies (CSIS) List of Significant Cyber Incidents, the University of Maryland\nCyber Events Database (UMCED), the European Repository of Cyber Incidents\n(EuRepoC), the Maritime Cyber Attack Database (MCAD), and the U.S. DOT\nTransportation Cybersecurity and Resiliency (TraCR) Examples of Cyber Attacks\nin Transportation (2018 to 2022). These were classified by a fine tuned LLM\ninto five transportation modes: aviation, maritime, rail, road, and multimodal,\nforming a transportation specific cyber incident database. Another key\ncontribution of this work is the development of a Retrieval Augmented\nGeneration question answering system, designed to enhance accessibility and\npractical use by enabling users to query the curated database for specific\ndetails on transportation related cyber incidents. By leveraging LLMs for both\ndata extraction and user interaction, this study contributes a novel,\naccessible tool for improving cybersecurity awareness in the transportation\nsector.", "AI": {"tldr": "This paper introduces an LLM-based approach to extract and classify transportation-related cyber incidents from public datasets into structured databases, while developing a Retrieval Augmented Generation (RAG) QA system to improve accessibility and cybersecurity awareness in the sector.", "motivation": "Citing 95% of data breaches as human error-driven and noting the scarcity of centralized records for transportation cyberattacks, the study emphasizes the urgent need to enhance cybersecurity awareness and actionable incident data accessibility in transportation systems.", "method": "The authors fine-tuned large language models to categorize cyber incidents into five transportation modes (aviation, maritime, rail, road, multimodal) from heterogeneous datasets across six public repositories. They also implemented a RAG-based question answering system for database interaction.", "result": "A transportation-specific cyber incident database was successfully created by classifying 2018-2022 incidents from CSIS, UMCED, EuRepoC, MCAD, and TraCR datasets. The RAG QA system demonstrated practical utility in enabling targeted queries across the curated database.", "conclusion": "The work establishes an innovative framework for transforming fragmented cyber incident data into a transport-sector database using generative AI, with the RAG system significantly improving data usability for cybersecurity awareness. This approach provides a scalable solution for incident monitoring and preparedness in critical transportation infrastructure."}}
{"id": "2508.02543", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02543", "abs": "https://arxiv.org/abs/2508.02543", "authors": ["Guillaume Quispe", "Pierre Jouvelot", "Gerard Memmi"], "title": "Nicknames for Group Signatures", "comment": "27 pages, 2 figures", "summary": "Nicknames for Group Signatures (NGS) is a new signature scheme that extends\nGroup Signatures (GS) with Signatures with Flexible Public Keys (SFPK). Via GS,\neach member of a group can sign messages on behalf of the group without\nrevealing his identity, except to a designated auditor. Via SFPK, anyone can\ncreate new identities for a particular user, enabling anonymous transfers with\nonly the intended recipient able to trace these new identities.\n  To prevent the potential abuses that this anonymity brings, NGS integrates\nflexible public keys into the GS framework to support auditable transfers. In\naddition to introducing NGS, we describe its security model and provide a\nmathematical construction proved secure in the Random Oracle Model. As a\npractical NGS use case, we build NickHat, a blockchain-based token-exchange\nprototype system on top of Ethereum.", "AI": {"tldr": "Nicknames for Group Signatures (NGS) integrates SFPK into GS to enable auditable anonymous transfers, demonstrated through the NickHat prototype on Ethereum.", "motivation": "existing group signatures lack flexible public keys for anonymous yet auditable transfers, necessitating a system where third parties can securely trace transactions without compromising privacy", "method": "mathematically constructs NGS by combining group signature anonymity with SFPK's flexible key generation, formalizing its security model within Random Oracle Model assumptions", "result": "successfully implemented NickHat (blockchain-based token-exchange system) that maintains message anonymity while preserving auditability for authorized parties", "conclusion": "NGS establishes a robust balance between user privacy and transfer accountability, proving its practical viability through Ethereum-based deployment"}}
{"id": "2508.02551", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02551", "abs": "https://arxiv.org/abs/2508.02551", "authors": ["Shafizur Rahman Seeam", "Ye Zheng", "Zhengxiong Li", "Yidan Hu"], "title": "PrivAR: Real-Time Privacy Protection for Location-Based Augmented Reality Applications", "comment": null, "summary": "Location-based augmented reality (LB-AR) applications, such as Pok\\'emon Go,\nstream sub-second GPS updates to deliver responsive and immersive user\nexperiences. However, this high-frequency location reporting introduces serious\nprivacy risks. Protecting privacy in LB-AR is significantly more challenging\nthan in traditional location-based services (LBS), as it demands real-time\nlocation protection with strong per-location and trajectory-level privacy\nguaranteed while maintaining low latency and high quality of service (QoS).\nExisting methods fail to meet these combined demands.\n  To fill the gap, we present PrivAR, the first client-side privacy framework\nfor real-time LB-AR. PrivAR introduces two lightweight mechanisms: (i) Planar\nStaircase Mechanism (PSM) which designs a staircase-shaped distribution to\ngenerate noisy location with strong per-location privacy and low expected\nerror; and (ii) Thresholded Reporting with PSM (TR-PSM), a selective scheme\nthat releases a noisy location update only when a displacement exceeds a\nprivate threshold, enabling many-to-one mappings for enhanced trace-level\nprivacy while preserving high QoS. We present theoretical analysis, extensive\nexperiments on two public datasets and our proprietary GeoTrace dataset, and\nvalidate PrivAR on a Pok\\'emon-Go-style prototype. Results show PrivAR improves\nQoS (Gamescore) by up to 50%, while increasing attacker error by 1.8x over\nbaseline with an additional 0.06 milliseconds runtime overhead.", "AI": {"tldr": "PrivAR is a client-side framework for real-time location-based AR applications that addresses privacy risks through two lightweight mechanisms (PSM and TR-PSM), achieving 50% better QoS and 1.8x higher attacker error with minimal overhead.", "motivation": "High-frequency location reporting in LB-AR introduces serious privacy risks due to real-time immersion demands, creating a unique challenge for privacy solutions to balance strong per-location/trajectory protection with low latency and high QoS.", "method": "1) Planar Staircase Mechanism (PSM): Staircase-shaped noise generation with expected error control\n2) Thresholded Reporting with PSM (TR-PSM): Selective location updates only when displacement exceeds private thresholds\nCombines differential privacy principles with many-to-one mapping strategies.", "result": "\u2022 50% improvement in QoS (GameScore) over alternatives\n\u2022 1.8x increase in attacker error\n\u2022 0.06ms runtime overhead on Pok\u00e9mon Go-style prototype\n\u2022 Validated through theoretical analysis, public datasets, and proprietary GeoTrace dataset experiments", "conclusion": "PrivAR successfully addresses the real-time privacy-QoS tradeoff in LB-AR applications by innovating both per-location and trajectory-level privacy protection, demonstrated through empirical validation on commercial benchmarks."}}
